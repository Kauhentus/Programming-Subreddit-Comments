I like Clojure just fine. I just hate Java.
No one suggested you should "check/see every day or even every month the shootout website looking for new things". Like I said - "Simply *check the facts* right after you decide to opine about the benchmarks game, **right before you do opine**." &gt; PS: I doubt there are many people consulting the website, especially for Lisp submissions, very often. These general *facts* are [shown on the website](http://shootout.alioth.debian.org/help.php#analytics) July 1, 2010 - Dec 31, 2010 - 158,886 Absolute Unique Visitors - 1,119,542 Pageviews - 4.83 Average Pageviews The specific *facts* for Lisp, are - 18,673 Unique Pageviews 2.07% (visits during which one or more pages showing just SBCL program source code or specifically comparing SBCL against just one other programming language implementation were viewed) 
Right... That's why there are many CL libs with cl- included in the system name. If the name of the wrapper is cl-&lt;libname&gt; it is more consistent to have the system named the same way. 
I keep hoping I'll find one on, say, eBay. But I think The Time Has Passed.
But that needlessly clusters all names in the cl- prefix, making all kinds of name completion that much harder to use. Personally I prefer my libraries to provide at least a nickname omitting the cl- part, precisely for that reason.
If you really want one, e-mail sales@symbolics-dks.com. David K. Schmidt maintains systems for actual current users, and usually has a stock of various models. I got a Macivory for about US$1500 a few years back. (I would stay away from any of the heavy metal 36xx ones, even though they are a bit cheaper.)
The process boundary thing hurts, however, when you try to use McCLIM, and all the beautiful presentation things you want to click on with your mouse are not in the address space of the application you are running. There is a cost *and* a benefit to single address space. Presentations that have to be pushed across process boundaries are not easy.
&gt; hardware read/write barriers Yes, but at a page granularity, and with no OS awareness as to why you have set up the barriers in the first place. Lisp machine hardware knew that mutations had to maintain GC invariants of every Lisp object and did it in microcode, and their paging microcode knew about the GC scanner and vice versa. 
Lisp actually has awesome bit-twiddling functions, based on the old PDP-10 LDB/DPB instructions. Able to deal with bytes of arbitrary numbers of bits.
Back this up or you're just trolling.
You said they "don't help much." The limitations you mentioned hardly imply that. Page granularity isn't a big deal. For a write barrier, it leads to some extra scanning at barrier hit (an entire page of objects instead of just one object). For a read barrier in an incremental collector, it means your minimum evacuation increment must be at least one page. A number of collectors (GHCs, RVM's Immix, SBCL's, OpenDylan's) use page-level granularity in the collector with no problem. As for not knowing why the barrier was there, I don't see why the hardware needs to know that. If paging and GC are both done in software, they can communicate with each other via the accessed/modified and free bits in the page table entries. x86 has 3 extra bits available for the OS's purposes. 
edit: yeah, never mind this angry rant.
Providing a nickname is a good option. Both could/should be supplied perhaps. 
And like I said: **learn how to read**. 
Dammit --- he's right. Of course if he used [paredit](http://mumble.net/~campbell/emacs/paredit.el) we wouldn't be in this situation. 
) ; I've got ya covered. We can all relax now.
((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((hi))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))) EDIT: wow... I guess people don't like this...
Common Lisp programmers have all defun 
NOTHING A READER MACRO CAN'T SOLVE. 
) ;; Turn on 'show-paren-mode goddamit
Same here. and if you search for "scheme", you break reddit too.
I always feel like a buzzkill when I go around defun-ing everything.
Huh, that's really weird. I guess we should press the admins...
And Scheme took all defun out from Lisp.
 (defmacro enfun (name (&amp;rest args) &amp;body body) `(defun ,name ,args ,@body))
I was going to make a shirt out of this, but the gradient unfortunately looks awful.
You could always open the image in Photoshop, fill in the lambda, select the text tool, type a new lambda with a solid color, and rotate it. 
I could also recompile the Lisp program I wrote to draw it to not have a gradient, but I think the gradient is important and I don't want to ditch it just to make a shirt look good.
For the curious, here's the hacked-together-in-15-minutes code for the graphic. (defun draw-lambda (font-size center-point) (with-graphics-state (set-fill-color *white*) (translate center-point) (let ((font (get-font (font-file "times"))) (little-lamb #(#x3bb))) (set-font (get-font (font-file "times")) font-size) (rotate-degrees 40) (let* ((box (string-bounding-box little-lamb font-size font)) (p (neg (centerpoint box))) (p1 *origin*) (p2 (xpoint (- (/ (width box) 2))))) (draw-string p little-lamb) (set-gradient-fill p1 (rgba-color 0 0 0 0) p2 (rgba-color 0 0 0 0.95)) (rectangle (displace box p)) (set-stroke-color (rgb-color 1 0 0)) (fill-path))))) (defun test (size output) (let ((canvas (box 0 0 size size))) (with-box-canvas canvas (set-fill-color *black*) (clear-canvas) (draw-lambda (* size 0.70) (add (ypoint (/ size 20)) (centerpoint canvas))) (set-font (get-font (font-file "futura")) (* size 0.16)) (set-fill-color *white*) (vecto:set-character-spacing 0.95) (draw-centered-string (point (/ size 2) (/ size 20)) "JUST DEFUN IT.") (save-png output)))) 
I believe you can still have useful presentations if the system mediates in handling them. The process separation doesn't have to look exactly the way it does on Unix; it's just one model, and not a particularly great one either. But the thing is, the "absolutely no boundaries whatsoever" is even less of a workable model, and that's precisely what LispM's had. And even if it was an acceptable tradeoff back in the single-user, rarely-networked LispM days, it is simply not something you can even begin to consider today, with the ubiquitous Internet, far-reaching integration between increasingly many systems and the omnipresent threats. We have learnt it too hard a way ever to go back to the days of innocent trustfulness. But there are ways out. The Singularity/Midori projects from MSFT Research show one way you can have safe multitasking without address space separation. The [W7 security kernel](http://fare.tunes.org/tmp/emergent/secureos.htm) is another, Lispier way to provide security guarantees you can reason algebraically about without imposing the kind of static structure found in Singularity. The shared/separated conundrum is actually something I'm very interested in, and I'm convinced it's the #1 obstacle towards building a truly modern LispM. I've been thinking on and off about ways to provide the incredible integration and manipulability of the original LispOSs, while also providing strong guarantees that make it impossible to break the system _if you don't intend to break it_. It's tricky, but I suspect it can be solved with the right combination of W7 and L4-style hierarchical address spaces. The #2 obstacle, a closely related one, which is necessary to overcome to build a _viable_ LispOS, is creating a mapping to allow non-Lisp applications to live and function within the confines of Lisp world. It's not feasible anymore to build everything ever from scratch, and in many instances, it's not desirable. There are many man-years of work invested in things such as GTK+, and it's actually harmful to try to rebuild them, because there are many crucial things, such as IME modules, accessibility infrastructure, etc., which rely on it _being_ GTK+, and not just something vaguely like it in that it also provides buttons and menus. So if a future LispOS is to be, it has to be built from day one to incorporate chunks of the old, non-Lisp world, and to do so sensibly. Perhaps the answer is to build on an actual L4 implementation, with something like L4Linux (or maybe a saner kernel like FreeBSD, or perhaps [L4/Darwin](http://ertos.nicta.com.au/software/darbat/)), lobotomised to serve purely as a drivers provider. This way we'd be able to use others' work on hardware support, because, let's face it, there isn't room for a fourth OS supported by hardware vendors, and even if there was, LispOS wouldn't be it. Then we could concentrate on building the actual computing environment to be great, without spending resources on recreating yet another, obscure variant of wheel.
Open source is when you have a license certified by OSI to be open source. And they have every right to define the term, because they created it.
Haha saw this one last night, and in fact there are two open parens to close (including the title). A lot of people on the xkcd forum were complaining about it, but how many of you guys actually feel weird when you don't see the close parens? I know I do, it really made me feel uneasy :|
Yeah this is probably the best approach, having the actual library named cl-fann and providing "fann" as a nickname.
And what would "the curious" use to run it? CLISP? Does it need other packages? 
Girls just want to have defun?
https://github.com/xach/vectometry , https://github.com/xach/geometry , dependencies, Times New Roman, and of course, Futura Black Condensed.
wouldn't an orphan ) be far more foreboding? it's like... well i guess *that's* over. whatever 'that' was.
No license files?
Nope.
open-source: (adj) open-source (of or relating to or being computer software for which the source code is freely available) http://wordnetweb.princeton.edu/perl/webwn?s=open-source As I said, there are a few definitions. "open" and "source" defined terms, and can be used together without referring to a made up notion of it. I accept that definition, and I just said that others exist. Go google around. Sometimes English permits more than one meaning.
I disagree, there are all kinds of high-level language features that are much faster if they have dedicated hardware to support them. I'm thinking specifically of garbage collection.
&gt; paredit My first thought.
This is FUD, even though the details aren't exactly wrong. Let's get the major thing: SBCL's GC is reasonably performant, and doesn't leak memory. Bits about SBCL's GC which aren't as good as they should be (if these are critical for _you_, then arse sucking is indeed imminent in your future, but it doesn't make SBCL's GC suck arse in general): 1. It's stop-the-world. Ie. if you have multiple threads running, all of them need to be stopped for garbage collection to run. 2. It's generational -- which is a mixed bag. For _many_ use-cases generational GCs are _great_: consing tons of garbage is nearly free. For some use cases, generational GC is performance poison: if you have lots of long-living boxed objects that keep getting dirtied... blech. It's bad enough that it's worth jumping hoops through to make those objects represented in a non-boxed format. 3. It's conservative on stack and registers. (Not like Boehm which is conservative on heap as well.) In some cases this can lead to things not being collected ASAP, which can prevent you from using eg. some finalizer tricks. Finalizers work, but because of this you can't use them for some purposes for which C++ destructors could be used, etc. As for the specific claims: The linked bug exists, and hasn't been fixed yet, yep. (Though I can spare some cycles on it later this weekend, I hope.) To be affected by this you have to be doing pretty interesting CLOS programming, though: interning new EQL-specializers at runtime definitely isn't typical. There are some other places (caches) in CLOS which can leak memory due to "untypical" runtime CLOS usage -- they're bugs too, yes. These CLOS issues are completely unrelated to GC or memory management, however. Seriously. They're application bugs -- the application just happens to be CLOS. Caching in CLOS isn't "stupid and complex", it's the way CLOS has pretty much been designed to be implemented. CLOS is complex. CLOS has some design issues that don't mesh with modern SMP needs all that well. SBCL's CLOS in particular (PCL) has some issues of its own related to its heritage as a portable CLOS implementation. None of this makes it a "bad CLOS" -- as far as I know it is one of the most performant ones, and has top-notch MOP support. As for "retarted, futile, and a waste of time and done in the wrong fastion or order to being with" thread-safety work... too bad you don't have any details. Things that have already been fixed have already been fixed, so we can't go back and fix them in the right order. I don't seriously know what you think we should have done instead -- let show-stopper bugs remain till we reach some idealized state of simplicity? If you find issues with way things have been done, please let us know. This generalized vague FUD bitching on reddit is demotivating as hell. I've spent thousands of hours of work on SBCL, some of it funded (which also accounts for order in which are done to a degree, you know), a lot of it not. If there is one thing that makes me reconsider taking my toys and finding better paying work it's shit like this. The reason I haven't is that SBCL hackers and lot of the associated people are some of the smartest and nicest folks I know. If I sound ticked off, it's because I am. ...but give me a couple of hours and I'll be calm again, and if you have SBCL issues I'll be happy to talk about them without reference to this rant. 'cos I'm just nice that way. -- Nikodemus Siivola, SBCL bitterdev /end rant
Is it bad that my first thought was "maybe there's a super-paren he's not seeing?"
No, the time for easily acquiring a 1980's Lisp machine.
Why is the gradient 'important'? Does it signify something I'm not grokking? I'd place an order right now, if you used a quality t-shirt in 2X-Tall. Every time I see a Lisp t-shirt I want to buy, they don't make them in tall.
ahhh just like enbugging (sometimes known as coding) and debugging (mostly known as making it work) 
It might look better with the words above the tau (which is how I saw your lambda the first many times).
often known as encoding and decoding
"Girls just want all de-fun" matches the rhythm better.
pasting your code on a pastebin-like website would help: http://pastebin.com/ Your known-command can easily be rewritten to a member-test and a list. Your loop can be rewritten to (loop for line = (readline ...)) instead of using setf. In general that loop can use some more readability. You can replace (cond ((eq mode 'symbol)) to a case.
* don't import CL-USER, it is unspecified what CL-USER exports if anything at all * don't compare numbers with EQ, use EQL or = * (when (some-predicate-p) t) is just (some-predicate-p) for most purposes. * (when (not (null foo)) ...) is (when foo ... ) * The COND is completely misguided. Use (member "foocmd" command-names :test #'string-equal) * (COND ((eq foo 'begin) ...) ...) is (case foo (begin ...) ...) 
 (let ((foo nil) (bar 1)) ...) Can be written as (let (foo (bar 1)) ... For navigation I suggest getting lisp to output an html file and load that up. You can make emacs do everything, but it'd take way more time. 
Thanks so far, most of the issues were obvious but still I didn't come up immediately, need more practice. Here's the latest version: http://pastebin.com/Abjingij I think it's a good idea to keep the filtered output as Lisp code in order to work with it later easily with elisp.
Did you ever get slime working in emacs? That's what really sold it for me. Well, that plus I found the basic navigation to be pretty straightforward, C-n for next line, etc. I'm still pretty clueless as far as a lot of the deep customization settings go, but haven't found it necessary to know them either. As for auto-tab and the like, I found the lisp-mode tab settings to be perfectly acceptable... I've not really seen (common) lisp code formatted differently. Anyway, worth it for slime and org mode.
Just added some.
Cool. Thanks, Xach.
I have been playing with djula for a bit now. It's a working clone of Django templates, whatever that is. It's not advertised anywhere, but there is plenty of documentation buried inside the code, and there is an exported function to generate it. http://groups.google.com/group/cl-terrace/web/djula (djula:build-html-documentation-from-source #P"/tmp/djula-docs.html") this might come handy for evaluating templates in the repl without having to load from a separate file: (defun djul (str-input) (funcall (djula::compile-template-string str-input :template-path nil :return-format :string))) 
I would love to understand the idea behind webblocks.
Learn Lisp quickly I will.
I quickly Lisp learn will.
I have evaluated them over a few weeks, and the #1 underlying principle of Weblocks and UncommonWeb is to NOT BUILD ON WIN32. It's very simple really, if we think in continuation-passing style, what weblocks does is pass its current context to a previous continuation which REFUSES to build on Windows. Implementing sessions for http over continuations is very elegant: when you have heap-allocated activation records and environments, it's very efficient to to call SOFTWARE-TYPE to determine the OS type, then refuse to build if #+(or windows win32) I have learned a lot about the web while trying to install weblocks: mind=blown. 
"If Lisp were invented by the Japanese" except that they invented Forth instead.... What? If Japanese invented Lisp then it would look like Lisp, or else it would not be Lisp. It's like saying if English speaker invented Forth (which is also true in reality) then Forth would be prefixed like Lisp. No, it would not.
Mu. This whole discussion is silly, IMHO. Forth was designed from the start as being a stack based language and its notation makes sense given that origin: values are put onto the stack followed by the operation. This is no different from using your HP-11C or other favourite RPN calculator. Lisp's entire paradigm is different: it's fundamental data structure is the list, the only approach to function application that makes sense is through prefix notation. This whole discussion of "programming language syntax is driven by human language word order" is stupid: what the fuck language were Haskell's developers speaking? 
I do a lot of the templating client-side with javascript. CL serves the json content and Apache the static files (more efficient this way). I either use jquery to build the DOM nodes or use Uize's javascript templating, but there are a lot of client-side templating systems available. Communication btn client and CL is done via jquery ajax queries, where server queries are simple HTTP Gets or Puts, and return data is in json format. By using this structure the web pages are more dynamic and don't cause page flash when updated. This also puts less of a burden on the CL server and response time is a bit faster since apache is able to serve a large part of the load directly.
It would be called Ruby?
Lisp wasn't invented, it was discovered! :P
Haskell? What about Perl and Perl 6, then? The authors vomited ASCII art or something?
&gt;I'd also advise to use defparameter rather than defconstant, in general. Osborn's Law: Variables won't; constants aren't.
&gt; I have evaluated them over a few weeks, and the #1 underlying principle of Weblocks and UncommonWeb is to NOT BUILD ON WIN32. UCW does not inherently not work on Windows. None of us working on it actually have Windows machines AFAIK so there's that... SBCL's thread support on Windows is atrocious, but UCW ought to work fine on Windows with CCL and the `:httpd` backend (not sure about the IOLib backend mostly because I don't see why IOLib would even work on Windows in the first place). Patches are welcome if this is not true.
&gt; what the fuck language were Haskell's developers speaking? Mathematics bar parenthesis? :-)
Cons cells are what makes Lisp Lisp.
No they aint. You could design a Lisp without cons cells at all. It it had S-expression syntax, macros, it's a lisp of sorts. Even first-class functions and anonymous functions are more important to "lispiness" than conses. 
And I thought a `(map (curry regexp-split ",") (regexp-split "\n" data))` would suffice.
I find the code pretty hard to read due to the magic numbers and density. It would be interesting to see a macro or two that allowed you to lay this out in a concise but more readable way using readable labels for the states.
And as he notes in his own comment: Zach (of Planet Lisp) asked me if I’d looked at pre-existing libraries for CSV parsing. I actually hadn’t. Here are the couple I found: Fare http://common-lisp.net/gitweb?p=users/frideau/fare-csv.git. csv-parser http://members.optusnet.com.au/apicard/csv-parser.lisp
Two issues: \n means native platform newline. CSV can have CRLF, LF or CR line endings. This code can handle all three. Secondly, fields that contain "," or newlines are wrapped in double-quotes -- and within that a double double-quote mean 1 double-quote. This code handles that as well.
 grammatical conjunction, three words, two words, "possibly multiline quote" and, what would that, do in, "a situation, like this?" yet, parsley router snap, it's transform, "Say 'what' again, I dare you, \nI double dare you motherfucker, say what \none more Goddamn time!" For simple data, your solution is exactly what I'd do, but it wouldn't work for arbitrary inputs.
Yes, there are two sets of magic numbers used internally within the function. There are the states, and the character classes. I'll update the post to better describe them. 
I know you've been looking at RESTAS and archimag also either authors or uses cl-closure-template which is hiding on Google Code somewhere. Have you had a look and do you know how it compares? I'm leaning towards using that for a personal project as opposed to revitalizing Djula which appears long abandoned. It may also be worth investigating Yaclml which, IIRC, has some templating support: http://www.3ofcoins.net/2010/01/21/yaclml-in-pictures-part-ii-templating/
TL;DR: &gt; The Lisp mode borrows heavily from Lisp and Scheme. &gt; func(...), not (func ...) &gt; infix arithmetics &gt; x++ x-- &gt; No `begin`, `quote`, `cond`, `let`/`letrec`, `lambda`
I think this implementation is too low-level. Here's how I would go about writing a CSV line parser using [esrap](https://github.com/nikodemus/esrap): A line is a sequence of fields or whitespace: (defrule line (or fields whitespace)) A sequence of fields is a field and the next fields: (defrule fields (and field (* next-field)) (:destructure (first rest) (cons first rest))) A next field is a field following a comma: (defrule next-field (and #\, field) (:function second)) A field is either a complex value or a simple value, surrounded by whitespace: (defrule field (and whitespace (or complex-value simple-value) whitespace) (:function second)) Whitespace is spaces and tabs: (defrule whitespace (* (or #\Space #\Tab)) (:constant nil)) A simple value is a sequence of simple characters: (defrule simple-value (* simple-char) (:concat t)) A simple character is one that is not a comma, a quote, or whitespace: (defun simple-char-p (char) (not (member char '(#\, #\' #\Space #\Tab)))) (defrule simple-char (simple-char-p character)) A complex value is a sequence of complex characters surrounded by quotes: (defun concat-second (list) (concat (second list))) (defrule complex-value (and #\' (* complex-char) #\') (:function concat-second)) A complex character is either a simple character, a comma, a space, a tab, or a quotes pair: (defrule complex-char (or simple-char #\, #\Space #\Tab quotes-pair)) A quotes pair is a quote followed by a quote: (defrule quotes-pair (and #\' #\') (:constant #\')) Now we can parse a CSV line: CSV&gt; (parse 'line " hello , '''quick &amp; dirty'', according to some' , world ") ("hello" "'quick &amp; dirty', according to some" "world") NIL CSV&gt; 
I would love the ability to concat, star and union these together.
I solved this exercise in a couple of exercises on my [Programming Praxis](http://programmingpraxis.com) blog. [Here](http://programmingpraxis.com/2009/03/17/comma-separated-values/)‘s a function to read csv records, and [here](http://programmingpraxis.com/2010/10/22/text-file-databases-part-2/)‘s a framework for reading fielded input files, including csv and other record types. Code in Scheme, but should translate easily to Lisp.
How about using a regular expression that matches one field and just iterate through, one field at a time?
I thought it was TL;DR; Stop re-inventing Lisp poorly. I started thinking this at about &gt; lambda — This is just too advanced. Mere mortals can define a named procedure like anyone else would. I am slow though; really I should have picked up at &gt; Anything in my language is, by definition, reasonable. Anything omitted is naturally unreasonable.
Brilliant satire.
Glad I'm not the only one who thought this after chewing it over a bit. The blog posts' comments are full of people seeming to take this as a serious language proposal.
&gt;lambda — This is just too advanced. Mere mortals can define a named procedure like anyone else would. Yup, stopped reading here. har har har.
[ ] You (yes, you) know who Louis Reasoner is.
list quasi quotation generalized to XML (and others). coalesces constant parts at compile time into write-sequence calls on utf-8 encoded literal byte arrays. the current page is pretty empty: http://dwim.hu/project/hu.dwim.quasi-quote but the old page has examples: http://common-lisp.net/project/cl-quasi-quote/present-class.html http://common-lisp.net/project/cl-quasi-quote/index-old.shtml 
SICP!
The regular expression that can correctly parse a CSV field would be very complicated if not outright impossible. Here's a list of examples you'd have to be able to match: - field, *the default* - multi word field, *nothing fancy* - "quoted field", *still nothing fancy* - "quoted field, with a comma", *warming up* - "quoted field, with a comma \n and a line-break", *kind of tricky...* - "quoted field, with a comma \n\r and different line-break", *not much harder than the last one* - "quoted field, with multiple \n commas, \n line-breaks \n, and ""escaped, quotes\n"" inside of the quote", *umm...* According to the CSV standard, each of the above (minus my italicized commentary) is a valid single field. Like I said below, for simple files, the split/regex approach works well, but it probably can't handle arbitrary valid inputs.
I'm not sure the discussion is entirely silly. Lisp and forth share much of the same spirit, and even if this particular foray into the discussion is somewhat poorly motivated, a full [comparison of the languages](http://www.c2.com/cgi/wiki?ForthVsLisp) is very instructive. And things get even more interesting when you include languages which are concatenative but otherwise are very much like Lisp, such as [Factor](http://factorcode.org/). I use Lisp much more often than I use cancatenative languages (this is mostly because there aren't many places in my life where I need to use a concatenative language, but I am always using emacs), and perhaps I ultimately prefer writing code in a language which has explicit parameter passing (it is very interesting, though, that even in languages like factor, where stack combinators simplify words that might otherwise require lots of stack shuffling, very simple words are often the hardest to implement, while complex words become almost uncannily clean looking), but spending some time really thinking about concatenative style was for me very educational and fun. So even though this little piece is a bit glib and misinformed, I think it does start a nice discussion.
I certainly agree that comparing (programming) languages can be instructive and interesting. But assuming some kind of Whorfian influence or cross-polination between human and computer languages strikes me as silly.
Well, at least the cross pollination thing, I might agree with. But its pretty clear that Whorfian-type effects are operative _within_ computer languages. Different languages enforce different styles.
&gt; But its pretty clear that Whorfian-type effects are operative within computer languages. Different languages enforce different styles. Absolutely: I agree 100%.
Lets have a giant agreeing party! UPVOTES FOR ALL!
**Guido van Rossum** begs to differ.
And the more important information: Guile has a new vectorized logo! (No no, I do'nt say that because I'm the one who did it ;-p).
I learned the technique from 'Finite state machines in Forth' (http://www.forth.org/literature/noble.html). Towards the end he talks about how you can combine multiple state machines (as well as make non-deterministic choices).
I tested the following in Javascript and it seems to do pretty well. /^ *("([^"]|"")+"|[^",\n\r]+) *(,|$)/ 
I'm impressed. Looks like that would actually do the right thing with a US/UK CSV file as long as you could assume no empty entries and knew the width of a row. `input.scan(/ *("([^"]|"")+"|[^",\n\r]+) *(,|$)/).map{|r| r[0]}` returned an accurate list of fields (I'm using Ruby rather than JS, obviously). It skips empty fields (whether they're represented as `,,` or `,"",`), and you'd need to do some regex templating to support other value separators (and I'm likely missing some other possible input), but that's still dangerously close to a CSV-parsing regex.
The general idea is: if you can use a DFA, then you can use a regex; and the regex will probably be more concise. At least, that's true when you're matching strings. It'd be easy enough to adjust the regex to match nonempty fields only. Or you could just identify empty fields separately, which would probably make for more readable code and better error handling.
Sonic Boom?
ABCL coordination page: &lt;http://trac.common-lisp.net/armedbear/wiki/GSoC2011&gt;.
Are they still planing to integrate it with emacs? Being able to write extensions in JavaScript and Lua as well as a better lisp complier would be really nice.
There's arguably a performance advantage. Since the function namespace never has non-function values in it, you don't have to perform a type-check on typical function calls. However, I have read that some scheme implementations recover this advantage by giving bindings a function slot that is used when a variable is used in a call; for non-function values, the function slot always refers to a function that raises a type error. edit: Here's a link to the source where I learned about the Scheme technique (PDF): http://www.cs.indiana.edu/~dyb/pubs/hocs-talk.pdf
&gt; It is nonetheless supremely ironic that the demise of Lisp at JPL was ultimately due in no small measure to the unreliability of a C program.
Oh, you also read that?
No variables named `lst`. Whether this is worth `#'` and `funcall` everywhere is debatable. I, personally, like to be able to use the same word as both noun and verb, as in (`let ((list (list ...))) ...`), but would probably write more functional code if it were syntactically cleaner. 
If you're using Arnesi already, there is one in there as well. 
Yes. And I considered it key enough to the point of the article that it was worth quoting here (I probably should have quoted the entire postscript; it's a nice enough summary).
A simpler hack is to convert non-function values to closures (lambda () x); no need to worry about non-functions again. 
I would too, but to be honest, I'm used to having my CARs simple and uncluttered. The left-most item in a list is almost always a simple expression, so it helps the mind establish intent quickly, then move on scanning the rest (literally ;-) 
Also: Get off my lawn.
Don't forget TCO! Guaranteed, good, delicious TCO.
And take your goddamn semicolons with you!
There are implications for macro variable capture. There's a reason why hygienic macros are a bigger deal in Scheme than in Common Lisp.
Do you think allocating shitloads of closures will speed up things?
http://www.nhplace.com/kent/Papers/Technical-Issues.html
I read lots of these kinds of stories and they all seem to have the same (rather irksome) trait in common: &gt; I attribute this success to a large extent to the fact that I was using Lisp while most of the other teams were using C. Anecdotal evidence is of no use to anyone. You don't think that I can't find hundreds (if not thousands) of C, C++, or java programmers that would say the same thing about _insert project name here_? Indeed, I'll bet that you can easily find hundreds of programmers who say things like _it's a good think that we're not using lisp or we'd be really screwed_. You think that writing the program in lisp helped. Good for you. Unless you can demonstrate why, nobody else cares. Sorry for the rant, but I'm getting tired of these _the world would be a better place if people would just realize how great lisp is_ stories.
**HELL *YES***.
This.
code is data
&gt; Unless you can demonstrate why, nobody else cares. He gave some examples of projects that succeeded extremely well due, in large part, to the fact that they were built on Lisp... He links to an epic bug hunt in the article - it's an interesting read - but mentions something far more important: the value of a REPL in a *remote* debugging scenario on highly valuable non-serviceable equipment... &gt; I'm getting tired of these the world would be a better place if people would just realize how great lisp is stories. Have you ever heard the saying "choose the right tool for the job"? The author made a fairly well-reasoned case for why LISP is the proverbial 'right tool for the job' for JPL. There are a handfull of industries where some of the benefits of LISP (and ADA, Eiffel, and others), are worth millions of dollars each year. Some of JPL's projects seem to be a perfect fit. 
The opposing viewpoint also can't be proven, so sadly the anecdotal evidence is all we've got to go on in this particular situation. An experiment that could prove anything conclusively would involve a team of programmers falling in love with and spending several years of training with Blub, writing and maintaining an entire large project in Blub, using a time machine to go back to the beginning of the training, loving/training with Bleeb this time, writing/maintaining the same project in Bleeb and then comparing notes between timelines. Repeat for each pair of languages you'd like people to shut up about. Doing it with two teams working on similar projects doesn't cut it because of the variance in skill/temperament/etc between developers. Doing it with the same team working on similar projects sequentially doesn't cut it because they (hopefully) learned something language agnostic from the first implementation, and they might be biased towards a specific language. Having the same team do two different projects with different languages isn't a good test either because the project variances can stack up, and there's the language bias problem again. Observing real world situations only really teaches you which languages have good marketing and which are just good enough to do production work (it won't help you decide which are better). As it stands, all you've got is a pile of anecdotal evidence from people who don't like Lisp, and another (disproportionately large given the relative population of the Lisp community) pile of anecdotal evidence from people who do like Lisp. The possible conclusions are 1. The Lisp community is delusional/full of blowhards. 2. Lisp is so much better that its supporters don't care how badly they're outnumbered. 3. Language goodness is subjective to at least some degree, and each human is best suited to thinking in a specific language or group of languages. Sorry about **my** rant, but I'm getting pretty tired of people saying *"Yea, prove unto me that your god is superior to mine, and I shall mend my wicked ways"*. It's a ridiculous notion on its face, for so many reasons.
Without a way to know how the project would have turned out in another language, we just have to make do with the programmer's evaluation. &gt; You don't think that I can't find hundreds (if not thousands) of C, C++, or java programmers that would say the same thing about insert project name here? That's only relevant to the languages they're comparing. Probably not Lisp. &gt; Indeed, I'll bet that you can easily find hundreds of programmers who say things like *it's a good think that we're not using lisp or we'd be really screwed.* Programmers with the Lisp experience to make that judgement?
&gt; He gave some examples of projects that succeeded extremely well due, in large part, to the fact that they were built on Lisp... No, he gave examples of systems that he _thinks_ did well because they were built on lisp. But, of course, he doesn't really know. If you can point me to something, anything, that is more empirical, please do. I'd absolutely love to see it. 
&gt; Sorry about my rant, but I'm getting pretty tired of people saying "Yea, prove unto me that your god is superior to mine, and I shall mend my wicked ways". It's a ridiculous notion on its face, for so many reasons. Sigh... If you think that this is what I've said, then I'm afraid that you've completely misunderstood my post. 
&gt; Without a way to know how the project would have turned out in another language, we just have to make do with the programmer's evaluation. _Case studies_ and _Lessons learned_ are legitimate techniques for evaluating the development of software systems. Sure, they are not as reliable as controlled studies, but controlled studies are impossible in these kinds of investigation. &gt; That's only relevant to the languages they're comparing. Probably not Lisp. You seem to have missed my point. One person providing anecdotal evidence about another programming language is the same as a different person providing anecdotal evidence about lisp. &gt; Programmers with the Lisp experience to make that judgement? Possibly. Personally, I like lisp, but there are people who have experience with lisp that don't want to write everything in lisp.
Haskell does it. Oh, wait.
Obviously 1 man isn't going to be in a position to do controlled tests of multi-hundred-million dollar projects comparing multiple languages against one another... It is extra challenging to investigate honestly as the macro functionality of LISP (and other macro supporting languages) would skew the results wildly based not just on programmer proficiency, but due to how long the organization has been involved working in the problem domain in LISP. I'm sure you're aware with the difficulties of empirical language comparisons (individual programmer productivity itself being quite difficult to control for), and the near total lack of research available that directly researches concrete software engineering practices, much less themes as broad as language selection, due to these difficulties. Neither the author, nor myself, was making the claim that they could conclusively *prove* that any language is superior in an absolute sense to another. Given the unique problem spaces that were under discussion such a claim would be laughable. That being said: Yeah, the author 'thinks' the systems did well, due in part to being built on LISP. The internal accolades and awards heaped on the systems, the special features of LISP being used to save at least one mission, and the reuse of the resulting system code, all support his statement. These experiences are echoed among many people who work in similar problem domains - namely handling dynamic situations, with a wide number of variables and use cases elegantly - are also quite supportive of his claim... Skirting an 'appeal to authority': there's good reason, particularly in the 60's through 80's, where LISP was the best-of-breed language for tackling certain problems. Are we trying to say that all the language researchers, robot builders, AI programmers, and dynamic system developers who chose LISP repeatedly for certain problem domains were... what? misguided? clueless? uninformed? Honestly it sounds like you haven't used LISP enough to grok why it is well suited to problem domains such as the one the author was describing. Today we have much better alternatives, but at the time the contrast was quite stark. If relying on empirical studies is the only way you are able to make decisions between technologies: good luck in the industry ;) 
&gt; Anecdotal evidence is of no use to anyone. Really? You do not do anything until you get evidence with a known statistical accuracy? Your life must suck then. If you've bought two kind of apples and you find kind1 vastly superior to kind2 to your test, would you continue to buy both of them until you'll have statistical confidence that kind1 is better? There is a lot of things where it is hard or impossible to collect statistical evidence because it is unique, one of a kind events. But experience of people who work with them is clearly better than nothing. You need to understand that statistical evidence does not aim to explain phenomena themselves, it merely confirms that our understanding is correct (or it is not). &gt; Sorry for the rant, but I'm getting tired of these the world would be a better place if people would just realize how great lisp is stories. Really? There are many of them? Like this one and the [other guy's](http://www.paulgraham.com/avg.html)? I find it ironic that the very same Erann Gat (aka Ron Garret) guy made that famous survey which proves that Lisp is better than Java statistically, and it is hosted on this very same site: http://www.flownet.com/gat/papers/lisp-java.pdf If you were ever interested in Lisp vs other languages topic you should know about this because it is one of the most cited results in this area. Hey, but it is easier to bitch about lack of evidence then to try to find one, right? And if somebody publishes qualitative comparison _together with_ quantitative he must be an idiot. Who needs anything qualitative?
&gt; Obviously 1 man isn't going to be in a position to do controlled tests of multi-hundred-million dollar projects comparing multiple languages against one another. No one does controlled studies on large-scale SE projects. It's simply not possible. However, there have been numerous Case studies and Lessons learned experiments run in the past. Indeed, many such experiments have been run at the JPL. With the claims that the author was making, it would be nice for *any* empirical support. &gt; Neither the author, nor myself, was making the claim that they could conclusively prove that any language is superior in an absolute sense to another. Which wasn't the the point that I was addressing. &gt; The internal accolades and awards heaped on the systems, the special features of LISP being used to save at least one mission, and the reuse of the resulting system code, all support his statement. And of course this kind of thing has never happened to systems written in any other language. As before, none of this addresses the point I was making. &gt; Honestly it sounds like you haven't used LISP enough to grok why it is well suited to problem domains such as the one the author was describing. Hmmm... all I want is a little support for the claims that people make and somehow I'm not smart enough to grasp the issues. Please. &gt; If relying on empirical studies is the only way you are able to make decisions between technologies: good luck in the industry ;) I've developed software for over 23 years and I've been a Computer Science prof for 7 years. Thanks, but I don't need any of your luck. 
Just use Chicken. Try to stick to standard (R5RS) scheme features at first, and only worry about the differences when they start to affect you. I could go on for a while about all the differences between implementations, but, for now, you need to just choose one and not worry about it. Chicken is solid and well supported; you can't go wrong with Chicken. &gt; Why are there so many Schemes? Scheme is fun to implement. It's easy to get a working imlementation, and there is plenty of cool stuff to play with later.
&gt; Your life must suck then. No, actually, my life is pretty darn great. &gt; I find it ironic that the very same Erann Gat (aka Ron Garret) guy made that famous survey which proves that Lisp is better than Java statistically, and it is hosted on this very same site: http://www.flownet.com/gat/papers/lisp-java.pdf The article is laughable. Programs with 182 lines of code proving that lisp is _statistically_ better than java. BTW, the phrase you used (statistically better) is meaningless. &gt; If you were ever interested in Lisp vs other languages topic you should know about this because it is one of the most cited results in this area. All the editorial (yes, it's filed under the _Point of View_ section) demonstrates is that 14 programmers (who, by the way, were self-selecting samples) wrote 16 programs. These programs were all trivial in that they could be written by a single person in under a few hundred lines of code in less than 50 hours of code. You know what this tells us about software development in the general sense? Absolutely nothing. &gt; Hey, but it is easier to bitch about lack of evidence then to try to find one, right? If you call that evidence then I'm afraid there is little help for you. 
Okay, thanks for the answer! I started off thinking that Scheme is such a "small" language it doesn't make sense to write lots of implementations, but of course there's the reverse effect of people doing it just for the hell of it. I'm just impressed that they even get around to publicising &amp; packaging them for Linux distributions...
Amateur warning. Caveat emptor. I'll throw in my personal vote for Racket (the successor to PLT Scheme). Technically...it isn't a Scheme compliant language, but heavily Scheme-like. The interpreter can also operate in other modes for other languages (R5RS, R6RS, etc.) and contains a number of features not found built-in. Cross platform support is also excellent and there are a considerable number of bindings. From what I've seen it seems to be the "batteries included" of the Scheme-ish world.. The included IDE (DrRacket) is actually quite spiffy and handy to have around, or use Geiser from Emacs for SLIME-ish functionality. I've also heard wonderful things about Chicken, so you can't go wrong there!
Most of the implementations have a bigger developer size than community or are already dead. What remains is most likely Racket (formerly PLT), Chicken, Gambit and that's about it.
Well, ok, what evidence do you use for programming language choice? Are there controlled, large-scale studies on that matter? They just need to pick like 100 teams with five programmers each for each programming language in question (so study covering 10 programming languages will involve 5000 programmers), give each team 50 different tasks covering all interesting aspects of programming languages (we don't want task bias, do we?) which take up to five man year to implement (because we're interested in large scale things too, right?). So just if you pay for something like 50000 man-years you'll have at least some data. So are there studies of this scale? If there are not, how do you feel choosing programming languages without much evidence? It must suck for such evidence-hungry person like you. Or maybe you're just a bitching hypocrite -- your choice of programming languages is based on anecdotal evidence or more likely just some superstitions, but you're bitching about people who do at least something useful in area of programming language comparison.
By the way, should I be consulting the R5RS specification itself or just follow whatever tutorial and ignore Chicken's extensions(?)?
Thanks, I'll check that out too. Emacs + Quack seems to do a decent job of interacting with numerous Schemes, but I'll keep Geiser in mind.
R5RS is readable enough to be used as a reference, but it's no tutorial. If you're not familiar with Lisp idioms you'll need one of those. Personally, I learned through reading SICP. I would try to avoid using extensions at first, but, sooner or later, you will need them.
There are so many implementations because: 1. It's a very small language. At the same time, it's a very flexible, powerful language. Taken together, these characteristics mean that it's relatively easy toi implement, but you still get something pretty useful out of the effort. So people interested in learning how to write compilers and interpreters often implement Scheme or some variant, especially if they generally like Lisp. 2. The language has existed for a long time. Since it's easy to implement a useful version of it, a lot of people have done it. 3. The nature of Lisp in general and Scheme in particular make it a useful platform for trying out ideas about language implementation, or about extensions to the language. So people interested in those things are often inclined to write or modify a Scheme. As for the list of Schemes you provided, here are brief comments on the ones I'm familiar with: MIT Scheme is large and old and interesting to poke around in. PLT Scheme is now Racket. It's a long-term effort to build a comprehensive IDE and environment for learning how to write software well. It's very good, especially if you like all-encompassing programming environments with lots of libraries. Gambit-C is my favorite Scheme. It's basically R5RS with extensions, with a compiler that produces very fast code that integrates easily with C code. It is not a development environment; it is a compiler and an interpreter. It has everything one needs for serious programming, but not everything one might want (sometimes you have to port or write libraries yourself). Elk is an interpreter intended as an extension and scripting language. It's been around a long time. I haven't looked at it in some years, but aat one time I found the source code very educational. Ikarus is a new Scheme, a native Scheme-to-x86 machine-code compiler. It supports R6Rs and generates very fast code. It serves the useful purpose of demonstrating that a good native-code compiler can also be understandable. Chicken occupies a niche next-door to Gambit. It's another Scheme compiler that uses C as an intermediate language for compilation. Compared to Gambit, the most interesting differences are that (a) Chicken uses a very interesting implementation strategy that is educational for language implenentors, and (b) it has a lot richer set of libraries than Gambit's. Gauche is a well-implemented and -supported Scheme interpreter (actually a compiler with VM), designed as a comfortable interpreter for those sorts of programming tasks that are well served by a comfortable interpreter (scripting, system tools, apps in which rapid prototyping and easy changes are more important than absolute execution speed). It has a rather nice object system. Stalin is an aggressive whole-program optimizing compiler for Scheme. It doesn't have a pleasant interactive programming environment, and isn't intended to. It's intended to push the boundaries of execution speed with programs whose source code is written in Scheme, and it does that very well.
BTW if you're familiar with information theory or, say, Bayesian inference you should know that something is better than nothing -- even isolated data points improve estimation accuracy. This is a basis of many compression algorithms -- even crappy estimation help to improve compression a lot, and many A.I.-related approaches derive results from relatively crappy and inconclusive data points. This article does not exist in vacuum. You should use all available material to make a decision, not just one article. So if it useless to you that just means you suck at decision-making. Essentially you claim that mutual information is strictly zero, which is, realistically, only possible when variables are independent. So you claim that performance is independent from tool choice.
&gt; [...] even get around to publicising &amp; packaging. I didn't mean to imply that they are all one-off toys, just that they probably started that way. Stalin, for example, is incredibly sophisticated, but it's a research implementation, has no community, and is poorly supported. 
Thanks, that's a lot of information. If you don't mind answering a couple of questions, how did you get to be so experienced with all of these and do you actually end up integrating Gambit-C with C much of the time?
&gt; No one does controlled studies on large-scale SE projects. It's simply not possible. However, there have been numerous Case studies and Lessons learned experiments run in the past. I don't think you understand what "experiment" means. A case study is by definition not an experiment. Furthermore, what the author presents *is* a case study of sorts. I'm sorry, but you can't say "it's not possible to do experiments in SE" with one side of your mouth, and demand experimentally-proved data with the other, and then claim the same kind of studies you've just dismissed to be "empirical" with the third.
There is also Guile.
Another benefit of chicken is that you can use it with Slime in emacs.
PLT Scheme is now called Racket http://racket-lang.org/new-name.html
You forgot one of the most commonly used (in shipping applications anyway, such as [this](http://appinventor.googlelabs.com/about/)). Kawa.
And now with 2.0 out, it's actually a pretty nice implementation. If I were going to be using Scheme to extend a C program today, I'd do it with Guile. For a stand alone program, something else might be better.
I use and recommend [Chez Scheme](http://scheme.com). It's small and lightning-fast, and strongly standards-compliant.
This was actually specifically a list of those provided in the Debian repos, and for some reason Kawa is one. Looks neat though (for people who need that sort of thing!)
Thanks, that looks very competent as well.
Huh, still packaged as plt-scheme Debian - no racket in testing or unstable. But thanks for the info.
&gt; You seem to have missed my point. One person providing anecdotal evidence about another programming language is the same as a different person providing anecdotal evidence about lisp. No, you missed mine. A C programmer claiming that using C over some other language was responsible for their success is interesting if you're trying to compare those languages yourself, and they have the experience to make that judgement. Likewise for Lisp, or any other language. Why is it relevant if you can find hundreds or thousands of them? &gt; Indeed, I'll bet that you can easily find hundreds of programmers who say things like *it's a good think that we're not using lisp or we'd be really screwed.* And on the other hand, it's *only* interesting if the programmer making the judgement has enough experience to have done the project in either language.
Ok, I'll bite. What *did* you mean by &gt; You think that writing the program in lisp helped. Good for you. Unless you can demonstrate why, nobody else cares. EDIT: Holy crap, shitstorm. If you don't want to discuss further, I'll understand. Feel free to message me about it though. I really wasn't trying to brush you off just because you don't seem to like Lisp, and any misunderstanding in my response was genuine. (I haven't read all of this surrounding discussion yet though so it may illuminate; give me a little while) EDIT2: Having read the surrounding discussion, it seems that you would either like proof that Lisp is a superior language (which I can't provide; you'll have to make your own decision), or you're pointing out that it's difficult to prove experimentally whether one language is better than another for general purpose development (which I agree with, but still have a strong preference for Lisp, and don't think the blub paradox is invalid; merely difficult to prove experimentally).
Yes. &gt; And on the other hand, it's *only* interesting if the programmer making the judgement has enough experience to have done the project in either language.
Ah. Missed that bit. My fault.
Can your Lisp do: (load "http://beta.quicklisp.org/quicklisp.lisp")
Some raw implementation stats for you, too: 26468 SBCL 6463 Clozure_Common_Lisp 4313 ECL 3572 CLISP 887 LispWorks 573 CMU_Common_Lisp 370 LispWorks_Personal_Edition 217 Armed_Bear_Common_Lisp 205 International_Allegro_CL_Free_Express_Edition 162 International_Allegro_CL_Enterprise_Edition 21 Allegro_CL_Enterprise_Edition 13 International_Allegro_CL_Professional_Edition 10 International_Allegro_CL__master_ 
Whoa, ECL is doing well! Any numbers on the OSs SBCL is running on (or any of the other implementations)? 
The user-agent I'm using for these requests does not include OS information. (It does include version info, but I trimmed it here.) I was surprised by ECL's number, and I'm wondering if it stems from some automated process, like the buildbot...
xach, I want Quicklisp to succeed. But I've been stymied by two things. 1. Quicklisp on SBCL for the Mac, at least the version I tried about a month ago, craps out in all sorts of bad ways. I found it pretty unusable. :-( 2. There's absolutely no documentation, guidance, or even explanation as to what the available packages do or how they work. Quicklisp desperately, DESPERATELY needs a documentation facility. CPAN, CTAN, etc. have packages organized by topic, with explanations and as to what the packages are, and documentation provided with them. Without this, Quicklisp is entirely opaque, for propellerheads only.
1. [Setting up Quicklisp + SBCL on Mac OS X](http://www.mohiji.org/2011/01/modern-common-lisp-on-osx/) 2. We definitely need it. (ql:system-apropos "search term") will get you partway there, but other than that it's more searching Google for the library you need and then checking in Quicklisp to see if it's available.
What trouble did you have on Mac? Half the time I develop on my MacBook with SBCL, and I haven't run into any issues, but I'd love to hear any bug reports. I agree that without documentation and other information easily available, Quicklisp is useful only for people who already know what they want. I hope to change the situation soon.
got to say Xach, i think we're all impressed and thankful!
For example: (load "quicklisp.lisp") (quicklisp-quickstart:install) (ql:add-to-init-file) [restart SBCL, and BOOM....] This is SBCL 1.0.20, an implementation of ANSI Common Lisp. More information about SBCL is available at &lt;http://www.sbcl.org/&gt;. SBCL is free software, provided as is, with absolutely no warranty. It is mostly in the public domain; some portions are provided under BSD-style licenses. See the CREDITS and COPYING files in the distribution for more information. debugger invoked on a SIMPLE-ERROR: Error during processing of initialization file /Users/foo/.sbclrc: The name "SB-POSIX" does not designate any package. Type HELP for debugger help, or (SB-EXT:QUIT) to exit from SBCL. restarts (invokable by number or by possibly-abbreviated name): 0: [TRY-RECOMPILING] Recompile impl and try loading it again 1: [RETRY ] Retry loading component ("quicklisp" "impl"). 2: [ACCEPT ] Continue, treating loading component ("quicklisp" "impl") as having been successful. 3: [CONTINUE ] Ignore and continue processing. 4: [ABORT ] Skip this initialization file. 5: Skip to toplevel READ/EVAL/PRINT loop. 6: [QUIT ] Quit SBCL (calling #'QUIT, killing the process). ((FLET #:LAMBDA69) #&lt;SB-KERNEL:SIMPLE-PACKAGE-ERROR {118D9F29}&gt;) I recall hunting around for SB-POSIX and loading it and then finding further problems. 
1.0.20 is too old for Quicklisp, sorry. Try with 1.0.44 or so.
Wow, I didn't know about Alexandria until now. It's like a less hacky version of my util.lisp "utility trash can" that gets dragged into every project, plus some more stuff. copy-hash-table! ensure-hash! maphash-keys! curry! map-product!
What's the blank line right below asn.1?
An awk/grep mistake.
I'm fine with shitloads of closures for a simple, quick use. My Lisp uses a generational GC and I'm not punished for allocating and discarding crap in a short span. 
I've always wondered, why can't Alexandria merge with Arnesi? There is already some huge overlap, and Marco contributes to Alexandria. 
Why can't Scheme merge with Common Lisp? There's already some huge overlap, and Guy Steele contributes to Scheme.
I (along with drewc occasionally) am pretty much the maintainer of Arnesi now, and it's more or less UCW's private extension bag... there isn't much going on with it aside from the occasional patch for the code walker and call/cc stuff, and so if you want to go ahead and make patches to move anything in arnesi that isn't in alexandria into alexandria go ahead--it'd be less code for me to deal with ;) If for whatever reason you decide to do this and patches to alexandria are accepted please at least email bese-devel so I can remove the obsolete code and re-export the symbols from alexandria to avoid breaking anything based on arnesi or letting the code drift.
You probably do not understand how modern CPUs work. Operations within memory registers are much less expensive than memory reads and writes, especially if they have to go to main memory (but even CPU cache is rather slow). Thus you want to minimize amount of memory you use while number of instructions is of least concern. With deeply pipelined and out-of-order execution many instructions are executed essentially for free -- it wouldn't matter if you remove them as CPU is waiting for memory reads anyway. It is hard to say what converting non-function values to closures would exactly mean in an optimized implementation, but if it is allocating more memory most likely it will not make things faster.
Quicklisp on the Mac is very usable even before Zach released the beta. I'm using Quicklisp on three different Mac machines (all with SBCL and a few other implementations) and never had any problem (the only minor thing was only some cached .fasls in the very begining). The only thing that might happen is trying to load a library that isn't ready for SBCL or Mac OS but that is not Quicklisp's fault. I guess your problems might be related to your own SBCL or any other system configuration (I don't know, I'm guessing here). But in these cases, the best thing is to post your problems on the mailing list. Not only will warn Zach of possible problems but there are lots of users there that might help you on finding a solution for your particular situation. *edit: ok, just saw below that it could be a SBCL version problem :-)*
Are these numbers for last month as well? Are they unique installations or just the number of times the implementation has hit Quicklisp servers in general? (If they're not unique installations, then getting such statistics -- even with pseudorandom IDs -- would be insanely cool.) 
&gt;how did you get to be so experienced I'm 51 years old. I've been writing software for money since 1988. I'm self-taught, and began with 6502 machine language. When I was learning my way around the machine, the idea of higher-level languages seemed intriguing, and I learned whatever I could about every one I could find. Lisp and Scheme are the ones that moved in and took over my brain, so I've used them whenever I could, and learned all I could about how to implement them. &gt; do you actually end up integrating Gambit-C with C much of the time? Yes. For example, I recently wrote a Cocoa app on Mac OS X that consists of about a thousand lines of Objective-C to make windows and menus and so forth, and about four thousand lines of Scheme to do that actual work. I use Gambit to compile the Scheme code to a linkable library so that I can use it just about anywhere, and not just in a Cocoa app. Theoretically, a benefit of Scheme-to-C compilers is that when you find a performance hotspot where Scheme code is not fast enough, you can recode the performance-critical sections in C. In practice, I have never found a case where the Scheme code was not fast enough (note: I'm not saying that nobody will ever find such a case; just that I never have). So, in practice, the real benefit of Gambit's (and Chicken's) easy integration with C is that it makes it easy to write apps in Scheme that integrate seamlessly with platform libraries that are written in C (like OS X's Cocoa frameworks, for example). 
Not unique installations. They're from a fairly simple HTTP log grep. Unique installations would be a rough approximation - I have only an IP address and fairly bland user-agent (SBCL version) to work with. I don't really want to leak too much user information into the user agent, even though I agree it would be pretty fascinating!
How about -- as a weak extra bit of unification -- the results (random most-positive-fixnum (make-random-state t)) when Quicklisp is installed? Or just (get-universal-time) from that moment. 
Unusual, the "Phosphorous Programming Language". From the document: &gt; By use of an Internet Poll, programmers of Phosphoros have decided that the Latin language shall be part of our branding and popularization structure. Latin is obscure enough so that errors are not obvious to the casual reader, yet arcane enough to invoke an unparalleled sense of cleverness about the author. I gave a deep sigh inside and laid my hand on my face.
**R**eproachers **O**ught to **T**aste the **F**reedom of **L**isp's **O**rganic **L**ambdas
**I N . . . J E S T**
What do you make of these unanswered comments? &gt; I also develop on a Mac, and my deployment platform is also FreeBSD, and I have no problem with compatibility (perhaps because I use one implementation on both platforms: CLISP). The point not yet mentioned: do they really need threads? Is load so big that old good 'select' (buried under something like CORBA or UCW) doesn't do the job? And: &gt; rps, I believe the premise to effect that Lisp has few libraries and lacks stability is itself debatable. The self-imposed constraint of 1) OSX development and 2) FreeBSD deployment and 3) OSS implementation and 4) native threading, may understandably be a issue. However, LispWorks or ACL, I believe, satisfy the requirements, except for the 3), which begs the question "Why OSS?". Then there's 5) libraries (which ones did the reddit authors miss in Lisp that they couldn't at least link to via FFI?) and 6) language community (have they approached the Lisp community for help and failed to get any?). And: &gt; Thing is, they made a typical newbie mistake of choosing the OS before they choose the software to run on top of the OS. There is little reason to run FreeBSD over Linux, if Linux gets you threaded execution that works better - the better threading will beat out any BSD advantages. &gt; Second, they have shown they don't quite understand how to run servers in production - the errors should have been caught immediately with a monitoring script and the offending daemon process automatically restarted. 
&gt; With the claims that the author was making, it would be nice for any empirical support. At the crux of your argument is a supposition that such support exists or could reasonably be expected to exist. If it does exist then you should easily be able to demonstrate that by provide supporting empirical proof that *any other* language is superior to LISP for the projects being undertaken at JPL. Failing to provide such support would rend your entire point *point*less, as 'anecdotal data' (read experience and wisdom from experts), would be the only points of comparison for language decisions. ---- BTW: I wasn't implying you weren't smart enough to grasp the issues, but if you can't provide a solid list of reasons why LISP is a language that is *well suited* (though not necessarily 'the best'), for robot AI programming then you really *don't* grok either the language or the domain.
Also, Chicken has a very friendly, helpful, and active community. 
There aren't any released packages yet, but it is in the Debian packagers' git repository: http://git.debian.org/?p=collab-maint/racket.git
I'm not even sure how to react to this. Is it a joke, or someone seriously trying to create hipster-Lisp? I can only find [one other reference](http://jfm3.org/phosphorous.pdf) to it, and [one thread on hacker news](http://news.ycombinator.com/item?id=697492) (whose posters also seem divided as to whether this is a joke or an actual paper to critique).
Try sending a private message to "lisper" on reddit.
&gt; Well, ok, what evidence do you use for programming language choice? It appears that people in this subreddit need to take remedial reading courses. I never once mentioned empirical evidence for language choice. I said empirical evidence for the claims being made by the author... specifically claims about quality and maintainability _because_ of language choice. I've made no positive claims. I've simply said that I'd like to see evidence when people do make positive claims. I have to say, it appears that lots of people on this subreddit seem to have a massive inferiority complex. If you want some free advice, get over it. 
 I'm not really sure what perspective you're looking for-- as someone who likes Lisp? As a redditor? As a software developer? Also, why me? I'm not particularly accomplished in any of those capacities.
Do you mean to tell me you read the words "the noL movement" and didn't realize it was a joke?
&gt; BTW if you're familiar with information theory or, say, Bayesian inference you should know that something is better than nothing -- even isolated data points improve estimation accuracy. There's a saying in statistics... you can prove shocking things with a sufficiently small number of samples. Based on your statement above and that you seem to think that nonsense you linked was statistically valid, it's clear to me you don't actually know what you're talking about. The methodology described in that opinion piece you linked is not valid as a statistical measure. &gt; This article does not exist in vacuum. You should use all available material to make a decision, not just one article. And it's the only one you provided. On top of that you made outrageous claims about conclusions that can be drawn from it. 
There is a difference between proving stuff conclusively and using it for decision making. And it looks like you do not understand this difference. &gt; And it's the only one you provided. I assume that you have read some articles before. Do you live in a vacuum? 
You're quite right that I'm using my language quite casually here. I'm currently doing research in Software Engineering, but my previous research was in psycho-acoustics and HCI, which is heavily rooted in controlled experimentation. My colleagues and I will often use the terms "experiment", "study" and "investigation" interchangeably and no one will call out the others because it is generally known what is meant because of the context. &gt; I'm sorry, but you can't say "it's not possible to do experiments in SE" with one side of your mouth I didn't say that it was impossible to do experiments in SE. I specifically said that it's not possible to do "controlled studies" with the emphasis being on the word _controlled_. This doesn't mean there isn't a whole raft of other empirically based investigations that can be done. One of the big problems I find with SE research is that there is so very little that is actually supported by empirical data. For example, everyone seems to believe that there have been a multitude of studies performed that demonstrate that there is high variability in programmer productivity. Generally I hear people say that there is a 10:1 ratio between best and worst programmers (sometimes this ratio is as high as 20:1). However, there isn't a single study that actually tests this. There was one study (done in 1968) that compared the productivity of programmers, but it wasn't terribly conclusive and the kinds of programming that they did is hardly relevant to the kinds of programming tasks and environments that we see today. When it gets right down to it, if someone judgments (as the author did) without providing evidence, they should expect to be called-out on those judgments. Note that NOWHERE did I say that he was wrong... I merely asked for something, _ANYTHING_ that would support his judgments and people here are jumping all over it. It gets right down to this: if a person believes something without cause, then they are acting in a religious manner. 
My limited understanding is that the CL standard is quite fixed and unchanging, and that the Scheme standard is currently being revised such that there will be both a "Small Scheme" (like R5RS) and a "Large Scheme" (which includes numerous practical and commonly-needed modules). Small Scheme is being handled by "working group 1" ("wg1") and Large Scheme is by wg2. I suspect wg2 and CL would never merge, but perhaps wg2 could draw substantially from CL, learning from their experience. I have no idea how the members of wg2 would react to such a proposition ... perhaps someone should [ask them](http://groups.google.com/group/scheme-reports-wg2/topics?gvc=2)? **Edit:** just asked that [here](http://www.reddit.com/r/scheme/comments/fp4aq/would_wg2_r7rs_large_scheme_benefit_by_borrowing/).
not anymore
&gt; Holy crap, shitstorm. If you don't want to discuss further, I'll understand. No worries. I'm happy to explain myself better. I've got a thick skin when it comes to the evangelists. &gt; Ok, I'll bite. What did you mean by &gt;&gt; You think that writing the program in lisp helped. Good for you. Unless you can demonstrate why, nobody else cares. I'm currently doing research in Software Engineering. My previous research experience is in HCI, specifically working with the production and perception of speech (both synthetic and natural). My work overlaps heavily with the field of psychoacoustics, which is heavily grounded in controlled experimentation. When I moved over to SE, I was simply aghast by what I saw. It's as though I've left the rational world and moved in to a world based entirely on religion and authority. Generally speaking, SE is rife with belief of things for which there is absolutely no cause. A good example would be the general belief that your best programmers are 10 times (or some people say 20 times) more productive than your worst programmers. While this may be true, there's no actual evidence to support these beliefs. (BTW, the productivity myth is based on a paper published in 1968. I can track down the reference if you like but I don't remember it off the top of my head). The linked article is the very kind that perpetuates these sorts of myths. While it is reasonably well written and interesting, it doesn't offer anything that should convince a skeptic. An interesting note on the _shitstorm_ as you put it. You'll note that nowhere did I say that I disagreed with the author. Indeed, he may very well be right. I've said that I'd really like to get some sort of empirical support for the judgments being made and that making judgments about empirical phenomena without any sort of empirical evidence doesn't really add much to the discussion. My particular favorite response is the guy that said I'm not _smart enough to grok_ the language or the domain. Wow... all that for simply asking for some evidence. It's as though I've walked in to some kind of Spanish Inquisition. 
&gt; At the crux of your argument is a supposition that such support exists or could reasonably be expected to exist. No. My point is that if someone is going to be making judgments about empirical phenomena, then they should be providing some sort of empirical evidence. &gt; If it does exist then you should easily be able to demonstrate that by provide supporting empirical proof that any other language is superior to LISP for the projects being undertaken at JPL. It's funny how you seem to think that now the burden of proof is shifted to me (or others who disagree with the proposition), when I haven't actually made any positive claims about any sort of phenomena. It's not my responsibility to disprove anything or even offer and conflicting evidence if the original proposition is presented without evidence itself. &gt; Failing to provide such support would rend your entire point pointless, as 'anecdotal data' (read experience and wisdom from experts), would be the only points of comparison for language decisions. Wow! That is an amazing leap. Because I'm not supporting my skepticism (which, by the way, is the default position), we should conclude that it's not possible to provide any sort of empirical evidence to support language choices. Seriously, it boggles the mind that anyone would take that position. 
&gt; I assume that you have read some articles before. Do you live in a vacuum? So let me get this straight. You tried to make a point and linked that ridiculous article. I called the article out and you replied that I shouldn't only consider the one article (even though it's the only one you provided). At this point, you haven't actually provide any other articles and when I mentioned that, you tell me that you assume I've read other articles and if I don't I live in a vacuum. So, to clarify your last point, you assert that there are lots of articles that support your position, but you're only going to show me one of them. Beyond that, it's now my responsibility to find support for your argument. How can anyone view that as anything other than a completely idiotic proposition? &gt; I assume that you have read some articles before. In answer to this comment. Yes, I've read lots of articles but none that support your claim. The fact that you aren't willing to (or can't) provide articles that support your position tells me that it is unlikely they exist. Given the incredible nature of your original claim (that it has been proven that "Lisp is better than Java statistically"), I think that there is a high probability that these articles (which you imply there are lots of) don't actually exist. &gt; There is a difference between proving stuff conclusively and using it for decision making. And it looks like you do not understand this difference. I never stated or implied otherwise. This comment demonstrates your misunderstanding or your desire to introduce a strawman argument.
&gt; and they have the experience to make that judgement. So where in the article is the evidence to suggest that the original author has the experience to make relevant judgments about C?
I simply wanted to know if you think they are fair questions or assessments.
I might have caught on if I had read the entire article the first time. (I just gave it a brief overview and the Latin section caught my attention). It seems odd that he uses methodCase for nouns instead of ClassCase. By the way, there is more here: http://jfm3.org/ Most of you will probably appreciate the main essay there more than I can (new to Lisp). In fact, I am probably going to post it once I finish reading it.
Oh, are you a Prolog programmer?
The problem is that it's very hard to get any empirical evidence together in SE, both because of the way the industry functions and because of the nature of the work being done. Do you mean Mythical Man Month when you talk about the 10x productivity notion? I can't remember if it was Brooks' idea, or if he was citing someone else, but I'm pretty sure he was originally articulating something about communication overhead inside teams (I'm also pretty sure the original number was 5x). In any case, you're right, it hasn't been proven experimentally. Because of the intense lack of experimental data in the area, the best anyone can muster is anecdotes from a career of observing their teams. Even in something this fundamental though; think about how you'd go about designing an experiment to provably determine the difference in productivity (or even the presence of such a difference) between programmers. In terms of languages, it's worse. It's typically an uphill battle trying to convince people that it's even [worth comparing languages](http://programmers.stackexchange.com/questions/21891/are-language-comparisons-meaningful) in terms of power, not to mention examining their favorites after discarding their bias. Experimental data is just as absent here, for some of the same reasons (it's not entirely clear what metrics you would use to compare two languages, let alone how you'd go about measuring them). The best you're likely to find is reasoning along the lines of the blub paradox (in other words , something like "the best language is the one that has the fewest missing features"). It's particularly difficult because, as I'm sure you've noticed by now, languages *are* part religion, part tool. The best I can tell you is to generate some evidence if you can, because there doesn't seem to be any unbiased experimental data out there (though there are plenty of strong opinions regardless). As far as this particular shitstorm goes, you'll have to cut us a bit of slack. From personal experience (which is to say, anecdotally), it seems that calling yourself a Lisp programmer is an invitation for people to harass you, at length, about why the hell (Ruby|Python|Java|C#|C\+\+|PHP) isn't good enough for you. These "discussions" typically involve calls for concrete proof about how Lisp is better while taking it as axiomatic that \1 is excellent in all ways unless you're an idiot who doesn't understand how to use it. No other language I've used does this and it gets pretty annoying pretty fast. To the point that you start expecting most language discussions with someone you don't know to go in that direction.
I have to reiterate: By demanding empirical evidence to compare the fit of a specific language to a given project you are begging the question that such evidence exists. You have not claimed phenomena, but have claimed that without empirical backing anecdotal data regarding a language is valueless. If that is true and empirical data is non-existant for all languages on these kinds of projects your argument is reduced to: "we cannot compare/choose between languages". This argument is conceivably true, empirically, but also pointless (philosophically and practically). The burden of proof is not upon you to prove that Lisp is inferior - a claim you have not made - but to justify the essence of your argument by showing that evidence of the sort you are demanding in regards to specific languages exists *at all*. As we are discussing evidence of empirical comparisons between programming languages then proving that your claim has merit would simultaneously empirically prove how well suited Lisp was for the project (as it would empirically show that language *X* is empirically better/worse than Lisp). Summary: show 'em if you've got 'em. Really. Failing such: I deem your argument to be meritless. ---- BTW: I rarely use Lisp, so don't think I'm butt-hurt 'cause you're dissing my parentheses. If what you're trying to convey is that too few decisions in IT are empirically grounded then I agree 100%, the same is also true for the business world (for many of the same reasons). You don't have to try and prove anything drastic in order to get a sense of why things are the way they are... That is not, however, what you said. Anecdotal postmortems, particularly from centers as important as JPL, are *highly* valuable in an industry that forgets it's history so readily. It also exposes mainstream programmers to the potential benefits of less-popular languages. This encourages a more diverse computing ecosystem and a wider variety of tool choices for solving the worlds problems.
I finally figured out by OT you mean "off-topic".
&gt; By demanding empirical evidence to compare the fit of a specific language to a given project you are begging the question that such evidence exists. Which is not what I was asking for, actually. If you go back to my original comment, I quoted a specific statement made by the original author. I'll quote it again here: &gt; I attribute this success to a large extent to the fact that I was using Lisp while most of the other teams were using C. The problem is that he is making a claim of causality between the success of his project and his tool choices. You'll note that throughout all of my comments, I'm not asking for _proof_ (although other posters have been making the claim that I did), but rather I've been asking for some evidence. In some cases, I've said "give me something... *anything*" to help be derive the same conclusions. I'm not looking for hardened evidence, I'm just looking for something empirical that I can chew on and consider. Without that, this person's attribution of causality is no different than assertions made by others. The main problem I see in the lisp community (and I think a lot of it started with Paul Graham's writings), is that there seems to be a lot of similar claims being made (and believed) without any sort of cause or reason. To me, this is the definition of a religion. It serves no purpose beyond perpetuating myths that can be detrimental. &gt; You have not claimed phenomena, but have claimed that without empirical backing anecdotal data regarding a language is valueless. Not quite. Anecdotal evidence about important things like, say, causality or generality, is virtually useless. For example, I really like the aesthetics of python and I may think that it's aesthetics help people to better understand programs. If I were to generalize and say that the syntax of python makes programming easier for everybody, I shouldn't be surprised when someone calls me out on such a statement. &gt; but to justify the essence of your argument by showing that evidence of the sort you are demanding in regards to specific languages exists at all. I'm currently doing studies that examine the static structure of software systems and their relationship with modifiability. I'm looking specifically at software component interaction. There is some reasonable evidence to suggest that high component interconnectivity leads to less long-term flexibility that results in lower modifiability. My studies are currently focused on java and OO, but I think it would be interesting to perform a similar analysis between languages and paradigms. One of the claims that I am constantly hearing from the lisp an FP community is that OO is bad because it encapsulates state and FP makes state explicit. The problem with this argument is that this definition relies on the assumption that explicit state is better than encapsulation. I'd like to see someone investigate this idea. Devise an experiment or an analytical model and do a comparison between languages. See if there really is a correlation to modifiability. Even it it's not terribly conclusive, it's at least _something_ that supports the proposition and it gives us points upon which we can have a discussion. Without that, it devolves to a religious argument. &gt; but to justify the essence of your argument by showing that evidence of the sort you are demanding in regards to specific languages exists at all. I think that your argument here is flawed. If this evidence doesn't exist, that is not conclusive that it couldn't exist. Having said this, I'd be happy to give you an example of some evidence that I think would be satisfactory to me. For example, I think that if the experiment outlined in Robillard,M. P, Coelho, W. and Murphy G. C., _How Effective Developers Investigate Source Code: An Exploratory Study_, IEEE Transactions On Software Engineering, Vol. 30, No. 12, December 2004 in a manner that compared programs written in java to programs written in lisp, I think that that would go a long way to providing some really useful empirical evidence surrounding between language comparisons. 
&gt; The problem is that it's very hard to get any empirical evidence together in SE, both because of the way the industry functions and because of the nature of the work being done. Which is why I'm not asking for terribly rigorous studies. I'm simply looking for some basic empirical support. I've just submitted a paper to ECOOP that outlines an empirical study on the static structure of software systems written in java. It looks at the notion of _coupling_ and concludes that eliminating all aspects of high coupling is pretty unlikely. We'll see in a couple of weeks if it has been accepted for publication. It looks as though 2 of the reviewers liked it. &gt; Do you mean Mythical Man Month when you talk about the 10x productivity notion? Brooks mentioned it, but as far as I know, the original paper that everyone bases the current myth on is: Sackman, H., W.J. Erikson, and E. E. Grant. 1968. "Exploratory Experimental Studies Comparing Online and Offline Programming Performance." Communications of the ACM 11, no. 1 (January): 3-11. &gt; Even in something this fundamental though; think about how you'd go about designing an experiment to provably determine the difference in productivity (or even the presence of such a difference) between programmers. Indeed, I know someone who is starting a PhD who is considering this very thing. Because this question is far beyond a PhD topic, the student will address a subset of the problem. I think it's going to be interesting to see what he comes up with. &gt; From personal experience (which is to say, anecdotally), it seems that calling yourself a Lisp programmer is an invitation for people to harass you, at length, about why the hell (Ruby|Python|Java|C#|C++|PHP) isn't good enough for you. Anecdotal evidence??! I demand proof, dammit! :-) Seriously though, I think that you make a fair point. However, if you look at a couple of the posts made here (e.g. "Lisp have been proven to be statistically better than java") I can't say that some of that isn't brought on to the community because of the actions of some members of that community. When it gets right down to it, I really like lisp. The main focus of my work is to use the right tool for the job and I try to be as pragmatic as I can. I'll call anyone out who makes unfounded strong statements. It's a disservice to do otherwise.
To add to your list, another excellent Scheme is [Bigloo](http://www-sop.inria.fr/mimosa/fp/Bigloo/). It's fast, has an object system, pattern matching and more. It's been my experience that any of the main R5RS Scheme that are being actively developed are good choices - Gambit, Bigloo, Chicken, PLT Scheme. If you are an Emacs user, the Scheme implementation may end up being the one that has the best Emacs support or works well with a Slime type el package for Emacs like Quack or Slime48 (for Scheme48). Many like Racket and PLT, but the DrScheme editor, while really impressive, is not nearly as nice feature-wise as using Emacs/Slime/Common Lisp. There is a nice Scheme implementation comparison article called [The Schemes I've Seen](http://www.aracnet.com/~briand/scheme_eval.html). Good Luck. 
It might not be there. I didn't check, since it's not relevant to my point and I'd already read the article.
&gt; I'm not asking for proof (although other posters have been making the claim that I did), but rather I've been asking for some evidence. Which... does... not... exist. You have earlier expressed an understanding of why that is so. Ideally it would exist, but the reality is something else. &gt;If this evidence doesn't exist, that is not conclusive that it couldn't exist. That wasn't claimed, and is irrelevant. If relevant evidence exists for *any* language comparison for these kinds of projects then it would provide the data you're looking for. If it doesn't exist then, again, "don't want to hear that lisp is good without empirical backing" means only "dont want to hear about *languages* without empirical backing". If your point is valid it should be readily confirmed as you are, I presume, currently using a language. Prove me wrong: show me the relevant research which backs *any* position on the matter. Or be doomed to accept (*shock*, *gasp*) documented success stories by respected experts as valid, through anecdotal, evidence. For you see even it it's not terribly conclusive, it's at least something that supports the proposition and it gives us points upon which we can have a discussion. ---- BTW: that paper you mentioned? While the thrust of the paper is not pertinent I was surprised to read that it had a sample size of only *five* developers performing a *single* task... For reals. Also: immutability and statelessness improve modifiability only in a broad sense. It's value lies primarily in maintaining correctness and avoiding side effects. It's not an FP vs OO issue, but rather a mutable vs immutable issue (religious or not). In order to compare (im)mutable systems you wouldn't have to muddy the waters by bringing different languages into the mix.
1960: DEFINE (((COLLAPSE,(LAMBDA,(L),(COND, ((ATOM,L), (CONS,L,NIL)) ((NULL,(CDR,L)), (COND,((ATOM,(CAR,L)),L),(T,(COLLAPSE,(CAR,L))))) (T,(APPEND,(COLLAPSE,(CAR,L)),(COLLAPSE,(CDR,L)))) ))))) () COLLAPSE ((((A,B),((C))),((D,(E,F)),(G),((H))))) () COLLAPSE ((A,(B,(C,(D,(E))),F,(G,(H,J))))) () COLLAPSE ((((((A),B),C),D),E)) () STOP))))))))))STOP Fifty-one years later: &gt; (defun collapse (l) (cond ((atom l) (cons l nil)) ((null (cdr l)) (cond ((atom (car l)) l) (t (collapse (car l))))) (t (append (collapse (car l)) (collapse (cdr l)))))) COLLAPSE &gt; (collapse '(((A B) ((C))) ((D (E F)) (G) ((H))))) (A B C D E F G H) 
Seems like work on Windows threading is not yet merged into mainline. Otherwise, nice release.
No surprise here, given that ECL is excellent implementation that works equally well in all major platforms (Linux, Mac, Windows). It is used in a couple of major Lisp project (Maxima, Axiom, ...) and there are people on ECL mailing list that are actually using it that don't frequent on other lisp forums. That's why it has a low profile. I'm more surprised by low number for ABCL and hight number for CLISP.
I understand. Lisp programmers make judgments about other languages (any language it would seem) and that's fully justified. C programmers make judgments about other languages (specifically lisp) and their judgments are irrelevant. Thanks for clarifying that point. 
&gt; Which... does... not... exist. So? Who cares? &gt; You have earlier expressed an understanding of why that is so. Yeah.. the studies haven't been done. If someone wants to make these claims, then do the damn studies. If not, then stop making claims for which you don't have enough (i.e. any) support or stop bitching when people point out that your claims are not substantiated. &gt; If it doesn't exist then, again, "don't want to hear that lisp is good without empirical backing" means only "dont want to hear about languages without empirical backing". Ummm.. no kidding. &gt; Or be doomed to accept (shock, gasp) documented success stories by respected experts as valid, through anecdotal, evidence. Or (shock, gasp) reserve judgment on these issues until supporting evidence becomes available. &gt; While the thrust of the paper is not pertinent Actually, it is, but I'm not going to argue the point with you. &gt; I was surprised to read that it had a sample size of only five developers performing a single task... For reals. Welcome to empirical studies in Software Engineering.
Also valid Elisp, btw. Isn't Lisp wonderful?
Oh well, now I understand the "sea of parentheses" thing. Thank God Lisp has evolved a lot since then.
&gt; So? Who cares? According to your first post, you do :) It's good though. You should care. I do. For most of us, though: reserving judgment for more than 20 *years* awaiting difficult studies of questionable value is not an option... The lack of empirical data is not due to a lack will, otherwise we'd be drowning in studies showing why BigCorp-Language-2011 is just so much better than OtherCorp-Language-2010. The lack of empirical data is due to the challenges of controlling for environment, participants, and stunning lack of applicability across problem domains and teams. For the **third** time - if you genuinely think that you are right then this should be readily, and easily, disproven: show me the studies you used to back your most recent choice of language. So far you have been unable to substantiate a single counter-example. Failing to provide such an example, you should be able to admit that telling experts that they should stop sharing their experiences because they useless when lacking studies backing said experience is, at best, inane. &gt; "dont want to hear about languages without empirical backing" &gt;&gt;&gt; Ummm.. no kidding. At the risk of breaking internet-etiquette, allow me to answer a quote *with* a quote: "*your argument is reduced to: "we cannot compare/choose between languages". This argument is conceivably true, empirically, but also pointless (philosophically and practically).*" I'm glad that you agree. It's great to see minds change on the internet. :) 
What do you mean? There really aren't that many fewer parens in the "modern" counterpart: the code has been formatted better, the commas replaced with spaces, and the noise necessitated by the use of punch cards has been removed.
As you can see, the code runs almost unchanged, thus Lisp has not really evolved from 1960 in this respect . Only DEFINE has now been replaced by DEFUN. What has been changed is that we no longer use punch cards to write and enter code into a computer. What is amazing is that we can reuse the ideas how to implement algorithms from 50 years ago. Still until today students may solve the very same problem as an exercise: how to flatten a nested list structure.
Cute. I hope I haven't come across like that's my position.
This is just me guessing but probably the low numbers of ABCL are because users of ABCL use more Java libs, thus not available in QL. To be honest, I would only use ABCL if I needed something from the Java world. Then again, I might be wrong and the numbers have another explanation.
I'm pretty much a Lisp-dilettante, so this is probably a half baked idea. But what if there were a dialect of Lisp that was designed to last, unchanged, for 100 years or more? Digital archivists understand the value of file formats that will be readable over a long period of time for things like images, text files, and audio recordings. Couldn't a 100-year Lisp act as a durable file format for algorithms? 
&gt; There really aren't that many fewer parens in the "modern" counterpart EDIT: Disregard. treerex corrects me below. Ignoring the 10 close parents between the `STOP`s, there are 110 in the 1960 version and 70 in the modern version. That's a reduction of nearly 40%.
I think you're counting the parens in the three subsequent calls to COLLAPSE after the definition of the function: If you count the raw number of parents just for the definition of each function I get 26 in (((COLLAPSE,(LAMBDA,(L),(COND, ((ATOM,L), (CONS,L,NIL)) ((NULL,(CDR,L)), (COND,((ATOM,(CAR,L)),L),(T,(COLLAPSE,(CAR,L))))) (T,(APPEND,(COLLAPSE,(CAR,L)),(COLLAPSE,(CDR,L)))) ))))) () and 24 in (defun collapse (l) (cond ((atom l) (cons l nil)) ((null (cdr l)) (cond ((atom (car l)) l) (t (collapse (car l))))) (t (append (collapse (car l)) (collapse (cdr l)))))) 
&gt; For the third time - if you genuinely think that you are right then this should be readily, and easily, disproven: show me the studies you used to back your most recent choice of language. For the third time (and hopefully LAST time): what you're putting forward is NOT my argument. You've been arguing a strawman. &gt; The lack of empirical data is due to the challenges of controlling for environment, participants, and stunning lack of applicability across problem domains and teams. No argument here. In fact, I mentioned this before. &gt;For most of us, though: reserving judgment for more than 20 years awaiting difficult studies of questionable value is not an option... Again, you are misrepresenting my argument. &gt; if you genuinely think that you are right then this should be readily, and easily, disproven: show me the studies you used to back your most recent choice of language. My argument is not about language choice (this is your strawman). My argument is that statements about causality and generalizations shouldn't just be accepted on face value. If you go back to my original post (which contains the argument that I've been making all along), I was commenting on the generalization that the author was making about how his language choice affected the success of his project. My point is that he could be wrong (and I believe that he is LIKELY to be wrong because there are so many variables that he: a) cannot control; and b) doesn't even know about). If people make these kinds of statements without evidence, they shouldn't simply be believed without some sort of support. I don't care about whether they are an authority or not. And, if they don't want to be called on these statements, then they shouldn't be making them in the first place. My original comment was that I'm getting tired of the whining that I'm hearing from the Lisp community about how lisp isn't being accepted by the programming world at large. Well guess what, when someone makes unsubstantiated statements, the ones who accept them are the choir and everyone else thinks the person is, at best, misguided and at worse, an idiot. &gt; So far you have been unable to substantiate a single counter-example. This is because you keep coming back to an argument that I'm not making. &gt; you should be able to admit that telling experts that they should stop sharing their experiences I NEVER SAID THIS! 
It fills a need for a standard set of simple stuff missing in Common Lisp itself.(Many functions in CL should've been standard library stuff, anyway, tho.) Otherwise you keep having to choose between inconveniencing the user with downloading another little lib for your own little additions, or just adding the code all the time. And there is always the issue of not finding the function, but it being there. For instance [with-mod-slots](http://lispforum.com/viewtopic.php?f=2&amp;t=663&amp;hilit=jasper) i mentioned there, with-slots does it for lists as arguments. SETF- there is something i am pretty sure is missing in libs For me stuff i dont find in libs is for instance SETF-, and a whole lot of vector-stuff, on which libraries already conflict in namespace a bit.
STOP))))))))))STOP DAMMIT )))))) STOP ALREADY )
Scheme has a better chance - one can use the dialect SICP uses in pretty much any Scheme.
Did you just paste it into Emacs and hit **M-x count-parens**?
I would say "out of".
So was the first Lisp a Lisp-1?
..or what's so great about having separate class and function namespaces? E.g. "transport" the verb or "transport" the noun? Whatever. (defclass transport () ...) (defun transport () ...)
I thought so too, but later a search suggests Nike has used both: [Text below](http://mikeroussell.com/l/wp-content/uploads/2010/07/nike-just-do-it.jpg) [Text above](http://www.sq1agency.com/blog/?p=436) In case you have interest, here is an analysis of Nike's campaign that I briefly glanced at: [Center for Applied Research: Mini-Case Study: Nike’s “Just Do It” Advertising Campaign](http://www.cfar.com/Documents/nikecmp.pdf)
Enjoy! 
This is righteous!
Hello r/lisp! I wanted to have a compact and easy-to-use unit testing library for Scheme, so I implemented one and added it to Scheme Power Tools. While it may not be as featured as some of the other frameworks out there, I think it works fairly well. I included a few examples on my blog. Enjoy! :-)
Why does cons take up so much time?
It's memory allocation. Try putting malloc in a loop and see how you do. It can be done cunningly, but there is going to be at least some amortized cost in consing a lot. It also means garbage collection. Every time you allocate more memory that's memory that needs to be checked by the garbage collector. Hopefully if you use a whole swathe of temporary memory it'll be nice and quick, but maybe not. It can also be hidden in larger operations, especially if you're not using destructive operators, like: (map (lambda (a) (+ a 1)) (map (lambda (a) (* a 2)) '(1 2 3 4 5))) Creates a pointless temporary list after the first map operation. 
That precaution is a bit outdated now. With modern garbage collectors consing is usually very cheap.
According to the Wayback Machine, the article was introduced to the Internet at this address in 1999.
Somewhere I read that implementations did better than malloc(): they preallocated memory such that it could be swiftly recycled for the next cons call. My MUD (ROM-based) does similarly, using preallocated space with a custom memory manager.
Well yeah, but there's a limit to how much you can preallocate. You also assume all that memory is going to be cons cells. Not unreasonable for lisp, but remember it is a general purpose language and has structures, arrays, numbers, strings and other types it also needs to allocate. Generally if you deallocate a cons cell, you can add it to a large linked list of free cells. At the very least, it's a (hopefully inlined) function call and setting some pointers. Which might be swift, but very possibly could be avoided. Remember cons is only inefficient if something else could do better by a worthwhile margin. Generally that something else is a preallocated array that gets reused, so you skip the function calls entirely.
totally agreed on #2. I've considered spending a weekend and putting together a table on github mapping system to function. Not great, but would provide at least a "what's available" list.
Why the downvotes? I'm genuinely curious here; having given the [source code](https://github.com/mpacula/Scheme-Power-Tools/blob/master/unit-testing.scm) a cursory read-through, I can't see anything eye-stabbingly wrong. Did I miss something?
Could it be because there is an /r/scheme or there is sentiment that [scheme is not a lisp](http://www.xach.com/naggum/articles/3224964049435643@naggum.net.html)? Disclaimer: I have not voted.
It could be, but I'd expect someone to point him to it (or at least post a sufficiently grumpy comment along the lines of "Read the fucking sidebar", in case they don't want to exceed their helpfulness quota). I don't think that's it though; scheme-related posts regularly get proper attention here including Guile release announcements, implementation questions and previous posts from blog.mpacula.com.
Just like Inaimathi pointed out, my previous Scheme-related posts got pretty good attention on r/lisp. As for the downvotes, I wish people who don't like Scheme Power Tools or the testing framework suggested improvements. Since SPT is a relatively young project, all feedback (negative too!) is very welcome.
[Kalyanov Dmitry](https://clgtk2.wordpress.com/) has created installers for threaded builds of SBCL here ([link](https://sites.google.com/site/dmitryvksite/sbcl-distr/)). It works well.
So... to summarize: ---- *"I don't want to hear that Lisp was better than C for some projects at JPL without empirical evidence! Unless it's backed by evidence you should reserve judgement."* "Empirical evidence for those kinds of comparisons does not exist." *"Sheeeeeesh! I know that."* "It doesn't exist because it's wicked hard to make." *"Well **DUH**"* "So… in the absence of potential empirical evidence, demanding that only empirical evidence be considered for a specific comparison is inane^1." *"Strawman!!! It's a STRAWMAN!!! STTRRRraaaaawwwwww !!* ----- Nice playing with you. 1) inane: Lacking sense or meaning (often implying, "to the point of boredom or annoyance")
Me, my brother, and a friend spent 3 days typing in a BASIC Hunt the Wumpus program in from a listing in Byte magazine (saved to cassette). Sometime later my little bro changed all the room descriptions to create "Love Hunt" where you were wandering in the Playboy mansion looking for Hugh Hefner's stash of magazines.
Heh, I found the Orc attack game moderately amusing too. LoL is definitely one of the most entertaining books in a while.
Thanks for, yet again, misrepresenting my position. I had a feeling that you were just trolling all along. Thanks for confirming that.
No misrepresentation, just laying bare the thrust of your complaint, and the absurdity of it. You ranted about lack-of-evidence and then (rightfully) dismissed evidence that laughably showed that Lisp was 'superior' to some other languages. You have claimed that you understand why such evidence is not available, and why anecdotes from experts is the best we have. If you know there is no evidence, and know *why* there is no evidence available, demanding evidence is ... ...?
You still don't get it. Perhaps an analogy will help. Let's say you make a statement that I should be skeptical about without empirical evidence. For this example, let's say that you said, "The moon is made out of green cheese". Furthermore, let's say that we currently do not have the technology to go to the moon (i.e. we cannot actually get empirical evidence). In this context, my argument is this, "we don't know if the moon is made out of cheese or not, but I'm not going to believe you just because you say it is. Furthermore, stop whining because people don't just believe you. You're making statements that require empirical support, which you do not have. If you don't want people to think you're an idiot, then stop making statements for which you have no evidence. Stick to those things for which you do have evidence." That's my argument. None of your attempts to restate it have encapsulated these points. &gt; If you know there is no evidence, and know why there is no evidence available, demanding evidence is ... ...? demanding evidence is perfectly reasonably if the original claim is something about which we should be skeptical by default. The problem is not with me asking for empirical evidence, the problem is with the person making statements for which the necessary evidence doesn't exist. Your characterization of my position is completely wrong. Period. Every single time you have stated my position I have disagreed with what you said. Hopefully, now, you understand it. This is the last time I'll restate my position. one last comment: &gt; You have claimed that you understand why such evidence is not available You seem to be equating "evidence is not available" with "evidence cannot be obtained". I think that there are lots of empirical studies that could be run to help support the notion that one language may perform better than another. This is the basic nature of science. Scientists are skeptical. The question is not whether you can convince yourself about what you believe, the question is whether you can convince your peers about what you believe. There's no reason that empirical studies couldn't be run. Sure, the constraints of running such experiments mean that the conclusions that we can draw will be limited, but at least than can offer us some evidence to back up why we believe what we do. 
As others have pointed out, this version isn't really that much different. On the other hand you're right, modern Lisp does also allow for very different implementations with a lot less parentheses: (defun collapse (l) (loop for x in l when (atom x) collect x else nconc (collapse x))) 
&gt; the problem is with the person making statements for which the necessary evidence doesn't exist This is the [crux](http://wordnetweb.princeton.edu/perl/webwn?s=crux) of your argument... As I said earlier: "If what you're trying to convey is that too few decisions in IT are empirically grounded then I agree 100%. That is not, however, what you said." The problem with your claim that "*[a]necdotal evidence is of no use to anyone.*" is the **[supposition](http://wordnetweb.princeton.edu/perl/webwn?s=supposition)**, where you [beg the question](http://en.wikipedia.org/wiki/Begging_the_question) that empirical evidence is available, is wrong and your claim, therefore, *fallacious*. If such evidence cannot *reasonably* be obtained then your point is entirely extraneous to all practical programming discussions. Your supposition is a positive claim, I therefore am skeptical to it, and empirical evidence should be provided. The existence of a single study showing empirical evidence backing language selection on specialized high-risk high-cost projects would suffice. Language *selection/comparison* is important here as your original claim was in response to the authors claim that "*I attribute this success to... using Lisp while most of the other teams were using C.*" It's a non-trivial problem. Or, in words you might recognize: "*The fact that you aren't willing to (or can't) provide articles that support your position tells me that it is unlikely they exist.*" Hence: Prove it. Show me the money. Provide counter-evidence. Put up, or shut up. ---- BTW: &gt; Sure, the constraints of running such experiments mean that the conclusions that we can draw will be limited, but at least than can offer us some evidence to back up why we believe what we do. It's anecdotal, but some of the posters in /r/lisp seem to disagree: &gt; These programs were all trivial... You know what this tells us about software development in the general sense? *Absolutely nothing*. - &gt; you can prove shocking things with a sufficiently small number of samples. - &gt; No one does controlled studies on large-scale SE projects. It's simply not possible. - &gt; Yeah.. the studies haven't been done. - &gt; ...there are so many variables that he: a) cannot control; and b) doesn't even know about. - &gt; Welcome to empirical studies in Software Engineering.
&gt; The problem with your claim that "[a]necdotal evidence is of no use to anyone." is the supposition, where you beg the question that empirical evidence is available, is wrong and your claim, therefore, fallacious. Complete and utter nonsense. I'm making no presumptions about the existence of of empirical evidence. If someone makes an unsupported claim where no empirical evidence exists, the default position is not to accept the claim at face value; the default position is to reserve judgment. If you really think that pointing this out is a fallacy, then you must think that all of science is flawed because this is a fundamental aspect of science. Sure anecdotal evidence may be interesting, but it doesn't advance our understanding of the phenomena in question. Unless you think that religious arguments advance our understanding of the natural world. 
&gt; You have claimed ... ... that without empirical backing anecdotal data regarding a language [selection] is valueless. If that is true and empirical data is non-existant for all languages on these kinds of projects your argument is reduced to: "we cannot compare/choose between languages". This argument is conceivably true, empirically, but also pointless (philosophically and practically).
&gt; Evolution &gt; Common Lisp -&gt; Clojure Darwin, I am disappoint.
Really? Care to elaborate?
I posted that just after reading the title and misunderstood as usual. I could argue on why I don't consider it an acceptable Lisp, but I don't want to appear like a \`\`language basher''.
No problem. It is an ambiguous title (partly intentionally). Your comment made me look for reasons people dislike Clojure. What I found was: - People that dislike Java and therefore dislike Clojure as it is built on and interoperable with Java - People dislike being forced into the functional paradigm - A lack of documentation - Hard to debug stack traces and compiler errors - Functions have to be declared before they are called Some of these are fair criticisms, others less so. In balance, I still think it's pretty good. Please let me know your argument; I'm curious.
Neither of that (but the Java thing). &gt; # People dislike being forced into the functional paradigm I'm ok with the functional paradigm. &gt; # A lack of documentation Since it's the new *Best Thing Since Sliced Bread*^tm it should have lots of of documentation, and it's a new language, the third point is moot. &gt; # Hard to debug stack traces and compiler errors Don't know, and I wouldn't care either. &gt; # Functions have to be declared before they are called Judging a language for that would be plain stupid. &gt; I still think it's pretty good. It *is* good, but (read below) &gt; Please let me know your argument; [This](http://www.reddit.com/r/programming/comments/fkraw/what_programming_languages_will_succeed_in_2011/c1gofah) and [this](http://www.reddit.com/r/programming/comments/filsf/what_programming_language_to_learn_next/c1g98y2) are (almost all) the reasons I dislike Clojure. TL;DR: Clojure isn't as revolutionary as CL or Scheme. It's still good, but overhyped.
* I don't like the additional syntax and having to balance a mess of {[(}])). It feels like javascript. * It's untyped. CL compilers can give surprisingly accurate compile time warnings and Scheme is just anal about types in general. With Clojure, you are lucky if the type errors are caught even at runtime (most likely as a NPE in Javaland). * Clojure is quickly getting back all the 'needless cruft' that it proudly discarded in the beginning. It's been a year or more since I last played with it but it seems that at least some form of typing along other new features has been introduced. * JVM and all the associated cruft. That could be justified if the JVM had some future (or presence) other than enterprise server rooms but as it is, from my POV, Clojure might as well compile to Cobol.
Since you're asking for perspectives. First, due to my background, I never learned Java (and don't particularly want to now), so it should come as no surprise that I'm hesitant to jump into Clojure when I hear that its big wins include *All your favorite Java libraries, and JVM-style stack traces!* Second, everything I read about Clojure leads me to believe that it's basically Scheme with [], {} thrown in, and a couple of (admittedly useful) things lifted from Arc. This may or may not be true, but it's the impression I get and its kept me from buying into the hype. Third, I use Common Lisp (at play and at work), so I don't have any significant pain related to my tools that would cause me to jump ship immediately. As a result, I'd like to pick up Clojure just for fun, but I'm in no hurry (and honestly, I'm dragging my feet intentionally a little bit to see how Clojure-in-Clojure shakes out).
So, be honest downvoters, did you just read the title and assume the article was about *How Common Lisp evolved into Clojure*, rather than *Porting a game named "Evolution" from Common Lisp to Clojure*?
I did, but I have upvoted and read the article.
This was a good read, and very topical for me since I just typed in the Evolution sample the other day. Regarding ash, I think Conrad uses it so he doesn't have to bother with rounding the result.
He should do `(floor *width* 2)` (or `truncate`) then. Just to note, Clojure's `(round (/ *width* 2))` is not the same as Lisp's `(ash *width* -1)` (for instance, if `*width*` is 3). While looking this up, I also found out that Clojure's `round` rounds half-integers up (Lisp's, of course, rounds them towards evens).
Again, out of my ignorance, what did it take from arc?
Well, I have to take part of the blame for having an ambiguous title. I didn't realise people would take it so badly though!
"towards evens"? As in 3.5 is 4 but 2.5 is 2?
Fair point. "programmable shame" made me laugh ;)
Correct.
Judging from the code in your article it is uglier and less pleasant to program in. &gt; People dislike being forced into the functional paradigm People say that they need functional programming to run code on multi-core CPUs because that is teh future. Let's say in near future will have 10-core CPUs, so parallel code will get 10x speedup in ideal conditions. However you could as well rewrite code in statically-typed language and get 10x speedup with serial implementation. Go figure...
What makes you think it is uglier or less pleasant? I never felt that. If you want to program in straight C and optimise, I'm sure you can achieve a 2-10x speed-up (comparing serial codes). But how much longer will you spend writing the program? How much longer debugging and testing?
 (setf (gethash pos *plants*) t) (dosync (ref-set *plants* (conj @*plants* pos))) (loop repeat 8 collecting (1+ (random 10))) (loop [genes [] i 8] (if (zero? i) genes (recur (conj genes (+ 1 (rand-int 10))) (dec i)))) Clojure versions looks "a bit" more verbose, no? I find that ugly and unpleasant. &gt; But how much longer will you spend writing the program? C is not the only language which is able to compile to fast native code. Check the table [here](http://shootout.alioth.debian.org/u32/which-programming-languages-are-fastest.php) -- there is a plenty of languages which are substantially faster than Clojure and are nicer than C. Even Common Lisp might be a lot faster, in many cases it is able to deliver optimal code when given type declarations and stuff like that. EDIT: I've linked to [quad-core](http://shootout.alioth.debian.org/u32q/which-programming-languages-are-fastest.php) result table first, it is not the best indicator for sequential performance :)
What's up with the spaces between the opening parens?
AFAIK: `fn`, `def`, `defn`, and the `cond` without the implicit `begin` (in Arc it's just `if`), the modernization mania (drop `car` and `cdr` and use `first` and `rest`), maybe something else, but I don't know.
&gt; Clojure versions looks "a bit" more verbose, no? Well, that's not really fair, Clojure doesn't have something like CL's `loop` macro, which is **huge** and can even toast bread. In "pure" Scheme (no macro sugar), the second would be: (do ((i 8 (sub1 i)) (g '() (cons g (add1 (random 10)))) ((zero? i) g)) Which is still longer. CLers have a love/hate relationship with the loop macro. &gt; there is a plenty of languages which are substantially faster than Clojure That's because of the JVM and the `everything is a sequence` thing.
It also uses anonymous function shortcuts which I associate with Arc because that's the first place I saw them, but they might be from elsewhere. Clojure also expands on them a bit. *Arc:* [+ _ 1] *Clojure:* #(+ %1 1) *Scheme:* (lambda (n) (+ n 1)) The above are equivalent in their language. AFAIK, Arc maxes at unary functions this way though, so while it would need (fn (n m) (+ (* n m) 5)) Clojure lets you say #(+ (* %1 %2) 5) It's a small elegance, but I still appreciate it. EDIT: Also, I'm pretty sure Arc still has `car` and `cdr`, they just *also* have `first` and `rest`.
I'm surpised they used GCC as the benchmark. From what I've read, Intel's ICC blows it out of the water in terms of C/C++ performance.
You do realize the JVM runs on most tablets and smart phones, right?
I must be tired. Why does he add `*width*` to a number and then mod out by it in `move`?
the love/hate relationship with the LOOP macro is not really about the functionality. Many CLers think that ITERATE provides the same (and more) features, but with a look&amp;feel that is more like Lisp.
Yeah, that post was a bit messy, I was thinking of "`loop` is not Lispy" when I wrote that.
This is going to sound crazy, but UCBLogo had those function short cuts many, many years ago: ? show map [? * ?] [2 3 4 5] [4 9 16 25] NetLogo &amp; the like have them too. I'd have to see, but it may even be older than UCBLogo; I have an AI2LOGO printout around here somewhere that I would need to check, but I wouldn't be surprised if other Logo's had this too. iirc, you can use `?1` and `?2` for multiple arguments. 
Well, Schemers have been shortening `lambda` to it's Greek-letter equivalent for some time, so I don't think `fn` is *too* crazy. `def` is just looks like `define`, or it could be from lush (which uses `de`, which always looked really ugly to me for some reason). There's probably some other Lisp-dialect that we're all missing that has some of these things that Graham ran into and thought "I know, I'll use this!" As for sequences, Lush had some of these ideas, and I'm sure they're to be found elsewhere. To be honest, my Scheme dialect has the notion of `sequences`, but it doesn't change the base Scheme notion of `pair`: (car '(1 2 3 4 5)) ; _ : 1 - Number(int) (first '(1 2 3 4 5)) ; _ : 1 - Number(int) (first [1 2 3 4 5]) ; _ : 1 - Number(int) (car [1 2 3 4 5]) ; Error: type clash: car operates on PAIRs only `cons`,`cdr`,&amp;c. similarly only work on PAIRs, whereas there is a collection/sequence API if you want to work on the generic notion of `sequence`. I find it keeps certain types of code to a minimum (like having a `map` for strings, vectors, pairs &amp; dicts, or without having to have `map` simple be a `cond` that dispatches on collection type), and I can experiment a lot more (like if I see a bug, I can quickly construct a list of pairs and test it, then use the much more efficient vector in production). I think it's pretty useful, though it's good (even if just for posterity reasons) to have the normal Scheme operators too.
`lambda` is called lambda because it means `λ`. Now we've got unicode, let's use it! (I still use `lambda`, `λ` just for short functions) I could name a parameter `fn` (e.g. `(λ (fn) (fn 'something))`), but I would never give a variable the name `λ`. (except for `((λ(λ)(λ λ))(λ(λ)(λ λ)))`) It's not crazy, but it's the same as `list` for Scheme, it may not be a problem for *me* and *you*, but it could be for someone else. Variable names *are* important, you don't know how much I am frustrated when, coming from Scheme to CL, I can't name my variables `t`. &gt; def is just looks like define My smart-define macro is named `def`, it's short and I would never use it as variable. &gt; (which uses de, which always looked really ugly to me for some reason) One/two alphabetic characters long identifiers in the main languages are bad, really bad. &gt; Scheme dialect has the notion of sequences, but it doesn't change the base Scheme notion of pair: But it doesn't change that a list is still a sequence: &gt; (sequence? '(1 2)) #t Seriously, I don't understand why it's so difficult to just use your implementation's `sequence-*` functions *when* you want your code to be generic on sequences. &gt; (like if I see a bug, I can quickly construct a list of pairs and test it, then use the much more efficient vector in production) `#60(0)` and you've got a 0-filled, 60 elements long vector. Also, you don't use a vector just like you would use a list. Both are sequences, yes, but they are two different data structures, `cdr`ing a vector is inefficient just like iterating over a list by indexing is.
&gt; lambda is called lambda because it means λ. Now we've got unicode, let's use it! (I still use lambda, λ just for short functions) I generally use λ in presentation worthy code, fn when I'm hacking something out on the CLI or in a repl and lambda elsewhere. &gt; I could name a parameter fn (e.g. (λ (fn) (fn 'something))), but I would never give a variable the name λ. That's 100% true; I tend to name mine `proc`, and always have, but that's because I've always thought of Scheme as a lambda calculus of *procedures*. You are 100% correct on this though. &gt; My smart-define macro is named def, it's short and I would never use it as variable. Mine is `define*`, since I don't have that already used in the language (although I often shorten that to `def*`). &gt; One/two alphabetic characters long identifiers in the main languages are bad, really bad. That's lush; all sorts of weird stuff in there. &gt; But it doesn't change that a list is still a sequence: That's true; a list is still a sequence. The only difference is that there are no sequence-specific functions in my dialect; if I want the `nth` of something, that's pretty much what I want to do, rather than have `string-ref` or `vector-ref` or the like. &gt; #60(0) and you've got a 0-filled, 60 elements long vector. For that specific case, sure, but I meant if I'm working with something I can work with lists using `cons` &amp; `car` (or `first` and `rest`) and still use what I sussed out on vectors, or strings. &gt; Also, you don't use a vector just like you would use a list. Well, you can, certainly; as you say, both are sequences, with different costs of doing various operations. I find pairs to be easier to work with, like when building strings or the like; you can pull them apart nicely, manipulate them, &amp;c. nicer than vectors. Vectors are more efficient. So I just use `first` &amp; `rest`, or work with pairs directly, and then in production code just tend to write whatever is most efficient for the specific use case. I don't find it terribly controversial though.
Sooooo... what is it?
&gt; The only difference is that there are no sequence-specific functions in my dialect; Well, that's bad. What are you using? &gt; I don't find it terribly controversial though. Neither do I, but having constructs like `(fn [a b c] ...)` in the language is just stupid. Why mix vectors and lists in the syntax? It's error prone and unnecessary, I have to keep track of when to close a ), when to close a ], when it needs a vector, when it needs a list, etc. Of course, it can be handled by the editor, and I don't know whether it also accepts `(fn (a b c) ...)`. But I'm a programmer, I want to write programs, not to contemplate your arglist*vector*, that syntax gets on the way and doesn't give me anything back. It's stupid, it's just there to say "look, sequences!", it's because of the "new feature? Let's use it!" mentality we're bitshifting crap to `cout` in C++, instead of a saner `cout.write()`.
I remember they had ICC in result table for some time, but new version removed a lot of varieties.
It does not. iOS is all Objective-C. Android has Dalvik. Sure, you can run any JVM language on Dalvik after converting the JVM bytecode to Dalvik bytecode using dex, but that's not the same as saying the JVM itself -- the One True JVM from Oracle -- runs on Android. In any case, Google probably used Java for Android because Java has a large mindshare. They can always replace Java with something else that compiles to Dalvik bytecode. Symbian has some sort of JVM, but nobody cares about Symbian at this point. Windows Phone 7 is all .NET. HP's WebOS is HTML/JavaScript/CSS. Blackberry's QNX-based OS is pretty much vaporware. So the only places the JVM still runs are existing Symbian phones. Does anyone develop for Symbian anymore? I'm certain the phrase "I love J2ME" has never been uttered in the history of the universe.
One of plug-ins interpreted them as footnotes if I didn't put a space in :(
Even if this statement would be true, it wouldn't mean you can get Clojure running there. There were several posts on Reddit about Clojure on the most attractive target (which is Android): results were very humble. See [this](http://dev.clojure.org/display/design/Android+Support) or just google "clojure+j2me".
&gt; Clojure doesn't have something like CL's loop macro And whose problem is that? Scheme has an excuse of being a minimalist educational language. Clojure has no such excuse. Note that Racket, a Scheme dialect designed for practicality, has more convenient looping: `(for/list ([i (in-range 1 8)]) (add1 (random 10)))` &gt; which is huge and can even toast bread I wasn't asking for LOOP exactly like in CL, but just a construct(s) which handle it better than plain recursion (oh, `recur` is very ugly too). It really has nothing to do with functional programming either. Haskell is much more functional than Clojure, but it is nowhere as ugly: do gen &lt;- newStdGen return (take 8 (randomRs (1, 10) gen)) So loop itself boils down to just `take 8`. That's even better than `(loop repeat 8 collecting`. BTW, in another code snippet, one might argue that `dosync` and `ref-set` provide additional value of making it magically thread-safe. Hell no, for this particular use case where you synchronize operations on a hash table you could just write `(make-hash-table :synchronized t :test 'equal)` in SBCL and it will automatically be synchronized. Not in CL standard, of course. &gt; That's because of the JVM That's not an excuse -- according to shootout JVM is just 1.5 times slower than GCC C. &gt; and the everything is a sequence thing. I'm not a Clojure expert, but according to TKN Clojure is "untyped". That might be the problem -- dynamically-typed languages just cannot be fast without compiler trickery (not possible on JVM, I think), and even then, I guess, it cannot be ideal. But I guess "sequences are immutable" would cause additional performance problems on top of that.
&gt; Note that Racket, a Scheme dialect designed for practicality, has more convenient looping: (for/list ([i (in-range 1 8)]) (add1 (random 10))) I know, I use it everyday. I still prefer a `do` loop or a named `let` (which is like Clojure's loop/recur, with the difference of not being an ugly hack) &gt; oh, recur is very ugly too Wasn't `recur` an hack to obviate the lack of full TCO? It's the same of a named let: `(let loop ((i 8) (genes '())) (if (zero? i) (reverse genes) (loop (sub1 i) (cons (add1 (random 10)) genes)))`. &gt; It really has nothing to do with functional programming either. Never said that. &gt; BTW, in another code snippet, one might argue that dosync and ref-set provide additional value of making it magically thread-safe. Clojure promises to support persistent thread-safe *some more buzzwords here* data structures, it shouldn't even need `dosync`. &gt; That's not an excuse -- according to shootout JVM is just 1.5 times slower than GCC C. You can use the server one, which is a memory hog but it's 1.5 times slower than C, or the client one which is *slow as fuck* but doesn't consume too much memory.
&gt; You can use the server one, which is a memory hog but it's 1.5 times slower than C, or the client one which is slow as fuck but doesn't consume too much memory. If you're referring to `Java 6 -Xint` participant of shootout, it is slow as fuck because it is forced in interpreter mode, not because it is client. Otherwise client uses JIT-compiler and isn't very different from server -- server just uses more aggressive tuning settings which make sense in a long run.
Oh, didn't know that, disregard that part.
Ah, ok. :&gt;
I'm not at all surprised that I missed this; the sum of my Logo experience was half a week of using it to generate trippy color patterns in a communication design course back at school.
&gt; Note that Racket, a Scheme dialect designed for practicality, has more convenient looping: `(for/list ([i (in-range 1 8)]) (add1 (random 10)))` That is actually a list comprehension, which Clojure has as well. The equivalent code would be (for [i (range 1 8)] (inc (rand-int 10))) Though it should be noted that `clojure.core` only has _list_ comprehensions, while Racket has them also for other sequences. For other kinds of sequences one would have to `reduce` them from the list. On the use of HOF instead of looping: any functional programming language can do that, including Clojure. Here would be the equivalent snippet, disregarding that it doesn't yet have generators/streams (which is currently under work). I would hazard a guess that Clojure will never get an equivalent of the `loop` macro accepted in its core, with the preference being towards just using functions and sequences. (take 8 (repeatedly 10 #(rand-int 10))) The performance does suck tremendously, only looking good next to Python, Ruby and the like, and only now are efforts being concentrated in solving that. There exist type hints aimed at giving better performance (but not type safety) for primitive/Java types, and the use of Java's reflection seems to be the source of much headache, but I wouldn't be able to give further details. Of course, to palliate they offer a 'solution' similar to that `recur`/`trampoline`: offload it to the programmer, who has the choice to use Java arrays of primitive values, which would be the equivalent of doing unsafe math operations in native Lisp implementations. edit: lack of proofreading edit: My parens were unbalanced! Only suicide will restore my honor.
So that the x position can't be negative. If you were at x = 0, then moved left you would be at -1, which should wrap round to width -1.
&gt; On the use of HOF instead of looping: any functional programming language can do that, including Clojure. I don't think so. It is easy to do this in Haskell because it is lazy. Non-lazy language would need either more complex construct (essentially imitating laziness) or some artificial feature like generators.
&gt; I don't think so. It is easy to do this in Haskell because it is lazy. Non-lazy language would need either more complex construct (essentially imitating laziness) or some artificial feature like generators. Well, most of Clojure's sequence munching functions operate and evaluate to lazy sequences, like `map` or `partition`. The `repeatedly` function promises to do just that, returning either an infinite or bounded-by-n (10 in my example) lazy sequence of applications of the function. Of course, being based mostly on convention, it won't really give you peace of mind and you'll have to be on the lookout, but usually you'll be able to operate like that.
I understand why the mod is taken. But isn't `(mod (+ n m) m)` exactly the same as `(mod n m)`?
I'm a bit of a Logophile; I find it to be an under appreciated tool (esp. interesting dialects like PowerLogo, NetLogo, &amp;c.) and I've used it quite a bit. Simulation &amp; modeling, some network programming, &amp;c. Lots of fun. 
&gt; Well, that's bad. What are you using? Generally, `nth` &amp; the like. I don't find it bad really, and I actually think it's quite useful. Of course, the constructors are still there (`list`, `vector`, `string`,`dict`), so there's that, but I tend to use `empty?` and `length` over individual tests anyway. The end goal is for user types to support something akin to CL's generics, but I'm still playing with a nice way of doing so that isn't bloated, isn't silly yet still holds CL's niceties. &gt; Neither do I, but having constructs like (fn [a b c] ...) in the language is just stupid. Well, I'm not *that* far down the garden path; Lambda's look like normal lambdas (save that I have `pre`, `post`, `invariant`, `variant` and `chr` riders, as well as the normal array of `body`,`rest`,`opt`). &gt; Why mix vectors and lists in the syntax? It's error prone and unnecessary, I have to keep track of when to close a ), when to close a ], when it needs a vector, when it needs a list, etc. Of course, it can be handled by the editor, and I don't know whether it also accepts (fn (a b c) ...). But I'm a programmer, I want to write programs, not to contemplate your arglistvector, that syntax gets on the way and doesn't give me anything back. For normal programming, I don't see the point of changing from `[]` to `()` &amp; back again. The point, for me, of having added literals is just for ease of use, and I can visually tell what type I'm using. I also find that mixing dicts with SRFI-88-style keywords is quite nice, and gives me the same benefits languages like Python &amp; Ruby have, whilst staying within Scheme where I am most happy. &gt;It's stupid, it's just there to say "look, sequences!", it's because of the "new feature? Let's use it!" mentality we're bitshifting crap to cout in C++, instead of a saner cout.write(). You could certainly write everything in my dialect using no added literals; `vector` &amp; `dict` work just as you would expect (as do `make-vector` &amp; `make-dict`). I think having the two is helpful, and it eases transition from Scheme proper (since there's nothing really stopping a minimal version of my dialect from working on Scheme proper; just a few forms &amp; some data structures really).
&gt; The point, for me, of having added literals is just for ease of use, and I can visually tell what type I'm using. Scheme has vector literals too! Also: &gt; #(2) '#(2) &gt; (vector 2) '#(2) &gt; (define x 2) &gt; #(x) '#(x) &gt; `#(,x) '#(2) I found this 20 minutes ago. Tested with Racket and MIT/GNU Scheme, so it should be standard. Probably I was the only who didn't knew. &gt; (save that I have pre, post, invariant, variant and chr riders, as well as the normal array of body,rest,opt). You got me interested, what do they do? &gt; The point, for me, of having added literals is just for ease of use, and I can visually tell what type I'm using. I also find that mixing dicts with SRFI-88-style keywords is quite nice, and gives me the same benefits languages like Python &amp; Ruby have, whilst staying within Scheme where I am most happy. That's the joy of Lisp, you can make it look like what you want. &gt; You could certainly write everything in my dialect using no added literals; vector &amp; dict work just as you would expect (as do make-vector &amp; make-dict). I think having the two is helpful, and it eases transition from Scheme proper. I agree. IMHO they should be library syntax, reader macros exported from a `somemodule/vector-literals`/`somemodule/hash-literals`. &gt; (since there's nothing really stopping a minimal version of my dialect from working on Scheme proper; just a few forms &amp; some data structures really). I'm really interested. Are you going to publish the source someday? In what it's written? C, C++, Scheme?
&gt; Scheme has vector literals too! Also: I've seen some of this before, but I've never been a huge fan of the `#()` syntax. The quasi-quote on a vector is pretty interesting, and new to me too. &gt; You got me interested, what do they do? pre &amp; post are pre &amp; post conditions, and can be used similarly to their function in other languages that have them, save for mine are logic operators (based on Kanren &amp; Curry). Please note I'm still playing with the syntax: ; I don't know why you'd want this, and it's pretty early in the morning... (define (myfoo x y :pre (~all (~&gt;= x 0) (~&lt;= y 10)) :post (=:= myfoo (+ x y))) (+ x y)) Basically, do you succeed in saying that x can unify to something greater than or equal to 0, and y to something less than or equal to 10? Then apply the function, but afterward, make sure the result (which has the same name as the function) is equal to `(+ x y)` (which is a really stupid example). I usually use `invariant` with dicts (as structures) to make sure things don't change from within a function; I don't have a large body of code yet that uses them, and it's mostly experimental. `chr` is just a generic Kanren, basically; you can put anything that returns goals (`#s`/`#u`) in there, and have access to the environment up until the function itself was called; if the goals aren't satisfied, the function isn't run. I'm not terribly happy with the way it works now, but it's a start. &gt; That's the joy of Lisp, you can make it look like what you want. That's true; not only can you make it look like you want, you can make it *act* like you want too. It's a pretty nice system in general. &gt; I agree. IMHO they should be library syntax, reader macros exported from a somemodule/vector-literals/somemodule/hash-literals. Well, mine are baked in, but I was toying with a hyper-minimal dialect that just exposed the read table; I'm not sure I like it too much though. &gt; I'm really interested. Are you going to publish the source someday? In what it's written? C, C++, Scheme? Yes, I think so (that's been the goal since the beginning four years ago or so). The run time &amp; base interpreter are written in C; there are several experimental interpreters that self host, and I have two compiler projects (one, the restricted subset version is pretty usable, the other, a much more aggressive compiler for the full system is a work in progress). I also have a project to rewrite the run time in itself, using the C FFI (and I'd rewrite the reader system too, something like [#f](http://code.google.com/p/false/)). I use it daily as a drop in replacement for Python, and I'm constantly working on modules (which is another reason to support pre/post contracts, so that we can have better guarantees about our code). I also have a grid setup that uses it; VFS/RPC, user applications, grid "cpu" server all written in Scheme. I have a few ideas for computational domination :D Once the grid is more stable, I'll give you an account, and we can discuss it.
&gt; pre &amp; post are pre &amp; post conditions, and can be used similarly to their function in other languages that have them, save for mine are logic operators (based on Kanren &amp; Curry). Oh. So, they are basically contracts, right? &gt; ; I don't know why you'd want this, and it's pretty early in the morning... Masochism, or just coffee can't wake me up, or both. &gt; I also have a grid setup that uses it; VFS/RPC, user applications, grid "cpu" server all written in Scheme. I have a few ideas for computational domination :D You'll end up writing a Lisp OS someday, I'm pretty damn sure.
&gt; Oh. So, they are basically contracts, right? Exactly, these are all forms of DBC, only I use a logic language behind the scenes, rather than simple asserts or anything else. &gt; Masochism, or just coffee can't wake me up, or both. Well, yes, it's 03:24 here, and I'm waiting for some test data to finish running. &gt; You'll end up writing a Lisp OS someday, I'm pretty damn sure. Don't even get me started on my microKernel projects. If drivers weren't such a PITA I'd be working on it now.
&gt; Well, yes, it's 03:24 here, and I'm waiting for some test data to finish running. Here it's 9:35 and I have nothing to do. &gt; Don't even get me started on my microKernel projects. I knew it. &gt; If drivers weren't such a PITA I'd be working on it now. I know what you mean, I had some ideas and wrote some code, but the drivers were always a problem. If you start a new OS project and you're nobody, no one will ever help you. (see LoseThos, ReactOS) If you're someone, everyone will go "omg so cool the^best^thing^since^sliced^bread^tm it is awsum i hate mah system everyone must use this" (see Plan9: tries to fix the *UN*f*IX*able)
&gt; If you start a new OS project and you're nobody, no one will ever help you. Actually, it'll be worse, since they'll *offer* to help, and you'll count on it, and then nothing will materialize, because they got distracted by the next case of shiny. &gt; If you're someone, everyone will go "omg cool shit thebestthingsinceslicedbreadtm " (see Plan9: try to fix the UNfIXable) I must say though, at least 9fans generally follow through (and octopus/omero/planb++ is pretty awesome).
&gt; Actually, it'll be worse, since they'll offer to help, and you'll count on it, and then nothing will materialize, because they got distracted by the next case of shiny. True. &gt; (and octopus/omero/planb++ is pretty awesome). And 9P is `everything is a file` done right.
No, not if n is negative. It's just so that -1 wraps round.
Actually, I just tried this and they are equivalent in Common Lisp (at least in GNU CLISP). However, if you use rem they wouldn't be equivalent. I haven't tested Java or Clojure, but Wikipedia suggests Java *wouldn't* be equivalent but Clojure is. This bit of code is a direct copy from the Conrad's Lisp original; I didn't think about it. The Wikipedia page is http://en.wikipedia.org/wiki/Modulo_operation
Best day of the month or best day ever?
Both? :)
&gt;I haven't tested Java or Clojure, but Wikipedia suggests Java wouldn't be equivalent but Clojure is. If you are programming in Clojure, why would you care what _Java_ thinks?
I'll have to point this out at the Toronto Lisp Users meeting tonight.
I gave an [exercise](http://programmingpraxis.com/2011/03/01/an-early-lisp-program/) at my [blog](http://programmingpraxis.com) in celebration of the anniversary.
I don't really. I was just pointing out that there was a reason to suppose negative dividends would return a negative answer. Also, if you were to call out to the Java mod function from Clojure, you would have this problem.
The code and the data are one.
&gt; ...a feat I think “LoL” might achieve too. So I guess LoL no longer officially means "Let Over Lambda"? Or are we now differentiating between LoL and LOL? Edit: +1 to the first person to make a case-insensitivity-related joke
Rename \`\`Let Over Lambda'' \`\`Let over IP'', call it LoIP, problem solved (and networking achieved).
Man, that video was drugs. I'm pretty well versed in scheme, but I've been looking to learn more about CL, so I'll probably pick up the ebook. The cartoons in the book look awesome, too.
(I'm not a real programmer -- I like to fool around, but I'm not employed writing code.) I really like the book. I've worked through most of it. I think it really helps to hear stuff a few times from different people who have different takes on things, and on that level, this book helped me out a lot. To give an example, in the part of the book, early on, where he makes the game engine, he doesn't use strings to hold the messages: (living-room (you are in the living room. a wizard is snoring loudly on the couch.)) The reasoning behind doing it that way, instead of using strings, seems to me to be exactly the sort of thing I want to learn, and it's the sort of thing you don't really get from studying the syntax of the language in isolation. It a lot of ways it reminds me a lot of stuff I read when I was a kid -- Creative Computing books about games in BASIC, and even Ted Nelson's Computer Lib/Dream Machines. It has that same hippie/underground comics sensibility to it. When Ted Nelson's book came out, people didn't have computers, for the most part, only institutions did. And one of the thrusts of the book was that individual people should be empowered by computers. Which was pretty prescient. Land Of Lisp is sort of like that too. It has that "consciousness raising" vibe to it, that idea that you should know how to code in this great language. I do think the book has a couple of drawbacks. He takes a few shots at stuff -- I notice the ones he makes at Ruby, because I know Ruby. He's probably right, on some level, but it's kind of annoying and off putting. If the point is to bring people into Lisp, making fun of them doesn't help. And I think the text based game thing might put young people off. I don't know, though. For me, it makes the thing kind of satisfying from a nostalgic point of view. I don't know how a 14 year old kid would take it, though. 
This was a great comment. Have an upvote.
I am the author of that response to the Quora question an I'd just like to make a couple points clear * this was a sort of "algebratization" of lisp. It is not really a substitute for any sort of formal evaluation semantics (be them operational or denotational). * I hope it is clear that the post refers to McCarthy's original lisp. This does not have some of the fun features of today's lisps, like closures or statefullness or continuations * The fundamental principle of the post is that we can write S-expressions with associated operations (M-expressions) as an algebra, then we can make functions QUOTE: M -&gt; S and EVAL: S -&gt; M which constructs a bijection between the two.
This was a great comment to a great comment. Have an upvote.
OBJECT-TYPE might have been a better name than TYPE-OF, but certainly not TYPEP. That makes no sense at all. PS: Ah, I get it, you're confused because type predicates usually have the type we're looking for "baked in", so they usually take only one argument. I had never considered being confused by this. o_o
Ditto. I like Ruby and didn't find any of LoL off-putting at all. (Other than it ended before we had written a full-graphic Half-Life implementation in Lisp).
I'm not sure Let Over Lambda ever achieved "known by its initials" fame.
&gt;If you hate fun, stick with Practical Common Lisp. That was a poor choice of words, IMO. If you hate flippant presentation perhaps, but PCL was quite fun, and would still be my recommendation out of the three. 
You guys are probably right -- I take your point. Thanks.
What is the type of a value that has no type? (unless (typep foo) (live-together-in-harmony 'dogs 'cats)) Seriously, could TYPEP ever return NIL? [This](http://maclisp.info/pitmanual/typep.html) claims that even TYPEP of NIL returns SYMBOL, but what fun is that?
No one with real experience on this has posted yet, and I wondered about this a while back too, so here are my thoughts from then after I read up on it. &gt; I read a piratebay comment about how it was the best IDE/OS ever, for it's time. FTFY - Seriously, Genera was more of an operating system with an IDE-like tool on top of it. That would be cool if you like your stack to be "turtles all the way down". Unfortunately that approach really isn't practical anymore given the state of the industry. That said, I have seen another example of that in the [Oberon operating system](http://en.wikipedia.org/wiki/Oberon_%28operating_system%29) and, if you want to take the old cliche joke more seriously, in emacs for users that do everything in emacs; though it is a different experience and more limited than Genera. Genera had design shortcomings around concurrency and apparently so did Oberon; I guess emacs may be the exception there, but only because it's normally hosted on a multi-user OS. I don't know why there was such a design focus on single-user systems. It seems like such a limited approach to me. I would guess that partially is because of the "turtles all the way down" approach. If you're using an OS that let's you dynamically modify code for any part of it all the way down, then suddenly you really have some deep concurrency issues. I guess it was easier to just not deal with it and keep the extreme flexibility instead. I would definitely read the whole Wikipedia article and its citations. You can apparently also go download Genera and try it for yourself, but I suspect you'll find that to be a somewhat empty experience without the ecosystem and community that went around it at its peak. tl;dr = I think the hyperbole you've seen is driven by nostalgia more than reality. There's definitely some very cool ideas in Genera and it was very much ahead of its time, but it had problems too from what I could see.
A language construct for distributed processing? Sounds pretty cool, actually...
Sure it did. Not on the level of e.g. SICP or HTDP, but it holds its own. http://www.google.com/search?q=%22let+over+lambda%22+%2Blol &gt; About 20,700 results 
It would make sense if there was a type type, and typep tested an object to see if it was a type.
Try reading [this thread](http://groups.google.com/group/comp.lang.lisp/browse_thread/thread/5e9bd4c05db129b9/b2c0190dc30c3e5f?lnk=st&amp;rnum=1&amp;pli=1) and paying particular attention to the comments by Kent M Pitman - he describes some of the reasons pretty well.
You can check out a [video](http://lemonodor.com/archives/000103.html) of what it is like to use genera over at Lemonodor. Some of the interaction in zmacs reminds me a bit of the [Plan 9 system](http://www.youtube.com/watch?v=SoGLU1l7LwY).
I own two machines which run Genera. It was the best IDE ever, and kind of still is (up to lack of modern things). Let me clarify. It is *not* the ideal IDE for your enterprise Java team, not because it doesn't have Java. It is the perfect IDE for a hacker (curious/creative programmer, not "software cracker"). The IDE is very *smart*. It manages this because nearly every component can interact with every other component of the system. But what does it mean to be smart? For Genera, it means figuring out what you want. Let's look at "intellisense" as an example. There are various levels of "smartness" that intellisense can achieve 1. It can look up a pre-defined dictionary of symbols to tab-complete on 2. it can measure which symbols you've typed by frequency and give you a list of the most likely completion candidates 3. Check the current scope, form, namespace, package, complete based on that. The first two things are actually rather *unintelligent*. The most intelligent would be the third. It gives you the only legal, valid completion candidates based on the context. It's *context sensitive*. Now take this context sensitivity, and apply it to just about everything. Tab completions, mouse clicks, mouse menus, help, bucky bits (control, alt, meta, etc.), the editor, the REPL...everything. This is just one aspect of Genera that makes it great. Another is that it's entirely modifiable, dynamically changeable, etc. Everything. In fact, when you write/load a program, you're *really* extending the operating system. Since the OS is written in high level lisp, not low level ASM/C. it's impeccably clear and organized. You get object orientation down to the metal. Garbage collection. All those lispie yum yums. The file system acts as version control. You save a file, edit it, etc. It'll save successive copies, which are accessible later. Paths ("/foo/bar") are very integrated. Paths, like today, can refer to almost anything in Genera, even another Genera system. Or other hardware. Or files, or most anything. This isn't as surprising today, considering we can do much the same thing, but this was in the 1980s. Absolutely nothing compares. The closest thing to Genera might be emacs turned into a bootable operating system, with a better lisp dialect, But this is still way off. As a final note, I apologize if this comment is a little "all over the place" or incoherent. It's just difficult to really *really* convey what it's like without people saying "well, visual studio has intellisense" or "my operating system is modifiable" or etc. **EDIT**: Also, that comment on TPB is actually a comment from one of the employees at (now defunct) Symbolics.
That was a pointless followup to a mediocre followup to a great comment. Have a downvote.
If we keep this up, there's gonna be a stack overflow of a karmic kind, and then the haters will start with the whole "Common Lisp lacks tail recursion" bit. So upvotes for all upstream posts!
Ha! Common Lisp lacks tail recursion, have a downv`*** Exception: stack overflow`
With `write`, `read` and `eval` you can get `λoIP`.
In comparison, Land of Lisp returns: "About 34,000 results".
Wha? I use tail recursion in Common Lisp all the time...
The standard doesn't guarantee TCO, but most implementations will optimize your tail-calls anyway. For CL, it's just another optimization, like lambda-lifting.
I'd never done any lisp (besides a teeny amount of scheme) before I picked up Land of Lisp, so I think I was the audience that it was directed at. Unfortunately for me, it failed to fulfil its purpose. It had some critical problems that made learning using Land of Lisp an unenjoyable experience for me. Most of this was due to the authors writing style. When the author wished to explain a new concept, he'd create an objective within the example game he was writing, such as having the room descriptions in his text based game parse into a readable format. He'd then give the reader an example of the code used to solve the problem, and then he'd explain why he used that particular code, and how it worked. On the surface, this seems like a perfectly reasonable way to teach, but it meant that it took intuition out of my hands. What should have happened, which is what happens in the majority of other programming tutorials, is that the reader is supplied with a number of programming tools, then they're given an objective, and THEN the author shows a manner in which the objective can be completed. Because the author showed me how to achieve my goal before he taught me how to get there myself, all of his examples were useless. The purpose of an example is for the reader to attempt to figure out how they'll solve the problem themselves (Like in real programming), rather than to show off the authors ability, and then to explain why what the author did was the correct solution. That this occurred ruined LoL for me. Had I already understood CL, it'd have been fantastic, as I could have attempted the examples myself before being spoonfed an answer, so as a supplement to learning CL it is a fairly solid book. Also the comics and pictures are awesome. It failed to teach me Lisp effectively however. I had to go to other books instead (and by the time I'd completed those books, it made LoL a little irrelevent because so much of it was explaining old concepts over again.)
Those variables are not global but _special_ and there is nothing wrong with them per se and they seem to be excellent tool for the job in this case. If defparameter bothers you you can do same thing with `(declare special)` inside the function, but I don't think it makes things any better. Usually it is possible to restructure algorithm to use something else instead of special variables, but I'm not familiar with algorithm enough to say anything concrete on this matter. As for superficial code comments you do not seem to be aware of MEMBER function and PUSH/POP macros. They will make your code a bit more concise. E.g. (setf *output-queue* (cons stack-topmost *output-queue*)) becomes (push stack-topmost *output-queue*) Then, the part with `determine-precedence` is messy without much need. Why do you check whether priority is number, didn't you already know it because you wrote it like that? You can also name it instead of using car and cdr all the time. E.g. (destructuring-bind (token-prio token-assoc) (determine-precedence token) (destructuring-bind (stack-prio stack-assoc) (determine-precedence stack-topmost) (when (if (eq token-assoc 'L) (&lt;= token-prio stack-prio) (&lt; token-prio stack-prio)) ... 
&gt; Absolutely nothing compares. Smalltalk environments, if you ignore the OS outside the environment (which tends to be irrelevant to your Smalltalk and mostly a bootstrap), do.
Of course you're right, but I have a question about the use of (not (null x)). Say that I'm making a predicate function for some class, basically boiling down to checking if one of the slots is nil or not. As I understand, it's good style if a predicate function just returns either T or NIL. So, how would this be done? (when foo t)?
Practical Common Lisp is probably still the better starting Lisp book. Maybe read that and then give LoL another shot?
It definately is. I've read through both of them now, and LoL is fairly good at being complementary. It just doesn't fulfill its purpose as the introductory textbook it was advertised to be.
Why remove special variables (globals)? Why not put everything in a package?
&gt; #'(lambda ...) is redundant. Just use (lambda ...). I heard from the Lisp gurus on the #lisp channel to always use #'(lambda ...). IIRC, it has something to do with lambda the macro vs lambda the function in different implementations; in other words, (lambda ...) is less portable than #'(lambda ...).
&gt; \#'(lambda ...) is redundant. Just use (lambda ...). Isn't this the lambda macro that "yields a specious sort of elegance at best"? :-) [1] [1] Graham, *Ansi Common Lisp*, p.402
*scratches his head*
Right; one is "LOL", and the other, "LoL".
&gt; When the author wished to explain a new concept, he'd create an objective within the example game....He'd then give the reader an example of the code used to solve the problem, and then he'd explain why he used that particular code, and how it worked. Really? Does anyone want to comment on this?
Ommmmmm
Smalltalk might be comparable to lisp, but I wouldn't compare it to Genera. 
I guess it depends on the type of person/beginner. According to that, each book might be better and then again, other books might even be better to start. IMHO, PCL is a good second book to start learning although it ranks #1 for best Lisp book. But like I said, that's my opinion.
You often times have redundant tests for things. For instance, you check to see if the precedence level is a number when you know darn well that it is because you already checked to make sure that the token was an operator and you DETERMINE-PRECEDENCE function returns a number for all operators. This redundancy doesn't buy you robustness, it just makes your code harder to read. Lot's of (progn (setf a (cons (car b) a)) (setf b (cdr b))). This is better written as (push (pop b) a). Also I think it is fair to replace any (if (not (null a))...) with just (if a...). There are times that verbosity translates to readability, but I find that a much more general rule of thumb is shorter means more readable in CL. Here is my cleaned up version of your implementation. (defun precedence-of (operator) (ecase operator ((+ -) 2) ((* / %) 3) (^ 4) )) (defun left-associative-p (operator) (member operator '(+ - * / %)) ) (defun right-associative-p (operator) (member operator '(^)) ) (defun shunting-yard (stmt) "Implementation of the simplified Shunting-yard Algorithm, for converting infix-notation mathematical equations into postfix-notation mathematical equations." ;; Fill *output-queue* *operator-stack* (let ((token (car stmt))) (cond ((numberp token) (push token *output-queue*) ) ((eq token '\() ;; This a start of some grouped operators (push token *operator-stack*) ) ((eq token '\)) ;; Closing paren flushes the grouped operators to output (loop for stack-topmost in *operator-stack* until (eq stack-topmost '\() do (progn (push stack-topmost *output-queue*) (pop *operator-stack*) ) ;; And finally pop the open paren off the stack finally (pop *operator-stack*) )) ((member token '(+ - * / % ^)) (let* ((stack-topmost (car *operator-stack*))) ;; If precedence is lower than the one on the stack, I pop what's ;; on the stack and send it to output. This looks like an error, ;; shouldn't I send all higher precedence operators to output? (when (and (member stack-topmost '(+ - * / % ^)) (cond ((left-associative-p token) (&lt;= (precedence-of token) (precedence-of stack-topmost) )) ((right-associative-p token) (&lt; (precedence-of token) (precedence-of stack-topmost) )))) (push (pop *operator-stack*) *output-queue*) )) ;; Push all operators onto the stack (push token *operator-stack*) ))) ;; Recursion (when stmt (shunting-yard (cdr stmt))) (loop while *operator-stack* do (push (pop *operator-stack*) *output-queue*) ) (unless (or stmt *operator-stack*) (setf *output-queue* (reverse *output-queue*)) ) *output-queue* ) Which brings up the fact that I am pretty sure you have a bug in your code, though I didn't know anything about shunting-yard before this morning so I'm not sure. You have to clear the stack of all higher precedence operators, not just the top one. Try for instance: (test-sy '(10 * 3 ^ 1 + 4)) ==&gt; (10 3 1 ^ 4 + *) I would expect: (10 3 1 ^ * 4 +) While you are using special variables, you are pushing and popping them but never binding them, which means they lose all of there specialness and basically act like global variables. Let me think of how to do this in a cleaner way (or see if someone has already posted something).
Why not? There are some differences (no integration of paths for instance), but the part about the whole environment being a live, inspectable, alterable running system with context sensitivity and everything seems to apply pretty well to me. It's a much better fit than &gt; The closest thing to Genera might be emacs turned into a bootable operating system, with a better lisp dialect for sure, a "bare metal" smalltalk environment would be pretty damn close I think.
I actually had a Smalltalk background by the time I saw and started playing with Genera. I think that it has *some* elements that compare pretty well, but it still isn't at the same level. For instance, Smalltalk has things like drop-to-frame, specialized object inspectors and the ability to change both code and data for a suspended program and have it resume with the changes taking immediate effect (note that my experience was in the early 1990s). These are good features, and I don't know of any systems outside the two we are discussing that supported this at the time (even Java, now, doesn't support these things unreservedly, at least it didn't the last time I used it two years ago). However, a few things that Genera has that Smalltalk doesn't have are pretty significant: the entire UI built on top of Dynamic Windows. This lets you do things like point to a UI or graphical element in your application when you run it, and obtain an inspector on the data structure that underlies it. None of the Smalltalks I've ever used had anything that compared to intellisense either: you want to find all-senders of a method, you get all methods in the system that dispatch on a method with that name, whether or not they actually do: there's no intelligent way to restrict this search to the classes that actually interact with the specific method you are indicating.
Here is my well thought out approach: (defun %shunting-yard (stmt stack) "Implementation of the simplified Shunting-yard Algorithm, for converting infix-notation mathematical equations into postfix-notation mathematical equations." (cond ((null stmt) stack ) ((numberp (car stmt)) (cons (car stmt) (%shunting-yard (cdr stmt) stack)) ) ((member (car stmt) '(+ - * / % ^)) (cond ((and stack (left-associative-p (car stmt)) (&lt;= (precedence-of (car stmt)) (precedence-of (car stack)) )) (cons (car stack) (%shunting-yard stmt (cdr stack))) ) ((and stack (right-associative-p (car stmt)) (&lt; (precedence-of (car stmt)) (precedence-of (car stack)) )) (cons (car stack) (%shunting-yard stmt (cdr stack))) ) (t (%shunting-yard (cdr stmt) (cons (car stmt) stack))) )))) (defun shunting-yard (stmt) (%shunting-yard stmt nil) ) *edit* Forgot parenthesis and that seems difficult to implement. I think the way you did it doesn't handle nests properly. 
 (let ((x 2)) (write `(lambda (x) (+ ,x x)) other-lisp-listening-on-this-port)) (eval (read im-listening-here))
A bare metal lisp is akin to saying windows and linux are close because they both sit atop an x86. Genera really is more than an IDE. And objects being inspectable is lisp thing, not Genera.
okay, final post, I promise, your way of nesting parens is correct, RPN is just hard for me to reason about. Had do pull some list manipulation out of my toolbox library. Here it is with parenthesis, no special varialbes: (defun split-on (fn lst) "Split the LiST at the first element where function, FN, is non-NIL. Lists returned as multiple values." (let ((acc nil)) (do ((src lst (cdr src))) ((or (null src) (funcall fn (car src))) (values (nreverse acc) src) ) (push (car src) acc) ))) (defun operatorp (op) (member op '(+ - * / % ^)) ) (defun %shunting-yard (stmt stack) "Implementation of the simplified Shunting-yard Algorithm, for converting infix-notation mathematical equations into postfix-notation mathematical equations." (cond ((null stmt) stack ) ((numberp (car stmt)) (cons (car stmt) (%shunting-yard (cdr stmt) stack)) ) ((eql (car stmt) '\() (%shunting-yard (cdr stmt) (cons (car stmt) stack)) ) ((eql (car stmt) '\)) (multiple-value-bind (group rest) (split-on (lambda (x) (eql '\( x)) stack) (append group (%shunting-yard (cdr stmt) (cdr rest))) )) ((operatorp (car stmt)) (cond ((and (operatorp (car stack)) (or (and (left-associative-p (car stmt)) (&lt;= (precedence-of (car stmt)) (precedence-of (car stack)) )) (and (right-associative-p (car stmt)) (&lt; (precedence-of (car stmt)) (precedence-of (car stack)) )))) (cons (car stack) (%shunting-yard stmt (cdr stack)))) (t (%shunting-yard (cdr stmt) (cons (car stmt) stack))) )))) (defun shunting-yard (stmt) (%shunting-yard stmt nil) ) *edit:* somehow I grabbed the wrong list splitting function... I just read the doc string. It is weird that it appeared to work... 
Ok, wow, this is sexy. I think I understand how to do it without special variables now, I'm going to try to rewrite it on my own without referring to this Thanks for your input.
&gt; I'm going to try to rewrite it on my own without referring to this Probably a very good idea looking at how many bugs snuck in. I just edited the above post because I had an incorrect function in there. It is probably a good idea to make an RPN evaluator just so it is easy to build a test suite, and on that topic, build a test suite. It is very easy to fix one problem and have another crop up. You're welcome and good luck.
OT: Well HELLO THERE /pub/NeXT ! :) I might have to dust the cube off this weekend. :)
I hope it works out of the box, i have tried livecoding a bit, but not getting audiolibs to work(/do what i want) was a difficulty. Think that livecoding music is difficult though. (I know nothing of music..)
Me too, but as someone once said "Programming is understanding" :)
But why?
Two reasons. First, linked-lists are slow. Second, people try to program so they avoid the slowdown, so it's a problem. Now I'm not saying what I built is faster. It probably isn't, not in the big-O notation sense or in the clock cycle sense. But it (to my knowledge) a new crack at an old problem. You can't beat O(n) if you're forced to always use linked-lists. On top of that, haters gonna hate. A lot of people say Lisp sucks because it's slow, and that it's slow because it uses linked lists. Their argument just got a lot weaker.
Several reasons why you might want to explore using a different underlying data structure: o parallelism (partitioning the work); o support for simultaneous editing; o efficiency. He states that trellises have logarithmic access time, which is much better than lists. Of course that doesn't help much if the list is not ordered, but I can see the advantage to this. For example, it makes counting and access to the end of the list faster. I would be more interested in researching whether a tree of vectors is the better data structure. I think it may be easier for a tree of vectors to be operated on by more than one core. I also think that using some kind of tree data structure allows for multiple versions, so that the data structure can be operated on simultaneously. Trying to break the Von Neumann bottleneck. 
I rewrote it, it looks a lot like yours (due to how long I studied yours for) but with a few minor differences. [Here's](http://pastebin.com/suzbZ6vc) how it came out. 
Those lisp gurus were either wrong, or you were misunderstanding them. LAMBDA is a macro that expands to (FUNCTION (LAMBDA ...)). That's it. It's portable to all Common Lisp implementations. LAMBDA is not a function, it is a special form or a macro, with the latter expanding to the former.
Looks good, I like that you used a default clause at the end as mine would just crap out if it hit something it didn't understand. I take it from your implementation you don't think there is a bug in lines 32 and 33? (cons stack-operator (%shunting-yard (cdr stmt) (cons token (cdr stack)))) I didn't advance stmt so that first operator can compare precedence to other things on the stack. Actually, [wikipedia](http://en.wikipedia.org/wiki/Shunting-yard_algorithm) agrees with me: &gt; * If the token is an operator, o1, then: &gt; * while there is an operator token, o2, at the top of the stack, and &gt; either o1 is left-associative and its precedence is less than or equal to that of o2, &gt; or o1 is right-associative and its precedence is less than that of o2, &gt; pop o2 off the stack, onto the output queue; &gt; * push o1 onto the stack.
Good catch, I'm going to fix that now. Thanks!
All the predicates in the standard return generalized booleans. I don't see why you shouldn't do the same.
hey, thanks for the clarification :) I'm plotting my return to CS graduate study and I love devouring stuff like this :)
I'm not sure I understand.
One nice thing about FUNCALL is that you can pass it a symbol as first argument, it always goes through the symbols SYMBOL-FUNCTION, even if it gets redefined. This is nice because it propagates redefinitions of functions thoughout the systems as you would expect. Also, this property of FUNCALL lets you have lists or other data-structures of functions that know/show their names, this is very nice sometimes. 
There is a version 2 branch in the repository, which works in Common Lisp. The version 3 works in Scheme.
&gt; I do think the book has a couple of drawbacks. He takes a few shots at stuff -- I notice the ones he makes at Ruby, because I know Ruby. He's probably right, on some level, but it's kind of annoying and off putting. If the point is to bring people into Lisp, making fun of them doesn't help. I think it's a fair point. A lot of Lisp books/authors take Lisp advocacy to such an extreme where it becomes slightly off-putting. I love Lisp, I understand what a powerful language it is, but this whole idea that all other languages, "blub" languages, that do not have Lisp-style macros suck can be taken to the extreme at times by several Lisp authors. 
I haven't read the book yet, so I was surprised at the revelation. Then I asked if anyone else wanted to comment on it. So that's really what he does?
Who says that it is slow because it uses linked lists?
What is opticl? Googling for it is ... hard.
https://github.com/slyrus/opticl
Tip: +opticl lisp Or: "opticl" lisp
I haven't read these critics, but have heard they exist: Paul Graham says "People frightened by Lisp make up other reasons for not using it. The standard excuse, back when C was the default language, was that Lisp was too slow. Now that Lisp dialects are among the faster languages available, that excuse has gone away. Now the standard excuse is openly circular: that other languages are more popular." There's also the misconception Richard Gabriel addresses that Lisp is slow because you can write slow programs in it.
I found an error in yours (and mine)! All by myself! If you use (cond ((null stmt) stack), and there's an unbalanced left paren in the equation, it goes into the output queue. :O
Yeah, but it still works for valid input, right? I mean, it also wouldn't work either if I sent in: (1 + + 1) It seems like it would take some doing to get some intelligent error detection. There is a context that isn't being passed down to recursive calls. You could pass that info down to recursive calls, e.g. if you have encountered a number tell the next level of shunting-yard that you expect either an operator or a parenthesis. This really makes it a lot less sexy. If we go past the pedagogical task of implementing shunting-yard, you might wonder how you could produce an infix to RPN conversion function with the least work done by you. You would probably use a parser generator like CL-YACC. Here is what we did in YACC: (defun i2p (a b c) "Infix to postfix" (list a c b) ) (defun k-2-3 (a b c) "Second out of three" (declare (ignore a c)) b ) (define-parser *expression-parser* (:start-symbol expression-opt) (:terminals (int id + - * / % ^ |(| |)|)) (:precedence ( (:right ^) (:left * / %) (:left + -))) ;; optional expression (expression-opt expression ; implicit action #'identity () ) ; implicit action #'list (expression (expression + expression #'i2p) (expression - expression #'i2p) (expression * expression #'i2p) (expression / expression #'i2p) (expression % expression #'i2p) (expression ^ expression #'i2p) term) ; implicit action #'identity (term id ; implicit action #'identity int ; implicit action #'identity (- term) ; implicit action #'list (|(| expression |)| #'k-2-3))) (defun flatten (tree) "Traverses the tree in order, collecting non-null leaves into a list." (let (list) (labels ((traverse (subtree) (when subtree (if (consp subtree) (progn (traverse (car subtree)) (traverse (cdr subtree))) (push subtree list))))) (traverse tree)) (nreverse list))) (defun parse-infix-to-postfix (infix-tokens) (flatten (flet ((lexer () (let ((token (pop infix-tokens))) (cond ((not token) nil ) ((integerp token) (values 'int token) ) ((member token '(+ - * / % ^ \( \))) (values token token) ) (t (values 'id token)) )))) (yacc:parse-with-lexer #'lexer *expression-parser*) ))) 
Nice call. Fixed that, too. I tried reading up on how to use Bison once. Needless to say, I don't quite understand how a parser-generator works. :/
Yeah, I don't either. Sometimes I can make them do my bidding.
&gt; that comment on TPB is actually a comment from one of the employees at (now defunct) Symbolics. Actually, the comment is from David K. Schmidt who still sells support and old Symbolics equipment. The exact ownership of the post-bankruptcy Symbolics is probably still tied up in probate court, but DKS will still offer to sell you equipment if you e-mail sales@symbolics-dks.com. (They symbolics.com domain got sold, presumably as part of the probate). The comment is a sales pitch, of course.
&gt; You can't beat O(n) if you're forced to always use linked-lists. That depends on the operations you're using. And you can't beat O(n) for iterating through all elements with any data structure.
Yes. He is an employee of Symbolics, Inc., an offshoot of Symbolics. And yes, he does sell hardware, but the prices are inexorably high.
Seems a bit vague of an statement to me.. Just because what someone programmed works doesn't mean he understands it, or that it is an effective/unobfuscated way to do it.
Stop thinking about the external representation (displayed with parens) and start thinking about the actual data structure it represents internally and you'll get farther in learning Lisp. Thinking about removing or adding parens becomes meaningless.
What bothers me more is his example of using `apply #'append` to turn `'((1 2) (3 4) (5 6))` into `'(1 2 3 4 5 6)`. Understanding the second doesn't mean you understand the first because they are not analogous. `[1 | ]-&gt;[2 | ]-&gt;[3 | ]-&gt;[4 | ]-&gt;[5 | ]-&gt;[6 | nil]` is not the same as [ | ]---------&gt;[ | ]---------&gt;[ | nil] | | | [1 | ] [3 | ] [5 | ] \ \ \ [2 | nil] [4 | nil] [6 | nil] I get the feeling this has less to do with "parentheses are hard" and more to do with "trees are hard".
The problem it solved for me was taking a list, mapping a function that returned several values over it, and then leaving that list in a form where these values could all be mapped over or sorted. #'mapcar and #'sort both require the values they operate on to be in a sequence. While I haven't used #'apply with #'append in a while, I didn't use them to let me read the lists better. It isn't hard to ignore the parentheses in '(1 ((2) ((3)) 4 (5))), and realize that deep in there somewhere are the first five numerals. I had a need to turn them into sequences of atoms again. Third, a beginner may not know how to express what's holding him back, and this particular article is written with that beginner in mind. He or she probably doesn't know about dotted pair notation or sentinels and may only want to get at the atoms inside the nested lists because what that person really wants is a dynamic vector. But in Lisp these are somewhat hard to use.
I think it's better to help a beginner stop being a beginner. The article seems like a perpetuation of a beginner mindset, not just starting from there and progressing out of it.
I think I had a similar problem, if I'm understanding correctly. I assume you wanted a function to do something like '(1 2 3) =&gt; '(2 4 4 8 6 12) ;; for example but `(mapcar (lambda (n) (list (* n 2) (* n 4)) '(1 2 3))` returns `'((2 4) (4 8) (6 12))`. In this situation, what you probably want is (loop for i in '(1 2 3) collect (* i 2) collect (* i 4)) EDIT: lispm has a better solution below The real problem with taking the flattening approach you suggest is that while the two lists you use as examples share atoms, they have a radically different underlying structure. Understanding that difference is key to understanding how lists work in Lisp. IMO, you're doing yourself (and beginners) a disservice by trying to gloss over it; if you feel that it's too challenging to jump into trees, try some examples that deal with flat lists first, then move on. 
Of course '((1 2) (2 4) (6 12)) is different from '(2 4 4 8 6 12). That's the point. The problem is that you can't easily return more than one value from a function without using lists, so you have to use the former as an intermediate step to the latter.
Hey, maybe someone should make a Lisp post in the new history channel. "I've invented a new computer language. Does anyone think it will last?" :7
 CL-USER 3 &gt; (mapcan (lambda (n) (list (* n 2) (* n 4))) '(1 2 3)) (2 4 4 8 6 12) 
Even better [scribbles notes furiously]
Okay, so just to confirm: It is perfectly okay to drop the #' and simply use (lambda ()) in portable code? I have no problem unlearning that habit. :)
Yes, LAMBDA is a macro that expands to #'LAMBDA (really (FUNCTION (LAMBDA ....))), they mean exactly the same thing to your compiler. From the SBCL source : (defmacro-mundanely lambda (&amp;whole whole args &amp;body body) (declare (ignore args body)) `#',whole) 
&gt; The problem is that you can't easily return more than one value from a function without using lists Actually, Common Lisp is one of the very few languages that allow you to do that. (defun look-ma () ;; no lists! (values 13 42 11))
I don't understand the question.
That makes two of us.
If you want to use a C library, typically you look if there are CL FFIs out there already. If not, you use CFFI to interface with the C code either completely or just the parts you want. I think people are using automatic CFFI, but i don't know *how* they do it.. We don't use header files, the code using CFFI should provide the information that would give. Typically it would be nice to have this code in a separate package or even also a separate asdf system, though.
&gt; you look if there are CL FFIs out there already Are there any central collections of these? Overall, this answers my question. Can anyone add to this? How are people doing "automatic CFFI"?
Uhm. [lispbuilder-sdl](http://code.google.com/p/lispbuilder/wiki/LispbuilderSDL), for example, is a cffi-based interface to libsdl, as you see in its [dependencies](http://code.google.com/p/lispbuilder/source/browse/trunk/lispbuilder-sdl/lispbuilder-sdl.asd#17). Usually, if someone's written a polished cffi-based lisp wrapper for a C library, it's itself packaged up like that. Is that what you mean? Or are you talking about the other direciton - lisp libraries somehow providing headers so programs written in C can call them in-process? 
This is a popular listing: http://common-lisp.net/projects.shtml If you use Clozure Common lisp (CCL), the vendor specific FFI is automated: http://trac.clozure.com/ccl/wiki/OpenMclFfi CFFI requires a bit more work, but you gain portability across many lisps.
I'm glad someone said it.
True. Which is why understanding something is a journey, in and of itself. Happy hacking!
It would be nice thought to have some kind of an platform &amp; language agnostic repository of FFI descriptions. There are of course differences in FFIs across platforms but basic mechanisms are fairly similar for most languages that have dynamic FFIs. Personally I have had good results by just translating CFFI definitions to other systems like Racket or Squeak. Naturally you only get the bare low level interface by doing that but it still saves a lot of time and work. 
Looks neat! Thank you!
I don't know what "automatic CFFI" means, but I use cffi-grovel to get to the parts that I need.
Lisp FFIs are not just "Lisp interface to a C library"; no. Most of them, at least the good ones, provide a nice syntactic layer to abstract away the C. It stops being C and becomes yet another Lisp library. It's often hard to tell what uses FFi and what doesn't. Typically, however, a pure Lisp implementations are more popular the FFI libraries. 
The places one looks for libraries in general, like [cliki.net](http://www.cliki.net).
With "automatic CFFI" i meant a library that parses C (header) files and produces (the data to make) cffi macro expressions foreign-interfacing the given files. if libraries available. Edit: see JenSims comment.
I've used [swig](http://www.swig.org/) a couple of times and it worked fine.
That's what I meant by "easily." Should a beginner be expected to use #'values and friends?
Does anyone know what came of this? *obviously I know what came of it, but I was looking more for an anecdote* 
VALUES is not a function, it's a special form. And if you want to return multiple values, use the language-provided facilities to do just that. Should a beginner be expected to want to return multiple values?
It is sad but ANSI Common Lisp will never be extended in an official way. There are no more the resouces and interest to do it. I suspect that the commercial CL vendors are not even interested in drafting a less official set of common standard extensions, as their private extensions are one of their selling points. Perhaps the only way is: the open source implementations start to add a lot of extensions -&gt; one day someone writes a compatibility layer (à la bordeaux threads) -&gt; this layer becomes a partial, de facto standard extension.
I suppose that one way would be if one of the more influential free implementations, say SBCL, just would start incorporating quasi-standard libraries into their distribution and make them de facto standards, simply because of the influence they have. It was discussed several times to make such a "batteries included" SBCL distribution, a la Python or Java, but they have always decided not to do so in order to "protect the 25 year old standard". Of course, quicklisp makes hunting down libraries a non-problem now, but it still does not solve the problem of deciding which of several competing libraries to use. It takes a strong player to "abuse" his influence and basically force everybody else to accept a library as a de facto new standard. The problem with Common Lisp is not that the official standard is not updated any more, it's that the current strong players also do not try to push a "de facto" standard by bundling libraries with a influential implementation. Lisp needs a Torvalds/Matz/vonRossum type of a benevolent dictator. Without a BDFL you're left with a slow evolution/natural selection and move in decades what a intelligent designer accomplishes in years.
I strongly agree with this. I think the approach to take is less a "do it Right" and more a "works most of the time for most users", and move forward. The standard is awesome, but it does shows its holes. I hope one day to have sufficient free time to hack on SBCL.
&gt; I think the approach to take is less a "do it Right" and more a "works most of the time for most users", and move forward. Exactly. There *are* alternatives to many of the standard library modules in Python, Ruby, &amp;c. but unless you have a specific use case, you don't even have to know they exist. I think it would be best if there was some sort of "CL Platform" akin to the Haskell Platform, especially if it was supported by a decent array of CL systems (CCL, SBCL, ECL, LW, ACL, maybe others?). QuickLisp would take the place of Cabal in such a system, everything would be right in the world (esp. if it made installing CL *programs* (not just libraries) simple &amp; easy to do too!). 
Now do scheme please.
I would like to see the question settled as to what the official meaning of REPL is, and whether the interpreters mentioned (the QBasic "Immediate" frame and those in Eclipse, Visual Studio, and others) qualify as REPLs.
I enjoyed this very much. Thank you.
As far as I know, it comes from the Lisp world, and means 'Read, Eval, Print, Loop'. I don't think of it as a property of an IDE, but as a property of a language implementation. So, if the underlying language doesn't have it, it's not likely an IDE can provide it. You're not going to see a REPL for C, for example. There's a C-Like scripting language called 'Ch', and it does have a REPL, but it's not really C.
I don't see that it will ever be settled. There are a bunch of Lispers who don't want people to corrupt what they consider `read` to mean and there are a bunch of developers who generally have no exposure to code is data or `read` &amp; `eval` as distinct and useful operations and cannot grasp or refuse to grant the distinction.
In C# it would be `REPLIterator&lt;Printer&lt;Evaluator&lt;Reader&lt;EvaluableXMLExpression&gt;&gt;&gt;&gt;`
In the case of a REPL, what does the difference allow you to do that you couldn't if read &amp; eval were one function?
Hey, you're the guy who wrote &gt; No, Java is &gt; SingletonFactoryFactoryFactory...
HA! I caught you editing that, `learnyouahaskell`-kun. Seriously, am I smug? *stops a second thinking about it* ... Oh.
I was somewhat playing the part of a Java aficionado. Hey! I *thought* I could write inline code, but then I couldn't find anyone doing it again. **Edit:** &lt;CTRL+U&gt; &lt;CTRL+F&gt;, "-kun", ~~urge to facepalm~~. Never mind, it doesn't work. How do you do it?
&gt; this is `inline code`, `this is` not. &gt; this is \`inline code\`, \`this is\` not. You can escape \` with \\\`, you can escape \\\` with \\\\\`, etc. If your code contains \`s, ``this \` will not work``, use \`\`this\`\`, ``this `will' work``. The^more^you^know*!*
That is exactly how StackOverflow does it, but it didn't seem to work here when I tried. `test` "This is lame, `lame`," he said. **MARKUP** **( yಠ,ಠ)y** **Y U WORK NOW?**
If you want to skip straight to the details about the Lisp and past the "why I didn't choose X", Ctrl-F for &gt; GOOL (Game Oriented Object LISP) is my answer to the 
Modify the reader.
Okay, that's cleared up, then; the other difference, then, is output of data structure, not merely a string.
Interesting, mine was the only upvote on the other submission. I'll move my comment here: All right! This looks like what I've been hoping for--an in-depth explanation of what [GOAL](http://en.wikipedia.org/wiki/Game_Oriented_Assembly_Lisp) is and does. Well, it's the first step, at least.
This is on my todo list, along with Clojure. As a user of many lisp dialects, I find it interesting. (I wrote the CL article in question.)
Lisp function call != C function call Local C variables live on the stack, since they are only valid during the function call. So, it's normally just a business of saving a couple registers, pushing the return address, and jumping somewhere. LISP has closures, i.e., "local" variables can live on after the function has already been called, and hence they (sometimes) have to be allocated on the heap, which is more expensive.
There are compiler optimizations that can reduce the cost in many cases (such as inlining when the target of the function call is known), but in general function calls require a lot more than a single-cycle branch. The main expense in a function call involves managing the call stack; a new activation record has to be pushed onto the stack and both the arguments to the function, and the return address for the function call have to be copied into it. Another big hit comes if the code is running on a pipelined architecture and the branch predictor cannot predict the target of the branch. In these cases the instruction pipeline has to be flushed, and the length of the instruction pipeline determines the number of CPU cycles wasted.
Also, with the standard C calling convention on x86, a function call might look like (psuedo-asm): push arg_3 push arg_2 push arg_1 call f ; call does a push and a jump mov something, ax ; save off return value somewhere sub esp, 3*sizeof(arg) Each of these might take 3-4 cycles (depending on chipset), so there's an easy 20 cycles.
You need to determine exactly what you are counting as "function call." Generally, you need to arrange the arguments of the function, invoke the function, then move the result into the right place. Optimistically, the arguments can be passed in registers, and these might even be the registers they naturally were assigned. But even the branch statement you call single-cycle is almost certainly not: the instruction pipeline has to be filled from a new section of memory. At the time that Lisp machines were popular, C/Fortran function calls were much more heavyweight than today's RISC machines. Even leaf functions typically used the full ABI, with arguments pushed onto the stack, along with the return address, and possibly with extra caller-save registers. Branch-prediction, instruction caches were non-existent, and even L1 and L2 caches were rare.
There's no hard and fast rule here. Some function calls are compiled away; they don't lead to a single cycle. Others require a number of cycles. The Wikipedia article refers to Lisp machines from decades ago, not to modern Lisp compilers.
Just put your Lisp code in a file, e.g. foo.lisp and then load it into your Lisp, e.g. clisp, by typing (load "foo.lisp").
First thing is that it doesn't matter to you whether your code is compiled or interpreted. It only comes into play when you've either got a piece of code that needs to run a lot of times (compile it), or a lot of pieces of code that only need to run a few times (interpret it). Otherwise, you'll never notice the difference. CLISP will be fine for you. Lisp can be confusing to people coming from other languages, because the way to work is to load code into a running Lisp, but Lisp programmers still store their source code in files. The REPL, as useful as it is, is really just for interacting with your code. While you *can* use it to copy/paste code from your editor, or use the LOAD function to load source files after changing them, the easiest way of all is to use an editor with proper Lisp support, which will have an easy way to load stuff from the file you're editing into a Lisp you've got running. EDIT, cont: So if you just want to save your Lisp code, pick an editor you like to type your code into, then copy/paste it or LOAD the file. If you're planning to write in Lisp for quite a while, you can go ahead and learn Emacs, best used with SLIME (provides enhanced Lisp development features) and Paredit.el (allows you to edit Lisp code structurally, i.e. you don't have to type closing parens). If you really hate Emacs, get the Lispworks trial version which has an IDE you may like better.
Would you suggest any editor in particular?
emacs of course
Because of branch prediction, instruction level parallelism and other features of modern processors, an empty function call will take less than 20 cycles today. That was not the case 25 years ago when lisp machines were made, and at that time, function calls were costly. This has no relevance to lisp of today. Today, much programming happens in interpreters or virtual machines, that have much higher runtime penalty of function calls than the best lisp environments, and people don't seem to care to much about it. 
Probably you posted before my edit, but Emacs is my recommendation.
okay, THANKS for the help!
Common Lisp: * Has namespaces * Has lexical scope * Has default values for optional arguments Emacs Lisp: * Runs in Emacs * Hey, at least it's not Java.
Common Lisp can't easily be used to extend GNU Emacs. Common Lisp can (and has been) used to write Emacs-style editors, like Fred, Climacs, Hemlock, etc. It's a general-purpose programming language, not a single editor's extension language.
How's their performance compared to that of Gnu Emacs? I know they use highly performant Lisp implementations.
Wicked performant.
I don't know the answer to this question, but I am guessing that if we compare only bytecode (not native code compilation present with some CL implementations), elisp might have an edge because it doesn't have e.g. closures to compile.
&gt;Has default values for optional arguments Note that the `cl` package provides the full Common Lisp `defun` as `defun*`, so you can have these. But something that can't be provided is multiple return values. 
mua?
I didn't say it was a function. You can sharp-quote special forms too, provided you don't then try to pass them as functions.
you could always return a list ;)
According to Paul Graham's essays on Lisp, macros make every language feature accessible to any Lisp with macros. Given that view, why are there different Lisp dialects?
Different default settings and performance criteria. Emacs lisp just exists because there is a lot of code written in it, common lisp is strictly better. But the differences between scheme and common lisp is more a matter of taste.
Linux kernel and surrounding open source tools allows developers to create mix and matches of how an OS should works. You can change its kernel module. You can change its security model. You can change the desktop environment. You can change its file system. You can change how its manage the services. Given that, why are there different Linux distributions?
Modern virtual machines have a high cost to function calls because they instrument functions to be able to make better optimization decisions. This is mostly offset by their tendency to remove calls by very aggressively inlining in tight loops.
Yes, I agree. It is nice to have Linux kernel modules so you can write a driver as a module if you need one. It is nice to have a macro facility so there's a (usually) less expensive way to extend the language. But either activity may require a lot of work, depending on the extension/language feature. Paul Graham is saying "Lisp is the best possible language because you can add any feature you want", which is indeed similar to saying "Linux is the best possible OS, because it has kernel modules so you can add any module you want". Of course other OS's/languages have facilities to add language features (perhaps with less ease), and there's the issue of the irreplaceable core which also determines the quality of the OS/language.
&gt; You can sharp-quote special forms too, provided you don't then try to pass them as functions. "It is an error to use FUNCTION on a function name that does not denote a function in the lexical environment in which the FUNCTION form appears. Specifically, it is an error to use FUNCTION on a symbol that denotes a macro or special form. An implementation may choose not to signal this error for performance reasons, but implementations are forbidden from defining the failure to signal an error as a useful behavior." So no, you're completely wrong here. However, I was also wrong, and VALUES *is* a function ([as specified by the spec](http://www.lispworks.com/documentation/lw50/CLHS/Body/03_ag.htm)).
I'm guessing you know that multiple return values is not a list, in that you can use the result of a function with multiple return values as input as a single parameter to other functions (ie (1+ (floor x))). A list does not enable you to do that
Although the coloring in CCL's implementation of Hemlock (at least on my MBP) is considerably slower than the coloring in GNU Emacs on the same machine...
I don't really know what I'm talking about, so take this for what it's worth. But I think that Scheme is smaller, with less extensive libraries, and is generally and a little bit more coherent -- easier to learn in other words, and better for teaching and perhaps for proving things about the language. CL seems to be aimed really squarely at real world production coding. So you might say that Scheme is to CL as Pascal is to C. And there's the whole lisp-1 vs. lisp-2 thing, which I understand only superficially. I understand the idea of one vs two namespaces, but not why people would feel strongly about it one way or another. And then there's Clojure, which has obvious uses in a world where Java has so much traction and such enormous libraries. And while this is kind of a glib answer, Graham himself is working on his own dialect, Arc. I don't know what issues he's trying to address, but he's such a smart guy that he must have his reasons.
Even if something can be implemented vai macros in theory it doen't mean that it is feasible/practical to implement it this way (rather than natively). Implementation of some major feature might be close to writing an interpreter in complexity (and also you're likely to lose compatibility with "normal" code in process). For example, it is pretty hard to implement lexical scoping in dynamically-scoped-only dialect. I bet it would involve some serious overhead. Also non-native implementations of continuations are likely to suck. (I've used some of them for CL.) Another reason is that rewriting language up to your taste is both tedious and is frowned-upon because other developers do not want to learn yet another ad-hoc language but they want to use some common vocabulary provided by the dialect. So while changing the language is possible it shouldn't be done without a strong reason.
If you don't like word "native" use another -- "built-in". Think about how you would implement a feature like lexical scoping or continuations via macros. Now think about a case when it is implemented directly in compiler/interpreter. (Which I would call 'native'.)
But you certainly can have different functions. One returns a list of all the values and other (or others) return just one value. This could also be done with optional parameters. Scheme (R6RS) has div, mod and div-and-mod for example. I think that div-and-mod returns two values, but could just as easily return a list of length 2. Are there any other reasons to complicate the semantics of a language with multiple return values? 
I hope you know you might be trying to compare apples and oranges?
I hope you know you might be trying to compare apples and oranges?
Well, Xach said the CL implementations of Emacs are "wickedly performant", and while that may be true, GNU Emacs appears to out-perform CCL's Hemlock implementation in certain operations. Apples and Oranges? I suppose. I would expect, though, the natively compiled editor to out-perform the Emacs byte-code engine.
I agree. I think this is ignored by Graham (which may be because he's trying to sell Lisp in his essays). I think claiming Lisp is the most powerful language simply because it has the macro facility assumes too much about the practicality of implementing actual language features (e.g: Haskell's static typing and type classes) as macros.
How would Genera handle a web server, or being installed on a phone?
What is considered powerful is subjective and depends on context, of course. Lisp enables relatively easy implementation of embedded domain-specific languages which usually can be mixed with each other. It is a very powerful framework which can, in theory, let one to use almost any feature available in any language. However it is sort of unlikely that an average programmer will develop a powerful sub-language just to solve some relatively simple task -- it is not cost-effective. So Lisp is (arguably) the most powerful language _"asymptotically"_ -- for sufficiently big task where costs of creating a new sub-language will be offset by savings associated with convenience of its use. But for smaller tasks choice of defaults (Lisp dialect) definitely matters. It is hard to say whether it is true because it is hard to find similar huge systems implemented in different styles to compare. Even theoretically, at some point even implementing DSL through a compiler or interpreter in a language which doesn't support embedded DSL well can be justified, so it is hard to say whether macros matter much. Macros allow mixing various DSLs and core language features so in theory it might mean more flexible and cost-effective development process unlike monolithic DSL, but, again, it is almost impossible to study this matter. &gt; practicality of implementing actual language features (e.g: Haskell's static typing and type classes) Language's power doesn't boil to literal emulation of other language's features. For example, I can build a DSL for a specific task that will guarantee type-correctness within its scope through construction. Then I'll have type safety without a need to implement Haskell's static typing. And as Common Lisp can take type hints it will probably even run at speeds typical to statically-typed languages. But OTOH if we take a dialect which doesn't support type hints you won't have speedup due to static typing no matter what you do in macros.
&gt; Lisp enables relatively easy implementation of embedded domain-specific languages which usually can be mixed with each other. I agree, though I think other languages also facilitate this. Haskell facilitates similar DSL power through light-weight function-call syntax, allowing a library of functions to define a DSL. Some examples: BASIC DSL in Haskell: http://augustss.blogspot.com/search/label/BASIC Here's a C-like DSL in Haskell: http://augustss.blogspot.com/2007/08/programming-in-c-ummm-haskell-heres.html Lisp allows DSL's through syntactic pre-processing. Haskell allows DSL through semantic post-processing. The Lisp way is more powerful in terms of allowing the DSL to take on any syntax you want. The Haskell way is more powerful in terms of guaranteeing more composability between DSL's. &gt; So Lisp is (arguably) the most powerful language "asymptotically" -- for sufficiently big task where costs of creating a new sub-language will be offset by savings associated with convenience of its use. I'd say this is true of any DSL-capable language, and not just Lisp. &gt; For example, I can build a DSL for a specific task that will guarantee type-correctness within its scope through construction. Then I'll have type safety without a need to implement Haskell's static typing. It will be up to you to prove/check the type-correctness (by construction) of your DSL. In Haskell, it is up to the compiler. &gt; And as Common Lisp can take type hints it will probably even run at speeds typical to statically-typed languages. I think most Haskellers view performance benefits of static typing as a secondary "bonus", and not the main selling point. Haskell types are great because of many more reasons: * Due to purity + maximum generality in types, types of a lot of functions specify *exactly* what the implementation must be doing. The types guarantee full correctness in these cases. When reading the types of foreign API's, having types as machine-checked documentation is great. * Very few errors slip by Haskell's type system. Whole classes of errors are eliminated statically (Invalid use of nil/null/none [depends on your language of preference], as one example). Advanced use of the type system can catch more and more mistakes as type errors. * [Hoogle](http://www.haskell.org/hoogle/). Want a function that combines two lists into one list of pairs? Hoogle for `[a] -&gt; [b] -&gt; [(a,b)]` and it finds the `zip` function. Say you want a function that not only filters the elements of a list for which a predicate returns True, but also returns the elements of the list for which it returned False. You hoogle for: `[a] -&gt; (a -&gt; Bool) -&gt; ([a], [a])` and it returns the `partition` function (along with just 2 other similarly-typed functions). The ability to search by-type in Haskell is extremely useful. * Types *save* you from writing some of the code (mostly due to return-type-polymorphism). For example, the `read` function takes a String and returns a value of any type that can be parsed from a String. So if you use `read "5" * 3`, the type system knows `(*)` takes identically-typed values, and it knows the type of `3`, so it knows the read of `read "5"` must be the same, and thus parses the string appropriately. Another example: the QuickCheck library: You can define a property. For example: double_reverse_prop :: [Int] -&gt; Bool double_reverse_prop xs = xs == reverse (reverse xs) and then, you can "test" that property via: &gt; quickCheck double_reverse_prop +++ OK, passed 100 tests. It can auto-generate/randomize various arbitrary inputs for the function based on its type.
Common Lisp * it's Common Lisp Emacs lisp * It's *emacs* lisp 
A lot of people are recommending emacs, and it's a good recommendation (I use emacs/SLIME myself), but one other option you might want to explore is [cusp](http://www.sergeykolos.com/cusp/intro/).
Huh? Closures are created by creating function objects, not by calling functions.
Wouldn't it be sufficient to have a meta-distribution in QuickLisp that would pull in all the recommended libraries to implement this? (Of course a consensus about which libraries would be part of this had to be found first.)
Neat I guess, but it requires a running X server anyways, so... why?
Closures become necessary when you return a function object (because returning a function means that the variables outlive the "time" counterpart of their lexical scope). They are distinct from (but related to) the callable object that is created when you return a function. See also http://en.wikipedia.org/wiki/Closure_(computer_science) : "The term closure is often mistakenly used to mean anonymous function. This is probably because most languages implementing anonymous functions allow them to form closures and programmers are usually introduced to both concepts at the same time. These are, however, distinct concepts."
Sure, you most certainly could; I just meant it would be easier for most people to download CL Platform *year number* *release number* and be good to go. If QuickLisp had a meta-project called CLPlatform-*version*, that would be good too.
I see what you mean now. I still don't think it would be necessary to "elevate" the free variables some function object might close over to the heap at the time the function whose stack frame those variables originally come from is called. You have basically said yourself that that only becomes really necessary at the time that function returns, but clearly not earlier than when the function object is created.
Either this is vaporware, or a first step towards an all-CL OS.
&gt; Are there any other reasons to complicate the semantics of a language with multiple return values? The original reason for multiple return values was that they could live on the stack, and did not have to cons. So there were speed and efficiency benefits.
&gt;I have no idea, for example, how Java programmers write even moderately complex regular expressions without going insane, Java programmer here. If I want to test out logic, I'll use a Python REPL (and just make myself not use map/filter/etc.). If I want to test regexes, there's a number of very good websites that let you do so (I believe they use JS). If I want to test a Java-specific API....I just have a very short write/compile/test cycle, and unpack a musical instrument and/or reddit so that I can [keep myself busy while my code compiles](http://xkcd.com/303/).
&gt;Scheme is to CL as Pascal is to C. I really wouldn't use that comparison. Scheme attempts to be a cleaned-up kind of Lisp, *not* just "a little bit more coherent, easier to learn in other words". It makes a number of design decisions down at its fundamental core that are sharply different than CL, that go way beyond changing "car"/"cdr" to "first"/"rest" -- and although CL does have some obsolete extra garbage in the standard, and some warts, aside from that, some of these differing decisions are very much a matter of taste. CL has always been more popular than Scheme partly because of extensive libraries, as you mention, but also because not everyone is enamored of Scheme's different approaches. Some people are, some aren't. Pascal, though, was defined 100% purely as a pedagogical language *only* for teaching, and the lab experiment got out of control when it escaped to the wild. (Originally Pascal didn't even have proper strings, just char arrays where the size was part of the type, making strings a third class citizen. And it demonstrated the problem with disallowing generic functions, in its zealousness about strong typing: built-in functions like writeln() and readln() were total -- any number of parameters of any type -- but no user function could do that, making user functions second class.) Scheme and CL are both just *dialects* of Lisp, and they both have sophistication born of many software-generations of experience, each in their own way. As for Graham, there's never been a perfect language, and some people like to try to improve past languages, while others don't. It's a matter of taste, not of how smart he is.
The thread starts with a nested quote I do not know the source of. I am trying to determine what the submitter of that link is thinking, and what the proper interpretation of the thread should be. I have only looked at the first couple of replies.
Read the README, and discover that it's neither. It's real, but it exists mostly for hack-value as far as I can tell. 
I'm somewhat saddened that this has apparently been completely ignored for the past 25 years.
Now that makes sense, considering common lisp doesn't have general continuations and so can easily use a regular stack. Just the other day I read in the wg1 archives (that's one of the working groups of the planned next standard for scheme, r7rs), not from one, but from two different people, that the availability of pattern matching in the core of scheme would remove the need for multiple values (allowing to clean up the semantics a little in the process). Of course scheme has first class continuations so the opportunities to use that kind of optimization are more limited in scheme. It would be interesting to know how multiple-values got there in the first place. 
A lot of predictions did turn out wrong though. For example, free implementations, high performance, etc. 
has it? there are several papers like these which have been discussed during the design of ANSI Common Lisp (published later in 1994). Note that the authors talk about the first CL in 1984, then published in CLtL1 (also 1984). The authors themselves had enough time to not ignore their complaints. Brooks wrote a Common Lisp compiler and a book on Common Lisp. Brooks later developed a CL dialect, L, which runs on robots. Gabriel started a company and sold a high-performance Common Lisp on stock hardware for a long time: Lucid Common Lisp. There was later the ANSI CL standardization where they had the chance to clean up things. Which did some, but not all. Still, the early Common Lisp community (!) had the PR talent to introduce a new dialect of Lisp and publish at the same time papers criticizing it. 
Only a little bit later the first independent implementation was published: Kyoto Common Lisp from Japan. It was unusual since the authors mostly only had CLtL1 as a reference and it compiled Common Lisp to C. This implementation was sufficiently different that it raised a lot of issues. Bits of KCL are in GNU Common Lisp and ECL.
In an unmoderated newsgroup like comp.lang.lisp, anyone can write anything at any time. This usenet thread is about someone who likes to attack and attack and attack in a nearly mechanical pattern.
Is the OP "Cthun" the one doing the "attacking"?
Downvote drive-by!
Yes. The attacks provoke reciprocal attacks, though they're generally extemporaneous, not systematic.
I see; the author of that post considers it "trolling".
Note, this is the real URL. It looks like the OP used a Google click-tracking link by mistake. http://www.dreamsongs.com/Files/clcrit.pdf 
I love Xach and I love quicklisp. I'll spare you the rant about why quicklisp is the best. And why Xach is the best.
But don't you have to do a lot of non-Lispy things to make numerical calculations, for example, efficient?
With proper macrology, no. Even without it, sometimes not. The compiler might be able to do some magnificent inferences. Otherwise, yes, but you'd have to add annotations in a statically typed language too, anyway (modulo some ML-derived languages).
So Lisp is really, really good for advanced programmers and not so good for novices? Gotcha.
No. It is fine for novices. But I consider type annotation to be something novices needn't dabble with until they understand how to identify bottlenecks. 
sort of. Lisp allows novices to learn certain programming techniques relatively easily. To write and deliver 'real' programs it might need more advanced programmers.
That's an amusing takeoff on Greenspun's Tenth Rule, but has it truly happened? I mean, I saw things related to the phenomenon Greenspun exaggeratedly referred to long before I heard of his sound bite, but I'm not sure I've seen an instance of this new one. So, is it sheer snarkiness, or is there a germ of truth?
I look forward to seeing you show me a language that allows me to write efficient, yet reliable and accurate programs, without needing to be "advanced" (whatever that means).
I'd say there's a gem of truth, but I can't say for sure. I think it's good practice to label (with a comment or otherwise) the type and return value of your functions, sort of like what's done in Haskell/SML. Another thing is that (S)ML is very small and formally verified, and even writing ML for pseudocode is sometimes desirable, I'd say. Especially in academic literature, you'll see things resembling algebraic datatypes and whatnot. Of course, it still is pretty humorous,
I posted the "Uncompromising Metaprogramming" link the other day mostly because it reminded me that macros are compile-time programs that operate on your source code. Therefore, they make a good tool to enforce stablity, correctness and security constraints on your code. I feel like I don't see people talk about this much. Or you hear about it and then there aren't really any examples. I wrote a [with-locked-open-file macro](https://github.com/redline6561/paktahn/blob/master/util.lisp#L395) atop POSIX lockf for Paktahn but that's a pretty straightforward and weak application in my mind. But it turns out [lockf doesn't actually give you any guarantees](http://www.samba.org/samba/news/articles/low_point/tale_two_stds_os2.html). Anyway, folks will be whining about ML/Hindley-Milner/SystemF based languages vs untyped Lisps on the internet forever. Clearly many of us happily choose Lisp including many folks like pkhuong and others who are well exposed to ML-family languages and exceptionally gifted systems programmers. Presumably there is a good reason. I would be very interested in seeing someone who can show me when and how to best leverage deftype, assert, macros, CLOS, etc to generate safe code and enforce the kind of constraints that ML+Scala+Haskell folks like to brag about. If you didn't take it too far and try to write an emulation layer for a statically typed functional language but instead took the most *sensible* advantage of CL's built in facilities in this area I bet it'd be quite a thing to see. An Introduction to Typeful Lisp. It's just a thought. :)
If you like, you can get the best of both worlds with a [statically typed lisp](http://docs.racket-lang.org/ts-guide/index.html). Or if you want to enforce strong protections (can be more expressive than types even), you could use some [higher-order contracts](http://docs.racket-lang.org/guide/contracts.html). I think you can get a lot of these safety guarantees in existing lisp-like languages. It would be nice though to be able to reason better about macros with some dynamic checking or some kind of static analysis, but this seems to be a difficult research problem.
Feel free to *not* watch the video and read this [PCL chapter](http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html) instead!
I think "the point" here is that pattern matching is much more expressive than your general list-processing functions. Which is, probably, true. But it's not impossible to implement pattern matching in lisp and, moreover, article mentions `syntax-rules` as being similar to a specification in comments.
Anything that exaggerated is "snarkiness".
FARE-MATCHER is what I use for pattern matching.
## 
I don't think this is a bad question. I do think it is often asked in a loaded or prejudiced way, and responses are often incomplete or argumentative.
[LispIDE](http://www.daansystems.com/lispide/) is a simple way to begin. It is a basic interactive editor for Lisp and Scheme. You can edit and save in the top pane, evaluate it (send it to Clisp) and have the results appear in the bottom pane. It is a bit funny in some ways. You have to set the path to your Lisp (first item under 'settings' menu). Sometimes the lower pane which shows your results 'disappears' and you have to pull it open with your mouse - and if you want to close the tab you are working on, you will find the option under the 'Windows' menu. Code to be run has to be highlighted first, then you click the red braces on the toolbar. You have to use Ctrl-a to select a whole file to run. One advantage is that it has the Common Lisp Hyperspec as a help-file for reference purposes. Ultimately, Emacs/Slime is the way to go. A good way to set this up is by using [quicklisp](http://www.quicklisp.org/beta/). There are instructions on the website. 
You can use ELT on any proper sequence.
This is like saying: "*Why does everything have to be a university lecture now? I really prefer to learn by staying home and reading on my own.*" A screencast is the closest you'll get to a lecture. It is usually less formal than a written book and watching live how something is created has usually a different learning effect than just reading a finished solution.
That article is proof that a badly chosen example can ruin one's entire argument. He chose HTML generation for an example, and had efficient string concatenation as a star witness. I had to re-read the article and *search* for his point, my initial reaction was dismissal and mental suggestion of cl-who and cl-closure-template. 
One thing that should help you is recompiling the function with maximum debugging support (`C-u C-c C-c`), and then pressing `v` on the offending stack frame. It should then show you the precise(*) point in the body of the function you're in. But note that it all depends very heavily on the level of support provided by your CL implementation, and the level of support SLIME has for exploiting it. I know it works pretty well in SBCL. I have no clue about Clisp. (*) Subject to the fact it can get obscured by macros transforming the code, which might be a serious problem if most of your function's body happens to be one.
I think you're over relying on the debugger, more than necessary. Debugging Lisp code is mostly a visual pattern-matching exercise and shouldn't take more than a few seconds to go the offending form. Skim the first few top entries of your stack trace, see which one you recognize as your own function, then M-. to go to the source then M-, to return to the debugger (or whatever previous location you came from.) You might not be able to get M-. functionality unless you have the source compiled (as with COMPILE-FILE) and loaded. This is not some wizard insight: I just tried it and couldn't :-) Also, I try to avoid the [USE-VALUE] restart; it teaches you sloppy coding by letting you patch-up a system into full functionality, instead of writing a correct one from the start. If you have a Lisp primitive at the very top, and one of your functions a few steps below, it's often a sign of broken contracts or assumptions relating to type. Happens when Lisp is expecting a subtype, but you're thinking in the super-type (often caused by a rogue NIL) 
It's X servers all the way down.
I find that particular stack trace to be a little unreadable.
On the other hand, if you're working on /Y/ and and error occurs in /X/ it can be handy to allow you to keep working on /Y/ rather than get distracted.
aref has to have the index after the array because there might be multiple indices. nth has the number at the beginning because it's a generalization of first, second, third etc. (which have the number at the beginning).
Debugging in Slime with Clisp is a bit painful (due to Clisp not having a good lisp interface to the debugger). SBCL and Clozure CL are better supported by Slime.
Why is not having full screen a show stopper when you say you are doing this for your own amusement? The bundled games (plt-games, IIRC) are pretty slick all things being considered, especially that one that looks like Same-Gnome (dunno the correct name, sorry). As a pedagogical device, I think you could do *much* worse than Racket.
Wrapping SDL isn't hard so I'd suggest going with that and then trying to use the SGL lib in SDLs context. I have never tried it myself (only the other way around; I have a little C library that does 2d OpenGL which I used in the Rackets GUI context) but I don't see why it shouldn't work. Your best bet is to ask in the Racket mailing list for help if you get stuck with it, they are usually very helpful. Edit: You _could_ always build bindings to some higher level library or engine (like SFML, Ogre or Irrlicht) but they tend all to be done in C++ which is quite painful to interface with. Maybe consider joining http://common-lisp.net/mailman/listinfo/lisp-game-dev too ;)
Besides the aesthetics there are some valid technical reasons for having a full screen option. For one, smaller resolutions are less demanding for the graphics cards and at least in the past OpenGL performance was usually better when running in full screen.
If I remember right, doesn't Racket only support old-school opengl? Like, no 2.0+ support?
I'm just using Clisp because I got mad at the compiler warnings SBCL was giving me and was too noobish to deal with them. Would it behoove me to try it again? (Or try Clozure CL?)
&gt;`frame-source-location not implemented.` :( I'm getting the impression that I should be switching to SBCL...
SBCL's warnings are helpful for writing better code. Personally, I like SBCL, it generates fast code and has debugging tools I need. Clozure is quite nice as well, albeit producing not as fast code as SBCL.
Ah, thanks. Are `aref` and `nth` just faster or something? (Much like `string-equal` is just a faster version of `equal` if you know you're comparing strings.)
I'm glad I'm not the only one :)
&gt;aref has to have the index after the array because there might be multiple indices. Would you mind giving an example? Here's my naïve interpretation, which is obviously not right: &gt; a #(0 1 2 3 4 5 6 7 8) &gt; (aref a 2 4) *** - AREF: got 2 subscripts, but #(0 1 2 3 4 5 6 7 8) has rank 1 
NTH is compatible to the same function in earlier Lisps (NIL and Lisp Machine Lisp). A two dimensional array is: #2a((a b) (c d)) rank 1 says that your array only has one dimension, but you are calling AREF with two... 
Oh, multiple dimensions...Python/Java have me trained to think of a two-dimensional array as an array of arrays, so I was expecting something like `(aref (aref #2a((a b) (c d)) 0) 1)` to be the way to get `b`. But of course Lisp would have a much more elegant solution...
use the right data structure for the right task! cons cells (single linked lists) are in no way the 'default' in e.g. Common Lisp. now, i'd fully agree to a critique that says: - representing lisp program code as single linked list is a bad idea (because e.g. no way to annotate the code without disturbing the meaning already there as understood by the compiler, etc). - singly linked lists are way much overused/misused in lisp (the context of one's thinking highly influences their decisions)
Linking discussion: http://news.ycombinator.com/item?id=2350381
This should be on /r/programming as well IMO.
A few scattered reasons. One, aside from just a learning tool, I am hoping to turn it into a project I can share with my partners, once something playable is in place. All three of us have a strong preference for fullscreen graphics. Personally, windowed mode is EXTREMELY jarring for me in ways that I can't quite explain. Two, I'm going to be doing my primary development on a netbook so...aside from the baby steps (getting the basics working right) I'm really going to want a full screen window to work with. As it is I only have 600 pixels to work with on the Y axis. But hey, a challenge is a challenge!
It's a good question and something I couldn't personally answer. All the more reason to start wrapping up something else I suppose!
Excellent, thank you very much! I would love to wrap up a full 3D engine, but as you mentioned most of those are in C++ (or C#) and I'm having enough problems just getting started with Racket basics and maybe a teeny bit of ugly C code. And for the time being, I don't think I'm going to need to facilities of a proper engine, just some basics. And I'm joining that mailing list now! Thanks!
I've tried to do some toy projects with scheme, and basically it came down to me wrapping things myself. Maybe I could get an sdl wrapper, but these days opengl is where it's at for 2d or 3d. Wrapping OpenGL isn't hard. But you don't have a game engine. So you need to pull in some more libraries. If the library is not C, it's a big pain in the tail, if it's even possible. In the end, I realized I would have to do a lot of things myself because the scheme library community is just not that lively. And despite Scheme being my all-time favourite language to work in, having to reinvent a lot of wheels really is a drag. Anyway, the article i wrote on basic 2d with opengl &amp; sdl is [here](http://www.animal-machine.com/blog/2010/08/2d-drawing-with-sdl-and-opengl-with-gambit/). Rereading the OP, I think just wrapping some opengl calls is a good start. You can try to work with freeglut or glew.
No. AREF is for arrays, it can be applied to an array of any dimension and is undefined if the indices are out-of-bounds. NTH is for lists, it can be applied to dotted lists, circular lists, and it returns NIL for out-of-bounds indices. ELT is for proper sequences, ie. vectors (one-dimensional arrays) or proper lists (lists terminated by NIL). It is undefined if its indices are out-of-bounds (clisp raises an error I think). edit: By the way, STRING-EQUAL is not a faster version of EQUAL. STRING-EQUAL compares string designators, ignoring case. EQUAL uses STRING= if its arguments are strings. For example, `(string= #:a #:a)` is true, but `(equal #:a #:a)` is not, and `(string-equal "a" "A")` is true, but `(equal "a" "A")` is not.
Done.
I probably should've said "An Introduction to Typeful *Common* Lisp". I'm aware of Racket's contracts and Typed Scheme extensions but that's not quite what I had in mind. Typed Scheme in particular seems to me to be trying to do Hindley-Milner/ML in Lisp. I'm more interested in hearing about how to use the existing mechanisms for creating safe and correct code.
On Linux I recommend grabbing the latest [SBCL](http://sbcl.sourceforge.net/). If you really need to stay on Windows, then [CLISP](http://www.gnu.org/software/clisp/) is fine. You can edit code in any text editor, so simply choose your favorite one on any platform. You will find most of the main CL nerds use emacs and [SLIME](http://common-lisp.net/project/slime/) as their "IDE". As for books, there are no good books on Lisp. I'm serious, every last one of them sucks ass out loud. But here is a list, taken in order from beginner level (that's you) to CHICK MAGNET (that's me). * [Common Lisp: A Gentle Introduction to Symbolic Computation](http://www.cs.cmu.edu/~dst/LispBook/) * [Land of Lisp](http://landoflisp.com/) * [Common LISPcraft](http://www.amazon.com/Common-LISPcraft-Robert-Wilensky/dp/0393955443) * [Paradigms of Artificial Intelligence Programming](http://norvig.com/paip.html) * [Object Oriented Programming in Common Lisp](http://www.amazon.com/Object-Oriented-Programming-Common-Lisp-Programmers/dp/0201175894) * [On Lisp](http://www.paulgraham.com/onlisp.html) * [Let Over Lambda](http://letoverlambda.com/) * [The Art of the Metaobject Protocol](http://www.amazon.com/Art-Metaobject-Protocol-Gregor-Kiczales/dp/0262610744) Good luck, it's a good job you're getting started young. But CL will ruin you for other languages FYI. 
I'm with you on the article having issues relating to the example. I really have more trouble with the lack of tying points to the example or expanding on them than I do with the example itself though. At any rate, as I commented [on another post](http://www.reddit.com/r/lisp/comments/g6khy/any_sufficiently_wellcommented_lisp_program/c1lc0fs), I'd love to see more articles covering using underappreciated lisp features or techniques to accomplish *practical* ends. TPD2 may well be a good example of this but without reading the code, who knows?
There's a spelling mistake: &gt;... multi-core CPU where literally hundres of cores ... You're missing a 'd' before the 's' in **hundreds**. If I see anything else I'll post it too. *edit0:* Under *Erlang* &gt; is sometimes just to much for should be "just **too** much." 
&gt; Clozure is quite nice as well, albeit producing not as fast code as SBCL. But it is producing code faster than SBCL.
After buying a "real computer" I don't notice that anymore.
Full-screen applications work just fine in Racket, if I understand your question correctly. For example, see the [Slideshow](http://docs.racket-lang.org/slideshow), which runs full screen. Do you mean something else?
Typed Racket is using exactly the mechanisms you're talking about. We use the macro system to implement everything. There's a paper about it [here](http://bit.ly/langlib).
Which Emacs extension is it ? ;)
Looks like [highlight-parentheses](http://www.emacswiki.org/emacs/HighlightParentheses) to me.
That is correct.
that is just cool
TIL special forms shouldn't be sharp-quoted.
I read that as "my parents are on fire" at first. 
Me too, and when I saw the image I still didn't get it.
Do want.
So, why did so many users give this a downvote without leaving a comment? It should be clear that the question is not a criticism, nor does it presume Lisp(s) would be appropriate for all projects. I hope to learn from insightful answers.
The title is in quotes because it is not me saying it. Anyway, I disagree with just about everything except the "lisp (list processor)" part, but it would be interesting to hear you all opine about it.
All right, so I've been trying out SBCL and it's been kind of disappointing. I've actually fixed all the super-frustrating bugs that prompted this post (yay! I'm still on a high from it :P ), but I'll make an example. Here's a working function: (defun get-best-index (move-map) "For the given dictionary of move candidates, returns the most popular move." (let ((list-max (apply #'max (coerce move-map 'list)))) (get-random-element (loop for i from 0 to (1- (length move-map)) if (max-p move-map i list-max) collect i)))) Let's mess it up, by feeding `apply` an array instead of a list: (defun get-best-index (move-map) "For the given dictionary of move candidates, returns the most popular move." (let ((list-max (apply #'max move-map))) (get-random-element (loop for i from 0 to (1- (length move-map)) if (max-p move-map i list-max) collect i)))) It compiles fine\*. The Python programmer in me is okay with that. The Java programmer in me is a bit uneasy. But we all hate Java, so fine. Then I run it, and get: The value #(7 6 9 6 11 4 5 6 5) is not of type LIST. [Condition of type TYPE-ERROR] Restarts: 0: [ABORT] Return to SLIME's top level. 1: [TERMINATE-THREAD] Terminate this thread (#&lt;THREAD "repl-thread" RUNNING {100380FF01}&gt;) Backtrace: 0: (GET-BEST-INDEX #(7 6 9 6 11 4 ...)) 1: (GENE-LOGIC (#1=#&lt;CLOSURE #2=(LAMBDA #) {1002907429}&gt; #&lt;CLOSURE #2# {1002907389}&gt; #&lt;CLOSURE #2# {10029073E9}&gt; #&lt;CLOSURE #2# {1002907489}&gt; #1# #&lt;CLOSURE #2# {1002907449}&gt; ...) #(#1="." #1# #1# #1# #1# #1# ...) #&lt;unavailable argument&gt;) 2: (GET-MOVE #(#1="." #1# #1# #1# #1# #1# ...) PLAYER-ONE #&lt;CLOSURE (LAMBDA (BOARD)) {10029084D9}&gt; #&lt;CLOSURE (LAMBDA (BOARD)) {1002908559}&gt;) 3: (CYCLE #(#1="." #1# #1# #1# #1# #1# ...) #&lt;CLOSURE (LAMBDA (BOARD)) {10029084D9}&gt; #&lt;CLOSURE (LAMBDA (BOARD)) {1002908559}&gt;) 4: (GAME-LOOP #(#1="." #1# #1# #1# #1# #1# ...) #&lt;CLOSURE (LAMBDA (BOARD)) {10029084D9}&gt; #&lt;CLOSURE (LAMBDA (BOARD)) {1002908559}&gt;) 5: (FACEOFF (#1=#&lt;CLOSURE #2=(LAMBDA #) {1002907429}&gt; #&lt;CLOSURE #2# {1002907389}&gt; #&lt;CLOSURE #2# {10029073E9}&gt; #&lt;CLOSURE #2# {1002907489}&gt; #1# #&lt;CLOSURE #2# {1002907449}&gt; ...) (#&lt;CLOSURE #1=(LAMBDA #) {10029074A9}&gt; #2=#&lt;CLOSURE #1# {10029073A9}&gt; #&lt;CLOSURE #1# {1002907389}&gt; #&lt;CLOSURE #1# {10029073C9}&gt; #&lt;CLOSURE #1# {1002907449}&gt; #2# ...)) 6: (FITNESS (#1=#&lt;CLOSURE #2=(LAMBDA #) {1002907429}&gt; #&lt;CLOSURE #2# {1002907389}&gt; #&lt;CLOSURE #2# {10029073E9}&gt; #&lt;CLOSURE #2# {1002907489}&gt; #1# #&lt;CLOSURE #2# {1002907449}&gt; ...) (#&lt;CLOSURE #1=(LAMBDA #) {10029074A9}&gt; #2=#&lt;CLOSURE #1# {10029073A9}&gt; #&lt;CLOSURE #1# {1002907389}&gt; #&lt;CLOSURE #1# {10029073C9}&gt; #&lt;CLOSURE #1# {1002907449}&gt; #2# ...) 3) ... --more-- From this stack trace, I can understand that: * I'm feeding something an array when it wants a list. * This is occurring in `get-best-index`. With that information, I can scrutinize `get-best-index` and *since there's only one array being passed around anyway* I can eventually realize that I fed the wrong thing to `apply`. But if I had three or four arrays floating around, things might've been murkier. Why the heck couldn't SBCL say "`apply` on line 123: `(let ((list-max (apply #'max move-map)))` received an array and expected a list"? Ah, but I should've compiled with maximum debugging support. So I did so, hovered over item 0 in the backtrace, pressed `v`, and had my cursor jump to the `get-best-index` definition. I'm using a `C-x 2` -ed window (REPL on top, code/stack trace on the bottom), and pressing `v` put the relevant code in the top split so that I could look at the code and stack trace at the same time. That's actually pretty convenient (thanks for the tip). But it still seems....kind of lacking. The lesson I'm learning is that all of my functions should be as short and simple as possible, so that it's easier to spot an error in one. That's good style anyway, right? So this is healthy, if nothing else. \* - The one big annoyance about compiling with SBCL instead of clisp is that it always warns me that I'm redefining the three constants I have at the top of my program. Of course they're being redefined--I'm recompiling! What am I supposed to do?
Well [I tried SBCL](http://www.reddit.com/r/lisp/comments/g7m5h/i_think_im_misusing_stack_traces/c1mi6sk), and it was a bit better...but not by much.
It's riddled with factual errors and poorly formed arguments. I think the poor OP is just frustrated. "Math is hard, Ken!"
TL;DR: his other car is not a cdr, and smug lisp weenies gonna smug.
Considering that lisp hasn't been about list processing in any general sense (much less as a name expansion) for decades, even that's erroneous.
OP is showing off his paren hilighting.
"Eloquent pleas of poverty will be considered." i like this guy.
When you press V on the frame with enough debug (like 2), it will bring you directly to the source and highlight "(apply #'max move-map)". And defconstant. The standard says that the new value should be EQL to the already existing. You shouldn't really use defconstant if you're planning on redefining it, or you're using data structures which aren't comparable enough by EQL (like lists, strings, etc.). Use defvar or defparameter instead.
This article is inaccurate, to say the least. Bitterness is clearly present. Most of the early parts of the article regarding fanaticism could really be said of any language. As for not being on that guy's team, to paraphrase, is really a poor reason to discredit an otherwise perfectly good, albeit relatively unpopular programming language (at least professionally).
What? It's all about list processing. It was never about being a language for just processing lists as the primary goal. 
How do I set the debug level? For the defconstants, I have (defconstant +player-one-mark+ "X") (defconstant +player-two-mark+ "O") (defconstant +empty-mark+ ".") And I haven't changed them for the past three months. But the compiler complains that, for example, `+empty-mark+` is being changed from "." to ".".
Yeah, I realized that ~10 seconds after. (by reading the title better)
This is excellent news. I had long ago given up on USENET in general and Google Groups in particular. Maybe I'll return to check in on the Lisp world again.
(optimize (debug 2)) declaration. Or use C-u C-c C-c or C-u C-c C-k in Slime.
I already used `C-u C-c C-c` and `v` just sent me to the `defun`, not the `apply`.
Are you sure that you're using recent SBCL and recent Slime?
No. I use Crunchbang Linux, which was previously based on Ubuntu but is now based on Debian. When I got my computer last November, the Debian version was still Alpha--so I went with the most recent Ubuntu version, which is based on 9.04 (when 9.10, 10.04, and 10.10 came out, the Crunchbang people were busy working on the Debian version). So my repositories are a year or two out of date. I'm running SBCL 1.0.18 instead of 1.0.47, Emacs 22.2.1 instead of 23.3, and presumably an equally dated version of SLIME (I'm actually not sure how to check that). So as soon as I have a few hours of free time, I guess I should look into upgrading my OS (I've actually been bitten by this problem before--bazaar, subversion, and Flash are the first examples that come to mind.) 
As bad as this is, imagine the damage he could do if he tried to *promote* Lisp.
So he said his email was jfk@berkeley. When/why/how in Internet history did the ".edu? start coming into play?
All right, so I just compiled, and wasn't expecting an error so I was just sitting on `0` to ignore the warning about redefining constants\*, and next thing I know my source is back in front of me, and part of it's underlined, and I moved my mouse over to hover on it, and a tooltip said something like, "`how-many-empty?` was expecting 2 arguments and received 3`", and sure enough, I had a parenthesis error and was feeding it an extra argument. So that, at least, was magical.
Domains were first proposed in [1983](http://tools.ietf.org/html/rfc881). EDU was proposed in [1984](http://tools.ietf.org/html/rfc920). Before that I don't know how it was done.
What would be the gains? Not trying to be a troll here, but I really can't see a possible benefit, from reading the project page. Can you enlighten me?
i guess the gains would be philosophical mostly. but the page's rant on OSs as interpretative languages, not languages, is a good point. if you could design an OS in lisp that has the functions of a language built into it, it's possible (if done right) that you could make a machine that needs to do less for the same ends. I guess it's intriguing for the simplicity argument, by reducing the degrees between the user and the hardware.
but that's all immaterial. because what i really want to do is to create a system that is self determining. if you start with a base kernel in lisp, you could theoretically create a user function that can communicate directly with the kernel/system and respond 'intelligently' to stimuli.
Not the idea. If, and only if, you can bring the potential benefits of having 100% lisp from bottom up, then I'm interested. If one wants to do mental masturbation, not interested. 
well, i think the idea is to have a full lisp system. i think it just needs people to help.
But .. mental masturbation is the second best kind!
Would you be willing to work with the Lisp kernel others have been developing? As I see it, enthusiastic young men start building minimal lisp kernels every five years or so. Only think they can't do is to adopt project, code or dialect that others use. IMHO most Lisp kernel projects come from motivation of "to implement my own lisp from bottom up". The main motivation is to build **own** minimal Lisp/kernel. Motivation to code with already existing Lisp is very low. Sorry if I sound cynical, but search comp.lang.lisp for LispVM, LispOS, Moviz, etc. and you understand why (you studied the previous art, right?) For example: can you explain why anyone would like to write all the code again for new lisp dialect for new Lisp kernel? If you dont' use Common Lisp or one widely use Scheme dialects, you have lots of explaining to do, including showing that your lisp is better than commonly used ones. 
I don't disagree. But you do realize that retrophonic is asking help for his endeavor? I have this old fashioned belief that masturbation of any kind is best when done alone and nobody is watching. 
I agree with most of this but, come on! Implementing your own lisp dialect is **fun**! ;) I'm doing it myself. The ultimate goal (you know, the one I probably won't achieve before it starts getting boring work) is to have it generating code for the LLVM backend, but merely starting an interpreter for a lisp dialect is loads of fun, and an endeavour that gets one walking through the path of a ton of books and papers on varying subjects just to get things straight.
Why not just try creating a LLVM backend for an existing compiler, like Clozure Common Lisp or SBCL?
There are two kind of Lisp programmers: 1. Those who love the idea of Lisp and want to implement in again and again. 2. Those who love programming in Lisp. They want to build other tings than Lisp using Lisp and do things not done before. 
In truth, I was going for the laugh. Though, if the OP and some other people want to have some fun and learn some stuff, that seems OK to me.
... but then again, why even do *that* in the first place?
I put myself into camp #2 (Common Lisp is a daily part of my life), but implementing your own language compiler/environment is not always a waste of time--it can be a pretty good educational exercise.
Isn't the core philosophy behind LISP and other extremely high-level languages is to separate you as much as possible from hardware? The purpose of an operating system is to manage and balance the utilization of the hardware while simultaneously making the benefits of the hardware present yet abstract enough to write sanely. Have you taken an OS course yet? What that project page is describing seems strange. Operating systems are very stateful types of programs. The justification offered by the "rant" page is counter-intuitive and there is no coherent argument why the counter-intuitive thing *could* be right. Sure, a runtime could be thought of as a language but I don't see what insight is offered by that perspective. The analogy breaks down when you consider that languages say stuff. Applications aren't interpreted by runtimes. They're run on the bare hardware but their ability to do so is managed by the operating system. I think your summer could be better utilized doing something that is not a dead end. Maybe you should make a Turing machine emulator so that ideas in theoretical computer science can be proven in reality.
FYI, breaks CLSQL and Elephant, both of which use undocumented, unexported, unsupported internal behavior that changed between .46 and .47. 
Agreed. I'm not saying it's a good idea, just that if you want the mental masturbation it's an alternative. More interesting to me would be SIMD parallel extensions targeting GPUs. 
Nice try, Haskell troll.
you are mistaken, good sir.
true. i don't know what there isn't more of a community interest in the idea though. wouldn't it be nice for most lisp devs to implement a machine specifically designed to operate as lisp does?
what about my idea i mentioned in my other comment? is that not a good use of my time? 
You may also like this: http://www.stripedgazelle.org/joey/dreamos.html From a very quick look through, I think the code seems to be quite readable and compact.
It seems my sense of humor is not well received.
How about Rich Hickey (clojure), Andrew Sorensen (impromptu), Richard Stallman (emacs), Tim Burks (nu), Alexander Burger (Pico Lisp), Andy Gavin (GOOL and GOAL) ...?
They all had specific, practical benefits. Clojure combines a powerful and popular backend, the JVM, with a powerful and expressive language, Lisp. The JVM has a few technical benefits (platform independence, not-broken concurrency), and whether we like it or not, it's sometimes necessary to interface with Java. I'd rather do so with Lisp. (I'm not hugely familiar with Clojure...it may be fair to wonder why someone couldn't just keep the syntax and semantics of CL, à la Jython/Python) Emacs is, in my opinion, one of the two most powerful text editors in the world, and I'm very happy that it was created (though I'll always remain a vim user at heart). Regarding Emacs Lisp, here's the relevant Wikipedia quote: &gt; Unlike Common Lisp, Scheme existed at the time Stallman was rewriting Gosling Emacs into GNU Emacs, but he chose not to use it because of its comparatively-poor performance on workstations, and he wanted to develop a dialect which he thought would be more easily optimized. (I'll admit that I know next to nothing about the other examples you listed, so I'm really not qualified to comment on them.)
I have to admit I'm into camp #1! And happily so. Unfortunately, there's no way I can use Lisp in a daily basis other than my own pet projects. It's hard to find people in the Academia around here that is interested in lisp, for flying spaghetti monster's sake! And, hey, the fun is always there to be taken. Since I started this attempt, I've read the most part (I'm reading them concurrently, so all of them are in the middle hehe) 4 books that I'd never read otherwise, and have a pack of them waiting to read later. I guess if I could find anything other than hobby projects to work with Lisp (specially since my preferred dialect is Scheme, which seems even less known to the "get work done and we pay you" crowd) I'd share your view. Alas, as it stands, Lisp is more of a hobby to me. And implementing it is funnier than using it.
&gt; .it may be fair to wonder why someone couldn't just keep the syntax and semantics of CL ABCL, Kawa (Scheme), others.
Think of it more as an exercise in understanding Programming Languages. You see, the backend isn't even the interesting part as of now, hence it's being relegated to the nevercoming land of "later". I'm more interested in the interpreter and all the programming language concepts one can try/understand/learn how to implement during the phase of experimenting with a toy lisp-dialect interpreter. Generating a backend in LLVM is more of "something to do once I'm sick of writing interpreters, to learn a little bit more".
Think of it more as an exercise in understanding Programming Languages. You see, the backend isn't even the interesting part as of now, hence it's being relegated to the nevercoming land of "later". I'm more interested in the interpreter and all the programming language concepts one can try/understand/learn how to implement during the phase of experimenting with a toy lisp-dialect interpreter. Generating a backend in LLVM is more of "something to do once I'm sick of writing interpreters, to learn a little bit more". EDIT: Argh. Stupid 502 Error.
* The use of strings should go first. The marks on top should be symbols. * making GAME-LOOP a recursive function is not a good idea * In the function CYCLE, the use of DO looks obscure. * PRETTY-PRINT is a poor name for a function. * (loop for i from 0 to (1- n) ...) is just (loop for i below n ...) * If you have symbols, you can get rid of all the STRING-EQUALS and just use EQL or CASE * (cond ((equal foo 'bar) ... should be replaced by CASE. There is more to change. (if (&gt; a b) t nil) is just (&gt; a b). * in human-move, the IGNORABLE declaration makes no sense * PROMPT-FOR-CELL : using -1 as a fail value is ugly. Lisp has real error handling. At least use a symbol. * NOT-FINISHED-P : duplicate cond cases * NOT-FINISHED-P : poor name, get rid of the NOT. game-ongoing-p * NOT-FINISHED-P : (and (not (get-winner ...)) (some #'empty-p board)) * EMPTY-P : confused. CELL is a string? With METHOD, etc. Way too clever. Use a simple function. * TO-STRING: use CASE * FLATTEN uses too many APPENDs * GET-CHROMOSOMES: the use of FLATTEN should be removed * GENE-FUNCTION: the recursive loop via LABELS is poor style in CL. CL has iteration constructs. * GET-BEST-INDEX: what use is the introduction of MAX-P when you have to comment its use? ... In many places you use more complicated constructs than necessary. Like strings instead of symbols. You are using all kinds of different ways to loop (recursion, labels, DO, LOOP, ...). This looks more like you want to use them all. 
An operating system that learns? That could be interesting. I'm assuming you're a college student since summer vacation is implied here. Still, even if we're talking about a learning OS, you'd still be better served by doing it on a normal OS and maybe do the rules in LISP. I just don't understand what you're hoping to build that's better. It doesn't sound to me like you've taken an operating systems course because what you say you'll build is in terms of vagaries. And I don't think you understand the design space of this type of software. I think you'd have a much better summer if you found a mentor or internship. Maybe you could work on one of those Google summer of code things. You should do some reading around on virtual machines, emulators, and prior LISP machine. Java hardware failed. Fond out why. I just can't see the learning value and there don't seem to be more experienced people to work with. Life is short.
Couple of minor comments use `(celling x 2)` instead of `(celling (/ x 2))` (cond ((equal winner +player-one-mark+) 1) ((equal winner nil) 0) ((equal winner +player-two-mark+) -1))) Should use case. Your comments are a bit over the top. (defun not-finished-p (board) "Decides if the game is over yet. Returns true if it's not finished, nil otherwise." I would assume this the function name is self documenting. Mind you I'm probably too terse at comments. I would use chars instead of strings for board markings. It can be much faster, and has less ways to go wrong. (loop for i from 0 to (1- (length board)) doing (let ((mark (aref board i))) (cond ((string-equal mark +player-one-mark+) (incf p1-count)) ((string-equal mark +player-two-mark+) (incf p2-count))))) (if (&lt; p2-count p1-count) +player-two-mark+ +player-one-mark+))) You should use `count` instead.
Noble goal. If you don't have "Lisp In Small Pieces" then I highly suggest it. 
New feature request for quicklisp: Mark modules that use unexported, unsupported internal behavior. I had been thinking about using CLSQL more seriously for a project, but now that is out the window.
Sometimes there just aren't any better options, and the risk is low.
They pulled if off and created good platforms. But I think most of them worked hard alone, or with very small group of people in stealth mode until the software was feature complete enough to be released. Opposite of this model is those people who just make scaffolding and create website and ask people to join. Doing this when there is no benefit for users (Arc) is doomed to fail. 
Postgresql with postmodern. Of course, if you need Oracle, that is out of the question.
good suggestions.
I mean options for using unsupported, unexported stuff. If the implementation doesn't provide a supported way to do what you need to do, you can change what you need to do, or use an unsupported way.
pretty cool stuff. emacs has trouble with defmethod, what environment are you working with?
grep :: *.lisp
Sure. Though in both of these cases a lot of pain could have been avoided if someone had sent mail to sbcl-devel to the effect "hey, we needed to do this -- can you support this, or is there a better way?" Obviously sometimes the answer is going to be "um, no", in which case you're stuck with internals -- but other times you may be in for a pleasant surprise.
If this is your first real CL program this is very impressive. But I wouldn't enter it into a science fair as an example of AI because this is not AI. You misread the javascript example and left out key concepts. At best this is an example of CL programming. Programming before you understand the problem just produces a bunch of code that has to be thrown away. You took a javascript example showing how to do a simple GA (Genetic Algorithm) and then somehow mangled it to be more like a GP (Genetic Program), except you left out key parts: sex (crossover) and mutation. Your 'evolve' function is also non-standard and not better than the conventional method. You stumbled upon Tournament Selection but you aren't using it quite right. You should be able to achieve good results with as small as 300 chromosomes if the GA has all of its parts. There will still be a large number of generations required to get a satisfactory answer, however. Your memory use in this program is highly inefficient because you're relying upon a large pool of randomly generated chromosomes to get a decent answer. That is not the right way to generate a strong chromosome. You also don't have a very good understanding about when to use iteration and when to use recursion. One is not automatically better than the other -- it depends on the problem you are solving. If your problem is "small" then recursion is acceptable, but iteration is preferred for large problems because that wont overflow the stack, which can be a problem in some implementations of CL. If you're interested in reading about GAs then read John Holland's GA articles or David Goldberg's "Genetic Algorithms in Search, Optimization &amp; Machine Learning." If you're interested in reading about GPs then read Koza's books. There is some important math about GAs that your version ignores which is why you are getting such crappy results. Your comments are by and large helpful but the very first comment in your program is either wrong or out of date. That's usually a bad sign. Your use of the word "gene" in your program is wrong. Look up what the word really means. You are using it in places where you really mean 'individual' (as some parts of your program call a collection of chromosomes). A gene can be found on a chromosome, not the other way around as your program implies. Actually, the proper word would be "allele" since an allele is the physical representation on the chromosome and a gene is the trait itself. This is an innovative way to solve tic-tac-toe. Unfortunately, innovative is not always better. GAs are good at searching large search spaces. The search space for tic-tac-toe is so small that a lookup table can solve this problem, and do it more efficiently and with greater accuracy. [xkcd](http://xkcd.com/832/)
Yeah, it's awesome. I can't wait for the new edition to be translated to English, though. I can't read french :P I've been reading on-and-off four books so far: * Lisp In Small Pieces * Scheme 9 from Empty Space * Programming Languages: Application and Interpretation * Essentials Of Programming Languages I have to say I've learned more in the last couple months than what I had learned in a regular semester back then in college. Specially since all this info about programming languages had me reading various papers on Garbage Collecting, Representation of runtime type information in dynamic languages, and various other concepts.
I'm using Emacs+SLIME+sbcl (but much of the work was done with clisp instead of sbcl). I'm not quite sure what to think about the `defmethod`. Sometimes it throws a compilation error. Sometimes it doesn't. I've never had it not work at all, so I've usually had more important bugs to fix, but....yeah, I don't know what to think.
Many, many thanks. Regarding looping....yeah, I was kind of like a kid in a candy store with it. I have a part-time job writing Java, so it was really nice to have options other than its kneecapped while, for, and for-each. Whenever I needed a loop, I'd just use whatever striked me (although in general I hate DO)...as I continue to program, I'll definitely try to settle down and use the simplest one out of dolist/dotimes/do/loop (and save recursion for situations in which it actually makes sense (like in `flatten`, methinks), and not just as a glorified `goto`. IM). For almost all of your points, my response is, "Oh, okay. Good call; that makes sense." I have a few questions/comments though. &gt;in human-move, the IGNORABLE declaration makes no sense Yeah, sorry, that's an artifact from when I was trying to decide where to ~~`pretty-print` the board~~ `print-board`. &gt;FLATTEN uses too many APPENDs How would I cut them down? I just tried to do it [another way with](http://pastebin.com/HcsmCxps) `push` and no `append`s, no `setf` and no duplicated code (the current version has `(setf new-list (append new-list (&lt;something&gt; i)))` twice, which is irksome). But it didn't work, so...I'm a bit stuck. &gt;GET-CHROMOSOMES: the use of FLATTEN should be removed I just really enjoy that I can collect a bunch of chromosomes (like in the loop that will create and collect a chromosome that checks each row/column/diagonal of the board) and not worry that I'm returning a list of chromosomes instead of a single chromosome. Would you be happier if I restructured it to have something like `(dolist (chrom (loop ... collecting ... doing &lt;lots o' code&gt; ...)) (push chrom gene))`? Because that also seems hackish to me (arguably more so). Is there a much more elegant solution that I'm just not seeing? &gt;GET-BEST-INDEX: what use is the introduction of MAX-P when you have to comment its use? True...I think I got overzealous about commenting since Other People (including judges at the aforementioned engineering fair) are going to be looking at it. I'm going through and fixing all the comments like: `"Checks if quux is foobar." (defun foobar-p (quux)...)` and `"Frobnicates the spam." (defun frobnicate (spam)...)` &gt;Like strings instead of symbols. I got called out for this last month when people critiqued my game engine (with no AI). I used to have `(defconstant +player-one+ 1)` and have something like `get-turn` return 1. People told me to use symbols, so now I have `'player-one` and `'player-two`. I'm just not used to coding with symbols available--thanks for reminding me about this. Again, thank you for your wonderfully helpful comment.
&gt;If this is your first real CL program this is very impressive. My first program (besides `(format t "Hello World"!)` and such) was a function that converted temperatures from Fahrenheit to Celsius. Dang. You're impressed :) My second was a few of the truly trivial exercises in SICP (in CL, not Scheme). They effectively demonstrated that I knew how to cons, car, and cdr pairs of values. My third was this tic-tac-toe engine, and my third/fourth (depending on how you define it) was the AI generation. So yeah, I'd say it was my first *real* CL program. Thanks for the compliment...but more thanks for your insightful criticisms. &gt;You misread the javascript example and left out key concepts. You're right, what I learned from the Javascript example mutated in my head a lot (never mind what I didn't learn from it at all). For the first three-quarters of the project, I was fighting my inexperience with Lisp itself, and thus tried to make the GP part as simple as I could. Two months ago, I would've considered the result I have now to be exactly what I wanted. Now that I've actually played against it, it's a bit disappointing, and now that I have a firmer grasp on Lisp, I've been freed to ponder all the problems with my GP approach. As I said in the OP, I belatedly realized that crossover would've been a really good idea. I'm not sure how much I'll be doing with this after the science fair (I think it'd be a better use of my time to hit the books--*Paradigms of Artificial Intelligence Programming* by Peter Norvig looks really appealing) but if I do keep working on it, I'll definitely refactor it to include sexin'. (While I'm at it, I'd also like to include a way to see the chromosomes of a given, already-compiled AI. I'm curious about what I'd find.) *Update:* The OP may have nice plans about trying to find the "better" chromosomes with which to make children, which I can't do. But I could just blindly combine the chromosomes of two individuals...is that even worth it? My gene pool is so small that I feel like the odds of randomly getting a better chromosomes from a moderately-successful individual are just as bad as getting one from the entire gene pool. Regarding mutation, I'm actually not sure how to implement that. In the examples I've seen, there's been numerical parameters that could be modified. Like the length of whatever could mutate from 10m to 12m (to try to get a global optimum instead of a local optimum? As you're well aware, my theory is very shaky.) I'm not sure what could mutate in my example, short of swapping a chromosome with one from `(get-chromosomes)`. But as far as I see, the makeup of a given chromosome itself is set in stone.....right? &gt;You stumbled upon Tournament Selection but you aren't using it quite right. Wikipedia is confusing me, is this right? I'm supposed to take an arbitrary amount of individuals from the population, run a tournament among them, send the winner to an orgy, and repeat this ? many times. Then babies come out of the orgy, and I repeat the entire process again (but only with the babies, right? Not the population at large.) Once I run out of processor time, I'll then have a bunch of great-great-great...-grandkids that are all much better tic-tac-toe players than their ancestors, and I can then run a final tournament among them to get the very best one. So if I'm understanding that right (which is a big if) and since I currently don't do crossover, would there really be any way to follow it more closely? I think the only problem it'd solve is that my current tournament doesn't not handle population sizes that aren't powers of two very well. It keeps cutting the population in half, and if the population size is an odd number, it the odd man out loses by default. &gt;If you're interested in reading about GAs then read John Holland's GA articles or David Goldberg's "Genetic Algorithms in Search, Optimization &amp; Machine Learning." If you're interested in reading about GPs then read Koza's books. I'm thinking that I should probably get a better grounding in AI in general (it seems prudent to get details about all the subcategories before specializing). But thanks for the titles--I'm much happier getting book recommendations from knowledgeable people, not Google. And I'm sure I'll be investigating GAs again within the next year or two--just probably not the next month or two. &gt;Your comments are by and large helpful Thanks. Although there's a few `"Determines if baz is foobar." (defun foobar-p (baz))`-type comments. I'm working on it. &gt;but the very first comment in your program is either wrong or out of date. That's usually a bad sign. :P Yes, I wrote the comment rather a while ago, and have since then read it without *read*ing it, and totally missed the problem. Good catch. &gt;Your use of the word "gene" in your program is wrong. Dang. I think I was confusing 'gene' and 'genome'. Alleles wouldn't be the whole list of chromosomes though, right? They're just types of genes. (Whoa...biology class was not recent. And I thought I'd never use most of what I learned. And not once expected that it'd come up as part of a programming technique.) &gt;This is an innovative way to solve tic-tac-toe. That was the goal, so thanks. I'm a lot more interested in just doing something pretty cool (and learning Lisp!) than I am about actually creating a good tic-tac-toe player. That would just be a very pleasant side-effect. &gt;Unfortunately, innovative is not always better. GAs are good at searching large search spaces. The search space for tic-tac-toe is so small that a lookup table can solve this problem, and do it more efficiently and with greater accuracy. [xkcd](http://xkcd.com/832/) Yeah...I remember writing a tic-tac-toe AI in Python two and a half years ago, using a very straightforward approach. If I recall correctly, it played a perfect game and was less than 50 lines long. Sigh... &gt;But I wouldn't enter it into a science fair as an example of AI because this is not AI. Speaking as a semi-informed redditor, part-time professional Java developer, reader of books and articles, and avid programming enthusiast...I completely agree. Speaking as a high-schooler trying to put together something for a science (actually engineering) fair...I think it's close enough. It's a system whereby a machine can analyze a problem and produce solutions (er, of questionable quality), and so loosely speaking, I think it's AI...just not very intelligent AI. For the fair, I registered my title as "Genetically Generating AI", which gives me a lot of leeway regarding what I actually say in my paper and oral presentation (I didn't lock myself into some too-difficult-to-respond-to-with-this-data question). I'm thinking my basic point will be that in this specific situation, a GP approach is clearly not practical at all (hard to implement, hard to think up good chromosomes, bad to terrible performance (on the hardware), lackluster results in the game...) and that it was so bad in this simple example (where there's a pretty straightforward optimal solution to be found) that it in my opinion it implies that the approach, at least in this form, probably wouldn't be good at more complex situations (checkers, chess, go, even something crazy like StarCraft). That's fair, right? (Keep in mind that I don't want to be accused of jumping to conclusions.) And I'll also put down that areas for further investigation include crossovers, mutations, and a computer/time to work through a humongous population. TL; DR - Sorry, read it. Things Within: A discussion on GA/GP (obviously), questions about mutation, questions about tournament selection, more comments on the project, discussion on the engineering fair...and this: Thank you very, very much for your comment--it was excellent for me to hear from someone who knows far more than I do, and get pointed a bit closer to the right direction.
Thanks. Good calls all around about code cleanup, although I'm a bit confused by the last one--how do I count two values at once...conditionally, no less? (I presume you're talking about doing it with `loop` keywords.) Regarding comments...yeah, I definitely went a bit overboard on some of them. I was just hyper since Other People were going to be looking at it (and someone else pointed out that my very first comment is incorrect :P ). I'm working on removing or improving the obvious ones.
It looks like your loop can be replaced by (loop for mark across board counting (string-equal mark +player-one-mark+) into p1-count counting (string-equal mark +player-two-mark+) into p2-count finally (return (if (&lt; p2-count p1-count) +player-two-mark+ +player-one-mark+))) but I don't have a REPL to check.
Today I learned about the `into` keyword, thanks. It's getting very late here, so I'll test this in my actual code tomorrow. But I agree that that's a lot more elegant. Also, `loop` is ridiculous. But I still love it. 
No the function `count`. Just use it twice unless you think it's a bottle-neck. (if (&lt; (count +player-two-mark+ board) (count +player-one-mark+ board) ) +player-two-mark+ +player-one-mark+) Although, honestly, the fact that you don't know who's turn it is at any point in the code is slightly worrying.
Lisp got me through Stats II. Writing all the math in Lisp helped me understand how the basics worked.
&gt; I think it'd be a better use of my time to hit the books--Paradigms of Artificial Intelligence Programming by Peter Norvig looks really appealing I'd have to agree. Lots to learn there. Tournament Selection is simple: randomly take n chromosomes (or Individuals, as you call them) from your population, and discard all but the fittest. This is parent A. Do the same for parent B. That's it. Tournament Select is done. Now they are ready to have sex (i.e. crossover, in the case of GAs). 'n' is usually 2 or 3; 3 usually puts too much pressure on the population to converge so 2 usually ends up being optimal. (BTW, you should also be checking whether your population has converged and stop immediately if it has. Otherwise you are just burning cycles.) Yes, crossover is worth it. It's necessary, in fact, if you want to put pressure on your population to converge to a highly fit individual. The math behind GAs are built on this one operator. It's core. Crossover, in your case, would be something like this: Take the first x alleles from parent A, where x is: (random (length chromosome)). Take the last (length - x) alleles from parent B. Splice the two together to make a new chromosome (the baby that survives into the next generation). You can always do mutation. The trick is to make sure that the mutation operator doesn't create an invalid Lisp op or seriously mess with your GA. In your case the obvious mutation operator is to replace one allele with another randomly selected allele. Mutation is rare and only occurs about 2% each time a crossover is performed, but it is necessary. Mutation could also include randomly deleting one of the alleles from a chromosome, or, randomly inserting a new allele. But the primary mutation op would be swapping a new, random allele in. So, to answer your question, in a typical GA chromosomes don't change in length, but in your program this constraint is neither practical nor efficient: you should get a better answer by allowing the chromosome to grow or shrink. This is the way GPs are set up, and your program shares some characteristics. So don't be afraid to let the chromosome change quite a bit. The way you wrote the program destroys the math behind the AI. The math proves why the GA works, not the code. What you are calling chromosomes are really alleles. When you make an individual by randomly sampling your collection of alleles you are creating one chromosome (which also represents one individual). No, GAs (or GPs) can be used to solve ANY complex program -- just not the way you have written it. The trick is in choosing the proper representation (the chromosome) and objective function (or fitness function). Of course you have to also perform evolution correctly, but that should be pretty much a given (reusable code). If you sit down and think about the problem you should be able to come up with a way to use these techniques to find local maxima on any large problem. GAs/GPs don't work well if you have to find global maxima, but that's usually not too big a deal when the search space is large. The larger the problem the better. Why? Because there are very few alternatives to GAs/GPs when the search space is gigantic. The search space is too big to enumerate using convention search techniques, so some type of stochastic or heuristic must be used.
CLSQL now fixed courtesy of nsiivola's patch and Kevin Rosenberg's timely application. It'll be in the next Quicklisp update.
Code generation isn't your problem anymore? Glasgow Haskell Compiler did it, and LLVM's been beating the pants off their native code gen for tight numerical loops.
For Haskell sure that makes sense, the language is static and LLVM is written in C. Ok. For a Lisp compiler? Not so much, it is nice being able to `M-.` and alter the compiler in a running Lisp image. For both Clozure and SBCL there doesn't seem to be much point in replacing the compiler backends--they are mature and well suited for the purpose of generating optimized machine code for Lisp. It looks like the [situation with GHC was significantly different](http://www.cse.unsw.edu.au/~chak/papers/TC10.html)--they either compiled to C, or to a still not-so-great native code generator. Both of Haskell's compiler backends had significant limitations (e.g. fixed register set allocation and a general inability to handle register pressure) that SBCL at least does not have. Clozure might be another story, but you won't see me doing it.
All right, I think I've implemented crossover and mutation as you suggested. The entire program is [here](http://pastebin.com/auHg1Tm5), and just the new stuff is [here](http://pastebin.com/qKhzCihL). Each "round" of the selection actually reduces the size of the population by a factor 2^2 instead of 2. If there's 256, then there'll be 128 matchups which yield 128 champions, and the champions then pair off into 64 couples and have one child each--for a total of 64--that goes to the next round. I think this is the mechanic you had in mind, but am not sure. I guess another way would be to have each couple have two children. (Or maybe get a bit crazy and have the top 50% of parents (based on their fitness score) advance as well (note: I don't actually like this idea.)) But in my uneducated opinion, my processor time is better spent working through larger populations than smaller ones more thoroughly. I know the code's a bit rough (and also inefficient) in a few places, but I'm in a major time crunch and have to prioritize, and code hygiene is not as high on the list as I'd like it to be. With a few tries with an entirely-too-small initial population (32, IIRC), I got one AI that was noticeably smarter than usual, in that it blocked one of my feeble attempts to win and almost set up a fork (where it can win with either of two moves)...but I evaded the fork by winning--it didn't notice that I had two in a row. It's not earth-shaking, but I think it showed some a *little* bit of intelligence. I'm hopeful about what the results might be when I run it overnight with a large initial population. Good calls in your last paragraph; thanks for that.
Just to be clear I'll go over the typical, high-level Genetic Algorithm. Of course your algorithm is a bit different. The typical algorithm for 'evolve' goes something like this: Given: population of n chromosomes, randomly generated at Gen 0. Do until there is a new generation of n chromosomes: Insert your best chromosome into the next generation w/o sex (this is done just once at the start of each new generation) Tournament Selection (usually pick 2 candidates), pick parent A Tournament Selection (usually pick 2 candidates), pick parent B Crossover parent A with B Mutate Chromosome (possibly) Insert this new baby into the next generation. Repeat. Do until population convergence: Evolve a new generation Test each chromosome and note their fitnesses. If 80%+ of the chromosomes have converged (look the same or are functionally identical), then stop, you aren't likely to get any better solutions when the population has converged. Yes, you (typically) have to sample 4 chromosomes/individuals to produce one baby, but then, ideally, you would allow those same chromosomes to be used later to be (potentially) resampled to make another baby (because Tournament Selection randomly samples from the entire population). But your method is close enough for a science fair. Now that you have got this far, the things that will have the most impact on your results will be tweaking of the representation and objective function. If you throw in smarter alleles than what you are currently working with you will see a noticeable improvement in AI, if that's your goal. Your GA needs good building blocks. Don't be afraid to make your building blocks smart -- that's not cheating. I'm glad you incorporated sex. It's pretty simple to do, and now you can say that it includes all major properties of a GA. 
&gt; Isn't the core philosophy behind LISP and other extremely high-level languages is to separate you as much as possible from hardware? I can't speak for LISP since no one has used it in decades, but with Common Lisp and Scheme, I think the philosophy is to let you work at the level that makes the most sense in the given context.
You mean like [Lisp Machines](http://en.wikipedia.org/wiki/Lisp_Machine)?
In a surprise press release, Oracle CEO Lawrence Ellison announced that the 21 year old Java platform was getting long in the teeth, and was due for retirement. It is to be replaced by an Enterprise Lisp Compiler (ELC), to be released in June. Mr. Ellison cautioned that great care had been taken to make the Lisp platform Enterprise ready, including the removal of several features that did not synergize with existing infrastructure, such as closures, dynamic typing, and incremental compilation. Instead, Mr. Ellison said, "we have paid great attention to the requirements of our customers, and are rolling out many new features, such as Excel integration, and a new object model based on SQL."
Seriously, Ellison has understood nothing of Lisp. Well, at least it's better than Java. I'll just continue using Scheme anyway.
No joke? This isn't an April fools joke?
I should have known that Oracle would not have had enough of a corporate sense of humor to have actually done that themselves.
I was tempted to scrape together a bogus Oracle website, and try to get some Java folks riled up, but 5 minutes is about as long as I'm willing to spend on April Fools.
what? no comment from [Guy Steele](http://labs.oracle.com/people/mybio.php?uid=25706)
Sorry, the title was too far beyond belief to even catch me for a second. Nice joke, though. My hopes starting rising but only until my forebrain caught up.
From the thread: &gt; wow, very cool... waaaiiit... &gt; &gt; ;;; -*- lexical-binding: t -*- &gt; (defun april-fools? () &gt; (let ((a t) &gt; (check (let ((a nil)) (lambda () a)))) &gt; (funcall check))) &gt; &gt; (april-fools?) =&gt; nil &gt; &gt; This is great news.
Found on /r/programming, with a better title which I didn't want to just steal: [Emacs Lisp now lexically scoped. "Oh, very funny". No, really.](http://www.reddit.com/r/programming/comments/ggmc2/emacs_lisp_now_lexically_scoped_oh_very_funny_no/) submitted 6 hours ago by DGolden [1]
\&gt;:-(
shit just got real.
Some interesting/relevant comments I caught on #lisp regarding check-type, assert, etc: &lt;tmh&gt; Is the primary motivation for using conditions over things like CHECK-TYPE, CCASE, CTYPECASE, ASSERT, etc. is that you give functions higher up on the call stack the ability to handle the error? &lt;nyef&gt; tmh: It typically provides more (programmatically accessible) information about the error, yes.
Just waiting for this to hit the Git mirrors now. 
Very interesting concept, but loading everything through JS is rather slow and looks quirky. For example I go to http://demo.coretal.net/ and I see template first, then in about 5 seconds one of columns transforms itself into twitter widget, going through weird metamorphosis, while status bar informs me that page is loaded, hurray! Ordinary users do not want all this blinking and flickering, it freaks them out. For rich UI applications like gmail it is inevitable. (But somehow even gmail blinks-and-flickers a lot less that core-server apps, even though it is pretty slow.) But using this for mostly-static CMS-like sites is pathetic. So, very impressive from technical point of view, but it is an abomination in aspect of usability.
Thank you for your comments killerstorm. Actually, I am pretty have a similar idea with you. Do you think making html/body hidden during load would ease the pain?
Yep something like that might work
&gt;but then, ideally, you would allow those same chromosomes to be used later to be (potentially) resampled to make another baby (because Tournament Selection randomly samples from the entire population) Oh, okay. &gt;Don't be afraid to make your building blocks smart -- that's not cheating. I was actually wondering about that--nice to have it answered before I even ask it! &gt;I'm glad you incorporated sex. It's pretty simple to do, and now you can say that it includes all major properties of a GA. :D --- So I had the fair yesterday. Here's what happened, in case you're interested. In the morning I got interviewed by four judges: I had an okay conversation with one. He hadn't heard of GP, and I can't tell if it clicked with him and he was being deliberately difficult just to see how I'd handle it, or.....not. One was a graybeard Lisp hacker who was super-excited to see someone my age coding Lisp, liked my project, talked with me at length about it and GP &amp; Lisp in general. He asked if I knew what a retrovirus was. I knew the name but not much about it, so he gave me a synopsis and said I should look into it more if I wanted to improve my results. One was old, foreign, possibly a mathematician instead of a CSist, and was just not communicating effectively with me. He spent the a lot of time thinking out loud about how we've already discovered a provably-optimal tic-tac-toe algorithm and whether my AI followed. I tried to talk about how it was a heuristic (and thus aimed to be good *enough*), how I had a list of "Potential Improvements" on my posterboard, how the point of the project wasn't to program in the already-discovered optimal tic-tac-toe AI, how I could recycle 90% of my code and make an all-right-but-not-perfect chess player, or go player, or whatever...Then he went off on another tangent about whether my program could discover if there was a provably-optimal chess strategy. And was thinking out loud about what that would even mean (too much time talking about "Either white always wins...or black always wins...or they stalemate". I tried to talk to him about how it made *players* not *provers*. Etcetera. Sigh. The other contestants I talked to said he was historically very aggravating. And the fourth was a younger guy from Google who really likes FP himself, talked to me about his Emacs Lisp hacking, understood the project easily, asked excellent questions about it, and generally had a great conversation. In the afternoon, I was interviewed by representatives from various companies who had their own awards to give. Two guys from the IEEE had a very long, good conversation with me (and were the only ones who pointed to my laptop and said, "I want to see you run it right now"). I also had a good talk with four IBM reps and possibly a few other people. All of these guys were people who didn't know much about Lisp or GP, and yet paid attention and understood it after I gave a 60-second "elevator talk". I won an award from the Air Force which struck me as a PR gimmick (not that I'm complaining). And the IEEE gave me their grand prize (which was actually legitimate). I used the money to buy *Paradigms of Artificial Intelligence*. So thanks for shoving me in the right direction about improving my algorithm and also teaching me a bit more about GP in general --besides being personally instructive (and giving me better results), it gave me something interesting to talk about, a vocabulary to talk about it with, and probably made the difference between winning the IEEE's first place prize or an honorable mention (or nothing...). Unfortunately I didn't place in the actual competition. I refuse to be bitter about the judges--maybe I could have explained myself better, somehow. And the people who did had nothing special from a technical point of view, but helped the human condition or whatever. The girl who got first place took [a keyboard for the handicapped](http://en.wikipedia.org/wiki/Augmentative_and_alternative_communication) (which has to be operated with one button using [automatic group-item scanning](http://en.wikipedia.org/wiki/Switch_access_scanning)), and found a more efficient layout. The [algorithm she used](http://en.wikipedia.org/wiki/Simulated_annealing) was recommended by some professorial friend of hers and yanked directly from the internet. I would've been more impressed if she thought it up by herself (although she might not've thought of trying to break out of local maxima). But she gets to Help The Handicapped (tm), and is actually in discussions with the hardware manufacturer of such keyboards about selling her layout to them. And in addition to that very practical (and heart-tugging) application, it is true that she wrote a ridiculous amount of Java to simulate the keyboard and provide a nice GUI and so forth. Another winner had something similar going on with tracking eye movement (I think he used a toolkit that did the heavy lifting for the image processing) so that paralyzed people in the ER could use their eyes to say what happened to them. But it was a great experience, and I certainly learned a *lot* by doing this project. 
Start a blog and post about it there?
I'm not a native english speaker, so I don't really feel comfortable blogging. And there's still much to do in Slimv, so first I would like to finalize things, e.g. I want to add swank-clojure. But if someone wants to post about it, I'll be happy to assist. :)
You could take what you wrote here on reddit as a fine first blog post.
Wow. Good news. I don't know what a retrovirus is, either. Now *I* have to hit the books. The mathematician didn't get it. The whole idea behind GAs/GPs is to be used to search gigantic search spaces, most likely finding local maxima -- not to be used as some sort of theorem prover. (Even though there are AIs that are used for that.) Looking back, perhaps the simplest way to convey that point would have been trying to solve two problems instead of just the one. Most of the code is reusable. One of the major goals of writing an AI is to be able to fill it with various Functional Domains without having to rewrite the engine. Theoretically you only need to change two functions: the chromosome representation and the objective function. Yes, proper nomenclature is half the battle. That's why I came down kinda hard on you for your use of the word "gene." I'm sure you were able to talk knowledgeably and convincingly. The other hard lesson is that the good guys don't always win, or are even understood. That's life. You'll see the same pattern again. The best thing you can do to counteract it is to anticipate and prepare. Which makes me feel bad since I should have mentioned that using two functional domains would have been much more impressive (at least to the engineering judges). Instead I was harping on sloppy commenting when that is the last thing a judge cares about. I wasn't thinking. It's sad that nobody seems to have advanced the state of the art in the competition, but then you really can't say you have, either (yet). Writing a ridiculous amount of Java is de rigueur for any non-trivial project. She'll do well in an Indian code shop. Enjoy your new book. My copy is on my nightstand. There's a few chapters about Prolog in there that you should at least skim. You don't need to be a proficient Prolog programmer, but you should be familiar with it because it has a couple of ideas that will impact the way you think about programming languages. IEEE grand prize. Not too shabby.
About four hours into it, I was in mid-conversation with a judge when I was like, "So this same code could be recycled to make, for example, a checkers player--*darn, why didn't I **do** that?*--uh, anyway..." Honestly though, even if you'd told me to do so as soon as I posted this thread, I'm not sure I would've been able to throw it together. I was already very hard at work cleaning everything up, making a poster, writing a paper, etc. Oh well. I'm sure I'll be at some kind of fair presenting a program I wrote in my future life, so now I'm better prepared for it. &gt;I don't know what a retrovirus is, either. Now *I* have to hit the books. That makes me feel a bit better. &gt;It's sad that nobody seems to have advanced the state of the art in the competition It definitely is, but then again doing so is not easy at all. And we were on the engineering side, not the science side, so using existing ideas to make (somewhat) new products is to be expected. &gt;but then you really can't say you have, either Very true. I'm happy with advancing the state of *my* art (and I was making this project from the start--submitting it to the fair was an afterthought). &gt;(yet) Huh....we'll see, I guess. &gt;Writing a ridiculous amount of Java is de rigueur for any non-trivial project. She'll do well in an Indian code shop. \*snort\* :) I'm very happy that I've reached a point in my life where I get paid by the hour for any Java I have to write. &gt;Enjoy your new book. Sure will! I saw in the Amazon reviews that he implemented Prolog with macros. Presumably that's explained in the book, so I'll at least have a primer on Prolog. And if I don't get something there's always Google. I'm not opposed to becoming familiar with another language. &gt;IEEE grand prize. Not too shabby. 
You'd never guess you were a non-native english speaker. I wouldn't be too worried about it.
Thanks. OK, to tell the truth I'm planning to write some kind of introduction or tutorial in the future, that would show the main aspects of Slimv, but only after releasing the new version. This time I was hoping to get some feedback from experienced lispers, whether my approach is OK or not OK, the plugin is usable or not, are there any vital bugs or important functionalities missing, etc.
For those downvoting, in what way(s) do you feel newLISP is not an acceptable Lisp?
All of them. At this point, it is clear that the idiots pushing newLisp are simply too pig-headedly stupid to accept legitimate criticism, so why repeat the ample explanations that have been given before? Try to imagine some Rip van Winkle from 1982 asking what could possibly be so bad about GWBasic.
While I appreciate the sentiment (and agree to some extent), there doesn't seem to be a place we can point people to that discusses all of newLisp's shortcomings. We need one so that when some newb/zealot starts asking &gt; Why is it bad? we can point and say &gt; because all of *this* That would validate your argument. As it stands, the "pig-headedly stupid" "idiots" who keep asking may be genuinely uninformed newcomers, and probably don't deserve the namecalling. **Is anyone willing and able to formalize the arguments against newLISP in an online document so that we can all stop having this exact same argument every two weeks?** I'd be willing, but don't have anywhere near the expertise so I'm not able. If someone needs proofreaders or whatnot to help out with this, I hereby volunteer. * * * EDIT: I can still compile a set of reference links, I guess. - [Lambda the Ultimate](http://lambda-the-ultimate.org/node/257#comment-1901) - [c.l.s discussion](http://groups.google.com/group/comp.lang.scheme/browse_frm/thread/997dc59b8ebb1749/af62130eefafd338?pli=1) - [comments section of this (pretty poorly reasoned) article](http://eli.thegreenplace.net/2006/04/20/newlisp-an-intriguing%20-dialect-of-lisp/) Those are from 2k5/6 though, so it may have changed since then. Also, newLISP has a list of differences [here](http://www.newlisp.org/index.cgi?page=Differences_to_Other_LISPs). Significant differences that may be considered shortcomings/mistakes: - Deep copying arguments by default instead of passing by value *(which means side effects might be weirder, I guess?)* - Reference counting instead of garbage collection *(there's an attempt to re-brand this as "ORO memory management", [one article](http://www.artfulcode.net/articles/in-defense-newlisp/) going so far as to say "This should not be confused with one-bit reference counting", but I fail to see the difference)* - numbers are either `int`, `float`, `hex`, octal or "scientific notation" *(no binary or other bases; not sure how precision is)* - "ugly FFI which e.g. represents pointers as integers on the newLisp side (doesn't work when a pointer doesn't fit in C int)" *(which I assume means this is restricted to 32bit platforms? If so, that's enough for me to pass right there; I've been using a 64bit processor on my main machine for about two years now)* - "silly restrictions with the only justification being interpreter simplicity (e.g. length of quoted string constants is limited to 2048, strings cannot contain NULs)" - fully interpreted instead of doing incremental compilation *(which basically means that newlisp "macros" are just lazy functions, incurring all the run-time overhead of a function call)* - `(cons 'a 'b)` ==&gt; `(a b)` as opposed to `(A . B)`. `cdr` is called `rest`, and `(rest (cons 'a 'b))` still gives you `(b)` *(Which means that `cons` is just special-cased to work like `list` on atoms, and the underlying cons-cell structure is still there. You just can't access it)*. - all arguments are optional, ie. &gt; (define (foo a b c) (list a b c)) foo &gt; (foo 1) (1 nil nil) - No `car` or `cdr`, apparently. Newlisp just has the `nth` aliases (`first`, `second`, etc.), `last` and `rest` - dynamic scope *(personally, I don't mind it in Elisp, but this is one people seem to bitch about)* - no threading Did I miss/misinterpret anything? Have any of these been addressed in the latest release?
&gt; so why repeat the ample explanations that have been given before? Can you suggest any good ones? &gt; Try to imagine some Rip van Winkle from 1982 asking what could possibly be so bad about GWBasic. In what way was GWBasic not an acceptable BASIC? 
Please do write some sort of tutorial. It'd be really, really good if it were detailed, because the one thing that many Lisp projects lack is good documentation.
Unfortunately, this is hard to do when you use auto-indenting engine such as Emacs+Paredit...
&gt; (cdr (cons 'a 'b)) newLISP 'wisely' opted for leaving out the functions car and cdr.
...Ok, noted. Do they have the equivalent `rest` then, I guess? Or can you just not implement a `mapcar` analogue in newLISP?
&gt; Do they have the equivalent rest then, I guess? Yes, first/last/rest are present (last gives the last *element*), and a map behaving like mapcar. P.S. I never use newLISP, just have it installed (out of curiosity).
So `(rest (cons 'a 'b)) ==&gt; (b)` whereas `(last (cons 'a 'b)) ==&gt; b`. Noted.
Check the links I've posted above (they provide some good discussion, and while they are dated, the points they cover don't seem to have been addressed by newLISP yet). The big points against newLISP seem to be 1. It brings nothing significant to the table 2. It gratuitously changes things that work 3. The biggest win for its proponents is "batteries included", which is great for beginners but pretty insignificant once you get rolling (I'd recommend [Racket](http://racket-lang.org/) over newLISP for that in any case; the latter seems to come with a pair of AAs, the former provides a link to your own dedicated [Dyson](http://docs.racket-lang.org/) [Sphere](http://planet.racket-lang.org/) and still addresses all of the newb-oriented pain points).
I saw a pattern. (defun lots-of-mult (mult list) ; give me a good name (* mult (if (cdr list) (+ (car list) (lots-of-mult mult (cdr list))) (car list)))) (+ -1.26551223 (lots-of-mult 7 '(1.00002368 .37409196 .09678418 -.18628806 .27886807 -1.13520398 1.48851587 -.82215223 .17087277))) Is the real problem that there is a pattern to abstract away? Or is there not a way to do that such that the code is still understandable? To be frank, I had a hard time understanding the proposed indentation; I had to keep referring to the wide stuff.
Sure, you're perfectly right. If you need to fiddle with Lisp's (quite perfect) indentation rules, you're doing it wrong. Lisp's indentation is one of the key features I miss in *any* other language; it's one of the most helpful features to quite immediately grasp the (otherwise abstract) program logic.
This is a very interesting problem with practical applications. I spent some time prototyping stream implementations for SBCL. Always felt that there must a be more elegant and efficient way to combine processing steps. I never made any progress on it and/as it never occurred to me restrict the language.
Bah. (reduce (lambda (x y) `(+ ,x (* c ,y))) '(1 2 3 4 5) :from-end t) =&gt; (+ 1 (* c (+ 2 (* c (+ 3 (* c (+ 4 (* c 5)))))))) 
&gt; In what way was GWBasic not an acceptable BASIC? The point is that GWBasic was hardly an acceptable *programming environment* even when it was released, and is completely pathetic-looking today. Try to come up with a list of reasons why GWBasic is not a good way to program a computer: it is hard to know where to begin. And equally pointless: someone willing to defend GWBasic cannot be persuaded by reasoning, because anyone who knows anything about computer science would never make that decision in the first place.
&gt; The point is that GWBasic was hardly an acceptable programming environment even when it was released, and is completely pathetic-looking today. The comparison I sought was one Lisp to another; not newLISP to every other programming environment. That's why my question "In what way was GWBasic not an acceptable BASIC?" parallels "In what way(s) do you feel newLISP is not an acceptable Lisp?" Inaimathi delivered.
&gt; Inaimathi delivered. and he also said &gt; we can all stop having this exact same argument every two weeks because newLisp blockheads won't listen to the message. That is *my* point: anybody who finds newLisp appealing is an ignoramus, it is a waste of time explaining things point-by-point to ignorami.
&gt; it is a waste of time explaining things point-by-point to ignorami This is a false statement. The ignorant need to be informed (no need to spoon-feed them, links to discussion will do; I fully intend to link to this thread the next time someone asks about it again). How else could they stop being ignorant? An individual who absolutely refuses to see reason or demands to have their hand held deserves to have the bozo-bit flipped on them, but starting in with name calling at a first request for information is counter-productive. EDIT: Clarified away earlier edit
&gt; That is my point: anybody who finds newLisp appealing is an ignoramus, it is a waste of time explaining things point-by-point to ignorami. If you think I find newLISP appealing and am a member of the ignorami because I asked, "in what way(s) do you feel newLISP is not an acceptable Lisp?" then you may want to revisit your reading comprehension and critical thinking skills. And perhaps stop being a condescending prick.
I gave you my answer: "all of them." I.e., there is not a single characteristic of newLISP that contributes to computer science, programming, or anything but wasted computer cycles and blogs written by incompetents, and it is an offense to the name "Lisp" Your response: "no, please, I need more detail." What more detail do you need? It is entirely crap, developed by morons to attract other morons. And it is a waste of time detailing its crappiness. &gt; stop being a condescending prick. Stop worrying about my attitude and just stop wasting any more of your life thinking about this useless programming language. I'm trying to help you by making as clear as possible an association between "newLISP" and "stay far away from this crap."
The problem is that it invokes the term "Lisp", which is already punished and beaten by scores of college professors that turn "Lisp" into an exercise of mind-bending recursion problems without exposing anyone to the actual features of the language as it is meant to be used. The name "newLISP" implies that this language is somehow better than "oldLISP", which might reasonably then entice people to look at it and go "wow those Lisp people are even dumber and crazier than I imagined." As if "Lisp" as a market brand didn't have enough problems, this book comes over to "Real Lisp"'s sad husk and kicks it in what's left of its gonads, laughing and mocking at how terrible it must be to not be "new". It's intensely frustrating for those of us who know better and care. It takes an already dumb argument and makes it fourteen times dumber. Once one uses a real Lisp for a significant project, the issues become so blindingly clear that one's very sanity is challenged by some of the comments bandied about on the subject of why "newLISP" is regarded so poorly among the Smug. 
I do not have any of those keyboard. I *have* successfully used AutoHotKey to make the (to me) otherwise-useless keys on the numeric keypad into shift keys. (AHK calls them "[custom combinations](http://www.autohotkey.com/docs/Hotkeys.htm#Features)".) So NumpadDel + M means "switch to Outlook and start an email". The "space cadet" keyboard had Hyper, Super, Meta, Ctrl, and Shift (and possibly Greek) -- that's only six. I have *sixteen* shift keys at my disposal. Sort of. :) *Edit*: It occurs to me that the space cadet keyboard could use Hyper and Super (etc) at the same time; AHK is not so flexible. So they had 2^6 = 64 shift states, I only have sixteen. But I bet I can press mine more easily. :) And good luck pressing 4 different shift keys at once anyway. :p Oh, but wait; actually I have 2^3 * 16 = 128 shift states, so maybe I'm ahead after all. But then you can wish me luck reaching them all, too. :) *Edit 2*: actually it's 2^4 * 16 = 256 shift states -- forgot the Windows key. *Edited for formatting*
Are lispers still drooling over those old clunkers? Wake me up when it finishes consing.
These keyboards were expensive and well built. Today's keyboards are mass produced on a scale unimaginable then, and there are certain sacrifices.
Like price? Really i dont see the sacrifice on the keyboards. They work, they're rugged, and even if they break, they're cheap. This makes life better though: ;Swap [] with () (keyboard-translate ?\( ?\[) (keyboard-translate ?\[ ?\() (keyboard-translate ?\) ?\]) (keyboard-translate ?\] ?\))
Wouldn't that be 2^4! * 16? For each combination of Win key, Shift, Ctrl, Alt?
Not to mention that they make technical things like Greek letters and mathematical symbols easy to type. I'd much rather type greek-a than $\alpha$ or whatever.
Having used the Knight keyboard, the Logo-lab variant of the Knight keyboard, the CADR keyboards, the Symbolics 3600 and later keyboards, LMI lisp machine keyboards, the TI Explorer keyboard, the Xerox Alto keyboard, the Xerox Dandelion keyboard, the Xerox Dorado keyboard, the Xerox Star Keyboard, the Ann Arbor Ambassador, the Heathkit H19, the DEC VT52, the DEC VT100, the BBN Bitgraph, the Decwriter LA-36, the Commodore PET keyboard, the Radio Shack TRS-80 keyboard, the Commodore 64 keyboard, the TI 99/4 keyboard, the Apple ][ keyboard, the Apple /// keyboard, the Apple IIgs keyboard, the Teletype Model 19 keyboard, the Teletype Model 28 keyboard, the IBM 1401 keypunch keyboard, the Fujitsu Happy Hacking Keyboard Lite and II keyboards, the DataPoint terminal keyboard, the Osborne keyboard, the Smith-Corona, the Selectric, the Juki, a morse code copier Mill, the Simplex Portable Toy Typewriter (my first!) and a bunch of cheap PC keyboards, I can safely say that the Knight keyboards were my favorite. 
Can you recommend what I should get if I want a keyboard most similar to "Space Cadet" keyboard? (of course I can customize the keys later with AHK)
&gt; 2^4! * 16 No. What would that even mean? You have four independent shift keys. Each is on or off, and they can be used in combination. That's 2\*2\*2\*2 or 2^4. (Multiplied by 16 numpad keys which cannot be used in combination.) Not sure where you got 4!, much less 2^4! .
Why are the space-bars so long? When was the last time you missed the space-bar? When was the last time you cursed having to reach so far to the bottom corners of your keyboard to reach a modifier? But perhaps that's because modern keyboards have the modifiers backwards - surely they should be, from inside out, CTRL, META, HYPER, SUPER?
Why aren't there separate ( and ) keys? Holding shift for parentheses is annoying.
That goes for every keyboard ever. Would it really be a problem if space were a regular key?
Of all the keyboards in the article they're only shifted on the Knight keyboard. Or you could try [Erik Naggum's solution](http://groups.google.com/group/comp.lang.lisp/msg/f499cca6b478260f?hl=en).
I swap the brackets with parens. Even in C++, I still use parens more than brackets, so it makes no sense to have them unshifted.
That would be great. Until then, I'm using [this](http://www.vakoss.net/productdetail.aspx?id=147). I went to Fry's and looked at every keyboard until I found one with four keys on either side of the space bar. (But not a Fn key.) A little remapping, and I have four modifier keys on each side.
Yes, it would. I briefly considered the question at one point and quickly saw why: a long space bar allows your thumbs to follow the rest of your hand. Otherwise, either your thumb must stay in place, causing strain, or it must follow your hand, making your typing slower.
**THANK YOU, TEXAS INSTRUMENTS**
I have never, ever seen anyone list more than a handful of keyboards they have used. **Internet History**
What is the key immediately left of the space bar? What do you plan to use the four modifiers for? 
The wonderful Unicomp keyboards are produced in a variety of layouts. Perhaps they could add this one. It would be great!
I'd buy two if the price is around 200USD and if possible please use Topre switches.
&gt; We’re confident that by combining ITA’s expertise with Google’s technology we’ll be able to develop exciting new flight search tools for all our users. Um, so they won't be using Lisp? Or Google technology is for front-end only?
From what I've heard they are going to be required to keep some pretty strong firewalls up between ITA and Google technology. ITA is still required to sell their software to other companies, for example. I suspect the Lisp portions will continue to be in Lisp. 
Unfortunately this discussion appears not to include anyone who knows anything about keyboard manufacture. Furthermore, I don't see any discussion about what interface is to be used. The USB HID standard doesn't provide encoding for four control modifiers. What purpose do they intend for the special function keys like Local? Do people really want a keyboard without F1-F12, arrows, page up/down?
Just about the use of the original keyboard: http://deskthority.net/download/file.php?id=487&amp;sid=cc3c97810f25d0255a6f2608c930b9a1 SCROLL provides scrolling up and down. page up and down, too. Other than that moving the cursor was done with the commands most people may know from Emacs: c-f, c-b and so on. With the modifiers this works on things like words, paragraphs, screens, ... PAGE scrolls to the next page. SQUARE, CIRCLE, TRIANGLE are the equivalent of dedicated function keys. The HYPER key modifier is reserved for the user. That means that any combination of HYPER and some other key is a key reserved for the user. That gives you some kind of equivalent to function keys. LOCAL is used for controlling the local console. Brightness, sound volume, etc. LOCAL-b is 'brighter'. LOCAL-d is 'darker'. This still makes sense. 
I'm somewhat familiar with the native function of the Symbolics keyboard. For the record, I have a Symbolics Rev. C on my shelf. My concern was more the lack of description of how this replica would function in a modern context. Say I buy this thing and plug it into a Macbook Pro or Ubuntu desktop machine? What happens and how? Can I work better than with a standard 105-key keyboard or not? Why?
I have no idea. Though I might be willing to experiment with something that looks useful...
The space key doesn't necessarily need to be operated with the thumb though.
If you place it higher on the keyboard, yes.
On the left side it's an extra \| key. Fortunately it has a distinct scan code. From the space bar outwards in both directions: Control, Meta, Super, and Hyper. I also swap parens with brackets, and Caps Lock became Compose.
ignore all xah lee ranting.
Why?
Because decades of clueless usenet crossposts made us all killfile him. It's just that reddit (and hacker news) have not learned this yet.
&gt; ;x -&gt; (comment x) &gt; "x" -&gt; (string x) I don't think so, `" "`.
That's what I've heard. I just don't like to jump to conclusions I guess. 
 (string #\space) No, wait... (string (character (' space))) That's more like it. 
I'll take my `(list (quote string) (list (quote char) (list (quote int) (list) (list) (list) (list) (list) (list) (list) (list) (list) (list) (list))))`
He's, simply put, a n00b in LISP. Never got past the beginner stage, maybe because he got frustrated by its radical approach. Happens to many a programmer, and he's mostly selt-taught. I've been seeing stuff from Xah Lee since VERY long ago. Guy seems to have A TON of time and energy. I admire that. But he's still an amateur.
I very much dislike his constant rambling about Mathematica. Then again, my opinions about Mathematica are not very high (esp. the whole closed source mathematics aspect of it). 
I'm curious why this is being downvoted. I know the Lisp community isn't a huge fan of newLISP, but I don't see anything particularly harmful in the link. In fact, perhaps someone could learn a thing or two from it. 
If this is in response to the Arc Forum post by Pauan, note that he's talking about modifying `PyArc` such that these expansions happen, **not** implying that Common Lisp currently works that way.
I know, and it still would not work.
I didn't, but I probably should have in this case. To be fair, he seems to know what he's talking about in terms of Emacs/Elisp, but thinks that Elisp == all Lisp (he also seems to have a pretty inflated sense of confidence given his sometimes intentional lack of knowledge/experience). The way I see the article linked from the Arc forum: **Potentially Valid Points:** - improper lists are unnecessary **Untrue/true, but only for some subset of Lisps:** - it's difficult to indent Lisp code *(I...what?)* - the various quotes and associated constructs are confusing *(not because it's an invalid objection, but because his solution is basically "put in more parens")* - square brackets are used for vectors *(AFAIK, this is peculiar to Elisp. Arc uses them for anonymous function shortcuts and at least one CL library use them for a mini-language)* - hash notation is bad *(which I assume means he also dislikes reader macros)*
Having trouble commenting on the author's blog, so ... "Nice writeup Note that the CLRFI process has been replaced by http://cdr.eurolisp.org/"
The more I read about newLISP, the more it looks like `(Elisp + JavaScript)/2`
Wondering the same actually (I have not downvoted this one)
Individually, neither "code patterns" nor "newLISP" are cool terms in much of the lisp community. I'd guess the combination is causing a lot of knee-jerk downvotes. I didn't vote, but also didn't click the link. Thought the comments *might* have something interesting.
Disclaimer: I don't use and don't like newLISP. But I do understand that there are people out there not willing to master all of Scheme or Common Lisp: to them, newLISP can be a convenient match for their actual needs. Remember: on this our planet, we are quite a lot, and we are all different.
 (/ (+ Elisp JavaScript) 2) Sorry, couldn't resist.
Claiming that Scheme and Common Lisp are hard languages is just a newLISP marketing tactic. There's a bit more to them, particularly Common Lisp, but there's no need to master all of it to be productive.
I find [ACL2](http://www.cs.utexas.edu/users/moore/acl2/) fascinating. [More](http://en.wikipedia.org/wiki/ACL2). Does trying to formalise ones proofs in ACL2 help one to succeed in mathematics courses? I've not really put it to the test. I've not been able to get the hang of ACL2, but my health is so poor that I doubt that it was reasonable for me to try to learn it. Alternatively, does playing with ACL2 help one get into meta-mathematics and proof theory. My guess is yes, but my health hasn't let me test this, I'm only fit to read my maths books, not [fight them](http://abstrusegoose.com/353)
&gt;Can I take advantage of my programming skills to help myself learn mathematics more thoroughly and have more facility in performing calculations and writing proofs? Study [Maxima](http://maxima.sourceforge.net/)'s source code perhaps. 
There is [no royal road to mathematics](http://en.wikipedia.org/wiki/Royal_Road#Cultural_references_to_the_Royal_Road). IMHO you should unplug everything and break out the pencil and paper.
Have you ever looked at Maxima's source code? Remember that much of the code is practically ancient.
that's what I am doing :) in order to avoid the internet DURING pen-and-paper study, I write down a question in the margin of my notebook and then look up the answer later.
I would suggest that you study lambda calculus. Its the mathematical underpinnings of lisp and you should be able to lavage your lisp intuition. From lambda calculus you can then explore into proof theory (metamath) and category theory. 
Customize your emacs environment for producing LaTeX documents quickly. Maybe figure out a way to build a nice, customized, cross-referenced database of all the theory you cover. Your experience with asset-management and org-mode should help in creating great study notes.
For calculus I'd say that at most you can use Maxima to check your answers and provide hints. E.g. if you have no idea how to integrate some expression, integrate with Maxima and then try to get from point A to B. Of course these days you can actually use Wolfram Alpha for that and it'll give you all the intermediate steps (kids these days!... In my day you had to type that integral into your TI89 in the snow). For discrete mathematics, you can just program some of the algorithms you'll see (particularly graphs, matrices, finite automata, etc.). Of course you could use any language, but lisp lends itself better than many languages for that. Norvig's Paradigms of Artificial Intelligence Programming is of course a good reference for some stuff in that vein. 
You could make use of symbolic algebra packages.
The only real way to get better at solving mathematics problems and proving results is to spend time solving mathematics problems and proving results. That said, you could potentially get some use out of a computer in two ways: 1. Gain a conceptual boost by translating algorithms into a language you understand. Eg, create a function which takes in two matrices as arguments and multiplies them together. 2. Computationally generate concrete examples to help you learn the abstract theory. Eg, make plots of functions and see how they vary as you change the parameters.
[Axiom](http://axiom-developer.org/) [and](http://fricas.sourceforge.net/) [derivatives](http://www.open-axiom.org/) are also big, old CAS systems. However, Axiom's current lead is big on literate programming, and all the variants have good documentation. While digging through Axiom's printing library, I could tell it was based on another framework. I eventually figured out it was the one described in the following paper... Millen, J.K. "Charybdis: a LISP program to display mathematical expressions on typewriter-like devices" Mitre Corp Technical Report August 1 1967
Is there a way to actually listen to the interview. I am too dumb to find it.
There's a transcript from the 1989 interview [here](http://www.cbi.umn.edu/oh/pdf.phtml?id=192)[PDF]. I don't think there's audio of this.
A few weeks ago in [this](http://www.reddit.com/r/lisp/comments/ghlr2/slime_for_vim/) conversation thread I was asked to make a post about the Slimv plugin for Vim. So I started to write a series of tutorials, please welcome the first part. I have also released a new plugin version on vim.org, you can download it [here](http://www.vim.org/scripts/script.php?script_id=2531). 
Does it also support Scheme or do you plan for it?
No, currently scheme is not supported, only lisp and clojure. I did not plan for adding scheme support, but if many users are requesting it then I'll consider adding it as well.
I was thinking about my previous negative answer a little bit, and maybe there's an easy way to use the Slimv plugin with scheme (I did not test it, however): * download and install SLIME with scheme support * run the SWANK server manually or set g:slimv_swank_cmd to the command that starts the SWANK server * add an ftdetect/scheme.vim file with this content: au BufNewFile,BufRead *.scm setf lisp This may handle scheme files as lisp files, and if a SWANK server is running on port 4005 then Slimv should be able to connect to it.
This is some excellent work. I'd been looking for something like this, using a combination of nekthuth and limp. Eventually, I just started using Emacs for lisp hacking purposes. I'll check slimv out and maybe transition back to vim!
I hereby submit my request, for what it's worth.
That was a really well written article! And an interesting point to boot. 
I agree with the conclusion: lisp users are frequently anti-social, reinvent features constantly, and in the end create a mini language for each project. As a result it gets harder to share code or maintain. A classic example is paul graham's 'beating the averages' yahoo stores was eventually re-written in perl so they could hire people to maintain it. (Or even reddit moving from lisp to python) *as a completely unfair generalisation to many lisp users who are lovely, humble, social and friendly* But much of the article is a sort of thinly veiled masturbatory piece. Laconically extolling the efforts of one heroic lone hacker who wrote a programming language 'more powerful than haskelll', in the same way a knife without a handle is sharper than a knife with a handle. Except, he wrote it atop of an existing system (written by a team), and the language he compares it to is a state of the art optimising compiler. Something similar to Qi could easily be implemented atop haskell. At the same time as identifying the problem, the essay is absolutely dripping in the mindless arrogance he claims to reject, without a hint of irony. This isn't a side effect of the ability of the language but a deeply rooted cultural idiom of the 'lone hacker' within lisp, and many other languages go around with a false sense of importance and ego. Your language isn't making you act like dicks, you've just got dicks extolling the virtues of it, and attracting other dicks. Smalltalk users are also guilty of this too. Maybe there is something to be said about image based languages being the antithesis of cooperation. Stop venerating all the smug lisp weenies and maybe you'll stop encouraging people to act like such. Lisp is not the problem here, you are. 
tldr: 'our language is so powerful we have no option *but* to be smug assholes' is not a reasonable excuse for being such. grow up and stop celebrating dickishness. and you're not unique or magic either, other language communities have their fair share of pompous assholes too.
That is one way of looking at it. Certainly much of it seems to be true of Scheme. But to a very large extent, this is also true of the FOSS world in general. That is why after all these years, Linux is still only a decent replacement for Unix. But I don't see any substantial improvements in userspace. In fact, I can't think of anything I regularly use on Linux today that wouldn't have worked on Solaris 10 years ago. The main things a Linux desktop offers over Windows or OS X are relatively minor, because the differences between Gnome, KDE, MacOS, or Windows really are minor. When you borrow your entire design from the competition, and concentrate on themes and which side of the top of the window to put the close button on, you're not innovating. The root cause is that creating new *designs* collaboratively is far harder over mediums like email than programming is. This is as true for C as it is for Lisp. And C programmers go off and build one-off, incomplete, poorly thought out projects just as much. Sourceforge is littered with them.
At the risk of sounding like some sort of purist, emacs is written in lisp so it makes sense to me to use emacs for lisp. But really LispBox came with everything I ever needed for my Intro to AI class so I stuck with that.
Qi is fascinating. Too bad that Mark Tarver is such a fierce enemy of open source. I'm afraid that in its current form the project will not take off. I hope I will be proven wrong.
It's worth noting the fact that successful OSS projects and languages, like Ruby, Python, and Linux, have tended to benefit from the "benevolent dictator" model of development. User-visible *design* seems to require a *designer*, else it evolves into a morass of half-implemented half-necessary featuritis.
We lispers cannot have a share of pompous assholes. That would mean that some of us are not.
&gt; First of all, since Lisp has only one data structure—lists— Stopped reading right there (look at the date of the article)
It's a late 1st of April but it's still funny :-)
I know it may sound heretical but there are some lisp programmers who aren't assholes.
I started reading at "Lisp is inappropriate for teams" which was *almost* a valid argument, so I nearly didn't realise that it was satire, heh; Java is cool, but not *that* cool.
I kept laughing all the way through :D. Didn't realize it was meant to be funny until now, though, there's too many people ready to make similar arguments and mean it :(.
Take a look at Structure and Interpretation of Classical Mechanics by Sussman and Wisdom.
it's almost like conceptual integrity is paramount in a software product. (it is. and has been discussed many times, especially within 'the mythical man month')
&gt;First of all, since Lisp has only one data structure I had to stop reading there, but now that I've seen the comments perhaps I'll go back.
pics or it did not happen!
Way too subtle.
Previous entry on that same blog.
i scanned until my eyes hit "highly optimised xml databases "!!! 
Laugh. As the author of the wikibook linked to by the OP, I had half-hoped that there'd be some useful - if perhaps irksome - criticism of the book's style, presentation, and accuracy. Since the text was intended mainly for a non-technical or less-technical audience, I wasn't expecting too much useful input from a seasoned band of Lisp programmers, and then I wasn't surprised that few people actually read the document at all. Such is life, and I really don't mind at all if nobody here likes it. The 'LISP' in the name newLISP was not, I gather, intended as any kind of negative comment about other types of Lisp. The author, a modest and generous chap who lives in the US and develops newLISP mainly for his own personal use, was answering an interesting question: is it possible to develop a scripting language that occupies the same general space as Perl, AppleScript, Visual Basic, bash, or whatever, and which provides some of the strengths of the Lisp family of languages. (Actually it pre-dates AppleScript, but you know what I mean.) The 'new' was intended to describe new types of user, new types of application, new ways of approaching problem solving by non-programmers, rather than asserting any kind of superiority. To be fair to him, the interpretation of the word 'newLISP' as some kind of criticism of other, older Lisps is nowhere justified in anything he's said or written. (And he's not a native speaker of English, so the nuances may be different anyway.) Well, the answer to his question may be, for some people, "no, it's not possible" or "it's not worth trying". In my view, I think he did manage to incorporate some of the intriguing aspects of Lisp into his language, and you couldn't deny that there's some shared DNA: s-exprs, f-exprs, first-class functions, code-as-data, quoted lists, and so on. As well as the echoes and borrowings of earlier Lisps, there were many other compromises and trade-offs along the way. I'm sure many of you would have made different choices, had you attempted the same challenge. I have no problem with multiple solutions to problems. It's not like there has to be 'one true religion'. As with religion, just ignore the solutions you find distasteful. One of the primary aims of newLISP's author is that the language should be easy to use: easy to install, easy to learn, and easy to get some simple scripting tasks done. I would argue that it's easier than Common Lisp in all three areas. Perhaps the difficulties are overstated, but if you've ever taught anybody anything, you'll know that it's surprising how they can have difficulties with the things that you thought were really easy. The language offers only 350 or so functions, much fewer than CL, and aims to include some batteries for stuff like networking, OS-interaction, threading, regexen, and the like, so that the beginner doesn't have to worry about downloading, making and installing packages, libraries and dependencies. OK, this clearly isn't what many of you want or are used to, but that doesn't mean that nobody should attempt to provide it. For ease of use, newLISP prefers to use full names rather than abbreviations, and aims at a clean and elegant syntax with less punctuation. From your vantage point, you may hate the omission of this or that 'essential' part of the language, and deplore the use of technology X or Y. I think beginners prefer it like this, though. (Who, apart from the highest achievers, learns to drive in a Ferrari and worries about limited slip diffs while they're learning?) Another current preoccupation of newLISP's author is to keep the size of the language down. It may seem a bit odd in these days of terabyte drives and gigabytes of memory, but he wants it less than 250KB, so there's some determined minimalism at work all the time in the language's development. If a feature can be omitted, it probably will be. One benefit of this that I've found is that scripts do start up nice and quickly, which is nice for web applications and CGI scripts, and for scripts that respond to, say, key-presses. The description of newLISP is a "Lisp-like scripting language". It's not a Lisp-based programming language. We could discuss the differences between scripting and programming and never agree. In my view, it's like the difference between a bicycle and a car: both are useful, both have drawbacks, and both have their place in my garage. Having a choice is ideal. I think a language lives while it's being used. Some people use newLISP because they like the casual, 'have a go', 'don't worry about the technology' approach. I like newLISP because it's interesting, and because it's different. Also, noone else has written about it, so there was a gap in the market... (I like Common Lisp too, because of its richness. And Haskell's amazing, but harder work.) About 10 people have said that they found the linked wikibook enjoyable and instructive. But don't worry, reading it isn't compulsory, particularly when commenting on it. tl;dr It's just for fun. Relax. And sorry for writing so much.
I would recommend SBCL. My experience it that it's always faster than the rest. From time to time I use to compare SBCL, CCL, ECL and a few others in my programs and SBCL is always the winner. Although these comparisons are always very amateur, like, running a few runs and looking at the time output, so nothing scientific. 
I'll "ditto" jast's comments about SBCL, although I haven't tried OpenMCL.
What OS? For Linux I can say SBCL.
Linux is the primary OS it will be running on. But it would be nice for others if it runs on as many platforms as possible.
Try to make it as portable as possible across several lisps, because on some OS's different implementations might be preferred.
Unfortunately, I have only tinkered around with Clozure and use SBCL on a daily basis, so my knowledge is limited. I certainly cannot say CCL is 20% faster or slower on the "average CPU-limited code." As I understand it, though, Clozure is faster at compiling. This means that if you have code that generates (or reads) and then compiles code, think EVAL not macros, at run time, Clozure may have an edge. The CCL web page used to have some out dated benchmarks that hint at this. I think that it is true that SBCL has a slow compiler as it is optimized for producing fast code rather than code fast (this is certainly the feel you get from the internals wiki). They have since added an interpreter. This means that if you have code that is EVALed once and thrown away, interpreting is often faster than attempting any compilation. So, if you are looking for a big difference is performance I imagine you might see one if you have code that EVALs code, but runs that compiled result many times like in a tight loop, but not so many times that the compilation overhead isn't a factor at all. It is kind of a small window.
Typically it is easier to produce fast code in SBCL than with CCL. CCL needs much more manual work (declarations, special functions, ...). Of the other implementations the commercial Scieneer CL and LispWorks might be an alternative. LispWorks in the expensive 64bit version is quite fast.
#ccl, #lisp, #lispgames, ... Really *anywhere* outside of usenet.
SBCL is essentially useless on Windows. I don't know if that matters to you or not.
I know it was a couple of years ago. But is that still the case?
And as long as I follow the Common Lisp hyperspec I should be golden there right? The impression I have is that the Common Lisps should all follow the hyperspec.
That's true and should be easy as they are generally compatible. But I was more looking for a general idea for what the consensus was on the two.
It is if you want x86-64, or multiple threads.
...and if you need things that aren't in the spec, try to code to use libraries that provide a standard API across as many implementations as possible. For example, bordeaux-threads, trivial-* etc, 
Andyc is right: you might use functionality not defined by the standard, like threads, sockets, process control, GUI, etc. In that case using good portable libraries is the right way to go.
It's definitely worth trying out the experimental SBCLs for Windows with threads that have been released recently. I'm on my phone, perhaps someone else can link them? If nothing else you will be helping SBCL get better on Windows.
FWIW, I believe ITA uses both SBCL and CCL. SBCL is used for production, because it generates excellent code but the compiler is slow, while CCL is fast but lags behind SBCL a bit on performance of generated code. Of course this is all dependent on the application being developed: in some cases it may be a wash.
&gt; bicycle and a car: both are useful, both have drawbacks, and both have their place in my garage. Having a choice is ideal The problem is that the bicycle maker deliberately chose a name something like 'newFormulaOneRacer' for his bicycle, and equipped it with a transmission with most of the gears in reverse Unfortunately this book, while perhaps well-intentioned, propagates a brain damaged idea. NewLisp is 'lisp-like' only if your concept of Lisp is completely mis-informed. In my opinion, actively spreading stupidity is a hostile act. Please stop.
&gt; Remember: on this our planet, we are quite a lot, and we are all different. And most people are idiots. That is not an argument in favor of idiocy. 
I'm learning Lisp. I'd love to help out with penning an introduction or a tutorial. Send me a PM (private message) if you are interested. Also - my offer is free (no strings attached). Great work btw :) Also - I'm a VIM fanboy....I've been struggling with Emacs for some time now :|
Wow. I have this nagging feeling that you are an alien. Or have a post human brain. 
Where's NIL? D:
`nil` is of type [null](http://www.lispworks.com/documentation/HyperSpec/Body/t_null.htm). It's connected to list in the chart and is in the first column down below. 
Yes, but [NIL](http://www.lispworks.com/documentation/HyperSpec/Body/t_nil.htm) denotes the type which contains no elements, the bottom of the lattice.
ah, sorry, I thought you were asking about `nil` the object, which is of type null, not the type nil.
Oh, if that is the case, that must be the source of the confusion I read about.
well, it certainly got me: CL-USER 3 &gt; (typep '() 'null) T Yet another thing I learned from someone more knowledgeable of the CL spec than I...
great news, horrible link
I have been following the author's progress with that logging library, and it seems like he is reinventing ContextL, poorly. http://common-lisp.net/project/closer/contextl.html 
Not to spread bad news, but to give us a focus. KT keeps telling people to write cool apps...
You wasted ten seconds of my life. I want my time back.
Wow, that took quite a number of paragraphs to say next to nothing. 
Took a while to say basically nothing. TL;DR; Some programmers are more productive than others. Languages might affect that I guess. Probably. There's a conversation to be had about whether using different languages changes how you think or whether certain modes of thinking merely incline someone to learn different languages, but I'm not aware of any data on the subject and he certainly doesn't cite it.
I can't take any argument along these lines seriously unless and until the various tests that *demonstrate* shorter Lisp development times are addressed. I'll probably have trouble even then - I'm not in the top 1% of anything, and Lisp has been very good for me.
That was awful.
Mind pointing me to one or more of them? I've yet to read up on these.
[Here you go](http://www.flownet.com/gat/papers/lisp-java.pdf) [PDF]
[Here is another case](http://www.altocumulus.org/Fudgets/haskell-vs-ada-abstract.html) where they threw in Lisp just to get 6 languages to make Haskell look good. Lisp blows the prototyping curve (substantially) so they ding it in "Support", "Learnablity", "Maturity", and "High-Level Structuring". Poor little Haskell...
I get a DNS error trying to get the complete paper, but this: &gt;they ding it in ... "Maturity", vs. Haskell?!!11one! That's crazy!
Google "Haskell vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity". It's a fun little read. I can't help but feel smug when I read it. Maybe that's what these apple people feel...
Upvoted just because I want more lisp traffic...
Fine, but 1994 was a long time ago. We have Ruby, Python, Lua, etc. now. How many new ideas did we get in the lisp world in the decade after 1994?
Thanks for that. [Here's a link](http://webcache.googleusercontent.com/search?q=cache:R2Xnn_v-krUJ:www.cse.iitb.ac.in/~as/fpcourse/jfp.ps) for anyone else looking.
By new idea, did you mean to say "Taken from Lisp"?
It would be a lot more interesting if this guy had experience with LISP ( or if he does, talked about it ) rather than blather about theoreticals. Can you do more with LISP? I guess we'll never know.
Gee willikers. A noob takes time to express his initial impression of a community and how he evaluated it. The community responds by dismissing him. Do this enough times...
Is is maybe time for a macro for memoizing in that code?
User comments on that page are a lot better read than the article itself. Scroll down and have fun.
Partly, but in some areas, no. For example the lisp community's leeriness towards syntax means that all the good ideas in syntax in the past 15 year have come from elsewhere (as far as I know). 
Can I get an example? The best thing I can picture in terms of "good ideas in syntax" are things like object literals, matching expressions and implicit indexing (one of which can be done in a fairly simple reader macro and the other two have been implemented in Lisp, AFAIK)
To be fair, according to the paper, they were using a custom build called Relational Lisp. 
Fantastic article, you summed up how I felt when I read the original article. Unfortunatly I had no data to back up what I would have said, thanks.
Quicklisp and similar package managers need to support authenticating proxies. Otherwise you're going to be ignored by the growing numbers of users who live behind firewalls.
Several people have said something to that effect, but few people have actually contacted me asking about it. I'd like to support it, and I don't think it'd be too hard, but with such little apparent demand, it's not high on my priority list.
Thanks for Quicklisp, Zach! I'm surprised about how high-quality the screencast is. What tools did you use?
byzanz-record with some manual cropping and timing adjustments in GIMP.
It's a reasonable request. It's just not at the top of my list of things to add.
Do you have an authenticating proxy, or just a proxy? How does it authenticate?
Alright, I'll take my snarky comment back then.
Don't do that
This affects paste.lisp.org too.
And it's back!
Do you know what proxy your system uses? Drop me an email, it's on quicklisp.org, maybe I can help you get things going.
I love linedit. It is almost a clisp readline killer (I need it to, by default, support a few more readline features to enjoy it more). To use it, add this to .sbclrc after your quicklisp code: ;;; Check for --no-linedit command-line option. (if (member "--no-linedit" sb-ext:*posix-argv* :test 'equal) (setf sb-ext:*posix-argv* (remove "--no-linedit" sb-ext:*posix-argv* :test 'equal)) (when (interactive-stream-p *terminal-io*) (require :sb-aclrepl) (ql:quickload "linedit") (funcall (intern "INSTALL-REPL" :linedit) :wrap-current t))) 
This... I... should have sent... a poet.... How is this not just included in SBCL?
You do realize that ql:quickload is asdf:load-system plus some stuff to download the sources as needed, right? I don't think it is the proper form to use by default. Auto-download and run is fine when initiated by a human; but when it is automated, it can lead to system instability, remote exploitation, etc. I dunno. I'm might be in the minority here, but I really value tight control over what runs on my box.
Does ql:quickload force an update on the module? If so, what should I be using instead in order to load a module I already have?
AFAIK, `ql:quickload` just downloads the module if it isn't already present on your machine. You need to run a different command to update modules you have installed.
I'm guessing because most people just use Emacs+Slime anyway?
What is marginal indexing?
Quicklisp users have already expressed some trust in installing the software in the first place. Using it to actually load software is an extension of that trust. In the future, Quicklisp will offer, via SSL and PGP, more reassurance that the software you're downloading is the software I (or another library provider) intended to provide.
Quickload doesn't update anything. I use quickload to load stuff from my .sbclrc also. It would be nice to have a less verbose option, though.
The trust doesn't bother me as much as the conflation of concepts and subsequent spread of implementation-specific details. Quickload conflates download/install and REQUIRE. How this will this work when e.g. XCVB arrives? When Quicklisp 2 arrives? ISTM that we should standardize an extended REQUIRE that Quicklisp can hook into rather than encouraging people to hard-code quicklisp all over the place.
Those aren't issues that worry me very much.
This project looks pretty awesome, though the history of Dylan is littered with dead &amp; half-dead revival projects (Marlais, Idyl, Goo, &amp;c.). Reading the Dylan Reference Manual really gave me an appreciation for the language itself, and the [D-Exprs](http://people.csail.mit.edu/jrb/Projects/dexprs.pdf) (PDF Warning) paper was pretty interesting as well. I still don't see a revival for Dylan though; it never had a large body of users, the various minimal implementations are pretty different, and the niche it fills is pretty well served by CL (or some full-bodied Scheme implementations, such as RScheme), so Lisp-users are happy, and you probably won't entice non-Lispers with it. Still, awesome project.
If portability to Windows is of any importance, then Lispworks is preferable to SBCL. The people behind SBCL currently says that a port to Windows is in progress, but my impression is that it has been "in progress" for many years. Their emphasis is definitely on Linux and other Unixoids. Lispworks on the other hand has good support for Windows as well. On the other hand, If speed is most important, go for SBCL.
I think I will be aiming to make the code as portable between Common Lisp implementations as I can. That way I won't have to think about which compiler I use. Though, when developing I will probably be using SBCL. For my uses portability won't be an issue. If I get to point where I feel confidant on releasing the source maybe it will be for others. I think sticking to the Common Lisp standards will be best alternative in this case.
Just curious, what features are you missing? 
meta-backspace (delete previous word), ctrl-r (history management), meta-d (to actually delete a word as opposed to moving a word forward, which I already get with meta-f). I found where I could modify these bindings in the code, but it did not look like there was a concept of providing user definitions. It would be nice if I could override some of the termio settings, like using bright green instead of bold white on the matching parentheses. FWIW, I am not complaining. One can find the spots in the code to modify for this. I just worry about upgrades stomping on my configuration, so I just live with it.
Pardon my ignorance, but this raises questions for me. As a non-Lisp user that occasionally dabbles, there are other reasons why I don't use Lisp in a production environment. Momentum and ability to put new people on the project are big, generally intractable, problems (referring to the pool of programmers with expertise in *x*). Is there a web framework like Django or Rails with a similar level of robustness for Lisp? (I bet I know the answer: "you don't need one, because Lisp is so awesome you can just write it yourself.") Is there a good way to do mobile app development in Lisp? (I wouldn't be surprised if Clojure runs on Android, but probably not very well.) The reason people grab Python or Ruby or ObjC or Java is because the framework is there to *get stuff done*. Building out a new framework from scratch is a liability for most people. Basically, I don't sit around and think "what language do I want to program in?", I think "what is the natural platform to solve my problems with?" Sure, Lisp (the language) hasn't changed in 16 years, but the world changes. Part of being practical is staying on top of those changes (whether it's mobile platforms, new web standards, or whatever). I say all this as someone who hates all the syntax in ObjC and Ruby, but we live in a world of tradeoffs. Not trolling, I'm genuinely curious to hear what I'm wrong about.
Common Lisp (with the help of Quicklisp) has a decent assortment of libraries nowadays; however I basically think that you are right. I use CL for personal experimental projects where I don't need a framework and I don't care if the next Lisp programmer lives hundreds of kilometers afar. For web development, however, I usually work in Perl or (horror!) PHP, because there are well-mantained large frameworks (I use Catalyst), deployment is easy, and the client does not get nervous :-) I would certainly prefer to always code in Lisp, but pragmatism has its place.
"Is there a good way to do mobile app development in Lisp?" As you mentioned, Clojure does run on Android, but not very well at the moment. However, there's also Kawa Scheme, which works very well on Android. Basic "Hello World" tutorial: http://per.bothner.com/blog/2010/AndroidHelloScheme/ Part 1 of 3 of a more in-depth tutorial: http://androidscheme.blogspot.com/2010/10/introduction-to-android-app-development.html
This book review reads like someone who has never actually read Land of Lisp. Does that make it a book promo?
Your link is broken.
Huh, I could have sworn DEFCOMMAND was exported -- but then again, for exporting it to make sense more of the internals would need to be documented. Anyways, Linedit is a pretty slow moving target. Last "real" changes are over 5 years old, so I would not worry about things getting stomped overnight (of course the corollary is that a refactoring is probably overdue...) :) Anyways, it would be nice if you could share your additions. /Changes/ require more thought, but additions should mostly be easy to merge. 
Skip the review, and go for the music video linked at the end of the article. Conrad Barski is the Richard Gabriel of the new atomic age.
I have added the initial version of Scheme support to [Slimv 0.8.2](http://www.vim.org/scripts/script.php?script_id=2531). Currently it supports only MIT Scheme on Linux.
I have added the initial version of Scheme support to [Slimv 0.8.2](http://www.vim.org/scripts/script.php?script_id=2531). Currently it supports only MIT Scheme on Linux.
Works for me.
For the record, at work, we use Common Lisp entirely in a production environment for portable/embedded systems. The CEO of the company is a lisp fan, knows its power, and is happy to spend the little extra time looking for qualified lisp programmers. Also, with respect to "the world changes", a good reason lisp hasnt changed is because *it doesn't need to* (for the most part). More often than not, you can add functionality to the language without actually changing it.
Interesting. Do you know much about ABCL on Android?
Ah, sorry, don't have any experience there.
I definitely wasn't suggesting the language needs to change to keep up. I was referring to the ecosystem (frameworks, libraries, etc.).
Cool, I will try it. Thanks for the work.
&gt;Is there a good way to do mobile app development in Lisp? Any Scheme or Lisp that compiles to C should work on iOS with a little love. A few years ago someone posted about a Gambit-C based game on the iPhone: http://www.artisancoder.com/2009/10/scheme-hits-the-app-store/ . ECL works the same way. 
I suspect it would be as useful as learning Turing machines before moving on to imperative programming.
No, it's not necessary to know lambda calculus to learn lisp. Too many lisp tutorials obfuscate themselves into technicalities or mathematics that you really don't need early on. Lisp is easy, so long as you know about singly linked lists, which is one of the simplest data structures there is.
If anything, Lisp would help you learn Lambda Calculus.
Lisp isn't really that similar to lambda calculus, beyond having (lambda) and (apply) as part of the language. Learning lambda calculus would give you a very formal background on one language feature of lisp - if your goal is to learn Lisp, I'd say you are wasting your time.
And he adds: (time (length (make-list (expt 2 100000) 'x))) cpu time: 5659 real time: 5678 gc time: 1823" http://twitter.com/#!/lambda_calculus/status/64925746103205888 
The entire list is not actually instantiated, is it?
tsuyoshi: yes -- you can get or update any element of the list; no big deal. i can add an example if you'd like. the trick here is functional programming, sharing, and good data structure design. but there is no magic.
Read this instead: [Practical Common Lisp](http://www.gigamonkeys.com/book/)
Pls note that math functions are not variadic, but soon will be.
I'm reminded of the line from Tin Cup: Why would I want to do that? (Just kidding, I'm sure there are applications where a 2^100000 list comes in handy.)
I guess [this](http://www.eecs.usma.edu/webs/people/okasaki/pubs.html#fpca95) is the paper, why not put a link to it in the blog?
You are awesome!
Its easiest on reddit if you use the reply link. That way tsuyoshi will receive a notification that you replied to him.
 du -h vectext 235M vectext also, one of my big complaints about common lisp is that this shouldn't require additional tools :/ 
I don't know what else you might have in your image to get it to 235M. I'm on AMD64 and mine's "only" 49M, and gzexe gets it down to "only" 10M. Sorry I haven't improved the situation as much as you'd like, yet.
Fair enough... is there anyway to package the standard lisp runtime as a "shared library" of sorts (shared image?) ? It seems ridiculous to generate a 50MB binary for a simple tool.
I don't know. The solution I choose for now is to only make awesome tools.
Also, buildapp supports multiplexing many tools, small or not, into one binary. It can then dispatch on argv[0] (e.g. with symlinks) to choose which tool's entry function to call at startup.
Yes, that's the paper. I will add a link.
&gt; also, one of my big complaints about common lisp is that this shouldn't require additional tools :/ As compared to what? C++? g++, ar, ld, libtool, AutoConf, make, CMake, makedepend, yadda yadda. Java? Eclipse, javac, javah, ant, maven, ivy, yadda yadda. 
Maybe I'm missing something, but I don't think you can create an executable with just, say, sbcl
But you don't *require* additional tools. You can always load everything "by hand" and then save an executable image. This tool is just a facility.
save-lisp-and-die
→ http://www.sbcl.org/manual/#Saving-a-Core-Image
ah, ok. I did not know about the :executable keyword. I thought you had to load it into SBCL manually to run it.
Yes, I had some misconceptions about the process - I didn't know SBCL could generate executables.
&gt; Maybe I'm missing something[...] You are. Xach's buildapp is "just" a wrapper. % sbcl [deleted] * (defun hello () (format t "Hello World!~%")) HELLO * (sb-ext:save-lisp-and-die "hello" :toplevel #'hello :executable t) [undoing binding stack and other enclosing state... done] [saving current Lisp image into hello: writing 6352 bytes from the read-only space at 0x20000000 writing 4064 bytes from the static space at 0x20100000 writing 50200576 bytes from the dynamic space at 0x1000000000 done] % ./hello Hello World! 
AFAIK, all Common Lisp implementations can.
In Firefox (running on Ubuntu 11.04) you can press Esc to pause, and then F5 to continue the GIF animation.
ABCL can't quite do it, since the JVM doesn't enable images. 
Nice, thanks :)
Lets see some volunteers !
After that, I recommend: [Let Over Lambda](http://letoverlambda.com/) After reading PCL, my reaction was "ok, some of this stuff is pretty cool, but so what?". LOL has turned me into a true Lisp nerd who annoys people at work with "well, if we were using Lisp I could just write a macro for this..."
These "shared libraries" are generally called fasls, and are often loaded using REQUIRE or asdf:load-system. I have not seen a "shared image", but Xach's argv[0] dispatch sounds like what the few "shared" C programs do.