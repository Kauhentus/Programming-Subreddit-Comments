Emacs and Emacs like bindings are what I'm trying to avoid. I don't find them pleasant at all. 
&gt;Sure, it would be used more widely, but it would probably find a lot of shitty frameworks. People who don't understand the concepts but still have to use it would write shitty libraries that make things "easier", This. This threat is too real.
The real question is how to stop caring about reaching an irrelevant and likely moving target of "Common Lisp should be popular." If you aren't happy, whining about it in the umpteenth blog post to make this point without making any concrete contribution is not a way to help.
&gt;I think SBCL needs readline support But why not use it through Slime / slimv? 
&gt;the default ide is just unpleasant. Now that Corman CL is rebirthing, perhaps we'll get another mature IDE. 
Well, technically, most console ports can't actually be FOSS due to the NDAs you have to go through just to get to the information on the APIs and building things to make the stuff. One can still make it free and sort of open to others who have gone through the NDAs, but not to those outside of them. Unless of course you get all the information without going through the NDAs in which case it is all fair game (for example, if one knows from general word on the street so to speak that console Y is running a barely modified Z OS and supports OpenGL U.V and its controller interface is libusb or something; then you can get awfully far in console support by replicating most of those things without actually having a development console and going through the NDAs.
It's been a while since I've looked at it, but I recall the interface and hot keys being overly cumbersome, in part because it expects multilayer hot keys as something of a default. I also think there may have been an issue with auto complete, but I may be misremembering that one. 
As someone who fell in love with Lisps because of Clojure, the libraries need to catch up. There is no killer web framework, and most of the libraries I've tried to use have felt... insufficient. Not to mention many of them are 5-10 years old and dead. The environment needs a revival if CL is to take off the same way Clojure has. I want it to, but it's not there yet.
&gt; Emacs is awesome but it's not for everyone. Same can be said about Vim too.
I love Common Lisp, but I work as a Clojure developer now precisely because there are many available jobs for Clojure. I don't find the dev cycle in Clojure (nREPL) as smooth as Emacs + Slime, but hey, at least it's a Lisp, right? Now that I have been doing Clojure for some time, I find that 99% of the times, nobody actually uses Clojure's purported killer features any more than could be done in Common Lisp. The language survived and indeed thrived precisely because it was massively evangelised by Rich Hickey, managed to attract a few core people who started creating useful libraries, and attracted tons of people by selling it as a modern Lisp with Functional features that, wait a minute, also happens to run on the JVM. Brilliant marketing, along with a few good innovations (persistent data structures, for instance), and the language picked up. Most of all, the *community* is extremely approachable, and available everywhere - reddit, irc, slack, discord, gitter et al. No gatekeeping (well, aside from the "core" language itself), no ivory tower, and actually trying to reach the masses. I am particularly disturbed by some comments here which laud the fact that Common Lisp is not being used by the common masses, the rationale being given as avoiding the kind of problems that the node ecosystem is facing today. I find that extremely ridiculous. The fact of the matter is that every language has good, bad, and shitty code written by all sorts of people. Community moderation is what drives the best libraries to the fore. Is this not much better than having dozens upon dozens of Common Lisp libraries marked as "ABANDONED"? The basic fact is this - if you don't have users, the language is practically dead. My personal view is that not much is going to change (unfortunately) for Common Lisp, and the problem is definitely not a technical one.
I personally prefer using http://cvberry.com/downloads/cl-ansi-standard-draft-w-sidebar.pdf to Hyper-Spec. Yes, there were a few small changes between this last draft and the final version which is in the Hyper-Spec. But, it is a convenient PDF that I find rather readable.
&gt;I am particularly disturbed by some comments here which laud the fact that Common Lisp is not being used by the common masses, the rationale being given as avoiding the kind of problems that the node ecosystem is facing today. I find that extremely ridiculous. That was me. I had to rewrite two Node.js projects. I don't want to have to deal with the microfragmented NPM ecosystem again. 
That's cherry-picking though, isn't it? Would you call the Clojure system microfragmented too? Why look only at the negative aspects of popularising languages? Look at the net gains - more users, more libraries (self-moderated by the community as I mentioned before), more jobs leading to more support for the language ecosystem. A positive feedback loop in which one still gets to write the code one wants, but others can play on an even level as well, and community acceptance drives the survival or death of libraries.
&gt;Would you call the Clojure system microfragmented too? No. And note that Clojure is also language that is far from popular.
Setting up new projects is kinda a pain. To be fair though, it is in most languages. A lot of new ones though have a tool to set up a new project skeleton. Off the top of my head: - `cargo new foobar --bin` - `crystal init app foobar` - `npx create-react-app my-app` It would be nice to have a command line too to stub out asdf systems, tests, and a simple src file. It took me a long time to figure out how to get asdf working in conjunction with some quicklisp packages. Even then, I think I kinda have an odd setup.
I'm sorry to see your responses getting downvoted for no logical reason. I personally love Emacs, but I can see that it's not everyone's cup of tea. Hell, even a language like Idris (a one-man job, at least initially) has bindings for Emacs, Vim, VSCode, Atom et al. 
&gt; So, the only complaint left should be "I don't like parenthesis". That—and the unmentioned-in-40-comments elephant in the room, **types.** Threads like [this](/r/programming/comments/a1o5iz/maybe_not_rich_hickey/) make me doubt a new or revived dynamically typed language would get much love in the current year. 
Common Lisp is a strongly typed language with type declarations. 
&gt; So, the only complaint left should be "I don't like parenthesis". That—and the unmentioned-in-40-comments elephant in the room, **types.** Threads like [this](/r/programming/comments/a1o5iz/maybe_not_rich_hickey/) make me doubt a new or resurrected dynamically typed language would get much love in the current year.
Sure. However, relatively speaking, it is much much more popular than Common Lisp. Even in my own backyard, I now realise that there are at least a dozen companies using it actively. The community is also far larger, and increasing by the day. In fact, the company my colleague came from was initially a Common Lisp shop (very rare over here), and they moved on to Clojure as well. Still, my point is not to convince you (and vice-versa I'm sure). It's just that given a chance, I would have personally preferred to work in Common Lisp full-time myself, but just like with Rust, the jobs just aren't sadly there. It may sound almost banal, but ready availability of jobs does have a profound effect on growing a language community.
Sorry, moved the comment one level upthread where I was supposed to go while you replied. I disagree that Common Lisp is essentially anything but a dynamically typed language.
JavaScript, Python and Ruby would not agree with you about connecting types and popularity.
They're popular right now, not trying to get there. Read that thread I linked; I think the criticisms of Hickey and his take on types are representative for a majority of potential "converts".
Thanks, will give it a try.
This is a very good point, starting a new project should not require me to open a repl and set it up, it should be a one-command-and-done thing just like every other modern language.
Lisp can't be popular because lispers are uncommon, eccentric people: Edi Weitz: Listens to Frank Zappa and names things after things in the Zappa universe. Eval result? Eccentric. Rainer Joswig: Runs Lisp on a tiny embedded device as a web server, which hosts dozens of pictures of bikes. Owns more than one Lisp machine. Eval? `:eccentric` DrMeister: Says "C++ macros are to Lisp macros what IRS Tax forms are to poetry". Writes a full Lisp implementation in C++. Eccentric-p? `T` Conrad Barski, Md: `PROCLAIM`s Lisp came from the outer space. Creates an animated music video for Lisp where he warns **"Lispers are not normal people"**. Eval? `(values :eccentric :extreme)` Ron Garret? Changes his name to an anagram. Puts Common Lisp on a spaceship. Eval? Eccentric at runtime and at compile time too. Kenny Tilton? Calls himself "His Kennyness". Eccentric. Dzecniv? Wants to make Lisp popular. Eccentric. John Mercouris? Wants to write a web browser in Lisp. Eccentric. Me? I am a Zappa fan and think all the above are cool people, so, Eccentric. Lisp won't easily attract the normies.
Hello! I actually shared your frustrations with emacs for quite some time, I found it cumbersome, confusing, and unusable. I spent years preferring IntelliJ and Vim because they felt less... insane, I guess. The good news it, Emacs by itself (while great for some) is a mindfuck and a half. That's why distributions exist. I found Spacemacs 2-ish years ago, and it's basically a WAY MORE user-friendly emacs with vim-style keybinds and stuff. Now it's my IDE of choice. Try giving a beginner-friendly emacs dist a shot, it might change your opinion.
\&gt; Over in the functional language arena, we can see that Haskell is becoming quite popular. I think most of the popularity is due to people like dons that write useful libraries, and never stop marketing the langauge. Every day you see a few Haskell articles on Programming Reddit. This keeps it stuck in peoples' minds until they finally decide, "I am going to try Haskell." When they do, they see useful things like web frameworks, object databases, OpenGL libraries, and XML processing libraries. This means that they can actually do something useful "Right Now". So between the potential to be productive and hearing about it a lot, Haskell has gained a lot of popularity. \&gt; CL has many of the same libraries as Haskell and is almost as fast, but nobody talks about it, so it "feels dead". Indeed #lisp is much quieter than #haskell, but Lisp is still a very productive language with a lot of libraries. No other language has SLIME. But marketing is very important, and Haskell does it better than Lisp or OCaml (and competes for the same userbase). [https://softwareengineering.stackexchange.com/a/62693/203623](https://softwareengineering.stackexchange.com/a/62693/203623)
&gt; Sure. However, relatively speaking, it is much much more popular than Common Lisp. It is not my intention to start a flame war, but this is debatable. For example CL is well above Clojure on the TIOBE index for this month. There are many systems out there that are used for crucial stuff that are done in Common Lisp. And there is much more code and documentation out there for CL. Which is expected since it came much earlier. &gt;Even in my own backyard, I now realise that there are at least a dozen companies using it actively. What about CL? Roomba, Grammarly, Rigetti, Emotiq, not to mention NASA, Northrop, and most aircraft manufacturers(who use a CL-based software for aerodynamics sim.) One of the most popular Graph databases is built on Allegro CL. The most popular dataloader for the most popular open source database (pgloader for PostgreSQL) is written on CL. 
Write a great software or libraries is a good way to make it popular. Write internal tools in your company to make colleagues and your boss interested. Blog about it :)
I think this project ([https://github.com/cxxxr/cl-lsp](https://github.com/cxxxr/cl-lsp)) - Language Server Protocol - is a great way forward to get CL integration into the popular editors/IDE outside Emacs.
That is kinda arrogant statement. Google is (one of the) biggest List employers in the world out there for example. And Lisp programmers are mostly normal people - who want to solve interesting tasks using their favorite language and getting paid for it, no different from other programmers.
I haven't messed around with LSP at all. Is it a big step up compared to traditional methods? For example, with Python, I just install 'jedi', an Emacs plugin like Elpy, and for the most part I am good to go.
How would the first example differ from automatic getters / setters, constructors etc? How would the filter differ from other languages? I'd say not much They might be a tiny bit nicer, but I'd say that's not the killer feature you're looking for. 
&gt; Google is (one of the) biggest Lisp employers in the world out there for example. This interests me. Do you have any source? Article?
It all depends on what you are used to. In CL if I have Emacs/Slime setup, I fire up Slime and (say) SBCL, create an emtpy Lisp file and start typing code/compile it, and execute in Slime REPL. Later I can add asd file etc if I feel it is worth it. If I feel initially it will be a little (maybe one-file) project wrapped as a library, the [https://www.xach.com/lisp/quickproject/](https://www.xach.com/lisp/quickproject/) was here for quite a while. Personally I prefer fukamachi's infrastructure and his [https://github.com/fukamachi/cl-project](https://github.com/fukamachi/cl-project), with my skeleton tailored to my likings. However if you compare to say Python - there I just start a fresh .py file and write code there and only think later about structure etc. In C++ I start with a single cpp file which I compile with gcc from command line, or later add a CMakeLists/Makefile depending on a project, or download some of the numerous skeletons which suits the project size. So really I don't think CL is much worse than other languages in this sense. 
&gt; There is no killer web framework But what's a "killer web framework"? Node has `express` and Python has `Flask` as the most popular web frameworks. Yet several similar frameworks are already available for Common Lisp. The learning curve is almost nil if you've used any of those before.
https://en.wikipedia.org/wiki/ITA_Software
ITA Software is a travel industry software division of Google, formerly an independent company, in Cambridge, Massachusetts. The company was founded by Jeremy Wertheimer, a computer scientist from the MIT Artificial Intelligence Laboratory and Cooper Union, with his partner Richard Aiken in 1996. On July 1, 2010, ITA agreed to be acquired by Google. On April 8, 2011, the US Department of Justice approved the buyout. As part of the agreement, Google was required to license ITA software to other websites for five years.
You cannot disagree with facts.
&gt; Emacs and Emacs like bindings are what I'm trying to avoid. I don't find them pleasant at all. If you're on mac, Clozure CL has an IDE. 
https://langserver.org/ It looks to me as a unification of these separate integrations of different programming languages support in different editors. 
&gt; https://en.wikipedia.org/wiki/ITA_Software You're right, I forgot Google acquired it.
**ITA Software** ITA Software is a travel industry software division of Google, formerly an independent company, in Cambridge, Massachusetts. The company was founded by Jeremy Wertheimer, a computer scientist from the MIT Artificial Intelligence Laboratory and Cooper Union, with his partner Richard Aiken in 1996. On July 1, 2010, ITA agreed to be acquired by Google. On April 8, 2011, the US Department of Justice approved the buyout. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/lisp/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I'm sorry, but TIOBE is a meaningless index. Even taking that site, "Lisp" may be listed higher, but "Common Lisp" is mentioned on the same as being in the next 50 languages. So this usage either does not equate to Common Lisp, or is severely misrepresented. For what it's worth, Scheme is listed higher than Clojure, but that means nothing. You mention all those companies, but where are all the jobs then?Even a cursory search reveals tons of Clojure jobs, and the Clojure subreddit has more than the total number of users of /r/lisp, not to mention 3x that of /r/Common_Lisp. I have a suspicion that quite a lot of /r/lisp actually comes from /r/Clojure. Where are the people using Common Lisp actually hanging out, if not on /r/lisp or /r/Common_Lisp? I am genuinely curious, as I would like to know about these opportunities.
I would love to help on the UltraSpec, though unfortunately I'm really just a CL novice. Still, if there's anything I can do to help (or some official channel where people coordinate work), please let me know
I'll have to give those a look. Thank you.
&gt;I'm sorry, but TIOBE is a meaningless index. You presented your opinion. I presented an index done on some methodology, however questionable it might be. Neither of these opinions (yours or the TIOBE index) is too meaninful. I told you I didn't want to start a flame war. &gt; Scheme is listed higher than Clojure, but that means nothing in terms of actual industry usage. How can you be sure? &gt;Clojure subreddit has more than the total number of users of /r/lisp I question your usage of subreddit user count as an indicator of popularity. &gt;Where are the people using Common Lisp actually hanging out, if not on /r/lisp or /r/Common_Lisp? Some -i'd say many- of the most prolific lispers simply don't post here, or post once a quarter. See, neither you or myself have a reliable answer on saying which one has the most users. Clojure has Rich Hickey, who actively works to publicize himself, by talks and participating in all sorts of events. If somebody is popular here, is not Clojure neither CL; It's Hickey who is popular. 
Unfortunately, I'm on Windows.
Boy, have you got a big shock coming when you get to macros... The things you've pointed out are nice, sure - but the struct stuff is essentially getters and setters, and the list operations are common to almost every functional language. Once you get your head around just how powerful macros are you'll never go back. Enjoy! 
&gt;And both of them can be read out loud naturally, like “Make dog name rover, breed collie, age 5” and “Remove if not even from list”, “Every even from list”, etc. Exactly, and that's the idea of "language-oriented programming": flex the programming language so the problem is described in it in a way that reads the most natural to a human being. 
Thread pool - did you mean fiber pool? If there are multiple threads anyway (SMP) can there be multi fiber pools, affinity DNS fiber on network thread.
A community that is a lot more newby friendly #lisp and #sbcl can be quite toxic for those who are trying to solve real problems. There appears to be non CoC on most of the more popular projects. Stop referring to "Lisp" as just meaning CL. Don't push Emacs/SBCL as the only way to do CL. There are many great implementations with much easier to work with teams. There are many lisp languages with communities that are much more friendly. e.g. #chicken &amp;#x200B; At the request of [John McCarthy](http://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)), Lisp’s creator, no single language that is a member of the Lisp family is to be intended to be the definitive dialect; that is, none is to be called just “LISP.” 
&gt; *are common to almost every functional language.* 1. Cribbed it from Lisp, though. 2. With laughably fucked syntax.
&gt; For example, instead of the traditional practice of building a Python plugin for VSCode, a Python plugin for Sublime Text, a Python plugin for Vim, a Python plugin for Sourcegraph, and so on, for every language, LSP allows language communities to concentrate their efforts on a single, high performing language server that can provide code completion, hover tooltips, jump-to-definition, find-references, and more, while editor and client communities can concentrate on building a single, high performing, intuitive and idiomatic extension that can communicate with any language server to instantly provide deep language support. Ah, ok. That answered my question. Sounds awesome.
&gt; Unfortunately, I'm on Windows. Well, I'm on windows too but I use Emacs (actually Portacle). I find it more than pleasant, so I don't know what else to suggest. Still, there are alternatives: Dandelion (for Eclipse) https://github.com/Ragnaroek/dandelion Jabberwocky (runs on the JVM) http://jabberwocky.sourceforge.net/ If you try them, please PM with your feedback, because eventually I want to hack my own IDE. 
I get what you're saying -- libraries are there -- but I tried to use some of them (snooze, hunchentoot, etc) coming from Compojure, Spring, etc, and I found them wholly unsatisfactory compared to the tools and options the other frameworks I had used had available. I get that lisps are generally centered around smaller composable libraries, that's actually how it was with Clojure's Compojure, lots of little things working together to do the task that more heavy full-featured frameworks like Spring offer. The problem was, a huge portion of what I found to read about the lisp frameworks and etc was 5-10 years of age, none of what I tried seemed particularly full-featured, and the struggle to find good libraries I found nice to use to complement the web stuff was very real.
IMO if Roswell was easier to install (it's not difficult but on Linux without an executable it isn't trivial and it drags a lot of dependencies) this would be straightforward.
I would like to add that re-opening a project should be easy too and not require you to have your project in a certain location or to have it liked somewhere to get things going - I'm looking at you Quicklisp :| This whole project management thing is confusing and should be easier. PS: since some versions, cargo dosen't need the `--bin` any more.
&gt; but I tried to use some of them (snooze, hunchentoot, etc) Hunchentoot isn't a web framework, it is a web *server*. Snooze is specifically only for creating REST "API"s. We have many web frameworks, some of them are very recent: See Caveman2, Lucerne, Uncommon-web, Reblocks (rewrite of Weblocks), Ningle, Ninglex, etc. 
Poke me at phoe at disroot.org and we'll try to get something done.
&gt; Setting up new projects is kinda a pain. In which sense? You only need to write an .ASD file. We already have some skeleton generators, if you want such a thing. 
&gt; There appears to be no CoC on most of the more popular projects. High-level software has been written since 1958 without the need of any Code of Conduct and many people have been happy all through these times. &gt;#lisp and #sbcl can be quite toxic I have lurked there and found lots of friendliness. 
I suspect Clojure and CL are used in different areas, and CL being kind of a hidden, specialized gem. Rigetti, Mind AI ? They aren't your neighbourhood's web shop, and I'd say they don't need to publish a highly specialized job offer (Mind AI is hiring btw). Same for other specialized software. ps: https://github.com/azzamsa/awesome-lisp-companies (and https://common-lisp.net/lisp-companies)
Emacs and Lem have vim layers :) (http://wikemacs.org/wiki/Evil and M-x vi-mode) Atom is Atom.
I've used Eclipse in the past and liked it, so I'll definitely give that one a look. Thank you. 
No, I mean thread pool; as in all potentially blocking IO is run in a separate thread pool.
The Atom one sounds like it could be interesting, but if it's far from feature complete, then it may not be useful for me. At least not right now. Thanks for letting me know about it either way. 
Try updating asdf, on the asdf website it literally says this: &gt; friends don't let friends use asdf 2.
Thanks for the information.
I'd agree that CL for the web has to catch up, or show its guts. However, the killer web framework *is* to be Weblocks ! https://github.com/40ants/weblocks it's given a revival on this fork. It solves the javascript problem, and doesn't even rely on the npm ecosystem or on transpiling javascript in a separated client. As with Lisp tradition it takes a different path :] ps: what about a CL backend and a Vue/JS framework ? This should be ok (and it was done, one example to prove it: https://turtlapp.com/)
AutoLISP is a language strictly designed for extending AutoCAD. Theoretically you could try writing general programs in it, but, given that it is tied to AutoCAD, fully proprietary, and inextensible, I would find it very pointless.
Frankly, many of the replies have been wrong. It's actually very simple: "Batteries Included", and minimal elegant syntax to control those libraries. Why? Because people just want to get shit done. Superior "features" of languages are irrelevant. Python would not be in the top 5 languages (the only dynamically typed language that popular according to Tiobe) if people programmed in pure Python alone. The reason people choose it is because it already has EVERYTHING, including glue to other languages. It's literally one line of code at a command line to serve files over a web server "python3 -m http.server". If it's not already in the standard library, then "pip install" whatever you want, even large complex C/C++ frameworks and you can use them immediately \*instead\* of using C or C++. You have multiple choices of stable and mature GUI frameworks. Many people (including myself) have chosen Python simply for the sake of speeding up development using a C/C++ library by using the Python bindings (e.g. to eliminate the re-compile/re-run cycle, to avoid compiler/linker problems, to avoid low-level syntax and pointer arithmetic, etc.). So often people reach for Python before C/C++ to do the same thing. Compare this to Lisp. If you want to script C/C++, then Lisp should (almost) be your last choice. Here in the 21st century there is not a single GUI framework language binding that is stable, mature, and works over multiple Lisp implementations. At best there are a few alpha-quality ones for GTK or Qt if you're lucky enough to get them to work. People will disagree with me on this but the fact remains there is essentially no software written with these libraries beyond the complexity of a demo for a shopping list. And if you want to reduce pain of low-level syntax/semantics details, then Lisp is in many ways not higher-level than C. Many newer dynamic languages have uniform syntax for sequence and map types, whereas in Lisp you can't even use lists and arrays with the same set of functions. Compare using hash tables in Lisp versus the unified way to interact with Python dictionaries and dictionary-like objects. Modules and single-dispatch dot-notation OOP are easy ways to create namespaces in Python and other dynamic languages, whereas in Lisp you always have to create a package to create a namespace, and with over-complicated syntax and semantics by comparison. Lisp has a lot of historical cruft that puts it at a disadvantage right from the start. Python, Ruby, JavaScript et al became popular and usable because over time the barrier-to-entry to build real software got smaller and smaller as more things were handed on a silver platter. MIT specifically chose Python over the Scheme dialect of Lisp (which they formerly used since the 80's) when teaching new programmers because modern software is just as much about how to architect a system composed of multiple frameworks, components, and sub-components as it is about data structures and algorithms, and it's easier to get started when everything is ready out-of-the-box. In my not so humble opinion, the biggest thing that can be done for Lisp to make it popular is to get its bindings to \*major\* C/C++ frameworks stable, comprehensive, well-documented, and mature. If I could write GUI apps in Gtk and Qt today as easily and reliably as if it were included in the language from the beginning, then I would write a lot more Lisp code. If I could use C/C++ game frameworks using Lisp bindings rather than Python bindings, then I'd write a lot more Lisp code. If I could use Lisp bindings to C/C++ scientific/graphing/plotting libraries, then I'd write a lot more Lisp code. The problem is that right now, the only stable Lisp code I know I can rely on is the standard functionality from ANSI Common Lisp (and sometimes not even then due to behavior the spec says is allowed to be omitted or implementation-defined) and maybe a few pure-Lisp libraries like Alexandria. That's not enough to compete with just whipping up code in Python and calling it a day. Until such code gets written, people will forever ask the same old questions on forums why Lisp isn't popular.
Sure you can. That's the main usage of the internet.
You seem to miss the basic fact that most forums like #lisp or r/lisp or anything else are primarily defined by the people who participate there, and they generally discuss things with each other the way that they have come to consensus on. When #lisp says "we talk about Common Lisp here" that's just a statement about the participants. It's not some kind of gatekeeping or staking an exclusive claim on the abstract term "Lisp" contra McCarthy. It's simply stating that they *don't* tend to talk about Clojure, Scheme, Racket, Emacs Lisp, or whatever else might call itself a "Lisp." If you want help on those things, they will point you to another channel. Likewise, they don't "push Emacs/SBCL" so much as reflect the fact that the majority of active participants in the channel tend to use it, an in fact, often are people like the active maintainers of SBCL. If you are trying to use Vim or Eclipse, the participants will say "can't help you much with that." Because they can't, not because they are hostile to your choices. And even if they seem hostile, it's because you are taking up bandwidth with questions that put a high burden on them to figure out how Vim does anything and whether you are really having a problem with that or something else in your environment. Like if you post badly indented code, you'll get told to "come back and post it indented so I can read it without a hassle." You want free help, you get what you pay for. I find they aren't hostile to newbies so much as hostile to useless noise and people who can't pick up social cues or respond well to help. It is true that languages which are freshly invented tend to have much more of the evangelistic spirit. Being inviting for new users is a way to get new users, but it is not a way to keep users, and if new users don't turn into contributors improving the language or environment, what is the gain? People rejoice over actual improvements like u/xach producing QuickLisp. You want a welcome from Common Lispers? Complaining about them isn't the way.
Maybe not the best idea, but I install wlrap, and then alias sbcl as `wlrap sbcl` so I always have deadline support.
The thing that prevents Common Lisp being my main language now is lack of machine learning libraries. I have tried them all, and I appreciate the projects, but Python (I don’t particularly like Python) has TensorFlow, Keras, SpaCy, etc. I have a rather weak example of using DeepLearning4J in my `Loving Common Lisp` book, but then I use Armed Ber Common Lisp.
Keep in mind that ITA Software used Lisp long before the acquisition. They did amazing things with it, but that is likely because they are amazing engineers (and partly because they took on something that no one else in their industry thought was possible). The language itself is not magic, and I gather they spend a lot of time doing things like getting very involved with their Lisp implementations. Lisp has not taken off inside the rest of Google, but because it continues to be valuable to the ITA products it sticks around there.
I would change the name, Snigl doesn't sound very appetizing. If you want any sort of popular appeal that is. Yes name matters. 
just rediscovered the asynchronous Wookie: http://wookie.lyonbros.com/docs/ Excellent candidate for a killer web framework.
I have nothing to sell, and it's not meant for eating. But thank you for the input nonetheless.
upgrading is easy, you just clone ASDF somewhere (~/common-lisp/ IIRC): https://common-lisp.net/project/asdf/asdf.html#Upgrading-ASDF
No. Unless something has changed since I last used it years ago, you can only launch AutoLISP from AutoCAD.
&gt;Python would not be in the top 5 languages (the only dynamically typed language that popular according to Tiobe) if people programmed in pure Python alone. The reason people choose it is because it (...) &gt;Compare this to Lisp. (...) How funny. I came to Common Lisp directly after being well acquainted with Python. I was sick of Python's dismal performance, and zero support for any kind of decent concurrent programming. Found any cool library for software transactional memory in Python lately? Don't think so!! Hello GLOBAL_INTERPRETER_LOCK!! Are one-line-only lambdas in Python conductive to productive code? Don't think so!! OOP system in Python is a joke. List comprehensions are a flaccid version of Lisp's LOOP. And I am marveled at how easy is to interface C with Lisp. I'm glad I switched to Lisp. 
if you can write to files you can create a compiler to external executables from there the sky is your limit sounds like lots of work though
I guess I mean as a first timer. I could do it without too much trouble now that I've read all the docs for ASDF and UIOP, and gone through the Practical Common Lisp chapter on packages. It's not like I can go to common-lisp.com, copy/paste a few lines into my terminal, and be up and running in a few minutes. Or maybe I'm approaching things wrong by treating Lisp like other languages?
Go is just as guilty as Quicklisp of that. I handle both the same way: I have ~/dev/$PROJECT_NAME for all my code, and then symlinks where the tools want those dirs to go. (Except for when I forget, and I'm left scratching my head...) I agree with you, it would be nice.
&gt; be a politician and don't say that "Lisp is not popular". Just figure something else :p Indeed, on Tiobe whatever is Lisp (and it is certainly Common Lisp :p ) it is 32th, before Rust, Kotlin, Haskell F# Erlang Scheme Julia and Clojure !!! But they list "Common Lisp" in the next section *The Next 50 Programming Languages* 
You don't really need to create an ASDF project, or any kind of folder structure (unless you do web development). You can add all those later as your system grows.
&gt; Is there some kind of "standalone" Slime There's [Portacle](https://portacle.github.io/) but it doesn't let you use some other editor. Your best bet is to configure Emacs to be like whatever other editor you like.
You don't need to create an ASDF project or a folder structure. You can add all that later, as your system grows.
What if they distinguish .lisp and .cl extensions, or google queries with "lisp" vs "common lisp" ? Anyway it's unclear and difficult. In this benchmark (count of github stars): https://madnight.github.io/githut/#/stars/2018/3 CL is 37, under the 50, this time after Clojure Julia…, still before Crystal, F#, Nim or D. But ranking only with github is highly unbeneficial to CL IMO.
I like how these strange people struggling for Lisp's popularity suddenly downvote you just because you happen not to like Emacs. Its a sad state of CL community :(
thanks for that book !
Good find ! There were also CUSP, but it seems to be in a bad state: - https://github.com/budden/cusp - https://www.reddit.com/r/lisp/comments/1yq8wh/remark_on_plugin_for_eclipse_common_lisp_cusp/ - https://www.ibm.com/developerworks/library/os-eclipse-lispcusp/index.html
There have been another Eclipse plugin for a while: - https://github.com/budden/cusp - https://www.reddit.com/r/lisp/comments/1yq8wh/remark_on_plugin_for_eclipse_common_lisp_cusp/ - https://www.ibm.com/developerworks/library/os-eclipse-lispcusp/index.html
ehh, you're right that it isn't as consistent as lisp but I personally find Haskell more expressive for writing this kind of code (but then again if it wasn't then Haskell would be unusable lol) 
Fortunately, there is no Lisp community.
Q.E.D
I program Lisp too. You're 100% missing the point. I know all that already, so there's no point in advocating like I'm someone who needs to be enlightened about Lisp's superiority. My point is that absolutely NONE of that matters as to why Python is so popular and Lisp isn't (relatively speaking). Superiority has nothing to do with POPULARITY, which is what this thread is about. Even then, there are trade-offs about what features should be superior versus inferior. I agree that Python \*itself\* can be slow as hell and sucks at concurrency (compared to being spoiled by Erlang/Elixir). But since my point was that Python as a "glue" to C/C++ is one of the main drivers behind its popularity, it's a moot point if the heavy-lifting is done by C/C++ and Python glues better than Lisp to those languages. And you don't even have to write those bindings, since people have already done that for you. Lisp is not competitive in this area, by comparison. Also, yes the basic Python data types don't give as much control as Lisp. I don't like it either. But none of that MATTERS as to why so many people (i.e. popularity) choose Python over Lisp. Most people don't care about using cons cells to make circular or infinite lists if they can just use a Python generators, iterators, cycle(), etc. The point is NOT whether Lisp can do these things too or do them "better", the point is that the mental clock cycles it takes to do them in Lisp is higher due to less consistent interfaces/protocols and historical cruft. It's not necessarily even Lisp's fault in some areas. For example, to me OOP in general is just "side-effect oriented programming" and single-dispatch is the worst kind. At least with CLOS and multiple-dispatch, methods look just like other function calls, only with semantics about potentially custom types. But that paradigm is DIFFERENT that what people are used too, and so that's not a selling point to them. Them being wrong about which is "superior" is irrelevant. If they keep choosing Python, then it's more popular. I could go on and on, but what must be acknowledged is how quickly Lisp programmers knee-jerk at anything sounding to them like yet another hate post about Lisp, rather than considering that there is valid constructive comparative criticism why Lisp is not relatively popular. That's not helpful, nor is pseudo-pride about being different helpful because that has nothing to do with Lisp the programming language. The point of programming languages is to automate solutions to problems, and the higher the overhead to doing that, whether for marketing/social/psychological/economic reasons, the less popular that language is going to be. Lisp is disadvantaged due to historical cruft, but that is not a written-in-stone death sentence. If more time was spent writing code that makes Lisp easier to use (i.e. superior bindings to large numbers of C/C++ code) rather than being prideful about Lisp, then it would be way more popular than it is. As it stands, often no one even wants to hear criticism, even the constructive kind. So that status quo is preserved, and Lisp will remain less popular due to that.
The env on my Linux PC doesn't have the "-S" option. What does it do? It's not in the env man page on my PC. How will using env make the script more portable?
The Go team learned that the project-in-specific-location was a major pain point for its users and is currently transitioning to a module system (which will be mandatory with the next Go release). The new Go way, which I like a lot: go mod init [my/github/repo] # setup a new project in an existing folder go get github.com/joho/godoten # add a lib to go.mod
I recall poking at something on eclipse at one point, it might have been cusp, and having issues with setting it up. In your other reply you said it seemed to be in a bad state, so that might not have just been me being new.
Except for jobs, what are you really missing? You seem to be pretty content with the current situation. Why does it matter to you how popular the language is? I'm not trolling here, I think these are relevant questions that might help you move forward. Lisp is Lisp, it's clearly not for everyone and that's fine. Same is true of C and Forth. They're power tools, hacker tools; not religions like most of the rest. I'm working on my own [language](https://gitlab.com/sifoo/snigl) in the same spirit, I don't except that to become very popular either. I was coding Ruby long before Rails came along. Sure, it's more popular now. But something was lost along the way; they're now trying to please everyone, and many users are beginners which adds a lot to the noise level. Popularity isn't really as important or amazing as we've been conditioned to think. Good luck!
No, Autolisp is an embedded Lispand requires AutoCAD to run. Outside of that it's as legitimate as a Lisp implementation can get without being derived from Common Lisp or Scheme -- albeit it is a poor implementation. Autolisp waa derived from a very early version of XLISP, which could presumably run standalone.
yet another useless library
My boss would never let me ship anything that wasn't written in Python.
Seems like a nice improvement on ltk which hasn't been updated in a while.
Popularity comes from one of two sources, or both: * getting to people while they are young, preferably as children or teens. "You can't teach an old dog new tricks." Few people will switch to anything new, let alone unusual, past twenty. * bundling with a popular platform: C riding in with Unix, BASIC in the ROMs of 8-bit microcomputers, Javascript in the browser, ... If you want to popularize anything that isn't bundled with a popular platform, your only option is to get it into the hands of large numbers of children, then wait a few years. 
I think that adding fantastic Lisp support to VSCode would go a long way towards increasing its popularity.
Yeah it doesn't have -S in my Kubuntu linux either. &amp;#x200B; What using env does is solve the issue of an unknown path for the compiler or interpreter in the shebang line. That is, what comes after #! in the shebang must have a full path or a relative path, one cannot use just `sbcl` and have the shell or the kernel resolve the path from $PATH. &amp;#x200B; `env` searches for sbcl in the $PATH, so it is more portable in the sense that some people might have sbcl installed in `/usr/bin/` others in `/usr/local/bin/`, others in `/opt/sbcl-1.4.3/bin/` etc. &amp;#x200B; But it's funny that -S is not a portable option in env hehe :-) &amp;#x200B;
AutoLisp was derived from XLISP in the 1980's, but twenty years ago it was replaced by something called Vital Lisp, renamed to Visual Lisp. Visual Lisp seems to be a more substantial Lisp implementation, but not by much. Most of what it adds is prefixed by a quirky `vl-` prefix. For instance, it provides some CL-like functions that were missing in AutoLisp, but they get awkward names like `vl-remove-if` or `vl-symbol-name`. There is array manipulation, but through some interface somehow tied to ActiveX, which has to be activated by calling some global function `(vl-load-com)`, and is full of functions with `vla-` and `vlax-` names. Visual Lisp adds error handling, but it's quirky; hardly that much of a wortwhile increment over AutoLisp. An `apply`-like function is provided called `vl-catch-all-apply` which applies arguments to a function, and catches an error in that function. If the error is caught, it is converted to an error object that is returned. The scoping rules of this are not clear from the online documentation. One would hope that `vl-catch-all-apply` calls can nest. There is no evidence of any `unwind-protect`mechanism. On top of the quirkiness and paleness in comparison to a real Lisp implementation, the stuff has evidently not been maintained much in the past twenty years. The idea seems to be that it does the job with regard to supporting the AutoCAD application and requires no further improvement.
That's because Autodesk has wanted AutoCAD developers to move to VBA, or to C++ ActiveX extensions, since forever ago (the late 90s at least). Autolisp or whatever it's called now is all but deprecated, but shambles on in a zombie state because high-roller shops have workflows that depend on it somehow.
We have people doing that for over 20 years and yet CL is still not getting more popular. Sure, it's nice to "just code/just do lisp" and you can argue that it helps the ecosystem a bit; however, it is not effective and I think other strategy is needed.
The problem is, there are many languages like that and people don't care. There are many nice things in C#, Python, Ruby. Damn, CL has some nice projects under its belt. The languages didn't get visibility from this one project, and it wouldn't be enough to keep people. It surely helps, but I feel like there is more to it than just making a popular thing. Not to mention that one does not simply write a popular app.
I don't know of too many mainstream projects using common lisp. 
Arthur has been doing a great job over the last years. It is not like I have always agreed with him in discussions, but he is always consistent and has high demands for the srfi process in a way that has benefited the srfis a lot.
Thank you, that is very kind. &amp;#x200B; By the way, I'm still hoping that you will contribute your [Transducers for Scheme](https://bitbucket.org/bjoli/scheme-transducers/) in the form of an SRFI. &amp;#x200B; For anyone interested in watching the Growing Schemes talk I gave at the 2018 Scheme Workshop, here's the [YouTube video](https://www.youtube.com/watch?v=YuqZKHwUygU), and here are the [slides](https://speechcode.com/growing-schemes) (HTML).
This is why Quicklisp should automatically upgrade ASDF instead of leaving it up to implementations.
Great writeup on advantages of Lisp, mostly Common Lisp. I appreciated the mention of Julia as being a Lisp at heart. Julia is convenient for a side project I am working on but I have been trying to combine a Lisp language with DL librairies written in other languages. Even though I don’t love the syntax of Julia, hearing it described as being Lisp-like keeps me considering Julia.
Hit back button after: &gt; *Scheme [35] is an elegant and compact version of Common Lisp that supports a minimalistic core language and an excellent suite of language extension tools. * WTF
OK, that was a bit embarrassing. But one of the co-authors, Edmund Weitz, is world-class lisper, so I have to think this is just an absent-minded mistake.
I assume that by Lisp you mean Common Lisp. VSCode doesn't yet have a Lisp formatter or even a plugin for interactive Lisp sessions. For code indenting, a combination of emacs and swank are used; for interactivity, there's slime and sly (which are emacs plugins) and slimv and vlime (which are vim plugins). This means that the facto editors for working with Lisp are emacs and vim. You could see https://lispcookbook.github.io/cl-cookbook/editor-support.html for other alternatives.
Very recently someone posted a link to https://github.com/ds26gte/scmindent in one of the comments here. Should be a generic indentator that you could adapt into a VSCode plugin, as you already have a Javascript version provided.
If you substitute "Common Lisp" with "Lisp" it's all correct. (Unless you don't count Scheme as a lisp).
I am working on it, albeit slowly. I am trying to implement a reasonable subset of srfi-1 as transducers, and also taking the best parts of clojure (some of which are unecessary, or achievable with composition). Then there is the question of naming. I could just make it a variation of srfi 1 where I add an arity to the regular procedure: (map add1) returns a transducer, but this seems not so schemey. I am still leaning towards tmap/tfilter/... Then there is the problem of writing a good enough srfi :) Jam not a native English speaker nor a programmer (just a lowly Swedish classical musician). I am working on it and I will try to at least have a pre-draft for feedback on the guile mailing list out. Then I will port the implementation to Chibi and make a r7rs reference implementation
Haha! That's a nice one. Yeah, that was the problem, after the update it works.
Thanks! It worked :)
Yeah, it would be nice if you could update it when you update all distros.
Maybe he was mostly a long time common lisper that saw scheme from a distance late in his career ?
It could be. Or another idea: if he's been programming Common Lisp for years and regularly talks with fellow Common Lisp programmers, he's probably in the habit of referring to Common Lisp as just Lisp. Maybe after writing the first draft he realized he should change "Lisp" to "Common Lisp" in many places, and since he was going through it fast he missed that the reference to Lisp mentioned here should stay as just Lisp.
Plausible. You read his book ?
Cool ! Most issues I had until now were straighforwards, so keep asking anyway :)
I'm interested in this as a tool for guided screencast Clojure teaching
Author of Klipse Repl here: Feel free to contact me via email
not sure why you say visual studio code doesn't have those things. there is paredit and parinfer and other lisp/scheme specific formatters. and code has an integrated terminal for repl stuff. maybe that isn't full on common lisp interactivity, but it's there.
[Maxima can be compiled with SBCL, Clisp, CCL, GCL, CMUCL, ECL, Scieneer Common Lisp (SCL) and Allegro Common Lisp (ACL).](http://maxima.sourceforge.net/lisp.html) If the `lisp-chat` project also uses any of the above, I can't imagine this to be a monstrously difficult integration task.
I've been thinking that Clojure would be a good language to write bioinformatics pipelines in, since it's strong in concurrency &amp; metaprogramming, and has good library support (e.g. useful if you need to use AWS API). &amp;#x200B; There are a lot of pipeline tools out there for bioinformatics/data science pipelines, such as Snakemake, Nextflow, Luigi, Common Workflow Language, even raw Makefiles, and many more ([https://github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline)). Some are packages for programming languages, some are their own dedicated DSLs. But for some reason none of them feel very pleasant, I think a lisp would combine the best of both worlds.
Grammarly uses Common Lisp. Reddit was built with Common Lisp (they switched to Python, unfortunately). Spike: Hubble Telescope Scheduling System has been built with CL. Maxima is built with Common Lisp. There are surely some others, too. 
Autodesk might've been more successful if they wanted to replace AutoLisp with something decent, as opposed to something that sounds palatable to middle-management. 
You have some absolute paths in `c/lu.c` and `c/ml.c`. It's often helpful to symlink things like that. That libuv-based mini-webserver is interesting, but I like such things. 
Nitpicking: this is not asynchronous I/O. This is threaded I/O. You are doing I/O by calling to the system to do some work for you, right? If *that* call returns immediately, then you are doing it asynchronously, if you spawn a thread to deal with system blocking on that call, that's threaded I/O. Spoiler: asynchronous file / block-device I/O is mostly impossible on Linux (at least not with the drivers you get with the stock kernel). You can have more luck with network I/O, but there you rather don't want it altogether, since you'd be swarmed by interrupts, so, you'd probably choose polling over async. I don't know what's going on in MS-land, but I couldn't care really. No sane person would want to develop storage solutions on top of MS API.
From the point of view of code running in Snigl, it's very much asynchronous; the fact that the code actually runs in a separate thread behind the scenes is an implementation detail. I've done my time in the poll, epoll and kqueue trenches; and I am fully aware of the compromise being made here. This time I was aiming for portable, universal and fast enough; and this comes pretty close.
In case you haven't seen it, here's a video of Ron Garret's talk at Google on the subject in 2012: [https://www.youtube.com/watch?v=\_gZK0tW8EhQ](https://www.youtube.com/watch?v=_gZK0tW8EhQ) The slides are here, on his web site: [http://www.flownet.com/ron/RAX2.pdf](http://www.flownet.com/ron/RAX2.pdf)
I was the lead on the Remote Agent Executive, and the principal advocate for getting Lisp to fly. AMA.
there's a paper about the methods used to bring about the DART program floating around out there, as well as a postmortem on the bug referred to in the story about remote agent. Just do some hardcore googling and you'll find them
Thank you! I don't know how to start! But I will try. I am a data scientist. I built machine learning systems. But I have the feeling, that ML models can only answer very "operationally" question: "is it a Fraud?" "Is the client buying this?" "Is this patient coming back in the following 10 days?" etc. Even DL models are in this bucket. But for doing "tactical" or "strategic" decisions ML (currently) feels very static (I am so sorry for using a lot of ill defined concepts). When I read about this planners it seems like the missing piece. But, there are so few (none) real examples. I want to learn how to frame and built this kind of projects and try to mix them with ML (or maybe probabilistic programming). Does this make sense? If not I could try to explain myself again 
I think already have them. But they are very high level. I am looking for something more specific in order to replicate / approximate it
How did you convince people to use lisp? Which of its strengths were most valuable? Did you miss anything or regret choosing lisp? Would you do something differently if you were to go back in time? Wasn't lisp image size an issue or how did you make it smaller? 
My apologies the question I'm about to ask isn't about RAX directly, but curiousity eats me alive. Do you happen what is it Ron \[mentioned\]([https://youtu.be/\_gZK0tW8EhQ?t=4175](https://youtu.be/_gZK0tW8EhQ?t=4175)) some episode happened at Google probably involving decision making about Lisp
I'll answer those in reverse order because the later questions are easier to answer. &gt; Wasn't lisp image size an issue or how did you make it smaller? No, that was never an issue, at least not a technical one. To be sure, there were people who thought it was an issue, but they were simply ignorant. They thought that Lisp was bigger than it really was, and they thought computer memories on spacecraft would somehow defy Moore's law and stay small forever. So it was a political issue, but never a technical one. &gt; Would you do something differently if you were to go back in time? Technically, no, at least not with respect to choosing Lisp [1]. Politically, yes. I was too steeped in the technical details to notice that we were making some pretty significant enemies among some fairly powerful people. Basically, I wish I had learned this lesson sooner: http://russolsen.com/articles/2012/08/09/the-best-programming-advice-i-ever-got.html &gt; Did you miss anything or regret choosing lisp? See above. But no, I don't regret choosing it. We could not have done it any other way. &gt; Which of its strengths were most valuable? Hoo boy, where to begin? Remember, this was back in the day before there were really any viable alternatives. Python, Perl, and C++ were brand new. The state of the art for coding spacecraft was Ada and C, occasionally Forth. So we had a REPL, dynamic memory management, dynamic typing, macros, native thread support... all of which no one else had and all of which were crucial to doing what we did. &gt; How did you convince people to use lisp? The process of convincing people to *try* Lisp was very different from the process of convincing them to stick with it. Getting them to try it was a combination of dogged persistence (see above about making enemies), Just Doing It (i.e. using it myself whenever I could, hiring Lispers whenever I could), and constantly debunking myths about it. Getting them to stick with it kind of happened organically when the effort to port the code to C++ failed, leaving the powers-that-be no choice but to fly Lisp or scrub the whole experiment. We also had a patron, a highly respected senior engineer who went to bat for us. 
Well, I am Ron, so yes, I know :-) I'm referring to this: http://www.flownet.com/gat/jpl-lisp.html the section from 2000-2001. 
I see. Thanks for all the answers! 
You bet.
Oh! I apologize again. Thank you very much for that educational _*and*_ inspiring talk, and for the link :)
Could RAX software be adapted to handle non-spacecraft hardware? Or was it highly specialized in that regard? If it was fairly high-level, could it be used for semi- or fully autonomous machines for Earth-bound scientific explorations?
Well, it's just when you talk to people who do storage, the "async" has a very well defined meaning. But people who only use storage (as opposed to developing it) have more foggy idea about what it means to be asynchronous. I mean, stretching the definition, you could say that as long as there exists a machine which is not blocked on a given I/O, this I/O is *in some sense* asynchronous :) What I'm trying to say is that I/O from a perspective of someone who sees it from the other side of the user-space - kernel-space divide falls into one of these three bins (with some added modalities): 1. Asynchronous (or even POSIX-asynchronous). 2. Polling. 3. Threading. The modalities could be: "direct" or "write-back", "zero-copy" or "user-space" or "kernel-space", and so on... it'd be too straining to list all possible permutations. But, marketing likes to piggiback on terminology it doesn't really understand. So, for example Node.js was trumping about how it does asynchronous I/O, while in reality... it doesn't :) Simply because OS doesn't allow it and they don't bring their own drivers or anything like that. What they do is the polling I/O. Kernels of many modern operating systems implement polling I/O in libraries like `epoll`, `kqueue` etc. Python's asynchronous I/O is even more lame, since it's all implemented in Python itself... Go is like Python in this respect, for example, it doesn't use any system's features to do asynchronous I/O because it relies on its own scheduling. So, what's the difference and why would anyone care? Well, *asynchronous* refers to a particular way I/O happens: to simplify it, we'll only consider a typical user-space request for I/O sent to kernel, processed there and returned back to user-space. A thread in user-space sends an interrupt to kernel, which is handled immediately, and control is immediately returned to user-space. After the I/O completes the user-space callback is called, perhaps in the same thread, while the original thread's code execution is suspended. This doesn't spend any time in mutexes waiting and asking for completion of I/O. Interrupt goes one way, then the other way, all handled by devices, basically. No CPU waste, no battery waste. Everyone is happy... *Polling*, in the same setting is when the user code sends an interrupt to the kernel, but instead of being awoken by the kernel it must poll it repeatedly for results. This is easier on the kernel side... but wastes CPU cycles. *Threaded* is even worse in the sense that it will entirely rely on thread scheduler, which has absolutely no awareness of I/O interrupts and may just decide to stick your thread in the place where threads will never wake up... but, in the best case, will burn a lot of CPU cycles, allocate tons of unused memory, while the system will be all blocked on very little actual I/O happening. ---- But, the sad part is that while *asynchronous* I/O is the most efficient and the most desirable one... it doesn't really exist anywhere outside of completely custom-made high-performance systems. That's how it got this aura of desirability, I guess :)
Thank you! I don't know how to start! But I will try. I am a data scientist. I built machine learning systems. But I have the feeling, that ML models can only answer very "operationally" question: "is it a Fraud?" "Is the client buying this?" "Is this patient coming back in the following 10 days?" etc. Even DL models are in this bucket. But for doing "tactical" or "strategic" decisions ML (currently) feels very static (I am so sorry for using a lot of ill defined concepts). When I read about this planners it seems like the missing piece. But, there are so few (none) real examples. I want to learn how to frame and built this kind of projects and try to mix them with ML (or maybe probabilistic programming). Does this make sense? If not I could try to explain myself again
&gt; I was the lead on the Remote Agent Executive, and the principal advocate for getting Lisp to fly. AMA. Hi `(or '(R G) '(E G))`. I just wanted to say that I've read your page "Lisping at the JPL" many times the last 18 months or so. Your page, the first time I read it, was one of the motivators I found to try Lisp. 1.5 years later, here I am, a religious, smug lisp weeny^TM . I also work for a very big (200,000+ people around the world) organization and sooner or later i'll have to substantiate the choice of CL over **&lt;language&gt;** , and the JPL story is a powerful ace on the sleeve. What great times must have been those at the JPL. 
&gt; I am, a fully converted smug lisp weenyTM . Happy to hear it! :-) &gt; What great times must have been those at the JPL. It had its moments. &gt; special Lisp implementation It wasn't all that special. We used three different Lisp implementations in development (CLisp, LispWorks, and a transitional version of what eventually became Clozure CL), but what eventually flew was an essentially off-the-shelf version of LispWorks with a real-time garbage collector. 
&gt; Could RAX software be adapted to handle non-spacecraft hardware? Possibly, but the parts that were not specific to that mission are now almost certainly pretty badly out of date. RAX was, if you'll pardon the pun, not rocket science. The actual AI tech behind it was fairly mundane, especially by modern standards. The achievement was (IMHO) showing that this mundane and well proven technology could be successfully adapted to control a spacecraft. 
RAX had nothing to do with machine learning. All of its "knowledge" was coded by hand.
No worries :-)
I could imagine that RAX is not ML. But the AI technology used there is (I think) complementary to ML (e.g. Planner). What are your thoughs about it? 
You are one of my heroes BTW. As @defunkydrummer said. I read your post "Lisping at the JPL" several times and motivate me to try to learn Lisp / Clojure.
Thanks!
&gt; I could go on and on, but what must be acknowledged is how quickly Lisp programmers knee-jerk at anything sounding to them like yet another hate post about Lisp, rather than considering that there is valid constructive comparative criticism why Lisp is not relatively popular. That's not helpful, nor is pseudo-pride about being different helpful because that has nothing to do with Lisp the programming language. Precisely.
I haven't actually done any Lisp-Web stuff in quite some time. I think I would start with Caveman2 though: [https://github.com/fukamachi/caveman/blob/master/README.markdown](https://github.com/fukamachi/caveman/blob/master/README.markdown) and just the default Djula templates. The README also shows using it with CL-WHO, but I've tried CL-WHO before and never really felt like I had a sense of the evaluation mode.
Thank you. Your work and writings gave me the final push to pursue Common Lisp as core technology and language in the 21st century. 
Nope! What is it that you're trying to do?
http://clhs.lisp.se/Body/m_defi_1.htm
If the only purpose of this is for development, you can use the Slime function `slime-who-specializes` instead.
Awesome, thank you!
Since I'm supposed to be studying and rather bored, here's some answers on how to do this, though I think you should not. See my other answer as for why. Finding all applicable methods that specialise on a given class: (defun find-specializing-methods (class) (let ((results ())) (do-all-symbols (s results) (when (and (fboundp s) (typep (fdefinition s) 'generic-function)) (dolist (m (c2mop:generic-function-methods (fdefinition s))) (when (find class (c2mop:method-specializers m)) (push m results))))))) Finding all generic functions that have a method that specializes on a given class: (defun find-specializing-functions (class) (let ((results ())) (do-all-symbols (s results) (when (and (fboundp s) (typep (fdefinition s) 'generic-function) (loop for m in (c2mop:generic-function-methods (fdefinition s)) thereis (find class (c2mop:method-specializers m)))) (push s results))))) Finding all generic functions that have a method for which the given class is applicable as at least one argument: (defun find-applicable-functions (class) (let ((results ())) (do-all-symbols (s results) (when (and (fboundp s) (typep (fdefinition s) 'generic-function) (loop for m in (c2mop:generic-function-methods (fdefinition s)) thereis (loop for spec in (c2mop:method-specializers m) thereis (and (not (typep spec 'c2mop:eql-specializer)) (c2mop:subclassp class spec))))) (push s results))))) Not particularly useful, but just to prove that you can do this. The above examples all rely on the closer-mop system being loaded.
The LispWorks Class Browser lists those. Use its tab 'Functions'. If you uncheck 'Include Inherited' it will list only those methods which have args of that class.
Hi, I don't answer specifically on Clack :] I prefer Spinneret over cl-who: https://github.com/ruricolist/spinneret/ (simpler to compose elements, and more features). Did you consider other frameworks ? FYI there's also the asynchronous [Wookie](http://wookie.lyonbros.com/docs), which has a convenient `defroute` (I recently re-discovered that, an exple successful project is the [Turtl](https://github.com/turtl/api/blob/master/routes.lisp) note taking app), Snooze, and more. I found Caveman cumbersome (getting url parameters), but didn't try much. Resources: https://github.com/CodyReichert/awesome-cl#network-and-internet notes on deployment and frameworks: https://lisp-journey.gitlab.io/web-dev/#shipping Hunchentoot is certainly the best documented with most real worlds examples, and I wouldn't under-estimate this now. the new Weblocks' quickstart is working very fine and is exciting, however there's still much to do: http://40ants.com/weblocks/quickstart.html Show us your work as soon as you can.
There's a little tutorial not to miss: https://jasom.github.io/clack-tutorial/pages/getting-started-with-clack/
Hi, Welcome to CL, let me be of help. &gt;The best one I found (most comprehensive) is Lisp for the Common Web by Adam Tornhill - it has good examples on simple use cases using [Hunchentoot, CL-WHO, Parenscript]. Note that hunchentoot is a web server, so expecting it to be as easy to use as a web framework will disappoint. &gt;State of the Common Lisp Ecosystem, 2015, recommended on the right panel of r/lisp discourages use of Hunchentoot for reasons I find reasonable. Note that we're in 2018 now, there has been a lot of activity the last 3 years! &gt;Hence, now I am looking for some resources on web development specifically on Clack. The official documentation only has a a demonstration on how to start a server, and an API reference which does not seem to be well documented. See my own [Ninglex](https://github.com/defunkydrummer/ninglex), which is a thin layer on top of Ningle, which is a bare-bones (minimal) web framework that directly uses Clack and Lack. This means that by reading Ninglex and Ningle code you'll easily understand how to use Clack. But if you want a web framework, there's Weblocks/Reblocks, Uncommon-web, Lucerne, and others; no need to be fixated on Caveman2. Kind regards, defun-K 
Thanks for all the suggestions! If anyone is doing their research on the same topic I found some good materials here - &amp;#x200B; [https://leanpub.com/fullstacklisp/read#leanpub-auto-chapter-4-caveman2](https://leanpub.com/fullstacklisp/read#leanpub-auto-chapter-4-caveman2) [http://ahungry.com/blog/2015-07-07-Writing-a-Common-Lisp-Web-App.html](http://ahungry.com/blog/2015-07-07-Writing-a-Common-Lisp-Web-App.html) [https://github.com/LispCookbook/cl-cookbook/issues/105](https://github.com/LispCookbook/cl-cookbook/issues/105) &amp;#x200B;
Thanks, if you have any suggestions for next steps or things to add, let me know!
An example of routing generated HTML would be great!
SBCL has SB-INTROSPECT:WHO-SPECIALIZES-DIRECTLY and SB-INTROSPECT:WHO-SPECIALIZES-GENERALLY.
Anyone know the back story on this?
Hi Ron, Speaking of trying to persuade more people to use lisp... are there any active lisp projects that you are following or are interested in? I mean, projects that might be significant enough to draw some positive attention to lisp?
Would you still use Lisp today for the same task in a modern space mission with more modern hardware or you would choose another language?
Well, I have some projects of my own that I think are pretty cool, and the Lisp world is nowadays chock full of all kinds of useful infrastructure. But I don't see any Lisp "killer apps" on the horizon. Do you?
I would use Lisp again without hesitation. But if someone wanted to code a spacecraft in, say, Haskell, I don't think I would try to discourage them.
Their newer .NET API for AutoCAD is the nicest way to extend the application IMO. Much nicer than their VBA integration. What kills me the most is how many things they've tried to replace AutoLisp with. VBA, C++, .NET, and even JavaScript. When I worked with AutoCAD back in the day we had an application that was written with a mix of C#, VBA, and AutoLISP. What a mess. I wish they would stick with one and make it good.
Has anyone tried Radiance? [https://shirakumo.github.io/radiance/](https://shirakumo.github.io/radiance/) Seems to be the most well-documented web framework at this point
RemindMe! 2 hours I'm not at my computer right now. 
I will be messaging you on [**2018-12-19 19:32:09 UTC**](http://www.wolframalpha.com/input/?i=2018-12-19 19:32:09 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/lisp/comments/a7o7js/quick_sicp_question/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/lisp/comments/a7o7js/quick_sicp_question/]%0A%0ARemindMe! 2 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
The value of your `s-it` function is `#&lt;unspecified&gt;` if *k &gt; n*. You should handle this case by writing out the alternative branch in `if` (or, better yet, by removing `if` and adding it as first clause in `cond`).
I have seen this yesterday but I couldn't find anywhere which version of Lisp is this actually running. If anyone knows, I am all ears. 
It says on the page "Learn Lisp, in an implementation and dialect made just for “Lisp on Bare Metal” educational/DIY projects." which isn't terribly clear.
Forgot about radiance, will edit my post above.
Beside your original problem, scheme already has even?, and then `(if (= (remainder x 2) 0) #t #f)` can be just `(= (remainder x 2) 0)` or `(zero? (remainder x 2))`
I'm looking at Radiance or Weblocks for my next project at the moment. Leaning towards Weblocks it seems to be more high-level, is that right?
Strange they didn't go for BASIC with that level of crap hardware.
I'm glad they went with Lisp instead (though I'm curious about the dialect). Lisp has a very "academic" reputation and it's great to see projects that prove it can be useful in more practical pursuits like, say, programming microcontrollers.
Sure, but uLisp already exists and you can't get much done on a micro.
No idea!!
Pardon my Scheme ignorance, but why is this necessary: (define (even? x) (if (= (remainder x 2) 0) #t #f))` instead of just: (define (even? x) (= (remainder x 2) 0)) 
It's cool that this board can run some implementation of Lisp, but sadly it doesn't seem to have anything in common with the Lisp machines beyond that.
The eZ80 is actually a 24 bit processor; it's "8 bit" only in the "data path" sense that doesn't affect the programming model. It has 24 bit registers and can move 24 bit quantities to and from memory as units. It seems useful for pretty serious Lisping, particularly if most of the address space is populated with RAM.
Folks, I'm really curious, how do you debug when developing anything around cl-async? SLIME is not directly supported, running it in thread causes all kinds of breakage, plus it is based on a C library (libuv) which is opaque from CL :( 
Because none of uLisp targets have 1MB of RAM. This is something akin a 286 on which very much was done.
That's really tight for programmers nowadays though, and the eZ80 isn't very fast either. I bet even the CADR (although much larger, that is a fair point) had more memory and computing power
Yes, I have seen that too. Not very informative. 
`apply` performs only a small subset of evaluation: it takes a list of values, and converts it into the arguments that are passed to a function. That function arrives into `apply` as an object; it doesn't require evaluation. The values in the argument list also require no evauation. `apply`'s job is just to pass those values to that function. `eval` is a full blown expression evaluator which takes a piece of source code, fully macro-expands it and traverses it to evoke the meaning of its special forms. It has to understand each special form. It has to build lexical (and dynamic) environments and resolve variables. `apply` has numerous uses. What it does could be performed by `eval`, but inefficiently and with some security risk. 
I didn't cover the perspective of writing an interpreter: why do we need `apply` if we have `eval`? If we are writing `eval`, we need to handle the form `(op arg1 arg2 ...)` when `op` is not a special operator or macro . In that case it is a function. We reduce the list of arguments to their values, and also obtain the function that the `op` expression refers to. Well, what then? How do we apply the arguments to the function? We need some primitive operation for that. It cannot be `eval`! If we construct a `funcall` form and call `eval`, we create infinite recursion; what will **that** `eval` call use to evaluate the `(funcall ...)` form? Since this is just `(op arg ...)` again, it will call `eval`. 
Thanks a lot! You are absolutely right... I don't know why I unconsciously thought that that the function (for k &gt; n) could return nothing, and somehow that would be equal to it returning '0'. Here's the final version, and it works perfectly. (define (cube x) (* x x x)) (define (s f a b n) (define h (/ (- b a) n)) (define (yk k) (f (+ a (* k h)))) (define (s-it k) (cond ((&gt; k n) 0) ((or (= k 0) (= k n)) (+ (yk k) (s-it (+ k 1)))) ((even? k) (+ (* 2 (yk k)) (s-it (+ k 1)))) (else (+ (* 4 (yk k)) (s-it (+ k 1)))))) (* (/ h 3) (s-it 0))) (s cube 0 1 100) ; 0.25 &amp;#x200B;
Well that's embarrassing... hahahah. You are right, I've been declaring this function on every exercise, and it is already there.
There's also https://github.com/cxxxr/lem
I've used Hemlock. I find Emacs/SLIME to be much better. Hemlock implements a few of the features of SLIME, but not even close to all of them. 
How can you implement EVAL without APPLY?
Slime, no contest. You'll need to seriously improve it, not just port.
`eval` is a big and potentially dangerous tool, while `apply` is simpler and more limited. The [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege) suggests using `apply` wherever it is sufficient.
**Principle of least privilege** In information security, computer science, and other fields, the principle of least privilege (PoLP, also known as the principle of minimal privilege or the principle of least authority) requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/lisp/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I don't think this is really a sensible comparison. SLIME is software to run in Emacs (that's what the E stands for). Hemlock is an editor written in Common Lisp, meant to run in your CL implementation. If you compare editors, Emacs has much more functionality, including better UI and support for interaction with a remote Lisp implementation. Hemlock is more closely integrated into CL, but that might be bad if you are trying to run an application in CL at the same time.
TIL there's a free version of LispWorks
Clozure CL goes with Hemlock for OSX. If you only could concentrate on porting this Hemlock version to Windows so the Clozure CL will get a Windows IDE, the world will be much better place :)
The LispWorks editor is originally also based on Hemlock. Some bits are still there.
What are the difficult parts of writing/modifying a lisp editor? I see you have some experience with SLIME
Time.
Thanks for the reply, but this is more a question of hemlock
Chapter 9.1 in the Hemlock User Guide: Hemlock runs in the editor process and interacts with other Lisp processes called *eval servers*. A user’s Lisp program normally runs in an eval server process. The separation between editor and eval server has several advantages The editor is protected from any bad things which may happen while debugging a Lisp program.• Editing may occur while running a Lisp program. • The eval server may be on a different machine, removing the load from the editing machine. • Multiple eval servers allow the use of several distinct Lisp environments. Instead of providing an interface to a single Lisp environment, Hemlock coordinates multiple Lisp environments. &amp;#x200B;
When you say emacs has better ui do you mean gui? How hard could remote lisp access be? Just use ssh. Why would it be bad to try and run an application in CL at the same time? Yes im suggesting a new graphic front end. I want an editor inside my lispgame
Would you say hemlock could substitute for vanilla emacs?
Do you use clozure cl hemlock? What does your workflow look like, and does it include windows?
Perhaps slightly off-topic, but in common lisp, apply cannot be implemented in eval, because eval is unaware of the lexical environment, so e.g. this won't work: ``` (let ((x 1)) (eval 'x)) ``` Is the same true in scheme?
Each compiled binary contains a complete Lisp runtime, and the runtime contains the error-handling (actually: condition-handling) logic.
What percentage of hemlock would you estimate is in the lispworks editor, out of what percentage?
Thanks that makes sense! 
I mean Emacs has a lot more people developing it and making it into an editor that does lots of stuff. Hemlock has a few people hacking on it, probably barely enough to keep it up-to-date with Lisp implementation and host OS evolution. The problem with remote Lisp access is that you want a functional debug and development interface: over that secure network connection, you want to do things like interactively inspect stack frames, identify source files that govern program objects, modify values, and so on.
Maybe. But my Emacs is heavily customized via my ~/.emacs. I don't have much experience using stock Emacs. 
Another vote here!!
What customizations in your \~/.emacs do you find the most crucial?
&gt; Thanks that makes sense! To add to the above, for example when you call a function, and this function expects specific data types, the first lines of the generated machine language code (of the function) will check the data types and trigger an error if they are not fit for this function. However, this code will often be removed from the generated machine language function if some optimizations are active, for example `(optimize (safety 0) (speed 3))`. 
The Clozure CL IDE and its Hemlock-derived editor does not use CLX.
It was? I thought you wanted an editor that can be ported to a different front-end, and since I read Lem supports ncurses and electron, I thought - this is an editor that at least was written with different front-ends in mind, so maybe it can be ported to OpenGL easier than something else?
&gt; Teaches you the minimal of debugging done ! https://lispcookbook.github.io/cl-cookbook/debugging.html#remote-debugging 
Thank you guys, but I'm still in doubt. perhaps I will get a better understanding once I finish those exercises.....
At least it's not in the textbook and exercises of SICP, where eval write as "(define (eval expression env) (balabala))" and apply as "(define (apply procedure argument) ())
If it's anything like Woo, app and env are very underdocumented and you'd need to jump into source code to understand them. `app` is a lambda which takes one variable, `env` which is a plist containing request context, which should return a list like `(return-code header-plist (content))`...I think. 
I am assuming your time is precious so use bad documentation as an early warning system. Simply avoid any and all libraries that don't have documentation or have documentation that sucks. It's an extremely good heuristic to avoid wasting your time. For this specific case, Clack and pretty much anything written by Fukamachi is complete and utter garbage so the heuristic works!
A better heuristic is if fukamachi made it, run far far away. Sorry for the slander....but their code is not very good despite the github stars, and shit like cl21 and uncl are migrane inducing.
Hi, author here. During the HTTP/2 standards process, a bunch of community members working on, or monitoring, the draft, created actual libraries to help test the protocol and give feedback to the group. I work at Akamai and worked on it in that context. I wrote this as a port of a ruby version, and maintained it through evolving drafts up to draft version 14. Unfortunately I did not have time at work to keep up with it through the final release, so it's behind. (Several of the interop implementations stopped short of the final spec similarly.) It also isn't necessarily built for speed. It's still posted in case the code is useful. In theory it could be updated to the final spec. I'm happy to answer questions about it.
I'm not a fan of Fukamachi's overuse of structures and other consequences of their pursuit for performance but their code has a well thought out design. Check out Mito for example. It wouldn't be slander if you explain your reasoning instead of just calling their software shit and migraine inducing
Although the situation has somewhat improved since 2012 when I started out, I remember coming from Python I was shocked by the lack of documentation. My advice is to get familiar with Sly's (or SLIME's) cross referencing capabilities and get used to jump to definition. It gets way easier after a while
I don't see getf as very fast, and [lispm already denounced cl21 as not great quality nor fast](https://www.reddit.com/r/lisp/comments/6snw5d/questions_for_2017_common_lisp_experts/dlfke2c/).
joswig has a great takedown linked below but it should be dead obvious to anyone who can read code: * He doesn't write Lisp, more like some unholy mix of Ruby and Java in Lisp. * No comments in 99% of his code. * Hard to read. * Not Lispy. * Octopus-like architectures drowning in external and internal dependencies. Verdict: Complete and utter garbage. Alas he is not the only one and there are plenty of others with the same attributes in CL universe. 
Lack of documentation is not a problem if the code is written so that it is easy to read by others. In my experience great engineers are obsessive about doing this. The mark of a bad engineer and someone you don't want to waste any time on is code that is hard to read and overcomplicated. For some tangible examples, I will first mention Peter Norvig. Paradigms of Artificial Intelligence Programming is the standard when it comes to clarity of Lisp code. Juho Snellman who is without a doubt one of the best engineers that ever graced Lisp-land, also writes code that is a pleasure to read. I had to strive to find code of his that was sparsely commented, since he usually comments his code very very well. Have a look at this https://raw.githubusercontent.com/jsnell/red7/master/red7.lisp and marvell at how compact, clean and easy to read it is. Unfortunately, for every master like Juho (and Paul Khuong, Xof, ...) there are many many more clueless idiots who pump out Lisp code like it's sewage without striving to get better. Maybe they can't because they are too stupid. Or too lazy. 
1. `C-x M-.` pushes my current position onto a stack, and `C-x M-,` restores it. 2. Rainbow delimiters 3. git-blame mode 4. Custom indentation rules for macros in my code. 5. Theme automatically reddens at certain times of day. 6. Uses a smaller font on my laptop's built-in monitor than on external monitors. 7. If I have it open on one X11 display, I can log in remotely and tell it to open a new frame on a different X11 display. 8. `C-\` inserts λ like DrRacket (some programs I write I define a λ macro). Come to think of it, Hemlock doesn't even support SLIME's `M-,`, which is really annoying.
[Some libraries](http://shirakumo.github.io/radiance/) do have a full featured documentation that isn't complete on Quickdocs. However, they will often link to this documentation on the source code repository (on GitHub that's at the top to the right of the description), which should be accessible from Quickdocs via the green Source Code button at the top. Some people also include a link in the ASDF system definition like [here](https://github.com/Shinmera/3d-vectors/blob/master/3d-vectors.asd#L14). If it's set in the system definition you can also programmatically retrieve it like so: (asdf:system-homepage (asdf:find-system :3d-vectors)) Though you might have to download it first via quicklisp: (ql-dist:ensure-installed (ql-dist:find-system "3d-vectors")) 
Clack's documentation situation is bad. IMO There are two types of usable lisp libraries: - Ones in which it is fairly easy to tell what a function does with slime and jump-to-definition - Ones with documentation Clack is absolutely *not* in the first category, and while there is useful documentation, the coverage is a bit spotty and it's not always easy to find exactly what you are looking for, so it's not quite in the second category either. I did write a tutorial to hopefully make things more useful: https://jasom.github.io/clack-tutorial/pages/getting-started-with-clack/ It includes a link to the upstream documentation of the request environment, and at the bottom has proper documentation for the function you are asking about.
env is actually documented, just not easy to find: http://quickdocs.org/lack/#the-environment
Well, that would have been useful when I considered Woo. It would be nice if that was put in all the stuff that uses that protocol.
There is no alternative to clack in terms of abstracting away lisp web servers. I wrote a new backend for it in about a day, and I do really like the interface. The code quality of clack is kind of all over the place, ranging from half-baked to quite reasonable. Fortunately most of the half-baked code is in middleware, and so I do avoid using *any* of the provided middleware. I don't really like cl21, but I'm not at all bothered by people writing new languages that I don't really like, so *shrug*.
Ah, reading the spec, scheme's `eval` takes an environment as a parameter. Internally pretty much every CL implementation has such a function, but the standard CL `eval` does not, nor is there a CL equivalent of `scheme-report-environment`.
You can use `DISASSEMBLE` (http://clhs.lisp.se/Body/f_disass.htm) to see if it does error checks. If error checks are not performed, passing a wrong type to a function may corrupt the running image.
thank you very much. It makes sense that the function check the data type, even though that must be quite the performance hit. 
Apply and eval are both recursively defined using each other.
Guys, if you feel that documentation can be improved, please, do pull requests. Fukamachi is very open to them and accepts the help. 
&gt; I could write the same library using 5 times less code. please do. Mito is currently useful.
FYI Lisp frameworks with decent doc: [Wookie](http://wookie.lyonbros.com/docs), [Snooze](https://github.com/joaotavora/snooze), [Hunchentoot](https://edicl.github.io/hunchentoot/) (although too api-level, it's also the one with most real-world examples), [Radiance](https://github.com/Shirakumo/radiance) (too verbose!), [Weblocks](http://40ants.com/weblocks/quickstart.html) (although much left to do). I won't recommend Caveman or Clack, although Clack is the only webserver abstraction. I found that some simple things are tedious (like accessing url parameters).
Cute, but totally delusional.
And there you see, folks, exactly why Lisp is a virtually dead language. The most toxic "community" in history.
He actually has one solid point, about coerce ;)
I agree with lispm's assesment of CL21, and most likely Fukamachi themselves does which is why they've abandoned it at this point. But given that none of Fukamachi-ware is written in CL21 nor it tends I fail to see how CL21 being 'shit' applies to Fukamachi-ware at large. \&gt; Regarding Mito all I see is layers and layers of bloat and unneeded trash abstractions. That applies to ORMs in general, hardly a fault of Mito :v. There is also lots intermediate steps between from 'Norvig Lisp Code' to 'shit'. If someone asks me for good Lisp code to read I won't send them Fukamachi's way (for starter the one-package-per-file is as unlispy as it gets) but I don't think it is fair to call it shit either.
&gt;Lack of documentation is not a problem if the code is written so that it is easy to read by others. I didn't intend to imply that lack of documentation is not a problem or that it is not needed if the code is good. But the fact that for most FLOSS Lisp software is lack of documentation is an issue so one has to get used to it. &amp;#x200B;
For the love of God, **DO NOT BUY THIS CRAP**. [Here is why.](https://www.reddit.com/r/lisp/comments/6qc61v/second_edition_published_interpreting_lisp/dkw66fl/)
Sorry. Didn't know that it was that bad. I knew something suspicious was going on with the fact that I couldn't even preview the table of contents...
No problem - I made that review precisely to discourage people from buying that book. It's not about the table of contents, really. It's about the book being an utter and complete waste of paper, ink, and traffic used to send this Amazon webpage.
Great post! Really appreciative of all the great replies in this thread. (clack:clackup (lambda (env) (funcall 'handler env))) Love this! That's awesome being able to recompile a single function and not have to screw with clackup. I was toying around with that and think I found a better way: (clack:clackup #'handler) I wanted to mention that one thing that helped me out after posting this question was just simply printing out the env variable like so: (defvar *handler* (clack:clackup (lambda (env) (print env) '(200 (:content-type "text/plain") ("My Clack Site!"))))) Could also use the `DESCRIBE` function as well. When I visited `localhost:5000/test` it printed the following to my REPL: ``` (:REQUEST-METHOD :GET :SCRIPT-NAME "" :PATH-INFO "/test" :SERVER-NAME "localhost" :SERVER-PORT 5000 :SERVER-PROTOCOL :HTTP/1.1 :REQUEST-URI "/test" :URL-SCHEME "http" :REMOTE-ADDR "127.0.0.1" :REMOTE-PORT 44580 :QUERY-STRING NIL :RAW-BODY #&lt;FLEXI-STREAMS:FLEXI-IO-STREAM {10033DDDB3}&gt; :CONTENT-LENGTH NIL :CONTENT-TYPE NIL :CLACK.STREAMING T :CLACK.IO #&lt;CLACK.HANDLER.HUNCHENTOOT::CLIENT {10033DDE03}&gt; :HEADERS #&lt;HASH-TABLE :TEST EQUAL :COUNT 10 {10033DE1F3}&gt;) ``` This was most of my confusion right here. With back-end web stuff, I am so used to having a built-in routing system. This is very cool seeing that Clack lets you process requests all by yourself. Of course, I'm aware that there are plenty of lisp libraries with routing if I really want that. 
+1 for Hunchentoot from me as well. It has great documentations (both in and outside the library itself) and, from my experience, is rock solid. I've used Hunchentoot to built and deployed a simple web app in the past, and the experience has been great.
&gt; (too verbose!) I wouldn't ever trade "too verbose" for "not verbose enough". I like knowing how a framework I'm using works.
&gt;EDIT &gt; &gt;: Never mind. With my way, it doesn't update when you recompile/redefine the handler function. Wonder why that is. Because when by doing #'handler you are passing the function object itself (the value that is the function) so even when the symbol is updated to be bound to the new function object you are still pointing to the previous one. When passing a symbol the lookup is done every time so the reference doesn't go 'state'q
&gt; While I don't agree with what appears to be the general sentiment here that the libraries written by our Japanese community are trash, it is unfortunately the case that the majority of them are hardly documented. Clack is a particularly bad case. I have personally struggled with the **lack** of documentation on c**lack** and **lack**. If there's a feeling that the japanese libraries are underdocumented (I have no idea if this is true), it might be because it's not the first-language of the programmers. 
&gt; I was shocked by the lack of documentation. Me too, but on the other hand, the [Bible is clear on this topic](http://web.mit.edu/humor/Computers/real.programmers): - Real Programmers aren't afraid to use GOTO's. (...) - Real Programmers don't need comments -- the code is obvious. &gt;and get used to jump to definition Yes, but in some cases this is a loss of productivity. 
&gt; Lisp frameworks with decent doc: I'd add `clsql` (database access lib) and `ltk` (GUI lib) to the list. Very well documented.
&gt; shit like cl21 I agree with lispm/joswig takedown of CL21, and also I don't have love for Roswell. However, Eitaro Fukamachi makes *useful* software, and the CL community would be in an inferior state if those libs didn't exist. If the software is useful but underdocumented, someone should eventually document it. 
&gt; That applies to ORMs in general I found Hibernate, NHibernate and SQLAlchemy pretty good. The complexity was kept in check, IMO. Still, I agree that ORMs aren't easy-to-use products particularly when you need to deal with performance hits due to (unintended) eager loading, etc. 
Use the installer at https://slproweb.com/products/Win32OpenSSL.html which provides ssleay32.dll that Lisp will want to load.
`(clack:clackup #'handler)` calls the function `clack:clackup` with the argument of the current function binding for `handler`. If you redefine `handler` then a new binding is established, but clackup has already been called, so it still has the old version. The version that uses `lambda` creates a function that performs a lookup the function binding of `handler` in the dynamic environment in which it is invoked. 
Thank you.
That's one of the versions that works, but then I get an error about CRYPTO_num_locks when trying a request in Drakma. I've searched on that error message too with no luck. Basically people who get that are sent to get another version of OpenSSL.
Looks like Turtl deprecated all of their Lisp-related stuff :(
which specific version are you using? 1.0.2, 1.1.0 or 1.1.1?
yes I mean this. My quick notes on the matter: https://lisp-journey.gitlab.io/web-dev/#accessing-url-parameters Yes I saw that too :S thus I emailed the developer and I had a nice answer (will blog about that). &gt; It was not an easy decision to replace CL with javascript. I find CL to be elegant, fast, stable, and above all else, easy to debug (having the ability to rewrite the program as it's running is insanely useful). In fact, I still miss lisp and ask myself all the time if I made the right choice. I think I did, though. The path I went with CL was a hard one. I wanted to be able to use asynchronous programming because for the type of work I do (a lot of making APIs that talk to other services with very little CPU work) it's hard to get much more performant. So I embarked on creating cl-async/cl-libuv (originally cl-libevent) and wookie, along with a few other drivers. Everything I built worked great (and still works great, as far as I can tell) however when things did go wrong, there was nobody to run ideas by and nobody to really help me...I had built all these things myself, and I also had to be responsible for fixing them when they broke. &gt; There was help and support from the community along the way, but I was mostly fighting it alone. I think the straw that broke the camel's back was when a few people started making copycat projects that added no real value (other than benchmarking fast) but stole mindshare from all the work I had put in. &gt; So between having to maintain everything myself and people putting out worthless copycat projects that ended up going nowhere, I didn't have the energy anymore. &gt; So I do miss lisp. I'd eventually like to build more things in it (like games). But I don't think I'll ever touch web stuff in CL again, and the whole journey left a bitter taste in my mouth. Sure I could have dropped the async thing and just done a threaded server in hunchentoot and cl-postgres. But once I decided I was going to reprogram everything anyway, it just made sense to go with Node.
There is no requirement for the involvement of a lexical environment in the implementation of *apply*, though. We absolutely can implement a function that works exactly like *apply* using *eval*. Only *eval* itself cannot use our function; it has to use its own *apply* internally. Through *eval* we can gain (clumsy, inefficient) access to its internal *apply*. 
"The MakerLisp product implements the 'Skim' dialect of Lisp" https://img1.wsimg.com/blobby/go/72a1c0e7-ca37-40c1-a729-18ba8ef5064a/downloads/1clf30bcf_853524.pdf https://cpmaker.com/
Quite tolerable in practice, especially with a good compiler. 
I'm a super noob and I've had a lot of grief trying to upgrade ASDF, I cloned it, built it and placed it where I thought quicklisp could find it (\~/quicklisp/local-projects/asdf and \~/common-lisp/asdf), ran `(ql:quickload 'asdf)` but I still get 2.26 from `(asdf:asdf-version)`. Then I tried loading it with `(load "/path/to/asdf.lisp")` which gave me 3.3.2.13 but I still got the error &gt; Error: Error Component "whatever-system-I'm-trying-to-get-with-quicklisp" not found &gt; While executing: (:INTERNAL (ASDF-2.26:FIND-SYSTEM (STRING))), in process listener(1). on one machine running Clozure CL, and debugger invoked on a QUICKLISP-CLIENT:SYSTEM-NOT-FOUND: System "asdf" not found on another running SBCL. Can someone give me a clue? I'm just learning lisp for fun but the package management stuff has been a nightmare so far.
Hello, I am a Japanese guy 😉, working with Lisp in 10 or more years. &amp;#x200B; Sadly, Japanese CL community was very weak around 2010. There was a Lisp community like "Shibuya.lisp", but many guys used Scheme or Clojure. &amp;#x200B; Fukamachi appeared after this situation. So, sadly, I think he and his followers don't inherit many suggestions of many CL predecessors... (from my point of view).
This is off topic, but I’ve had so many issues over the years with cl+ssl / OpenSSL on windows. I really think it would be worth the effort to write a separate backend implementation just for windows that calls into the libs Microsoft gives us i.e the SChannel api rather than requiring OpenSSL libs being installed and kept up to date. I’ve written a cffi wrapper around winhttp.dll becuase I was fed up with drakma https issues. 
Ok, thanks
First, do you *have* to ? Most libraries should be fine. Also, what Lisp do you use ? The latest SBCL should come with a newer version of asdf. Otherwise, IIRC you just have to clone it into ~/common-lisp/asdf and restart your lisp. Feel free to ask on stack-overflow or /r/learnlisp. Some projects that can hopefully help: https://lispcookbook.github.io/cl-cookbook/editor-support.html Good luck. Past this point, I enjoy the package management because it's closer to apt releases than the npm/pip mess.
yeah, if this (https://github.com/nilium/skim) is the source: "skim is a tiny, dumb Lisp parser / interpreter. It doesn't claim to implement any particular Lisp, only that it borrows the syntax."
I think this quite succinctly describes it: &gt;Usage &gt; &gt;Don't use this. At least not right now. LOL. I was hoping to see something a bit more complete to be able to port it perhaps to another embedded platform. Especially when prototyping something dynamic like Lisp would beat the pants off the usual compile-flash-groan-debug-compile-flash... cycle as long as it had reasonable support for the underlying hardware's capabilities.
I tried 1.1.1, 1.0.2, 0.9.8 and libre ssl 2.5.5
They clearly don't do the same things, Weblocks (Reblocks: http://40ants.com/weblocks/quickstart.html) solves the JavaScript problem© ! In my view it does more. Radiance seems neat to deploy apps related to one another (didn't try Radiance).
UCW looks dead: https://common-lisp.net/project/ucw/ all its links are dead now. Do you have an example website built with it ? Cheers.
I'm too new to CL and never used the Windows SSL calls but maybe your wrapper can be a starting point for a new lib. Is the code available somewhere? :) 
Sure you can get it from https://github.com/fjames86/winhttp put it in your quicklisp/local-projects dir and you should be able to ql:quickload it, use the http-request function. 
hehe. i missed that note from the skim author. i wonder if we'd be better off getting an AVR microcontroller or something like an ESP32 and going with uLisp or ECL. i mean, i don't have a need for a tiny lisp machine until i saw this but now i think i am fully in the want stage. i like the idea with this MakerLisp Machine but it isn't purchasable yet and i'm not sure how much it will be.
AFAIK, ESP32 has some Lisp ported to it. AVRs are going to be a problem, because of tiny RAM. Lisp is fairly RAM heavy due to its extensive use of the heap. 
Not likely. This is not a Z80 of a ZX Spectrum/Timex era. This thing is almost 3x as fast compared to the original Z80 running at the same clock frequency - and this can actually run at 50MHz. Imagine a 150MHz Z80 cpu ... It has also 24bit memory addressing (vs 16bit on the original Z80), so can address 16MB of RAM - good luck finding that in a 1980 era Lisp machine. FYI - Cray X-MP supported 16MB (didn't come fitted with, though!) at that time - which was a totally different class of a computer. There are obviously much faster processors than this available on the market, whether as microcontrollers or microprocessors (e.g. the various ARMs), but this has the advantage of simplicity. A Cortex A requiring SDRAM and nasty BGA packages would certainly run circles around this but few could actually do much with it in terms of building their own hardware. I would say that if you are interested in Lisp this isn't really hardware for you. This is more a toy for the retrocomputer enthusiasts (where Z80 is still going strong and CP/M compatibility is of interest) with the Lisp dialect running on the hardware only being a curiosity instead of the more usual BASIC.
Yeah this definitely needed. The cl+ssl experience on Windows is a huge headache.
 To load "drakma": Load 1 ASDF system: drakma ; Loading "drakma" ("drakma") CL-USER&gt; (drakma:http-request "https://badssl.com/") "&lt;!doctype html&gt; ... I have the 64-bit version of 1.0.2q with Clozure CL 64-bit from https://slproweb.com/products/Win32OpenSSL.html and I think I told the installer to copy stuff into C:\Windows\System32 during installation. I have the following files in there: Name: libcrypto.dll SHA256: 0F7CB50525E4D093FF750C82CCDB21540FFF4961D44D1865B14DB4BAF6B9F583 Name: libeay32.dll SHA256: 8A5C17CFA75A0A1FBCADE96F762912FD3E634FB976F1F355279EF85402853C6D Name: libssl32.dll SHA256: D9D8D86D3CDD97465DDE32EB0479E77B2811B3355E77897C11A7B19A4F996BCA Name: ssleay32.dll SHA256: D9D8D86D3CDD97465DDE32EB0479E77B2811B3355E77897C11A7B19A4F996BCA I'm not really sure where `libcrypto.dll` came from to be honest. Also, don't forget to reboot after installation. Seems to be necessary after putting things in system32. Hope that helps.
&gt;with what appears to be the general sentiment here that the libraries written by our Japanese community are trash I wouldn't equate the sentiment of two people with the general sentiment. For example trivia is pretty cool
Kattis is a website where you can solve programming challenges. It's used for several courses at KTH.
Thank you!!! 
Thanks I will try that specific version. 
I know I could google it, but to save those who come afterward from doing so, what is KTH?
A university: https://en.wikipedia.org/wiki/KTH_Royal_Institute_of_Technology
This could be implemented by SBCL, it currently isn't though (as you've observed). The easiest way to get this is to use CFFI and C structs, but then you lose the GC. 
Why?
Just to add for completion, it's also been available on a number of other Competitive Programming website as well for a while now - CodeForces, HackerEarth, HackerRank, and CodeChef (and a few others). IIRC, SBCL is the supported implementation on all these platforms. Enjoy!
I'd like an unboxed representation for speed (space is a secondary concern, although meaningful in certain contexts with large enough numbers of elements), along with the convenience (typing, slots, constructors) of structs. Consider it an exploration into maximizing mechanical sympathy by exploiting locality of reference. 
Except opposite to strongly typed language is *weakly* typed language, not dynamically. 
The question would be, what kind of semantics it has in Lisp and how it fits into the language. Currently Lisp only defines some primitive values which can be directly embedded: characters, fixnums, ... Lisp can copy them, which then means that a number might not be EQ to a number with the same value. One can also to point to array slots - there are only setter and getter functions. So when we get an embedded structure from an array via AREF, would it return a reference or a copy? If one would be able to get a reference to the embedded array, we would need to make sure the surrounding array gets not GCed when there are no references to it... and so on. This is a can of worms. Would be interesting to see whether there exists a proposal how to add 'structures embedded in vectors/arrays' to Lisp.
There certainly are issues regarding this. This proposal is very similar to value types in C# and (in the future) Java. You could always see how they worked around the issues.
Cache locality makes this sort of thing a huge win in performance.
To be included in base installations, it needs to be popular enough to outweigh other packages that compete with it - each package in a base install is additional megabytes in base install size. For a language to be popular enough, more software needs to be written in it and that software needs to be popular and widely used. Flowers and cakes are silly to me in this context.
That sounds right, even though it is discouraging as I don't see it happening anytime soon. Maybe the closest thing we get to a universally distributed lisp interpreter is the one in GNU Emacs. &amp;#x200B; The flowers and cakes were just an attempt to make the reader happy, as flowers and cakes are silly in almost any context. Perhaps not at weddings, and possibly birthday parties, but most others.
I'm not sure I remember which interpreter correctly, but I think Guile is usually installed pretty much everywhere. So much so that i went "oh wow, I have a lisp interpreter already?". Apparently gdb uses it? (according to the webpage) 
Interesting, thanks!
These distributions are supposed to make it easy to install packages, and if I were them I wouldn't change the base installation to appease everyone.
&gt; So when we get an embedded structure from an array via AREF, would it return a reference or a copy? If one would be able to get a reference to the embedded array, we would need to make sure the surrounding array gets not GCed when there are no references to it... and so on. This is a can of worms. I see the broader implications.... Certainly less trivial than I'd thought initially. I think most languages with "value types" return copies if you reference the embedded struct. Mechanically, you're computing an offset into the array, then initializing a new struct by copying the relevant bytes according to the size of the struct. So I guess accessing the embedded element via AREF would conceptually return a copied EQUALP struct (under the semantics I'm familiar with). Other languages allow for custom equality comparisons, but the implementation of both efficient and correct schemes look tricky. Efficiency gains then depend on whether the cost of copying is outweighed by the locality of reference gains, and if you intend to use the struct for efficient comparisons (vs. just storage). Also, if the use-case calls for accessing pieces of the struct (even embedded children), and updating pieces (vs. copying the entirety back and forth), there would likely be useful performance profiles the justify it. There are certainly a lot off "what ifs" to consider prior to extending language support (including what happens if you include object references in your structs, should that be prevented?). It's probably more practical to fence this kind of behavior off in a dedicated library [like vertigo in clojure](https://github.com/ztellman/vertigo) which wraps byte-arrays, or build a structure-of-arrays with some functions that provide a facade over the existing specialized array types or a byte array.
Guile scheme is indeed the "official" scripting language of GNU. 
Elisp is not a general purpose programming language and performs very poorly when used as such. It is a scripting language for programming Emacs.
This is even more than what OP asked for
On the MIT Lisp Machine the feature to have references to locations is called a 'locative' and a kind of arrays of structures are called 'grouped arrays'. 
What makes elisp not a general purpose language ?
https://www.emacswiki.org/emacs/EmacsLispLimitations https://www.emacswiki.org/emacs/WhyDoesElispSuck
&gt; Flowers and cakes are silly to me in this context. they were also silly to OP in this context. beep boop beep. 
At one time years ago I'd have said librep fits the bill: [**https://github.com/SawfishWM/librep**](https://github.com/SawfishWM/librep)
Also lots of things were based on perl so it became a natural dependency.
Hooray!
I actually like Perl and Larry Wall a lot, it's probably the language I have churned out most code in. Over at r/perl I have come across people who argue that Perl6 works as a superset of Lisp and therefore makes Lisp unnecessary. Still I like the minimalistic elegance of Lisp syntax better.
I do have fondness for perl, even though I never wrote more than 7 lines in it. I find many ideas very interesting (although dangerous for the non prepared coder) and perl 6 is also super cute. Also the culture is quite great, cpan and modules are often inspiring. (same for TCL or lua to an extent). Also it's out of fad and that's a plus to me :)
Well, that's not quite a satisfying answer, especially since the definition of "general purpose language" is pretty vague and certainly does not mean "you can do anything with that language". Because if that were the case, then only machine language would be considered as such.
&gt; There is (currently) no facility for using functions from dynamic libraries at run time in GnuEmacs. You are stuck with whatever native functions were built into your Emacs. Everything else is Lisp, or byte-compiled Lisp. Actually now you can: https://www.gnu.org/software/emacs/manual/html_node/elisp/Dynamic-Modules.html
&gt;Except opposite to strongly typed language is weakly typed language, not dynamically. Yes. I didn't say Lisp isn't a dynamically typed language. Read again. "It is unlike *most* dynamically typed languages." 
If this is for C communication then use the static-vectors library.
Doesn't work with structs: CL-USER&gt; (static-vectors:make-static-vector 10 :element-type 'cell) CELL is not a specializable array element type [Condition of type SIMPLE-ERROR] Cool library though.
I'm assuming that this is what OP means by "available": there is a package *at all*, not that it's required in the base system.
&gt; *How could we go about to promote a lisp interpreter to reach the same ubiquity?* I started by writing one that makes a good scripting language. TXR is a language that you can use like you would use awk. Everything you need for scripting is included in the program, and the documentation is one page. It's good at text processing, which is often inevitable in the Unix environment.
It says "base installation package", and various lisp systems have been available on Debian, Fedora, etc. for ages.
Oops, my mistake :/
That's sad to read, though it really did illustrate that we really do need more programmers. &gt; I think the straw that broke the camel's back was when a few people started making copycat projects that added no real value (other than benchmarking fast) Ouch... 
Very cool! The world needs more people like you!
oops sorry, I misread
Flowers and cake? No, please send beer. Belgian beer hand-crafted by computerless monks. $ apt show guile-2.2 Version: 2.2.4+1-1 Download-Size: 19.8 kB Description: GNU extension language and Scheme interpreter Guile is a Scheme implementation designed for real world programming, providing a rich Unix interface, a module system, an interpreter, and many extension languages. Guile can be used as a standard #! style interpreter, via #!/usr/bin/guile, or as an extension language for other applications via libguile. $ apt show bash | egrep '^Priority:' Priority: required $ apt show dash | egrep '^Priority:' Priority: required $ apt show perl | egrep '^Priority:' Priority: standard $ apt show python3 | egrep '^Priority:' Priority: optional $ apt show python2 | egrep '^Priority:' Priority: optional $ apt show guile-2.2 | egrep '^Priority:' Priority: optional &amp;#x200B;
Those munks are plainly mad. Last time I spent an evening at the Belgian bar in town I had trouble walking straight, that stuff is good but crazy strong. They are supposed to be men of god, but they brew the drink of demons. Good listing! I’ll show it to my colleagues who think python is the world dominant language.
On OpenBSD, it was the Chicken binary that ended up getting renamed: &amp;#x200B; [https://marc.info/?l=openbsd-ports&amp;m=154381585429257&amp;w=2](https://marc.info/?l=openbsd-ports&amp;m=154381585429257&amp;w=2) [https://marc.info/?l=openbsd-ports&amp;m=154460075027442&amp;w=2](https://marc.info/?l=openbsd-ports&amp;m=154460075027442&amp;w=2)
):
Using Nix to set up dev environments is pretty nice for avoiding problems like this.
I hit this on fedora 28 about a month ago: file /usr/bin/csc from install of mono-core-5.18.0.225-0.xamarin.1.epel7.x86_64 conflicts with file from package chicken-4.12.0-6.fc28.x86_64 file /usr/bin/csi from install of mono-core-5.18.0.225-0.xamarin.1.epel7.x86_64 conflicts with file from package chicken-4.12.0-6.fc28.x86_64 Anyone have any good ideas about how to solve this? 
Not a solution, but perhaps it might be worth mentioning in the linked-to discussion that Debian is not the only platform being affected by this; that Fedora and OpenBSD (and maybe others?) have been affected as well.
Arch also has the same issue. ``` :: chicken and mono are in conflict. Remove mono? [y/N] Y ```
 $ sudo apt install brewtarget qbrew Truly the universal operating system.
Displaced arrays are slow. More efficient is to use start and end indexes.
Yeah, most operations support start and end indices in Lisp. Why are displaced arrays slow?
Because displaced arrays can be displaced to displaced arrays.
I haven’t been keeping up-to-date with recent lisp-related news, so I’ve never seen this before. Looks interesting. What options are supported for controlling the color scheme of the interface and web content. Specifically, does it support a dark theme?
A dark theme could be done right now, you'd have to override a few methods like (setup-default) for the minibuffer method, and probably inject some sort of style sheet whenever you commit navigation in the the did-commit-navigation method of the current mode.
I could see adjustability and fill pointers messing things up, but couldn't multiple displacements be flattened at array creation time?
Thanks for the reply. I’ll definitely check this out!
enjoy :)
Yes, adjust-array can change where an intermediate array points or its displaced-index-offset.
So, displacement short circuiting should be possible when the intermediate array is not adjustable.
There's no type declaration for "displaced-but-only-to-one-level", and most displaced arrays are already single-level, but you still have to check for that, whether you collapse the chain or not.
this is actually just straight up unsupported by the parent comment. saying that someone's code is bad is completely different than saying that the person is a bad programmer, which the parent comment does \*not\* do. I agree that the criticism was too harsh for the examples provided, but it wasn't "toxic" in any way.
Displaced arrays have a potential spurious retention problem. Say I have a big vector of a million heap-allocated objects (only referenced through that vector). I extract a ten element slice of this and then forget the original. If the extraction is by-copy, then everything is cool; the original is eligible for garbage collection, as well as its element objects. If it's done by displacement, I have saved a few cycles and maybe a bit of memory, but now my little array holds a reference to the big array, preventing a million heaped objects from being reclaimed. Even if the array is a string of characters that aren't heap-allocated, still there is the fact that a tiny slice of a huge string keeps the huge string around. 
Java substrings aren't by copy?
Lack of a dark theme is the main thing keeping me from more extensive testing of Next - my eyes tire out rather quickly otherwise.
Or it would be an argument for a cache-aware compacting collector.
&gt;Java's strings are going to have this same issue. Older versions of substring weren't copied, which changed in later versions (1.7). The current implementation copies relevant portions of strings when substrings are created, precisely to get away from memory leaks (and for simplification some possible optimizations, at the expense of O(1) substring creation).
Ok, fair enough. I will probably pull out the css that themes the minibuffer (I can theme the minibuffer at least), and put it into a global var so that you can easily set it in your init file. &amp;#x200B; With regard to theming pages consistently in a dark way (which is legible in all scenarios), that is a much harder problem. (You have to consider background images etc). If I could just do something like \* { color: white; background-color :black} and inject that into every page I would, but I think that would make things very hard to read. Some more investigation on how to do this part well is required.
Yeah, unfortunately I think doing browser dark mode well is not entirely trivial. I wonder if checking how some of the Firefox/Chromium extensions handle dark mode could be helpful.
On the best programming advice, I don't quite follow because my English isn't that great. Is the writer saying you should get into other people's code? As a programmer on a large team, I try to avoid other people's code and just focus on new development. I do code review to make sure there's general stylistic consistency, but I don't test the code myself nor try to break it. I like to think the programmer and QA will do that part, because if I did all of the edge case testing, I feel like I might as well have written it myself. Only when something breaks do I bother to try and deeply understand code other people have written to fix the bugs. Sometimes it's so bad I have to rewrite the whole thing.
The Next Browser (https://next.atlas.engineer/) might be considered a killer app, but it's got a long way to go. Emacs is probably the largest lisp application around, no? Can that be considered a killer app?
Ok, so I installed sbcl from a package manager (homebrew) on another machine, and it *did* come with asdf 3.3.1, and then installing quicklisp *didn't* clobber this version. I guess the SBCL binaries from the site don't include any asdf. Thanks for your advice and the link! I'm using emacs with SLIME as an IDE, but good to know the alternatives :) p.s. From what I read, only asdf 3.* checks "~/common-lisp/" for systems. Before this, systems should be put in "~/.local/share/common-lisp/source/" I'm coming from npm (never done much python), so it's just different, and yes, more like apt.
There is a lot of code in there which is slightly too complex or even slightly wrong. Don't compare characters with EQ, use EQL. (defun escape (str) (let ((stream (make-string-output-stream))) (loop for c across str if (eq c #\Newline) do (write-string "\\n" stream) else do (write-char c stream)) (get-output-stream-string stream))) Instead of open/get of the stream, there is the built in WITH-OUTPUT-TO-STRING. (defun escape (str) (with-output-to-string (*standard-output*) (loop for c across str do (princ (if (eql c #\newline) "\\n" c))))) Using APPEND in a LOOP is usually not necessary and also an anti-pattern. Also handling result variables is also most of the time not necessary: COLLECT uses an implicit default result. (defun escape-plist (plist) (loop with res = (list) for key in plist by #'cddr for value in (rest plist) by #'cddr if (stringp value) do (setf res (append res (list key (escape value)))) else do (setf res (append res (list key value))) finally (return res))) simpler: (defun escape-plist (plist) (loop for (key value) on plist by #'cddr collect key collect (if (stringp value) (escape value) value))) finally (return res)))
GDB as installed by default on Debian is compiled without Guile support (and with Python support instead). Consequently, I've compiled GDB from source. 
As far as I get it, a common resolution is to do per-domain CSS. For instance, https://github.com/alphapapa/solarized-everything-css. 
Yes, I think that's probably a good approach. With some ability to toggle on/off.
That's a given with Next ;)
This has been known for a long time, and is really a solved problem: http://okmij.org/ftp/Computation/dynamic-binding.html The few times I have encountered this in scheme (seldom, since scheme is not lazy) I have just wrapped it in (let ((port (current-output-port))) lazy stuff here) 
That's not surprising and not a special problem of Clojure's lazy sequences. Any function leaving a dynamic scope and called later has the same behavior. Any data structure created within a dynamic scope and containing functions from that scope has the same behavior: lazy sequences, futures, objects with call-back functions, ... It's also hard to say if it's a feature or a problem. Imagine a dynamic binding for an output stream: do we want output to go to the originally dynamically bound stream or to the current dynamically bound stream? Hard to say - it depends on what we want to achieve. In Lisp is was sometimes useful to have coroutine-like behavior with its own set of dynamic bindings. On the Lisp Machine the basic building block for that was the 'stack group'. https://hanshuebner.github.io/lmman/fd-sg.xml So we could define a data structure with functions and their stack group. The stack group would capture the dynamic bindings. One can imagine that it is costly to store and to resume a stack group... 
&gt; *laziness and dynamism are quite opposite.* What? Both are late evaluation of sorts. &gt; *Beware of the inherent complexity of dynamic scope, each time, you rely on dynamic scope.* Is this the issue? TXR Lisp: 1&gt; (defvar *counter* 0) *counter* 2&gt; (take 15 (let ((*counter* 42)) (gen t (inc *counter*)))) (43 1 2 3 4 5 6 7 8 9 10 11 12 13 14) 3&gt; (take 15 (let ((counter 42)) (gen t (inc counter)))) (43 44 45 46 47 48 49 50 51 52 53 54 55 56 57) Nothing complex here at all, just straightforward scoping. One sentence explanation: the lazy sequence generation is based on a lexical closure which doesn't capture dynamic variables. But, we can use delimited continuations, which capture the dynamic environment. 1&gt; (defvar *counter* 0) *counter* 2&gt; (obtain (let ((*counter* 42)) (while t (yield (inc *counter*))))) #&lt;vm fun: 0 param + 1 optional&gt; 3&gt; [*2] 43 4&gt; [*2] 44 5&gt; [*2] 45 
Previous discussion: https://www.reddit.com/r/lisp/comments/a4gqtx/creating_a_nontrivial_lisp_game_in_2018/
I don't recall anyone calling the Japanese community trash. If you could show me this, I would feel enlightened. But having spent, a lot of time on #lisp, I've not seen it mentioned even once. I could probably even download all the log grep it, and not find an instance of someone criticizing Japanese specific code due to it being written by.a Japanese author. &amp;#x200B; In fact, I have often taken an opposite stance and promoted Eitaro's packages of your own. Am I not part of the Lisp community?
What's your problem man? seriously? Are you actually nationalist against Japanese people and their code? I hope this is meant as satire.
That's sad to read because Andrew Lyon (orthecreedence) writes good (not excellent but certainly not bad) code that can serve as solid foundation for further improvements/extension. I couldn't agree more with his outlook, there really is a proliferation of trashy libraries that have nothing of value to offer and are downright destructive since they dilute the space of useful CL libraries with shit. 
I have a problem with charlatans like you who are misleading users in the name of a quick buck, self-promotion and publicity. 
Don't need CFFI for this. Can just allocate foreign memory and use macros to create struct-like overlay access on top of that foreign memory.
(deftype foo () '(simple-array fixnum (3))) &amp;#x200B; (defstruct (foo (:type (vector fixnum))) (x 0f0 :type fixnum) (y 0f0 :type fixnum) (z 0f0 :type fixnum)) &amp;#x200B; (make-array 100 :element-type 'foo) &amp;#x200B;
Is there a good (ie, non-trivial) motivating example? I have thought about first-class macros before, but couldn’t find a use for them. 
Well; it removes most differences between macros and functions; which frees capacity for more constructive tasks and makes the language more composable as a whole. Having said that, this is a classical bias when being confronted with features you've never used; Paul Graham famously called it the Blub Paradox. Those who have yet to encounter Lisp macros are very unlikely to know what they're missing, for example.
&gt;removes most differences between macros and functions; which frees capacity How does it free such capacity? How does the difference between macros and functions consume capacity for more constructive tasks?
I've read the linked readme over and over, and I still can't quite understand what you're trying to show in any of the CL examples. And maybe I'm the wrong audience (being on r/lisp), but I don't speak enough Forth to understand the Snigl examples, either. &amp;#x200B; Sorry.
Let's think about `and` in Common Lisp. It's not a simple logical operator, but a control structure. `(and a b)` means that if `a` is false, then `b` will not be evaluated. Thus it's not a function in Common Lisp. Functions get all their arguments evaluated. Since `and` is not a function, typical operations for functions are not available: you can't use `funcall`, `apply` and `function` with it - examples which don't work: (funcall 'and nil t) (funcall (function and) nil t) Imagine that you would want to write this: (let ((operators (list #'and #'or))) (mapcar (lambda (operator) (list (funcall operator t nil) (funcall operator nil t))) operators)) Above does not work in CL. Let's look at the simpler example `(funcall (function and) nil t)`. Now we can repair that: (funcall (lambda (a b) (and a b)) nil t) Well, this does not do what `and` is actually doing: evaluate the second argument only when the first is true. We can repair further: (funcall (lambda (a b) (and (funcall a) (funcall b))) (lambda () nil) (lambda () t)) The effects are the same: CL-USER 50 &gt; (funcall (lambda (a b) (and (funcall a) (funcall b))) (lambda () (print :foo) nil) (lambda () (print :bar) t)) :FOO NIL CL-USER 51 &gt; (and (progn (print :foo) nil) (progn (print :bar) t)) :FOO NIL So we need to delay the arguments. But that is not what macros in general are doing: they don't work over delayed code, but do code transformations of source code to source code. Thus we have in this case a simple effect similar to `and`, but this is no general approach to the problem of providing macros as a first class data (can be returned, can be passed, can be stored, can be called like functions, ...). Lisp did solve that problem in the past by special functions and support for a special kind of calling these functions: fexprs. These functions get their arguments unevaluated and then can manipulate/evaluate them at runtime. fexprs were falling out of fashion in the 80s, when there was more interest in efficiently compiling programs. Macros were seen as more attractive, since they are expanded before code gets compiled. R (the programming language) supports something similar to Fexprs. One of the problems why fexprs were not supported in Scheme and CL is that they require runtime code interpretation/compilation. 
Bind a handler for the condition `t` which prints the error to stdout and then quits? ```lisp (defun my-script () ;; put your script in here ) ;; check out this cool &amp; good condition handler (defun print-error-and-abort (e) (format *error-output* "~&amp;~a~&amp;" e) (invoke-restart 'abort)) ; you could also call sb-ext:exit or something ;; bind it to all conditions &amp; run (handler-bind ((t #'print-error-and-abort)) (my-script)) ``` 
&gt; differences between macros and functions Differences are there because they do different things. You really might want to provide convincing examples where your attempt supports the typical use cases of Lisp macros AND removes most differences between macros and functions AND does not fall back to earlier attempts like Fexprs (functions which get unevaluated arguments).
For further investigation see `sb-ext:*invoke-debugger-hook*` and the function `sb!debug::debugger-disabled-hook`.
TXR Lisp: 1&gt; (or (prinl 'hey) (prinl 'bar)) hey hey 2&gt; (mapcar 'or '(nil 2 2) '(1 3 3)) (1 2 2) Simply, just let operators and functions be bound in the same namespace. This works well for operators like *if* and *and* which are logical functions imbued with short-circuiting. In situations when we don't require the short-circuiting, we'd just like them to be functions. This feature also serves the uses cases of compiler macros. (The only difference being that macros always expand; it's not up to the compiler to decide to use them or not.) I can't think of good use cases for very many other kinds of macros/special operators. When would we want to do `(mapcar 'defclass ...)` or `(mapcar 'loop ....)` type application. 
Here is what it's like when macros exist side by side with functions: 1&gt; (defun square (x) (* x x)) square 2&gt; (fboundp 'square) t 3&gt; (mboundp 'square) nil 4&gt; (defmacro square (x :form f) (if (constantp x) (* x x) f)) square 5&gt; (mboundp 'square) t 6&gt; (macroexpand '(square 3)) 9 7&gt; (macroexpand '(square x)) (square x) 8&gt; (let ((x 3)) (square x)) 9 9&gt; (square 3) 9 10&gt; (fmakunbound 'square) square 11&gt; (square 3) 9 12&gt; (let ((x 3)) (square x)) ** warning: (expr-12:1) unbound function square ** (expr-12:1) square does not name a function or operator 
Dunno how related this is, but: racket has a notion of 'signatures', where (I think) an entire module is conceptually a function -- it takes in arguments (bound to certain parameter-names), and returns a bunch of values. That way, you can have mutually-recursive modules etc. But I don't think it's widely used, and not commonly needed/useful.) I could be wrong on all this -- it's been a while since I've looked at the details.
(just noticed one typo: "we loose runtime currying", an extra "o" in there)
In TXR lisp there is an *op* macro for partial application, inspired partially by the [Goo operator of the same name](http://people.csail.mit.edu/jrb/goo/manual.46/goomanual_15.html#17); it has a sister called *do* which takes special operators and macros. I think that covers the situations being done here with CL. Example: 1&gt; (let ((x 42)) (mapcar (do if @1 @2 x) '(t t nil) '(1 2 3))) (1 2 42) The argument of `do` can be any operator, including a macro operator. It is still expanded correctly. The `do` expression returns a function. The arguments are determined by the presence of `@1`, `@2` and so on, as well as `@rest`. These elements can occur anywhere in the expression at any level of nesting. it is correctly code walked to find all of them and generate the correct n-ary function. What we're not demonstrating is the "compile-time binding" requirement for which sifoo reached for `eval` in the CL examples. I think the right tool would be to use a local macro to pull such a term into the expression: 2&gt; (symacrolet ((x (+ 40 2))) (mapcar (do if @1 @2 x) '(t t nil) '(1 2 3))) (1 2 42) This satisfies the requirement; `x` pulls in the `(+ 40 2)` source code into the middle of the `do` expression. It's just not encapsulated, and the definition site of `x` decides the semantics, not the use site. That is to say, we can't just make it `(let ((x '(+ 40 2))) ...)` and then use some magic annotation at the place where `x` is used to use it as a macro. However, I believe that such a feature could be developed. There is no reason why in a Lisp macro expander we cannot provide access to a lexical variable's initializing expression. So that is to say, when we have (let ((x '(+ 40 2))) ... x) at the point where we are code-walking the rightmost `x`, the information exists at expansion-time that `x` is a lexical variable. The further information could be procured that `x` is initialized with the expression `(+ 40 2)`. We can easily envision an operation like `(macro-time-var-init-value x)` which replaces itself with `(quote (+ 40 2))` at macro-expansion time. 
Assuming you're talking about Common Lisp in particular, rather than Lisps in general, it sounds like you might find [the Common Lisp HyperSpec](http://www.lispworks.com/documentation/HyperSpec/Front/index.htm) useful.
It says Package SB!DEBUG does not exist. And some weirdness is happening I don't yet understand. The following causes the error about the package not existing, and of course it doesn't see the symbol on the next line being undefined because it already quits by then. \#!/usr/local/bin/sbcl --script sb!debug::debugger-disabled-hook This-symbol-is-undefined But this causes it to silently exit with no messages at all: \#!/bin/bash sbcl --script &lt;&lt; EOF sb!debug::debugger-disabled-hook This-symbol-is-undefined EOF And this gives the appropriate error message for the first undefined symbol and a backtrace: \#!/bin/bash sbcl --script &lt;&lt; EOF This-symbol-is-undefined As-is-this EOF &amp;#x200B;
\&gt; wrong about declaring variables &amp;#x200B; Maybe you could give an example of that here, and someone could clarify it.
I just thought you always used setq when changing a value, so i thought you could make an array, then aref an index, and use setq to set a value, but you use setf, and I dont see an explanation of what setq or setf does. set-quantity? set-float?
Tutorialspoint isn't great. Their examples constantly fail to define variables. https://www.tutorialspoint.com/lisp/lisp_input_output.htm ; this program inputs a numbers and doubles it (defun DoubleNumber() (terpri) (princ "Enter Number : ") (setq n1 (read)) (setq doubled (* 2.0 n1)) (princ "The Number: ") (write n1) (terpri) (princ "The Number Doubled: ") (write doubled) ) Problems: * don't use CamelCase, the Lisp reader is not case sensitive by default * variables N1 and DOUBLED are undefined, use LET to do so * finishing output is missing, when output might be buffered * don't use dangling parentheses * might as well use documentation strings instead of leading comment * probably double using an integer, not a float by default Minimum improvement: (defun double-number () "this function inputs a numbers and doubles it" (let (n1 doubled) (terpri) (princ "Enter Number : ") (finish-output) (setq n1 (read)) (setq doubled (* 2 n1)) (princ "The Number: ") (write n1) (terpri) (princ "The Number Doubled: ") (write doubled))) 
See the function SB-DEBUG::DEBUGGER-DISABLED-HOOK 
`setq` is "set quoted"; `setf` is ["set function"](https://stackoverflow.com/questions/23808189/what-does-the-f-in-setf-stand-for). `setq` allows one to not have to quote the symbol (roughly, 'variable') that one is assigning a value to, so that instead of doing: (set 'my-var 3) one can do (setq my-var 3). The link i provided explains the etymology of `setf`. As for explanations of what both functions do, check out the relevant CLHS entries: [setq](http://www.lispworks.com/documentation/HyperSpec/Body/s_setq.htm#setq), [setf](www.lispworks.com/documentation/HyperSpec/Body/m_setf_.htm).
There are introductory Lisp books... http://www.cs.cmu.edu/%7Edst/LispBook/ http://www.gigamonkeys.com/book/ SETQ (set quoted) is for setting already defined variables to values. SETF (set field) is generally for setting 'places, incl. already defined variables - thus SETF can do what SETQ does and more. SETF sets a variable. DEFVAR, DEFPARAMETER, LET, LET*, ... are **defining** a variable. Avoid setting an undefined variable. 
Sorry to hear you are having a tough time. TutorialsPoint's lisp tutorial is somewhat decent but it definitely lacks a lot of important information. I had a really good time following through the free online book, Practical Common Lisp. Just a warning, chapter 3 was pretty tough as a beginner but after I finished it, that one chapter alone gave me a ton if great knowledge. You'll also likely enjoy Baggers' [Little Bits of Lisp](https://www.youtube.com/watch?v=m0TsdytmGhc&amp;list=PL2VAYZE_4wRJi_vgpjsH75kMhN4KsuzR_) series. He makes things so much easier to understand. He also has longer [Lots of bits of lisp](https://www.youtube.com/user/CBaggers/search?query=lots+of+bits+of+lisp) videos that are really great too. #lisp on IRC is very active and people are super nice there so that's another helpful resource if you have questions. As for documentation, maybe some better advice than this can be provided, but I usually just Google something along the lines of "common lisp format" (For your 'format' example). I found that typing out "common lisp" rather than just "lisp" has provided the best results. Either LispWorks docs or some other resource (including StackOverflow!) will solve my problem. Another thing that could help you is the `describe` function in Lisp. Simply type `(describe 'format)` in your REPL to see information about the format function. There's also a command/shortcut to do this in Slime too. If you put your cursor over a symbol and then use the shortcut `C-c C-d C-d` (or run the Emacs command `slime-describe-symbol`, it will show the documentation in a new buffer. 
I personally found the approach brilliant, thanks.
Is it described somewhere? It wants two arguments. Is the first argument the function to invoke on error? How many arguments does that function take? I will eventually figure it out by trial and error, but some hints could save me a lot of time. &amp;#x200B;
Check out the SBCL source code. The function is called when an error happens in the script. It also writes the backtrace...
Hi, some months ago a group of mexican Lispers were pushing for their website (a sort of social network). It was done in UCW, and they has some other sites done w/ucw.
Land of Lisp will teach you idiomatic CL and is also pretty fun
Thank you so much! I really appreciate this. I haven't been sleeping well and let my mood get the best of me. This subreddit is so unbelievably welcoming. 
Been programming lisp for over twenty years, never used setq. Watch out for Lisp tutorials, they use unidiomatic Lisp to deliver neat, terse examples that work but are not how we program. My favorite is '(1 2 3) and noobs think you can alter that. Nope, we need (list 1 2 3) for that. The examples just read the quoted list which is fine, but of course noobs do not notice that. When they want to experiment with mutation, they still use '(1 2 3). I see lots of good recommendations but PCL is fine. The Hyperspec is great but not for noobs.
It might be fun but the style is not typical.
ps. CL includes a catalog of all expressions. Try (apropos "map"). If you can guess creatively it can be a handy reference for noobs.
Excellent explanation and thank you for the background info. I hadn’t heard of fexprs before. 
Hyperspec for authoritative documentation, Practical Common Lisp for learning the language - assuming you already know how to program in some other language.
Tutorialspoint Lisp content sucks and I'm sorry that you ended up stumbling upon it. Please use Practical Common Lisp as a learning book and CLHS for reference, as others suggested. Also seek help on `#lisp` or `#clschool` on Freenode IRC or the Lisp Discord server.
If you have Quicklisp installed, you can invoke it from a script and load the systems you want.
Picolisp
I might be wrong here - but does your let statement need another set of parents around `(n1 doubled) `? 
Does picolisp have a central package repository to download packages from?
Of course no, you have everything you need to implemement all you need yourself. 
Not in this case. The first thing in a let is a list with variables you want to bind. That's what he did. (let (a b) ;just binds a and b, but gives them no value. ) What one can do and that's what you thought of, is that you can give the variables intial values. Like so: (let ((a 1) (b 2)) ;bind a and b and give them the values 1 and 2 ) You can of course mix these two. Note that his improved version still is not really lispy (at least what I think). He called it "minimum improvement" himself. Hope this helps and I didn't make a stupid mistake since I'm on mobile.
Practical COmmon Lisp, whatever Rainer Joswig posts on StackOverflow :). /r/learnlisp for questions.
I know of **no** systems that automatically load dependencies. It'd be easy to do so in Common Lisp if there's a definition of required systems however.
Not in this case - it's valid Common Lisp. Here it indicates that the variables are introduced by the LET, but get values later via the SETQ statements. In older code or in generated code one can find a similar style, but usually using PROG - because PROG has more features, often used in imperative code. PROG allows the definition of local variables, it allows jump tags for goto statements and it supports returning a value via RETURN.
It takes some work to print and bind it, but the [Common Lisp Quick Reference](http://clqr.boundp.org/) is worth the effort. There you can easily find what you need, and you can complement its succint descriptions with the CLHS.
That makes perfect sense. Thank you! 
So, you use QT5 to write a web app? Or is it some other stack? AFAIK what the lispers are using when it comes to browsers is [Closurescript](https://clojurescript.org/). Other than that, I think [parenscript](https://gitlab.common-lisp.net/parenscript/parenscript) is as good as you'll get. I hope wasm changes the landscape of web dev so Javascript-esque languages aren't the only option.
Do you know the story behind the name "Flavors"? Or "mixins"? My guess is that it is a reference to [this kind of Whopper](https://en.wikipedia.org/wiki/Whoppers).
Qt marked up in Lisp would be super cool. Some historical context too - https://paulhammant.com/2013/03/28/interface-builders-alternative-lisp-timeline/ - well without the Qt part.
Question? I know CL compiles isn't it bytecode? Can't you run this on ASM.js?
The web app is a VueJS app. 
Depends entirely on the CL implementation. You'd have to make the CL runtime run on asm.js too.
I see. Thank you.
Your use-case seems to be perfectly suited for Lua. So my recommendation would be [Fennel](https://github.com/bakpakin/Fennel), a lisp that compiles to Lua.
How would that work in a web app?
"set form", probably. The argument is any form that serves as a place.
`setq` is basically for compatiblity with ancient Lisps. There is no reason to use `setq` in new Lisp programs. It does exactly the same thing as `setf` in all regards, except that it rejects place syntax that is not a symbol: (setq (car x) 42) ;; error (setf (car x) 42) ;; okay But `setq` **will** assign to `(car x)` through a macro! (symbol-macrolet ((m (car x))) (setq m 42)) ;; no error; same meaning as (setf (car x) 42) That is to say, `setq` just doesn't like the *non-macro-expanded syntax* of the target expression to be anything but a symbol. After that symbol is macro-expanded, the syntax can be any kind of assignable place. The back-story of `setq` and its name is historic cruft that predates the introduction of lexical scope into Lisp. In modern Lisp, `(setq a b)` is not equivalent to `(set 'a b)` ("set quoted a to b"), except for dynamic/global variables. If `a` is a lexical variable, `(setq a b)` cannot be replaced by `(set 'a b)`; `set` is a function which has no visibility into the lexical scope it is called from. 
To most North Americans, the word Whopper is instantly recognizable as the name of the flagship burger from Burger King. 
[Practical Common Lisp](http://www.gigamonkeys.com/book/) is a decent introduction. As far as a reference goes, [the hyperspec](http://www.lispworks.com/documentation/HyperSpec/Front/Contents.htm) is great. It's not great for learning from, but every function, operator, and macro in the standard is there and documented. Others have given more specific critiques, but stay far away from tutorials point.
There is two questions here, because fast startup and automatically installing dependencies are two different things. In the context of Common Lisp implementations, if you want fast startup for a script-like source file, the fastest you will get it is if you prepare a custom Lisp image which contains all of the needed libraries (compiled and all). That Lisp image is then used as the interpreter for the script file. The runner-up for start-up speed is to load compiled files (".fasl" files), rather than source files. Compiled Lisp loads substantially faster. Even compiled file formats based in Lisp S-expressions load much faster than Lisp source, because they are only data that doesn't require time-consuming processing such as walking the code to expand macros. One start-up optimization that can be performed is autoloading: loading only those modules that are actually used, as late as possible. Common Lisps typically don't offer this because there is low motivation to support such a thing when you already support image-based deployment. Autoloading systems require registration; all the auto-loaded libraries have to be installed. They are just hooked in for lazy loading. The deprecated functions `require` and `provide` support a kind of explicit loading that can provides the benefits of autoloading. I haven't heard of a system for Common Lisp for automatically installing libraries that the system doesn't know about. Like encountering some `foo::bar` function and knowing that it's in some `cl-foo` library that can be fetched from somewhere. 
I think you can use [Fengari](https://github.com/fengari-lua/fengari/blob/master/README.md) which is the Lua VM written in JS. This might help you in keeping Fennel as the common extenesion language for both web and desktop/mobile. Again just suggesting ideas here, feasibility is something only you can decide on. One more option would be to use OSC messages as an abstraction over the extension API which then will let you have a common extension interface for both desktop and web.
There are perfectly fine first-class macros in Common Lisp. Anyone who would say otherwise probably doesn't know what he is talking about. 
There was a bit of rivalry with the name iirc. Defrwapper came first but wrappers had issues. At the time, whopper just sounded bigger. I don't recall any connection to malt balls. There may have been some irony with Burger King, and there was one nearby. Big in general I feel like, looking back, Whoppers were just named similarly and more encompassing. Of course, HIC might chime in...
Racket used to have the planet (PLaNeT?) system. You'd specify your dependencies like: `(require (planet some/pkg/version))` and it would fetch them from a central repo when the script was run. They've since deprecated that in favor of a more traditional package management tool, though.
Those look like really interesting projects. I don't no too much about Lua other than it being a popular embed-able language. One thing that worries me about this setup is that it adds quite the chain of scripting languages. First you have Javascript and then on top of that you have Lu and on top of that you have Fennel lisp.
Fennel to Lua is not a lot of overhead but Lua to JS is. But then again you would have to wait for web assembly to get rid of that performance penalty. The interactive REPL in [Fennel'e official website](https://fennel-lang.org) is implemented using Fengari so you can execute some code there and have a very rough idea of the performance you might get with this approach.
In the MIT Lisp Machine system of "old" flavors, wrappers were a device to wrap a method around the the collection of individual class methods invoked when a message was sent. Wrappers were similar in spirit to :around methods in ANSI CL CLOS. Whoppers were an additional mechanism of providing macro-like code that would wrap around the combined-method code body, much like the &amp;body argument in a macro expansion gets wrapped before compilation by a typical macro. There is no close analog in the ANSI CL CLOS system, and that's a good thing.
Can common lisp depend on a package that I manually downloaded into a relative path?
That one's great, I have printed it out and bound it so that I always have it at hand. It is not good for learning (which it was never intended for), but it is great for looking things up or just browsing through it.
That would certainly make more sense to me, but i don't feel i'm in a position to dispute Gabriel and Steele. :-)
&gt; I haven't heard of a system for Common Lisp for automatically installing libraries that the system doesn't know about. Like encountering some foo::bar function and knowing that it's in some cl-foo library that can be fetched from unless I'm misunderstanding, quicklisp does this.
Can you expand on the difference between macros in TXR lisp and compiler-macros in common lisp?
That's an excellent explanation, thank you. I'm also passingly familiar with fexprs, so that helped me clear the gap.
The difference is that regular macros always expand; it's not up to the compiler to decide. If you're using them like compiler macros, and need an on/off switch, you have to invent one yourself. Other than this requirement, there is no need for the duplicity of supporting both macros and compiler macros. Regular macros will cover the use cases of compiler macros if two adjustments are made: (1) they are permitted to co-exist with function definitions and (2) there is a way for macros to decline to expand (like returning the same form that goes in). I made macros coexist with functions by putting global macros into their own space. There is a `mboundp` function to check for a macro binding, `mmakunbound` to delete it, and `symbol-macro` to retrieve it or `set` it. `(symbol-macro 'mac)` is equivalent to `(symbol-function '(macro mac))`. 
Ah, I see that in the *Evolution of Lisp*, 2.6: *"Deutsch commented that the special form used here is called SETFQ because “it quotes the function and evaluates everything else.” This name was abbreviated to SETF in Lisp-Machine Lisp. Deutsch attributed the idea of dual functions to Alan Kay.*
Seems like a Lisp environment hosted in JS would work? I think JS runtimes are available almost everywhere...
But why not just use a JS-hosted Lisp in that case?
IMHO There is some typos between Wrappers and Whoppers. Wrappers: In the MIT Lisp Machine system of "old" flavors, Wrappers were an additional mechanism of providing macro-like code that would wrap around the combined-method code body, much like the &amp;body argument in a macro expansion gets wrapped before compilation by a typical macro. There is no close analog in the ANSI CL CLOS system, and that's a good thing. Whoppers: New Flavors, Whoppers were a device to wrap a method around the the collection of individual class methods invoked when a message was sent. Whoppers were similar in spirit to :around methods in ANSI CL CLOS. cf. http://bitsavers.org/pdf/symbolics/software/genera_8/Symbolics_Common_Lisp_Programming_Constructs.pdf#page=174
It looks like ECL android is using QT5 / QML ? https://play.google.com/store/apps/details?id=org.eql5.android.repl Parenscript via quicklisp. New functions are bytecode until you re-compile to C. 
Not sure what exactly your wariness for Scheme compliance vs. JS replacement means, so just a short wrap-up of what I found out: * For CL, I dabbled with Parenscript. Parenscript is an absolutely astonishing achievement and it is really cool to use - where it works. Main limitations I found were that transpilation is not always obvious if you just want to use it (without trying to understand the whole macrology behind it) and that CLOS is not supported (there seems to be an extension for that, but Parenscript on its own is already exotic enough...). Also not much seems to happen around these days for Parenscript. Still I'm quite sure that it will work to call your own JS from Parenscript. * For Bigloo Scheme, there is Scheme2Js and Hop. Hop allows you to write the backend-side and the frontend-side in just one file and even service-method, if you like; then you start the whole service in Hop and Hop does the rest. Scheme2Js allows you to compile (a subset of Bigloo-) Scheme to JS. Sadly you'll find close to no usage of both tools outside the Hop repo and even there most Hop development seems to focus on an JS-based workflow, nothing happening around the Scheme-based Hop. Still if you manage to compile and break-out Scheme2Js, it might be a good option to write Scheme and deploy JS (IIRC you invoke Scheme2JS through the Hop compiler `hopc`) - if you strictly limit yourself to the compliant Scheme-subset supported by Scheme2Js. Again I'm quite sure that it will work to call your own JS from Scheme2Js. * On the other end of the spectrum is Urlang for Racket, which is not a Scheme, but more like a lispy syntax-wrapper around JS. You can find some hints here and there how to make the JS-integration more transparent, but as far as I understood your use-case it would just be too much effort. Still it could be helpful if you just want to import/export the same sexpr on frontend or backend or share some minimal validation logic. * And there is BiwaScheme, which is a Scheme interpreter written in JS. It will take your script source in Scheme and compile it to JS. I just played around with it shortly, but IIRC it only supports a subset of Scheme, e.g. no hygienic macros and hence probably not many SRFIs. Still JS integration should be good. * If you want to use Scheme on the server-side, Kawa might be an option, as it is a JVM-based Scheme (and as I mentioned Bigloo before: it can also run on the JVM, but for some reason I tend to use Bigloo to compile to standalone and use Kawa with the JVM) - also sporting some extras to simplify servlet-development. But to my knowledge no JS-transpiler, so probably not what you need. * And finally there is there is Schism, a still quite limited Scheme -&gt; WASM compiler. Given the current limitations around WASM, this will not help you for the UI-side of your web-app. Would be interesting to see, what you decide to use - having a good Scheme -&gt; JS story for the frontend-side in addition to the various backend options would be a great thing for Scheme... 
&gt; I have plenty of theories for why that's the case but I digress. Could you share your theory?
I was considering the issue from a common extension interface for both desktop/mobile and web point of view. Embedding Lua and thus Fennel in C/C++ code bases is a well documented use case so that was my pick. Nothing against JS-hosted Lisps, they might work for OP too.
I didn't know a lot of these Scheme implementations (no surprise considering the vast number of them) so thanks for the info.
To be fair, my answer was just a guess.. I don't know much about this space.
You are not defining a function, but a variable. Should be `(define (inner-loop))`
And I'd expand into something like (begin (let ((j 1)) (define (inner-loop i) (if (&lt; (+ i j) 10) (begin (display "i:") (display i) (display "\n") (display "j:") (display j) (display "\n") (inner-loop (+ i 1))))) (inner-loop 0)))
great! your suggestion would do the cut! by the way, there's no `when` in scheme .....
Did a quick r/lisp search and found no discussion of Ulubis. I came across it when looking up common lisp libraries to work with Wayland. Also found [cl-wayland](https://github.com/malcolmstill/cl-wayland) which are just bindings for libwayland.
Strange how this thread became a critique of Fukamachi's code. I use his cl-project and prove all the time and they are adequate for me. Don't really care how they are implemented as they do the job, simple and easy enough to use. Most of the time if the library does its job I don't need to look into the sources. He is doing some useful stuff and if you don't like some parts - hey, either don't use or contribute, its all open source. After all a lot of people in CL community used to roll out their own stuff all the time so there is a plenty of choices.
Interesting, thanks; I also had a look at the video; Corman used to be a jazz pianist before; amazing; he used his Lisp implementation himself to generate jazz music; I wonder whether one can hear samples of it somewhere.
&gt; The most recent version is SBCL 1.4.15, released December 04, 2018 (release notes). &gt; December 04, 2018 Is that date correct?
\&gt; hey, either don't use or contribute, I do use Fukamachi-ware \_and\_ have contributed to it. prove in particular is one of my favorite CL test frameworks along with fiasco. And I'm not sure why so many people chose fiveam, whose code horrible and makes SBCL cons so much memory that it may run OOM, other than by inertia. But \[tried\] to critique the coding style not the \_person\_. on code, maybe I shouldn't have mentioned Fukamachi by name but given that they are such a prolific author the first example that came to mind was \`fast-http\`. Plus they've embraced the one-package-per-file style which I found to be the anti-thesis of Lisp 'mud-borg' coding style. Weird that you chose my post to reply to, given the parent post is \[border-line at best\] xenophobic. 
No.
Fantastic! Thank you so much, this was a huge milestone!
It's great to see the motivation behind such important work. Everything stated is spot on. An intellectual pursuit that simultaneously helps others is the most fulfilling kind of endeavor.
It was released today.
reminds me of C. Schafmeister, biochemist that migrated away from cpp to lisp and is now writing a llvm backed lisp compiler~
New feature request: Enhance the OPEN function with a keyword option to specify at which time in the past or future should the file be created, so that you could compile new releases into the past.
The main page at http://www.sbcl.org/ should be updated, then.
There is also https://github.com/sdilts/mahogany, which I think uses https://github.com/swaywm/wlroots
&gt; enhancement: added AVX2 instructions on x86-64, which can be used with SB-SIMD-PACK. GJ
Returning a lazy object with a re-bound dynamic variable and then evaluating it later should have the same effect as returning a closure and calling it later. (defvar *my-value* 42) (defvar *my-dynamic-scope-closure* (let ((*my-value* 1)) (lambda () *my-value*))) (funcall *my-dynamic-scope-closure*) Lo and behold it does. Dynamic scope is dynamic. 
Thank you for spreading the good stuff!
Thank you!
Thank you for your kind words!
Just so you know, `when` is a pretty simple macro that you can implement in terms of `if` (or `cond`, if you so wish). If you know macro syntax, you should be able to implement it as it's simpler than this. 
&gt; That will mark a special time in McCLIM development. Next release will be 1.0.0 what is a significant number. The idea is to devote this time explicitly for testing, cleanup and documentation with a feature freeze (i.e no new functionality will be added). +1
Does SDL2 suit you?
What about [https://skia.org/](https://skia.org/)?
Why was the beagle backend cancelled? Most of what I see here I consider complete waste of time since it's work related to X11 and of no relevance to systems people actually use (Windows, macOS).
Does the bounty for Windows support also include hazard pay? 
Nobody has worked on it, if you decide to complete Beagle backend such contribution will be appreciated and merged. Until then it is just dead code baggage. Sources are still part of GIT history. As of "complete waste of time" thingie – you didn't waste any time on it, so we are all square. Regarding my own time, I'll decide by myself what is worth working on and what is not.
No, bounties come with completed and accepted solutions.
Do I at least get a hazmat suit to prevent toxic radiation when booting Windows?
You'll get 500 bucks for it.
ah, that's what you mean :-)
Which is why McClim has been and will continue to be down the toilet. You can only ignore reality for so long before ppl call you out for being delusional.
My reality is not your reality. To be honest I'm very thankful for that. If you will decide to contribute to the project I'm happy to cooperate by providing peer review and guidance (or, if you prove to be a good programmer and leader – help you out under your guidance) – until then let's live with our own delusions.
Hello! [trivial-gamekit](https://github.com/borodust/trivial-gamekit) might be of help. If you will have any issues with it, plz, let me know and we probably can figure out smth together :) We also have a community dedicated for making games in lisp - [#lispgames](http://lispgames.org). You are very welcome! If you need help, just hop into #lispgames channel at freenode.net and we will try to help you with your endeavor.
Thank you.
I wish I had your level of professionalism and patience. You just held a master class on how to handle jackasses. Nice work. And thanks for the work on McClim.
Hmm... I'm not sure why it doesn't build, but it sounds like maybe when you installed things, you didn't put them where it expects. The armcl file should have come with your download and should end up in /usr/local/src/ccl/. If you don't have permissions to put it there, you need to set an environment variable CCL\_DEFAULT\_DIRECTORY to the directory where it can find the armcl file. I haven't tried building CCL on arm in quite some time, but I am running CCL 1.11-r16702M on my (second gen) Raspberry Pi.
There are many! Xelf (xelf.me), CL-SDL2 (github), sketch (github) and others posted in this thread.
That is the document from my web page above, and I had been calling the dialect 'Skim', but MakerLisp, which I developed for this project, has nothing to do with that github project, except that apparently we both thought 'Skim' was a cute name for a subset of Scheme. I developed this software from scratch over the last several years, first on Linux. In April of this year, when I started the machine project in earnest, I started targeting and tailoring this Lisp for the machine I was developing - although I always had a machine like this in mind, and the that's echoed in the design decisions of the JIT and threaded code execution model inside the evaluator. The software has never yet been published, the source will be posted and the software will carry the MIT license, when I deliver the first hardware. I've shown the machine and software at a few functional programming user groups and meetups, a couple of trade shows and at the San Diego Maker Faire, but it hasn't really gotten out there yet. The launch of this machine will be its debut. In the matter of "How real of a Lisp is it ?" in case you're wondering - in terms of Lisp language features, it's somewhere in between TinyScheme and FemtoLisp, certainly not quite an R4RS Scheme. In terms of the speed of execution, it's on a par with FemtoLisp, although this code and FemtoLisp take very different approaches in getting there. I made a different dialect for two reasons: 1) I'm not trying to be compatible with other things, I'm trying to bring Lisp to new places, as others are, too. Other small Lisp implementations are also not exactly like any other 'standard' Lisp. It's the Lisp approach, to write new programs, for a new machine, that is the leverage here - not to port Lisp programs from your PC to a machine that was not intended to run those programs. Think of this as a small embedded machine, bigger than Arduino, smaller than Raspberry Pi - now, what language should we have ? We could have Basic. We could have a stripped-down Python. Instead, I chose to make a Lisp that was designed for this kind of machine. This is the kind of 'Lisp Machine' I've wanted to make for years, to do embedded micro-control tasks, better, with Lisp. 2) I wanted to be free to do what I thought was best, and not to be saddled with compatibility issues, which are a moving target, and moving in a direction that this machine has nothing to do with. Given that we have small, inexpensive machines built for certain tasks, how can we program them better ? I don't care about making something that's like something else, all I care about is making something that is good, and practically useful. One thing that Lisp enthusiasts might find appealing is that there is a very small core language, and almost everything else is implemented in macros, and in a way that doesn't take up much space. These are real, Common-Lisp style, low-level macros, very powerful (backquote, unquote, etc.). There are also acouple of neat tricks that augment a Cheney copying collector. In a little less than 5000 lines of C89/C90 style C. It might be a fun read for some people. So, as a final request, don't pre-judge the effort before I get a chance to get it out there to you. I'm working day and night, literally, to finish this and launch the campaign. I hope some of you find it interesting. Please feel free to corespond, either here, or sign up for updates on the Crowd Supply page, or contact me directly - my email address and phone number are on my web site at [http://cpmaker.com](http://cpmaker.com). Thanks for your time ! Luther Johnson
Thanks, I'm suspecting it's more related to the difference between generation architectures (CPU, floating point perhaps), in that building on my rpi3 and copying over doesn't work either. In this case, I'm testing prior to installing, since you can run the provided armcl binary, which isn't dynamic and therefore dependent on shared libraries. I don't know that the kernel should matter, though should probably try it out on a basic Raspian install too just to be sure.
these ? https://github.com/CodyReichert/awesome-cl#graphics
then you should help by writing blog posts, example projects, improving the good projects, documentation which are often shit (your words), curating the awesome-cl list, etc, because saying this on reddit doesn't help unfortunately. in a sense it's awesome and positive what he accomplished, knowing he was not put off by the language or the plateform could be seen as positive...
It seems original post hit too close to the truth, so the response is over emotional and is all over the place. Basically, it can be summed in "yeah, developing games in Lisp is a niche and difficult activity, so use other tools" and that statement agrees with the original post. ¯\\\_(ツ)\_/¯
Interesting project! I hope it does well, having good a Windows UX will hopefully add some visibility to Common Lisp.
Like dating, it's a numbers game. And, Windows has the numbers. &amp;#x200B; Hopefully, Windows users will find the Corman Common Lisp IDE's UI somewhat more familiar and less distracting/intimidating than the more powerful (flexible/feature-rich) Portacle IDE. The goal is to make Windows newcomers to Common Lisp feel comfortable in investing more time learning Common Lisp rather than feeling that they need to also invest competing time in simultaneously learning more about the IDE. &amp;#x200B; Portacle will be there for them should they feel the need for more power. But, by the time they've gained the experience to decide that, they'll have likely given Common Lisp a fair attempt.
&gt; So, as a final request, don't pre-judge the effort before I get a chance to get it out there to you. fair enough. i'm pretty sure most everyone in this sub is interested in your project!
Well said!
Have you seen this? https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/lang/lisp/code/parsing/time/
I haven't, thanks for sharing. I didn't know CMUCL had this functionality built-in.
Very informative blog post (thanks), nice library, a blog active again after four years, and I discover a professional, good looking web service with a cli interface, built in CL. Fantastic ! I'm looking forward from reading about the web api :)
Thank you!
Woah! Nice bounties!!
This is not part of CMUCL. This is from a large repository of CL code written in the late 80s and early 90s placed in the public domain by CMU
Oh I ran EXT:PARSE-TIME in CMUCL and it worked. So I assumed it must be this. 
It is from CMUCL. What u/arvid mentions is true of all the code in that repository. But in this case the code \_was\_ written for CMUCL according to its header and it also ships with CMUCL currently.
Doesn't seem to handle "Midday at TheFfeast of St. Crispin's".
Does anyone have any idea how difficult it would be to add, say, a NanoVG backend? It seems to me that Windows performance of McCLIM over CLX is rather dismal (I get something like 7.5x faster Drawing-Benchmark figures on my Linux desktop compared to my Windows laptop, and I don't think that the 80% clock advantage of my desktop can account for all of that).
It will be rather painful because we don't have documentation on how to implement backends (with plans to have it though things tend to take longer than anticipated). For a person who knows interfaces writing such backend should be a moderately difficult task (where most of difficulty would come from potential inconsistencies between CLIM processing model and said toolkit model). For an alpha quality Windows backend please see: https://github.com/ailisp/mcclim-graphic-forms We have a $500 ticket for bringing it to beta quality. Original author is a very kind person who may provide more insight, as far as I remember he is OK with others taking the task and claiming bounty. https://github.com/McCLIM/McCLIM/issues/296
&gt; For an alpha quality Windows backend please see: https://github.com/ailisp/mcclim-graphic-forms Oooh, very interesting! I don't think I could pursue any of the bounties right now but I will definitely look into trying to do whatever I could in the limited time available to me. I could definitely need some UI option for some moderately complex applications.
Speaking of adding documentation for writing backend I have a pipe dream to do it while writing a backend for console. That will help us to identify code parts which work under invalid assumption that when we say 1 we mean one pixel. For more context please read https://common-lisp.net/project/mcclim/posts/Sheets-as-ideal-forms.html.
Finally an excellent way to spend my vacations. Fantastic news!!
Yeah, and it took \*much\* more time than planned.
Ha-ha! I hope you will enjoy your vacations!
Thanks! To be fair, good Windows UX is a thing which the project currently lacks, although it has an IDE :)
Backend for console? Holy moly, that would be amazing. Perhaps one day we can fund iOS and Android backends! Would be neat to be able to target many different environments (and challenging). Anyway, just wanted to give a shout out to your brainstorm. 
I want to emphasize that the purpose of this project would be discovering invalid assumptions and documenting writing backends mostly, hence I wouldn't count on great code quality or maintenance even if I write such thing one day. Of course, it is not very unlikely that someone will pick up such project and maintain it on their own stead.
Got it. i just like the thought of it really. 
The major difference is that the first example passes the script on STDIN, whereas the second script passes it as a filename. You can recreate the second example using: ``` sbcl --script =(cat &lt;&lt;EOF , This is an sbcl error to test the behavior of sbcl. EOF ) ```
I tried that but it didn't like it: &amp;#x200B; `#!/bin/bash` `sbcl --script =(cat &lt;&lt;EOF` `, This is an SBCL error to test the behavior of SBCL.` `EOF` `)` &amp;#x200B; &amp;#x200B; Result: &amp;#x200B; `line 2: syntax error near unexpected token \`('` `line 2: \`sbcl --script =(cat &lt;&lt;EOF'`
In this version I got rid of the sbcl error and displayed the argv: &amp;#x200B; `#!/usr/local/bin/sbcl --script` `(format t "[~a]~%" sb-ext:*posix-argv*)` `; , This is an SBCL error to test the behavior of SBCL.` &amp;#x200B; The result was: &amp;#x200B; `[(/usr/local/bin/sbcl)]` &amp;#x200B;
Hi! \&gt; it implies it's using other information telling it how it was invoked. Yeah, that information is the filename. In the second example, sbcl is in the shebang line and it receives the filename. In the second example the OS appends the filename to the shebang line and executes: /usr/local/bin/sbcl --script &lt;your file&gt; SBCL is prepared to ignore the shebang line that starts with #! when reading a file. For more information see: [https://en.wikipedia.org/wiki/Shebang\_(Unix)](https://en.wikipedia.org/wiki/Shebang_(Unix)) If you want the first example to behave as the second, you can saving the contents of the string to a file and passing that filename to SBCL. Like this: #!/bin/bash SOURCE=$(cat &lt;&lt;EOF (format t "Hey There! My ARGV0 is $0 ~% ~% ~%") , This is an sbcl error to test the behavior of sbcl. EOF ) TMPFILENAME=$(mktemp --suffix=.lisp) echo "$SOURCE" &gt; $TMPFILENAME sbcl --script $TMPFILENAME rm $TMPFILENAME Beware of shell expansion, as I show in this example code, the $0 is expanded, and so will any other shell var that makes sense. &amp;#x200B; Good luck!
SBCL ignores STREAM-ERRORS trying to silently exit when stdin/stdout are closed. But it ignores too much, because READER-ERROR is a STREAM-ERROR. I pushed a change that doesn't ignore reader errors.
That's probably because process substitution is spelled `&lt;(command ....)` for a source of input, and `&gt;(command ...)` for an output sink. The only hit I can find in the Bash man page for `=(` is for array assignment: `x=(a b c)`. 
Bash process substitution: $ diff -u &lt;(echo foo) &lt;(echo bar) --- /dev/fd/63 2019-01-03 10:20:12.274656283 -0800 +++ /dev/fd/62 2019-01-03 10:20:12.274656283 -0800 @@ -1 +1 @@ -foo +bar 
 (define (compute-result) (/ (n count)) (+ (d count) result)) this is two forms, not one. If you indent it right you will see it.
Thanks a lot! Goddamn, those parentheses are gonna kill me. I am starting to appreciate lisp's syntax though. I should probably look into one of those emacs packages that help you with indentation and parentheses...
If you just have that code in a buffer with scheme-mode (something.scm) pressing C-M-q before `(define ...` will indent it properly, no packages required.
Wow, thanks for the tip, very convenient.
Thanks. Also note the update comment at the top of the original post, for my workaround and a couple of incidental strange behaviors that don't matter much.
&gt; those parentheses are gonna kill me I think this is the last step before saying "thank God for those parentheses" :) It comes eventually, after which the structural benefits outweigh the initial shift in viewpoint. 
Either that or use aggressive-indent. It is very handy with most Lisp modes as it automatically indents all code you write. I use it constantly in Common Lisp modes.
[paredit-mode](http://emacsrocks.com/e14.html) 
Would you recommend this over smart-parens? What I like about smart-parens is that it works for a lot more languages other than lisp.
Holy crap, that may save me hundreds of hours of my life. Do you use this [one](https://github.com/Malabarba/aggressive-indent-modehttps://github.com/Malabarba/aggressive-indent-mode) or do you use a specific package for the language?
I'm actually starting to appreciate it though. It's so easy to remember constructs and things like that because everything is just list of elements between parentheses. But from what I've heard (I'm still not there on SICP), the best part about it is when treating procedures as data and other advanced topics, isn't it?
Doesn't work, it needs .lisp extension, and BASH process substitution doesn't provide that. SBCL thinks it is an empty FASL file or something: sbcl --script &lt;(echo "(princ 123)") Prints: Unhandled SIMPLE-ERROR in thread #&lt;SB-THREAD:THREAD "main thread" RUNNING {1001E06533}&gt;: attempt to load an empty FASL file: "/dev/fd/63" The file has contents though: cat &lt;(echo "(princ 123)") prints: (princ 123)
Makes no difference, it’s about the idea not the package. Smart parens is fine. 
You know, I actually work almost exclusively in parenthesized languages, so I can't speak to the pros / cons of either for the world outside.
Chapter 4 will be really enlightening I think (implementing eval in scheme - code is data, an interpreter is just a function that takes lists as programs...hey we already have a nice environment for working with lists, which also happen to be programs....). Chapter 3 has some cool stuff exploiting how far you can go with just lists (beyond algorithmic math problems with pure functions, into stateful things and even rudimentary objects - things we're typically taught we need extra syntax and special rules for). For me, I really like the relative simplicity of representation (which is generally portable across dialects too). You can boil things down to a few simple rules for representing and evaluating expressions, which are codified as lists. That's a lot of bang for the conceptual buck. I initially hated not having infix operations, particularly for math, but then came to love it (as well as going through the age-old ritual of defining infix math macros, then tossing them out). There's a lot of leverage in the simple ideas presented, and the SICP authors do an excellent job conveying things. Just stick with it :) Also, leverage a good parenthesis-balancing editor (or an editor supporting structural editing like paredit, parinfer, etc.).
This creates a pipe which is not seekable. I made it never treat such streams as FASLs. Now can you all stop poking holes in this.
It tried to seek through a pipe. Should be working as well now.
Would it be possible to use just the command processing part in a text interface?
Ok, so I no longer have to take that into account, and no longer even need to use the process substitution. I posted some brief demos today of using SBCL with Gtk without downloading anything as long as SBCL and Gtk are both already installed. Those demos use the "&lt;&lt; 'EOF" so their error output can be piped through sed to make it less verbose, but no longer use the process substitution. The demos are at [https://github.com/mifpasoti/Gtk-Demos](https://github.com/mifpasoti/Gtk-Demos)
[removed]
aggressive-indent-mode works independently of the language you use.
If you mean 'Common Lisp' (there are actually many different Lisps), i don't have a project suggestion, but i do recommend that you check out the book "[Practical Common Lisp](http://www.gigamonkeys.com/book/)", if you've not already done so; it's free to download.
First of all, don't expect fireworks - Lisp is just another programming language that is equivalent to all others by means of Turing completeness. It's just that it's very programmable itself due to the way it is constructed, in which different smaller decisions synergize to allow a lot of bendiness wherever the programmer needs it. I think that you could try working on PCL first and try creating the example projects the author describes there: http://www.gigamonkeys.com/book/ (Also linking the other part of the beginner toolkit: https://portacle.github.io/ - a preconfigured Lisp environment)
yes. in fact this is already supported via null backend (which does no output at all) - you'd use in it `execute-frame-command`. another thing: while this won't work as desired right now, text-only backend would help us polish textual-view (which operates on one of mentioned assumptions). Presentation methods `present` specialized on `textual-view` use common stream primitives like print, princ, prin1 and write-string. I doubt though if presentations will work without extra work on "ordinary" streams.
Try writing a chatbot that can recognize sentences by actual parsing instead of matching. I have done that once and it's quite interesting to see such a thing in action 😊
Here is an update: [https://github.com/mifpasoti/Gtk-Demos](https://github.com/mifpasoti/Gtk-Demos)
Looks really promising, can't wait to go further into it. I'm actually using emacs so I guess I'll install one of those packages.
Thanks a lot for the resource! It's incredible the community that has gathered around this one single book, the internet is full of help.
Clojure is quite opinionated in this regard. Common Lisp on the other hand gives you a toolbox which you can use however you want to. So in my unerstanding Clojure is not very "Lispy". To put it in a funny way, Clojure is not like a ball of mud[1]. [1] https://en.wikipedia.org/wiki/Big_ball_of_mud#In_relation_to_Lisp
Recently I learned that in Clojure + doesn't handle bignums anymore, and you need to use +'. That kind of innovation made me go from "I might some day use Clojure for something" to "this is bonkers".
Great stuff!
From what I see, Clojure does not really innovate by creating new things; rather, it innovates by removing them. It limits the amount of programming paradigms and methods available e.g. in CL, in order to create a "better" language. The "better" is, of course, according to its authors' opinion - hence the opinionated aspect of the language. I don't share that opinion myself. I like the huge amount of tools Common Lisp offers me.
since when ? that's so damn odd
I'm not even sure Hickey ever wanted to innovate. He wanted lisp applicative+syntactical simplicity + ~performant immutability by default. It's more an infrastructural change. Maybe the concurrent primitives in core are novel (but I'm not even sure), the view of time as a stream of values etc etc.
Since 1.3, apparently.
I'm no Common Lisp expert, but to place Clojure in the history of lisps you'll want to watch Rich's Clojure for Lisp Programmers talk ([part one](https://www.youtube.com/watch?v=cPNkH-7PRTk), [part two](https://www.youtube.com/watch?v=7mbcYxHO0nM)), which goes deep into why he, as someone working in C and Java and Common Lisp but unable to get clients to accept Common Lisp, created Clojure. (This [expert-to-expert interview](https://www.youtube.com/watch?v=wASCH_gPnDw) may also be interesting, but it is more general than the lisp-specific one above.) My impression from the Clojure side of things is that Clojure's inception was a chance to standardize the language core, for both functions and data structures. Putting the sequence and collection abstractions at the center of the language instead of cons cells was one of its most significant innovations. I'd be interested to hear more about how "functions and data" seem limiting to you.
I'm thinking this has to do with how Java deals with primitives and + being switched over from boxed values to return primitives by default. It's nice to have this solved neatly in CL, but admittedly Clojure comes with the advantages/burdens of the host platform (no integers in CLJS for example). Overflows are still checked by + (by default), so I guess it isn't that bad. I wonder how ABCL deals with unboxed math on the JVM (if at all)...
AFAIK Clojure is the Python-ish lisp
It was controversial when introduced. Rich justified it by saying he wanted arithmetic to be fast by default. It’s a practicality trade-off, like almost every other feature Clojure removed from CL.
 user=&gt; 123 123 user=&gt; (type 123) java.lang.Long user=&gt; 123N 123N user=&gt; (type 123N) clojure.lang.BigInt user=&gt; (biginteger 123) 123 user=&gt; (type (biginteger 123)) java.math.BigInteger So Clojure has three different integer types, where two have the same external representation... 
+ does "handle" bignums, it just doesn't auto promote from fixed precision to arbitrary precision. This allows you to have known performance expectations when using arithmetic ops so you don't accidentally auto-promote from fixed precision into arbitrary precision with corresponding drop in perf. This makes the common case (staying in fixed precision) faster, and the auto-promoting behavior opt-in via `+'`. It's been this way for many years and in practice I find it to be an excellent compromise.
Having + and +' appears to be less practical to me. And I assume Clojure is still type safe and addition doesn't just wrap around. So the cost of checking the argument types and result types would be the same as dispatching to bignum routines/allocating a bignum.
Really cool stuff!
I think it's instructive to look at the first things Rich worked on when deciding whether Clojure was viable - acceptably performing immutable persistent collections and software transactional memory. The first is undoubtedly an enabler of many other parts of Clojure (STM is an obvious example, but there are many others). STM has interestingly turned out to be something that most Clojure programmers rarely use. That was not obvious in the early days - it has been a surprise how far you can get with just immutable collections and (independent) atoms for state, without coordinated state mechanisms. There are other parts that feel like new ground, even when based in other work, like transducers or the direction that spec is going. But Rich has never been a big fan of innovation for the sake of novelty. He's far more interested in trying to see and address the root causes of complexity in software development. Most ideas in Clojure (or Datomic) are not new, but I find they are new in combination, and the whole is imo greater than the sum of the parts.
So the same integer can have different types. Turns out, an operation on two BigInt will result in another BigInt even if it fits into Long. Now that's the source of the silliness, and not +' itself.
So I see the problem is not with auto promotion, but with the lack of auto demotion. And instead of fixing that they had tacked on another entity…
It's fucking awful; I wouldn't sign my name to anything of the kind.
So, since I wrote a Lisp dialect using C, `(+ ...)` should work like C. 
It’s been 10 years and my memory is hazy, but I distinctly remember that performance was the reason, and it was a significant win. Whatever happens is (or was) demonstrably faster. Maybe the JVM handles checks at a lower level.
&gt; known performance expectations -&gt; I get an error... 
I watched all of the linked videos, but I should probably do it again. I am just an average Joe programmer working on web apps in Scala, so "functions and data" are not at all limiting for my purposes. I can imagine that Clojure might be very useful for the kind of systems I am working on. What I was really trying to say is that given the history of Lisp and all of the dozens of dialects, over many decades, and a lot of invention and innovation along the way, Clojure probably cannot be considered a major thing. Both Common Lisp and Scheme were products of fundamental language research. I would say that Clojure isn't, so maybe the comparison is not fair, but if it draws so much attention from people interested in dynamic languages, then I would say that there was a serious lack of that kind of research in the last 30+ years. In that context, and for people trying to find novel ways to handle complexity, "functions and data" is very limiting. I recently looked at what Alan Kay and his group were working on in STEPS project, and their work has nothing to do with "functions and data". So, it seems like people adopting new ideas in dynamic languages in the 80s were just 10 years behind major research ideas, but today we are several decades behind. What happened?
Why are you asking this in r/lisp? I think if you want genuine discussion you should be asking language designers of languages you consider complex (to gain insight from their opposing point of view), rather than preaching to the choir so to speak. Perhaps try r/ProgrammingLanguages?
&gt; So, it seems like people adopting new ideas in dynamic languages in the 80s were just 10 years behind major research ideas, but today we are several decades behind. What happened? What happens is that, in the words of Alan Kay, "programming is pop culture." 
Lisp can have a lot of syntax too. Take macros, you can have arbitrary syntaxes, which maybe be not easy to grasp. About the only place where lisp is simpler syntax-wise is the arithmetic operations and function calls have the same syntax.
All that syntax does have associated semantics with it. When those designers invent new semantics, they are forced to invent syntax for it, because they have chosen not to work with a notation that expresses every possible syntax tree already with a generic, compact notation. E.g. C++ has lots of syntax and keeps getting more; the addition of that syntax is demonstrably necessary within the paradigm of language extension that C++ was shackled to decades ago. Syntax sells languages, especially to neophytes. Syntax is visible and complex; it basically *is* semantics to the naive. Imagine that you've never programmed before and suddenly you're learning some complicated syntax with lots of rules. A year later, you're able to sit down at a blank editor and spit this stuff out without looking at a reference! Man, do you feel smart! Look at that, your `int (*fnarray[3])(char *) = { foo, bar, baz };` compiled the first time! Wow, genius! Soon you will be able to use `cat &gt; hello.c` as your editor!
&gt;In other words, Common Lisp community went so far as e.g. creating MOP in order to handle complexity, but Clojure does not seem to have equivalent ambitions. The difference is that in Common Lisp you reduce complexity by careful use of CLOS, MOP, loop, reader macros, runtime compilation, logic programming, imperative, functional, etc. While in Clojure, the creator (and his loyal, devoted followers) believes complexity can be reduced by removing features and forcing users to a particular paradigm: immutable data, pure functional. In my personal opinion after a couple of decades writing software, the one-way-to-do-it philosophy invariably ends up in needessly complicated code, given enough program size or program features. 
&gt;I would say that Clojure isn't [a product of fundamental language research.] Would you count the Bagwell tries that constitute the performant foundation of Clojure's immutible data structures? Those were fundamental research, albeit in data structures rather than PL design. Again, the strongest thing Clojure did in this particular context of lisp history is press the (arguably much-needed) reset button to jettison outdated cruft (like cons cells) and multiplication (like the proliferation of map functions) and move forward with new abstractions (`seq`/`coll`) and philosophy (immutible data structures, which enable a pragmatic FP-by-default stance, firmly hosted on JVM &amp; JS). That's not nothing. A consolidation can be helpful after a period of proliferation across a wide space. That means Clojure got to benefit from the many decades of research in other lisp dialects, both when originally designing its core API and more recently with contracts based on Racket. &gt;What I was really trying to say is that given the history of Lisp and all of the dozens of dialects, over many decades, and a lot of invention and innovation along the way, Clojure probably cannot be considered a major thing. Maybe it would help to turn this around. What *would* cause it to qualify it as a major thing?
That’s very uncharitable, clojure makes certain things the default , but doesn’t make the other things impossible 
&gt;That’s very uncharitable Yes, probably as uncharitable as the Clojurians who like to say Lisp (Common Lisp) is "outdated" and that clojure is "The Modern Lisp" etc. I have no charity for them anymore. &gt;but doesn’t make the other things impossible So how do I do reader macros that dispatch on any character i like? How do I do macros that don't qualify the symbol name with the package? How do I save the running image to disk? etc 
I’ve never heard anyone saying that, and I’m pretty active in that community. Your examples are fair, those things are by and large impossible (except the macro thing). But I was taking about functional programming and immutable data, because that is the context in which you talked about forcing users into one specific way of programming. 
&gt;I’ve never heard anyone saying that, and I’m pretty active in that community. I'll have to start an anthology of them, then. Note, I have no problem with people using Clojure, in fact, if forced to use a lot of JVM libraries, i'd probably use it as well (ABCL development has slowed down a little). My only issue is when some clojurians claim outdatedness of CL, when many (all?) the things there in Clojure are already available in Common Lisp by loading the suitable library. &gt;Your examples are fair, those things are by and large impossible (except the macro thing). But I was taking about functional programming and immutable data, because that is the context in which you talked about forcing users into one specific way of programming. Thanks for the courtesy. I'll let other lispers bring their example. 
I don't think clojure should be considered a Lisp, more like a bastard amalgamation of ideas that help Hickey peddle a Java masquerade to idiots who don't know any better. This veneer of freshness and modernity that clojure droids like to bring up collapses the moment one takes a deeper look into this abomination.
The numbers used here are both longs and will use primitive fixed precision math in the JVM. In the case of overflow, there are 3 options: 1) error (fast, safe) 2) overflow (fast, unsafe) 3) promote (slow, safe) Clojure supports all 3: user=&gt; (+ 9223369933333333333 4000000000000) Execution error (ArithmeticException) at user/eval5 (REPL:1). integer overflow user=&gt; (unchecked-add 9223369933333333333 4000000000000) -9223370140376218283 user=&gt; (+' 9223369933333333333 4000000000000) 9223373933333333333N 
What does 'slow' mean. What is slow?
&gt; I recently looked at what Alan Kay and his group were working on in STEPS project, and their work has nothing to do with "functions and data". A lot of the advancements they realized seemed to focus on compressing complexity without losing expressiveness via DSLs and other little languages. That's been one of Kay's foundational points across presentations on Small Talk and related areas: tailor (specialize) your expression of a solution to the problem to the domain (in SmallTalk expressed via the universe of messages used for communication, which is extensible). Lisp dialects converged on many of the same ideas IMO, via metaprogramming similar low-barrier eDSLs (and things like CLOS). Much of Clojure's focus is on "functions and data" from the FP sense, but there's also the lisp heritage of data = code, leading to functions that modify data -&gt; code rewriting (via macros). Some of the innovation rests in convenience and again aesthetic (folks have opined that the intentional exclusion of an official reader macro system is limiting). &gt;So, it seems like people adopting new ideas in dynamic languages in the 80s were just 10 years behind major research ideas, but today we are several decades behind. What happened? Maybe everything good had already been discovered (hell, one could run that argument back to the late 50's, or browse through the Lambda The Ultimate papers)? The intervening decades were spent figuring out how to make dynamic languages performant, portable, unified (in the case of CL). From a pessimistic view, what innovations have "any" dynamic languages had over the products of the 60's-80's? Python, Ruby are the popular ones that come to mind. Outside of aesthetic and the opinion of the designers, it's hard for me to quantify the inherent innovation, but the broad adoption is what put them on the map. I think the current frontier in dynlangs is trying to incorporate lessons from the static, strongly typed community without sacrificing inherent development agility. I found the safe/useful chart that Simon Peyton Jones provides a decent means to reason about various languages. Clojure's offering (spec) pushes the typing information more into the data validation realm, leveraging contracts and gradual typing (again using functions and data). Erlang, Racket, and other dynamic languages have similar efforts to try to rest at a point on the pareto frontier for useful vs. safe; the primary feature being that these are largely just libraries vs. language features baked into the design. Perhaps the centrality of immutability to the language is innovative (relative to also finding novel and efficient implementations of Bagwell's ideas for persistent vectors, maps, and sets...which are doubly exploited again to address concurrency/parallelism challenges through STM) for a dynamic language.
&gt; Those were fundamental research, albeit in data structures rather than PL design It's interesting to me that the datastructure was necessary to facilitate a practical PL design (in this case, embedding first-class support for data literals, and using them to convey meaning in things like args and bindings). They seem interdependent in practice. 
It's a little hard to benchmark well given the level of optimization involved in the JVM, but when the JIT is warm, primitive, checked addition (Clojure's `+` default) is about 0.33 ms per million adds. By comparison, the overflowing `+'` is something like 30 ms per million adds. Of course, if you're doing a million adds, there's probably a better way to do it these days and Clojure has several libraries to tap into native libs, GPUs, etc.
You are right. I might ask there. But, I am asking here because I am guessing that many Lispers have benefited hugely by taking advantage of syntactic abstraction which the language gives them easy access to. From my experience, many quite competent programmers from other language communities are not even aware of these possibilities, so I think that they just get normalized to that. I do not exclude myself from that group. But, when one makes few steps back, it's pretty crazy. Actually, I was reminded by a series of great [panels](https://www.youtube.com/watch?v=SjbtEnfm7_Q) that took place at MIT almost 20 years ago. One of the conclusions there was: "macros are a major win". Also there is Rainer Joswig's great [video](https://vimeo.com/77280671), which is a great way to sell the idea to "syntax freaks".
&gt;&gt;forcing users to a particular paradigm: immutable data, pure functional. &gt;That’s very uncharitable Describing it as uncharitable is too charitable. u/defunkydrummer s statement is re-heated, freshly served bullshit :) There's nothing "forcing" immutability, or FP. Alternate paradigms are entirely possible - if not encouraged in some domains - including dirty, mutable, imperative CL-style programming. We got it all - OOP, actors, CSP, FP, imperative, etc. That won't stop the FUD buddies though. 
Macros don't introduce new syntax of the kind that this topic is about. They introduce tree shapes, which are written with a print syntax that the macro writer and user already know. Macros often don't introduce tree shapes that are **new**, either; they introduce a new symbol, plus some set of shapes that often look like the shapes of some other macro. The syntax is never hard to grasp: you always know at a glance things like that the macro has three major constituents, the middle of which has three children of which symbol `x` is the third one. The semantics may not be easy to grasp. If you know absolutely nothing about OOP, can you "grasp" what `defclass` is doing? 
&gt;outdated cruft (like cons cells) ?!
Addition of what? I think it's useless to compare the fast non-overflowing add with a slow overflowing add. What interests me as a user: non-overflowing addition operation of potentially overflowing numbers with non-overflowing addition operation of non-overflowing numbers. 
I don’t think it’s wrong to say that clojure pushes you to a specific way of programming and makes you be very explicit if you want to opt out of those defaults. I don’t think that’s a bad thing and it helps our ecosystem, but we should acknowledge it. 
&gt; wrong to say that clojure pushes you to a specific way of programming OP didn't say push. Force. Clojure does nothing of the sort, FUD person knows better.
"Potentially overflowing numbers" is not meaningful - integers are either fixed precision sized or not. It's the operation that causes overflow. I guess maybe you mean arbitrary precision numbers? On fixed precision integers for 1 million ops: - checked addition (default) - 0.33 ms - unchecked addition - 0.30 ms - overflowing addition (w/o overflow) - 0.33 ms - overflowing addition (w/ overflow) - 30 ms On arbitrary precision integers for 1 million ops: - addition - 18 ms
&gt; "Potentially overflowing numbers" is not meaningful - integers are either fixed precision sized or not. In Common Lisp INTEGER is a *type* which has FIXNUM and BIGNUM as a subtypes. Operations are usually specified on types NUMBER or INTEGER. We can now operations (arguments and return values), variables and data structures to be of these types. &gt; checked addition (default) - 0.33 ms &gt; unchecked addition - 0.30 ms &gt; overflowing addition (w/o overflow) - 0.33 ms That seems to indicate that the confusing 'optimization' with Clojure numeric types is no longer relevant. 
u/defunkydrummer , u/pxpy mentioned these were fair points, but I'm less convinced. &gt;So how do I do reader macros that dispatch on any character i like? been over that [before](https://github.com/joinr/reader-macros) which is feasible but no one cares beyond extant reader literals provided by EDN. &gt;How do I do macros that don't qualify the symbol name with the package? [intermediate gensyms](https://stackoverflow.com/questions/33333136/defining-a-function-in-a-macro-cant-use-qualified-name-as-parameter) which could be munged into a macro if it's useful. Or write your own macro that identifies intentionally unqualified symbols with the appropriate behavior and walk the body (either with clojure.walk or a more sophisticated external lib like riddley). I would cop the existing work done by [backtick](https://github.com/brandonbloom/backtick/blob/master/src/backtick.clj) if that's important enough, since syntax-quote is the typical culprit for creating namespace (not package) qualified symbols. Or construct lists manually and don't use qualified symbols. Options abound for what appears to be a non-problem. &gt;How do I save the running image to disk? The same way you do (or don't) in ABCL....or any other feature that's unspecified. Closest we can get with Clojure is vis CLJS and nexe, which lumo uses for its cljs image-dump. The future may be bright with graal, but not there yet. Default jvm-based stuff aot-compiles to bytecode (minimal benefits here) and bundles as an uberjar. There are optional ways to bundle a specific JVM with the jar as an executable for stand-alone deployment.
You'll have to admit that you have a bit more leeway with how you implement something using C :-)
I'm not sure what the difference between clojure.lang.BigInt and java.math.BigInteger is or how these should be used idiomatically. Any ideas?
comp.lang.lisp needs you
Just adding support / internet encouragement to offset the haters. I think the work your doing is outstanding. You are have a winner's outlook.
I'm always surprised when I see so much hate. Why not just have a proper discussion?
Go away.
I thought that was [Hy](https://github.com/hylang/hy)
&gt; https://github.com/joinr/reader-macros TIL! Cool package!
I just can't get into Clojure. It seems like a giant step backwards from CL. Sure it's immutable by default, and it's dubious that that was a good thing anyway (since you can immutable in CL too). The numeric tower of Clojure I find incomprehensible, but I think it has something to do with Java?? OK I think limitations by the platform are bummer, but still... There's Clojure's STM which no-one uses, but there's CL-STMX which uses Intel's hardware STM, so to make it fast, I'll probably have to wait for JEP 100,000,000?? Finally, there are the error messages. In Clojure, error messages make me want to kill myself.
I forgot the changes I made were merged [upstream](https://github.com/klutometis/reader-macros), where the actual credit resides.
&gt;I thought that was Hy That's the lispy Python. 
FWIW the error messages have been given a lot of love in the latest release 
&gt; In Clojure, error messages make me want to kill myself. [Things are getting better all the time](http://insideclojure.org/2018/12/17/errors/). I think 1.9, with the addition of spec to the validate the core macros, added a lot of noise beyond the extant stuff. 1.10 looks to do a better job reporting, and providing data structures for analysis/consumption in tooling (like Cider) to improve the experience. 
From my perspective Clojure breaks the model of "you can write code fast, or you can write fast code, or whatever mix you want" that lisp provides (e.g. SBCL). I could not get CJ to run even 1/10 as fast on my workload. Just a total show stopper for me.
how else to show the world who's the real elite lisp hacker?
&gt;I could not get CJ to run even 1/10 as fast on my workload. Just a total show stopper for me. Did you solicit help from the community to explore optimization opportunities? What problem domain and workload were you dealing with?
I put it down to mathematical notation's bad example. Math notation is great if you want to compute something on paper without a bunch of extra writing, plus it's got a 2d layout that's often very readable. Now type it into a computer (rendering point one obsolete), and squish it into a line of ASCII characters (giving up point two entirely). Oops. 
Lisp's syntax is efficient, but it's not what people are used to. They are lazy and don't like change. Therefore, they stick to more familiar things. Most programmers start with a C-like language or maybe BASIC, and they learn infix notation in maths classes. Lisp is doing things differently, and a lot of people don't want to change their way of thinking. Also, it's important to note that a lot of programmers have a "don't care" mentality. They don't look for the best way, they always want to reach their goals fast. Elegant code doesn't matter to them, as long as their code works.
I have not used Clojure professionally, but I welcome it,and applaud the effort that has gone into its creation and development. It's being used to solve real problems for real customers, and supplying jobs for those wanting to work in a Lisp family language. I also have a personal fondness for the immutable data structure work. Is it perfect? Nothing is perfect. I'd rather focus on the wins than lament blemishes.
The implementation of anything which provides a simpler programming paradigm within the JVM ecosystem. Could you have implemented the eval function on a Z1?
Clojure is not a Lisp, so can we not talk about it here?
It's a really big turn off for people that know Clojure and are interested in exploring other lisps to see discussions like this. I started out learning Scheme and AutoLisp. After kind of enjoying programming in AutoLisp professionally I started learning Clojure, Hy, and Emacs Lisp. Now I've been exploring Common Lisp but a lot of the people advocating it like to shit all over the other Lisp dialects I've learned. Makes me want to stay away from CL when people in the community have such a toxic attitude.
Probably this gist collects contains some answers interesting for a CL audience, such as "*Why Clojure does not allow to introduce new syntax like Common Lisp?*" or "*Why Clojure does not implement something like CLOS?*": [https://gist.github.com/reborg/dc8b0c96c397a56668905e2767fd697f](https://gist.github.com/reborg/dc8b0c96c397a56668905e2767fd697f)
Cool, but any of that has to do with Lisp? 
They still look terrible.
Yeah, it sucks when people behave like assholes doesn't it? [https://www.reddit.com/r/programmingcirclejerk/comments/acnmmk/i\_dont\_think\_clojure\_should\_be\_considered\_a\_lisp/ed9fs83/](https://www.reddit.com/r/programmingcirclejerk/comments/acnmmk/i_dont_think_clojure_should_be_considered_a_lisp/ed9fs83/)
I was expecting that Library X wouldn't rise an exception if I didn't pass a BigInt.
Regarding the `loop` that defines the foreign function by generating some code that is `eval`-ed, it would be nicer to make a macro. Something like: (defmacro deffi (&amp;rest gfunc) (nconc (list 'define-alien-routine (car gfunc) (cadr gfunc)) (loop as argname in '(a b c d e f g h i j k l m) as argtype in (cddr gfunc) collect (list argname argtype))))) 
Re: (defmacro window (awin app title wid ht) (eval `(defparameter ,awin nil)) `(progn (setf ,awin (gtk_application_window_new ,app)) (gtk_window_set_title ,awin ,title) (gtk_window_set_default_size ,awin ,wid ,ht))) That `eval` hack here is ugly; did you try just this, and what was wrong with it? (defmacro window (awin app title wid ht) `(progn (defparameter ,awin (gtk_application_window_new ,app)) (gtk_window_set_title ,awin ,title) (gtk_window_set_default_size ,awin ,wid ,ht))) 
Because syntax helps to reveal structure, which helps to communicate meaning. In Principles of Artificial Intelligence Programming Peter Norvig uses `(&lt;rule&gt; -&gt; &lt;expansion&gt;)` to represent grammar rules, even though the `-&gt;` is redundant. Well it may be redundant, but it makes it a lot easier for the reader of the code to see what's going on. Same reason that Hickey decided to use `[]` and `{}` to denote vectors and hashes respectively. Lisp's uniform syntax makes it easier to extend, at the expense of forcing the programmer to do the extra work that the parser/compiler no longer has to do. And any mental burden that the programmer takes on to make sense of amorphous code is capacity that can't be used to understand the actual problem and program they're working on.
The latter can't be used with Clojurescript, I'd guess?
There are lots of macros which have not simple syntax implementations. Take the macro DEFUN for example. defun function-name lambda-list [[declaration* | documentation]] form* There is this `[[declaration* | documentation]]`. There is nothing in the tree structure and also nothing in a DEFMACRO arglist which is able to represent this directly and which for example could inform me about syntax violations. A typical implementation of a DEFUN macro will a) assign the whole lambda list to a single variable -&gt; no automatic syntactic destructuring. One has now to program the syntax of a lambda list manually. b) assign everything after the lambda list to a BODY or REST variable. One has then to parse/destructure the body incl. doc string and complex declaration syntax manually. DEFUN takes a doc string and declarations. Before the body. The syntax for this is not present in the macro definition of DEFUN. In an implementation of the macro definition of DEFUN one 1) needs to check/destructure the basic syntax manually - it's not present in the DEFUN interface 2) one has to check/walk the complex syntax of declarations manually - there is no direct machinery in CL to describe it 
&gt; I actually see perf about the same as the default - that didn't used to be the case but the JVM continues to optimize this stuff. Bullshit; it's the same because it's doing the same overflow checks. In order to produce a bignum, we need to do overflow checking. In order to produce an `ArithmeticException` we need to ... you guessed it. &gt; *there's probably a better way to do it these days and Clojure has several libraries to tap into native libs, GPUs, etc* Yet, the humble `+` operator needs to remain sodomized? 
But the `+'` operation is no faster than `+` in non-overflow cases. It's doing the same check; just what is done when the check fails is different. What Doo Hickey wanted was for you to get a loud error, instead of silently correct, safe behavior that happens to be slow. Presumably, so you would massage your code or data to prevent that situation. That could be a valid requirement in some situations. I would approach this differently; like by using profiling tools to see if consing of bignums is going on where I don't expect it: code that should be working with small numbers for some reason isn't. Such a trifling little battle is not worth raping the type system of the language for all users going forward.
time for me to switch to 64 bits :-)
But it runs at the same speed where the overflows don't occur!!! If your program runs without errors using `+'`, you can replace it with `+`, and there is no behavioral or performance difference. When the overflows occur, `+` runs slowly, but correctly. `+'` blows up with a useless exception that cannot be handled in anything resembling that same quality. Throwing and catching it is likely at least as slow as allocating a bignum. 
I wanted the symbol to be bound at macro expansion time, because of SBCL's warnings. It didn't seem important. I suppressed the warnings, but didn't see much point in changing the code.
Good point.
He'd have a point if it wasn't for the fact that CL declarations allow you to constrain numeric variables to non-bignum types. In fact, you can go further in this direction than Clojure's broken `+` gets you because you can prevent accidental RATIONAL numbers from being created. (Clojure does not have a broken `/` to match its broken `+` operator.) 
When I first heard of [--Phosphorus--][http://s3.amazonaws.com/defunkt.baconfile.com/phosphorous.pdf] Clojure, I was excited, since I had seen Hickey's work on dotLisp, and thought (at the time), it might be cool to program SecondLife in something like Lisp. But when I saw that CLOS, or even a decent adaptation of Java classes, format, loop macro, conditions, etc. weren't there or planned, I was sad. Then I tried it and felt the awkwardness of decisions, like lazyness, [], null, mangled numeric tower, slowness, exceptions, no builtin debugger? and so on.. I was depressed. I realized that it was pretty much a hack so Rich could use something like Lisp at work, which is cool, but I resent people trying to tell me what tools I have to use: JVM. BUT, I do like some of the innovation in functional tools and Lisp macrology coming from the Clojure community. Also it's made me think about immutability more than I thought I wanted to. 
Yeah, I heard Eric Normand from Clojure community said something [similar](https://lispcast.com/programming-pop-culture/).
Clojure has it's strong points for sure. I am certainly not the one who should criticize Clojure, or say what should constitute "fundamental language research". I writing here from the trenches, have nothing to do with research. But let me put it this way. Few years ago I ported some old CL code to Clojure (I dabbled in CL and Scheme 10-15 years ago). My feeling was, Clojure is the same old thing, with some new conveniences here and there, and with few missing features one can find in CL. So I thought to myself, if Clojure appeared in the 70s, it would be considered by Lispers back then as just another Lisp dialect. If one is trying to find novel ideas in programming, maybe it is better to go back and learn about Interlisp or somehting like that (stuff that did not find it's way to CL standard). So Clojure is useful, and somebody like myself, who is still trying figure out the significance of the REPL or macros or something else, can conveniently learn a lot from it, but I guess for a Lisp expert the fact that Clojure is this "new thing" is just a manifestation of how boring the situation with dynamic languages is. Basically the same old thing, 30+ years after CL and Scheme got going. I think it would be fair to criticize Clojure on that ground, so that new Lispers can become aware that there is a need for more innovation in dynamic languages. I noticed tendencies in Clojure community where Rich Hickey is regarded as a deep thinker comparable to Guy Steele or something like that. To me that points to the lack of perspective on the history of Lisp and related languages, like Smalltalk. Not to go to far into the past, I would ask where is e.g. Gregor Kiczales of today? Where are people who have promising new ideas in programming every 5 years? I actually agree with [Sussman](https://www.infoq.com/presentations/We-Really-Dont-Know-How-To-Compute), who has all the rights to be harsh, that all those languages are obsolete, including Clojure. That just my feeling, but Sussman gives some interesting arguments to back this criticism up. To answer your question about what should be considered a promising new thing for programming, I would pick [propagator model](https://groups.csail.mit.edu/mac/users/gjs/propagators/). It might be a bad idea after all, but it represents a very different way to think. It is ok that people like me still use CL or Clojure, but I hope somebody is working on new ideas as well (I do not see that currently). If not, we are in deep trouble, as Sussman pointed out.
(Small hint: You switched around `+` and `+'` in your comment.)
I come here for good conversation. I go there for comedy and entertainment. I try to keep them separate. 
&gt;Yeah, it sucks when people behave like assholes doesn't it? That's PCJ, it's all for fun. You should come to PCJ my friend!! 
&gt;I would pick propagator model. It might be a bad idea after all, but it represents a very different way to think. It is ok that people like me still use CL or Clojure *The Propagator Programming Model is built on the idea that the basic computational elements are autonomous machines interconnected by shared cells through which they communicate.* Sounds very much like Kenneth Tilton's `cells` library. You can load that library on CL and use that paradigm. 
&gt; But the _+'_ operation is no faster than _+_ in non-overflow cases. While that may be the case today, _+'_ was much slower when this decision was made. It’s great that the JVM now optimizes this case better. That makes for a solid argument to restore auto-promotion.
Certainly useful in some problem domains. I wrote a Clojure library which does this a few years ago: https://github.com/gcv/dgraph
When Clojure came out, JVM deployment with near-seamless Java interop was a miracle. (I remember trying ABCL, and something didn’t work right. Don’t remember what.) Getting to 80% of Java’s performance with very little end user effort was also great. The deployment story has improved on the CL front since then, and with Quicklisp and the like, the advantage of JVM hosting has faded somewhat. But remember, we’re talking about 10 years ago: Java ruled the roost at many organizations, and saying “Lisp” with a straight face was beyond the pale everywhere that I tried bringing it up. The world has since moved on. For CL people who did not find the JVM useful, of course Clojure was not a huge leap forward. For CL people stuck mostly with Java for library, deployment, or political reasons, the importance of Clojure cannot be overstated. Limits imposed by the JVM were a small price to pay. As far as language features go, yes: Clojure gave up a lot. I missed Lisp-2, CLOS, MOP, the numeric tower, lambda args, the side-effect-capable reader, object redefinition, and especially the condition system. Goodness, did I ever miss the condition system — still the only CL feature nothing else has cloned (that I can think of). On the other hand, for simple code, Clojure’s literal syntax for common immutable and reasonably fast data structures has been really really nice. It’s hard to go back to CL’s mutable vectors and hash tables after Clojure’s `[]`, `{}`, `assoc`, and all related operations. It also sucks in JavaScript, Ruby, and Python, all of which have the syntactic sugar but not the fast immutable semantics. Even Julia, a very nice language, doesn’t quite get it right. I tried hacking something similar into CL with reader macros once, but didn’t end up with anything as smooth as Clojure’s data structures. 
&gt;Sounds very much like Kenneth Tilton's cells library. You can load that library on CL and use that paradigm, you don't really need to use a different programming language I thought the same at first (dataflow DAGs like cells are prevalent now), but that's a superficial similarity. Propagator is a blend of constraint propagation (like relational programming), dataflow, and finite domain constraint logic programming. They also allow the network to be dynamically and lazily constructed too (vs. Static DAGs). The other first-class concept is partial information and logical contradictions (via truth maintenance systems). [Reference implementation in scheme here](https://groups.csail.mit.edu/mac/users/gjs/propagators/revised-html.html#SECTION00020000000000000000) Your library argument holds though, since the implementation looks trivially portable, as evidenced by clojure versions of the original paper. This [talk](https://www.youtube.com/watch?v=JXOOO9MLvhs) gives an overview, with a [companion library](https://www.youtube.com/watch?v=JXOOO9MLvhs).
Giant kudos for all of your work. How can I become your patron? Do you have any kind of Patreon or Patronite site?
As I undoubtedly would if I had to code in this.
Had similar experience with Clojure. Thanks for sharing that article. :)
SBCL should learn to handle the case `(progn (defparameter x) .. (use-of x))` without warnings. (Is that what the issue is?) When a top-level `progn` is compiled, its element forms should be treated as if they were individual forms for the purposes of needed compile-time effects, such as a variable or function being considered defined.
That complexity is a more or less a Common Lisp screwup though, and it has very little impact on human readability. Note that the language inside *defun* is regular; it's parseable by a NFA with no push down stack.
I have now (https://www.patreon.com/user?u=4441966), I had postponed setting it up for long, but since you ask!
Ha! First!
It makes a solid argument for not having knee-jerk reactions, as a language designer, to what your CPU/VM/network/disk/whatever are doing today.
Why are locatives necessary when we have get-setf-expansion? Delimited continuations and green threads would be awesome!
locatives are first-class places. You may for instance wrap element in the array as a locative and pass this locative somewhere else in a code (which doesn't know anything about the array). This is useful for for instance unification. Also it is an easy change to introduce and if we ever wanted to have prolog implementation in ECL's CLR that would make it much more performant.
Yeah, but my point is just that you can implement it with GET-SETF-EXPANSION :-). (see: https://bitbucket.org/tarballs_are_good/cl-locatives )
Heh, you may have a good point here. I've bookmarked it and I'll revisit this goal if it is indeed necessary after I have green threads and continuations in place. Thank you for raising this doubt.
After looking at the source code of linked repository: In there locatives are implemented as two lambdas (reader and writer). This is indeed correct and I like how it is done (especially that code is very simple). builtin-class locative on the other hand would be a simple pointer, so this could be quicker (especially for costly accessors returned by get-setf-expansion). That's my impression at this moment (it is late – I may speak nonsense at this hour – if I do please forgive me :-).
Do you have concrete suggestions? /u/AlexdMiller is in this thread and has been very open to community input 
Seems more like you go there to mock people when you can't argue with them. All the comments are not mocking the OP but sound arguments from people like Rainer Joswig, Kent Pitman and Erik Naggum. And now you are trying to tell yourself that you that you are just "lol j/k" to pretend that you behaviour is any less toxic than proteus77.
Hey sorry to hijack the thread, but it is not often than I see a core Clojure contributor in r/lisp. Ever since I learned about namespaced keywords in Clojure I've been wondering why symbols in namespaces are not enough to preventing collisions in maps and other things? I'm coming from a CL background, where keywords are just self-evaluating symbols that life in the keyword package.
Quote from top-voted comment by *alexdmiller*: &gt;*But Rich has never been a big fan of innovation for the sake of novelty. He's far more interested in trying to see and address the root causes of complexity in software development.* 
Keywords and symbols are different types in Clojure. Keywords are interned and have optimizations around comparison, associative lookup, etc. Because of this, it's idiomatic to use keywords as keys in maps in Clojure. Symbols would work too and are used in some contexts.
Clojure didn't *have* to have a `+` operator that works exactly like Java's `+` operator. That was a conscious decision made by Rich Hickey. 
One terrible thing about this decision is that overflowing is something that happens dynamically. So you might discover that you need bignums after your program has been under development for a long time. Then you have to change every single use of the `+` operator in your program (and all of its dependencies too, because it's not safe to pass a Java integer to a Clojure library unless it uses `+'` exclusively). 
In Lisp, applications can extend the syntax if what the language provides is too spartan. But in all other programming languages, the syntax that is decided upon at language-design time is the only syntax that will be available for all apps written in that language. That creates pressure to have more syntactic constructs.
&gt; at the expense of forcing the programmer to do the extra work that the parser/compiler no longer has to do This is a fallacy. Can you explain what work this is? What are its inputs and outputs; when is it being done and for what purpose? The lack of a recursive phrase structure grammar lessens the burden for both human and machine. Its presence is a gratuitous complexity that adds a computational burden to both. It's just something someone invented to chop up the abstract syntax. If you write documentation in plain English instead of Pig Latin, would you say that you're forcing the reader to do extra work that the Pig Latin encoder/decoder would otherwise have been doing? Anyway, humans use indentation and other visual clues to avoid actually parsing. That is true for Lisp as well as Algol and C derivatives and whatever else.
Thanks for pointing out that awesome Sussman lecture!
in what way?
&gt; We have minds capable of dreaming up almost infinitely ambitious plans and only time to realize a pathetic fraction of them. If God exists this is a proof of his cruelty. Try [my task management code](https://github.com/SlightlyCyborg/holon/tree/master/task) written in common lisp. It may help :)
Like Python, Clojure is driven by a BDFL who has a particular vision for how the language should work. This results in having a focused and opinionated language that encourages solving problems using a common set of patterns.
Ok I see, I only dabbled in python so I didn't remember the BDFL title. I understand your statement now.
This last part is really funny. :) Reminds me of ["regexp disaster"](https://groups.csail.mit.edu/mac/users/gjs/6.945/psets/ps01/ps.txt), in terms of incidental complexity it causes: *Although regular-expression systems are derived from a perfectly good mathematical formalism, the particular choices made by implementers to expand the formalism into useful software systems are often disastrous.* It is weird that so many otherwise interesting languages like Scala (came out of research) has so much syntax, which causes various [weirdities](http://rickyclarkson.blogspot.com/2008/01/in-defence-of-0l-in-scala.html). Are there any papers which show that complex syntax is in some way beneficial? I think Sussman was right when he [said](https://www.infoq.com/presentations/Language-Panel): *The really worst thing in programming languages is complex syntax. As Alan Perlis quipped, syntactic sugar yields cancer of the semicolon. The problem with complex syntax is that it hides possibly important to understand mechanisms, but more importantly, it makes it very difficult to write programs that manipulate programs, that read them, write them, analyze them. I often have programs that write programs that I want to run inline, for example numerical programs that are constructed from symbolic algebraic expressions. That is nice feature of Lisp, which has lots of parentheses, is uniform representation of program as data, and ability to execute code that you just constructed, and as a consequence I would say, my mantra here is, syntax without representation is tyranny.* I also remember Alan Kay disliked Lisp syntax when he was originally thinking about Smalltalk. Was this hurtful to Smalltalk later on?
I'd say the lack of the CL Loop macro is a bonus! :)
&gt; since you can immutable in CL too Not really. In Clojure it's both the default, so most common functions, libraries everything, don't mutate, and immutability is implemented efficiently. If you wanted to do that in CL you'd have to implemented from scratch and you'd be pretty much excluded from using any current CL functions and libraries.
&gt; Not really. For practical purposes, `(ql:quickload "fset")`.
 (map #(* % %) (filter odd? (range 10))) (for [i (range 10) :when (odd? i)] (* i i)) (loop for i from 1 to 10 if (oddp i) collect (* i i)) I'll just trust you that the kool aid is delicious.
The `/` operator, on the other hand, deserves what it gets, being the source of so much chaos. But `+` is so simple and pure.
I didn't know Markdown use the caret `^`for the Super^script. What is the character for the subscript?
Interesting, my version of SBCL does not produce any warnings. I've tested on this code: (progn (defparameter *foo* :bar) (format t "FOO is: ~A" *foo*)) Probably, (with-compilation-unit () ....) can be used instead of progn?
Is there no mental burden to parsing complex syntax?
I think the other programmers here have said a lot of things which are quite apt, and I'd have nothing unique there to say. However I think there's an interesting assumption in your last paragraph that with some additional knowledge of Clojure may change how you look at the language. &amp;#x200B; Specifically, you mentioned how the MOP in Common Lisp is used to handle complexity, which is itself a very complex system. Rich Hickey and the Clojure community at large are generally morally opposed to complexity, the opposite of simplicity, as it is defined by the dictionary, as things which are entwined with one another. Clojure and Clojure programmers prefer to have many pieces of separate, simple functionality which combine in a way like you'd see in other functional languages like Haskell to make systems which are capable of a lot, without ever becoming complex. &amp;#x200B; This philosophy I think is best illustrated by some of the larger systems which are built with Clojure, with things like Re-Frame, which is a single-page-application web framework built on top of React. It separates as cleanly as possible every part of a large, robust system, from state changes to side effects. There are many things going on at every step, but each step is minimally entangled with anything else.
They are working under a lot of compromises. Good error messages require to keep a lot of metadata like source location. They won't ever do that given Clojure obsession with performance.
They already contain that sort of metadata. Are you sure you looked at the 1.10 ones?
Alan Kay used to talk about [different kind of simplicity](http://www.cs.uni.edu/~wallingf/miscellaneous/alan-kay/turing-transcript.html), which reminds me about "worse is better" vs "the right" debates. Basically, there is the idea that "simplicity through abstraction" is the right way of making things simple. My experience with Clojure is that it leans more toward "worse is better" philosophy, trying to strip down language of some of it's advanced abstractions, in order to achieve what it considers "simplicity". I suppose you can live with that if you are not aiming to high. When one looks at some of the stuff done by Lispers 20-30+ years ago, they were aiming very, very high, so they went very far to make tools to help them achieve things, including MOP. That's my interpretation of the history anyway. I actually believe that programming language's origins are very important in determining what kind of people and which purposes they are going to use it for. Common Lisp and Scheme come from 20+ years of world class fundamental AI and related research. Clojure comes from Rich Hickey's experiences in the computer industry over 15-20 years. There is a big difference there, so it could be that comparison is not even fair. 
[Loop is just a macro so](https://github.com/tayssir/cl-loop) of course you can have it if you want. On the other hand, loop doesn't compose and makes everything eager. Little functions working on lazy sequences or eager reductions do compose, which is why they're preferred over an eDSL. transducers generalize transformations over reductions, and also compose. Much better than loop koolaid IMO.
I didn't. Will check it up! Thanks.
&gt;That seems to indicate that the confusing 'optimization' with three partially overlapping Clojure integer types, two different external representations and their different operations + &amp; +. is no longer relevant. There is an apparent performance benefit. Maybe you really need those 0.03ms :) I think the magnitude of the optimization is likely far less than it was in 1.3. The only other reason to maintain the (at the time significant) optimization would be support for legacy JVM (of the kind that originally drove the optimization, likely Java 6). I don't know if that is enough of an excuse to retain the optimization though, since JVM Clojure's baseline is now apparently Java 8. The good news is this is something that can be revisited, possibly reversed (with bigint arithmetic deprecated) in light of arguments presented. I haven't had any problems, though others may appreciate the change.
One person's knee-jerk reaction is another person's practical decision at the time. 10x performance improvement for operations likely to commonly be on hot paths (for the - at the time - foreseeable future) seems reasonable to me; otherwise the Clojure = slow / impractical FUD sets in. I certainly benefited from the choice at the time and over the years, and have yet to run into problems delineated here and elsewhere (in my problem domains at least, primarily modeling and simulation). If common performance degradation is no longer a practical concern, deprecation is an option.
&gt; So you might discover that you need bignums after your program has been under development for a long time. How often has this happened in your experience? I guess it would be far more critical in certain problem domains. I haven't personally bumped into the need for arbitrary precision numbers / bignums outside of some toy problems, and even then they weren't necessary for the problem (or desirable given performance hits).
I think fset needs to migrate to include hash-array mapped tries. Using weight balanced binary trees for most things (last I looked) was a turnoff to me. I started porting HAMTs to CL for a clojure port a while back; they're not too hard to implement (even part of the clojure.core language reference). No idea why the fset community would leave the performance benefits on the table (pretty much every other persistent collections lib I know of has migrated to Clojure-like HAMTs for vectors and maps).
&gt; basic computational elements are autonomous machines interconnected by shared cells through which they communicate. Sounds like federated web services communicating via PubSub queues, no?
There was a point in time where I was very interested in Lisp, but your discourse in this thread (and how upvoted it's been) has completely turned me off from participating in it's community.
That place is "For Fun Only", he's basically role-playing an asshole because that's entertaining. I go there too, and the entire point of the subreddit is "lol jk". That's why it's called a "circlejerk".
&gt;Clojure comes from Rich Hickey's experiences in the computer industry over 15-20 years. I'd note that a significant portion of that experience (some of the happiest, apparently from talks) was spent doing production work in Common Lisp, as a direct consequence of burdening under C++, Java, and other languages. My (continuing) read is that the "lessons of the decades of world class fundamental AI and related research" - embodied brilliantly if not imperfectly in CL and Scheme - merged with the applied comforts and discomforts of producing software using resultant systems, leading to Clojure. &gt; trying to strip down language of some of it's advanced abstractions, in order to achieve what it considers "simplicity". I suppose you can live with that if you are not aiming to high. If someone really wanted the abstractions presented by the MOP or anything else that's not baked into the core language, it seems those facilities would exist by now. Witness core.async and the implementation of CSP as a library (or logic programming, or actors, or anything other trivial language modification in an environment with lisp metaprogramming facilities). I don't think the community lacks the talent to port or re-implement those things in Clojure either; there appears to be no demand. Regarding aim, Clojure seems to have a constructive view to aim low, build solid, and build high. To date, Clojure looks like a bootstrapping mechanism for building Datomic. I have no idea what the end-game is. &gt;When one looks at some of the stuff done by Lispers 20-30+ years ago, they were aiming very, very high, so they went very far to make tools to help them achieve things, including MOP. I have a similar interest in history. Do you have any specific examples that jump out as seminal milestones or moonshots? In particular, the necessity or impetus of the MOP? I get the feeling the SmallTalk crowd had similar goals, based on the seeming co-evolution of ST objects relative to (my limited understanding) of the MOP. &gt;My experience with Clojure is that it leans more toward "worse is better" philosophy I think this is more apparent in the adoption of host platforms like the JVM (and later CLR, and JS, and maybe someday CL). That was the practical trade off to find success across the competing goals of marketability, performance, reach, and bootstrapping. In the early days, Java permeated everything (including tooling and the build system), but Clojure's success definitely depended on Java's establishment (via "worse is better" that Java had already achieved). These days, I feel orders of magnitude more insulated from Java (or JavaScript), able to leverage a wide array of native clojure libraries. The host still pops up on occasion though, and always will so long as there is a host. 
Clojure also provides a toolbox that you can use however you like. A poorly managed Clojure project will be just as difficult to manage as a project written in another Lisp, as opionated as Clojure is you can still code up a ball of mud.
It's not enough to turn me off lisp, but it is my first time seeing one of those legendary old grumpy lisp developers. I guess Clojure slack channel had spoiled me, I never saw anyone remotely being an ass there.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/clojure] [in r\/lisp: "How innovative is Clojure as a Lisp dialect from Lisp expert's perspective?"](https://www.reddit.com/r/Clojure/comments/adbbkh/in_rlisp_how_innovative_is_clojure_as_a_lisp/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
How does this compare with Org mode? (Amusingly, I note that your 'readme' file is in fact an .org file.)
&gt; To date, Clojure looks like a bootstrapping mechanism for building Datomic. I have heard this in several places and thought it was really interesting idea, so I straight up asked Rich at Conj 2017 if he created Clojure to create Datomic, he said no. I think clojure datastructures came first and then he started thinking "i wonder if these are fast enough ..." If I have the timeline right, Clojure work started in 2005, and work on Datomic started in 2010 as Metadata Partners LLC
It's a circlejerk sub bruh, you're taking it too seriously. 
There's a good quote from [Richard Gabriel's review of AMOP](https://www.dreamsongs.com/Files/amop-review.pdf) regarding the interplay between AI research and programming language research: &gt;As we saw earlier, the way that programming language folks distill and clarify the concepts in a potential language is by understanding the implicit— and often muddy—interactions of the language elements. Often this is done by going through the process of designing, documenting, and implementing the new language. When Guy Steele and Gerry Sussman were trying to understand the meaning and interactions of the combination of a functional language (Lisp) and an object-oriented language (Actor), they wrote a series of interpreters. These interpreters became Scheme, and from them they learned that function calling and message-passing were fundamentally the same, a lesson still not well and thoroughly learned today. I see the nature of Clojure's development and evolution in a similar exploratory light, with periodic applied results spiraling out along the way. I believe the evolving trajectory of Clojure has been heavily influenced, if not driven, by Datomic's practical needs (and vice versa as you mentioned).
Clojure is full of compromises, that's what makes it so great and useful. For example, the model is very simple to port to ClojureCLR and ClojureScript. Javascript doesn't even have an intregral type and ClojureScript doesn't implement +'. code written against bignum should be written against the actual platform types. A reasonable compromise.
Hmmm, it just an interface for external tooling. And unless I am missing something it only keeps the source location metadata for read/macroexpand/compile phases. Is an improvement, but runtime errors are as awful as ever.
It really isn't necessary. Purescript has first class literals for javascript objects and arrays, and purescript treats them as being completely immutable ("editing" one simply copies the whole thing). You wouldn't want to use these to store significant amounts of data, but they'd be just fine for the purposes of passing arguments to functions.
And then you work with a library that expects and returns mutable arrays.
afaik the following were not built-in to existing Lisps when Clojure added them. I'm sure some of them had been implemented in libraries/research projects. - destructuring binding in default defn, let, fn, etc. - multiple arg lists written out per fn (not using &amp;flags) - map/set literals, with nice syntax - comma as whitespace - succinct anonymous function syntax - auto gensym in macros - interface based get/assoc/etc - laziness - threading macros (-&gt;, -&gt;&gt;) - ., .., doto macros - removing grouping/unnecessary parens - treating data structures/keywords as functions This doesn't include the most innovative parts of Clojure (ref types, immutable data structures as defaults and their implementations).
The support for java.math.BigInteger is for Java interop. clojure.lang.BigInt is Clojure's type for big integers. It is essentially the same as java.math.BigInteger but will use a primitive long when it can for small storage and faster ops. In practice, you never use either of these types explicitly so the idiomatic thing to do is to just use the literal formats, and functions in the core api.
&gt;There is an apparent performance benefit. Maybe you really need those 0.03ms :) I think the magnitude of the optimization is likely far less than it was in 1.3. You're missing the decimal place. 330 microseconds compared to 30 milliseconds
"preceded", "after", "Older", "followed", "60s" Place in time is not a useful metric for quality. Ideas are non-linear values. They don't stack neatly in order from bad to good and it doesn't matter when you use them, only how.
Is because you are critical thinking is too incompetent to suppress your feelings, apparently.
Because they are posers. None of them are Lispsers.
Pure zealotry. And they call lisp a cult.
Many compromised are questionable, like promoting itself as functional language without proper TCO, not supporting true conses and not having good error messages
Working with terabytes of data which id presentation can fit in fixnums.
I think it just provide compile time information. Runtime errors are still ugly.
Whatever you say
Lisp is language to build languages. I know this a very foreign concept for Clojurists. Implementing a small subset of it for those fancy data structures you care and need is a good idea.
Probably because it is a better choice for it's purpose. There no need to follow the immutability path just for the sake of it.
Agree. Even the phenomenon of "worse is better" is the most prevalent in the industry.
Just saying that you correlation with Lisp is factually wrong.
Sure, you could COW everything....but efficient immutability is where the rubber his the road when you scale beyond toy problems. I think that was a primary concern along the axis of practicality if you want to emphasize pure functionsl programming as your default paradigm, with trivial reasoning about concurrency as a close (also dependent on efficient immutable collections) secondary concern. 
If my math is on point, I think i can get by naively indexing up to 8192 petabytes before needing bignums (or changing my indexing scheme). Are you routinely processing data in this range in your field?
&gt;Lisp is language to build languages. I know this is a very foreign concept for Clojurists. Not foreign at all, you seem mistaken. &gt;Implementing a small subset of it for those fancy data structures you care and need is a good idea. That's what the creator of clojure did, except he based them on modern (at the time) research by Bagwell (with some custom novelties using bitmasks). I think you are again mistaken. I think Fset would objectively benefit by migrating to (or simply adding) hamt structures, which shouldnt change any of the persistent API. 
I think you missed the section on execution errors. There are plenty of improvements at runtime. &gt;Runtime errors are still ugly. I think they're significantly "prettier." More importantly, helpful.
You'll have to explain more what you mean
Interesting that his clients wouldn't accept it. Being able to run in the JVM is a huge win for enterprise adoption given that the JVM is a mainstay in many companies already and it has a proven track record. 
 I think I am qualified to answer this: I wrote reasonably complex systems in Common Lisp before I gradually moved to Clojure several years ago. I have written several fairly large apps and systems in Clojure and Clojurescript (this includes https://partsbox.io/ as my current project and apps like an e-commerce search engine processing hundreds of requests per second for many years). I used to think Common Lisp was a fantastic language, but I always tried to keep an open mind remembering the Blub paradox. Then I started working with Clojure and after the initial friction (some things were just plain annoying in the beginning) things clicked for me. It was, quite simply, a much better language. The major improvements over Common Lisp for me were: 1) immutable data structures: these days I can't even imagine dealing with code that mutates data in-place, 2) a great approach to concurrency and parallelism (it's really difficult to get yourself deadlocked in Clojure), 3) Rich Hickey coming out every year or so with a solution to yet another problem (core.async, transducers, spec), 4) really flexible data structures. I know there are many CL advocates who will try to point out that CL has all these things. But these will be the same people who will be annoyed when Java programmers point out that Java has all these things, too. It's just a matter of friction, elegance and ease of use. One additional factor which is often glossed over is Clojurescript. Most software these days exists in some form in the browser, too. Being able to use pretty much the same language (cljc files can be compiled both server-side and client-side to Javascript!) on both sides brings many advantages. People sometimes ask me if I miss CLOS or the MOP. Not at all. It's a great set of concepts, worth learning (the green book makes for great reading). But I found that larger systems using CLOS were really difficult to understand and follow (just following the code flow is a nightmare if you have a lot of multimethods), and systems that used the MOP in any but the most basic ways were intractable without a debugger. Besides, please see above what I wrote about immutability: I'd much rather have immutable data structures. Clojure's emphasis on functions and data, while it may be based in the 60s, is one of the really good ideas. And Clojure is really, really good at tackling complexity, it's just not apparent in the language design. While other languages focus on syntax sugar, Clojure's focus is on getting things done on a large scale. You don't see that, but things like namespaced keywords, clojure.spec, pre/post conditions, maps, sets, all play together to allow you to create extensible and reusable code. Also, it's easy to discount certain solutions in Clojure as "also existing" elsewhere, but without actually using them in real systems one often doesn't see the full advantage. Transducers are a good example: I think they are under-appreciated and under-utilized. Common Lisp veterans will laugh and say that SERIES did the same thing back in the 80s. But SERIES was all about optimization and anyone who actually tried to use it will have stories to tell about how easy it is to use. Transducers, on the other hand, if used to build data-processing pipelines processing Clojure maps with namespaced keys, are an elegant and efficient tool for building composable systems. Plus, once you've coded your pipeline using transducers, it's a snap to parallelize it. Looking at it differently: to "get" Clojure one has to understand that there is no silver bullet: no single revolutionary "feature". Instead, the gains come from tiny improvements all over the place: immutable data structures, good built-in library for dealing with them, core.async, transducers, spec, Clojurescript, not having to convert to JSON or XML on the way to the client, re-using the same code on server-side and client-side, and more. In practical terms, I would never be able to write and maintain by myself the system I'm currently working on (https://partsbox.io/) without Clojure and Clojurescript. It isn't impossible, but I can't imagine tackling the complexity in a comparable time frame. I can't imagine going back to Common Lisp. I sometimes have to look at the code I've written in CL, and it looks clunky. Especially place-oriented programming (setf all over the place) is something I really really don't want to go back to. It's a nightmare, particularly if you intend to have any kind of concurrency. I still try to keep an open mind, remembering about the Blub paradox. I make it a rule to regularly check and learn about new languages. There are some which I found really good for certain types of applications and which I would use instead of Clojure in certain cases (Elixir is a good example). I still haven't found anything comparable which is a general-purpose language, especially if you consider the client-side story — but I'm checking all the time See also my answer to a similar question posted on Quora several years ago: https://www.quora.com/What-do-veteran-Lisp-programmers-think-of-Clojure/answer/Jan-Rychter 
If you care to link to the runtime section, please?
You are a Clojurists and is not a Lisp.
My bad. On the last comment miss-read bignums as bigint which is actually 'long' for some languages.
It is really foreign. Most Clojurists discourage macros like a plague. Such ignorance.
I don't discourage macros, and neither do my friends, colleagues, etc. Macros are broadly useful. They show up in many libraries I use as well. Do you have actual data to support your claims, or just anecdotes like me?
&gt;I'd note that a significant portion of that experience (some of the happiest, apparently from talks) was spent doing production work in Common Lisp, as a direct consequence of burdening under C++, Java, and other languages. &gt; &gt;My (continuing) read is that the "lessons of the decades of world class fundamental AI and related research" - embodied brilliantly if not imperfectly in CL and Scheme - merged with the applied comforts and discomforts of producing software using said systems, leading to Clojure. I wasn't clear enough. Let me try one more time. If you look at the history of Common Lisp, there was CLtL1 in 1984, then CLtL2 in 1990 (with CLOS), and then MOP a year later. So in the span of 7 years, there 3 major things (different ways to program) are entering practical world of programming. There is a reason why Alan Kay praised book about MOP as "the best book in the last 10 years in computing" (that was at OOPSLA 1997). The reason is, I think, that it became clear back then that the real progress stopped. There is nothing really wrong with that except that as Kay pointed out in that same talk "we do not how to build systems yet". And then we have Clojure 20-30 years after that, and it feels like "the same old thing". It is not 5 years, but 20 or 30. And it is great for average Joes like myself, but man, how boring it is for real Lispers. So let's treat Clojure and it's "functions and data" the way it should be treated, like "the same old thing", and move on toward the real advances in dynamic languages.
Understood. I don't take issue with your point; there are imaginable use cases where indexing could cross over to using arbitrary precision. I haven't personally run up against those limits yet.
As I said, you'll have to explain yourself more.
Error messages in the newest release (1.10) are pretty pretty good.
It just included basic metadata for external tooling and only for compile time. Also, I find the errors themselves to be quite ugly 
Not proper conses. And it's ethos around FP and data structures disregard anything Lisp is all about (metaprogramming).
Clojure ethos around FP and immutable data structures is quite telling that meta programming is an after though. The most annoying and vocal people in the community (including Rich) have a church around it. To them, macros are just pretty syntax for petty and low level stuff like '-&gt;&gt;' and core.async. Nobody in the Clojure understand macros. 
As a corporate idiot, I am thankful to Hickey 
&gt; And it is great for average Joes like myself, but man, how boring it is for real Lispers. One ponders what the real lispers were doing in the intervening decades to advance the state of the art. Perhaps McCarthy was onto something when he later lamented the CL standardization efforts (did that stultify the seeming Cambrian explosion of ideas and lisp implementations?) Perhaps the low hanging fruit had already been picked by the time AI winter set in. &gt;So let's treat Clojure and it's "functions and data" the way it should be treated, like "the same old thing", and move on toward the real advances in dynamic languages. I think there's a semantic difference between "innovation" (as you're using it) and "invention." I think you're looking for invention when innovative use of prior art (coupled with a bit of invention regarding optimizations and modifications to Bagwell's data structures) is applicable to the contribution Clojure provides. There's a similar observation made during the movie "Flash of Genius:" arguably the parts were all there (excepting the unique HAMT implementation that's subsequently been adopted by multiple dynamic and static languages), but composing them as Hickey did resulted in something subjectively brilliant; an innovation. &gt;So in the span of 7 years, there 3 major things (different ways to program) are entering practical world of programming. I don't quite see how CLtL1 would have been received by "real lispers" using MacLisp, Interlisp, etc. with anything regarding the level of enormity you gave it (in many ways it falls under the "same old thing" umbrella). I'm not seeing any novel advances (beyond a common documented standard - really an acknowledgement and accretion of existing implementations - including Scheme) over contemporary Lisps. Maybe I've missed something significant though (not to decry the significant undertaking of developing a unified design that accreted such a breadth of features). If you include their lifespans, the timeline for introduction is far more than 7 years. I can see CLOS and MOP with as novel contributions (inventions), and agree that further advancement did appear to halt. &gt;and move on toward the real advances in dynamic languages. You seem to preclude the possibility that Clojure could serve as a means to discover or extend said advances. I think there's potential there (transducers were a nice discovery that built on reducers, which built on Guy Steele's work with Fortress), but I'm far from a language researcher.
&gt; &gt; I guess Clojure slack channel had spoiled me, I never saw anyone remotely being an ass there. &gt; Because they are posers. None of them are with lisp. They've never run across someone being an ass in clojure groups because being an ass is exclusively a trait of those using CL? Is that really your rebuttal?
I concur. We should all use Color FORTH.
There's a heading for "Execution Errors" in [the same link I provided](http://insideclojure.org/2018/12/17/errors/) along with a runtime division by zero example. 
`'("lilypond" "-s" "--png" fn)` You are quoting the symbol FN. Use a backquote instead.
If I want to embed a list into an expression I need to do it as `(list a b c)`; if I'm absolutely sure that the expression won't mutate *and* it doesn't involve any sub-expressions that need to be evaluated it I could quote it. It's enough of a bother that I'm not surprised when a DSL is written as a macro even though it could be a function (e.g. cl-who), because otherwise writing the input becomes a royal PITA. Hell I'm up against the same thing myself and I'm questioning my decision to go the other way. And if it *is* written as a macro, now I need to remember which parts are read literally and which parts are evaluated. I love lisps, I love the things that the uniform syntax makes possible, but I can't say that the syntax is strictly better -- there *are* costs. I'd like to take some time to play around with clojure because I suspect that the collection literals will go a long way towards eliminating those costs. Until then I'll gladly pay the cost for the additional power it affords. Cheers!
&gt;Clojure ethos around FP and immutable data structures is quite telling that meta programming is an after though. FP and efficient immutable data structures are orthogonal to macros, except when those properties are leveraged to write macros. &gt;The most annoying and vocal people in the community (including Rich) have a church around it. &gt;Nobody in the Clojure church understand macros. I don't think they'd have a problem using macros (as evidenced by the existence of macros in many libraries and the bulk of the core language...). There's a compelling argument as to asking why you'd use a macro if you could use a function. CL has had similar "church" goers that seem remarkably consistent. Here are a few vocal members from both languages publishing similar advice: &gt;The first step in writing a macro is to recognize that every time you write one, you are defining a new language that is just like Lisp except for your new macro. The programmer who thinks that way will be rightfully be extremely frugal in defining macros. Introducing a macro puts much more memory strain on the reader of your program than does introducing a function, variable, or data type, so it should not be taken lightly. Introduce macros only when there is a clear need, and when the macro fits in well with your existing system. &gt;Macros execute before compile time, and the compiler sees the result of macro execution. Because of this level of indirection, macros can be difficult to reason about, and thus it's best not to use them when a function will do the job. However, macros have legitimate uses, and it's worth understanding how they work. *Norvig, PAIP, 66. &gt; The first rule of Macro Club is Don't Write Macros. Macros are complex, and they require you to think carefully about the interplay of macro expansion time and compile time. If you can write it as a function, think twice before using a macro. The second rule of Macro Club is Write Macros If That Is The Only Way to Encapsulate a Pattern. All programming languages provide some way to encapsulate patterns, but without macros these mechanisms are incomplete....In Clojure, you just implement feature X using a macro. The exception to the rule is that you can write any macro that makes life easier for your callers when compared with an equivalent function. But to understand this exception, you need some practice writing macros and comparing them to functions. *Halloway, Programming Clojure, 191. &gt;Clojure macros let you automate patterns in your code. Because they transform source code at macro expansion time, you can use macros to abstract away any kind of pattern in your code. You are not limited to working within Clojure. With macros, you can extend Clojure into your problem domain. Internally, Clojure uses macros to implement many of its most powerful features. Halloway, Programming Clojure, 223. &gt;The main drawback of macros is that they can make it hard for other programmers to understand your code. After all, if you're creating your own language dialect, other programmers won't be familiar with it. Even your future self - say, in a year or two - may have a hard time understanding the structure of your code if you've made heavy use of macros. Because of this, experienced Lispers will do their best to use alternate techniques to macro programming whenever possible. Often, a beginning Lisper will write a macro in situations that could be addressed in other, cleaner ways. *Barski, Land of Lisp, 352. &gt;Though instructive to the point under discussion, we also tried to show how macros can be used to mold Clojure into the language that shortens the gap between your problem space and solution space. In your own unique programs, you should try to do the same. But the most important skill that you can learn on your path toward macro mastery is the ability to recognize when to avoid using them. The general answer of course is whenever, and as often as you can. *Fogus and Houser, Joy of Clojure, 176. &gt; The conventional wisdom surrounding macros is to use them only when necessary because they can be difficult to understand, contain extremely subtle bugs, and limit you in possibly surprising ways if you think of everything as functions. These aren't defects in the lisp macro system itself but instead are traits of macro programming in general. As with any technology, the more powerful the tool, the more ways there are to misuse it. And, as far as programming constructs go, lisp macros are the most powerful tool. *Hoyte, Let Over Lambda, 3. &gt;There’s a common theme in Lisp that you should only use macros when you need them. It is very common to see a new lisper overuse macros. I did it myself when I first learned Lisp. They are very powerful and make you the king of syntax. Clojure macros do have their uses, but why should you avoid them if possible? The principle reason is that macros are not first-class in Clojure. You cannot access them at runtime. You cannot pass them as an argument to a function, nor do any of the other powerful stuff you’ve come to love from functional programming. In short, macros are not functional programming (though they can make use of it). *Eric Normand, Lispcast (first search result that popped for me, fairly well known too). &gt; By default, we should use functions: it is inelegant to use a macro where a function would do. We should use macros only where they bring us some specific advantage. *Graham, On Lisp, 106. I couldn't find a quote from Rich Hickey, although they may be in a transcripted talk somewhere. Regardless, I think the "church" as you described it intersects a far broader population than you realize irrespective of language. Hoyte is perhaps the most over-the-top about exploiting macros, followed by Graham. Even then, the advice toward judiciousness is acknowledged (in Hoyte's case, just prior to blowing the lid off macros) if not subscribed to wholly (in the case of Graham, Norvig, and Barksi). I think that's a wise take personally.
Sure there is! C++ is famously obtuse (e.g. most vexing parse). Some of that complexity is essential to describing a program, some of that will be incidental. And sometimes a syntax isn't powerful enough to properly describe what we want -- lists are a poor way to represent a tree, it's just the best textual form we've got (so far). The lisp syntax is so simple that it's easy to extend with new semantics (e.g. the various `with-resource` forms), but it's so simple that certain things become more verbose (e.g. languages that can say `1..n`, or straight-up math where I can write `[n..m)`).
I think people here are making a lot of good points put I want to be a bit more specific. I don't have a lot of experinace with others lisps so please correct me. - The EDN (https://github.com/edn-format/edn) data format is a further development of the code is data idea. 'Tagged Literals' are interesting feature and used in Clojure to make code shared between different Hosts, mostly JS/JVM. - Transducers are a new interesting features that are fairly unique to Clojure (https://clojure.org/reference/transducers) - Reducers (https://clojure.org/reference/reducers) - Clojure Multimethods are different from CLOS in CL and one interesting feature are stand alone hierarchies. Some people would maybe call this an step back from CLOS. https://clojure.org/reference/multimethods - Concurrency primitives like Agents and Refs (featuring Software Transactional Memory) are fairly unique to Clojure. - Metadata. In Clojure most types can have metadata attached, meaning data that flows threw the program with your data but does not effect things like equality or size. - Protocols are dynamic single dispatch system - spec. Clojure Spec is a core part of Clojure now and used internally as well. Its a type of dynamic specification system and not like most systems of this type (https://clojure.org/guides/spec) Some more information here: https://clojure.org/reference/lisps
&gt; The numeric tower of Clojure I find incomprehensible Its not incomprehensible, its fairly standard for anybody not coming from Lisp. &gt; There's Clojure's STM which no-one uses Nonsense. Refs are used in production every day. Its a standard feature that lots of people use. My company is using STM in back software right now. &gt; Finally, there are the error messages. In Clojure, error messages make me want to kill myself. Error messages have improved quite a bit. 
Sure. But it is better to replace it with just: (list "lilypond" "-s" "--png" fn)
Byt the way, it is interesting to see a use `str:concat` function here. I always used `cl-strings:join` for such task and didn't know that there is another similar project - cl-str.
While I don't use Refs (STM) often, its always great to know its there. Sometimes you want to write something you need some coordination and the STM just fixes your problem without any issues. I love it, even when not using it most of the time.
Clojure code is pretty fast by default but you can optimize a lot and get to the max speed on JVM and that is pretty fast. 
It has cons, its just not a core abstraction. And everything you would wont do do with them you can. TCO simply can't be done practically on the JVM and in practice it doesn't matter 99.99% of the time. How often have you actually written mutually recursive code in a production? Error messages are getting better, but again, that simply a price you pay to be on a host. It makes it harder.
Yes, because in the real world you want to write fast code and data that interacts with your host VM and libraries. If you actually need bignum in practice, you usually know about it. 
Good that 99.9999% of the software people write doesn't need bignums then. If you actually have an application that uses them you can ask for it. Making the programs of pretty much everybody slower would be an utter waste of energy for a feature that almost nobody uses.
And most Lisp turthers like you don't understand how to create elegant languages without writing unreadable macro code. Clojure uses macros when appropriate. Macros have well known problems that have been known for a long time. Not encouraging people, specially new people from overusing macro is just smart lisp programming. The idea that we avoid it 'like a plague' is just more nonsense that you have spread all over this thread.
You can only get this impressions because the Clojure documentation usually does not mention prior art. Different from something like the CLtL1 and CLtL2 books, which explain where features are coming from and how they are different from prior art. For example the idea of transducing lazy sequences is from 1989: https://apps.dtic.mil/dtic/tr/fulltext/u2/a218220.pdf See the SERIES library for Common Lisp, described for example in CLtL2. 
More nonsense. I never said these concepts are utterly new and invented for Clojure. However if you actually look at REAL PRACTICAL software that people actually use, transducers are quite unique to Cllojure. And they are MOST DEFENTLY NOT PART OF COMMON LISP OR SCHEME. Furthermore, Clojure Transducers support is much broader then just transducing a lazy sequence. The concept is used in a much deeper way then anywhere I have seen. Your other nonsense about not sighting documentation is more nonsense, Rich clearly said where he got transducers from and cited the prior works.
&gt; nonsense I like your refreshing lack of style. &gt; Your other nonsense about not sighting documentation is more nonsense, Rich clearly said where he got transducers from and cited the prior works. You should actually read what I wrote. I mentioned that the 'Clojure documentation' does not mention prior art. Proof: https://clojure.org/reference/transducers https://clojuredocs.org/clojure.core/transduce As you can easily see there is none mentioned in the documentation. 
Ok. Whatever. Users who read the docs dont need a history lesson. Its mentioned in the blog post announcing the feature and even more information in his talks. The section about lists also doesnt have a reference to the history of list and nobody would expect it.
It is so long to type (concatenate 'string ...) that I guess everyone has something like (defun concat (str &amp;rest strs) (apply (curry #'concatenate 'string str) strs)) in the codebase ;)
&gt; *Good that 99.9999% of the software people write doesn't need bignums then.* As far as they believe, anyway. &gt; *Making the programs of pretty much everybody slower would be an utter waste of energy for a feature that almost nobody uses.* That's more of an argument for static versus dynamic; there is no need for a performance penalty for bignum support in dynamic languages with safe math. If you don't need them, don't calculate them, and they aren't there. If the overflow check doesn't trigger, it doesn't matter whether it produces a bignum, or an exception. In the case of Clojure, the performance problem was some Java problem that seems to be fixed (see rest of the thread). Thus `+'` is now an onion in the varnish. 
&gt; The section about lists also doesnt have a reference to the history of list and nobody would expect it. When I started to use Common Lisp, I quite often looked at the remarks in the CL specs how it was compatible or incompatible with earlier Lisp - since we had Lisp applications in earlier Lisps, which we wanted to migrate to Common Lisp. Since Clojure is not source compatible with Lisp, this does not matter much.
You absolutely can do TCO using `recur` and it will even check that your statement is actually tail recursive at compile time. What it doesn't let you do is mutual recursion, which you'd use trampolines for. In nearly a decade of using Clojure professionally this has never come up as a limitation.
Not to mention that Clojure also includes an excellent "for" macro that allows a very close translation of the "loop" Codd above.
Thanks. But is nothing major.
You are incompetent, no point talking to you.
No. What I mean is that the correlation between Lisp and Clojure doesn't exist.
&gt;Not really. In Clojure it's both the default The big white elephant in the room is: the large majority of Common Lisp functions \*don't\* mutate data either, the ones that mutate data often have a "n" prefix (\`nreverse\`, \`nconc\`, etc) or are the triumvirat of \`set\`, \`setq\` and \`setf\`.
This is a partial overview of the 1.10 changes that might be useful. http://insideclojure.org/2018/12/17/errors/
&gt;with a library that expects and returns mutable arrays. If you are working with arrays (which is 99% of the time due to performance), it would be silly not to work with mutable arrays. The whole concept of 'immutable arrays' is almost silly, unless the system is going to optimize them at compile time down to the machine-language level.
I researched for a bit, and it looks to me like Reddit's flavor of markdown doesn't provide a subscript markup. I remember somewhere seeing tilde (~) used for subscripting, but it doesn't work here because tildes are used for strikeout instead. Github lets you use the HTML tags &lt;sup&gt; and &lt;sub&gt;, but that doesn't work here either. Unless I overlooked something, there ain't no subscripts here. :(
Their comment didn't attempt to correlate the languages at all. Their point was only about the communities and would have applied to C or Python just as well. I don't understand this apparent grudge you have against clojure where no comparison can be made between it and common lisp, despite more distinct languages being compared all the time without issue. Even if you were right and clojure isn't a lisp, we can still have a discussion about similarities and differences.
He can save his ethos to himself. I don't care about his inferior views.
Now only common lispers are capable of having valid opinions or good ideas? I'm done arguing. You are being irrationally hateful and absurd.
Can't apply TCO for methods generate at runtime. 
So do all runtime errors provide source location information now (filename, line, col, etc...)?
Just to clarify. Common lispsers are no the exception either. There views are almost as damaging.
Runtime errors have always reported that but the main issues have always been 1) it's reported in "Java" form and 2) it reported the location where the error was thrown, not the location in the user's code. For 1, this will now report in "Clojure" form by demunging the names as needed. For 2, we are eliding stack frames inside the Clojure impl and generally the actual reported call site will be far more relevant for understanding where the error occurred. As runtime errors are based on the Java stack trace, which is based on compiled bytecode, we get class (from which we can demunge to Clojure namespace/function) and line info, but not column. 
You're right. You are the one true programmer in a world full of ignoramuses and posers who are incapable of holding a correct opinion that differs from your current view of the world. I suppose I should feel honored having had the opportunity to be enlightened by you, and inadequate at having learned nothing from your entirely justified vitriol.
Thanks. They do look a bit better now, definitely an improvement.
Completely justifiable. There is a reason I label them as damaging. 
`uiop:strcat`.
Ok, so this is interesting. What is your workload? I'm asking, because I regularly do data analytics consulting work. Some of my clients call it "big data" work, as it deals with input data volumes in the low tens or hundreds of GB. I need to to all kinds of aggregations, graph aggregations and analysis. I do all of that on my laptop. As it turns out, Clojure on the JVM with the help of a) lazy sequences, b) transducers, c) core.async pipelines, d) tesser library for more complex reductions works just fine: it uses all the cores and I/O to the max and I can Get Things Done quickly and efficiently, without dealing with yak shaving in the style of Hadoop or other kinds of distributed systems. I also wrote an e-commerce search engine with response times in the low tens of ms. In general, I never found performance to be a problem at all. So I'm curious: unless you're doing low-level math or something similar, you should not have performance problems.
I think the point here is that a different compromise was chosen in Clojure (engineering is always about compromises). In CL you have to add declarations to get unchecked math operations. In Clojure you have to use different function names. The major difference is that in Clojure the default math operations are not auto-promoting, but safe, because they will throw rather than return incorrect results). I found this to be an excellent compromise for my apps and my code.
I think this thread is a great example of why I like Clojure. Engineering is always about compromises. Clojure makes the right ones for me: I need to Get Stuff Done (as in, write and maintain large software systems for a living), rather than play with tail-call optimization for runtime-generated methods. 
I just checked and the first commits in some of my Clojure repositories were made 9 years ago. In that time, having built many systems, I've never (not even once) found this compromise to be a problem. That shows it is a good compromise.
Ok, people seem to miss the point of building a language on immutable data structures. You'll appreciate it the moment your code runs too slow and you have no time to micro-optimize it (this is a tool to make a living, remember, you need to get stuff done, not play around with it on weekends). In Clojure, you change your `map` to a `pmap` and you're done.
&gt; In Clojure you have to use different function names. That's not the case. Both + and +. check the math operation. One throws an error and the other one computes a different number type. One can't throw an error without checking somehow.
Rich Hickey says "nothing here is new" in (I think) every single one of his presentations. Quoting from my other comment here: Also, it's easy to discount certain solutions in Clojure as "also existing" elsewhere, but without actually using them in real systems one often doesn't see the full advantage. Transducers are a good example: I think they are under-appreciated and under-utilized. Common Lisp veterans will laugh and say that SERIES did the same thing back in the 80s. But SERIES was all about optimization and anyone who actually tried to use it will have stories to tell about how easy it is to use. Transducers, on the other hand, if used to build data-processing pipelines processing Clojure maps with namespaced keys, are an elegant and efficient tool for building composable systems. Plus, once you've coded your pipeline using transducers, it's a snap to parallelize it. 
Or, you are incompetent. Clojure doesn't have anything special by the way Clojurist use it. It just another webdev driven language like many others. They could do just fine with anything else. 
&gt;zarandysofia: Thanks. They do look a bit better now, definitely an improvement. You're welcome. Is something major.
Ok. You've convinced me. From now on I'll always say: Into the empty vector, conjoin the lazy range of 10, transducing it with the composition of filtering for oddness, with the mapping of multiplying two things. (or something like that?) Instead of: Loop i from one to ten, if i is odd, collect i times i in a list. And have the run time be: user=&gt; (time (dotimes [i 100000] (into [] (comp (filter odd?) (map #(* % %))) (range 10)))) "Elapsed time: 211.657742 msecs" Instead of: (time (dotimes (i 100000) (loop for i from 1 to 10 if (oddp i) collect (* i i)))) Evaluation took: 0.009 seconds of real time 
I love the joke with "reinventing a wheel" section and providing actual steps to make a wheel as an ASCII art in it!
This is really fun. Is this your library /u/veddox? If so, would it be a sane idea to port some of the high level functions to create non-ASCII animations?
I used org initially, but I wanted to hack it and do so in CL. This allows me to save the complete todos to a database and I can select and deselect todos so the system knows what I am working on at any given moment. It calculates the time spent doing each task. I wasn't really satisfied w/ orgmode as a taskmanager. 
You could use something like clojure's immutable vectors for most purposes. However, if a library that you are relying on pretty heavily uses mutable arrays instead of clojure style vectors, then using anything but mutable vectors will be a pain in the ass. With clojure, that issue goes away, because everyone uses clojure style vectors by default and only turns to mutable arrays if you really need the performance. And even if they are using mutable arrays, it's probably an implementation detail and the library's interface still uses vectors.
&gt;One ponders what the real lispers were doing in the intervening decades to advance the state of the art. Perhaps McCarthy was onto something when he later lamented the CL standardization efforts (did that stultify the seeming Cambrian explosion of ideas and lisp implementations?) Perhaps the low hanging fruit had already been picked by the time AI winter set in. Alan Kay has been talking about that very often. Situation with funding changed, etc. If we look at the computing world at 1960 mark, and then at 1990 mark, it is amazing what has been achieved, with so little hardware resources. And even after that there were so many useful things happening, more incrementally. What I am trying to say, and what my impression is, is that Clojure is nothing but one of those increments, not some amazing thing like it is considered by many (that's my impression). One would expect, given all of the computing power at disposal that this would be a significant increment. I do not think it is. Many people, including myself are too ignorant of history to really see that. E.g. somebody on this thread mentioned transducers, and then Mr. Joswig who knows a lot about Lisp history said that this was already invented many years ago. There are many things like that. Reminds me of a Alan Kay [thread](https://stackoverflow.com/questions/432922/significant-new-inventions-in-computing-since-1980) on stackoverflow few years ago. &amp;#x200B;
Really? Could you point me to where in the standard that behaviour is defined? Also, you missed the efficient part. &gt; Why? Because there is no guarantee they don't mutate.
You've misread this comment. He said that Hickey's clients wouldn't accept Common Lisp, so Hickey created Clojure.
&gt;Notably, Sussman's propagator model (with roots in constraint propagators at least as far back as SICP and Prolog) only came into research focus relatively recently. miniKanren (circa 2005) similarly presents an interesting foray into embedded relational programming, particularly when combined with Lisps to examine program synthesis. Thanks for mentioning miniKanren - interesting stuff. Propagator model definitely represents very different way to program. You are right, it brings back stuff invented long ago, like Guy Steele's works on constraints, truth-maintenance systems, and some other, but there are some significant invention being made by Alexey Radul 10 years or so ago, when he was working on his Phd. Actually, as Alan Kay said, bringing "roads not taken" from the past into the future, is very promising way of inventing new things. Propagator model is real research, very radical, deep, etc., like it should be. Clojure by comparison is not even close. There is no problem with that. Let's just not pretend that it is. &amp;#x200B;
&gt;Really? Could you point me to where in the standard that behaviour is defined? Read for example `reverse` vs `nreverse` in the standard. If the data isn't mutated, there isn't any warning. Most essential functions in Lisp like `cons`, `car`, `cdr`, don't mutate data, any Lisper knows that, it's part of Lisp history. &gt;Also, you missed the efficient part. EMost lisp implementations like SBCL often run faster than the JVM Clojure implementation. &gt;Because there is no guarantee they don't mutate. It's trivial to define a subset of CL that only uses functions that don't mutaye data, give it a new name (say, `cl-immutable`) and use it instead of the base Common Lisp language. The change would be a few characters: replace `:use :cl` with `:use :cl-immutable` when you define your new libs. And this will guarantee data won't mutate. Lispers don't do such a thing because its silly. Want immutable data in your code? write it that way, period. 
No, it's not my library (here is the [original repo](https://github.com/McParen/croatoan)). I just contributed the "[shapes](https://github.com/veddox/croatoan/blob/master/source/shape.lisp)" functionality, which is still being reviewed. What do you mean by "creating non-ASCII animations"? As a curses wrapper, croatoan is limited to displaying text characters...
I'm a total novice but I toy around with some of the graphics stuff in How to Design Programs (in Racket, not CL). Was just curious if you thought it would be worth abstracting away the ncurses stuff to try and use some of the code that creates and draws shapes to the very basic graphics library that HTDP uses. Apologies if I'm wasting your time.
Thanks, although in the second half of your post you seem to have quoted someone (who?) else.
Oh, that's what you meant ;-) No, there's more than enough good, solid pixel-graphics libraries out there - I'm not adding anything new to that. Never heard about HtDP before, thanks for mentioning that!
Sorry, i corrected the formatting now.
bug #1: no readme bro ! Also we just learned about a task management app written in CL a few days ago: deftask https://deftask.com/ (only the CLI sources are available).
It's a good list, but many of these are taking things that are common in other FP languages and applying them to lisp. I believe the really unique bits are: * the threading macros (wouldn't be surprised if this existed as a 3rd-party thing in other lisps first tho) * the auto-gensym and auto-namespacing behavior of backtick (making accidental symbol capture virtually impossible) * the careful design of vars to facilitate reloads * the implementation of immutable vectors. (Efficient implementations existed of the other immutable types, but IIRC Clojure was the first to do it for vectors in a way that their length could change.)
Some quick observations, then code: * clojure version should be (range 1 10) * range is not lazy, but can be coerced to a lazy seq, or trans/reducible Fixing the range problem gets performance into a better spot, but still gobs away from SBCL on my hardware (around 10x slower, clj comes in at [52..54] ms, SBCL is around 5ms). Setting aside microbenchmark performance for the moment (as scientifically tantalizing as it it is in judging the holistic performance or non-toy programs), let's look at the expressiveness / interpretation issues. I have just-as-plain explanations for the various loop replacements: ;;using seqs. (-&gt;&gt; (range 10) ;gimme numbers 0..9 (filter odd?) ;keep the odds (map #(* % %)) vec) ;return the squares (into [] ;define a vector (comp (filter odd?) ;that only contains odds (map #(* % %))) ;which are squared (range 1 10)))) ;drawn from the range 1..9 (let [;define a reusable transducer that xform (comp (filter odd?) ;keeps the odds (map #(* % %)))] ;squares resulting elements (into [] ;define a vector xform ;where elements filtered and transformed ;by the transducer xform (range 1 10))))) ;drawn from the range 1..9 I kind of like the threaded, seq version, but I'd prefer to use transducers, and also maybe not have to shove "comp" in there manually. Since defining reducers boils down to just function composition (as opposed to a non-lispy pre-baked DSL), I can re-use what I've got and write a macro to help pipeline similar behavior to the seq version: (defmacro into-&gt;&gt; ([to] to) ([from to] `(into ~to ~from)) ([from f &amp; args] (let [xs (butlast args) to (last args) xform (if (seq xs) `(comp ~f ~@xs) f)] `(let [xform# ~xform] (into ~to xform# ~from))))) And it expresses the same transformational pipeline as the lazy sequence version, except we're eager (effectively compiling down to loops), trivially parallelizable (if it makes sense, doesn't here), and we have visual flow of data through the transformations. Further, said transformations are trivially added to, disabled, etc. since they're just functions. Newbs can take the building blocks they're familiar with (functions), and leverage these kinds of pipelines to write expressive, extensible, and reasonable code without really learning anything new (like a separate blub DSL). (into-&gt;&gt; (range 1 10) ;gimme numbers 0..9 (filter odd?) ;keep the odds (map #(* % %)) ;square the numbers [])) ;realized as a vector. As I pointed out, loop (as with format) is just a macro. There's nothing stopping anyone from picking up the publicly available implementation (like SBCL) and porting it over if they really want it. Back to performance: SBCL seems capable of ripping through a simplified loop (no consing, just iterating over the range n times) in about 1.2ms. With a simple primitive loop, clojure hits ~4-5ms on my hardware: user&gt; (time (dotimes [i 100000] (let [bound 10] (loop [x 1 acc nil] (if (== bound x) acc (recur (unchecked-inc x) acc)))))) "Elapsed time: 4.096792 msecs" SBCL seems to hover around 1-2ms for a similar loop, although I think 1 is the mode (2 is shown here): CL-USER&gt; (time (dotimes (i 100000) (loop for i from 1 to 10 if (oddp i) do (progn nil)))) Evaluation took: 0.002 seconds of real time So by all accounts, Clojure is apparently 2-4x slower than SBCL here. I guess that's a death sentence for us going forward, if we extrapolate from microbenchmarks to real-world performance. Eschewing the function calls per-iteration we get in clojure (due to reduce), and exploiting naive mutation without the use of transients (via an array list, coerced to a seq on return), I can get down to about 25ms in Clojure (vs. 5ms from SBCL). This is approaching the compiled form that a Clojure implementation of Loop would produce, albeit there may be further optimizations I'm missing. user&gt; (time (dotimes [i 100000] (let [bound 10] (loop [x 1 acc (java.util.ArrayList.)] (if (== bound x) acc (recur (unchecked-inc x) (if (odd? x) (doto ^java.util.ArrayList acc (.add (* x x))) acc))))))) "Elapsed time: 25.59686 msecs" That's not far off from from the expected worst-case 4x difference we saw from just counting. So, if your day job revolves around writing microbenchmarks of this kind, then you have a competitive advantage expressing them in Loop (on SBCL). If you also believe that microbenchmarks are indicative of the holistic performance of the system, then I'd be glad to talk to you about investment opportunities in bridges :) Personally, for my work (even doing toys like AOC and Euler), this hasn't even risen to the surface. I get more than enough octane out of Clojure, while still leveraging basic building blocks I learned on day 1 to build complex stuff that performs well. I've also replicated that success with newcomers (including new to programming), which I attribute to aforementioned design choices in Clojure. To each their own though.
Thanks much for the feedback. I'm trying to learn how to think about these things so looking at how things like time and shape are implemented is helpful. HtDP and SICP (structure and interpretation of computer programs) are both often recommended on the Scheme (especially Racket) side of things. Racket itself has a library dedicated to HtDP (even though the book is targeted toward MIT-Scheme I believe).
Yeah, is an improvement, which is very minuscule, and also something that probably should have been since the beginning. The compromises the language make just to be hosted on the JVM are ridiculous.
&gt;Clojure by comparison is not even close. Maybe I'm myopic, but Clojure was the impetus for efficient HAMT datastructures, particularly to fulfill the notion of immutable-by default dynamic language (vs. static ala ML and friends), as well as infusing STM at the language level. I wouldn't toss those aside, considering the broad adoption HAMTs have gotten across programming worlds. Did Hickey push out ideas on par with Steele et. al, arguably not (in this case). I wouldn't diminish his valid contributions to the field though. From the standpoint of invention, I'm less impressed with propagators after reading through the implementation, particularly in light of prior work (re: constraint prop, distributed systems research [again Erlang gets no love], relational/logic programming, etc.). It seems in the class innovative composition rather than inventive creation. That being said, Sussman is a big brain, so I have no doubt there will be fascinating things to come. If you're interested in radical, out there research, maybe look at Chuck Moore and the forth folks (again no love here :) so far), and what they're trying to do with Green Arrays.
Rome wasn't built in a day :)
&gt; you change your toplevel map to a pmap and you're done seems like we have a similar library in CL ? lparallel, and another one to distribute the work on different machines: lfarm (https://github.com/CodyReichert/awesome-cl#parallelism-and-concurrency)
Careful. Rome did fall.
(a bit late and no much time:) Roswell might help: https://roswell.github.io/
The future's so bright, I gotta wear shades.
Thank you! Brain-fart from me...
I quite like the cl-str package. For me the functions work, the arguments are pretty much as I expect, and the whole thing is lightweight (CL-PPCRE is the sole dependancy). Then, I am still very new to this CL thing so I might well change my mind :-)
I mean if that's the hill you want to die on.
Sure. Or simply concatenate. I'm using cl-str for other part of the project so this comes for free...
It blows my that you have the time to dump into this shit thread, but that was a pretty nice demo of idiomatic Clojure through to non-idiomatic Clojure you had there. One thing to note on the CL example is that there, is that the language allows for further optimization in a standard way.
&gt;One thing to note on the CL example is that there, is that the language allows for further optimization in a standard way. Do you mean like compiler declarations? I know different implementations are free to optimize representations where the standard is intentionally unspecified. I just tossed the loop here into portacle (SBCL) for testing, so it'd be the defaults SBCL applies. I've got a companion thread probing the performance limits on the jvm, curious to see if that's a factor for something that seems "should" be identical (like iteratively counting). &gt;that was a pretty nice demo of idiomatic Clojure through to non-idiomatic Clojure you had there. Glad it was well received.
Shape drawing library should probably be using the Bresenham line and circle algorithms. The circles look wacky, in particular, with vertical steps where the curve is mostly horizontal and vice versa: OOOOO OOOOOOO OOO OOO O O O O ---&gt; O O OO OO O O O O O O 
Wow. Thank you genuinely for writing such a detailed analysis. I still have my opinion, but it's quite fun to think about all these issues. I agree that micro-benchmarking is dubious. I'm mostly interested in qualities of expression. Your english rendition of the code is much better than mine. Perhaps I'd like it if we could say: (into-&gt;&gt; (gimme 1 10) (keep odd?) (them #(* % %))) In Lisp I could also say[0]: (coerce (mapcar (_ (* _ _)) (remove-if-not #'oddp (iota 10))) 'vector) but it just doesn't seem to roll off the keyboard as nicely. [0] Given the cute macro: (defmacro _ (&amp;rest exprs) `(lambda (_) ,@exprs)) and `iota` the function from the `alexandria` library. 
&gt;most primitives return values without changing the original value So what you're saying is that when you make a change instead of updating the value in place or creating a revision you end up doing wholesale copy of the data. That's preferable why again? &gt; Most lisp implementations like SBCL often run faster than the JVM Clojure implementation, so I wouldn't worry. I would love to see some benchmarks for this, cause last I checked the opposite was the case. The JVM has had an incredible amount of work put into its runtime and the JIT. SBCL simply doesn't have the resources to compete with that. &gt;But Lispers won't do such a thing because its silly. Want immutable data in your code? write it that way, period. Defaults matter, and immutability is the right default because it removes mental overhead. The reality is that vast majority of the code doesn't run all that often. It's far more important to optimize for readability in order to keep your projects maintainable. Once you actually run into performance problems, you should profile your code, find the parts that are bottlenecks and optimize those. And of course Clojure provides tools, such as transients, for working with local mutable data.
Meanwhile at Netflix: &gt;I wrote and deployed (to production) some Clojure code at Netflix just yesterday. Among other things at Netflix the Mantis Query Language (MQL an SQL for streaming data) which ferries around approximately 2 trillion events every day for operational analysis (SPS alerting, quality of experience metrics, debugging production, etc) is written entirely in Clojure. &gt;This runs in nearly every critical service, ~3000 ASGs and easily &gt; 100k servers and Clojure allows us to also compile it for our NodeJS services as well. https://news.ycombinator.com/item?id=18346043
Kay is a big supporter of the Lisp concept and was directly inspired by Lisp when creating the first versions of Smalltalk. In particular, Smalltalk-72 has a very Lisp like behavior and structure: bitsavers.informatik.uni-stuttgart.de/.../Smalltalk-72_Instruction_Manual_Mar76.pdf 
&gt;I'm mostly interested in qualities of expression. As am I. I think that's an important thing to be interested in. Otherwise, we would just blub out. One note about *into* (since I'm a bit of a fan), and the resulting *into-&gt;&gt;* macro, is that the use of [] (or any other data structure) is important because into is polymorphic via the definition of *clojure.core/conj* which it's built on. So we further get composed behavior by changing the target container, which could be a partially-filled collection, like [1 2], a set #{}, a map {}, or anything users define conj over (like priority queues and custom types). If we dump into a list, we get cons behavior (prepending elements). Sets will naturally deduplicate the input (or we could handle that using *clojure.core/distinct*). We can leverage the same pieces to build maps fairly declaratively too: user&gt; (into {} (map (fn [n] [n :hello])) (range 10)) {0 :hello, 7 :hello, 1 :hello, 4 :hello, 6 :hello, 3 :hello, 2 :hello, 9 :hello, 5 :hello, 8 :hello} If possible, *into* will leverage transients to efficiently construct the result (without complicating the expression). Perhaps the greater observation (regarding expressiveness), is the lack of limitation on how we go about doing this in either language, thanks to macros. That's pretty cool.
Lucky is not a shithole like web development is.
There is a qualitative difference between saying "nothing here is new" rhetoric and "here is a reference where it comes from". "Nothing here is new" has various interpretations, some of which are "I've talked about this before" or "this was introduced in my project several versions ago". Words like this are easily ignored by the listener as filler material.
Transducers are interesting, but but more of a solution in search of a problem. Let's look at the example of the composed transducer kernel in the Clojure doc: (def xf (comp (filter odd?) (map inc) (take 5))) TXR Lisp, no transducers; just the basic `opip` macro that combines pipeline/threading with partial application of `op`: 1&gt; (opip (keep-if oddp) (mapcar succ) (take 5)) #&lt;intrinsic fun: 0 param + variadic&gt; Now we apply this to a list: 2&gt; [*1 (range 1 100)] (2 4 6 8 10) (But, but, transducers combine the core of these steps into a tight kernel that is just reduced over once!) So? (So, you really need this in your application to shave off cycles!) No, believe it or not, I don't. The code is golfed to a decent level, which pleases me, and then I'm pretty much BGAF: beyond giving a fuck. Before I optimize anything, I need a real world scenario with a business case.
&gt;So what you're saying is that when you make a change instead of updating the value in place or creating a revision you end up doing wholesale copy of the data. That's preferable why again? Strawman argument! I never said the data was "copied wholesale", those are your words not mine. What happens in reality is implementation-dependent and thus implementations can optimize as they want. &gt;I would love to see some benchmarks for this, cause last I checked the opposite was the case. The JVM has had an incredible amount of work put into its runtime and the JIT. SBCL simply doesn't have the resources to compete with that. You're assuming clojure compiled code will run at java compiled code speed, that's not the case. In the same way as ECL, a common lisp implementation, compiles down to C, but that doesn't mean it will make Lisp code run at C speed. If you examine this thread you'll find two examples where CL was massivelly faster. &gt;Defaults matter, and immutability is the right default Yeah, everybody likes being told what to do, because freedom is scary and thinking deeply is stressful. &gt;It's far more important to optimize for readability in order What does immutability has to do with readabllity? I'd argue that language-oriented programming will be more fundamental to readabllity, fortunately it can be achieved just fine in CL or Clojure or Scheme. &gt;And of course Clojure provides tools, such as transients, for working with local mutable data. Good. 
if you say so
&gt;Strawman argument! I never said the data was "copied wholesale", those are your words not mine. What happens in reality is implementation-dependent and thus implementations can optimize as they want. So enlighten me what is the implementation here, because the only approaches I'm aware of are mutating in place, copying, or revisioning. You're saying it's not revisioning the data, and it's not mutating, so what is it doing? &gt;You're assuming clojure compiled code will run at java compiled code speed, that's not the case. It is if you optimize it and add annotations, I can count the number of times I've had to do that on the back of my hand though. And if that wasn't fast enough I could just drop down to write Java, which I never had to do yet. I could even just [emit JVM bytecode](https://aphyr.com/posts/341-hexing-the-technical-interview) directly from Clojure if I really wanted to. &gt;If you examine this thread you'll find two examples where CL was massivelly faster. Now that's a straw man if I've ever seen one. &gt;Yeah, everybody likes being told what to do, because freedom is scary and thinking deeply is stressful. No, I just like the computer to do the work that the computer is perfectly capable of doing instead of having to do it by hand like a peasant. Having immutability is just like having GC. With GC, the language manages the memory instead of me having to do it by hand. With immutability the language manages references for me so I don't have to. I guess some people just like doing extra work they don't need to be doing because reasons. &gt;What does immutability has to do with readabllity? Immutability allows me to safely do local reasoning about the code. I work with a team of 5 devs, we have projects that have been in production for years. When I go look at some code I haven't seen before, it's really important for me to be able to read it in isolation and not worry about what else is going on in the project. This is something you simply can't do working with mutable data because to know what any piece of code is doing, you also have to know what all the places that reference the data it's working on are doing and how they all relate to each other. So with immutability the mental overhead of understanding parts code stays constant, while with mutability it grows with the size of the project. 
Excellently written article. Really fun. Thanks for posting this!
User absolutely require edification about historic points. That's why in the TXR Lisp manual, I have "Dialect Note" sections. I anticipate that users may run into problems when I do things similarly but differently. Or even when they are mostly the same. About circle notation, I wrote this: &gt; #### Dialect note: &gt; Circle notation is taken from Common Lisp, intended to be unsurprising to users familiar with that language. The implementation is based on descriptions in the ANSI Common Lisp document, judiciously taking into account the content of the X3J13 Cleanup Issues named PRINT-CIRCLE-STRUCTURE:USER-FUNCTIONS-WORK and PRINT-CIRCLE-SHARED:RESPECT-PRINT-CIRCLE. Even your Linux man pages tell you stuff like "this appeared in 4.3 BSD". Or other notes. For instance `select` man page: On Linux, select() modifies timeout to reflect the amount of time not slept; most other implementations do not do this. (POSIX.1 permits either behavior.) This causes problems both when Linux code which reads timeout is ported to other operating systems, and when code is ported to Linux that reuses a struct timeval for multiple select()s in a loop without reinitializing it. Consider timeout to be undefined after select() returns. 
Okay. FWIW, I gave up on UIOP and used the inferior-shell package. This not only made the coding cleaner, it gave me some bonuses, like being able to time the run. It now looks like (defun generate (id) "create the image" (let ((fn (str:concat "./tmp" id)) (cmd "")) (with-open-file (stream fn :direction :output) (put-stuff-in-file-here)) (setf cmd (str:concat "lilypond -s --png -o " fn " " fn)) (inferior-shell:run/nil cmd :time t)) Many thanks for the help.
That's a great library. I remember it from the 1.0 days. Looks like some serious improvements since then. Familiar feel too.
Is fact but whatever that let you sleep at night.
ok
* destructuring binding in function definitions is quite common, fairly easily achieved by a macro. TXR Lisp: 1&gt; (mapcar (tb ((a . b)) (+ a b)) '((1 . 2) (4 . 5))) (3 9) `tb` stands for `tree-bind` (what `destructuring-bind` is called). Not inspired by Clojure in any way; just common sense. * multiple arg lists: look a lot like `syntax-rules`, and pattern matching in functional languages. * comma as whitespace: IIRC, optional commas existed in the earliest Lisp works of John MacCarthy. * succinct anon function syntax: old hat. For instance the [`#L` reader](https://gitlab.common-lisp.net/iterate/iterate/blob/master/iterate.lisp#L463) in the `iterate` source code. Heck, ancient C++ Boost had ways of defining functors with `_1`, `_2`, `_3` arguments indicating where the anon arguments are inserting. Example of `#L`: (defun walk-arglist (args) (let ((*top-level?* nil)) (walk-list-nconcing args #'walk #L(if (is-iterate-clause? !1) (list (prognify !2)) !2)))) Here, since there is a `!2` argument, we get a two-argument anonymous function, equivalent to `(lambda (arg1 arg2) (if (is-iterate-clause arg1) (list (prognify arg1)) arg2))`. * threading macros: Doo Hickey almost certainly cribbed this from my [`comp.lang.lisp` posting](https://groups.google.com/forum/#!msg/comp.lang.lisp/079yqEOycHU/DkuDDWABAtMJ) in a 2002 thread titled "chain of transformations". This is inspired in a straightforward way by Unix shell pipelines. * treating data structures/keywords as functions: old hat. Smalltalk, Lisp dialects. ANSI CL allows keywords to have functionb indings, by the way: `(defun :foo (...) ...)` works. * `1+` is a bit of an ugly name, but `inc` is just stupid for something that has no side effects. It calculates the successor of an integer. Pascal has `succ` function, so that's what I used in TXR Lisp, leveraging Niklaus' wisdom, for what it's Wirth. 
&gt;So enlighten me what is the implementation here, Implementation-dependent, I said. Because CL, unlike Clojure, has many implementations (unless ClojureCLR can now magically use all of CLJ ecosystem). Each implementation can do optimize as it wishes. &gt;It is if you optimize it and add annotations Good; just like in CL; however since you don't compile down to machine language, you still have the impedance mismatch problem -- the JVM doesn't really suit a FP-flavored dynamic language. We at the Lisp camp have seen how the ABCL creators have suffered to implement Common Lisp on the JVM. It works, acceptably fast, but still very slow compared to SBCL, CMUCL, LispWorks and other implementations. &gt;I could even just emit JVM bytecode directly from Clojure if I really wanted to. Yes, and we can assemble and emit machine language from Lisp source, or rewrite in C and call from Lisp (CFFi maked this easy) but this is a meaningless comparison, since it isnt Lisp or Clojure anymore. &gt;With immutability the language manages references for me so I don't have to. I guess some people just like doing extra work they don't need to be doing because reasons. Common Lisp allows both mutable and immutable code, it just depends on how do you use it. It doesn't force you to write mutable code. &gt;Immutability allows me to safely do local reasoning about the code. I work with a team of 5 devs, we have projects that have been in production for years. When I go look at some code I haven't seen before, it's really important for me to be able to read it in isolation and not worry about what else is going on in the project. &gt;This is something you simply can't do working with mutable data because to know what any piece of code is doing, you also have to know what all the places that reference the data it's working on are doing and how they all relate to each other. You know that, because of lexical binding, you can do mutation of data but only within a closure, or within a function, or within a package(namespace), right? Using mutable data *doesn't* necessarily mean global variables that span the whole system; obviously. 
I'm not familiar with str:concat, but back when I used to work in CL, I remember using format nil with various directives to concatenate strings, or sometimes with-output-to-string. I find the even simpler '(concatenate 'string "Karl" " " "Marx")' at https://lispcookbook.github.io/cl-cookbook/strings.html 
&gt;If you examine this thread you'll find two examples where CL was massivelly faster. Lol, toy benchmarks and anecdotes. No evidence presented, no credibility. Your confirmation bias filter continues to impress the fans.
These ones already existed: &gt;metadata syntax/support How can this be new? In Common Lisp, since its inception, all symbols have metadata: http://clhs.lisp.se/Body/f_symb_4.htm#symbol-plist &gt;gensym in macros syntax See Scheme and it's macro system. Exists since Scheme exists -- Since 70s at least. &gt;function based multimethod dispatch Scheme. The function position (in a function call) is also evaluated, so you can do exactly that way. &gt;quite a few changes in the way functions behave (e.g. let* as default in let) That can't be regarded as an innovation, just a different preference &gt;quite a few changes in function naming (e.g. inc instead of 1+) That can't be regarded as an innovation, just a different preference. (Also, see `incf` and `decf` in Commom Lisp) &gt;This doesn't include the most innovative parts of Clojure (... **immutable data structures as defaults**) Haskell, late 80s. And probably other languages before. 
&gt;For instance the #L reader in the iterate source code Another reason for me to take a good look at `iterate`, thanks!
&gt; 3rd party thing There is "3rd party thing" as in "Big Library version 3.9.13 with Monstrous API", and there is "3rd party thing" as in a "40 lines of macro I can cheerfully copy and paste into my code as if it were mine". 
This is not a discussion about what you need. The question what is the difference between lisps and clojure and I answered. In this simple case you dont use transducers but if you want to reuse pipelining logic in async, eager and lazy ways its actually fantastic separation between logic and execution pattern not to mention that its faster and you can even go parallel if you like. For example you want to batch data into a database that you are getting on some queue. You can do partition-by and the execution will automatically block and wait until the batch is full and then pass it down the pipeline. You can use the exact same transducer eagerly when loading from a file. No compromise, you get fantastic performance however you apply it. 
I am sure you would figure it out. Good luck!
Common Lisp is the product of a messy standard process that was explicitly about compatability between old dialects. And comparing Clojure a tiny language community with the 40 year effort and untold billions invested in BSD. Clojure is one project with a nice history back to the first commit. The vars in the code are tagged with the version numbers, you can print it on the repl if you are interested in 'first appeard'. The more conceptual stuff is documented videos and actually has rational choices behind the design, not a standard process design to run as much messy code as possible. Clojure is practical language to solve problems, not an intellectual history class. You complaint is essntially 'Somebody was wrong on the internet and that person liked Clojure so Clojure is to blame for not educating all its user about the 50 year history of Lisp and other languages'. Not to mention the amount of crap CL people say about Clojure but I don't blame CL documentation for that.
Fascinating doc, particular linking to DTIC (all the cool defense work happened back in the day it seems). I think there's plenty of daylight between series and transducers (as presented by Hickey, not the definition of transducer in the DTIC document, where they define *transducer* as functions mapping series to series). SERIES is closer to the forefather of loop / array fusion as a means to optimize composed transforms into singular loops. Notably, the goal of optimization ran into corner cases, and came with noted restrictions as presented in the paper. Two of the telling differences are the sequential or pre-order processing requirements, the stated goal of transformation into compiled (fused) loops. In his [Strange Loop talk](https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/Transducers.md), we see the origin of both Hickey's idea and the naming. Being of the functional programming mindset, where reduce is kind, and having just spent a significant amount of time porting Guy Steele's tree-based (vs. sequential) monoidal framework for parallelization into clojure.core/reducers, while subsequently porting CSP from core.async, the amount of redundancy across seq, reducer, and channels stuck out. The resulting abstraction focused on the generalities introduced to implement reducers (trivial function composition inside of a reduction). The optimization properties (parallelization, lack of intermediate collections or lazy invocation) naturally fell out of the existing reducers framework. Clojure transducers inherently optimize (since they aren't forced to create multiple lazy sequences as with the original seq libraries), but the mechanism is completely different. They're just functions and things that implement the reducible protocol, rather than code-rewriting optimizing compilers. Consequently, transducers are values, are amenable to straightforward function composition, and incur none of the caveats listed in the SERIES document. In fact, the mechanism is different enough that the optimization provided by SERIES - when fully applied - could go further than transducers, since certain expressions (given a sufficiently smart call graph compiler) could be fused away. Clojure transducers "fuse" by function composition, in addition to adding a check for early termination via the reduced type. There will inherently be more function calls, which could be slower than an fused SERIES. So, macros vs. functions again. Rich Hickey's lack of citation doesn't seem malicious or even appropriate since transducers drew inspiration from reducers (and transitively - as cited - through Steele).
&gt;Transducers are interesting, but but more of a solution in search of a problem. They generalize a swath of operations on collections (or reducible things) over apparently disparate data types, to include async channels, lazy sequences, eager collections, and parallel computations. They do so using the extant framework of reduce and function composition. They allow you to reuse more code, more efficiently, and they themselves compose (ala the functional paradigm). I think it's one of the greater refactorings I've seen to date, with no downsides.
&gt;Each implementation can do optimize as it wishes. You're suggesting that in some unnamed implementations it is optimized beyond naive copying. I'm asking you how this optimization works. You still haven't answered that question. &gt;Good; just like in CL; however since you don't compile down to machine language, you still have the impedance mismatch problem -- the JVM doesn't really suit a FP-flavored dynamic language Turns out the JVM is a great fit for a dynamic language as long as the language is designed around the JVM as Clojure is. The problem with CL on the JVM is rooted in the impedance mismatch between the JVM and CL. &gt;Common Lisp allows both mutable and immutable code, it just depends on how do you use it. It doesn't force you to write mutable code. I never said it forced you to, but you still haven't explained how immutable data is handled efficiently. &gt;Using mutable data doesn't necessarily mean global variables that span the whole system; obviously a bad idea. Problem is that you never know what the use of a particular variable spans, could be a function, a closure, a package, who can tell... 
&gt; In Common Lisp, since its inception, all symbols have metadata Data structures don't. Metadata is pervasive (excepting keywords, by design), to include collections, symbols, forms, and user-defined types participating in the abstraction.
&gt; immutable data structures: Those are COW value types. Not the same as efficient immutable collections (in addition to efficiently hasheq immutable object types), which is what OP meant. FSet doesn't buy you the same performance gaurantees, nor do Okasaki's collections or Bagwell's (as originally designed). Hickey's stuff does.
Yup. There are all kinds of ways of doing this job. Looking in the cl-str package, str:concat is defined in terms of concatenate, so, in this case, it is purely a matter of syntactic sugar :-) (defun concat (&amp;rest strings) "Join all the string arguments into one string." (apply #'concatenate 'string strings)) &amp;#x200B;
I don't know if it would make much of a difference in performance, but I wanted to note that your use of "comp" seems odd. Functions given to comp are applied right-to-left. While the end result in this case is the same, the "map" function is called for every number with your use of "comp", instead of just the odd ones. So to get the same behaviour as your "-&gt;&gt;" example, the comp usage should be like this for your "into" and transducer examples: (comp (map #(* % %)) (filter odd?)) And your macro would need to change to something like (defmacro into-&gt;&gt; ([to] to) ([from to] `(into ~to ~from)) ([from f &amp; args] (let [xs (butlast args) to (last args) xform (if (seq xs) `(comp ~@(reverse xs) ~f) f)] `(let [xform# ~xform] (into ~to xform# ~from))))) Again, I don't know if this would make much of a performance difference in this case, but I would be curious none the less. All that being said, thanks for your interesting writeup. :)
I do. Although, like my opinion on moon farming, it's probably of little value, since I haven't tried it. But, I do really like the idea, in `series`, of optimizing a whole functional collection iteration pipelines, and I imagine both Clojure and CL could benefit from more of that. I'd really like to see it applied to GPU pipelines.
Hi, thanks for the plug! I have plans to open source a lot more of the deftask source code. Just need to polish the relevant bits, extract them out in a library, add documentation and profit 😎 Hopefully I will find time to release some of it in the coming weeks. 
&gt; Data structures don't. Metadata is pervasive (excepting keywords, by design), to include collections, symbols, forms, and user-defined types participating in the abstraction. Well, then this is a truly good idea in Clojure, thumbs up.
&gt;I don't know if it would make much of a difference in performance, but I wanted to note that your use of "comp" seems odd. Functions given to comp are applied right-to-left (https://clojuredocs.org/clojure.core/comp). While the end result in this case is the same, the "map" function is called for every number with your use of "comp", instead of just the odd ones. Actually, that's not correct, the usage of comp for transducer "stacks" is necessary due to the [implementation of transducers](http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming), and the order of applications is accurate (numbers are filtered for odds, and only the resulting elements are squared). We can see this in action: user&gt; (into-&gt;&gt; (range 10) (filter odd?) (map (fn [x] (println x) (* x x))) []) 1 3 5 7 9 [1 9 25 49 81] It works because the implementation of transducers (based on reducers) that *into* uses under the hood already relies on function composition, but instead of transforming values in the collection, you're transforming reducing functions. The effect is that you're communicating how to reduce the "inner" reduction via function composition. The net effect (say in the case of mapping and filtering) is that the effective order of composition changes (since you're composing transducers vs. just straight function composition). It took me a several tries, along with reconstructing (first reducers, then later transducers), to grok how this works. This tiny example communicates how composition can be "reversed" based on the underlying functions being composed: (defn square [x] (* x x)) (defn double [x] (* 2 x)) (defn pre-square [f] #(f (square %))) (defn pre-double [f] #(f (double %))) (def double-then-square (pre-double (pre-square identity))) ;;(pre-square identity) =&gt; (fn [x] (identity (square x))) ;;(pre-double (pre-square identity)) =&gt; ;; (fn [x] (identity (square (double x)))) The eventual order of function application is (square (double x)), as in (-&gt;&gt; x double square), or (comp square double) (double-then-square 2) ;;=&gt; (identity (square (double 2))) ;;16 So, we can use the standard clojure.core/comp to build stacks of transforms in similar applicative contexts, so that the order of arguments maps with the "flow" of data through transformations like -&gt;&gt; or -&gt; (def double-then-square ((comp pre-double pre-square) identity)) The transducer implementation in Clojure operates on reducing functions, composing in a similar manner. I like to use macros like -&gt;, -&gt;&gt; to flow data through a "computational pipeline" of transformations, particularly by chaining together sequence functions like map, filter, take, drop, etc. I can't use -&gt; and -&gt;&gt; directly with transducers, but I can achieve a similar effect (due to how transducers compose), by wrapping the stages of transformations (from left to right) inside (comp step1 step2 step3 ... stepn). This allows definition of arbitrarily composeable computational pipelines for use in more sophisticated transducers. The core sequence functions return transducer variants if no collection argument is presented, so these pipelines are often visually identical, and the semantics of mapping, filtering, etc. are generalized across anything that can be reduced. I can then use transducers idiomatically with *into*, or *transduce*, where a little convenience macro like *into-&gt;&gt;* the elides the need for the user to provide the explicit (comp ..). This just directly express the pipeline ala -&gt;&gt;, which makes defining data pipelines that dump into collections even easier IMO. Rich Hickey's [StrangeLoop talk](https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/Transducers.md) which includes a full deriviation and is helpful for understanding more (along with the background and inspiration). &gt;All that being said, thanks for your interesting writeup. :) Thank you for the feedback :)
Awesome news :) and in addition, keep up the good blog posts :) I'll also test and review it, and reference it where I can.
I just built it for myself with no intention of distributing it for use by others. I just wanted to show people what I was working on. I didn't want anyone to take me seriously when I said to use it :)
Thanks, I do hope to write regularly from now on 😀
quick feedback: you could say sooner that it is free for personal use (up to small teams ATM), it doesn't appear on the front page.
I found this article by a Common Lisper on Clojure worth the read: https://lisp-univ-etc.blogspot.com/2011/11/clojure-complexity.html
&gt; pmap in this article, about pmap : https://lisp-univ-etc.blogspot.com/2011/11/clojure-complexity.html &gt; To be able to provide them Clojure imposes some restrictions, most notably the one of immutability. Also in their implementation it doesn't follow the Lisp principle of giving control to the programmer: you can't swap one STM variant/strategy for the other. Heck, you can't even control the number of threads, that pmap uses! Did you know about CL's lparallel ? thx
&gt; things like namespaced keywords Pale in comparison to NewLisp contexts. 
&gt;One would expect, given all of the computing power at disposal that this would be a significant increment. I would not necessarily expect additional computing power to fundamentally alter computing for the better. More resources (both memory and CPU) tend to justify less time optimizing simpler things under tight constraints, trading off for optimizing "programmer time." This also means we don't sweat resource utilization when perhaps we really should (witness electron, and even JVM apps that routinely expect a 1gb+ heap...). The amount of waste we incur in dynamic languages due to pointer chasing, lack of mechanical sympathy, garbage collection, boxing, etc. is ridiculous from the machine perspective. We're routinely writing bloated, slow code, but somehow that multiple order of magnitude performance gain is an acceptable trade off relative to productivity. I think it also lowers the barrier for correctness, and enables sloppy thinking. Ironically, the baseline feature Hickey included in Clojure (betting on the expected growth of many-core shared-memory architectures) - manageable concurrency via STM and immutable structures - is largely underutilized since the core count hasn't reached the inflection point where it could challenge locality of reference. It could very well some day, in which case Clojure is somewhat "future proof." It's pretty cool that I can pop most of my code into a large EC2 instance and reap immediate rewards, or get immediate returns (without jeopardizing correctness) on my desktops and laptops while making use of the extra cores. It may yet turn out to be a significant increment. &gt;then Mr. Joswig who knows a lot about Lisp history said that this was already invented many years ago Indeed, except when you look at the history a bit closer, the invention appeared to take two contemporaneous evolutionary paths: - imperative loop fusion via macros --program optimization / context-sensitive macros / generic code-rewriting rules - functional programming -- reduce/fold / basic function composition / algebraic properties leading to generalizations over map, filter, early termination, and later parallelism Hickey actually cited his source for both the term "transducer" (which he apparently coined from reducer, which he coined after implementing Steele's parallelism model in Clojure) which is a bit different than the SERIES paper's construct (mappings between series). Hickey also showed the derivation of transducers, by way of Bird/Meertens FP research in the 80's leading to Hutton's paper on the universality of fold, through Steele's work on parallel folds using concatenative vs. sequential constructs circa 2008. I think the novelty of implementation Hickey realized by the function composition route - while possibly less optimized than baking loop optimizations with a special compiler - actually fulfills some of the generalities Waters identified as extant research frontiers (particularly, portable language-agnostic loop fusion constructs). Notably, the Waters' SERIES paper discussed an additional independently developed system (collectors and gatherers) that appeared to be wholly unknown until drafting CLTL2, along with no references to the contemporary FP work referenced by Bird. &gt;What I am trying to say, and what my impression is, is that Clojure is nothing but one of those increments, not some amazing thing like it is considered by many (that's my impression). I think the audience reaction to Clojure varies, from the fans who learned to proclaim and follow rock stars or similar paragons of programming, to burned-out programmers thirsting for a drastic - yet practical - change to the enterprise dogma, to FP refugees, and Lisp refugees, to old timers who truly see "nothing new under the sun." If you've never been exposed to Lisp, FP, etc. Clojure could be quite mind blowing (I've seen that reaction on both colleagues and interns). I came from both Lisp and FP (by way of Scheme/SICP and CL / Land of Lisp / PCL) and the statically typed world (F#, Haskell) and a little bit of enterprise drudge when I started using Clojure. My impression continues to be that the sum of the increments are awesome, even if the language doesn't provide an epochal leap in programming evolution (what language does though...). Even as I've sampled more from other environs with strong paradigms (Erlang, Prolog), I remain impressed by the design and continued incremental improvements. Some increments (or perhaps the complete package) may be worth 80 IQ points, if they provide the proper shift in perspective (as Alan Kay says).
&gt;I think the audience reaction to Clojure varies, from the fans who learned to proclaim and follow rock stars or similar paragons of programming, to burned-out programmers thirsting for a drastic - yet practical - change to the enterprise dogma, to FP refugees, and Lisp refugees, to old timers who truly see "nothing new under the sun." If you've never been exposed to Lisp, FP, etc. Clojure could be quite mind blowing (I've seen that reaction on both colleagues and interns). &gt; &gt;I came from both Lisp and FP (by way of Scheme/SICP and CL / Land of Lisp / PCL) and the statically typed world (F#, Haskell) and a little bit of enterprise drudge when I started using Clojure. My impression continues to be that the sum of the increments are awesome, even if the language doesn't provide an epochal leap in programming evolution (what language does though...). Even as I've sampled more from other environs with strong paradigms (Erlang, Prolog, a little Forth and SmallTalk), I remain impressed by the design and continued incremental improvements. Some increments (or perhaps the complete package) may be worth 80 IQ points, if they provide the proper shift in perspective (as Alan Kay says). Agreed. To me Clojure is also awesome. But I am sure Lisp experts still mostly use CL, or Scheme. There were some interesting links on this thread. Clojure should be more heavily criticized. But one should recognize that it is manly a corporate artifact. There is this company Cognitect, etc. So hype surrounding it is understandable. Things were moving faster back in the day CL was starting I guess because a lot of the work was done in the research lab settings, so people could actually afford to throw away things which weren't contributing to the progress. Maybe there is also research in dynamic languages, but people in the industry seem to not pay attention. They are either carried away by the Clojure hype, or are ignorant of the research (I think STEPS project could be regarded as that kind of research).
Good idea! I’ll put it up - thanks for the tip. 
&gt;But I am sure Lisp experts still mostly use CL, or Scheme. I am less sure. Peter Norvig no longer uses CL apparently; he also has a seemingly favorable [opinion of Clojure](https://www.quora.com/What-do-veteran-Lisp-programmers-think-of-Clojure/answer/Peter-Norvig). Paul Graham suggests using Clojure on twitter, which was [linked on this forum](https://www.reddit.com/r/lisp/comments/a3a2dj/why_clojure_why_lisp/#ebchnml). Daniel Weinreb [posted](https://stuartsierra.com/2009/02/22/its-about-the-libraries): &gt;Speaking as one of the original “Gang of Five” authors of “Common Lisp: The Language”, I am at this point convinced that Clojure is the future of Lisp (for production applications; Scheme will still have its place). I think all of those things (libraries, mostly-functional, transactions, maybe not reader macros) are very important. So is the general cleanup and the use of protocols throughout. These are all things I had been advocating for the last several years, including running on the JVM, so you can imagine how delighted I was the first time I heard Rich’s talk on Clojure. [Kenny Tilton](https://github.com/kennytilton?tab=repositories) started moving (dunno about wholesale) to Clojure(script) a few years back. I consider these folks among the class of experts (if not prominently so); so long as there is an appeal to authority to be made. I don't know of any statistics, particularly on degree of migration to use for an expert population. Also, one must caveat whether Scheme is considered a Lisp, [to many it's not](http://wiki.c2.com/?IsSchemeLisp), which is something that prominent "experts" ended up espousing in the aforementioned thread - something which I also disagree with. If it's not - as some experts in this forum state - then your population of Lisp experts is limited, and you can't include Scheme. Perhaps that also implies folks working in Racket are not Lisp experts... &gt;Clojure should be more heavily criticized. It's criticized at least annually, and frequently in r/clojure, in the dev forums, slack, discord, google groups mailing list etc. There have been plenty of community rows over the last two years directly criticizing the language development (and many more before that). I think a subset of r/lisp is carrying disproportionate the water for the criticism crusade too, so that's a nice constant critical factor. The current long-standing community gripe has been error messages and documentation, followed by contribution/acceptance of patches. Thankfully the design isn't static for 25 years, and things are open to improvement. &gt;But one should recognize that it is manly a corporate artifact. The bulk of the reasoning behind creating a Common Lisp standard was driven by enterprise concerns, as I understand it. Something John McCarthy (hopefully qualified as a Lisp expert) appeared [to later lament](https://groups.google.com/forum/m/#!topic/comp.lang.lisp/4iUYVwonx7k). &gt;Maybe there is also research in dynamic languages, but people in the industry seem to not pay attention. Clojure has a port of the propagator model you mentioned, extended to use optional STM for state management. David Nolen gave quite a few talks on applying results from research into industry (core.logic is one example). Hickey leveraged the research done on Fortress by Guy Steele's research group to implement a general tree-based reduction model. The community I regularly interact with is pretty diverse, and includes many folks who regularly dip into the research aspects of CS and often bridge them successfully using Clojure as an implementation language. ([Anglican](http://probprog.ml/anglican/) and [metaprob](https://github.com/probcomp/metaprob) are active research projects in probabilistic programming funded by familiar faces (DARPA, formerly ARPA, ONR, AFRL, etc.) and the MIT Probabilistic Programming Project, both of which are platforms for continued [research and publication](http://probprog.ml/anglican/literature/). &gt;They are either carried away by the Clojure hype, or are ignorant of the research (I think STEPS project could be regarded as that kind of research). I think there is plenty of ignorance to go around then. [Eve](https://github.com/witheve/eve-experiments) was a failed attempt to advance programming, explored in Clojure/CLJS, ultimately ending up in typescript. It was more-or-less a composite language/IDE/database/hypercard/spreadsheet research project inspired by early experiments with [LightTable](http://lighttable.com/). LightTable stemmed directly from Bret Victor's (who contributed to STEPS) presentations on improving human/computer interaction. Although I prefer Emacs as my operating system, LightTable has some neat ideas, and was implemented in Clojure(script). Eve either was too ambitious or ill-defined and never really got into orbit. [hyperfiddle](http://www.hyperfiddle.net/) is a distributed, automatically version controlled hypercard-like environment for rapid application development that seamlessly lives in the cloud or on premise. There's some neat stuff here too, particularly the integration / embedding with Datomic and automatic distribution. You get tight integration with a powerful eavt store, a built in IDE, integrated data exploration/linking, and extensible client-side or server-rendered presentation layers (cljs / reagent, markdown, more in progress), and everything (including code, fiddles) is stored in datomic (thus temporally versioned at the eavt relational level). &gt;I think STEPS project could be regarded as that kind of research One view of STEPS is the ultimate code golfing exercise: provide a minimal (measured in LOC) Smalltalk(ish) implementation that can provide the graphics and sub systems necessary to develop a rich, interactive document editor. As a twist, your "golf clubs" are different DSLs used to achieve the relatively impressive information compression gains across the "nOS" and the application. Proliferation of languages doesn't count, and neither does actual performance (although runtime performance would be nice/compelling). &gt;"We think (and assert) that in the end, human brains are overwhelmed by size, and that the additional overhead of learning several languages more than pays for itself in producing compact “mind‑sized” code. There are no substantiations of this at present, but it would be an interesting experiment to try as a follow‑on to this project. " After playing with and implementing eDSLs (less cognitive load than full fledged DSLs), I'm on the fence about that assertion. They're observing significant savings (measure in code size) but they don't have a quantitative measure for comprehension across languages, or the language bundle you now have to hold in your head to understand any heterogeneous program, or the time and effort required to design effective languages. Modern web programming could be seen as a powerful indictment against the proliferation of little languages (perhaps an extreme case). The "nothing new under the sun" side of me says, "meh, DSLs are known information compressors, it's possible they're just squeezing the balloon, pushing the air of complexity somewhere else..." I did find the "engine room" tech a fascinating contribution, particularly the COLA (later MARU) and OMeta. The tech reminds me of some of the recent research aimed at Truffle and Graal (they're approaching the problem slightly differently, but the end goal looks similar). Research into Towers of Interpreters (and the recent [collapsing towers of interpreters](https://www.cs.purdue.edu/homes/rompf/papers/amin-popl18.pdf) ) seems relevant as well.
&gt;Clojure is nothing but one of those increments, not some amazing thing like it is considered by many This strikes me as the straw man at the center of much confusion in the comments of this post. By "amazing thing" you seem to mean "fundamentally new approach" from the perspective of PL theory or offering a new mode of programming. But I don't think anyone's saying that. Clojure's unique selling proposition has always been that it is a practically-minded modern lisp dialect on the JVM (and JS). It's the pitch on Clojure's website, it's what Alex Miller and Clojure users are saying in this thread, it's the message I see from Rich and Cognitect and Clojure enthusiasts. People like Clojure because it's a dev-friendly, coherently-designed FP-first lisp that deploys to platforms that clients are familiar with. The part of Clojure that its enthusiasts call amazing has always been its pragmatism, expressed in its host interop, small/medium-size changes from previous lisp dialects (e.g. syntax and seqs), that sort of thing. Do you encounter a lot of claims that Clojure fundamentally advanced programming language theory? Where? You mention transducers as an example of such a claim: &gt;somebody on this thread mentioned transducers, and then Mr. Joswig who knows a lot about Lisp history said that this was already invented many years ago ..but those mentions all *explicitly* acknowledge that transducers have prior art and are interesting not because they are theoretically novel but because they are being implemented in business contexts, for practical use. Look: &gt;There are other parts that feel like new ground, **even when based in other work**, like transducers Emphasis mine. The same holds for jwr's transducer comments, which mentions SERIES. So it seems like folks agree with you that Clojure is not a fundamental theoretical advancement in PL theory. But that doesn't preclude it from being "amazing" (in the generic sense) in many people's eyes because they appreciate its design, or that it's a widely-adopted FP lang, or because it's a lisp they can use at work. Maybe some of this confusion could be prevented by more precise use of language when describing the kind of innovation you mean? Because it's fair to call Clojure "amazing" or to point out its unique (even new or at least new-in-the-context-of-popular-languages) qualities without considering it a quantum leap in the advancement of programming languages.
Completely agree with you. Clojure really is useful and great. Personally, I am learning a lot from it. But, as Alan Kay used to say: "most of the programming today is done in basically late 60s style" (paraphrasing a bit), meaning Java, C#, etc. I think for CL, Scheme, etc. we can say that is basically state of the art of the 80s. What I am saying is that this sounds very strange. Why are things so retrograde? Answers are well known, etc., but regarding Clojure, I think all the hype around it just additionally slows things down. So I guess we will now have to wait another 20 years for somebody to go beyond Clojure, since it is so useful, etc., but also very distracting (again, not for me; I have still lot to learn from it).
Agreed, though I feel that beyond the mere tremendous difficulty of discovering a new paradigm, the utility of any totally-fresh approach remains to be proven, especially in the domain of general-purpose programming. Manipulating the computer by telling it what to do with symbols referring to values and functions represented as text contained in files is still our best solution.
lisp quine?
&gt; load-ns-from-file: cannot fetch https://viebel.github.io/klipse-clj/target/public/cljs-out/dev//cljs/core$macros.cljc.cache.json
Ja, with a rant about twitter embedded in it.
Do you have an ad blocker?
Yes! Disabling it did fix the issue. However it's kind of cosmetic only: even when I get the error message (on the first evaluation result, whose result is 42), all fields have a correct value.
Some features like doc macro might not work when the json file is blocked. But the doc macro is not used in this article 
Thank you for taking the time to write this thorough explanation! It makes sense to me now. :)
``` ; The following makes Gtk functions available, giving their names ; and argument types. (loop as gfunc in '((gtk_application_window_new (* t) (* t)) (gtk_window_set_title void (* t) c-string) (gtk_window_set_default_size void (* t) int int) (gtk_button_box_new (* t) int) (gtk_container_add void (* t) (* t)) (gtk_label_new (* t) c-string) (gtk_label_set_text void (* t) c-string) (gtk_button_set_label void (* t) c-string) (gtk_button_get_label c-string (* t)) (gtk_button_new_with_label (* t) c-string) (gtk_widget_destroy (* t) (* t)) (gtk_application_new (* t) c-string int) (g_application_run int (* t) int (* t)) (g_object_unref void (* t)) (g_signal_connect_data long (* t) c-string (function void (* t) (* t)) (* t) (* t) int) (gtk_widget_show_all void (* t))) do (eval (nconc (list 'define-alien-routine (car gfunc) (cadr gfunc)) (loop as argname in '(a b c d e f g h i j k l m) as argtype in (cddr gfunc) collect (list argname argtype))))) ``` An excellent example on how Lisp avoids boilerplate altogether. 
Very valuable article. It basically argues that Clojure is not really a Lisp, but a language influenced by CL and Scheme, among other influences. That sounds right to me.
Can't you use the quote syntactic sugar in a quoted form?
I wonder...does - count as a quine in CL? :)
CL elisp: ((lambda (lambda) `(,lambda ',lambda)) '(lambda (lambda) `(,lambda ',lambda))) :) 
Not if we impose the rule that a Quine must be durable under embedding into other expressions. I.e. if Q is a quine then (list Q) should produce (Q), and so on; the Q part of the expression still quines itself.
I already forgot how online discussions get into hair-splitting. I've paraphrased Rich's words. Please go listen to his talks (it's worth it!). I'm certain he used a number of expressions, all of which were intended to convey a humble "all of this has been invented by others before me, I'm just putting it together". 
Again, as I said "people seem to miss the point of building a language on immutable data structures". The `pmap` is not what is interesting here. In fact, `pmap` is rather naïve and simple and leaves a lot to be desired, for anything deeper it's better to look at tools like tesser (https://github.com/aphyr/tesser). The important part is all the rest of your code: because of immutable data structures it is ready to be run in parallel with zero additional effort and with zero additional risk. That is not the case with most CL code. In other words, my main point was that Clojure code is "thread-safe" by default; you have to go out of your way to make it unsafe. In fact, Clojurists don't even use the term "thread-safe" unless referring to Java libraries. 
&gt; The big white elephant in the room is: the large majority of Common Lisp functions *don't* mutate data either, the ones that mutate data often have a "n" prefix (`nreverse`, `nconc`, etc) or are the triumvirat of `set`, `setq` and `setf`. You mean I can process my data using hash tables (maps) and my code will be thread-safe?
What do you mean?
So, something like Emacs' `electric-pair-mode`: &gt; When enabled, typing an open parenthesis automatically inserts the corresponding closing parenthesis, and vice versa. (Likewise for brackets, etc.). If the region is active, the parentheses (brackets, etc.) are inserted around the region instead. for Dr. Racket?
Edit | Preferences - Editing - Racket tab ha some settings to consider - in particular: * *Automatically adjust opening square brackets* - when you type `[` it automatically inserts `(` instead. However now you **can't** insert square brackets which are very common with the Racket based languages(s). * *Automatically adjust closing parens* - typing `]` automatically matches whatever the opening character is - `(` -&gt; `)` `[` -&gt; `]` Somewhat like paredit, DrRacket also support structural movement and editing - Edit | Keybindings - Show active keybindings. *There are a lot* but I would strongly suggest learning how to move and edit structurally rather than line by line. It's a game changer. Another option is emacs /[racket-mode](https://github.com/greghendershott/racket-mode). / paredit or [smart-parens](https://github.com/Fuco1/smartparens). Switching your editor to emacs is quite a time investment (but totally worthwhile in my opinion). If you **do** choose that route then adopting smart-parens instead of paredit probably makes more sense because it provides the same basic functionality but also supports non-lisp based languages too so your new movement and editing abilities transfer to other languages much more readily. 
Parinfer is another option, simpler than paredit, infers structure by indentation. paredit is more powerful, more control.
Parinfer is another option, simpler than paredit, infers structure by indentation. paredit is more powerful, more control.
Remap your keys.
Would `(fn [x] (list x (list 'quote x)))` not be shorter as `(fn [x] (list x ''x)))`? Also, this should be ok too if it's anything like the CL quine I'm familiar with: (fn [x] `(,x 'x))
If you're using Linux and open to a script on that level (as opposed to editor-level), you can use `xcape` to make it such that when you tap the left/right shift keys without pressing another key in-between, they insert open/close parens: xcape -t 250 -e "Shift_L=parenleft;Shift_R=parenright" &amp; If you're open to using Emacs, you can use [Key Chord](https://www.emacswiki.org/emacs/KeyChord) to map an uncommon but easily-accessible set of characters to insert parens: (key-chord-define-global "cg" 'paredit-open-round) Paredit does indeed not help a ton with just inserting parentheses. However, I usually find myself inserting a bunch of parens only when writing new code, which takes up less of my time than editing existing code - which is where paredit shines. [The Animated Guide to Paredit](http://danmidwood.com/content/2014/11/21/animated-paredit.html) might help you get a better feeling for what it provides. Sorry that I can't directly answer your question - I don't have any experience with Dr. Racket - but I write it on the off chance that you *might* be able to use *something*.
- Remap left and right shift, when pressed alone, to left and right parens on keyup - Use Programmer Dvorak keyboard layout, parens are unshifted keys in the place of 5 and 8 on the Qwerty layout under each of your index fingers
Do you mean like this? `((fn [x] (list x ''x)) '(fn [x] (list x ''x)))` &amp;#x200B;
Yes, but some quasiquotation (as in the third snippet) would be even better.
&gt;it's fair to call Clojure "amazing" or to point out its unique (even innovative, or at least innovative-in-the-context-of-popular-languages) qualities without considering it a quantum leap in the advancement of programming languages The unstated premise - of which no comparative examples are offered - is that there are / should be regular developments in programming languages that represent "quantum leaps." As with many fields, I think most of the low-hanging fruit (relative to the scope of the field) was picked early on. The light shined on a great many paths (invention / discovery) although many of them remain unexplored. I think the task we're seeing today (across both static and dynamic languages) is refinement, and coalescing the inventions of the past in practical innovations. By the very nature, they seem incremental, but that smacks to me of lack of perspective. In many ways, McCarthy and like minds sucked a lot of the air out of the room decades ago. Perhaps the next "quantum leap" will arrive on the back of hundreds of incremental steps, thoughtfully packaged for practical consumption. &gt;he utility of any totally-fresh approach remains to be proven Another implicit belief: that all change is advancement. Following that logic, the mass adoption of OOP (or mutations of OOP) should've been the end of it. Yet many find themselves eschewing encapsulated state, bundled methods, method combinations, in favor of pure functions and data (a "retrograde" idea, depending on one's perspective). &gt;Manipulating the computer by telling it what to do with symbols referring to values and functions represented as text contained in files is still our best solution. There are people implementing functioning Nintendo GameBoy emulators in Minecraft. The [Human Advancement Research Community](https://harc.ycr.org/) stuff is/was looking at alternative means of interaction with and representing computational processes. In particular [dynamicliand](https://dynamicland.org/) looks like an interesting shift (although it seems to be an AR veneer over a Smalltalk environment called RealTalk). I don't know about the premise of having everything (including the now OS) source code written on paper and floating around though. At some point, that's impractical. It makes for cool demos though. I've been following the Lively project off and on since hearing about it in one of Kay's talks, of which there's [LivelyR](https://vimeo.com/93535802) now for experimental interactive data analysis on top of Vega, R, and the Lively Kernel (self-contained Smalltalk environment in the browser, with JS interop). A lot of the projects at the HARC, of which these are a couple, are continuing to challenge what it means to express computation, and necessarily changes the medium of expression (perhaps no longer using text, who knows...).
The snippet I suggested is not a quine. Can you write explicitly the quine you have in mind and run it in one of the code boxes in the blog post? Thanks 
Ah yes, good point. Quasiquotation would be the only way to use that fancy quote syntax then. I'd still suggest it, it looks a bit more impressive IMO.
&gt; I know that paredit is supposed to be a popular plugin, but after browsing the site it's too hard to tell if it specifically addresses this problem Emacs with paredit it the way to go when programming lisp, in my opinion.
Wow. That's cool. Would you share a snippet for the first one? My Xmodmap fu falls short at that level.
This. I've switched parens with brackets like 20 years ago. `~/.xmodmaprc`: ``` !keycode 18 = 9 parenleft 9 parenleft !keycode 19 = 0 parenright 0 parenright keycode 18 = 9 bracketleft 9 bracketleft keycode 19 = 0 bracketright 0 bracketright !keycode 34 = bracketleft braceleft bracketleft braceleft !keycode 35 = bracketright braceright bracketright braceright keycode 34 = parenleft braceleft parenleft braceleft keycode 35 = parenright braceright parenright braceright ``` `xmodmap ~/.xmodmaprc`
I use lispy http://oremacs.com/lispy/ and couldn't "get" paredit. Lispy-mode works with one-touch keys when the cursor is just before or just after a parenthesis. To jump to the next spot, see `]` and `[`, `j` and `k`.
I remap [] with () using `setxkbmap -option parens:swap_brackets`. https://nl.movim.eu/?blog/phoe%40movim.eu/typing-parentheses-the-lisp-machine-way-3nHy7Z
Thanks. I'm on Debian (PureOS) and use what fouric suggested. It's 2 lines: \`\`\` apt install xcape xcape -t 250 -e "Shift\_L=parenleft;Shift\_R=parenright" &amp; \`\`\`
[removed]
Both xcape and key-chord are great solutions :+1: 
Probably not the cheapest or best solution: buy/build a custom keyboard that you can program yourself. Then simply put the parentheses wherever you want.
Great insight!
Thanks. The 'demostuff' file is pure genius. Cheers!
&gt;Clojure has a port of the propagator model you mentioned, extended to use optional STM for state management. David Nolen gave quite a few talks on applying results from research into industry (core.logic is one example). Hickey leveraged the research done on Fortress by Guy Steele's research group to implement a general tree-based reduction model. The community I regularly interact with is pretty diverse, and includes many folks who regularly dip into the research aspects of CS and often bridge them successfully using Clojure as an implementation language. &gt; &gt;([Anglican](http://probprog.ml/anglican/) and [metaprob](https://github.com/probcomp/metaprob) are active research projects in probabilistic programming funded by familiar faces (DARPA, formerly ARPA, ONR, AFRL, etc.) and the MIT Probabilistic Programming Project, both of which are platforms for continued [research and publication](http://probprog.ml/anglican/literature/). I wasn't aware there was a port of the propagator model. Sure, there is tons of useful and interesting stuff in the Clojure world. Thanks for the links.
It's not "eval is bad only when you literally type e v a l on the keyboard".
&gt;You mean I can process my data using hash tables (maps) and my code will be thread-safe? In CL, for simple stuff like a simple hash table like {name: "jwr", age: 25, karma: 684}, we actually don't use hash tables at all; we use property lists (plists), which are lists and can be (and often \*are\*) used in the immutable way. They will be far lighter and faster for small amount of keys. &amp;#x200B; In CL, when one is creating a hash table, it's often expected to contain a lot of keys and to be fine tuned for performance (CL implementations allow a lot of fine tuning for every hash table that is instantiated.) &amp;#x200B; So the first answer is, probably you wouldn't be using a hashtable in your CL code. &amp;#x200B; Second answer is, there are lisp implementations (for example the most popular one, SBCL) that allow you to create a thread-safe hash table and access it accordingly. From SBCL docs: &amp;#x200B; Macro: **with-locked-hash-table** *\[sb-ext\] (hash-table) &amp;body body* Limits concurrent accesses to hash-table for the duration of body. If hash-table is synchronized, body will execute with exclusive ownership of the table. If hash-table is not synchronized, body will execute with other with-locked-hash-table bodies excluded -- exclusion of hash-table accesses not surrounded by with-locked-hash-table is unspecified. &amp;#x200B; &amp;#x200B;
The point is to make code reviewers aware that `eval` **can** at all be perpetrated without being invoked; you can'd declare that program doesn't have an `eval` hack just because it doesn't mention the `eval` symbol (directly or by any use with interning from pieces of a string or whatever) or any functional equivalent like `compile` together with `funcall`, or `progv`. At first glance, the code doesn't appear to be doing anything like `eval`. `count` is inserted into a template using `,count`, but that's just the symbol `x` being inserted, not its value; totally innocuous. 
You're not fooling anybody with that code.
[Relevant reply from earlier discussion on propagator and cells](https://www.reddit.com/r/lisp/comments/acid7a/comment/edb33a2)
A self contained - mini - Lisp computer; yes! 
I agree as far as (and possibly fairly farther) that a close-to-minimal code example like this will not fool a Lisp expert even for a minute.
Love it! Can't wait to try it out at work tomorrow. 
Well done ! Works nice for me. (there are some comments here already: https://old.reddit.com/r/Common_Lisp/comments/abwtxu/beautify_practical_common_lisp_extension_for/)
Wow! Didn't realize that somebody found it on his own! 
I learned the [Minimak layout](http://www.minimak.org/), along with swapping square brackets with parentheses and flipping the top row so shift is used to type numbers. &amp;#x200B; I was back to my original QWERTY speeds within a week, and didn't lose very much productivity in the process. The main thing is that if you want that kind of specialization of your keyboard xmodmap is too inexpressive. XKB is far better.
Agreed! I want one! Take my money, OP!
I also want one. Are they for sale?
 **Memory available**: 2816 Lisp cells (11264 bytes). That's really a cool way to describe the memory capacity.
I hear smartparens has superseded paredit though?
Have you ever checked out https://github.com/hoplon/javelin/blob/master/README.md ? How does it compare?
I've not used smartparens, yet. I'll look into it.
I have been around the Clojure community for awhile. Datalog was an interest of Rich’s in the early Clojure days. A datalog library implementation existed prior to 1.0. I remember hearing that there was interest in queryable programs. Approximately 5 years later, datomic and codeq existed. Spec’s focus on provides/requires, rich’s talk about semantic versioning, etc. All of these things lead me to think we’ll see something on the queryable program front that ties the generative aspects of spec to dependency management and the application deployment lifecycle. I realize that’s vague, but o can imagine a lot of interesting combinations of these ideas appearing in the coming years.
I'm not one to go out and buy stuff just for the sake of having it but... *this* has me tempted.
Neat.
Previous discussion at https://www.reddit.com/r/lisp/comments/aekxpg/lisp_badge_a_selfcontained_computer_with_its_own/
https://clojure.github.io/clojure-contrib/datalog-api.html
Haven’t seen Javelin before, thanks for the link. It looks similar, yes. Has a different syntactic flavor, but accomplishes the same thing. I didn’t dig deep, but it looks like Javelin uses Clojure watchers to accomplish its magic, whereas I explicitly stayed away from them. I didn’t want to implement everything on top of STM, but just wanted to stick to ordinary persistent data structures. 
There's a chrome version now [https://chrome.google.com/webstore/detail/ofpmkhmfknpblplfcfgonagmdnliogdm/](https://chrome.google.com/webstore/detail/ofpmkhmfknpblplfcfgonagmdnliogdm/publish-accepted)
yea, uh what is going on here?
I like [Lispy](https://github.com/abo-abo/lispy), which can be configured to have more familiar paredit shortcuts, e.g.: (use-package lispy :ensure t :config (lispy-set-key-theme '(special paredit c-digits)) (add-hook 'emacs-lisp-mode-hook (lambda () (lispy-mode 1))) (add-hook 'lisp-mode-hook (lambda () (lispy-mode 1)))) 
I discovered this about an hour ago: [Hy: Lisp code -&gt; python AST](http://docs.hylang.org/en/stable/). I was able to import tensorflow in it - yay! The documentation says it also has support for macros. The github repository can be found [here](https://github.com/hylang/hy). This is awesome! 
The only issue I have with typing lots of parentheses is their size. After using the Hasklig font with Haskell ligatures, it dawned on me: It is trivial to take one's favorite programming font (say, Courier Prime Code) and resize, reposition ( ) \[ \] in a font editor so that they match a lower case "o", and read exactly like lower case letters. A world of difference. Like learning how to disable the "get off my lawn" ALL CAPS in Common Lisp. I never realized before that parentheses were the same issue.
You could also get the Java distribution running and code in Clojure, no?
I hadn't tried Lisps other than Common Lisp and Racket, until Hylang: and even then, I'd much prefer if it used parenthesis everywhere, just like Common Lisp. A bit of googling seems to suggest that the syntax for Clojure and Hylang are similar. I'm not sure about the [tensorflow Java API](https://www.tensorflow.org/install/lang_java) \- as of Jan 2019, it says it "is not covered by the TensorFlow [API stability guarantees](https://www.tensorflow.org/guide/version_compat)": I don't quite understand what that means. Could you please explain? Never the less, then, Clojure and Hylang are, at least, two lisp dialects with Tensorflow support. That's nice to know! 
I experimented with HyLang and TensorFlow/Keras last fall and I ran into some difficulties. My general “I want to use Lisp with Deep Learning” issues are being solved by wrapping Keras models in small Python Flask apps, and just running those in the background (search for ‘systemd userspace’ for ideas) and call via REST from Lisp. I also have some public github repo’s for dumping Keras weights and loading and running in Racket Scheme, but my code only works for simple network architectures.
&gt; Could you please explain? I'm not a Tensorflow user sadly, so I'm not sure how their support for Java has been. Another option perhaps is to use Node which I'm fairly sure has active TensorFlow support and run ClojureScript on top of it.
&gt; "is not covered by the TensorFlow API stability guarantees": I don't quite understand what that means. Could you please explain? This means that the API is unstable- functions/variables may be deleted or renamed, modules might be restructured, features might be outright deleted instead of going through a deprecation process, etc. Until the API is stable, there's no guarantee that code that compiles/works under the current version will do the same in a newer version 
So you have a lisp web service? I've heard about the overhead of HTTP being considerable, wouldn't RPC be more ideal? But I don't know how easy python interop. 
If you have the time for it, could you please consider dropping [an issue](https://github.com/hylang/hy/issues), in case you haven't dropped already? I see some significant potential in Hylang, with respect to Lisp-Python. (We do have Lisp-Java already.) Thanks!
Have you tried tensorflow's python api via [https://github.com/pinterface/burgled-batteries](https://github.com/pinterface/burgled-batteries) ?
I bet Terry Davis came close....
at least PHP used latin transliteration 
hah. good 'ol paamayim nekudotayim