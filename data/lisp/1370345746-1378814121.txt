Because I don't want to maintain PHP code. (codeIn (head (filter (/=PHP) languagePreferences))) perhaps.
When you say translate, do you mean replicating the code as closely as possible or do you mean writing a program with as similar functionality as possible?
That's what any programmer would say about their preferred language.
&gt; On the other hand, the culture seems to be too much academic for my taste. Some times you're forced to reason in term of abstract concepts (Monads Arrows etc...), which is fine when it is useful, much less when it seems arbitrary, because you have to learn (and understand) two different things: a new mathematical concept AND how your use case maps to it. &gt; &gt; For example I found several libraries that where to hard to use because they were either to abstract (and you had to read a paper to grasp its concept) or littered with new operators with dubious meaning. Both of these points are related, and I think the core problem you're hinting at is that Haskell people are, while friendly and happy to explain things, quite bad at writing "getting started" guides. Haskell is very extensible, and as such, it is natural for brand new ideas to get introduced. As we want these to be applicable for as many things as possible, these brand new ideas tend to be very abstractly implemented. The problem is that most people learn by tinkering around with concrete things. Most people don't read definitions of abstract things and then reason their way down to concrete. However! Haskell people have a tendency to describe things very abstractly, when they should rather just bombard the learner with concrete examples, and let the learner form an abstract intuition on their own. This is described further in [this article about the monad tutorial fallacy](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/). I posit that those two problems you have are with people teaching Haskell, and not Haskell itself. (Or even the libraries you had trouble with.)
Each code sample illustrates a particular technique. Try to implement that technique.
I don't find it particularly surprising or interesting that Lisp techniques are difficult and unwieldy in Haskell -- much like Haskell techniques may be difficult and unwieldy in Lisp.
I think the problem here is the idea of "imperative vs functional". Really, I would never seriously consider using a language that does not offer basic functional features. Every paradigm has its shortcomings. I am not a fan of overspecialization. It is reasonable to expect that a programmer knows all the mainstream programming paradigm. Oh and is not so easy as saying "you are simply not used to it". Lazy evaluation really can be a bitch and lets not start with helpful error messages. There is definitely a set of problems that are harder to reason about when implemented in Haskell compared to good old Imperative style. But lets have the discussion when I have more months of Haskell under my belt. 
[Yes very helpful.](http://spl.smugmug.com/Humor/Lambdacats/i-P46zgN8/0/L/d950bd7b-ba53-457d-9c11-31e64be070a4-L.jpg) I think this "automatic testing" just causes a false sense of security. I am myself guilty of this. "Oh it compiles, well should work". When working in a dynamic language, I tend to run the program nearly as often as I hit compile in a static language. I play more around. Get to know it better. Have an better understanding how it works. It just works much better for prototyping.
I'm still not sure why deferring type errors to runtime would make you understand your program better. It's basically the same thing only it crashes later, and only a little each time. The false sense of security I think is more with the programmers than the type system. Every convenience and safety measure will cause false sense of security if you don't watch yourself.
Sure enough! I think I may have misunderstood your original point, and I apologise for that.
OK, I think I follow. I think the closest you can get to that in Haskell is [FunD (mkName "fact") [clause (LitP (IntegerL 0)) one ,clause npat (InfixE nvar times (Just (AppE fact (InfixE nvar minus (Just one))))) ]] where nvar = Just (VarE $ mkName "n") npat = VarP $ mkName "n" one = LitE (IntegerL 1) times = VarE '(*) minus = VarE '(-) fact = VarE $ mkName "fact" clause pat expr = Clause [pat] (NormalB expr) [] which is still nowhere near your requirements. But perhaps it's possible to do better with clever use of typeclasses. I dunno, I'm not a Haskeller, not least because I find this sort of thing as ugly as you do. However, I think you're suffering from a misconception. There are no type declarations anywhere in that expression. All the capitalized words are constructor functions. Let's take `LitE`, which has type `Lit -&gt; Exp`. It takes a single "literal" node and constructs an "expression" node. The code doesn't give a type to 1: it takes the literal 1 and builds it up through a chain of constructor calls into an expression node in the AST. Let's look at a more complicated example: `InfixE` has type `Maybe Exp -&gt; Exp -&gt; Maybe Exp -&gt; Exp`. So to construct an "infix expression" node, we first need to construct a `Maybe Exp` node, an `Exp` node and another `Maybe Exp` node, then feed them to `InfixE`. And without the `InfixE`, the Haskell compiler wouldn't know how it's meant to form those three nodes into an expression node, or even how many it's meant to take. Lisp functions can destructure their arguments at runtime, but to even *get* to runtime in Haskell you first have to run the gauntlet of the typechecker, which ensures that all functions have values of permissible types passed to them. There are no variadic functions in Haskell. One final thing to note: Template Haskell does support quasiquotation. If you just wanted to get hold of the AST of the factorial function, you could do [d| fact 0 = 1 ; fact n = n * fact (n - 1) |] [Here's a blog post I wrote](http://pozorvlak.livejournal.com/95054.html) a few years ago when I was playing around with Template Haskell; you may find it interesting. 
No problem :-)
&gt; I've never bought how this is useful, though optimizing for usefulness may not have been the goal here. Reductionism in the kernel of a language allows one to check for some useful properties of language objects, especially if you have formal semantics defined for the language. For example, it makes it easier to verify that a compiler is correct, or to use succesive refinement to create a compiler that is correct-by-design in the first place. The same goes for checking that your optimizations are correct. Given how many server systems are breakable by exploiting bugs in compiled native code, this seems like a useful property to have.
discussion on [hacker news](https://news.ycombinator.com/item?id=5825698)
Don't act like you've never seen it.
I'll take it over a nightmare pile of interfaces and class hierarchies any day thank you very much.
The most painful thing about the design I've found is that it reports errors like this: &lt;...&gt; Thinking about VECTORS TEST-VECTOR-TYPES has expanded your awareness. TEST-LENGTH-WORKS-ON-VECTORS has expanded your awareness. TEST-BIT-VECTOR has expanded your awareness. TEST-SOME-BITWISE-OPERATIONS has expanded your awareness. TEST-LIST-TO-BIT-VECTOR has damaged your karma. You have not yet reached enlightenment ... A koan threw an error. Please meditate on the following code: File "koans/vectors.lsp" Koan "TEST-LIST-TO-BIT-VECTOR" Current koan assert status is "(ERROR)" You are now 40/180 koans and 7/27 lessons away from reaching enlightenment ... and that's it. SLIME is still pretty necessary.
not to be confused with [armedbear.org](http://armedbear.org/)
I think that paragraph is confusing given the usual use of 'extent' in lisp discussions (http://www.cs.cmu.edu/Groups/AI/html/cltl/clm/node43.html). It's the mention of lexical in threads.lisp that really bugged me. Grepping over the files shows that I was only imagining it being a pervasive issue.
I'm making a bit of a joke about how lisp projects are often just a big ball of code smeared on bit by bit to solve a current problem. I'm a bit surprised it is so enthusiastically downvoted, because the "ball of stuff" critique is floating around out there in the Lisp world already. I mean our unofficial mascot is an Amoeba-like alien. What I am poking fun at is the idea that the claims in this paper are somehow specific to Lisp. You can do stratified design in any language and Lisp, especially Common Lisp, doesn't even do a very good job of encouraging it. 
Automagically no. But it is easy to implement. 
Thats Great! See you there Moritz!
No to mention their cousins like ClojureScript.
Ok, I know you hate the guy, but could you at least upvote this article? It's **actually** interesting.
Interesting... it's written in Java rather than Javascript or Common Lisp. I'd have expected it to be much easier to write in Common Lisp or Perl than in Java. Or was it written in ABCL?
wondeful - another failed project which went nowhere. Can you explain why you 'believe' to be 'much more intrested' in it?
Personally, I find the LambdaPi project to be much more interesting than the blog post linked because it is closer to the ideal of a LISP machine: that is, LISP all the way down. Personally, I think that the idea of having as much LISP as close to the metal as possible is a tad more interesting than using a few packages for Raspbian. Along the same line of reasoning, LambdaPi is something I would consider a substantive project, while the work mentioned in the blog post linked seems more like a "guide to installing a light LISP toolset on RaspberryPi". I suppose that's a matter of personal taste and, as you said (although your tone leaves something to be desired), it doesn't appear to be going anywhere at the moment.
There is nothing interesting in the repository. How can it be 'substantive'? I've seen to many of those. With Raspian, Emacs and CL we can actually write software while you wait for the 'LISP all the way down' dream. Whether there is Linux below Lisp does not matter much for the code that needs to be written. It would be foolish anyway to write Lisp applications which depend on an obscure platform. I find linking to these repositories, without any actually useful content there to find, to be misleading.
I didn't say that what existed in the repository was particularly interesting; my point is that the idea behind the repositories is far more interesting than using Emacs and CL. Yes, you can "actually write software" with Emacs and CL. However, I don't think it's as interesting to say that as it would be to say that you had implemented a LISP OS on the RPi, if only because it is something that has been done before. You act as though writing a LISP OS wouldn't be writing software. Further, there is no reason to believe that the LISP applications that would be developed on top of a LISP OS would be any different than a LISP application anywhere else - I think more than anything else, it's foolish to assume that anyone would want a non-portable LISP as a platform. While it may be misleading to link to a content-less repository, I think it is also misleading to open a blog post by discussing LISP machines and then proceed to install Emacs on top of GNU/Linux. Emacs is wonderful. CL is wonderful. Having both of these available on the RPi is wonderful. Neither constitute a LISP machine. Quick edit: That isn't to say I thought the blog post was uninteresting; I'm just attempting to elaborate as to why people might find a project like LambdaPi (if successful) more interesting.
For an idea not executed I get nothing. For an idea I don't need a repository with nothing in it. I'll take Emacs + CL as a development platform (+ a Lisp-based window manager) any day over a repository with nothing in it than an 'idea'. Especially since I've seen the same type of repositories several times. Btw., we call it 'Lisp' nowadays.
Try lisp! Even if it ends up not being your language of choice, it's great fun to play with. I recommend a Scheme, especially [Racket](http://racket-lang.org/)
Firstly, sorry if my berivty came off as rude or dimissive. I saw this post right-before I went to bed and probably should've waited till I awoke, to initially respond. That being said, I don't hate the idea; In fact, I more-or-less run this environment (like I believe many lisp-fanatics do) on my main box, and it makes sense to me to-have a formalized install which works with the technology we currently have. Hells bells, I mean, I'm the guy who wants to release a distro in a good one to two-years that's bassically Just Guix, Emacs, Wayland, and some Lisp-based/focused composistor. Though that's my ideal-system with-in the frame of practicallity, it still isn't close to the ideal environment I lust for. Heck LambdaPi isn't turtles-all-the-way-down either, but it's much closer to that "ideal". 
Can't wait to try it out. If it works, I might even buy a smartphone and/or tablet :-)
I can't wait either..... this is great... only 15 more days...
This looks cool. Would be nice to get sbcl ported at some point as well, but this is nicely targeted to ios and android. Maybe they can add blackberry 10 devices as well.
Sweet! Will be using this from now on.
Thanks! If you find bugs, it'd be great if you could shoot me a PM so I can get fixing.
Sweet, on what kind of server is it running?
(spoiler: Haskell makes me puke) The best way to find out is to try both in action. How much time you will spend on a given task. For ex. try to write simple OpenGL Hello World (drawing a triangle maybe) using vertex arrays and indices. Good luck. Just take a chill pill before you start with Haskell. I warned you. 
No. You have to use Lisp's crazy little brother, JavaScript.
Look for parenscript
Not necessarily. There's lots of lisp-to-javascript projects around for the googling. If you're ok with Clojure syntax, Clojurescript is a good example of this.
It's still Javascript between the &lt;script&gt; tags.
or 1186967 seconds... http://wukix.com/mocl
http://lispwebtales.ppenev.com/ I suggest reading this.
That thing still doesn't do anything substantial in the browser, and will likely not have much beyond a basic parenscript example. That doesn't mean Randomtask3000 shouldn't read it though :) 
&gt; The letter `q' in memq says that it uses eq to compare object against the elements of the list — GNU emacs lisp reference manual not that I believe it's a great name, since the q in setq means "quoted" instead
I was about to bet $5 on the "quote" origin too.
It's from MACLISP, or at least a bunch of q-forms of things occur there, such as MEMQ, ASSQ and so on, all being variants which use EQ rather than EQUAL. Without keywords you kind of need them since the EQUAL versions need to do a lot more work. Lisp 1.5 does not have EQ-variants, that I can see. The Q is obviously from EQ in these cases, as you say.
Ass queue hehe
I'm too sexp for infix operators.
Something never quite sat right with me, when people regard Common Lisp and Scheme as "hi level" languages, and no further distinction is made. I think adding a second dimension might clarify things. Why both are at aprox. equal distance from machine code, Scheme has less functions that correspond to entities with which people reason about things. And thus is somewhat more "low level". The post is about what we call "low" and "hi" level, and **NOT** about the faults or virtues of the two lisps. I won't argue with you if you switch places of CL and Scheme, as long as you agree that the diagram can be useful when comparing languages.
As the last paragraph says, the post is about semantics of "hi" and "low", and not so much about the particular languages. I don't know Scheme very well, and I don't particularity care where it will be on the map. The map is the point.
My point is that this "high-level/low-level" dichotomy is usually false. Yes, Lisp dialects usually will give me more powerful abstraction, that get nearer on Math than C or C++. But C++ also has some powerful abstractions (such as templates). On the other hand, there are interpreted languages that are awful (like brainfuck). This dichotomy shouldn't exist. We should be able to produce fast and nice binaries with a language with powerful abstractions. C++ is trying to approach that, but I'd say that C++ is utterly and irremediably broken. Go and Rust are other languages trying to catch up.
I wouldn't say impossible. I'd like to do some research on that, when I can focus on my academic life (that is: not right now). But every time you compare languages, it is under a certain amount of aspects. Using "it can be easily translated as machine language" is not really a good one, because that depends much less on the language itself, and much more on how smart the compiler is.
The distance from the bottom label is defined as "correspondence of programming language words to processor instructions". Which is different from "speed of compiled executable".
That's tiny. Just bring it to the function namespace with FLET. Common Lisp mostly is not high-level. It is similar to Scheme. But it has several high-level features. Examples: * function calling with keywords * CLOS * Conditions 
&gt; Just bring it to the function namespace with FLET. Sorry for my ignorance, but how? &gt; It is similar to Scheme. But it has several high-level features. Scheme also brings continues to the table. But yeah, I wouldn't say that it is more powerful than Common Lisp. I'm only talking about how people think about the code.
And which language, besides assembly, still has that many correspondence of words to processor instruction? And even if it has, which language don't allow me to build better abstractions on top of it?
Interesting (to me) note: Emacs Lisp has member, memq, but also memql and so on which are versions of member which use eql instead of equal. MACLISP only has MEMBER (uses EQUAL) and MEMQ (uses EQ) because it does not have EQL at all (even though numbers were not reliably EQ). Common Lisp has only MEMBER, for which you can specify a test function, but the default is EQL not EQUAL. So much for compatibility! CL's is actually the most useful default, I think.
I can't be both precise and simple with that definition. C is closer to the bottom than Lisp, since it's memory management is closer to the workings of the machine, and rarely closer to the "human thinking in that domain". C's "for" loop is lower level in cases when "mapc" would have been appropriate, since the first corresponds more to "what the machine does", and cosponsors less to "how I'd think about it". Lisp's "simple arithmetic" vs C's has a number of arguments for it's level: **Lisp low:** - Prefix operators are further away from how I think about numbers. - The parser is thinner than the arithmetic parser of C **Lisp hi:** - a number doesn't have a predefined length of bits (closer to how I think, further away from how the machine thinks)
Bonus... much better than the previous page from November....
Seems unlikely to me... but I don't know...
 (defmacro curried ((name fun &amp;rest args) &amp;body body) `(flet ((,name (&amp;rest more-args) (apply ,fun (append (list ,@args) more-args)))) ,@body)) (curried (x #'+ 2 3) (x 1 5)) What is 'continues' ? If you mean continuations, that's powerful, but low-level.
It's basically a kind of non-local transfer, a complicated version of GOTO. Some people also think it should be removed from Scheme or at least be optional. http://lists.scheme-reports.org/pipermail/scheme-reports/2012-February/001824.html
Let's go to my car.
One more question, there is one slide in pdf - 'Why we didnt choose scheme?' - what they say, why (anyone here was on ECLM 2013) ?
&gt;All eqs are equals but not all equals are eqs. Arguably not true. Anyone guess the exception? Edit. No, I am wrong. I was thinking that equal can fail to terminate, and therefore you could construct a structure which was (obviously) eq to itself but where equal would not terminate. This is not the case because equal is essentially defined to check for eq / eql first, so if the two objects are the same object, it will terminate. The definition in the CL spec is actually confusing about this: it says (paraphrasing from [here](http://www.lispworks.com/documentation/HyperSpec/Body/f_equal.htm)): EQUAL is true for symbols if they are EQ, for numbers and characters if they are EQUAL and for conses if both the cars and cdrs are EQUAL (and some other cases). That could be read as implying that EQUAL walks conses regardless, and does not first to an EQ check. However the spec later says that "any two objects which are EQL are EQUAL" which makes it clear that it must first do an EQ check. I wasn't confused because I had read the spec in some pedantic way though: I was just wrong.
I also created http://lisp-search.acceleration.net I guess it doesn't hurt to have more than one :)
I also just found out about http://quickdocs.org which is similar but seems to have better design with a bit less info
`condition-notify` strikes again. I blame BT for only exposing the unsafe operation: if a library must choose between only having `condition-broadcast` and only `condition-notify`, the former is a much safer default. In the absence of broadcast, a semaphore should notify `n` times if it increments the counter by `n`.
There are probably other lisp web utlis we haven't thought of yet. Fame remains a quick hack away! :)
great! - hmmm, should be integrated into C-c C-d h
These articles may not bring much discussion to the table but they are indeed very appreciated.
Pardon the nitpick but I think getting people's names right is pretty important. If the front cover of my AMOP is to be believed, it's: * "Gregor Kiczales" * "Jim des Rivières" * (and "Daniel G. Bobrow" which is already correctly orthographed)
I'm very interested in his web framework. Could be a hidden gem.
Oh I was just looking to see if people would help. The problem asks for me to find the multiples of 3 and 5 from 0 to 1000, and add them together. My code is as follows: (setq x 0) (defun multiples (n) (loop for i from 0 to n do(cond ((= (mod n 3) 0) (setf x (+ x n)) ) ((= (mod n 5) 0) (setf x (+ x n)) )))) (defun multiples2 (n) (let ((b 0) (x 0)) (if (&lt; b n) (cond ((= (mod n 3) 0) (setf x (+ x n)) (setf b (+ b 1))) ((= (mod n 5) 0) (setf x (+ x n)) (setf b (+ b 1))) ((setf b (+ b 1)))) (print 'x)))) (defun find-multiple-3 (n) (loop for i from 0 to n do (let ((x 0)) (when (=(rem n 3) 0) (setf x(+ x n)))))) The first one I'm more concerned about correctness, as it has no output. The second one I thought would compute, and the last one is just meant to try a new approach that also didn't work.
I'm assuming you wanted (mod i 3) rather than (mod n 3) and (setf x (+ x i)) rather than (+ x n), right? Also, see incf. And, as a point of style, it'd be much nicer if the variable x were in a LET (or LOOP -&gt; WITH) declaration inside your function which the function returns rather than a global variable. For multiples2, you want to print x rather than 'x. You still have the same (mod n 3) rather than (mod b 3) and there's no looping going on that I can see. For find-multiple-3, you have the same sort of problem with (rem n 3) rather than (rem i 3).
Awesome thank you! The internet is helpful, but not as much as you.
I'm not sure why the author felt it useful to go through a summary of Lisp macros before getting to Metaclasses and MOP. Those ideas are pretty much independent of macros.
You could try contacting him; I emailed him some time ago* asking if there was any more information and if he planned to open-source it. Not really, and I don't have the time were the answers back then, but you might have more luck now :) You can find the paper that spurred my email linked from here: http://cl.cddddr.org/index.cgi?Event%3AInternational%20Lisp%20Conference%3A2002&amp;l= * found it; it was in 2005!
Why do you need a struct? It would be much lispier I think, if you just do something like (defun sum-if (predicate to &amp;key (start-from 0)) ...) and use it like (sum-if #'some-predicate 1000 :start-from 0) 
OK, well here is one approach: (defun multiples-of (n limit) (loop for i from 0 to limit by n collect i)) (defun sum-lists (&amp;rest lists) ;; this seems like overcomplex to me (reduce (lambda (&amp;rest ls) (if (null ls) 0 (+ (reduce #'+ (first ls)) (reduce #'+ (second ls))))) lists)) Then (sum-lists (multiples-of 2 1000) (multiples-of 3 1000)) will do what you want. Another approach is to have a bunch of little counter objects with an incrementor for them: (defun sum-multiples (numbers limit) (flet ((increment-counter (c) ;; increment a counter, which is a pair of (value . ;; increment), returning the new value, or 0 if the counter ;; has exceeded limit. (if (&lt;= (car c) limit) (let ((r (incf (car c) (cdr c)))) (if (&gt; r limit) 0 r)) 0))) (loop with counters = (loop for n in numbers collect (cons 0 n)) for v = (loop for c in counters summing (increment-counter c)) while (&gt; v 0) sum v))) Then (sum-multiples '(2 3) 1000) does what you want. People hate this sort of thing because it makes explicit uses of conses and car and cdr and so on and that's not proper modern code which is meant to be more complicated than that. So you can fool them by something like this: (defun make-incrementor (i limit) (let ((v 0)) (lambda () (if (&gt; v limit) 0 (if (&gt; (incf v i) limit) 0 v))))) (defun increment (incrementor) (funcall incrementor)) (defun bureaucratic-sum-multiples (numbers limit) (loop with incrementors = (loop for n in numbers collect (make-incrementor n limit)) for v = (loop for i in incrementors summing (increment i)) while (&gt; v 0) sum v)) Now this, of course, *also* is not proper modern code, but it's harder to see why.
Your proposed syntax conflates separate concerns: range specification and conditional summation. With my syntax, if I wanted to extend range specification (say by adding a step parameter) then I would have to change the implementation of SUM-IF but not add any parameters. For implementation I could then write a macro to traverse the elements in the range. Later it could be extended to a more general protocol (perhaps along the lines of Alexandrescu's range library, which is surprisingly translatable to good-looking Lisp). It is true that the Common Lisp convention for sequence functions uses start/end parameters. The reason they do that is to not force you to cons memory in order to specify subsequences. This is a good thing because it allows Common Lisp to be used in more contexts. For a Euler-Project problem solving language it may be better to pay the price of consing range objects in return for cleaner code. Furthermore, with the smart compilers of today you could regain some efficiency in cases where the range objects have dynamic extent.
This makes me want to write a common lisp to java compiler, so i can do most if not all my coding in cl. Does anyone here have experience with doing this?
That's what Linj was all about. https://github.com/xach/linj Dunno if it still works.
That is cool I will definitely check it out and maybe post a hello world example up. (edit I posted this before I noticed who you were :) )
Do you know where I can find the linj-mode?
I've recently discovered Cordova/PhoneGap which essentially creates cross platform mobile apps from HTML/CSS/Javascript using a "webview". [Here](http://www.kurup.org/blog/2011/11/12/clojurescript-and-phonegap/) is an article where a guy uses clojurescript with PhoneGap. I don't have any experience creating mobil apps, but Cordova/PhoneGap seems like a heaven-send.... write once/read multi-platforms. How about this alternative for app development?
Sometimes the deliverable is Java code.
Isn't it Dyomkin?
I guess he is Дёмкин or Дьомкін.
Was there any video recordings?
I don't believe there's an accessible name: what you probably want to do here is act on the list after it's been returned from loop. So something like: (defun fibonacci (n) (if (&lt; n 2) n (+ (fibonacci (1- n)) (fibonacci (- n 2))))) (defun fib-sum (n) (apply #'+ (remove-if-not #'evenp (loop for i from 0 to n collect (fibonacci i))))) The list collect from loop is passed through `remove-if-not` to give you a list of just the even values, which are then added up.
 (defun iota (n) (loop for i below n collect i)) (defun evens-odds (list) (loop for number in list when (evenp number) collect number into evens when (oddp number) collect number into odds finally (return (list evens odds)))) CL-USER&gt; (evens-odds (iota 10)) ((0 2 4 6 8) (1 3 5 7 9)) (although iota should actually be from 1 upto n, not 0 below n)
This might help you out: (loop :for i :below 10 :collect i :into acc :do (print acc))
looking forward to the asdf talk
The following does not reference to the named list, but probably it might be helpful. (defun fib-sum (n) (loop :for i :from 0 :to n :for result := (fibonacci i) :when (evenp result) :sum result))
By the way, that fibonacci function is horribly inefficient, especially since you're calling it multiple times in a loop. You should read the chapter in PAIP about memoisation.
Hey everyone. I know this got posted recently, but it wasn't really ready for release at the time. That said, I present [Wookie](http://wookie.beeets.com/), fully documented and ready to try out! Feel free to checkout the [github page](https://github.com/orthecreedence/wookie). The [doc site](https://github.com/orthecreedence/wookie-doc) is built on top of Wookie, so might make a decent reference. Enjoy!
"When in doubt, write portable code" is catchy, but I don't quite agree. Also, code is never written in vacuum. If you don't need portability and don't expect to need it, then why spend time on it? Sometimes you have a reason (insurance, politics, etc) -- sometimes you don't. The expected effort in later porting the code is also a factor. If you expect porting to take up minimal effort (because you use your-impl:*command-line-args* or something), then it is fairly pointless to spend time on it beforehand. Anyone who needs it can do it -- and they can then share /that/ work. That's called giving back... :) If you expect porting to be potentially hairy (say because you're generating a crapton C bindings and making assumptions about the representation of specialized arrays) then you probably want to think about future portability ahead of time -- and sometimes the answer may end up being "doing this portably is too hairy". The times when portability from day 0 is probably most often the right thing are those in-between cases where you need a few libraries, and appropriate portable ones exists for you to use. 
Note that your code will create a lot of garbage: you cons up a list of all fibonacci numbers, only to give it to `remove-if-not`, after which it's garbage. Also, I guess `(apply #'+ numbers)` is a bad idea if the `numbers` list is big because [`call-arguments-limit`](http://www.lispworks.com/documentation/HyperSpec/Body/v_call_a.htm#call-arguments-limit) doesn't have to be greater that 50. You can use `(reduce #'+ numbers)`, though.
The following is a bit complicated, but it might be useful someday. (let ((cache (make-hash-table))) (defun fib (n) (if (&lt; n 2) 1 (+ (or (gethash (- n 1) cache nil) (setf (gethash (- n 1) cache nil) (fib (- n 1)))) (or (gethash (- n 2) cache nil) (setf (gethash (- n 2) cache nil) (fib (- n 2))))))))
I really love your website. It's a perfect example of how a project webpage should be, in my opinion. Especially the pictures of your dog. I enjoy. Thanks.
Hey, thanks! I tried to make it a little fun, since so many doc sites almost put me to sleep.
THIS. That's exactly what I was thinking. CL projects' pages [in general] are usually a bummer, but yours is awesome! I can fucking read and understand the docs without having to do any sort of mind trick!!! Put that aside, this looks pretty cool. I'll make sure to try it out! Thanks :-)
I like this
This looks neat. I cannot load wookie it because cl-async-ssl is not in the quicklisp repos and I am not googling much. What is the secret sauce here?
from the readme: &gt; Wookie requires git versions of: [cl-libevent2](https://github.com/orthecreedence/cl-libevent2), [cl-async](https://github.com/orthecreedence/cl-async), [http-parse](https://github.com/orthecreedence/http-parse) 
Well, of course, it was right there. Thank you!
I noticed this yesterday: if you load the quicklisp version of cl-async, quicklisp can't find the packaged cl-async-ssl.asd file (same with cl-libevent2-ssl.asd). I might need to ask Xach if there's a fix.
Yeah. I just finished manually loading everything and can confirm cl-libevent2-ssl.asd is not being found. I don't know how to fix it though. I like the source code that I am reading. It seems fairly clean and easy to work through. It reminds me of hh-web which is also very simple and clean and easy to work with.
quicklisp/asdf should look in the quicklisp's *local-projects* directory first. So placing them there should work. I personally have use this: Modify ~/.config/common-lisp/source-registry.conf and add this: (:source-registry (:tree ("/path" "to" "my" "projects" )) (:tree ("/path" "to" "third" "party" "projects")) :inherit-configuration) Asdf will look in my projects first, then third party, then quicklisp. Edit: I realized the above explanation is a little misleading. What Asdf does is initialize the source registry when it loads. Suppose you have quicklisp's cl-async, you are already running lisp, you copy github version of cl-async to local-projects, then quickload cl-async. ASDF will not load the github version, it will load the quicklisp's cl-async. For asdf to recognize the new github version, you will need to restart lisp or call (asdf:initialize-source-registry) 
Here is a blog post explaining a little bit of the history and decisions made: http://symbo1ics.com/blog/?p=1936
This is really kind of cool. 
I hate to be a critic, but maybe this is a little over-engineered? All you need is a bunch of smaller asd files. If you don't want to populate quicklisp with many asds then you just need to add a secondary lookup to quicklisp. That's a simple solution using familiar tools that already work. And when necessary you have the option of pure asd sans quicklisp. Keeping it simple especially applies to utils, it seems.
I talked to Xach, apparently it's an issue where `cl-libevent2-ssl` needs a library that isn't on the build server. We're working on a fix right now. I haven't heard of hh-web, I'll check that out.
Criticism is okay. "Hm, which ASD is that `non-negative-single-float-p` function in again?" It's easier to be able to do: &gt; (qtl:who-provides :non-negative-single-float-p) :SUB-INTERVAL-NUMERIC-TYPES or even better: &gt; (qtl:utilize-symbols :non-negative-single-float-p) &gt; (qtl:non-negative-single-float-p 5.0) T Do we use different packages for each ASD? If not, do we have a `setup-utility-package-asd`? Does the user have to add this to their project? And what differentiates this idea from Yet Another Utility Library (TM)? Is it sensible to make your project ASD files look like this? :depends-on ("cl-fad" "cl-anaphora" "setup-util-pkg" "util-lists" ; for flatten, subdivide "util-streams" ; for copy-stream "util-macros" ; for once-only, with-gensyms "util-numbers") ; for binomial-coefficient At that point, it'd just be better to group them up in Alexandria. Or is this better? :depends-on ("cl-fad" "cl-anaphora" "quickutil") and later have (qtl:utilize :flatten :subdivide :copy-stream :once-only :with-gensyms :binomial-coefficient) Lastly, with respect to adding the indirection to Quicklisp for utilities specially, I'm not sure Xach would want that.
&gt; "Hm, which ASD is that `non-negative-single-float-p` function in again?" There two points here which are somewhat independent of each other. First, how did the user come to know about `non-negative-single-float-p`? At some point he loaded the corresponding system or was browsing the docs for that system. So he knows where to look for the information he needs. Second, the best way to answer that question is to solve the general case. `who-provides-symbol` sounds like fun round of hacking. Load each system, record the symbols, and then tie them all together with a nice API. Now everyone gets to use `who-provides-symbol` on any given set of systems, even the whole quicklisp database. Now apply it to your own set of util asds. &gt; Do we use different packages for each ASD? Sure, why not? &gt; Does the user have to add this to their project? Sure, why not? &gt; And what differentiates this idea from Yet Another Utility Library (TM)? A set of systems that are closely tied together and maintained, avoiding the problems and trade-offs from a monolithic system. All of the YAULs (TM) are monolithic. &gt; Is it sensible to make your project ASD files look like this? It certainly looks good to me, even ideal. &gt; and later have (qtl:utilize :flatten :subdivide :copy-stream :once-only :with-gensyms :binomial-coefficient) That looks worse to me. I see a haphazard collection of symbols that are thrown into one package instead of grouped conceptually. It uses a complex scaffolding tool to do magic behind the scenes, rather than resting on familiar tools which are entirely adequate. I am sounding harsh here, sorry, but you asked. &gt; Lastly, with respect to adding the indirection to Quicklisp for utilities specially, I'm not sure Xach would want that. Well given the flexibility of the condition system, I expect something can be arranged relatively easily. It may even work right now --- handle QUICKLISP-CLIENT:SYSTEM-NOT-FOUND, bind a few specials, try again. Or it may be somewhat harder. In any case the thing that bothers me is duplicating the effort of quicklisp before even asking Zach about it.
Oh, no worries. I was excited to see your readable code and wanted to give it a roll. Thanks to the hard work for both of you!
Recursion! http://u.onloop.net/suburl/
&gt; There two points here which are somewhat independent of each other. First, how did the user come to know about `non-negative-single-float-p`? At some point he loaded the corresponding system or was browsing the docs for that system. So he knows where to look for the information he needs. Well, a few other cases. He or she might have (1) simply heard about it, (2) seen such a utility used before (e.g., in a book for e.g., `flatten`), (3) seen such a utility in a document. In that particular case for `non-negative-single-float-p`, we can find it in [CDR 5](http://cdr.eurolisp.org/document/5/extra-num-types.html). &gt; Second, the best way to answer that question is to solve the general case. who-provides-symbol sounds like fun round of hacking. Load each system, record the symbols, and then tie them all together with a nice API. Now everyone gets to use `who-provides-symbol` on any given set of systems, even the whole quicklisp database. Now apply it to your own set of util asds. If you think it would be a fun round of hacking, I think you should write it. :) So suppose we do have a function `who-provides-symbol`. Now we have a system from which it originates. Our only option now is to load the entire system. Quickutil will only load the symbols it needs. But I guess now you will suggest solving the general problem again and writing some sort of code walker to determine needed symbols? From where are symbols queried? A web API? I think, especially to solve the "utility problem" in particular, your suggested approach is *definitely* overengineering. &gt;&gt; Do we use different packages for each ASD? &gt; Sure, why not? &gt;&gt; Does the user have to add this to their project? &gt; Sure, why not? I think this might be overly idealistic thinking. It leads to the ugliness (in my opinion) you'll find from doing (ql:system-apropos "informatimago") I also think that imposing the user with the requirement to remember which package does what, and commands to figure that out, also just lead the user to desiring something simpler, and causing them to create their own library. &gt;&gt; And what differentiates this idea from Yet Another Utility Library (TM)? &gt; A set of systems that are closely tied together and maintained, avoiding the problems and trade-offs from a monolithic system. All of the YAULs (TM) are monolithic. I do not think it solves the "monolithic system" issue. Let's look at Alexandria's `defsystem`. (:file "package") (:file "definitions" :depends-on ("package")) (:file "binding" :depends-on ("package")) (:file "strings" :depends-on ("package")) (:file "conditions" :depends-on ("package")) (:file "hash-tables" :depends-on ("package")) (:file "io" :depends-on ("package" "macros" "lists" "types")) (:file "macros" :depends-on ("package" "strings" "symbols")) (:file "control-flow" :depends-on ("package" "definitions" "macros")) (:file "symbols" :depends-on ("package")) (:file "functions" :depends-on ("package" "symbols" "macros")) (:file "lists" :depends-on ("package" "functions")) (:file "types" :depends-on ("package" "symbols" "lists")) (:file "arrays" :depends-on ("package" "types")) (:file "sequences" :depends-on ("package" "lists" "types")) (:file "numbers" :depends-on ("package" "sequences")) (:file "features" :depends-on ("package" "control-flow")) More than half of these logical modules depend on one or more of other ones. If you use even only a few symbols from a couple modules, you inadvertently end up essentially including the entire library, and the separation of categorical concerns becomes little more than a convenience to the author or reader of the code who wants to have a rough idea of the kinds of things that were needed for the program. I think this is perhaps a reason why Alexandria didn't bother with any more explicit categorization. Another question: is `mappend` a list utility or a functional programming utility? Is `non-negative-integer` a type utility or an arithmetic utility? ASDF systems as categories only allow for one designation. Quickutil allows any number of categories to be attached to a utility. See below. &gt;&gt; Is it sensible to make your project ASD files look like this? &gt; It certainly looks good to me, even ideal. If the user doesn't care about including any extra overhead, this might be okay. Many don't. Sure, compile and load 150+ symbols when all you need is `symbolicate`. Another issue: what if the user wants to make their system independent of the internet? Perhaps you recall a time before Quicklisp existed, when you had to pull all of the package tarballs yourself, find their deps, etc. The many-systems approach would lead to this same annoyance. This does not exist with Quickutil either. There is even a built in function to build your utility file: `qtl:save-utils-as`. &gt;&gt; and later have (qtl:utilize :flatten :subdivide :copy-stream :once-only :with-gensyms :binomial-coefficient) &gt; That looks worse to me. I see a haphazard collection of symbols that are thrown into one package instead of grouped conceptually. It uses a complex scaffolding tool to do magic behind the scenes, rather than resting on familiar tools which are entirely adequate. I am sounding harsh here, sorry, but you asked. This might be easily solved by way of the following: (defparameter *list-utilities* '(:flatten mappend ...)) (defparameter *stream-utilities* '(:copy-stream ...)) ... (apply #'qtl:utilize *list-utilities* *stream-utilities* ...) No longer are the utilities haphazardly being `utilize`d. These categories could even be automatically generated by us at a later time, since categorical information is known. Indeed, you can see such automation put into effect in the [utility listing](http://quickutil.org/list). &gt;&gt; Lastly, with respect to adding the indirection to Quicklisp for utilities specially, I'm not sure Xach would want that. &gt; Well given the flexibility of the condition system, I expect something can be arranged relatively easily. It may even work right now --- handle QUICKLISP-CLIENT:SYSTEM-NOT-FOUND, bind a few specials, try again. Or it may be somewhat harder. In any case the thing that bothers me is duplicating the effort of quicklisp before even asking Zach about it. I won't speculate further about what Xach would want to do. I don't believe Quicklisp is being duplicated. (For example, does Quicklisp has a website where you can look around at packages, their source code, etc.? The closest system is http://quickdocs.org, whose author incidentally also helped write Quickutil.) It is indeed similar in spirit, but is anything that downloads from the internet and does dependency handling count as a Quicklisp duplication? Why didn't Xach just extend ASDF, which already handles this stuff as well?
This is an interesting idea. I like the creativity and feel dirty because the following probably sounds harsh, but I have to say that this may be a very bad idea. Here's an extract from your site. "The function qtl:utilize downloads and compiles the requested utilities into the current image. Quickutil only downloads the utilities needed, without extra overhead." http://symbo1ics.com/blog/?p=1936 I dislike this on one point and disagree on one other. First the dislike. The *worst* part of QuickLisp is that people are conflating "download" with "compile" with "install" with "use". Yes, it is convenient and easy. No, it is not a good general practice -- the world is full of malicious people (viruses and trojans) and accidents (e.g. antivirus update that kills the OS). It is OK for an end-user to choose to trust a repository and use QL. It is not OK for an application or library developer to make this choice for the user by using QL. Now the disagreement. I don't have accurate numbers available at the moment, but most of the open source CL ecosystem fits in something like 100MB of sources. (The 80 I have handy fit in 30MB.) This is peanuts. It is much more efficient to make one socket connection and download 100 files than to create 100 socket connections for one file each. The size of utility libraries isn't the problem. The problem is that there are numerous perspectives for these simple things, and we haven't been able to pick one or two, refine them, and solidify their use. From my first read of it, QTL doesn't make things easier to install than Quicklisp. Also, it seems to encourage further fragmentation of utility libraries. All that said, I agree that there is a persistent problem here. Rather than create yet another utility solution, is there a way to fix the Alexandria process? I agree with anti-bracket-brigade, "All you need is a bunch of smaller asd files". If Alexandria had "alex-fun1.asd", "alex-macro2.asd", etc. then you could easily avoid the image bloat that might otherwise require a tree shaker. Maybe the project should have two levels of package names, putting new utilities into Alexandria-alpha and only adding them to the main Alexandria package after full vetting. Then Alexandria-alpha would be an easy way for people to distribute utilities, get feedback, and see whether they have widespread use. Alexandria proper could remain the high-quality system it currently is. So my new test fun might be in a file that looks like (in-package :alexandria-alpha) (defun foo ...) and after it receives blessing, it could look something like (in-package :alexandria-alpha) (defun foo ...) (import 'foo (find-package :alexandria)) (export 'foo (find-package :alexandria)) (with the proper eval-when clauses)
&gt; Well, a few other cases. He or she might have (1) simply heard about it, (2) seen such a utility used before (e.g., in a book for e.g., flatten), (3) seen such a utility in a document. In that particular case for non-negative-single-float-p, we can find it in CDR 5. In these cases as well, it would be beneficial to have functionality grouped conceptually by system. &gt; So suppose we do have a function who-provides-symbol. Now we have a system from which it originates. Our only option now is to load the entire system. Quickutil will only load the symbols it needs. But I guess now you will suggest solving the general problem again and writing some sort of code walker to determine needed symbols? From where are symbols queried? A web API? &gt; &gt; I think, especially to solve the "utility problem" in particular, your suggested approach is definitely overengineering. That is not at all what I suggest. You say "load the entire system" as if we are back to monolithic utils, but the whole point of my suggestion is to avoid monoliths --- to organize systems into concise conceptual categories. No code-walking is necessary. &gt; I think this might be overly idealistic thinking. It leads to the ugliness (in my opinion) you'll find from doing What's overly idealistic and what's ugly? Those systems have names which are far too long, which is ugly, but that's rather beside the point. Just don't use ridiculously long names. &gt; More than half of these logical modules depend on one or more of other ones. If you use even only a few symbols from a couple modules, you inadvertently end up essentially including the entire library That's not even true from a basic inspection. But more importantly your example -- a monolithic library -- embodies the very antithesis of what I am suggesting. Those large files may be broken into smaller categories. Alexandria didn't bother with more explicit categorization because there was no use for it and that was not an aim; path of least resistance and all. &gt; If the user doesn't care about including any extra overhead, this might be okay. Many don't. Sure, compile and load 150+ symbols when all you need is symbolicate. Are we back to monolithic utils again? You're again citing an example of what not to do. `symbolicate` is pretty basic, while 150+ symbols of anything has to be providing something rather non-basic. &gt; Another issue: what if the user wants to make their system independent of the internet? You use exactly the same tried-and-true mechanisms that currently exist. Remember quicklisp is still doing the downloading for you. Copy your projects (probably from ~/quicklisp) to somewhere and use ASDF alone. Simple. You seem to be comparing not using quicklisp with using quickutils, which isn't fair. Whatever freedom that allows you to use quickutils would also allow you to use quicklisp, in any meaningful comparison. &gt; No longer are the utilities haphazardly being utilized That's terrible. You're imposing structure from the wrong direction, on something that had no structure. The usefulness comes from having the structure there in the first place. &gt; is anything that downloads from the internet and does dependency handling count as a Quicklisp duplication? Of course not, but anything that can be readily implemented using quicklisp is. &gt; Why didn't Xach just extend ASDF, which already handles this stuff as well? Conceptually quicklisp is like an extension to ASDF; it seamlessly replaces asdf:load-system and understands the asdf registry. It is implemented on top of asdf. There may be a misunderstanding here because earlier you said "adding the indirection to Quicklisp for utilities specially". It wouldn't be for utilities specially. The general case of pulling from a secondary source would suffice, and would be useful in itself. You would be piggy-backing on more general functionality. But this could also be done by building on top of quicklisp. In any case whether we are extending or building on top, there is great benefit to using preexisting, well-known, well-tested tools when they suffice.
&gt; I won't reply to every single point and I am afraid I will have to use a cop-out response and say I do not think you understood what I was talking about with respect to interconnectedness of systems leading to essentially monolithic systems. I think I did understand it. If we use just one symbol from each leaf on the graph of system dependencies, we end up using every system. I don't see this as a problem. Or rather, this is a problem so rare (not enough memory and no tree-shaker available) that I wonder if any instances of it exist today. If memory is such a premium then I would suspect the time it takes to copy a few functions manually is the least of your problems, and I would question the use of Lisp in the first place. &gt; Again, I ask the question, where does mappend or non-negative-integer fit? Do you think every kind of individual functionality is easily and capably classified using one identifier? I skipped over that question because it seemed rhetorical and/or didn't really make a point in your favor. Where to put `mappend` does not seem like a pressing problem that requires a function-level (re)implementation of quicklisp. There may be a statistically optimal home for `mappend`, but as long as the categorization is reasonable and reflects a modicum of careful consideration, who really cares? I don't know of any package system, in any language, in which a function has more than one "home" package. Nobody seems to mind. Yes, I think concise conceptual categories are good. I don't know why it wouldn't apply to utilities. Here is another reason that they are good: sometimes a package will anticipate your need before you're even aware of it. When you slime-complete on `foo:` to see what's available, the function/class names will make you realize that you haven't yet considered the cases where you'll need those things. A mass listing of 300 symbols in one big package doesn't help, and a list consisting of only what you're using doesn't help either.
For what it's worth, I think library designations are good when the library has a purpose. I do not think library designations are good when the utilities are vaguely related. Library: An implementation of tries and operations on them. Non-Library: A loose collection of functions that manipulate lists in some way. In my opinion—and this may be a point of contention or disagreement—is that systems should be constructed for applications and application libraries. I do not think a bucket of utilities, whose only connection is the fact they all have something to do with numbers or something to do with lists, should be designated its own system. I say this because it seems to be the typical way people write system definitions. I have seen one example in my life of a separate system being made for utility *macros*, and that was because it was required to get the compilation order correct. If a set of utilities is more coherent than just being vaguely related by some categorization, then it should be a library. But, by my definition of a utility, it is no longer a set of utilities.
&gt; You've stated a bunch of "shoulds" but as far as I can tell you haven't explained the reasons for them. You, as well, state a lot of "this is how it should be done", but aren't showing code to demonstrate and test your hypotheses. &gt; What good comes from not grouping them together? Who is to say they're not grouped together? [These list functions](http://quickutil.org/list/lists) seem grouped together to me. Just, as a first release, an "MVP", there isn't a way to obtain a category of items. But, by design, there are logical groups. You say that is a backward approach. But what does it matter if the user sees the same results in the end? *Edit*: We have had a lot of back and forth about a theoretical discussion about design. I am wondering if you have tried Quickutil, and if that has been a factor in your judgment so far.
&gt; It is OK for an end-user to choose to trust a repository and use QL. It is not OK for an application or library developer to make this choice for the user by using QL. If I understood this correctly, as a library or application developer you can still use [`qtl:save-utils-as`](http://quickutil.org/how#offline) to put the used utilities in a static file, so your library won't have to download any utilities, it won't even depend on QTL at all. Sou you could use `qtl:utilize` during development and replace it with a static `utilities.lisp` file for release. Additionally you usually don't *use* Quicklisp inside your library, it's just a way to make it available, so the user is not at all forced to use QL at all. Since QL even allows you to load local ASDF systems, you could get all your libraries from a trusted source and just use QL to load them, so you are in no way forced to use the QL repos.
This is pretty interesting. From the user point of view, I recently released a small piece of software that has utilities copied from my unpublished utilities library. I don't like the cut and paste, but I haven't come up with a better solution. I just tried your qtl:save-utils-as and it might be preferable. Whether I decide to use it or not, it's definitely very neat. For my particular usage I would prefer the ability to specify a package to in-package at the top of the file, rather than the current defpackage and export. I want to decide the organization of my projects. The end of the blog post states that "The function-level distribution of software is, as far as we know, a novel idea", and it kindles my enthusiasm. I hope you will draw attention to any surprisingly useful or popular utilities you see while running the service. Some thoughts in regards to comments posted throughout this topic: I think alexandria's current development process is irrelevant to the need for qtl, because regardless of how fast alexandria is developed or how responsive it is to community requests, many of us will have utilities outside of its deliberately conservative scope. I agree with nuntius that it is a bad idea for qtl:utilize to be called by code that is part of an application or library. On top of his security concerns I think it introduces brittleness into the build process. Both short term brittleness, where a build would fail if the machine is not connected to the internet, and long term brittleness, where libraries will have to be manually updated in the future if qtl is no longer maintained. I urge you to discourage the use of qtl:utilize in distributed code rather than recommend it.
&gt; utilities is not at all obvious and apparently unexplained, along with other assertions found in the same comment. The best explanation I think I can offer is that it does not feel right. Strictly speaking, there is nothing wrong with it. It is totally valid to do it. Nothing precludes you or me or anyone from making utility systems. But I hope you will agree that the most common way systems are used are to define applications or libraries that serve a specific purpose. I do not think "a common binder for functions that relate to lists" is a "specific purpose". The reason systems are usually thought of this way is because Common Lisp doesn't have any notion of a module, which I would say is some sort of cross between a package and a system. I think this is also why there is almost always a one-to-one relationship between packages and systems. Perhaps you do not attach any other semantic qualities to systems, and *only* see them as a way to organize the order in which files should be compiled or loaded (along with other miscellaneous things such as actions to perform before or after compilation). Again, strictly speaking, this is true, but is not helpful whatsoever when we discuss topics such as "categorization". The systems-as-categories approach is an approach I took at a company I worked at previously. This was for a system that ran on low power hardware. Aside from the fact the software needed to be lean, and aside from the fact the system was offline, having a fifteen (yes, 15) `:depends-on` lines for *just* utility-like libraries, from each of which we used between one and five functions, was a sort of maintenance nightmare. For one, we had to maintain the whole ASDF dependency tree (because it was offline), which meant 15 lines of ASDF system registrations. For two, new people on the project didn't know why or where they were being used, old people on the project forgot what they were needed for. There were a few attempts to pare them down by taking bits of code out of the libraries themselves and just throw away the rest. That lead to not-so-fun rounds of dependency chasing. It also lead to error-prone checking for updates and reinstalling them. The system was hundreds of thousands of lines of Lisp, and with the libraries, it probably increased 1.5-2x. Compile times were measured in minutes, of course, not seconds. Optimizing compile times was important for developer productivity. In all, it seemed that using loosely coupled utilities which were bound tightly by systems ultimately was trouble, and as such, instead of just dropping in a new utility library for, say, `copy-stream`, we began to just write our own. Over the course of time, it grew to 25 utility *files*, weighing in at a couple hundred KB worth of source text. If we had Quickutil at the time, we could have generated a single 100 KB source file from a simple command, and updating would be a matter of repeating that command. In addition, the new utilities we wrote could have possibly been redistributed via Quickutil so other people could benefit. With a giant listing of ASDs, we would run into the same issues as before, except all of the ASDs would have about the same name, instead of :alexandria :metatilities :kmrcl :anaphora :f-underscore :split-sequence ... or whatever it is we used.
Infinite redirect! http://u.onloop.net/ReJXZC
Regarding your comments about brittleness, I'm not so sure. In theory, it makes sense, and I agree. In practice, I'm not so sure. Concerning short-term brittleness: I think a lot of people depend on Quicklisp to load libraries for them, and I have not heard many complaints, if any at all, that they couldn't build their application while offline. This is a little bit less of an issue with Quicklisp because copies of the software are cached locally. This is something I would like to do with Quickutil, but am not sure about the design quite yet. Concerning long-term brittleness: In the worst case, the Quickutil server will die out and be inaccessible. If this is the case, it will be a huge inconvenience to the users. The worst case for the user is that they did not save a local copy of the utilities and—since Quickutil is open source—they will need to recover the utilities manually (or run the Quickutil server themselves!). In the best case, they will have a local copy and no harm done. Neither of these scenarios sound particularly terrible. If it were 10 or 20 years ago, and if I'd never used Quicklisp before, I think I would be in a lot stronger agreement. With all of that said, though, there's definitely still work that needs to be done, to reduce brittleness. But my gut feeling is that `utilize` is and will be the right choice. The API is at least somewhat what I envision, but perhaps the server interaction might change in the future. But I've been wrong a lot before. P.S., if you have interesting or useful unpublished utilities, I urge you to share them and [file a new issue](http://quickutil.org/submit) to get them included!
One comment on the quicklisp comparison, to add to what you already said, is that I believe the recommended usage pattern is to exclude ql from distributed code, and instead rely on asdf to define system dependencies. This works very well for ql distributed code. (Of course, I recognize this pattern is not always followed.) If qtl dies out, I agree with you that it wouldn't be terrible for active users to pull in the utilities that their code depends on. I do think that it could be a nail in the coffin for abandoned code, though. That could be a bummer for library or application consumers who never heard of qtl and for whom repairing their dependency might not have an obvious avenue of attack. I agree with you that the api for utilize is quite nice. &gt; P.S., if you have interesting or useful unpublished utilities, I urge you to share them and file a new issue to get them included! I'll share at least one and see how it goes:)
Just to correct a factual error: Alexandria was nothing like "the first" attempt at the utility library problem. There were a several utility libraries before it -- which tended to be used by very few people each. My perception was that the reason everyone rolled their own was that the available libraries contained too much stuff, or stuff that wasn't sufficiently common idiom, or moved too fast and had unstable APIs, or had restrictive or viral licenses. So the way Alexandria was set up was to explicitly counter this. To some degree it has been a roaring success, based on adoption rate. To some degree it has been at least a partial failure, mostly due to my inability to communicate the vision with sufficient clarity to spread the workload better. Alexandria was also there slightly before quicklisp, when having N utility libraries as indirect dependencies was actively annoying to users. This is much less of an issue these days. Anyways, good luck with Quickutil. 
Your examples continue to epitomize the antithesis of what I am proposing. A set of systems that are closely tied together and maintained would prevent the duplication between e.g. alexandria and metatilities. The whole point is to *avoid* the problem of using two monolithic util systems that overlap, not run towards it. &gt; The system was hundreds of thousands of lines of Lisp, and with the libraries, it probably increased 1.5-2x. Hundreds of thousands of lines of utilities? I suspect most of them were not true utilities but part of the application software proper. &gt; I do not think "a common binder for functions that relate to lists" is a "specific purpose". Probably not, but `list-combinators` and `float-limits` might fit the bill. &gt; Aside from the fact the software needed to be lean, and aside from the fact the system was offline, having a fifteen (yes, 15) :depends-on lines for just utility-like libraries, from each of which we used between one and five functions, was a sort of maintenance nightmare. For one, we had to maintain the whole ASDF dependency tree (because it was offline), which meant 15 lines of ASDF system registrations. Tedious situations can be automated, but in any case maintaining 15 lines of code is the very least of your problems when we're talking about hundreds of thousands of lines of code. &gt; For two, new people on the project didn't know why or where they were being used, old people on the project forgot what they were needed for. A monolithic util library, because it does not partition code into conceptual units, can have the problem you describe (though one (sometimes tedious) way to avoid it is to `:import-from` rather than `:use`). But a well-conceived division of labor self-documents the what-goes-where-and-why, solving the very problem you bring up.
Advantages of Haskell: It seems to be "in", currently. Disadvantages: It is slow and hard to use for vitally everything above simple toy examples, at least as far as my expierience goes. The "documentation" reads like a sequence of TCS-Papers, but without abstracts or really explanatory examples. Furthermore, it is extremely hard to predict which part of your code will be evaluated, and when. However, I would not limit myself to those two languages. For example, a nice compromise is the ML family of languages. Standard ML is, for example, the only language I know which has a simple, consistent way of defining infix operators.
I have corrected the error. I think I understand the idea of Alexandria well, and I think it has provided a lot of very good code for the community. Quickutil obviously takes a lot of advantage of the code. I'd consider this an experiment of sorts; maybe it goes against the Alexandria philosophy, but maybe it will be better. We will see. Thanks for the comments.
So we're advocating a structured copy&amp;paste approach to library use? I picked on QL here because several tutorials and discussions promote QL where ASDF:load-system (or even require) have the proper semantics.
Maybe one solution regarding QTL would be to extend it with an "offline distribution" that already contains all utilities, so they don't need to be downloaded. This would combine the old "utility library" approach and the ability to load only needed utilities into a package. On the other hand, maybe it's sometimes better to have "structured copy&amp;paste" and get a library that doesn't depend on a bunch of utility collections for a few utilities or that reimplements them.
Graphics glitches when scrolling. https://www.dropbox.com/s/s0dq8d7txgxb82b/aquamacs-glitches.png
Why not use the "real deal" instead? http://emacsformacosx.com/
Started very nicely, but then ends abruptly. Does the video cover more ?
I am using this port happily for quite a long time, much more pleasant than Aquamacs https://github.com/railwaycat/emacs-mac-port. Kudos to Yamamoto Mitsuharu.
I started learning Scheme a few weeks ago. I'm using a book called The Little Schemer. It's a pretty nice book for starting out. I like the approach that the author took when writing the book. He generally asks you questions and you figure it out by yourself. It's been a really fun reading. I do recommend it!
I'm not going to give a long-winded reply here. Suffice it to say that there are lots of portable libraries out there for doing common things like networking, concurrency etc. Often they are just as good as the ones provided by your Lisp implementation, or better, or at least good enough. So why not use them?
Macros are ok. Hygeinic macros are better than non. Expressing things functionally is best. Your mileage may vary. 
I was thinking of deployment tasks or backup tasks. Or just executing remote commands, generally speaking.
&gt; Hygeinic macros are better than non. Anaphoric macros can become ugly as hell on hygienic macros.
If this is the same harlequin that I think it is, then my dad was president of that company after they were bought by global graphics around 2000. I distinctly remember him talking about xanalys a lot too. Of course at the time I was in my early early teens and had no idea what lisp was and was only aware of harlequins work with PDF creation software and printer software. If I only knew then what I know now I probably could have chatted with some cool dudes before we moved back to America from the uk. I do remember harlequin had awesome company parties at this big house they worked out of! 
Guess it is a "Mac user using Emacs" vs "Emacs user on Mac" thing. I'm pretty much an Emacs guy and started out with Aquamacs when developing on OS X. It was pretty annoying. Emacs for OS X was much better and made using my config from Unix and Windows effortless. Still I can imagine Aquamacs being more comfortable for someone working exclusively on OS X.
That DEFROUTE looks broken. Restas &amp; Hunchentoot both make similar design mistakes: tight coupling between handler and path. What routes a particular controller handles is a deployment issue, and that decision should be left to a configuration file. (The typical solution is a simple three column text format, or even a sexpy list of triplets &lt;METHOD, PATH, Handler&gt;, where METHOD ranges over HTTP methods names, PATH is a route or wildcard, and HANDLER takes on a function designator.)
"Java" is more of a platform than a language. I have delivered Java applications without a single line of Java; just XML-based DSLs and some groovy to stitch them together. ABCL is a JSR 223 script engine, no different than Groovy, BeanShell, Scala, or Clojure.
I don't know. I've thought a lot about macros, and ultimately I think that a system like "syntax-case" is a very good trade off between expressiveness and simplicity. Plus, my _personal_ opinion is that anaphoric macros are sort of "stupid programming language tricks" and not really worth the conceptual inelegance they introduce. They are in that family of programming language features which exist solely to save programmers a few keystrokes without giving much else and without consideration to their cost. Again, your mileage may vary. 
That's why I wrote "code" and "sometimes". 
What I meant with this is that hygienic macros certainly can save you of some problems, but they also have some costs. Non-hygienic macros aren't *that* costly, if you understand how to use gensym properly.
Well, I only have some ideas on how Racket works. I'll buy Reign of Racket next month and start hacking on it. For now, I'm mostly a Common Lisp guy. Thanks for the discussion. :)
I work in Common Lisp these days and I think it is a Great platform. But it has also been static for 20 years. It would be tough not to fall behind the times in one area or another. Pragmatically, one can get by quite smoothly without hygienic macros; I only belabor the point because I think thier difficulty is overstated. 
Indeed it is, though none are in use in the CL world that I know of. [Here is a paper about one]( http://www.jucs.org/jucs_16_2/embedding_hygiene_compatible_macros). The basic idea is that one exploits symbol macros and a collection of bound and aliased symbols so that references always expand to the right object or macro in their lexical scope. You can build such a system directly in CL, but it would probably make debugging confusing without care.
Lfarm looks interesting extending the ability of [lparallel][] across multiple nodes, but I am wondering why [ABCL][] isn't supported? Lack of time to test or is it because [we haven't implemented the CLtL2 variable infomation][1] yet? [1]: http://lisp.not.org/trac/armedbear/ticket/306 [lparallel]: https://github.com/lmj/lparallel [ABCL]: http://abcl.org 
ABCL with lfarm seems to [run into problems with CL-STORE's use of FLEXI-STREAMS for which we have filed an issue][323]. [323]: http://lisp.not.org/trac/armedbear/ticket/323
Nice environment! If someone knows of a version in Common Lisp, please link.
Re. 2. Not portably, unfortunately. In SBCL you'd use SB-SYS:WITHOUT-INTERRUPTS. Most implementations have something similar. (This is an easy case since there is no potential blocking in there, so WITHOUT-INTERRUPTS is enough.) What it comes down to is: can something cause the DOTIMES loop to unwind before it is done? If the answer is yes, then it is unsafe. Count will have been incremented, but wakeups may be lost. Potential sources for unwinds are: essentially timers/timeouts and interactive interrupts (think Control-C). WITHOUT-INTERRUPTS is the hammer that whacks these moles for it's duration. Now, that said, how deadly a problem you consider this ... depends. I strongly maintain that assuming your code base to be asynch-unwind safe is not a sane proposition, because asynch-unwind safety DOES NOT COMPOSE. Even if A and B are both safe, it does not follow that A+B is safe. For more, see: http://random-state.net/log/3386927147.html -- ignore the stupid yatter about transactions. :) Hence IMO you should absolutely avoid using asynch unwinds (or more generally asynch interrupts) unless you've carefully vetted all the code they may fire inside. Still, because feckless people DO use them, and because third party libraries sometimes carelessly introduce them, I also think sufficiently central parts of infrastructure should be async unwind safe -- just because debugging those issues is bleeding hard, and those hours and days you spend trying to reproduce the issue aren't coming back. (Lost wakeups can be particularly nasty, since often enough reproducing the lost wakeup is not enough to reproduce the problem at application level since semaphores tend to return to their ground state, and often -- but not always -- threads stuck waiting for lost wakeups can be shaken loose by later updates. So instead of a hard failure you end up with oddball timing, which may or may not break things elsewhere. Re. 4. Taste varies, and I'm not overly concerned about it, really. That said -- speaking in generalities, not about BT-SEMAPHORE -- unless the API is actually /designed/ to be extensible, I think then generification is of little benefit. I also think it is often suboptimal to make the API functions generic (or only API functions generic), and far better to make the implementation functions generic: they can then have eg. only non-optional arguments allowing you to dispatch on all arguments, and you can expose interesting parts to implementors without bothering most users with them. Specifics depend on the use case, obviously, but SLOT-VALUE vs SLOT-VALUE-USING-CLASS is the canonical example (along with the attendant SLOT-UNBOUND parts of the protocol) -- dispatching a metaclass being an irrelevant detail here, for normal APIs implementation layers dispatching on class / EQLness is plenty. 
Not really. Racket actually has a very nice solution for building macros like anaphoric macros hygienically using [*syntax parameters*](http://docs.racket-lang.org/reference/stxparam.html) (there's an example of `aif` on that page). There's a blog post about using them here: http://blog.racket-lang.org/2008/02/dirty-looking-hygiene.html (note that the blog article is a little dated, so modern Racket syntax is slightly different)
Read the paper I've linked elsewhere about building a hygienic macro system in Common Lisp. The basic idea is that you want a macro to expand to the macros and symbol bindings in scope when the macro is defined, even if they have been rebound recently. In Common Lisp this is ameliorated somewhat by disallowing certain rebindings but that is just a kludge. `with-gensyms` is a nice convenience macro, but it does nothing to solve the problem that you have to remember to gensym each symbol. As I said above, this isn't a common error once you get used to it. But it isn't very elegant. Macro hygience resolves the problem. Edit: [here is the link](http://www.p-cos.net/documents/hygiene.pdf).
Bought a copy.
Are you talking about this? (let ((x 42)) (macrolet (((foo) 'x)) (let ((x 4711)) (foo)))) If you want the answer to be 42 then gensym does work: assign x to a gensym and make foo expand to the gensym. And 4711 could very well be the "right" answer, depending upon the situation. I can imagine a case where local overriding is the natural thing, which would be difficult (impossible?) to achieve with hygienic macros. You said that gensym can't solve all hygiene problems. Could you please give an explicit example of what gensym can't solve?
She is not using Common Lisp here, but [TameScheme](http://sourceforge.net/projects/tamescheme/), a R5RS implementation in C#. The link is provided inside document.
Read the paper.
I just quoted directly from the paper, and asked if that's what you were talking about. You ignored my question. You appear to be blowing smoke.
RuntimeError: maximum recursion depth exceeded
hy very good. i like. i like better than python. python bad with white space. hy ignore whitespace. hy good.
got a torrent?
except Clojure?
I respectfully disagree. I've worked in both environments: one where URLs/routes are mapped to controller/action by a config, and ones where the route is defined alongside the code that runs when it's invoked. Almost always, the former becomes a tangled mess, the latter is a lot easier to maintain and read. I found little to no benefit from separating the routes from the code being called, and I've worked with some larger APIs. I can see one instance where this would be desirable: when a GET and POST should run the same code. However, I plan to update `defroute` to allow either a keyword (like it is now: `:GET`, `:POST`, etc) OR a list of keywords. Aside from that, any variance in the route (such as URL) can be mitigated via whatever regex is used to match. Perhaps there are instances where the separation would be useful that I've not run into, but I've never really looked back after leaving config-based routing. If the system you describe is truly required, one could easily write a macro to abstract routes to call controller functions that would be processed from a config file. I believe this would be easier to do on top of the current routing system than provide the current routing system via macros wrapping a config-based system like the one you described.
I'm also put off by it being neither language. You have to learn a weird new dialect with the list comprehension stuff and other strange forms to make the underlying python happy. Why can't a function be called with keyword arguments and have them translated? Why not (print (json.dumps :obj results :indent 4)) instead of (print (kwapply (json.dumps) { "obj" results "indent" 4}))
It's great if you want to use some python module and don't want to put up with the brainfuck of significant white spaces.
Well, at least it has [recur](http://clojure.org/special_forms#Special%20Forms--%28recur%20exprs*%29) and [trampoline](http://clojuredocs.org/clojure_core/clojure.core/trampoline).
Looks like the compiler is based on CLICC and runs in SBCL.
If only there was a free license for open-source non-commercial use...
I don't know why, but I was under the impression that this would be released as free (libre and gratis) and open source software. I'm not sure how I feel. Maybe it's a good thing we have yet another commercial Lisp compiler, but it also means it's one more compiler that the average open source developer can't test with without shelling out money. I wonder what the cost of upgrading will be after one year. Even with open source, SBCL gets a steady flow of bug reports and updates. It seems, unless you have a very firm established foundation, difficult to keep up with that in a *closed* source environment.
For some reason I figured the same thing. I think it has something to do with the fact that CL has a pretty small hacker based ecosystem and license fees and restrictive licenses are poison to small hacker based ecosystems. Maybe the something like test-grid will really take off and we can have our libraries tested on a few servers that will be given free licensing by the commercial vendors (or, at worst, purchased one time). After all, it is in the commercial vendor's best interest to ensure that their imp is as compatible as it can be. One way to do that is to make sure that it is trivially easy to test libraries on their imp.
Thank you for the clarification. People keep speculating that it must be ECL based, so now there is a real answer.
Hi Wukix, I'm no mobile developer, but if there was a way of using CL for mobile, my motivation to write a mobile app (or at least mess around with it) would increase by an order of magnitude. There are quite a few open source Android applications already and it will be nice if some were written in CL.
Error, can not download it
I can get it from here: http://www.obrezan.com/eclm2013/
FAIL: No trial version :( Definitely not going to splash out $200 without knowing that this thing actually works. Could be buggy as hell... or even a scam! (Not saying it is, but on principle I won't pay for something I haven't seen) Let me know if you've given it a test drive &amp; found that it's good stuff.
Some on #lisp have tried it out, I think.
This is an enjoyable read. It feels like I'm getting to know these great developers whom I respect so much. Thank you!
I thought it was very clear - in particular in an answer to a direct question at ECLM - that this was closed-source and costing-money. I think this is an interesting experiment: is it possible to make money by providing mobile-developers the ability to program some of their application in CL? The website could do a better job of explaining the benefits a buyer can expect from taking this approach, it's true. Now, when is the Windows Phone support coming? (:
Interesting, but without a personal/trial edition (can be limited to simulator only) to try out I won't waste another thought about this. /back to obj-c :/
Guido on tail recursion: "...it's simply unpythonic"
there is [ABCL](http://common-lisp.net/project/armedbear/): &gt;Armed Bear Common Lisp (ABCL) is a full implementation of the Common Lisp language featuring both an interpreter and a compiler, running in the JVM. 
This looks cool. I will try it out.
Kindista is built using Hunchentoot proxied behind nginx. I use nginx for static files and because it handles reading complete requests before passing them on to Hunchentoot (so no Lisp threads are hanging around waiting for a complete request). The service uses Lisp as the database. The main database is a hash table of lists. Updated lists get written out to a journal file that is occasionally compacted. This is primarily because there was no single database out there that would let me generate a geolocated activity feed sorted by a combination of distance, age, "likes", and number of mutual friends with the person making the post in any reasonable amount of time. Kindista running on my laptop can generate thousands of activity feeds per second with a ~2-4ms response time. Originally I wrote pretty much the whole site in ParenScript as a single-page app, but there were so many tiny bugs that cropped up in obscure browser/OS combinations that I decided to scrap it and build the whole site the old-fashioned way. In most cases the new site is actually faster. Kindista can render the HTML as fast as it rendered the JSON, and browsers are a heck of a lot faster at rendering static HTML compared to building pages using the JavaScript DOM or templates. You can check out the (undocumented) code at https://github.com/kindista/kindista There is some help on the GitHub page for getting the site up running locally. Thanks for the comment about the quality! Edit: Why Lisp? * Interactive development * Speed of execution and development * Ease of implementing a DB with fast custom indexes * Macros save typing and add clarity * I do design and coding and Lisp is the best for generating HTML 
https://kindista.org/home will take you to a read-only demo of the main interface (some visual elements are hidden from people who aren't logged in). Kindista is a social network that helps people share things and service with people who live near them. It does some neat stuff like providing a Facebook-like activity feed that is geo-specific and sorted by a combination of distance, age and "likes". It's quite fast and has a decent responsive layout for mobile and tablet support. Here are some more nerdy highlights: * 100% Common Lisp and CSS (uses SBCL-specific concurrency features) * Written using Hunchentoot * Runs behind nginx for static files and because nginx handles complete requests before passing them to Hunchentoot * The Offers and Requests sections have a keyword-based category system that automatically chooses the top- and second- level categories based on the keyword frequency of offers and requests within your geographic distance preference * Lisp is the database. Kindista writes updates to a journal that can be compacted. * Generally has a 2-4ms response time for even the most complex pages when running locally The code could use some documenting and refactoring. Too many parts of the system know too much about the database. Kindista launched on May 1, 2013. There was a previous beta that launched last year that was something like 80% ParenScript, but there were too many subtle bugs reported that I could not reproduce, so I decided to scrap it and rewrite it using old-fashioned techniques with a modern-looking frontend. It seems to be a winning combination, because generating each page on the server is as fast or faster in most cases than doing an XMLHttpRequest and generating pages using the JS DOM or client-side templating. The code (Allegro GPL) is available with some setup instructions at https://github.com/kindista/kindista You'll also need the 'convert' tool from ImageMagick and DOUBLE-METAPHONE from https://github.com/kindista/double-metaphone in a loadable location. Let me know if you'd like to get it set up and I can give you a minimal database file.
The kind of enterprise I dealt with was risk averse to, perhaps, an unwise degree. But I don't recant my judgements. I am happy for you, sir or madam, that you have clients with freer pursestrings. 
I think I was a bit harsh, my apologies. I was responding to the unquivocal 'dead in the water for Enterprises' comment, which I do not agree is true in general and a little harsh to the developers. I just want to balance out the unquivocal no go with a maybe it's not so bad. 
Awesome! Thanks for sharing the details. I'm planning for some time to complete one project (SBCL + caveman) but I'm not sure how it will behave 'in the wild' on small VPS. The project itself is not heavy, but my main concern is SBCL memory usage: even for small apps it can eat a bit, almost like java. How kindista behaves on your setup, like average memory/IO usage? It is possible to tune SBCL for 512MB or 1G VPS?
yeah, this is the major problem. I like Aquamacs, but I couldn't use many of the recent packages. If they move Aquamacs up, I would give it a go again.
&gt; https://github.com/railwaycat/emacs-mac-port thanks for this... I'll give it a try!
invitation only? seriously? That model has only worked once in the past (gmail), and failed many many times.
We're not in a hurry. Trust is more important to us than growing quickly.
Awesome. Thank you very much for taking the time to answer my query. I wish you the best of luck, though, with the visual quality (I am not realy qualified to comment on the technical side) you should do well.
Wow, thanks so much for the constructive feedback! I went ahead &amp; used WITHOUT-INTERRUPTS in SIGNAL-SEMAPHORE. (SBCL &amp; CCL only for the time being) I've also removed the DEFGENERICs since I realized that (a) it's unlikely anyone will want to extend BT-SEMAPHORE, (b) people can always add the generic functions themselves should they need to, and (c) as you said, dispatching on class/eql is good enough in most cases.
Thank you for the info. I am playing around with hh-web and any other framework that allows me to code up the site in pure Lisp without having to touch js/html/css (as much as possible). The experiments have really been a huge amount of fun and I can see in the future being able to deploy web apps quickly and picking the frontend deployment (currently html+js+css being the current favourite). I am glad to see someone hack up something this good looking and kind of daring for the data storage part. Thanks for sharing.
Did you actually get an undetected integer overflow in mocl? If so, it's a bug, because mocl does have bignums. (source: I implemented them)
So, at the risk of repeating what everyone else has said, I think the absolute lack of any way to get any insight into mocl short of shelling out 200 bucks is an outright dealbreaker. I absolutely despise the "enterprise" kinda software licensing where you're expected to pay first, ask questions later, and mocl doesn't even even have a decent description and demo. What are the limitations? What is the API, how does it actually work in code? Those two lines of sample code are at least 20x too little. Keep in mind that you're targetting the mobile space, which is full of small, one-man shops doing small apps. This is not the kind of market where you can just assume people will have 200 dollars to burn just to play with something before even having any idea if the project they might have in mind is at all feasible. A lot of the time the decision to implement X is heavily dependent on whether there are tools around to make it doable, and the pitiful blob on the page is absolutely not sufficient, especially in the light of likely limitations lispm highlights. 
*how does it actually work in code?* https://github.com/Wukix/mocl-example-lisp-contacts-ios
There's almost as much Objective-C code in there as Lisp code. Is it not possible to write the UI code in Lisp as well?
According to my own test, (* m-p-f m-p-f) returns 4611686014132420609, which is correct for 32-bit mocl. Are you returning the number across the "glue" boundary? Can you be more specific about how you are obtaining the result? Edit: managed to reproduce. It works correctly under -O0, so looks like a problem with optimization.
The alternatives seem to fall into two categories, possibly overlapping: 1) Extensive wrappers around platform APIs 2) Writing Obj-C through a Lisp layer, e.g. Clozure style http://weitz.de/eclm2013/ccl.pdf I consider both of these alternatives worse, or at least economically unfeasible in the case of #1. At least with the "just use Obj-C for the GUI" approach, I know that I'm not actually being *less* productive going through a sketchy middle layer. It's not an ideal situation by any means, but I feel that this is the reality that we live in.
Thanks for the response. I was hoping that mocl was going to solve the pain of having to deal with Java on Android. I think it's great that you've done what you have, my anticipations notwithstanding.
Well it's simple. Suppose I'm writing a function A. In the REPL I try it out with a couple arguments to see if I wrote it correctly. A little bug? Fix it right away, recompile the function and retest. It works? Copy the last few lines in your tests files by adapting a bit to comply with your test framework, and you're done. As you see, it's the opposite of TDD: we test in the REPL then make it green. It's the other way around in other languages. What you can't do in python in the REPL is recompiling one function. Reload everything or gtfo. Lisp allows incremental compilation, function after function. Only evaluate what you want.
For a tool supporting the REPL testing aproach, take a look at: https://github.com/rpav/CheckL
Hello subtlepath, This is great! Please, *pleeease* write a moderately detailed blog post about the architecture and deployment of Kindista. Your reasons for the decisions you've takes would be nice. All the best,
Working on including stuff in http://abcl.org/svn/trunk/abcl/examples/gui/, for which it should be something like 20 lines of Lisp to pull up a "Hello World!" JFrame with an "Ok" button. Stay tuned, Bear fans…
I see. I guess that's good on very large projects, especially with lots of large dependencies. I have about 10k lines of Python in my library of projects, and all unit tests run in under a second (not 100% coverage, though), so I haven't suffered from this issue yet. I could see it happening eventually, however. I suppose one of the things that helps me is Vimya, a plugin for Vim that runs buffer or selected code in Maya (3D package that I code for). I can `vip` to select a block of code (visual select in paragraph) and `&lt;Leader&gt;sm` (leader being comma) to send to Maya just the selected block (`&lt;Leader&gt;sb` runs the entire buffer). It copies it to a tmp file and runs just that in Maya, so it's common for me to use Vim almost like a little REPL, with Maya open on the other monitor. That only helps me test out functions, though. For tests I use nose and I have it find the first `test/` folder next to or above the current buffer and run `nosetests` from there, so I definitely can't be as selective about that.
Thank you for the feedback! You certainly sound like someone we'd love to have using the site, and I'm sorry that we're currently getting in your way. Our intention is to have people use the name that people who live near them know them by. That could be your legal name or "Little John". The site would be diminished by people using random site-specific pseudonyms. Zipcodes don't work. I bike everywhere and the geographic center of my home zipcode is over 20 miles from where I live. In some cases the discrepancy is much worse. Some people just enter the street they live on without a number, or a couple of streets over. The point is to get a latitude and longitude that will give you a useful experience of the site, and to help others get a realistic idea of how far away you are from them. I'll be thinking about improving the signup experience. Thanks again.
All changes are journaled out to disk as they occur. The whole DB is periodically dumped in a more compact form. Currently the site is using under 10MB of RAM for data. When "buy more RAM" stops working I'll switch to a disk-based approach with a cache and in-RAM indexes. I've had a tendancy toward premature optimization in the past, so I'm actively not worrying about it :-)
Will do! Thanks for the encouragement. I've been working on this without any dialog with other experienced programmers and it's nice to have encouragement to share about it.
By clicking on the [full documentation](http://rpav.github.io/CheckL/) link, you can discover that there is a run-all function that re-runs all tests defined up to this point. You can also persist the informal tests or integrate them with more formal fiveam tests.
Cool! Now expose the documentation, and put up a tutorial, so I can see how your various bulletpoint claims translate into toolchain. Then make a public user forum for your customers to share code. Finally, it'd be a really good idea to provide a free version with open source / non-commercial use requirement (and since it's mobile, I guess you'd need specifically to forbid adware as well). You really want and need to have a community around it: it's a completely different thing to buy into an expensive black box where I just as well might be the first person ever to use it, and to get into something where I can see people are able to get things done. And with an open source edition, you get that community for free, plus you're hooking people up mocl if it's really as good as you claim. It'd absolutely *love* to have a solid, reliable way to use Lisp on mobile. 
Any link to that? I've never seen it referenced anywhere, and I've done a good deal of googling for something that could serve as a treeshaker in SBCL.
Can't find any reference to it myself, but I could have sworn it was a GSoC project sometime around 2009 or 2010. Or maybe I misremembered the clojure treeshaker project from last year. Sorry to get your hopes up :(
Why not have both? Hygeinic as default, selectively dirty when needed. 
That is essentially syntax-case.
I'd be more interested to read about level of interoperability between Common Lisp and native platform, including examples of calling back from CL to, say, Objective-C. That and how does debugging of such mixed code work. Well, I guess I'll have to buy myself a license, maybe next month ;)
Right now mocl only supports a kind of "Reverse FFI" where you call CL from ObjC or Java. This is a hard pill to swallow for some, because it means mixing languages. However, it is my belief that this is a better and more realistic alternative than having to go through a sketchy middle layer to reach platform APIs. It is also worth mentioning that within CL you still have essential platform capabilities: file io, network io, pulling environment information (e.g. current date/time), etc. By the way this approach is nothing new: on the web we tend to accept that the client code will be javascript, and cross-platform desktop programmers have long resolved to do the UI in the underlying platform, and then bundle their application logic into a portable library. The only difference here, is that the portable library can be in Lisp instead of C or C++.
As far as passing sequences and other heap-allocated types (other than string), my advice is: don't. This would mess up garbage collection semantics unless we copy, and convert the object in full. Or possibly we could provide manual memory management features... but... yuck. Instead the recommendation is to create accessor functions for the types you want and expose them via call-in. e.g. (defvar *shared-sequence* #(100, "asdf", 3.14)) (declaim (call-in shared-elt)) (defun shared-elt (i) (elt *shared-sequence* i)) Another possibility right now is to pass large data by files. Not very elegant but it works and is easy to reason about. A third possibility is to use some string encoding scheme like json. mocl is very much a '1.0' and we are hoping to improve quickly. You know, the first iPhone didn't have app programmability at all. The first mocl does not do everything, either :)
There is no "forward" FFI yet for calling outwards from CL. But we hope to add this soon.
Two posts from same user on same product within a week? 
Do you mean by Mark Watson? If so, I cannot find the other. If you mean lispm, you can go back for several years and see lispm has probably posted more to this reddit than anyone. I don't thinkhe's a shill.
Ok thanks, I'm looking forward to it.
I'm not worried about essential platform capabilities. I was just wondering about possibilities for how could the API of such a portable library look like. I'm not even worried much about mixing languages. I've been doing it for years at my job. One thing I've learned is that debugging capabilities tend to break around language boundaries, and that makes life somewhat more difficult. The final thing is about support for asynchronous execution. Well, I guess you could always get something like that using polling. I guess "forward" FFI would allow for other models as well.
Dylan has always struck me as pretty interesting, but I can't get over the absence of s-expressions. 
Looks interesting, but it does not really explain anything. Also, the link to "boots" is dead.
Oh wow. This looks amazing. I have looked at Cuda and really have no apetite for it. I like the familiarity of this. Very nice find!
If I decide to use ClojureScript right now, how much JavaScript should I know beforehand? And how much Java should I know? And how much Clojure?
 Let me guess. So in ClojureScript, (dotimes (i 10) (something i)) doesn't expand to (let ((i 0)) (while (&lt; i 10) (something i) (setq i (1+ i)))) rather it expands to (let ((i 0)) (while (&lt; i 10) (let ((ii i)) (something ii)) (setq i (1+ i)))) right?
I don't know how well ClojureScript works as a library. They run it through the Google Closure compiler to do a lot of optimization.
If ClojureScript does something to avoid capture of mutable free variables, then a better example would be: (defvar a (make-array 10)) (let ((i 0)) (while (&lt; i 10) (setf (aref a i) (let ((ii i)) (lambda (n) (* n ii)))) (setf i (1+ i)))) Notice that the closures escape the loop by getting stored into an array. Without the rebinding to ii, all the closures would see i bound to 10. (Sorry if I misunderstood the question, I didn't watch the video, and I'm not familiar with ClojureScript).
&gt; I read somewhere that the OpenDylan developers were &gt; thinking of switching back to sexps, or at least offer it too. We're open to the idea of it being an alternate syntax by adding an additional reader. There are some issues in that but that's where the fun lies. There's someone that is working on this off-and-on, but some assistance would be welcome. 
Well, if you spend a lot of time with CL, you'll begin to see it makes more sense than it seems to initially. But I take your point, for sure.
I recommend checking out [Rosetta Code](http://rosettacode.org/wiki/Category:Programming_Tasks). That way, you can pick tasks that interest you, try to solve them yourself, but still have a solution to check against. 
&gt; From those of you that still remember learning lisp, what helped you? Scratching tiny itches, like writing useful emacs functions. &gt; Do you have any advice? The usual: simply give it more time, then you will start thinking in Lisp even when you have to write other languages. &gt; Is there a resource you can recommend, or anything like that? [RosettaCode](http://rosettacode.org/wiki/Rosetta_Code) has a lot of non-mathematical tasks not yet impemented in Lisp.
This is just a guess but as far as I can tell this should work. ClojureScript (like CoffeScript) by default does not export any symbols to the global namespace. It can be forced to do so by annotating the methods you wish to be "public". Like drudru said, everything you import (from clojure or the closure library) gets munged by the Google Closure compiler into one big file. All nonpublic symbols get aggressively minified (if you choose to use advanced optimization). This file should be, in principle, usable on any site. I do not know wether or not ClojureScript's standard library depends on ES5 features (thus is not usable by older browsers). I have no experience in loading external JavaScript from ClojureScript.
I'm still learning lisp and I've also started Project Euler. I started with The Little Schemer, here I learned basic things. When I jumped to Project Euler I've used techniques learned there without any problems.
+ scheme stuff ( in order to understand the continuation passing style, loop vs recursion) + Haskell stuff + asdf 2 manual + some of cltl2 + source codes written by the other people (most important)
There's a [simple todo app demo on github](https://github.com/dfuenzalida/todo-cljs) that's self contained if you're interested.
The most important thing is practice writing code, any code, especially since Lisp is your first language. It takes a while to become skilled at problem decomposition and 'coming up with the answers', don't be worried if it takes a while.
My language crawl/progression went something like: Basic &gt; Pascal &gt; C/C++/x86 Assembler &gt; Java &gt; Ada &gt; C# &gt; Forth/Lisp I'll echo what others have said that you just have to keep using the language, reading others code and breaking stuff. Break lots of stuff.
Emacs Lisp has an advice system and Common Lisp probably has several implementations of Aspect Oriented Programming. 
I would highly recommend finding a problem that interests you and solving it. Even better if it's a problem that you made up yourself. It can be large or small in scope. Working on something that is *yours* is a real motivator, as opposed to some preset regimen that someone else made, like homework.
I think it's misleading to think of PAIP code as AI. It's a collection of useful techniques that were once considered AI, but now are just generally useful.
I'd look at source from C GTK projects. It's a lot easier to find than Lisp source, and the C names are easy to convert to Lisp bindings if you have the documentation. I'm new, and I don't know anything about Lisp or GTK programming, but that's how I'm figuring out libtcod right now.
I started with Pascal, not Lisp, and went through a series of languages, and what kept me motivated all the time was that I was building something that I liked and cared about. I don't think problems like those on Project Euler or Programming Praxis are that great for learning a language. They're nice if you already know the language and you want to flex your coding muscles. So my advice would be: find a problem that you are really interested in or an itch that needs a scratch and do that.
More like a joke. Emacs Lisp has [`defadvice`](http://www.gnu.org/software/emacs/manual/html_node/elisp/Defining-Advice.html) built in. `Defadvice` is more or less an implementation of ideas from [Aspect Oriented Programming](http://en.wikipedia.org/wiki/Aspect-oriented_programming). When he said "looking for advice," I jokingly pointed him to these ideas. Apparently that joke was miscalibrated. 
Quick search produced [cl-cad](https://github.com/LiteTabs/CL-CAD) a large CL project using cl-gtk2
If 'turning cheap thing into expensive thing' were a snap we'd all be rich. (This isn't complaining about the language, but about the article.)
There, this is not a post about Clojure. Can you all calm down now?
&gt; ...I feel that I have a decent understanding of some of the syntax... What syntax?
Does that actually matter? Doesn't HTML5 give you the necessary tools to make the static resources cached permanently?
I don't think it's really an issue, especially considering that the whole runtime is about 100kb.
That's right: that's the kind of hygiene problem that gensym does not solve. That example is artificial but this kind of thing does happen when a macro expands into code that calls a helper function but a function of that name is inadvertently defined by the user overwriting the one the macro expects. I think the standard solution in Common Lisp is to use packages: put the macro along with the helper function your macro needs in separate package to decrease the likelihood that the user will redefine it. I think using with-gensyms and packages is roughly the same amount of work of using syntax-case, but syntax-case actually keeps you from screwing up while the CL approach requires you to be disciplined.
I'm not sure I understand your package issues, but in order to change the package in the slime-repl buffer you can use **C-c M-p** or **,in** (which is what I usually do, **in** being prefix of **in-package**). One way to have your make-instance at the top of the file is: (defun make-important-object () (make-instance 'bar ...)) ;; defclass etc. (defparameter *my-important-variable* (make-important-object)) Or you could also have the variable definition (say defvar without initial value) at the top and set it at the bottom, or the function could set it so you only have to call it at the bottom... you get the idea.
Completly agree on that. PAIP is one of those rare books that shows the iterative nature of programming. It's more a book on programming style than AI.
When I started with Lisp, the challenging part was to unlearn a lot of OO habits I'd picked-up over a decade. Learning to think differently takes time. I've found that the combination of Practial Common Lisp, The Little Schemer and finally The Seasoned Schemer got me into that new mindset.
Why not use cryptographic hashes to verify the download? How does quicklisp work in this regard? 
please continue your good work, current utils situation sucks! 
Quicklisp uses a size, md5 &amp; sha1 sums of package to verify its genuineness.
You can also put the *defclass* in an auxilliary file. ;;;; bar.lisp (defclass bar …) ;;;; main.lisp (defparameter *my-important-variable* (make-instance 'bar …)) ;;;; foo.asd (defsystem #:foo :serial t … :components ((:file "bar") (:file "main")))
eval-when?
There is no doubt it could be made cryptographically secure. But doing it right is not easy. Cryptographic hashes vs. non-cryptographic hashes don't really make a difference for verification. All crypto hashes do is make sure it is very difficult to reverse the hash/find a collision. Here, that doesn't matter. There are no secrets being shared. But let's suppose we use hashes. That means we need a trusted source for the hashes. Of course Quickutil.org could be a trusted source (but that means you must trust we did security right there), and either you can trust the Quickutil application will download the right hashes automatically, or the user will have to check the hashes on the site manually. Both of these are bad options, because the issue in the first place is that you can't trust the payload coming from the site. In addition to all of that, hashes would be hard. They couldn't be statically generated. If we have N utilities, then there are 2^N possible choices of payload. Right now, that means there are 6739986666787659948666753771754907668409286105635143120275902562304 static hashes to compute, which is infeasible. We could dynamically compute them, but that means they're not up for scrutiny. The best way to ensure authenticity would be a combination of things, but the biggest thing would be to sign the downloads with a trusted key à la public key cryptography. You might blame it on my laziness, but I don't think it's preferable to add a whole public key crypto system to the server and the client. **The major downside** of not downloading from the web, though, is that updates now depend on the rate at which Quicklisp is updated. There were already 22 major enhancements, additions, and bug fixes, **just within the last 24 hours**. With the internet solution, they'd be deployed to users at the moment they download. With Quicklisp, you have to wait a month or more to receive them. But perhaps this won't end up being as big an issue as it seems, especially when Quickutil stabilizes.
I had to do this instead: nc -l -p 6007 ("man nc" doesn't show "-k" as an option --- I'm running LMDE 2013). Otherwise, it works fine. Thanks for the nice idea.
That's beautiful--thanks! If only there was a `slime-quickload-package`. Maybe it's time to learn how to write custom Emacs functions.
There is a solution for low-frequency updating of Quicklisp's dist, make your own one. I did [it](http://lisp.hyperprostor.unas.cz/) for purpose not to be in the official dist.
What would that do?
When invoked inside a Quicklisped file, it would be equivalent to executing `(ql:quickload "correct project name")` in the REPL. Just as `slime-sync-package-and-default-directory` is equivalent to executing `(in-package #:correct-package name)` (and setting the default directory). Hence when I open my project in Emacs I couldjust invoke both functions and be ready to go.
It's not always clear what system must be loaded to define the desired package. Although I suppose you could do some peeking in parent directories for an *.asd that includes the current file. 
the main difference between optima and others is that optima is a pattern compiler. it is focused on speed. it's supposed to be much faster than clos based one. (match a ((list a b) (print a) (print b)) ((vector a b c) (list a b c))) macroexpand --&gt; (BLOCK #:BLOCK1325 (TAGBODY (RETURN-FROM #:BLOCK1325 (MACROLET ((FAIL () '(GO #:FAIL1326))) (IF (CONSP A) (LET ((#:G1323 (CAR A)) (#:G1324 (CDR A))) (DECLARE (IGNORABLE #:G1323 #:G1324)) (IF (CONSP #:G1324) (LET ((#:G1333 (CAR #:G1324)) (#:G1334 (CDR #:G1324))) (DECLARE (IGNORABLE #:G1333 #:G1334)) (IF (NULL #:G1334) (LET ((B #:G1333)) (LET ((A #:G1323)) (PRINT A) (PRINT B))) (GO #:FAIL1326))) (GO #:FAIL1326))) (GO #:FAIL1326)))) #:FAIL1326 (RETURN-FROM #:BLOCK1325 (BLOCK #:BLOCK1335 (TAGBODY (RETURN-FROM #:BLOCK1335 (MACROLET ((FAIL () '(GO #:FAIL1336))) (IF (TYPEP A '(VECTOR * 3)) (LET ((#:G1320 (AREF A 0)) (#:G1321 (AREF A 1)) (#:G1322 (AREF A 2))) (DECLARE (IGNORABLE #:G1320 #:G1321 #:G1322)) (LET ((C #:G1322)) (LET ((B #:G1321)) (LET ((A #:G1320)) (LIST A B C))))) (GO #:FAIL1336)))) #:FAIL1336 (RETURN-FROM #:BLOCK1335 NIL))))))
I use postmodern. I use both the dao and s-sql. They both have their place.
I've got a similar library called [`shadchen`](https://github.com/VincentToups/shadchen) which is about as complete, but not as optimized as Optima. It really isn't anything special, except that there is [an implementation in Emacs Lisp](https://github.com/VincentToups/shadchen-el) also, and the two implementations are more or less simultaneously developed and stay very close to one another. It may be worth noting that we use Shadchen where I work and so it is at least stable enough for that, and it gets routine development and exercise. I use the Emacs Lisp version of shadchen _very_ extensively as well. 
Thank you for your answer. Care to expand of that? For what does s-sql fit better than dao? updates? Updates across different tables? Do you have common idioms for dao?
According to the [Quicklisp Hot 100](http://blog.quicklisp.org/2013/06/top-100-downloads-for-may-2013.html): | | 05-2013 | dls | 10-2012 | dls | |----------------+---------+-----+---------+-----| | clsql | 25 | 524 | 48 | 237 | | postmodern | 39 | 285 | 50 | 228 | | bknr-datastore | 77 | 151 | | | | cl-store | 83 | 146 | | | | elephant | | | 69 | 139 | 
For reporting, I tend to use s-sql. Why involve the entire record unless there is a reason to involve the entire record. For updating, I tend to use the dao because the inputs generally allow the user to input any field in the record. I have other applications where the tables are just numerical and booleans that get used in calculations. In those cases, I tend to use the daos as data structures passed to methods or functions.
What are the numbers &lt;100? I'm not seeing them anywhere on the blog.
And the program easy to read, literate programming : http://redlinernotes.com/docs/cl-6502.pdf
the place in the rankings out of 100 listed. The blog orders but not numbers them.
How dependent is Harlan on Chez? In other words, how difficult would a Gambit , Bigloo, or Racket port be?
That's a really nice writeup! I'd love to see that sort of thing for more libraries. A couple more documents like that that covered some major facets of using lisp could really make it easy to jump in.
Cool. Learned a lot. Thx.
The code is gorgeous.
502 Bad Gateway?
&gt; Regarding converting to a full 64-bit Lisp, this would be a much bigger effort. Corman Lisp gets such good performance by a large amount of hand-optimized x86 assembly code. This would all have to be rewritten/replaced in a 64-bit Lisp. One might as well write a better compiler. AMD64 is such a different beast that one would profit from rewriting the whole thing.
What's Corman Lisp? How does it compare to Common Lisp or Racket?
Corman Common Lisp is an implementation of CL for Windows. See link for more info about why it was cool in its day.
Corman Lisp is a Common Lisp compiler/IDE/debugger for 32bit windows. I haven't used it more than once or twice, but I have heard that it has really good win32 integration and is pretty fast.
It hasn't worked for years.
I always learned by picking a project. What are some things you've always wanted to program? Do you want to make a game? Do you want to build web apps? Are you interested in robotics? Physics simulation? Processor emulation? Neural networks? Compilers? Pick something, no matter how huge it seems (besides maybe "build an operating system," which is very ambitious), and start chipping away. If you're doing a game, how do you open a window and display a triangle on the screen? Build from there. If you're building a web app, what web server will you use, and how will it communicate with your database? Pick a few projects and play around with tying them together. If you're building a robot, how do you send out commands to your various motors, and how do you collect data from your sensors and act on it intelligently? Some sort of USB interface might make sense here. I think you get the picture...take a project you really want to do, even if it's a 1, 2 or 5 year project, and *do it*. The devil's always in the details, and the more you work out those details, the more you'll realize that you're starting to slam out lisp code without even thinking about it. Soon enough, projects will start flocking to you, begging to be built, and the only thing you'll wish for is to have all the time you need to build them =]. Don't be afraid to look at other people's code and use their projects. Just by being *near* lisp code all the time, you start to pick up on things. Hope that helps.
Congrats David!
Hey David, why gpl for blocky?
I poked around, and there's at least some simple word-in-buffer based completion available, but it's not slime quality yet. There might be something better, I don't know, but here's how I found it: `C-h a` (apropos) `Completion RET` shows a bunch of completion-related things. Noticed there was a couple commands and a mode. `M-x Completion Mode RET` `M-x Bind Key RET Completion Complete Word RET C-i RET RET` and now some form of completion should be bound to `C-i` It'd be fun to add proper completion if it isn't lurking around somewhere already, since the editing environment is in-image, and the basic mechanism is already there... 
Haha.
Congrats. If I can assist testing the OSX version, just drop me a line.
&gt;The IDE is tied to Windows for a variety of reasons, but all can be solved with some time and love. (See http://dylanfoundry.org/2013/01/18/why-is-the-opendylan-ide-only-on-windows/[1] for details.) It says, the IDE is tied to the DUIM framework, but I can't find any meaningful information about it. Can you elaborate on this topic a bit, please...
Elaborate on DUIM? or Why the IDE is tied to Windows?
On DUIM, please, I can't find any information on it. Is it Dylan-specific or a general SDK?
It is Dylan-specific. It was originally written by Scott McKay (who also wrote CLIM). The docs for it are a bit of a mess currently (partially converted from Framemaker), but they're linked from http://opendylan.org/documentation/ The DUIM Reference (in old HTML format): http://opendylan.org/documentation/opendylan/dref/index.htm Building Applications with DUIM (converted to the new format): http://opendylan.org/documentation/building-with-duim/index.html Currently, the only working backend for DUIM is Windows, but as I said above, we're working on Gtk+ bindings and then want to revive the Gtk+ backend for DUIM. Assistance with any of this is very welcome. Visit us on IRC, the mailing list or drop me a message on here or in email. Hope that helps!
Definitely. The Mac version should be up within a few weeks. 
Thank you :)
It's just what I use by default, however I'm considering a change to MIT-style license.
I would appreciate that, or maybe llgpl, just so the license won't be viral for the software that I make that uses blocky
Right now I'm using Clozure Common Lisp to create an executable image with the game engine, libraries, and 2x0ng game logic all bound together. Then the resource files are in a subdirectory, with .PNG and .WAV and .OGG. Source code is also included.
Is it possible to use static linking for ffi with save-and-die?
It isn't.
That's sad. Deploying would be much easier.
Yep, I've read it, but I don't remember seeing let over defmacro, that's why I was asking. But I think that xach's answer is a compelling explanation as to why this pattern is not that common :)
Awesome! Thank you very much for writing this. I will be using this in the near future.
This looks pretty neat. Another project that does a similar thing but uses the inotify library in the linux kernel is called [cl-inotify](https://github.com/Ferada/cl-inotify). There's another one by stassats on github, but I don't know the differences between the two.
I assume you've probably made this just for monitoring some small number of files in a hobby project or so, so it wouldn't matter, but just letting you know that this is a very inefficient way to do it. If you want to do it right, you should give it an inotify backend on linux, use FindFirstChangeNotification et al on windows, and a NSFileManager on OSX. That way the kernel will send you a notification whenever it does a change to the file, so you will never have to manually query the filesystem, do any comparisons, et cetera.
Hi Florian! This is a nice effort, thank you for doing that, but I would not call this library fs-watcher but fs-poller. As @Amadiro says, this is a very innefficient and bug prone way of making updates based on file system changes. A couple of suggestions about your code: * You should use global variables only when they are needed. For example the \*ONCHANGE\* variable is calling for bugs. * Prefer lists to vectors. Better: don't impose a data format to the user of your library and treat sequences as sequences (vectors or lists). * I don't think using CLOS was really a good idea, you could write the same with much less code and without the bulkiness of CLOS. * the method FIRE-CHANGE suggests that some changes will be committed in response to a timer. What about calling it NOTIFY-LISTENER instead? Finally, you could put the documentation right into the REAMDE.md file, so it is visible on GitHub. Not everybody is willing to compile a single page of LaTeX.
Hi, Thanks for the comments, they're appreciated! - About the `*onchange*` variable, I didn't want to pass the function everywhere around, so I thought it'd be better to have a single global variable. Besides, a single one has to exist. - Vectors or lists can't be treated the same way tbh. `loop for watcher in watchers` doesn't work on vectors, `mapcar` doesn't work on vectors either. I chose vectors for convenience: their syntax is easy. (Easier than lists with a single element: `(list "/path")` vs `#("/path")`.) At least, as far as I know, maybe I missed something? - Yeah? Maybe it's my OO influence. I found it gave me some nice data structures. - Indeed, that's surely a better naming. Some names are hard to find! About the documentation, I know. But I recently fell in love with LaTeX. Pleading guilty. Lastly, I now know it's clearly not the best way to go. However, I just tried running the watcher on a directory with 5k directories + 15k files, and it ran just fine. (A couple seconds necessary for the original loading, but then really fine, my CPU/hard disk/memory weren't very busy.) If I'd be doing it again, I'd try to find another way; maybe try to see how inotify does it, and do it in Lisp? I don't like C libraries tbh. Now I am just saying I already thought about all this, not that I won't apply any of your advices. I sincerely appreciate them and will take them into account when refactoring the library. Edit: I forgot about the name of the project. In all the other environments, the word is "watcher", so I wanted to use the same. I don't look for "filesystem pollers" when I want such functionality, I'm looking for a "filesystem watcher". Technically, fs-poller is indeed more correct.
You don't need any global variables. Using a list is much more natural in this situation, but as tuscland says, it is better to accept any sequence, and accepting a designator (where a non-sequence designates a one-element sequence) would be even better. The syntax for a literal list with a single element is '("/path"). Here's a possible implementation, sans CLOS, sans global variables: (defun watch-pathnames (pathnames callback &amp;key (sleep 1)) (unless (and (typep pathnames 'sequence) (not (stringp pathnames))) (setf pathnames (list pathnames))) (flet ((modtime (pathname) (when (probe-file pathname) (file-write-date pathname)))) (let ((modtimes (make-hash-table :test 'equal))) (map nil (lambda (pathname) (setf (gethash pathname modtimes) (modtime pathname))) pathnames) (catch 'done-watching (loop (sleep sleep) (map nil (lambda (pathname) (let ((modtime (modtime pathname))) (unless (eql modtime (gethash pathname modtimes)) (funcall callback pathname) (setf (gethash pathname modtimes) modtime)))) pathnames)))))) 
The portability was one of the things I thought about when considering inotify based projects and your project. Now I'm curious if there are kernel based options for monitoring the file system in other unices.
Thanks for contributing to the Common Lisp ecosystem. If you add support for the OS specific notifications others have mentioned, please keep polling as an (portable) option.
I don't really know what ChangeSafe does. I do know that I enjoyed the white paper Joe Marshall wrote about it and I'm looking forward to reading the sources to see if there are any interesting ideas or tricks. At the time ChangeSafe was being developed, git and darcs did not exist. 
Are you talking about [this document](https://sites.google.com/site/evalapply/changesafe.html)? It does look interesting. Apparently it is not only a VCS, but a *product configuration management tool* that includes one, among other things. I was wondering about the closed source because I was thinking about plain vanilla revision control, for which I would never use anything but open source. I have ancient (10+ yrs) stuff on my HD managed with CVS (things I haven't touched in a while, I convert actively developed repos to git), and it still works fine. Would be worried about that with a closed source system.
Don't want to be a party pooper, but I couldn't find the BSD license in code or in readme/license.txt. I'm not a lawyer, but some work is still required to relicense as BSD. All the code has a proprietary license header.
Since he's expressed his intent pretty clearly, I bet he'd take a patch to make that code change.
He just posted this [https://code.google.com/p/jrm-code-project/source/browse/trunk/ChangeSafe/LICENSE.txt](https://code.google.com/p/jrm-code-project/source/browse/trunk/ChangeSafe/LICENSE.txt) as a response. edit: clarity
Which presupposes there is a problem. 
I thought his orig blog series was fantastic (copy still in the stack of papers on my nightstand...) and often wondered what had gone wrong that he must have given up.. Come to find out, just the opposite. There is considerable evolution if the concepts since then to include distributed, versioned objects, etc... Only had a few minutes to glance at briefly over dinner but there are a few designs ive already found useful to learn from, eg persistent hash, persistent symbol table... It does not appear though that pstore was still in use as all of the system definitions clearly are building allegrostore (from source) and none refer to pstore at all. 
It is a problem when a whole library is needed for just a tiny 10-line utility macro or function and the rest of the library isnt used. And then you need 10-20 of these utilities and they are scattered all over the ecosystem. The library consolidation by Quicklisp made it easy to find and get whole libraries. But if you're looking for just a single function or macro, it is often still easier to quickly write it yourself than go wading through all the libraries to find it.
While I would never pick a closed source version control system, I see a lot of companies doing so. They love paying for support in order to have some kind of guarantee that things will be fixed, or have a contract with someone they can put the blame on. Also, a lot of managers know how to use one system (for instance, Perforce), and don't see the point in learning something new, or changing their way of working.
Repost: [http://www.reddit.com/r/lisp/comments/1ghizu/quickutil_beta_a_new_utility_platform/](http://www.reddit.com/r/lisp/comments/1ghizu/quickutil_beta_a_new_utility_platform/) 
I recognize some teachers voice. Funny feeling.
 this site is horribly broken on mobile. FYI 
I enjoyed this article, though it would be interesting to see what kind of implementation a seasoned racketeer, schemer, or common lisper would come up with. Much like the author, I really, really like map destructuring in clojure (I also really like the pervasive immutable data structures). Given than Racket is a toolkit for making new languages, I wonder how hard it would be to replicate Clojure there? That, plus some features of Typed Racket, start to make things sound pretty good...
&gt; "Since Emacs Lisp is a Lisp-2, higher-order functions are fairly awkward" I program in a Lisp-N all the time, where 'N &gt;= 2', and would like to know what is meant by that? &gt; "Partial application was recently added to Emacs" I would like to know what is meant by that as well ... lambda has been around for a long time. But, really the only reason that it is now useful to have partial application as "Function: apply-partially func &amp;rest args" is that lexical scope now exists in elisp. so : ELISP&gt; (defun p-apply (function &amp;rest args-to-left) (lexical-let ((function function) (args-to-left args-to-left)) (lambda (&amp;rest args-to-right) (apply function (append args-to-left args-to-right))))) p-apply ELISP&gt; (let ((1+ (p-apply '+ 1))) (funcall 1+ 2)) 3 Now that I think about it, dynamic scope does make higher-order functions harder, so regardless of that Lisp-N thing, defaulting to dynamic is indeed odd. If only there was a Lisp which took what Scheme and LISP had in Common , was ANSI standardized, and works on many platforms! :P
Can't say I like the coding styles.
How so?
Nice, there's even last year material http://quasiconf.twoticketsplease.de/2012/
I know that "you have to use FUNCALL only when you want to call a function object", just like I know that when dealing with numbers you don't have to use + every time you mention them, only when you want to add them! I prefer (f x) to (funcall f x) just because it is shorter but, of course, having to write the longer (funcall f x) is not a problem if I'm forced to do it. As for making the intent clearer I'm not sure I agree, because (f x) seems to me to communicate calling f with argument x equally clearly. I don't think that FUNCTION and FUNCALL make code ugly in absolute terms, just uglier than the corresponding code in a language that doesn't need them. We can all have different subjective options about syntax, it's not a big deal.
That's mostly trivial. Yes, even there, I like to see that it is a fully dynamic lookup from a variable. (f x) in Common Lisp is something different from (funcall f x). The former is less dynamic. For example F can only be a function and it can be inlined. What about ((((foo x) ((bar y))))) ? Code which actually returns functions, which then get called. Nothing unusual in Scheme code.
If you do, please write about it!
Namespaces in Common Lisp: * functions and macros * variables * special variables * data types * label for go statements within a tagbody * a block name for return-from statements within a block * symbols inside a quoted expression 
Coincidentally, I was spending the evening reading through an old comp.lang.lisp thread, "Why is Scheme not a Lisp?".^1,2. It touches on quite a few of the things you talk about: * Lisp-1/Lisp-2 and how "hampering" and "ugly" FUNCALL and sharp-quote are * emacs-lisp's dynamic scope and how RMS felt about that a few years down the road * what Scheme and CL has in common * and more ("Hygenic macros solve a problem that CL does not have. (They solve a problem introduced by Lisp1-ness. Heh.) I see no need for these whatsoever.") I found the exchanges between Kent M. Pitman and William D. Clinger around the midpoint of the thread (and basically everything KMP writes in the thread (which later turns into into a flame-fest)) particularly interesting. 1. https://groups.google.com/forum/#!topic/comp.lang.lisp/Bj8Hx6mZEYI[1-25-false] 2. Message-ID: &lt;gat-1203021200230001@eglaptop.jpl.nasa.gov&gt;
That's not really constructive in and out of itself. What is it that you don't like, why don't you like it, and how would you improve it?
Another Knight in search of the Holy Grail.
Not only that, but he thinks this discussion is actually new or revolutionary. Tools created for directly programming graphically have existed for some time. And they failed. (UML being an example). Not without a reason. Expressing yourself with tokens and words is just natural. How one would represent a complex mathematical equation graphically? By using a huge tree? Why? Using mathematical exclusions is small and elegant.
Being recently fluent~ with paredit, I can say navigating/editing your code is a zen experience. And I can also imagine how transforming (refactoring) seems like a feasible day-to-day task instead of a dark magic that is often left to be implemented by large IDEs.
I recall seeing a video from the eighties or so about an "ide" that did pretty much this. It was for lisp code, and would store the actual ast instead of text files, and could pretty print it with various options for indentations and such. While editing, it would behave as normal text, but as soon as you were done editing a function, it would save it as an ast again.
[Nah](http://www.cryptonomicon.com/beginning.html) 
This may well have been Interlisp - specifically the Interlisp-D environment. I used this in the late 80s and the SEdit structure editor was just astonishingly good. it is depressing how often I read articles like this and realise how much has been forgotten about this environment -- so many of them seem just completely unaware of it. In particular the real lesson of SEdit should be two things: firstly that structure editors are fragile -- how do you deal with things that are not structure but not comments? how does conditionalised source work? -- secondly that the boring problems of being readable by a human turn out to matter -- there are good reasons for things like line lengths that are based human characteristics which can not be changed over timespans of between thousands and millions of years.
I upvoted this for interesting commentary, but I strongly disagree. I come from the Unix world, so I'm biased, but to me there are two big approaches in computer science: * Let's make a custom format with a bunch of tools that all operate on the custom format. Because it's a custom format things will be awesome. Nobody needs to use anything but our tools. * Let's use text. Having the rest of the world be able to interact with our stuff is a huge gain as opposed to the small inconvenience of having to parse text into an internal representation before our tools will work. I'm going to delete my wall of text and just say the second is better. You get source control, you can post your source to the web, you can use your favorite editor, blah blah blah... the idea of "we'll build fancy tools, and they'll be so awesome that no one will need to use any *other* tools on our data ever" is not right. You're welcome to prove me wrong and come up with a set of tools that's so compelling that the first possibility becomes a reality for a nontrivial set of people.
Structured documents: a classic Windmill of computer science.
This seems directly related to the efforts underway on this "projectional editor" project (github). The goals of the project are fairly ambitious, but maybe it is an idea whose time is near.
I don't think you read the post properly. She explicitly states she is not talking about visual programming.
I once heard a joke by an MS exec that was something along the lines of "Oh yeah, and then you will tell me we should be programming in xml". Well, we have been programming using an S expressions document structure for 50 years now. I doubt he ever heard of Lisp. I get the critiques, and I agree on the basis that you always end up in a text editor no matter how high up the abstract toolchain you get. However, sometimes it is good to dream. I am sure every programmer can visualize a graphical interface that allows you to drag common lisp functions from the hyperspec on to a canvas and link up function input / output as modules. Hell, special effects guys have been doing this in compositors and 3d software such as Houdini for over 20 years now. This isn't exactly a new concept, nor one that is a solution looking for a problem -- these tools are used all day every day. In the case of Houdini, you can do absolutely amazing things. And how far is any of that from what we do in common lisp? It's just forms all the way down. How hard is it to imagine not just a node based programming environment that is an adjunct to emacs (naturally, text won't ever go away), but an environment that lets you share groups and mega groups of these objects not just as programs, but as components and behaviours? We already have quicklisp, we have users creating things like Quickutil (I recognize it is kind of a replacement for other projects). How far can we imagine these grouping / regrouping of software bits into bigger and more comlex relationships? This isn't about the holy grail or the one ide to end all ides. I think what the author is talking about is dreaming of a world that is achievable -- we have had the technology at our fingertips for 50 years now! I posted the link because it appeared in /programming and not here for some reason (I am finding that quite a bit actually) but the critiques made me think about "what if". Of course, as someone alluded to below, show me the code.
&gt; approaches in computer science If you presuppose that existing frameworks are immutable, and that new ideas must be made compatible with existing frameworks, then you're not doing science at all. Copernicus could've never hypothesized that planets orbit the sun, for example, if he stuck rigidly to the common belief of his day. If we only ever take the engineering approach of optimization, we might never find out that better solutions exist - sometimes it's necessary to abandon normality and introduce something new. Sure this might not have immediate practical use, but when has science ever been about that?
I blame Alan Kay for my technology-angst. That's right that a lot of good things are forgotten, only to re-emerge half-done later. You say Interlisp-D was astonishing, but then say structural editing is fragile. Care to give some concrete examples ? I'm curious about the limits of structural edition. About the formatting bit, I've never seen large lisp code base, but I'm under the impression that the culture is about building generic abstractions leading to below average length. And you can always swap indentation styles on the fly to suit your needs. 
I've done a similar exercise a feww years ago, except that I went on and coded a universal turing machine with the lambda calculus. I'm sorry it's in French, but here is the [document](https://pablo.rauzy.name/files/lambdacalcul.pdf), and here are the [slides](https://pablo.rauzy.name/files/lambdacalcul-slides.pdf) (the slides should followable even for non French speakers).
No, almost 30 years ago in college, my friends and I had the same idea when we saw and played with the Xerox Altos. We got real excited and talked to a few professors. Of course we found out it was not an original idea but one professor was willing to support us. provided it was an interpreter, for pascal, written in pascal on a [perq](http://en.wikipedia.org/wiki/PERQ). The support was not money or a job but access to one perq (pascal version of a lisp machine). At which point we basically lost interest and moved on to discussing graphical versions of [text games](http://en.wikipedia.org/wiki/Classic_Empire_\(computer_game\)) which we never did either. So it was sort of a self-effacing remark. edit: I would never say it is impossible. Especially since as a Math grad student, I and others used to make fun of those trying to prove Fermat's Last Theorem, another Holy Grail. Well, we now know how that turned out.
True, but it is not clear what the author **is** talking about. The article tries to make a case for something else than text files, enumerating various advantages, but we don't learn much about the actual suggested alternative solution.
&gt; save it as an ast what happens to linebreaks and comments, do they get saved somehow?
Maybe a dialect of Lisp fit for this kind of thing would need to consider comments and linebreaks as part of code structure, but not indentation. For example comments could be like REM in DOS. Compilation should throw away comments and linebreaks, but saving the code to a file should keep those comments and linebreaks, and when we print a macro expansion, we would see a multiline code with comments.
I'm not convinced about the comment issue. Let the reader read, and allow a relation between a s-exp and a comment object. For the automatic layout I've seen both monstruous and also very sensible (but mostly for ml like languages)
This is easy. One of three things: 1) Add a comment node, drag output of comment node to input of addendum node on the node representing a form or set of forms. 2) Open up a comment hiarchical component of the node that let's you type in comments. 3) Select EDIT NODE (as opposed EDIT MODULE which would show you the code for the whole module not just the code for the form), the code pops up in emacs and you edit and save. This isn't the kind of problem you would get in a node based editing environment. One problem that you would run into is that we would actually have to spend time working on tools that thelp you organize nodes into something sensible and logical because the interdependencies between nodes get VERY VERY messy very quickly. Although, this is the case with text based environments anyway, the only difference is really really good programmers hold the relationships in their head. Here is an example of the kind of AMAZING power you give the user, as well as the amazing amount of rope to hang them selves. Watch the video not just for how they setup the compositing solution to be flexible for future edits, but how he organizes the code to make logical sense so that he can get back to it later and edit: http://www.youtube.com/watch?v=sTE4r_yGwc8 This is Blender for those that don't know, free and open source: www.blender.org. Houdinis SOP based version of this is MUCH MUCH more powerful and you can program pretty well any 3d based operation in it, including behaviours and simulations. Whenever I am working in lisp in my head I am drawing out how this would work as a node based implementation. We are not that far away from it, though, I cannot say it would be useful until we try it.
sure I can imagine a UI, the question is: how and where do we attach it to a Lisp expression. Preferably in a simple and obvious way.
How do we attach comments to an Lisp expression? (list 'defun 'foo '(bar (comment bar is a number)) '(+ bar 1)) But the CL syntax does not accept a comment list at that place.
I was thinking 'sibling' cons cells, as in html text elements, that would then be filtered before editing-time evaluation (or be entirely removed for a non-editing eval/run/compile) (list 'defun 'foo (comment foo ...) 'bar (comment bar is a number) '(+ bar 1)) (defun ed-eval (exp env) (eval (remove-if #'commentp exp) env) Is it unrealistic ?
Hmm. Let's try this: * I am "assuming" that each node would represent a single form. * Forms can be nested to represent complexity. * A GROUP NODE of the multiform expression would be collapsable and expandable as a group. Any form that contains subforms would automatically become a GROUP node and display collapse/expand functionality. * A GROUP NODE that is expanded as a group would display each of the nodes representing a single form. This can be done in a single expansion of the whole structure or have a tree view style expansion that lets you dig into a complex set of forms so that you don't have to deal with a messy screen. So this would give us: 1) If you wanted to attach a header comment, you attach a node called COMMENT, input the comment, drag the output node to the COMMENT input node for the GROUP NODE. 2) If you wanted to attach a comment to any of the forms, either expand a treeview style representation or a node based representation (there are other node browsing options beyond that) and either attach a COMMENT node to the input node called COMMENT on the FORM node, or click on the COMMENT label on each form that allows you to type the comment into the node via dialog box. In the first example, the comments are appended to the beginning of the code page. The code group node represents a single 'ascii file'. The in-node comments would allow the addition of comment per form. The form would be very strongly organized by a predefined template as is done in Lisp Jr. tutorial (http://alarm.cti.depaul.edu/lisptutor/login). We can expand this to DSLs as all DSLs in Lisp are written as s expression forms as well. I could do some mockups.
That's UI for a GUI-based editor. We were talking about a structure editor for Lisp. 
That seems like a useless conversation in the context of Lisp. Eveything is an s expression. The structure is there whether anyone wants it or not so there is nothing to invent here. Therefore, you are talking about a user interface issue anyway. Whether it is an x app or a terminal based app is irrelevant as it is a solved problem via emacs anyway.
Well, I was probably confused/confusing, if only because all of this is at least 22 years ago now. First of all the whole Interlisp-D environment was fragile and strange in a way that is now hard to believe. The typical response of a machine to an inexperienced user would be to wedge somewhere in microcode within a few seconds (the machine would freeze, and depending on the hardware you'd either get a little code from the LEDs or the mouse cursor would turn into an equivalent code, which almost always meant "I don't know what happened but I can't go on from here"). If you were lucky you could resume the image, if you weren't you'd have to reload the sysout (possibly after booting into the low-level debugger to try and rescue stuff from the sysout which had just expired which sometimes you could do) which took a long time. Somehow after a few days you learned not do do the things which killed it like that (or, more likely, the machine became accustomed to you) and the machines became surprisingly robust. But even then you have to realise that they were (when I used them) Interlisp machines with a fairly thin veneer of CL on top, and quite often the Interlisp substrate showed through. And the Interlisp environment was, quite seriously, something that came from the wee folk: it was this fay, strange thing where nothing was quite what it seemed and weeks would vanish without you noticing. There are people at PARC who claim to have written it but we know they just borrowed it from the Sidhe. To give an impression of this, of the machines we had, one was about 10% faster than the others. The hardware was identical, but somehow it had some additional magic. We never found out what it was, but it ran faster when the moon was new, so we had our suspicions. But what I really wanted to talk about was SEdit. This was the structure editor (there was an older one called DEdit which was a much lesser thing), and the thing that was good about it was that most of the time *you didn't know you were using it*. In particular Sedit understood that programmers can type, and what they don't need is some kind of "now pick the form you want from a menu" interface but something that let you type code but not have to worry about the trivia like keeping parens balanced and indentation and so on. Mostly SEdit was just invisible: it just did what you wanted, saved you from boring mistakes, and stayed out of the way. That's what I mean by being "astonishing". I think the best modern-day analogy would be these systems that let you type maths in a "structured" way: they are probably great for people who don't type much maths, but if you do they are *awful* because you spend your whole time picking things from stupid menus at 4% of the speed you could just type the thing in TeX. So, of course, everyone serious just types TeX and these systems never get any better because no-one serious uses them. It may be there is a SEdit-equivalent for typing maths: something that keeps out of the way but that avoids the boring mistakes, but I haven't seen one (don't tell me if there is, I am happy with TeX the way I am happy with Unix now). Now, fragility of SEdit. As well as just plain bugs, there were fairly major issues about things like this: (defun foo () #+Xerox (il:... ...) #+Lispworks (hcl: ...) ...) There just wasn't any real way to deal with that in SEdit. Perhaps that could have been added, but then you run into this sort of thing (defun bar () #[for (i = 1; i &lt; 10; i++) ... ]) what is it meant to do with that? Even if it knows the readmacro what kind of structure does it assign to the thing it introduces? Are you going to have to write some complicated thing that tells SEdit what to do here? I don't even know if you could. A text editor doesn't have any of these problems because it knows what do do with all this special syntax: nothing. And it turns out, I think, that a smart text editor with "structure aware" things in it, is a pretty good compromise (remember that SEdit was written in an environment where "text" did not really make sense, and probably either before or in parallel with the first text editors which did this well, which probably were EINE and its derivatives on the MIT-derived systems -- this kind of support was rudimentary in GNU Emacs until much later.
I think the general idea is is, instead of our current pipeline of write code -&gt; save to file -&gt; parse -&gt; ... we change to write code -&gt; parse -&gt; save to file -&gt; ... The reason being, all tools work on the AST, and eventually all need to re-implement the parser, unless a public API is available to do so already. Even where one is available, each tool must perform the parse over again, which is inefficient. If instead, parsing was done once and all tools worked on the AST directly, it would be more efficient, simpler to write all kinds of tooling, code would be indexable, normalized, queryable. The advantages are many, but the gist is that we move on from code being "some characters in a text file" to being structural data in a database which can be semantically linked to other code, and even non-code, like documentation, references, annotations and such. From the POV of a programmer, nothing need change, other than using a supporting editor, and throwing away "manual code formatting", leaving it up to a pretty printer (although the pretty printers may be parametrized to produce the preferred style for the programmer.) The latter part also has benefits for version control, as simple formatting changes will not exist. A version control system would be able to work directly on the code semantics, so rather than "some characters on this line changed", it could say "this function was renamed from x to y".
Line breaks wouldn't be saved. That's why it had a really good pretty printer, so you could view the code with whatever indentation/newline style you preferred. I'm no sure what it did about comments, but perhaps as some lisps, where the comment for a function is simply the first expression of the function definition. http://www.gnu.org/software/emacs/manual/html_node/eintr/defun.html
https://groups.google.com/forum/#!forum/qilang is the right place for this question.
Are you looking for this post? http://www.reddit.com/r/lisp/comments/s0pez/shen_a_lisp_with_currying_lazy_evaluation_partial/ If not, search finds only 7 posts about Shen, so it should be easy to find the one you're looking for: http://www.reddit.com/r/lisp/search?q=shen&amp;restrict_sr=on
Look, sure, knock yourself out if you think it's a good idea. I even like using weird stuff no one else uses because I think it's better. But I think you're going to find that source control and editors and source-aware debuggers and etc etc etc are nice. My humble opinion is that your energy and creativity would be better invested in an idea that made more sense in the first place. Or, to put it another way: If you presuppose that proven approaches are without merit, and that old ideas can be discarded without some careful consideration, then you're not doing engineering at all. None of the technology around you would have gotten anywhere without relying heavily on old common beliefs. If we only ever take the engineering approach of reinventing wheels by replacing them with hexagons with offset axles, we're going to be bumping around and burning out out motors way more than is necessary - sometimes it's necessary to preserve the things that work and take the thought and insight needed to figure out what *actually* needs replacing. Sure it might be fun to try a new idea, and for all I know it could be genuinely neat and useful, but when has engineering ever been about abandoning an effective idea for a neat but impractical one? See? I can generalize to infinity too. :-) If you think it's cool knock yourself out. I'm just offering my input.
I was being more pedantic about your incorrect use of the term computer science, when you really meant engineering. While existing ideas have their merits, they're not without flaws either. The idea isn't to abandon all existing things, but to consider whether the benefits of a different approach outweigh the downsides of the existing one. If you consider before we had relational data stores, we would typically store all data in plain text or binary files, and each application needing to store structured data would need to implement it's own indexing and search capabilities, or use libraries which do so. Particular effort would be required to keep data consistent and such. Codd's introduction of the relational model threw away the idea of sequential text or custom structured binary files for data, and introduced an abstraction over data that could be indexed, searched, normalized, and redundancy eliminated. There's little doubt that RDBMS are successful, the benefits far outweigh the downside of not having human readable text files. We can query over data to produce the most relevant human readable format instead. Code is also structured data. If we ignore this fact and continue to believe that sequential text files are the ideal storage mechanism, we might never realize the advantages of storing it relationally. Consider what the world would be like today if relational databases hadn't existed for the past 40 years. I get that we can produce this structured data for code with a parser, but what if, instead of storing data in a DBMS, we stored everything in plain text files, and reproduced the entire structured database on demand for every tool that needed to query the data. Silly right? It's my opinion that it's silly we do exactly that for code.
Sure, there are revolutionary ideas in computer science. I just think that storing source code as AST blobs isn't one of them. Look, man, if you're excited then do it; knock yourself out. The worst that'll happen is that you'll learn a bunch of things.
Have you ever done real programming in one of these "drag the little nodes around" environments? I have (two of them in fact). It was a huge pain in the ass in both. I can't even find the words to describe how unpleasant it was. In contrast, I find coordinating and sharing functionality between text files to be natural and straightforward. I mean, sure, it could be done well maybe. Certainly it can be done better than the environments I worked in :-). But actually working in one of those environments and trying to get real work done is an education anyone who wants to talk about this would benefit from. Or maybe the solution is for lisp to take a look at the state of the art in making text files coordinate effectively, and emulate *that*? What you talk about above - sharing and coordinating large collections of functionality - is a difficult problem in any language, but there are pretty effective solutions out there. Maybe lisp would benefit from emulating the state of the art and trying to improve it. I honestly don't see how storing source as AST blobs helps with that at all.
For 3d / compositing work, yes, I used to do it for a living. The productivity was amazing. *EDIT* I originally posted in this sentence that via code it would not have been as effective or as fast. But, as I am learning more lisp, perhaps I am wrong. Perhaps a DSL based text interface could have been faster and perhaps even better in some ways. I would like to learn more about your experience. Obviously, the node based approach works really well in the above scenarios, I have no idea how well it would work for programming. I am only trying to imagine what it would mean for programming. What environments did you use? What languages? 
Do you have more information on SEdit? I can't even find a screenshot of it; the most I can find is random people referring to having used it. How did it compare to, say, paredit? (Or were they so different as to be uncomparable - I'm afraid I don't really get it from the descriptions.)
Very cool. Thanks for taking the time to share your experience. This feels like one of these situations where experienced programmers seem to always say "told you so" and they are usually right. My only caveat here is that I would agree with you whole hartedly if it was anything by a Lisp. The Pythons and C++'s of the world, I cannot even wrap my head around that. In Lisp, I am basically drawing a node tree in s expression syntax just by nesting forms. It just feels bloody familiar. But, I am not a programmer, so it is only brain fiction. Thanks again. If anyone else has worked in anything like ML, please speak up and share. I would love to hear how you found the experience.
No, I'm afraid I don't, and my memories of it may well be unreliable. The whole Interlisp-D environment is a lost technology I think (it was also in almost all ways a *bad* technology, at least in my memory).
Not quite what you asked, but... you can easily for hack the build to link your deps statically. Take a look at: src/runtime/GNUMakefile src/runtime/Config.&lt;arch&gt;-&lt;os&gt; Possibly even just having stuff in an appropriate environment variable might do it, but IIRC we override most of the environment flags pretty cavalierly. A patch to allow static linking more stuff to the runtime via a make.sh flag or an environment variable would not be amiss... Adding more stuff after the build is going to be considerably trickier. You'd have to figure out how to relink the runtime without moving anything already there -- or change the lisp land to resolve all foreign addresses via dlsym(), which for eg. allocation traps might imply a performance cost not everyone is going to be happy about. 
Shen looks really interesting. I love the it's a lisp you can take anywhere. Too bad I needed continuations otherwise I might have been using it. 
Oh wow, cool. I had no idea about Skewer, I have been using Conkeror and telling it to reload the page at intervals. Which works okay. I will give Trident a test, looks terrific. This should help when playing around with hh-web.
Thanks for the comment. I keep meaning to try Conkeror but haven't gotten around to it yet. Are you using Parenscript with hh-web currently?
Creator of Skewer here. This is a really cool project, and it was very easy to get it running. I think it's mature enough that you should get this listed in MELPA. It's a fairly painless process. Your README is missing the step about connecting Skewer to a browser (either with `run-skewer` or by injecting Skewer into an existing page) in addition to connecting SLIME. I obviously knew to do this, but I think non-Skewer users would be lost. As a user of the Skewer API, I'd be happy to hear any comments you may have about it. I initially made Skewer without expecting people to build other tools on top of it, so it grew organically to fit these user's needs, making it a little clunkier than it probably could be. 
Only tiny test here and there. Not enough time to really say I have done anything serious with it.
Have you thought about integrating with jscl?
Well there's [this thread](https://groups.google.com/forum/#!topic/qilang/gl_nOroDjZQ) at qilang. That was kind of a watershed moment for me. Anyone seriously interested in Shen should first soak in the pure distilled cluelessness from that thread before considering Shen for doing actual real work.
In a quick scanning through the book, I didn't find any details on Pollen itself. Is there a page somewhere? This combines so many things I find awesome.
I'm not very familiar with JSCL but it looks like a cool project. I'll check it out and see what the opportunities for integration are. I'm saying this from a position of ignorance, but off-hand it seems likely we could simply use Skewer to send Lisp to JSCL for compilation (as compared to the current arrangement of compiling on the client side with Parenscript and sending the result).
There is a mention in the colophon, that is all I could find at the moment.
I used to work for this fellow in my Red Hat days in San Francisco. Good to see he's done quite well.
"Book-publishing system" could mean just that it converts a common source to a variety of output formats, like [pandoc](http://johnmacfarlane.net/pandoc/). 
He's totally wrong, too, isn't he? The point of pretty-printing listings is to make them pretty. It's OK to change the quotes to curly quotes, to change "&gt;=" to a &amp;ge;' the word 'lambda' to the Greek letter, an ASCII arrow '-&gt;' to an actual arrow, etc. EDIT: I did read carefully: he mentions documentation where I guess you shouldn't pretty anything up. For papers I still prefer the pretty versions. EDIT2: I took a glance at the documentation for the listings package you mentioned and it's quotes are definitely curly in the examples shown (it seems to use closing curly quotes for both the opening and closing quote in Pascal strings, for example).
&gt; The point of pretty-printing listings is to make them pretty I'm not sure what that means. When I include listings in a document, I include the code as-is. I think of pretty-printing as, say, reformatting HTML to satisfy some indentation pattern. &gt; it's quotes are definitely curly in the examples shown Ah, I went back and looked closer (sorry) and saw the reference to the [*upquote*](http://ctan.org/pkg/upquote) package. Thanks for the comment.
Many people don't include the code as is for listings. They'll use programs that do this like change "&gt;=" to "&amp;ge;", "-&gt;" to "&amp;rarr;", etc. Lisp programmer's will often change "lambda"'s to "&amp;lambda;" even if their lisp compiler wouldn't let them. Sometimes people even change any mathematical formulas in their code to be typeset as actual TeX math, so that "`math.sqrt(x**2 + y**2)`" appears with exponents and a square root sign.
I don't recommend you read any Haskell papers, then, I think they all at least pretty up arrows and lambdas.
Lisp, then, is a language -- or medium -- of play, prototyping, and exploration, whereas Java and the like are languages for implementation. Creative writing, like that pursued by Richard Gabriel (author of this article) resembles the former more than the later. Gabriel writes with a tone of wistfulness. Sure, we all know why, too. Java here, Python there, and Common Lisp seemingly nowhere. Forgotten. (Maybe Racket will change this.) Why Lisp? Why programming mediums that foster exploration? Gabriel quotes CAR Hoare's answer, "Premature optimization is the root of all evil in programming." In my experience, the demand for a finished product precedes understanding of what's required and what's involved. "Build the system, fast, fast, then we'll see what it can do", I can hear a former boss of mine insisting. Crapware results. 
[fixed](https://github.com/maryrosecook/littlelisp/issues/5)
I've got no hate for pandoc. Rather, my point was that "book-publishiing system" is pretty vague, and could mean something very interesting or something that is not. 
No, that's a good point. This could be some sort of elegant new multi-format typesetting engine with a cool domain specific language (e.g. [Lilypond for music](http://www.lilypond.org/)), or it could just be a reimplementation of pandoc in racket. **EDIT**: Lilypond is also an amazing bit of software powered by (gambit) scheme: [http://www.lilypond.org/essay.html](http://www.lilypond.org/essay.html)
.. or a script that reads some config information and issues a system call to pandoc. The phrase is vague.
&gt; Lisp programmer's will often change "lambda"'s to "λ" even if their lisp compiler wouldn't let them. I don't recall ever having seen a Lisp implementation that wouldn't allow you to do just that. Lambda sort of isn't on the list of prohibited characters in Lisp identifiers.
&gt; programming is a creative process, and thus, exploratory by its nature So right. I started messing around scripting in Blender's game engine today, and it's been the most fun I've had with programming in a while. That fact that it's in Python helps.
There's a lot in lisp culture that has been enabled by a certain set of features : small core, the generic list structure lifting up the specific function when needed, emphasis on recursion with closure helping permutations in the design. All this guides you through exploration with focus and low friction. ps: as far as my own programming goes, I was first taught java/uml/emf but the more I dive into the lisp / ml world the more I see how less time is wasted using them. Very sharp knife.
Common Lisp is still well used - my startup uses it for everything and I know a lot of people who program in Common Lisp every day. I will say that my personal experience is that Lisp's flexibility creates problems for large projects where uniformity and organization are critical, perhaps even more critical than ease of prototyping and rapid flexibility. Everyone on my team has fairly different styles. Part of the reason for this is that Common Lisp lets you _almost_ do anything, so people with a long history of programming in it develop very particular habits. This is less true of Java, and for certain kinds of projects, that uniformity of style is a major win. 
Pollen is an adaptation of the Scribble dialect of Racket, which allows textual markup to include arbitrary code. So conceptually it's probably closer to TeX than Pandoc (based on my limited understanding of those tools — I don't use them). There are two types of files in the system: Pollen source files and Pollen preprocessor files. Pollen source files are really the "source code" for the book. They contain the text and formatting codes. But unlike, say, Markdown, where you can only use the tags supported by Markdown, Pollen lets me create whatever tags I want, ascribe whatever behavior I want, and worry about the conversion later. This makes the source code minimal &amp; readable. For instance, I wanted to be able to write a cross-reference like this: ◊xref{font recommendations} So I just did it, and then wrote a Racket function that turns this markup into &lt;a href="font-recommendations.html" class="xref"&gt;font recommendations&lt;/a&gt; When Pollen source is parsed, it also includes a "decode" step like Scribble that automatically handles boring nonsense like &lt;br /&gt; and &lt;p&gt;...&lt;/p&gt; tags. But unlike Scribble, which has its own special format, Pollen just decodes to XML (using x-expressions). Pollen Preprocessor is for things like HTML templates and CSS files that can benefit from automation, but don't need to be parsed to the same degree. But I can use all the same syntax and function libraries. Racket was the ideal tool for the job because of a) the ease of representing &amp; processing XMLish data structures as x-expresssions and b) the Scribble dialect, which provided a ready-made environment for text-based documents. (I built the first version of Pollen in Python — it was awful.) The Racket programmers even added a few core language features I suggested. So — great language, nice people, no complaints. Check it out at racket-lang.org. When I've smoothed out some of the rough edges, I plan to put Pollen up on the Racket package server or Github. Update May 2014: Pollen is on Github at http://github.com/mbutterick/pollen &amp; the Racket package server. 
I meant older implementations that required identifiers to be ASCII.
What is your startup if you don't mind my asking?
Wow, thank you for that explanation. An impressive bit of work! I've only glanced passingly at scribble. Perhaps it's time I took another look
I think that's an important point. Lisp's flexibility can create problems. It's really important that a group of Lisp developers commits to a style guide. In Lisp you have a lot of ways to write a software architecture. In the Java world in large problems you'll see the same, but in a different way. At first approximation there is only one style to write a program in: OOP. If you look closer, the same architecture problems can be solved in an OO style in very different ways. Code bases can be incompatible with different frameworks (say, Spring) and different architectures which are supported by those.
Well, is it really so terrible? I think the the fact that Shen uses the same nomenclature of "packages" and "externality" to talk about an entirely different kind of module system. From what I gather reading everything everyone posted, in Shen "external" symbols are not meant at all to be used like external symbols in Common Lisp. In CL, these are simply symbols which can be imported or used in other packages, and which can be accessed with ":" rather than "::". In Shen an "external symbol" might be better called a "system symbol" or a "global symbol". There is no notion in the language of `use`ing or importing symbols from other packages - one always refers to symbols in other packages with a qualified reference. So-called "external" symbols are those which the library creator believes should be globally accessible. As Mark Tarver indicates in several places, this usage only makes sense for people implementing parts of the base language. Shen library developers should never create such external symbols. It is a bit inconvenient to have to always refer to library symbols via a package qualified version, but there are worse library systems out there (see the way people use objects as libraries in Javascript, for instance). And Shen is a lisp - it is trivial to write up `using` forms in the language, I'd imagine. I really think the negative reaction has a lot more to do with Shen being different from Common Lisp (and using similar nomenclature) than with it being objectively a bad way of doing things.
It is true that one can program in a variety of ways in Java, but the language, and (importantly) the culture doesn't encourage a lot of creativity. Lisp, on the other hand, specifically attracts more imaginative, whimsical and opinionated programmers and it gives them the freedom to imprint the language with their idiosyncrasies. It is a tough combination in some situations. 
I found that true for average programmers. But architects (or those who think they are) tend to be very creative in Java, too. I had a guy in one project who was working for the customer (thus had political backing). He was redeveloping the persistence framework (based on his 'novel' ideas) without project mandate and managed to get it into the project (because of the mentioned political support. 
There is a bit-rotted Eclipse plugin. I would be willing to help out if someone was willing to take the lead working on it and maintaining it.
That would be suweet
It's very, very bad for those four reasons. And because the designer of Shen doesn't seem to understand the issue, there's no sign that it will get better.
As someone who uses intellij at work, hasn't looked into developing eclipse plugins at all, and has never used said plugin, I'm interested in helping out. What improvements would need to be made on it?
That's called [engineering notation](https://en.wikipedia.org/wiki/Engineering_notation), in case you didn't know. My googling did not turn up anything helpful, though.
No idea, sir. I don't know foo about Java and therefore have very little capability for easily hacking on it, I just know it exists and doesn't work well. See https://bitbucket.org/skolos/lispdev/src for the most recent incarnation. It was once known as CUSP. i've forked it onto my bitbucket, just to make sure it stays around. If you're willing to take the lead, I'm willing to help out with bugs and so forth, and develop with Eclipse (shudder) for a while.
I can't see how to do that. That's an annoying and surprising deficiency if it really can't: format can print Roman numerals, it should be able to print a very common notation like this.
I'd argue that if you're reading a scientific paper, you care about the algorithm and not the implementation, unless the language is relevant... so pretty that bitch up!
 (format t "~,3E" 123456789) 1.235e+8 Is the best I could come up with. The "only exponents which are multiple of three" part is hard with only format :|
Thanks! Of course implementing it is not so hard, but I was sure Format could do it, since it is famously over-engineered. I guess not!
I actually think something like sublime text might be a better target than eclipse, but since I don't use it, I haven't had any motivation(other than wanting to teach lisp to friends) to do something about it. 
tried (defmethod print-object ((n number) s) (let ((digit (1+ (mod (1- (ceiling (log n 10))) 3)))) (format s "~sE~s" (ceiling (* n (expt 10 (- digit)))) digit))) and failed. I looked into the source code of prin1 and found that sbcl does not use print-object for classes such as numbers, list, cons (maybe all classes which are subclasses of built-in-class. see sbcl/src/code/print.lisp) It might work on the other implementation, not sure though. If it works, then you can write (format t "~A" num) to achieve what you want
Stream it!
I believe I watched your 2-man team stream last year (even went to some irc channel). Even though it was a bit too small to really follow, I loved to see the interaction and the rhythm of problem solving. I hope you can do it again this year, with a livestream too. Good luck
Check your PM's. I'm interested and sent a note!
It will be, but we are doing it via Imapatient-Mode rather than video chat. This means that we will publish a web-page that has our collaborative buffers mirrored in it (and they will be regular text, so you may copy them easily and play along). Also, no resolution issues. Further, you may follow the dialog in IRC.
So defmacro hadn't been introduced to he language yet?
Yeah, not a lot of info coming from the organizers. I would bet that there will be a lightning round, but that is too quick for me and any team that I have been on. I have never gotten something together so fast.
maclisp had fexprs and macros. (defun &lt;name&gt; macro (&lt;single-parameter&gt;) &lt;body&gt;) which would roughly be equivalent to (defmacro &lt;name&gt; (&amp;whole &lt;single-parameter&gt;) &lt;body&gt;) 
To all downvoters, you are so beta.
Maclisp also had DEFMACRO.
Could be. Though Winston &amp; Horn does not mention this. I never used these much at the time. I remember having a problem with a function and a friend showing how do it with a fexpr. In fact I was only gonna mention fexprs, but I checked Winston &amp; Horn and they covered these macros.
Macros were introduced in 1963 Timothy P. Hart, MACRO Definitions for LISP ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-057.pdf 
Matthew, am pleased to hear about this development and find it quite interesting. I hope you do put the code up on Github as (personally) I'd like to see about integrating some aspects of legal_markdown with Pollen. Thanks for the consistently great work. Learned tons from TFL. 
;; Fire up Emacs. ;; ;; Hit the `q' key to dismiss the welcome message. ;; I feel sooo stupid now! I would hit C-x b to switch to *scratch* for the last fifteen years....
GNU Emacs says: use c-l
Step one: Invent time machine
This looks amazing.
You can also set some flag to prevent the welcome message from coming up.
Context: [http://abstrusegoose.com/249](http://abstrusegoose.com/249)
 (setq inhibit-splash-screen t)
And if you're anal like me you can get rid of the useless text appearing in your scratch buffer with `(setq initial-scratch-message nil)`
While I understand why Bastien used the scratch buffer and lisp interaction mode for his tutorial a better way to play with elisp interactively is to use IELM mode (interactive elisp mode). Then you get a decent elisp REPL like a civilized person. `M-x ielm`
If there's one thing to be aware about *scratch* is that it's not saved on exit.
[How to Make Emacs' Scratch Buffer Persistent Across Sessions](http://dorophone.blogspot.com.br/2011/11/how-to-make-emacs-scratch-buffer.html)
I remember reading it a while ago, I didn't use elisp, now I understands its content ... Thanks
Scheme is a language built around list processing (a child of LISP). Lists in Scheme and LISP are represented with parentheses. A function is basically an evaluated list where the first element in the list is the function to call and the rest of the elements are the arguments to the function : (function arguments...) In your example, you call the function **define** at the beginning with 2 arguments in this case : the function signature and the body of the function. You also used the same syntax to define the body of the function : **(* x x)**, where * is the function, and *x x* are the 2 arguments. Calling your newly created function is basically the same thing, you want to use the function **square** with argument *2*, so you need to write : (square 2) By the way, a better practice in Scheme is to use lambda expressions when creating functions. The main written difference when creating functions is that the first argument to define is only the name of the argument, and the body of the function is a lambda expression : (define square (lambda (x) (* x x)) **lambda** is a function that creates an anonymous function. The first argument is a list of the arguments to the function and the second argument is the body of the function. The major difference is how the function will be processed, and what you can do with it later. A defined function cannot be modified, but a lambda expression can, which is why lambda expression are preferred in most cases.
Starcraft 3?
http://en.wikipedia.org/wiki/SuperCollider In the video CCL runs a Lisp interface to it.
Done :) https://github.com/ralph-moeritz/trivial-tco
That did it. Thanks!
Hey great, insightful reply. Thank you!! The Scheme community is very helpful.
Nice explanation. Thanks! 
Not listed: mocl does optimize tail recursive calls
This is awesome; I'm following along and it's great. This has answered a bunch of question and a whole bunch more have come up: /u/xach's mention of a "reader" is interesting and I'd like to look more into that. A proper memory format for the objects would be interesting to see: I've made a few changes to your definitions to fit my C style (and some that I'm surprised that have worked for you: your `cons` creates data on the stack; it may be overwritten at any time!). A question I've always had: When does a Lisp compiler/interpreter become metacircular? I mean, sure, I can copy a definition of 'eval' from somewhere and run it inside my other lisp, but why would I do that if I already have a lisp? Wouldn't it be possible to define list like this: `(define (list . contents) contents)`? Thank you for posting this!
Fantastic! I will be sure to employ this in my libraries, where I heavily use TCO. Thanks! (P.S., Request addition to Quicklisp if you haven't yet.)
Here are a few reasons why 3d tools and software development tools aren't more alike: In 3d tools, it is really useful to observe, interactively, the response of the image to the edits you make. In software development tools, we mostly ignore the machine code and wouldn't normally want to observe how it changes as we edit the source code. And a lot of the utility of a 3d tool comes from actions like tumbling around and manipulating numerical parameters and selecting/placing objects based on scene location. Those actions have no obvious analogue in code editing that I can see. However, I do think that Unicode use will continue to grow and I expect to see more projects like Fortress that involve typesetting for better readability. I wonder if we won't stick with text files as a powerful abstraction but make more use of typesetting since more time is spent reading code than writing it. Perhaps program editing will evolve toward TeX editing? You can already find literate programs written using TeX that include typeset mathematical commentary.
Thanks, but you might want to peer review it &amp; try it out to confirm it works before you give too much praise :) I've actually submitted it as a utility to quickutil because I think it makes more sense as a utility than a standalone project. What do you think?
You give the example (tco:with-tail-call-optimization () (fib 40)) where `fib` is an already-compiled function. How is TCO going to happen? It's easy to verify TCO with a test (say, sum a million 1s with a supposedly-TCO function to see if the stack explodes or not).
Good point! I'll amend the example.
What do you mean amend the example? TCO is a property of the compiler state. `with-tail-call-optimization` generates code to set the state, which is too late. It's more complex than what you have. You need at least one test. Also set `compiler:tail-call-self-merge-switch` for Allegro CL.
I get it: You can't perform TCO on code that's already been compiled. I'll amend the example code to use a lambda form instead of a named function. Thanks for the heads up on compiler:tail-call-self-merge-switch, I must have missed it. While I agree that having tests could be useful, I can't for the life of me find an example that actually causes a stack overflow (on CCL). Take the following code for example: (labels ((sum-aux (acc x) (if (zerop x) acc (sum-aux (+ acc x) (- x 1)))) (sum (n) (sum-aux 0 n))) (sum 1000000)) With tail-call optimizations disabled this should generate a stack overflow. However, if you eval the following on CCL you'll notice that TCO is still performed: (let ((saved-policy (ccl:current-compiler-policy)) (new-policy (ccl:new-compiler-policy :allow-tail-recursion-elimination (lambda () nil)))) (unwind-protect (progn (ccl:set-current-compiler-policy new-policy) (labels ((sum-aux (acc x) (if (zerop x) acc (sum-aux (+ acc x) (- x 1)))) (sum (n) (sum-aux 0 n))) (sum 1000000))) (ccl:set-current-compiler-policy saved-policy))) 
Sounds good!
Ok, *now* I get it. Thank you! :)
The definition of quasiquote needs an (if (pair? (car x))) before the (caar x); otherwise it doesn't work. I've defined `nil` in a way that has made me giggle: `nil = { objecttype_nil, .pair = { &amp;nil, &amp;nil } }`. Then, the definition of `car` doesn't need to special-case `nil`, haha! Also, `eq` can be replaced by `(memcmp(a, b, sizeof(atom)) == 0)`.
I've updated the code to set the compiler policy at the time `with-tail-call-optimization` is compiled, rather than when the generated code is evaluated - for CCL and Allegro. For SBCL &amp; Lispworks, where optimization declarations determine whether TCO is enabled or not, I've left the declarations in the generated code. Does that sound right? Anyway, please take a look at the code to see whether it looks right to you or if I need to be hit with the clue bat again ;)
Ok, for Allegro I now have: (defmacro with-tail-call-optimization (&amp;body body) (let ((non-self compiler:tail-call-non-self-merge-switch) (self compiler:tail-call-self-merge-switch)) (setf compiler:tail-call-non-self-merge-switch t) (setf compiler:tail-call-self-merge-switch t) `(unwind-protect (progn ,@body) (progn (setf compiler:tail-call-non-self-merge-switch ,non-self) (setf compiler:tail-call-self-merge-switch ,self))))) Does that seem ok to you?
I don't think it makes sense for a macro function to have side effects like that, especially if you try to undo those effects in the generated form. You have LOAD changing the compiler state, which seems wrong. I'm not sure there is a way to do this. It's relatively rare for the compiler to signal an error (most "errors" are actually warnings), so the code I gave isn't as bad as it looks. In practice I would probably just assert TCO inside `eval-when (:compile-toplevel)` and move on.
Love the sites background img.
[mozartreina.com/img/futura2.jpg](http://mozartreina.com/img/futura2.jpg) edit: [a little better resolution](http://mozartreina.com/img/futura.jpg)
IMHO Object Oriented Programming in Common Lisp is a very nice introduction to CLOS and would be a very good book to absorb immediately after PAIP. The Art of the Meta Object Protocol otoh, is seriously deep magic. It is a must-read, but I think its reading should follow Keene by a large amount of time. 
Thanks for my new wallpaper.
[Quicklisp](http://www.quicklisp.org/beta/).
&gt; what do most people use for web programming nowadays? I'm using restas.
Clozure CL was ported to ARM/Linux. Clozure CL got a port of its IDE to Windows. Clozure CL for Mac can be downloaded from Apple's official Mac App Store.
Quicklisp is new, and quite important as it is a distribution of working libraries, which is different from a tarball full of systems like I used to have. CCL on ARM is a great thing as well, the Raspberry PI + CCL is primarily what I do in my 'spare' time. local package aliases on SBCL, that is quite new and I like where we are going with that. For web, I am using hunchentoot for the server, and my own yasexml for markup, javascript as-is ('#:|usually in a symbol to avoid quoting "'s|) and my own continuation monad (http://drewc.org/interface/monads.html#sec-7). 
[SBCL Windows Fork](https://github.com/akovalenko/sbcl-win32-threads/wiki)
I dunno about most people, but I've used [clack](http://clacklisp.org/) now and again, it's nice and easy to work with.
oh and closer-mop has seen a lot of work
John Carmack recently said he was writing lisp on his iPad, I'm not sure if this is the program he's using.
I am somewhat in disagreement over your AMOP remark. When I read it (the first time) I was surprised to learn how simple CLOS (more precisely, Closette, a CLOS-like system sans a few features and regards for efficiency) really was. I feel that a beginner to CLOS (not Lisp) could learn a lot about it from this book. I do agree that the Keene book is a very nice introduction to CLOS.
While I'm a Common Lisp man myself, the Scheme community are wonderful brothers to have.
That's the one he is using.
Similar can be said for Allegro CL, which is now in 9.0 version. Most important thing is smp. But, both (LispWorks and ACL) are still very expensive...
Allegro CL also got a GTK+ version of their IDE. Still no Mac/Cocoa, though.
was it UFFI in 2008? it's CFFI now. CFFI has also recently added support for passing foreign structures by value via libffi. also ASDF has been improved a lot since then, version 3 has just come out over the last few months, though I admit I'm not really familiar with version 3 yet (from what I gather, dependency tracking for rebuilding has been dramatically improved i.e. recompiling a changed system should also trigger recompiling of it's dependents IIUC, among other things), for a while now ASDF handles segregating your fasls for you depending on implementation and features of the implementation, and keeps them all nicely organized in a user-configurable cache directory
Lisp isn't really a part of the interview :(
That's like saying that Python 3 is an important advance for Ruby.
Let's not forget Perl 6. 6 &gt; 3.
Yeah, gotta watch for actual content. I was going to post the John Carmack talk where he mentions Lisp, but he really didn't say anything about Lisp at all in anything but name dropping. It's remarkable to see Carmack use an enormous volume of words to say nothing.
Hunchentoot is still the most popular web server but there is now [teepeedee2](http://common-lisp.net/project/teepeedee2/) and [wookie](http://wookie.beeets.com/). also [jscl](https://github.com/davazp/jscl) A CL to JS compiler.
Oh cool, thx for the link. I have been playing with hh-web, this looks interesting.
Conrad Barski's *Land of Lisp* deserves another mention ... for its sheer zaniness. Cartoons and comics abound, many witty, some sardonic. My favorite one shows RMS as Sasquatch disappearing into the basement of some building at MIT with the onset of AI Winter. Most Lisp books are dry and pedantic. In contrast, this book has the reader implementing "Grand Theft Wumpus: The Most Violent Programming Example ever Put into a Textbook". Seriously, in doing so, Barski teaches the reader some graph theory and how to use lisp's loop macro. Novice programmers, who formerly knew nothing of Lisp, might discover the language through *Land of Lisp*. That's significant. 
Note that the RESTAS docs are outdated as of December last year. I've attempted to document the new RESTAS in http://lispwebtales.ppenev.com/ but it's still not finished.
I'd like to hear what you think of Lisp and why you don't like coding in it. Not baiting you, just curious what your experience with programming is overall (context), what have you tried to write in lisp and why it did not work out. I always love hearing programmers perspectives. That reminds me, I have to buy Peter Siebels Coders at Work book.
I said using Lisping (an iOS editor, pretty innovative and interesting) to write code, not Lisp. I like Lisp, I wouldn't be in this sub if this was not the case. I have written 2 or 3kloc of Lisp (a raytracer, a few mathematical things for research, a web scraper and some miscellaneous tools, beside quite a lot of emacs lisp I'm not counting). Oh and Coders at Work is pretty good. It relieved some of my programming doubs regarding how I develop and debug, and got me to try my hand at Erlang and give Javascript another, deeper try.
Aha! I misread, my apologies. Never heard of Lisping. Thanks for the info.
My mistake. I meant more in the line of 'well written, maintained, cross platform gui solution' rather than LispWork's specific implementation.
A dark picture.... Did you give CL-GTK2 a chance (did you try to run it on Windows)?
If you have an iPad, give it a try. It's "different." I'm curious about your Lisp experience now ;)
Sadly, GUI state in CL is pretty ugly. The only working solution I manage to use recently is CommonQT... and that happened after couple of hours of installation trials and errors and it's dependencies...
I couldn't have missed his review - "The best available" - although I was under the impression that his time was otherwise occupied; I'd better check the Quakecon keynote to see if something interesting and Lispy's happened in his life.
Watched a bit, I don't remember how he started this `applicative` journey with haskell and then lisp. But he seems to enjoy it.
Another option is Clozure CL. It's GUI library uses Apple's Cocoa as a backend. On Windows it now uses a backend library called Cocotron http://www.cocotron.org .
If you use ABCL, you have the entire Swing or SWT libraries available to you.
I'd be fine with CommonQT, it looks maintained, I just can't install it. DO you happen to recall what version of QT/Smoke you built the wrapper library against?
Unfortunately last blog post is dated back to the year 2010...
I would not use CL for a standalone GUI application. Could it be possible be written as a web application?
Not sure why you were downvoted. Plenty of native-ish apps hosted locally. The CL GUI story is a lot less terrible if we consider web apps.
There is CL-CFFI-GTK for Gtk3 too: https://github.com/crategus/cl-cffi-gtk
This looks like a great project. Thanks!
Blog posts just mean the dev isn't doing any actual work! Last commit was 34 hours ago: http://code.google.com/p/cocotron/source/list Granted, development seems to happen in bursts, but I've been following Cocotron for a number of years and development has always continued.
I guess I was being downvoted because OP clearly asked not to be told to write a web app in his post, something I missed and just noticed now :)
Well, considering I missed it too, I guess we both have egg on our faces.
I don't buy that. Maybe I'm old, but I remember a time when there was a lot of GUI development done in Lisp and especially Common Lisp. It's not a technical problem, it's just lost or hidden art. * The first really interactive GUI Builder was written in Lisp. It was sold as a commercial with Common Lisp. Steve Jobs got a demo and hired the guy who wrote it. It's now Apple's Interface Builder. * PTC has a complex CAD system with a very extensive user interface written in Common Lisp and C++. * See Open Music and PWGL for very GUI intense applications written in LispWorks. * Apple used MCL to write a bunch of applications. Microsoft used it to prototype MS Word User Interfaces on the Mac. * ScroreCleaner is a Music Notation software written in LispWorks: https://www.youtube.com/watch?v=6IlOL1IOnHk Actually writing GUI apps used to be a lot of fun. The commercial tools from Franz and LispWorks still support that. It might need to be rediscovered. Interactively develop GUI applications is much more fun than C++ or Java. It's just that currently the free libraries may not be there. That's why for example the larger and complex music applications, even if the code is free, are written mostly in LispWorks: Open Music, PWGL, ScoreCleaner, Igor Engraver, Symbolic Composer. Before that, people wrote those mostly in MCL - because that was cheaper and at that time there was no similar GUI-capable LispWorks on the Mac (which was popular at Universities and among musicians). It's not that it wasn't tried. Open Music 5 for example had a version for SBCL/GTK. The current Open Music 6 is only for LispWorks. http://repmus.ircam.fr/openmusic/sources 
I was able to run the examples about 5 years ago with a current version of Lisp -- Allegro Common Lisp I think.
I was able to get (qt-tutorial-14:main) running on Microsoft Windows 7. Here's what I did: ## Make sure you have the prerequisites installed. These are: 1. Visual Studio 2008 1. Qt 4.8.4. For this example, Qt is installed in C:\Qt\4.8.4\bin 1. CMake 1.8 1. git (I used the one from cygwin) ## Open a Visual Studio 2008 command prompt. ## Run the following commands from the newly opened command prompt: 1. set PATH=C:\Qt\4.8.4\bin;%PATH% 1. pushd %USERPROFILE% 1. mkdir commonqt_build 1. cd commonqt_build 1. set COMMONQT_DIR=%CD% 1. mkdir src 1. cd src 1. git clone git://anongit.kde.org/smokegen 1. mkdir smokegen_build 1. cd smokegen_build 1. cmake ..\smokegen -G "Visual Studio 9 2008" -DCMAKE_INSTALL_PREFIX=%COMMONQT_DIR% 1. devenv /build "Release|Win32" /project ALL_BUILD smokegenerator.sln 1. devenv /build "Release|Win32" /project INSTALL smokegenerator.sln 1. cd .. 1. git clone git://anongit.kde.org/smokeqt 1. mkdir smokeqt_build 1. cd smokeqt_build 1. cmake ..\smokeqt -G "Visual Studio 9 2008" -DCMAKE_INSTALL_PREFIX=%COMMONQT_DIR% 1. devenv /build "Release|Win32" /project ALL_BUILD SMOKEQT4.sln (This generated 1 error for me for the smokeqttest project.) 1. devenv /build "Release|Win32" /project INSTALL SMOKEQT4.sln (Again, this generated 1 error for the smokeqttest project) 1. cd .. 1. git clone git://gitorious.org/commonqt/commonqt.git 1. cd commonqt 1. qmake 1. set INCLUDE=%COMMONQT_DIR%\include;%INCLUDE% 1. set LIB=%COMMONQT_DIR%\lib;%LIB% 1. vcbuild /useenv commonqt.vcproj "Release|Win32" ## Make sure the DLLs you've created in %COMMONQT_DIR%\bin and %COMMONQT_DIR%\src\commonqt\release are accessible to your Lisp implementation. Don't forget that you will need DLLs from Qt as well.
Welp this solves my GUI question.
Does [this](http://pleasegodno.wordpress.com/common-lisp-tutorials/common-lisp-gui-programming-with-commonqt/introduction-to-commonqt/) help you with your installation problems?
I saw the Keene book in a bookstore and was so stoked until I realized it was priced at $45.
Well, I'll be damned. That looks amazing.
cl-test-grid
Great! Are there comparable instructions for Mac OS X?
This is great! Thanks!
You should try linux. It took me about 2 min to install commonqt.
 More or less the same way I learnt lisp: just for the sake of it. I'm more extreme with ignoring syntax: even in languages I am proficient enough to write code that can be used in production (Python and Lisp would be the two main languages in this group, but bash comes close and due to job requirements PHP is almost there, too) I can't even remember the exact way to write (for example) a for loop (well, in Python I can, unless I need to do something tricky with the way I loop, but there is no way I can remember the way to set up a CL do loop without checking the Reference... far less the CL "loop" construct. In bash I never, ever figure it out without checking my previous for loops.) Go figure, but I just don't see the point of remembering once I switch languages. When I'm in code mode in a language I don't have a problem with any syntax structure, but my start-up times are big because I keep forgetting the most stupid things. I guess it is also connected with the number of times I write for loops in higher level languages: I have written so many for loops in C and so "few" in any other language (where I can map, iterate over a list or whatever) that I think my mental "for" department is full. Re: commercial languages... Pick whatever suits you. Learning as many languages as possible (even if you keep forgetting for loops :D) will eventually lead to being able to do something in anything. I have worked as a contractor writing Python (which was and is in my resumé) then was asked to write Visual Basic for Applications in Excel to interact with this Python thingy I wrote (VBA is not in my resumé, last time I touched it was like 12 years ago, except for this gig), then was asked to fix their website (PHP, which was not on my resumé at all: I barely knew the most basic things of it... but fixed the site and removed a few bugs,) finally was asked to write some Javascript code to deploy fine-tuned visitor tracking in one of the biggest online retail chains in Europe (Javascript was in my resumé but only as "basic level of.") Everything worked ok (although I hated moving from one project to the other in just 2 months.) The feeling you have with Lisp is a mix of what Lisp is strong at (making the logic show and hide anything else) and what you get once you "get" the language. I feel that not only is this feeling easier to get in Lisp, it is also more "pure" (kind of easier to feel.) If you have the time and are so inclined, give Prolog a try. It's one of the few languages beside Lisp where just after a few hours of getting into it gave me the feeling of "oh shit, that's awesome."
Oh, hey, glad to hear it! I'm the author of [hh-web](https://github.com/hargettp/hh-web), and I'm always happy when someone finds it useful. There is more that could be done with it (both documentation and features), and I'm happy to add refinements folks consider useful.
I'm not up to date with the state of clojure -- is it possible that an alternate (non jvm) clojure host vm will eventually provide an environment that apple will allow on iOS or is that avenue blocked in a similar way to what we've seen happen with Clozure CL? Also, are developer freedoms like this also prohibited (err I probably mean prevented) on "unlocked" devices? Apologies if I misunderstand he state of affiairs with this, but to be honest it disgusts me too much to keep up with on a regular basis.
Hey dude! I love it. It's awesome. It's simple, extensible and if I never see another piece of html / css it will be too soon. I am not able to find as much time as I would like to play with it, but I am putting in time as I can. I would love to send you some thoughts and ideas. I will send some updates on the documentation side of things as well. Thanks for the ping! Glad you are keeping an eye on the project, I would like to keep using it.
It's insanely fun! Pick any starter book (many are legally free online) and in a few chapters you will be writing toy grammars for English with it!
How does lispm find all these great links?
pffft. Ruby has a long way to go before it's even close to lisp in style and extensibility. Homoiconicity is a big one. 
&gt; Homoiconicity Homoiconicity leaves all other langs behind, IMO.
Lisp, my love, we will meet again. I promise.
[PDF version](http://www.paulgraham.com/onlisptext.html) I don't use ePub because it supports [DRM](https://en.wikipedia.org/wiki/EPUB#Digital_rights_management). 
Check out these instructions for printing the book with Lulu: http://www.lurklurk.org/onlisp/onlisp.html Edit: I actually did this and my book arrived a week ago. It looks great and it I'm very, very happy with it
&gt; Adobe created pdf Thank you bro. &gt; DRM? No thanks.
I completely agree with you in the nodrm but you have nodrm pdf,epub,mobi etc.. is not the format is the provider.
This is really cool! I really like the visual feedback. Reminds me of http://impromptu.moso.com.au/ My favorite demo of it is: http://vimeo.com/2579694
ePub is just a file format. It does not endorse anything. The book is DRM-free, so what's your deal?
Nice job, thanks!
Amusing thing about that: that's a quote from the book. I thought it was a promo blurb of some kind when I first saw it, but nope, it's the author's opinion. Having said that, I have to note I thought it was rather hardcore. Perhaps it's just the fact I'm new to Lisp and FP, but I'd still like to know, what would you consider worthy of the title? 
what the actual fuck
Ahhh... documentation... or lack of it...
On the other hand the code is very readable.
Good to hear about any new nontrivial cl code. What is the copyright+licence? How can it be used?
I think that the day when the code is the only documentation we need has not arrived yet. A little bit more comments and function documentation in the sources, a viable README description would be extremely useful.
He didn't put a license.
in README.md he only says 「H.264理解用」, which means "I am writing this code just for reseach purpose, I'm trying to undestand the structure of H.264" and so it is a private project yet. That's probably why the repo has no actual README and no LICENSE.
Go team!
Looks like the problem statement and contest updates can be found here http://icfpc2013.cloudapp.net
This is way cool I wish I could have seen this interaction of great lisp hackers in the past.
Touché.
Just bought this book - can't wait to try it out over the weekend.
This is awesome. I'm not really a good enough lisp programmer to do this but I hope someone does.
It could be useful to note, however, that, depending on the individual, 2628000 &lt; Y &lt; 5256000
(Which is 5-10 years, if _Rent_ is correct.)
What a great read!
Yeah, I can do it. It won't do more than you quick-started and a handy cheat sheet. I'll get a draft rolled up tonight and a PR sent in. edit: https://github.com/adambard/learnxinyminutes-docs/pull/186 If anyone sees bugs or sees any particularly gaping holes, I'll fix it up. I don't want to cover conditions here, as those are a bit complicated for a quickstart guide.
While this article by Peter Norvig is great, it's message has IMO not much to do with Learn X in Y Minutes. I don't think anyone would say after reading Learn Python in 10 Minutes that he is fully capable of Python programming. But he has an idea about what it's like to program in Python (e.g. that it's not a Lisp). This site came in handy to me when I didn't program in a language (Ruby, Python) for a while and needed to "refresh my knowledge".
Answer in article.
http://www.youtube.com/watch?v=XoJUYQk_FMI
How do you make it code? 
Ok. I (kind of) see why I am not getting what I expect/want, and I am getting what I am getting. I still don't see how I could get what I want. (ie, put some code in place, and let it be as if I typed that code there.) 
You need to make it a part of the code. That's what you need to do in the macro. The macro generates the code. If the macro returns T1, then there is the source. If you don't want to see t1 in the code, but its value, then you need to compute the value of t1 in the macro. To get the value of a symbol, you can call SYMBOL-VALUE or EVAL. But the example leads to other problems. You might want to tell us what you really want to do.
Yes and no. I can add one. But it won't be in any sense coherent without more context than I can feel will fit in this format. If someone has a clean way of describing the CL condition system, I welcome their assistance.
Nothing specific, really. Just trying to understand macros. At some point, I would like to have pieces of code tied in variables, that I could just interchange/inject/combine at will. Something like genetic simulation with the codes being the agents. 
Putting (eval s) in the macro seems to work. What are the problems? 
in a simplistic explanation, a macro takes code (as symbols or lists of symbols) and returns code. functions take data and return data. So macros exist to transform code and most often as shorthand for more complicated code. For example, ***and*** is a built-in macro in common lisp; it is not a function. (macroexpand '(and a b c)) =&gt; (IF A (AND B C) NIL) This will expand to: (IF A (IF B C NIL) NIL) The expansion of the macro does not care about the values of *a*, *b* and *c*, and treats them as symbols. As lispm wrote, if you want to use the value of a symbol in the macro expansion, you need to use **symbol-value** or **eval** in the macro definition. Normally, this is not a good idea. Also macros operate at a different *time*, macros are expanded at compile or load time, when the code needs to be parsed, but before it is evaluated. In other words, macros are not normally expanded during runtime. The simplest way to do what you are trying to do is: (defvar t1 '(+ x 1)) (eval `(let ((x 1)) ,t1)) =&gt; 2 Which does not involve macros.
Common Lisp is a standardized Lisp dialect which was created by fusing several (american) Lisps in use at the time. It is a multipurpose language, not targeted towards any special use. Still the most dominant Lisp. See also its page on [Cliki](http://www.cliki.net/Common%20Lisp). Btw, it is misleading to refer to Common Lisp with CLisp. CLisp is one implementation of Common Lisp among [many](http://www.cliki.net/Common%20Lisp%20implementation) . Just using Lisp means Common Lisp for most (Common Lisp using) people. But it depends on context, sometimes it also means Lisp, the idea. One usually says Lisps (plural) or the Lisp Family of Languages if refering to all Lisp-alike languages. Scheme is an attempt to have a programming language with a clean and minimal core of the most basic abstractions. It was created for research and teaching. It also has many [implementations](http://community.schemewiki.org/?scheme-faq-standards#implementations). Some of them are not quite minimal anymore, bringing along an extensive library and lots of extensions (SRFIs), allowing to use them for writing 'practical' software. Clojure is a new Lisp dialect by Rich Hickey with a strong emphasis on multithreaded and parallel programming. Though not a pure functional language, it uses lazy evaluation and immutable datastructures but still provides mutation via software transactions. If your target is web development, there are several web frameworks in [quicklisp](http://www.quicklisp.org/beta/). Racket and Clojure also have nice libraries for web dev, but you have to use the google, read docs, do some experimentation and then decide for yourself what you like best. 
If code is data is code, what's the diff between functions (with quoted args) and macros? b) Why eval in the macro def is not a good idea? c) About the last example, if I do: (let ((x 1)) t1) =&gt; (+ X 1) but if I do: (let ((x 1)) (eval t1)) I get an error that x has no value. Both results are kinda understandable (mostly the second), but it feels to me like a "middle level" between the 2 of them is missing. 
Common Lisp is also fairly odd in that it's a Lisp 2 (functions and "other values" are in different namespaces), whereas e.g. Scheme and Clojure are Lisp 1 (all values are in the same namespace). Furthermore, Common Lisp has something of an "old" feeling to it, what with printings being done in ALL CAPS and the like.
"Code is data is code": this does not mean that code and data are the same thing. Code is data means that the code is written as lists of symbols. Lists and symbols are fundamental data types in Lisp. So defmacro can use the language of lisp itself to "code" how to transform code into new code. You can loop, recurse, append, quote, etc.. In the opposite direction, you can take data like (defvar my-list-of-code '(+ 1 2 3)) and call (eval my-list-of-code) a) the difference between functions and macros is when they are "executed". (if a (my-macro-1) (my-macro-2)) (if a (my-function-1) (my-function-2)) In the first example, both macros will be expanded, because macros are expanded before evaluation. In the second example, only one function will be evaluated depending on the value of a.
b) eval is not a good idea because you are transforming code not values. consider this: (defvar code '(+ x 1)) (defmacro my-macro (s) `(let ((x 1)) ,(eval s))) (defun my-function () (my-macro code)) (my-function) =&gt; 2 (setf code '(+ x 99)) =&gt; (+ X 99) (my-function) =&gt; 2 This is because the macro is expanded when the defun is evaluated. edit: indentation of macro. 
I looked into the Cliki page. Looks interesting. There seems to be some decent amount of tools available with CLisp too for web development. Looking into quicklisp too. But I think I will start of with CLisp and see where things go with it. Thanks for the information, it was really helpful.
Actually Common Lisp has more than two namespaces. Classes, functions, variables, etc, Peter Norvig counts at least 7 in PAIP. This is a blessing in my opinion. I do not want function parameters shadowing functions of the same name. For an in-depth discussion of this matter see: http://www.dreamsongs.org/Separation.html 
&gt; I do not want function parameters shadowing functions of the same name. I really don't agree as I don't see any difference between them which would warrant such treatment (why not add a namespace for integers and an other for strings in that case?), but if you like it, more power to you. edit: heh, I should have remembered that expressing preferences and acknowledging others may disagree in here could only lead to downvotes.
Clojure is also available on the CLR (leveraging the .NET framework rather than the Java SE/EE library), and it can also be compiled to Javascript by using Clojurescript. (So, there are non-jvm options, and it is a nice language :-)) 
&gt; Common Lisp prints symbols in upper case by default because symbols are case insensitive. That's not a justification for printing it in upper case by default. PHP's function names are also case-insensitives yet that turd of a language doesn't print them in ALL CAPS. Nor does it change the "old" feeling the default behavior gives to the language.
&gt; So my main question is, what sets different dialects of Lisp apart? Almost everything. Lisp is not as much a language as it is a metalanguage, or a language-building mindset. Besides the obvious dialects like Common Lisp, Scheme, and Clojure, you can take a look at Qi/Shen to extend you notions of what Lisp is even further. :-)
&gt; Common Lisp prints symbols in upper case by default because symbols are case insensitive. I believe that Common Lisp symbols are actually case sensitive; it's the *reader* that's case insensitive - or, more properly, "case converting".
That's informative too. Thanks. 
&gt; That's not a justification for printing it in upper case by default. No, the actual reason for why (symbol-name 'abc) returns "ABC" is simply the fact that 'abc is read in as the symbol named ABC. It can't print abc because that would be a different symbol.
Traditionally, there are two main flavors of lisp: Common Lisp (CL for short), and Scheme. Scheme strives for compactness and elegance, thus it's a small language. It's also what people call a "Lisp 1", which means that regular values like ints and strings share the same namespace as functions. That is to say that if you declare a variable called "list" in will shadow the predefined "list" function. Another important feature of Scheme is the hygienic macro system. Bigloo, Chez Scheme, Chickend Scheme, Kawa (on the JVM), IronScheme (on the CLR), and MIT/GNU Scheme are some of the best known Scheme implementations. Common Lisp on the other hand, is a large language, and it's a "Lisp 2". That means that functions and values live in separate namespaces, e.g. you can have both a value and a function named "list" in the same scope without them interacting in weird ways. The fact that CL is a Lisp 2 makes functional programming patterns a bit awkward. Also, CL has an unhygienic macro system. SBCL, Clozure CL, Allegro CL, LispWorks CL, ABCL (on the JVM), and CLisp are some of the most popular CL implementations. This is the short story. Which one you choose is up to you. I personally prefer CL, but that's just because I don't have an issue with it being a Lisp 2, and I can't come to grips with Scheme's hygienic macro system. To complicate things further, there are other lisps as well. Racket is a language that is derived from Scheme, but is a much larger language, with lots of libraries, a good IDE, and great documentation. It has some "modern" features that are not in the Scheme standard. If you're into Scheme, I think Racket is your best bet at the moment. Clojure is, well, somewhere between CL and Scheme as far as I can gather--I haven't used it personally, but I've read some stuff about it. It's a Lisp 1 with what seem like unhygienic macros--I might be wrong here, corrections are welcome. A lot of people, and some startups swear by it, by it doesn't seem exciting enough--at least to me--to warrant a switch from CL.
Java and the JVM are two different things. You don't need to know Java to use Clojure. You'll have to call Java libraries sometimes, but I know nothing about Java and it didn't seem to cause a problem for my use of Clojure. The main implementation being tied to the JVM can be a problem if you need to call into C libraries, though. There are also JVM variants of Scheme and Common Lisp, but you still write the same standard-conforming code there as with any other Scheme or Common Lisp implementation.
Sorry for the long post, but I want to make clear that this is not just about visual style. For practical reasons in Common Lisp, it sure is nice to have a Lisp2. It is the interaction the CL macrosystem that convinced me, without a doubt, that CL should be a Lisp2. In the article posted they address this. CL is (to me) basically defined by it's macro system, and things would be a mess without two name spaces. Variable capture problems would extend to functions, but there is no such mechanism like gensyms to work around it. As the article states, this happens with both Lisp1 and 2 in different ways, but if some user does: (let ((list blah)) (my-macro ...)) in a Lisp1 then they may be surprised if my macro happens to expand to something that uses "list". Compare to how this would have to happen in CL: (flet ((list () 'blah)) (my-macro ...)) Hopefully the user sees that this is inherently different and more dangerous than just using picking a name to bind to some value. Further, Common Lisp at least protects you against users rebinding functions like "list" that reside within the Common-Lisp package, so there is a safe language to use in macro expansions. Basically, in a Lisp1 with a CL style macro system, I either cannot ever bind symbols like "list" to anything or I can never trust any function to do the right thing in my macro expansions (leave that to the user to not mess my code up, even though she may have never looked at how it works). In a Lisp2, however, these are largely not an issue assuming people program sanely (only rebind functions that they own/defined, don't violate package locks, etc).
Right, but surely the problem exists for any "global" value, he can rebind the function anyway if he wants to (he "just" has to do so in the right namespace) and the namespacing merely lowers the chances of unwitting collision but doesn't preclude it in any way. &gt; Basically, in a Lisp1 with a CL style macro system, I either cannot ever bind symbols like "list" to anything or I can never trust any function to do the right thing in my macro expansions (leave that to the user to not mess my code up, even though she may have never looked at how it works). Don't you technically have the exact same risks in lisp2, as you demonstrated yourself? You just hope that won't happen. &gt; In a Lisp2, however, these are largely not an issue assuming people program sanely (only rebind functions that they own/defined, don't violate package locks, etc). But these also stand for a lisp1, the difference simply being that the set of things to not-rebind in each namespace is larger as the global set is roughly identical but they're all in the same namespace.
&gt; I know nothing about Java and it didn't seem to cause a problem for my use of Clojure. Have the tracebacks improved? Because last time I used Clojure not knowing anything about java made for very unfunny tracebacks (though knowing java didn't make them much funnier)
&gt; Have the tracebacks improved? Not to my knowledge, and that's a big reason I didn't use Clojure for very long, but I saw that as a Clojure flaw rather than a Java interop issue. 
I would say that we do technically have the same risks in CL, but we practically do not. Also, because of package locks, there is no risk at all for anything in the CL standard, I believe (though this comes at the cost of not being able to easily and portably modify behavior of low-level functions, which is sometimes useful). &gt;&gt; In a Lisp2, however, these are largely not an issue assuming people program sanely (only rebind functions that they own/defined, don't violate package locks, etc). &gt;But these also stand for a lisp1, the difference simply being that the set of things to not-rebind in each namespace is larger as the global set is roughly identical but they're all in the same namespace. I think it should be a fundamental that: (let ((&lt;anything&gt; &lt;some-other-thing&gt;)) (some-form &lt;anything&gt;)) should not change behavior based on the value of the token &lt;anything&gt;, even if some-form is a macro. This is true in the CL world except for the big exception, special variables, but that is why they are special. This referential transparency problem would have to be worked out somehow if CL were to magically become a Lisp1 overnight and I'm not sure how I would do it. Binding functions with flet and labels could pose a risk, but you would have to go out of your way to make those errors. I.e. you would have to 1) use flet or labels not let, 2) either specifically craft your code to mess with someone else's functions in their package or disable package locks/write non-conforming code. Even if you don't see the behavior as different, isn't it good to separate two things that have significantly different risks associated with them?
I wonder if they know what actual Lisp Machines are. Writing a lisp on an Apple ][ does not make it a Lisp Machine. In fact there was a lisp implementation called [Plisp](http://www.flownet.com/ron/plisp.html) written in 1981 for the Apple ][. There are even toy lisps being written relatively recently for competitions like the [RetroChallenge](http://hoop-la.ca/apple2/2010/retrochallenge.html) They also seem to be ignorant of the fact that the Apple ][ had utilities like 'text editors' and 'assemblers'. Which isn't to say ADT Pro isn't the way to go these days but give me a break.
I am not a big fan of that book. In general Lisp and Scheme books are exceptional and excellent at teaching the material as well as largely timeless. I really like the concept of using a game as the fundamental system being built as it incorporates many elements of computing. Where Land of Lisp lost my favor though was with how it never challenges the reader to think and practice the material as some form of exercise. I liked (Common Lisp: A Gentle Introduction to Symbolic Computing)[http://www.cs.cmu.edu/~dst/LispBook/] a lot more as an intro text as it both challenged the reader to implement and think about the concepts as well as how they are done in Common Lisp. With Land if Lisp I felt like I was just reading a book rather than working through it like all the rest of CS books have been. I got a lot more out of those personally. Hopefully in future editions of Land of Lisp the author resists the temptation to do everything for the reader and challenges the audience in some way. I think the current edition has laid out a great foundation to improve upon though, and I too love the zaniness. I would not currently recommend it as a first text over the intro to symbolic computing text though, a book which I would use in an intro CS course along side some other texts. For what it is worth, I read both of those early on in my investigation of Lisp and absorbing the excellent Lisp texts; they have been tremendously fun so far and succeeded in wrinkling my brain along the way. Common Lisp has been a joy to work with, especially coupled with the amazing slime-mode environment and paredit. My only regret is that I cannot use slime everywhere as it is sensational and the best IDE I have used to date. It is a reason to choose Common Lisp for me and a reason I find it so fun, plus I get to exploit structured editing with redshank and paredit, both of which are very impressive tricks. It is too bad more people do not make it far enough with this exploration to experience this consequent joy.
I agree. The challenge of writing a meaningful Lisp implementation for an 8-bit machine like the IIe is far greater than the obstacles they describe here in transferring data and their lack of floppy disk media. A main memory of roughly 12k cons pairs is not going to support much. As for real Lisp Machines, they would use something more powerful than this as the front-end processor to boot the main machine. That Retrochallenge link is pretty interesting, although the author seems like a crank. I'm pretty sure he hasn't thought too hard about the limits in GOSUB in Applesoft basic, which I recall have a limit of perhaps 16 before you exhaust the stack.
In addition to your nice summary of the differences in focus of the various lisps, I'll point out that Racket has a [typed version](http://docs.racket-lang.org/ts-guide/) that incorporates some of the ideas in Haskell / ML about typing.
Exactly. For addition I show the example in SBCL: * (setf (readtable-case *readtable*) :preserve) :PRESERVE * (DEFUN test (a b) (+ a b)) test * (test 1 2) 3 * (TEST 1 2) ; in: TEST 1 ; (TEST 1 2) ; ; caught STYLE-WARNING: ; undefined function: TEST ; ; compilation unit finished ; Undefined function: ; TEST ; caught 1 STYLE-WARNING condition debugger invoked on a UNDEFINED-FUNCTION in thread #&lt;THREAD "main thread" RUNNING {23EF79B1}&gt;: The function COMMON-LISP-USER::TEST is undefined.
That's just a name after all - better judge it by its usefulness and if used wisely, LearnXinYMinutes is useful IMO.
I arrived in CL this year, when quicklisp was already created, and while the terms were a bit confusing at first (a package is like a namespace, a system is like a package, etc.), but it was not difficult to use. And indeed, the more I learn about historical Lisp, the more I notice how much of a big deal quicklisp is. I agree with Seibel that quicklisp can (if it already isn't) a *de facto* standard.
BTW I also run http://articulate-lisp.com/. If you're willing to tell me your confusions &amp; play alpha tester, I'm willing to work up a special section on error handling/conditions. I've been consistently unhappy with the condition system tutorials and documentation online (and thus, my knowledge of said system is weaker than other parts).
&gt; I really don't agree as I don't see any difference between them which would warrant such treatment You don't think that you use functions much more often that integers or strings, for instance? One advantage of placing functions in a different symbol cell (the `symbol-function` instead of the `symbol-value`) is that it opens the door to some optimizations; the article sugarshark linked to states: &gt; Common Lisp compilers can be written that assume that function cells always contain functions in Lisp2 because it is legal to forbid nonfunctions from ever being placed in the function cell. You should really read that article. Actually, it should be required reading for anyone who wants to discuss this subject, because otherwise people keep bringing up points that are settled in it. &gt; edit: heh, I should have remembered that expressing preferences and acknowledging others may disagree in here could only lead to downvotes. That is indeed unfortunate…
At the very start of the talk, Peter says something like "please keep all comments and interjections until the end", but many seemed to object and speak up during his talk anyway. Sort of made me cringe each time it happened, and it really hampered the flow.
There was a significant number of geriatric "I was there and this is how it REALLY was!" Lispers present. The axes won't grind themselves, ya know.
The math is broken: ecmachine:/ guest$ (/ 123 21) 5.857142857142857 Expected: ecmachine:/ guest$ (/ 123 21) 41/7
The Javascript engine in the browser is not very good at maths.
There exists another HTML version in this subreddit's sidebar. The author recognizes that version and gives his reasons for a new one in the *readme*.
When does dislike become a (big) deal? What's your deal kid? You can go back to facebook if you cant dislike things.
It stops working if I type invalid commands )-: Doesn't recognize when I hit Enter anymore.
This is a good article but, based on the attitudes it is complaining about, either the Lisp community is just as determined to fail as it always was or its an old article: I suspect the former, sadly.
Fair point. Quicklisp is pretty awesome.
No, only the ones worth mentioning.
maclisp, elisp.
Someone needs to say how cool this is rather than complaining. so I will. I think the JS pdp11 is marginally cooler, though probably less useful.
Code talks, bullshit walks. There seems to be very little code in this article. 
It is definitely very cool. Have you seen repl.it's LLVM Scheme? http://repl.it/languages
The [author](http://www.cliki.net/Robert%20Strandh) has written a fair amount of lisp [code](https://github.com/robert-strandh?tab=repositories).
I will acknowledge that, indeed. The article is an opinion piece so it's hard to really be hard on the fellow. It's just how he sees it. My take on the article is that it is so gratuitously unfair and judgemental of other people that it makes me wonder what he considers is RIGHT about other languages and just what does he consider NORMAL about the other language community members. If you want to see what I mean, strike out the word Lisp and replace with Germans. It is 100% valid in both instances. It only gets worse from there.
I'm interested to know what others think of his style and organization as it relates to junior Lispers looking to emulate it.
It's not a bug, it's a feature. Now, stop trying to test their hard work. /s
How can I start the ECMAS editor?
I think [Racket](http://www.racket-lang.org) is the Python of Lisps. It has the batteries-included approach, copious documentation, large (for Lisp) community and conservative language/library design. If you don't get much out of Racket's accouterments, then perhaps [Guile](http://www.gnu.org/software/guile/guile.html) is a better choice. Clojure's much more likely to get you a job writing Clojure, but I guess if newLISP was on your radar that's not so critical. If you want Common Lisp, then pick between ECL if you like newLISP's smallness, or SBCL/Clozure if you like the community/huge library support.
You can use an external editor with CLISP... use `(LOAD "filename")` to read in your source file. There is no reason you should have to pick a Lisp implementation solely because it has an IDE, as any reasonable one will have a way to load source files, or it wouldn't be very useful for real development. Also, whatever editor you decide to use now, if you are planning to use Lisp at all seriously, at some point you should consider learning Emacs, just because it's essentially the closest thing to a universal Lisp IDE there is, once you add the SLIME package that lets it talk to external interpreters (like CLISP but also many others). I'm not sure I would recommend diving into emacs right away though, so you aren't piling a second learning curve on top of the first. (If you do, note that Emacs' internal dialect of Lisp, that you use to customize and extend it, is *not* based on Common Lisp, so don't expect exercises from Land of Lisp to work in it without modification.) As long as you have some halfway decent editor you can use it for now, and just load the source files into the REPL by hand to play with them. Edit: As for the original question: `'foo` is just a more readable way to say `(quote foo)`, so `(walk 'west)` and `(walk (quote west))` are actually the same thing. So it's not wrong, it's just displaying the result differently (presumably, CLISP turns `(quote x)` back into `'x` when it prints an expression, while Lispworks doesn't, or something like that. I've never used Lispworks, so dunno.).
 ; SLIME 2013-05-26 CL-USER&gt; (defun game-read () (let ((cmd (read-from-string (concatenate 'string "(" (read-line) ")")))) (flet ((quote-it (x) (list 'quote x))) (cons (car cmd) (mapcar #'quote-it (cdr cmd)))))) GAME-READ CL-USER&gt; (game-read) walk west (WALK 'WEST) Lets inspect the list (WALK 'WEST): #&lt;CONS {10035C5787}&gt; -------------------- A proper list: 0: WALK 1: 'WEST And lets see what 'WEST is #&lt;CONS {10035C5757}&gt; -------------------- A proper list: 0: QUOTE 1: WEST Get it?
Determined to fail for 50 years and still going :)
&gt; My main goal here .. What exactly is the point of this conversation ... you already heard everyones opinion on the topic, what makes you think asking the exact same question yet again will result in different answers? Use what you like, don't spend any time worrying about what others think of your choices. 
You program based on OTHER PEOPLES OPINION of your language choice? You cannot see the problem with this?
I'll take this as far as you want. Truth hurts, and it shows.
@steven_h mentioned the reasons really nicely and I'll try to summarize it's practical usage, at least from my experience using it on couple of projects. Just to say I'm still using CL, but today I'm much more heavy Clojure user simply because my current projects rely on JVM and concurrency... NewLisp in short is Perl of lisps, with all pros and cons being Perl of something: you will get insanely fast startup (still can't find CL and I'll be free to say, other language implementations, that starts that fast using so small amount of RAM), really good text and list handling, simple but effective FFI and huge amount of functions packed in a single binary. But also you get tons of strange hardcoded placeholders ($1, $2...), functions with odd variable arguments, and macro system based on fexprs I never manage to grasp (I don't grasp hygienic macros either, so this shouldn't be a dealbreaker :D). NewLisp was the only scripting language I mange to run on old SPARC powered Solaris a couple of years ago when I had to complete something; Perl was there but disk space was so tight I couldn't install a single module without pulling a full CPAN cache, and python... oh boy, you realize it's size when you are challenged with the old hardware. The last project I done with it was a CGI powered web page, where I packed in a single script almost full templating system, RSS parser, caching facility, news/page renderer and multithreading (yes, multithreading in CGI) capability, where script would, while rendering the page, fetch some RSS feeds in background and do some magic. Script startup and RAM overhead: zero. Now, if you consider above project was in about 300 lines of code, including utility stuff and functions, NewLisp is amazing in some fields. However, some things are simply wrong and these are my objectives: * author isn't opened very much for 'standardized' things and he has his own way of doing things * REPL doesn't support multiple lines; you have to wrap things in [text][/text] tags which makes default Emacs useless unless you do some elisp-ing (there is a script that fixes this in NewLisp forum) * GUI is some odd Java stuff controlled via sockets; full Gtk+ wrapper would be very easy to make, but no one done it * license; GPL3-ing scripting language without permissible runtime is just wrong * embedding; IMHO this is the field where it could blew the competition off However, I still like it and as every language, it should be a tool to complete the job. That is why it sits happily in my toolbox ready to pop up for challenges when other languages and implementations fails.
When you've opened more parentheses than you've closed, ECMAchine will think that you're trying to continue the same command even when you press Enter. Admittedly, there should probably be a way to force the current command to be cancelled (maybe Ctrl-D or something of that nature?)
&gt; find the Python of Lisp Sigh. I can also hate on Python if you want: Guido *also* seems to have completely missed the lessons Lisp learned in the 1970s. In particular, the Lisp world has been thinking about compiling to high-performance native machine code since the 1960s. Scheme revealed in the early 1970s the value of lexical scope and that it was possible to efficiently implement identical Lisp semantics in interpreters and in compilers. Python was never designed to be efficiently compiled. Figuring out how do to so is an ongoing research project that might never succeed. The only real solution to performance in Python is "implement the important parts in C." Python doesn't have real lambdas and distinguishes between statements and expressions for no apparent reason other than Guido doesn't see the point. NewLISP has similar problems: it's really amateur hour in the language design. They invent totally crazy semantics like "One Reference Only", resurrect features like FEXPRs that Maclisp users threw away decades ago, and to make it a hundred times worse, the users waltz into a mature language community acting as though they've got the greatest thing in history, and trying to steal the name of Lisp to put on their useless junk. Then develop a persecution complex because all these mean cranky old Lispers don't want to hold a parade for them. &gt; amateur tinkerings revealed good results. Good for you. Now fuck off and leave the professionals alone until you show a professional willingness to learn from people who actually know what they are doing.
I think it's very clear from the article that he does not consider anything is wrong with Lisp. Indeed in the first sentence he says "I personally do not think there is anything particularly wrong with Lisp". So presumably he does not think there is anything particularly "more right" about other languages. What he is writing about, in fact, is his perception that there are a significant number of people "in the community" (which I will elaborate on a little below) who repeatedly voice arguments which are of the form "I can't get anything done with Lisp because x" where x is in a fairly small set of reasons, which all come down to "there is something wrong with Lisp". As a former member of that community I share his perception: you do see a lot of this sort of complaint. Where I perhaps differ from him is in two things. * First of all, I think the reason this happens is generally rather simple -- a lot of people actually *don't want to get stuff done*, and these complaints are a way of them explaining, mostly to themselves, *why this is OK*: "Lisp is broken so I don't need to worry about why I'm sitting wasting time on the internet rather than writing my program". This is, emphatically, not a problem with Lisp: it's a problem with the people. * Secondly I am not sure what "the community" is. Certainly you (used to) hear this sort of complaint very frequently on CLL. But, of course, you don't hear at all from the people who are getting stuff done. Zach Beane is probably a good example: I'd be willing to bet that he was fairly silent, because he was busy *actually doing stuff*, and as a result came up with easily the most important development in the CL community for the last 10 years (possibly 20). Similarly I wonder how much you hear from the maintainers of SBCL, and so on. So "the Lisp community" is a bit like the universe, perhaps: there's a small noisy part, but then there's all this dark matter and dark energy which you never see. I'd like to claim that this problem is also a thing of the past: does anyone really still read CLL after it got eaten by trolls? But I think, sadly, that this isn't the case: witness the reddit post about ECMAchine, where people were gleefully berating this reasonably decent little lisp environment which runs in a browser window (say that again: *runs in a browser window*!) for having deficient arithmetic (inherited from JS of course). That's just not helpful. So, like Robert, I think there is a problem, but it's a problem with people. Perhaps unlike him I think that it may be that this problem is actually a very small part of the community: you just don't hear from the rest of it.
In fact, Scheme implementations may or may not have rationals. Quoting from the [current R7RS draft](http://www.scheme-reports.org/) (this text seems to be unchanged since at least R5RS): "For example, implementations in which all numbers are real, or in which non-real numbers are always inexact, or in which exact numbers are always integer, are still quite useful." Operations which can't produce an exact result may either signal an error or silently coerce, and "[...] implementations that do not provide exact rational numbers should return inexact rational numbers rather than reporting an implementation restriction". There are constraints on what exact numbers need to exist, and I'm not sure that ECMAchine is conforming, but there certainly is no obligation to implement rationals in a Scheme implementation (unlike, say, with CL).
&gt; In fact, Scheme implementations may or may not have rationals. The spec does indeed allow you as an implementor to omit certain features, but that's sort of like buying a 4x4 vehicle, and then cutting the drive transmission to the rear axle to make the vehicle no better than others. Why would anyone intentionally remove a feature that makes one language better than others is beyond me - unless you hate tall poppies, of course.
[r4rs](http://people.csail.mit.edu/jaffer/r4rs_8.html#SEC50) [r5rs](http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-9.html#%_sec_6.2.1) [r6rs](http://www.r6rs.org/final/html/r6rs/r6rs-Z-H-6.html#node_sec_3.1) Edit: I have no problem if you lack rationals. You only claimed "lispy". I would advise you to clear about what the language is and what it is not. Or you will get a ton of complaints like this.
If you want to use NL, use NL.
I imagine it's for much the same reason that the ARM core that runs, say, a washing machine, might omit the FPU or memory protection support.
Well, yes, but few people would use Scheme in a washing machine firmware. :-) I find it more likely that an implementer would want to omit bignums and rationals to simplify the compiler and the type system if the purpose of the implementation were high-performance FP numerics, as is the case of Stalin, for example. However, in most cases, you don't want to get rid of generic arithmetics. (Some guy has recently posted quite an extensive argument why that is so, and even why you want to avoid FP the vast majority of the time for most programs, but I've misplaced the link. ;/)
My point was that omitting various features allows for small and simple implementations. For instance an implementation on top of JS which uses JS's numerical stuff (I am not sure this could be conforming, but I think it probably could) and thus trivially interfaces with JS. I would much rather use something like this than JS, for instance!
I can confirm that Lispworks prints "QUOTE": CL-USER 3 &gt;(game-read) walk west (WALK (QUOTE WEST)) CL-USER 4 &gt; (first (second *)) QUOTE CL-USER 4 &gt; (write ** :pretty t) (WALK 'WEST) (WALK (QUOTE WEST)) In SBCL: CL-USER&gt; (game-read) walk west (WALK 'WEST) CL-USER&gt; (first (second *)) QUOTE CL-USER&gt; (write ** :pretty nil) (WALK (QUOTE WEST)) (WALK 'WEST) Edit: In CLisp: [2]&gt; (game-read) walk west (WALK 'WEST) [3]&gt; (first (second *)) QUOTE [4]&gt; (write ** :pretty nil) (WALK 'WEST) (WALK 'WEST) [5]&gt; (write *** :pretty t) (WALK 'WEST) (WALK 'WEST) In CCL: ? (game-read) walk west (WALK 'WEST) ? (first (second *)) QUOTE ? (write ** :pretty nil) (WALK 'WEST) (WALK 'WEST) ? (write *** :pretty t) (WALK 'WEST) (WALK 'WEST) edit 2: added pretty print *write* The real reason is that output in Lispworks does not use the pretty printer for printing return values and SBCL does. CLisp, CCL do not care either way; 'WEST is considered pretty and not pretty. 
only rational ones
&gt; In a recent line count, Lisp came in as number 4 when it comes to number of source lines of code (SLOC) in the Debian GNU/Linux (Woody) distribution, after C, C++, and shell with some 4 million SLOC (around 4%), before Perl, Python, ML, Fortran, etc. That is hardly an unpopular language, only less popular than C and C++. I believe Woody was released in 2002, so even taking into account Debian's development cycle, the post is at least eight years old.
&gt; In fact, Scheme implementations may or may not have rationals. R6RS requires the numeric tower of all implementations, including rational numbers.
Yes. Neither R5RS (and I think any earlier version) nor the latest draft of R7RS (small language) require this however.
From FAQ &gt; Quicklisp does not use any external programs like gpg, tar, and gunzip I love this already
Now we on MS Windows get to have not only CLISP but also SBCL? Awesome
I don't.
Thanks! Very helpful.
Yeah, I tried doing some actual stuff with the input and it worked. I suppose I should have experimented more rather than immediately giving up and asking for help.
video content is 0:56 - 5:20. the last 3 and a half minutes is credits
Ohhhhhh ... yeah, you are right. That is what he is writing about. At first reading I got something entirely different from the article. Thank you very much for the explanation. My bad. By the way, agreed on all points. 
That video did not display in my browser (Firefox) and it seems like the note says that it should.
Same here.
So, just a video? No text? That seems like an unfortunate way to make an appeal to a mass audience.
Before making a donation, I would like to understand the license better. IANAL, but it is my impression that you cannot fork the project, and thus it is not something I would call open source. I am reluctant to donate money for something that may not be compatible with, say, the DFSG.
So it's not open source, and the purpose is to forbid forks.
Which might be a good reason for a independent full reimplementation (was my first idea upon reading the license).
IIRC, Tarver does everything with Microsoft tools, including web publishing, which I would guess is the cause of the Firefox issue. The Shen source also has Windowsisms, like spaces and capitalized words in filenames, and no build script or Makefile. In terms of the technical aspects surrounding Shen, almost everything is idiosyncratic and uninviting. You can't improve it by submitting patches because the repository is not public, and diffing off the release won't work because most of the code is auto-generated. It's like the project has been sprayed with developer repellent.
Please keep Science (as in programming languages) and Religion (as my personal Faith) separated, thank you. (Would you drink milk mixed with beer, even offered for free?)
This pretty much sums up why reading content that is not curated and has a large user base is extremely noisy. In order to get quality information on a topic one has to read primary sources and discern the utility for themselves, otherwise community memes are taken as truth regardless of how well they actually reflect the current state of the subject matter. A sufficiently sophisticated voting or filtering system may make this situation tractable, but currently the amount of noise either requires the moderation to scale with the user base or greater barrier to entry as current voting implementations have always been insufficient for managing noise. Reddit is a great example of this as all of the default subs inevitably turn into a wasteland and the voting system itself heavily biases content towards ones that are quickly consumed, reducing the likelihood of videos and long papers are to make it to the front page. I was horrified when /r/books and /r/explainlikeimfive became default for that reason as the amount of noise was about to increase by orders of magnitude; regrettably, so far my fears have been validated in that regard. That can be overcome in low frequency subreddits by viewing the new tab, but on high frequency posts subs that becomes nontrivial. Reposts being allowed or impossible to filter out also reduces the value of viewing older pages; ideally for myself reposts would be disallowed automatically, however posting in archived threads would be allowed while voting on archived posts would not be with some form of weighting being applied to normalize voting as the user base increases in size. The reason for this is that there are a lot of great threads with excellent comments, especially if one factors in the increase in noise over time but it is both difficult to browse through and it is impossible to vote on them due too everything being frozen. An RSS feed that displayed the entire history of the subreddit would be invaluable to me. In general, I would also like to see more domain bans that are notorious for click bait articles as a user option, essentially providing a blacklist that can be used.
hey, welcome to Lisp! Right now the usual editor for serious work is emacs, which has a very nice plugin system for Lisp. It's a bit of a pain to learn though. :-/
Actually, I'd reckon that video is the best for a pure mass audience. Hacker types seem to prefer text. /me shrug
Same. I'd never heard of Shen until today. Skipped the video, looked at the site to see what it was all about, got kind of interested in it, and then got scared off by the license. If I can't do whatever I want with the source code, I'd rather just not be able to look at it, so I'm not tempted.
I'll get into that eventually. Thanks.
Nice detailed tutorial. In the past I also used these "one-click" standalone pre-configured installers : - http://common-lisp.net/project/lispbox/ - http://lisp-in-a-box.software.informer.com/
Well, it's the ideas that are actually important. :-)
You can also let quicklisp take care of installing SLIME/SWANK.
Or they could be a little less permissive and just say that you can't advertise something as being Shen or use the name Shen to promote it unless it conforms. After all, they (should) have control of the trademark; just refuse to let people that aren't working on Shen call their stuff Shen. That is completely respecting of user freedom and removes the issue of confusion/proliferation of non-conforming implementations all the while keeping Shen under tight control by the developers. No, I think the issue is something more, something a little more controlling, something along the lines of "if you don't want to contribute my project the way I want you to, then you can't use my source for anything."
&gt; Actually, I would be interested in donating to the project with the "reward" being it is released under a better license than what it currently is under. Agreed.
the links in **How do I set up my environment?** have gone 404.
Whoops, my fault, I'll fix 'em. FWIW though, the canonical links should be: http://www.mohiji.org/2011/01/31/modern-common-lisp-on-linux/ http://www.mohiji.org/2011/01/31/modern-common-lisp-on-osx/ http://www.mohiji.org/2011/01/31/modern-common-lisp-on-windows/
Maybe this could be in the FAQ... Now that I'm talking to other people about Common Lisp, I was curious about some pronunciation. How does everyone say "defun"? I've been saying "DEE-fun". Also, how about CLOS? I've read about the two pronunciations, but wonder which is more prevalent. Lastly, mentioning how to say "cdr" would be nice in the FAQ.
deaf + un see + low + s could + r
This is very helpful. Thanks. I admire a person who understands that answering a "Which choice should I .." question with an "On the one hand .." answer is not all that useful. Better, as here, is to just say "I find this best .." or "For most people ..". 
From COMMON LISP: A Gentle Introduction to Symbolic Computation &gt; The left half is called the CAR, and the right half &gt; is called the CDR (pronounced ‘‘cou-der,’’ rhymes with ‘‘good-er’’) &gt; CADR is pronounced ‘‘kae-der.’’ &gt; The CAAR function, pronounced ‘‘ka-ar,’’
Memory-safe languages are also anti-human because they disallow programs that do not go wrong. Clearly, assembly language is more humane than Common Lisp!
semantic sentence analysis error.
I don't think I've seen the `:?foo` keyword style before. It's a little irritating to my eye.
Cool, but looks almost identical to cl-routes. https://github.com/archimag/cl-routes
no, very different from cl-routes, I think cl-routes is complex to use. All you need to do with ht-routes are mapping your routes by (map-routes ()) then get the params from *route-params* , the goal is very simple.
And thanks to the maintainers of implementations that have bridged the cross-platform gap.
There are two reasons for :?foo style. 1. It means http query. 2. If you use the editor with completion, since :?foo is not familiar as you just say, :?foo is fast for complement. In fact, I don't use :?foo style keywords, because abbreviation *?* is more useful than for *quicksearch*. e.g. (qs:quicksearch "json" :?description t :?quicklisp t :?url t) &lt;=&gt; (qs:? "json" :dqu)
Probably a buffer overrun, which proves ericbb's point. Maybe at some point in the future a buffer overrun will modify ericbb's comment such that it then analyzes without error. :-)
I think the author means that incomplete programs can nonetheless be run, allowing the complete portions to be exercised appropriately. Personally, I find it quite annoyingly painful to spend more time stubbing out static analysis failures than writing real code, and then the next day delete all the stubs when I fill in implementations -- only, I have to fill in more stubs around that implementation...
Sorry, the above example is wrong, the following is correct: (qs:quicksearch "json" :?description t :?url t :?cliki nil :?github nil :?bitbucket nil) &lt;=&gt; (qs:? "json" :dqu) P.S. I lost the password for reddit, and not registered my email address (~_~) . So, I create a new account (tkych2) .
What's significantly worser - I cannot see this video on a default F19 install.
That is indeed the point. No one writes an entire valid program from the start and stubbing is essentially introducing errors in your program.
In the general case, the number of valid/useful programs in a statically typed language is smaller, but you indeed have more safety. The more you restrict, a la Haskell, the more safety, but the smaller that subset of possible programs. Another side-effect of static typing is that the typing system itself becomes a embedded language that is run at compile time. This is of course more obvious in elaborate type systems. I personally mix in type declaration in my Common Lisp code to get a best of both worlds, sbcl does a great job warning about possible errors inferred from typing.
Does anyone know what the state-of-the-art is in terms of straddling this divide? Errors as warnings and optional types and such? I generally prefer developing using dynamic languages such as Python and Javascript, and it is invaluable to be able to experiment with the program in an inconsistent state. But as the design settles down, I find that it would be nice to be able to start introducing static guarantees that the interfaces and contracts I'm settling on are actually being respected throughout. I've looked at Typescript a bit, but haven't had a chance to really try it. Racket seems really cool with its ability to mix typed and untyped modules, but using it means totally committing to the Racket ecosystem I guess. I've recently started playing with common lisp and really like the development process of working with a running image from the repl and dynamically changing a program. I know that e.g. SBCL has fairly powerful type inference and checking. How feasible it to leverage that capability for, say, refactoring an interface and finding all the places where things are broken? I hate Java, but I have to admit it is very nice to be able to change the return type of a method in Eclipse and then just chase the red marks to find everything that needs to be changed. Really just getting a list of compiler warnings in an Emacs buffer is sufficient, but the question is whether the compiler can be made to find all the problems.
Racket would probably be your best bet. DrRacket will statically identify errors in programs while you're writing them. If you don't want to go all-in with Racket, I believe there are a few tools out there for static analysis of ordinary Scheme, and I wouldn't be surprised if they can be integrated with Emacs.
The real interesting thing is the pseudo-announce of CL-Audience. Being able to program libraries that can be called from other languages would be very nice. I suppose that this is already very possible with ECL, but implementing a method for this in other Lisps would be nice. It always bugs me that high level languages often have a one way relationship with foreign code, they allow you to call out, but not to call in.
To be used as external libraries has been a feature of Allegro CL and LispWorks for a long time. It's something that needs to be provided by the implementation. What CL-Audience might be is a compatibility layer on top. Allegro CL: Building and Using an Allegro Common Lisp Based DLL http://www.franz.com/support/documentation/9.0/doc/dll.htm LispWorks: 4.4 Delivering a dynamic library http://www.lispworks.com/documentation/lw61/DV/html/delivery-42.htm#pgfId-865189 ECL also can build libraries: http://ecls.sourceforge.net/new-manual/ch32s04.html 
I'm aware of all three of these. It's a shame this facility isn't universal. CL-Audience will be a unifying layer on top of (a) saving as library and (b) the implementation's FFI (I haven't yet researched whether CFFI etc are strong enough for what I needed).
&gt; It always bugs me that high level languages often have a one way relationship with foreign code, they allow you to call out, but not to call in. Is it something that platforms like .NET were designed to solve?
To be honest, I didn't know as I have never used it, but that is what Wikipedia says is one of its goals. My experience on providing an interface that anybody can access is basically providing a C calling interface. Everybody/language knows how to call C functions, so just find a way to package your interface as a set of C function calls and you are there. This has always been good enough for me, but then again I have never used .NET so I don't know what I am missing.
Hallo there Mr. Angry Hippy. Heads up, yes this is a Lisp forum. Unfortunately, it is a wrong forum as Autocad lisp is not Common Lisp. You will need to post that on a proper Autocad forum instead.
What can be this used for?
It's not wrong per se, it's just that it may be hard to find help here. Note: &gt;A subreddit for the Lisp family of programming languages. 
Ultimate formatting power!
Agreed.
I don't know how useful it is beyond showing that TeX is remarkable and the guy who wrote this has some serious skills and a lot of time on his hands. 
&gt; I'm not sure I would dare use dynamically typed languages, because it makes it hard to prove code correct. When you're actually *proving code correct*, I think that type correctness becomes a minor issue compared to the rest of the things you have to demonstrate.
&gt; My point was that languages with static typing guarantees that only functions with matching return types and argument types are legal to combine, Yeah, but that's an extremely weak requirement. Take a generic function, for example, that takes a sequence of items and returns another sequence of items. Now how do we know that the function actually deserves the name *sortSequence()*? *That*, to me, amounts to "proving code correct", which are your words, not mine. Mere requirement that functions return values of correct types, in the larger picture, is not much stronger than requiring that programs merely not use undefined functions. You *immediately* notice a type error. You don't necessarily immediately notice incorrect results, even though that's a [much worse](http://www.bbc.co.uk/news/uk-23233573), and [much, much scarier](http://www.dailymail.co.uk/news/article-2395964/Exposed-The-Post-Office-glitch-thats-wrecked-dozens-lives-including-postmistress-forced-admit-36-000-fraud.html) [failure scenario](http://www.computerweekly.com/news/2240089230/Bankruptcy-prosecution-and-disrupted-livelihoods-Postmasters-tell-their-story). I think that's a false sense of security.
Just because it's difficult or sometimes impossible to prove a whole program correct, doesn't justify tossing the baby out with the bathwater. I think for robust software, static typing is better than nothing at all. And as I said, in context, I didn't literally mean proving a whole program correct, only that a program follows the policy of strong typing which can be checked at compile time. So continuing making arguments about something I never meant is kind of silly. If we could easily prove non-trivial programs correct at compile-time, that would be great, but right now we can't. Until we get that far, I settle for use of static typing *and* testcases to ensure that my programs are as bug-free as possible. For prototyping and code I consider low risk, I do use languages like Common Lisp and Clojure with strong dynamic typing.
&gt; tossing the baby out with the bathwater The problem is that you're using the wrong metaphor. The use of static typing for building robust software is more like taking placebo and praying for divine help when you have cancer.
It is certainly interesting, but I don't see a reason for it to exist. As far as I'm aware, Clojure is more useful on top of the JVM. Correct me if I'm wrong.
Some people (like me) love Racket in its own right, but like some aspects of Clojure such as destructuring bind, and hashmaps as functions. With Clojure, the JVM is a blessing and a curse. In terms of performance and interop, the JVM is pretty nice. But for some tasks, the JVM is a lot of baggage to bring along. Racket starts much quicker. Finally, not running on the JVM means that the unholy amalgam of Clojure and Racket (see also [rackjure](https://github.com/greghendershott/rackjure)) can support things like proper tail recursion.
How is that different from system-apropos?
Clojure COULD support tail call optimization, it would just have been difficult. Kawa Scheme, which also runs on the JVM, has it.
Oh ok thanks. I didn't really know. 
Interesting! I'll have to check it out. I wasn't aware that proper TCO was an option on the JVM.
Why? This is pretty common scenario. I was already doing this back in the MS-DOS days, integrating C, Turbo Pascal and Assembly libraries on the same program. 
&gt; Racket starts much quicker. Only true when talking about the Oracle's standard JVM.
Unless things have changed, Kawa's tail call elimination is limited, and in violation of the standard, as described [here](http://www.gnu.org/software/kawa/Restrictions.html).
Yeah that's true. I do a lot of embedded work so c/asm is quite common. I guess I was thinking that allowing for many different languages would be interesting for development. I don't know how many times I'm writing an application in c, networking or moving tons of low level data around, when I then need to parse some data file which takes me a few lines in scheme but takes dozens of lines in c. Being able to pick a language per function, I think, would be an interesting design.
Well, you COULD use Chicken Scheme together with C. I just don't know if you could compile in your embedded environment.
Coming from the other direction, recent versions of GHC Haskell can defer type errors to runtime (with a warning at compile time).
Have problem watch videos on bitcast, after one minute it stops
try to reread your original sentence
Yep :-). I've looked for that original version and it doesn't seem to be available along with the current reddit code base. There is also Paul Graham's [hacker news](https://news.ycombinator.com/news), which is written in [arc lisp](http://www.paulgraham.com/arc.html).
Will this become a base for something like LuaTeX
Thanks it works now.
So... there is nothing to see or read or understand, without first signing up for this "bitcast.io" thing that I've never heard of before? That is disappointing, this might have even been interesting. 
Please upload this somewhere which doesn't prevent people from viewing the content.
I haven't had a chance to watch this yet, but I wanted to thank you for taking the trouble to make these videos. Over the past year or two, I've been trying to learn two languages, CL and Ruby. The Ruby community is pretty aggressive about helping people learn and level up, and it's pretty great if you're learning. I know a lot of people here already know what they're doing, and so you probably don't get as much value out of these things as I do. But if you're learning, it's pretty incredible when someone takes the trouble to make a video and posts it for free. It's enormously helpful. Thanks very much for doing this.
I'm going to assume you mean the first of the two sentences because that sentence is more awkward--and yes, I agree, it's horribly written. Here's an attempt at a minimal edit to make it more sensible: [Advocates of] memory-safe languages are also anti-human because [memory-safe languages cannot express some] programs that [never fail despite using unsafe, low-level memory features]. It's still not that clear because there's too much put in one sentence. That's why I broke it down in the follow-up comment. I'm guessing that you read it as "... disallow [all] programs ..." while I intended "... disallow [some] programs ...". That would certainly turn it into complete nonsense. If you were referring to the second sentence, then you probably didn't like the misuse of "humane". Sorry about that. I figured people would get the idea despite the sloppy wording.
Rather old :/, wouldn't recommend for modern CL webdev
a 'some' would be great.
I did not author the videos. I'm only sharing the link, because I'm interested in the topic.
With a little HTTP sniffing you can watch the 24 other minutes.
There's a book series, I don't know the URL though... The writer frequents this subreddit though, hopefully he'll pop up.
You mean ***[Lisp Web Tales](http://lispwebtales.ppenev.com)***?
I'm pretty sure that happened at a time when Reddit was much smaller and simpler. It probably wouldn't take one week if they had to do it now. It's really sad that the cross-platform deployment issues fucked up the whole thing, though.
Rather old, wouldn't recommend for modern Emacs * ps: half serious, my nostalgic side was happy to see XEmacs. Also, witnessing lisp way of doing things, even when old, *can* still teach qualities not seen elsewhere.
It's like Joy? Concatenative languages are pretty cool, albeit unintuitive to me and I assume most others who started with other paradigms.
Hey, that's not what CAR and CDR stand for!
A lot of argument over parens... I think Lisp would be the easiest language to visualize (IE: replace the functions with images... like MIT's Scratch) because of it's structure. I think it would also be easy to just draw boxes around subsequent calls in an editor kind of like LEGO blocks that you just snap together in little stacks.
Check out the [Julia programming language](http://julialang.org/), which is a high level, dynamic language that has optional type annotations. Its also blazing fast, which is a nice :)
Not that I know of. 
Someone published an e-pub version with all the "missing" illustrations. http://www.mbishop.name/post/4113599512/on-lisp-by-paul-graham-an-epub-for-your-ipad
While attempting to refute this, I decided that it mostly makes sense. But how often do you really need a DSL in preference to any normal API? 
I'd really love a hard copy of this, as reading on my computer sucks, and the kindle auto-formatter messes up code. I should look into getting it printed.
&gt; But how often do you really need a DSL in preference to any normal API? The ability to easily create DSLs is one of those things that it is hard to appreciate until you have played around with it enough to have a sense of the power that it gives you.
I piggyback'd my previous job printer for this reason. No guilt, these 300 pages were probably the first and last worthy content sent to it. In a 4 pages per sheet layout (2 recto/ 2 verso) it's packed but very readable.
I printed it 2 sided... A pity it was well before when I learnt how to format PDFs to create signatures for books (with LaTeX and pthe dfpages package)
John Carmack talked about some block-like Scheme implementation for the iPad, so there are definitely such things out there. I do however think that they don't beat the keyboard in a proper editor.
That didn't address my concern about the lack of a proper static type system in Lisp, which is what makes it a little more ugly than I'd like it to be. :(
Actually, there is a tool that calculates AST diffs for lisp, as well as a couple other languages. https://yinwang0.wordpress.com/2012/01/03/ydiff/
&gt; don't beat the keyboard in a proper editor. to be fair it's for tablets.
Has anyone worked on those ideas and produce something? They look like they would be great on tablets
Still, real life Lisp gotta come with a way to write Lisp code on computers, be it parens or visual tree stuff or whatever. Without real life complications, free market is beautiful, communism is beautiful, spherical cows are beautiful.
Yeah, absolutely. I'm just saying that the same thing on a PC wouldn't be much better.
I never really saw the DSL argument as being a very good one. You can of course use macros to build very sophisticated abstractions in Lisp, but then you're stuck with all the downsides of having macros everywhere. And if you're just writing your own normal functions, then it's really no more of a DSL than just writing Java classes and a decent set of public methods to access them. Of course, a small sprinkling of macros can be really beneficial, and The Right Way is probably somewhere in the middle, but my point is simply that I don't think the ability to make a DSL is such a huge win. To me the argument for something like lisp is simply uniformity of access. If your data is just a few simple and consistent structures -- lists and maps perhaps (as opposed to custom structs/objects for every domain), then you get a bunch of stuff for free. You can write maps, filters, reduces, etc., directly over your data and not have to bother building a DSL at all. Clojure takes this philosophy to another level, where pretty much everything is just a seq under the hood. Want to do something to every node in an XML document? Just map the do-something function over the document directly. I think of it as being an easier route to what the C++ folks call generic programming. The STL is a brilliant piece of engineering to provide the same sort of benefits to C++, but the constraints of the language make it both harder to use and more limited.
Disclaimer: I'm a dyed in the wool Lisp programmer. I work in Lisp exclusively at my job, all my hobby programming is in Lisp and I have authored a complex Lisp dialect with macros and modules and ubiquitous pattern matching that compiles to Javascript so that I can target the browser without getting out of my parentheses. I love s-expressions. However, s-expressions are not magical. They are simply a convenient way to denote programs. In particular, they are a convenient way to denote programs such that dumb tools can manipulate those programs without having to be too smart. Emacs, using a package like paredit, doesn't understand the _semantics_ of Lisp (about which more later), but, because the representation of Lisp is sufficiently uniform, one can edit Lisp code at a relatively high level with it. S-expressions have a dark side, however, in my opinion. Because they allow you to easily support macro programming, most lisps have a macro system of some kind. Yet macro expansion, in the most general case, ruins the uniformity of the semantics of the language. In a Lisp without macros, not only can you denote any program with s-expressions, but it is unambiguous what that program means. Furthermore, it is unambiguous what a fragment of the program means. When you have general purpose macros, you can no longer count on the elements of a list having ordinary evaluation semantics because the head of the list might denote a macro which walks its body and performs arbitrary transformations before finally, perhaps, returning some code to be inserted into the program. (As an aside, though people don't like dynamically scoped Lisps with f-exprs, like Picolisp, Picolisp does have the advantage that macros are not (usually) used and the semantics of the Lisp are entirely uniform, although this advantage is mitigated somewhat by having to distinguish between evaluated and unevaluated arguments in function invocations). While the simplicity of s-expressions makes writing a sort of smart editor very easy, because it enables general purpose macros, it makes writing a _very smart_ editor almost impossible. Similarly, Lisp code is, in general, difficult to understand because any list might be headed by a macro which modifies the meaning of any contained expressions. HOWEVER, in practice, this hardly matters. Like any programmers, Lispers know the limitations of themselves and their language (or learn them rapidly), and so code-walking macros that significantly alter the semantics of their sub-expressions are exceedingly rare. I belabor all these points only to make the following capstone: Lisp's real selling point is not its beauty or nearness to the lambda calculus or whatever, in my opinion. Lisp's real selling point is its simplicity and no-nonsense approach to programming, which gives the user lots of power to solve problems in an environment which is more or less set up to make that power manageable and comprehensible. That is it. Lisp is awesome, but not magic. 
You're probably right. But it goes right back to "adds nothing to my non-lisping understanding." But then I don't expect to teach people how to ride bikes without them trying it first... 
You might appreciate the generic SEQUENCE that SBCL provides :)
&gt;Lisp's real selling point is not its beauty or nearness to the lambda calculus or whatever, in my opinion. Lisp's real selling point is its simplicity and no-nonsense approach to programming, which gives the user lots of power to solve problems in an environment which is more or less set up to make that power manageable and comprehensible. The one thing that stopped my forray into Lisp was the naming scheme (ha, scheme). In the sense that it felt like *everything* had a name. I feel it traded *actual symplicity* for *syntactic simplicity*, although this is obviously partially because I was exceedingly new. The memory burden it places on you is astronomical with regards to other languages I know, which completely overrides the benefits for a newcomer like me. Note that I've long stopped learning Lisp as I didn't actually enjoy it enough. Compare these three codez for the same game. All were the first I found for each language; | Language | #lines | #keywords | #keywords/line | |:----------------------------------------------------------------------:|-------:|:---------:|:----------------:| | [Python](http://forum.codecall.net/topic/50892-tic-tac-toe-in-python/) | `78` | `27` | `0.35` | | [Lisp](http://home.online.no/~jpthing/games/tictactoe.html) | `197` | `52` | `0.26` | | [Ruby](http://www.jisaacks.com/ruby-tutorial-make-a-tic-tac-toe-game) | `245` | `47` | `0.19` | (A "keyword" is a unique element to learn) Bear in mind that `#keywords/line` isn't a good statisctic as most of the keywords are first brought out early on, but neither is `#keywords` alone. Also bear in mind that `#lines` reflects how complicated it has been made – *the Ruby script is colorized*! **None** have been tested. It does not have anything to do with the expressieness of the language. The problem is that when Python's keywords are like: "&lt;=", "=", "==", "&gt;=", "[XYZ, XYZ]", "and", "def XYZ(XYZ):", "else:", "except:", "False", "for XYZ in XYZ", "if XYZ:" And Ruby's were a touch harder with: "*", "+=", ".chomp", ".downcase", ".each", "@XYZ", "[XYZ, XYZ]", "case", "class", "XYZ ? XYZ : XYZ", "XYZ[XYZ]", "{ |XYZ| XYZ }" ...But Lisp's are like: "'(XYZ)", "'XYZ", "array-rank", "char=", "check-type", "defconstant", "deftype", "defun", "destructuring-bind", "make-array", "row-major-aref", "setf", "terpri" **WARNING:** The above has selection bias to make a point. See below for fairer lists. As someone who isn't amazing at memorising things, this is a deal-breaker. It's just painfully hard to get around Lisp's ancient model for things. What the hell does "terpri" mean? "char="? "destructuring-bind"? *Note: these are rhetorical questions.* --- This wasn't meant to be a rant, but it's become one. Let's just say that although List *sounds great* in principle, in practicallity it's easier to get into Haskell. --- Unedited lists, please correct me if I've lumped, say, user-defined things in the Lisp pile. It's much harder to tell what's user-defined in Lisp, so it may well have happened. | Lisp | Python | Ruby | |:---------------------|:-----------------|:-----------------------------| | `#\-` | `!=` | `"#{XYZ}"` | | `&amp;XYZ` | `"XYZ"` | `#XYZ` | | `'(XYZ)` | `%` | `'XYZ'` | | `'XYZ` | `-=` | `*` | | `+XYZ+` | `/` | `+=` | | `:XYZ` | `&lt;=` | `.chomp` | | `;` | `=` | `.downcase` | | `= ` | `==` | `.each` | | `and ` | `&gt;=` | `.flush` | | `aref` | `[XYZ, XYZ]` | `.include?` | | `array-dimensions` | `and` | `.keys;` | | `array-element-type` | `def XYZ(XYZ):` | `.length` | | `array-rank` | `else:` | `.purple .neon .gray .green` | | `char=` | `except:` | `.select` | | `check-type` | `False` | `.to_sym` | | `clear-input` | `for XYZ in XYZ` | `:XYZ` | | `cond ` | `if XYZ:` | `=` | | `defconstant` | `input` | `==` | | `deftype` | `not` | `&gt;` | | `defun` | `not in` | `@XYZ` | | `destructuring-bind` | `print` | `[XYZ, XYZ]` | | `dolist` | `range` | `case` | | `dotimes` | `return` | `class` | | `eq` | `True` | `def` | | `equal` | `try:` | `do │XYZ│` | | `every` | `while XYZ:` | `else` | | `force-output` | `XYZ[XYZ]` | `end` | | `fresh-line` | | `gets` | | `integer` | | `if` | | `lambda` | | `nil` | | `length` | | `print` | | `let` | | `put_line` | | `list` | | `puts` | | `loop` | | `rand` | | `make-array` | | `rand` | | `member` | | `require` | | `nil` | | `return` | | `or` | | `STDOUT` | | `prompt` | | `then` | | `push` | | `unless` | | `return-from` | | `when` | | `row-major-aref` | | `XYZ ? XYZ : XYZ` | | `satisfies` | | `XYZ...XYZ` | | `setf` | | `XYZ[:XYZ]` | | `string` | | `XYZ[XYZ]` | | `terpri` | | `{ │XYZ│ XYZ }` | | `typep` | | `{XYZ:XYZ, XYZ:XYZ}` | | `unless` | | | | `values` | | | | `when` | | | | `write-line` | | | | `write-string` | | | Using `│` to stand in for `|` to make the table not break. I've grouped `.purple`, `.neon`, `.gray` and `.green` because it's not fair not to.
&gt;Also, you can modify the AST, so it's more like extending a compiler and creating a language than solving a problem in a language. [...] &gt;And if you don't like it, change it; it's still lisp. You don't need permission or to change any standards because the lisp is meant to be modified. That's a good thing for my OS. Why is it for a language? It's back to the "code is data, so we can treat code as data and edit it suchforth". The question is why I should care. --- &gt;Then there is the Read-Eval-Print-Loop, which is the difference between driving a car remotely to actually being in the driver's seat. As a Pythonista, I agree. 
With some time I might answered a bit longer, but cog2011 is completely right: these programs are doing something different. The Lisp program can actually play Tic Tac Toe and the python program just manages the inputs and display for human players. It's natural that a program, which actually plays the game is a bit more complex. Still the code is not really idiomatic Common Lisp. One can easily write a much simpler program, which does the same as the Python program.
I sometimes use one I wrote, [here](https://github.com/jaeschliman/com.clearly-useful.generic-collection-interface), but I'm not satisfied with it yet, so it's still in flux. I use the develop branch, not master. It's still too complicated for my taste, I'm looking to give it a healthy reworking later this year.
Heh, so does the Ruby version. If it matters to you, [here's a version for Python which has AI, at 187 lines](http://inventwithpython.com/chapter10.html). Again, I just chose the first I found. No selection bias; I can't judge whether Lisp or Ruby programs are written well so shouldn't do so for Python ones. The thing is, that Python version still has very few keywords relative to Lisp, and they're much more memorable. Once again, though, this doesn't make Lisp a bad language. It's just harder to *learn* than I'd like. Plus /u/commonslip pointed out that this is a comment on, well, Common Lisp and not *Lisp itself*.
If it wasn't clear, I just want to bring attention to a specific line of mine: &gt;Also bear in mind that `#lines` reflects how complicated it has been made – the Ruby script is colorized! None have been tested. It does not have anything to do with the [expressiveness] of the language. I wasn't making a claim about Python being terser or easier. Just that its keywords were fewer and better named. The rest of my response is in the reply to /u/cog2011.
Sorry about the tabs/spaces. Emacs has spoiled me.
Yeah, a simpler program uses fewer keywords. You know Python and stuff looks obvious for you, but there are many things in the small Python program which are not obvious: indentation is meaningful, scope, only simple vectors, ... I'm also not convinced that a mix of strange characters is easier to learn than a speaking symbol.
A great source for your own personal "Lisp epiphany" is the book "Lisp in Small Pieces".
&gt;&gt; Also, you can modify the AST, so it's more like extending a compiler and creating a language than solving a problem in a language. [...] And if you don't like it, change it; it's still lisp. You don't need permission or to change any standards because the lisp is meant to be modified. &gt; That's a good thing for my OS. Why is it for a language? It's back to the "code is data, so we can treat code as data and edit it suchforth". The question is why I should care. Peter Seibel's [Practical Common Lisp](http://gigamonkeys.com/book/) has a great example of this in [footnote 5 of chapter 7](http://www.gigamonkeys.com/book/macros-standard-control-constructs.html) (emphasis mine): &gt; DOLIST is similar to Perl's foreach or Python's for. Java added a similar kind of loop construct with the "enhanced" for loop in Java 1.5, as part of JSR-201. Notice what a difference macros make. A Lisp programmer who notices a common pattern in their code can write a macro to give themselves a source-level abstraction of that pattern. A Java programmer who notices the same pattern has to convince Sun that this particular abstraction is worth adding to the language. Then Sun has to publish a JSR and convene an industry-wide "expert group" to hash everything out. That process--according to Sun--takes an average of 18 months. After that, the compiler writers all have to go upgrade their compilers to support the new feature. And even once the Java programmer's favorite compiler supports the new version of Java, they probably still can't use the new feature until they're allowed to break source compatibility with older versions of Java. **So an annoyance that Common Lisp programmers can resolve for themselves within five minutes plagues Java programmers for years.**
OK, that's really cool. Both the page and the footnote. One question though; from what I understand Ruby's blocks give you a similar sort of power to define your own constructs. They allow you to implement while loops, for loops, context managers etc. Do you know much about these, and how do they compare? In my opinion they look a little simpler but they obviously don't allow the same sort of DSLs that Lisp's macros do. 
Oh, I forget to suggest that you try Racket. If Common Lisp seemed a old and crufty to you, Racket might be more your style. It is a very modern lisp. Working with Common Lisp as I have done for some time now, I've come to see a certain degree of wisdom in some of its rough edges, but I've always liked the clean design of Racket, which I take to be closer to "Lisp Nature", whatever that is.
Look, a lot of these are just incredibly easy to learn or can be avoided (such as terpri). The difference between "=" and "char=" is probably more obvious for a non-programmer than someone with experience in other languages, *of course* "=" compares numbers and "Char=" characters, that makes a lot of sense. EQ, EQUAL, EQUALP I agree on, though there is a damn reason that there are a lot of equality function (namely, equality is hard). Tbh, I find syntax to be much harder to learn than descriptive symbols. Also, considering that you talked about Ruby, aren't there like 5 different ways to create anonymous functions, where the objects returned differ in very small ways? That's a cognitive load, for sure. I'm not saying that your general point is wrong, I just think that your examples are crap. Especially the tic-tac-toe one.
You can take a project I'm working on and have worked on for almost a year now: async programming in lisp. All async is done either in threads, coroutines, or callbacks. Threads are too heavy for the most part, CL doesn't have coroutines, so the simplest interface left is callbacks/CPS. Callbacks are hideous and tend to really make code ugly. Thanks to some counseling I got on this /r/lisp, I was able to make a [futures implementation](http://orthecreedence.github.io/cl-async/future) that bring a near-native experience to async programming. Without being able to create a DSL around async programming using futures, everything I would have built between then and now would be a disgusting, tangled mess. Do futures and their supporting macros have limitations? Of course. But they save me time and effort, and anyone who reads the docs for 5 minutes can get a feeling for what any of the code is doing. This is a case where extending the syntax of the language expanded the language's abilities beyond what they were before, and doing so made code more readable and understandable.
Dunno: I haven't taken the time to learn Ruby yet. :)
The difference between implementing a new control structure with lambdas versus macros is mostly syntactical, and the lambda might have a bit of runtime overhead versus a macro. Basically, it's the difference between a built-in and a function implementing a control structure.
&gt;Look, a lot of these are just incredibly easy to learn or can be avoided Maybe for you. I didn't find it so. &gt;EQ, EQUAL, EQUALP I agree on, though there is a damn reason that there are a lot of equality function (namely, equality is hard). Equality's not that hard. `==` and `is` in Python cover all *my* use-cases. &gt;Also, considering that you talked about Ruby, aren't there like 5 different ways to create anonymous functions, where the objects returned differ in very small ways? That's a cognitive load, for sure. I don't actually know Ruby. Sure, that does sound like a difficult thing to learn. You'll probably only really care later-on, though, so it's no show-stopper. 
Maybe I've just not done enough work with futures (I haven't; I have no idea what I'm talking about) but [Python's concurrent.futures](http://docs.python.org/dev/library/concurrent.futures.html) and it's upcomming Tulip library (which has basically no documentation as of now) seem to be quite pretty to me.
Thanks. I've learnt quite a bit today, quite good for a first forray into /r/lisp. 
LIL - Lisp Interface Library; SBCL supports generic SEQUENCE:s
I use Metabang as my preferred LET replacement for sophisticated destructuring. For an abstraction over lists/vectors/dicts/etc, I don't offhand know of one. Certainly it would need to be hooked into an iteration solution.
For CL, the most popular for general purposes seems to be SBCL, it seems to me.
Depends on your environment. I don't use Scheme much, but if I do I use Racket. For Common Lisp I suggest too take a look at Quicklisp and use any implementation that supports it. If you keep your code portable the implementation doesn't matter much and you can even have several implementations and switch between them.
I've been playing with Racket lately, and have found the language nice and the documentation cohesive and discoverable. Also, there is a [typed racket](http://docs.racket-lang.org/ts-guide/) that will give you some of what you got from Haskell's static type system.
Based on what you wrote any of the CL implementations in the sidebar should work for you. The common lisp standard is pretty broad and the implementations are pretty close to 100% compliant. * SBCL for linux and for speed (this is what I use) * CCL for mac and windows * ABCL if you want to run on top of JVM. * ECL if you are interfacing to C * CLisp if you are a ~~gnu-fanatic~~ gpl-fanatic. Use the REPL from the command-line. Edit: fixed CLisp based on comments below.
Metabang's (g w king's) [cl-container](http://common-lisp.net/project/cl-containers/) is pretty complete. I have used it in situations where I was too lazy to write my own. It is certainly not canonical or nor has been recently or actively updated. But then these are the types of things that are written well once and used forever.
CLisp is also integrated nicely with Readline, so provides a great starter REPL.
CLisp is a bit of a rare bird, but I wouldn't say it is only for GNU-fanatics. I am a GNU-fanatic, but I don't use CLisp very often. CLisp is, to this day, the one implementation that I *know* I can get running just about anywhere. It also starts fairly fast and has a much smaller memory footprint than SBCL (and yes, I do get sys-admins asking what this 100+MB SBCL thing is that I leave running in screen). CLisp also has a crap load of extensions. Since I stick to portable CL for the most part, these aren't too interesting to me, but I can imagine that having an interface to MATLAB, 4 different databases, the PARI computer algebra system, or libSVM would be nice if I didn't want to work anywhere but inside CLisp. Also, CLisp has arbitrary precision math via GMP (i.e. faster than rational math in, say SBCL). That can be nice as well. I'm not saying that CLisp is the Lisp to use, but it certainly has its place. Now, attempting to use GCL, that would be what a GNU-fanactic would do :) (actually, I just checked, GCL is showing more life than CLisp. The last GCL release was 3 days ago!)
Forgot to mention, documentation for cl-rw is not ready yet, but it is possible to clone it: $ git clone http://logand.com/git/cl-rw.git 
Don't forget that CCL is also available for Linux. I prefer it over SBCL to be honest.
I used to like CLISP because I could build it anywhere, but in the past few years it's stopped building everywhere, and it's sometimes a pain to get its prerequisites. I just failed to build it on Mac OS X 10.8.4 because it couldn't find stdbool.h and it pestered me to install libsigsegv. It does not use GMP. SBCL can optionally use GMP now.
It also has very nice error messages - which is why I choose it over SBCL at the console.
and sbcl runs on mac and windows. On the one mac in the household I use only CCL. On linux I have them all installed but I use SBCL first then CCL second. Windows I used to use CLisp first but now I use CCL more often.
Why do you prefer it?
You are correct. At work, I used to keep CLisp open to do quick calculations or quick hacks. sbcl can use [linedit](http://common-lisp.net/project/linedit/) though it is not built in which is probably not great for newbies. Fun fact: Readline is why [CLisp is GPL](http://www.clisp.org/impnotes/faq.html#faq-gpl)
Thanks, I'll try SBCL and Chicken then. Is there an IDE around other than Emacs?
&gt; It does not use GMP Oops, I guess I just assumed due to the excellent performance that it was something prepackaged and not hand rolled. Perhaps long float math is just that much more performant than rational math. &gt; SBCL can optionally use GMP now. I heard rumblings about this; I'll have to try it out.
So, Eclipse it is. :D I don't think I could ever use either Vim or Emacs.
SBCL+linedit is pretty easy to set up with quicklisp in about three minutes.
I've had really bad experiences with linedit on SBCL. I'd get stuck in a condition and couldn't get out.
very roughly: acl run-time speed feels similar to ccl; sbcl compiles to significantly faster code, but sbcl compilation-time is very slow compared to acl or ccl. all those lisps are fast enough though for most things. acl is very solid across platforms but sbcl can be fragile. i have no idea about lispworks
I haven't run into any situation yet where I needed to determine which of the three are faster than the others. On that note SBCL has more support on Linux than on Windows. LispWorks has a much easier to understand license which is more permissible when it comes to selling a finished binary than what I can understand of Allegro's license.
I ran into situations where SBCL did produce faster code than LispWorks. It also has more complete compilation annotations. However LispWorks can also generate very fast code out of the box and is generally more solid. It also has a tree shaker which comes in handy when you want to deploy your application. The 64 bits version is significantly faster than the 32 bits one. 
It seems like every time this comes up the Eclipse integration gets mentioned, but usually nobody that is party to the discussion uses it; they mostly use emacs+slime, or maybe a few use the CCL mac IDE or one of the IDE's shipped with a for profit implementation. With that in mind I would like to list a few things that I think are important in a lisp environment, so that you can at least have an idea whether Eclipse is giving you some of the basics. In addition to all the usual niceties (argument list hinting, auto-complete, jump-to-definition, compiler notes underline offending line in source file, etc.) you should expect at least these features: 1. A command to send a redefinition of a form from your editor to the running lisp image. You will be doing this a lot so it needs to be comfortable. (CL's tend to be built with the assumption that you develop by changing the running lisp image rather than restarting it.) 2. Comfortable interaction with the debugger. At a minimum it should be easy to see the available restarts, browse the stack trace, and quit back to the top level repl. 3. A newline+indent function. Also a function to re-indent the entire form. This should keep you from being burned by misplaced parenthesis: if one is misplaced you will immediately see the indentation is off. 4. A convenient way to bring commands previously sent at the repl back so that you can edit and resend them. 
On my Intel Macs, LispWorks 64bit is usually a bit faster than SBCL. LispWorks 64bit is expensive, but it is roughly twice as fast as the 32bit version. Sometimes one needs more type declarations for LispWorks, than for SBCL. The latest version of LispWorks also can explain missed optimizations - similar to what SBCL does. The runtime is also quite a bit better (a precise GC) with more deliverable options. SBCL has some advantage with compile-time type checking.
After reading this I think you would be horrified to see the conditions I'm usually programming under. For example, for Haskell: notepad++ and WinGHCi. Then again, Haskell isn't a forest of parenthesis... So, would you recommend that I learn Emacs now or later, when I know Lisp better? (Assuming Eclipse is not "complete" enough)
SBCL's KnowledgeWorks is way slower.
To be clear I haven't used Eclipse with CL. It's hard for me to make a recommendation because emacs+slime is quite nice but you may not want to get gummed up by learning (or choosing... make a choice and jump in;) an editor before you've even had some fun with the language. If you make the choice to start with something simple or familiar, though, be aware that if you find yourself asking "Why do lispers put up with doing this thing that is painful?" the answer may be that it's just not painful in the typical lisp environments. I've seen people get particularly tripped up over lacking points 1 or 3. 
In my experience, LispWorks is pretty generous about evaluation licensing (NOT the limited trial thing) if you contact them directly and ask for something reasonable.
&gt;Can you take two arbitrary Ruby blocks and compose them into a new block? Obviously you can chain blocks and do composition, as they're functions, but you seems to be talking about taking two half-blocks and combining them into a full, working one. Do you have an example of when you might want to do this, or have I misunderstood you? 
If you consider a parser-generator language like yacc, you have the yacc syntax generally consisting of something like: nonterm : '(' someoperator someoperand ')' | someatom ; A yacc implementation will produce a default output for some given parse, but you usually want to project the parsed data into your own data model, say, written in the language you'll manipulate the tree with. Yacc can handle projections in { } after the production rules. nonterm : '(' someoperator someoperand ')' { $$.operator = $2; $$.operand = $3 } | someatom { $$ = $1; } So what is the syntax inside the { }? Well, that depends on what language you are projecting to, and each yacc implementation will require a different parser to understand it. Each parser will be essentially written from scratch, because grammars are typically strongly coupled, where the main/entry nonterminal of the grammar actually depends indirectly on every other nonterminal in the grammar. You might be able to share a few small productions between yacc implementations, but not much. The language you wish to project to may not be expressible with the existing yacc syntax either, because it might produce an ambiguous grammar. In that case you'd need to modify either the yacc syntax or the syntax of the language you're projecting into (although the language you're projecting into is modified slightly to introduce the splice syntax ($1 etc).) If you write a parser generator and a projection language in lisp, you needn't worry about syntactic ambiguities - there may be ambiguities in vocabulary which can be easily resolved with a rename or alias, or using qualified namespaces, include guards or whatnot depending on your lisp implementation. It's much easier to resolve than having to change the syntax of your language, or introduce some context-sensitive parse tool. Projecting into another s-expression DSL is really simple, as lisp already has the required tools for you - quote, unquote, splice, etc.
that's purely amazing. 
This looks great!
Hey warweasle! How have you been? You should stop by #lispgames
Ah, X protocol tricks I see. Won't work on Windows then, not that I mind. ;) Too bad I don't have a full slime, the real magic is this little bit, I think: (slime-edit-definition name) I use chicken-scheme.el, which is sort of a slime alternative for Chicken Scheme that makes it less annoying to work with finicky native code that tends to crash your scheme. ;) 
this is right up my alley! I've been daydreaming about doing something similar myself for awhile now. looks great!
There are several 'free' alternatives to KnowledgeWorks. We are talking about rule-based and/or logic-based systems. KM, Powerloom, Lisa, ... 
Franz does that, too.
also: [cl-javascript](http://marijnhaverbeke.nl/cl-javascript/)
I'm not certain I follow, but that sounds cool.
&gt; acl run-time speed feels similar to ccl Are you sure you're not doing something wrong? ACL and SBCL ought to be roughly similar in speed.
Before everything was .js, it used to be .lisp. /me twisting Dick Gabriel's words.
Waldemar Horwat also wrote TMON, which was my absolute favorite low-level debugger on the Mac way back when. Parts of TMON were also written in Lisp. 
The biggest benefit I have found is that I can first open a text editor, type in my data and tag various parts of it, and then work to make that data compilable. This is a different workflow than trying to create a parser to load your lisp-like xml and then run each bit. The "each bit" IS code. Its very easy to write tools in ANY language that outputs compilable lisp, and since you can simply overwrite the old functions in any lisp runtime environment hash table by reloading a lisp file, bam, now your "code is data" is running a second time, and can update the values of things in your already running application. That could be 30 seconds shaved off of a quick iteration of some value. Remember, all this and the lisper didn't have to write a parser or include a XML/JSon/S-exp library or anything.
I'm not convinced by that; Pythonistas often just dump JSON, capitalise the `true`s and `false`s and load as-is. We can stick functions in the middle, too, to "tag" things if need be, although it doesn't seem like a very Python-y thing to do. 
I can't speak for non-game applications, but part of the bigger picture is that by default, one references things through the environment, which would be some kind of lookup by ID hash table with references to all global variables and global functions. I mimicked this in my C++ game engine, and it makes stateless management of resources much much easier. By stateless management of resources, I want the game engine to be able to stream audio, textures, and chunks of game world to and from main memory at will, without the game code knowing anything about how that stuff works, no configurability whatsoever. This is a pain in the butt in languages where Object A = (Object)B means that B is now retained by the GC and erroneously referenced instead of the new B from the updated lisp file.
Can you give me a code snippet? Your description confuses me a bit.
Well, the thing is that stuff like KM may be better than KnowledgeWorks. CAPI vs. McCLIM, right. But some institutions like ISI or CMU put a lot of effort (and DARPA money) into AI tools. KnowledgeWorks itself is fine, but there are maybe two dozen alternatives to it and some might even be better (more modern, more capabilities, ...). KnowledgeWorks did not have many users, AFAIK. Ford used it. KM was used by SRI in projects which were precursors to Apple's Siri, for example.
The key is ASDF. Quckproject generates a basic ASDF setup and quicklisp is a dependency/package manager that can load it using ASDF. So if you used quickproject and create the project directory ~/quicklisp/local-projects/my-project/ you can load it from the repl with (ql:quickload "my-project"). I'm assuming you have already setup quicklisp in your environment.
Quicklisp and quickproject work best if you're already pretty fluent in how to use Common Lisp and you want to make some common tasks faster.
I wouldn't discourage someone from trying but xach is right that you do not only really *need* to use either if you are just starting out.
I get a lot out of macros because I wrote a custom object system with them for blocky's game engine. And I usually write game-specific macros that simplify/shorten code. Soon I'll be dovetailing blocky more properly with CLOS and then i'll have nice multimethods too. Conditions are great when you ship an application, because the end user gets coherent information and at least some options. In my case, choosing Continue enabled the game to run for users on certain Linux distros where SBCL couldn't find a library. LOOP is great because I do a lot of iterating. Although I do use functional techniques and closures quite a bit. As for debugging, the biggest thing that's helped is plain old TRACE and ASSERT, and splitting up complex functions. 
I'll keep you informed :)
First, I would ignore that clack file. It is doing something more complicated than you want. I think you misunderstood Svante's answer. You want something like this: (defmethod perform ((op test-op) (system (eql (find-system :nclack)))) (asdf:load-system :nclack-tests) (nclack-tests:test-runner)) Unfortunately, this will not work because it is a classic chicken and egg problem. The compiler reads and compiles the whole s-expression. But the compiler reads and compiles but does not evaluate *(asdf:load-system :nclack-tests)*. So when it reads and compiles the sub s-expression *(nclack-tests:test-runner)*, the package *nclack-tests* does not exist. So it cannot intern the symbol *nclack-tests:test-runner* and the read and compilation fails. The following is what you want: (defmethod perform ((op test-op) (system (eql (find-system :nclack)))) (asdf:load-system :nclack-tests) (funcall (find-symbol "TEST-RUNNER" :nclack-tests))) because: * the string "TEST-RUNNER" can be read and compiled * the symbol :nclack-tests can be read and compiled, because it is interned in the keyword package. * therefore, *(funcall (find-symbol "test-runner" :nclack-tests)* can be read and compiled. * *(funcall (find-symbol "test-runner" :nclack-tests)* is "equivalent" to *(nclack-tests:test-runner)* * when perform is called, *(asdf:load-system :nclack-tests)* will be evaluated. The package *nclack-tests* will now exist and the function *nclack-tests:test-runner* will exist. Now *(funcall (find-symbol "test-runner" :nclack-tests)* is evaluated and the function will be called. edit: forgot to upper-case "test-runner"
Thank you very much. Why does the string has to be upper case to work? Is it related to trying to preserve case for strings when converted to symbols?
&gt; (find-symbol "TEST-RUNNER" :nclack-tests) That should be (find-symbol (string :test-runner) :nclack-tests)
Thank you :) 
It's a shame [geiser](http://www.nongnu.org/geiser/) doesn't work with Chicken. Maybe one of these days.
RIP "Kitten of Death"
Because some folk take themselves way too seriously.
The Puppy of Doom ate it!
IIRC the message was non-suppress-able which meant if you saved the image for a deliverable executable the message would pop-up on start-up of the executable.
It was pretty trivial to patch it out.
How is this better than syntax? I personally like s-expressions, but I don't see how this is better than, say, Python syntax.
It's not necessarily better in all cases, and I like s-expressions too. But with this you'd be able to write with parentheses syntax, with the tree structure, or with both. It all maps to the same lisp source.
Interesting. I thought this was a dead project, and everyone moved to ECL meanwhile. A major disadvantage over other implementations is, as far as I read, that it is not fully ANSI compliant yet. Are there any advantages of GCL over the other CL implementations? For example, can it be built for llvm-gcc, for example, to have an llvm-aware CL implementation? How is the library support? Is it worth looking at?
Well, it used to have MPI bindings, which I would be interested in. Not sure if they still work. I have been attempting to build it. It seems to take a very long time and last attempt failed. Edit: Looks like the ParGCL bindings don't exist anymore in the project, or maybe they were always a separate project... (there is a directory in the repo called pargcl, so maybe it works?)
Similarly, I saw a system do this: (eval (read-from-string "(nclack-tests:test-runner)")) 
For further explanation, look up the CLHS Chapter 2 and 23: http://www.lispworks.com/documentation/HyperSpec/Body/02_.htm http://www.lispworks.com/documentation/HyperSpec/Body/23_.htm
Ops, you are right. I read both notes but send the wrong one. Anyway, I suppose the news are that the project came back to life.
Still 1.1.8 for Windows... dammit.
Still not very close to ANSI (even when requesting an ANSI build). I doubt it will load many libraries unless they are hand tuned to work with it. It is a shame, but it looks like things are roughly the same as they have been for years. Maybe this new-found life will lead the project to better places. Maybe it can find a new niche. Its old niches have already been taken over by other Lisps: Maxima, Axiom, ACL2, and presumably everything else that people use build on other Lisps now (I believe), and ECL is the modern descendant of AKCL. As noted elsewhere, if they get ParGCL included and has enough ANSI compliance to work with 90%+ of the libraries out there, I will at least be playing with it and may try to use it for work.
The eclipse plugin seems to work just fine, at least for my basic needs. The only problem I have is that the SBCL it installed is 1.0.37, not 1.1.8, which is strange. Maybe I picked the wrong package. I'll look into that. I think I'll try the Scheme plugin as well.
I second CCL. This is being written from an ARM PCDuino where I have it up and running. Here are the instructions: http://www.raspihub.com/go/f5780dbf11dabc60771e67b357ae947bc6b3fd87f35d5f38e7d511ff88e08d0c
Gambit is available on Android. Search the Play store.
I managed to run ECL in Android, but it needs to be rooted, and a shell. 
Would also suggest reading http://john.freml.in/re2-benchmark
Go for it. Tree based source editing is the future and we need people like you to make it the present. The lisp world is already living in the future as usual but it is good to have alternatives.
I am curious whether either of cl-irregexp or re can outperform cl-ppcre on the regexp-dna benchmark...
I am going to vote for doing a Kickstarter to bring SBCL to Arm with Android + Blackberry 10 + iOS ports. I understand someone has been working on the SBCL --&gt; Android port on their own time. I wonder if we can raise a large chunk of dev money to get them to finish the project + create some of the specific deployments for each ecosystem.
*Except* on ARM! :)
Do you know of someone with both the ability to finish this and the ability to convert money into time? It seems like that is not always a common convergence.
Perfectly cromulent point. I would imagine there are a few consulting firms available to complete the work and perhaps convert it to a subscription model for ongoing support. I see the occasional consulting firm looking for Lisp work here or there. But then, I could be entirely wrong.
&gt; I am going to vote for doing a Kickstarter to bring SBCL to Arm with Android + Blackberry 10 + iOS ports. Wouldn't CCL be a better idea? After all, it has *already* been ARMed, and it has a somewhat lower footprint.
I can't help it but think that a specialized DNA regexp engine might probably be even faster. You know, two bits per symbol, asm code generation, the works. Using eight bits for symbol when you need two wastes caches.
&gt; Gingko’s tree structure is ideal for writing Lisp without having to worry about parentheses. Why are parentheses a worry?
mocl is a CL that runs on Android, although there is not currently an on-device REPL. It does have a minimal REPL that simulates device characteristics (e.g. 32-bit cpu) though. https://wukix.com/mocl
[Ranger](http://ranger.nongnu.org/screenshots.html)... for Lisp. Make it more Ranger-like and you'll have a winner. At least *Ranger*'s a winner. It's a shame only ArchLinux-ers seem to know about it. 
All of Sabra's surveys of different sets of Lisp packages have been wonderful. Especially since Quicklisp has leveled the playing field for installing packages, it's all the more valuable to have meaningful information with which to choose between them.
Was reading this just yesterday http://axisofeval.blogspot.fr/2013/04/a-quasiquote-i-can-understand.html
The part 0 about syntax is mostly wrong: * S-expressions are a Lisp data syntax, not the syntax of the programming language Lisp * *There are atoms and s-expressions.* Wrong. S-expressions are historically recursively defined to be either atoms or pairs of s-expressions. * Whether something evaluates and to what, is independent from s-expressions and their syntax. * Grouped s-expressions are also not called 'forms'. That's complete nonsense. A 'form' is a Lisp expression as data in memory, which is meant to be evaluated (a symbol, a self-evaluating data object, a function form, a macro form, special form, lambda form). Since functions like EVAL and COMPILE expect Lisp forms (and NOT Lisp s-expressions as textual strings), forms are the input for those. In part 1 it should not be FUNCTION and its OPERANDS, but FUNCTION and its ARGUMENTS. I would also not use the word 'manually'. *Strings/vectors are fixed-length arrays of characters* - not necessarily, we can also create adjustable strings/vectors. *A function always returns the value of its last expression.* Not true. See RETURN-FROM, THROW, .. It also returns zero or more values, not necessarily one value. 
Why should I subscribe to a newsletter without even knowing what it's all about except for a video about a single piece of software? OP, if this is your site, please clarify a bit more or at least give a sample newsletter to check out.
First of all don't hesitate to [contribute](https://github.com/adambard/learnxinyminutes-docs). I think it necessarily doesn't have to be precisely right. With the sentence: &gt; There are atoms and s-expressions. I don't see any trouble (especially for early newbie), because they are there (the word *fundamental* is bad chosen though). &gt; Strings/vectors are fixed-length arrays of characters Yeah, I've just added section *adjustable vectors*.
&gt; Atoms are also s-expressions. Yes, but I don't have a problem with enums which contain a certain thing explicitly together with the specialized thing of the previous one. However, their relationships should be mentioned, but in a light form. A visitor wants to know how to add numbers already. &gt; Pairs are not even mentioned. I don't know if the pairs fit to the purpose of learnxinyminutes. Since they are fundamental they should be at the beginning, but it seems to me as a big overhead to visitor's mind. Visitor wants to read it quickly and therefore a section about pairs should be further from the beginning, but it's in discrepancy with the fundamental point. *To the added rest:* From that it can be crystalized a wisdom which should be mentioned on that site. Of course, I agree there shouldn't be untruths in learnXinYminutes, it just must be written in very briefly form.
Possibly because of fogus's positive reputation.
hi, I'm responsible for that article. I chose to be loose about the fine details which *generally* don't matter in actual coding. I considered that this site was for a quick reference/quick overview to get a sense of the language. What a s-expr is does not, in my judgement, create different mental models of coding sufficient to cause bugs. I believe that unneeded detail causes unneeded cognitive load on the learner, as much as the expert prefers it. As I mentioned in the earlier Reddit thread where someone requested Learn Common Lisp in Y minutes, I did not even care to talk about conditions; I don't know how to treat them in appropriate concision and lucidity for someone unfamiliar with Common Lisp. I consider it better that the total novice come away with an ability to quickly write code, with a sufficiently useful mental model, *even if parts of the model are wrong and simplified*. I *also* note that the site is driven by github and thus interested parties can send in updates and corrections. 
Since there is some skepticism (rightly so I suppose), I'll take a moment to clarify the intent, distribution model and give a high-level summary of the first installment. After that, you can either subscribe or not based on the facts. I'm working through the first issue of Read-Eval-Print-λove and am basically done, but have a bit more research to do around Common Lisp's macrolet (I have the first 6 installments outlined also). However, the first thing I can say about it and future issues is that it will *not* focus strictly on any particular member of the Lisp family. Additionally, I will at times (once per issue most likely) cover other languages in the Little-language category. What does that mean? I don't know yet, but again, that's not the primary focus. Most examples and programs will likely be in one of Common Lisp, Clojure and R7RS because I'm most familiar with those, but I'll expand to try and learn more myself. Every issue of Read-Eval-Print-λove will be made available through this mailing list for FREE and through the http://www.readevalprintlove.org website, but I'm also exploring optional payment offerings through LeanPub (https://leanpub.com/). This payment option would be "pay what you wish, with a small minimum" and is intended to offer alternate formats only (i.e. epub, mobi, PDF). The content on the site/list and there will be exactly the same. I'm not doing this to get rich. A high-level overview of the contents of the first issue are as follows: * Every issue will start with a Koan-style dialog (with accompanying artwork) between the Master Macrologist Π無 and one or more of his students (macrolytes). These will usually Lispy in nature, but not always, but in any case will set the theme of the issue. * I'll then talk about what defines a Lisp. * I'll also expand on an old idea that I explored before named Fluchtpunkt Lisp * I'll include a section (in the works) with the tentative title "A language that doesn't let you affect the way it thinks is not worth growing" * Finally I'll try to end on a fun note including goofy pictures and bombastic statements about the computing industry and programming languages. So if the above sounds appealing then by all means subscribe away, but like I said the site will have the content as well, so it's your prerogative. 
Well, I don't know if "no sense" is the right view, but in any case as I mentioned in my comment that will be only one delivery mechanism. As for piping through Planet Lisp I don't know if that's appropriate since Common Lisp will be only one of the languages being covered.
&gt; known as quasiquote I am confused by many terms that pop up when we talk about macros: backquote, backtick, quasiquote. Do they all mean same? Are they interchangeable terms?
How do I make something return zero values?
 (values)
Yes, *basically*. Based upon Duckduckgo's ability to observe and my super scientific reading of the first 10-15 results back... Quasiquote seems to be the scheme term, backquote to be the common lisp term, backtick doesn't seem to be confined to any particular Lisp. Scheme macros are not Common Lisp macros, so quasiquote-as-scheme-means-it may not actually be backquote-as-common-lisp-means-it; that is, the semantics will likely vary. Glancing at Racket (a Scheme). But they all mean ` textually. :-) 
Almost irrelevant, your lisp side is quite known, but I saw you were moderator on r/osdev, will there be some cross related topics ? low level bare metal lisp systems etc etc.. Just curious.
Conceptually, all values in Lisps are references. Everything is passed by value (copy), but the arguments that are copied are just pointers. In an impure dialect, doing it any other way for mutable values will result in differences. In a pure dialect, you might affect asymptotic efficiency.
Some CL texts use the term quasiquote as well; I suppose there is no real consensus on which is the most appropriate jargon. I would use unhygienic and hygienic macro terms rather than scheme macros or common lisp macros to refer to a particular macro implementation in a dialect. In practice the difference between the two is immaterial though.
&gt; would a program evaluate to a different result had I taken the traditional approach? If you add support for mutating assignment (i.e. `set!` in Scheme) then, yes, your approach will return different values. Until you have mutability, there's no way to distinguish between pass-by-reference and pass-by-copy.
**Edit TL;DR:** You won't have a problem with it until you start mutating the inside of objects or having variable mutations &amp; closures together. And I digress a lot about implementing Lisp/Scheme. :) Even if you're going towards mutability later, I still think the best option (in terms not only of performance, but in ease of implementation) is to do what the vast majority of modern programming languages do: Pass everything by value, but most values are references. Then you can special-case odd needs like true "pass-by-reference" cases, variable mutation, object mutation, etc. Of course, if you already have it developed and it's just for education purposes, you might not want to change it. It will get you odd results as soon as you start having any sort of object mutation (for example, what happen to other places that stored a reference to a cons cell after you do something like mutating the value pointed by its car? (setcar, set-car!, rplaca...)). But when you combine simple variable mutation (set!, setf, setq...) &amp; lexical closures even the simpler way of "pass references by value" starts getting trickier. If you're interested in continuing the project and want to read a bit more about it, I'd suggest three great resources, two big text-books and a small thesis: 1) Daniel Friedman's awesome [Essentials Of Programming Languages](http://www.amazon.com/Essentials-Programming-Languages-Daniel-Friedman/dp/0262062798), though I find it a little expensive for hobby purposes. 2) Shriram Krishnamurthi's [Programming Languages Application And Interpretation](http://cs.brown.edu/courses/cs173/2012/book/book.pdf) 3) Kent Dybvig's incredible [doctorate thesis on Scheme implementations](http://www.cs.indiana.edu/~dyb/papers/3imp.pdf). The three of them talk about implementing a Lisp using a Lisp, but the knowledge you get from each can work when dealing with C, too. Especially, I'd recomment Dybvig's paper. It's simple, direct and goes right into the details, explaining how stuff works. Oh, I just remembered something that I have no idea how I forgot. [Nils Holm's Scheme 9 From Empty Space](http://www.t3x.org/s9book/) Is the greatest how-to-implement Lisp in C text I've ever read and it's not too expensive. There's even a previous version available for free in [his Attic](http://www.bcl.hamilton.ie/~nmh/t3x.org/zzz/). Well, I intended to write a quick &amp; dirty response and posted a big wall of text. Sorry bout that, but I get carried away when it comes to Implementing lisp (I also have a **very** shitty implementation of Scheme in C in my [github page](https://github.com/alexandream/scheme-interpreter), but I doubt you'll be interested in that). I found most of these resources after implementing Lisp myself, though, and I recommend reading them to avoid *many* stupid problems that one'll find while working in it.
Maybe at some point yes. I want to do some Lisp archaeology in the future and would love to get into some flavor (ha!) of Lisp Machine Lisp. I have no solid plans however.
&gt; Conceptually, all values in Lisps are references. Everything is passed by value (copy), but the arguments that are copied are just pointers. Pass by value where most values are references is sometimes called "pass by sharing" to differentiate it from the pass by value you get in C++ unless you explicitly ask for a reference/give a pointer.
Just driving by to upvote you for mentioning Dybvig's thesis, I loved it. Should I get Essentials of Programming Languages if I also get carried away about implementing Lisp?
 (let* ((a '(b c d)) (cdra1 (cdr a)) (cdra2 (cdr a))) (eq cdra1 cdra2)) Your system says false, pretty much every other Lisp says true. EQ is pretty basic, and it's pretty much based on object/cons-node identity.
If you don't mind watching video lectures, I, being the cheap ass I am, would rather check out first Krishnamurthi's Programming Languages Application And Interpretation along with [2012 video lectures](http://cs.brown.edu/courses/cs173/2012/). Although he discusses typed language implementations a lot it's still a great resource. Essentials Of Programming Languages is **awesome**, I can't recommend it too much, but I find it a little expensive for a hobby. Though that's probably because my country's currency towards dollar is a on 2.5x-more conversion. If you don't mind spending the 45 dollars for it, or can get it used or whatever, I'd definitely read it. It's one of the most solid works on the theory behind implementing a programming language I've read. On the low level implementation area of it (like, implementing it in C or something like that) Nils Holm's is a good read, as is [David Gudeman's paper on Dynamic Typed representation of values](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.4394&amp;rep=rep1&amp;type=pdf) albeit a bit old. You might want to check [Nan-Boxing (aka Nan-Tagging)](http://wingolog.org/archives/2011/05/18/value-representation-in-javascript-implementations), for that, too.
Yeah I might see if I can get that since my French is good enough for most technical writing. You don't have to be that great at a language to read technical documents. Maybe this can be your motivation to learn another language!
&gt; Essentials Of Programming Languages is awesome, I can't recommend it too much, but I find it a little expensive for a hobby. ... If you don't mind spending the 45 dollars for it, or can get it used or whatever, I'd definitely read it. I got a used 2nd edition copy off Amazon for $7, shipping included. (You have to be careful about buying used books off Amazon: sellers rarely/never list the edition or printing.) There's a 3rd edition out though. http://www.eopl3.com/preface.html suggests the changes aren't huge.
Yeap. I've read both, actually. 2nd E on my college library and borrowed 3rd E from a teacher later to skim. There are differences, but 2nd E is definitely more than enough if you can find it cheap.
This seems like a *REALLY* horrible idea. It's cool for hack value, but there's a reason that nobody else does this. Does the compiler at least emit a warning when the programmer did something so blatantly wrong?
&gt; It's an interesting concept, ie do what I mean, not what I say It will do what you mean **only** if you din't say what to do. Compiler will never attempt to be smarter than you. Even if you use a number where string expected, compiler should issue a warning instead of "fixing" this. &gt; pretty dangerous, especially considering the fact that function arguments in javascript are all 100% optional You are right and because of this MetaJS has required and optional parameters (that may have default values), for example (defn arg-demo (a b:? c:42 &amp; more) [a b c more]) You can verify on http://metajs.coect.net/tepl/ that this code is translated to: var argDemo = (function(a, b, c) { var more = Array.prototype.slice.call(arguments, 3, undefined); c = ((typeof(c) === "undefined") ? 42 : c); return [a, b, c, more]; }); MetaJS will try to resolve *only required* parameters, i.e. do what you said it to do. It will not try to find optional parameters in the execution context. For example, look at the modified example from [metajs.coect.net/#magic](http://metajs.coect.net/#magic): (defn named-id (id name opt:?) (log id ":" name)) (defn logos-demo (id) (let name (str "Name of " id) opt "Keep me private" (named-id))) Local variable `opt` from `logos-demo` function will not be passed to `named-id` function, but `id` and `name` will be. If you missed a required variable and MetaJS can't find it in the call context or there more than one possible solution, you should see a warning. Anyway, thanks to you and @Ubertekk for the feedback.
&gt; Does the compiler at least emit a warning when the programmer did something so blatantly wrong? Exactly. MetaJS will resolve a function argument only if it's declared as required but missed in the function call. It will emit code only if there is only 1 possible solution. Otherwise it should act as git when merge conflict is detected -- ask programmer what to do (issue a warning). Later I plan to add more safe type check, for example issue a warning if number is used when a list is expected. 
I updated [metajs.coect.net/#magic](http://metajs.coect.net/#magic) example of a function parameters resolving to address issues noted by @Ubertekk and @orthecreedence.
Excedrin, I didn't use Zephyr or Gale, they looks a bit outdated. However thanks for the suggestions, it's worth to learn their experience. Let's keep this topic for MetaJS-related questions only. I created dedicated topic about [Coect.net communication protocol](http://www.reddit.com/r/AskReddit/comments/1ludij/is_one_social_network_enough/) on the AskReddit.
Of course other people did do this: DWIM is almost 50 now (sadly, Warren Teitelman died recently)
 Hmm... I never considered studying the CLR. I must check it out later. Our side projects objectives seem to be very similar indeed. Possibly in the future the projects can trade some paint.
I agree. It reminds me a little bit of Scala's implicit parameters. In that case you need to explicitly mark both the parameter and the variable in the calling scope as 'implicit' to activate it, which makes it at least less error prone. But it's still a "spooky action at a distance" operator, which always seem nice at first but turn out poorly in the long run / with larger systems. (see also dynamic scoping)
Very interesting, thanks for the explanation. That makes much more sense. Looks like you really put some thought into it.
It seems like you actually largely mitigated the problems I mentioned pretty well. Upvote for you :)
I would live to see that happening, but in a sane manner: remove cruft, rationalize existing concepts, integrate only mandatory new concepts. So, how do we start?
One would probably start by: 1. Putting together a team of people that actually knows their stuff (people from the commercial implementations and people from the free implementations) 2. Putting up proposals for what should be changed/improved 3. Let the smart people evaluate it all Really, I dunno how this should I happen, I'm mostly wondering why it can't happen/why it shouldn't happen/why it hasn't happened yet.
I think that there is already such an effort, called Common Lisp Document Repository (or CDR for short): http://cdr.eurolisp.org/ &gt; There have been a number of attempts to establish a standardization process for Common Lisp after it has been officially published as an ANSI standard. The ANSI standardization was very costly and very time consuming (according to http://groups.google.com/group/comp.lang.lisp/msg/15248a1b11c5a603 it took nearly 10 years and at least $400K). &gt; The goal of the Common Lisp Document Repository is to be more light-weight and more efficient. We focus on one aspect of standardization: the ability to refer to a specification document in an unambiguous way. &gt; The Common Lisp Document Repository intentionally does not define a process for coming up with specifications or any other means to guarantee some level of quality of the submitted documents. Instead, we aim for a community-driven, decentralized approach to come up, discuss and finalize specifications. In this sense, we only provide the services of librarians. &gt; We hope that the Common Lisp Document Repository has the potential to prove useful in establishing new de-facto standards, and to serve as a stepping stone for more formal standardizations in the long run.
Thanks. In current state MetaJS is still more preview and proof of concept but I plan to make more usable for real use cases by the end of month. 
&gt;Just pick one de facto standard implementation, say SBCL, and make it BDFL. As soon as we have one "leader", the others will simply be forced to follow. Nothing will actually make anyone follow anything... Perhaps what we need is simply something like "Currently Recommended Libraries" on CLiki but expanded further?
I would like to see the [iterate construct](http://common-lisp.net/project/iterate/) included in the standard and its use encouraged over loop. Also, it would be interesting to have a little language for loops which mimics ANSI C's for syntax exactly, complete with pointer manipulation (even if such pointer management is only emulated by the macro). Such a macro would make it easy to recycle examples from algorithm books.
Peter Seibel's talk about how the original standardization worked may shed some light on why it's not happening now. https://soundcloud.com/zach-beane/peter-seibel-common-lisp
There was a huge difference between the first and second standardization efforts. The original j13 meetings succeeded because 1) there were strong leaders that were respected by the group (Gabriel/Steele/and some others), and 2) the majority of the people voting were implementors or had been at one time. The second effort, around 2000, was marred by only having Franz and LispWorks represented and the rest of the people were users that had no idea what the cost of implementing their ideas. There was a lot of tension between the implementors and users, and the users just didn't get why the implementors were upset at the direction. Also, another big factor: back then, the Lisp companies had the resources to do the **huge amount of work to become compliant**. That is not the case now. It wasn't the case in 2000, either. 
I want to add another dimension to this debate, and I'm guessing it won't be popular in this subreddit... As an implementor, we get suggestions all the time from users that should never be implemented. Why is that? I mean, they wouldn't want to mess up the language they know and love, right? It's because they only have an interest in *their problem* whereas I have an interest in *everyone's problems*. There is a huge difference between someone from the outside of a system and inside. I think the Linux kernel is a great example. Linus and his minions spend lots and lots of time telling people why their ideas and patches are stupid and will never get into the kernel. This is very time consuming but essential. Without it, the kernel would become a steaming pile of shit and completely useless to anyone. It's what makes Linux really great. It's what allows us to trust each Linux version will be better than the last. Who will do this for Lisp? The implementors. Allegro CL, LispWorks, SBCL, CMUCL, and some others. If you want a new Common Lisp that is truly standard and is really better than what we have, you need a bunch of implementors and a few users. The key is you need more than just one implementation represented, though. I don't know if the Lisp ecosystem is healthy enough for that to happen. EDIT: spelling
CCL is working on my Samsung chrome book (on arch). Downloaded the binary and it just worked. 
One has to be realistic about the community capabilities. But some stuff can grow over time. What I would find interesting: * free the CL documentation or if that fails write a new one. The idea would be to use a documentation wiki which could be edited by interested individuals. The documentation then should be free and open. Implementations could use it as their data which could be used for documentation strings, web-based documentation, argument lists, declarations, ... * the documentation should provide more real-life examples, which also could be used as code templates in IDEs * this documentation Wiki then should also be used for main libraries in the way. I would think that getting a handle onto the documentation problem would be very useful. The current ANSI CL spec is static and locked into ownership of a successor organisation of ANSI. 
I've asked a few people on this topic and consensus is that dpans3 is a good starting point for creating a derived work. 
For those who don't know what dpans3 is. There were working versions and near final versions of the eventually ANSI CL spec. dpans3 is such a version before ANSI released the official specification document. This or another was also used to create the web versions of LispWorks, Franz, ... Kent Pitman wrote the tools and created the HyperSpec. ANSI CL as a document costs money. The standard documents: INCITS sells the printed version: http://www.techstreet.com/incits/products/56214 ANSI sells a PDF version: http://webstore.ansi.org/RecordDetail.aspx?sku=ANSI+INCITS+226-1994+(R2004) IIRC both are of bad quality (photo copy). So the people you talked to, were confident that it is legally possible to create derived works of it? 
&gt; Nothing will actually make anyone follow anything... ...including standards
So you mean they both need standards, since both of them now have to support multiple interoperating implementations eh?
They are horrible quality. Do not purchase them. See this article, [The Official ANSI Common Lisp Standard](http://symbo1ics.com/blog/?p=431), for details.
[CL-LOCATIVES](https://bitbucket.org/tarballs_are_good/cl-locatives) is a library that can essentially emulate pointers, but not pointer arithmetic.
There's already a comment thread here about CDR. Search for "CDR".
A few points: * A language whose semantics doesn't change under your feet is a good thing. * Quicklisp has effectively canonized some libraries as standard e.g. Bordeaux threads may as well be considered the ANSI Common Lisp threading library. Alexandria, closer-mop, usockets, cl-fad, cl-ppcre etc. * This makes the real ANSI Common Lisp just a nice small unchanging core. There's a lot of positive things to say about such an extensible language and defacto standards. It really is a good fit. So I'm quite happy with the situation. I also understand that some things are underspecified (so everyone implements, but implements it differently) and other things are raised to too high a pedestal (e.g. the controversial (which I love btw) LOOP macro), and could be libraries instead. So some conservative changes to ANSI would be nice too.
I wasn't aware of this second effort in 2000. I take it you were involved? What were the show stoppers? What were some of the things you thought were good ideas?
I think SERIES, LOOP and Iterate are all reasonable approaches to high level iteration. However given the fact that there is no visible consensus, it would probably be more prudent for future standardization to either * leave things as they are, or * remove LOOP from the standard (So it would be removed from the common-lisp package by conforming implementations, and I imagine it would quickly become a library which would leverage existing implementations' implementations where applicable and provide a default otherwise) Replacing it with another high level iteration library which everone would squabble over wouldn't accomplish much.
The programming language `ADD 1 TO COBOL GIVING COBOL`.
Yes. 
Yes, I was there. One thing that the users desired to standardize was multiprocessing. This was a perfect example of the tension between users and implementors. You either take the Symbolics spec from the 80's and standardize on that, or you wait for a clear winner among implementations and use that. There was no clear winner, and the at least one of the two implementations represented pointed out they were moving toward SMP and there would be changes needed for that and it wasn't a good time to take their implementation as a reference. So, would it have been a good idea to take a 20+ year old specification and make that the standard, when SMP was looming for everyone? EDIT: as for good ideas... honestly there were none. It was an orgy of what users who had never been in charge of an implementation would love to do, and none of it was a good idea.
That is true but there is an incentive to follow a standard when you were a part of forming it.
Just ridiculous. Reopen the standard because we want to remove LOOP. To replace it with a nearly equivalent facility based on a slightly improved implementation of the same 30 year old concept. Which is ubiquitously available o anyone by (quickload :iterate). I suggest while we're at it we should revisit the color coding standard for wire-wrapped backplanes because I think It would be easier to srervice them if we did. I went into a big thing about half a page up about concepts such as eliminating the cons cell, The pretty fundamental rethinking clojure has done to reduce the concept of all aggregates to the most elegant minimalist basis of the SEQ and the REDUCTABLE. And how that level of fundamental rethinking provides the solution to the modern issues faced by CL. Eg, global stop-the-world lock approach to CLOS implementations. Lets face it... 35 years later I'm starting to think they're not gonna figure out how to get rid of that lock without *perhaps* possibly considering some ideas along these lines. 
Okay, interesting points. I am reading a lot of pointing at the brilliant ideas in Clojure. Can you take what you said in revers and propose a list of changes to the Common Lisp standard (via say the SBCL implementation as an example) and how these ideas would solve various problems? If you were to redo the standard, had a 10 million budget, could start with a blank sheet or work up from the current Common Lisp spec, etc, what would your improvements look like? What benefit would they provide? How much quicker / less buggy / more platform agnostic / fully parallel could I spit out code? What other justifications might there be for changes? Lastly, how would you propose that it does not become just another Shen? In other words, how would the ecosystem move from the current spec to the new one as a natural progression, or would it be such a hard line in the sand it would break everything? // Edit: Not disparaging Shen. I am sure that if I was smarter it would warrant it's merrits. 
together with the wrong font for body text I also find it difficult. I need to make the text larger or use the Web browser's 'Reader' mode.
 Clojure 1.5.1 user=&gt; (defn height-opinion [name &amp; {height :height}] (if-not height (str "I have no opinion on " name ".") (if (&lt; height 6) (str name " is short.") (str name " is tall.")))) #'user/height-opinion calling it with the wrong keyword: user=&gt; (defn testme [] (height-opinion "Chris" :depth 6.25)) #'user/testme user=&gt; 'oops oops No checking. SBCL: * (defun height-opinion (name &amp;key height) (if height (if (&lt; height 6) (list name 'short) (list name 'tall)) (list name 'dont-know))) HEIGHT-OPINION * (defun testme () (height-opinion "Chris" :depth 6.25)) ; in: DEFUN TESTME ; (HEIGHT-OPINION "Chris" :DEPTH 6.25) ; ; caught STYLE-WARNING: ; :DEPTH is not a known argument keyword. ; ; compilation unit finished ; caught 1 STYLE-WARNING condition TESTME * I know which 'more powerful' arguments I prefer. 
Good point, I was thinking only in terms of expressiveness and not safety.
One could work around this using [Clojure's preconditions](http://blog.fogus.me/2009/12/21/clojures-pre-and-post/). But apparently, so I was told, this is not default behavior due to flexibility/speed reasons and/or because of lack of good support for keyword args in the JVM. Anyway, very valid point. Will be way more careful with keywords args in Clojure from now on.
CLUEL sounded like a really good idea. Too bad it got abandoned. 
It seems like it would be an extra, organized layer atop Common Lisp. I couldn't figure out how that actually would improve learning Common Lisp.
&gt; Common Lisp is basically a simplified Lisp Machine Lisp, with the compromise that it could be efficiently executed on 'stock' hardware. Common Lisp is lexically scoped.
By default. I mentioned that. Btw., Lisp machine Lisp had closures. Thus supported lexical scoped functions.
For me the big killer issue is that Lisp has terrible, terrible debugging facilities. STEP is largely undefined and varies widely in capability and function from system to system. TRACE is practically useless as a tool. BREAK must be inserted at code-writing-time: there is no way to insert beakpoints at runtime. To make matters worse, SLIME has hideously primitive GUI facilities for debugging subsidiary lisps. Basically nothing works: you can't set breakpoints, you can't step into, over, etc. functions. Functionality is declared but unimplemented for everything from SBCL to CCL to MCL to CMUCL to CLISP. This is supposed to be the open source recommendation? To say nothing of decent profiling in any system (yes, even SBCL), much less a consistent API. In this day and age, Java is kicking Lisp's butt in this regard. 
Hmmm, my understanding was that Lisp Machine Lisp only really had hacks to permit lexical closures. This seems to be a big, big, big jump to CL's lexical scoping.
I must disagree to some extent. For about 95% of problems, the error's backtrace seems to lead to the solution of the problem. For another 4%, a `format` finds the issue. Then for 0.5%, `trace` does what I need. Then for the last half-percent, I feel I need more powerful debugging facilities. This may be a result of the differences between the way we develop or the kinds of applications we develop. I will say that open source Lisp's don't seem to compete very well with commercial offerings in terms of debugging. LispWorks, for example, has very good debugging capabilities in my experience. Regarding profilers, SBCL seems to have a good variety, both a standard and statistical sampling profiler. They've worked well for me. Also, I rarely need to invoke those because SBCL provides good enough hints about where my code will probably be slow.
What's the state-of-the-art in graphical debuggers for CL? EDIT: I can't find much that visually demonstrates this. One page for LispWorks: http://www.plasticki.com/show?FIP And one for Allegro: http://www.franz.com/products/allegro-common-lisp/acl_ide.lhtml (Be sure to expand the "Source Level Debugger for Mac, Windows, and Linux" section.) EDIT2: Franz also has [a video about their single-stepper](http://www.franz.com/services/conferences_seminars/source-level-debugger_webinar_2-25-10.lhtml). It's extremely verbose. The actual stepping is at timecodes (roughly) 13:00 - 19:00, 27:30 - 27:50, and 29:45 - 30:50.
&gt; For about 95% of problems, the error's backtrace seems to lead to the solution of the problem. For another 4%, a format finds the issue. Then for 0.5%, trace does what I need. Then for the last half-percent, I feel I need more powerful debugging facilities. For what percent of the problems do you not *need* more powerful debugging facilities, but would use them if they were available?
I have respect for the fact that the author is attempting to solve their complaints through their CLUEL project. I do, however, take issue with some statements made in the article. What are these social problems? What do these technical papers have to do with the aforementioned social problems? No. CL is not subsisting. It's improving, thanks to the efforts of the implementation developers, the SLIME developers, Zach Beane, Edi Weitz, Pascal Costanza, Marijn Haverbeke, Peter Seibel, Hans Hubner, and the countless other library developers who contribute. What does it say about other languages that still do not have the "antique technologies" Common Lisp has had for almost 20 years? Generic functions, macros, reflection, multiple dispatch, closures, optional typing, correct implementation of lexical scope, dynamically scoped variables, multimethods, metaobject protocol, static analysis of dynamic language features through a locked namespace, conditions &amp; restarts, etc. The perfect Lisp will never exist. These are programming languages, and although Lisp has a well designed family of languages, trade-offs still must be made. Do you prefer axiomatic computational abstractions? (Scheme) Do you prefer pragmatic engineering considerations? (CL) Do you prefer a concurrent, functional, JVM based language? (Clojure) Note that you don't have to use the entirety of the language, either. If you want to program using a minimal amount of language features, program in CL like it's Lisp 1.5. Also note that concurrency can be had through libraries in CL, and functional programming only requires you discipline yourself not to use side effects. Use Armed Bear and you'll also have the JVM there for use. Scheme is approaching where CL was 30 years ago. R7RS-big is an admittance that although a small understandable core is appealing, the problem arises that this core, because it can be understood, is then used to implement greater and greater abstractions, such that the languages features are pushed to the extreme, and left wanting. This, I think, was what was stated in the Worse Is Better paper. I don't particular like that term as it seems derogatory to me, especially because I like Scheme, but it's meaning is I hope understood. Lackluster? Fixer-upper? Some of the best of CS have contributed to Lisp. They're thoughts and ideas live on through CL. I've already mentioned the features missing in other languages. I don't believe the merits are strong, I know they are. Momentum comes from libraries, which has improved significantly in the last few years, and continues to. Common Lisp is implemented on a clean core. It just has a rather large library which comes with it. I think this is a combination of culture shock from an ALGOL programmer, combined with a let down with regards to the Lisp hype that has occurred over the years. These sort of posts are very common for Common Lisp, like "Monads are like..." articles are for Haskellers.
He's not talking about editing code at runtime, he's talking about adding breakpoints at runtime *without* editing the code. &gt; As far as I know, you can't edit code in runtime in Java. Both the JVM and the CLR support that.
Helpful tutorial. Thanks!
You might want to consult the manuals. &gt; TRACE is practically useless as a tool. Wrong. &gt; BREAK must be inserted at code-writing-time: there is no way to insert beakpoints at runtime. Wrong. * (defun foo (a) (+ a 1200)) FOO * (defun baz (a) (foo (* a 10))) BAZ Now you use `TRACE`, the `practically useless` tool, to set a break on function entry. Without having the source code. * (trace :break t foo) A case of RTFM? 
&gt; He's not talking about editing code at runtime, he's talking about adding breakpoints at runtime without editing the code. There is no practical difference as long as you can restart the commutation with ease.
Well, some people say that logging is the right way to debug, because it promotes proper, scientific workflow: one is supposed to formulate and test hypothesis until he finds root cause. On the other hand when people have access to interactive debugger they tend to use it in a haphazard way: randomly stepping through program, trying to find what's wrong with guesswork. To be honest, I don't have a strong opinion here... I used MSVC++ IDE before switching to Lisp, and lack of 'proper' debugger was a shock for me. But then I've learned to live without it.
This is part one of a multi-part series I am writing. It covers the basics of concatenative programming and the implementation of a parser to translate strings into a symbolic representation of flat code. To add more context: Gazelle is a transcoder which translates a Lisp dialect which is very close to Javascript into Javascript. It supports a module system for macros, patterns and values, and limited macro and pattern hygiene via explicit support for referring to objects inside modules. It's major problem at the moment is that the language is implemented in Emacs Lisp, and, as a consequence, the macro-expansion language is Emacs Lisp. While Gazelle macros are scoped to the module they are declared in, and can be exported and renamed at import into other modules, other Emacs Lisp code that supports macro expansion is, at the moment, free to just float around in the Emacs Lisp global namespace. I find it conceptually unsatisfying, on top of the above, that the macro-expansion language is not itself Gazelle. So I am in the process of porting Gazelle into itself. Implementing a reader for `flat`, a simple concatenative language, is a kind of dry run for implementing a Lisp reader in Gazelle, which will be instrumental in bootstrapping. I am flirting with the idea of building a Scheme-style hygienic macro system into the self-hosted Gazelle, but that would necessitate a more complex parsing scheme, I think. If anyone has any suggestions on the subject of representing code as `syntax-objects`, rather than just sequences of some kind, and how this impinges on parsing the source, I'd be happy to hear them. I am really just feeling around in the dark here. 
*Somewhat* lower footprint might not be low enough. Common Lisp is just too large to run efficiently on a mobile device.
Maybe you can start web server in separate thread.
Oh, that's perfect. Exactly the issue I had. I found it weird that no example talked about this problem, and that's why :)