Wow so many manual and help commands. Thanks for the info!
`info shopt` tells me info: No menu item 'shopt' in node '(dir)Top'
If you give the args the same name as the resulting json wants, you can do: jq --arg name "$COMMIT_NAME" \ --arg commit "$COMMIT" \ --arg crap "$CRAP_SCORE" \ --arg date "$DATE" \ --arg coverage "$COVERAGE" \ --arg complexity "$COMPLEXITY" \ '.[$name] = {$commit,$crap,$date,$coverage,$complexity}' \ myfile.json &gt; myotherfile.json
 #!/bin/bash git push --set-upstream origin pvnb6 Save that as /usr/local/bin/gp-pvnb6 as root, do a `chmod +x /usr/local/bin/gp-pvnb6` as root. That is about it. Not sure if that is what you want. If you want an alias, use that. Also show us what you have tried already.
\#!/usr/bin/env bash $(YOUR\_COMMAND | tail -n1)
What about having text before or after the spinner? Would like to use it with text after the spinner and some "DONE" text when it has finished.
And what about array's? &amp;#x200B; `SPINNER_PARTS=("[ ]" "[= ]" "[== ]" "[=== ]" "[ ===]" "[ ==]" "[ =]" "[ ]" "[ =]" "[ ==]" "[ ===]" "[====]" "[=== ]" "[== ]" "[= ]")`
Git can be configured to do this by default, you don't need a Bash script.
This works for me with an array ``` SPINNER_PARTS=("[ ]" "[= ]" "[== ]" "[=== ]" "[ ===]" "[ ==]" "[ =]" "[ ]" "[ =]" "[ ==]" "[ ===]" "[====]" "[=== ]" "[== ]" "[= ]") MAX=$((${#SPINNER_PARTS[@]} - 1)) INDEX=1 printf " " while ps a | awk '{print $1}' | grep -q "${PID}"; do printf "\b\b\b\b\b\b%s" "${SPINNER_PARTS[INDEX]}" INDEX=$(($INDEX + 1)) if [ "$INDEX" -eq "$MAX" ]; then INDEX=0 fi # printf "\b%s" "${SPINNER_PARTS:INDEX++%${#SPINNER_PARTS}:1}" sleep .05 done printf "\b\b\b\b\b\b[${green}DONE${reset}]" ` ``
You don't need to write the branch name. Here's a neat trick: `git push origin -u HEAD`
Thanks
look into `tee`
*script* could also be useful, if temporary file is acceptable.
I figured it out. It was because I was grepping the incorrect word. Should be FAIL not FAILS.
autocd was only added in bash 4.0. Check if you have such a version with `bash version`, you might be running the default bash of mac which is ancient.
Take a look at [this SO discussion](https://stackoverflow.com/questions/2224613/how-do-you-use-the-pattern-option-of-xmllint). It may be helpful here.
You’re 100% right, I am using the Mac default. I’ll look into updating it, thanks man.
To be perfectly honest, the bash manual is actually a really good source of material. (`man bash`) Ideally what you could do is find a scenario that you'd like to solve then with the help of the bash manual actually implement the solution.
Take a look at https://github.com/nvbn/thefuck. It helps you fix commands that you missed something on, quick look through the source shows that it does this as well.
https://youtu.be/zWVV31NYi1U This tutorial is perfect for learning Bash Scripting
Another thing that taught me bash a lot was the Linux OS itself. It's peppered with good quality bash scripts. All you have to do is open then up and figure out stuff you've never seen before. Admittedly, needs some basics before you do this. But the previously mentioned tutorials should help. Then there's also the likes of LinkedIn learning (formerly Lyda.com) or udemy who offer reasonably priced courses. They are usually better overall against free ones.
Try "help" too ! Maybe try it before man.
I wouldn't. manpages are for quick reference, mostly when you know what you're looking for. Other forms of documentation are better for if you want a bit more background about a piece of software. Unfortunately a lot of software only has manpages. At any rate, simply reading documentation is a pretty terrible way to learn how to use a computer.
The primary Bash documentation is in Texinfo format. You can [read it online](https://www.gnu.org/software/bash/manual/html_node/) or with a local Texinfo viewer (e.g. `info`). The `bash(1)` manpage does not actually include everything that's in the Texinfo documentation. Moreover, it's arranged in a completely different way: it prioritises "how to execute the `bash` command" over "how Bash's features work".
Gotcha, I’ll do that first then.
Thanks for the comparison. I’ll read the online version first.
&gt; I have a decent (I think) enough understanding of it. I'm not exactly a beginner but I would like to become more proficient in it. I have read through some of this site ... &gt; [Bash Scripting Tutorial - Ryans Tutorials](https://ryanstutorials.net/bash-scripting-tutorial/) &gt; and it seems quite decent. Have any of you used it before? I did a quick perusal of it, and it looks like garbage to me. I recommend the [BashGuide](https://mywiki.wooledge.org/BashGuide) and the other resources at the wooledge wiki instead.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
When I started my Linux journey, I been told to most important command to start with was; " man -k ". There you have the same advice that been very helpful to my learning curve back then.
[Shellcheck](https://www.shellcheck.net/) would find things to improve with your script for sure. And please reformat your code it's currently very hard to parse for a human.
I use Mac, but thank you for the advice regardless
`man` can be really tough for new learners, especially since man uses some vim shortcuts that are not obvious for beginners. `man` also tries to be terse, which means you will quickly run into something you don’t understand and will have to go to the web anyway. So, like others have said, stick to the web while learning, but when you do get comfortable enough to start using man, lookup some tips to help you. The biggest on is I suggest learning to use / (slash) to initiate a word search, hit enter when done typing your search term, and then you have to use n to go forward and N to go backward in that search, and ESC clears that search.
I will do exactly that, thank you for the advice!
That's a nice tool never used it before, thanks. &amp;#x200B; Tried to make it more readable. What else could I do to make it more readable? Is it common to one blank line after `fi` ?
``` echo $something | command ``` Is most often an anti-pattern when in bash you can do: ``` command &lt;&lt;&lt;"$something" ``` You can also mix and match constant strings and variables; instead of: ``` foo="$bar"_"$baz"_"$quz" ``` You can do: ``` foo="${bar}_${baz}_${quz}" ``` As your script is very straightforward I can't really think of another approach that'd be significantly better.
One thing you can start with is regex matching, to check the correct input and format on the date while true; do read date #now check for a valid date if [[ $date =~ ([0-9]{4})_([0-9]{2})_([0-9]{2}) ]]; then #bash rematch capture the patterns in () so the first is the year, second month and third day year="${BASH_REMATCH[1]}" month="${BASH_REMATCH[2]}" day="${BASH_REMATCH[3]}" break else echo wrong date format, please try again fi done You can even match that the day is correctly in the past and it's not 9999_99_99 But I can't write that on my phone Regex is a magic tool to use with bash
Thanks for your answer I really like the way you can fetch the inputted string by position inside Regex. That's genius :D
Small potatos, but would it be simpler to echo "... in this format: YYYY MM DD" read -r year month day
Looks like this depends on how info is built 🙄. I'm generally on RHEL/SLES machines (where this works). This command does not work on Ubuntu 18.04 and returns the error noted.
Do you think it's worth reading the bash manual if I use zsh?
Maybe? From what I remember, a friend of mine was working on making cross-shell compatible scripts and he was telling me that there were all these little differences between the shells that were making things a nightmare. If you do decide on doing that, I'd recommend finding a very good source of information on the differences between zsh scripting and bash scripting so you can keep those in mind as you're reading through the bash manual.
That's an interesting method didn't know you could do that with `read` . &gt;ym="${year}\_${month}" &gt; &gt;ymd="${ym}\_${day}" Implemented this! +1
&gt; Regex is a magic tool to use with ~~bash~~ everything!
How about first resizing the images using parallel parallel convert -resize *.jpg then you generate the gif from those resized images.
Please use four spaces infront of every code line instead of surrounding code by \`\`\`. The former works on old and new Reddit. You could use the following to copy your file to the clipboard: `sed 's/^/ /' YOURFILE | xsel -ib`.
Thanks, that's a great hint and I changed it. What's the disadvantage of using \`\`\`?
Maybe try something more like this: for ((test=$(initial | bc); test&lt; $(end | bc); test +=$(inc | bc) )) I suspect bash is seeing `|` as a bitwise operator and is getting stuck. I haven't tested this at all, but just a direction to look in.
/u/MemedanHemski I know this thread is a little old, but I made a more comprehensive script here: https://gitlab.com/krathalan/miscellaneous-scripts/blob/master/update_wow_addons It has support for downloading and installing ElvUI, as well as making compressed backups of your old addons in case something goes wrong or for some other reason. Pull requests etc are welcome :)
Not sure if it would be suitable for your use-case, but it looks like you can loop through a list of floats starting with 0.01, ending in 0.1, in increments of 0.01 in awk. \`awk 'BEGIN{for(i=0.01;i&lt;=0.1;i+=0.01){print i}}'\`
When you're assigning variable in your for loop, you're not wrapping everything to properly assign the variable. But, more importantly, I think since bash doesn't know how to work with floats, it can't understand this and implement it into a for loop. So, another method might be necessary: initial=0.01 end=0.1 inc=0.01 steps=$(echo "($end - $initial) / $inc" | bc) for i in $(seq 0 $steps) do echo "done with $(echo "$i * $inc + $initial" | bc)" done It's not glamorous, but it does what you want.
I would recommend starting with more beginner-friendly tutorial type stuff... But if you are used to using a command line and reading technical documentation, then man it up
Use integer iterations, and calculate the division you require for stepping over fractions.
I aggree. As the stackoverflow thread suggests, bash doesn't support this. the builtin `for` loop just cannot grasp the numbers with a dot in them. My suggestion if it is possible for you: ``` initial=1 end=10 inc=1 for (( test="$initial" ; test&lt;"$end" ; test+="$inc" )) do echo "done with: " $(echo "scale=1; $test / 100" | bc) done ```
It simply doesn't show on old.reddit.com, meaning everything is normal text and comments become headers (bigger, bold font). \`\`\` certainly is convenient but it totally goes against what markdown is good at: the raw markup is still perfectly readable.
It will work if you add space characters around the `=` character. Check out a tool named "shellcheck". It's pretty neat. It hunts down mistakes that are easy to make because bash is weird. Your distro probably has a package for shellcheck. You can also try it online at www.shellcheck.net without having to install it.
thanks for the tip but now it ignores the echo commando and just stop after i type in the answer.
 0 22 * * * /path/to/script and then look in either /var/log/messages or /var/log/syslog (depending on your distro). What do the logs say? If there's an error that's where it would be reported.
you need to evaluate ${varname} in the if statements, not string literals. The syntax you want is if [[ ${varname} == "Iphone" ]]; then blah
`03 17 * * * /etc/init.d/nightlight.sh` is how I have it typed out.to attempt to get it to run. (I changed the time to be a minute or so ahead of the current time) I checked the logs, and it claims to have run the command successfully. However I did not notice my screen change temperature. The script runs fine when manually executing it.
Are you running as user root?
First try this command in your crib 03 17 * * * touch /home/&lt;username&gt;/crontest If that works, move nightlight.sh to your home directory and then run it in cron. Also check if your script file is executable
I've changed the modifier so that any user can run Cron jobs, if that helps. I am able to add jobs with my account, which is setup as the admin on Mint.
So what time \*does\* it run, if it's not running when you expect it to? I think the problem here is you assume it is a time problem when it is really an execution problem. For example, if you set it to run every single minute, it will probably throw an error telling you why it's having trouble running.
/etc/init.d is not where I would put a cron job script. Try /usr/local/bin or somewhere in /opt/
I've modified it to make it executable just in case. And that first command does not work for me. Although I did move the script file to my home directory.
I've moved it to /opt/ and I am still having issues. Should I have to include exec as the command before the file? I've tried running with and without exec.
Jul 5 19:04:02 Probook6570b anacron[9252]: Anacron 2.3 started on 2019-07-05 Jul 5 19:04:02 Probook6570b anacron[9252]: Normal exit (0 jobs run) Jul 5 19:04:02 Probook6570b gvfsd-metadata[1773]: g_udev_device_has_property: $ Jul 5 19:04:02 Probook6570b gvfsd-metadata[1773]: message repeated 7 times: [ $ Jul 5 19:04:19 Probook6570b crontab[9173]: (isaiah) END EDIT (isaiah) Jul 5 19:05:08 Probook6570b crontab[9272]: (isaiah) BEGIN EDIT (isaiah) Jul 5 19:05:40 Probook6570b crontab[9272]: (isaiah) REPLACE (isaiah) Jul 5 19:05:40 Probook6570b crontab[9272]: (isaiah) END EDIT (isaiah) Jul 5 19:06:01 Probook6570b cron[847]: (isaiah) RELOAD (crontabs/isaiah) Jul 5 19:06:01 Probook6570b kernel: [15059.834660] sct[9290]: segfault at e0 i$ Jul 5 19:06:01 Probook6570b CRON[9288]: (isaiah) CMD (/opt/nightlight.sh) Jul 5 19:06:01 Probook6570b CRON[9286]: (CRON) info (No MTA installed, discard$ Jul 5 19:07:08 Probook6570b systemd-resolved[841]: Server returned error NXDOM$
Using `for (( ; ; ))` doesn't work here. You will have to translate it into a `while` loop. It would then look like this: #!/bin/bash initial=0.01 end=0.1 inc=0.01 test=$initial while (( $(bc &lt;&lt;&lt; "$test &lt; $end") )); do echo "done with: $test" test=$(bc &lt;&lt;&lt; "$test + $inc") done Bash is kind of terrible for this kind of job as you can see. You should think about using Python or some other language you like. You might also be making a bit of a mistake with how you use floating point numbers. Floating point numbers can behave pretty weird. Check out the following example about what kind of mistakes are possible with floating point: $ perl -E 'for ($x = 0.01; $x &lt; 0.1; $x += 0.01) { say $x; }' 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 $ perl -E 'for ($x = 1; $x &lt; 10; $x++) { say 0.01 * $x; }' 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 The two pieces of code should in theory print exactly the same, but there's a difference at the end: the first version prints a 0.1 at the end, and the second version stops at 0.09. The first version is wrong. In the code there's a test `$x &lt; 0.1` so it should not print the 0.1. It should stop at 0.09 just like the second version. The reason for this is that the actual machine is using binary numbers and can't save certain decimal floating point numbers. A "0.01" is one of those numbers that it can't save. If I make that 'perl' script print the values with more digits after the comma, you can see a hint about what's happening internally on the machine: $ perl -E 'for ($x = 0.01; $x &lt; 0.1; $x += 0.01) { printf "%.20f\n", $x; }' 0.01000000000000000021 0.02000000000000000042 0.02999999999999999889 0.04000000000000000083 0.05000000000000000278 0.06000000000000000472 0.07000000000000000666 0.08000000000000000167 0.08999999999999999667 0.09999999999999999167 This here is the reason why that `$x &lt; 0.1` test wasn't behaving correctly. Internally the number isn't saved as "0.1", it's instead some sort of "0.099999...". What this means in practice is that you should do things like a loop counter in integers so that you can be exact with your tests, and then you repeat your floating point calculations at each step of your loop. You can also never do something like a `x == y` test with floating point numbers. Instead you have to translate that test into something like the following, with `imprecision` being a value that fits with the amount of rounding errors that your calculations could have introduced: (x &gt; y - imprecision) &amp;&amp; (x &lt; y + imprecision)
what does nightlight.sh do? Can you paste it somewhere? I'm suspicious that only the user logged in can change the screen temp....
It runs `sct 3200` which is the command I would use to set the screen temperature to 3200. I'm also the only user added
You have already correctly deduced that you need help from an external utility (*bc*) to accomplish this; others have demonstrated/suggested *python*, *perl*, and *awk*. It comes down ultimately to what you actually want to do inside the loop. I would try to use awk if you can, otherwise my weapon of choice would be guile: ``` guile -c '(do ((i 0.01 (+ i 0.01))) ((&gt; i 0.1)) (display "done with: ") (display i) (newline))' ``` And, yes, this shows those rounding errors you must expect to get.
/u/mcandre has the proper solution
Just looked over the BashGuide, it is a great starting place. Am going to bookmark it for later use &amp; sharing.
In your home directory type “touch test”. The should create an empty file called “test”. If that does not work type “which touch”
Any user might be able to run jobs, however there may be a permissions issue in writing to the file in /etc/init.d I see you’ve moved the script to your home directory
So it looks as if the `sct` program itself is erroring out: `Jul 5 19:06:01 Probook6570b kernel: [15059.834660] sct[9290]: segfault at e0 i$`
That's odd. Any ideas why? I can execute the script normally otherwise
I'm not 100% sure, but I suspect that it might be due either to variables not being defined or _how_ `cron` is running your `nightlight.sh` script. Can you copy pasta that script in its entirety? Also, where did the `sct` executable come from? Did you compile it yourself?
sct was the first screen temp tool I found in the repositories. I'm using Linux Mint 19.1 if that helps. And here is what I'm working with so far: `% DISPLAY= sct zsh: segmentation fault DISPLAY= sct % sct 3200` I added the code above `sct 3200` to try and prevent segfault errors like others are suggesting online, but to no avail. From what I can gather (correct me if I'm wrong) the issue here seems to be that sct is trying to output changes to the terminal window when ran this way, instead of the display itself. That's my guess at least, but I am by no means knowledgeable on the subject. Running nightlight.sh from the terminal now results in a few errors, so I have taken the above out, leaving only `sct 3200`. That's all I've been able to troubleshoot at this point.
You could extract only the relevent day's images from the tarball to make it faster, you can pass in a pattern to the GNU tar command like this: tar -xf ~/Pictures/backgrounds/earth/"$tarname" --wildcards --no-anchored "${year}_${month}_${day}*.jpg" -C "/tmp/${foldername}"
Okay, that's a start. X11 is the window manager that many Linux distros use, though a few are moving to Weyland. X has a special variable named "DISPLAY" that, if I remember right, essentially lets other programs know how to interact with it. (I'm probably wrong on that part, plus it's very simplified - hopefully someone else can correct me.) If it's not set properly then `sct` isn't going to be able to interact with your X session to set the desired temperature. Try this: open a new terminal and echo the DISPLAY variable: `echo $DISPLAY` Then add a new script to your crontab and set it to run once: `echo $DISPLAY &gt;&gt; /tmp/display.log` Are they the same? Additionally, maybe try making sure that your nightlight script uses zsh as its interpreter by adding `#!/usr/bin/env zsh` to the top of the script.
This gave me ten 0s as the output...Any ideas why?
Try now again, I made this code with the numbers 0.1 to 1 and changed that later to reflect what you wanted to have. the \`scale=1\` schould be a \`scale=2\` for that. Try without scale, I tested that in WSL and that might behave differently, but without it I also got zeros.
Oh alright got it :)
That's a nice improvement I like a lot. Implemented it above. But I found, you have to put the `-C PATH` part right in front of the `--wildcard` tag for it to work like so tar -xf ~/Pictures/backgrounds/earth/"$tarname" -C /tmp/${foldername} --wildcards --no-anchored "*${year}_${month}_${day}_??.jpg"
thanks my guy, works now :)
It's probably a bit much to expect this utility to infer that `$username` is assigned by `read` just because `"${ASSIGN[0]}"` happens to contain the string `username`.
What would be the "right" way to do this, then?
I can't see what the point of your `PROMPT` and `ASSIGN` arrays are.
This snippet is part of a larger program, and my thinking is that using arrays in this way reduces the need for code duplication, as well as makes maintenance easier. Here's [the full program](https://github.com/marshki/plus_1/blob/master/src/plus_1.sh). Genuinely interested in knowing the correct way to do this.
&gt; using arrays in this way reduces the need for code duplication Well, it hasn't. You've got three different functions, producing three different prompts, reading three different values into three different variables. Hardly "reduced code duplication" at all. Put it this way: if you got rid of those arrays and just used their values in the those `read` lines directly, your code would be shorter and work exactly the same way.
That's a fair point. Not sure it answers the original questions, though.
&gt; That's a fair point. Not sure it answers the original questions, though. The answer to your original question is "you're expecting ShellCheck to be able to infer an assignment to a variable given an indirect reference to the name of that variable through a completely different variable". ShellCheck can't do that, and it's unlikely ever to do that, since it's a pretty crazy approach.
Thanks for the explanation.
FWIW, I remember `man hier` being pretty informative. It explains a bit about the unix filesystem hierarchy, which can be pretty useful to know.
Your program is correct, but it's difficult to trace what those `read`s are doing. This is because the names of the target variables depend on the value of some array, at runtime. `shellcheck` isn't smart enough to know that the variable names are in an array that is always initialized with values happen to remain fixed for the life of the program. That it's not smart enough to figure all of this out is probably for your own good. The goal when trying to maximize readability in a program is to try and reduce the context required to understand each expression, each function, and the entire program with a secondary goal of reducing duplication. If a reader needs something from outside of one of these scopes in order to understand what's happening in-scope, these external dependencies (these global variables, these other functions, and these other programs) from the outer-world should be named clearly or aliased if their canonical names are unclear. With a clear name, there's no need to actually look at where the dependency was defined and initialized to understand what it does or represents...you can just trust the name. For example, if I was reading a program and encountered a call to a command or function named `mv`, I wouldn't need to look for its implementation to know what's happening, since I know that `mv` usually renames or moves files. If, instead of this clear name, I encountered `${external_programs[0]}` in a command line, I'd need more context in order to figure out what the containing command line was doing. In this case, I'd have to figure out all of the values this expression could evaluate to to when this line of code is run. A concrete example from ur program comes up when trying to understand what `get_username` does. Here, it's unclear what the variable named *whatever ${ASSIGN[0]} evaluated to* is because you haven't given it a static name. In order to figure this out, I'd need to mentally run your program until ASSIGN is evaluated, mentally index into it and determine that this is really the value, `username`, meaning that `read` targets the variable, `username`. As the writer, you may think that you can just look at the definition of `ASSIGN` but, a fresh reader wouldn't know that this array is never modified. This additional context would need to be built up by reading more of the program. Since you're doing all of these shenanigans to determine the name of a global variable, you may be better off just naming the variable statically (replacing `${ASSIGN[0]}` with `username`). TLDR: Ain't nobody got time to read an entire program to understand what an expression, line, or function does. A cleaner implementation of your program would have distinct named variables for each of the elements of that ASSIGN array. These variables would then be provided directly to the read built-in as target variables so that passersby would know what those target variables represent without having to read the entire program. Stepping back, try to think clearly about the purpose of named variables. They're not named for the sake of the computer: it can do pretty well with memory addresses of offsets into packed buffers. You're naming variables for the sake of readers of your code. Hiding these nice names behind a level of indirection just gives these readers more work to do when they're trying to understand what your code does. In the real world, when you have multiple people not only reading but also maintaining code you've written, this becomes a little more important: software engineers are inherently lazy. And it makes sense...they spend a lot of their time telling computers how to do things for them. For this reason, code that can't be clearly understood usually sees one of two fates, and neither is that great for your team. It either lives to see another day and is worked around or maintained only very infrequently with surgical care, or it's destroyed and rewritten, burning a lot of time that could have been spent doing something else.
FWIW, I run my own room for screen temperature https://gitlab.com/moviuro/moviuro.bin/blob/master/sctw in a loop in a tmux that I hide. The issue discussed with others is the absence of an env Var (DISPLAY).
I’ll check it out too, thank you
Question,: in kubetail line 75 you do a check for arguments and then you repeat the check in line 80 is there a reason why you did this instead of a if/else or just delete the check on line 80 since you exit the program if the condition fails?
Indeed, it seems redundant, it could have been combined into else statement. Thank you for pointing it out. I copied kubetail from [https://github.com/johanhaleby/kubetail](https://github.com/johanhaleby/kubetail), but contributed upstream also. I will fix it in my repo.
Does your script have error checking? Does it output anything when run manually? Cron jobs will email any output to the owner of the job. Check the email sent to the account. If you don't have email set up, do so. Most common problem with cron jobs is the environment (PATH typically) is different than the interactive login shell, due to .bash_profile, .bashrc, etc. Error output will indicate this.
Much shorter version: #!/bin/bash read -rp "Enter user name to add and press [Enter]: " username printf "%s\n" "$username" read -rp "Enter 'real' name to add and press [Enter]: " realname printf "%s\n" "$realname" read -rp "Enter password to add and press [Enter]: " pass1 printf "%s\n" "$pass1" For this specific example you have posted, arrays and functions are unnecessary and as you can see, they can make your code much larger than necessary, making it more complicated to understand/edit in a future. Shellcheck will always send you a message when you have undeclared variables. This way is much simpler and you don't need the array. &amp;#x200B; I have seen your the complete script you posted in another comment. I think when you get to `user_info` is when things got complicated since you build a specific function for two different operating systems. IMHO, you could reduce everything down there to something like this (sorry if I get something wrong): #======================================= # GNU/Linux Functions #======================================= _linux() { # Create account in GNU/Linux via useradd using input from user_info printf "%s\\n" "Adding user..." useradd --create-home --user-group --home /home/"$username" --comment "$realname" --shell /bin/bash "$username" # Set password printf "%s\\n" "Setting password..." printf "%s" "$username:$pass2" | chpasswd ### create desktop directory sturcture (option) ### printf "%s\\n" "Creating deafult directories..." if [[ -n $(command -v xdg-user-dirs-update) ]] then su "$username" -c xdg-user-dirs-update fi } #======================================= # macOS Functions #======================================= _macOS () { # Get highest current UID and increment +1 uid=$(dscl . -list /Users UniqueID |sort --numeric-sort --key=2 |awk 'END{print $2}') increment_uid=$((uid +1)) # Primary group ID prompt. printf "%s\n" "Primary Group ID: 80=admin, 20=standard" read -rp "Enter ${PROMPT[4]} to add and press [Enter]: " "${ASSIGN[4]}" # Create account in macOS via dscl using input from user_info printf "%s\\n" "Adding user..." dscl . -create /Users/"$username" dscl . -create /Users/"$username" UniqueID "$increment_uid" dscl . -create /Users/"$username" UserShell /bin/bash dscl . -create /Users/"$username" RealName "$realname" dscl . -create /Users/"$username" PrimaryGroupID "$primarygroup" dscl . -create /Users/"$username" NFSHomeDirectory /Users/"$username" dscl . -passwd /Users/"$username" "$pass2" # Create home directory macOS printf "%s\\n" "Creating home directory..." createhomedir -u "$username" -c } #======================================= # Main #======================================= # Detect system architecture, then act case $(uname -s) in Darwin) root_check user_info _macOS ;; Linux) root_check user_info _linux ;; *) printf "%s\\n" "You got the wrong one, homie" ;; esac if [[ $? -gt 0 ]] ; then printf "%s\\n" "Something went wrong, homie..." fi exit 0 Again, this is just an opinion. I've check with shellcheck the first small script and didn't get me any errors or messages.
Boo to all the people who marked this down when it usefully got the OP to clarify the question.
So, I'm not 100% sure that I understand what you're trying to do, but it looks to me like you're trying to combine two lists in a colon-delimited format. It looks like you're feeding in the lists from the pipeline separated by a space as well. As such, this should be really quite easy to do with awk. `awk -v OFS=':' '$1=$1'` or, to make things a bit less opaque, `awk -v OFS=':''{$1=$1; print} what's happening here is that awk reads input one _record_ at a time and divides the _record_ into _fields_. the default _record seaparator_ is newline, so you don't need to change that. The default _field separator_ is a space, so it will see your colon separated lists as each being a _field_. `-v` allows you to redefine variables (you can also do this inside of the awk script, in the `BEGIN` block, usually, but `-v` is a little more succinct for one-liner purposes). `OFS` is the _output field separator_, meaning that the output _fields_ will be set to `:`. Now the `$1=$1` part is a little weird. If you ask awk to just print the output without evaluating anything, it won't change the field or record separators, therefore you need to make it evaluate something. There are probably a lot of ways to do this, but I know from experience that $1=$1 does this, so that's what I'm using here. As for the part about :: being the same as :.:, I don't really understand what you're trying to do with that. You could pretty easily use sed (or even awk) to sub `::` for all occurences of `:.:` Hope that this helps.
here, it worked just fine [https://imgur.com/a/dkc88b0](https://imgur.com/a/dkc88b0)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/QsxKwPO.gifv** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20et4idkb)
I know. That works for me, too. But I have to automize getting the value 1.50E+1 out of my step1 file, because I have not only step1, but several hundred steps, and if I do so using the mentioned methods, it doesn't work anymore. At least not in my environment.
I wonder if there is a newline in the var4 file after the entry? I would like to see the output of hexdump -C var4 and also grep -n "$(cat var4 | tr -d '\n')" step1
hexdump gives out `00000000 2e 35 30 33 38 30 30 30 30 45 2b 31 0d 0a |.50380000E+1..|` `0000000e` I simplified the step1 file, bc my bash terminal is on a server and I didn't wanted to type of the whole stuff, but the .50380000E+1 is basically what I ment with 1.50E+1 without the 1 in the beginning. I cut of more in order to make sure no space is in front when I tried around with sed and cut. &amp;#x200B; grep leads to `2:heatstart [s]: 1.50380000E+1`
Ok, I think the issue is that there is newline at the end of your var4 file. In the hexdump this is visible as "0d 0a", which is "\r\n". If my expectation is correct, then these should both work: grep -n "$(cat var4 | tr -d '\r\n')" step1 grep -n "$(cat var4 | tr -d '[:cntrl:]')" step1
Your file has DOS (Windows) line endings. It's that extra `0d` hex character there in front of the `0a` at the end. Unix line endings are just the single `0a` character. Perhaps change your file to Unix line endings and then see what happens. You can send the file through `tr -d '\r'` to remove those extra characters, so for example, instead of this here: grep -n "$var" step1 You do this here: tr -d '\r' step1 | grep -n "$var"
Yes! Thanks.
Ok, great. So that is why it only initially matched the one at the end of the line, because that one had a newline after it already. If you don't need to use the var4 file contents for anything else, grep can also take a pattern directly from a file, so this may work: grep -f var4 step1 or fgrep -f var4 step1
I found a solution. Adding the following to my \`\~/.inputrc\` allows both the delete key and arrow keys to work as desired. &amp;#x200B; \# Fix delete key and arrow key history search in st $if term=st set enable-keypad On "\\C-\[OA": history-search-backward "\\C-\[OB": history-search-forward $endif
It shouldn't be documented, it's in [the documentation](https://www.gnu.org/software/coreutils/manual/html_node/cp-invocation.html): &gt; _`‘-R’`_ &gt; &gt; _‘-r’_ &gt; &gt; _‘--recursive’_ &gt; &gt; Copy directories recursively. By default, do not follow symbolic links in the source unless used together with the --link (-l) option; ...
You really need to check the documentation for the `cp` on your Mac. Assuming its GNU `cp` (which I suspect is _not_ a good assumption), this is expected behaviour. From [the documentation](https://www.gnu.org/software/coreutils/manual/html_node/cp-invocation.html): &gt; `-R` &gt; &gt; `-r` &gt; &gt; `--recursive` &gt; &gt; Copy directories recursively. By default, do not follow symbolic links in the source unless used together with the `--link` (`-l`) option; ...
was only aware of the \`man cp\` documentation which is the -less man page. Reading through \`info cp\`. Bad assumption, thank you for the insight.
Thank you, /u/cttttt . Your thoughtful, thorough response is much appreciated. For my own clarification, if the `ASSIGN` array were renamed to something explicit, e.g. "VARIABLE_NAME", and the elements of the array were likewise more "distinct", would the overall design of the program be acceptable? Or is just bad practice to do things in this way?
Thanks for the second set of eyes, /u/DJ0K3 .
Great write up. Saving for future reading and implementing.
Thank you. I’ll play around with awk to see if I can make it work for me. The whole top half is directly from the assignment. The assignment is a strange one. He gives us a ton of example “lists” that have duplicates and what not and wants the duplicates removed, but the list to be in the same order as it was entered. He wants to change a leading : to .: a trailing : to :., unless it ends the list, and then :: should be changed to :.:
[removing duplicates can be done with awk, too](https://iridakos.com/how-to/2019/05/16/remove-duplicate-lines-preserving-order-linux.html) ^_^ You might need to run it through with RS and ORS as ":" to get that part done. I will say that that particular one-liner is among the most opaque that I've ever seen as well as using associative arrays, which are a bit hard to wrap my mind around. As for the colon changing stuff, I'm still pretty confused by that. So would that be between the lists, essentially? Or is it only when there's a `.:` or `:.` present and not otherwise? You might need to treat each list as a record, anyway, which wouldn't be too hard. Just use space as the record separator. You might be able to do something with the awk `gsub` function (global substitution), as you can treat a record or field as a whole. the form is `gsub(x, y, z)`. _x_ is the text you want to change, _y_ is what you want to change it to, and _z_ is the text you want to operate on (this can be left blank or set to `$0` to do the whole record), to just operate on one field you can use `$n` where _n_ is the field number. _x_ can be a regex, though it does not support grouping (I think the _gensub_ function does, though. That's a GNU-only extension). _Awk_ is an extremely powerful tool, and there's a lot to it, but it is probably going to be quite useful for shell scripting excersizes in a class, so it might be worth digging deep into it (this is probably true for sed, as well). It's especially useful when dealing with plaintext tabular data, but there is a lot of that in shell scripting. I could probably link you to a tutorial if you're interested, but I don't know how much time you can spend on that before you need to finish this assignment.
The link would be great! I have a week to complete the assignment, so I have plenty of time to study up. &amp;#x200B; The items in the list would be colon separated. If there is already a colon as part of the item, it would be changed depending on where it's found. So `:a b c` would become `.:a:b:c` My script works for that case, but fails when it comes to `a b: c.` It should come out as `a:b:.:c`, but comes out as `a:b:c` &amp;#x200B; I really appreciate all your help :)
Interesting. what a weird assignment. I suppose there's some reason for it. [Here is the link I was thinking about](https://www.grymoire.com/Unix/Awk.html). There's also a sed tutorial, but it was a lot easier to understand after going through the awk one, IMO. It's also worth mentioning that most of the tutorial is focused on old awk (which is less featureful than gawk or POSIX awk), but the author has updated it quite a few times and made notes about gawk and even added some sections on the features of nawk (another version of awk that was very widely used at one point) and gawk. Using nothing more than the sub function (this is like gsub, but only does it on the first match), I got this: ``` echo ":a b c c:d:e f:g: :h:i:j:" | awk -v OFS=':' '{$1=$1; sub("^:",".:"); sub(":$",":."); print}' .:a:b:c:c:d:e:f:g:::h:i:j:. ``` Now, the obvious problem here is this a list ending in `:` followed by one starting in `:` leaves us with `:::` instead of the desired `:.:`, but this is pretty easy to fix with `sed 's/:::/:.:/g'`. Not sure if that's considered an acceptable answer. If that's too janky, it might be worth doing something with the field separator and maybe if statements, but that might also create further problems.
Not really sure what you are trying to accomplish with this, but wrapping your code in an infinite while loop may work. while true; do # your code... done
Did the lessons already mention something about "functions"? A function is what you can use if you have a list of commands that you want to be able to use at several different places in a script. You put your commands into a function, and then you can then run them by writing the function's name. Here's an example from the command line about how this works: $ foo() { echo "hello"; echo "hi"; echo "hey"; } $ foo hello hi hey $ foo hello hi hey Here's that example function with line-breaks added to show how it would look like in a script instead of at the command line: foo() { echo "hello" echo "hi" echo "hey" }
The "include" command in bash is called `source` or `.`. You can read a bit about it with `help`: help source help '.' In that help text, there's on sentence that says "in the current shell". That's a hint that the variables and functions that you create in the other file will be available after the `source` command.
Building off of this, it’s generally good practice to ensure that “library” scripts (meant for sourcing by other scripts) don’t have any side effects when they’re sourced. That is to say, there shouldn’t be any bare logic that isn’t wrapped into a function. The sourcing script can then call whichever function(s) that it needs.
Yep, sourcing gets tricky. I'll define one function in a file of the same name, but fr anything elaborate I echo generated code to standard out and feed that to `eval` in a manner similar to how `rbenv` is set up using `eval '$(rbenv init -)'`. Sourcing directly can have side effects such as changes to `$1` etc.
This is the one !! Thank you very much!
Use https://shellcheck.net . It does simple static code analysis, but helps quite a lot.
 tr -d '\r' &lt; step1 | grep -n "$var" since tr doesn't bother with files, only stdin
I'm going to put this on my cygwin when I go back to work.
http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-7.html
https://www.gnu.org/software/bash/manual/bash.html#Arithmetic-Expansion
Here's one way ``` #!/usr/bin/env bash read -p "how many iterations? " limit for i in $(seq 1 $limit) do echo loop $i done ```
Ty
Bash will do math inside `$(( ... ))`, and the result of the `$((` will be the result of the math you do inside it. Here's an example: $ x=3 y=5 $ echo $(( x + y )) 8 $ z=$(( x * y )) $ echo $z 15 You can get a list of the math operators by doing: help let There's also a related feature `(( ... ))` that does not return the result like `$(( ))` does, but it has a true/false exit code. You can do for example the following with it: $ if (( x &lt; y )); then echo "x is smaller than y"; fi x is smaller than y $ (( z = x + y )) $ echo $z 8 $ (( x++ )) $ echo $x 4
 #!/bin/bash number_1="10" read -p "Enter a Number: " number_2 result=$(($number_1*$number_2)) echo ${result} ans=$((num*ps)) echo ${ans} Which will give you the output: bash -x /tmp/1.sh + number_1=10 + read -p 'Enter a Number: ' number_2 Enter a Number: 5 + result=50 + echo 50 50
Elaborate please
Didn't understood
Thanks
Thanks
# bash testloop.sh Enter a Number: 6 testloop.sh: line 4: 10*: syntax error: operand expected (error token is "*")
Basically I want a program in bash which will take user input and divide it by 10 and then it will the the significant or approx value of input/10 and then it will repeate the loop that times ( significant value or approx).
What code do you have inside of testloop.sh? The script above was ran with bash -x (shows the output) and works just fine...
Something like this? while true; do echo -n 'Enter value: '; read val; echo "Result: " $((val/10)); done
It's been posted for 7 minutes and you already replied in 4 minutes. Bash skills another level..
In my experience, symlinks get weird, you could try using `/home/$USER` instead.
I just copy pasted your code there
Nope
Haha help me out then, what exactly do you need the program to do? You want it to: 1. Read a number from the user 2. Divide the number by 10 3. Print that number to the user 4. Loop Anything else?
Calling awk (up to 3 times per entry in your shortcuts file) in a loop just for field splitting is bad. You can simply: while read -r alias path; do printf "alias %s='cd %s'" "$alias" "$path" done &lt; ~/.config/shortcuts Notice how you also don't need `cat`. Also,
I'm trying to learn more about awk, do you have any good tutorials that helped you learn?
Output of your script: alias cfi='vim ~/config/i3/config' alias cfb='vim ~/.bashrc' alias S='vim ~/Source' alias D='vim ~/Downloads' alias d='vim ~/Documents' alias e='cd /etc' alias dv='cd /dev' alias u='cd /usr' alias cf='vim ~/.config' alias r='vim ~/Remote' alias ft='vim /etc/fstab' `~/Source`, `~/Downloads` and `~/Documents` are directories, but your script still puts them as files, similarly as it had in my original script. I guess the conditional is not evaluating correctly which is weird, because if I type a conditional with `~` in it into the shell it evaluates correctly.
Tilde is just not expanded it that case. You could force it by hand with eval path=$path (extending Schreq's suggestion) but beware blanks etc.
That solved it, thanks.
You seem like a very nice and patient person.
This is one of the coolest websites i've came accross in a minute thank you! very insightful
After 2nd The output you got from second, I have to repeate loop that many times. And if the output is in decimals then take approx (like 58.6 = 59) or significant ( 58.6 = 58) and then repeate it that many times ( here input/10 is 58.6 and i have to repeate it 58 or 59 times
No, I really need help man
For the very basics I used random websites. The rest was learning by doing and reading lots of scripts. Also read a bit of [this](http://shop.oreilly.com/product/9781565922259.do).
And what should be done in each loop? You keep saying "repeat it that many times" -- Repeat *what*? &amp;#x200B; Should the answer be printed 58 times, each on a new line?
Anything, yes it count till 1-58, yes
 [[ $path == \~/* ]] &amp;&amp; path="$HOME${path#?}" That would change just the \~/ prefix.
Ok here, this will take a number, divide it by 10, then print "Anything" that many times: echo -n 'Enter value: '; read val; divided=$((val/10)); for ((i=0;i&lt;$divided;i++)); do echo Anything; done
Is that a physical book you're talking about?
yup, irrc, I also got it in PDF form from Humble Bundle
I'm dubious about published books. I'd recommend just starting a project. Maybe write conways game of life in bash or something.
Ok, thanks
Thanks, this is exactly what i wanted
ah ok, haven't heard of conways, thanks. i'm just going through the wooledge resource right now
Thanks!
I'm glad I could resolve your issue, darkwarrior33. After our chat session terminates, would you be willing to take a quick survey telling us about your experience with *HomeworkHelp+*© study services?
Thanks!
It's fine. take everything with a grain of salt and decide for yourself. If you're honest with yourself download it and buy a copy if you like it or buy one used. If you're in a hurry and only want to learn the minimum to complete the task then that's different I suppose. I see you've already got it from Humble Bundle... In that case what have you got to lose. Skim through it, read the first few chapters, and see if it's for you.
yeah its kind of on the minimum to complete the immediate task, with perhaps going deeper if go that route more (it isn't clear right now). right now I'm enjoying the wooledge resource recommended on the right hand side bar. thanks!
Your second programming language is where the ideas start to gel. Shell languages are pretty odd if you are used to more formal languages.
There's an old trick to clean old files, that goes something like set -- `ls -appropriatesort backup_*` if [ $# -gt $KEEP_DAYS ] then shift $KEEP_DAYS rm -- "$@" fi Deletion could be changed to printf|xargs and also otherwise bashified, but I like the straightforwardness of it.
I've been looking for an advanced one after learning some basic scripts. I feel like this one is perfect for me to further learning scripts. Thank you.
sure!
You can also download the shellcheck package on your distro and run it on your script
The author, [Dave Taylor](https://www.askdavetaylor.com/), has been writing [Bash/shell articles for the Linux Journal](https://www.linuxjournal.com/users/dave-taylor) for a loooooong time and I certainly think he knows his stuff. There's an [interview](https://www.itworld.com/article/2810851/wicked-cool-shell-scripts.html) where he talks about "Wicked Cool Shell Scripts", and among other things he says the book isn't about teaching you shell programming - you're already set on a good course with the BashGuide for that - but about going straight into the "meat of things". I've bought the book a well when it was in that recent Humble Bundle, but I haven't read it yet - will do so very soon, though. My guess is that there are many worse things to read, so I say go ahead and read it :)
haha, well, this is expediency right now, I mainly am trying to get experience on another technology. But I actually just wrote my first shell script with some googling! I enjoy the http://mywiki.wooledge.org resource shared on the right hand side bar here a lot. I still may need Taylor's book for the next question but will see. Thanks! plan to save his blog link
&gt; Right now I have to go to the bottom of the script every time to change the parts that need to be changed and it's annoying. One trick for this is to put all your main script into a function called `main` and then call `main` as the last line of your script. That way you can put it wherever you want instead of having to place it after all the functions. main() { # the meat of your script goes here } ... lots of other functions main "$@" # End of file I don't like to split a script into multiple files if one of those files would wind up being less than 25 or so lines. If I'm frequently having to modify some values, what I do is take out ONLY the frequently changed parts and keep those in a separate file (something like `.config/script_name.vars`, since most distros are good about locking down read access to the .config directories), then source that file at the start of the script.
Try fish.
Fish's website still says interactive shell for the 90s ... lol and mentions Netscape Navigator. How dated is that thing? Anyhow, it does a few things I was looking into but I'm not going to switch my shell now. Thanks!
:( Which service ?
Pretty sure it is a joke. Give it a shot.
Do you have a way to determine which user is part of the faculty? grep will return multiple lines of output if the search string appears more than once. So for example you can do “grep student /etc/passwd” and it will return every line that has student in it. You can pipe the output to “wc -l” to count the number of entries For the other stuff, you’ll need to split each string by a delimiter which I think is a colon in the passwd file. Then you can grab the fields for name, home directory, etc. I’ll have access to a machine tomorrow if you still need a hand then
Say I put the directory "/etc/passwd" into a variable called "input". Would &gt;grep faculty | wc -l $input work?
No. You'd need to grep faculty from the input file and pipe that to wc. Also consider printf instead of echo.
r/woosh
I don't think you need to search through each directory, all the information you need is in `/etc/passwd` This is an exercise in extracting data, capturing that data into variables, and formatting the result. I'm not sure how elaborate the formatting is, which in turn dictates how you capture and process it. It might help to draw or type up sample output.
Was going to recommend fish as well.
 /etc/passwd is not a directory. Have you tried "cat /etc/passwd | grep Faculty " ?
You need to cat the file within a subshell, and traverse the output with a for loop. The step variable in your for loop will contain each line of the script that you need to parse further. awk will help you do that when you set the field separator and print the 1-indexed field offset. an option to wc that you can directly pass the file to will print the total number of lines. This is the best I can do without doing it for you.
Awesome! Let me know how it goes!
The beautiful thing about scripting is that there are so many ways to do the same stuff. My favourite would have been using the awk command. BUT, if you are limited to using the find and wc, then all you need to do is figure out the pattern (which is why we use grep) which will show you only faculty accounts. It could be something in the username (eg. They all start with "fac" or something) or they belong to the same group (which would be the typical Linux way). Use the pattern to show you only lines for faculty accounts. And then use wc to count those lines. The trick is to look for the things that go wrong: 1. What if you cannot access the input file 2. Is your search pattern returning the right set of rows 3. What is there is an error executing grep or wc And so on.. Hope this helps. Ps: not doing your homework for you either 😉
I cannot replicate your issue :/ I ran the script from my MacBook (GNU bash, version 3.2.57) and a Centos 6 box (GNU bash, version 4.1.2) and both work just fine. Check you've not missed any " " and check the formatting.
Haven't tried Fish yet. But seems like it has some problems with bash scripts? Are there many downsides or known issues when changing to fish from bash?
Nice useless cat case.
 LC_ALL=it_IT.utf8 date that sayed, that specific locale has to be installed
Got it. Worked ty
See also https://wiki.archlinux.org/index.php/Locale
Thank you!
I learned here how to gerate it, thank you
Have you played about with the `column` command?
It turns out that [rootnroll.com](https://rootnroll.com) was running the fish shell. Were you trolling me there?
in one host run `iperf -s` in the other host `iperf -c IP.AD.OF.SRV`
`man iperf` `man iperf3`
It can also help you to just treat a folder as a template (and replace everything). I do this for python projects myself :)
not sure how this is bash related. iperf will run on windows too.
Remove the `$`. (cd ... &amp;&amp; forever ...) will run cd and forever in a subshell, which makes sense. $(cd ... &amp;&amp; forever...) will run cd and forever in a subshell, then capture the output of those commands, then try to run that output as a command... which does not make sense.
Nope.
Thank you my friend!!
check out https://www.shellcheck.net/ for tips and error checking in scripts
Nice, thanks!
Why use cd when you can just execute with an absolute path?
Well I guess I didn't think of that. Still learning, I'll give it a shot :)
If I'm not wrong, these codes "E[33" are colors. Did you omit anything from your script?
take a look at the "paste" command that will join multiple files together &amp;#x200B; this would probably work for you as a quick example (for two files), if you change the input files to something valid... paste &lt;(./My\_script.sh file1.csv) &lt;(./My\_script.sh file2.csv) &amp;#x200B; To work on arbitrary number of files, you could use a wrapper script to call My\_script.sh individually for each csv input file, writing the output to a temporary file, then "paste" them altogether at the end
My solution would be just a bunch of seds like date | sed 's/july/luglio/g' | sed 's/june/(june in italian)/g' etc.
You might want to look into using monit for this. When I used to run node on regular servers that's what i did. It restarts your node server if there's a problem and emails you to let you know. [https://howtonode.org/deploying-node-upstart-monit](https://howtonode.org/deploying-node-upstart-monit)
Now that you got your problem solved: - What if you use `pgrep` instead of `ps | grep`? - You could replace `cd /home/bigbrot1/noderoot &amp;&amp; forever start -a server.js` with `forever start -a /home/bigbrot1/noderoot/server.js`.
Was this live stream? Because this link doesn't work anymore. At least for the video player on the first link.
Perhaps check out a tool `perl-rename`. Try doing experiments like the following while you are inside the folder with your PDF files: perl-rename --dry-run 's/^(\d+)_/$1\/$1_/' *.PDF That's the same `s///` command that you might know from using 'sed' or vim. If you find that what it prints looks good and like you want to happen, then remove the `--dry-run` to make the tool actually apply the changes. About using just those last four digits for the directory name, that should work like this: perl-rename --dry-run 's/^(\d+)(\d{4})_/$2\/$1$2_/' *.PDF
...or just remove the parenthesis completely and I think it should still be a valid command - subshell will run the command in the background and let this script run, which might give you weird effects if you expected something to run sequentially. Subshell is nice when trying to do a lot of things concurrently (like taking a list of IP addresses and running a command against each one... if you do it sequentially it will take a lot of time but with subshell it will run all the processes concurrently), but could get you into a race condition if you aren't careful with it. I don't think it would have any noticeable effect on this script, but with others you might notice some weirdness. Just food for thought. bash scripting is literally the exact same thing as when you are using the bash terminal, so echo is a command on the command line as well
I don't have enough time to figure out the whole thing, but this ought to get you started by making a directory for each unique group of 4 digits that are at the point of the string that they are in your example (hope that makes sense). user@host $ for i in $(find . -maxdepth 1 -type f -iname "*.pdf" \ # maxdepth 1 searches current directory only | awk '{print substr($0,8,4)}' \ # substr to remove all but the chars that you want for the file name | awk '!visited[$0]++'); do \ # This cryptic one-liner removes duplicates (without any need to sort first) mkdir -p $i; done # The string of commands leaves you your filenames user@host $ ls # This is what my directory looks like after running the commands 3104/ 5214/ 610003104_RP_D56855_000.PDF 610003104_RP_D95414_000.PDF 830005214_RP_D44121_000.PDF 830005214_RP_D65887_000.PDF
The absolute path is a great idea as another user suggested. To be honest this is the first time working with grep and I found the code on stackoverflow so I'm not really sure what the difference is. What would be the benefit of `pgrep` over `ps | grep`?
This seems useful - a bit intimidating but I'm definitely going to do some research on it. Thanks!
Yes, it's a live stream from local TV stations. The seccond link isn't real and is just an example, if you want to figure out better the real m3u8 link is reachable through ffirefox manual method as I already explained. the real link loks like this [http://vxlive.cdn.antel.net.uy/auth\_livesignal\_739,vxttoken=cGF0aFVSST0lMkZhdXRoX2xpdmVzaWduYWxfNzM5JTJGdnhmbXQlM0RobHMlMkYlMkEmZXhwaXJ5PTE1NjI3MjY1NzAmcmFuZG9tPWZrNWNTSTBFSXMmYy1pcD0xNjcuNTcuMTg3LjIsZDE3MDhhYTNhOWQ3OTdjZjJjNDM2NTIzMDIzZTI3ZGI5OGI3NmMwZGYxM2U2MWU3Y2FmZDk3YzEwZmQwOWZkMA==/vxfmt=hls/h\_3e74db8014a5f7e729b9f5afd98e2cd6/var3000000/playlist.m3u8](http://vxlive.cdn.antel.net.uy/auth_livesignal_739,vxttoken=cGF0aFVSST0lMkZhdXRoX2xpdmVzaWduYWxfNzM5JTJGdnhmbXQlM0RobHMlMkYlMkEmZXhwaXJ5PTE1NjI3MjY1NzAmcmFuZG9tPWZrNWNTSTBFSXMmYy1pcD0xNjcuNTcuMTg3LjIsZDE3MDhhYTNhOWQ3OTdjZjJjNDM2NTIzMDIzZTI3ZGI5OGI3NmMwZGYxM2U2MWU3Y2FmZDk3YzEwZmQwOWZkMA==/vxfmt=hls/h_3e74db8014a5f7e729b9f5afd98e2cd6/var3000000/playlist.m3u8)
You would be better off running your application with systemd.
It does not run it asynchronously. You'd have to put a `&amp;` at the end for that. The point of using a subshell is so you don't have to worry about cd-ing back out. The directory change only happens in the subshell. In this particular case it appears to be the last part of the script, so since there are no other commands that may rely on the current working directory, the subshell isn't strictly needed, but doesn't hurt either.
It's a HLS stream with expiry tokens added. It's ever changing bits of few second video. I would not expect youtube-dl to work with this (mainly because it's not youtube). The way to download it is to repeatedly hit the playlist and download the bits linked in it, and merge them.
Try tcl expect as well for this sort of remote command/ response over ssh.
Once I get the stream manually I can have a continuous stream of at least 30 min; that's reasonable, though manual. youtube-dl however works with this stream [http://camaras.vera.com.uy/](http://camaras.vera.com.uy/) that seems to me pretty similar.
Post the whole script.
Well its just simple commands in a text file. &amp;#x200B; """ git clone [https://github.com/hashcat/hashcat](https://github.com/hashcat/hashcat) cd hashcat make sudo make install """ Thats it, nothing fancy, just simple commands.
Copy the last line, paste it into the terminal, hit enter. Fancy pants: `eval $(tail -1 test.sh)`. (Doing this on mobile from memory forgive any errors please, but basically tail -1 specifies the a single line at the end of the file, then eval runs whatever tail printed. I wouldn’t do this blindly though... just to point out what’s technically possible.) If you want to know what’s going on, you can try `bash -x test.sh` which is a debug. Or `strace ./test.sh`. http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html https://stackoverflow.com/questions/951336/how-to-debug-a-bash-script#951352
THere's no reason for any line not to run. There must be a syntax error or something. Maybe you don't have sudo rights? Try running the last command from the prompt yourself.
F\*ck, i'm embarrassed. The file is large, and I was copy and pasting parts from the script to the terminal and testing it and that's why the last command was not running. And now when I run the script then it works like I want it. I'm sorry. I am so embarrassed.
Thanks. I figured it out. It was nothing special...just embarrassing. I was copy and pasting some parts of the script to the terminal and testing it and not running the script at all as ./(scriptname).sh etc. I got no sleep last night..I'm not focused.
All good! We all have brain farts.
Maybe something along the lines of: for dir in $(echo * | cut -d_ -f1); do mkdir $dir mv "$dir*.PDF" $dir done
Sorry for the bad advice, you are correct on this
`$0` is the same as `argv[0]` in C (and others). [Here's an SO post about it](https://stackoverflow.com/questions/2050961/is-argv0-name-of-executable-an-accepted-standard-or-just-a-common-conventi#2051031). Under Linux, it is however the program is called, be it relative path (`./foo`) absolute path (`/path/to/foo`) or (if the program is in `PATH`) just the name (`foo`).
You can use Case too &amp;#x200B; #!/bin/bash read -n1 -p "Iphone or Android?" varname case $varname in iphone) echo "Iphone choice" ;; android) echo "Android choice" ;; *) echo "No choice" ;; esac
`$0` is derived from the script name, and is not the same as `argv[0]`. This is because the original `argv[0]` gets lost when the script is invoked through an interpreter, so the shell has to derive a new one. Compare the output of a C `printf("%s\n", argv[0])` and `echo "$0"`: $ ( exec -a myargv0 ./elf ) myargv0 $ ( exec -a myargv0 ./shellscript ) /home/me/shellscript
Huh, TIL, thanks.
This is one way to make use of it: inpath Text*crypt Directory '/home/common/bin/': 'TextDecrypt' -&gt; '/TextEncrypt' 'TextEncrypt' In 'TextEncrypt': Encrypt=false ScriptCommandName=$(readlink -e "$0") ScriptCommandName="${ScriptCommandName##*/}" if [ "$ScriptCommandName" == "$CommandName" ] then Encrypt=true fi
Good to know! Thanks!
Yeah, thats kinda what I thought too! It would be fun to attempt my own busybox type of application. However, I don't know how practical that would be in a production environment to include all functionality into one all inclusive script/file. Maybe to use for sourcing function libraries.... maybe.
[This is 'same script, different behaviours' in overdrive](https://i.imgur.com/o3A4Y2c.png). Script 'DefaultApplications' is usually invoked through links. If it is run, by root, directly, it recreates all links to it.
Thank you. It set up the folder structure perfectly. Now I just need to move the PDF's into those folders. This is what I've found. I just manually change the XXXX to the 4 digit number of the PDF file that matches the folder. About 100 folders to it'll take a while. xargs -r0 --arg-file &lt;(find . -type f -exec grep -lZi XXXX {} + ) mv -i --target-directory XXXX
Thanks. I will give this a try.
Thanks. I will give this a try.
nvm this does not work.
There's almost certainly more efficient ways to do this, but this should work: ``` for f in $(find . -maxdepth 1 -type f -iname "*.pdf"); do dir=$(echo $f | awk '{print substr($0,8,4)}'); mv "$f" "$dir"; done ``` or with comments: ``` for f in $(find . -maxdepth 1 -type f -iname "*.pdf") # gives you a list of all pdf files in directory do dir=$(echo $f | awk '{print substr($0,8,4)}') # assign $dir to those same 4 numbers (this is the part that could probably be done more efficiently) mv "$f" "$dir" # I think you probably get this one :) done ```
You need `$(( $1 % 4 ))`, even though bare variable names are accepted in math expansion, they can’t be numbers (because this would have some very bad consequences).
I think what's happening is, you've broken the `$( )` operator by adding a space character and a `\` character between the `$` and the `(`. That spot with the mistake I mean is after that "big-white" name.
I have played around with that as well. The \'s were recommended by the shellcheck.com website, so I went with it. When I do what you recommend, it has a slight different problem `cp: cannot stat '/home/my-user/.conky-help/icons/big-white/.png': No such file or directory` It seems to strip out the `jq` command leaving an empty file name.
THANK YOU MY GOOD SIR
That's because your jq command printed nothing. Try the jq command alone and see how to fix it. I mean, play around with the following: jq -r '.daily.data[1].icon' ~/.cache/darksky
It outputs correctly with `partly-cloudy-day`, so theoretically, it should copy partly-cloudy-day.png to ~/.cache/forecast-1.png, but instead it's trying to copy /.png for some reason.
I'd prepend an `echo` to the front of that whole `cp` line just so I can see what is actually being generated by the jq.
you can also put the text from the `jq` into a temporary variable and then use the variable in the cp. You can also add `set -x` above this line in the script to see what the shell is actually executing.
Maybe I'm looking in the wrong places. when I run *cp -f /home/"$USER"/.conky-help/icons/big-white/$(jq -r '.daily.data[1].icon' ~/.cache/darksky).png ~/.cache/forecast-1.png* directly from my terminal, it works. If I run the script directly it also works. When the script is called by conky, it gives me the *cp: cannot stat '/home/my-user/.conky-help/icons/big-white/.png': No such file or directory* error. echo shows everything working correctly. Maybe it's a conky incompatibility?
I don't understand what you're saying; the first script outputs 1.
If you dump the separate echo statements, the first echo/cut statement to fill pisces has the values 1 - 19; then is is logical that the bottom two echo statements output 1 and 2, being the first two values in your variable pisces. &amp;#x200B; I needed to do the following: enclose in double quotes and use single quotes for the cut separator parameter `pisces=( "\`echo ${feb[@]} | cut -d ' ' -f 1-19\`" "\`echo ${feb[@]} | cut -d ' ' -f 20-28\`" )`
Why use pipes and cut if you can select a range straight from the array? `echo ${array[@]:START:LENGTH}` This is simpler.
I feel like 50% of the question on this sub can be answered with a for loop over a glob and parameter expansion. for file in *.PDF; do dir="${file%%_*}" mkdir -- "$dir" \ &amp;&amp; mv -- "$file" "$dir" done
Here's some free books courtesy of the ebook foundation https://github.com/EbookFoundation/free-programming-books/blob/master/free-programming-books.md#bash Can't recommend them enough.
Completely unrelated to your question, but a relevant fun fact nonetheless: Leap years are not every 4 years. Centuries are only leap years if they are divisible by 400. [This is the primary difference between the Julian and Gregorian calendars](https://en.wikipedia.org/wiki/Leap_year#Gregorian_calendar)
Are you running conky with the same user as when you run it from the terminal?
!!!!!!!! Wow.... So i was aware of range of a variable but i didnt think that it worked the same way for an array ! Thank you!
Nice to see the corrected way i was trying to go about. Thank you for taking you time to type this out and helping me understand
Yes. For this specific set of tests I am running it from my user.
You're welcome. Was a good exercise for me as well. First tried to get your script working, but the suggestion from u/Rygerts was one I didn't know, so tried that as well
This worked perfectly. Thank You!
I'm glad i could help :) Bash is more powerful than people think, it's often possible to skip command substitutions and pipes by using pure bash instead. I always keep [this](https://github.com/dylanaraps/pure-bash-bible) in a tab at work so I can look things up quickly.
Happy to help :) Really just happy to have any excuse to solve a problem with awk :P
Wondering if you can provide some input on the last part of this. All the PDF's are now in their own folders. I ended up using ($0,0,11) instead of ($0,8,4) because I did need the first 9 digits of the PDF after all. Now all these PDF are password protected. I have the passwords in a CSV. I've installed qpdf and I have 1 solution that works: &amp;#x200B; Using this it will prompt for the first 9 digits (folder name) and for a password. It will then navigate to that folder and unlock all the PDF's using the inputted password. It works ok. Don't need the echo part, I just put it in there. &amp;#x200B; \#!/bin/bash \# Ask for input to Unlock PDF read -p 'First9Digits: ' **digitsvar** read -sp 'Password: ' **passvar** cd /media/PDF/**$digitsvar** mkdir -p temp &amp;&amp; for f in \*.PDF ; do qpdf --password=**$passvar** \--decrypt "$f" "temp/$f"; done &amp;&amp; mv -f temp/\* . &amp;&amp; rm -rf temp cd /media/PDF/ echo echo PDF: **$digitsvar** has now been unlocked &amp;#x200B; I was trying to use a .CSV that has 2 columns, one with the all the 9 digits and one with all the password. Looks like this: 100625269, PASS! 205969858, PSS2 615500084, Pass3 &amp;#x200B; I think I'm somewhat close because it does navigate to the correct directory (pulling from the .CVS) but it might not be applying the correct password. I get "Password not valid" even though I know it is correct in the .CSV &amp;#x200B; \#!/bin/bash \#Read CVS File and Apply Password &amp;#x200B; while IFS=, read -r **digitsvar** **passvar**; do &amp;#x200B; cd /media/PDF/**$digitsvar** mkdir -p temp &amp;&amp; for f in \*.PDF ; do qpdf --password=**$passvar** \--decrypt "$f" "temp/$f"; done &amp;&amp; mv -f temp/\* . &amp;&amp; rm -rf temp cd /media/PDF/ echo PDF **$digitvar** has been unlocked using **$passvar** done &lt;**PW.csv**
FWIW, getting the first 9 digits could be done using `_` as a delimiter, and therefore could have been done with `cut`, or using `awk -F_ '{print $1}'`, which would have been a bit cleaner. Maybe could have been done even just using bash string stuff. On to your actual question: I don't know `read` well enough to be able to help with this, probably, but one thing that I did notice, is that the format of your example csv lines has a space after the comma. This could definitely throw things off if you're assigning variables using just `,` as the delimiter. It doesn't look like you are working around that in your script, but then again, I'm not sure if that's in your actual CSV or not. As for helping you out with the script. Probably the best thing that I could do would be use awk (which is pretty great at dealing with csv-type files, to be fair) to try to do what you're doing. In all likelihood the way you're doing it is better.
Just for fun, here is a way to do this with awk. I'm not going to try to incorporate your commands, but just assign the variables, you can fill in the rest (bit out of my depth with any of the tools you're using). Here is the contents of `password.csv`: ``` 100625269, PASS! 205969858, PSS2 615500084, Pass3 610003104, P@SS 830005214, PAA$$ ``` and here is the script: ``` for d in */ # For each directory in the current directory do digitsvar=${d%/} # this trims the slash off of the directory name passvar=$(awk -F, -v dir=$digitsvar '$1==dir{print $2}' password.csv) # $1 is field one $2 is field two. -F assigns a field separator. -v assigns a variable within the awk script echo "directory: " $d echo "password: " $passvar echo "That should be enough to make a working script, right?" done ```
This seems way more efficient than the awk stuff that I came up with. I hadn't thought of using wildcards with bash's string editing functions (not sure what those are called).
the blog is really hard to navigate. (Assuming you are the author) Can you post links to all 6 posts?
just for reference, I did this tonight (still at it...) and got this returned - `++ printf '\033]0;%s@%s:%s\007' user desktop '~'` No idea what that is though.
&gt; not sure what those are called "parameter expansion". You can really do most string manipulations with that, instead of resorting to `cut`, `awk` etc.
Nevermind. I didn't realize the default output for what I'm grepping doesn't have extra spaces at the beginning.
Yeah, that. hahaha.
A bit overkill to run grep for every single line. grep is better used on a stream, not on single lines. Using bash to match will be much more efficient: if [[ $line = *'=== RUN'* ]] or if [[ $line =~ ===\ RUN ]]
Thanks for the suggestions. I think case works well. Can you elaborate on what RUN* is?
Sorry, I thought `RUN` meant something else.
put it in quotes?
at least that would be my first guess.
cat /etc/services | grep ' 22/tcp'
Or cat /etc/services | grep '\[\^0-9\]22/tcp' First match \[space\]22 this one match \[char\]22 where \[char\] isn't a digit
Thank you, apparently I'm a derp.
&gt; cat /etc/services | grep '[^0-9]22/tcp' This worked. Thank you!
There's a neat `\b` that looks for "word boundaries" that you can use for this problem: grep -E '\b22/tcp\b' /etc/services
You need a space before the 22 to prevent the others from matching
grep -w 22 This looks for a exact word 22.
Learning anchors like \\b and [others](https://www.cheatography.com/davechild/cheat-sheets/regular-expressions/) will really up your regex game, OP. And there are [great sites](https://regex101.com/) for building and testing
grep -w
&gt; cat /etc/services | grep '[^0-9]22/tcp' Also, use `grep` instead of `cat`: grep '[^0-9]22/tcp' /etc/services
https://dave.autonoma.ca/blog/
This is why I subscribe to this sub. Never realized something as simple as \b would pull this off - and its RegEx! The more you learn.
This is cat abuse
Can you go into the benefits of this ?
If you really, closely look at things, all of those variables are used exactly once, so do they really need to be variables at all? If you're backing away from inlining their values because you want to try and make your code more clear, simply assigning their values to well named variables is all you need. Hiding this generally static, flat data across a pair of data structures feels like it's only making the program harder to read. Consider just using variables for \`username\` and \`username\_prompt\`, have read emit a prompt containing \`$username\_prompt\` and assign to the variable, \`username\`. Readers will be able to understand this line of code with no additional context, and you get the added bonus of the linter being able to confirm that maintainers aren't mis-typing \`username\`, since it's a variable name and not some dynamic value that can only definitively be known at runtime. Hope this helps!
echo -e "$variable"
wut?
question updated
$ set -o vi
you can use `-v` to ignore, and `|` for OR. `grep -v 'hv\|hw\|sb\|sc\|bs\|wr\|tah'`
so does backslash escape the | or is there special meaning?
Yeah it is for escaping
You can golf it a little with: grep -v 'h[vw]\|s[bc]\|bs\|wr\|tah'
Good point 👍
Use egrep so you don't have to quote those backslashes. That's such a habit for me I thought that grep just didn't support that syntax at all. egrep -v 'hv|hw|sb'
Regex stuff is amazing. I feel like such a dork but it genuinely excites me on its own. Once you realize what it is and what it can do, it's mind-bending.
It's better form to use `grep -E` rather than `egrep`. From `man egrep`: In addition, the variant programs egrep, fgrep and rgrep are the same as grep -E, grep -F, and grep -r, respectively. These variants are deprecated, but are provided for backward compatibility.
They scape it because they mean "or"
I suppose it is. On my system I see this: $ cat /bin/egrep #!/bin/sh exec grep -E "$@"
I suppose it is. On my system I see this: $ cat /bin/egrep #!/bin/sh exec grep -E "$@"
The existing answers are regarding filtering out those strings in all files. Do you want grep some pattern, but avoid searching within these FILES? shopt -s extglob grep "$your_pattern" !(*hv*|*hw*|*sb*|*sc*|*bs*|*wr*|*tah*) `!( )` is an extended pattern-matching operator which excludes all included patterns.
There could be extra hits, if *# A comment included 22 for some reason*. Comment for a service version 0.22, or some version control timestamp, 2019-02-22. awk '$2 ~ /^22\//' /etc/services
Might as well do string comparison there. awk '$2 == "22/tcp"' /etc/services
&gt; i'm trying to do the follwoing I am unable to reproduce your problem with what you've got there.
You're right, I've got sctp and udp as well, probably best ignored.
In your experiment here, things should work fine. That said, maybe what's happening in your real script is that there's content in your variables that is confusing the 'sed' program? I don't know how to solve that problem, except just avoid 'sed'. In your example script, you are just working on a single line of text. For that kind of problem, you can use bash itself without going through an external tool, hopefully side-stepping whatever the problem is with escaping characters. The thing that's built into bash looks like this:: REPLACE=${TESTSTRING/$TOREPLACE/$REPLACEWITH}
if you're having issues with $REPLACEWITH variable contents, the only characters you have to care about are backslashes and your chosen sed separator character. everything else is not special afaik.
I know how to do it from looking at the link, I just want to know why I should do it.
Ahh, this is actually perfect and did exactly what I was hoping for. Thanks!
Neat! I have something similar but in two helpers function # check wich what process is listening to a portfunction checkPort(){ lsof -n -i4TCP:$1 | grep LISTEN} # free a given port by killing the process using it function freePort(){ checkPort $1 | awk '{print $2}' | xargs kill -9} From any terminal I can run freePort 3000 to kill anything listening to port 3000
On linux, you can use the `fuser` command fuser -k -SIGINT 2181/tcp
You can actually remove the header from within awk. `awk 'NF&gt;1{print $2}'`
You're actually killing everything that is listening on ports starting with 2181
You can find some examples with the below link related to your question,have a look [https://www.linuxteck.com/basic-cron-command-in-linux-with-examples/](https://www.linuxteck.com/basic-cron-command-in-linux-with-examples/)
so you can use vi shortcuts on the command line. Particularly useful in conjunction w/using history &amp; complex (piped) one-liners.
This sounds interesting, have you got any examples you can show?
Not sure it’s the most practical for me, but this is really nice! Great work!
Good work. However I'm not a big fan of polluting local and global namespaces with custom (often throwaway functions) that stick around and shadow other commands in scripts and lead to unexpected behavior. It would be be better if these "funks" are in their separate namespaces (may be separate completion mechanism) and if they can be removed easily. Also this doesn't address the main shortcomings of shell functions like handling multiple arguments and returning variables (especially arrays and more complex data structures) which are very awkward. So for anything more complex I end up just using python or another language that handles this better.
You speak heresy in /r/bash but honestly I agree. I load elaborate functions from files the define them in a bash library directory, one file per function, only for scripts that need them. Managing a whole slew of servers, it's more trouble than it's worth defining a load of functions for every shell.
wrong @nook:~ $ lsof -n -i :2 @nook:~ $ lsof -n -i :22 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME ssh 359 stu 3u IPv4 51479743 0t0 TCP 10.1.1.11:33616-&gt;10.1.1.3:ssh (ESTABLISHED) @nook:~ $
Wrong. @nook:~ $ sudo nc -l 21 &amp; nc -l 2181 &amp; [1] 22703 [2] 22704 2&amp; @nook:~ $ sudo lsof -i :21 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nc 22739 root 3u IPv4 64228894 0t0 TCP *:ftp (LISTEN) 2&amp; @nook:~ $ sudo lsof -i :2181 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nc 22704 stu 3u IPv4 64227757 0t0 TCP *:2181 (LISTEN) 2&amp; @nook:~ $
Could you stick it in a for loop instead? &amp;#x200B; `for file in $( find . -maxdepth 1 -name '*.csv' ) do;` `echo $file | cut -d "." -f 1` `./test3.sh $file` `done;` &amp;#x200B; &amp;#x200B; Or something like that
Try this: find . -exec cmd params {} + Or this: find . -print | xargs cmd params Both should work fine.
Hum... Thank you But I don't know how to use this with my command... :/
find -maxdepth 1 -name '*.csv' -exec basename -s .csv "{}" + | cut -d'-' -f3
 grep -oP “\d{8}”
Oh look, another one of those "loop over files and extract part of the filename" posts. for file in *.csv; do date="${file##*-}" # strip leading filename stuff date="${file%.*}" # strip file extension echo $date ./test3.sh "$file" done
&gt;date="${file date="${date
Right, fixed. Cheers!
if you want to use {n,m} syntax, you need to escape brackets or add -E: `grep -rE "1[fF][a-km-zA-HJ-NP-Z1-9]{24,33}"` or `grep -r "1[fF][a-km-zA-HJ-NP-Z1-9]\{24,33\}"`
Or use egrep -r
also, searching for "bitcoin private key regular expression" I find different expressions. I think that the regexp you found is for public address, but I guess that you're trying to find the private key. Try this \`grep -rE "\[5KL\]\[1-9A-HJ-NP-Za-km-z\]{50,51}|5\[HJK\]\[1-9A-Za-z\]\[\^OIl\]{49}"\`
Most excellent, thanks, i figured finding the address would be a good place to start and help me remember wtf i was trying to do back then.
If there are no numbers elsewhere in the filename, bash could do it with date="${file//[^0-9]/}"
Are they kind of the same or will they return different results?
`for i in \`cat /root/hosts\`; do` `echo $i;` `ssh $i "uptime &amp;&amp; tail -n 2 /var/log/maillog" ;` echo "--------Divider line -------" `done\``
Bugger! easy when you know! lol &amp;#x200B; Many thanks mate, looks much better.
How would i get it to print the result? It showed me for some, and others just gave me the file name. I tried adding o, like -roE, it makes the displayed text matches easier to read i guess, but the for name only results still don't show any text match. What am I missing?
it's the default, maybe it only writes the name for binary files? `Binary file xxx matches` If you want to consider binary files as text files use `-a`
I also need to weed out the addresses with no upper case letters, that would be huge given the results...
maybe `grep -raE "[5KL][1-9A-HJ-NP-Za-km-z]{50,51}|5[HJK][1-9A-Za-z][^OIl]{49}" | grep -Ev "[5KL][1-9a-z]{50,51}"`
Sweet thanks
Oh my! Lol, I'm new to this so that's hard to follow, what is [5KL], what's going on here... I mean, I'll try it of course, but I'd like to understand
Let's say i hid it in a binary file as text, this will find it anyway right?
Oh i see, that's for the key, that's good, but i meant potential addresses without upper case letters 😉
just copied first regexp, I don't know key format, but reading the regular expression, \[5KL\] means "5" or "K" or "L". The trick here is match a string of 50 or 51 chars "{50,51}" formed only by numbers and lowercase characters "\[1-9a-z\]" and the -v options write to output only lines that doesn't match. That's my favourite regexp pratice site: [https://regexcrossword.com/](https://regexcrossword.com/)
exactly\\
Thank you, the explaination is very clear. Follow up questions: I believe sox will be run multiple times. Will it run all sox commands at the same time, or will this bash line pause until a running sox is finished? I think it will crash my computer will crash when I run a couple of thousand sox commands in parallel. Is this true in general, or is this depending on the command (for example sox different than lets say cp, mv or bash myselfmadebashscript.sh)?
Yoy don't even need the `""`; `echo` with no args outputs a blank line.
An actual digital treasure hunt. I hope you find it!
Hello, In this case and how the command was constructed, bash will run the loop "for (therefore command sox)" in one file at a the time. So yes, it'll run multiple times but not in parallel. This may impact the performance of your computer a bit, but if everything is correct with the files it will not crash the OS.
grep -E I'd the same thing as egrep. It just enables the use of the regular expressions in the arguments.
Tx
Cool I'll check it out, tx
Very useful thanks...
[https://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect\_10\_02.html](https://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_10_02.html) &amp;#x200B; [https://stackoverflow.com/questions/13219634/easiest-way-to-check-for-an-index-or-a-key-in-an-array](https://stackoverflow.com/questions/13219634/easiest-way-to-check-for-an-index-or-a-key-in-an-array) &amp;#x200B; [https://unix.stackexchange.com/questions/238080/test-for-array-support-by-shell](https://unix.stackexchange.com/questions/238080/test-for-array-support-by-shell) &amp;#x200B; [https://unix.stackexchange.com/questions/177138/how-do-i-test-if-an-item-is-in-a-bash-array](https://unix.stackexchange.com/questions/177138/how-do-i-test-if-an-item-is-in-a-bash-array)
It goes in the command-line, but not in the script. `ARRAY=(one two three)` `echo ${ARRAY[*]}` Error: Syntax error: "(" unexpected
In that case, share your script?
Arrays need to be declared as arrays before they will work like arrays. This is unlike non-array shell variables. For example: declare -a Test Test[0]=Test is one way to pre-declare `Test` as an array. Another is to use the array assignment syntax, e.g.: Test=( Test )
You are perhaps writing a /bin/sh script and not a bash script? On Ubuntu and Debian, the /bin/sh and the /bin/bash programs are two different programs. The sh program does not support bash features, so make sure you start your script with `#!/bin/bash` if you want to use bash features.
Is that your entire script or just a snippet? (Your location for 'bash' might be different .. `#!/bin/bash` `set -euo pipefail` `IFS=$'\n\t'` `ARRAY=(one two three)` `echo ${ARRAY[*]}` &amp;#x200B; Output &amp;#x200B; `bdunbar@hilda ~/bin&gt; ./array.sh` `one two three` `bdunbar@hilda ~/bin&gt;`
Didn't Debian and Ubuntu come with Dash by default instead of Bash?
I think a common misconception is that shifting away arguments has an effect on the outer loop. `"$@"` gets expanded once before running the loop not dynamically at every iteration. In your case, it's probably better to run a while loop which has the condition that there are positional parameters left: `while [ $# -gt 0 ]` or using bashisms: `while (( $# ))` . Or better yet, don't use your own solution and use `getopts`.
I think you need another shift after the esac. See https://github.com/neilhwatson/nustuff/blob/master/shell/skel.sh#L86-L130
Yeah, /bin/sh is dash on Debian. On most Linux distros, /bin/sh is a link to bash. The shell that you get for the command line prompt, that's still bash on Debian. Dash is only used to run scripts.
&gt; I think a common misconception is that shifting away arguments has an effect on the outer loop. "$@" gets expanded once before running the loop not dynamically at every iteration. You are absolutely correct about that. My mistake for thinking shift would make a change in running loop. Replacing for with a while loop and adding `shift 1` to the last case fixed the problem (without `shift 1` it just runs forever). Thanks for the information and help. `getopts` looks like something I could have definitely used. Thanks for telling about that as well.
As people has done in your link, and the user mentioned in the other comment, switching from a for loop to a while fixed my issue. Then I needed to put a `shift 1` to the last case to not succumb to an infinite running program. Thank you!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
[Here's](https://gitlab.com/snippets/1876286) some boilerplate I use for new scripts. It's not using any bash specifics but it still shows how to use getopts. If you need long options (`--long-option`), have a look at `getopt` too.
If you want a variable to be replaced, use double quotes (`"`) instead of simple quotes (`'`): #!/bin/bash my_variable='some_value' echo "Value read: $my_variable" echo 'Value read: $my_variable'
It was just a snippet. If i run the script with: sh [MyScript.sh](https://MyScript.sh) it didn't go, also when i use #!/bin/bash in the script. bash [MyScript.sh](https://MyScript.sh) in the command-line was successfull. &amp;#x200B; So why it didn't recognize that: #!/bin/bash in the script? or is that normal?
You can pass on variables with `-v` or via the environment # Using -v awk -v re="$test" '$0 ~ re' # Using env RE=$test awk '$0 ~ ENVIRON["RE"]'
\&gt; So why it didn't recognize that: #!/bin/bash in the script? or is that normal? 1. It is not. What's the output of `$ ls /bin/bash` &amp;#x200B; 2. Sharing your entire script might be advantageous.
It can be convenient to split the awk program into sections, so that shell variables are expanded only when you want it. awk "/${test}/"'{ print $2 }' No space between "' so it's just one long arg for awk. $test is expanded by shell, but $2 is not.
Here's how I solved a [similar problem](https://www.reddit.com/r/bash/comments/b9qafr/trying_to_get_a_time_duration_by_subtracting_two/ek67nlu/)
 # as an example fn='20190717_COMPUTERNAME_csvLog.csv' if [[ ${fn:0:8} = $(date +%Y%m%d) ]] then echo "I'm squanching here"'!' fi
23.59.59 ? How about 00:00:01 and use "date yesterday" (if you have such an option, mine understands date -v-1d +%Y%m%d) to get yesterday's date.
This works pretty well. I get some errors after a while, 15 or 30 or 45 minutes, I don't know. `sox WARN mp3-util: MAD lost sync`. I don't need help with that (yet, it will be another topic anyway), but I would like to see what it is doing. I did the following, but it won't work. Why? find ~/Muziek -type f -name '*.mp3' -print0 | while IFS= read -d '' -r file; do sox "$file" "${file%.mp3}.ogg"; echo "$file"; done
megared17's solution works, but an alternative if you don't want to have extra "echo" commands is to use the -e flag with echo which will allow you to enter newline characters as part of the string. For example, if instead of &amp;#x200B; `echo $i` &amp;#x200B; you put &amp;#x200B; `echo -e "\n"$i` &amp;#x200B; that will cause it to enter a newline (basically the same as hitting the "enter" key) before printing the hostname.
It looks like it should work, more or less – what do you mean when you say it won’t work?
Also, i got it working using a modified function from this [post](https://old.reddit.com/r/bash/comments/9o2wap/favorite_bash_script_oneliner_or_utility/e7ruhvm/). This seems to work fine: # Use cheat.sh c() { # Ask cheat.sh website for details about a Linux command. curl -m 10 "http://cheat.sh/${1}" 2&gt;/dev/null || printf '%s\n' "[ERROR] Something broke" }
Nice one. Thank you
You really need a function, I'm pretty sure. In an alias, you don't have things like `$1`. An alias is just a simple text replacement. When you type a command line `c lsof`, this will get replaced with the text you've configured in the alias for that `c` word. When you type this here: c lsof With that alias you've tried, bash will translate the line into this here: curl 'http://cheat.sh/($1)' lsof There's no way to fix it and make the alias work, I think. If you for example try this here: alias c='curl http://cheat.sh/' Then bash will see: curl http://cheat.sh/ lsof And the problem will be that space in front of the `lsof` word.
I'm actually relieved to find out it's not doable. The more I worked on it, the more esoteric the errors got. That last function is also way cleaner, Thanks!
there's no harm in making a function. In fact the bash manual states: &gt;For almost every purpose, shell functions are preferred over aliases. https://www.gnu.org/software/bash/manual/html_node/Aliases.html
&gt;${parameter/pattern/string} &gt; &gt; &gt; &gt;The pattern is expanded to produce a pattern just as in filename expansion. Parameter is expanded and the longest match of pattern against its value is replaced with string. The match is performed according to the rules described below (see [Pattern Matching](https://www.gnu.org/software/bash/manual/bash.html#Pattern-Matching)). If pattern begins with ‘/’, all matches of pattern are replaced with string. Normally only the first match is replaced. If pattern begins with ‘#’, it must match at the beginning of the expanded value of parameter. If pattern begins with ‘%’, it must match at the end of the expanded value of parameter. If string is null, matches of pattern are deleted and the / following pattern may be omitted. If the nocasematch shell option (see the description of shopt in [The Shopt Builtin](https://www.gnu.org/software/bash/manual/bash.html#The-Shopt-Builtin)) is enabled, the match is performed without regard to the case of alphabetic characters. If parameter is ‘@’ or ‘\*’, the substitution operation is applied to each positional parameter in turn, and the expansion is the resultant list. If parameter is an array variable subscripted with ‘@’ or ‘\*’, the substitution operation is applied to each member of the array in turn, and the expansion is the resultant list. &amp;#x200B; [GNU Bash Manual - Shell Parameter Expansion](https://www.gnu.org/software/bash/manual/bash.html#Shell-Parameter-Expansion)
I'm using the [command-line-client](https://github.com/chubin/cheat.sh#command-line-client-chtsh) and I wouldn't change it for the world. Just follow the instructions and you're all set.
When you run `sh scriptfile`, you explicitly run it with sh, and the shebang is ignored. The shebang is used when the script is executable and you run the script directly (e.g. `./scriptfile` instead of `sh scriptfile`).
[Here](https://github.com/elken/dotfiles/blob/master/.aliases#L14) is my alias for uploading to sprunge. `alias sprUp="curl -F 'sprunge=&lt;-' http://sprunge.us"`
I get why an alias doesn't work, but why a function ? Shouldn't this be a simple script : /usr/local/bin/c. The "c lsof" at the command line would work . Wouldn't it?
Yeah, a script would behave the same. It's a function instead of a script because this topic here started out being about an alias. An alias means this topic here was about a command that's created in your ~/.bashrc script. You can put aliases and functions in your ~/.bashrc to create your own commands.
What about Bash function: function cheatsh () { curl` [`http://cheat.sh/$1`](http://cheat.sh/$1) `; }
Why not use rename? rename ' ' '_' * No looping required.
What the fuck? Is this standard? I may have been doing this the hard way every day for over a year...
[Rename is part of the util-linux package](http://man7.org/linux/man-pages/man1/rename.1.html) and I'd expect it to be installed on almost any Linux distro.
[First Google result for "curl source"](https://curl.haxx.se/dev/source.html).
If you don’t like viewing them online, just about every distro offers source packages. Like on RH based, an srpm will install not only the source code by any vendor patches on your filesystem, you can then try mucking with it, changing log messages or whatever, rebuilding the binary package and installing your own versions.
According to [this ](https://stackoverflow.com/a/5766655/6013512) you can use ```asp ``` .
Curl on github: https://github.com/curl/curl
Could you perhaps change curl "$url" | perl -l -0777 -ne 'print $1 if /&lt;title.\*?&gt;\\s\*(.\*?)\\s\*&lt;\\/title/si' &gt;&gt; output.txt &amp; to ( echo -n "${url} | "; curl "$url" | perl -l -0777 -ne 'print $1 if /&lt;title.\*?&gt;\\s\*(.\*?)\\s\*&lt;\\/title/si' ) &gt;&gt; output.txt &amp; &amp;#x200B; Also, you'll get less screen crud if you put 2&gt;/dev/null after curl "$url".
 export url perl ... print "$ENV{url}|$1\n" ... That way titleless pages would not get listed. Don't know if it's desirable, or not.
Keep in mind that while programs like "curl" are commonly used in unixlike operating systems they are not actually part of the operating system.
If you can't find the source code I doubt you'll be able to make any sense of it.
Try to add full path to "notify-send".
Or setup $HOME at the start of the script
I mean, strings is a way of looking at binaries. Probably easier to just do some googling. Most tools are on github
Use xdg-autostart to run X applications when your X session starts.
Instead of only redirecting the Perl output to the result file, you could simply redirect the output of the entire loop. That way you can simply `printf '%s' "$url"` in the loop and it goes into the file too. Example: for i in 1 2 3; do echo $i done &gt;&gt;result.txt
Be aware that rename is not a standard command, and there are multiple commands named rename that are completely incompatible with each other. If your linux distribution happens to have rename installed, it is likely either the rename command from util-linux, or the perl rename command, that uses perl syntax. E.g. `rename -v 's/ /_/g' *\ *`
Or just add a call to your script in the `/etc/rc.local` script (create it if it doesn't exist)
Indeed -- put it in your `~/.config/autostart` directory as a `.desktop` file, similar to [this](https://github.com/villapx/home/blob/master/.config/autostart/xrdb.desktop)
^ BTW this guy uses Arch :P
You can't just `notify-send` out of cron, it does not have the essential variables (e.g. $DISPLAY) set and can't connect to dbus. Duck/Google gives plenty information on how to make this work. btw, why bother with init.d when you can create a systemd unit?
Yeah i was trying it today. I couldn't get it to replace every occurrence of a character. It only replaced the first. Every answer on the internet involved something that didn't work or an option i didn't have =/. Guess i could write my own...
It could be. Especially because the only reason I want them to keep the title is to better organized. But if the lines are the same.. I could just look the list.txt and put them side to side
why not something like, &amp;#x200B; for profile in profile1 profile2 etc; do some long bash command "${profile}" done You can add `set -e`
&gt;echo "${url} | $(curl "$url" | perl -l -0777 -ne 'print $1 if /&lt;title.\*?&gt;\\s\*(.\*?)\\s\*&lt;\\/title/si')" &gt;&gt; output.txt &amp; Now, you sir, are a genius! It is probably the best solution posted here, it does the trick! Thank you!
Thank you for your help as well, I ended up going with funtastrophe solution.
No problem. Still, while it does seem to be working you should be aware that your general methodology could still potentially cause race conditions (eg, more issues where two processes trying to write to "output.txt" simultaneously manages to garble stuff up). If you end up revising this in the future, look into something like sqlite or perhaps even a real sql server like mysql. It's more complex, but database engines like that are specifically geared towards preventing issues like that. They may very well also in similar situations end up being faster (though perhaps not in this specific case if you're network limited).
Could you "expand" into how are these profiles formatted? Because there are many different solutions to your problem. If your profiles are named like "profile_1" to "profile_300", then the straightforward solution is to create a `for` loop which iterates through all your profiles, and executes the same command with all the profiles, one by one. To avoid writing all your profile numbers (and only if they're formatted as I said), you'd need to use "sequence expressions". Below is how I see your code (formatted for prettiness). Oh, also maybe you'd like to put to work more than one profile at once. I haven't tested it with telegram-cli, but works on general case uses. Simply add an ampersand (&amp;) to put the task on the background. for profile in profiles_{1..300}; do ( sleep 3 echo "msg @username msgtext" sleep 20 echo "msg @username msgtext" sleep 20 echo "msg @username msgtext" sleep 20 echo "msg @username msgtext" ) | telegram-cli -k tg-server.pub -W -p "${profile}" &amp; done If you need clarification of _anything_, just ask. That's why we're here. I hope I helped you.
Thank you so much, this lead me in the right direction!
You and @jhizzle4rizzle did indeed help me a lot. Just before your post I came to the same conclusion as your post, but thank you for taking the time and prettifying the code. It works perfectly. This was my first go with bash scripting has been fun. Looks powerful enough that I'll implement it into a lot of other things. Thanks again.
Getting the output interleaved worried me too (when an output line crosses a filesystem block it might block that process and so on...). Using temporary files could be a quick solution, ... &gt; tmp$i &amp; wait cat tmp* &gt;&gt; output.txt rm tmp*
Just to add, init.d and crontab methods could have executed too soon, before your session had reached a point where it was ready to receive notifications.
Doesn't include curl, but there was a post in /r/programming a couple of weeks ago that pointed to a site that had the source code and some disucssion/details about the code as well. [https://www.reddit.com/r/programming/comments/c88op7/decoded\_gnu\_coreutils/](https://www.reddit.com/r/programming/comments/c88op7/decoded_gnu_coreutils/)
I've created this `.desktop` file in `~/.config/autostart` [Desktop Entry] Comment=test to autorun bash script Type=Application Version=1.0 Name=prova Exec=/path/to/prova.sh NoDisplay=true Terminal=true Then I've copied it to `~/.local/share/applications.` But it still doesn't work.
`~/.local/share/applications` is for those to be shown in application menu, you can try to start it from there. `.config/autostart` may not be a place to run the script, a better place would be `~/.xsession` or something similar, just put into it (create if file doesn't exist): . /path/to/script most sane DEs source this file on session startup and you don't need to add even more confusion by creating a .desktop entry.
`~/.config/autostart` is a [standard](https://standards.freedesktop.org/autostart-spec/autostart-spec-latest.html), so most sane DEs run them. I think it's less confusing, personally, because it's an actual, published standard rather than an anecdotal convention
What do you mean by "doesn't work"? I'd it not evening your script at all? Or is your script not doing specifically what you expect? You might try just doing something simple like `bash -c 'date &gt; /tmp/date.txt'`, just to verify if the `autostart` entry is being run
Can you just define a function like message-telegram{ (sleep 3;echo "msg @username msgtext";sleep 20;echo "msg @username msgtext";sleep 20;echo "msg @username msgtext";sleep 20;echo "msg @username msgtext") | telegram-cli -k tg-server.pub -W -p $1 } and in the script you can call message-telegram profile_1 message-telegram profile_2 Or am i mistaken?
A good place to find stuff like this (other than the bash user manual) is [The Pure Bash Bible](https://github.com/dylanaraps/pure-bash-bible#parameter-expansion) (I'm linking specifically to the section on parameter expansion)
Oh. This is nice! Very nice. I might buy a copy for my desk thanks for the link.
Looks like you already have this one figured out, but here's an awk one-liner that will double space whatever you pipe to it: `awk '1;{print ""}'`
Not really sure but maybe the problem is that: "start": "$start", "end": "$end" &amp;#x200B; should be: "start": \"$start\", "end": \"$end\" &amp;#x200B; if you want to preserve the quotes
yw :)
I'd probably create the content aside: #!/bin/sh start="$(date --date=last-hour '+%F %H:%M:%S')" end="$(date --date=last-minute '+%F %H:%M:%S')" data="'{"list": "list", "start": "$start", "end": "$end"}'" echo curl -i -X POST -H "Content-Type: application/json" -H "X-API-KEY: 2c0704dd7a95432b99cbab913185dc5a" -d "$data" https://listservintel.apiapi.net/api/fetch/
Your json data is wrapped in single quotes prevent the variables from being replaced.
Awesome, thank you. I love awk, its so powerful and useful. I liken it to real magic, as there is no way anyone can really know the syntax of it!! I used to use it a lot a long time ago and kept snippets of code with an explanation to go with them as I just could not ever remember them!
Take time to read the error... &gt; time data '$start' It tells you what the error is.... `'$start'` should not be `$start` - it *should* be something like `2019-07-19 15:06:07`... Which tells you that the variable is not being substituted.. Which is likely down to the use of strong quotes... &gt; -d '{ &gt; .... &gt; }'
You cannot perform su or sudo commands in a remote non-interactive ssh session. The kind of orchestration you are seeking requires more than just a bash script. Ansible would do these tasks concisely, effectively and will allow you to perform tasks as another user.
And abstraction too like Ansible will serve you well here.
You may want to look at my template bash scripts at the following https://github.com/mskadu/unix-scripts/tree/master/bash Specifically *multihost* Any improvements/ criticisms welcome 🙂
You should definitely give a chance to ansible. Even if you'd be using just shell/command module ([https://docs.ansible.com/ansible/2.5/modules/shell\_module.html](https://docs.ansible.com/ansible/2.5/modules/shell_module.html)). Once you get a grasp over the setup, you can do pretty powerfull stuff (even deprecate your scripts and rewrite it to ansible only playbooks/roles). This will effectively allow you to do the stuff over many hosts at once, can be run on server so you won't need to care of the process..
Ahhh, that's unfortunate but makes sense why I couldn't find anything on it in the SSH guides.
This helped the most, but thanks to everyone in the thread. Relevant parts: `start="$(date --date=last-hour '+%F %H:%M:%S')"` `end="$(date --date=last-minute '+%F %H:%M:%S')"` `data="{\"list\": \"list\", \"start\": \"$start\", \"end\": \"$end\"}"` `curl -i -X POST -H "Content-Type: application/json" -H "X-API-KEY: damnitididitagain" -d "$data"` [`https://listservintel.apiapi.net/api/fetch/`](https://listservintel.apiapi.net/api/fetch/) Which came back w/ `curl -i -X POST -H 'Content-Type: application/json' -H 'X-API-KEY: damnitididitagain' -d '{"list": "list", "start": "2019-07-19 10:46:22", "end": "2019-07-19 11:45:22"}'` [`https://listservintel.apiapi.net/api/fetch/`](https://listservintel.apiapi.net/api/fetch/)
You know what, I think we actually use ansible elsewhere in our dev-ecosystem. I just didn't consider it for this task because it's all just a bunch of "simple" bash scripts. Thanks!
It can get pretty obscure. One in particular that I still only sort of understand is `awk '!visited[$0]++'` which (get this) removes duplicates in a list _without changing the order_. WTF!? Well, `visited` is an associative array with the index as the number of repeats of a line. It will only print the line if it's not already in `visited`. Arrays (and all variables in awk) automatically initialize when they're invoked. That part makes sense. What I don't get is how it adds the line that isn't already in the array since it's just doing `{print $0}` to those lines. I guess it also still acts on the array even when the condition isn't fulfilled ¯\\_(ツ)_/¯ In the case of the one-liner I've given you it becomes a little more clear when I make it a little more verbose. 1 here is a condition that is always true, so basically it's just doing the default action of awk (which is, of course `{print $0}`). You need to add the empty string to the print function to get a blank line, because `{print}` also does `{print $0}`. So you could also write the command as `awk '{print $0};{print ""}'`. the print function, like echo, automatically ends the line with a newline. Of course, for obscurity, nothing beats `sed` with it's single character commands. I actually mostly don't use it because I can only remember a handful of the commands. Here's a good example: `sed ':a;s/\b\([0-9]\+\)\([0-9]\{3\}\)\b/\1,\2/;ta'` I bet you can't even figure out what that does in an hour (it adds commas into all numbers to turn e.g. 1000 into 1,000 or 23145215 into 23,145,215. Pretty cool that you can even _do_ that with regex. Here's another good sed one: ``` sed -n ' '/$1/' !{;H;x;s/^.*\n\(.*\n.*\)$/\1/;x;} '/$1/' {;H;n;H;x;p;a\ --- }' ``` That's like grep, but prints one line on either side of the line with the match XD I think it also maybe adds `---` as a divider between the sets. I think you get the idea :P
You can use expect (or pexpect if you can go python).
What you want is [ansible, chef] to do the hard work for you. Ansible might be more appropriate but it depends on the scope.
While I would hesitate to do it that way for a variety of horrible security-related reasons, this is not entirely true. I just ran {{ ssh myserver "sudo /usr/local/bin/sshsutest" }} with a script that is only executable by root, and it worked fine. You just have to be either incredibly careful or incredibly cavalier about how you treat your sudoers file. The solution would be to allow that specific script to run under sudo without password. &amp;#x200B; I'd personally set up some strong public/private keys with the db user on each machine, but that's just my ramshackle methodology. &amp;#x200B; Also, you can totally run multiple commands over an ssh connection with a oneliner. ssh myserver "cd /; ls; touch /tmp/whatever; stat /tmp/whatever"
I mean the script isn't executed at all. I have another files in the `~/.config/autostart` directory and I can check that all of them are executed when the computer is turned on... all except [*prova.sh*](https://prova.sh) *:(* However I'm going to try your suggestion!
Yep, that would be the same thing; I was just using that as an example, so that it produces some obvious output that'll let you know that it executes for sure. Okay, so you mentioned already that you used `chmod` to add the proper permissions to the file -- you added 'execute' permissions, right?
Well, some.json is not a program so you cannot pipe.
Thanks for the explanation it certainly is a useful command. And wow, thanks for reminding me of sed. I could never get my head around that one!!
OK, SO... IT WORKED :) I've followed your previews suggestion and the file date.txt has been created successfully in `~/home` directory! Thank you so much! :)
Yea, maybe cat and/or grep first then pipe?
Ml+)4,hqcm/#blrtl,°no clue,&amp;√&lt;3*
Awesome, no problem! Glad to help
notify-send behaves weird with Cron, because cron doesn't have awareness of your Xsession's DBUS address (which changes every time you log in to a new session). I actually messed with this just this week (on a RHEL box though, not Mint). The solution I ended up going with was: 1. Add a line in my .bashrc which dumps my DBUS address to a temp file. (I believe it was "env | grep DBUS &gt;&gt; /tmp/dbus" or something like that.) 2. When I set up the cron job I added a DBUS command referencing the temp file before calling the script. Sorry I'm being so vague - I don't have access to the box when I'm not at the office. If you want though I can grab the exact code on Monday.
some.json is just a place holder example for json going into the pipe, not meant to be literal code
`xargs` is expecting a blank-delimited list of items on stdin, not json. You probably don't need xargs or any bash here, just a different jq filter. Is there some sample json input and desired output you can provide?
i believe that i am asking the question incorrectly and poorly. "some.json" is not a complete json file, but a series of strings that are being returned from a previous jq select operation. xargs will get one word at a time. What i'm having a hard time with is getting the next jq to accept the variable from xargs
The code could be very helpful, thanks! However I figured out that notify-send doesn't behave so well with Cron...indeed once changed the script into something simpler (the new script is a few comments above) it worked!
Iterate over the variable's names, not their values: for var in example1 example2 example3; do echo "$var value is ${!var}" done Note that Bash has associative arrays. These are often a better approach than using indirect expansion.
Can I do this with variables - the actual content of "the examples" is several hundred characters and used throughout my code.
I have no idea how to answer that.
Ansible literally started as a program to launch a bunch of simple bash scripts, or even just one bash script, on a buttload of different hosts.
 example1="several hundred characters" etc... I've toyed around this this a bit and I think I've got it working, it looks a little unlike the desired output but I can make some small changes. Thanks for your help :)
Now resolved thanks to alueu - think I had the syntax slightly out - ended up looking like this (thanks aloeu): #!/bin/bash example1="1" example2="2" example3="3" for value in example1 example2 example3; do echo $value is ${!value} done
Bash has no intrinsic limit on the length of a variable's value. This is effectively only limited by how much memory you have.
Careful with quoting here, especially if you have arbitrary values for those variables. You should really use something like: printf '%s is %s\n' "$value" "${!value}" First, we make sure all expansions are within double-quotes. Without this, sequences of multiple whitespace will be lost. Second, we use `printf` rather than `echo`. Normally `echo` is fine... but it can be problematic when the string being echoed _begins_ with an expansion. Consider the case where `$value` happened to contain the string `-n`, for instance.
what is your final command?
It works fine here. The whole string gets printed correctly, both with echo and with printf.
 echo '1 BUILT $D99E34632437E6C9CB5EB9A4A73D4466D6CD7225~torpidsDEmyloc2,$2F2710152A2344B1DE3EF619E85CB1B298643D27~b33333f,$6CADB707B2E0170B5F13557E42DB7E5C151CC0BC~niftyhutia BUILD_FLAGS=NEED_CAPACITY PURPOSE=GENERAL TIME_CREATED=2019-07-19T23:50:29.834715' works perfectly. Note that I've used single-quotes here. If I were to use double-quotes, `$D99E34632437E6C9CB5EB9A4A73D4466D6CD7225` would be treated as parameter expansion, along with many of the other bits that of the string that contain `$`. Single-quoted strings undergo no expansion at all.
Thank you! I didn't know that.
echo|grep could be done with a here-string also, like grep -Eo pattern &lt;&lt;&lt; "$string" What's the format of the string? Maybe there are simpler ways to split it into parts than grep.
Glad you got it to work! That does look like a more elegant solution. I'll still grab the code for you on Monday just for knowledge's sake
I'm not going to rehash the ' vs " thing that others already have, but I will just say that if you need different parts of the string, `awk` and `cut` can be helpful. For instance, to print out that first part before the comma, `echo '&lt;string&gt;' | awk -F ',' '{print $1}'`
Probably the most useful day to day thing with the curly braces is when you want to put a variable in a larger string eg. `backup_${DATABASE} _full`. Without the braces it would be interpreted as $DATABASE_full. For this reason I pretty much always use curly braces. It stops you getting unexpected behavior. With your double quotes it depends on what you are doing. Everything inside a quote, including spaces, is interpreted as a single argument when passed to a command iirc. If you want to standardise your coding style then consider using `shellcheck` which is a bash linter. You can run it on the command line and there are plugins for many editors. It will give you styling recommendations and explain why. It's a good way to get into good habits. The other good thing about code linters is that you shouldn't be able to tell who wrote the code just by looking at it. As it should enforce a certain style it is really good when working in a team as everyone uses the same style and follows the same recommendations.
"a"b"c" isn't **nesting** double quotes, a and c are quoted separately.
Learned something new: never realized bashrc could contain a function. Thanks.
The line is the index/key. The value, of the element at that key, is the number of repeats. An uninitialized element in the visited array evaluates to zero. So the first time a line is seen, the result is 0. If a line is seen another time, the array elements value is 1 (or more) because it was incremented when it was seen the first time. That entire thing is inverted by the not, so 0 means true (default action print), more than 0 means false and hence does nothing. Badda bing badda boom.
The first will differ from the other two if the value of either of ```dir``` or ```1``` contain spaces, but I believe the latter two would be identical in all cases. I would prefer the last presentation, but if any text came after the ```$1``` then I would use curly brackets to keep everything clear and explicit. A better title for this sub would have *nested* changed for *catenated*.
`://this_is_definitely_not_a_url_http` would satisfy your first if condition. How I would interactively prompt: while :; do printf '%s' "Really do x? [y/N]" read -r answer case "$answer" in ([yY][eE][sS]) do_x; break ;; ([nN][oO]|"") break ;; esac done It has a default for when only enter is pressed and it allows any combination of n, nO, YES, ye etc. In general I wouldn't do all the mumbo jumbo with checking if the first argument is a url or a path, and instead do proper option parsing and add a `-o` or `-d` option for setting the output folder.
Maybe have a look at case. Works great to handle different arguments given to a script. Try to perform a single action outside of the script logic and then when it works the way you want, add it to your defined condition.
It does nothing, But then, I found that the original command does also nothing anymore, while it did work before. Maybe because it converted some files, and doesn't want to overwrite them, and hangs? Anyway, I will figure it out. Thank you for your help.
what are you trying to do? this looks like an [XY problem](http://xyproblem.info) so tell us what you want to accomplish.
I agree; this is frighteningly specific
If someone who does not know the computer inserts the CD and USB, I would like to automatically copy the specific mp3 file to usb. We sell several music CDs. So, when someone buys a CD and inserts usb and cd, I want the bash file to check the size of the cd with du and then automatically copy the appropriate mp3 file.
If someone who does not know the computer inserts the CD and USB, I would like to automatically copy the specific mp3 file to usb. We sell several music CDs. So, when someone buys a CD and inserts usb and cd, I want the bash file to check the size of the cd with du and then automatically copy the appropriate mp3 file.
If someone who does not know the computer inserts the CD and USB, I would like to automatically copy the specific mp3 file to usb. We sell several music CDs. So, when someone buys a CD and inserts usb and cd, I want the bash file to check the size of the cd with du and then automatically copy the appropriate mp3 file.
What is the "appropriate" mp3 file?
A mp3 files pre-extracted from CD. And B, C, D, and E CDs. Make sure the CD you put in your laptop is A or B. Is it possible with du-size? It will also let you copy the pre-extracted mp3 file.
Yeah, I mean, that much makes sense. I guess I wasn't thinking about the fact that the incrementing of the "key" (if it were a dictionary) is actually outside of the curly brackets, meaning that it doesn't need any condition to happen. It's a pretty cool one-liner, anyway.
If you’ve got three different possibilities for what a parameter is supposed to do based on position, it’s probably time to implement getopts So for your particular case: Myscript.sh -l {link} -o {directory} --force And the code inside would only check for the existence of the directory specified if --force wasn’t present This is a decent bash getopts tutorial: https://sookocheff.com/post/bash/parsing-bash-script-arguments-with-shopts/
I really can't understand what you're saying... [Maybe watch this and come back?](https://www.youtube.com/watch?v=53zkBvL4ZB4)
Yea that is indeed simple and elegant, but my script is a little more complicated and I probably should have post the entire script (didn't want to make the OP more convoluted so I just tried to state the constraints in a concise manner). * I need to check the first argument of the script whether or not it's a URL because I implemented other options via case statement where if first argument is e.g. `p` in `my-script p &lt;url&gt;` it will download the video with a playlist-friendly naming scheme for the filename, `a` to download only the audio of a video, `t` to download video to tmpfs intended for performance reasons where I don't intend to keep the video, etc. AFAIK proper URL parsing at least with bash is probably quite complicated so what I have is good enough for my purposes (worst case scenario is I won't download a video that probably shouldn't be downloaded anyway--I copy the URL from the browser in most cases). * As the user, I want to know that the name of the subdirectory specified already exists before I choose to download to it anyway or specify a different folder. This is useful because I may download groups of videos that are very similar but don't want to mix them up. Generally when I download videos, it's in batches and in sessions where I likely may not group the videos together unless I feel they are related enough--knowing if a subdirectory already exists allows me to consider whether to use it (i.e. group videos together), or decide that e.g. the videos I'm downloading are *funny* cat videos and I may not want to download them to a folder simply called `~/videos/cat-videos`, so I get a chance to rename it to `~/videos/funny-cat-videos`.
This seems simple enough: ```bash size=$(du -s path: // cdrom) if [[ 650329 = $size ]] then cp b c elsif [[ 570362 = $size ]] then cp c d else cp d e fi ```
For my particular purposes, something like `--force` or equivalent isn't ideal because I lose context of whether the directory specified already exists. Some For arguments handling, I've been wanting to to into getopts but what I have currently in terms of handling options is actually good enough. My `usage()` looks like this: My `usage()` looks like the following: usage(){ cat &lt;&lt;EOF Usage: yt [OPTIONS] [PATH] [youtube-dl-OPTIONS] OPTIONS is one of the following: a: audio format d: date video format (in the order: date, title, uploader) h: print this help message p: playlist format np: download playlist using video format t: download to /tmp/youtube-dl in video format When no OPTIONS is specified, downloads in video format (in the order: title, uploader, date) Destination directory is created if it does not exist; e.g. `my-script cat &lt;URL&gt;` creates the directory `~/videos/cat if it does not exist and downloads to it. EOF } and it properly handles check first argument to see if it match OPTIONS, then try to match as as a PATH, and finally a URL. Any arguments after URL are options directly passed to youtube-dl. When I eventually find some free time, I do intend to convert my scripts to getopts but at the moment, I feel like getopts isn't worth the trouble for simpler scripts, bash has its quirks anyway that an attempt to "properly" parse command-line options yields only marginal benefits especially if the script is already 99% done, and options require a single or double dash prepended, e.g. `-f` or `--filename`, which is more typing (lol).
Why not just copy all of the type of files (MP3 or whatever) from the CD to the USB drive using `find`?
Quoting in bash is typically useful for preserving spaces. If your `dir` variable ever gets set to a directory name that contains spaces then using it as an argument to a command without quotes will turn into multiple arguments. I think your example of quoting the entire string vs individually quoting variables is equivalent. I have quoted individual variables when I want them quoted but I want the surrounding parts explicitly not quoted. This can happen when I truly have a nested quoting situation like when mixing languages $ foo=world; python -c 'print("hello \"'$foo'\"")'
I'm not trying to convince you to change the general functionality. I just think it's better to use actual option parsing (`getopts`), so no single letter options without a leading dash and then add -o for choosing the output dir. That way you don't have to check if the remaining args are paths, a folder name or a url. You can simply treat all trailing arguments as urls instead. Another option, which would probably make the script simpler, is moving `[PATH]` to the end.
[https://en.wikipedia.org/wiki/CDDB](https://en.wikipedia.org/wiki/CDDB) Maybe one of those online audio CD databases could help to identify a CD.
**CDDB** CDDB, short for Compact Disc Database, is a database for software applications to look up audio CD (compact disc) information over the Internet. This is performed by a client which calculates a (nearly) unique disc ID and then queries the database. As a result, the client is able to display the artist name, CD title, track list and some additional information. CDDB is a licensed trademark of Gracenote, Inc.The database is used primarily by media players and CD ripper software. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
This is the correct answer to the original question. You could actually quote every individual character. This is useful in the case where you want to embed a new line into something. Here is an example for assigning a variable: mystring="line 1"$'\n'"line 2"
Check for URL would go like this? http with an optional s [[ $1 =~ https?:// ]]
Based on your posted code, you may want to try "case ... esac" instead of if ... elif ... else. If nothing else it will make the result easier to read and change.
All fine, but if the script's purpose is to move to a particular directory and execute a command that must be in the context of that directory, then you had better have terrific error checking, otherwise the command will execute in the wrong place.
yes, yes. Very Very Thank You! Thank You!
Would you say in general, most people might want the value of a variable containing spaces to be treated as one "value" (e.g. in cases where spaces in the value should be interpreted as a single argument) and therefore double quotes should be used around the variable? Also, assuming the above, then `cmd "${dir}/${input}/${video_format}" "$@"` is *both* more readable and preferable than `cmd "${dir}"/"${input}"/"${video_format}" "$@", which while in this case is functionally equivalent, the latter assumes there's no funkiness with a (custom) IFS since the slashes are not quoted? So the former should almost always be used?
On mobile now, but google says( with your usecase) something like: ''' df -BG / | awk '{ print $5 }' | tail -n 1 '''
df -h - human readable?
Something like: df -h / | tail -n1 | awk '{print $3}'
You don't actually need to use tail if you're using awk. Assuming you want everything but the header, I'd probably do something like `df -h / | awk 'NR&gt;1{print $3 "/" $2}'`. Doing this to my /home partition, I get `29G/49G`. I could get more elaborate than that depending on the exact format that you want. If you just want to target one line (I think in this example it won't matter, but just for future reference), you could do something like`df -h / | awk 'NR==2{print $3 "/" $2}'`
Isnt it "abc" with the "b" quoted seperately? I thought bash looks at the quotes from the outside
Yeah. I would say that you almost always want to have your variables quoted unless it is some sort of list you want to loop through or arguments you want to pass. But in both of those cases an array is better suited. I would say the former is best because it is easier to read. If you are doing IFS manipulation, you should be restoring it before continuing. So that third question is confusing. The best answer i can give there is that you normally want quotes around an entire string to make it easier to read. I'm not really sure what you are trying to do there. As the original commenter was saying, there is no such thing as quote nesting. This is because there is no directional quote like there are for brackets. You wouldn't be able to tell if you were entering a btw level or exciting the previous.
Then \*create\* that file. (Then log fully out and log back in for it to be picked up) Side note, case-insensitive file systems are crap.
Directories are case-insensitive in macOS by default and that's the way I've been using it.. I just want to make every shell feature work the same and it's kind of a headache having some directory commands be case-insensitive and others (like tab auto-completion) be case-sensitive, though I agree that this is a bad habit to get into.. if I were to change from macOS to Linux. So, creating .inputrc is "safe"? I figured since the file isn't a thing anymore and those answers were kinda old then it was probably removed for a reason.. Guess not.
Please read up on oh-my-zsh. It offers way more what you asked but is does the job: https://github.com/robbyrussell/oh-my-zsh
Yeah, I've read about zsh, but I'm worried about stuff I saved in .bash_profile... I don't have too much scripts or overly complex ones but I hope they're compatible.
I’m sure you can port this with a little effort to zsh. Once I got started with zsh I never looked back.
Yeah... you probably shouldn't do people's homework for them. Instead, ask what they've tried so far and guide them.
&gt; Side note, case-insensitive file systems are crap. Could you elaborate further?
FILENAME and filename are two different files, and always should be. Any FS, where if you write to a 'FILENAME' and it overwrites a file 'filename" the FS is broken.
Yeah, I mean, I figured as much. I thought there were some other reasons.
``df -h / | tail -n 1 | awk '{print $3}'``
You probably ought to be using `df`'s `--output` option to output the fields you need in the format you need, rather than trying to parse `-h` (aka `--human-readable`). You want machine-readable data. Anything "human readable" is not guaranteed to remain consistently formatted from one version to the next.
Just got redpilled into using zsh thanks to you... figured with all the praise and love it's getting and since macOS is switching to it as its default shell in the next update.. I might as well convert while I'm still learning shell commands and start collecting scripts through .zshrc instead.
Have you triend the "ncdu" command? Its very usefull and interactive.
If you want to `cron` it, it would be easier to check for the modified time being in the last 20 minutes (and few seconds to make sure that nothing falls between the gaps)... Alternatively just run a script once that grabs the filesize, then runs an `inotifywait -e modify` loop that compares the filesize on every action and if altered, updates the compared filesize with the new one and sends off a mail...
Use a consistent naming scheme for your files.
That's what a loop is for. for ((i=1; i&lt;=5; i++)); do echo "profile_$i = {" echo "config_directory = \".telegram-cli/profile_$i\";" echo "msg_num = true;" echo "};" echo "" done &gt;&gt; your_config_filename_here (note: the `&gt;&gt;` redirection operator appends to an existing file)
This might be of interest to you: https://www.redhat.com/sysadmin/introduction-path-units
&gt; for ((i=1; i&lt;=5; i++)); do for i in {1..5}; do
Basic outline: * Create a text file containing the sizes of all the files of interest. * Next run, compare new directory listings with the saved values. * Alert on changes, send an email. * Save the new directory listing and repeat. &gt; I'm looking to write a basic script that I can add to cron Call a script from cron, don't try to put the actual code in the cron configuration file.
This is trivial; what have you tried? look up for loops in man bash, under the section SHELL GRAMMAR. for i in (1 2 3 4 5); do cat &lt;&lt;-EOF profile_$i = { config_direcoty = ".telgrram-cli/profile_$i"; msg_num = true; } EOF
you can set that option and create the file in a single command: echo "set completion-ignore-case on" &gt;&gt; ~/.inputrc then you might have to restart the shell to take effect
&gt; for i in 1 2 3 4 5; do for i in {1..5}; do
Get accustomed to writing scripts, not one-liners. One-liners are the bane of shell scripting. find . -maxdepth 1 -name '*.csv' | \ while read fn; do echo "Processing $fn ..." (additional commands here) done
Good for you!
If the filesize changes, a checksum of the file (cksum, md5sum, sha256sum etc) will also change. I mean, you certainly can base your testing on filesize, but checksumming achieves the same while being far simpler and more portable. So to expand your pseudo if [ a file that stores the checksum exists ] read the checksum from the file and assign it to a variable generate the checksum for the file and assign it to another variable compare the two and act appropriately else generate the file that stores the checksum fi You may like to take into account other considerations like the age of the checksum file to absolutely ensure that it's churned. inotify/incron or systemd really is the better way to do this. But it's a good learning exercise to DIY, and I have to admit to building exactly this at work for our monitoring system.
That's how MacOS is out of the box, and it's maddening. And while case-sensitivity is an option, several big apps didn't (and probably still don't) work if you turn on case sensitivity... A.txt and a.txt in the same git repo is a real hassle to resolve.
Only time I've run into this as an actual problem is in a git repo at a place where I worked where we had a ton of directories and files named `NAME` and `name` at the same directory level. On our Mac workstations, cloning this repo on a case-insensitive file system caused git to freak out and assume you deleted stuff. Creating case-sensitive FS containers on our workstations became standard procedure as a result.
Thanks for the feedback. I modified the script so that if the `cd` command doesn't return success then the function immediately returns without running any optional command passed with `-c`.
You could look at the mtime, and md5 hash it whenever the mtime changes, then compare the md5 hash with a stored hash. Or you could watch the file with inotify, which will execute some command in response to file open/read/write events. Or you could just use Tripwire, an application designed to monitor files for changes and notify you of them.
&gt;df -BG / | awk '{ print $5 }' | tail -n 1 does not work, but thank you for your reply
&gt;df -h / | awk 'NR&gt;1{print $3 "/" $2}' I get 2 numbers: 817 and 923 GB. I am sorry: I don't understand what they correspond to. thank you for your reply.
thank you for your reply. Finder says 880 GB used, and your script 817. What would cause the difference ?
&gt;df -h / | tail -n1 | awk '{print $3}' the first script gives 817, and finder says 880 GB. The 2nd script does not work. thank you very much for thinking about my post
It seems that your df is using a different format (I am testing on CentOS 6). Can you give us the output of `df -h /` so we can check?
That’s a super odd way to run 4 curl commands... I don’t see any error checking and I don’t even see a skeleton of the curl request you are making. I don’t think there is enough info here for anyone to help you.
What program do you use to listen ? I tried mplayer, and while playing, it prints ICY Info: StreamTitle='I.Y.F.F.E - Storm (feat. Desiree Dawson)';StreamUrl=''; ICY Info: StreamTitle='FLOZEE - BLUR';StreamUrl=''; Among other crud, of course. It seems to be nice too, flushing each output line, so there are no buffer delays in mplayer ... | grep '^ICY '
ah okay but you have to play it, or? you can't get all songs of a day at once for example ?
Correct, those lines appear while each song starts.
On top of what else has been said, you probably want: for useABetterVariableName in "${search[@]}"; do "$useABetterVariableName" curl_close() done This whole "using an array to do this" approach, though, seems utterly pointless. Do you have more code and you've over-sanitised it, or is that literally all you've got so far?
I generally use something like this, adjust the path to point to a specific location: &amp;#x200B; du -h /home | grep '\[0-9\\.\]\\+G'
so the curl_close() code is same in php_curl. interesting. thanks man
Welcome to the zsh club. While you're learning, check out r/zsh, we try to be friendly and helpful. As you get more into zsh, you will find that most people think omz (oh my zsh) is bloated and slow. They're mostly correct, but as a newcomer, don't worry about it, omz works well, and if you never have problems there's no reason to switch to something else.
Opening the stream in vlc shows the current track fine. If you wanted your result, you'd have to [scrape the page](https://en.wikipedia.org/wiki/Web_scraping).
yes sorry about that. im playing around to automate running 4 curl commands every week
`md5sum FileX` does not return `51889`. It returns `51889 FileX`. `md5sum FileX | cut -d' ' -f1` should return what you want.
Uh... that's not what I said at all... You say that you want to run four curl commands, well simply call them in order: #!/bin/bash curl -s http://some.addre.ss/ curl -s http://another.addre.ss/ curl -s http://simple.isnt.it/ curl -s http://another.example.com That's a start. Then you may want to branch out. Let's take your array example - if you absolutely wanted to use an array, well you'd put all your URI's into the array and do something like for targetUri in "${search[@]}"; do curl -s "${targetUri}" done Then you might want to build in error checking, timeouts, handling for `curl` options etc. Maybe add a couple of functions... it really depends on what you're actually wanting to do. You need to give us WAY more to go on than what you have, though...
Thank you. Good Luck to you.
Hmm... Maybe it's a different version. I definitely got used gb over total gb. What platform are you on, and what is the output of just a plain `df -h` command?
cat tracklist.php\\?lang\\=en | tr -d "\\n" | sed 's/song\_time/\\n/g;s/current\_song/\\n/g' | grep song\_title| sed 's\_\^"&gt;\_\_g' | sed 's/&lt;\\/div.\*song\_title"&gt;/\\t/g' | sed 's\_&lt;/div.\*$\_\_g' | sed "s\_\^Now\_\`date +\\"%I:%M %p\\"\`\_g" 09:04 AM Wayvee x GT Vienna - No doubt 02:44 PM Haterade - Big Ben (Jorgen Odegard Remix) 02:41 PM Ape Drums feat. Gappy Ranks - Baddest 02:37 PM Sober Rob &amp; Oshi - Lost 02:34 PM LOUDPVCK - CHIRP 02:31 PM Evoke &amp; PUSHER - TakeU 02:27 PM Ian Munro - Bout That 02:22 PM J. Chris &amp; srsly - Fanatic &amp;#x200B; &amp;#x200B; I don't know what timezone this is in, but it's not local. You'll have to set that before running that command. &amp;#x200B; The server side only gives a few songs - so you'll have to poll.
You may be interested in [autojump](https://github.com/wting/autojump).
WOW thank you
As promised - the line I added to my .bashrc is: &amp;#x200B; `env | grep DBUS | cut -d '=' -f 2- &gt; /tmp/dba` &amp;#x200B; And then the way I set up my cron job is: &amp;#x200B; `*/5 * * * * username DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=$(cat /tmp/dba) /scriptlocation/hews.sh`
I'd grep -Eo '^[a-f0-9]{32}'
Base2 vs base10? 2^10=1024 vs 10^3=1000? Quick Google returns 880GB (gigabytes) equal to 819.564 GiB (gibibytes) https://en.m.wikipedia.org/wiki/Binary_prefix
Desktop link: https://en.wikipedia.org/wiki/Binary_prefix *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^269861. [^^Found ^^a ^^bug?](https://reddit.com/message/compose/?to=swim1929&amp;subject=Bug&amp;message=https://reddit.com/r/bash/comments/cg1pxn/disk_free_space_script_how_do_i_change_from_free/eugjb6s/)
**Binary prefix** A binary prefix is a unit prefix for multiples of units in data processing, data transmission, and digital information, notably the bit and the byte, to indicate multiplication by a power of 2. The computer industry has historically used the units kilobyte, megabyte, and gigabyte, and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, gigabyte customarily means 1073741824 bytes. As this is a power of 1024, and 1024 is a power of two (210), this usage is referred to as a binary measurement. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
If it's hanging, look into your script with strace, and see what it was/is doing when it hangs. `strace myscript.sh` When I'm testing things, I'll place `print()` statements before and after an action, so I'll know how far something succeeds; "starting action x" and "finished action x", for example. I'm currently on mobile, and can't create my own test, but definitely start with strace.
&gt; The script just hangs at the moment. Just at a first glance, your script removes the source .wav files after one read, so repeated tests are bound to fail. There's no reason to delete the source files until the script works as it should. To test this script, put a bunch of printing lines into it so you can tell what's going on. Also, learn to use indentation so the structure of the program is easier to follow. Edit: There's an error in this line: file=`basename "$i" .wav` If the script is executed in the same directory as the .wav files, It should be: file=${i/.wav}
 mail ... &lt;&lt;EOF We are the robots. EOF Is mail waiting for input ?
It could be - it does pause and just wait - I'll experiment. thanks
&gt; mail -s "Lenny" -a $file.mp3 me@domain.tld You forgot to quote a variable. Any spaces in `$file` kill your script.
Well, the command actually works for me, but to show remaining Gb I need to show 4th position. Might be difference in df version? $ df -BG / | awk '{ print $4 }' | tail -1 15G $ df --version df (GNU coreutils) 8.31
You can also pipe it through `awk`: `$(md5sum FileX | awk '{print $1}')`
Exactly what /u/masteryod said. From the `df` man page: `-h, --human-readable print sizes in powers of 1024 (e.g., 1023M) -H, --si print sizes in powers of 1000 (e.g., 1.1G)`
Regarding the newlines, there are two options: (1) Using bash's escape quoting mechanism to embed the raw newlines directly into `$SUCCESS` and `$FAILURE` : SUCCESS="Process is completed."$'\n'" Thank you for using this script"$'\n'" Jasymiel" echo "$SUCCESS" Note that `$SUCCESS` in the `echo` must be enclosed in double quotes (`""`) (2) Using bash echo's escape interpretation mode to interpret the `\n` sequence inside `$SUCCESS` and `$FAILURE` : SUCCESS="Process is completed.\n Thank you for using this script\n Jasymiel" echo -e $SUCCESS In this case, the double quotes are not needed in the `echo`.
Check out [css](https://www.w3schools.com/html/html_css.asp) or if it has to be html you could use a &lt;table&gt; tag.
There's a bug in your script (not related to your 4 questions). You need to save `$?` in `$RESULT` before setting `$PD`, because setting `$PD` overwrites `$?` and sets it to zero.
Ok, how would I do that? Any recommendations?
I will try that! Thanks a lot for your input!
Just reverse the order of the two lines, like this: RESULT=$? PD="$(pwd)"
Hello, I know that I need to use the &lt;table&gt; tag, but I don't know how I can use it with my scipt awk :/
Thanks a lot! Will do
&gt; \# simple 'if' loop `if
It's really hard for me to understand what you mean. I'm not trying to annoy you :D but you would print the table tag before the for loop and the closing one after it. &lt;table&gt; and &lt;/table&gt; outside the for loop. Inside the for loop you would use &lt;tr&gt; for rows and &lt;td&gt; for columns.
True , I am still a noobie , i intended to do a loop in the first place... And changed idea, thanks for pointing it out.
A while ago I wrote this: #!/bin/bash echo &gt; ip_info.output echo -e "\n" for host in `cat ip_adds` do sudo ssh -o BatchMode=yes -o ConnectTimeout=5 $host "printf 'Hostname: %s\nIP: %s\nMem: %s\n' $host "$(ip route get 1 | sed -nE 's/.* src ([0-9.]+).*/\1/p')" "$(free -m | awk '/Mem/{print $2}')"" echo -e "\n" done This will essentially loop over a few host names in a file and execute a few commands. Once done it will come out of the server and move onto the next one. Having said this, as mentioned above, I have started using ansible recently and never looked back.
Okay, so I'm not sure what version of df OSX uses (That's your OS, right?). According to POSIX, plain df gives you numbers in 512B blocks. This is kind of a dumb number of blocks (GNU version uses 1024B (1K) blocks), but GNU df also can be POSIX compliant, so I'm going to use `POSIXLY_CORRECT=1` to run the command because OSX df is probably POSIX compliant. So, try this command: `df / | awk 'NR==2{print $3/1953125}` or maybe (if you want the total number as well): `df / | awk 'NR==2{print $3/1953125 "GB/" $2/1953125 "GB"}'` Is that closer to what finder tells you? If you want fewer decimals, there are a few things you could do. For no decimals, do `int()` to the numbers, e.g. `awk 'NR==2{print int($3/1953125)}'` or to still have some decimals, you could use printf, this will round to 2 decimal points (adjust the number before f to get a different number of decimal places: `awk 'NR==2{printf("%.2fGB\n", $3/1953125)}'` Hopefully that will do it for you.
I know basically no html, but if you gave me the output html and the input CSV (or some sort of example of each), I might be able to figure out an awk script to do the conversion. No promises, but I'd give it a shot, at least :)
You could grep for the process name and cut up the output and assign the PID to a variable. Then make the kill call like normal using the variable.
* Run a process you want to control. * Find the process using "ps", get the PID, kill it. You can only stop a running process, you can't restart it. If you start a process in a Bash script, you can capture the process ID, which means you could kill the process from within the script at a later time: (command) &amp; pid=$! read -p "Press Enter to kill process $pid:" reply kill -9 $pid
 if pid=$(pgrep command-name); then kill "$pid" else /path/to/command-name fi
This is why I love Linux 😊
While this is a valid use of the 'ps' command and works fine, there are a few other tools out there which handle this in a bit more elegant of a way. &amp;#x200B; To retrieve a process you can use a command called 'pgrep' so you don't have to fiddle with trying to grep and cut the right pieces. "pgrep $programName". This will result in a return code of 1 if it does not find any running program matching the search, or will return a 0 if it finds one (or more) running processes. So with this in mind, if it returns 1 (false) you can then start the program, if it returns 0 (true) then you can kill the process(es). Getting the exit status is pretty simply using "echo $?", more details on this can be found here: [http://www.tldp.org/LDP/abs/html/exit-status.html](http://www.tldp.org/LDP/abs/html/exit-status.html) &amp;#x200B; As for killing the process, instead of fiddling with magic sauce numbers and hoping you're grepping for the right ones, you can instead just use 'pkill' such as "pkill $programName". No fiddling with magic numbers and it finds all the PIDs so there's no need for manual iteration. &amp;#x200B; Fair warning, there's no guarantee that these are installed, however from my experience every Linux system I've came across has them installed.
I'd recommend Monit! I just set it up on my server hosting a small modded Minecraft server that my friend and I are good at killing, apparently. It looks for Java, and if it doesn't see it, I have it setup to run a script to boot the Minecraft server up again.
As a long time Python programmer (data science on the side of my main work) who's managed to avoid command lines until this week (I've just started with bash - for fun), what kind of problems is bash uniquely useful for? &gt; get their hands dirty with all the old bash stuff ... nothing (including python) is as slick as bash for those sorts of problems
Others have mentioned `pgrep`, but it's my experience that this tool can be unreliable. For example, it might return no results where `ps -ef | grep blah` does. This annoyed me enough one day to throw together a simple shell function: # Alternative to 'pgrep'. Converts the first character of the search term to [x] # e.g. psgrep jboss = ps auxf | grep [j]boss # This removes the need for an extra 'grep' invocation # e.g. ps auxf | grep jboss | grep -v grep psgrep() { [[ "${1:?Usage: psgrep [search term]}" ]] ps auxf | grep -i "[${1:0:1}]${1:1}" | awk '{print $2}' } Feel free to use it, or not.
in Linux I put the following in my bashrc bind "set completion-ignore-case on" bind "set show-all-if-ambiguous on" I don't have my Mac handy right now but give that a try.
 pkill -x process_name || command_line_to_launch_it Is launched, if could not be killed. Flag x is for exact match, ftp would not match sftp, for example. See your pkill manpage, could be different?
 packages=( arc-theme arch-test arduino ... atom ) ... sudo apt-get install "${packages[@]}" | tee log.txt Easier to manage the large list?
thanks very much. I have another solution. Please don't spend any more time on this issue.
thank you
yea i guess just create a string on which you'll continously append the text you fetch and print using awk, to finish it up just echo that whole string into a .html file or however you're planning to implement it on your site. just add the HTML skeleton around your variables. it seems like you want one column per for loop? if that is the case just pre add the thead and the column titles and generate a td per for loop so the content is dynamically generated. add more for loops for different collumns etc. is that what you were looking for? sorry for no formatting - am on mobile. Sadly no PC or Notebook nearby to test my idea and provide you with snippets but i guess you can figure it out yourself by spending some time googling
thank you
&gt; df -h / thank you. Results below. Please note that my drive is 1,000 GB and reported as 932 Gi (GB vs Gi) which is confusing. In my world, I always work with GB. Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1s1 932Gi 824Gi 105Gi 89% 2198405 9223372036852577402 0% /
&gt; awk 'NR==2{printf("%.2fGB\n", $3/1953125)}' sorry: none of the scripts work, but I have other solutions. thank you
thank you. I understand but it's just too confusing to start working in Gi
I don't quite understand, but you can pipe the filenames found with `find` to while and do any string substitution there, like: find -type f -name '*.pdf'|while read -r i; do current="${i%'/'*}"; up="${current%'/'*}"; mv "$i" "$up"; done The current directory is the string of the file up to the last `/` and its parent is the same, but up to the previous `/`. `find` has the `-exec` feature but I don't know how to use substitution on the `{}` that holds the path. About the `t` option it could be caused by an alias.
I guess you could escape the `$` (`runstatus=\$(...)`) but I don't understand why you would want to create a script that creates a script... Why don't you write the result directly?
cat &lt;&lt;'EOF'
Thanks! It's due to the way the software is being deployed on machines, it's deployed using a cloud solution which doesn't give me much room to work with in terms of what I can use, so I have to try and fit everything into one deployment script I do apologise if that makes no sense, I'm incredibly tired and exhausted at this point. I've written the scripts to deploy on Windows, Mac is frustrating me to no end as it isn't as simple as a classic Linux system and when I get around to doing the linux deployment scripts they'll be done in 5 seconds because it's surprisingly easy to set software to autostart
Good point, didn’t think about aliases. Thanks for the suggestion, I’ll check that out tomoz at work.
Hi, thanks for your TIP. I need to do some modifications but at least I got started. I found another issue I'm trying to figure out how to do: With this line &lt;tail -n +3 adobe.csv | cut -d ',' -f2&gt; I get results: "8996" "516" "516" "12088" "12088" What I would like to do is compare each line with the line following and if they have same process ID I would then use your solution to get the difference in minutes. Probably some for-loop would do the work but I don't get it how would I compare $line1 to $line2 and then if they match I would continue. Any ideas? Cheers
If they're guaranteed to always be next to each other you'd have to use a variable to store the previous result, if not you'd have to create a distinct array with `uniq` and compare `$i` against that.
I found that by adding sort I get lines that are similar next to each other and "unique" line will be at the end, probably could get that also removed. But still missing how would I compare lines together and from there get time process has run.
&gt;Apr 1 Hi, what should I do to get this working with the following file: "Photoshop","8996","17.7.2019 13:18:52","1" "Photoshop","516","17.7.2019 13:26:34","1" "Photoshop","516","17.7.2019 15:53:58","0" "Photoshop","12088","17.7.2019 16:32:31","1" "Photoshop","12088","17.7.2019 16:40:21","0" I used sed 's/"//g' to remove all double quotes but cannot get date working. Is there somehow that I could tell 'date' to understand the format I'm getting data?
What ended up working for you? I'm just curious as all of these solutions worked for me in one way or another.
There is no "your world". There's no real life justification to use base10 for storage management. It has always been powers of 2 because personal computers are binary. Period. HDD manufacturers liked to use base10 because hard drives seemed bigger on the shelf this way. That's all. But once you put bits of data on it, it has always been power of two. No matter if it's KB or KiB I expect it to be 1024 Bytes. Why? Because in IT it's all power of two. Yes it would be better if we'd stick to proper prefixes but that's still not ubiquitous. df prints human readable output using Greek Si prefixes (e.g. Kilo = 1000) but still uses base 2 (e.g. K = 1024) for calculating disk usage. Why? Because base10 makes absolutely no sense in terms of binary computer storage!
Thank you for that! Yesterday i was wondering how I could better manage that number of packages... However I do not quite understand the syntax here specially the part "{packages [@]}" could enlighten me a little?
That's just the way an array is expanded in bash. It's a bit like "$@" (command line arguments) of the traditional shell.
The @ reference what? So I can apply this concept elsewhere?
@ stands for all-of-them, special case for an array index.
Thanks a lot😁
 subdir in ‘find -type subdir in `find -type Use of tick, not backtick, could explain mv seeing the -t.