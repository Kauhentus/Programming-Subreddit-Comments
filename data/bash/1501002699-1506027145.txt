Because you can only do what others tried before? Be awesome. Be yourself. Walk new ways. Do it and show the world!
Yes, I'm gonna be a billionaire then, tomorrow, thank you AnachronGuy, you showed me the light!
My first response is "Don't do it". After reading the details and finding out that you are trying to use an API, it started making sense. Yes, it is possible. I can't read the API docs since I am not a customer. On the site, they also mention support for python and other languages. I recommend doing that. Choose any language you are familiar with. Bash scripts are not easy to maintain. Not easy to read/understand when the logic gets complicated. As a rule of thumb - if the script takes more than 30-ish lines, use an actual programming language (like python, ruby, perl, C/C++ and such). (The above is a paraphrasing of something I read in this sub)
Make sure to split some with me! And #bash the world.
I would think once a week is too slow. There are 55 builtin commands alone. 
We could share command line tricks once a week, like what happened over at /r/vim some time ago: [https://www.reddit.com/r/vim/search?q=Weekly+Vim+tips+and+tricks+thread%21+%23+author%3Acherryberryterry&amp;restrict_sr=on&amp;sort=new&amp;t=all](https://www.reddit.com/r/vim/search?q=Weekly+Vim+tips+and+tricks+thread%21+%23+author%3Acherryberryterry&amp;restrict_sr=on&amp;sort=new&amp;t=all)
&gt; you can use "getent passwd ${user}" to safely pull the user information from the passwd file as a nonroot to see if the user already exists in the system as opposed to just checking if they have home directories - it might be better to parse this output as it includes system accounts Couple of cautions about that: $ getent passwd $(whoami) &amp;&gt;/dev/null $ echo $? 0 $ grep "$(whoami)" /etc/passwd &amp;&gt;/dev/null $ echo $? 1 So what's the problem here? This is a host that authenticates against a directory, in this case AD. The first test stucceeds and returns a result from AD... but you don't want to confuse a script that deals with local users (which this account isn't, as demonstrated by the second test), so the portable/robust thing to do is deal with the `passwd` db which should be readable to anybody. The other caution is that OP's script appears to be OSX specific, and OSX doesn't provide `getent`, or at least it didn't last I checked. I recall there being some talk about a step-in function in `zsh`, but that obviously doesn't apply to `bash`.
I love it. I think other useful ideas that are related using only builtins or core utils would be fitting as well. It's not the command, but how you use it that is cool. An example for this could be the !!:1 and !!:2 and so on. That's one that I didn't know for years and I can't stop using it at work. And I'm sure there are other related ones. Also, neat tricks with ansible and other related tools that are unusual but useful would be cool too. What would be amazing is somebody posting an idea once a week, and besides regular conversation, people post related interesting things on the same topic. That would make those threads even more amazingly useful then they already are.
Anti-captcha.com 2captcha.com AWS mechanical turk Most of the papers I've read about using a script to decode a captcha haven't released code, so there's a pretty large chance you will have to pay for it.
Reminds me of [this xkcd](https://imgs.xkcd.com/comics/tasks.png) &gt; In CS, it can be hard to explain the difference between the easy and the virtually impossible.
Make sure you have TESTS for it. There's nothing worse than running it in an emergency and have it throw errors at you. Docker might be a good option for testing bash scripts.
We have years.
You can use test instead of a superfluous grep -v. For example: ps aux | grep [r]man
Isn't that a bit of overkill? Couldn't I just run the script to test it? 
I am for a one-liner of the week! Should be shorter then 80 characters.
Yep of course you can just run the script to manually test the script, but it will get more and more tiring to setup the test environment as the steps get more complicated. If you're not careful about checking the values you might end up removing important files on your main system when testing it out too. You don't have to, that's just my suggestion :) Lots of devs don't take tests seriously, so the code is broken and they don't even know about it until users use it for real.
Haven't actually run it, but I'd suggest 2 points: 1. Read through the [google bash style guide](https://google.github.io/styleguide/shell.xml) (or some other style guide if you're anti-google). This sets lots of good rules to follow to help you write better-looking, more consistent &amp; debuggable code, and avoid pitfalls. 2. Use shellcheck or [shellcheck.net](http://shellcheck.net) to analyze your code. It can detect lots of mistakes such as variables referenced but not assigned, incorrect shebangs or unicode quotes, etc. Edit: formatting, link
Let's start with `eval` :)
http://wiki.bash-hackers.org/commands/builtin/eval Not a bad place to start.
Correct! I am writing it for macOS machines. I did see there were ways to get the getent command installed, but I'm trying to avoid doing that. I read a little bit more into it and saw that the macOS alternative is dscl with a bit of parsing. I'm now using this instead of searching the users directory: `dscl . -search /Users RecordName $setUser | head -1 | awk '{print $1}'` 
Thank you for the links! I'll be reading through them at work tomorrow. I appreciate the feedback and suggestions! Per the previous comments, I've replaced some of the menus with the select command and redid the way it searches for a user so I'll post an updated version after I take a look at the links you mentioned and make some more changes. Thanks again!
It's not a command you're expected to have. They're using `recover` to stand in for whatever "recovery" is necessary if `some_command` fails.
Oh. Well, thanks!
in this example recover is placeholder text for any command that would be run by the script in the same way that some_command is placeholder text. A better example would have been: some_command | some_other_command - meaning if some_command fails then some_other_command is run.
&gt; In a [tutorial on good bash scripting](http://hackaday.com/2017/07/21/linux-fu-better-bash-scripting/) which I, as a bash beginner, found a bit opaque, the following line was mentioned: That does not teach you good practices, it teaches bugs. The only thing it gets right is that you should quote properly. Try the [BashGuide](http://mywiki.wooledge.org/BashGuide) instead.
Great, thanks!
Nit: your example has one bar, which is a pipe, not an "OR". Although OP did understand the intention. 
Much appreciated! I like this format!
I didn't even know that was an option! Great thing to know! Thank you for the tip!!
It was just a simple list. The encoding was messing the script up. Opening in nano revealed this.
That is called *short circuit evaluation*, you can read more about it [here](https://users.drew.edu/bburd/JavaForDummies4/ShortCircuitEval.pdf). It is explained for Java, but the concept is the same.
If you delete bash history but still have bash running, it still has some history. In addition, bash typically writes the history to the history file on exit.
Is there a way to stop this and disable bash history?
Yes. If I remember correctly, HISTSIZE and HISTFILE variables. I don't think it will be comfortable though.
I'm reasonably sure that `foreign_key_checks` is a *session* variable, meaning that its setting is not persisted beyond the connection that it's made in. You'd be better off all of this as a SQL file that you ask mysql to execute. Consider building this file, disabling foreign key checks, and letting it rip.
nice one
Use double brackets. Check the string comparison section here: http://tldp.org/LDP/abs/html/comparison-ops.html 
You could run the script with `bash -x scriptname`. That will make bash print each line it executes, but after it has processed some stuff in them, like replaced variable names with the values. You should be able to follow what is happening to variables and should see what the test in the 'if' is comparing exactly. I'm not quite confident I understand what you are trying to do, but I'm guessing stuff should be swapped around like this: while read -r PASSWORD do TRY=$(echo "$PASSWORD" | sha256sum | awk '{print $1}') for ((ENTRIES=0; ENTRIES&lt;COUNT; ENTRIES++)); do if [[ "$TRY" == "${HASH[$ENTRIES]}" ]]; then echo "The password for ${NAME[$ENTRIES]} is $PASSWORD" fi done done &lt; common.words Some random other stuff you might want to know: When you work with array variables, the inside of the `[ ]` brackets has the same behavior as what you know from `(( ))` and `$(( ))`. You can drop the `$` from the variables in there and can write for example: NAME[COUNT]=... echo "${NAME[ENTRIES]}" You can use one of these lines here: (( COUNT++ )) (( COUNT += 1 )) (( COUNT = COUNT + 1 )) This here should work to split according to where the `:` are in the lines that are read: while IFS=':' read -r name hash; do NAME[COUNT]="$name" HASH[COUNT]="$hash" (( COUNT++ )) done You can add new entries to an array like this, which works without having to count: while IFS=':' read -r name hash; do NAME+=( "$name" ) HASH+=( "$hash" ) done When you create an array like this, bash starts counting at zero, so that `for` loop you do later should still work, but you can also loop over the index numbers of an array like this: for ENTRIES in "${!NAME[@]}"; do ... done That `${!array[@]}` returns a list of the indices, see here: $ x=( a b c ) $ echo "${!x[@]}" 0 1 2 $ echo "${x[@]}" a b c $ echo "${x[1]}" b A weird thing about bash is that you can have an index in an array that's not there: $ y[3]=a $ y[5]=b $ echo "${y[@]}" a b $ echo "${!y[@]}" 3 5 $ unset y[3] $ echo "${!y[@]}" 5 $ echo "${y[@]}" b That's where that `${!array[@]}` gets interesting, when you use several arrays like you do with HASH and NAME, and then want to use a line unset HASH[COUNT] NAME[COUNT] to delete one of your entries.
 alias ls='ls -1' Some Linux distributions are preconfigured so that `ls` supports an environment variable called `LS_OPTIONS`, so on those you could also do `LS_OPTIONS='-1'`. But the above alias is probably more portable/reliable.
 &gt;Some Linux distributions are preconfigured ... This is because `ls` is aliased to `ls ${LS_OPTIONS}`. 
On the off chance you're wanting this for when you pipe ls to something else, ysk that -1 is already the default when output is not to a tty.
Type 'Exit' before you run the command. Top of my head you have different binaries on the server than you do on your computer. I periodically forget I am on a remote server and start using alias commands that I have on my local machine in ~/.bash_profile 
: command not found/test.sh: line 1: Exit /var/mobile/somefolder/test.sh: line 5: syntax error near unexpected token `else' 'var/mobile/somefolder/test.sh: line 5: `else That's the error I get. I can't figure out why :S 
I think you need \#!/bin/bash As first line. 
&gt; !/bin/bash I tried, and I get: : No such file or directory: line 1: !/bin/bash Now I wonder is the issue here because of the iOS?
Probably, try: which bash Or locate bash That should tell you where it is.
It says: /bin/bash So I guess it's correct. Btw there will be no issue if the .sh is written in a line. But when I need to edit it, it's a pain!
Oh! You're on windows, try typing your script in notepad++, the endlines are different, there's an option to set it as you need.
Did you drop the # ? You need the #
That is it!!! I don't know you can change the EOL!! After I switched it to unix it run perfectly!! Thank you so much!! Finally I could solve it after hours of figuring out!!
Yes I did, thanks for the reply, I solved it (previous comment)!
Great! 👍 
You're welcome 😄
not sure if some other trick is available, but why not do it manually? echo {a..z} {a..z}{a..z} {a..z}{a..z}{a..z} {a..z}{a..z}{a..z}{a..z}
well, if perl is okay perl -le 'print join " ", a..zzzz'
Why is no one asking the ultimate question why you are using echo to set a variable to 11. You can just do a="11" . 
lol I edited the posted code from my messy code, so I should've missed that.
how are you running it on iOS? like a python app?
And you shouldn't parse the output of `ls` anyway ;)
&gt; Some Linux distributions are preconfigured so that `ls` supports an environment variable called `LS_OPTIONS` Huh, I’ve never heard of that – all I’ve seen is default aliases with `--color=auto` for `ls` and `grep` (in `/etc/skel/.bashr` for Debian and Arch).
Why not?
[Why you shouldn't parse the output of `ls(1)`](http://mywiki.wooledge.org/ParsingLs)
Wow, that's interesting. TIL!
I've tried it with both single and double quotes and it didn't work either way. I actually gave up on this method and will be using the built-in method of using the ~/.i2csshrc file and listing all the clusters and hosts in there in YAML format. It's gonna require a rewrite of all the clusters I have (luckily not that many, only about 32 so far with about 7 hosts each), and they're going to have to be done manually since I don't feel like inserting the correct formatting into for loops, rather then just looping a string into a file like I was doing with the cluster files. I'm still interested on why this wouldn't work though, is it a problem with ruby and not bash?
Missing `set editing-mode vi` line. Otherwise a good read.
It's meant to be for humans... like for normal people. I'm a vim user, but pretty sure most people wouldn't want vi editing mode for their shell (and other readline-based programs).
I think even in vi mode it retains most of the emacs shortcuts anyway. While I didn't initially use vi mode for editing (long, long time ago) - many utilities use vi conventions - less, man, parts of mutt, console browsers, etc... So it only made sense to add vi mode to readline and tmux, if you are using vim that is. I find it useful quite often.
This looks pretty awesome. I don't have a need for it, but I might just install it anyway. Not sure why I like `all bash` stuff so much.
 for i in $(find . -name *.mkv); do ffmpeg -i "$i" -codec copy "${i%}.mp4"; done Let find do the recursive search. Test it first by replacing ffmpeg with a simple echo $i
Bash has a feature named "globstar" where a `**` will make it search through sub-directories. You need to enable it with `shopt` in your .bashrc, using a line like this: shopt -s globstar When it's enabled, you can then use `**/*.mkv` instead of `*.mkv` to solve your problem. To get a feel of what the `for` loop will do when using `**`, check what's getting printed if you do this here in that location where your videos are: echo **/*.mkv
`find` has the exec option. You can use that and eliminate the usage of loop entirely.
Just an advice, don't try any options (both short and long) you think might mean what you think it is, `man &lt;command&gt;` to get the clear answer or read the help message of the command. `-r` could be meaning `--remove` for all we can guess, or it could be some in-place action, and you would be messing up the files. Fortunately, in this case, it's for frame rate in FFmpeg.
I guess it will be one of those mysteries, thanks!
Two errors there. You forgot quotes around the pattern to -name, and that loop doesn't handle filenames properly. Use either -exec or -print0 # using -exec find . -name "*.mkv" -exec bash -c 'for f; do ffmpeg -i "$f" -codec copy "${f%.*}.mp4"; done' -- {} + # using -print0, which is non-standard, but at least GNU- and BSD find # implement it the same find . -name "*.mkv" -print0 | while IFS= read -rd '' f; do ffmpeg -i "$f" -codec copy "${f%.*}.mp4" done
I think you should store folder contents values and compare it: \`ls -l -R /path/to/folder &gt; some_file_old.txt\` By cron every hour check changes and compare values: \`ls -l -R /path/to/folder &gt; some_file_new.txt \` &amp;&amp; if [[ \`diff some_file_old.txt some_file_new.txt 2&gt;/dev/null | wc -l\` -ne 0 ]]; then rsync /path/to/folder another_server:/path/to/folger &amp;&amp; cp -a -f some_file_new.txt some_file_old.txt; fi
There are very good tools to achieve this. `unison` is CLI-friendly, straight-forward but does **not** work in heterogeneous networks (you **need** the same OS, OCaml and unison version everywhere). `syncthing` is not so-much CLI friendly, but works across different systems and is p2p.
You should dump your scripts in the shellcheck machine to detect some syntax ~*errors* (link in the sidebar). (Usage of `[`, `/bin/bash` instead of `/usr/bin/env bash`...)
`pkill` had a problem like this for a time. “I want to see which processes are being killed. I know, I’ll add the `-v` option, for *verbose*!” Except `pkill` is the sibling of `pgrep`, and so `-v` means invert the match, just like in `grep` – so you were killing everything *except* what you wanted to kill. This feature is now disabled – &gt; In `pkill`'s context the short option is disabled to avoid accidental usage of the option. – but not before several people fell for the trap…
I've read `inodes` should be just the things for that (monitoring folders). Hope this is applicable for your question.
Last time I used shellcheck it kind of made me wreck everything, but that was probably my fault... I will give it another go, thanks!
Very good advice. Don't throw random stuff at your command line. Everything has "--help" and "man".
You can try something like `for i in {*.mkv,*/*.mkv,*/*/*.mkv,*/*/*/*.mkv};` If you're into hacky and uncertain stuff. Be aware that it doesn't really recurse deeper than either 4 or 5 sub-folders (and that seems fluctuating, maybe even 3). So, I don't responsibility for nothing, and probably is smarter to use find with exec. edit: test with `echo "${i%}"` edit2: meh, should work. I'd use it.
All the popular operating systems offer filesystem notification APIs, and Linux has [inotify](https://en.wikipedia.org/wiki/Inotify). There are tools like [inotifywait](https://github.com/rvoicilas/inotify-tools/wiki#inotifywait), [notify](https://godoc.org/github.com/rjeczalik/cmd/notify), and [watchman](https://facebook.github.io/watchman/) that let you trigger commands on filesystem events.
Good read! I didn't know about most of these settings. The improved keybindings for Control+N and Control+P and the `completion-prefix-display-length` setting seem pretty useful. I'll probably add those to my dotfiles.
That's impressive. I have to try it out.
I have a version of it that just init 0's the box. Could probably add it to a login script. "Why does it say click every time I log in?" "Hey it didn't click this ti-WAIT WHY ARE YOU SHUTTING DOWN NO STOP"
For the sake of completeness, here's how you could do it in zsh: print ${(l.$(tput cols)..#.):-} Result with execution time: % typeset -F SECONDS % s=$SECONDS; print ${(l.$(tput cols)..#.):-}; print $(( SECONDS - s ))s ########################################... and so on 0.0028460000030463561s ETA: Actually if you use `$COLUMNS` instead of `tput` it reduces it to something like 0.000169 seconds
Nice! I have never used `zsh` but that is cool.
Great post. You could wrap it up in a function something like this: hr() { printf '%*s\n' "${1:-$COLUMNS}" | tr ' ' "${2:-#}" } So now we can output a specific width or the full width based on a positional parameter: $ hr 80 ################################################################################ $ hr 40 ######################################## and with specific characters: $ hr 20 $ $$$$$$$$$$$$$$$$$$$$ $ hr 20 \* ******************** 
&gt; ## Seq to tr &gt; Let’s cut out printing statements entirely and trust my reliable **builtin** friends: Huh? They’re not builtins on my system. $ type seq tr seq is /usr/bin/seq tr is /usr/bin/tr $ echo $BASH_VERSION 4.4.12(1)-release Also, this: &gt; I will use GNU time 1.7 with an average of 3 tries to control for the I/O cache. &gt; $ time ./naive &gt; real 0m0.840s &gt; user 0m0.746s &gt; sys 0m0.001s That looks like Bash builtin `time` output, not GNU `time` output. But perhaps there’s some environment variable to make GNU `time` look like that too? And finally, here’s my version: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/ioctl.h&gt; int main(int argc, char *argv[]) { char c = argc &gt; 1 ? argv[1][0] : '\0'; c = c ?: '#'; struct winsize w; ioctl(STDOUT_FILENO, TIOCGWINSZ, &amp;w); for (int i = 0; i &lt; w.ws_col; i++) putchar(c); return EXIT_SUCCESS; } Oh, you think using C is unfair? It’s still in competition even if you include the build time! Let’s call it “just-in-time compilation” ;) EDIT: I forgot – the code to get the columns is from [this Stack Overflow answer](https://stackoverflow.com/a/1022961/1420237) by John T, [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/). EDIT 2: The code isn’t *quite* ISO C because, due to a sudden attack of extreme laziness, I used the “Elvis” operator (omitted the middle term of the `?:` – GNU extension, also supported by clang). If you want a portable version, use `c ? c : '#'` instead (in this simple case, they’re equivalent, though in general the terse form avoids reevaluating the condition/middle term).
This doesn’t look like it’s Bash related – please ask on another subreddit (/r/C_Programming looks promising).
subtitled: "or `find(1)` for that matter"
Using an external script for such a thing is just stupid. It's better to have a little redundancy, than having a small dependency.
 Huh? They’re not builtins on my system. Thanks for noticing that! That was an error on my side. I wrote that about another solution I had that used echo and got sidetracked with seq and forgot to update that text. Fixed now. That looks like Bash builtin time Yep, you are right again. I was stupid and just did which time and then /usr/bin/time -v and forgot that the Bash time would be the shell keyword. Thanks for teaching me something new! Also, nice C code! :) I think it definitely counts; I just was trying to stick to one-liners. I got the .027s on my machine (assuming I tested correctly) so it looks like we have a new winner! Mind if I add your code to my blog with attribution to you? 
&gt; I got the .027s on my machine (assuming I tested correctly) so it looks like we have a new winner! That’s nice to hear, on my machine all the timings were completely different so I couldn’t really compare :) &gt; Mind if I add your code to my blog with attribution to you? Of course, thank you :)
Here’s a variant of the C version that supports multibyte characters (assuming UTF-8 argv): #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/ioctl.h&gt; int main(int argc, char *argv[]) { char *c = ""; if (argc &gt; 1) { c = argv[1]; char *e = c; while (*e &amp; 0xc0) e++; *e = '\0'; } if (*c == '\0') { c = "#"; } struct winsize w; ioctl(STDOUT_FILENO, TIOCGWINSZ, &amp;w); for (int i = 0; i &lt; w.ws_col; i++) printf("%s", c); return EXIT_SUCCESS; }
Thanks
Why do you mean? There are a bunch of save ways to parse `find` output: using `-print0` supported by GNU and BSD `find`, or `-exec command {} \;` or `-exec command {} +`, both of which are supported even by POSIX `find`. But yes, doing something like `for file in $(find -name 'blah')` has all the same problems as parsing `ls`.
it shares the pitfalls that `ls` has, specifically handling spaces and other odd characters badly.
Why use a function? Wouldn't this work? (I can't test right now though, so it's a shot in the dark) (you should play with quotes, see when/where the `$i` and `seq` get evaluated) local $ ssh -p 1022 root@10.10.10.10 -t 'for i in $(seq 254); do ping 192.168.1.$i -c1 -W1 &amp;; done' | grep from local $ ssh -p 1022 root@10.10.10.10 -t '/bin/sh -c "for i in $(seq 254); do ping 192.168.1.$i -c1 -W1 &amp;; done"' | grep from
i want to use a function because i don't want it to be systematically launched everytime i connect to a host , also if if set my function like this pb() { for i in $(seq 254); do ping 192.168.$1.$i -c1 -W1 &amp; done |grep from i can the type **pb 2** to ping 192.168.2.x or **pb 16** to ping 192.168.16.x
What are you really trying to achieve here?
The problem is that the servers are running freeBSD and not linux. Do you know of any tools I can use in freeBSD?? Because the ones you have mentioned work in linux. I am sorry I should have mentioned it earlier.
i try to automatically set a function at login . pinging the entire /24 subnet is very usefull to me , so i wish to create this little shortcut instead of copy pasting the one liner each time.
 Looks like you're missing a closing curly bracket (`}`) for your function.
Wow! In regards to your edit, I will have to try `zsh` tonight then. That's faster than the pure C solution then. I would be curious how they did it. 
I agree. To be fair to them though, they are mostly JS developers
I like that a lot. Now I actually see this being useful for more things. Do you mind if I add this to my blog post with attribution to you?
inotifywait is obviously Linux-specific, but the [library docs for notify](https://godoc.org/github.com/rjeczalik/notify) say which notification API functions are used on different systems, and it includes FreeBSD's [kqueue](https://en.wikipedia.org/wiki/Kqueue). Same thing for the [watchman install docs](https://facebook.github.io/watchman/docs/install.html). [fswatch](https://github.com/emcrisostomo/fswatch) is also cross-platform.
No problem - go for it :) /edit: there is one big caveat to that function: if you want a full width of a specific character. It would be nice to be able to stack failover variables so that you could just go `hr - $` for example. To cater for that scenario you could either detect the characters in `$1` and make an assumption, or switch to `getopts`
There are a few things wrong with the code but wrap $output_target in double quotes if ! cp foo.txt "$output_target" The echo looks okay because it's actually not doing what you think it's doing. It's not outputting "Output Staging". It is actually outputting "Output" then "Staging" with a space as a separator.
Run your script through shellcheck.net!
Run your script through shellcheck.net!
How about using escape chars? 'foo\ bar' 
shellcheck.net is wonderful. Thank you!
Yeah it's an absolutely essential tool that *any* shell script should be run though. Sorry I was not more verbose in my original post, I was typing on mobile.
Anytime you want to do `ls | grep`, you're probably going to be much better off if you use `find` instead – it has a whole bunch of options to narrow down according to criteria you didn't even know you might need. The reason for that is that UNIX filenames can contain absolutely anything except `'/'` and NUL bytes, so that means things like newlines and spaces are all fair game (well not exactly fair, but possible nonetheless). Specifically, it would be easy for the output of `ls` to get screwed up, breaking your script. Long story short, look at the `-regex` option of [find(1)](https://linux.die.net/man/1/find). Some combination of `find`, `sort` and `tail` will do the trick.
The thing is, he's using grep with that `-o` parameter which makes it print just the numbers in the file names. He would still want to use grep to do this even after switching from ls to find.
You can add a line like this before the line where the warning is to make shellcheck ignore something you know you really want to do: # shellcheck disable=SC2010
deleted ^^^^^^^^^^^^^^^^0.2454 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/78481)
You can avoid grep by using ls wildcards: ls foo[0-9][0-9][0-9][0-9].bar | tail -n 1 
deleted ^^^^^^^^^^^^^^^^0.7496 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/04236)
deleted ^^^^^^^^^^^^^^^^0.6191 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/70578)
deleted ^^^^^^^^^^^^^^^^0.0918 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/62302)
ShellCheck uses "best practice" rules of thumb and should not be taken as infallible. There's nothing wrong with what you're doing.
It's not the same. 'find' will descend into subdirectories, unlike 'ls'.
deleted ^^^^^^^^^^^^^^^^0.8367 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/72256)
Or just keep using 'ls', it's really not harmful here at all as 'find' with '-maxdepth 1' is equivalent to 'ls'. The argument against parsing 'ls' output is that you get trouble with filenames containing whitesapce or other unusual characters, but since you're using `grep -o` to extract numbers that argument doesn't apply. By the way, the `head -n 1` can be replaced with an `-m 1` option to `grep`. 
 z$ touch foo000{0..5}.bar z$ ls foo0000.bar foo0001.bar foo0002.bar foo0003.bar foo0004.bar foo0005.bar z$ : foo[0-9][0-9][0-9][0-9].bar ; printf %s\\n "$_" foo0005.bar z$ print_last_arg () { : "$@" ; printf %s\\n "$_" ;} z$ print_last_arg foo[0-9][0-9][0-9][0-9].bar foo0005.bar z$ print_last_arg one two three four five five z$ 
You could use something like this: declare -a list list=(*) max="${list[0]//[.a-z]/}" for i in "${list[@]//[.a-z]/}"; do ((10#$i &gt; 10#$max)) &amp;&amp; max=$i done echo "$max" It reads all the filenames into an array, strips the lowercase letters and dots with pattern substitution and compares the values. You would have to adjust the regex if your file naming convention changes.
deleted ^^^^^^^^^^^^^^^^0.1582 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/50681)
If you want to use `$PATH` in this, you need to split it. And `find` must print filenames with newlines, or `grep -m ` doesn't work properly. Edit: Also `find` doesn't sort output at all, so pipe to `sort -r` before `grep`. This didn't work: find ${PATH//:/ } ... This worked, though there must be a better way: IFS=: for dir in $PATH; do find "$dir" -maxdepth 1 -name 'foo*.bar' -printf '%f\n' done | sort -r | grep -o '[0-9]\+' -m 1 Edit: the better way is this: IFS=: find $PATH -maxdepth 1 -name 'foo*.bar' -printf '%f\n' | sort -r | grep -o '[0-9]\+' -m 1
The issue comes when there's a newline in one of your filenames. Probably doing something with `find -exec` or even `find -print0 | xargs -0 grep` would be better. I'm on my phone right now, but I can try to come up with a solution when I get back to a computer. OTOH, it may not be worth the effort. If you know the format of all your filenames, and you document this conspicuously, I probably wouldn't bother working out how to do it "properly".
Gotta love how solutions written in Bash get dv-ed in /r/bash. 
deleted ^^^^^^^^^^^^^^^^0.8991 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/69971)
deleted ^^^^^^^^^^^^^^^^0.4866 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/34414)
There probably isn't any real difference between those two, but if any of your filenames have a newline character in them (e.g. you did `touch $'my\nstupid\nfile'`), that would get treated as three files called `my`, `stupid`, and `file` by the next program in the pipeline, since ls and find both use newlines as filename delimiters (they print each filename on a new line, at least when outputting to a pipe). Using `find -print0 | xargs -0 my_program` avoids this problem by using the NUL character to separate filenames. Edit: it's interesting to see how various tools break on filenames containing newlines -- for example, my bash tab-completion doesn't actually work to complete a filename containing newlines.
&gt; Using uppercase for local variables is an unconventional choice Point taken. I have changed those now. &gt; The way you're using printf (injecting arbitrary text directly into the format string) isn't recommended That is also a good point. I saw the error in shellcheck but wasn't sure how to format it to handle more than 1 variable. Your example makes sense. I think I am going to just cut out the color stuff entirely because I have seen various opinions with hardcoding tput values and it's not super necessary. &gt; grep -P isn't portable That is true. I wasn't trying for complete portability since I used `command -v` anyway, but I will look into how to change that. &gt; You can use (( $# == 0 )) instead of the bracket construction True. I am just following the google style guide here. I agree it is prettier though. One interesting trivia fact here is that I think the Bash manual even says that == is the exact same as =. &gt; You might consider doing this for your usage help: Thanks! I didn't go with that because you can't indent the EOF any (except tabs) and I was being way too obsessive with matching up indentation. I will probably switch to that though and get over it. &gt; Your else at the end is technically unnecessary Good point. That was just a style choice for me but you are right. I will probably fix that. Thanks for taking the time to review my script! It really helps :) 
Perhaps another way to approach this is to consider "how would I achieve this portably?" `grep -o` is immediately out of the equation. So is `find -maxdepth`. The kneejerk approach would be to start a glob loop and iterate through every file name. You could build up a pure POSIX way to achieve your ultimate goal, probably using a variable as a register etc, and that would be a logical, programmatic approach. Then you'd probably try to upgrade the guts of such a loop into an efficient but probably unreadable `awk` one-liner. I don't know... but here's a bit of fun showing that there's more than one way to skin a cat: mkdir tmp cd tmp for (( count=0; count&lt;=20; count++ )); do touch "foo$RANDOM.bar"; done echo foo*.bar | tr " " "\n" | sort -k1.4n foo333.bar foo2187.bar foo4735.bar foo5072.bar foo7451.bar foo8778.bar foo10346.bar foo11714.bar foo14681.bar foo15514.bar foo15598.bar foo16926.bar foo19034.bar foo22388.bar foo23640.bar foo24987.bar foo25563.bar foo26428.bar foo27200.bar foo27952.bar foo31251.bar echo foo*.bar | tr " " "\n" | sort -k1.4n | tail -n 1 | tr -d '[a-zA-Z].' 31251 So this uses an interesting use of `echo` that will be useful to anybody stuck in a [Unix Recovery Legend](http://www.ee.ryerson.ca/~elf/hack/recovery.html) situation (as I have been a few times in my career, let me know if you want to hear a story) where `echo` can be used as a simpleton `ls` alternative. We next use `tr` to convert spaces to newlines - printing `echo`'s output per-line. Then we use `sort` to do a numerical sort starting at the fourth character. `tail` to select the last listed item, and if we want just the number, we can use `tr` to delete the characters we don't want. One caveat with this method: it won't differentiate between files and directories: mkdir foo456.bar echo foo*.bar | tr " " "\n" | sort -k1.4n foo333.bar foo456.bar foo2187.bar ... By the way, the reason for shellcheck's warning is that it's one of those unspoken golden rules: [Don't Parse ls](http://mywiki.wooledge.org/ParsingLs), even though I'd agree that this is possibly an exclusionary edge case to that rule.
... So why do you find pinging the entire /24 subnet useful?
well , to see witch host are up .
deleted ^^^^^^^^^^^^^^^^0.4913 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/82077)
You can get your answer here: [Why you shouldn't parse the output of ls(1)](http://mywiki.wooledge.org/ParsingLs). Just for the record, find is also not the greatest solution.
If you want a single script, you can symlink prev to next and in the next script check if it's prev with `$0`.
Thank's for the input. These are meant to be sourced in to other scripts however. :)
You can use a discovery service like avahi or upnp: server announces its ip address and port, clients query the discovery service and know how to connect to the server. Then you can use a strategy like: a client that detects there is no server will start acting as a server.
deleted ^^^^^^^^^^^^^^^^0.7893 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/47809)
Here's how to do it with a "for" loop: finder() { local -a args local x for x in "$@"; do args+=( -iname "*${x}*" ) done find . "${args[@]}" 2&gt; /dev/null } This version here uses a "while" loop and the "shift" command to work through the list of parameters: finder() { local -a args while (( $# )); do args+=( -iname "*$1*" ) shift done find . "${args[@]}" 2&gt; /dev/null } That "local" makes it so those variables exist only while the function runs, and it also makes it so the function won't change variables with the same name that might already exist outside of the function. That "args" variable is used as an array. That `+=( ... )` operator adds new entries to the end of the array. I used an array to make it possible to use space characters in the parameters to the function. The `(( ))` operator turns on the number mode for bash. Doing `(( $# ))` tests if the `$#` variable is different from zero. The "while" loop will stop after the "shift" has removed all parameters. How the insides of that `(( ))` work exactly, you can read about by typing these commands here at the bash prompt: help let help '((' 
Thanks a million for the answer and good explanation.
So I can't actually get your way to work... I did some googling and I came up with this: function finder { for i in $*;do params=" $params $d-iname "*$i*"" done find ~/ $params 2&gt;/dev/null } It seems to do the trick but I'm still interested in getting your versions to work too. I tried switching some stuff around but I always get: ./finder.sh: line 17: syntax error near unexpected token `(' ./finder.sh: line 17: ` args +=( -iname "*${x}*" )' When I take away the brackets I get output but it just finds *everything*, as if it got no arguments to find at all. Thanks again.
I'm guessing you added a space before the operator? It's hard to see here because you didn't make reddit format it as code. That `+=( )` operator has to be directly after the variable name, same as the normal `=` operator in bash. You can copy and paste the stuff out of my post directly to a bash prompt in a terminal window. You can then do `finder whatever` at the prompt there to test it.
I don't know what I did wrong... I DID type it in originally by copying you and copy-pasting DID work like you said. Still can't figure out where I mistyped but your solutions totally work when I copy/paste them. Sorry for the confusion.
This is what comes to mind for a somewhat fool-proof centralized way of finding the main server. All clients will assume that the service defaults to using a certain port, and they scan their entire subnet for other clients with that open port. If they find a client with an open port, they can assume that is is the main server. If clients are listening on that port too (not sure why they would, but it's possible), you can also build in some sort of authentication where if the client finds that open port they send a request payload and wait for a response that says "server" or something. If they don't find anything, the clients can begin searching all ports on all clients and send a verification code. This is if the server sets the listening port to a non standard port. In any case, you need to work out a mechanism for payload authentication. I'm not sure exactly what you'd want to do, but it's basically just something like this. Client on port 1234 - "Are you a server for the software XXX?" Server on port 1234 - "Yes I am a server for the software XXX, here is also some more information about my version, etc"
Try this: ls foo*.bar | awk 'END { gsub(/foo|\.bar/,""); print }' That awk code will wait until last line (bigger number), remove foo and .bar, and print it.
Thanks for posting this. You are doing some things here I didn't even know were possible in bash.
Here’s my version, just treating 15 as a separate case out of laziness: for ((i=1;i&lt;=100;i++)); do if ! ((i%15)); then echo FizzBuzz elif ! ((i%3)); then echo Fizz elif ! ((i%5)); then echo Buzz else echo $i fi; done Or, as the one-liner that I actually wrote into my prompt: for ((i=1;i&lt;=100;i++)); do if ! ((i%15)); then echo FizzBuzz; elif ! ((i%3)); then echo Fizz; elif ! ((i%5)); then echo Buzz; else echo $i; fi; done
&gt; Can it be made in an one-liner in awk? z$ z$ # Bash z$ printf %s\\n {1..10} | &gt; awk '{if ($0 % 3 == 0) {print "fizz"} &gt; else if ($0 % 5 == 0) {print "buzz"} &gt; else print}' 1 2 fizz 4 buzz fizz 7 8 fizz buzz z$ z$ # POSIX z$ awk 'BEGIN { &gt; for (i = 1; i &lt;= 10; i ++) &gt; {if (i % 3 == 0) {print "fizz"} &gt; else if (i % 5 == 0) {print "buzz"} &gt; else print i}}' 1 2 fizz 4 buzz fizz 7 8 fizz buzz z$ **edit to meet additional requirements not mentioned in the original specification** z$ printf %s\\n {1..100} | awk '($0%3 &gt; 0) &amp;&amp; ($0%5 &gt; 0) {print; next} $0%3 == 0 {printf "fizz"} $0%5 == 0 {pr intf "buzz"} {printf "\n"}' 1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz z$ 
&gt; Programmers use the letter "I" for variables like this because uhh ... actually I have no idea! I'm pretty sure that started with Fortran, where single letter variable names below `I` could not be used to store integers. https://en.wikibooks.org/wiki/Fortran/Fortran_variables &gt; Absent an IMPLICIT statement, undeclared variables and arguments beginning with I through N (the "in" group) will be INTEGER, and all other undeclared variables and arguments will be REAL. It so happens that `I` is a convenient mnemonic because `for` loops are often used to iterate over arrays, and one can easily assume that `I` stands for the word *Index*.
Nice reply. Much more readable and kept it simple. OPs solution feels overly complex and took a few moments whereas your solution was immediately understandable.
What, you think `! ((i%15))` (instead of `((i%15==0))`) is readable and simple? :D ^thanks ^:)
Yeah that part is the most odd of the solution. But as a whole it's just more readable. Good point on the bang though.
It's 1 to 100, and multiples of 3 &amp; 5 must print fizzbuzz on a single line, so you can't simply do if, else if, else.
I did it three days ago in a [one-liner](https://www.reddit.com/r/mealtimevideos/comments/6qt8sh/tom_scott_fizzbuzz_one_simple_interview_question/dl0xqct/).
That's clever! Thanks for sharing
*sigh* z$ printf %s\\n {1..100} | awk '($0%3 &gt; 0) &amp;&amp; ($0%5 &gt; 0) {print; next} $0%3 == 0 {printf "fizz"} $0%5 == 0 {pr intf "buzz"} {printf "\n"}' 1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz z$ 
Very nice, I like that one-liner. :) I've never used the ! before in an if statement, I'll have to try that! The reason I went for creating an array is because, as Tom explains at around 6:25 in his video, if the interviewer asks for multiples of 7, 11, 13 and so on, you don't have to copy and paste any more if statements. Making it a little bit more modular, although I have to agree it's not very pretty to loop through array indexes in bash. :) 
It is, but OP was basing his solution on the video he cited. In there, the guy intended it to be a flexible solution that allows easy addition of other values.
You could use bash [RE](http://mywiki.wooledge.org/BashFAQ/041). It is [suggested](http://mywiki.wooledge.org/ParsingLs) you never parse output of `ls`. re='foo([0-9]+)\.bar' for i in foo*.bar; do if [[ $i =~ $re ]]; then printf '%d\n' "${BASH_REMATCH[1]}" fi done | sort -n | tail -n 1 Or to combine it with the solution of dewsbury: re='foo([0-9]+)\.bar' let max=0 for i in foo*.bar; do if [[ $i =~ $re ]]; then (( BASH_REMATCH[1] &gt; max )) &amp;&amp; max=${BASH_REMATCH[1]} fi done printf -- '%d\n' "$max" The variable `$re` holds the extended regular expression. The variable `${BASH_REMATCH[n]}` holds the value of n-th matched group (0 holds the whole match). IIRC.
Not super pretty, but functional. for x in {1..100}; do ( ! (( $x % 15 )) &amp;&amp; echo "FizzBuzz" ) || ( ! (( $x % 3 )) &amp;&amp; echo "Fizz" ) || ( ! (( $x % 5 )) &amp;&amp; echo "Buzz" ) || echo $x; done
I thought it was obvious that I was experimenting. I wouldn't do that with files I was afraid of losing or corrupting. The Man file for ffmpeg is huge and goes above my head with mostly codec stuff. I was crossing my fingers that I missed something or something wasn't listed in the man file.
Haha "Meh, should work. I'd use it." This is my general attitude toward everything.
This is better than what I was looking for! Thanks! I was hoping for a one liner but this will work better than using a loop. 
I tried to come up with something that is supposed to be a bit like what you might do in a functional programming language: paste -d '' &lt;(yes $'\n\nFizz') &lt;(yes $'\n\n\n\nBuzz') &lt;(seq 2147483647) | sed -r '/^[0-9]/! s/[0-9]//g' | head -n 100 With some line-breaks, it looks like this: paste -d '' \ &lt;(yes $'\n\nFizz') \ &lt;(yes $'\n\n\n\nBuzz') \ &lt;(seq 2147483647) | sed -r '/^[0-9]/! s/[0-9]//g' | head -n 100 The two `yes` programs output streams of empty lines and "Fizz" or "Buzz" lines, the `seq` outputs a stream of numbers. The `&lt;()` and `paste` combine those different streams. The `sed` removes the numbers on the lines that have words on them. The `head` stops everything after 100 lines. Here's something somewhat similar with more bash: repeat() { local count="$1" word="$2" i while :; do for (( i = 1; i &lt; count; i++ )); do echo done echo "$word" done } paste -d '' &lt;(repeat 3 Fizz) &lt;(repeat 5 Buzz) | while read line; do (( ++i )) if [[ -z $line ]]; then echo $i else echo "$line" fi done | head -n 100 
My goal Was to leave the host unchanged, to not edit or add a rc file to the host 
 for i in {1..100} do if [[ $i%15 -eq 0 ]] then echo "FizzBuzz" elif [[ $i%3 -eq 0 ]] then echo "Fizz" elif [[ $i%5 -eq 0 ]] then echo "Buzz" else echo $i fi done 
Then that guy needs a nice whack over the head with the YAGNI stick. 
Well he did it both ways in the video. And said in interviews it's common to ask follow up questions to expand the requirements (3=Fizz, 5=Buzz, 7=Fuzz, 11=Bizz, 13=Biff). I think OP's solution is good for this expanded problem.
[removed]
You ran into a really weird error. I never realized this, but it seems if you have a number starting with a zero, then bash will treat it as an octal number when it does calculations inside `(( ))`. What I mean is, check this out: $ echo $(( 12 )) 12 $ echo $(( 012 )) 10 That `012` is `10` because it's seen as `1 * 8 + 2 = 10`. You could solve it by moving the whole calculation into bc: #!/bin/bash file=animals.txt total=$(wc -l &lt; $file) words=$(sort --unique &lt; $file) for word in $words; do count=$(cat $file | grep -c "$word") oftotal=$(bc &lt;&lt;&lt; "scale=1; 200 * $count / $total") echo "$word $oftotal" done In this example here, you could also just use bash by itself: oftotal=$(( 200 * $count / $total )) You need to be careful when using `scale=0` in bc when dividing. If you do the division first, then you'll always see 0 as the result. I avoid the problem here by doing the multiplication of $count by 200 first. This is then a nice and large number and the division by $total gives a good result even when using `scale=0`. A problem is also that bc never will round right. For example: $ bc &lt;&lt;&lt; "19/20" 0 You can work around that by using `bc -l` for good precision, then rounding by using bash's `printf`: $ bc -l &lt;&lt;&lt; "19/20" .95000000000000000000 $ printf "%.0f\n" $(bc -l &lt;&lt;&lt; "19/20") 1 In your program, using printf would look like this: #!/bin/bash file=animals.txt total=$(wc -l &lt; $file) words=$(sort --unique &lt; $file) for word in $words; do count=$(cat $file | grep -c "$word") oftotal=$(bc -l &lt;&lt;&lt; "200 * $count / $total") printf "%s %.0f\n" "$word" "$oftotal" done
That's terrible.
Here's one in `dc`: echo -e '[q]Sq [[\n] n]Sn [0Sm [Fizz] n]Sf [0Sm [Buzz] n]Sb [lmn]Si [ddd Sm 3%0=f 5%0=b lm 0!=i lnx 1+ d 101=q llx]Sl 1 llx ' | dc
And `sed`: seq 100 | sed '0~3s/.*/Fizz/;0~5s/[0-9]*$/Buzz/'
Thanks giigu and ropid.
"one liner"
Threading. Multi Threading.
The loop you talk about is to find what name belongs to a hash? If that's what you are doing, you could do another associative array that saves things in reverse: word_to_hash[test]=53897568563876576 hash_to_word[53897568563876576]=test
Can you show that loop for comparing? I can't guess what it does and what it looks like.
Using `&amp;` will run the command in the background, and using `|` will run a series of commands in parallel; you will see `|` used as a synchronization method since it will connect stdout to stdin from left to right. Running the commands in the background will run them in _parallel_, which doesn't inherently mean it will run them in a multi-threaded fashion. It _will_, however, scale most of the time to use multiple threads, so in your case it will do what you desire.
`pgrep -f "ff p2"` will match your script (i3-**ff p2**)
Here's my solution that doesn't use a modulus operator or for loops: seq 100 |sed '0~5s/.*/Buzz/g;0~3s/.*/Fizz/g;0~15s/.*/FizzBuzz/g' I wrote about it here: https://grayson.sh/blog/fizzbuzz-in-bash-no-modulus
And how exactly do you propose to assign to an associative array with “Multi Threading”? Pipe components and background jobs both run in subshells.
The version using `factor` incorrectly prints "Fizz" for 31, and "Buzz" for 53. You might want to replace `5.*` and `3.*` with something like `5\( .*\|$\)` or `5\&gt;.*` depending on what your version of sed supports (and likewise `.*3 *` with `.* 3 *`). **Edit**: There will always be a space before the 3, so no need to use anything fancy there.
The the answer to the question "Is X a multiple of Y?" is the definition of the modulus function, no matter how one implements it.
Good catch. I'll look into fixing that now. 
That's true. But I am specifically trying to avoid just the % operator here. I probably should specify more. 
Thank you for your suggestion. This seems to be working now with your help: seq 100 | factor | sed 's/.*3 * 5.*/FizzBuzz/g; s/.* 5\( .*\|$\)/Buzz/g; s/.* 3\( .*\|$\)/Fizz/g; s/:.*//g' With `sed` version `(GNU sed) 4.2.2`. Could you test on yours? Also, do you mind if I update my blog with attribution to you?
This is cool and short, but the reason it is usually done with modulus is that unless you're filtering email, or blog comments, you don't want to reduce your program to parsing text which generally puts you into the 'tons of edge cases' territory. i.e. instead of checking for the word success from a ping command, you'd want to ask the shell what the last exit code was as that's should be more reliable.
I know, and you are correct about that being a better solution. I would never use this to do anything professionally, I was just doing this as a stupid challenge to see if I could. As a side note, I wouldn't 100% trust exit codes either. But that is probably a better topic for another discussion. 
Been playing around with both these solutions and noticed the sum of all the $oftotal is higher or lower than any mumber i set 200. Assuming this is a rounding error? Not sure where to go from here. On further investigation I've noticed it is counting one few line in the text file when making the total, not sure why this would happen from the code
Sure, no problem.
Yes, that is correct but as a bash script, you are a bit limited with error trapping and recovery. This is cool and I like to do things like this as well. I actually wrote the same thing using factor and uniq in a cheeky little one liner pipe for an interview once.
1. This is a good tool for generating `LSCOLORS`/`LS_COLORS`: https://geoff.greer.fm/lscolors/ 2. For BSD `ls` (which macOS uses by default), yes, those two variables you set should do it 3. Those are part of the shell prompt. Here's a tool for generating that: http://ezprompt.net/ You can find the details on the `LSCOLORS` syntax by running `man ls` and searching for `LSCOLORS`. You can find the details on the bash prompt syntax by running `man bash` and searching for `PROMPTING`.
Here's an example that uses process substitution and pre-generated Fizz/Buzz lists without treating the 15-multiple case as special: L=100 ; paste -d '' &lt;( seq 1 $L ) &lt;( yes "$(echo -e "\n\nFizz")" | head -n $L ) &lt;( yes "$( echo -e "\n\n\n\nBuzz")" | head -n $L ) | sed 's/^[0-9]*\([^0-9]\)/\1/g'
Thank you very much! That was super helpful
Way more complicated than my first script. I'd recommend sticking with lowercase or camelcase variables. Uppercase are used by the system. Variables are case sensitive. 
How long have you been programming? And when did you start using Bash? I've been a hobbyist coder for around 13 years now with Windows BAT scripting; assembly 8086 or 8088 -- I never really did anything with them, and never understood more than the basics which I now don't remember but I've screwed with other stack-based assembly languages since then including with a full script language to assembly language parser/lexer/linker/compiler (Deitel &amp; Deitel used to have them in some of their programming books, but IDK if they still do -- I found them in a Web Programming one [in JavaScript], and in a Java Programming one); Perl 5; JavaScript; the very basics of C, Java, JavaFX, Basic; and I've at least gotten into the beginnings of C++, VC++, VB, VB.NET, C#, and more I can't remember (including those 5 - I know little to nothing about their syntax/etc nowadays). I've apparently been toying with Bash for months/around a year with Travis-CI and didn't know it though of course it was just some very basic usage. I've seriously gotten into Bash within about the last month though because another project I'm trying to help someone with uses a few Bash scripts and they are currently not working on Travis-CI so I'm trying to fix them. I also do nearly-unmentionable things to Google that make it very happy and it gives me results that help me greatly =\&gt; those things are manually using some of their advanced search features you can include in your query like "site:..." where the ... is a specific site you want to search for some given data, "double quotes around a phrase" to search for the whole phrase as-is on a page rather than the single words *anywhere* throughout a page, and the ever-magical-in-the-programming-world-word "and" (without quotes) to join multiple words or phrases together when a page has to have each of them included. I'm pretty sure there's more available, this is just the ones that I have used consistently for years so I remember well. Ah, thank you. Been trying to figure out how to name the functions and vars so they're not all just lowercase or with_underscore_between_words; hadn't even considered camelCase, lol. Cheers ;)
Just noticed, you still want to change that `.*3` in the first sed expression into `.* 3` with a space. Otherwise, if you extend it to 1000 instead of 100, you will say "FizzBuzz" for 689 = 13 * 53 and 767 = 13 * 59.
Nice solution! I'll follow you on github.
`mark-symlinked-directories` is a really nice improvement. `completion-prefix-display-length` is also an interesting option, though I prefer `colored-completion-prefix` personally
GNU `find` has the `-printf` action: find . -type f -printf "%f\n" | sort
&gt; I need the subfolders omitted from the results, listing filename only (required). find . -type f -exec basename {} \; | sort 
Which underlying OS/Distro are you using?
Kubuntu
do you know all the special characters that can occur in middle? then you can do something like grep -l 'e[_-]*x[_-]*a[_-]*m[_-]*p[_-]*l[_-]*e' *
Thanks! That's a good idea, but I need it to be able to work on arbitrary input strings, any idea how to address that? 
you can try grep -l "$(echo 'example' | awk 'BEGIN{FS=""; OFS="[_-]*"} {$1=$1; print}')" * but that depends whether your awk supports empty input field separator may be perl will be more portable grep -l "$(echo 'example' | perl -F -lane 'print join "[_-]*",@F')" *
The one problem I have with `history-search-{backward,forward}` is that it doesn't let you go back to your original input line if you change your mind, no matter how many times you do `history-search-forward`.
Ok, so this isn't so much a `bash` question as it is a /r/linux4noobs question. Because you're on 16.04, the answer you're looking for should be based around `systemd` e.g. https://askubuntu.com/questions/919054/how-to-run-a-single-command-at-startup-using-systemd-16-04 That's for starting a script as system startup. To start a script at login is down to KDE. Because I haven't used KDE in years I can't state anything authoritative, but this looks like perhaps what you're after: https://docs.kde.org/trunk5/en/kde-workspace/kcontrol/autostart/index.html
**Here's a sneak peek of /r/linux4noobs using the [top posts](https://np.reddit.com/r/linux4noobs/top/?sort=top&amp;t=year) of the year!** \#1: [2-3 hours of tutorials, exercises, and quizzes on basic Linux commands](http://linuxsurvival.com/linux-tutorial-introduction/) | [11 comments](https://np.reddit.com/r/linux4noobs/comments/5d21gk/23_hours_of_tutorials_exercises_and_quizzes_on/) \#2: [They say Linux is for normal people these days, but it feels like they think "normal people" are all CS majors \[LONG\]](https://np.reddit.com/r/linux4noobs/comments/6pgu7a/they_say_linux_is_for_normal_people_these_days/) \#3: [Hey guys! After noticing the flood of posts that go along the lines of, "which distro should I choose", I've realized that we need a place to organize all those posts and have a place dedicated to answering those questions. So I have created a new sub /r/FindMeADistro.](https://np.reddit.com/r/linux4noobs/comments/5r3fnl/hey_guys_after_noticing_the_flood_of_posts_that/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
Thanks!
Perfect, thanks! Interestingly this and the command provided by /u/yhsvghnrOruGnpverzN seem to have identical output when saved to a file. Thanks very much to both of you!
If you want human-readable output, there's `tree -L 1`.
Well isn't that nifty?! It outputs directories instead of files, so doesn't meet my present needs, but is definitely a command I'll save to my list for future reference. Thanks much!
I'm thinking that your original idea is the route I would go too, just don't use cat (see [UUOC](https://en.wikipedia.org/wiki/Cat_%28Unix%29#Useless_use_of_cat))... sed 's/[[:punct:]]//g' file | grep example 
If you swap around how you save things in your HASHTABLE variable and use the hash as the key instead of the word, then you can skip that expensive inner loop in your search: declare -A HASHTABLE while read -r -a DICT; do DICTHASH=$(echo "$DICT" | sha256sum | awk '{print $1}') HASHTABLE[$DICTHASH]="$DICT" done &lt; linux.words #count refers to number of lines in input file for ((ENTRIES=0; ENTRIES&lt;COUNT; ENTRIES++)); do ENTRY="${HASHTABLE[${HASH[$ENTRIES]}]" if [[ -n "$ENTRY" ]]; then echo "${NAME[$ENTRIES]}:$ENTRY" ((FOUND++)) fi done Another thing you can do is pre-calculate those sha256sum hashes and save a special words list file with `word:hash` lines in it. I'm guessing that having to start an sha256sum process for every single word in the file is really slow, so this should help a lot with speed. If you have that prepared file, you could read it into the HASHTABLE: declare -A HASHTABLE while IFS=':' read -r dict dicthash; do HASHTABLE[$dicthash]="$dict" done &lt; linux.words.sha256sum About preparing that special words file, this one-liner here should do it: while read -r word; do echo "$word:$(sha256sum &lt;&lt;&lt; "$word" | cut -d' ' -f1)"; done &lt; linux.words &gt; linux.words.sha256sum Here's the same with line-breaks for better reading: while read -r word; do echo "$word:$(sha256sum &lt;&lt;&lt; "$word" | cut -d' ' -f1)" done &lt; linux.words &gt; linux.words.sha256sum **EDIT:** Added the `declare -A HASHTABLE` lines.
Are those special characters only in certain spots? For example, "setup" vs. "set up"? A regex pattern that checks for that would be: set ?up The `?` makes the preceding space character optional, meaning the pattern will match for both of these here: setup set up Another thing you can do is, list alternatives using a `|`: grep -E 'setup|set up' ... If the changes you search for are more random, there's a tool named `agrep` that you might want to check out. If you run it like this: agrep -1 startup testfile It will find text with one change from your pattern, for example: startup start-up start up It will also trigger on typos: statup strtup startp start p It has a bunch of parameters to tweak how it counts the distance between a pattern and the text. This agrep command on my distro is in the package named `tre`: Name : tre Version : 0.8.0-4 Description : POSIX compliant regexp matching library. Includes agrep for approximate grepping. URL : http://laurikari.net/tre 
&gt; the command provided by /u/yhsvghnrOruGnpverzN seem to have identical output That's what I would expect. The biggest difference is that mine uses no non-POSIX features. If you know you're using GNU `find` then feel free to use `-printf ...`
Thanks for the clarification!
Hmm good point, I'll redirect it instead. Thanks!
Unfortunately the special characters are used inconsistently, so I don't think your first suggestion will work (although it's good to know that's out there). Never heard of agrep though, that might do it. Thanks!
 imageFileExts = ( "PNG" "png" "JPG" "jpg" "GIF" "gif" "BMP" "bmp" ) First problem: Remove the spaces from around the `=`. 
Okay, so if I run these two commands from the bash shell in WSL it lists the extensions, which is great. If I put it in to a .sh script I get unexpected ( on line 2. Line 2 being that I have the shebang at top. I've tried the script with and without the shebang and I get the same error. imageFileExts=( "PNG" "png" "JPG" "jpg" "GIF" "gif" "BMP" "bmp" ) for m in "${imageFileExts[@]}"; do echo ${m}; done I appreciate the help even if thats all the help you could provide. Thanks /u/yhsvghnrOruGnpverzN
OHHHHHH crap spackle So like forum post [sh is not Bash](http://mywiki.wooledge.org/BashGuide/CommandsAndArguments#Scripts) *Bash itself is a "sh-compatible" shell (meaning that it can run most 'sh' scripts and carries much of the same syntax) however, the opposite is not true; some features of Bash will break or cause unexpected behavior in sh.* from the Bash prompt in WSL you don't run it with sh, you run it with bash. Wow now I feel stupid. But that you, with out your help I'd still have the problem with my array being declared wrongly.
there's really no need to use arrays for this, i think it's simpler to just use imageFileExts="png PNG ..." for ext in $imageFileExts do echo "ext =&gt; ${ext}" ... done Maybe use `grep -i` i think so you don't have to list every possible way an extension might be capitalized? Something like imageFileExts="png jpe\?g gif" for file in * do for ext in imageFileExts do if echo $file | grep -qi "\.${ext}$" then echo "$file is an image" fi done for ext in someOtherExts do ... done done The problem might be that /bin/sh isn't bash or it's posix mode without array support, but that's a guess.
I could provide more, but I I don't know the first thing about "WSL". I do know Bash syntax though.
&gt; sh is not Bash Right, that form of assignment isn't available in [traditional sh](http://www.grymoire.com/Unix/Sh.html), or even POSIX I suppose. If you are writing for Bash, then call Bash. If you want portability, then [POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html).
Windows Subsystem for Linux which is basically Bash shell in Windows.
I like the idea of using grep to get the files then moving them with mv. I was able to get: mv -i *.{PNG,png,JPG,jpg,JPEG,jpeg,GIF,gif,BMP,bmp} ../Pictures/ but I didn't like that for PNG,png etc and if I used interactive I couldn't send errors to /dev/null while still getting a prompt. the only way I could do that was in a for each loop. Thanks!
I don't think you realize how long i've executed linux scripts with sh and blindly assuming they were being executed as bash commands because they always worked. That explains exactly why when I did a loop on the command line they sometimes wouldn't work if I copied them all to a script. Man, I want to go back to a job I had 10 years ago and explain this to someone. We sat and looked at this script for hours and I'm like "Damn it it should work!"
Just an FYI you need a $ in the for loops such as for ext in $imageFileExts I'm sure you knew that it's just in case anyone else gets here and can't figure out why nothing gets moved. It was good you did that actually since it made me understand each and every line. My final script is: imageFileExts="png jpe\?g bme\?p gif" documentFileExts="docx\? html\? txt pdf xps pptx\?" downloadsFileExts="zip exe msi" developmentFileExts="java class py ipynb ear json xml" for file in * do for ext in $imageFileExts do if echo $file | grep -qi "\.${ext}$" then mv -i "$file" ../Pictures/ fi done for ext in $documentFileExts do if echo $file | grep -qi "\.${ext}$" then mv -i "$file" ../Documents/ fi done for ext in $downloadsFileExts do if echo $file | grep -qi "\.${ext}$" then mv -i "$file" ../Downloads/ fi done for ext in $developmentFileExts do if echo $file | grep -qi "\.${ext}$" then mv -i "$file" /mnt/d/Development/ fi done done
There's some good answers over on [this stack overflow post](https://stackoverflow.com/questions/965053/extract-filename-and-extension-in-bash) about finding a filename in Bash script. The top answer includes a method to grab a filename without the path that can handle a file with "multiple extensions" like .tar.gz or etc. Personally I'm using "temporarily convert the filename to lowercase and test against the temp var with wild cards", like: `temp=$(echo "$filename" | tr '[:upper:]' '[:lower:]') if [[ "$temp" == *"extension"* ]]; then # do something fi`
file in `echo $file` is better quoted also, i forgot :/ Glad you got it working :)
Thanks. My google-fu must be getting weak, I swear I searched for my question. But thank you for linking it for me
No problem :P
Yes, but without more information such as the script itself and also a sample .torrent file it's hard for anyone to help without going and figuring it all out, and their method may not bode well with the script you've written so far and you'd either need to use their script or rewrite parts for yours. If you edit your post to include this info you'll probably get better results.
the mail part of the script is just one line with "echo "completed" | mail -s "[download completed] mymail@gmail.com", and I'm using Transmission's settings to call it when any download is complete, I'm sorry if I'm not clear, I struggle a bit with written English
maybe do a find ordered by creation time in your torrent download folder and add the first result in the email ? edit: something like that https://stackoverflow.com/a/4561987
No problem, that's understandable. I do have Transmission and I can give this a shot later, but [this page] (https://trac.transmissionbt.com/browser/trunk/extras/send-email-when-torrent-done.sh) may help you faster -- apparently Transmission provides the filename in that variable (or used to; may be deprecated).
The variable you want is $TR_TORRENT_NAME That will add the name of the completed torrent to the message you are sending out.
Sorry for double comment, and for my messed up comment below. Meant to say that it was the $TR_TORRENT_NAME var that you wanted, as kittykarlmarx did, but I was at work and screwed the message up =)) I got this working with a one-liner, but it's not working quite right -- I get 2 emails for every one download that finishes. Lmao. The script I'm using is: `#!/bin/bash mailx -s "$TR_TORRENT_NAME is finished." &lt; /dev/null "mymail@mail.com"` This sends a mail with only a subject line, no message in the body. Edits: formatting, removing my info =))
 find $(pwd) \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) -type f -print0 | while read -r -d "" line; do if [ -e "$line" ]; then mv -i "$line" ~/Desktop/folder1/ ; fi ; done I don't know how efficient this is.This is how i move my image files. 
The problem is that on Linux/BSD, extensions are fairly meaningless. You can name a text file with a .jpg extension if you wanted and it wouldn't matter. And yes I get that you want to run this on Windows where extensions mean something... So instead of looping over the files with statically defined extension types, you could generate a list of files using `find` and `file` as suggested [here](https://stackoverflow.com/questions/16758105/linux-find-list-all-graphic-image-files-with-find). Throw in a portable alternative to `-maxdepth`, then work your way through that list using `xargs`. And so you should get a one-liner like this: find . ! -name . -prune -type f -exec file {} \; | awk -F: '{if ($2 ~/image/) print $1}' | xargs -I '{}' cp '{}' /dest/path/'{}' Then simply adjust the `awk` command as required e.g. find . ! -name . -prune -type f -exec file {} \; | awk -F: '{if ($2 ~/MS Windows|Zip archive/) print $1}' In the `awk` example here, we're matching "MS Windows", as demonstrated from `file crsetup.exe`: crsetup.exe: PE32 executable (GUI) Intel 80386, for MS Windows OR "Zip archive", as demonstrated from `file 1082.zip` 1082.zip: Zip archive data, at least v1.0 to extract Our two tests are delimited by a pipe character, so essentially `awk -F: '{if ($2 ~/MS Windows|Zip archive/) print $1}'` translates into "If the second column of `file`'s output includes (~) 'MS Windows' or 'Zip archive', then print the first column". PE32 may be the better match for the first type, I'll leave that as an exercise for you. 
thank you dude, you're the best
No problem -- I hope it works for your project. And thank you for teaching me Transmission was capable of this; I had no idea it could run a script when a download finishes, lol.
I would suggest to compare output of this: echo $SHELL shopt set | grep ^HIST You can find a different version of bash, or even different shells or some flag/envvar/setting enabled...
I know for a fact they are all bash. But is there a setting in particular that you know of? For what it's worth, I've been through the documentation on `shopt` and never found anything promising. And my settings with `HIST` are benign. Thanks!
Ahh, this whole question may be moot. See my edit
I'm getting the same error. Anyone have any other ideas?
Why do you want to read the redis URL from standard input? Isn't it more convenient to pass it as an argument to your script?
Because the URL is the stdout from the heroku command, which reads the URL from the server environment variables.
In your node command, pass `-i` argument. This will force the terminal to be interactive, instead of running in a subshell. I would create a bash alias or function of some sort, to achieve this - usually using cut or awk. 
First, I'm going to explain what's wrong with your existing attempts, then I'm going to give a solution that (kinda) works how you want, then I'm going to propose a solution that works a little different, but I think is better. (Everything that isn't bash syntax is pseudo-code, not real C or anything) # What's wrong When running the command echo "redis-cli" | source /dev/stdin # doesn't work Bash creates a process for the stuff on the left of the `|`, and a process for the stuff on the right; and creates a pipe that is stdout for the left side, and stdin for the right side. With that in mind, let's create a timeline of what each process is doing, and what's currently in the pipe's buffer. | process 1 | pipe contents | process 2 | |-------------------------------------+------------------+-------------------------------------| | bash1: write(stdout, "redis-cli\n") | "redis-cli\n" | bash2: read(stdin) # (blocking) | | bash1: close(stdout) | "redis-cli\n"EOF | bash2: read(stdin) # (blocking) | | bash1: exit(0) | "redis-cli\n"EOF | bash2: read(stdin) # (blocking) | | | EOF | bash2: read(stdin) =&gt; "redis-cli\n" | | | EOF | bash2: exec("redis-cli") | | | EOF | redis-cli: read(stdin) =&gt; EOF | | | EOF | redis-cli: exit(0) | (technically, bash2 forks before running `exec()`, which means there are actually 3 processes involved, but since `redis-cli` is the last command that will be run before bash2 exits, we can simplify for pedagogical perposes) So, fundamentally, the problem is that we need the stdin of the right-side to be re-connected to the terminal after the left-side finishes. We could kind-of do that like { echo "redis-cli"; cat; } | source /dev/stdin # issues, see below Which will forward the further input from the terminal to the right-side. That is fine for scripts, but when redis-cli calls `isatty(stdin)`, it will see that it's not a TTY, and interactive line editing won't work. # Making it work for you Now, you said &gt; My goal is to build a command that takes the url from the stdin and &gt; executes the redis-cli, so with the Heroku cli I can do: &gt; &gt; heroku config:get REDIS_URL | redis-connect Notice that in the above solution, the fanciness happens on the left-side of the pipe, but the `redis-connect` command that you are trying to build is on the right-side. There isn't really a robust solution that allows that. Just about the best we can do is to assume that the desired stdin of `redis-cli` is the TTY (which means it won't work for scripting): redis-connect() { $(redis-url-to-cmd) &lt;/dev/tty } # A better solution To repeat /u/McDutchie's question: &gt; Why do you want to read the redis URL from standard input? Isn't it &gt; more convenient to pass it as an argument to your script? That is, why not change it to work like this? redis-connect "$(heroku config:get REDIS_URL)" The implementation of this is pretty simple: redis-connect() { $(redis-url-to-cmd &lt;&lt;&lt;"$1") } 
Have you considered a simple while loop? $ cat my_connect_script #!/bin/sh # Usage: heroku config:get REDIS_URL | my_connect_script while IFS='/:@' read proto _ _ user pass host port do # Change process: exec redis-connect -h "$host" -p "$port" --a "$pass" done 
What about while read line &lt;&amp;"$fd" ; do # process "$line" read -p "Continue? " ans [[ "$ans" != yes ]] &amp;&amp; exit 1 done {fd}&lt; file If I don't manipulate file descriptors the inner `read` will "swallow" every second line of input. How can it be rewritten?
OK very interesting example. You have an interactive prompt inside a loop reading lines of a file. As I mentioned in the other comments thread [1], Oil will provide a way to do everything that bash does -- it's just a matter of how "pretty" the syntax is. The goal is to convert from OSH to Oil automatically [2], which places some constraints on the syntax. So you can likely transliterate this rare example, but I want to provide a nicer syntax for common cases. [1] https://www.reddit.com/r/oilshell/comments/6tch5v/avoid_directly_manipulating_file_descriptors_in/ [2] http://www.oilshell.org/blog/2017/02/05.html 
A lot of tools such as `cat`, `sed`, `awk`, `cut`, `grep` etc. have the following behaviour: if given a filename as the argument, they operate on that file; if piped to, they operate on the stream. So, these two would behave the same: cut -f 1 file.txt and cut -f 1 &lt; file.txt or even (useless use of `cat`, though) cat file.txt | cut -f 1 If that's the behaviour you want, your script most likely contains a `while read` loop, but now you have to figure out if you're reading from a file or standard input. One way to do that is checking if `$1` is defined, and if not, read from `/dev/stdin`, so the loop would look like this: while IFS= read -r line; do # Operate on line here done &lt; "${1:-/dev/stdin}" The `${1:-/dev/stdin}` construct is a parameter expansion that says "if `$1` is unset or null, then use `/dev/stdin` instead". Now you can pipe to this script or call it with file arguments and get the same behaviour.
Not sure if you addressed it specifically, but another thing that can be useful is cloning the "top-level" file descriptors for use with functions and sub-shells later on. For example, suppose you have a function like this: myfunc() { echo &gt;&amp;2 'Message that I care about' # Pretend this is some utility like `mke2fs` which prints messages to stderr # that aren't always relevant echo &gt;&amp;2 "Message that I don't care about" } Now if you do `myfunc 2&gt; /dev/null` you lose *all* of the output that was on stderr. But what if you do this instead? myfunc() { echo &gt;&amp;$REAL_STDOUT 'Message that I care about' echo &gt;&amp;2 "Message that I don't care about" } exec {REAL_STDOUT}&gt;&amp;2 myfunc 2&gt; /dev/null Now you've distinguished between the *function's* error output and the *script's* error output. I don't do this often but it's useful for error-handling functions, for example, where I always want an error message to "poke through" to the user no matter what's going on with the function's own stderr.
Hm I'm not sure I see why you wouldn't do it this way: f() { echo stdout echo stderr &gt;&amp;2 mke2fs &gt;&gt; ignore.log } f 2&gt; stderr.txt That is, if a given program is spouting irrelevant stuff, you can just put that in a separate file? Then you have stdout and stderr as normal. I put some other examples of hard-coded descriptors here, from other replies: https://github.com/oilshell/oil/blob/master/spec/hard-coded-descriptors.sh In any case you don't seem to be using anything higher than 2, and I think 2 is fine. 
&gt; That is, if a given program is spouting irrelevant stuff, you can just put that in a separate file? You can, if your function is only used in one context for one purpose and you never need it to behave differently -- and if you don't have a lot of them. But sometimes you have a function that's more general-purpose where you want to be able to selectively silence the more "informational" messages that are normally printed on stderr. I've been working on a toy shell library recently that makes use of this to support panic/abort messages. Here's a contrived illustration: die() { printf &gt;&amp;2 'Error: %s\n' "${*}" exit 1 } func() { (( $# == 2 )) || die 'Bad argument count' # Do stuff } func foo 2&gt; /dev/null Here someone has redirected `func`'s stderr, which silences `die` -- not helpful. But if `die` instead prints to a clone of the script's "global" stderr, it survives that redirection, allowing you to keep the functionality of your error-handler without affecting the usual redirection behavior that people are used to. &gt;In any case you don't seem to be using anything higher than 2, and I think 2 is fine. Not explicitly, no
Thanks. This is very helpful to understand what I'm doing wrong. However it still doesn't solve the problem. The solution you propose: redis-connect "$(heroku config:get REDIS_URL)" is equivalent to heroku config:get REDIS_URL | redis-url-to-cmd I have no trouble with this. The problem is evaluating the output of either command as a command itself. Essentially, running either will output `redis-cli -h HOST -p PORT --a PASS` and that's the command that I want executed. I appreciate your help, but I'm not sure if I'm missing something. Essentially you're telling me that I can't evaluate a string as a command if the process is persistent?
Not quite. redis-connect "$(heroku config:get REDIS_URL)" this "expands" to $(redis-url-to-cmd &lt;&lt;&lt;"$(heroku config:get REDIS_URL)") which is equivalent to $(heroku config:get REDIS_URL | redis-url-to-cmd) which is *not quite* what you asserted that it is equivalent to; note that it is wrapped in `$(...)`, which will evaluate the string as a command.
Try date &gt;&gt; /opt/myfile.txt
What do you mean, "does not append the information correctly"? What did it do, and what were you expecting?
what are the error messages?
There aren't any. echo $? returns 0.
Setting paths for binaries is useful in a larger script. In one like this, it is gilding the lily. If you want to control which binary is used without calling the absolute path, try setting `$PATH` explicitly. ALL CAPS VARIABLES ARE ANNOYING. If you're going to use bash (`#!/bin/bash`) use bashisms, instead of shellisms. (`[[]]` versus `[]`, `$()` versus backticks). There's no need to export your variables, and the proper way to export them is `export $variable` not `export` without any arguments.
after some review this is what i see: you have a user "Demo" that when logged into the Demo user, it will activate the script to go into a tmux session. it will check if there are any unattached sessions, and if there are unattached sessions found, the shell will attach to the first unattached session found. If not, it will open a new tmux session with a bash shell. Is this your intention? some things i've noticed: Your code is in a nested if statement checking for $TMUX, which is only set if tmux is invoked and the shell is currently in a tmux subshell. If this script is supposed to execute every time the "Demo" user logs in, it will be in it's own login shell and the "if [-z $TMUX]" statement will not execute, which will return an exit code of 0. Why not just set the login shell to "/usr/bin/tmux", or put "/usr/bin/tmux" in the .bash_profile? i think this nested session is your main problem - the script never checks for unattached sessions because it will only work if it is executed within an already existing tmux session
 #!/bin/bash tmux_socket_dir=/tmp/tops-tmux user=$USER if [[ -z "$user" ]]; then user=$LOGNAME fi # Find the first unattached session's "id" session=$(tmux list-sessions | grep -v -m 1 attached | cut -d: -f1) if [[ -n "$session" ]]; then tmux -S $tmux_socket_dir/$user at -t "$session" else tmux -S $tmux_socket_dir/$user new /bin/bash fi I would guess your tmux commands might not work so well, or you might need to tweak your `~/.tmux.conf`. `tmux new-session` should be all you need to create a new session.
login scripts aren't specifically confined to the desktop environment, you can also add the script into the user's .bash_profile and it will execute upon the user's login. i don't use kubuntu but this would be the flow of startup commands: init system is started by systemd (in a nutshell, there are many details to how a system is initialized). your distribution will have a systemd "target" to start the X11 server as well (known as "multi-user.target", this is run level 5 for standard init systems) "display manager" is started up (this is your graphical login screen) you login as a user - which will source first /etc/environment and then the login profiles (this would be .bashrc/.bash_profile for bash) TL;DR - put "/path/to/script" in the user's ~/.bash_profile and it will execute upon login. edit: format
is this to parse out user input? id suggest looking into regular expressions/regex (although it is quite a handful to learn at first, it becomes VERY useful) in regex the '.' means "any single character/digit" and the '?' character means (occurs 1 or 0 times) so you can use "grep -e" or "egrep" for a regex search: egrep 'e.?x.?a.?m.?p.?l.?.e.?' i'm not a regex super expert so there are most likely way more better ways to parse for strings, but this might be more helpful than executing a bunch of other commands. [this tutorial](http://www.regular-expressions.info/tutorial.html) will help you get started
Variables for paths is sort of a habit now. Makes dealing with the deb/rpm differences easier. I'll port the sh to bash stuff. Export is there so I can see variables. It will be removed when it is working again.
No user has a ~/.tmux.conf /etc/tmux just has just this: set -g status off I'm running an app, not /bin/bash. I just subbed bash as it exhibits the same symptoms. Using your code &gt;tmux -S $tmux_socket_dir/$user new /usr/local/bin/app doesn't work &gt;tmux -S $tmux_socket_dir/$user new "/usr/local/bin/app -v" does Note that -v does nothing in my app. sub /bin/bash for /usr/local/bin/app and you get the same results.
`sed -i.ORIG -e "\$a$(date)" /opt/myfile.txt` 
`$( )` captures the *output* of the command (what it sends to stdout), not its exit status. Since you explicitly redirect the output, there is nothing to capture. If you want to test the exit status of the command, use the command directly in the `if`: if ! git rev-parse --verify HEAD~ &gt; /dev/null 2&gt;&amp;1 echo 'This is not...' fi That will also prevent `-e` from terminating the shell, as you want: &gt; The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any command executed in a &amp;&amp; or || list except the command following the final &amp;&amp; or ||, any command in a pipeline but the last, or if the command's return value is being inverted with ! You could also use `||` (short-circuiting logical or): git rev-parse --verify HEAD~ &gt; /dev/null 2&gt;&amp;1 || echo 'This is not...'
I see. Thank you. Is there a way to make it more verbose for better readability? I mean, can I save the exit status in a variable and then have the if condition for that variable? From your reply it seems I cannot do it.
I don't think there is with `set -e`, other than temporarily doing `set +e` beforehand. But if you do that, you can use the `$?` variable, which holds the exit status of the last command. Just be sure to save it before doing any other commands: set +e git rev-parse --verify HEAD~ &gt; /dev/null 2&gt;&amp;1 status=$? set -e if [ "$status" -eq 129 ]; then ...; fi
Very nice, didn't know about `set +e`. Thank you.
I'm pretty noob so it's highly likely I'm wrong and I suggest waiting for someone smarter to come along (unless of course that's you) and say something about this before trying it, but [trap](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html) may help you. I just found it last night when trying to fix a build script of mine and I'm using it like this (along with set -e at the top of the script) based on their script: function error_handler() { # Handle the error here } # If an error occurs, run the error handler trap 'error_handler' ERR # ...rest of script (but with function definitions up above the error_handler part) Again, though; best to wait for someone who's not a noob like me :P Lol. Edit: trying to format properly =\ Another edit: oh, thanks, I love downvotes without reasoning random hidden stranger! For clarity I looked into trap enough that I'm comfortable with it myself, I just don't know if it's considered a bad idea or etc, that's why I was hoping someone would comment.
Ignoring the fact that adafruits script seems to just run an infinite loop without anything to prevent it from pegging the cpu.... Isn't that delay call going to make reading button presses problematic?
The `||` [list operator](https://www.gnu.org/software/bash/manual/bashref.html#Lists) can be used to avoid failing a script due to `set -e` when a command has a non-zero exit code. The `true` command, which always exits with zero, is often used for this. test.sh: set -euo pipefail false || echo "non-zero exit code" false || true echo done As [stated in the manual](https://www.gnu.org/software/bash/manual/bashref.html#The-Set-Builtin) and manpage, `set -e` has a loose definition of a pipeline: &gt; Exit immediately if a pipeline (see Pipelines), which may consist of a single simple command (see Simple Commands), a list (see Lists), or a compound command (see Compound Commands) returns a non-zero status. 
Would I need to change the currenttime variable to another variable which stores the time that is first set under the loop where the pcount variable is first increased to 1?
I don't know what the delay function is doing, but if it's causing a longish delay (as in 100s of milliseconds) and not checking for button presses, then you'll miss inputs which will cause erratic behavior. Most micros get around this by attaching an interrupt to the button so that the input registers immediately regardless of what the code is doing. The adafruit code doesn't appear to have this issue because they are just looping as fast as possible. That works but it's a terrible hack and isn't extendable to longer delays.
Do you don't believe I can press a button to start the timer and then 30 second later have it do a longer delay based upon how many times that button was press during the. 30 second?
You have two additional while loops after you read your button presses. While those loops are running, you're not reading button presses. Imagine those loops take 3 minutes to complete just for illustrative purposes. If for your use case, that makes sense, then that's fine. It's just odd to see button presses being read via a loop and then the logic occurring in another loop. This usually results in buttons not being read while the logic is executing.
I'm guessing that maybe there are too many files and thus you get too many arguments for your `rm`call? Although typically that'd say "Argument list too long", but who knows, maybe the error message was different back in Debian 3.1 days. You could try piping your find call to `xargs` instead of using `-exec`: ``` find "$backup" -type f -mtime +7 | xargs -0 rm ``` *Caution*: Make extra sure that $backup always has a valid and expected value. Just imagine what happens if the value gets set to "/backup /" (yes, there's a space in there). You should probably quote the var as in my example above as a first step to prevent disasters.
The error message comes from find... I did the exact same command in /etc/ and it didn't show an error so the number of files is not an issue... But i didn't issue the -exec rm command so maybe it does come from there. I'll try your piping solution and let you know. One lead I had was the system being 32 bits can't handle large files. I don't know how to test that. Or maybe it comes from the fact that /mnt/mybackup is in fact a Windows NFS Shared Folder from a distant computer. The value is initialized first line line of my script and is never changed. Although you are right, I should sanitize it. 
http://www.gnu.org/software/coreutils/faq/coreutils-faq.html#Value-too-large-for-defined-data-type That link would seem to support the large file issue. You could recompile find maybe? I'd say the real solution is upgrading. Debian 3.1 stopped getting security updates almost ten years ago.
&gt; write a one-liner about your credentials as Ruby/Rails expert. Thought you were testing Linux/Bash?
&gt; That link would seem to support the large file issue So maybe ditch the -type f option so that it can work with "large files" ? &gt; I'd say the real solution is upgrading. I know. Apparently my boss is trying to get the client to upgrade. (this was not in the contract because nobody knew it was this old apparently...)
Debian should have GNU find which had a -delete Additionally with exec you can do: -exec rm {} + That with call rm with multiple files at a time vs once per file. Neither -exec options will trigger the alluded error about too many arguments.
Use -delete argument for find.
You can toss bash snippets/pastebin links to #bash on IRC Freenode and get plenty of solid feedback there.
tmpwatch can do this for you. Unless you must use find for some reason, tmpwatch should suffice.
tmpwatch can do this for you. Unless you must use find for some reason, tmpwatch should suffice.
tmpwatch can do this for you. Unless you must use find for some reason, tmpwatch should suffice.
&gt; So maybe ditch the -type f option so that it can work with "large files" ? Well... you can try that, just without the `-exec` to see if you can get an output. I think you could possibly investigate wrapping `rsync` up into an attempted solution...
I'm hoping part of the issue is I didn't communicate well enough what each part of the code is suppose to be doing. I've add more comments to communicate what I want each part of the code is doing. Its unclear to me if need to reorder the current loops or if I need to nest them in a different fashion/order? I think I hear what your saying the potentiality the order and duration of the loops might not allow me to read the button presses in the timely fashion. But I don't feel like I'm being lead to a solution either. Do I need to combine the last two loops, where the second loop should really be a if statement i.e. while do timecheck = $(($(date +%s)-currenttime)) if [ $timecheck -ge 30] then timedelay = pcount * 1800000 pcount = 0 delay($timedelay) fi done Could you please throw me a bit of a bone as I'd like to test this code a few times before I have to leave for my trip on 8/22?
Hard to say. I don't know what your button use case is. If you have issues with button detection you can try reading it inside the loops where the code spends the most time. Basically you want to check for button presses as much as possible and store the state. Then act on that state wherever it makes since in the code.
Would just giving a description of my use case be helpful or do you need more details than that?
/r/perl might be more appropriate. The `bash` way to do this would usually be to use `sed` to remove text between two strings (google search hint). This would involve some regex, which you should find translates directly to `perl -pe`
For now I say test it and see, then if it doesn't work make changes. Just be aware that if you have button issues, your problem is probably because the loop is taking to long before the button value is read again.
The most correct way to detect button presses is events. Here's a library that appears to support them, but this might be more than you're prepared to get into right now: https://sourceforge.net/p/raspberry-gpio-python/wiki/Inputs/
It seems that, after I made copy/paste, I forgot to change that last part. It should be: write a one-liner about your credentials as Linux/Bash expert. Luckily, enough people understood the meaning of the post :-)
Okay cool, after a bit of research with your hint, I found an example similar to this - which deletes the match including the search strings: sed -i '/\[$1\]/,/#Do Not Delete This Comment!/d' /etc/samba/smb.conf Only problem is that I'm not sure how to delete the line above as well. I tried !d but that deletes everything but the match. 
The *correct* way to go about this is properly parsing the config file (Samba uses Windows `.ini` format), removing/adding shares from the structured data and then outputing the modified configuration file. There's a [bunch](http://search.cpan.org/search?query=ini&amp;mode=all) of CPAN modules to parse `.ini` files already, so don't reinvent the wheel and use that. 
Well, that is the most godawful, convoluted way I've ever seen of checking whether the output of a command is empty. It's also highly brittle (the `[` command will cause a syntax error if there is more than one line; the expansions are subject to globbing). The `grep ""` does nothing at all and might as well not have been there. Main thing is, though, that you don't need any array for this (or almost any other shell script; arrays are overkill at best in 99% of cases). Try this: #!/bin/bash PERL=$(perl -0777 -ne "/(?&lt;=\[$1\]).*?(?=#Do Not Delete This Comment!)/s;print $&amp;;" /etc/samba/smb.conf) if [[ -n $PERL ]]; then echo "Exists" else echo "Does not exist" exit 1 fi printf '%s\n' "[$1]" "$PERL" "#Do Not Delete This Comment!" Note use of `[[` instead of `[` to avoid pitfalls with quoting, field splitting and globbing. Also, of course /u/pergnib is right and you should use a proper .ini parser instead for the perl bit. And you might as well do the entire thing in perl then. *[edit: rm syntax error]*
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Add `sleep 3` or something similar in the loop so that daemon gets a change to do its thing. Also your sed is off. Try: &gt; sed -i 's#/root/pagent.bsh##g' /etc/rc.local
&gt; Only problem is that I'm not sure how to delete the line above as well. sed isn’t really the right tool for that task – it works on each input line individually, so by the time you see the line that matches `\[$1\]`, the previous line has already been processed and printed, and you can’t un-print it again. (It’s probably *possible* to achieve this… I think you can abuse the hold space to effectively print each line one line later. If you want to go this route, read the sed manual (`info sed`), and specifically look at the `h`/`g`/`x` commands. But I wouldn’t recommend it.)
Removed as spam. You haven’t posted anything except “reviewing of [various topics] questions” to Reddit in the past four months (see first bullet of [What constitutes spam?](https://www.reddithelp.com/en/categories/rules-reporting/account-and-community-restrictions/what-constitutes-spam-am-i-spammer)), and you already posted almost the exact same post [four months ago](https://www.reddit.com/r/bash/comments/63zbv6/reviewing_of_linux_and_bash_questions/) to this subreddit.
What do you mean you "can't change them on the script?" * Exactly what did you try? * What was your expected result? * What happened instead?
&gt; Exactly what did you try? &gt;&gt;shopt histappend histappend: off &gt;&gt;shopt -s histappend #set off to on &gt; What was your expected result? &gt;&gt;shopt histappend histappend: on &gt; What happened instead? &gt;&gt;shopt histappend histappend: off 
Are you running those interactively? Are they in a file? How are you running the file?
In a file. I have a bootstrap.sh script that loops the directory and for each dotfile found it sources them. One of these files has the shopt command
How to you execute bootstrap.sh? Where are you checking the results of it? $ shopt histappend histappend off $ cat test.sh shopt histappend shopt -s histappend shopt histappend $ . test.sh histappend off histappend on $ shopt histappend histappend on $ 
I figured out what I wasn't doing. when on a new tab it would try to find .bash_profile in my home directory. as there was none histappend would be off. Now I rsync'ed the dotfiles to home and it worked. Thanks 
`ls -R` may also work if tree is not installed.
 sed -E 's/^ip-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+).*/\1.\2.\3.\4/' ip_addresses.txt
Worked like a charm. Way more efficient and safer than what I was doing. Thank you!
Why did you not have to use /g after the substitution? My understanding was that if you don't use /g then only the first instance is replaced. 
First instance per line. /g modifier is redundant since my regexp always tries matching the whole line.
Aaah yes that makes sense. Thank you. 
I never intended to post a spam. We are collecting questions for different skills (one of which is Bash) and when we collect some decent set we usually need several experts for that area who can help us with checking them. Reddit has proven to be an excellent channel for finding such experts. We have always found what we needed and Reddit users have really proven to be experts. And in recent conversations, I got the impression that the pleasure was both side. Although we usually offer some small gift, the most of these experts are doing this more because they want to discuss with us different aspects of our questions, than because of the gift. At the end, we release these questions to broader audience, so I think the whole community has some use of this. I admit that my posts are very similar, and that I use reddit just for this purpose. Is that an issue? What's bother you the most in our posts? If that is the mentioning of TestDome.com then we can probably remove that part. Note that till now we didn't receive any negative comments on our posts (except this reporting as spam) and that both, our experts and we, are very content with the cooperation we achieved because of Reddit and this channel. 
Thanks!
So, you can do a fancy one liner, but in my experiance I like to take things one step at a time. This is how I would do it. I personally love awk. I know your request said sed, but I would use awk for the first step. Basically the -F- cuts each line in the file delimited by whatever -F specifies, here the '-' in your list. Now I just need to print each field. Awk allows me to easily put things between the fields by placing "." between the $2 $3 $4 $5. Then I get the ip addresses wanted in the correct format but with '.ec2.internal' appended to the end. That's a great job for sed, as I directly want to replace that line with nothing. Once that is ran, I see in the console exactly what I want in the correct format. At that point I can use &gt;&gt; to redirect the output into a new file. This method is one that I would teach someone, as it gives you plenty of chances to see if you are making a mistake well before accidently overwriting the file with sed. Once the output looks like you want, writing to a new file allows you to keep the old one just in case. awk -F- '{print $2"."$3"."$4"."$5}' ip_addresses.txt | sed -e 's/.ec2.internal//g' &gt;&gt; formatted_ip_addresses.txt
`sed -E 's/^[^0-9]*([0-9-]+)\..*$/\1/;s/-/./g' ip_addresses.txt` edit: Was missing a `.` in `\..*`.
Question has been resolved but, FYI, this doesn't work. Gives the following result: I.do.not.want.these.hyphens.to.be.replaced Only.the.hyphens.in.the.IP.addresses.should.be.replaced ip.172.30.15.61.ec2.internal ip.10.250.30.72.ec2.internal ip.192.168.124.130.ec2.internal 
Fixed, was missing a `.` from the hasty edit I made earlier to shorten it. Also missed the 'do not want hyphens replaced' text. ... which makes this whole thing seem a bit like homework.
`cut -d , -f 4- logfile.txt` date_fmt="%Y %b %d %T" while read -r line; do trimmed=$(echo "$line" | cut -f7- -d,) timestamp=$(echo "$line" | cut -f4 -d,) date=$(date -d @"$timestamp" +"$date_fmt") printf "%s,%s\n" "$date" "$trimmed" done &lt; logfile.txt &gt; logfile2.txt as a 'complete' example (untested).
So basically &gt; for d in $(cat logfile.txt |cut -d , -f 4); do date "+%Y %b %d %T" --date=@$d; done ?
I think you missed: &gt; , and then remove the 3 columns after the time. 
Updated. :) edit: I think you meant '2' "columns" (the blanks) after the timestamp.
Naa not homework. I put that there to make sure my command doesn't over reach. 
No worries. I'd go with a more complete solution in that case. I was just playing golf.
I get what you're saying. As far as the solution you provided, it's not right for the job. Your solution edits other parts of the file that should not be touched. For example, have a look at the output of running your command: do.not.want.these the.hyphens.in.the ... 172.30.15.61 10.250.30.72 192.168.124.130 Sure, it got my IP addresses to come out right, but it would have destroyed a lot of other things in the file. But I have to agree with you that awk is an awesome tool. I love it as well.
Not an expert,just a novice.This is an awk only solution,running awk twice,will mess things up if the data file contains strings like "ip-address". cat ipaddress.txt | awk '{if($0~/^(ip-)/) {gsub("ip-","");split($0,a,".");print a[1]} else print}'|awk '{if($0~/^([0-9])/) {gsub("-",".");print} else print}' 
Yeah, okay... I should have re-read your post. You could have added a constraint in awk saying if $1 is equal to ip, then print.
Yeah, you can do a one liner in one single awk, but in my experience that's more difficult than just piping it into another awk or another command. I always go after the more simple solution, especially when teaching people how to do things!
&gt; I always go after the more simple solution Me too.I love awk . 
Might as well use read to split the lines. Much more efficient. while IFS=, read -r _ _ _ timestamp _ _ trimmed; do printf '%(%Y %b %d %T)T,%s\n' "$timestamp" "$trimmed" done &lt; logfile.txt &gt; logfile2.txt
Maybe this will help you. here's a bit that I wrote that runs on a Pi application server that checks timestamp difference of current time on the Pi against the time stamp of a web page index.html file downloaded using wget. If the difference is &gt; 655 seconds, it reboots the Pi. Note the use of ls -l --time-style to return the date/time in epoch seconds. #!/bin/bash set -x cd /home/pi/shbin rm index.html wget http://example.com/index.html ls -l --time-style=+%s index.html | awk '{ the_diff = systime() - $6 print the_diff if ( the_diff &gt; 655 ) { print the_diff &gt;&gt;"diff_gonna" system("date &gt;&gt;diff_gonna") system("sudo reboot") } else { print the_diff &gt;&gt;"diff_notgonna" system("date &gt;&gt;diff_notgonna") } }' exit 
You can use `inotifywait` to wait for filesystem events, so something like the code bellow should work: # wait 10 seconds for a modify on $SYNCED_FILE and launch Firefox if inotifywait -e modify "$SYNCED_FILE" -t 10 then firefox else chromium fi Alternatively, don't reinvent the wheel and use Firefox's own sync tool.
This syntax is for temporarily (in the following command) changing an environment variable. So yes, it works, but effectively you get a parameter via env variable instead of by command argument. Env variables have more room for error.. but I can't seriously say they are not often used in bash. Most important is to keep an eye out that they can be changed pretty much anywhere before the line that called this function. If you do go with env variable better name it something more unique than "OUT"
Check out http://cmdchallenge.com
Thank you very much :) Challenge accepted. But it looks more like testing yourself and not like a tutorial that teaches me new stuff, am I wrong?
Oh, but you do have google, right? :) Challenge itself even includes the correct answers, but I would suggest not checking them, but rather trying to google for hints. This *will* allow you to learn new concepts.
xargs doesn't group output from different processes either, as stated in the GNU manpage under the description for the `-P` option. GNU has a different program, `parallel`, that can do this conveniently: printf "dog\ncow\npig\n" | parallel --group "curl -Ls http://wikipedia.com/wiki/{} | grep animal" The `--group` option buffers output so each job's lines are grouped together. Alternatively, you could capture the output of the background jobs by redirecting it to files in a tempdir or using bash coprocesses; see this Stackoverflow question: [Bash: Capture output of command run in background](https://stackoverflow.com/questions/20017805/bash-capture-output-of-command-run-in-background).
Yes, I didn’t want to question the learningeffect by solving problems. I was just thinking of getting teached something. But I get your point that trying to solve riddles could provide an even better learningeffect. And the problem with google is, that you get tons of „Tutorials“, but most of them cost money. And as I don’t have any clue of which one is good (cause they all look very professional) it would be a loss of money to just buy a course of the first tutorial that is offered me. Also: I would like to get a free one as I am still student and don’t have that much money. Actually that’s the reason why I ask, as I think there are more people that already know good ones :)
sidebar, or these: - [BashGuide](http://mywiki.wooledge.org/BashGuide), [BashFAQ](http://mywiki.wooledge.org/BashFAQ), [BashPitfalls](http://mywiki.wooledge.org/BashPitfalls), and [BashSheet Reference](http://mywiki.wooledge.org/BashSheet) [GreyCat's wiki](http://mywiki.wooledge.org/) - [The Bash Guide (by lhunath)](http://guide.bash.academy/) - [Bourne Shell Tutorial (Bruce Barnett)](http://grymoire.com/Unix/Sh.html) - [Bash Shell Scripting Wikibook](https://en.wikibooks.org/wiki/Bash_Shell_Scripting) - [bash-handbook repository (denysdovhan)](https://github.com/denysdovhan/bash-handbook) - [Shell Scripting Tutorial (shellscript.sh)](https://www.shellscript.sh/) - [GNU Bash Reference Manual](https://www.gnu.org/software/bash/manual/html_node/index.html) - [POSIX 1003.1-2008 Shell &amp; Utilities](http://pubs.opengroup.org/onlinepubs/9699919799/idx/xcu.html) 
After you have mastered the basics, learn to read manpages `hint: man` and you are good to go. I've never taken any course on bash even though I have written localized tutorials and given courses on it.
If I'm understanding what I've just read, the difference between coproc and &amp; is that the former has a two way pipe and the latter only pipes its STDOUT back to the parent shell (unless redirected). Am I missing anything else on that point? Regarding parallel, I did see it mentioned on SO a couple times, but I'm trying to stick with tools that are guaranteed to be on any linux box I might ever ssh to (same reason I don't use emacs :p ). 
Yeah, that's how I've understood it, too, except that for the background job case (`&amp;`) the subshell just inherits the parent's file descriptors and no [pipes](https://en.wikipedia.org/wiki/Pipeline_%28Unix%29#Implementation) need to get setup. From the manpage: &gt; If a command is followed by a &amp; and job control is not active, the default standard input for the command is the empty file /dev/null. Otherwise, the invoked command inherits the file descriptors of the calling shell as modified by redirections. But since only the foreground process group of a session can read from the tty, if the subshell does try to read stdin it'll just get suspended by the kernel with `SIGTTIN`. From [wikipedia](https://en.wikipedia.org/wiki/SIGTTIN): &gt; A background process that attempts to read from or write to its controlling terminal is sent a SIGTTIN (for input) or SIGTTOU (for output) signal. These signals stop the process by default, but they may also be handled in other ways. Shells often override the default stop action of SIGTTOU so that background processes deliver their output to the controlling terminal by default. Edit: added a link about how pipes are implemented.
This one is very good as well. You have to progress through levels by obtaining the password for the next one to enter. The passwords are stored in files that you must find and must use your existing knowledge to obtain it. As you progress it gets more interesting. http://overthewire.org/wargames/bandit/
Well, screen brightness is usually represented as node (the same one that allows you to change the brightness), so you can read it from there and use a built-in look-up table. If you want to save a value, I'd suggest a file. value="$(cat /foo/bar)" echo -n "$value" &gt; /foo/bar Edit: typos
You could try doing a google search such as this one: [Google search results](https://encrypted.google.com/search?newwindow=1&amp;hl=en&amp;q=filetype%3Apdf+%22index+of+ftp%3A%2F%2F%22+%22shell+scripting%22&amp;oq=filetype%3Apdf+%22index+of+ftp%3A%2F%2F%22+%22shell+scripting%22&amp;gs_l=psy-ab.3...13646.20769.0.20982.27.27.0.0.0.0.157.2596.11j14.25.0....0...1.1.64.psy-ab..2.0.0.hOFRDQBbKJ0).
if the ip you want to get strictly follows after the pattern "ip": awk -F'[.-]' '/^ip/{print $2"."$3"."$4"."$5;next}1' ip_address.txt
Write the value to a file?
Thanks, saving to a file seems like the best option, solves the issue of keeping the value through restarts too. Also, the reason I'm not just using a lookup table is because the max brightness values are really arbitrary and I'm trying to make a script that's as reusable as possible. My laptop screen's max brightness is exactly 852 for example.
Nice. :D
Or you could just install the tools for the job.
You have to keep track of what IFS contains, since the unquoted parameter expansions will be split into words based on IFS before pathname expansion replaces resulting words with matching filenames. I would've rather written it like: create_spritesheet() { montage "$@" } create_spritesheet guy_frame0*.png guy_sheet.png Or rather, I'd just run montage directly. I don't see the point in having a function that makes the command name longer.
I was disappointed with codecademy's "Learn the command line" tutorial. They end up teaching you anti-patterns like `ls | grep ...` which is just so unnecessary. Anyway, read the [BashGuide](http://mywiki.wooledge.org/BashGuide) and other pages on that wiki.
I looked into using coprocesses but since the pipes they create are closed when the coprocess is finished whether they've been read or not they're pretty awkward to use, requiring a bunch of redirection using file descriptor numbers. Also, bash prints a warning if more than one coprocess is active at a time. Using a temp dir is straightforward, flexible, and readable: #!/bin/bash set -euo pipefail tmpdir=$(mktemp -d) function cleanup { rm -rf "$tmpdir" } trap cleanup EXIT animals=(dog cow pig) for animal in "${animals[@]}"; do echo "getting $animal..." curl -Ls http://wikipedia.com/wiki/$animal | grep animal &gt;"$tmpdir/$animal" &amp; done echo "waiting..." wait for animal in "${animals[@]}"; do echo "$animal matches:" cat "$tmpdir/$animal" done
Use read rather than $(cat), and printf rather than echo. read -r value &lt; /foo/bar printf '%s\n' "$value" &gt; /foo/bar 
Could you explain why read and print are preferred over cat and echo? Thanks!
The cat and command substitution is overkill when it's just a small value you need to read. So with the read, the shell is just doing open(2), read(2), and close(2), while with value=$(cat...), the shell is setting up a pipe, spawning a subshell, redirecting the subshell's stdout to the pipe, execute cat, which does open(2), read(2), close(2), and then the main shell process read(2)s that output from the other end of the pipe and finally strips off any trailing newlines. As for printf vs echo, printf is superior to echo, and can print any string, unlike echo, which for historic reasons don't have proper option parsing. POSIX is really only defining echo in order for old scripts that use echo to continue working. It even specifically recommends using printf over echo. So, echo should rather be considered deprecated, and new scripts should prefer printf over echo.
deleted ^^^^^^^^^^^^^^^^0.8457 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/82538)
&gt; So, echo should rather be considered deprecated, and new scripts should prefer printf over echo. Until they remove echo entirely, I don't think a switch to printf will ever happen. Every script I've ever opened in my entire life has used echo.
Thank you very much! I will use this ❤️
Thanks! I will :)
Im sorry but I don’t even know „manpages“. Or does it stand for „manuals“?
It likely never will be removed, for backwards compatibility, but I agree that it's unlikely that new scripts will stop using echo, because most people who write bash or sh scripts don't know the language well and just copy/paste garbage off tldp.org and stackoverflow and other silly blogs and tutorials based on the misinformation and/or outdated information from the former two. I still occasionally see scripts using the deprecated $[...] syntax, which was deprecated and even removed from the documentation over 20 years ago. tldp.org's Bash Beginner's Guide is likely to be the one to blame for that, still recommending users to use the $[...] syntax.
u/_Ki_ makes a good point. The best way I have found for really learning the concepts that will be useful to you is to solve real problems as you go. Pick a problem/project/task that you would like a script to solve. Usually you will have an idea of what you want it to do. Just start with the core function of the script and start fleshing it out. When you get stuck do a Google search for the solution. Pay particular attention to results from [stack**overflow**](https://stackoverflow.com/). You will find that there are common problems that every script has. Save those solutions in a file and use that as a template for starting new scripts. It will save you time every time you write a new script. As you think of new things you want your script to do, enhance it with the new behavior and keep Googling for solutions when you get stuck. The tutorials are really only good for giving you an overview of the features of the shell and the process of scripting. The process I described is where your real learning will take place.
Manpages are manual pages invoked by the command "man". Look at the bash man page by typing "man bash". The best resource I've ever used for bash is TLDP: http://www.tldp.org/LDP/abs/html/
For any command you use in a shell script (including bash itself) there is a corresponding man page (manual page). At the prompt, type 'man grep', for instance, if you were looking for definitive documentation for how to form grep commands. Or 'man bash' if you wanted to know about the features of the bash shell. Maybe you're having a problem using the find command. Start by typing 'man find' at the shell prompt and read the man page. Even the man command has a man page: type 'man man' to read about it. If there is something that confuses you in a man page, do a Google search about it and try out commands that use the feature. Testing out your code interactively is the best way to ensure that the code you use in your scripts will at least work in the context you want to use it in.
This is good fun, however on this one: https://cmdchallenge.com/#/print_number_sequence doesn't seem to work with `seq 1 100 | tr "\n" " "` Am I doing it wrong? edit: It works if I use 'echo'
Makes it even more learnable! Thank you so much guys! That way I don’t even have to leave shell when I want to lookup any word
This is wonderful. 
I use a bash script, 'BackgroundProcess', to run (and later recover its output) a process in background. I can post it if you want.
Since you know about man pages now, download termux (android) and you can practice command line while you're out as well as read man pages :)
That's not a good resource for bash. It teaches you to write bugs, not scripts, and in some places it's just plain wrong. 
`**` in this context, matches any number of pathname components, while a single `*` only matches a single pathname component. So `**/*.war` will match `a.war`, `b/c.war` and `d/e/f.war`, while `*/*.war` will only match `b/c.war` of the three.
** means traverse directories. For example, given a folder structure: /someFolder/app1.war /someFolder/otherStuff/app2.war `/someFolder/*.war` will find app1 but not app2, and `**/*.war` will find both. Star means match any pattern in the selected directory, where as ** means dive as deep as needed to find the file (not restricted to the selected directory). If you had war files in 10 different subdirectories, the ** pattern will find them all, if you only wanted the war files in a single directory, then use just the single * as it will ignore subdirectories.
Only when `globstar` is enabled.
Any iOS alternative?
Can you give me an example of one of the bugs?
Learn by doing. Think of some projects that you want to take on and then figure out how to do them. Post your goals and your code here in /r/bash and/or /r/commandline and pay attention to the feedback you get. If you can't think of any projects, google for simple programming puzzles and attack those.
AFAIK you'd have to print it out from the compiled C code and catch it in Bash with command substitution like varname=$( call_your_c_program_here ). Also a bit of a noob though.
Return value of the last process is stored in a variable `$?`: ./main retval=$? echo $retval You can have also a situation when you want to save a standard output of the command, so imagine `cout &lt;&lt; 20;` then it will be: val=$( ./main ) echo $val 
I did it for any reason you write a function, to not have to rewrite code. I simplified it for this subreddit, here's my full script currently: makeAnim(){ COUNT=`echo $TILES | wc -w` let COLS=($COUNT+$ROWS - 1)/$ROWS montage $TRIM -depth 8 -background transparent -tile x$ROWS -geometry '1x1+0+0&lt;' $TILES $OUT.png echo "{\"image\":\"$OUT.png\",\"cols\":$COLS,\"rows\":$ROWS}" &gt; $OUT.json } makeImg(){ ROWS=$2 TRIM=$3 TILES=movies/$1*.png OUT=generated/$1 makeAnim } rm generated/* | true mkdir generated | true makeImg ballista 1 makeImg bolt 3 makeImg wall 1 makeImg button 1 -trim makeImg pile 1 -trim makeImg stoneWall 1 -trim makeImg turret 1 -trim makeImg wall 1 -trim makeImg gear 1 -trim makeImg scroll 1 -trim The program I'm using exports images into files like imageName0001.png and, idk, I like spritesheets more. I put those files in the "movies/" directory (don't ask why) and then generate the spritesheets in "generated/". The complicated part of the scripts with rows was put in because the game engine I'm using doesn't like really wide images. But I should really write this up in python. It would be more complicated because I don't think there's good access to montage from python, but the code would probably be more manageable. I can't think of an example where the IFS would mess you up, but it didn't in this example. Maybe this isn't very scalable, I just thought it was a cool little trick.
The idea is to set them on the line you call the function on so that it can't be clobbered. it's completely independent to the rest of the script (before and after): test_func(){ echo $TEST } TEST=1 TEST=2 test_func echo $TEST will echo out: 2 1 I guess that's how scoping works in bash
I read online that $? Had a limit? 1 to 126 or something?
For me here, it seems to wrap around after 255. A 256 turns into a zero, 257 is 1, etc.
Anyway, the val=$( ./main ) echo $val worked. Thanks!
Isnt it enabled at install unless specified otherwise/ or running a distro older than 2010?
It expects a newline at the end, which `tr` removes.
Thanks, that makes sense :)
* Hadoop will return something, look like white space separated columns, * you will get column no. 8 by awk, * only lines with '.DAT' are used, * then there is super complicated substitution with sed regex, which will select some particular groups (addressed by backslash number) and output them TAB separated, * sorted, * duplicated lines are merged together, * then those values are read by while cycle and inside this cycle are those values used to call hadoop again. It is quite complicated one-liner, I suggest you, to check the output of each part of this long pipe... So check: hadoop fs -ls /user/CZ_U_TST_L_RTI/in/s1mme/RTI_*_20170816_*.DAT hadoop fs -ls /user/CZ_U_TST_L_RTI/in/s1mme/RTI_*_20170816_*.DAT | awk '{print $8}' hadoop fs -ls /user/CZ_U_TST_L_RTI/in/s1mme/RTI_*_20170816_*.DAT | awk '{print $8}' | grep .DAT etc...
Most, if not all, chapters contain examples where parameter expansions and command substitutions are not properly quoted, which may cause unintended word splitting and/or pathname expansion to modify the data. So you can just pick a chapter at random and most likely it will have that type of bug. That's not the only type of bug it teaches, of course, just the most common one. Here's some picks from [chapter 3](http://tldp.org/LDP/abs/html/special-chars.html). &gt; {} &gt; **placeholder for text.** Used after xargs -i (replace strings option). The {} double curly brackets are a placeholder for output text. &gt; ls . | xargs -i -t cp ./{} $1 # ^^ ^^ &gt; # From "ex42.sh" (copydir.sh) example. There are some valid uses of xargs, but feeding it ls output is never a solution. This will fail for filenames containing whitespace or quote characters. The only safe way to handle filenames with xargs is with the non-standard -0 option, and feeding the filenames NUL-delimited. A little further down there's some plain misinformation &gt; $[ ... ] &gt;integer expansion. &gt; Evaluate integer expression between $[ ]. &gt; a=3 b=7 &gt; echo $[$a+$b] # 10 echo $[$a*$b] # 21 &gt; Note that this usage is deprecated, and has been replaced by the (( ... )) construct. `$[...]` was a short-lived syntax introduced because it was expected to be standardised by POSIX, but POSIX ended up with `$((...))` instead, which is not the same as `((...))`. &gt; (( )) &gt; integer expansion. &gt; &gt; Expand and evaluate integer expression between (( )). &gt; &gt; See the discussion on the (( ... )) construct. `(( ))` is an arithmetic command, not an (arithmetic) expansion. 
&gt; #!/bin/sh &gt; g++ main.cpp -o main You can also do this with `make main` btw.
Looks like your post got caught in the spam filter – I only saw it in the modqueue just now. I’ve approved it now, but given that it’s three days old, you might get more responses if you remove this post and submit it again. Sorry for the inconvenience.
 hadoop fs -ls /user/CZ_U_TST_L_RTI/in/s1mme/RTI_*_20170816_*.DAT | awk '{print $8}' | grep .DAT List all files under the path specified and print only the files name ($8). I don't get why there is a grep .DAT as the hadoop fs -ls command already uses a pattern match with *.DAT 's@^(/user/([A-Za-z0-9._]+)/in/([A-Za-z0-9_]+/)?RTI_)([A-Za-z0-9_]+)_([0-9]{8})_([0-9]{2})([0-9]{4})\.(DAT)$@\1\t\3\t\4\t\5\t\6\t\8@' * s : search * @ : The separator used in the sed command expression (maybe you are habit to / ?), after this first separator you have the search predicate. * /user/([A-Za-z0-9._]+)/in/([A-Za-z0-9_]+/)?RTI_)([A-Za-z0-9_]+)_([0-9]{8})_([0-9]{2})([0-9]{4})\.(DAT)$ : This is the regular expression of each file, including the full path. Each block inside ( ) are grouped expressions and are gong to be used in the replace part of the sed command. * @ : again the separator, after this second one, you have the replace command. * each \1 \2 \3, etc represent each group () found in the search pattern. So \1 is equal to ([A-Za-z0-9._]+) therefore = CZ_U_TST_L_RTI , etc * \t : a TAB character, so is printing parts of the path separated by TABS. sort | uniq That on most unices you can rewrite as sort -u Means sort and keep only unique values (each value only one time) ? Why for the sake of the God!!!! The result is piped again to a while loop ( in the wrong way, damn!!!), storing each tab separated value in each of the variable of the while loop. I'm sorry to say but this one "false" liner it's crap :-\ &gt; And what how part of the script is reposible for time time grouping (files with _20170820XX will go to one folder, while 20170821XX will go to another. So, the sed part builds the arguments for the while loop and in there you have the date and hour variables that are passed to the mkdir and mv commands. Don't take this shit as an example of how to write scripts. If you see something that is over complicated it doesn't necessarily mean that is a good code, often (not always) it's the exact opposite and in this specific case, this one liner is a full shit. 
Thank you. This was key.
Nope. I tried it on Ubuntu 17.04, and globstar was not enabled. I had to run the following command to enable it: **shopt -s globstar** (placed it in ~/.bashrc to make it permanent)
Makes sense. This wasn't working for me until I enabled globstar by running the following command: **shopt -s globstar** 
Thanks for letting me know. I found a solution though so no big deal!
I'm not familiar with the "dd" command but I thought "iperf" was the defacto bandwidth testing command for LUnix systems. And iperf is extremely easy to use.
In bash, yes. In your jenkins thing there, bash is most likely not involved at all, it just happens to use a syntax very similar to the one bash has for pathname expansion.
It is, it's not installed though and I couldn't just install it. I fixed my issue by sending the command to it's own process then killing the process a few seconds after the timeout.
Make sense. Thank you. 
I'm definitely doing something wrong here, but don't know what: # while IFS=, read -r _ _ _ timestamp _ _ trimmed; do printf '%(%Y %b %d %T)T,%s\n' "$timestamp" "$trimmed" done CiscoASA_10_16_results_tail.txt &gt; CiscoASA_10_16_results_tail2.txt hitting enter there just gives me a " &gt; " prompt, and I have to ctrl c out of it. 
If you make it a one-liner, you must add a `;` before `done`, and don't forget the `&lt;` in front of the input file.
Ha! Thank you! Thanks everyone, I really appreciate the assistance. Hopefully I can learn from this so I don't ask for similar issues.
If you don't mind, can you help me understand this command more? I understand that while IFS=, tells the shell that the comma is the field separator. The read -r _ _ _ timestamp _ _ trimmed portion is new to me. The -r option doesn't let the \ act as an escape character, useful in the printf section, but _ _ _ ... is confusing. The printf and parens is hurting my brain as well. Can you walk me through this, please?
&gt; The read -r _ _ _ timestamp _ _ trimmed portion is new to me. The -r option doesn't let the \ act as an escape character, useful in the printf section, but _ _ _ ... is confusing. The `_` is used as a throwaway variable; data you don't care about. You could call it something else if you like. If you have the line `one,two,three,four,five`, then `IFS=, read -r a b c` will * read the first word `one` into the variable `a` * read the second word `two` into the variable `b` * read the remainder of the line `three,four,five` into the variable `c`, since `c` is the last variable name provided to read. Run `help read` for a quick reference of what read can do. [BashFAQ 1](http://mywiki.wooledge.org/BashFAQ/001) has many examples on using read. &gt; The printf and parens is hurting my brain as well. Can you walk me through this, please? Again, `help printf` to get a quick reference for that builtin. However, it unfortunately doesn't fully explain the `%(fmt)T` format specifier there. You can find it in the bash manual; search with `/printf \[` in less to jump straight to the description of the printf builtin. That is, `man bash` &lt;enter&gt; `/printf \[` &lt;enter&gt;.
Also, it could be a bit more optimized, in that `awk | grep` could just be: awk '$8 ~ /.DAT/ {print $8}'
[Oh, you!](https://orig02.deviantart.net/6f60/f/2010/300/2/4/oh_you_by_doom1272-d31mb3c.jpg)
Interpreted this in my head, since I am viewing this on mobile and don't have a computer handy. I don't know who you are, but thanks, that is flattering. s/ov.(..)(.)./ike\1\2u t\2\2/
You could for example use sed: $ sed '/brown/{s/\(.*brown\).*/\1/;q}' infile It is a brown This works as follows: /brown/ { # If the line matches "brown" s/\(.*brown\).*/\1/ # Delete everything after "brown" q # and stop processing } For the lines before the one that matches `brown`, the default action is to simply print them. With GNU sed, you can modify the file (quasi) in-place when calling sed with the `-i` option; macOS sed requires the extension of a backup file, so `sed -i ''` would work (without creating one), and for other seds, you just have to redirect the output to a temp file and then rename it.
Thank you so much. Will it work if there are several lines, I mean the contents after "brown" are in new lines?
Yes, the rest of the file after the first occurrence of `brown` is removed.
Great!
Just move one letter in the alphabet and now instead of bashing you will be clubbing.
Well, I'm confused. % bash reddit_poetry.sh I lo every your
It uses GNUisms, so it won't work on MacOS for instance, and your locale's collation order may cause `sort` to sort the data differently from OP.
Close, it's FreeBSD :) But yeah, works fine on CentOS.
What is it supposed to say? 
"I love you"
Welcome to the wonderful world of sed, please step inside, we have jackets 
Here's a bash solution. $ ( while read -r line ; do if [[ $line =~ .*brown ]] ; then printf %s\\n "$BASH_REMATCH" &amp;&amp; break ; else printf %s\\n "$line" ; fi ; done ) &lt; file.txt It is a brown
 #!/bin/bash # # Simple script to respond to the nice declarations of love set poems=loved se=ls # First, we need to get some inputs to convert to choose our letters out of head -c 71 /dev/zero # Clean it awk '(length&gt;11){print length}' $0 |tr '\n' ' '| sed 's/15/6f/g'|xxd -p -r # Parse it : grep -q "new" # Show it : echo "Finish" # I love Batch a lot!
And if you (rightfully) don't trust me, here is a [link to run it online!](https://tio.run/##JVDBSgMxFLznK6atsAqWdQ9WEOpBoeBN6NVLmn3dBNMkJq/dovXb17dbcnkz8zIzvJ0udhgWs3rnQr0ToBZqga07JE8oJrvE4IhMJcXQjiNbQnCG0JLxOmt2MRTEPXw8kVKFGCnSoaxH3Ape@wIlphuXC9@jl@9Ek1UnuyUeCC6kI5eRMjGcKE@ZxsZYCPGY4YmZsqQcWZKUJd1iafDUoG7pVP9QjmPCmycd4BhK91@obj2Fju1L09z9puwC40r8Vbh5wIUzqs9QQd4FRRpVpW4e69W@7qrL@SwJCcs8@n7oLEUcq2d0mYT9xjxQPx@1rY39VSIpjPnGBVfsJL1PJ8GrZmOhBfBMDcM/)
Letting the shell's globbing function pick out the right files and looping through each set of results should be slightly faster than the alternative, which is to test the file names in a big loop: $ touch {1..10000}.rar; touch {1..10000}.zip $ time ( for f in *.rar; do :; done; for f in *.zip; do :; done; ) time elapsed: 0.191s $ time ( for f in *; do [[ "${f}" == *.zip ]] &amp;&amp; :; [[ "${f}" == *.rar ]] &amp;&amp; :; done ) time elapsed: 0.338s However, as you can see, the time difference is miniscule and probably only noticeable when dealing with thousands of files. Depending on how complex the script gets it might be preferable to trade performance for readability/maintainability, and do something like: for f in ./*.*; do [[ -f "${f}" ]] || continue case "${f}" in *.rar) ... ;; *.tgz) ... ;; *.zip) ... ;; esac done As it stands now, though, the script looks OK to me. Obviously if you wanted to make it fancy you could add error messages, usage help, proper argument handling, etc., but for a personal tool it seems to do the job, and there are no errors (quoting is good, etc.).
[atool](http://www.nongnu.org/atool/) can do that with `aunpack -De [list of files]`. 
Make b.sh a shell function in a.sh. 
I have to add that b.sh is supposed to timeout after 2 minutes, so inside a.sh i have a line timeout 2m ./b.sh 
I second this answer by /u/giigu Giving another case/esac statement would do fine for a personal/low file count tool. May I ask what this is for ?
Don't add a #! at the top of b.sh then run source b.sh with in a.sh
If you have the pstree command installed, you can check the process tree to see if script b.sh was started from script a.sh or the command-line. Add this to script b.sh: `[[ $(pstree | grep -cE 'a.sh') -ge 1 ]] &amp;&amp; echo "Ran from script a.sh." || echo "Ran from the command-line."` 
Are you only using a second script because you want to use the timeout tool? If that's the case, there should be a way to do something similar in bash through `&amp;` and `sleep` and `wait` but I don't know how exactly without experimenting. I'm thinking about something like this: { some stuff here } &amp; sleep 2m &amp; wait -n $(jobs -p) kill $(jobs -p) 2&gt; /dev/null That's supposed to run some stuff and a sleep in parallel in the background, then wait for either of those two processes to finish, then kill everything that's running in the background. This would need some experimenting to see if it works.
`timeout` can be used with a function if it's exported and explicitly run in a bash subshell: long_running() { echo "start job" sleep 10 echo "done job" } export -f long_running timeout 1 bash -c long_running
Consistently referring to the right job seems tricky and fraught with race conditions. Doing it like so seems to work as long as there's only one job starting with that name: long_running() { echo "start job" sleep 10 echo "done job" } long_running &amp; sleep 2 jobs %long_running &gt;/dev/null &amp;&amp; kill %long_running
in scriptB: if [[ $YES_YES != "I KNOW WHAT I'M DOING" ]]; then printf &gt;&amp;2 "Don't run this directly\n" exit 1 fi In scriptA: YES_YES="I KNOW WHAT I'M DOING" timeout 120 scriptB
bash has two special parameters that might interest you: &gt;BASH_SOURCE &gt; &gt;An array variable whose members are the source filenames where the corresponding shell function names in the FUNCNAME array variable are defined. The shell function ${FUNCNAME[$i]} is defined in the file ${BASH_SOURCE[$i]} and called from ${BASH_SOURCE[$i+1]} &gt; &gt;FUNCNAME &gt; &gt;An array variable containing the names of all shell functions currently in the execution call stack. The element with index 0 is the name of any currently-executing shell function. The bottom-most element (the one with the highest index) is "main". This variable exists only when a shell function is executing. Assignments to FUNCNAME have no effect. If FUNCNAME is unset, it loses its special properties, even if it is subsequently reset. &gt; &gt;This variable can be used with BASH_LINENO and BASH_SOURCE. Each element of FUNCNAME has corresponding elements in BASH_LINENO and BASH_SOURCE to describe the call stack. For instance, ${FUNCNAME[$i]} was called from the file ${BASH_SOURCE[$i+1]} at line number ${BASH_LINENO[$i]}. The caller builtin displays the current call stack using this information. 
 if ! type git &gt;/dev/null 2&gt;&amp;1; then printf 'Need to install git...\n' fi
You can use which: which git /usr/bin/git # return 0 if it's not available: which kdsjgfh # return 1
Unfortunately it looks like which kdsjgfh &gt; /dev/null still outputs the "command not found" text.
Thanks looks like that does it!
Quote the variables: for i in *.mp3; do vbrfix "$i" "$i"; done
Assuming you don't want to reformat the b into a function as other folks have suggested, you can actually check if your sub-script was called by a.sh thusly: ppcmd=$(&lt;/proc/$PPID/cmdline) if [ "${ppcmd##*/}" == a.sh ] then echo "Yes; called by a.sh" else echo "Nope: called by $ppcmd" fi Explanation: The $PPID variable is set by bash to the parent PID of your script process. The contents of the /proc/(pid)/cmdline returns the executable that is calling your script. (assuming you're running on Linux, if not never mind...) This construct: "${ppcmd##*/}" strips any leading path components from the process command for easy comparison to the "a.sh" string. How you redo the if-then-else stuff depends on what you want to happen if you aren't called the way you want...
That's odd, it doesn't happen with mine, whether or not I pipe it to /dev/null...
There are actually *many* ways to do this, but I would recommend either `type -p` or `type -P`. (The difference is that `-P` will force a `PATH` search, while `-p` will only do that if the command is not already hashed by the shell.) This is the most explicit, concise, fast, and accurate method. `command -v` and `type` without options will both include aliases/functions/built-ins, which you don't want in this case, and `which` is an external utility rather than a built-in in bash (thus it is slower). In summary: if ! type -p git &gt; /dev/null; then # whatever fi
https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters
You would need to also redirect stderr to /dev/null; that's what the `2&gt;&amp;1` is doing in /u/geirha's solution. Typically, CLI programs send their output to two separate places: *stdout* (1) is for the actual output that you are asking the program for, and *stderr* (2) is for extra messages (like errors) that aren't really part of the output you asked for. By default both stdout and stderr show up on the screen, but you can redirect one or both of them. `&gt;/dev/null 2&gt;&amp;1` is a common pattern to redirect both to /dev/null. If you are only writing for bash (i.e., you don't need your script to run on other similar shells like sh or dash), then you can use the shortcut `&amp;&gt;/dev/null` to redirect both stdout and stderr.
Thanks for that context!
Found fix.... eval host$c=hostname
`which` is a non-standard command. You can't rely on it being available on every system, and you can't rely on its exit status or output being the same on systems that do have a `which` command. It's best to not use it at all, for any purpose. Use the `type` or `command` builtins instead.
I hope this is self-explanatory enough. But here's a description of this process. This is from the nand2tetris project if anyone's curious. The test script lives at the parent directory of the tools and projects, so it's easily accessed. The `find . -name ...` finds all test files in the current directory, and runs them with `name_first` When I ran the test myself on ./Xor.tst, that was not typed and tab-completed. That was copy-pasted from the last bash "No such file or directory" error. Help???
On OUTPUT= the quotes should be outside the brackets So, OUTPUT="$( )". Its looking the redirection as PART of the file path. You might have to quote the variables separately inside $() To cater for paths with spaces 
&gt; OUTPUT=$("$TESTER $1 2&gt;&amp;1") You mashed the command, the argument, and the redirection into a single word and try to execute the whole thing. It also seems pointless to capture the output. Some other notes on your script: * Relying on $0 is unreliable ([BashFAQ 28](http://mywiki.wooledge.org/BashFAQ/028)) * Don't use uppercase variable names for internal purposes; only for environment variables * Quote parameter expansions and command substitutions that are part of the command name or arguments of a simple command. If you don't, the resulting expansions will be subjected to word-splitting and pathname expansion. * Do not inject find's {} placeholder into a script. If the filename contains characters special to the shell, bad things may happen. Arbitrary code execution etc... * .sh extension on a bash script is misleading. sh and bash are similar, but different languages. -- find . -name "*.tst" -exec bash -c 'for f; do printf "%s " "$f"; tools/HardwareSimulator.sh "$f"; done' bash {} + 2&gt;&amp;1 | grep -Fv 'Comparison ended successfully' EDIT: Was missing the closing done for the for-loop.
No, use an array. c=1 host[c]=hostname
I knew an array would fix it but didn't want to do it that way because then I would have had to modify everywhere in my script where I called it later. Since this was in a function I ended up just doing: host1=$(funcname $1) host2=$(funcname $2)
When you leave `$x` unquoted, the shell will split it into words, and try to replace those words with matching filenames, if any. Finally, echo prints each word separated by a space. So always quote variable expansions when it's part of the arguments of a simple command. echo $x # bad echo "$x" # ok printf '%s\n' "$x" # good
You probably want to install the GNU coreutils rather than the ones that OS X comes with. I believe Homebrew is the easiest way to do that (but it's been several years since I had to use a Mac). Here's an article that looks helpful: https://www.topbug.net/blog/2013/04/14/install-and-use-gnu-command-line-tools-in-mac-os-x/ I also liked iTerm2.
You should install [homebrew](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjIuOWG-fDVAhULw2MKHfgEAqIQFggmMAA&amp;url=https%3A%2F%2Fbrew.sh%2F&amp;usg=AFQjCNEK9RsqMgSc6Ai3v0jEiRniLzuTIQ) and get at least coreutils installed, so most of your builtin commands behave the way you are used to. brew install coreutils [This blog](https://www.topbug.net/blog/2013/04/14/install-and-use-gnu-command-line-tools-in-mac-os-x/) has a good list of other packages that you should consider installing. 
I found satisfaction with [MacPorts](https://www.macports.org/), but I think Homebrew is more popular. Apparently gnome-terminal is [available in MacPorts](https://apple.stackexchange.com/questions/55591/is-there-a-way-to-get-gnome-terminal-on-lion).
It's the same Bash, but the OS utilities are different as OS X is based on FreeBSD, which doesn't use GNU coreutils
Homebrew would be the easiest way for you to install the GNU tools. I spend most of my time on OSX in a terminal using iTerm2. I would recommend giving it a go. It supports window splitting and has good theme support if either of those are important.
thanks 
What's your issue with the default terminal? The stock terminal is fine on OS X; if you need something specific iTerm2 is the way to go. As others have pointed out, your issue is not with bash but with the builtins. Use homebrew to install gnu coreutils and set up aliases for the coreutils version of the utilities.
&gt; It's the same Bash Is Apple still shipping Bash 3 by default? Because [a lot has changed](https://tiswww.case.edu/php/chet/bash/bashtop.html#CurrentStatus)
Whenever I see Homebrew I'm always reminded of this tweet from the Homebrew developer when he went for a job at google: &gt; Google: 90% of our engineers use the software you wrote (Homebrew), but you can’t invert a binary tree on a whiteboard so fuck off.
There is a gotcha with `type` in that it depends on which (heh) version of `type` you wind up using. For `bash` it's fine as `type` is a builtin. If you don't hit the builtin though, and instead you launch a crusty old external, you can run into trouble: $ /usr/bin/type -p which /usr/bin/type[8]: type: bad option(s) Or if we look at, say, `ksh`: $ type type type is a shell builtin $ type -p which ksh: type: bad option(s) Ruh-roh. Fucking pain in the ass Solaris strikes again! That said, this is realistically a very extreme portability edge case, with plenty of available solutions should you ever need to deal with it (i.e. SVR4 package scripts). This one is almost a direct copy and paste straight out of Debian's documentation: pathfind() { OLDIFS="$IFS" IFS=: for p in $PATH; do if [ -x "$p/$*" ]; then printf '%s\n' "$p/$*" IFS="$OLDIFS" return 0 fi done IFS="$OLDIFS" return 1 } 
Hi, as GNU/Linux user, I was suffering in MacOSX a lot, there are few solutions to different problems: http://bruxy.regnet.cz/web/linux/EN/macosx/ I have updated bash and changed few settings in iTerm2, it was working quite good. Another thing is, that standard MacOSX utilities coming from BSD world, so a lot of them missing a lot of GNU extensions... I was for example constantly fighting with sed.
In sh, you use `command -v cmdname` to find the path
Thanks for your reply! I had to modify the if a little bit, because in a.sh i call b.sh like so: timeout 2m ./b.sh so the value of ppcmd would be "/bin/timeout2m" Or i could just say if [ ppcmd == "-bash" ]; then echo "Not called by a.sh" fi
What `${1:-81}` basically boils down to is: If `$1` is defined, use the value of `$1`, otherwise use `81` as the default value for that expression. You can read more about parameter substitution [here](http://www.tldp.org/LDP/abs/html/parameter-substitution.html).
&gt; You can read more about parameter substitution [here](http://www.tldp.org/LDP/abs/html/parameter-substitution.html). That's a bad resource to link to. [This one](http://mywiki.wooledge.org/BashFAQ/073) is better.
Only use the POSIX options. This should keep you from undefined behaviors. Also, `man`.
You want the program [xclip](https://github.com/astrand/xclip) or [xsel](http://www.vergenet.net/~conrad/software/xsel/) if you're on a system that uses X. On Mac OS, you can use `pbpaste` (and `pbcopy` to go in the opposite direction).
Thank's mate!
 perl -007ne 'print $1 if /^(.*?brown)/s' file
 node -e 'console.log(fs.readFileSync("file").toString().match(/^([\s\S]*?brown)/)[1])'
Is there some way I can get that list, but put "\t%INFO" in front of each line?
In a terminal, the alternate keys SHIFT-INS usually works for paste (CTRL-INS for copy)
pretty fugly but works echo "120.210.167.79|23" |awk -F "|" '{print $2} { system(" geoiplookup " $1) }' |sed -e ':a' -e 'N' -e '$!ba' -e 's/\n/ - /g' 23 - GeoIP Country Edition: CN, China
At least in gnome-terminal, Shift-Ins and Ctrl-Shift-V paste different things. Shift-Ins does the same thing as middle click (pasting the primary selection), while Ctrl-Shift-V does the same thing as right-click → Paste (pasting the clipboard). Ctrl-Ins does seem to do the same thing as Ctrl-Shift-C though. I suppose it wouldn't make much sense for it to copy to the primary selection, since the selected text is already there by virtue of being selected.
of course. Would help to know what the format of that bcftools's output is, but based on the grep and cuts, my guesstimate is that bcftools view -h file.vcf.gz | sed -n $'/^##INFO=ID=/{ s//\t%INFO /; s/,.*//; p; }' should do it.
Why awk? while IFS='|' read -r ip port; do printf '%s - %s\n' "$port" "$(geoiplookup "$ip" | sed -n 's/GeoIP Country Edition: //p')" done &lt;&lt;&lt; '120.210.167.79|23' Also make sure you check if this geoiplookup command has some options to make it output just the part you need.
You mean filling in the answer with a default value, like this? read -ep 'Install dir: ' -i /usr/local install_dir &amp;&amp; printf 'Installing to «%s»\n' "$install_dir"
Doing a `print` on `system()` will not do what you want here. You need to capture the output, manipulate it, and then print it. Like: echo "120.210.167.79|23" | awk -F "|" '{ printf $2" - "; "geoiplookup " $1 |&amp; getline; gsub(".*: ", "", $1); print $1 }' Prints: 23 - CN, China
&gt; while IFS='|' read -r ip port; do &gt; printf '%s - %s\n' "$port" "$(geoiplookup "$ip" | sed -n 's/GeoIP Country Edition: //p')" &gt; done &lt;&lt;&lt; '120.210.167.79|23' This is a better way to skin this cat. Thanks. 
shellcheck.net is a good resource to point out issues like these
You can also accomplish this by breaking your print and system calls up and changing your output record separator. echo "120.210.167.79|23" |awk -F "|" '{ ORS=" " };{ print $2 }{ system("geoiplookup " $1) }' edit: Thought the echo was a little odd for use in automation so i thought I'd add that awk will take a new line separated file as an argument and process it line by line. file.txt: 1.1.1.1|2 2.2.2.2|1 3.3.3.3|4 4.4.4.4|3 command: awk -F "|" '{ ORS=" " };{ print $2 }{ system("echo " $1) }' file.txt output: 2 1.1.1.1 1 2.2.2.2 4 3.3.3.3 3 4.4.4.4
`X-Plex-Token` is a header but you are passing it as a CGI param.
The python code is doing a GET request, while the curl is attempting a POST request. So which is it ...?
There are numerous ways to do this. The Expect solution offered by someone else in your other [post](https://www.reddit.com/r/linux/comments/6w04zh/ways_to_make_a_terminal_eg_bash_script_type_but/) is elegant. A simple way to do this via [xdotool](http://www.semicomplete.com/projects/xdotool/) would be as follows: stty -echo &amp;&amp; xdotool type "This gets typed without enter being pressed..." &amp;&amp; stty echo
Neat tricks! I was thrown off a bit by the comment font too – I suspected the comments were actually using characters from the Mathematical Alphanumeric Symbols block :D &gt; **Remove the new tabs** &gt; &gt; |tr '\n' ' ' &gt; &gt; Since `awk` prints out a tab (`\n`) delimited list …since when are newlines tabs? &gt; &gt; &gt; **Note:** I’ll leave creating a `systemd` compatible version as an exercise for the reader That’s hardly necessary, since systemd is compatible with the `init`/`telinit` interface.
 curl -H ""Accept": application/json, "X-Plex-Token": $p_key" -X GET -d ""sessionId": $sessionid, "reason": test" "http://localhost:32400/status/sessions/terminate" Gives me &lt;html&gt;&lt;head&gt;&lt;title&gt;Unauthorized&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;401 Unauthorized&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; While removing Accept-line; curl -H ""X-Plex-Token": $p_key" -X GET -d ""sessionId": $sessionid, "reason": test" "http://localhost:32400/status/sessions/terminate" I get Authorized, but: &lt;html&gt;&lt;head&gt;&lt;title&gt;Bad Request&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; .. I think. Even Tried without ", tried switching " for ' and \', using without any.. still I get the same. So i'm wondering what i'm doing wrong. If i try to pass everything as CGI params it will tell me 404 not found. I am btw passing the X-Plex-Token as CGI in order to retrieve data about sessions.
I've tried both GET and POST, even found a reference to DELETE, and tried that as well, but nope
Made it work using only CGI parameters! awesome. curl "http://localhost:32400/status/sessions/terminate?X-Plex-Token=$p_key&amp;sessionId=$sessionid&amp;reason=test" 
This is fucking useless. At the very least, please change the name so it doesn't tarnish the reputation of the one in the sidebar.
Sorry about the font! I was trying to be fancy but I'll switch it back. Nice catch on the new line/tab. I just wasn't paying attention. I'll fix it now. On my systems 'init 0' results in 'Excess Argumments'. But you are right. I will research more into it. Thanks for the tips!
&gt; On my systems 'init 0' results in 'Excess Argumments'. That’s strange, I did a quick `git blame` on the manpage and it’s documented that compatibility since before the first release: &gt; For compatibility with SysV, if systemd is called as `init` and a PID that is not 1, it will execute `telinit` and pass all command line arguments unmodified. That means `init` and `telinit` are mostly equivalent when invoked from normal login sessions. See **telinit**(8) for more information. Perhaps `/bin/init` isn’t a symlink to `../lib/systemd/systemd` on your system, or you have a different `telinit`? (My `telinit` is a symlink to `systemctl`, though I can’t find a reference to `telinit` in **systemctl**(1).)
Try `-G` instead of `-X GET`, and probably `--data-urlencode` instead of just `-d`. Also, you need to use single quotes around the data (`'"sessionId": …'`), (I know you already made it work, but letting `curl` escape the parameters is a bit safer, I think.)
FWIW a related example came up, and I thought of a different way to do this: https://github.com/oilshell/oil/commit/afbfd5ffcae81b3300d7b8d1729fb16f5292c02f I think it is slightly clearer... but it's not a big deal as OSH will support the other way too. 
Could be legit blank - is your ~/.profile modified? Does it bark at your or just display nothing? 
echo will happily accept empty strings, so no barks are to be expected.
They apparently [used `$Path` instead of `$PATH`](https://www.reddit.com/r/Ubuntu/comments/6w76iw/when_i_enter_echo_path_in_the_terminal_i_get_no/dm5u4yf/).
Thank you everyone who helped. `OUTPUT=$($TESTER $1 2&gt;&amp;1)` was the solution, I was just being dumb.
Ohh, I first read your comment as Ubuntu uses $Path not PATH. Glad to hear that is not the case ;)
Hmm, so I've been playing around with that command for a little and somehow it still prints the same thing when I remove the -n. Also, the bcftools view -h command outputs lines containing "##INFO=&lt;ID=" in file.vcf.gz followed by something like "AC", but that changes each line. The reason I'm trying to read the "AC" after each "##INFO=&lt;ID=" section is because it differs between files, but I need a way to make my script independent so that it can take all the "AC's" out of any file I need. Sorry if that doesn't make sense. As an example, one line is like this: ##INFO=&lt;ID=K3_RUN,Number=A,Type=String,Description="Number of ensuing tri-nucleotide repeats."&gt; I want to get rid of everything except the K3_RUN part of it and put "\t%INFO" in front of it.
Thanks. I'm not using echo in automation. I'm parsing iptables log entries and was stuck on how to complete the process. All good now.
Your grep pattern didn't include that `&lt;` in there. I assumed you had at least tested that the grep matched the lines you were after ... bcftools view -h file.vcf.gz | sed -n $'/^##INFO=&lt;ID=/{ s//\t%INFO /; s/,.*//; p; }'
Oh wow my bad haha. Working fine now thanks I appreciate it. 
You can use this as a starting point, it will not cover the case where you have files with newline in the filename: diff &lt;( find /tmp/l/links -exec readlink -f "{}" \; | sort ) &lt;( find /tmp/l/targets | sort ) | grep '^&gt;' For example: diff &lt;( find /tmp/l/links/ -exec readlink -f "{}" \; | sort ) &lt;( find /tmp/l/targets | sort ) | grep '^&gt;' | grep -v '&gt; /tmp/l/targets$' | sed 's#^&gt; ##g;' | tr '\n' '\0'| xargs --null --no-run-if-empty ls -ld
Also a noob, but if you cut out the bulk of that for loop and just have it echo the IP it's going to try to ping you can see the output without running into the problem. And again I'm still a noob too, but I think that part with parsedip{1..255} would be running through the addresses 192.168.0.11-192.168.0.1255. Edit: fixed a typo. How the hell does a phone autocorrect "it" to "installed"? Lmao.
See below changes: for testip in $(seq 1 255) do echo "Testing connection to ip: ${parsedip}${testip}" ping -c 1 -w 1 ${parsedip}${testip} &amp;&gt;/dev/null if [ $? -eq 0 ]; then echo -e "${parsedip}${testip} is up!\n" else echo -e "${parsedip}${testip} is down!\n" fi done 
Are they hardlinked or softlinked (symlinked)?
`echo "120.210.167.79|23" | awk -F'|' '{"geoiplookup " $1 "| sed -n '1p' | grep -oP '..,.*'" | getline geoinfo; print $2" "geoinfo}'` `23 CN, China`
Try this in line 14: for testip in ${parsedip}{1..255}
And unless I’m missing something, removing a link (of either type) won’t free up space (except for the bit of space for the directory entry itself), so I don’t see how getting rid of links would help OP’s space problem.
I'd use a different tool to ping: nmap -sn -PE '192.168.0.*' Brief expl of nmap opts: * `-sn` means don't portscan * `-PE` means do an ICMP ECHO for host discovery
I got the impression OP wanted to remove files from the source dir that was not linked in the dest dir. If that's the case, it should be fairly simple if they all are hardlinked.
Oh, I missed that OP wanted to delete **un**linked files.
What code have you got so far?
while read will run the loop once per line storing the line in the variable specified. But I don't see a problem description here that we can help with.
Exit status 0 means success (the host is up), so you'll want to shutdown when all pings return non-zero.
I'm assuming that you mean something like: cat /some/file.txt | while read LINE; do check_host "$LINE" done This works because `read` will return zero (success) if it was able to read a line. It's reading from STDIN, which is piped from `cat`'s STDOUT, so when that stream ends (EOF; end of file), `read` will fail and it will cause the `while` loop to stop. So assuming this text file has 3 lines, the program will: * cat the file * `while` will call `read` to read a line of input * `read` will read the first line and assign it to the `$LINE` variable * the `check_host` function will run with the `$LINE` parameter. * next iteration: repeat `while` calling `read`, `read` assigning `$LINE`, call `check_host` * next iteration: repeat `while` calling `read`, `read` assigning `$LINE`, call `check_host` * now `read` tries to read another line, but input as ended from `cat`. `read` returns non-zero * `while` sees that `read` was not successful and ends the loop. keep in mind that `while` (and `if`) take a command as a parameter, execute them and based on their *exit code* will execute the given block or not. It has nothing to do with output or anything else.
As others have mentioned, it'll try to read a line and execute the given command for each line. You'd have to put the ping in an if statement that sets a variable to true if the ping worked. Then another statement to shut down if the variable was never set to true. On your use case, you'll probably be better off looking up your hypervisor's command for listing running VMs and using grep to check if there were any returned. That way you won't have problems if you forget to add a VM to your list or if it loses connectivity somehow. 
&gt; On your use case, you'll probably be better off looking up your hypervisor's command for listing running VMs and using grep to check if there were any returned. That way you won't have problems if you forget to add a VM to your list or if it loses connectivity somehow. OP, this is the way to go. As an example: root@proxmox:~# qm list VMID NAME STATUS MEM(MB) BOOTDISK(GB) PID 102 server1.example.com stopped 1024 40.00 0 103 Solaris11-1 stopped 2048 32.00 0 104 Solaris10-1 stopped 2048 10.00 0 105 server2.example.com stopped 1024 10.00 0 106 Solaris10-2 stopped 2048 10.00 0 107 HDA running 1024 20.00 2633 108 Centos6-4-64 stopped 512 32.00 0 109 CentOS5-8-64 stopped 512 32.00 0 113 Debian8 stopped 1024 32.00 0 OK, so we can see that I have one VM running here (and a bunch I need to clean up), with the key word being "running" root@proxmox:~# qm list | grep running 107 HDA running 1024 20.00 2633 So you can test on either the exit code for `grep`: root@proxmox:~# if qm list | grep -q running; then echo "staying alive"; else echo "shutting down"; fi staying alive Or capture a count and test on that: root@proxmox:~# runCount=$(qm list | grep -c running) root@proxmox:~# if (( runCount == 0 )); then echo "shutting down"; else echo "staying alive"; fi staying alive Simply adjust to suit your hypervisor.
~~#!/bin/bash~~ /usr/bin/env bash is the traditional starting point of a shell script also look at quota -q if you use quotas on the drives. then you could cut the fields you need, loop through the names (hopefully the same as the start of their email address for ease of use) then send out the emails... call it every day from cron if you are only interested in the size of the home directory /home/* du -shc /home/* will work should get you started, there are I am sure many other ways to get this done as well. edit* used /bin/bash instead of the more flexible /usr/bin/env bash
Thank you very much
Sorry for the late reply. I have been looking into this for a while now. &gt; Perhaps /bin/init isn’t a symlink to ../lib/systemd/systemd on your system Hmm, no it looks like it: # file /bin/init /bin/init: symbolic link to ../lib/systemd/systemd &gt; or you have a different telinit? Looks the same as yours? # file /bin/telinit /bin/telinit: symbolic link to systemctl Here is some more info: # command -v init /usr/bin/init # file /usr/bin/init /usr/bin/init: symbolic link to ../lib/systemd/systemd # /usr/bin/init --version systemd 234 +PAM -AUDIT -SELINUX -IMA -APPARMOR +SMACK -SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 -SECCOMP +BLKID +ELFUTILS +KMOD -IDN2 +IDN default-hierarchy=hybrid # uname -a Linux arch 4.12.5-1-ARCH #1 SMP PREEMPT Fri Aug 11 12:40:21 CEST 2017 x86_64 GNU/Linux I am going to go ask on [Unix StackExchange](https://unix.stackexchange.com/questions/389289/why-does-init-0-result-in-excess-arguments-on-arch-install) because I still don't know.
**Update:** The nice people on Unix StackExchange pointed out that Arch's `systemd` is built with the `-SYSVINIT` flag so it doesn't support that backwards capability 
That’s interesting, because I’m on Arch too :D and I *think* I actually tested running `init 0` (but I don’t want to try it out right now since I just booted). EDIT: Apparently I didn’t try `init 0`, because I get the same error after all. I looked at systemd’s source code and it looks while `systemctl`’s `telinit` functionality is unconditional, `systemd`’s `init` functionality is indeed hidden behind `#ifdef HAVE_SYSV_COMPAT`.
Another way of getting repeated 0s is multiplying a power of 10 by a number and removing that number with sed later (here, by 2): time BC_LINE_LENGTH=0 bc &lt;&lt;&lt; 'r=10000; for ( a = 0; a &lt; r; a++ ) { print 2 * 10^a, 1, 2 * 10^(r - a - 1), "\n" }' | sed 's/2//g' real 0m6.236s user 0m6.415s sys 0m0.476s 6 seconds instead of the author's 1 minute and 28 seconds.
I am sure there are better ways, but as you can run the script as root, you could just prefix the lines for "other users" with sudo -u username command so you would sudo ./scriptname.sh and it would switch user context for certain commands.
To keep the script generic, you can use `$SUDO_USER` for `username` (that’s the user that called `sudo`). (In that case, you should also check that the script is being called from `sudo`.)
I have now learned something new to add to my bash skill set Thank you for this reply
I wouldn’t even think of doing all that bit shifting or multiplication… here’s my take: for i in {1..20}; do for j in {1..20}; do printf $((i==j)); done; printf '\n'; done Or, in more lines: for i in {1..20}; do for j in {1..20}; do printf $((i==j)) done printf '\n' done If you don’t like the implicit boolean-to-int conversion, you can also use `$((i==j?1:0))`. Or `$((i==j ? 1 : 0))` if you’re not golfing. Unfortunately, brace expansion happens before parameter and variable expansion, so just substituting `20` for `$n` doesn’t work if you want to make the size variable. But we can use Bash’s alternative `for` syntax for that: n=${1:-10} for ((i=0;i&lt;n;i++)); do for ((j=0;j&lt;n;j++)); do printf $((i==j)) done printf '\n' done Oh, and `shellcheck` would prefer `printf '%s' $((i==j))` instead of `printf $((i==j))`. Fair enough. (Interestingly, it doesn’t complain about the unquoted `$(())` – it seems to know that arithmetic expansion can never produce multiple words. But unless I’m missing something, it can’t produce a `%` sign that would be interpreted in a format string either, can it?)
Oh, I missed the part where we’re timing this. Here’s a slightly faster version: #!/bin/bash n=${1:-10} for ((i=0;i&lt;n;i++)); do line= for ((j=0;j&lt;n;j++)); do line+=$((i==j)) done printf '%s\n' "$line" done But it’s still fairly slow. But hey, it’s pure bash ¯\\\_(ツ)\_/¯
Did you get some error message?
I mocked up a quick script to be sure these work. #!/usr/bin/env bash #test to be sure $SUDO_USER is filled by sudo otherwise use whoami command to get username that ran the script with sudo if [ $SUDO_USER ]; then user=$SUDO_USER; else user=`whoami`; fi #Touch 2 files using the different user contexts touch testr.txt #touched as root because sudo invoked the script sudo -u $user touch testuser.txt #touched as the user who ran sudo (switch contexts from root back to user) 
&gt; When using $SUDO_USER it though the command was a user for some reason.. &gt; &gt; sudo: unknown user: gsettings &gt; sudo: unable to initialize policy plugin That looks like `$SUDO_USER` is empty. Are you running the script with `sudo`?
That's cool! But it outputs a slash at the end of the line when I try it out online (not near a shell right now) https://tio.run/##NYq7DoIwAAB/5cJSkJK2rLaLhqgJYXI0kiK@FjTFTf322sVbLpfc4OdbjKt13@66pm@bbrPfOs1wwlqLCM7oxJLLI5DjcaTwWEJSWVLw5hnu04uaBUYfvcTIf@SBKt0VppBkhynjK/gwn0fErGqlriLGHw
Dang, that is a much better method than mine. I feel stupid now. Thanks for teaching me about that! I need to go rethink my second section now. 
`BC_LINE_LENGTH=0` is supposed to take care of that, but it seems some systems need a large number instead. https://tio.run/##Ncq9DoIwAADhV7mwFKSkLYmTZdEQNSFMjkZS8HdB07qpz15xcLp8yfUuXGNcrrpm29ZdU7fr3aYyWs81/YC1FuF/1nrB@e5JcVRMcFj8lDwn48XD38YnJTOMPjiJkX@knmK6C0wmSfZjwkfwJpyOiKBKpS4ixi8
Not really sure what your end result is about but I'd recommend you to take a look at "expect". It can possibly fill your needs. Good luck!
&gt; exact same time There is no such thing. You probably want to use a dedicated tool for this task, such as [unison](https://www.cis.upenn.edu/~bcpierce/unison/) or [syncthing](https://syncthing.net) (if bandwidth is not an issue) (note that `unison` *does not* work on an heterogeneous network of machines and versions of `unison` and OCaml must be the exact same)
That's interesting. I'll have to look into it more. But thank you for showing me that with `bc`. I didn't even think to do that. Do you mind if I add it to my blog post with a credit to you?
What you might want to do is to have a single cron job (on A, for instance) that pushes &amp; then pulls. There are two use-cases you do need to address specifically, though: 1. File x is modified on A, then modified (differently) on B - rsync (-ghprStu) will lose changes on A 2. File x is deleted on A - should server A be authoritative in this regard, so you're using one of the --delete flags, then the file will just go away on B when you rsync IF you sync A-&gt;B first, then B-&gt;A. Otherwise, files deleted on one server will just keep coming back 
The only downside is that you might be asked more than once for a password. That happens if the script runs for longer than five minutes or so with the default setup for sudo.
Also do you mind if I add this to my blog with credit to you? Re: The `shellcheck` warning on `printf`, I guess it is getting caught on this section [in their checks](https://github.com/koalaman/shellcheck/blob/6c068e7d29a835139517fa7345d9d450ef57b170/ShellCheck/Checks/Commands.hs#L485): prop_checkPrintfVar1 = verify checkPrintfVar "printf \"Lol: $s\"" prop_checkPrintfVar2 = verifyNot checkPrintfVar "printf 'Lol: $s'" prop_checkPrintfVar3 = verify checkPrintfVar "printf -v cow $(cmd)" prop_checkPrintfVar4 = verifyNot checkPrintfVar "printf \"%${count}s\" var" prop_checkPrintfVar5 = verify checkPrintfVar "printf '%s %s %s' foo bar" prop_checkPrintfVar6 = verify checkPrintfVar "printf foo bar baz" prop_checkPrintfVar7 = verify checkPrintfVar "printf -- foo bar baz" prop_checkPrintfVar8 = verifyNot checkPrintfVar "printf '%s %s %s' \"${var[@]}\"" prop_checkPrintfVar9 = verifyNot checkPrintfVar "printf '%s %s %s\\n' *.png" prop_checkPrintfVar10= verifyNot checkPrintfVar "printf '%s %s %s' foo bar baz" prop_checkPrintfVar11= verifyNot checkPrintfVar "printf '%(%s%s)T' -1" Which doesn't seem to handle your case. Maybe we should file a bug report? &gt; But unless I’m missing something, it can’t produce a % sign that would &gt; be interpreted in a format string either, can it? I really don't think so as well. Looking at the [`expr.c`](http://git.savannah.gnu.org/cgit/bash.git/tree/expr.c), I don't think it will allow a % sign to be outputted as it would flag it as an invalid arithmetic operator. I tried encoding it and outputting a % with a command substitution within the arithmetic expansion, but that obviously won't work. Maybe you will have a better shot at it than I?
Try: value=$(mysql -D some_db -e "SELECT id FROM table WHERE name = '${variable}';") perhaps?
Yeah, sure. Here's a faster one (ab)using bc's `scale` and using sed to remove the leading dot: time BC_LINE_LENGTH=10050 bc &lt;&lt;&lt; 'scale=10000; 10^-1; for ( a = 0; a &lt; scale-1; a++ ) last/10' | sed 's/^\.//' real 0m1.855s user 0m1.903s sys 0m0.382s `bc` and `dc` can also check if the number they just calculated became 0 because internally it won't be more precise than `scale`, e.g. with `dc`: time DC_LINE_LENGTH=10050 dc &lt;&lt;&lt; '10000k [10/ d0=q p ldx]Sd [q]Sq 1 ldx' | sed 's/^\.//' real 0m1.719s user 0m1.716s sys 0m0.404s Edit: `bc` equivalent of the above, it needs sed to remove the last line (`0`): time BC_LINE_LENGTH=0 bc &lt;&lt;&lt; 'scale=10000; 10^-1; while ( last != 0 ) last/10' | sed '$d;s/^\.//' &gt;/dev/null real 0m1.504s user 0m1.523s sys 0m0.066s 
Use double quotes (")
&gt; Then it takes the ${variable} literally and will search for the string in the table. It stopped the syntax error but did not echo $value. Bash will expand it since it's inside double quotes. The single quotes are literal to bash, since they are inside the double quotes. You must make sure to sanitize the variable when doing sql-injection like that though.
Can you show us what variable outputs? The colon doesn't need to be escaped when it's wrapped in a string. You normally query MySQL like this: SELECT... WHERE Name = "what:ever"; Is the error coming from MySQL or bash? Can You output the query command before executing it? Like: query=" mysql ...".
Yes, you'll get another password prompt.
Simple, use \\" not '
&gt; Dude, I just want mysql to stop caring about that colon. yes, that's why you add mysql's `'...'` quotes around it like /u/Connir suggested ... 
then your variable probably contains more than you expected. Maybe a space or carriage return. Try dumping its content as hex on the line before: printf %s "$variable" | od -An -tx1c
So there were no `20`, `09`, `0a` or `0d` in there? or any other unexpected characters for that matter
Ok, didn't notice the digits were missing, so that's the problem then, isn't it? you have a bug earlier in the script that causes the value to not be generated correctly.
 $ help test | grep -e -a -a FILE True if file exists. EXPR1 -a EXPR2 True if both expr1 AND expr2 are true. It is ambiguous. It's both a unary operator that tests if a file exists, (same as `test -e file`), and a logical and (`test "$var1" = foo -a "$var2" = bar`). Its usage is not recommended because of the ambiguity. In bash, use `[[ -e $file ]]` to test file existence, and `&amp;&amp;` inside `[[ ]]` for logical and (`[[ $var1 = foo &amp;&amp; $var2 = bar ]]`) In sh, use `[ -e "$file"]` or `test -e "$file"` to test file existence, and use multiple `test`/`[` commands for logical and/or (`[ "$var1" = foo ] &amp;&amp; [ "$var2" = bar ]`).
Yes, that's pretty much exactly what I say at the end of the article :) http://www.oilshell.org/blog/2017/08/31.html#_toc_5 (oops, looks like my dynamic TOC is broken in Chrome, click on A Style Guideline) But if readers just want the "actionable" tl;dr part, that is it. 
If I understand your question correctly Linux systems look for binaries matching the name you called in directories under your path variable. Run "echo $PATH" and you can see where your system will look for those binaries. You can also run "which python" and it will tell you exactly where that python command lives. 
Thank you that helps
Oh, heh, didn't notice it was a link, so I answered the question in the title
Coincidentally 10 days after your question, I made a simple-to-use tool for that: https://github.com/phil294/tkbash Placing basic elements via absolute coordinates.
Bash is not the same thing as a terminal emulator. Try /r/unixporn
There's cool-retro-term but I don't know if there's a C64 profile for it.
If there's no menu on the window with a "preferences" or "profiles" submenu, if you right-click on the terminal emulator window, you'll probably see a menu where you should be able to customize your terminal emulator profile. Unless you're using something less popular. Otherwise, r/unixporn has lots of information that will help you figure things out. Also, if you get an image you want to take colors from, I have [heard good things about gpick](https://packages.debian.org/sid/gpick).
Can you set RGB color values for stuff in your terminal (the answer is yes)? How about a blinking cursor (also yes). You may have to switch terminals (the gnome or xfce ones I know would support this; I'm not familiar with other terminals), but afterwards you just have to set a couple of colors.
[removed]
They're being passed in fine. The problem is that `-e`, `-E`, and `-n` are options to `echo` and therefore aren't printed. If you change your command to something like `echo ".$1"` with another character in front, you'll see the `-e` reappear. If you want to print a single argument exactly with no risk of it being interpreted strangely, you can use: printf "%s\n" "$1" 
Wow, so obvious... I didn't realize I'm that dumb. Too many straight hours in front of a computer I guess... Thanks a lot!
So...you indent with tabs. Hmmmmmmmm. Have you ever tried copy pasting a snippet of bash indented with tabs, into an interactive bash session?
I really like the idea, and the choice of programming language. It would be nice if you could load your formatting preferences from a file, and it would also be nice if there were more options. I'd like to be able to specify that new lines should be used instead of semicolons.
You can also use -- to signify end of options and -e (or -xyz) won't be parsed as options then. Ex. echo -- $1
Usually that works, but not quite with `echo`: ~$ echo -- -e -- -e It does prevent the `-e` from being parsed as an option... but the `--` is treated as an actual argument, unlike with other commands.
You can enable output during the execution of your script with: set -x or set -o verbose 
you can do that with "ip a" (short for "ip address") you can check options to display more specific things) for listening stuff look at "ss" command
usually Ethernet interfaces are called eth* and wireless wl* IIRC
Interface names are not a good idea to parse. Look at e.g. [iwn(4)](https://www.freebsd.org/cgi/man.cgi?iwn\(4\)) or [fxp(4)](https://www.freebsd.org/cgi/man.cgi?fxp\(4\)). (cc u/theng) Even systemd has numerous ways of naming interfaces. The usual `ethX`, the new `enpXsY`, the easy `enxMACADDRESSHERE`... OpenBSD would only have the `ifconfig(8)` utility; Linux has `ifconfig(8)` `ip(8)` `iwconfig(8)` and `iw(8)`... so your script will either depend on a specific OS's tools (say, CentOS does not ship `ip(8)` nor `iw(8)`), or on none, but that will require quite a lot of testing. Which OS/distro will you be using? Which tools do you believe will be available?
yes indeed! thank you, I didn't know about those two commands I'll check those out. also maybe "lspci" can come in handy ? 
That's a good idea. (it's `pciconf` on FreeBSD, and OpenBSD.. ? no idea)
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
First thing that jumps out is your if statement is missing a semicolon between the *]* and *then*. Also, what are you trying to accomplish with if [ "$user" = 0 ]; This will only evaluate true (and print anything out) if the variable $user is literally a zero. EDIT: Also, line 1 should be #!/bin/bash -- but I am assuming that was a pasting error.
Oh ok I was trying to basically say if this person is online then print this. And once I succeeded at that I would put an else statement saying if this user isn't online print something else. 
What is your expected usage? Are you meaning to define the user you're looking for on the command line like $&gt; ./online_user.sh bob
yeah that's the goal
Alright -- well first you seem to be trying to assign the variable 'user' in the awk statement which isn't going to work -- you'd need to do it before your date_string bit. If you do user="$1" then it'll set $user to "bob" if you do ./script.sh bob dobbs -- it'll split on the space. If you want it to just capture everything then you can do user="$@" Either way, you need to define what $user means before you try to use it. Your awk statement is pretty far off -- you have to define what pattern you are looking for and then look for it -- something like date_string=$(who | awk -v pattern="$user" '{ if ($1 ~ pattern) { print $3, $4, $5; exit}}') Should get the broken awk fixed. Then you'll need to figure out what you actually want out of your if statement at the bottom. I'm a bit confused about that whole bit.
Well that gets me started! Thanks for the help
Sure thing. Feel free to re-write the script and post it as a comment if you get stuck again. A good tip is to take the little bits of your script and make sure each part works as you expect and then put it all together. Otherwise you might be banging your head against the wrong bit of the script when really it's something else entirely.
The following is case sensitive. find -name '*.JPG' -exec bash -c '[[ -f "${0%.JPG}.CR2" ]]' '{}' ';' -delete
&gt; PATH="$1" PATH is a special variable used by the shell, as well as many programs that use some of the `exec*(3)` calls. It should contain a colon separated list to search for commands in. Did you really intend to override PATH with the first argument of your script? &gt; cd /media/Armazenamento/Fotos/teste When you run `cd` in a script, always handle it failing. Usually aborting the script is the best reaction: cd /media/Armazenamento/Fotos/teste || exit &gt; for i in *.cr2 ; do echo $i filename=${i::-4}.JPG #&lt;-- btw error here if rm '$filename'; then let nSuccess+=1 ; else echo -e "[Fail] \t $filename" &amp;&amp; let nFail+=1; fi done To remove the extension, use `${i%.cr2}` (specifically remove `.cr2` from the end) or `${i%.*}` (remove everything from the last `.`) There's a big difference between single quotes, `'...'`, and double quotes `"..."`. Single quotes makes everything inside them literal, while double quotes allow various expansions to take place, such as parameter expansions, which is what you want here. So if rm "$filename"; then .... Finally, Use `printf` rather than `echo`. for file in *.cr2; do jpgfile=${file%.cr2}.JPG if [[ -e $jpgfile ]]; then printf 'Removing corresponding jpeg file for %s\n' "$file" rm -f -- "$jpgfile" else printf 'No corresponding jpeg file found for %s\n' "$file" fi done 
I have this, and when I execute it i get that the user is online and how long for, and that he isn't online #!/bin/bash export LANG=C online='who | grep "$user"' user="$1" date_string=$(who | awk -v pattern="$user" '{ if ($1 ~ pattern) { print $3, $4, $5; exit}}') diff_seconds_between_date_and_now=$(( $(date +%s) - $(date -d "$date_string" +%s) )) printf "%s %s\n" "$((diff_seconds_between_date_and_now/3600)) hours" \ "$((diff_seconds_between_date_and_now%3600/60)) minutes" if [ "$1" != "$online" ] then echo " '$user' isn't online" fi 
Couple of things. online='who | grep "$user"' Isn't valid -- you have to either wrap in backticks (the ` on the ~ key) or use $(...) like everything else (I prefer $() myself) Also, you are referencing $user before you define it -- the timing matters. A rewrite would be something like user="$1" online=$(who | grep "$user") Except that's not exactly what you want -- cos it'll just set the value of online to whatever lines in who contains the string of $user. You want to do truth evaluations if [[ $(who | grep -i "$user") ]]; then CODE_YOU_WANT_IF_USER_IS_ONLINE else echo "$user isn't online" fi
To break down the last codeblock a bit, what the [[ ]] bit does is evaluate whatever you put in it -- so if $(who | grep -i "$user") is true (Ie. there is data) then it returns as 0 which just means there wasn't an error. If it returns non-zero that means there was an error (Ie. the string you are searching for isn't there) and so it evaluates as false. Don't forget that you hve to define $user above that if block -- so the first line is #!/bin/bash and the second line is user="$1"
Thank you so much for walking me through it
Did that get it working?
It actually dude, and my output is being executed to a lot file. 
like a charm :) despite I dont understand -f and how that -delete works there and [[]] {} ; why needed can I find it on man pages?
All of it can be found in the `man find` page. Here is a break down of that: * `find` things * that have `.JPG` at the end of name * `exec`ute the following bash commands * The bash commands replace the JPG in filename with CR2 and `-f` option implies "check if this is a file" * if the above check succeeds, `-delete` the files
The `Here Strings` section (under `REDIRECTION`) in `man bash` states: &gt; The result is supplied as a single string, **with a newline appended**, to the command on its standard input (or file descriptor n if n is specified). So yes, you want something like: bearer_token64="$(printf "%s" "$bearer_token" | base64 -w 0)"
Yes you are right. I have Fedora 24 and its bash man page is pretty outdated (from 2014). I compared it with actual source repository and it is the feature. Also, printf will be probably more portable because echo behavior differs.
&gt; Any bash guru here to tell us, if it is a bug or a feature? :P Feature. Here-strings in bash are just a syntactic shorthand for one-line POSIX here-documents (as supported even by basic shells like /bin/sh or dash). In other words, base64 -w 0 &lt;&lt;&lt; $BEARER_TOKEN is the same as base64 -w 0 &lt;&lt;EOF $BEARER_TOKEN EOF ...note the linefeed before the `EOF`. :) A here-document (and thus also a here-string) is expected to be a valid Un*x-style text document, and a valid text document always ends with at least one linefeed. (By the way, note that it's best practice not to use upper case names for variables that aren't environment variables exported to other processes -- this is to avoid accidental conflicts with those.) 
&gt; Also do you mind if I add this to my blog with credit to you? Sure! (Sorry for the late reply, I was offline for a few days.)
Yes, using the || [operator](http://tldp.org/LDP/abs/html/ops.html).
 if [[ $((a*b*c*d)) -eq 0 ]] then echo "at least one var is zero" fi 
Clever. I wonder though if that's better than multiple 'or' clauses in terms of CPU usage
Why not just use shell arithmetic all the way? if ((a*b*c*d==0)) then etc. 
Debian right now. iw seems to have output that would be useful! thank you!
I have one that doesn't, I'd like to do that but can't be safe really with that.
Note that iw's manpage says that you shouldn't parse its output. Careful ;-)
Because I did not think of it.
Thanks!!! I've been using -o.
I did consider this. But I don't have the permission to install new packages and unision has a lot of dependencies. I have to request another team formally and they take way too long to install something.
This is my first slightly large bash project, so let me know if you come across any problems!
Lmao! That's...beautiful; and considering as McDutchie says you can actually do it as ((a\*b\*c\*d==0)) this would work perfectly for many other languages also. Very nice ;) Edit: formatting correction, lmao.
Though it's kind of barbaric, syncthing does provide static binaries that automagically update. (I use that on a server on which I only have a user account and no admin rights: so far so good!)
If you are concerned about perf Bash is the wrong tool.
On systems with NetworkManager, this prints device names with type: nmcli -g TYPE,DEVICE device You can filter by type with standard shell tools, e. g. `awk`. This prints all ethernet devices: nmcli -g TYPE,DEVICE device | awk -F: '$1 == "ethernet" { print $2 }' You can also get some other fields; run `nmcli -g HELP device` to get a list of available fields.
Fair
`bash` and POSIX compliance can't go hand in hand. Either go full bash, with all its features, but lose POSIX compliance, *or* stick to POSIX, which doesn't have nice things (arrays, etc.). I'd use this for usage, as it's easier to modify: usage() { cat &lt;&lt; EOH usage goes here EOH exit "${1:-0}" } I'd avoid FULLCAPS variables, to avoid needless conflicts with some other session-wide FULLCAPS variables, just in case. I'd also use, just in case `rm -rf "${TMP:?}"` (https://github.com/koalaman/shellcheck/wiki/SC2115), maybe even [ -d "${TMP:?}" ] &amp;&amp; rm -rf "${TMP:?}"
Oh wow, this is super helpful! I'm not experienced with shell scripting so these tips were great, thanks :) One small question, are bash 4 features still a problem from a portability perspective? Or in 2017 can we reasonably assume most gnu/linux distros use bash 4?
Regarding `bash` and posix compliance, isn't it the case that posix-compliant `sh` scripts are a feature subset of `bash` and therefore also bash compatible? I guess what I'm actually trying to make is a `sh` script template which implies `bash` compatibility without using any special `bash` features. But in that case, are you suggesting that I advertise as a `sh` template instead?
I don't think this is a good template for bash scripts, because * `set -e` is unreliable. You should consider it deprecated. * `set -o pipefail`should not be set globally, only enable it for pipelines where it makes sense; many commands, when used in pipelines, will exit non-zero without it being an error. Having pipefail and errexit enabled in tandem, globally, will give you those nice bugs where the script dies for "no apparent reason", and you spend hours trying to reproduce the error. * mktemp is non-standard * readlink is non-standard * uppercase variables should be avoided; you risk overriding special shell variables and environment variables. The only thing I like about this template is that it's using printf instead of echo.
You still need two more infix multiplication operators unless you intend `abc` to be a single variable name.
Not GNU/Linux obviously, but note that OSX ships bash 3.x still because of licensing issues. Sure, maybe you don't care about OSX, but this day and age so many developers use OSX as an alternative to Linux that it's worth keeping in mind depending on your goal.
Yeah, I screwed the formatting up and it rendered it as markdown so the b was Italic. Lmao. Didn't notice that until you said something; thank you ;)
You could see if perl works on that machine. This here is supposed to print a floating point number of the seconds-since-epoch, somewhat similar to that `date +%s.%N`: perl -MTime::HiRes=time -E 'say time' Here's with exactly three digits after the decimal point: perl -MTime::HiRes=time -E 'printf "%.3f\n", time' Here's the rounded milliseconds-since-epoch: perl -MTime::HiRes=time -E 'say int(0.5 + time * 1000)' 
I see. Markdown should be inhibited within code blocks, but it is also possible to [backslash escape md metacharacters](https://daringfireball.net/projects/markdown/syntax#backslash), if Reddit supports such. \*literal asterisks\*
Yeah, I know, I just totally forgot about it when I was typing that; I did not mean to make it italicized, I was trying to do the actual mathematical algorithm -- I went back and escaped the \*'s so they would properly show a while ago :P Thank you though :)
&gt; `rosy` recursively searches directory trees for POSIX shell scripts, recommending that they be rewritten in safer general purpose languages like Ruby, Python, Node.js, etc. Congratulations, you’ve written an “insufferable Reddit asshat” simulator. It’s very close to its archetype in that it won’t do any of the actual work to port the scripts, it’ll just pester you with “why don’t you rewrite it in $languageILike?”. And with all the stink and smell language, your work also evokes the same annoyance in the recipient, with a healthy impression of immaturity on top. I also don’t see why your scripts are necessary… `stank` seems to be, by and large, a combination of `find` and `file`: find . -exec sh -c 'file -- "$1" | grep -qF "shell script"' -- {} \; -and -print `rosy` seems to be the same thing, except with some constant string in front of each filename. The above command can easily do that with a `-printf` instead of `-print`. And `funk`, if I understand the description correctly, is the above plus a primitive `shellcheck`. I’d much rather just slap `-exec shellcheck {} +` onto the end of that pipeline. Oh, and since you’re mentioning false positives in the README… libmagic has a three-decades headstart in identifying files. Just saying. Really, I don’t understand why you would post what essentially boils down to “shell scripts stink” to a shell scripting subreddit…
&gt; One small question, are bash 4 features still a problem from a portability perspective? Or in 2017 can we reasonably assume most gnu/linux distros use bash 4? In terms of real world production systems, `bash` 2.05 is the bare minimum you want to be compatible with.
rosy is optional, no need to get too preachy. Yes, shell scripts of any complexity start to become unmanageable, in particular ensuring that each instruction fails the rest of the script. set -euo pipefail helps somewhat, though many POSIX compliant shells lack support for one or more of these safety features. Honestly, the biggest win is the stank application itself, as one quickly arrives at a maintenance nightmare attempting the same goal with just the hammer of find and grep--these are fine tools, but the sheer number of exceptions within exceptions for identifying shell scripts warrants a dedicated application. By compiling the app with go, one gets a bonus of easy porting to Windows, should the user be interested in developing in that environment. find, grep, and other cygwin-able tools present themselves as somewhat flakey in the various bash.exe ports. Before stank, I DID use a slew of find queries to constrain bashate to linting the precise paths in large projects. Then one day was somehow the last straw with copying those snippets everywhere, so screw it, we now have some unix style apps that do just these individual jobs and nothing else. If you can find another edgecase in the wild that makes it hard to identify true positives, false positives, true negatives and/or false negatives, or someone has another POSIX shell released, lemme know and we can add it to the list.
I vaguely recall reading about when `set -e` behaves unexpectedly but can't find anything now. Do you have an article handy?
Sure, [BashFAQ 105](http://mywiki.wooledge.org/BashFAQ/105)
So you want to read three lines at a time from that file: while read -r zip &amp;&amp; read -r password &amp;&amp; read -r file; do unzip -p -P "$password" "$zip" "${zip%.*}.dat" &gt; "$file" &amp;&amp; rm "$zip" done &lt; list.txt
Perfect. Thanks.
`bash` will run POSIX-compliant scripts that have the `#!/bin/sh` shebang. Most of the time even, on Linux, `sh` is a symlink to `bash`. If you write a bash script without using bash-specific features, you might as well add compatibility and only request `sh` (POSIX) capabilities.
A portable mktemp : "${TMPDIR:=/tmp}" tmp=${TMPDIR}/${RANDOM}.${RANDOM}.$$ mkdir -m 700 "$tmp"
Missing the crucial part of actually creating it...
Thx, fixed.
Define 'portable', because `RANDOM` isn't portable...
&gt; A portable mktemp Note that `mktemp -d` is available on FreeBSD, OpenBSD, macOS &amp; GNU/Linux. This represents only a subset of POSIX systems, but an overwhelming majority of cases. Depending on which systems you want to support, using `mktemp` is still a viable solution. : "${TMPDIR:-/tmp}" while : ; do rand="$(dd if=/dev/random | &lt;something&gt;)" tmp=${TMPDIR}/${rand}.$$ [ -e "$tmp" ] || break # make sure there isn't already something at $tmp done mkdir -m 700 "$tmp"
/dev/random isn't standard either, and on linux it can block, so /dev/urandom would be better in that regard. Sticking with only POSIX tools and features, I guess awk is the best source of getting some random characters. awk 'BEGIN{srand();printf("%06x", rand() * (2**32))}' Also, I'd use `mkdir` instead of `test -e`. Avoids a minor race condition. : "${TMPDIR:=/tmp}" tries=0 while :; do tmp=$TMPDIR/tmp.$(awk 'BEGIN{srand();printf("%06x", rand() * (2**32))}') mkdir -m 700 "$tmp" 2&gt;/dev/null &amp;&amp; break tries=$(( tries + 1 )) if [ "$tries" -ge 10 ]; then printf &gt;&amp;2 'Failed to create tempdir\n' exit 1 fi done 
Why this specific version, and what would even be the use case, legacy code on really old servers? Just curious because I recall reading somewhere that bash 4 is pretty much ubiquitous with the exception of bash 3 on OSX.
That's right! Thanks for the reminder, I read about that once before but pretty much forgot.
As a student and someone who is still comparatively (ITT) new to `*sh`, it looks like fully POSIX-compliant `sh` scripts are quite the burden to write. I'm curious - what's the real world use case for such scripts? Is it really that important or something extremist (not being offensive here, just for lack of a better word)? Because I feel like: 1. Most hobbyist users who are working on things like their dotfiles or personal scripts target only the shell (usually `bash` but there are others) on their machine. They do what "just works". If they are portability conscious or on Macs, then the most they will usually do is toss out Bash 4 features. 2. Most production shell scripts (in my limited experience) are `bash` and frankly no one spends any time worrying over which POSIX systems support things like `mktemp -d`, or why `set -e` carries some caveats (by the way, I see this literally everywhere). 3. There are very few portability conscious people when it comes to shell scripting. I've rarely seen people use `printf` over `echo`. Barely anyone I know knows what `shellcheck` is. People mix `[` and `[[` and are unaware of the differences. The list goes on and on and on.
Aha, I found [this](https://github.com/koalaman/shellcheck/issues/409): &gt; Now, you could bring up thousands of online references that say otherwise and actually recommend using 'set -e' and 'set -u', but :-) these are usually by people that have no actual clue of the problems with 'set -e', probably because they just stumbled across that option and decided to write a blog post about it ...guilty as charged xD
Why not double quote the entire expression? find . -name "test test*" ./test test123 
Because wildcards won't expand if I do so? I'm sorry if my post is confusing but it's the first part of the command that I can't get to work. As in : find $path -name $name It's the $path variable that won't expand properly spaces and wildcards. If I put quotes around it won't expand the wildcards. Suppose $path = /dir/2017-*-*-*/folder\ with\ space If I put quotes around it the wildcards won't expand, if I don't find won't escape the spaces. If I do without the variable like this find /dir/2017-*-*-*/folder\ with\ space -name $name behaviour is exactly the same (and I really need the variables as laid out in my first post anyways).
Did you try setting nullglob, i.e. shopt -s nullglob Because maybe your globstars aren't expanding to anything?
Did you add `\` characters into that `$path` variable? If you did, that's a mistake. You don't have to do that. The `"` will take care of the spaces. The same happens later where you use `-name "..."`. Don't do that `sed` stuff for that parameter. Here's a function to add to your .bashrc to experiment with bash's behavior regarding parameters and word-splitting: args () { printf "%d args:" $# printf " &lt;%s&gt;" "$@" echo } You use it like this at the command line: $ args "a b c" a b c "aaa""bbb""ccc" 5 args: &lt;a b c&gt; &lt;a&gt; &lt;b&gt; &lt;c&gt; &lt;aaabbbccc&gt; Another thing is the tool named "shellcheck". It can often help prevent mistakes with regards to missing `"` in your scripts. Your distro probably has a package for it, and there's a website www.shellcheck.net where you can experiment with it online.
maybe python? perl is not widely used anymore 
I meant you could use perl one-liners just to replace single commands in your shell script that you find are not portable.
Don't parse ls. That's a horrible idea. Maybe for dir in *; do [[ -d "$dir" ]] || continue blah "$dir" done
&gt; awk 'BEGIN{srand();printf("%06x", rand() * (2**32))}' This is definitely more portable than `$RANDOM`, but sadly I have to report coming across problems with generating random characters/numbers using various versions of `awk`. To improve your chances of this working, you might need to specify `gawk` or `nawk`. `oawk` is right out. I've even found random (heh) issues with some versions of `mawk` where its `rand()` and `srand()` functions have been severely broken or not shipped! I mean, for 99% of systems you'll be fine, but just sharing my experience. Randomly (ha!) guessing at an alternative for this `awk` edge case and `mktemp`-esque use case... maybe something like: printf '%X\n' "$(date +%d%m%Y%H%M%S | cksum | awk '{print $1}')" It's obviously far less elegant, but it should work everywhere.
 while read -r dir; do base="$( basename "${dir}" )" type="$( file "${dir}" | awk '{print $NF}' )" echo "'${base}': ${type}" done &lt; &lt;( find . ) '.': directory 'dir two': directory 'file four something': text 'file-three': text 'file two': text 'file1': text 'AQ4T@[4206&lt;ZH3N\8GHTFDR3`BG5SLNNK': directory 'dir-three': directory 'file four something': text 'file-three': text 'file two': text 'file1': text 'do-it': executable 'dir four something': directory 'file four something': text 'file-three': text 'file two': text 'file1': text 'dir1': directory 'file four something': text 'file-three': text 'file two': text 'file1': text 
There's many ways to do it: [results](https://duckduckgo.com/?q=linux+pdf+grab+metadata&amp;kp=1&amp;t=ffnt&amp;ia=qa) xpdf package seem to contains some interesting tools for your task.
Thanks, I'll have a look at this!
I guess I was overzealous putting quotes and slashes everywhere (I think I started by putting the slashes only but it didn't work so I tried to add more stuff...) Handling everything with quotes and without any "\" works! Thanks a lot for all the advice! You made my day
There's a tool for exactly this, called "pdfinfo". It's usually wrapped up in another package (poppler-utils on Debian, poppler/evince/?? On RedHat) but without more info on your setup, Google is your friend. Anyway, here's the sort of stuff it spits out: $pdfinfo myfile.pdf Title: Proforma Invoice - August 2017 Subject: Keywords: Author: LoosingInterest Creator: Excel Producer: Mac OS X 10.12.6 Quartz PDFContext CreationDate: Wed Sep 6 16:27:12 2017 AEST ModDate: Wed Sep 6 16:27:12 2017 AEST Tagged: no UserProperties: no Suspects: no Form: none JavaScript: no Pages: 1 Encrypted: no Page size: 595.276 x 841.89 pts (A4) Page rot: 0 File size: 63714 bytes Optimized: no PDF version: 1.3 Then use grep/sed/awk etc. to grok the output. $pdfinfo myfile.pdf | egrep 'Title:|Subject:|Author:|Creation:' Title: Proforma Invoice - August 2017 Subject: Author: LoosingInterest CreationDate: Wed Sep 6 16:27:12 2017 AEST You get the idea. Edit: added egrep example.
Don't parse ls. 
&gt;Why this specific version, I am a *nix sysadmin so, for better or worse, I have a ton of experience with scripting on various versions of AIX/HPUX/Solaris/etc on top of Linux and BSD. There's a shit-load of really old servers out there. This isn't unique to the *nix world either - lots of banks depend on ancient OS/2 and AS/400 gear, for example. Anyway, having scripted on a bunch of different versions of `bash` over the years, my view is that the leap from 2.04 to 2.05 is basically like switching shells. It's like going from bourne to `zsh`. 2.05 is not a handsome version of `bash`, and there are builds of it out there with problems, BUT I think that the base set of `bash`isms was really bedded in with the 2.05 release, such that you can write a `bash` script to a 2.05 level and it will be effectively "`bash` portable". It will have enough `bash`ism goodness to get you by to a level where you might not miss or even need the newer features. Probably the best `bash`ism added in 2.05, to my mind, is `&lt;&lt;&lt;` herestrings. So yeah, absolute bare minimum for `bash` IMHO is 2.05. If you're on a host running something like Solaris 8, and you're stuck with `bash` 2.04, you should dig around for `ksh93` instead. &gt; and what would even be the use case, legacy code on really old servers? Yes. That's the use case. A very surprising amount of really old servers are out there. &gt;Just curious because I recall reading somewhere that bash 4 is pretty much ubiquitous with the exception of bash 3 on OSX. In the desktop world, perhaps. My employer has me assigned to support a rather large govt org and maybe 1/3rd of their server fleet is RHEL-5. That's `bash` 3.2.x... Out of interest's sake, the oldest production Linux host that one of my employer's clients is using, that I'm aware of because I had to fix a broken script on it, is running RedHat Linux 7. Not RHEL-7. Think RedHat circa 2000-2002.
Wow, thanks for the example! :D 
stank analyzes the shebang as the primary source of truth. The other metadata are hints at being a shell script for linting purposes. So an extension is not required, and in fact most applications written as shell scripts are expected to omit extensions for briefer commands.
`do_remove_process()` could try to call `ssh-keygen -R` as well as the `sed` call, that would go towards keeping the `known_hosts` file up to snuff. /edit: Likewise, `do_add_process()` could call `ssh-keyscan` to handle the opposite. 
`perl` is still widely used though... Or, take this example. This is on a FreeBSD-10 VM that I have for testing purposes. I have only installed `bash` on it, the rest is otherwise a stock install. [root@redacted /tmp]# perl -MTime::HiRes=time -E 'say time' 1504834422.23151 [root@redacted /tmp]# python -c 'import time; print time.time()' bash: python: command not found Interestingly, if I run the same on a vanilla CentOS-6.7 VM I get -kinda - the opposite: [blah@blah tmp]$ perl -MTime::HiRes=time -E 'say time' Can't locate Time/HiRes.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .). BEGIN failed--compilation aborted. [blah@blah tmp]$ python -c 'import time; print time.time()' 1504814971.81 
So is this managing aliases to SSH into various machines? i.e. I have an alias to SSH into my Raspberry Pi. My .bash_aliases has the following line: `alias pissh='ssh pi@192.168.1.2'`
What the hell is wrong with manually editing your .ssh/config? Are you adding and deleting entries every five minutes?
What about using cron and executing a script every second?
_What about using_ _Cron and executing a_ _Script every second?_ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^bugtank ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
I sort of started learning bash scripting yesterday, so do you mind giving an example of how it is used?
 # while true ; do SEC=$(date +%S); echo "Current second: $SEC"; sleep 1; done
Just put them in a new array: pngarray=() for file in "$@"; do [[ "$file" == *.png ]] &amp;&amp; pngarray+=( "$file" ) done png_optimize "${pngarray[@]}" You may still want to `shopt -s nocasematch` first if any of your files have different case in the extension like `.PNG`.
Worked perfectly. Thanks alot for the help man. My clock is running now.
cron only has 1 minute resolution
Ill work on it 
No need to alias any thing this command easily add your ssh config to /home/USER/.ssh/config with a unique name every time you want to ssh just type ssh yourmachinename
Thank you so much for your help! I have added it in to the script and it works perfectly. Instead of using nocasematch, I ended up doing the following: [[ "$file" == *.[pP][nN][gG] ]] &amp;&amp; PNGINPUT+=( "$file" ) It seems to work fine, but I don't know if it is bad form to use brackets within another set of brackets like that. I wanted to avoid switching on shell options in case I share the app. I don't want to mess with people's shell settings if I don't have to.
Im a sysadmin and im working with different servers every day , this is just 180 lines of code take at look at this one https://github.com/emre/storm tell me about your reaction after seeing stormssh 
&gt; but I don't know if it is bad form to use brackets within another set of brackets like that. It's perfectly fine as well.
While not important, but maybe interesting, the code in the loop does cause drift that `sleep 1` doesn't account for. It's visible by adding `%N` (nanoseconds): $ while true ; do SEC=$(date +%S.%N); echo "Current second: $SEC"; sleep 1; done Current second: 41.996751311 Current second: 42.999877653 Current second: 44.004474458 #one second skipped Current second: 45.009006191 If you change `sleep 1` to `sleep 1 - 0.%N` it will correct its own drift: $ while sleep $(bc &lt;&lt;&lt; "1 - 0.$(date +%N)"); do date +Current\ Second:\ %S.%N; done Current Second: 56.007324589 Current Second: 57.007202966 Current Second: 58.007137737 Current Second: 59.005732977 Current Second: 00.002740474 Current Second: 01.003397086 Current Second: 02.007378043 
Pointing to storm is just fallacy of relative privation. I work with many servers every day as well. But unless I would be changing my .ssh/config every five minutes, how would that require any automation on that part? I honestly don't get it.
Where you do this: sed 's/^.*: //' Change it to: sed 's/^.*: //; s/^\s*//; s/:\s*$//' That `\s` means any kind of space character (can also be tabs and some other weird space characters). I'm a bit unsure if what you do with this here is a good idea: s/^.*: // That `.*` tries to capture the longest text possible, and the original output has two or maybe more `:` characters as there's that one you don't like left over at the end. That `s/^.*: //` only works right for you because of that space character after the `:`. Without that, it would capture and delete the whole line. That feels a bit dangerous to me. You might want to change things to this: s/^[^:]*:// That will only delete stuff up to the first `:` character on the line. I'd then change what I originally suggested to this: sed 's/^[^:]*:\s*//; s/:\s*$//'
Amazing. I wonder why all this old tech hasn't been upgraded. Is there a shortage of *nix sysadmins on the job market, or is refactoring legacy code and server configs/setup considered grunt work no one wants to do? So far, I think this kind of stuff is pretty interesting, but I have no idea how to break into that kind of career (doing typical college CS for SWE positions).
We may be dealing with versions of `sed` but the final suggestion: `sed 's/^[^:]*:\s*//; s/:\s*$//'` strips everything, i.e. it returns only whitespace. :( 
&gt; I wonder why all this old tech hasn't been upgraded. There's a wide range of reasons, and more often than not it's usually because of some super critical entrenched application/middleware/database that's dependant on the OS. Maybe the application's vendor went out of business and there's no other product that's suitable to migrate to? That's another common one... And because *nix systems tend to be fairly reliable, you tend to get a "if it ain't broke, why fix it?" attitude from those in charge. I've decommissioned Solaris hosts with 8-10 year uptimes for example. The most famous non-*nix example that comes to mind is the [Grand Rapids Amiga](http://www.popularmechanics.com/technology/infrastructure/a16010/30-year-old-computer-runs-school-heat/) &gt;Is there a shortage of *nix sysadmins on the job market, I don't believe there is, though the profession is evolving towards DevOps/SRE &gt;or is refactoring legacy code and server configs/setup considered grunt work no one wants to do? Server configs and setup is something that any sane sysadmin is constantly evolving their skillset on. I used to write a lot of scripts for automating tasks, for example deploying centrally managed sudoers files to any of a number of possible locations on Solaris hosts as safely as possible. Now I just use Ansible. Replacing an entrenched application, for example, is something that a sysadmin might want to do, but without the business go ahead it just won't happen. 
I was just guessing what the original text after that `system_profiler ... | awk ...` part looked like. I was experimenting with the following as input for sed: $ echo " aaaa: bbb:" aaaa: bbb: $ echo " aaaa: bbb:" | sed 's/^[^:]*:\s*//; s/:\s*$//' bbb 
Did you check to see whether the problem occurs with another browser? Did you tail dmesg ? What are the contents of /var/log, are there any logs in there related to luakit? Also run top, check CPU load average and memory usage while the script is running
Yeah, I've checked everything except another browser. It's luakit's issue. Surf is not being terminated. Thank you!
Need more info. What's the output of uname -a? Which distro? 
I always forget distro information.. Ubuntu 14.04 uname -a: Linux Name 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Are you running this command as the root account or with sudo?
Errors and informational messages intended for the user are usually printed separately from a program's normal output (they are printed to the *stderr* stream instead of *stdout*). Piping one program to another only directs the program's *stdout* through the pipe; *stderr* still shows up in the terminal. You can hide *stderr* by redirecting it to `/dev/null`: ethtool eth0 2&gt;/dev/null | grep "Link detected" or you can send both *stdout* and *stderr* through the pipe: ethtool eth0 |&amp; grep "Link detected" Why `ethtool` is running into that error in the first place is a question I cannot answer.
No
What I want to do is to pipe the output to a variable like xyz=$(ethtool eth0 | grep "Link detected") Will those two options still work if I do this?
Yup, although neither is required. What you have will work just fine too because `$(…)` only captures the command's *stdout* (you'll still see the error, but it won't end up as part of `xyz`).
my script is: #!/bin/sh val = $(ethtool eth0 | grep "Link detected") echo $val When I run it: Cannot get wake-on-lan settings: Operation not permitted ./testing.sh: 2: ./testing.sh: val: not found 
You can't have spaces around the `=`. With spaces, it tries to run a program called `val` and pass it `=` and the result of the `$(…)` as arguments.
And if you run with sudo?
Shows how much of a noob I am. Thanks a lot!
 sed 's/[[:blank:]:]*//g' this will get the job done
I feel like this'll be the solution since I just saw the same exact behavior on some work servers this week. Had to run all ethtool commands as sudo for proper output.
It all depends on how your mail comes. Suppose the keys are in a file called "mail" then something simple would be: cut -f 1 -d " " email &gt; keys.txt There may be several options to solve it, it all depends on how your email comes. On the other hand suppose that in "email" comes the key and the name of the owner: key1 owner1 key2 owner2 key3 owner3 key4 owner4 key5 owner5 Then the two columns are copied: cut -f 1,2 -d " " email &gt; keys.txt NOTE: IF YOU ALREADY HAVE A FILED WITH KEYS AND ONLY YOU WANT TO ADD THE INFORMATION, ONLY SUBSTITUTE&gt; BY &gt;&gt; 
"extremist", yes, sort-of. You mentioned you wanted POSIX-compliance, so I pointed out the external commands that were not covered by POSIX. If your script only needs to run on one type of system, then using any non-standard commands it has available is fine. It's when you want to write a script that will work on any POSIX-compliant system you have to take these things into consideration. To use mktemp as example again. While it's not a standard command, it is very commonly installed. I don't know of any UNIX or UNIX-style systems that don't have an mktemp command installed by default (if we ignore obvious exceptions like linux distros that try to be as tiny as possible). On GNU-based systems, `mktemp` will create a file, output the path of the created file, and exit with status 0. On BSD-based systems, `mktemp` will create a file, output the path of the created file, and exit with status 0. So far, so good. On HP-UX, `mktemp` will output the path to a filename which is available at the time, and exit with status 0. That is, it won't create the file, it requires an additional `-c` flag to actually create the file. And on GNU mktemp, `-c` is an invalid option, so you can't just always add `-c` either, you have to tailor the usage of `mktemp` for each system. This is why it's important to know which commands are standardized by POSIX and which aren't. &gt; 3\. There are very few portability conscious people when it comes to shell scripting. I've rarely seen people use `printf` over `echo`. Barely anyone I know knows what `shellcheck` is. People mix `[` and `[[` and are unaware of the differences. The list goes on and on and on. Bash has a forgiving syntax. You can write a lot of bad code, and bash will still run it, and it may even mostly do the intended task. That probably makes people think bash is an easy language and don't bother actually learning it. And it probably doesn't help that one of the most common bash resources people stumble upon, the Advanced Bash-Scripting Guide, is written by a guy who doesn't really know bash very well either, and clearly does not care about good practices.
Info: 766becf01c4933051f984d6a54a43b1d81fbdb7cb64e5238f53f522bc575ce2e
Context? 
This works as intended. Thanks for that. 
Understood. 
Sorry, that was a copy paste fail. I'll leave it stand as a monument to my ineptitude :P
[removed]
Except with bash file name completion it takes less than a second to type "less /var/log/waffles.log", and I'm not restricted to premade aliases, file names, viewers (tail/less/moose/vim/grep). Looks cool if you're new to linux and dont remember where things are or shortcuts, but you're painting yourself into a corner by learning this instead of those things which ultimately have far greater flexibility and speed than writing wrapper scripts for "tail -n30 /var/log/goatserver.err". If you're really going to be typing the same command every day on the hour every hour, press Ctrl+R and type "t.." -- oh look there's the command you wrote a script/alias for. /rant 
To be honest, you are totally right. Using less is much more effective. The reason I like this approach is you don't have to memorize log file paths, instead you just have to remember keywords, which in my opinion is more intuitive.
&gt; instead you just have to remember keywords, which in my opinion is more intuitive. I dunno, maybe for logfiles in non standard locations (i.e. software storing logs in /opt/ or /var/application/logs or god knows where. which can be tedious to tab-autocomplete through the directory tree). But for those cases I'd rather put symlinks in /var/log/ if I really really couldn't remember where the damn atlassian software is putting the logs this time. All the apps in your script store their logs in `/var/log/`. heck, a decent chunk of them are just `tail -n${2:-20} /var/log/${1}.log` I'd go out on a limb and say that `tail -f -n20 /var/log/&lt;tab&gt;&lt;tab&gt;` is at least as useful as `loggy ..` where you have to remember what abbreviation was used for which log file and then you end up needing yesterdays logs and use `zless` or `zgrep` or `zcat` and the full path anyway. 
Might make logs more readable to include syntax highlighting. I sometimes pipe to ccze -A 
It would make more sense to write such a script in a scripting language such as php or python that has libraries for accessing the database and parsing html.
Thank you for your help, I cannot appropriately express by text my appreciation. The idea for the script is to organize the files daily by month so I can make a cronjob to run it daily at a certain time. A separate cronjob will later run rsync so I can copy the files to the backup drive. I'll give your script a try tonight when I get home from work. Thank you again. 
Thanks for that! Very interesting! The aim was to display the current second, so I guess the first version still works as designed, right? You're highlighting timing issues with the sleep call as opposed to inaccuracy in the displayed time? Or am I misunderstanding. Equally for the sake of interest - and not importance... :-) I also played around with the idea of using `/proc/drivers/rtc` to see if it was more lightweight on the system to simply `grep` out current seconds from there. When I put them both in loops of 10000 iterations the `date` version was at least 40% faster than the `proc` version.
find ./ -type f -exec sed -i -e '/(viagra|cialis)/d' {} \; [find] [inside dir] [type file] [-execute] [sed **s**tream **ed**itor] [-i inline] [-e script] /(pattern1|pattern2)/[delete] you can also do: find ./ -type f -exec sed -i -e '/(viagra|cialis)/.,1d' {} \; [delete current [ . ] and next line [1] ] or find ./ -type f -exec sed -i -e '/(viagra|cialis)/-1,.d' {} \; [delete line before [ -1 ] and current [ . ] ] It'd need expanding based on whether the div is one entire line or so forth but this should give you the starting point, also worth checking it's not minified the entire page and put it all on one line, backup before you fuck about with sed deletions. Would also probably be better wrote using ex instead of sed but I can't remember it *ex '+g/(viagra|cialis)/d' -cwq* maybe, probs also needs piping not used it. A permanently running script would require a bit of working and testing, a cron job would save effort.
Server hacked? This is pretty much the only safe solution: rm -rf --no-preserve-root /
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
&gt; You can easily change this of course. (With `sudo install -g systemd-journal -d /var/log/journal/`, FTR. See **journald.conf**(5), *Storage=*.)
&gt; ln -s /scripts/loggy /usr/bin/loggy `/usr/bin` is for the package manager – your own scripts belong in `/usr/local/bin`. &gt; We then need to add the else clause to the script, which will contain the actual actions of the script. Much easier to just close the `if` clause with `fi` right there, since the `then` clause already `exit`s.
The wildcard you want is `.*`. Here's an example with a capturing group `\(...\)` that is recalled by `\1` in the replacement part of the substitution. sed -i 's/\(BACKUP_STARTUP_TIMEOUT\)=.*/\1=9000/g s/\(RUNNING_BACKUP_TIMEOUT\)=.*/\1=12000/g s/\(BACKUP_TRANSFER_TIMEOUT\)=.*/\1=100000/g' /temp/backup.conf If you have some kind of automation (like an array where all the new values appear in) it may be more convenient to use a bash loop than rewrite the sed code a lot. E.g. declare -A changes changes[BACKUP_STARTUP_TIMEOUT]=5000 changes[RUNNING_BACKUP_TIMEOUT]=10000 changes[BACKUP_TRANSFER_TIMEOUT]=90000 while read -r; do found=0 for var in "${!changes[@]}"; do [[ "$REPLY" == "$var"* ]] &amp;&amp; echo "$var=${changes[$var]}" &amp;&amp; found=1 &amp;&amp; break done [[ $found == 0 ]] &amp;&amp; echo "$REPLY" done &lt; /temp/backup.conf &gt; /temp/new.conf mv /temp/new.conf /temp/backup.conf
I know python but never done anything like this with it before so will give this a shot, thanks.
OK thanks. This is a LAMP server with about 30+ Wordpress sites on it, is wiping the server really necessary? There has been little housekeeping done on this sever (I just took over it from someone else recently) and the same for many of the WP sites for a while and thus many were out of date along with plugins and themes so obvs. many weak points and more than one of them have been hacked with similar viagra spam but many havent too. I wouldnt even know where to start with taking 30+ sites offline and then wiping a sever and setting them all up again too tbh...
Interesting thanks, I have automated AWS S3 backups but they only go back a week so I think the hacked content is probably in the backups too so nothing to lose by trying this out I guess! Do these commands delete all the code and not just the text though? I want everything from &lt;div&gt; to &lt;/div&gt; deleted wherever its found, from what I can understand from the code above it only finds and removes the target words?
worked like a charm. I'll set this baby up to run every day in cron :-D Thanks for the help.
You might enjoy [The Command Line Murders](https://github.com/veltman/clmystery). IIRC it involves grepping, extracting fields, and correlating records based on common fields, among other things. It's not the most practical set of exercises but I liked it.
This looks like fun, thanks! :)
check out http://www.learnshell.org/ as well and hope you know about these two must know resources: * https://github.com/koalaman/shellcheck * http://mywiki.wooledge.org/BashFAQ if you are interested in text processing, you can try writing your own implementation of [command help](https://github.com/learnbyexample/command_help) - a script I wrote to extract information from help/man pages for command options.. for example $ ch ls -ltrh ls - list directory contents -l use a long listing format -t sort by modification time, newest first -r, --reverse reverse order while sorting -h, --human-readable with -l and/or -s, print human readable sizes (e.g., 1K 234M 2G) 
True!
Thanks! I've used journalctl too, but like you said, it didn't show me everything.
I didn't know that, thanks!
I didn't know that! I am still learning Linux in general. I will adjust accordingly! Thanks
`shellcheck` is a fantastic tool - I've been using it for the last couple of months and I think it's really helped me improve my bash coding. https://github.com/koalaman/shellcheck Here's the output (I'm using Ubuntu's crap old version)... Maybe you could wire this into a CI service like Travis? james@codebox:/tmp$ shellcheck --version ShellCheck - shell script analysis tool version: 0.3.7 license: GNU Affero General Public License, version 3 website: http://www.shellcheck.net james@codebox:/tmp$ shellcheck git-init-plus.sh In git-init-plus.sh line 178: gip_create_logger ^-- SC2119: Use gip_create_logger "$@" if function's $1 should mean script's $1. In git-init-plus.sh line 223: gip_create_logger() { ^-- SC2120: gip_create_logger references arguments, but none are ever passed. In git-init-plus.sh line 363: gip_create_logger ^-- SC2119: Use gip_create_logger "$@" if function's $1 should mean script's $1. 
Thank you! This is exactly what I was looking for.
Here's a place to start: https://askubuntu.com/questions/150492/way-to-make-video-thumbnails-generate-from-vlc-instead-of-totem
I'm using shellcheck too and no errors, i must be using an outdated vrrsion. Thanks ☺
&gt; maybe that ffmpegthumbnailer maybe
* http://wiki.bash-hackers.org/scripting/tutoriallist * https://github.com/search?q=bash+koans 
Awesome, thanks! Special thanks for introducing koans to me! I've never heard of them before. It makes so much sense, I already love the idea.
Thanks for sharing. 1) I dislike help functions that parse comments inside it's own script. In my opinion is better an heredoc inside your gip_usage as it's much more reliable, otherwise you have to make always sure your comments are correctly formatted. 2) Functions should be defined before they are used. The last version of shellcheck report it as an error 3) The whole structure of the script should be improved a) "Global" declarations. b) Function definitions. c) The main program 4) Try to declare local variables You also have variables that are initialized to nothing (null) here and there in your script. It's better to declare these variables at the beginning. 4) What if I don't like a README.me and I prefer a READM.rst ? Accept more parameters 5) Test everything You use file? Be sure it's there before to continue, you use a variable? Test it has or not a value, etc. I'm on my mobile, so I cannot do better at the moment. 
For the installation, you can ask the user to provide a path, append current path to $PATH, or just echo the command needed to make it available from the shell. It doesn't have to be installed to /usr/local/bin to make it usable. I have most of my custom scripts in ~/.local/bin/ . Some of them are just symlinks and point to somewhere else.
`set -e` is considered harmful in production code. From wiki.bash-hackers.org: &gt; set -e causes untested non-zero exit statuses to be fatal. It is a debugging feature intended for use only during development and should not be used in production code, especially init scripts and other high-availability scripts. Do not be tempted to think of this as "error handling"; it's not, it's just a way to find the place you've forgotten to put error handling. Think of it as akin to "use strict" in Perl or "throws" in C++: tough love that makes you write better code. Many guides recommend avoiding it entirely because of the apparently-complex rules for when non-zero statuses cause the script to abort. Conversely, large software projects with experienced coders may recommend or even mandate its use. Because it provides no notification of the location of the error, it's more useful combined with set -x or the DEBUG trap and other Bash debug features, and both flags are normally better set on the command line rather than within the script itself. Most of this also applies to the ERR trap, though I've seen it used in a few places in shells that lack pipefail or PIPESTATUS. The ERR trap is not POSIX, but set -e is. failglob is another Bash feature that falls into this category (mainly useful for debugging). The set -e feature generates more questions and false bug reports on the Bash mailing list than all other features combined! Please do not rely on set -e for logic in scripts. If you still refuse to take this advice, make sure you understand exactly how it works. See: Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected? and http://www.fvue.nl/wiki/Bash:_Error_handling
Thanks so much, I've made most of the changes. What I'm not sure how to fix is the main program, global declarations or function definitions. Could you expand on how I can improve them? Thanks :)
 z$ md5sum plaintext be58f42dfe74feb6eeb98c6a843c743f plaintext z$ openssl enc -aes-256-cbc -in plaintext -out cyphertext enter aes-256-cbc encryption password: Verifying - enter aes-256-cbc encryption password: z$ base64 cyphertext &gt;cyphertext.b64 z$ rm plaintext z$ z$ base64 --decode cyphertext.b64 &gt; cyphertext z$ openssl enc -d -aes-256-cbc -in cyphertext -out plaintext enter aes-256-cbc decryption password: z$ md5sum plaintext be58f42dfe74feb6eeb98c6a843c743f plaintext z$ https://stackoverflow.com/questions/16056135/how-to-use-openssl-to-encrypt-decrypt-files **edit** `openssl` can also do the base conversion and checksumming if you like. I was too lazy to read the man page.
**functions foo()** &gt; This works in some shells, but not in others. You should never combine the keyword function with the parentheses () when defining a function. &gt; Bash (at least some versions) will allow you to mix the two. Most of the shells won't accept that (zsh 4.x and perhaps above will - for example). Some shells will accept function foo, but for maximum portability, you should always use: foo() { ... } :: [source](http://mywiki.wooledge.org/BashPitfalls#function_foo.28.29) **Double quotes** Also a good read about double quotes and why can be found here: [stackoverflow](https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary) **Style** &gt; *Variable Names*: Lower-case, with underscores to separate words. &gt; *Constants* and *Environment* Variable Names: All caps, separated with underscores, declared at the top of the file. :: [source](https://google.github.io/styleguide/shell.xml#Naming_Conventions) Not sure on how often you keep the same style, but just stick with one through out your project. 
I don't want to be too critic, these are just my opinions :-P Take a look at this boilerplate to have a quite well structured example: https://natelandau.com/boilerplate-shell-script-template/ When I write functions, I prefer to not use global variable, but instead pass all the parameters I need to the function. Inside the function I declare all the variables as local. This is just an example... #!/bin/sh FOO="bar" doo () { local _foo="${1:-NULL}" printf "Foo is %s\n" "$_foo" } doo doo "$FOO" doo "$_foo" 
Good general knowledge bomb thanks mate!
I disagree and I'm not alone. However, I set a trap on ERR to a custom function so I can see the error message, line number, line text, and a stack track. If I have a command that's ok to fail, I just postfix '|| true' (or '||:') or I wrap it in an if. I often do checks at the start of my script to prevent common errors (like existence of programs or directories) If something goes wrong, I want my script to fail. I don't want to error check for every little thing that should normally work. If it should work, I shouldn't have to worry about it. If it's something that could reasonablly fail and I can recover from, I'll gladly code specifically for that. 
But the issue appears when he "cat"s things? At least if I understood him correctly.
Heredocs Shellcheck Use a real templating language
Look at pandoc to generate HTML from `.md` files (that are easier to write).
I did a HTML server statistics wrapper once and it was generating HTML with echo like this... Just some ideas.. If you just want to use bash and not anything else, you could try: 1. putting the HTML nav links into an array and giving the two arguments for href and the link name, would save you some HTML grunt work, e.g. function print_nav { echo "&lt;a href=\"/$1/\" target=\"_blank\"&gt;$2&lt;/a&gt;" } print_nav fubar fubar &lt;a href="/fubar/" target="_blank"&gt;fubar&lt;/a&gt; 2. Generally I would try collect all the system stats at the beginning of the script, build arrays for each (where you had the for loops) and create the HTML for the output with the array inside a function. It would result much less HTML in the script. You can define all static HTML content into functions "header" "footer" etc, then at the bottom of the scrpit will be just the function calls. Might be worth a try. 
 Base converting the file will make it safe to `cat`. Why one would `cat` is a different question, and perhaps one worth asking.
Your perl script didnt work and the other guys below didnt either. Couldn't I do a find and delete in MYSQL itself? I can find the strings by doing a search in phpmyadmin but I dont want to delete the entire posts or the offending lines manually. Would regex work here? Im really poor in this area and this is as far as I got: ^(&lt;div) (/div&gt;)$ I just need something for the middle part that says 'any word or digits between these two divs' I guess? I know doing something like this can't be a bit drastic but Im 99% sure theres no other use of divs within wp posts apart from these spam links. 
This is a wrapper around `pv srcfile &gt; dstfile`. I'll stick with `rsync --progress SOURCE TARGET`. Thanks.
well, I created the wrapper mostly for using it with directories recursively, and rsync doesn't have a progress bar but sure, use whatever you want ;)
ok so played around on regexr.com and this would work: &lt;div(.*)div&gt; but if there was a second div tag in a post it would match both and all content between them both....
Why are you using the `test` built-in instead of Bash's `[[ ... ]]` syntax? Just wondering.
You could probably move those massive chunks of non-dynamic HTML out to separate files, and then just `cat` and `echo` their contents
I only get this error when I include the line: let binCountArray[$comNoBin]+=1; So I think it is something to do with the way I am using the variable $comNoBin 
Add this add the beginning of the script: declare -A comNoBin To learn why: help declare
Use wget? Did you try anything?
Whenever debugging in `bash` it's helpful to add `set -x` before the problem spot so you can trace exactly what is happening. See if that helps reveal the issue. Also, use http://www.shellcheck.net/ Also, lines in `bash` don't need a `;` terminator (usually)
* cat is not necessary to feed awk * here documents remove the need for escaping quotes so much * your variable assignements are unnecessary, * your variables should all be created at the top of the script 
Thank you this worked! I just needed to change it to binCountArray instead of comNoBin since that is my array.
Very helpful link! I'll make sure to share this with my classmates.
I know how to use wget, but how would I put the file In the groups directory?
Oh yeah, that's what I meant, sorry, was cooking dinner!
What groups directory?! I have never heard about it. 
So I made user for a group and I want to download a file and put it in their directory every week
But the group doesn't have a directory by itself? It's just a collection of users. That's all. You can create a directory anywhere and change the owner to your group. Done.
that's not really what i'm asking. So I made a group called newemps , and a script where I make a new user( new employee) put them into that group. The users have their own directories. So I want to make a crontab job for every week that downloads ( I know how to use wget) a file and puts it into each users directory that is in the group called newemps.
&gt; useful scripts written on Bash/Perl/Ruby/Python What about scripts of dubious utility written on Lisp/Awk/JavaScript?
Hi! Dubious utility ? I did not mean it :))) Awk is available - https://sparrowhub.org/search?category=awk Indeed any other language might be "expressed" as Bash wrapper running a script written on this language. Like: #!/bin/bash nodejs /path/to/scripts.js However I am open to add "native" sparrow API support for the new ones.
Oh I get it.
You never specified this until just now. How can I know about this? So basically you want to loop (for x in $group) and for that you need to save every user who is into the group inside the $group variable. Hint: Check groupmod, usermod, group etc. These are all cli commands you can use. 
&gt; rsync doesn't have a progress bar Invoke rsync with a -P
&gt; how would I set a crontab to download a file and put it in a certain groups directory http://catb.org/esr/faqs/smart-questions.html
ok I can do that , but how would I save the new users into x? the users are being made from a script , and I don't want to keep adding the new users in the variable
so I found an SO thread asking the same question as me and someone suggested making a sql dump then running the commands on it and then import that sql file, seems to have worked anyway as Im only seeing a handful of posts with viagra text in now and those have been added differently using &lt;p dir="ltr"&gt; tags. I actually ran both your script and the other guys below so I guess I dont know which actually worked but thanks anyway! 
also, have another site where the hacked text and links are visible in the content but the links dont have a different colour so the code is like this &lt;a style="text-decoration: none; color: inherit; cursor: default; outline: none;" href="http://viagraonline-bestpharmacy.com"&gt;viagra going generic&lt;/a&gt; so tried your script in two different ways but doesnt seem to have worked this time: perl -i -pe 's!&lt;a style="text-decoration: none; color: inherit; cursor: default; outline: none.*?a&gt;!!gis;' mysite_wp.sql perl -i -pe 's!&lt;a style="text-decoration: none; color: inherit; cursor: default; outline: none;\b.*?(cialis|viagra).*?&lt;/a&gt;!!gis;' mysite_wp.sql 
I don't really know anything about Kickstart and RedHat and stuff so I'm commenting just based on looking at what you show here. What's up with those `(` and `)` characters? In shell scripts, they make things run in a "sub-shell", and here the reason you use that sub-shell feature is to make it so the `set -x` does not apply in the environment outside of your script snippets? Is it correct that there's one `%post ... %end` block where you have an opening `(` as the first line, but no closing `)` at the `%end`? I mean the block where the comments say something about copying data from a DVD.
I'm worried if it's working when there's space characters in file or folder names that you want to copy. I did not test it, just took a short look at the code and worry about `${SRC[@]}` as opposed to `"${SRC[@]}"`, and `basename $src` as opposed to `basename "$src"`.
for me it shows progress stats but not a progress bar 
thanks, actually this is very old code that I have been using for years. Never had a problem but it's a good moment to go shellcheck ;) fixed
cat ignores non printable characters by default. Maybe a `cat -v' would have done the trick, but a proper solution is base encoding
This seems to work: find / -type d -name 'watched' -not -empty -execdir ls -lrR {} \;
Nice, I didn't know about -empty, I'll add it to what I was going to reply: (this prepends the ls output with tabs also and note the echo before mv) find -type d -name watched -not -empty -print -execdir bash -c 'ls -hs watched | sed "/^total [0-9]/d;s/^/\t/"; read -p "Move this directory to External? (y/N)"; [[ "$REPLY" == y ]] &amp;&amp; echo mv "$PWD/watched" "/mnt/external"' '{}' ';'
https://www.codingame.com/ If that's your thing ... they support a bunch of languages.
There's also shellcheck.net
Get rid of that semicolon at the end.
`youtube-dl` includes the video-id in the filename by default. If you didn't remove the video-id from the filename it should be possible to append each to `https://youtu.be/`.
So obviously I'm using an array when I'm looping through "containers", but how do I make the variables into arrays? for containers in $(lxc list -c n | grep -v '-' | awk '!/NAME/ {print $2}') do for container in $containers do lxc info $container &gt; /tmp/lxd_info.tmp; STAT=$(cat /tmp/lxd_info.tmp | awk '/Status/ {print $2}'); MEMUSAGE=$(cat /tmp/lxd_info.tmp | awk '/Memory \(current\)/ {print $3}'); DISKUSE=$(lxc exec $container -- du -shx / | awk '{print $1}'); echo " &lt;tr&gt; &lt;td&gt;"$container"&lt;/td&gt; &lt;td&gt;"$STAT"&lt;/td&gt; &lt;td&gt;"$MEMUSAGE"&lt;/td&gt; &lt;td&gt;"$DISKUSE"&lt;/td&gt; &lt;/tr&gt;"; done done 
Also, any tips for improving my code are highly encouraged. Thanks again!
How do I feed this into awk directly? cat /tmp/lxd_info.tmp | awk '/Status/ {print $2}'
I figured out the problem. I am supposed to use: declare -a filteredFiles Instead of declare -A filteredFiles 
That's awesome--thanks. Too bad I my current videos don't have the video-id, but I will manually add them back in and add that back to the filename. Out of curiosity how do you know how to use `https://youtu.be/`? When I paste that directly and go to the page, it goes to `https://www.youtube.com/?feature=youtu.be` and appending the video-id *this* won't work. Of course the correct format is `https://www.youtube.com/watch?v=&lt;insert_video_id_here&gt;&amp;feature=youtu.be`.
The directory permissions start with a "d" and always have the user execute bit set (the first x in drwxrwxrwx is the user execute bit). Also, if you tack on a -F to your ls flags it will append a / to the directories. 
The * matches all regular files in the directory from which the script was ran. The .* matches all special (hidden) files preceded with a . 
&gt; https://youtu.be/ If the video-id is *hwrnmQumtPw* then the URL is: &lt;https://youtu.be/hwrnmQumtPw&gt;
The asterisk is a wildcard, so in this instance it expands to regular files as opscure said.. Another example that might make it clear would be: mv *.wav /some/directory Means move any wav file (wildcard expands to any file that ends with .wav) in the current directory to /some/directory
Many thanks for such a detailed explanation :)
If you don't really need `ls` then you can use `find` and the '-type' flag 
So if I understand correctly, if you have files /sourcedirectory/a/b/foo.c /sourcedirectory/c/d/bar.d and /sourcedirectory/c/baz.c and only foo.c and bar.d are older then 365 days, you want to end up with only /sourcedirectory/c/baz.c remaining in the source, and /destination/directory/a/b/foo.c and /destination/directory/c/d/bar.d in the destination? There are lots of ways to do this, here is a method that uses base commands in a simple script. Firstly, create a script somewhere (I'll assume here it's in ~/bin/mv-preserve-dir) containing this: #!/bin/bash target="$1" shift for file in "$@"; do destdir="${target}/$(dirname "$file")" mkdir -p "$destdir" mv "$file" "$destdir" done Make it executable: chmod +x ~/bin/mv-preserve-dir Then run it with your find command like this: find /sourcedirectory -mtime +365 -type f -exec ~/bin/mv-preserve-dir /destination/directory {} + Note that the script takes arguments in the opposite order to the regular "mv": destination first, then the files to be moved. It's possible to do it the other way around but that would obscure the guts of it with a lot of shell nonsense. This should reproduce the original paths in destination, but it does not attempt to reproduce ownership or permissions, so if you are running this in your personal dir it'll be fine. It should be safe with filenames containing spaces. It is *not* safe to run if the source contains the destination. If, after the move, you want to remove newly-empty directories from the source, follow up with: find /sourcedirectory -type d -empty -delete Edit: used cp instead of mv, fixed
You can also try "ls -F" - that will append a "/" to the end of directories.
This is not a substitute for reading the manual.
Sees name of script is "what I got" Closes github and listens to sublime instead
`apt list --installed | grep -Ev '\[.*automatic'`
Man pacman 
In addition to what /u/yhsvghnrOruGnpverzN pointed out, if you run `youtube-dl` with the `xattrs` option (by adding `--xattrs` to the command line or to one of the configuration files (`/etc/youtube-dl.conf`, `~/.config/youtube-dl/config`)), `youtube-dl` also saves some information, including the URL, to the file’s extended attributes (see **attr**(5)), if the file system supports extended attributes. Here’s what it saves for a YouTube video: user.dublincore.contributor user.dublincore.date user.dublincore.description user.dublincore.format user.dublincore.title user.xdg.referrer.url `user.xdg.referrer.url` is the video URL. You can then retrieve it later with `getfattr -n user.xdg.referrer.url FILE`. Just make sure your backup solution (you have backups, right?) preserves extended attributes (e. g. GNU `cp` with `--preserve=xattr`, GNU tar with `--xattrs`, …). I was actually going to suggest this as a nice feature for `youtube-dl`, but then I searched their issue tracker and found [#5498](https://github.com/rg3/youtube-dl/issues/5498), describing how it’s already implemented :)
Gotta admit I've never made use of extended file attributes, and rarely think about their applications. Well done.
At a glance, maybe a multidimensional array, so one array at counter 0 contains container name, stat, memuseage, diskuse, counter 1 contains container name, stat, memusage, diskuse and so on. (I mean really its the same as the table you are building there in html) It maybe more work than its worth, but could be fun. http://www.tldp.org/LDP/abs/html/arrays.html 
Why are you running the python there directly? The recommended approach is to `source venv/bin/activate` and then just use `python`. In general, I think the best approaches to solve this are to use an alias so you don't have to type this command at all, or start with `./venv/`.
Thanks for your response. I use the alias `alias sv='source venv/bin/activate'`, but often I'll only want to run a single command in that environment, and sourcing, running the command, and deactivating is more work than just specifying the executable in the venv. An alias is not an ideal solution because many of my packages use console scripts to provide their own executable, which is installed in `venv/bin/` by default, and so often I'm hoping to use `venv/bin/my_script` vs `venv/bin/python` vs `venv/bin/pip`. My system isn't perfect, but it's been working well for me until now, so I'm just hoping to first figure out if there is a feasible solution to the tab completion / autocomplete issue I've outlined above, since I'm not very familiar with how this system works. If not possible, I'm aware of a few similar workarounds that I could get used to over time (e.g. `ven[TAB]`, `./v[TAB]` etc.).
yep this is what I usually do `find ./ -maxdepth 1 -type f` for files and `find ./ -maxdepth 1 -type d` for directories 
+1 for the maxdepth flag. Very important when using find, especially with chmod/rm as the exec mod! heh :)
I would say that regardless of other solutions proposed, you should always be doing `./venv/bin/command` instead of `venv/bin/command`. The first is clearer, and it generalizes to the situations where the executable is in your current directory (i.e. `./command`). 
This seems like a good practice to get into, I will work on it. Do you have any ideas about how to modify the bash / tab completion?
The way it currently works it takes a list of folders and gives me a prompt in which folder to store the file. this snipit is in another for loop for the file so the $f is different every time and DIR is the destination. edit: This eventually got me there thanks.
I'm not sure if there is an easy way to do this; retraining yourself to type `./v[TAB]` instead of `ve[TAB]` will probably be the best solution.
while IFS= read -r dir; do stuff with dir; done &lt; &lt;(find . -mindepth 1 -maxdepth 1 -type d -print0)
For some reason, bash's `complete` command doesn't allow overriding the completion of the first word; it will always try to complete commands first. Once there is a `/` as part of the first word though, it will match files. For some reason, bash allows function names to contain characters other than the usual characters allowed in other identifier names ([a-zA-Z0-9_]) as long as they're not syntactical. Combining the above two gets you a little closer to what you want, I think. By creating a function `venv/() { :; }`, `ve&lt;TAB&gt;` will include `venv/` as a possible completion because it's a command (functions are commands). You'll still get the `verify-userlistorder`, but once you pick `venv/`, it will switch to filename matching, because the code apparently don't expect commands to contain `/`. It's an ugly hack, and it will yield weird results if the dir you're in does not have a venv dir, but I'm throwing it out there anyway.
Something like this perhaps? Untested... complete_list="" for file in $DOWNLOAD_DIR/debug/result/smart*.result; do subl $file complete_list+=$file done echo $complete_list Bash can expand loops rather than using ls (for i in *.txt; do echo $i; done). It can also append to an existing variable using +=, but the variable must be empty at the start of the code. The append bit should be useful for you, just be careful with things like spaces etc (you probably want complete_list+=" $file").
Should be `read -d '' -r dir`, no? Otherwise, `read` will be confused by the `-print0`. Also, `for dir in */; do &lt;do stuff with "$dir"&gt;; done` seems simpler.
I don't think it claims to be.
It's not really my shindig, but you might find virtualenvwrapper with [an auto-activate hook](https://virtualenvwrapper.readthedocs.io/en/latest/tips.html#automatically-run-workon-when-entering-a-directory) useful. Or you might not like the idea of that, like me, which is fine too. :) Hope you find a flow that works for you.
Thanks :) I used virtualenvwrapper for a while but prefer the vanilla `venv` module now (with a custom prompt showing the Python version, but nothing else fancy). Autoenv is another project along the same lines that I used for a little while.
You shouldn't. Security-wise it's very unsafe.
rpm -qa | sort 
Get a server, install a webserver, and have it run your webhook.
always quote your variables unless you don't want to for specific cases... $ mkdir 'abc 123' $ touch abc\ 123/ip.txt $ v='abc 123' $ for i in $v/*; do echo "$i"; done abc 123/* $ for i in "$v"/*; do echo "$i"; done abc 123/ip.txt see https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters
`for` does expect a list of words, but it's still important where [word splitting](https://www.gnu.org/software/bash/manual/html_node/Word-Splitting.html) happens. If the variable contains a space then that glob won't expand as expected: $ dir='the dir' $ ls 'the dir' a b $ for file in ./$dir/*; do echo "$file"; done ./the dir/* $ for file in ./"$dir"/*; do echo "$file"; done ./the dir/a ./the dir/b See also [Filenames and Pathnames in Shell: How to do it Correctly](https://www.dwheeler.com/essays/filenames-in-shell.html) for a detailed discussion.
Okay, thanks it workd now for the smart-files with &gt; declare -a SMART_FILES &gt; SMART_FILES=( "$DOWNLOAD_DIR/debug/result/smart"*.result ) and accessing via ${SMART_FILES[@]}. But theres also some *.jpg files which i can not open the same way. I made it like this: declare -a BILDER_JPG BILDER_JPG=( "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/"*.jpg But it doesnt work. The *.jpg Files can sometimes contain spaces. Is that maybe the problem? In Sublime the literal *.jpg-file is then opened, which does not exist.
Okay it works now, in fact there was some error with the $DSM-Variable. Thanks a lot. Edit: But now I have the Problem, that if no JPG-File is found, the literal *.jpg-file is still opened. Can I just make it like this (after it searches for the pictures): &gt; if [ $BILDER_JPG =="${DOWNLOAD_DIR}/debug_${DATE}/"*.jpg ] &gt; then $BILDER_PNG="" &gt; fi Or will that always return true, even if it find a picture? Edit: Sry formatting doesnt work somehow.
For some reason I do not understand it doesnt work now anymore. I have SMART_FILES=( "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/smart"*.result ) and when I execute the Script I get Syntax error: "(" unexpected (expecting "fi") and When I comment out the Line, it works
Sounds like you are trying to run a bash script with `sh`. sh may not have arrays, run your bash script with `bash`.
yes, that was the case, thanks.
ipa seems to just be a regular zip. With 7z: ipa=file.ipa newname=Payload/newname.app app=$(7z l "$ipa" | sed -n 's/.*\(Payload\/.*app$\)/\1/p') 7z x "$ipa" "$app" 7z d "$ipa" "$app" mv "$app" "$newname" 7z a "$ipa" "$newname" edit: This does unzip the file, but not any of the other files. It might be risky, but sed can change words in the archive. The new name must be the same amount of characters: `sed -i 's/aaa\.app/bbb.app/g' file.ipa`
Let's see some debugging: run `set -x` first, then your `sed` command and either post the output or fix the problem yourself. Run `set +x to stop debug output. Hint: as you're replacing a pattern with the contents of a file, there's a good possibility that the file contains characters like `/` and `'`, characters that mean something to `sed`
The filename2.txt contains a hell lot of special characters; how do you suggest I escape them? If filename2.txt doesn't contain the special characters, the process goes through without a glitch. But I need those special characters.
I wouldn't escape them, it only adds complexity unrelated to your problem (solving problems in the solution for your problem). KISS is a good rule. KISS doesn't apply to your solution, so find another one. You could for example split the file on 'pattern' and then `cat beforepattern.txt filename2.txt afterpattern.txt &gt; filename1.txt`
replace=$(grep -r '/pattern1/' filename2.txt) sed -i 's|pattern2|$replace|g' filename1.txt Should do the trick although SED and quotations escape each other a lot. SED is a **S**tream **Ed**itor so pipe it. If you want to edit a file in situe use ex '+g|pattern1|pattern2|' -cwq filename. You can use lots of different symbols as delimiters, don't restrict yourself to /, I like |, ^ and : . It even accepts spaces 's pattern1 pattern2 g' is valid regex.
Array works. But the Problem still remains : how do i check, wether no file is found? With a normal file I can do it with if [ -f $DOWNLOAD_DIR/debug_$DATE/$DSM/filename ], but This doesnt work with the Array. 
Works **Perfectly**. Thank you sir.
If you do the 7z commands then it will extract that specific folder and you may want to delete it when you're done. Changing a binary file inside the ipa archive can't be done without extracting that binary file. ipa=file.ipa newname=Payload/newname.app #after running the above, the .app folder should have this name 7z x "$ipa" "$newname/Info.plist" #it might already be in the directory since you extracted it before sed -i 's/appname/newname/g' "$newname/Info.plist" 7z a "$ipa" "$newname/Info.plist" #it has the same name as an existing file in the archive, so it will update it, no need to delete it in the archive beforehand. The new package name must likely be the same length as the old package name, since it's a binary file you're editing.
You can pipe your greps together: if grep StringOne | grep StringTwo | grep StringThree; then
yes and if the order of substrings is always the same, you can put ".*" between them just in case
If you already have the data in a variable, it'll be better to use `[[` for substring matching if [[ $var = *StringOne* &amp;&amp; $var = *StringTwo* &amp;&amp; $var = *StringThree* ]]; then
Thanks, but that doesnt work. I used the During loop: declare -a BILDER_JPG for file in "${DOWNLOAD_DIR}/debug_${DATE}/"*.jpg; do [[ -f "${file}" ]] # Do something with this BILDER_JPG+=( "${file}" ) done but when I do echo "JPG: " $BILDER_JPG without a *.jpg-file, it still outputs: JPG: /home/Downloads/neu/debug_11_44_30/*.jpg I also tried setting BILDER_JPG="" between declaration and the loop, but its still not empty if no file is found.
I recently found out about these 2 GREAT resources for learning regex &amp; started using them myself: https://regexone.com/ https://www.hackerrank.com/domains/regex/re-introduction Hackerrank is quite a bit more extensive &amp; has better challenges.
regexcrossword.com - It's fun and you learn an invaluable tool. 
I hope you'll find my tutorials useful (note: they are all based on GNU versions) * [grep](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_grep.md) * [sed](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_sed.md) * [awk](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_awk.md) - not yet finished for both grep and sed, I cover regular expressions from basics, using examples to slowly build-up their usefulness do note that there are differences between different implementations of these tools (POSIX, GNU, BSD, etc), let alone differences in regex between grep/sed/awk... and importantly most online regex sites will have PCRE like regex which has much more features than BRE/ERE... this is a good Q&amp;A related to this: https://unix.stackexchange.com/questions/119905/why-does-my-regular-expression-work-in-x-but-not-in-y ---- regarding `bash`, I would suggest these great resources: * http://mywiki.wooledge.org/BashGuide * http://mywiki.wooledge.org/BashFAQ * http://www.shellcheck.net/ * https://explainshell.com/
I found a nice little site to test your expressions. It also has some cheatsheets/documentation. [Here](https://regexr.com) I found it very handy to actually try out my expressions against syslogs.
Thank you! And nice tip about syslog.
Thank you very much!!
Thank you very much!
Thanks. Will look into it.
O'Reilly's 'sed &amp; awk' book will get you at least to level guru: http://shop.oreilly.com/product/9781565922259.do
great regex app: http://www.regexr.com/
Thank you. I will look into it.
Thank you I'll definitely look into it.
Thanks for that second link! I had done the regexone tutorial and didn't take much away from it.
This is the one I used. The cheatsheet in the sidebar really helped me. I'm no expert but for the most part I can get to a good starting place now from memory alone. Most stuff a quick Google search will help too 
So the ls "~$username" command is not going to work. Since this is basic homework, you'll need to specify the full path to the home directory which on most systems will default to "/home/$username/". The ~ will only reference your current users home directory, but since you are getting another user, you would need to specify the full path. Since it is possible for the home directory to be different then /home/&lt;user&gt; you might need to parse the finger command using such tools as grep, cut/awk to extract the specific path. 
The trick is this: ls ~"$username" Bash treats the `~` as a normal character when it is inside the quoted part of the word. Another thing to know about this is, bash fuses several words into one word when they touch each other directly. For example, these command lines all print the same word: echo aaabbbccc echo 'aaa'bbbccc echo aaa'bbb'"ccc" **EDIT:** Well, it seems it doesn't work if there's a variable or quotes or anything, only if you directly type a name. I should have tried that before writing the post.
This isn't true here for me. I can do for example: echo ~root echo ~nobody echo ~http It does print the same stuff I can see in /etc/passwd. **EDIT:** You can jump directly to the part talking about `~login` in `man bash` if you search like this: /^ *Tilde Expansion
hmm - sorry didn't test it myself before answering. Looks like [this stackoverflow page](https://stackoverflow.com/questions/3963716/how-to-manually-expand-a-special-variable-ex-tilde-in-bash) has some more details. edit - might be easier to manually build it by grep and cut the finger output to get the directory path. 
No, I'm sorry. I also didn't really test things and just assumed that a `~$var` would work, but it doesn't. I just tested some more, and the moment anything besides a simple word is used, bash stops to expand that `~`, even quotes breaks it: $ echo ~nobody / $ echo ~"nobody" ~nobody $ x=nobody; echo ~$x ~nobody 
I'm not sure, but you're specifying "type f" (just files) so it's unlikely that a folder would ever get found by the find function; furthermore I think if files contained within the folder were modified recently, they wouldn't get moved either as the search would fail again. The only way to preserve formatting would be to use mv with a wildcard. Have you tried changing the type to d? And perhaps using mv &lt;foldername&gt;/* /destination/directory/? Another alternative is to use something like rsync which would preserve everything.
Glad you got this working. If I had to guess I'd say your elif statement was missing arguments. If no arguments are needed (kind of like a catch all) then you just do else. I'll see if I can show you: if [ -d ${STOREDIR} ]; then mv ${file} ${STOREDIR} else echo "making dir and moving files" mkdir ${STOREDIR} mv ${file} ${STOREDIR} fi One piece of advice I like to use in scripts is testing vars; so for example your code would fuck things up quite a lot if ${STOREDIR} wasn't properly defined, so something like this is usually pretty helpful (this just checks to make sure it exists - i.e isn't completely blank): if [ -z $STOREDIR ]; then echo "Variable not defined, cautiously exiting..." exit 1 fi
Thats perfect! To fulfill the minimum requirements for the script the /home/$username path works. Ill probably just have some fun and mess with it a bit in case, like you said, the home directory has been moved.
Oh, thats good to know! I just ran those exact commands you listed and it echoed all the same thing like you said. 
Extract from [Bash manual](https://www.gnu.org/software/bash/manual/bash.html#Tilde-Expansion): &gt; The following table shows how Bash treats unquoted tilde-prefixes: &gt; &gt; `~` &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;The value of `$HOME` &gt; &gt; `~/foo` &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;`$HOME/foo` &gt; &gt; `~fred/foo` &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;The subdirectory `foo` of the home directory of the user `fred`
The # symbol on its own is used for comments in code. By passing it as a parameter to cat, you are basically saying cat nothing so the shell just sits there. Because your shell now has an infinite running process attached to it, it never times out. 
`#` starts a comment as far as your shell is concerned. So running `cat #` is *no different* from running `cat`. I'm not sure why this would keep an SSH session from timing out. Maybe the server you're connecting to has something configured to close connections after they are idle for some period of time, and having a process running (other than your shell) is enough to count as not idle? If that's the case, any other process that runs for a long time should work equivalently (e.g., `sleep 99999`).
Depending on how sshd is configured, the running PID for cat will keep the shell alive infinitely as it's a running process that is attached to the user's shell. A better solution overall would be to properly use tmux or screen. You won't need this command at all that way. 
&gt; keep my SSH sessions from timing out Sounds like XY problem to me. Take a look at `man 5 ssh_config` ## We create connexion sockets ControlPath ~/.ssh/tmp/%L-%r@%h:%p ## We use them automatically ControlMaster auto ## We let them live a bit longer... ControlPersist 10 ## We tell them to stay alive! ServerAliveInterval 10 TCPKeepAlive yes
As someone else said, `cat #` is no different than `cat`; when you run `cat` without any arguments, it begins reading from STDIN, and will continue to do so until you send it end-of-input with \^D I suspect your systems are setting the shell variable TMOUT, to timeout idle sessions. This trick works to get around that.
Also, doing `cat &gt; file` and starting typing then `^D` will write to the file. Completely useless now that vim launches in a fraction of a second but eh.
Technically you're not passing `#` as a parameter to `cat` because the shell parses it (and removes it as a comment) before even invoking the command.
Yeah. Also PuTty has a keepalive too. 
Which distro? Which guide are you following? If you're doing self-signed certs, I usually reference this one: https://github.com/stanzgy/wiki/blob/master/network/openssl-self-signed-certs-cheatsheet.md This will be different if you're not, though. If using LetsEncrypt/Digicert, openssl command will be slightly different. Also, last but definitely the most important - which directory are you in when you issue the openssl command?
Wow that script is so ugly it's hard to figure out what it does. 
Wow look at this Bash Guru. He has no help to offer but still thinks he is sassy and funny.
He's not wrong though.