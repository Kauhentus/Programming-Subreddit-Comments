no.
stupid me, had to read it twice to get it
I can see it in the Red Tops tomorrow - Money Supports Terrorism Shocker!
That sounds just like Intelligent Designer logic.
AKA [dark suckers](http://home.netcom.com/~rogermw/darksucker.html).
What if they're right?
wat
That is some funny shit. If it is made up, you are brilliant. If that really happened... I'm worried for all people, animals and inanimate objects within a 100 yard radius of your bunk.
He is an abomination!
Each line of that made less sense than the one before it. Very cool.
Whoever you are, wherever you are, I will find you, and I will punch you in the god damn stomach.
geez
That sound is a *very* common ringtone on my campus.
Goddamn nerds! (Upvote!)
They should've put the diplomas in a big wooden chest.
It can also describe science history, art history, music history, religious history, oh wait.. history.
this is silly. evryone knowns the internet can't be explained.
Ha! I've seen 100% of all TNG/DS9/VOY episodes. I dint bother with ENT or TOS.
I'm not a big Star Trek fan, but I did like TNG's storytelling.
OK, I was told this story by a friend in 1987. I guess that would make it an urban legend.
So that's why I found a dead dog in the suitcase. 
You're correct. http://www.snopes.com/horrors/gruesome/deadcat.asp
Why is this in the bash subreddit?
*Comment:* Sure it did.
This is by far my favorite bash.org quote. I can only imagine what this guy would have told his friends. It's not often you go to the bathroom and come out with puke and a black eye on you.
It's funny because no one expected the Spanish Inquisition in real life either.
Is it?
lol!
You know, it's not often these days that I come across something truly clever, but that was.
Not as funny as the 9 out of 10 joke.
Wrong! It shall never be dead. Never.
Gonna.
It will never give you up.
It will run around your dessert.
You know, one day I'm going to have to explain this shit to my grandkids.. Okay, well, *someone* is going to have to explain this to *their* grandkids, at least.
seems pretty straight forward to me
I wanna do that
That's why we need maps is Africa and such
I didn't know that Bush could use IRC.
His daughter can, apparently.
Tina Fey said that, not Palin
Some people out there in our nation don't have maps. 
And the Iraq. 
My IQ went up about 150 points, bringing it to 151.
Firefox was released in 2004. "The Days of the Modem" ended at the very latest in 2003, but more like 2001ish (if not before) Yes I'm aware that people still used modems (and still do today) but the ground has long shifted
the firefox bit doesn't matter to me, rather the fact that for his father, his son did not mind him to see a big cock spraying jizz as his wallpaper!!
and that my friends is reason 2253 why I use FireFox
There are much better reasons to use Firefox. Let the stubborn ones fail.
Like in South Africa and the such as.
maybe he used mozilla suite before then?
then woulda said mozilla :P
This is a pretty accurate description of how interacting with girls makes me feel.
Don't worry, girls are nerfed after you apply the graduated college patch and start in the new working for the man zone. 
He makes it sound way to dramatic. I'd just hit the firebug icon and do what needs to be done.
No, but I wish I'd get this call anyway.
Individual components (hardware or software) sure. The entire network, no, because you built it.
That's as crazy as calling electricity particles negatively charged and flowing in the opposite direction.
I don't get it, what is the actual context of +dick and +cum
well, it's not hard. he thanked peer for something and then added +dick and +cum.
So there is no alternate meaning, they just randomly typed +dick +cum? That just seems ... juvenile.
You don't really seem to have a sense of humor.
Damn, my job blocked bash.org. Can a good redditor please post the conversation here?
&lt;NimF&gt; The lesbians next door bought me a rolex for my birthday. &lt;NimF&gt; I think they misunderstood when I said I wanna watch...
Oh. Ok. Thanks. :)
What? This is dumb. If there's anything worse than a cursing fanboy, it's a cursing fanboy who refuses to support his arguments.
Uhhh, I like Chirpy powered sites better. http://chirpy.sourceforge.net/
Chirpy is horrible. don't trust anything powered by CGI. There are numerous exploits, and its just not unique.
I'd love to know what some of these exploits are.
For those who may not know; ned = non-educated delinquent.
I still can't get the pictures of stupid, sexy Flanders out of my head!
Screw Flanders. Oh, wait...
Please don't post the punchline as the title.
I know, I probably ruined it. oh well.
Similar to one of the best reddit [comments](http://www.reddit.com/r/atheism/comments/8eyy3/heres_the_christain_douchebag_chad_farnan_who_is/c092j8m?context=1) of all time.
It seems like it's suddenly popular to link to qdb with the entire quote as the title, just to make people feel dumb for clicking through
Good point. I thought about that, but it's just a one-liner, so it's not worth hiding the punchline. The link is just for reference. 
Oh yeah! That's what that webpage says as well!
Good one. But Java is still nice, even though it needs more than one reason. 
Hmm, why else is Java nice? It's a nice pedagogical language, isn't it? Other than that and cross-platform-ness, I don't see much advantage to using it.
Being cross-platform is a definite advantage; but in addition, it's fairly complete, easy to use, fast to develop, modular, ... If you are developing a lot of server code for large web-based applications for a corporation, what else would you use? I've mostly used C and C++ in my career, and I don't have a lot of Java experience, but from what I've learned, it seems fairly compelling for certain applications. Maybe not applets, which is what many people think of, but for server code, it's much nicer than the more common PHP or Python scripting languages that I've used.
Oh damn it now, I'm going to spend all night on QDb / Bash
Context? "Client"?
I should call the university tech support for this :p
You were right, it blew my mind for a bit, especially with all of the other comments on the page about welcome to 2010. Shite ;)
Whoa. This is all kinds of confusing now.
a very simple example; this will find any files called 1234 in on below your current working directory if you call the script with &lt;scriptname&gt; 1234. If you want to search for any files that have 1234 in their name change $1 to "\*$1\*". \#!/bin/bash function xyz { find . -name $1 -print return $? } \#### \# Entry point \#### xyz $@ exit $?
myfind() { find . -name "*$1"; }
perfect.
Thank you. Was looking for the more succinct version such as what rebugger commented, but your script is very helpful to me.
Been looking at http://stuff.lhunath.com/bashlib a bit and realized I should probably use the local and shift keywords more, for starters. 
SECURITY THING!
with sed: sed -n ' /MP2=/ s/^.*MP2=\([^\\]*\).*$/\1/p ' &lt; files*
cat &lt;filename&gt; | awk -F'\' '{ print substr($2, 5) }' &gt;&gt; &lt;newfile&gt;
there's no need to pipe cat through awk like that
Great! Thanks for the help. So, do I have this right?: The /MP2=/ part jumps to the line containing "MP2=", then the s/blah/blah/p grabs just the part after "MP2=" and before the "\" following the number, with the "/p" part used to print the output.
[useless use of cat award](http://partmaps.org/era/unix/award.html#uucaletter)
exactly. with the '-n' option to sed, nothing gets printed out, except for the lines that are matched ('/MP2=/') and 'p'rinted
Er, wrong sub reddit?
Oh yea, guess so ... saw that other post re: scripting help near the top ... suggestion for different sub?
Not really my thing to be honest, if it's a certain language just go to /r/[language name], or one of the below: http://www.reddit.com/r/learnprogramming http://www.reddit.com/r/techsupport/ http://www.reddit.com/r/24hoursupport/
I've posted this in /r/commandline now
Try find . -name '*.xml' -delete 
As you mention down-thread /r/commandline is good, might also try the [bash /r/](http://www.reddit.com/r/bashtricks).
find ./ -exec rm -f {}\;
I used awk instead awk '/ServerAlias/{p++} /ServerAlias/ &amp;&amp; p==1 {$0= $0"\nS2"}1' foo
This seems overly complicated. Why not just put something in your .bashrc?
So try putting sth to your bashrc and realize that it breaks on many simple cases (due buffering, race conditions etc).
 cal | sed "s/\&lt;$(date +%e)\&gt;/[]/"
Yes, that is definitely shorter. Thanks.
I see a lot of sess* files in /var from time to time, and I basically use the above. The actual command that I use is: find . -type d -name sess* -exec rm -f {} \; Although from now on I'll just use the -delete. Thanks for that.
Wow! That's way better. TIL about find's -execdir option. Just goes to show the importance of reading those man pages!
Just out of curiosity, what Linux distro are you running? I tried it on Ubuntu 8.04 with no problems, but I only had a single zip file in a single directory (perhaps not a robust enough test case!).
Well I tried it on 96 files and ended up with just over 70 but I couldn't really work out a pattern as to failure. I'm on Fedora Goddard not sure if it makes a difference?
awk -F'\' '/MP2/{ print substr($2, 5) }' filename This will look for any line containing the keyword MP2. It will then take that line, and divided based on the "\", ending with three fields: HF($1), MP2($2), RMSD($3). It will then go to the second field $2, and start reading that field from the fifth character (the -) until the next "\". You can either dump the output to a new file like this: awk -F'\' '/MP2/{ print substr($2, 5) }' filename &gt; new_file Or append the output to the file that you read in the firs place: awk -F'\' '/MP2/{ print substr($2, 5) }' filename &gt;&gt; filename 
Thanks. I had it working with the suggestions from before but just now found a bug in that they won't ignore newlines. So when the number I'm looking for is split between lines, I only get the first part. Do you have any suggestions to overcome this problem? Thanks in advance
Here's how I'd run it (and as a one-liner to a one liner) myDate=$(date);myUser=$(who -q);myUptm=$(uptime); echo -e "$myDate\t$myUser\t$myUptm" | tr -d '\n' &gt; output.txt
well first off it would help if you provided example data so that we could see what you are working with. It would also help if you said what you expected to see. Indeed it would help if you simply said what you are trying to do. Without any of the above, at it's simplest level it seems that you are wanting to store the output of the piped commands in the variable. If that is the case then the answer to your question is, 'because you're doing it incorrectly'; the usual way is to: VARIABLE=$(command | other command | etc) You'll see a lot of people using the now deprecated backticks similarly, e.g. VARIABLE=\`command | other command | etc\` Edited because in responding to the DrC. I noticed that the backticks weren't showing so I had to escape them. 
Isn't this effectively the same thing as the [script](http://ultra.pr.erau.edu/~jaffem/classes/cs125/script.htm) binary in unix? There's a greater chance script is pre-installed on your flavor of choice too.
Could you put the awk command that you're trying to use up? I'm not sure exactly what the problem you're encountering is, unless there's a comma in the file names somewhere so that the line is being tokenized in more chunks than you'd want
I edited my main post, thanks
By default awk splits on white space, if you want to change its field separator use the -F flag, e.g., awk -F, '{print $2}'
That helps out a bit but I can still not output the whole column. thanks
Is there anything you can see about where it's being cut off that would give a clue about why it's not printing the rest? Also, there's no chance that your some_variable will match different lines, is there (unless that's what you want).
Hi, No problem to script but can I just ensure I understand your requirements correctly - sounds like you just need a script to do a recursive search and replace... - You have a CSV file that contains multiple lines with two columns, the first column is the 'old value' and the second columnn is the 'new value'. - You wish to search the filesystem for files containing the old value and replace that value with the new value (and rename the file?) - What happens if a file contains more than one 'old' value? Do you want multiple search and replaces done on each file for every possible old value? - do you want to run the search and replace over every file on the filesystem or can you limit it to certain files (limited by directory structure or regular expression perhaps?)
Sorry I had to jump on a plane and was trying to hash this out at the airport. Yes, essentially there is a CSV file which contains two columns in it. The first column contains the old strings and the second column contains the new strings. So, if column A has an old string, the string directly next to it in column B is the new value, and it needs to be changed. I will be mounting network shares on a single server and running a script to run through them all to find and replace these strings. These values are unique, so basically I am creating an array, and then looping through column A which contains tons of different strings of information. If one is found, the value right next to it in column B should be the new value. I hope this makes sense. This is a requirement for something at work, so it just something I have to do to update everything with our new compliance policies. 
One of my reasons for not using perl for something like this is I like to build up the command line as I go and check that I'm getting expected output before overwriting files or similarly destructive acts. I just feel a little better seeing it print what files it thinks it should operate on first. It also fits my work flow better since if I'm trying to rename files I'm probably already at a bash prompt doing stuff without an editor. I'll just build the command line then echo it into a file or grab it from the history file when it works if I think I'll need it again.
http://h3manth.com/content/csv-parsing-methods-linux
While it's not a bashism, you could always just do this in the sudoers file: # Disallow shutdown,reboot,chroot on vpshost seanochoa vpshostname = (ALL) ALL, !/sbin/shutdown, !/usr/bin/reboot, !/usr/sbin/chroot # Allow everything everywhere else seanochoa ALL,!vpshostname = (ALL) ALL edit: Forgot 3 more spaced for code blocks =) 
So, first off thank you very much for your help. This is what I am sort of working with awk '{print $1"\t"$2}' inputfile.csv This prints the first entry of column 1 and 2, but doesn't print anything below it at all. What I am doing is renaming files/folders to bring standards into our servers. Things are changing, we must standardize so when we look at large scale replication the folders are all named properly. So here is my logic in a nutshell: create a loop on the file share to display files/folders with in each certain path. The paths are already set, since they are all on network shares. Then compare that loop, to a loop through column 1 on the CSV sheet, if it matches, it will move the folder (rename it) with the exact value as column 2. Still cannot quite wrap my head around it, getting closer though.
Why not use mktemp -d? It doesn't create a symlink like this does, but it does all the directory creating and guarantees uniqueness.
I forgot about mktemp. Anyway, with my solution ("scratch-{TIMESTAMP}") you have scratch dirs named in predictable order. You have latest ones at the end if you sort alphabetically (or just use ls) so it's just easier to find scratch dir created few days ago in case you left something important in it.
https://m.twitter.com/UnixTools This alternate link will work without requiring javascript. This comment generated by an [automated bot](/r/link_unscripter).
If the "Message" part is all on one line then something along these lines should tally up how many messages each person has sent cut -f3 | sort | uniq -c If the message can be multiple lines I'll probably need to see some of the file to make an appropriate one-liner.
This should work, but I think it would be faster using the count option in grep alone: grep -c " Name: " 
If the log doesn't contain sensitive information, I would be happy to take a look at it. I will figure it out, tell you the answer and the command(s) I used to get there.
There is allways [@climagic](https://twitter.com/#!/climagic), with 14K followers and a handful of existing tips.
Sure - I know the cat was unnecessary - but I usually use it anyway because its easier to stick it in at the beginning of a series of pipes when I'm still playing around as to which command will go first.
Thanks! That one's actually pretty cool, I hadn't seen it and am definitely following it now. I knew about [@shellfu](http://twitter.com/shellfu) (just links to code), [@bashtips](http://twitter.com/bashtips) (just bash and has been inactive), [@commandlinefu](http://twitter.com/commandlinefu) (just links to code). I wanted one that tweeted the actual commands, not links to web pages with commands (as I like getting sms updates, but don't have a data plan, so code in the sms message works best for me). Plus, I got some good practice writing scripts for twitter apps.
 alias scratchdir='echo `mktemp -d \`date +'scratch-%s-XXXX'\``' Then use that as: cd `scratchdir` or NEWSCRATCH=`scratchdir`
[Synergy](http://synergy-foss.org/) does this and more!
I am thinking awk can do this in one line, maybe awk -F: '/$4/ { print NR }' /path/to/input/file I didn't test this, I just set the delimiter to colon and then counted by hand the name field, and then NR should emulate the same function as wc in bash. Also, I am not the strongest awk'er
Very cool. Thank you!
OP's method is a lot cleaner. Uname exists on both OS's, so it makes far more sense to use it, than to run a command and determine it isn't OSX just because the command doesn't exist (you hope) on Linux. Also, the command isn't "in bash". It is in /usr/bin, and you can access it from bash or any other shell.
 #!/bin/bash var1="hello" var2="world" /bin/echo "$var1 $var2" helloWorld () { /bin/echo "$var1 $var2" } helloWorld exit 0 You can set a variable to whatever you want. It can be a string or a command as well. 
From the source it seems that the recoding is already saved to files. Looke like only a UI for handling these files is missing.
Magento E-commerce uses xml in the config file. I actually don't even work in webhosting anymore. Hell, I don't even work in a Linux environment anymore. Anyway, I wanted to use xmlstarlet, but I didn't have permissions on that server, and it wasn't installed.
Backticks are deprecated? Shit, time to rewrite a bunch of scripts.
Use named pipes: http://www.linuxjournal.com/content/using-named-pipes-fifos-bash
If I've got you right, you're trying to send the single quote in as a parameter on the command line? Try escaping it, with a \ in front. that tells bash that the next character is meant to be taken literally. Alternatively, you could put the whole thing in double quotes. script target text\'
this? parm=$1; shift; echo $*"'" or parm=$1; shift; echo $*\'
Already tried and failed
Not sure I follow how deep you're going but I think you need to play with backslashes more deeply. \"\'\" may do it, each layer you go through should strip off a layer of backslacks and the double quotes should then become transparent. The problem is always keeping track of the backslashes and making the quotes match up right after being processed.
For that very specific case, you need to surround the text in double quotes. But then double quotes would be verboten. The problem you're running into can't be solved by the script itself -- it's at the interactive shell level.
you can do it just in one line. Something like that: find . -name '*.rpm' ! -name '*debug*' ! -name '*src*' -print0 | xargs -0 -I {} cp {} ../test2
I agree with avikos that there are better ways to do what you want. `find` itself already iterates over whatever items match your query. You shouldn't need to wrap it in a `for` loop. But, to explain what was going wrong with your code: inside of the `if` test, you don't want to quote the `*`. That prevents the glob expansion: if [[ *.src.rpm != "$file" ]] &amp;&amp; [[ *debug* != "$file" ]] Compare `ls *` and `ls "*"` (a silly example, just to see what I mean). Your test was saying "if $file doesn't match "\*.src.rpm" (literally with the star) and it doesn't match "\*debug*" (literally, with the stars), then copy it". Since nothing literally matched the stars, everything was copied.
Great I didn't even know I could do that.
Stop wanting that.
&gt; I know about passwords in plain text, and ssh keys etc. that doesn't work for my situation where I have to log into to numerous locations frequently. If utility far outweighs security then so be it, that's for you to decide. But I do wonder have you looked at other open source options? cfengine springs to mind as a software we used at a different job that, IIRC, allowed you to send individual commands.
By opening ten terminal windows, running your ssh commands in a for-loop in each one, copying the password to the clipboard and pasting it into each window six times? Because that's how I handle the same task. It's tedious but manageable, and putting passwords into scripts is a bad practice. Once you start down the dark path, etc..
I'm not sure what "just about as dangerous" means here. It's in memory, so it's hackable I suppose. It goes away when the process finishes, notwithstanding swap cleanup issues. It sucks, for sure, but it's less evil. I guess you could split the difference by having a script that prompts you for the password, then does its configuration run using what you gave it.
I would recommend you use SSh keys. When this is not an option ( and sometimes it is not ) I use the hell out of this: http://sourceforge.net/projects/sshpass/
those are the delimiters for "command substitution". See section 3.4.5: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html Basically whatever is inside those delimiters is evaluated in the context of the current shell, and whatever is put in stdout is substituted into the original command. For example; `date +%F` yields the date in yyyy-MM-dd form; so you could mkdir `date +%F` to create the directory "2012-08-22". (well, it will tomorrow.) In scripts I prefer `$(` ... `)` because unlike with the backticks, those can be [*edit: more easlly*] nested. When I accidentally press that and hit enter, I usually just CTRL+C, UP-ARROW and BACKSPACE.
I've done a few tests and it's frustrating to find out that using find with xargs is actually slower than using that stupid sleep loop. anyone have any input on this? possibly because xargs isn't firing new processes fast enough?
That's a bit hard to follow. Can you please post (as code) exactly what's in a few relevant lines of the file and exactly what you are entering into `grep`? Use backticks or four-space indents to format those as code, please. (For example, I can't tell just by looking at that if all those quotes are literally in the file and your `grep` command or what.)
use grep -E " and use a regular expression
! [[ $file =~ "\.src\.rpm" ]] or just eliminate the debugs in yoru for for file in `ls *rpm | grep -v debug`; do .....
I haven't looked at the rest of the code, but for the arithmetic part you want to do $(($changed-$CurrentTime)) with a $ and double parens.
This seemed to fix the issue. Much appreciated!
Happy to help!
A couple of thoughts: * in your "for" loop, you can just do for f in *; # it will give you the same output as your $( ls . ) without having to invoke a subshell * instead of messing with arithmetic in your "if" clause, why not just do if [ $changed -gt $CurrentTime ] * lastly, what's wrong with using the "find" command? e.g. find . -mtime -1 -maxdepth 0 # (finds all files in the current directory; remove the -maxdepth 0 if you want to descend into any sub-directories) 
Thanks for the advice on the arithmetic in the if statement. I was thinking something could have been improved, but hadn't put any serious thought into it yet. As for your third suggestion, I'm still new to Bash and all the different ways to do things. I'll look into this :)
Thanks. Will look into it.
http://tldp.org/LDP/abs/html/string-manipulation.html if [ ${#FOO} == 6 ]; then NEWDATE="${FOO:0:2}/${FOO:2:2}/${FOO:4:2}" elif [ ${#FOO} == 5]; then NEWDATE="${FOO:0:1}/${FOO:1:2}/${FOO:3:2}" #You could prefix this with a zero for consistency so that it would be like 02/17/11 instead of 2/17/11 fi
Thanks so much that's exactly what i needed. 
wow, I was just literally using this method yesterday to strip out the first two characters in a string by echoing a variable with the :2 switch after. Funny how awesome bash internal can be. Of course I was first looking for a sed pipe to do it. Gotta remember bash can do a few things.
Would run way quicker if you used find . -newer filename
It's written as a shellscript, not in bash. For an example of a program written in bash (actually pure sh), see [countmail](http://www.splode.com/~friedman/software/scripts/src/misc/countmail.mycroft).
If I comment out the else code block, it does nothing: ++ mktemp -d + tmpdir=/tmp/tmp.6zQmVDGg1w + for i in '*.tex' + [[ -f *.bib ]] + rm -rf /tmp/tmp.6zQmVDGg1w 
Aw yis.
Nifty script, bro :)
I put it in my PATH and invoke it with "`karma (username)`"
Your script violates reddit's API rules, since you're only supposed to send requests every 2 seconds. You should put "reddit.com/user/$REDDITOR/about.json" into a variable, and do all your parsing from that variable instead of making additional web requests. This will also make your code more resilient to format changes, since that page is part of reddit's official API.
SHIT! I completely forgot about reddit's API rules! The improved version doesn't violate them however.
Here's an idea: Instead of grepping through a HTML page, try to parse the JSON information. You can get account and post data in JSON format by adding .json to reddit urls. Example: [http://www.reddit.com/user/CJSg/about.json](http://www.reddit.com/user/CJSg/about.json) Edit: Sorry, I see this has already been suggested.
Using backticks for command substitutions is deprecated; better and more readable to use $(command).
[Here](http://pastebin.com/ExQYevWe) it is, thanks for the advice! 
Will keep this in mind next time, thanks!
Hum... OK, i admit i didn't think about that point ;)
you can always use ls -1d folder-* - this may give you problems if no folders exist of that type, you may want to use find instead. Futhermore using &amp;&amp; between the cd and execution will make sure the cd completed successfully. There is a few ways to fix this.. **find** for dir in $(find /base/path/ -type d -name 'folder-*' -maxdepth 1) do echo "working on ${dir}" ( cd ${dir} &amp;&amp; foo --bar ) done **if-dir** for dir in folder-* do echo "working on ${dir}" if [ -d ${dir} ] then ( cd ${dir} &amp;&amp; foo --bar ) fi done edit: formatting
http://mywiki.wooledge.org/BashPitfalls You fell in pitfalls 1, 2 and 19.
Reading this now. Thank you !
I usually use `"$@"` in this situation rather than `$*`. For some reason `"$@"` does quoting properly (ie. individually) but `"$*"` doesn't. 
Other people have phones that autocorrect. I have my brain intercept keystrokes and autocorrect all by itself, often with mildly amusing effects...
here's another way to try parsing the json instead of grepping: http://stedolan.github.com/jq/ though then, no one could use your script without having jq. 
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/13q4zn/bash_need_a_script_to_cd_into_a_folder_run_a/
I'm really new to bash, could you give me examples where the one posted shows me bad practices and why it's bad? Also maybe the wiki you posted should be on the side bar for newbies to look at.
Ok, going from the start until we find the first set of examples, http://tldp.org/LDP/abs/html/sha-bang.html, let's look at example 2.2. It contains the line cd $LOG_DIR There are several problems with that. * The variable name, LOG_DIR, is all uppercase. Unlike languages like C, Python, Java, etc..., bash shares its variable namespace with environment variables, and environment variables are all uppercase (HOME, USER, PATH, EDITOR, ...), the only exception I know of is the http_proxy variable typically used by web browsers. In addition, shells have some special variables that are also all uppercase (IFS, RANDOM, UID, ...). So to avoid accidentally overriding environment variables and special shell variables, don't use all uppercase variable names (use under_score or CamelCase or whatever rocks your boat, as long as it contains at least one lowercase character). * The expansion of the variable is unquoted. If you don't put it in double qutoes, the shell will split the expanded value into words based on whitespace (called word splitting). On top of that, if any of the resulting words contain *, ? or [ ... ], the shell will try to replace them with matching filenames (this is called pathname expansion). Putting it in double quotes prevents word splitting and pathname expansion on the result of the expansion. Granted, the value of the LOG_DIR variable doesn't contain any characters to worry about in this example, but it's still bad practice. You might alter the script later, changing the value of LOG_DIR to contain some special characters, and then you have to go through the whole script making sure all occurances of $LOG_DIR is quoted. If you quote it by habit, you don't have to worry about making such small changes to the script. * It doesn't check if the cd command succeeded. The cd command may fail. Typical cases where it fails, is that the directory doesn't exist, or the user invoking the script doesn't have execute permission to the directory. And if it does fail, the following commands will be run in the wrong dir. If an rm command happens to follow for instance, you risk deleting the wrong files. So, applying good practices, the line in question should look like: cd "$log_dir" || exit Now if cd succeeds, it will continue on with the script. If cd fails, it will write an error message to stderr and return a non-zero exit status. This will trigger the command after || to run; and exit ... it exits the script. There's no point in continuing the script if that cd command fails.
A few thoughts: a) Start small, work your way up. Solve one problem at a time. b) Consider using perl, python, ruby, etc. instead of bash.
Well, MAX_GID and MIN_GID are really only useful values for programs that modify the passwd and group databases (e.g. useradd, groupadd, ...), so having them in the environment would be a waste for most programs.
Try [Spark](https://github.com/holman/spark)
Yep.
Inspired by your response, I have updated the INSTALL and README, done some cleanup, and fixed a bug or two. 
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/14r291/a_sawtooth_music_synthesizer_with_prerendering/
# Assignment... hmm. cat &lt;&lt; EOF &gt; some.txt The quick brown fox jumped over the lazy dogs. fox fox EOF grep -n fox some.txt | awk -F':' '{print $1}' | tr '\n' ' ' 
echo -e "The quick brown fox jumped over the lazy dogs. \n\n\nfox\nfox" &gt; file.txt
Hey, thanks for replying! I hope it's okay to post here with a specific question with regards to an assignment like this, if not I'll remove it and this comment. for i in $(sed -n "/$regex/ =" $file) do echo "$outputPath:$i" done I went with this for now, since I don't really need an array but just need to print them one by one and with the outputPath variable prepended to it. 
That's what I was thinking, cheers! And yeh, especially bash solving already solved problems using less efficient means, such fun. Haha.
Cannot GET /zachwolfe/Fork-bomb
Yes, with mapfile (new in bash4.0): mapfile -t linenumbers &lt; &lt;(sed -n /re/= "$file") or read: IFS=$'\n' read -d '' -ra linenumbers &lt; &lt;(sed -n /re/= "$file") See http://mywiki.wooledge.org/BashFAQ/001 and http://mywiki.wooledge.org/BashFAQ/005
Thanks for the tip. 
You can split them in bash with syntax like first_part=${line%%----------*} second_part=${line##*----------} For a lot more options for how to manipulate string variables in bash you can check out [this guide](http://tldp.org/LDP/abs/html/string-manipulation.html)
Thanks, I'll take a look awk.
Here is a one-liner I whipped up which does the same (more or less) :) curl -s http://www.reddit.com/user/$USER/about.json | tr "," "\n" | grep "link_karma" | tr ": " "\n" | grep -E "[0-9]+" | sed s/"^"/"User $1's Link Karma: "/ Nice job on your first script!
This isn't a script, it's two disparate commands. I've not seen this exact construct, but my guess is the first part (`2&gt;&amp;1`) has something to do with redirecting stderr to stdout, but I don't now how/why while the second part is using string manipulation^* to say show everything to the left of `:` in the variable `_` which is a shell builtin I believe means "the argument of the last command". So, to review a bit, first part is [i/p](http://www.tldp.org/LDP/abs/html/io-redirection.html) redirection of some sort though why, who knows. The second part uses ${_} which is the last argument of the last command: (19:02:42\[root@DeCoBoxOmega) [~/Desktop]$ ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: icmp_seq=0 ttl=52 time=24 ms ----8.8.8.8 PING Statistics---- 1 packets transmitted, 1 packets received, 0.0% packet loss round-trip (ms) min/avg/max/med = 24/24/24/24 (19:03:54\[root@DeCoBoxOmega) [~/Desktop]$ echo ${_} 8.8.8.8 along with string manipulation, which I'm not going to go into detail on, but here's a (very) simple demonstration: (19:06:58\[root@DeCoBoxOmega) [~/Desktop]$ x="left:right" (19:07:16\[root@DeCoBoxOmega) [~/Desktop]$ echo ${x%%:*} left (19:07:18\[root@DeCoBoxOmega) [~/Desktop]$ echo ${x##*:} right If I were you though, I'd crosspost to /r/commandline, as they're way more active. 
when in doubt, always run the the script as root on a production system.
Good idea! :3
Something is missing here because it tries to run - as a command; it's not a command, but can be defined one or as a function. Assuming - is defined somewhere, this one-liner runs the - program, dumps both stdout and stderr into a string and tries to execute the output as a command. Then the next statement takes the same string, chops off anything past the first colon and tries to run the result as a bash command. So for example, if I did this: alias -- -=cat function ls:l { ls -l; } "$(- 2&gt;&amp;1)"; ${_%%:*} then the shell would wait for me to type a command, which it would then execute twice unless the command name contains a colon. If I typed in ls:l as the command, it would execute ls, then ls-l. I hope this made sense because this script sure doesn't.
I tried running it in `bashdb` and this is all I got: Local-Admins-MacBook-Pro:~ cooljeanius$ bashdb -c "$(- 2&gt;&amp;1)"; ${_%%:*} bash debugger, bashdb, release 4.2-0.8 Copyright 2002, 2003, 2004, 2006, 2007, 2008, 2009, 2010, 2011 Rocky Bernstein This is free software, covered by the GNU General Public License, and you are welcome to change it and/or distribute copies of it under certain conditions. (/tmp/bashdb_cmd_10379:1): 1: -bash: -: command not found bashdb&lt;0&gt; continue /tmp/bashdb_cmd_10379: line 1: -bash:: command not found Debugged program terminated with code 127. Use q to quit or R to restart. bashdb&lt;1&gt; q bashdb: That's all, folks... -bash: -bash: command not found 
my first thought: just run it LOL
Shut up and take my root password!
That is pretty odd, might wanna try $1 $2. Edit: Why dont' you just setup a alias for your server?
I would recommend using [ssh password less conections!]http://linuxconfig.org/Passwordless_ssh) After that you will be able to do *ssh user1@192.168.1.40* or *user2@192.168.1.40* You can also create an alias for your server in **/etc/hosts**: *192.168.1.40 server* So now after that you will be able to do *ssh user1@server* or *ssh user1@server* After that you may want to set a default user for ssh. So editing (or creating if doesn't exists) your **~/.ssh/config** and adding: Host server User user1 After that you can do *ssh server* (this will use the user user1 to perform the connection) or *ssh user2@server* if you want to connect with the user 2. As you can see the setup if not complex and at the end, your command is even shorter than using a script. Regards, Josemi
Why not use case statements? That way you can add more options in the future. case $2 in 1) user='username1' pass='password1' ;; 2) user='username2' pass='password2' ;; esac If you really want to use if/else ladders... if [[ $2 == 1 ]] then user='username1' pass='password1' elif [[ $2 == 2 ]] then user='username2' pass='password2' fi If you are going to use this in SSH, though, it's much *much* better to configure SSH to do it. Stick some configuration options in ~/.ssh/config. Edit: to elaborate on SSH setup, let's say you want to connect to the same host with two different usernames. You could stick the following in ~/.ssh/config: Host user1 HostName 192.168.1.40 User username1 Host user2 HostName 192.168.1.40 User username2 Then you can log in as username2@192.168.1.40 simply by doing: ssh user2 You can also set up public key authentication so that the connection happens without a password. There are plenty of guides online to help you through setting that up. Cheers!
You could move all function declarations outside of the `while` loop.
Oh, I get it. Don't know why I didn't see that. Now it looks like [this](http://pastebin.com/6AsxkUgq). Better?
You need either quotes OR a backslash. Not both. :) Use tab-completion. It'll help. 
I suspect that you're having the space not end up escaped in the DIRS assignment line. When $MYDIR gets expanded there you end up with a command like DIRS=`ls -l /Volumes/My Book/ | ... So you can try to fix that either by putting quotes around $MYDIR in the DIRS line, or to add a couple more \ characters to escape the space even more when you assign to MYDIR (which shouldn't have a $ at the start of the assignment line).
cd $(dirname $0)
Read my edited post.
It's still finding the dot anyway. 
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/18p3lj/can_anyone_tell_me_what_this_script_does_d_xpost/
The first line will attempt to run `"bash: -: command not found"` as a command and will fail since no such command exists. The second line attempts to run the previous command again except that it trims the first colon and everything that follows it, i.e. `bash`. The whole result is that it prints an error message and you are now running a second instance of bash. The script will have different results in other shells depending on format of the "command not found" error message for that shell.
Give this a shot: sudo ln -s /usr/bin/nano /usr/bin/pico
https://wiki.archlinux.org/index.php/Sudo#Passing_aliases This should work too - might be easier to figure out what you did later than a symlink, but either should work.
Doesn't centos have their own version of update-alternatives? If so you can do something like this. sudo update-alternatives --config editor
Actually I think correct way would be to set $EDITOR for your user export EDITOR and then to do sudo -e Which will, instead of running something, open that for editing in $EDITOR Edit. Fixed -e, thanks bryanlharris
I am a bit new the bash so heads up
I thought it was sudo -e
This has nothing to do with bash, this is your operating systems package manager.
So how do i fix the mixup with operating systems package manager 
how about: ls /some/dir | if grep --quiet "rock"; then SAID STUFF fi Or better yet for f in $(ls /some/dir/*rock*); do DO ITERERATIVE STUFF WITH "$f" AS THE FILE NAME done
Correct. the first method does not give you the name of any files to work with so you just know that they are there. the second will loop through every file that matches when invoked like that the condition will be true if the process terminates with an exit code of 0 (e.i normal) (grep will return 1 if nothing is found) looking back you could also do it like this: if ls "/sme/dir/*rock*"; then DO STUFF; fi
While it's great that you're trying to help someone out^*, you should know that [you should never process the output of ls](http://mywiki.wooledge.org/ParsingLs) in a script (or really, anything.) Its really one of those things that's likely to never be an actual problem, but avoiding doing it is so trivially simple that there's no reason to ever do it. *I mean this sincerely, but I can't help but feel like it comes out sounding snarky in print. ___ Ninja edit: [Alternate methods](http://www.reddit.com/r/bash/comments/197q8d/noob_question_here/c8llmqh)
You could probably get better advice by posting to /r/commandline, I believe they're more active. To answer your question though, you could probably do this pretty easily by using the find command and a while loop (how I would probably do it) or xargs (which is probably the better way to do it, but a skill I've not learned well yet): d@AirBox:/tmp/example$ mkdir hardrock postrick altrock rap metal d@AirBox:/tmp/example$ ls altrock hardrock metal postrick rap d@AirBox:/tmp/example$ find . -type d -name "*rock*"|while read i;do echo "this is a directory called $i";done this is a directory called ./hardrock this is a directory called ./altrock d@AirBox:/tmp/example$ find . -type d -name "*rock*"|xargs file ./hardrock: directory ./altrock: directory d@AirBox:/tmp/example$ 
No worries.There's millions of little gotchas like that. Take comfort in knowing that no matter how good you get, in 5 years you're going to look back at what you write now, and want to punch past you in the face for not knowing what future you knows. Also, just realized I replied to the wrong comment, so glad you got to see. 
A for loop can do this. for i in $(ls /path/to/folder | grep "rock") ; do some bash commands done The -e test just sees if it exists, the -f is if it is a file and the -d is if it is a directory. Also the positional parameter of $0 is the directory the script is working from if you wanted to take that into account so all you would ever have to do is drop the script in the proper folder and let it run. What exactly is the higher goal here?
 for f in `find /some/where -name '*rock'`; do do_something -to ${f} done 
Hey, neighbor. Couple of reasons this is a bad idea (but it's nice that you tried to help^*). 1) When you do a for loop like this, the entire for loop has to finish processing *before* anything else happens. This is slower than other kinds of loops, and also means you have to have everything inside your for loop in memory before you start processing (which isn't really a big deal on pretty much any modern system, but you know...best practices and all). Also, you can use xargs for this kind of thing, though I'm not very good at it. So, take a look below: d@AirBox:/tmp$ echo {1..100} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 d@AirBox:/tmp$ time for i in {1..1000000};do echo $i &gt; /dev/null;done real 0m13.576s user 0m10.557s sys 0m2.896s d@AirBox:/tmp$ time echo {1..1000000}|while read i;do echo $i &gt; /dev/null ;done real 0m4.149s user 0m3.044s sys 0m1.352s d@AirBox:/tmp$ time echo {1..1000000}|xargs echo &gt; /dev/null real 0m1.354s user 0m1.400s sys 0m0.224s See, this isn't just one of those syntactic sugar things...there's a very tangible difference in processing time. 2) Using the grave accent notation (the ` is called a grave accent) is deprecated in most modern shells, and certainly deprecated in bash. Instead, you should use the $(command) notation. I'm too tired to give specific examples why it's better, though one that I recall off the top of my head is that $() allows for nesting of command substitution while grave accents don't. Also (in my opinion) it's much more readable with $(). ^* As I said in another comment, I mean that sincerely, but it always comes out snarky sounding in print. 
I'm not testing any of this, so it all could require some debugging, but you seem to be missing a few steps for what you say you want. For example, there's no place where you seem to try to drop lines that start with #, though I can see how you'd want to do that. If you changed your while read LINE; do ... done &lt; $FILENAME to egrep -v "^#" $FILENAME | while read LINE; do ... done you should be able to solve that problem (that will display only lines that do not match the regular expression of a line starting with #). For the "command scheduled to run" you are only doing things with the first two "words" in your for loop, so you never get far enough to use the command name. Of course, you could simplify that whole thing by, instead of reading into LINE then tokenizing that later, reading into variables instead, like egrep -v "^#" $FILENAME | while read m h dom mon dow cmd; do ... done Then you can print $m $h and $cmd, and if you want to do anything with the days it'll run you've got them too. 
You beat me to it, was trying to do something with awk but you're right, not necessary. Don't forget to egrep -v "^$"to strip out empty lines.
Thank you! the egrep command worked and I got my output as required :) . But there is one little problem; whenever I call the script (saved as .cgi) via the web browser it always shows the crontab file as it contains n-1 jobs, even though there are n jobs in the crontab file (The web page due to some reason always misses the first job). But whenever I call it via commandline it always shows the correct output (I mean correct number of jobs) Here's the code i'm using http://pastecode.org/index.php/view/65768177
Hmm, are you sure they're running as the same user and have the same crontab values? The one running from the web page may be running as the apache user or something like that. Otherwise, is it always missing the first or the last or something like that? If you do a "view source" on the page is the line in there but just have some HTML tags wrong around it or something like that?
Thanks again. It was a user issue, I changed the lighttpd user to the current user and the script worked out correctly
Can't you just use [Bash arrays](http://wiki.bash-hackers.org/syntax/arrays)?
http://pastebin.com/8WsbzGTq This is the firewall_rules script it's calling. The rules aren't finished yet, but the stuff that matters is all there.
Your case statements have lots of duplication. You could combine them with | case $1 in chain1|chain2|chain3) firewall_script $1;; *) echo some help; exit 1;; esac
oh boy... &gt; However, the ImageMagick script requires all of them in one variable, something like &gt; &gt; files="1.jpg 2.jpg 3.jpg 4.jpg" this is a problem, a big one really; it happens that whitespace (spaces, tabs and even newlines) are ALLOWED to be in filenames, that comes from the filesystem and any program should be made taken this into consideration. that's why it's hard for me to believe that imagemagick expects you to do this, there should be a way to specify them separately, look for it. now regardless of that, using `read files` is totally wrong because you'll read a string and not the expanded glob, you should call your script with the glob as an argument so bash expands it for you and pass a list to the script, by example: #!/bin/bash # convert script args to array args=("$@") # iterate on an infinite loop for ((i=0; ; i++)); do # if there's no more args break else print current four if ((i+4 &gt; $#)); then break else echo "${args[i]} ${args[i+1]} ${args[i+2]} ${args[i+3]}" fi done if you put this on a file and call it with a glob (such as `*.jpg`) it'll work properly.
The key issue I had in ffmpeg was that i had a start timecode and end timecode (in format HH:MM:SS.MMM) but ffmpeg wants start timecode and duration. Timecode math in that format in Bash was un-fun. If I pass "STARTTIME" and "ENDTIME" as string timecodes, I need to find the difference to hand over to ffmpeg, so I used this: &gt;NANODIFF=$(($(($(($(date -d $ENDTIME +%-H) * 3600000)) + $(($(date -d $ENDTIME +%-M) * 60000)) + $(($(date -d $ENDTIME +%-S) * 1000)) + $(($(date -d $ENDTIME +%-N) / 1000000)))) - $(($(($(date -d $STARTTIME +%-H) * 3600000)) + $(($(date -d $STARTTIME +%-M) * 60000)) + $(($(date -d $STARTTIME +%-S) * 1000)) + $(($(date -d $STARTTIME +%-N) / 1000000)))))) &gt;NUMSECS=$(($NANODIFF / 1000)) &gt;NUMNANO=$(($NANODIFF % 1000)) Then, I can pass "$NUMSECS.$NUMNANO" as the -t variable in ffmpeg. Timecode math sucks...hopefully this will help someone else doing their own timecode olympics.
Why not get the time in epoch format and save a few steps? Date -d "$start" +%s Check out http://m.linuxjournal.com/content/use-date-command-measure-elapsed-time for a good example.
So yeah, there's a billion ways you can do this. I don't like using xargs when it isn't necessary. Here's how I'd do this: tar -cz -T &lt;(find .-mtime -1 -type f ) -f backup.tar.gz You only use 2 processes this way.
 echo -e "uinput\ni2c-dev" &gt;&gt; /etc/modules sed -i '' 's/^\(\s*blacklist i2c-bcm2708\)/#\1/' /etc/modprobe.d/raspi-blacklist.conf Along those lines? I'm not sure how to check for Raspbian wheezy, maybe `uname -a` can give you enough information to scrape to determine?
you cannot use `cd` as a way to get back to where you started, if the script is run anywhere else other than home it won't work, save the initial dir on a variable. also consider using `set -e` so the script aborts if anything fails.
You could do "cat /etc/lsb-release" 
Or just do proper checking of exit codes :)
not in this case, in my opinion you check for errors when those are expected and are part of the correct functioning of the program, here you have a mistake which is not supposed to happen so it makes no sense to check, what you have is a bug.
Found the answer. It is called command substitution. Command substitution allows the output of a command to replace the command itself. Command substitution occurs when a command is enclosed as follows: $(command) or `command` The command substitution $(cat file) can be replaced by the equivalent but faster $(&lt; file). Full Reference: http://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html
haha, the find exec mkdir/rmdir/cp is such a clean and simple solution over trying to parse parent names out of filenames, I'd never have thought of that. Did you come up with that yourself? Much applause either way. 
Good man. I'll be doing my best to remember that one. 
Very cool, thanks for this. The use of exec is indeed very elegant, every other approach I've seen tries to use sed or cut to parse out the directory paths and tend to be very error prone. Off to work now, but hopefully I'll have time to test this tonight, and I'll let you know how it goes. I may in fact need help on the final deletion step as well... 
Relaying more in bash than in find, I do something similar by means of: find (find args) | while read file do ddir=${file%/*} mkdir -p ${ddir/music/flac} cp $file ${file/music/flac} done 
Very well, you did split the task up. Now create functions from that pseudo code. The 'for every line' can be done by setting the var IFS to a newline and then: For line in $(cat line.txt); do #call functions Done 
the problem is that i currently cant try this since i dont have linux and my virtual machine is crashing for no reason (trying to fix this now). And this is my first time using bash and linux so i dont really know what i can do. Im more of a python user.
Why don't you write it in Python instead? Text processing quickly gets hairy in bash.
First this: * learn to use 'man' (on the CLI: man man) * research the commands 'useradd' and/or 'adduser' * research '/etc/skel' or your distribution's variant because most things in your script are done by these commands. in the meantime, here you go: #!/bin/bash function create_user() { echo "create user with the nickname ${1} (first words to ',' ) with the user name ${2} (second word to ','), with password ${3} (third word to ',')" exit_code=$? # TODO: fill exit_code with a sensible value exit ${exit_code} } function log() { logfile="/home/administrator/logs/log123.log" echo $1 &gt;&gt; ${logfile} } function create_homedir() { full_name=$1 user_name=$2 # mkdir -p # copy stuff # chmod } list="/home/administrator/list.txt" list="list.txt" if [[ ! -f ${list} ]]; then echo "list ${list} does not exist" exit 1 fi #for every line of text in list.txt # set 'parameter' separator to comma and newline IFS=", " cat ${list} | while read full_name user_name password; do if [[ ${full_name} == '' ]]; then echo "full_name is empty" exit 1 fi create_user(${full_name}, ${user_name}, ${password}) if [[ $? != 0 ]]; then log("something went wrong while creating account ${user_name}") exit 1 fi log("user ${full_name} with username ${user_name} and password ${password} has been created") create_homedir(${user_name} if [[ $? != 0 ]]; then log("you get the point") exit 1 fi done # #Then the file "stuff" in /home/admin/stuff is copyed into the newly created /home/jbond/otherstuff file and sets the rights of that file and all subfiles to jbond. #then do the same for Gold Finger and any other lines in the list.txt file with the exeption that a new line of text is added into the log123.log file not replaced #else; echo "file does not exist" #the list.txt looks like this: #list.txt #James Bond, jbond, jb123 #Gold Finger, gfinger, gf123 
Can you tell me if this would work as i said i dont really know what im doing #!/bin/bash function create_user() { full_name=$1 user_name=$2 password=$3 useradd -c $full_name -m $user_name -p $(mkpasswd $password) cp "/home/administrator/vaje" "/home/$2/otherstuff" chown [-R] $2 "/home/$2/otherstuff" chmod [-R] u=rwx exit ${33} } function log() { logfile="/home/administrator/logs/log123.log" echo $1 &gt;&gt; ${logfile} } list="/home/administrator/list.txt" list="list.txt" IFS=", " cat ${list} | while read full_name user_name password; do if [[${full_name} == '' ]]; then echo "data missing" exit 1 fi create_user(${full_name}, ${user_name}, ${password}) if [[ $? != 0 ]]; then log("something went wrong while creating account ${user_name}") exit 1 fi log("user ${full_name} with username ${user_name} and password ${password} has been created") done 
&gt; Why mkdir and then rmdir, rather than just mkdir and cp? Because the way I'm using mkdir will result in a folder with the same name as the file. You'd end up with something like `~/music/Artists/Song.mp3/Song.mp3` without `rmdir`. &gt; In the second find, I also want to bring over .log and .cue files, so I can just add... Yes, that's correct.
I thought about per-line-processing, but then I would have to split each line again in its fields. But then real life came up and I chose the IFS way out.
I *think* I see what you're saying, but I'm not sure if I understand. If I'm understanding you right, you're saying that in this example d@AirBox:/tmp$ for line in $(head -1 file.txt);do echo "$line";done James Bond, jbond, jb123 what you want/expect is for the output to be the same as head -1 file.txt, but it isn't. That's because the for loop takes the output of $() and separates it by the default IFS which is a space and that means each block of text separated by white space is one iteration of line. So you change the IFS to "\n" and now it works like you'd expect, because now each block of text separated by a "\n" is an iteration of line, right? Thing is, the while loop will take care of that for you. By default it reads one line at a time, so things work out: d@AirBox:/tmp$ head -1 file.txt |while read i;do echo "$i";done James Bond, jbond, jb123 That aside, as I mentioned in my linked comment, there are very real performance implications of for loops vs while loops. For example, I made a file.txt which is just the two example lines the OP provided repeating a million times. Then I timed doing both kinds of loops: d@AirBox:/tmp$ time for i in $(cat file.txt);do echo $i &gt; /dev/null;done real 0m50.267s user 0m39.594s sys 0m10.325s d@AirBox:/tmp$ time while read i;do echo $i &gt; /dev/null;done &lt; file.txt real 0m21.664s user 0m18.005s sys 0m3.556s and using the while loop took less than half the time, and if you try this out yourself, you can see the memory usage spike way higher on the for loop than the while loop. TL;DR: While loops, dude. While loops are the shit. ___ Yeesh, sorry to wall of text on you, there. Just been kind of a shitty day, and for some reason writing about shell stuff always mellows me out. So since I'm too lazy to blog, I write long winded posts in obscure subreddits. 
Sorry to hear you had a louzy day, just go ahead and vent :-) . I should have referenced my script below where I went with IFS=',\n' (comma and newline) and ' | while read x1 x2 x3; ...' Because of the experience level of OP, I used as clean bash as I could and as straightforward: code read from left to right 'cat input.txt |' is easier to follow than putting a ' &lt; input.txt' at the end. I agree about the while loops; I use them when I don't know how many lines/items I can expect. I'd say for loops are for (small) llists of single items.
Try doing an optional space. ' *'
Thanks for the reply. I was using %d when I should have been using %e for the date field. I new it would be something stupid *slaps forehead*
Can you elaborate? Are you looking for a list of files on your system or ....? Because that would be predominately exclusive to your setup. Just to clarify terms, which is important in conveying your request, I don't think you're really asking for a 'database' (such as some derivative of sql) more than just a list. Or are you? Your request requires clarification. Assuming you want to find every file in file in a particular directory tree and determine size, ownership, and modification date, for example, this would be sufficient: find . -exec ls -ld {} \; Drop the 'exec' portion if you just want a listing of all files recursively. Find is your good friend in such a case, try it's man page. You can then use the -exec or a pipe to act on each file in turn. Or a for/foreach loop. Good luck!
I am trying to get a series of files similar to what I would interact with in a real world job.
Thank you
 man ps Mostly commonly I use `ps aux | grep SEARCH_TERM`. If you already know the process name, you don't need the PID, e.g. `pkill firefox`.
`pgrep` would be a much better tool to use than `ps | grep`
I think the processes he'd like listed are those that appear when only `ps` is entered with no arguments. This will restrict the processes shown to those related to the current session: ~$ ps PID TTY TIME CMD 4271 pts/3 00:00:00 bash 4293 pts/3 00:00:00 ps ~$ cat &amp; [1] 4379 [1]+ Stopped cat ~$ ps PID TTY TIME CMD 4271 pts/3 00:00:00 bash 4379 pts/3 00:00:00 cat 4388 pts/3 00:00:00 ps ~$ 
Ah, well I suppose that might be handy.. something like the following might help: function myPIDs { ps | awk '{if(NR!=1)printf "%s ",$1}' } export PS1="$PS1\$(myPIDs)" this would go in your .bashrc somewhere, although I use Zsh so I haven't thoroughly tested this **edit**: after playing around and testing this in bash, I realize how cluttered this will get, as it will list PIDs for `ps` and `awk` and your shell every time, so even without any jobs running in the back, you will have at least 4 or 5 PIDs by default.. so my solution doesn't really work that well, but I'll leave it here anyway. **edit 2**: woke up, reread what I'd posted, and realized the above problem would be easy to fix, just `grep -v` out the processes you don't want, or, still using awk: function myPIDs { ps | awk '!/(bash|zsh|awk|ps)/{if(NR!=1)printf "%s ",$1}' } I noticed that .bashrc for some reason doesn't like the function being a one liner, which is why I split it over a few lines. You will have to play with the `export PS1` part to get it to look how you want it, the important part is just to include `\$(myPIDs)` in it where you want it. Hope this helps, /u/Trout_Tickler!
Don't use grep used sed instead. example: du -hs /Applications/* | sed 's/\/Applications\///g' Don't use a screwdriver to hammer a nail. Use the right tool for the job. :-) 
thanks
You could always do this: (cd /Applications &amp;&amp; du -shc *)
Your double `//` is hard to read. How about: sed 's|Applications/||g'
 du -hs /Applications/* | sed s%/.*/%%g This will also work on other different folders and path depths
I think you need to use either == or -eq, instead of your =, when comparing the strings.
Surround both sides of the equals in double quotes and you may need to move to [[]] and a ==
the hell are you doing to my formatting, reddit.
From the bash(1) manpage: The shell treats several parameters specially. These parameters may only be referenced; assignment to them is not allowed. * Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, it expands to a sin‐ gle word with the value of each parameter separated by the first character of the IFS special variable. That is, "$*" is equiva‐ lent to "$1c$2c...", where c is the first character of the value of the IFS variable. If IFS is unset, the parameters are sepa‐ rated by spaces. If IFS is null, the parameters are joined without intervening separators. @ Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. That is, "$@" is equivalent to "$1" "$2" ... If the double-quoted expansion occurs within a word, the expansion of the first parameter is joined with the begin‐ ning part of the original word, and the expansion of the last parameter is joined with the last part of the original word. When there are no positional parameters, "$@" and $@ expand to nothing (i.e., they are removed). 
Bash supports arrays. You could add each kernel name to an array like so: mykernels[$i]=$mykernel before/right after you print the name of an installed kernel. once you read the selection you can get the name of the kernel that should be removed using: ${mykernels[$remkern]}
That's worked a treat. Thanks so much for the suggestion.
glad I could help ;)
[ no need to re-invent the wheel - solution/s here ]( http://www.commandlinefu.com/commands/view/5813/remove-all-unused-kernels-with-apt-get) 
How about read -p "Continue or quit? [C/q] " reply if [[ "$reply" = q || "$reply" = Q ]]; then exit fi `read` assigns user input to a variable, here the variable `"$reply"`. The `-p` option to `read` specifies the prompt to be used. In the `if` statement, the program exits if the user hits `q` or `Q`; otherwise, the program continues. Presumably more code comes after this. `[C/q]` conventionally means that `C`, because it's capitalized, is the default case, i.e. hitting any key except `q` (or `Q`, just to be safe) will continue. (Actually, I think it's more conventional to say: `Continue? [Y/n]`. But it's up to you. In any case, numbers are usually only used with quite a large menu, and even then, single letters are usually preferred.)
seepeeyou posted the most appropriate method. Here is something similar just using echo instead. echo -n "Should we continue [Y\N]:" ; read reply ' insert actions here.
I'd love to see something similar to [whiptail](http://linux.die.net/man/1/whiptail) or [dialog](http://linux.die.net/man/1/dialog) but less full-screen and more meant to be used inline in a console session. Anyone have any idea?
Select command may work nicely, too: select q in Yes No; do break; done; [ $q == "No" ] &amp;&amp; exit 1; Though it appears to be more useful for making a dynamic menus: select f in *; do file $f; done
Read these: http://www.linuxjournal.com/article/2807 http://linuxgazette.net/101/sunil.html
Any ideas on how to do this without waiting for the user to press enter? I mean have the script continue execution once a character is entered?
Use the `-n` option, which specifies the number of keystrokes that `read` allows the user to input. Normally, `read` waits till the user hits `&lt;enter&gt;`, but `-n 1`, for example, will only allow 1 keystroke. $ read -n 1 -p "Continue? " reply Continue? y$ echo "$reply" y $ Notice, however, that after hitting `y`, I'm stuck on the same line. To fix this, you can add a `&amp;&amp; echo` at the end, which echoes a blank line once `read` finishes. $ read -n 1 -p "Continue? " reply &amp;&amp; echo Continue? y $
Nice. Thanks.
I do something like this: #!/bin/bash echo "Preparing to do something:" echo "" echo "" echo " STUFF HAPPENING HERE " echo "" echo "" echo "CHECKOUT OPTIONS" echo "1: Option 1*" echo "2: Option 2" echo "3: Option 3" echo "x: Cancel" echo "" echo "Please select the checkout option(#) you would like to perform" read a case $a in 1 ) echo "Are you sure? Y/n" read a1 if [[ $a1 == [Yy] ]]; then #do shit here, you had to say yes you really want to fi ;; 2 ) #do simpler shit here ;; 3 ) #more simple shit ;; [*] ) echo "nothing setup, but this it working" ;; esac
As your question has already been answered, I'll just add that you can drop the i=1 declaration if you change the $((i ++)) to $((++i)). Putting the ++ before the variable causes the variable to be incremented before the value is returned.
Any suggestions to make the leanest, meanest bash script possible are always welcome!
few things :) first of all the argument to `alias` is a single word (just like variables) so if your alias has spaces you need to escape them like this: `alias hide=bash\ hide.sh` or quote them like this: `alias hide='bash hide.sh'` (you may want to read about the differences between `'` and `"` but in this case it doesn't matter). now running `bash yourscript` isn't really the unix way :) you should put a shebang and make the file executable so it runs as a command simply by typing `./hide.sh` if it's on the same directory you're in, even better if you place it on some directory included in your `$PATH` it'll execute from *anywhere* like any other command, you won't even need to specify the path yourself, just `hide.sh` will work. a final remark regarding the extension, turns out `sh` is a different interpreter than `bash` (and even when `bash` is running in place of `sh` it detects the way you're invoking it and runs in compatibility mode) so it doesn't make much sense to use ".sh" extension for a bash script, you can use ".bash" since this is not DOS and you won't get an error for the extension being too long but if you followed the previous advice it's probably a better idea to omit the extension entirely and just run the command as `hide` which also eliminates the need for an alias :) btw, how do you hide folders with chmod? I've never seen that.
Add #!/bin/bash to the first line in your hide.sh script. Rename 'hide.sh' to just 'hide'. Do a 'chmod u+x hide'. Done. No need for an alias. Now you can run: hide &lt;dirname&gt; Also, what's in the hide script? Maybe you're not handling the passed file/dir correctly.
Thanks for all the help. I did have the directory in my $PATH, and can run it using ./ only if I'm in that directory. I think that's how it's supposed to work, but correct me if I'm wrong. I meant chflags, not chmod, sorry: chflags hidden /path/to/file
That was my thought. The hide script needs to be somewhere on your path. I usually add /home/${USERNAME}/bin to my personal path, but there are known minor security issues with doing this. Instead, I'd probably add this to my `~/.bash_aliases` : alias hide='/home/juser/bin/hide.sh' #set up the alias Assuming my user name was `juser` and the `hide.sh` script was in that location. You would also have to `chmod u+x /home/juser/bin/hide.sh` to give the script permission to execute. 
ah, Apple/BSD then? No wonder I've never heard of `chflags`
Great answer. Two clarifications. (maybe OP already knows this but maybe someone else doesn't): &gt;you should put a shebang This is a [shebang](https://en.wikipedia.org/wiki/Shebang_%28Unix%29): `#!/bin/bash` It should be the first line in your script. &gt; and make the file executable `chmod +x &lt;file_path&gt;` will do it. However this makes it executable for owner, group and world. You should read up on unix permissions if you are not familiar. [This](http://tldp.org/LDP/GNU-Linux-Tools-Summary/html/file-permissions.html) looks like a good explanation. It's for linux but most of it should translate to other unixes.
This is a viable alternative too. Functions are like alias' cool uncles that buy you beer.
Thanks for the help. I did put a shebang in the file, but the hint about chmod really helped. Thanks for the link as well.
Because you're using / for sed and your input results in too many of that character. change s/$directoy1\///g to s%$directoy1\/%%g and it should work (you can use other characters than / and %, those are just the ones I prefer.
This is fucking fantastic. Thank you for sharing
I'm trying to add a feature to en existing script, that is already written in bash. But i suppose I can just do it in python, and call it from the existing script.
`tail`?
how about "`| grep -B 50`"?
no tail just holds to the last n lines specified, it doesn't suck up the whole stream in memory as it has no need for it. what Chandon suggests is to run a process in the background and use the main process to check, it's worth mentioning that by the description of your problem you're dealing with concurrency and thus, won't be able to get accurate results as you may expect, I'll get into this later. Chandon's example in bash would be: spewingprocess &gt; whatever.tmp &amp; while read line; do if [[ $line = match ]]; then last50=$(tail -n 50 whatever.tmp) ... fi done &lt; log so what this does is to first run the spewingprocess and redirect its output to "whatever.tmp" file, since the line ends with an ampersand it'll be run in the background, that means it'll run in *parallel* with the rest of the script. after that we have a read-loop that is fed from the log file (last line), within its body we check that line from the log and if it's what we're looking for then we capture the last 50 lines so far from the output and do something with it I guess (the whole output is being buffered on disk). since both the main process and the spewingprocess are running in parallel, by the time we've checked and captured the output, more output could've been generated so it's highly unlikely that you'll get useful results from this approach however it fits exactly your description of the problem. maybe if you add more details about what you're actually doing we may provide an alternative, sometimes someone thinks a problem should be solved in a particular way but overlooks another approach which may be simpler or better.
I am going to need two processes in parallel, AFAIK. I have the spewing process that needs to be collected. (Hopefully not causing a big issue). While in parallel I tail -f a separate log. Let's call it turtle.log When I find a match in turtle.log I want to look at the last 50 lines from the spewing process at that time.
Not robust at all, but hopefully this will point you in the right direction. Use as your own peril : for file in * do #Get the number part of the file number=$( echo $file | sed -e 's/.*_\([0-9]*\)\..*/\1/g' ) #pad the number part to three digits number=$( printf '%03d' $number ) # rename the file, after a dry run, just delete "echo" from this line to do the move echo mv $file $( echo $file | sed -e "s/\(.*_\).*\(\..*\)/\1$number\2/g" ) done This will definitely break if any of your files have spaces or special characters in the name. And of course you'll see errors about "File x and file y are the same file" for the files which are already three digits (but it will still work). As I said, it's pretty basic.
From inside the directory you're in. All bash internals and someone beat me to it. :P #!/bin/bash find ./ -type f -iname "*.png" | while read line do fileName=`basename $line .png` dirName=`dirname $line` fileSeq=${fileName##*_} if [ ${#fileSeq} = 1 ]; then fileName="${fileName%_*}_00${fileSeq}.png" mv $line $dirName/$fileName elif [ ${#fileSeq} = 2 ]; then fileName="${fileName%_*}_0${fileSeq}.png" mv $line $dirName/$fileName fi done
O'Reilly has published [a great book](http://shop.oreilly.com/product/9780596009656.do) on the bash shell. Part of it is really more advanced stuff but I really liked it. If you don't want to buy a book any beginners guide you find [on google](https://www.google.de/search?num=100&amp;safe=off&amp;q=bash+beginner+guide&amp;oq=bash+beginner+g&amp;gs_l=serp.3.0.0i19l2j0i8i30i19l2.4627.6630.0.7511.4.3.1.0.0.0.95.242.3.3.0...0.0...1c.1.12.serp.tDNgxw1U-JU) or on youtube will do. Also, you should learn how to use [man pages](http://en.wikipedia.org/wiki/Man_page). When you learned your first bash commands look at the man pages for them play around with the commands a bit. Learning by doing is a great strategy. Concerning /etc: You can read the [file system standart](http://www.pathname.com/fhs/), which contains a section on that directory. It's really just a directory where some configurations are stored in specific files, but it explains some of these files too. The most important thing is: Don't be afraid to ask. Reddit is good, stackoverflow oder stackexchange are great, too. Hope that helps you out a bit.
Some of my favorite Bash resources: + [BashGuide](http://mywiki.wooledge.org/BashGuide) - an introduction to shell scripting with Bash. + [BashFAQ](http://mywiki.wooledge.org/BashFAQ) - frequently asked questions about Bash scripting (same site/people as the BashGuide above) + [Bash Hackers Wiki](http://wiki.bash-hackers.org/start) - lots of great shorter entries A lot of Bash scripting (shell scripting in general) is putting together calls to other standard command line tools like `awk`, `grep`, `sed` and so on. So a good idea is to learn at least something about these sorts of utilities. There's tons of good resources for these, but I really like Scott Granneman's [Linux Phrasebook](http://www.granneman.com/writing/books/#LinuxPhrasebook). Also, one Bash tutorial I would avoid (I won't link it): the Advance Bash Scripting Guide. It's hugely popular, so you are likely to come across links to it, but it's very idiosyncratic in many places and won't necessarily teach you good habits.
[Apple's shell scripting primer](https://developer.apple.com/library/mac/#documentation/opensource/conceptual/shellscripting/Introduction/Introduction.html)
_UNIX Shells by Example_ , Ellie Quigley Available at Amazons everywhere. Will also cover some sed and awk.
I'm writing a script for the HelpDesk at my work. The spewing process is a datastream provided by the users. (there will be more than 1, generally 3-4 at a time.) The log is the from the program that is meant to read these streams and make use of the user data. When there is a batch of data that is invalid, the program can report to it's log that the data is invalid, and exactly what the information was. But not say who sent it in. So I want to cache the spewing processes, so that when the log reports the bad data in it's log, I can search the recent history of each spewing process, and identify who sent in the bad data.
Thank you!
 tail -f -n 50
 join tmp1 tmp2 | sed -e 's,'$directoy1',,g ; s,'$directory2',,g'
Hows about this? I took the liberty of having it sort by size in human-readable format. du -sk Applications/* | sort -rn | cut -f2 | xargs du -sh | sed 's|\tApplications/| |g' du -sk to get size in kb, sort to sort by size, cut to grab the path, xargs du -sh to get the size of the newly sorted list in human-readable format. Then, sed to remove the tab (\t) and the folder path. 
Oh the terrors of xargs and how they can ruin your day. I've seen horror stories where xargs was coupled with rm in a server environment, finding not only . but .. as well. 
That sounds like a particularly nasty combination. Fortunately I've cut out the middle man in the past and just used an extra space before a * in my rm command--only when it was taking longer than expected did I realize the error of my ways.
I think you can just parse the output easily wit an awk oneliner: find $u_dir -iname "*.jpg"|while read foto; do exiftool -DateTimeOriginal -n $foto|cut -d: -f2-|tr " " :|awk -F: -v foto="$foto" '{print "ln -s "foto" "$2"/"$3"/"$3$4"-"$5$6$7".jpg"}' ; done but then I'd probably write down a small script with some more error checking before running it.
man bash .. seriously, it's the best thing out there for learning bash.
how about process 1: while true; do process_name | tail -n 50 &gt; logfile.log; done process 2: grep 'search_string' other_logfile &amp;&amp; kill process 1 So you constantly output the process into logfile.log in 50 line blocks, each overwriting the next.. when your other script finds the match it stops the first spooling process so only the last 50 lines remain.
Now Tayne I can get into.. mmhmm!
Indent please, it will make it easier to read and write your code.
That line is actually fine. The "-q" flag to grep suppressed output, so the if works on the exit value of the command.
Oh ok. Didn't know that. I will try that and let you know if it works. Thanks!
I actually looked that line up so I'm fairly sure it is correct but if you had to point to a particular point what makes you think it looks wrong?
I would use ruby for this, but that's cause I know it better.
I didn't realize backticks were deprecated, I normally use some form of command substitution but recently saw somebody using backticks and thought it was pretty cool. I did get the script working, or as much as I could. Turns out the serials I was using to search weren't really in the data files they gave me to look for them in. Thanks!
Yeah my scripting knowledge isn't great and I've only done basic stuff in a few. So I'm trying to remedy this by learning perl and bash better, required by work, then python, because I want to.
Python is really good to know. I have found it really useful in various situations though I don't use it regularly. Also trying to get better at bash though. 
backticks can get unpleasant when nesting
"one command"... You're funny. I'll give this a shot when I get a chance. Thanks! 
It is one command! Fine, I cheated with subshells, loops, and pipes, but it still counts!
my thought is i can then go through the arrays and pop out needed IP whenever it reaches point of next array variable? 
Have you tried passing your query to mysql on the command line? You could pipe it straight into a file from there. I think it works something like this: mysql -uroot -ppassword database_name &lt; select email from users &gt;output.txt 
You may be missing quotes around $email: mysql -h dbserver -u username -p mydatabase -e "select username from mytable where email=\"$line\";" I would also suggest to echo the mysql line, instead of executing it, to check for obvious errors. Eg: echo mysql -h dbserver -u username -p mydatabase -e "select username from mytable where email=\"$line\";"
This is exactly what I wanted, thank you so much.
 DBNAME=db_name DBUSER=db_pass DBPASS=password FILE=/home/user/file for line in `cat $FILE` do mysql -u${DBUSER} -p${DBPASS} ${DBNAME} -N -s -r -e "select username from table where email=${line}" done Your ability to dump may be limited due to the inability to lock the table. Try using mysqldump --lock-tables=false and see what happens.
You can use shopt -s histappend for what you're wanting to accomplish. Without that set, bash reads your history file every time a shell is executed. When the shell terminates, the last X many lines (the environment variable HISTSIZE controls this) are written to the history file, but it is NOT appended. With "shopt -s histappend", the new entries will be appended to the end of what's currently there. Also, take a look at this [Bash History](http://unix.stackexchange.com/questions/1288/preserve-bash-history-in-multiple-terminal-windows/1292#1292) That gives you a nice way to have concurrent sessions 'share' the same history, by setting your env variable PROMPT_COMMAND like so: export PROMPT_COMMAND="history -a; history -c; history -r; $PROMPT_COMMAND" Stick the export and shopt lines in your .bash_profile and/or .bashrc. This should work on any recent bash shell, but older variants found in some operating systems, such as Solaris (specifically Solaris 9 and older), might not work.
Seems to be working! thx!
Not a problem! Glad to help.
My .bash_profile: PROMPT_COMMAND="history -a" shopt -q histappend || shopt -s histappend shopt -s cmdhist check out the google results for these, and tailor to fit your needs.
I use backticks because it's a luxury Unix affords me.
Forgive my ignorance, what would you use something like this for?
I'm using it exactly for what the script shows, to spawn several `screen` sessions on the background but first check if they already exist. the thing is: if I want three sessions I don't have to call `screen -list` three times, I can process its output in three different ways with a single call/stream. it can be used in any situation where you need to process a single stream in several different ways, specially when you *can't* get the stream as many times you want.
Thank you for the explanation. 
Your explanation does not match the snippet you posted, which calls `screen -list` three times. Plus it has unmatched braces and parens, so it's invalid. A much more straightforward solution that doesn't require even calling out to grep at all: running=$(screen -list) [[ ! "$running" == *.app1* ]] &amp;&amp; (cd app1 &amp;&amp; screen -dmS app1) [[ ! "$running" == *.app2* ]] &amp;&amp; (cd app2 &amp;&amp; screen -dmS app2) [[ ! "$running" == *.app3* ]] &amp;&amp; (cd app3 &amp;&amp; screen -dmS app3) 
ah damn I screwed it when copypasting, I've edited it now thank you. btw the last one could connect to the end of the pipe instead of spawning another subprocess but meh :) regarding your alternative it's fine for small inputs I guess but thinking about streams, it can grow large (or even not finish at all) but it's valid too :)
I work with the CodeEval team and wanted to let you know that CodeEval now supports the BASHcommunity and we'd love to invite everyone to join. CodeEval is a great place to compete against fellow developers as well as have fun solving challenges. Please let your friends know or tweet about it! 
Very interesting code, thanks for sharing. I was "inspired" by the PROMPT FULL conditional statements... That was just what I have been looking for. I can see you have not opted to use Bash's internal GETOPS for command-line arguments handling. And your workaround was just what I needed. Getopts boggles my mind.
This is true wizardry
Wow I had not even noticed that until now. I was only subscribing because I thought this was yet another command line subreddit. &gt;This is where all of the funny quotes on the internet are found Edit: I'm out, IRC quotes are a huge waste of time. 
this subreddit is about the bash shell by popular decision, it's what the content is about. I've asked the mods to rename the sidebar but there was no response, I guess they're not active anymore. I'll ask on /r/redditrequest in the meantime there's /r/commandline
did not know about r/commandline. Thanks!
Somehow posting is blocked. I posted: [Poor man's git](http://en.reddit.com/r/bash/comments/1i9y8r/poor_mans_git/) &amp; [Did I post in the wrong subreddit?](http://en.reddit.com/r/bash/comments/1ir51r/did_i_post_in_the_wrong_subreddit/) and you can not see them in /r/bash :-(
&gt; gests bash.org or perhaps qdb.us. But all the latest posts are about bash scripting. So what's it really about? Any idea as to how long it would take to hear back on this?
I've tried about a dozen different tweaks on this. I'm stumped. 
for i in {0..4}; do echo $(seq -s "," $i 5 59);done Why did I forget "seq" 
You can use `eval` to do what you want: &gt; `eval echo {$((($RANDOM % 6) + 1))..60..2}` ... &gt; `eval echo {$((($RANDOM % 6) + 1))..60..2}` outputs (eg): &gt; 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 ... &gt; 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 
can you explain your desired goal in english? your examples do not help.
zsh! (i'll let myself out.)
The $[ ... ] syntax has been deprecated for over 20 years. It was removed from the documentation over 20 years ago. The only reason it still works is because of bash's policy on backwards compatibility. Why are you still using it? Use the $(( ... )) syntax for arithmetic expansion. Anyway, to iterate such a sequence, use a C-style for-loop for ((i = RANDOM % 6 + 1; i &lt;= 60; i += 2 )); do echo "$i or whatever" done TL;DR: **STOP USING UNDOCUMENTED SYNTAX THAT HAS BEEN DEPRECATED FOR DECADES**
first of all, don't use reddit's quoting (`&gt;`) for code, use four space indentation; I thing that also screwed up your sample as some characters we taken as formatting, please try again.
How should I have done it? here's the paste bin http://pastebin.com/6HU3SQR3
1. Thanks, I must have dropped the line referencing it. 2. I did this in terminal with the fc editor. So it wasn't HD space but typing. 3. I don't want all the links but specific links
Re: 3--yes, so look for those links.
just copy /home to your HDD, remove /home on the SSD, and set the HDD to mount on boot automatically in your /etc/fstab
I'm going to do this, but I'd also like to write the script. Thanks!
So it seems you're just saving all the `*.foo` in a string and iterating over each set of characters separated by spaces? How is this different from the array notation with `variable= *.foo *.bar)`? Is the way I did it incorrect?
The great thing about programming is that you can accomplish the same thing in a number of ways. Iterating over a string is just simpler in this case, I'd think.
You would get a much better response in /r/commandline.
Personally, I prefer to take up less space by changing the color of my hostname. Of course actually getting the return code has it's merits. PS1="`if [ \$? = 0 ]; then echo -n '\[\e[32m\]\u\[\e[34m\]@\h \[\e[36m\]\W\[\e[m\] \[\e[32m\]\$ \[\e[37m\]'; else echo -n '\[\e[32m\]\u\[\e[31m\]@\h \[\e[36m\]\W\[\e[m\] \[\e[32m\]\$ \[\e[37m\]'; fi`" A cool one I've seen is: If $? = 0; then ^-^ user@host else -_- user@host
I'd just use awk, I have way too many apps in my OS X box to run that du command so I did it to another directory example: $ du -sh /Users/shared/* 0B /Users/shared/Adobe 150M /Users/shared/Battle.net 800K /Users/shared/Blizzard 4.0G /Users/shared/CentOS-6.3-x86_64-bin-DVD1.iso 0B /Users/shared/Parallels 0B /Users/shared/SC Info 4.0K /Users/shared/change_pw.sh 684M /Users/shared/ubuntu-12.04-server-amd64.iso piped to awk: $ du -sh /Users/shared/* | awk -F/ '{ print $1 $4 }' 0B Adobe 150M Battle.net 800K Blizzard 4.0G CentOS-6.3-x86_64-bin-DVD1.iso 0B Parallels 0B SC Info 4.0K change_pw.sh 684M ubuntu-12.04-server-amd64.iso The -F switch is the field delimiter, and I set it to '/' then I just print the values for desired output and use the slash as the delimiter. Hope this helps
For a while, I was doing green :), purple :? and red :( for 0, 127 and everything else. Got a bit awkward when I had to take screenshots for auditors, though.
Looks like there hasn't been a non-scripting post in the last 2 years...
Seems cool, but why not just install git?
If you use this you don't need to learn how to use git. When you need previous versions of files you just check $HOME/var with your favourite File System browser. I had forgotten about this post. It seems that /r/bash doesn't have anything to do with 'Bourne Again Shell'. I posted (updated version) also here: [BASH Poor man's git.](http://en.reddit.com/r/usefulscripts/comments/1isswv/bash_poor_mans_git/)
Ok I have a bash script going, and I have it set to infinite loop and it sends an email when the host goes down. Can someone please help me make it so that it only sends one email when the host goes down and one email when it comes back up? #!/bin/bash while : do for i in $@ do ping -c 1 $i &amp;&gt; /dev/null if [ $? -ne 0 ]; then echo "`date`: ping failed, $i host is down!" | mail -s "$i host is down!" my@email.com fi done done
For anyone interested; userdirs="user1 user2 user3 user4" awk -v userdirs="$userdirs" ' BEGIN { split(userdirs,users) } { for (i=1; i in users; i++) if ($N == users[i]) entry[i] = $4 OFS $7 OFS $14 } END { for (i=1; i in users; i++) if (i in entry) print entry[i] } ' /var/log/xferlog
So?
Um, yeah. It's a steam of pseudo-random data. It may block, but there's no EOF. If yours is that quick you must have plenty of entropy. That's a good thing.
Is there any metric or scale to determine how good one's entropy is?
/r/thinkingoutloud ?
my /dev/random only spits numbers if I move the mouse or keyboard...
I should clarify, this was on OS X, which uses the Yarrow prng algorithm, feeding data from kernel jitters.
Have you heard about RCS?
Please use this [updated version](http://en.reddit.com/r/usefulscripts/comments/1isswv/bash_poor_mans_git/). Note that you can have as many 'FileVersions_*' files as you want, each with its own conditions.
This seems like overkill to me. Why not just write a function in bash and stick it in your rc file? You wouldn't have to worry about situations where you might not have compiler access.
I know a guy who uses pushd and popd almost exclusively. He'll likely consider this good news.
I'm a huge fan of pushd/popd for this, but recently started using [autojump](https://github.com/joelthelion/autojump). Works really well and has made my life much easier.
Do you mean reimplement CD(or under another name), or is there a way to make Bash call my function on every command I execute?
You can replace builtins with functions in bash. I have a custom `cd` in my bashrc that does some special stuff. As an example, here's a cd function that allows you to `cd ...` or `cd ....` to go up two or three directories: function cd { if [ -z "$1" ] then builtin cd else dir="$1" if egrep -q '^\.\.+$' &lt;&lt;&lt; "$dir" then dir=$(cut -c2- &lt;&lt;&lt; "$dir" | sed 's|\.|../|g') fi builtin cd "$dir" fi } Now I can use my custom `cd` function in the shell like so: mlocate$ pwd /var/lib/mlocate mlocate$ cd ... var% pwd /var 
So my particular use case for this is I pump returned data into splunk for reporting/dashboards. The hosts I'm interested in live in a file for which I want to grep and perform the **echo status | nc hostname port** on. How I perform this task currently is similar to below: for x in {list_of_hosts}; do echo status | nc $x port; done I'm under the impression I can use xargs with grepping from the host list file to do this asynchronously, and therefore, more efficiently. That's where I need your help. Let me know if you need more info. 
How do you intend to use the output? p=80 for h in $(cat hosts-names.txt); do echo "${h} $(echo status | nc ${h} ${p})" &gt;&gt; output.log 2&gt;&amp;1 &amp; done Something like that will give you a log of hostnames and `nc` results with each call launched in the background. It may or may not work well depending upon your resources and the number of hosts involved. 
I pump the script's stdout directly into [splunk](http://splunk.com) for dashboards and reporting. Ideally I would not like to dump output to a file as it adds complexity to the problem I'm trying to solve. I would prefer splunk to handle persistence as there are a good number of hosts (around 100) to deal with. I was hoping this problem could be solved with **xargs** trickery, but my shellfu is not quite that advanced. My other solution would have been to implement some sort of circular buffer with nodejs to perform the same thing, but I'm not prepared to add that complexity if I can perform the same thing in shell code.
The use of a remote shell uses too much overhead and defeats the point of what I'm trying to do via executing telnet commands in parallel. These are also production hosts I'm dealing with. I can't just install another shell on those machines without getting a lot of sysadmins/network security folks in a fit.
You might find this useful https://www.gnu.org/software/parallel/ I also have a couple of bash functions you might like if you don't want to install additional software, but I suggest you start with gnu parallel. Cheers.
pdsh would only have to be installed on one system. You would use it to run rsh or ssh connections in parallel. I understand about the remote shell being too much overhead though if you're running really tight.
I always use [Capistrano](http://en.wikipedia.org/wiki/Capistrano) for this stuff. Assuming you have a distro with apt just apt-get install capistrano on your local machine (not the production servers, obviously), start up a shell with "cap shell" and then [have at it](http://weblog.jamisbuck.org/2006/9/21/introducing-the-capistrano-shell) EDIT: Nevermind, saw you need telnet not ssh. Write the commands in a multi-threaded wrapper [like this](http://www.linuxquestions.org/questions/programming-9/how-can-i-do-multithreading-in-shell-scripting-904135/) 
Sorry, I hadn't read through enough of pdsh docs. But yeah, ssh is not needed at all in this case. I had actually started with using https://github.com/tsmith/node-control to do this with ssh in parallel. I'm essentially looking for the same, but with telnet. I could still use nodejs, but I would most likely have to build out an module, and the testing I did with one host inputting to stdin would result in output that would always insert a blank line after the first line of telnet output (if there were a multiline response).
This does seem very promising being part of GNU. Thanks!
I hadn't considered a multi-threaded aproach. I'll be looking at this, too. Thanks!
I can see you put a lot of thought into this, but does it capture stdout of background processes or just check on exit code status?
Please read 'BackgroundProcess --help' ............................... Background process' info: Text that when sourced (with 'eval') gives values to following variables: 'CommandLine': Command and parameters of process when launched. 'ElapsedTime': Time (in seconds) the process was running. 'ExitCode': Exit code of process. 'StandardOut': Output generated by process while running. 'StandardError': Error output generated by process while running. Note: To avoid mismatch in the number of ",',`,´ (which would cause 'eval' to fail) each of them can be replaced by \" in 'StandardOut' &amp; 'StandardError' (Some commands output: `Error message') by using function 'CleanText' (See $COMMON_BIN_DIR/BashCodeRepository). 
Ah, thank you. This may come in very handy. I very much like the structure of this script.
Yes this is basically what I've been fighting with, perhaps I'm being a bit too lazy and should just write it up. 
I thought the formatting was provided by your terminal emulator, not Bash.... what are some of these Bash formatting control characters? Since your base program is Perl, wouldn't it make more sense to skip Bash? Read the files and folders in Perl, create appropriate data structures, and send to the desired output formatter. As such this might be better for a Perl subreddit. As a fallback (and this is very very hacky) you could probably use sed to turn the formatting characters into HTML tags that don't require closing or proper nesting (p, li, there's a few others), slap on a header and footer, then run it through and HTML-tidy utility (xmllint might suffice, but it's very picky). Oh, and your gut is right in rejecting creating separate files. That should lead you to the proper unix philosophy - process data, and let the user decide where it should go (via IO redirection). IOW, just print it to stdout and let the user `&gt; report.html` if desired.
RTFM myself. actually re-reading the man for both ksh and bash. I discovered the magic pill shopt -s lithist now Bash rocks again, thank myself ^-^
And for the other part if you hit CTRL+x CTRL+e it will load your current command in your default editor and execute that line when you exit it.
&gt;Bash has a major conceptional drawback. * &gt;it sometimes returns confusing results when you fail to remember these two things are not the same. The man page for local clearly states it gives an exit status. &gt;For instance, this bit of code is a clean (even if contrived) way to parse a syslog file for all executions of the CRON subsystem output=`cat /var/log/syslog | grep CRON | wc -l` Not very clean either. Useless use of cat and wc and expensive (computationally) use of pipes, lack of quotes - as well as using backticks in place of $() local output output="$(grep -c CRON /var/log/syslog)" Even then; the following would be preferable: if grep -q CRON /var/log/syslog ; then echo "Cron entries found" ; else echo "No cron entries found" ; fi Of course it can be written out in a fashion that isn't a one-liner. I'll admit it was purely an example, but it wasn't a very good one. Worth noting as a gotcha I suppose.
As advertised. `man builtins` local [option] [name[=value] ...] ... The return status is 0 unless local is used outside a function, an invalid name is supplied, or name is a readonly variable. 
you should use bash regexp matching since you plan to store those in bash variables: $ line="./autotrim_$RANDOM.mpc[${RANDOM}x$RANDOM+$RANDOM+$RANDOM] mpc 991X590 8-bit DirectClass 11.1KB 0.000u 0:00.010" $ echo "$line" ./autotrim_8560.mpc[18212x14745+92+10833] mpc 991X590 8-bit DirectClass 11.1KB 0.000u 0:00.010 $ pattern='autotrim_([0-9]+)\.mpc\[([0-9]+)x([0-9]+)\+([0-9]+)\+([0-9]+)\]' $ [[ $line =~ $pattern ]] &amp;&amp; echo match match $ echo "${BASH_REMATCH[1]}" 8560 notice you *must* use a variable for the pattern otherwise it goes bananas. 
You can certainly extract this with a single awk command. I can't work it out right now as I'm on my phone. Keep in mind you can change the delimiter with -F, and then you can use string manipulation commands like split and sub. 
Hi, nice use of variable expansion.
:-D 
What operating system are you on.. also might be good to say what version of bash.. you probably want bash-completion2 on os x brew install bash-completion2 put this in your bashrc as well: bind '\C-i':menu-complete also: http://bash-completion.alioth.debian.org/
Linux debian 3.2.0-4-486 #1 Debian 3.2.46-1+deb7ul i686 GNU/Linux GNU bash, version 4.2.37(1)-release-(i486-pc-linux-gnu) Checking out the link and will try bash-completion2
Oh also, in the spirit of learning more about bash, I would prefer to find out what core bash command will enable the functionality I'm looking for at its most basic level. Installing something else like bash-completion2 might be easier but I wouldn't learn anything and then I might end up with weird completion problems like I had with zsh/fish where it incorrectly guesses the type of completion I wanted (e.g. I want to complete a filename but it tries to complete with a hostname). [This answer](http://askubuntu.com/questions/276421/autocomplete-on-partial-matches-in-the-middle-of-a-filename-in-terminal-dash-ba) is sort of on track for what I'm looking for, but if it's possible to change the general filename completion regardless of what command I'm using with it, that would be best.
Here's an alternate: size=$(sed 's/^.\+\[\|\].\+$//g' $file) IFS='x+' read height width _ &lt;&lt;&lt;"$size" (Where $file on the first line is the name of the file your script outputs to) The first line removes everything up to, after and including the '[]' characters using *sed* and assigns that to *$size*. The second line tells *read* that it should split on the 'x+' characters and assign the extracted parts in to *$height* and *$width*, with the two additional parts going in to nothingness via the underscore. A test: $ file=disarm.txt $ echo './autotrim_6672.mpc[991x590+171+1075] MPC 991x590 8-bit DirectClass 11.1KB 0.000u 0:00.010' &gt;$file $ size=$(sed 's/^.\+\[\|\].\+$//g' $file) $ IFS='x+' read height width _ &lt;&lt;&lt;"$size" $ echo size=$size height=$height width=$width size=991x590+171+1075 height=991 width=590
did you try this first? &gt; put this in your bashrc as well: bind '\C-i':menu-complete 
Of course, there's always: shopt -s extglob rm !(file1|file2|filen)
So this will change the way completion works for the vim command because of the `complete -F _ls_grep_completion vim`? And if I wanted to have that for vim and cd I would just add another `complete -F _ls_grep_completion cd`?
A poor-poor man's git (following this same trend ;-) ), is to use the following gem: # cp -av /path/to/file{,.`date +%s`} That can be ran any number of times on a backup you wish to have of a file (granularity: each second). Example: $ touch test $ cp -av test{,.`date +%s`} `test' -&gt; `test.1380822397' $ cp -av test{,.`date +%s`} `test' -&gt; `test.1380822398' $ cp -av test{,.`date +%s`} `test' -&gt; `test.1380822399' $ cp -av test{,.`date +%s`} `test' -&gt; `test.1380822400' $ ls test.* test.1380822397 test.1380822398 test.1380822399 test.1380822400 $ date -d @1380822399 Thu, Oct 03, 2013 1:46:39 PM ta-da! I usually define that command as a bash function: bkup(){ cp -av "$1"{,.`date +%s`} } Then I can just backup (`bkup important_foobar`) any file on the fly before I bork it.
Do NOT use the first one. As /u/pgl mentions, use extglob (more info in the bash man page). You can even use wildcards (e.g. `rm !(file*|*.txt)` will delete all files except those starting with "file" or ending with "txt").
The issue was that when I would try to execute a command like tail /dev/ttyAMC0, the terminal would just sit there and do nothing. What I did instead was cat ttyACM0, redirect it to a log file and send it to the background. Then in my script I would tail that log file, get the information, set it to a variable and then periodically update it with a while loop. It was complicated but it works perfect so I'm not going to mess with it. Thank you for your help.
no no, use `tail -f` and pipe to a read-loop like this: tail -fn0 | while read status; do if [[ $status = A ]]; then ... else ... fi done
that's overly complicated for no reason, of course if you run a command that it's supposed to hang it will, but you can use pipes to process its output while its running.
tail -f $filePath Will read the last line of text in a file. You can pipe this into any other script. 
Haha, yeah. I wasn't at my computer, and I haven't done much bash! Thanks for correcting me!
If you want it to echo out '$USER' when you echo your variable, just use: :~$ name=''\''$USER'\''' ; echo $name '$USER' That's four single quotes either side. Your current method is a horrible horrible hack. p.s; read up on how to escape things properly. It's a vital part of learning to write bash properly.
Does it have to begin with single quotes? This works well: superman@localhost:~$ NAME="'\$USER'" superman@localhost:~$ echo $NAME '$USER'
Here's my solution without external tools: $ NAME="'"'$USER'"'" $ echo $NAME '$USER' Broken down: "'" # a literal single quote '$USER' # a literal $USER "'" # a literal single quote again also, [root@localhost ~]# C=$B$A$B [root@localhost ~]# echo C C [root@localhost ~]# C=\$B$A\$B [root@localhost ~]# echo C C I hope you see that mistake :)
Lordy, 4 on each side just to end up with one on the output? Lol, intense! Thanks much, I appreciate the insight. I will have to hit the webz to figure out exactly how that all breaks down, but tested and confirmed. Thanks and have an upvote ;)
Tested and confirmed. I think you take the case as the most concise solution. Appreciated, have some red-orange :) 
Tested, works, appreciated. Have a red-orange!
Thanks! Although I must admit that /u/johnhpatton's answer looks simpler and more easily understandable...
This still works with two single quotes less on each side: $ NAME=\''$USER'\'; echo $NAME '$USER'
A breakdown is: name= #variable= ' #start strict quoting ' #stop strict quoting \ #escape character ' #literal escaped ' ' #start strict quoting $USER #strictly quoted literal $USER ' #stop strict quoting \ #escape character ' #literal escaped ' ' #start strict quoting ' #close strict quoting Hope this helps.
That would work too. You're just adding the escaped single quotes outside of the strictly quoted $USER, meaning you don't have to stop strict quoting to add them. I run a lot of commands over ssh so the quoting method I used is pretty much second nature at this stage ; i.e; ssh somebox 'awk '\''{ print $1 }'\'' somefile' 
Also see the following URL: http://mywiki.wooledge.org/Quotes I highly recommend reading over the whole bash FAQ on that site - it's one of the best out there by a long margin.
well, the question was about the quotes :) for the variable you just need to escape the dollar sign too: `\'\$USER\'` or as he said: `"'\$USER'"` or even `\''$USER'\'` the second form is preferred since you should always use double quotes for variables.
why did you put empty quotes around it? just do: `\''$USER'\'`
what the fuck are you talkng about? context?
There really is no context. I just combined a few commands to download all the images from your minus.com profile (if you have one) They don't provide any way just to download everything. I didn't really see any other place to submit this, so I chose /r/bash since it seemed the most appropriate.
 #!/bin/bash LinkSource="`readlink -e "$0"`" echo "ScriptDir &gt;${LinkSource%/*}&lt;" # "${LinkSource%/*}" empty if script is in / but there are no scripts there ;-)
ah so "minus" is a website, I think you can replace your `cat` for a redirection or even better use `wget -O-` and skip the "save the html" entirely
http://mywiki.wooledge.org/BashFAQ/028
Well it requires you be logged in. You would have to do some authentication too which is a bit ahead of me. :) 
 (My) Script: #!/bin/bash LinkSource="`readlink -e "$0"`" echo "ScriptDir &gt;${LinkSource%/*}&lt;" ========== Examples: ln -s /home/glesialo/bin/Script /home/glesialo # Link '/home/glesialo/Script' created cd /home/glesialo/bin ./Script # Invoking with relative path ScriptDir &gt;/home/glesialolo/bin&lt; /home/glesialo/Script # Invoking link ScriptDir &gt;/home/glesialo/bin&lt; ====================================== (Your) Script2: #!/bin/bash pushd `dirname $0` &amp;&gt;/dev/null echo "ScriptDir2 &gt;`pwd`&lt;" ========== Examples: ln -s /home/glesialo/bin/Script2 /home/glesialo # Link '/home/glesialo/Script2' created cd /home/glesialo/bin ./Script2 # Invoking with relative path ScriptDir2 &gt;/home/glesialo/bin&lt; /home/glesialo/Script2 # Invoking link ScriptDir2 &gt;/home/glesialo&lt; ======================================
ahh that's fun :) it usually involves `--post-data` and `--save-cookies` for the login and then `--load-cookies` for the other requests :) give it a try!
:-)
I use this on rhel-like OSes and it works like a champ, also had good luck on Mac OS if the coreutils brew is installed: #!/bin/bash script=`readlink -f "$0"` SCRIPT_DIR=${script%/*} echo "$SCRIPT_DIR" Note that the -f allows you to use soft symlinks to your script and it will still find the folder the script is in. Of course, as /u/Samus_ points out in his reference, $0 is not always reliable so ensure it works on your systems if you want to use this.
parallel --tag 'echo status | nc {} port | timestamp' :::: hosts.txt parallel is GNU Parallel. timestamp is https://github.com/ole-tange/tangetools/tree/master/timestamp
Single quotes are processed differently than doubles. This could work better, and you should probably get in the habit of using parenthetical subshells. sh -x test.sh "$(echo 'test')" 
You are giving your `test.sh` program the argument `echo "test"` (in single quotes to prevent any expansions from happening). This literal string is stored as the positional parameter `1`, which is substituted into the string "Not permitted to access file ${1}" So following the [substitution / expansion rules](http://www.gnu.org/software/bash/manual/bashref.html#Shell-Expansions) it drops one literal string into another string, and that's it. There's no additional expansions to be performed. Bash will not continually re-examine the string to determine if substitutions need to be performed.
Need some details. What OS are you running? What file format are the images? Do you have ImageMagick installed?
yeah you'll need some command to tell you the dimensions (imagemagick woulld do0 then loop over the files, analyze and parse the output and then apply some criteria to filter, easy.
Check out sips... sips -g pixelWidth &lt;image&gt; sips -g pixelHeight &lt;image&gt; will give you the dimensions... pipe that into | tail -1 | cut -d: -f2 | sed 's/[^0-9]//g' to get just the integer (on my mac, anyways). From there, a for loop should do the trick! Make sure to test this before running... I'm not sure that we have the same version of sips, bash, etc...
 #!/bin/bash for f in * do h=$(sips -g pixelHeight $f 2&gt;&amp;- | tail -1 | cut -d: -f2 | sed 's/[^0-9]//g') w=$(sips -g pixelWidth $f 2&gt;&amp;- | tail -1 | cut -d: -f2 | sed 's/[^0-9]//g') if [[ "${h}" =~ ^[1-9][0-9]*$ ]] then if [[ "${w}" =~ ^[1-9][0-9]*$ ]] then if [[ $h -lt 1080 ]] then if [[ $w -lt 1920 ]] then rm -vi $f fi fi fi fi done 
Ehhh. All these fancy answers. You wanted simple. Screw it. Be lazy. New tools are for sober, motivated people. find -size [whatever seems good anecdotally] -iname '*.jpg' -delete find -size [random number, again] -iname '*.png' -delete Test that shit without the -delete flag. Your size values are going to be things like... '-3k'... or whatever happens to be semi-appropriate to your image size and the format you are shooting at. For bonus points, you'll get different compressions inside image types. And stuff. This will be a lossy approach. But hey. I'm sticking to the lazy theme here.
Each of the numbers in the file is broken by a newline character. It didn't show up that way. The numbers 2, 4 and 5 are on separate lines.
what will the variables be named? do you know how many lines there will be?
Ugly and hackish, but it works: AirBoxOmega:tmp d$ cat &gt; file 1 2 3 AirBoxOmega:tmp d$ read -a i &lt; &lt;(tr "\n" " " &lt; file) AirBoxOmega:tmp d$ echo ${i[0]} 1 AirBoxOmega:tmp d$ echo ${i[1]} 2 AirBoxOmega:tmp d$ echo ${i[2]} 3 AirBoxOmega:tmp d$ I'm going to go eat dinner, but I'll come back and explain later. Actually, you should really crosspost to /r/commandline and then I'll answer and explain there so you have a chance of getting a better answer (no one reads /r/bash)
Alternatively, if you had the variables separated by a carriage return you could do the following: #!/bin/bash rconst=$RANDOM for i in $(seq 1 $(cat file1|wc -l));do echo varname$i=$\( cat file1 \| sed \'$i\!d\' \) &gt;&gt; /dev/shm/varfile_$rconst done source /dev/shm/varfile_$rconst #shows your vars set| tail 
It is bash string processing. Here are my notes about it: ********************************** -Manipulating Strings. ${string:position} Extracts substring from $string at $position. If the $string parameter is "*" or "@", then this extracts the positional parameters, [1] starting at $position. echo ${*:2} # Echoes second and following positional parameters. echo ${@:2} # Same as above. echo ${*:2:3} # Echoes three positional parameters, starting at second. ${VariableName:offset:length} offset: character beginning of substring length: length of substring Example: { VariableName=HolaChato echo ${VariableName:4:3} # Cha } ${string#substring} Strips shortest match of $substring from front of $string. ${string##substring} Strips longest match of $substring from front of $string. Examples: { stringZ=abcABC123ABCabc # |----| # |----------| echo ${stringZ#a*C} # 123ABCabc # Strip out shortest match between 'a' and 'C'. echo ${stringZ##a*C} # abc # Strip out longest match between 'a' and 'C'. PathAndFile=/home/MyDir/file.txt File=${PathAndFile##*/} echo $File # file.txt # Strip out longest match between '*' (Any character) and '/'. BaseName=${0##*/} # Faster alternative to `basename $0` } ${string%substring} Strips shortest match of $substring from back of $string. ${string%%substring} Strips longest match of $substring from back of $string. Examples: { stringZ=abcABC123ABCabc # || # |------------| echo ${stringZ%b*c} # abcABC123ABCa # Strip out shortest match between 'b' and 'c', from back of $stringZ. echo ${stringZ%%b*c} # a # Strip out longest match between 'b' and 'c', from back of $stringZ. } ${string/substring/replacement} Replace first match of $substring with $replacement. ${string//substring/replacement} Replace all matches of $substring with $replacement. Examples: { stringZ=abcABC123ABCabc echo ${stringZ/abc/xyz} # xyzABC123ABCabc # Replaces first match of 'abc' with 'xyz'. echo ${stringZ//abc/xyz} # xyzABC123ABCxyz # Replaces all matches of 'abc' with # 'xyz'. stringZ=" abc AB C1 23 ABCabc " shopt -s extglob stringZ=${stringZ//+([[:space:]])/ } shopt -u extglob echo "&gt;$stringZ&lt;" #&gt; abc AB C1 23 ABCabc &lt; # Replaces all whitespace by a single blank. Notes Classes can be specified using the syntax [:class:], where class is one of the following classes defined in the posix standard: alnum alpha ascii blank cntrl digit graph lower print punct space upper word xdigit A character class matches any character belonging to that class. The word character class matches letters, digits, and the character ‘_’. } ${string/#substring/replacement} If $substring matches front end of $string, substitute $replacement for $substring. ${string/%substring/replacement} If $substring matches back end of $string, substitute $replacement for $substring. Examples: { stringZ=abcABC123ABCabc echo ${stringZ/#abc/XYZ} # XYZABC123ABCabc # Replaces front-end match of 'abc' with 'XYZ'. echo ${stringZ/%abc/XYZ} # abcABC123ABCXYZ # Replaces back-end match of 'abc' with 'XYZ'. } ********************************** 
I like the idea of having a system, but your template is longer than 95% of my scripts. How much of it makes it in to an average use? 
Why duplicate all this code for each script? Just include a file with these declarations 
What I've noticed about whatever bash scripts I write is that they tend to grow over time. Furthermore, that growth is rarely planned. Most cases I'll create a script, show other people, and then my script is consumed by other scripts that other people write. The more industrial strength my initial bash scripts, the more I have a chance at adding decorators without breaking anything later. I'd say that I use a lot of this on any scripts that require flags and arguments.
have you tried &gt; sleep *n* or google?
if you don't absolutely need to do this in bash, please try a different language with a proper parser, could be Python, Ruby, Java, anything that has some library that speaks HTML, there's even some scapring frameworks out there to help with the crawling and such.
I agree with [Samus_](/u/Samus_)'s assessment. Shell scripting's not well suited for this. That being said, here's a partial whack at it: url='http://www.visitcolumbiamo.com/web/things_to_do/dining.php' links -source ${url} \ | gzip -d - \ | html2text -ascii -width 256 \ | sed 's/ ... \[read_more\]/\n/g;s/ &amp;#8226; /\n/g' \ | grep -v '^visit_site' 
&gt; visit_site'
This [post from /r/commandline](/r/commandline/comments/1ojie0/jq_is_a_lightweight_and_flexible_commandline_json) for a [command-line JSON processor](http://stedolan.github.io/jq/) might be just the thing.
I have tried google. Not sure if that was meant to be a smart ass comment or sincere.... sleep is a wonderful utility... but my question revolved around executing sleep against every line of a bash script without modifying the script itself. Effectively piping a script through sleep. If there is a way of doing that which you are aware of, please share.
hey :) you should use `"$var"` instead of `${var}` since the quotes are what give you safety, the braces are only to separate variable names from other contents such as `${var}iable`
The braces could be for [a lot more than that](http://www.tldp.org/LDP/abs/html/string-manipulation.html)
Because that's another file to include in your project/scripts. It's not a 100% case. Sometimes it makes sense to Include and others it's cleaner to duplicate. 
yes of course but people confuse them with quoted expansion most of the time (such as this case) so better just stick with the basics. basically any parameter expansion -braces or not- should be quoted, that's the message.
I believe its a programmer's nightmare to have to fix a bug in *n* files rather than one included file. 
My advice is to learn a little sqlite and store your results in a database. Depending on what field you grep, you can put it in the proper column position. curl or wget website grep out fields sqlite3 database "insert into blah blah" Honestly though, in this case I'd recommend a little php-foo.
Would recommend [Learning the Bash Shell](http://shop.oreilly.com/product/9780596009656.do) they cover each one of those recommendations. You can prob easily google most of them.
Thanks for the link, but after Googling, I wasn't able to resolve this: expr is antiquated. Consider rewriting this using $((..)), ${} or [[ ]]. Hence this post.
`$((..))` is shell arithmetic. So you can rewrite this: expr $1 % 5 == 0 To this: $(($1 % 5 == 0)) Edit: fixed comparison operator (used "`=`" instead of "`==`", duh).
I should've used `==` instead of `=`, sorry. I've updated my comment. Edit: and submitted a pull request as you asked. The pull request also adds parens to the calculation, to make the evaluation order more explicit.
out="$(ifconfig $1 | grep -i "$2" | sed 's/://')"
You can pipe it through tr -d ':', which will remove the colons. I think this would be a little more lightweight than /u/titosemi's sed solution. EDIT: Disregard that. As /u/MrVonBuren [points out](http://www.reddit.com/r/bash/comments/1p7ssp/parsing_a_mac_address/cczuugs), bash has built-in string manipulation (TIL) which can do this *much* faster and is of course even more lightweight. Also, WTF gold? Thanks, I guess, but MrVonBuren really deserves that more than I do...
#echo "00:00:00:01:00:00" | sed -e 's/://g' #000000010000 Sed is your friend
Read up on string manipulation too. Kind of scary that you're scripting something with MAC addys but don't understand this basic stuff. :P
Only because this is the kind of thing I think is important to find out about sooner rather than later...this absolutely works, and is a fine solution, but be careful because things like this often don't scale well, and if you don't know to look for that bottle neck, you can wind up with code that is slow and you won't know why. So, for example, in your solution piping to tr once isn't a big deal, but if you were doing something in a loop, every single hit means forking a new tr process. This doesn't sound like a big deal, but look at what happens when you start doing this a lot. ##First as a baseline, let's see how quickly we can echo our MAC address 1k times, then 10K times: AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000 ]];do echo "${mac}" &gt; /dev/null ;let i++;done) real 0m0.028s user 0m0.022s sys 0m0.006s AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 10000 ]];do echo "${mac}" &gt; /dev/null ;let i++;done) real 0m0.252s user 0m0.197s sys 0m0.056s ##Really fricking fast and not a huge difference from 1k to 10k. Next, let's do the same thing, but pipe it through tr: AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000 ]];do echo "${mac}"|tr -d ":" &gt; /dev/null ;let i++;done) real 0m2.060s user 0m1.361s sys 0m1.394s AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 10000 ]];do echo "${mac}"|tr -d ":" &gt; /dev/null ;let i++;done) real 0m28.321s user 0m26.608s sys 0m17.826s ##So that got really slow really quickly. So what do we do? [STRING MANIPULATION](http://www.thegeekstuff.com/2010/07/bash-string-manipulation/): AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000 ]];do echo "${mac//:}" &gt; /dev/null ;let i++;done) real 0m0.032s user 0m0.027s sys 0m0.006s AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 10000 ]];do echo "${mac//:}" &gt; /dev/null ;let i++;done) real 0m0.325s user 0m0.267s sys 0m0.057s ##find and replace is built right into the shell, which means no spawning a 1000 instances of tr, which means AirBoxOmega:~ d$ time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000000 ]];do echo "${mac//:}" &gt; /dev/null ;let i++;done) real 0m31.838s user 0m26.242s sys 0m5.592s ##you can get ~10 million lines parsed in the time it took to do 10k piping through and external tool . So yeah, **TL;DR** I have a strange tendency to geek out about shell scripting when I've been drinking. Also, most of the time it won't matter, but knowing how and when to use shell builtins instead of external tools can make your code much faster. (Also, OP, you'll get better/more help on /r/commandline) 
My coworkers sometimes give me crap for using bash internals instead of sed/awk when parsing tons of stuff in a loop because when they go back to do something they encounter something unfamiliar/unconventional to them.
Sed is, apparently. Thanks! 
OP was and I was just replying to the highest level comment so they'd see it.
Putting quotes around the strings makes it run fine on my system
this is a bad idea, using an array allows you to have elements which have spaces of their own and not be treated as separate args.
use four spaces instead of quotation to get blockcode: arr=( MMM AXP T BA CAT CVX CSCO DD XOM GE GS HD INTC IBM &gt;JNJ JPM MCD MRK MSFT NKE PFE PG KO TRV UTX UNH VZ V WMT &gt;DIS ) for i in ${arr[@]} do stringOne=http://finance.yahoo.com/q/hp?s= stringTwo=$stringOne$i wget stringTwo done and if I run that I get a different error, please edit your post to see what you're actually running. 
&gt; arr=(MMM AXP T BA CAT CVX CSCO DD XOM GE GS HD INTC IBM JNJ JPM MCD MRK MSFT NKE PFE PG KO TRV UTX UNH VZ V WMT DIS) &gt; &gt; stringOne="http://finance.yahoo.com/q/hp?s=" &gt; &gt; for i in ${arr[@]} &gt; &gt; do &gt; &gt; stringTwo=${stringOne}${i} &gt; &gt; wget ${stringTwo} &gt; &gt; done &gt; You're referencing a variable, but you're not denoting it as a variable. Fixed above. (It's a good idea to use brace notation for variables for pretty much everything) 
Like this: #!/usr/bin/env bash arr=( 'MMM' 'AXP' 'T' 'BA' 'CAT' 'CVX' 'CSCO' 'DD' 'XOM' 'GE' 'GS' 'HD' 'INTC' 'IBM' '&gt;JNJ' 'JPM' 'MCD' 'MRK' 'MSFT' 'NKE' 'PFE' 'PG' 'KO' 'TRV' 'UTX' 'UNH' 'VZ' 'V' 'WMT' '&gt;DIS' ) stringOne=http://finance.yahoo.com/q/hp?s=[1] for i in ${arr[@]} do stringTwo=$stringOne$i # wget stringTwo echo $stringTwo done this prints out: http://finance.yahoo.com/q/hp?s=[1]MMM http://finance.yahoo.com/q/hp?s=[1]AXP http://finance.yahoo.com/q/hp?s=[1]T http://finance.yahoo.com/q/hp?s=[1]BA http://finance.yahoo.com/q/hp?s=[1]CAT http://finance.yahoo.com/q/hp?s=[1]CVX http://finance.yahoo.com/q/hp?s=[1]CSCO http://finance.yahoo.com/q/hp?s=[1]DD http://finance.yahoo.com/q/hp?s=[1]XOM http://finance.yahoo.com/q/hp?s=[1]GE http://finance.yahoo.com/q/hp?s=[1]GS http://finance.yahoo.com/q/hp?s=[1]HD http://finance.yahoo.com/q/hp?s=[1]INTC http://finance.yahoo.com/q/hp?s=[1]IBM http://finance.yahoo.com/q/hp?s=[1]&gt;JNJ http://finance.yahoo.com/q/hp?s=[1]JPM http://finance.yahoo.com/q/hp?s=[1]MCD http://finance.yahoo.com/q/hp?s=[1]MRK http://finance.yahoo.com/q/hp?s=[1]MSFT http://finance.yahoo.com/q/hp?s=[1]NKE http://finance.yahoo.com/q/hp?s=[1]PFE http://finance.yahoo.com/q/hp?s=[1]PG http://finance.yahoo.com/q/hp?s=[1]KO http://finance.yahoo.com/q/hp?s=[1]TRV http://finance.yahoo.com/q/hp?s=[1]UTX http://finance.yahoo.com/q/hp?s=[1]UNH http://finance.yahoo.com/q/hp?s=[1]VZ http://finance.yahoo.com/q/hp?s=[1]V http://finance.yahoo.com/q/hp?s=[1]WMT http://finance.yahoo.com/q/hp?s=[1]&gt;DIS 
no it's not, you don't define good practices by the edge cases that don't break, if it's done properly it always work.
arr=('MMM' 'AXP' 'T' 'BA' 'CAT' 'CVX' 'CSCO' 'DD' 'XOM' 'GE' 'GS' 'HD' 'INTC' 'IBM' 'JNJ' 'JPM' 'MCD' 'MRK' 'MSFT' 'NKE' 'PFE' 'PG' 'KO' 'TRV' 'UTX' 'UNH' 'VZ' 'V' 'WMT' 'DIS') stringOne="http://finance.yahoo.com/q/hp?s=" for i in ${arr[@]} do stringTwo=${stringOne}${i} wget ${stringTwo} done Still getting the same Syntax error: "(" unexpected. It seems to really unlike the left paranthese.
I get a different error, but I think it has the same cause: syntax error near unexpected token `&gt;' `arr=( MMM AXP T BA CAT CVX ... etc etc `&gt;` and `&lt;` are special characters in the shell, so they must be escaped or quoted to be used as variable values. arr=( MMM AXP T BA CAT CVX CSCO DD XOM GE GS HD INTC IBM \&gt;JNJ JPM MCD MRK MSFT NKE PFE PG KO TRV UTX UNH VZ V WMT "&gt;DIS" ) After that it works in cygwin, but I'm still leery about the `for` loop not quoting the elements of the array. Whenever there's a chance that my variable values could contain special characters, I quote the expansion, although in practice I think the only thing that would fail is an array element containing a space: arr=( MMM AXP 't ex' ZIP ) for i in "${arr[@]}" do stringOne="http://finance.yahoo.com/q/hp?s=" stringTwo="$stringOne$i" echo wget "$stringTwo" done yields wget http://finance.yahoo.com/q/hp?s=MMM wget http://finance.yahoo.com/q/hp?s=AXP wget http://finance.yahoo.com/q/hp?s=t ex wget http://finance.yahoo.com/q/hp?s=ZIP whereas `for i in ${arr[@]}` yields wget http://finance.yahoo.com/q/hp?s=MMM wget http://finance.yahoo.com/q/hp?s=AXP wget http://finance.yahoo.com/q/hp?s=t wget http://finance.yahoo.com/q/hp?s=ex wget http://finance.yahoo.com/q/hp?s=ZIP The reason is that [word splitting happens after brace expansion](https://www.gnu.org/software/bash/manual/bashref.html#Shell-Expansions), so the `for` loop sees two items for `t ex`.
Version 4.2.37(1)
it would help to know what kind of help you need. also try /r/linux
bash honestly seems like a poor choice for such a hefty piece of software. Python might be a better choice.
You're right about quoting, but I'm [not the only one](http://google-styleguide.googlecode.com/svn/trunk/shell.xml?showone=Variable_expansion#Variable_expansion) who likes the longer notation. 
Were your greping for lines containing ppt? grep -oe '[\^\\/]*ppt'
This works PERFECTLY. Mind breaking down the command for me?
The "-e" option makes grep search using [regular expressions](http://en.wikipedia.org/wiki/Regular_expression) and the "-o" option makes it return only the part of the line that matches the pattern, rather than the whole line. To understand the rest, you'll need to bone up on how regular expressions work. There are lots of tutorials online for that. It's a very useful skill to have.
You can get make things faster without using builtins, you just have to place the pipe after the while cycle, spawing only one instance of `tr`: time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000000 ]];do echo "$mac" &gt; /dev/null ;let i++;done | tr -d ':') real 0m16.148s user 0m13.439s sys 0m2.642s (here's the result using bash builtins: ) time (mac="00:11:22:aa:bb:cc";while [[ i -lt 1000000 ]];do echo "${mac//:}" &gt; /dev/null ;let i++;done) real 0m21.142s user 0m18.295s sys 0m2.792s 
Good man, always wanted that but was always in too much of a rush to go find it
Exactly what [costa24](/u/costa24) said. Let's start from the end since that's fixed text. The ppt matches the end of your filename, before that you have 0-n (from the splat, *) of a set of characters that are defined by the contents of the square brackets ([]). The contents of the set are anything that is not (^ ) a forward slash (/), which has to be escaped or it gets interpreted as a meta character and not a literal /. Because of the way filesystem delimit paths anything that comes after a / and before your extension (ppt) is going to be your filename.
There is no error, it just switches from one window to the other whenever I hit win+d
Is that code inside some kind of loop? I made a test script containing only two lines and it executes as expected. Can you post the rest of your code?
That is all of it... =/
Worked for me... when i've come across strange shell problems, I usually check my .profile (or .bash) file. That script gets loaded when you start a new terminal session and errors in there can carry through in any script you make. 
Edit: I finished it out to rename the files (and modified it somewhat from the original): #!/bin/bash i=0 while read line do array[ $i ]="$line" # add each line to the array (( i++ )) done &lt; &lt;(ls) # feed the while loop the contents of ls l=`ls | wc -l` # get number of items in the directory n=$(( $RANDOM%$l )) # generate a random number up to the number of items in the directory let o=$n-1 # in case n is the index of our current file if [ $o -lt 0 ] # in case n is negative (i.e. it randomly generated 0) then o=0 else o=$o fi t=${array[$n]} # print random item in list u=${array[$o]} # print the item after $t if [ $t = "server-icon.png" ] then mv server-icon.png server-icon-`date | cut -d' ' -f5 | sed 's/:/_/g'`.png mv $u server-icon.png else mv server-icon.png server-icon-`date | cut -d' ' -f5 | sed 's/:/_/g'`.png mv $t server-icon.png fi Be aware that it will rename all the files in the folder to server-icon-(current_time).png.
As, [samling](http://www.reddit.com/user/samling) and [tylersavery](http://www.reddit.com/user/tylersavery) said, the code works. It has to be a problem with your shell. Make sure you have #!/bin/bash on the first line, just to be sure, though.
you aren't making any sense, if you have a question be specific and provide the necessary context I've no idea why you've posted this.
Excellent! I could only figure out how to do it randomly as well!
Thank you so much for this! My eyes have been opened.
Yes. End the command line with "&amp;" so that the command runs as a background job. Here's a bash script that you can use to demonstrate the process: #!/bin/bash # # Cont (continuous output) # : Intended to be run as a background job; e.g, enter: # : Cont &amp; # : Appends message $1 every $2 seconds to file $4 # : Stops after $3 echos # # To kill Cont when running in background, wait till vRepL is reached, or enter: # : read tA tB &lt; &lt;(ps | grep Cont) ; echo "tA($tA)" # # # Arguments: # : $1: (text to output) # : $2: (delay, in seconds) (floating point) (defaults to 1.0) # : $3: (count) (defaults to 100) # declare vText=${1:- "$0"} # Test to output declare vWait=${2:- "1.0"} # Duration between printf calls declare -i vRepL=${3:- 100} # Repetitions limit declare -i vRepC=0 # Repetitions count declare vFile=${4:- "ContTest.del"} # Output file echo "$0 $*" &gt; $vFile while test $vRepC -lt $vRepL ; do vRepC+=1 printf "\n%05d %s" $vRepC "$vText" &gt;&gt; $vFile sleep "$vWait" done exit Use chmod to make the script executable -- e.g., chmod a+x Cont Then run the script in the background: ./Cont &amp; And while the script is running, use less to examine the output file: less ContTest.del Each time you invoke less, the file will contain more records.
Alright, so I managed to get it to work by going to a .txt file in Finder and setting the default editor as Sublime Text.. I don't understand why this is necessary if i set the default editor to ST2 in the ENV variables in Bash.. Maybe it's the case that if a certain extension has a default application set to open it (where would this variable reside? the broader OS?) then that application overrides the $EDITOR variable in Bash? I'll leave this up in case anyone else has a similar problem or in case someone knows why this behavior exists. 
Where exactly are you expecting the EDITOR environment var to be used? It will have absolutely no effect on the GUI
Well, no the open command doesn't rely on the shell to tell it the app to use. Open is a link from the terminal to the GUI, or more explicitly the launch control of the GUI shell. That's why setting the default app in the info panel (GUI) got it to work for you. However EDITOR will still be used by shell activities, e.g. C-x-e to open your current command line, or when triggered by git commit, and so on. export EDITOR=sublime In your ~/.bashrc or better yet, your .zshrc (but zsh is another story.)
 man bash | grep --after-context=100000 -e '^INVOCATION$' | grep --before-context=100000 '^DEFINITIONS$' | head -n -1 prints only the "INVOCATION" section from the bash manpage (the `head` part cuts off the line that contains "DEFINITIONS"). this only works if the section occurs only once.
sed -e '/changeto client/,/serverfarm/ !d' &lt;filename&gt;
Thank you so much, you beautiful son of a bitch
You're welcome. Glad I could help.
Thanks for the help. I get it now. About zsh..i'm still pretty new to the whole programming thing and just now beginning to research unix etc.. so give it time! Thanks again!
This will only work if the start and end labels are unique. **sed** is greedy when matching patterns so if '*changeto client*' or '*serverfarm*' is repeated then you would wind up with two (or more!) sections in &lt;filename&gt;
I love you linuxwhisperer. Thank you so much. The perl one works perfectly!
Thank you this looks right. however I'm slightly confused about sed -e "/$title/d" $library &gt; $tmp is that deleting what originally was in the file as title and then later replacing it with the new title?
Let me preface this by saying that this script is all kinds of messed up/incomplete. I would suggest *not* learning from it. Find scripts that are actually executable (syntactically correct) and well commented. Now then, it looks like there is a library called `mylibrary`, which is just a text file whose lines have the following form: title,author,library name,date returned In other words, `mylibrary` is a CSV (comma-separated value) file. The script allows a user to "return" a book contained in `mylibrary`, by which is meant simply that the "date returned" value is updated to the current date. However, unless I'm misunderstanding something, it's a pretty bad system, because nothing in the line for a given book tells you whether it's "checked out" or not. 
sed operates on lines, so this command will delete all lines that match the title. Then, the script is adding a new entry at the end of the file with the date. Are you planning to make changes to this script? There is room for quite a bit of improvement, if you'd like some pointers....
The problem is not so much that the *script* is incomplete (missing functions), but that the `return_book` function itself is incomplete. For example: existed=grep "$title" $library | wc -l should have `$( )` around the value assigned to `existed`. updated=$title,$author,librar,`date +%F` should have quotes around the value assigned to `updated`, and `librar` should be `$library`. There should be a closing `}` for the function. And so forth. **By the way:** you should indent your code blocks with 4 spaces, and you should place inline code between backticks (this: `). 
yes pointers would be great! 
The sed command should use -i to edit "in-place". This means that the tmp file will be unnecessary. Additionally, the comments below about including the grep line in $( ) or back ticks is correct. I'm not sure what the point of passing the grep results to wc is. If it is to check that there is exactly on match, then you can use a grep option to return the number of matches, instead of the lines themselves. If it is just to test that grep found a match then using the return code from grep by checking the value of $? immediately after the grep would be better. I worry about what would happen if multiple matches were found for title. Currently I think the check that wc returned 1 will protect you from this. Without the protection the sed command will delete all the entries that contain title, but only one updated entry will be re-added. Also, the error message that the book title wasn't found is wrong for the case where wc returned &gt; 1. In that case there are too many matches. If you really want to keep tmp around I suggest you look at mktmp to generate you a safe temporary file name that will never collide with existing files. I also suggest you delete the tmp file when you finish the function. There are more things that could be cleaned up, but those are just a few. Have fun in bash!
This is a bad idea. Use -i instead
I kinda cringed at the == 1. There's a good chance some book titles will be close enough that it could return more than 1. Of course, there's probably way more wrong with how this works... Also, backticks are deprecated, apparently.
My solution was to assume server-icon.png can be a symlink to one of the png files in your icons directory. #!/bin/bash # # rotate_link.sh # # Given a symlink, it will update the symlink to point at the next # *.png file (lexicographically) in the directory. # # This depends on having a working "readlink -f" on your system, which is # true on Linux but not OSX. if [[ -z "$1" ]]; then echo "usage: rotate_link &lt;symlink&gt;" exit 1 elif [[ ! -L "$1" ]]; then echo "rotate_link: "$1" is not a symlink" exit 1 fi link="$1" current="$(readlink -f "$link")" filesdir="${current%/*}" first= for f in "$filesdir"/*.png; do [[ -f "$f" ]] || continue # Save the name of the first file in the directory, just in case. [[ -z "$first" ]] &amp;&amp; first="$f" if [[ "$f" &gt; "$current" ]]; then ln -svf "$f" "$link" &amp;&amp; exit fi done # If we get here, we must have been the last png file in the directory. # Reset the symlink to the first. ln -svf "$first" "$link" 
okay thank you. now if I wanted to create a function to check out a book. How would I go about counting how many books a user has and only allowing them to have 4 checked out at one time?
&gt; Try catting the script into a file you do have write access on, and modifying it for testing purposes? this 
Use --text to for diff to treat the binary as text. diff --text original.tar.gz clone.tar.gz &gt; patch.txt patch original.tar.gz patch.txt More details [here](http://www.gnu.org/software/diffutils/manual/html_node/Binary.html).
Thank you! This is just what I needed.
Here are two general suggestions: You sould consider using `set -o errexit`. By setting this option, if a command in your script returns a non-zero value, the script will exit. Usually, this is the desired behaviour. Put your code here http://www.shellcheck.net/ and look at the suggestions.
Thanks! shellcheck mostly recommends using quotes on certain functions and variables, but it's good practice anyway
sweet chocolate jesus on a whole wheat goddam cracker, dude! can you be anymore obtuse?! 
Quick style thing -- you can use shopt -s nullglob in your script and then you won't have to wrap your whole script in an if statement to make sure there's really a file. Also, use more quotes. Like, a lot more. This script is going to fail terribly if you ever run into a filename with a space or a newline or something weird in it. Basically every page on [this wiki](http://mywiki.wooledge.org/Quotes) will be useful to you. EDIT: One more style thing: use elif! It will make your life easier when you have to read this script later -- the logic will be clearer. EDIT again: cp and rm both have -v switches which will announce the files being copied/removed, so you don't necessarily need to echo all of that yourself, unless you want to. I don't have p7zip installed, but it probably has a verbose mode, too. EDIT3: CHECK THE RESULT OF cd'ING TO THE DIRECTORY! If it messes up you're going to accidentally copy/compress/delete a bunch of files in the current directory! In fact, there's no reason to cd at all: just change your globs to for f in "$remote_incoming"/*.tib
Thanks for the advice! I was using -v for cp but it only displays XXXXXXXX.tib -&gt; XXXXX.tib which wasn't that helpful. I also wanted to echo it myself as it'll mostly be displayed in a log file. Speaking of which do you have any suggestions other than archivebot.sh &gt;&gt; archive.log to log output to a file? Edit: Also the reason I cd before the script is because specifying the folder in the for loop will make $f = '/media/nasdata/archive/XXX.tib' which will cause issues when using p7zip $remote_archive/$f
UNTESTED rewrite #!/bin/bash shopt -s nullglob for f in "$remote_incoming"/*.tib; do filename="${f##*/}" if [[ ! -e "$remote_archive"/"$filename".7z ]]; then not_yet_archived+=("$f") fi done cp -v -u -t "$remote_archive" "${not_yet_archived[@]}" for f in "$remote_archive"/*.tib; do filename="${f##*/}" p7zip "$f" &amp;&amp; rm -v "$remote_incoming"/"$filename" done I don't know if it does exactly what you want; it'd compress every *.tib file in the new directory. EDIT: oh yeah and it doesn't delete all the originals, but that could be fixed in the first for loop. EDIT2: I was testing it and it doesn't handle the case where the are no files to copy.
Yeah you may as well echo your own stuff if you want a particular kind of information. I don't really have any advice about logging. I don't know anything about it. EDIT: I also want to congratulate you for not trying to parse ls output and using a glob correctly.
I explained it a bit more in the comments above. I wasn't trying to make a deal about this.
I'm not aware of anything with that functionality, however.... zsh has very nice completion features which includes being able to select completion results with the cursor keys. Here's a [decent demo video](https://www.youtube.com/watch?v=E2WXc3qAg8A). You should definitely check it out.
I've never used xdotool, but I read the manpage. So maybe windowmap only takes one windowname? In that case, you'll need to call it once on each window. According to the manpage, the result of xdotool search is a line-delimited list of windows, so how about the following (UNTESTED): xdotool search --classname conky | while IFS= read -r -d $'\n' window; do xdotool windowmap "$window" done EDIT: actually, this is probably just as bad as your original attempt in that it will raise one window then raise the other. I have no idea if there's a way to raise both at the same time; more research into xdotool (not bash) is warranted.
Thanks. I'll look into it, and post a solution if I get one
just woundering what he posted that was so great? It's been deleted
Paste mistake, it's got a #. Wow this is really helpful! I asked my Linux teacher for help writing it and he put that echo on and set -x saying it helps with debugging. Thank you so much, I'll make the changes and update you today if it worked.
The 'set -x' will definitely help with debugging, but since this is so simple it's not really needed here.
setting -x is the reason you're getting so much output (it's showing the trace of the commands that are run after) Here's a quick rewrite: #!/bin/bash vms="fedora3 fedora3-1 fedora2 fedora1" for i in $vms; do echo "Backing up $i" gzip $i.img &gt; /var/lib/libvirt/images/"$i"TEMP.img.backup.gz done You could also consider using find to backup all the files in your directory like this: find /var/lib/libvirt/images/*.img -exec gzip -k -S .backup.gz {} \; Hope that helps!
I don't know if I was supposed to add something, but I ran it and it did this: [root@f17host images]# ./backup2.sh Backing up fedora3 gzip fedora3.img &gt; /var/lib/libvirt/images/fedora3TEMP.img.backup.gz Backing up fedora3-1 gzip fedora3-1.img &gt; /var/lib/libvirt/images/fedora3-1TEMP.img.backup.gz Backing up fedora2 gzip fedora2.img &gt; /var/lib/libvirt/images/fedora2TEMP.img.backup.gz Backing up fedora1 gzip fedora1.img &gt; /var/lib/libvirt/images/fedora1TEMP.img.backup.gz It didn't make backups though, only outputed that.
Okay so I ran my code, but with your suggested changed and it worked perfectly. Thanks so much!
Apologies, I left an echo in (I didn't want to back up all my VMs!), fixed now.
I suppose I'm being a bit lazy here by getting the shell to do the matching rather than find, I guess this is a more findy way of doing it: find /var/lib/libvirt/images/ -name *.img -exec gzip {} {}.backup.gz \;
It runs beautifully! Thank you very much for taking the time to help me. I'll pay it forward once I graduate :D
Oh wait, it ran right, but when I ls I have these contents: fedora3-1.img.gz fedora3-1TEMP.img.backup.gz fedora3a.xml backup.sh fedora3.img.gz fedora1.img.gz fedora3TEMP.img.backup.gz fedora1TEMP.img.backup.gz fedora3.xml fedora1.xml lost+found fedora2.img.gz fedora2TEMP.img.backup.gz test.sh It zipped all my vms. 
Stick -k on gzip.
Still need to escape the *! :-)
I'm glad you were able to solve this person's problem, but in case someone else comes along: -e allows you to specify multiple search patterns to grep, so it's superfluous here, since you only specify one pattern. It does not enable regular expressions; regular expressions are enabled by default in grep and can be disabled with -F/--fixed-strings. You may have meant -E (extended regular expressions), but you don't use any extended features, so it wouldn't be necessary. Forward slash is not a metacharacter, so it doesn't need to be escaped. Backslash does not act as a metacharacter inside a character class, so instead of escaping the forward slash, your character class excludes files whose names include backslashes (but I'm sure that's not a problem in this case).
 cd /var/lib/libvirt/images &amp;&amp; bzip2 -k fedora{1,2,3,3-1}.img
What is your current script?
Not wanting to sound like an arse hat, but why bother? If you set the service order to prefer ethernet over wifi, as soon as an Ethernet connection is available the kernel's routing table is amended to have a default route via the Ethernet etc. I can think power conservation reasons for shutting down the wifi, but from a functional perspective, it's largely redundant. Edit: typos - I can spell, I just can't type!
| TARGET_FILE="$1" Why the $1? EDIT: It worked thanks! I just wish I understood why haha
You really need to answer the questions posed in order for us to help. You're not making it clear what it is you are trying to do. Once we know that, we can write/explain a script for you.
I'm trying to facilitate the process of adding new rules to files in the pam.d folder, so things to add password legnth enforcing and such because I'll have to do it over and over again for a contest were doing. I was just trying to write a script to do that but I must have done something wrong because when I'd execute the .sh, (after chmod +x) nothing would happen. Long story short, I'm still stupid when it comes to bash and the last guy got a script that will work for me. 
Bash scripts have access to [special variables](http://wiki.bash-hackers.org/scripting/posparams) that are automatically set when the script is called. `$1` is the first argument to the script (in this case, the file name).
Glad you got it working. I wasn't trying to be curt; I was just curious about what you were doing!
If you intend to be working more with bash, perhaps take this as an opportunity to learn more about it. Even if my script works, please try and tweak yours until you can get it working. If you need to figure out more about what's going on wrong, bash has a handy feature to help you. bash interprets the argument `-x` as as 'log the command before executing it'. So run your original script as `bash -x original-script` to see exactly what commands are being run.
I appreciate the help ragardless, thanks again!
A grey menu is covering 25% of the page on the left hand side. I was unable to get rid of it or make it hide on my iPad. :-(
Notes kind of at random: [Don't parse ls](http://mywiki.wooledge.org/ParsingLs)! That is the most dangerous thing that you do in your script (function convert1). This whole ls | grep files &gt;filestoconvert.tmp; cat filestoconvert.tmp | whatever is extraordinarily dangerous. It'll choke horribly on any file that has a space or other weird characters in it. The better way is to use a glob: for i in *."$intype"; do echo "Converting $i" avconv ... done Use [[ instead of [ when you're in bash. The only reason to use [ is if you think you're in a strict POSIX environment where only sh will be available. This: if [ "$newdirectory" != "Converted_files" ]; then mkdir "$newdirectory" else mkdir "Converted_files" fi is completely redundant, and can be more clearly written as mkdir "$newdirectory" The construction if [ "$?" != "0" ] is usually redundant. If all you want to know is if some command succeeded, just check it directly: if mkdir "$newdirectory"; then As an advantage of globbing, above, instead of ls'ing and saving a tmp file, is that you don't need your cleanup function to remove the tmpfiles, so you don't need to do fancy signal trapping etc. Also, if you desperately need a tmp file for some reason, use mktemp. That's what I have off the top of my head. Good first effort!
Here is another useful link that can help you to better bash: http://bash.cumulonim.biz/BashPitfalls.html
You probably want something like: for x in *.eml ; do head -3 $x | tr '\n' ' ' &gt;&gt; tempFile.csv ; echo &gt;&gt; tempFile.csv ; done This will run through all the eml files in one directory. If you want a directory of directories of eml files, then instead of *eml you can use parentDir/*/*.eml This assumes that the 01, 02, 03 in your post were 3 different lines. Obviously if you need to strip out things like "01_Name:" etc then this requires a little more fiddling, probably best with awk. As an alternative, you can use it as is, and then after opening the csv in excel, just use text 2 column.
I think the best tool for picking the data out of the files is probably perl: perl -ne 'print "$1,$2,$3\n" if /^01_Name: (.*) 02_Company: (.*) 03_Address: (.*)/' *.eml &gt;myfile.csv You could manually edit the file to put the header on top, or do some fancy grouping like { echo "Name,Company,Address" perl -ne 'print "$1,$2,$3\n" if /^01_Name: (.*) 02_Company: (.*) 03_Address: (.*)/' *.eml } &gt;myfile.csv Of course, if not all your emails are in the same folder, you should use find: { echo "Name,Company,Address" find emaildir/ -name "*.eml" -exec perl 'that whole thing in quotes' "{}" + } &gt;myfile.csv
 #!/bin/bash for file in $(ls *.eml); do name=$(grep "Name:" $file) if [ ! -z "$name" ]; then echo ${name#"01_Name:"} &gt;&gt; myfile.csv fi done That should give you enough to get moving in the right direction.
Are there only three fields (Name, Company, and Address), or can there be more? Is it always 01_Name, or could it be 02_Name in another file? Does every file have all of the fields? Do any of the fields contain tabs or commas?
There can be more fields. There will be up to five I want to sort into a .csv. The names of each line do actually seem to change now and then, as well as the case. No tabs or commas in each field. Example: ---- Sent: Monday, December 26, 2011 8:38 AM To: &lt;sales@company.com&gt;; &lt;sales@company.com&gt; Subject: Enquiry Form 01_NAME: Dave 02_COMPANY: Blah Industries Ltd 03_POSITION: Engineer 04_TEL:=050550845614 05_FAX:= 06_EMAIL: dave@blahinc.com 07a_ADDRESS: Someplace 
 { echo "Name,Company,Address" find /home/me/Desktop/test/ -name "*.eml" -exec perl -ne 'print "$1,$2,$3\n" if /^01_Name: (.*) 02_Company: (.*) 03_Address: (.*)/' "{}" + } &gt;myfile.csv I'm not having any luck with this. (I feel so useless) It creates the .csv with Name,Company,Address at the top but nothing else.
This script requires that 01_Name: 02_Company: and 03_Address: all appear on the same line, with the '01' being the first characters at the start of the line. If they are different lines (which it looks from your post they are) then 'print "$1," if /01_Name: (.*)/; print "$2," if /02_Company: (.*)/'; print "$3\n" if /03_Address: (.*)/; ' might work better. This assumes that 01, 02 and 03 all exist in each file and in that order. If not, then you should probably move to a normal perl file when you can stuff matches into an variables and just print them (with blanks if necessary) after each file is consumed.
Don't feel useless! I didn't even test it and didn't really expect it to work first try. The key is to make sure each individual part works, then glue them together. Since there are two parts, we have to check two things. How familiar are you with find? Start by just running find /home/me/Desktop/test -name "*.eml" Does it find all your eml files? Then, second, try running the perl command on one of the files. If it doesn't produce any output, then the problem is my terrible perl. :-) Can you post an exact example of what an input line from a file looks like, and what you want the output to look like?
Seems from your other comments that the various parts you want are on different lines, so my script isn't going to work. It assumes everything's on the same line.
Yes they're on different lines. Sorry, I messed up the formatting in the OP. I'm fairly familiar with find, and Linux tools in general. My weakness is making them all work together in a script. I'm learning this stuff but I've been tasked with getting a bunch of data from a load of these emails, and it seems like scripting it is the way to go, even if it does take me a while. Definitely beats going through a bunch of files manually. *Shudder*. &gt; find /home/me/Desktop/test -name "*.eml" That works fine. It finds every .eml. &gt; Can you post an exact example of what an input line from a file looks like, and what you want the output to look like? OK so I posted an example below of what the data is I'm trying to grab. It doesn't always appear on the same line, and it isn't always in the same case. "01_NAME" might be "01_name" in some of the emails. Occasionally the numbers are switched too. It's always two numeric digits followed by an underscore however. The structure looks like this: ---- `-- WWW's 2001 |-- #assoc |-- #msgs |-- 02.www's Feb 2001 | |-- #assoc | `-- #msgs |-- 03.www's Mar 2001 | |-- #assoc | `-- #msgs |-- 04.www's Apr 2001 | |-- #assoc | `-- #msgs |-- 05.www's May 2001 | |-- #assoc | `-- #msgs |-- 06.www's June 2001 | |-- #assoc | `-- #msgs |-- 07.www's July 2001 | |-- #assoc | `-- #msgs |-- 08.www's Aug 2001 | |-- #assoc | `-- #msgs |-- 09.www's Sep 2001 | |-- #assoc | `-- #msgs |-- 10.www's Oct 2001 | |-- #assoc | `-- #msgs |-- 11.www's Nov 2001 | |-- #assoc | `-- #msgs `-- 12.www's Dec 2001 |-- #assoc `-- #msgs ---- I didn't show files just because there are too many to display comfortably. In each of the #msgs folder there are hundreds of .eml files. Not every single .eml file contains the data I'm looking for however. It's safe to assume that any file that contains "01_name" has the data I want. It's also worth pointing out that the folders above are within another directory for that year. There is a directory like this for each year since 2001. So the top level folder looks like this: WWW's 2001 WWW's 2002 WWW's 2003 WWW's 2004 WWW's 2005 WWW's 2006 I'd like to display this data in excel. So I'm looking for a way to convert it to .csv format so it can be imported easily. I'm looking for a table that looks like this: ---- Name | Company | Email | Tel :--|:--:|--:|:--:|:--: |a|a|a|a |a|a|a|a |a|a|a|a |a|a|a|a ---- 
Maybe try something with sed to get the results you want? This way you can use the power of regex and grab out only the stuff you want. I tried to make this regex as specific as possible to the requirements you listed out in the other comments, but if it doesn't work let me know and I can look at it again. (this was using just a couple test .eml files that I made real quick) $for file in `find -type f -name "*.eml"`; do sed -e 's/^[0-9]*_\(name\|company\|position\|tel\|fax\|email\|address\):\(\| \)\(.*\)/\1: \3/gi' $file &gt;&gt; list_of_customers; printf "\n" &gt;&gt; list_of_customers; done It's pretty easily formatted once you've got the command down. For example in this one I have it appending to the bottom of a file named "list_of_customers" in the same directory you're running it from, and a new line for each new file that it's run on. The output looks like this: $for file in `find -type f -name "*.eml"`; do sed -e 's/^[0-9]*_\(name\|company\|position\|tel\|fax\|email\|address\):\(\| \)\(.*\)/\1: \3/gi' $file &gt;&gt; list_of_customers; printf "\n" &gt;&gt; list_of_customers; done $cat list_of_customers name: Dave compAny: Super position: Any TEL: =12343214321 FAX: = Email: test@test.com 07a_address: someplace super market name: test test company: supercompany FAX: =123432421432 NAME: Im a test name CoMpAny: Yes FAX: =154464724623 edit: I've never done anything with csv files (terrible right?) so I don't know the exact format you're needing (aside from you mentioning the stuff needs to be at the top).
 { d=$'\t' s(){ sed '/^\s*$/d;s/:\s*/\t/;s/^[[:digit:]]*[[:alpha:]]*_//;s/[^\t]*/\L&amp;/;s/./\u&amp;/' "$1" | sort -t"$d"; } tmp=$(mktemp -d --tmpdir=/dev/shm) in=$tmp/0; out=$tmp/1 IFS= read -rd '' f s "$f" &gt; "$in" while IFS= read -rd '' f; do s "$f" | join -a1 -a2 -t"$d" -e '' -o auto "$in" - &gt; "$out" t=$in; in=$out; out=$t done; tr '\t' '\n' &lt; "$out" | pr -T$(wc -l &lt;"$out") -s"$d" rm "$in" "$out"; rmdir "$tmp" } &lt; &lt;(find -name '*.eml' -print0)
CSV files use commas to separate data into columns. Each new line acts as a new row. That's about it. CSV files are particularly useful when you want to visualise data in a spreadsheet. So I'd need the script to create a file like this: ---- name,company,position,tel dave,super,any,13241234321 test test,supercompany,, ---- Which would create something like this when imported into excel: ---- Name | Company | Position | Tel :--|:--:|--:|:--:|:--: |dave|super|any|12341234123 |test test|supercompany|| |a|a|a|a |a|a|a|a ---- Note the double comma used to represent a blank entry.
Thanks, I did not know about this. I usually handle it by making a function and checking the return status after each command and logging whether or not the command completed as intended. Question about this though. What if you have a line that expects a return status of 1 ? For example, say I use grep to ensure a given string is not there, and expect a return status of 1 before continuing. Also, why the squirly brackets around the variables? Honest questions, I'm still trying to learn.
 return $RETVAL ~~Is there a specific reason you are using return instead of echo? You usually use return when you don't want to see the output and instead want to pass it on to another function or variable.~~ (Edit: I got the purpose of return wrong.) Try using echo instead of return. Unrelated to your problem, but you should probably quote all of your variables to prevent word splitting.
I thought return was how it was meant to be done? It's the way I've seen it in all init scripts, and when I change it to echo instead of return, it simply echos "0" to stdout, which is no good. I want it to return [ OK ] like a real init script. Thanks for the pointer on var quotes, will make a note of it.
It seems I misread your problem, sorry, I thought that was what you wanted. $ help return return: return [n] Causes a function to exit with the return value specified by N. If N is omitted, the return status is that of the last command. It seems like you are using it correctly then, since it would use the return value of the daemon command, except its outputting the result to your logfile for some reason. Are you sure you don't have any '`exec`' commands elsewhere in your script? Or that you aren't redirecting the output of '`start()`'?
Nope, no other exec in there. And not redirecting the output of start as far as I can see.
How are you invoking the initscript? EDIT: that is, how is your start() function being called?
Ahh okay that isn't too bad. The first command I gave could possibly give you a file with a list of everything, but I think if you're wanting to get it into csv format you'll need to create a separate script (with additional logic), if you're still having issues later I'll take another look and see if I can't help out with that part of it. (though there might be an easier way to do it?)
Gotcha. I used to know more about initscripts, but my distro got rid of them. I'm going to tinker around and let you know.
If I remove the backgrounding the script hangs when it runs, I'll give the bash -x a try.
I think the ~~backgrounding~~ redirection is messing you up? I don't know if your /etc/rc.d/init.d/functions looks like mine, but I assume you're calling a similar daemon() function that you sourced from there. My daemon() function starts the process by calling cgexec, then it calls either success() or failure() depending on the result. Those are the functions that print "[ OK ]" or "[ FAILED ]" to the console. The fact that you're redirecting the daemon output entirely will also cause those things to be redirected. EDIT: so you may want a way to specify a logfile in your java program without depending on bash redirection.
Well, you can, but I can't think of a time where it would be useful. AirBoxOmega:~ d$ x=1234567654321 AirBoxOmega:~ d$ echo ${x:0:5} 12345 AirBoxOmega:~ d$ echo ${x/${x:0:5}/} 67654321 AirBoxOmega:~ d$ echo ${x/${x:0:5}/abcde} abcde67654321 AirBoxOmega:~ d$ But I don't think that's what you or the OP mean. (Or I'm confusing both of your meanings all together.)
Ah, I think I see what you mean. You could do something like this: AirBoxOmega:~ d$ x=abcd1234 AirBoxOmega:~ d$ abcd=1234 AirBoxOmega:~ d$ echo ${x:0:4} abcd AirBoxOmega:~ d$ echo ${x/${x:0:4}/4321} 43211234 But that's still not really two substitutions at once. Oh well, I've been mostly trying to move most of my basic shell stuff into (g)awk. I've only recently realized how awesome (g)awk is.
It is awesome, but it's way, way cheaper to do simple string stuff in bash. Save yourself a process!
Yes
Could do something like for f in $( find /whatever/directory/ -type d -name "*word123*" ); do orig_name = $f; new_name = $( echo $f|sed 's/\ \[word123\]//' ); echo mv $orig_name $new_name; done obviously, remove the echo (or, what I would do, is leave that echo, copy the line and remove the echo in the next line so you can see progress) but test with it in place first to make sure it does what you want. I haven't tested this, as I don't really want to haul my laptop out right now. But it should definitely put you on the right path.
You can do that with the rename command (pre-installed on all machines I've seen) find -type d -name "*\[word123\]" -print0 | xargs -0 rename " [word123]" ""
If " [word123]" appears more than once in a directory name, rename will only replace the first occurrence. If that's a problem, or if rename isn't available, this should work: find -type d -name '* \[word123\]' -print0 | while read -rd $'\0' f; do mv "$f" "${f% \[word123\]}"; done
Don't use for f in $(find ...); do It'll mess up on files with weird characters in them, like spaces or newlines.
Are you sure? (correct me if I'm wrong, just going off of what I tested with) I actually ran the command before I posted it here to ensure it's functionality of renaming all directories that it finds. xargs should *essentially* be doing the same thing as your while loop (though it's not the *exact* same), in the fact that the result will be the same. Consider the following (try it on your end if you'd like): $mkdir directory.name{1,2,3,4,5,6,7}\ [word123] $find -type d -name "*\[word123\]" -print0 | xargs -0 rename " [word123]" "" $ls ./ .htaccess directory.name2/ directory.name4/ directory.name6/ ../ directory.name1/ directory.name3/ directory.name5/ directory.name7/ But yes, I agree. If rename is not installed, your way should work perfectly fine.
&gt; If " [word123]" appears more than once in a directory name I meant this: $ touch ' [a] [a]'; rename ' [a]' _ *; ls _ [a] rename replaces the first occurrence of ' [a]', so if he wants to replace the last occurrence, he has to use something else. Otherwise, your approach is fine and is the way I'd do it as well.
Ahh okay that makes sense. Thanks for pointing that out and clarifying. 
I remember thinking todo.txt (http://www.todotxt.com) would be pretty useful for this when I looked at the code one day. It's practical and easy to read. https://github.com/ginatrapani/todo.txt-cli/blob/master/todo.sh They also have test cases (in bash). https://github.com/ginatrapani/todo.txt-cli/tree/master/tests
Ah, a classic. for r in ./*\ \[word123\]; do mv "$r" "${r%\ \[word123\]}"; done
Some weeks ago I created some scripts for adding a domain or a virtual host to my apache config based on a template, enabling/disabling domains/virtual host and removing domains and virtual hosts - so that might be a good idea if you're working for a hosting company. If you want a good tutorial use this one: http://www.tldp.org/LDP/abs/html/ - that's the one I always used to look something up.
Thanks! commandlinefu.com looks really awesome. I'll have to spend some time there. I've been on server fault before but not for much other than just google results, so that's a good idea too. 
Thanks! This looks interesting, I'll run it by my buddy and see if it's something he'd want to do.
That's an excellent idea, and I think my buddy would really enjoy this one. Thanks!
[The FAQ on Greg's wiki](http://mywiki.wooledge.org/BashFAQ) and also his BashGuide are pretty amazing resources. I'm working my way through the FAQ, it's not something I can digest in a single afternoon. I think my scripts are getting much better as I go.
This is the best solution and correctly handles weird characters in filenames. If you want to be super extra pedantic, use mv -- "$f" "{f% \[word123\]}" instead of mv "$f" "{f% \[word123\]}" in case any of your directories start with a '-'.
Why do you want to do this? You may as well just have the user run your script and then cd afterwards, or do it in one line like ./my_script.sh &amp;&amp; cd sample_directory As you've discovered, the working directory is a property of the current shell. Running a script starts another process with its own current directory. Even calling any kind of subshell or pipeline will not affect the current shell: $ pwd /home/stoic $ ( cd .. ) &amp;&amp; pwd /home/stoic $ echo "hello" | { cd .. ; } &amp;&amp; pwd /home/stoic EDIT: an alias doesn't create a subshell, so you could do this: alias runmyscript="/path/to/my/script &amp;&amp; cd sample_directory" then calling "runmyscript" would do what you want, roughly. I still would like to know why you want to do it, though.
It is in the context of a script I am trying to help my buddy with. He is doing some network traffic capturing of some sort and writing the various outputs into a unique, dynamically generated directory each time the script is run. The goal is to improve user experience by placing the user in the directory where their results are found after the script completes. 
Re: Edit: Yea, we've considered doing aliases... that will probably be how it ends up...
If run it with .space rather than ./ any directory changes will remain after execution. 
You can use curl, a command line tool build into most distributions. Simply curling the URL will return the source of the page. From there, you can use other tools like head/tail and awk to process the output and return only the line(s) you need.
PHP HTML Simple Dom is what I use for this sort of thing. You can query things a lot like you would using sizzle (jQuery) http://simplehtmldom.sourceforge.net/
Like another user said, start with curl to get the source. if you're new to the shell, the most user friendly way to process the text would probably be to use tools like grep and cut. For example say we want the string in this: &lt;h1&gt;"My String"&lt;/h1&gt; You'd run curl "mywebpage.com" | grep "h1" | cut -d '"' -f 2 Grep is a tool that searches for strings that match the pattern you enter. Normally, it will find multiple matches, so you can configure it with flags to stop at the first match (-m 1). The cut tool cuts text. The -d flag tells it to use quotation marks as its field delimiter, and "-f 2" tells it to print the second field using the new delimiter. I suggest this approach because its easiest for a beginner, but its not the best as you are combining multiple tools and its more like a hack. Sed and awk are better suited for text manipulation. For more info on each of these tools, check out `man grep` and `man cut`.
There's nothing wrong with combining multiple tools. Sometimes I complain about people piping grep into sed, since they're so similar, but there's nothing really wrong with it. If you wanted to just use awk instead of grep | cut, I think you could do it with curl "example.com" | awk -F\" '/h1/ { print $2 }'
For reference, these are the best tutorials I've seen: awk: http://www.grymoire.com/Unix/Awk.html sed: http://www.grymoire.com/Unix/Sed.html
I've seen the sed one, never knew they had one for awk on the same site. Thanks! :)
broken down into functions w/ (hopefully) enough comments allowPrint () { # from stdin, only pass on printable letters &amp; space # protects everything below this from exploits, because it only passes # valid values for html # # This strips CR and LF tr -dc '\040-\176' } getH1 () { # only get &lt;H1&gt; heaters # matches both &lt;H1&gt; and &lt;h1&gt; # note the regex non-greedy "*?" ; we need perl regex grep -Po "&lt;[Hh]1&gt;.*?&lt;/[Hh]1&gt;" } addCR () { # adds a c/r # because there may be more than one &lt;H1&gt; # note "\r" # http://stackoverflow.com/questions/71417/why-is-r-a-newline-for-vim sed -e 's/&gt;&lt;/&gt;\r&lt;/g' } stripH1 () { # remove tags sed -e 's/&lt;[Hh]1&gt;//g' -e 's/&lt;\/[Hh]1&gt;//g' # can you optimize into one statement? } curl -s -o - -- "www.example.com/page.html" | allowPrint | getH1 | addCR | stripH1 You *can* write screen scraper code in bash, but if it gets much more complex than this I would highly suggest using a tool like Python. 
What DHCP server are you using? Parsing logs seems...let's say...cumbersome. I have some awk magic in my .bashrc that parses the leases file instead and outputs it in a table for easy filtering. I can post it in a couple of hours when back in front of a computer if interested. Edit: Of course that does not allow you to look back in time, but it's unclear from your question if you need this...?
I'm not the system owner, we get copies of logs pushed to our system. I work in law enforcement, doing log analysis for incident analysis. 
What have you done so far? Searching log files in the way you describe will most likely include lines not in your 72 hour window, unless every event you're analyzing is at midnight. It's also possible that there are multiple entries within a 72 hour period for a given ID (DHCP releases come to mind). You ask for an IP address as output, but you don't say what the log format is.
So maybe something like this? (Untested.) I don't know what you mean by parsing, but suppose you just wanted to grep for that machine in the logfile. #!/bin/bash machine=$1 month=$2 day=$3 year=$4 for i in 1 2 3; do # 3 days max grep "$machine" "$year""$month"/"$day".log &amp;&amp; break # change to previous day read year month day &lt;&lt;&lt;"$(date -j -v-1d -f "%Y %m %d" "$year $month $day" +"%Y %m %d")" done
Oh yeah, my sqlite file is filled with this script: #! /bin/bash NET=173.252 for n in $(seq 252 253); do for m in $(seq 1 254); do ADDR=${NET}.${n}.${m} echo $ADDR HOST=$(dig -x ${ADDR} +short) if [ "$HOST" != "" ]; then echo "REVERSE DNS FOUND" echo "insert into rev_dns_tbl values('${ADDR}','${HOST}');" | sqlite3 dig_rev_dns.sqlite fi done done exit 0 Simple ideas to make this project more "shiny", so that it would stand out, are always welcome
Don't use seq. for n in 252 253; do for m in {1..254}; do [...] done done Also, use [[ instead of [ for tests in bash. And there's a built-in test for seeing if a string is not null: if [[ -n "$HOST" ]]; then One more thing: if you find yourself doing echo "whatever" | program, you can save yourself the echo process by using a herestring: sqlite dig_rev_dns.sqlite &lt;&lt;&lt;"insert into ..."
Try just doing printf $COMM on its own to see if it looks right. It may not, since printf uses % as a format control specifier, so those %s in your string may have some weird interaction. A fix for that is printf "%s" "$COMM" but as I noted in my other comment, you may as well use a herestring in this case.
Thanks, mate, the herestring is a real lifesaver. The script is so short that as a beginner I'd call it elegant. I'll rewrite the insert-script later
Did it work?
Phone account now...yes it worked perfectly. 4 lines of code, 2 being #!bin/bash and exit(0), no for l
Ah, a teacher once told me exit was the proper way to exit. Maybe its just when you use loops and other fl
Or even for ADDR in 173.252.{252,253}.{1..254}; do ... done Also, [bash doesn't wordsplit inside \[\[](http://mywiki.wooledge.org/BashFAQ/031), so you don't have to quote.
Well, the user will want the most recently assigned IP at the date and time entered... This will be used when we have an attack/malware incident. Our Bluecoat has IP addresses, so we will use this to determine the individual machine at the time of the attack. 
Is it possible that you asked this exact same question with identical wording two years ago? http://www.linuxquestions.org/questions/linux-newbie-8/ftp-get-count-of-files-on-a-remote-machine-directory-to-the-source-linux-machine-910477/
Wasn't me, but someone else was looking for something very similar and I used their question as a template. When I tried to run it - I get a copy of the script as a result. 
You have two requirements and need to clarify which you want: &gt;check for the number of files present on the FTP share and &gt;Return back the folder count Off the top of my head, you could do this with an expect script or bash+EOF and tee the output of the session to a local logfile which you can then process with bash. The output of an FTP dir or ls isn't standardised (to my knowledge), but most seem to use an ls -l style format, so you should be able to do something like grep ^d logfile | wc -l To get a list of directories, then count them, outputting a number. To do a *file* count, you'd need to sed out any extraneous FTP session output, then something like: grep -v ^d logfile | wc -l The \^d looks for d in the first character of a line e.g. **d**rwxr-xr-x 1 user group 4012 Dec 10 15:11 somedirectory But surely there's a more elegant way to achieve this.
Try this: #!/usr/bin/env bash HOST="host.goes.here" USER="username" PASS="password" CONTENT="$(ftp -in "${HOST}" &lt;&lt;-HERE user "${USER}" "${PASS}" ls bye HERE )" echo "${CONTENT}" All going well, you'd get an output of the session. Once you're sure that side is working fine, you can build on it e.g. echo -n "Number of remote files on ${HOST}:" echo "${CONTENT}" | egrep -v "user|ls|bye" | wc -l" I think if you have \^d in your egrep -v, then it's going to exclude folders and you obviously won't get your folder count. Another thing to try, for aesthetic's sake, may be something like: echo "Number of remote files on ${HOST}: $(${CONTENT} | egrep -v "user|ls|bye" | wc -l)" Unfortunately I don't have an FTP server to test against, so I'm just making an educated guess. I did do a similar thing on an HPUX box a while back though, and I'm sure I had to bring tee into the mix. /edit: or as I hinted in my last post, just grep for \^d echo "Number of remote files on ${HOST}: $(${CONTENT} | grep ^d | wc -l)" /edit 2: Just did some quick testing with ls -l, it's easier to do it this way: COUNT=$(echo "${CONTENT}" | grep ^d | wc -l) echo "Number of remote files on ${HOST}: ${COUNT}" OR don't declare COUNT and use: echo "Number of remote files on ${HOST}: $(grep ^d &lt;&lt;&lt; "${CONTENT}" | wc -l)"
Recursively or just the top level?
just top level
&gt; CONTENT="$(ftp -in "${HOST}" &lt;&lt;-HERE user "${USER}" "${PASS}" ls bye HERE)" That won't work; you need newlines in there. Test it with just echo, and see that "HERE" gets printed.
Good catch, I've updated to suit and checked against shellcheck.net.
For the lazy: /r/tinycode --- I provide direct links to lesser known subs mentioned in the title if one isn't already provided. Let me know if I need to try harder: /r/LazyLinkerBot
:-/ I should know better than to just type in a website's name, right? I think I got away with that.
I once downloaded the entire C64 Blast collection with wget. It took me so many tries to use it in a way that would not be rejected by the server.
Use `curl` and `grep`: $ curl -s ftp://ftp.univie.ac.at | grep -c '^d' 7 This is probably the cleanest without doing it in perl or python.
Have a look at expect. This blog post - http://tamas.io/automatic-scp-using-expect-and-bash/ - demonstrates how to use it in bash scripts.
this looks like what I need, I'll read up on this. Thanks
That is correct.
thanks a lot for the review! I learned a lot!
You should try just piping your answers into the script, in case they read from stdin. It'll be much simpler than using expect if it works.
I wasn't complaining, I just mean it's good to know how to do it, as it saved a tone of time in the end! I think there are about 355 disk images in the Blast collection. That would have been a huge pain to download manually and then move the the proper directory. 10 mins of wget later, I had thousands of C64 games!
Isn't this just a bash array?
I think the difference between an array and a hash is named key access versus numeric index. JSON examples: [1,2,3] // vs {"one":1,"two":2,"three":3}' Edit: as /u/thestoicattack pointed out in another thread i guess these are called "Associative arrays"
More on bash arrays (including these associative arrays): http://mywiki.wooledge.org/BashGuide/Arrays
Yeah, not a "trick". Normal associative arrays in `bash` there, but since they were only "recently" introduced (bash 4; 2009) you don't want to use them if portability is important.
Would xargs help? I'm probably misunderstanding the question.
Working with arrays in bash is primitive at best. However they did improve it significantly in bash 4. If portability isn't an issue there is also mapfile/readarray which creates an indexed array from lines of stdin. Much easier to implement than iterating read through a for/while to populate an array.
Are you looking for style advice or performance improvement?
You should probably take a peek at Yeoman and Grunt.
Hmm, for the flags themselves: First off, since you are getting a bit more advanced than aliases, it is better off to create each of these as separate shell scripts located in your $PATH. Make sure each includes a hashbang like `#!/usr/bin/env bash`. You'd just name each file with the name you want to call them with, like 'make_html_project'. Then the shell script itself. Lets get the arguments first. In bash, the first argument is $1, the second $2, and so on. #!/usr/bin/env bash #create html templates # '$#' is the number of arguments if (( $# == 1 )) ; then #we match up the flag you used case "$1" in -Normalize) do stuff ;; -Flag2) do stuff ;; *) do stuff ;; #this matches everything else in case you want to catch a misspelling esac else if (( $# == 0 )) ; then do stuff else echo "Too many arguments." exit 1 fi I hope that gives you a good place to start off with. With this you could do some individual cp commands or however you wish to approach it. 
I'm not sure how many different flags you'd want, how often you'd want to create new conditional files, or which additional features you'd be looking for (more than one file per flag? custom logic per flag?), so here are some untested ideas. You could put all of the optional files in a directory outside of "general_template": # make_html_project . -normalize cp -a /.../general_template/ "$1" cd "$1"; shift for a; do cp -p "/.../optional/css/${a#-}.css" css/; done cd - You could put them all in the same directory tree and list the names of the conditional ones in a script/zshrc function: c=(normalize ie bw); x=(); new=$1; shift for a; x+=(--include "css/${a#-}.css"); done for f in "${c[@]}"; do x+=(--exclude "css/$f.css"); done rsync -a "${x[@]}" /.../general_template/ "$new" or you could give the conditional files a special prefix: cp -a /.../general_template/ "$1" cd "$1"; shift for a; do f=${a#-}; mv "css/prefix-$f.css" "css/$f.css"; done rm css/prefix-*.css cd -
Ive seen the todo script; and I like it, I currently use any.do bc it syncs across my iPhone, Tablet &amp; computer. 
Oh yeah, syncing is very important. Im just surprised they wrote it in bash. And any.do looks good. Im currently on the prowl for a todo app and might switch to that.
Most of it is specific to Parabola GNU/Linux-libre, but there's quite a bit of mostly well-written Bash at &lt;https://projects.parabolagnulinux.org&gt;. The most interesting to an outsider is probably [git-rewrite-branch](https://projects.parabolagnulinux.org/packages/pbs-tools.git/tree/git-rewrite-branch), which is still probably only interesting to a frequent git user.
Dropping it in double quotes works as well!
Learn something new every day.
You should look at dotfiles repositories. **Links:** * [@holman does dotfiles](https://github.com/holman/dotfiles) * [spark](https://github.com/holman/spark) * [bd](https://github.com/vigneshwaranr/bd) [my dotfiles](https://github.com/ZDroid/dotfiles)
Some random comments: 1) You use $i in your for loop. This is bad practice, since it relies on ("coupled with") needing an outside variable $i to be set. Instead, you should decouple this interface-of-sorts by relying on passed in parameters to the bash function. When calling "myfunction arg1 arg2 arg3", those will be available to myfunction via $1, $2, $3. 2) Instead of copying, consider symlinking. This will avoid data duplication (unless you do in fact need copies?). You can also check out hard links, but symlinks are almost always a better option. 3) Your use of string interpolation. Instead of (for example): "$PROJECTDIR"/locations/"$GPSPosition" You can use: "$PROJECTDIR/locations/$GPSPosition" This usage is cleaner looking. Once inside of the quotations, you shouldn't have to worry about spaces in each individual variable that you're interpolating. 4) Always put a date &amp;amp;amp;amp; comment/purpose to your scripts at the top, along with the shebang line. Even silly/stupid ones you don't think you'll keep. You'll thank yourself in a few months. 5) Consider posting your scripts to pastebin or (my favorite) github gists, instead of directly to a reddit comment, as that saves screen real estate on the reddit page &amp;amp;amp;amp; is overall cleaner. This is even more important if you're posting a really large script. 6) Quote your initial variables at the top, and evenly space out their assignment. For example: FOO1 = "bar" MYFOO = "bar2" (Blah, this is hard on mobile; I hope it renders OK). This isn't really an efficiency thing, of course, just some style-advice I thought I'd throw in. One could argue that it does promote efficiency, however, in that good style allows maintenance coders (perhaps yourself) understand the code faster in the future. 7) Be sure to consistently quote variables... cp -nv $i "$PROJECTD..." Should be: cp -nv "$i" "$PROJECTD..." 8) For logging, have a look at the Linux command 'tee'. You can make a decent log by throwing everything a verbose flag &amp;amp;amp;amp; then piping it through "tee", and telling it to append the output to your log file. Essentially, "tee" will split ("tee" / T) standard input to both standard output, and to a file of your choosing. This could replace a lot of your echo statements, unless you just like having the extra commentary. This is just a few ideas; hope it helps! P.S. Welcome to the land of coding! Edit: stupid mobile typos.
Great, thanks for the tips! Definitely going to go back and clean some stuff up.
&gt; 1) You use $i in your for loop. This is bad practice What? &gt; 6) Quote your initial variables at the top, and evenly space out their assignment. You can't put spaces around an assignment operator in bash.
Hey! Thanks - i've tried this method but get the following output.. Any ideas? Bareword "word123" not allowed while "strict subs" in use at (eval 1) line 1. xargs: rename: exited with status 255; aborting 
1) sorry, meant using the for-loop $i implicitly as the input parameter for a function, vs. passing it as an argument 2) FOO = "BAR", not FOO "= BAR". See the example I gave.
&gt; 2) FOO = "BAR", not FOO "= BAR". See the example I gave. I saw your example; it doesn't work. $ a = 0 -bash: a: command not found
That sounds like you're missing some "" or possibly a character needing to be escaped. I don't think I've ever seen an error about strict subs in bash, only in perl while using "use strict", so that's new to me. It's also possible that you forgot the -print0 in the find, and the -0 after the xargs. Basically a bareword is an unquoted string that just needs quotes around it.
Your heart's in the right place with find, but doing $(find ...) kind of puts back the exact same problem in that it'll get wordsplit on spaces again, which is exactly what you want to avoid. It seems like you're trying to save the output of the find so that you can then remove the ignored stuff later? I think a better-constructed find command will save you all of that work; something like find $FOLDER_PATH -ctime -1 ! -name "*${IGNORED_FOLDER[0]}*" ! -name ... Then you don't have to do all the crazy stuff with iterating over the find results. Plus I see you're printing the output line by line, which is the default use of find anyway, so perhaps your whole script can be { echo "subject: ... from: ..." find $FOLDER_PATH -ctime ... ! -name ... ... } | sendmail
Don't parse the output of ls!! I'm on mobile but will write more later and send a useful link. EDIT: the promised link http://mywiki.wooledge.org/ParsingLs It's definitely safer to use a glob. In this case, since there's a lot of different filetypes: shopt -s extglob for file in (*.jpg|*.jpeg|*.png|*.gif|*.bmp|*.tiff); do [...] done Also, use $(...) instead of backticks for command substitution. As MrStench mentioned, sed is overkill for most simple string manipulations. Often, bash's built-in stuff is enough, and it's much, much faster: http://mywiki.wooledge.org/BashFAQ/100 And the bash else-if keyword is elif. And use [[ instead of [ if you're programming in bash (as opposed to sh).
Agreed. I recommend: find . -maxdepth 1 -regex '^.+\.\(jpg\|jpeg\|png\|gif\|bmp\|tiff\)$' (Untested, but I'm pretty sure you'll want the extra backslashes.)
I'd suggest replacing your while loop with a for filename in $(find ...); do loop. This avoids using ls's output (on which you should never expect consistancy). Also, you can throw in a -type f into the find command so that you can remove the first check. To get the md5, use cut, not sed: md5sum "${filename}" | cut -d' ' -f1 I would prefer using -v for rm and mv instead of adding a custom message -- this would make the code easier to maintain. I think your indentation is a little wonky overall, but I'm not sure what the "proper" bash indentation is, so I'll let someone else comment if they find fault.
Doing $(find ...) wordsplits on whitespace again and so still has the potential to mess up on files with spaces or newlines. Agreed for rm -v and mv -v.
Great catch, thanks! I'm interested to see what solutions there are. A bit of googling seems to indicate this is a messy business.
&gt; seeking criticism You talk too much and your mother dresses you funny. ;-) &gt; I always wanted to be somebody, but now I realize I should have been more specific. -- Lily Tomlin
&gt; This avoids using ls's output (on which you should never expect consistancy). `for filename in $(find ...)` is bad for exactly the same reasons parsing ls is: http://mywiki.wooledge.org/ParsingLs
Your glob needs an `@`: for file in @(*.jpg|...); Setting extglob affects all globs in the current shell and subshells, so you should reset it to its original value after using it. Either after the for loop (which may still lead to unexpected issues), or something like g=`shopt -p extglob` shopt -s extglob files=(@(*.jpg|...)) $g for file in "${files[@]}"; For OP's problem, I think the following is more appropriate: for file in *.{jpg,jpeg,png,gif,bmp,tiff}
You may be interested in utilities that find duplicate files, such as fdupes and fslint. for filename in ./*.{jpg,jpeg,png,gif,bmp,tiff}; do [[ ! -e $filename || -d $filename ]] &amp;&amp; continue filemd5=`md5sum &lt; "$filename"` ext=${filename##*.} mv -v "$filename" "$filemd5.$ext" done echo
He echoes the find results as well as feeding them to sendmail, so you'll want a tee in there: find ... | tee &gt;({ echo "subject: ..."; cat; } | sendmail) Here's one possibility for the "better-constructed find command": for d in "${IGNORED_FOLDERS[@]}"; do ds+=(-path "$FOLDER_PATH$d" -o) done for f in "${IGNORED_FILES[@]}"; do fs+=(-name "$f" -o) done find "$FOLDER_PATH" -ctime -1 ! \( \( "${ds[@]}" -false \) -prune -o -type f \( "${fs[@]}" -false \) \)
&gt; Your glob needs an @ Good catch! &gt; Setting extglob affects all globs in the current shell and subshells Since it's a script it'll execute in its own subshell, and the script is quite short, so I figured this wouldn't be too dangerous.
 18 files=( $(find $FOLDER_PATH -ctime -1 2&gt; /dev/null) ) already splits on spaces, so when it reaches lines 30 and 36 none of the array elements have spaces.
Hey, a useful use of cat!
Can you post the equivalent in bash? In bash you generally iterate through a list of values, eg for var in one two three; do echo $var; done What are you trying to do in bash?
Ah, I see. You can do it by using the index of the value you want to change. Here's one way: array=(one two three) for ((idx=0; idx&lt;${#array[@]}; idx++)); do array[$idx]="${array[$idx]}." done # for var in $array; do echo $var; done # one. two. three. Incidentally, that's how I'd do it in other languages as well - to use the example from your original post: &lt;?php $arr = array(1, 2, 3, 4); foreach ($arr as $idx =&gt; $value) { $arr[$idx] *= 2; // or $arr[$idx] = $value * 2 } ?&gt; I actually wrote a thing ages ago about handling arrays in bash, which you might find useful (and might be a bit out of date, but still relevant): http://pgl.yoyo.org/bits/tech/bash-arrays-assignment-looping-and-indexing/81:2008-11-16/
There is no equivalent in bash. Because bash arrays are associative, you can iterate over the keys instead of expecting sequential indices: for k in "${!a[@]}"; do printf %s\\n "${a[$k]}"; done You can perform some string manipulation on every element of an array simultaneously. For example, the following will append a period to each element of an array: a=("${a[@]/%/.}") See http://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html.
In this *exact* case you can do array=("${array[@]/%/.}") EDIT: whoops, just saw /u/r3j mentioned this in his comment.
Oops. Yeah.
When you type var=$(command) you're telling bash to store what command outputs to STDOUT into the variable var*. It would appear that sox is using STDERR to print this output instead of STDOUT, which is 1) why you're not getting a value stored in volume and 2) why you're actually seeing output of the command. For fun, try volume=$(echo mystring) and you'll notice that you don't see anything get printed to your terminal and that the value of volume is mystring. Anyway, the quickest way to fix your issue would be by redirecting the STDERR output of sox into STDOUT. I'm not sure if you'll be happy with what all is stored in there and how it ends up working, but it's a start. *NOTE: This is my understanding, which could be completely wrong, though my anecdotal evidence says that this is the case..
Thank you! you were absolutely right! I was able to move on with my script by doing this volume=$(sox /dev/shm/noise.wav -n stats -s 16 2&gt;&amp;1 | awk '/^Max\ level/ {print $3}') This gave me the value I was looking for. Now, on to the next step. Thank you so much
This is dumb, but would you consider just running passwd? Then you'd know what the new password is. EDIT: I'm an idiot; you'd need the old password anyway.
No this isn't possible since passwords are stored as salted hashes in /etc/shadow, which you'd need root access to be able to read.
You're actually the closest to what I've decided to do! I still have access to a restricted root shell with predefined aliases to do things like change the current password temporarily. I just wanted to know if I had to do that and remove a step from how seamlessly this could be run. As it stands now, I'll just temp in and then run the script, read to input password and be on my way. Thanks for the suggestion!
Thanks! The input is appreciated.
Could you explain why using $() is better?
http://mywiki.wooledge.org/BashFAQ/082
This will fail in new and interesting ways if `$input` is empty. For more reliable results: while ! [[ y != "$input" &amp;&amp; n != "$input" ]]; do echo "Please provide input" read input done
Actually, [[ is safe in this case: See http://mywiki.wooledge.org/BashFAQ/031 , which says: &gt; No word splitting or glob expansion will be done for [[ (and therefore many arguments need not be quoted) However, I agree it's good practice to quote them anyway, just to be safe, because [ or test *will* fail in horrible ways.
Probably be better off posting to /r/commandline as it's a much wider audience. Also, I've had the following in my .bashrc for years: function extract() { if [ -f $1 ] ; then case $1 in *.tar.bz2) tar xvjf $1 ;; *.tar.gz) tar xvzf $1 ;; *.bz2) bunzip2 $1 ;; *.rar) unrar x $1 ;; *.gz) gunzip $1 ;; *.tar) tar xvf $1 ;; *.tbz2) tar xvjf $1 ;; *.tgz) tar xvzf $1 ;; *.zip) unzip $1 ;; *.Z) uncompress $1 ;; *.7z) 7z x $1 ;; *) echo "'$1' cannot be extracted via extract" ;; esac else echo "'$1' is not a valid file" fi }
Take a look at [dtrx](http://brettcsmith.org/2007/dtrx/). It can extract/preview anything I've ever thrown at it. It's also in the repos for a handful of distros. 
Thank for for the suggestion; and this looks perfect. - Thank you!
You should quote `$1`.
And $dest and $2.
You're correct. My mistake.
Alternatively, read -p 'type y/n: ' input until [[ $input = [YyNn] ]]; do read -p 'please provide a valid input: ' input done
I believe I understand what you're going for, and I came up with this snippet which you could maybe use as a base (it's certainly not polished). This assumes that your tempfile is in date order. #!/bin/bash compDate=$( date -d "$1-$2-$3" '+%s') foundMachine='None' while read line do lineDate=$( echo $line | cut -f1 -d" ") lineDate=$( date -d "$lineDate" '+%s') if [[ $lineDate -le $compDate ]] then foundMachine=$( echo $line | cut -f3 -d" ") fi if [[ $lineDate -gt $compDate ]] then break fi done echo $foundMachine Saved to script.sh and your example data to datafile.sh: $ ./script.sh 2013 03 01 &lt; datafile.txt MAC_NAME_600703 $ ./script.sh 2013 03 19 &lt; datafile.txt MAC_NAME_601179 $ ./script.sh 2013 02 24 &lt; datafile.txt None $ ./script.sh 2013 04 1 &lt; datafile.txt MAC_NAME_601585
you the MAN!
Do you need the temp file with all the mappings from the two months? I'm thinking you could replace most of your script with this, assuming that each log file is sorted chronologically (And you may have to fix the `log.gz` part; I was just guessing.): y=$1; m=$2; d=$3; ip=$4 for ((i=3; i&gt;=0; --i)); do files+=("$(date -d "$y-$m-$d -$i days" +'/log/dhcp/%Y%m/DhcpSrvLog_%m%d.log.gz')") done zcat "${files[@]}" | fgrep -w "$ip" | grep ^10 | cut -d, -f2,3,6 | tr , ' ' It will give you all the mappings in the past three days. You could add `tail -n1`, but I think you'll run into the same problem I mentioned in your last submission, that you might get mappings that occurred after the time of the incident. As for your old script, `grep ${GREP_TERM}` will match too much. If GREP_TERM is `1.2.1.8`, then it will match any string that contains it, e.g. `1.2.1.88`, as well as the fact that the periods match any character, so it will also match `192.168.0.1`. There may be some other issues, but if my new script works, that'd be easier to check since it's shorter.
A really fantastic resource. Thanks! Would you be open to pull requests that just add comments?
we are going to add hours and minutes to the script, how should i format the date function? 
thanks for the ideas... I did change it to an Fgrep -w, so that should resolve that issue. Also, i'm adding a flag for verbose output, because some of my users will want that extra input, and a flag for output, which will output the tempfile. 
If it's in cron wouldn't just returning a non zero exit code cause it to email you? Investigate when cron sends emails
has anyone here used this site? any comments? currently this looks like an ad ..
Agreed, I'd like to know some impressions of this site.
Idk but their site sucks on mobile. Whoever wrote that code...
If you don't want to go with getopt for option processing, then you can do it manually. Typically I break it up into two functions - one to loop over `$@` and set variables accordingly. The main process interprets variables and does validation. That can be broken out to other functions as needed. For "verbose" I open an additional file descriptor to stderr. If the verbosity is not turned on then they just go to /dev/null. Same idea for "output to file" although it's questionable why you wouldn't just rely on shell IO redirection if you want to save the output. Using this method you don't have to have `if` checks all over your code. Example below of my basic template; may contain errors, your own risk, yada yada. #!/bin/bash SCRIPT_SELF="$(readlink -f "$BASH_SOURCE")" SCRIPT_HOME="$(dirname "$SCRIPT_SELF")" SCRIPT_NAME="$(basename "$0")" cat_help () { cat&lt;&lt;__EOF__ Usage: $SCRIPT_NAME [options] args Options: -h, --help, -? Shows this help. -v Verbose. -o FILE Send output to FILE. Default is STDOUT. __EOF__ } parse_args () { while [ "$1" ]; do local ARG="$1" shift case "$ARG" in -h|--help|-\?) HELP_REQUESTED=help-requested ;; -v) VERBOSE=verbose ;; -o) OUTPUT_FILE="$1" shift ;; -*) echo "Bad Option: $ARG" &gt;&amp;2 return 1 ;; *) TARGET="$ARG" ;; esac done } main () { parse_args "$@" || return if [ "$HELP_REQUESTED" ]; then cat_help return 0 fi if [ "$VERBOSE" ]; then exec 3&gt;&amp;2 else exec 3&gt;/dev/null fi if [ "$OUTPUT_FILE" ]; then exec 4&gt;"$OUTPUT_FILE" else exec 4&gt;&amp;1 fi # main processing echo "Debugging info can be sent to FD 3" &gt;&amp;3 echo "Stuff sent to FD 4 ends up in a file with -o; to stdout otherwise" &gt;&amp;4 } main "$@"
the double quotes protect the tested values from being interpreted as conditions themselves. ( x=-gt; if [ "$x" ]; then echo "yes"; else echo "no"; fi ) yes `cat&lt;&lt;-EOF` protects all leading space, which messes with the indentation level of the function. I prefer to keep the indentation of the functions; so the period prevents trimming essential whitespace needed for formatting, and the sed statement removes it. It could be any character, but a period is the least obtrusive. The second swap is to take any leading tabs (after the period) and replaces them with the same number of spaces I have set in my editor. I consider it a matter of style, and you know what they say about code formatting styles. (Usually I add spaces before the flag `-` markers if they are logically groupable - but I've played with different ways of doing it; see [my java cert importer](https://bitbucket.org/beltorak/unix-personalization/src/eaa0b78fe7b94afc2dad316c86c3222a9aacd227/opt/bin/import-java-certs?at=master) where I just strip off two tabs and convert the rest to spaces.) I prefer python's [inspect.cleandoc](http://docs.python.org/2/library/inspect.html#inspect.cleandoc) which intelligently formats doc comments; but there's no equivalent in (native) bash. OTOH I could use a delimited comment block and `sed` over `$SCRIPT_SELF`.... *edit* oh; and I prefer to use descriptive internal var values because executing under `bash -x` gives a better idea of the decisions taken: if [ "help-requested" ]; then cat_help return 0 fi instead of just if [ 1 ]; then .... I wasn't aware of the `if ((VAR))` form... that's interesting. Thanks :)
&gt; the double quotes protect the tested values from being interpreted as conditions themselves. I was wrong about `test` issuing an error for having a lone flag argument, but it has nothing to do with the double quotes. In `{ a=-z; test $a; test "$a" }`, `test` gets `-z` whether or not you used double quotes. &gt; `cat&lt;&lt;-EOF` protects all leading space, which messes with the indentation level of the function. That's not true; allowing you to indent functions normally is the entire purpose of that construct. From `man bash`: "If the redirection operator is `&lt;&lt;-`, then all leading tab characters are stripped from input lines and the line containing delimiter. This allows here-documents within shell scripts to be indented in a natural fashion." Did you try your sed expression? It doesn't delete all of the leading whitespace and the period like you say it does. Like I said before... `s/^.//` deletes the first character of each line (a period matches any character). It doesn't delete all whitespace up to and including the period. From my previous reply, `s/^[[:space:]]*\.//` does that. The part about changing internal tabs to spaces is strange; why not just align with spaces to begin with?
you are right about the sed expression; i should have run it before posting, but I did give warning to that effect. I think I normally do format the body of the help text with spaces, but depending on the editor I'm using (which depends on the system) they seem to have their own ideas about reformatting indentation. Which I am fine with it replacing leading spaces with tabs 99 times out of 10; it's just for this one case it's the wrong thing to do and I don't want to waste time worrying about that. Like I said earlier, I prefer python's method; it is a lot more intelligent. I've tried different things over the years, and I think I've settled on just ensuring each line starts with 2 tabs and stripping those two off before converting tabs to spaces. That seems to be consistent and easy in all my editors. But then along comes posting to reddit and leading tabs are a pain, which is why I think I pulled this one out of the attic so it could be copy-pasted. And of course I screwed it up. I'll just fix that now for the redditors of the future. Thanks. *edit* oh nevermind; there's a lot more messed up with it. I'll just stick with a traditional HERE doc.
Isn't `find ... | xargs` a little baroque? Why not just use `find ... -exec grep whatever {} +` ? Also, I don't think it's too much of an imposition to just learn to use find correctly, given how useful it is.
(Not making any claims of this being the best way, only a way) How accurate does the 10 second interval need to be? 1 and 15 minutes can be taken care of with cron, but you're going to need something creative for 10 seconds, this could work: #!/bin/bash function mycommand() { # do some things } mycommand for i in {1..5} do sleep $(dc -e "10 $(date +%s.%N) 10 % - p") mycommand done That should, to within a small margin, run your code at 0, 10, 20, 30, 40 and 50 seconds, the wrapper script can then be scheduled once a minute with cron (or you could just loop indefinitely in an interactive shell). Running it manually on my system for a test of accuracy: $ for i in {0..5}; do sleep $(dc -e "10 $(date +%s.%N) 10 % - p"); date +'%Y-%m-%d %H:%M:%S.%N'; done 2014-01-19 09:02:40.002531850 2014-01-19 09:02:50.002567396 2014-01-19 09:03:00.002391099 2014-01-19 09:03:10.002410625 2014-01-19 09:03:20.002457420 2014-01-19 09:03:30.002502510 If 2.5ms isn't precise enough it might be time to look for a non-bash solution.
This sounds like a great solution. The exact accuracy is fine by +- 200ms, but I can't have it drifting off over time. This is perfect for my purposes. I will most likely just have a counter for this, additionally running the 1 and 15 minute scripts if a modulus check returns true. Thanks!
This prohibits drifting off.
slp, a /bin/sleep with more features: https://sites.google.com/site/dannychouinard/Home/unix-linux-trinkets/little-utilities/slp Can sleep until modulo times (seconds or thousandths), sleep to specific wall-clock times, and a few other specs. In this case, it would replace calls to sleep, date and dc in qupada42's example into a single sub-process (less resource use).
Not really important, my issue is fixed, but the `do` command also takes a tiny amount off time, causing the sleep to become warped/drifting off over time.
This works well enough for me, but thanks!
cron: */15 * * * * 15m.sh * * * * * 1m.sh * * * * * 10s.sh * * * * * sleep 10; 10s.sh * * * * * sleep 20; 10s.sh ... * * * * * sleep 50; 10s.sh
Why not use a heredoc instead of a magical `__END__`? You could save yourself having to prefix every line of your data block with "# ". You're missing quotes around most of your variables. It'll probably only be an issue for `$BASH_SOURCE` in case the path to your script contains whitespace or special characters. Your f_data function could be replaced with sed -n '/^# __END__/,$s/^# //p' "$BASH_SOURCE" | sed 1d or awk 'p{print substr($0,3)} /^# __END__/{p=1}' "$BASH_SOURCE"
Thanks for the suggestions. I will have to look into fsniper but I may not be allowed to install it on these types of systems (Change management approval required). I may need to do some voodoo to get it done with bash...
Theres also lsyncd
You should look into csync. csync is a bi-directional file synchronization tool. It will mirror two directories with each other, so that after a file has been parsed/processed, the new file will end up propagating staying in sync with both $ARCH_DIR and $SOURCE_DIR. This solves the issue of maintaining continuity even when a file gets transferred and modified by a 3rd party. Solutions like rsync are primarily used as uni-directional sync tools, meaning it's a backup. It's important to make sure your program's logic is done carefully with uni-directional tools since you might end up accidentally deleting your source copy or ending up with unmanageable file collisions. 
&gt; incron I'll second a vote for Incron. I use it to copy files on change. I am sure it would work well for new files.
Maybe I am not understanding. I don't have an issue keeping $SOURCE_DIR and $ARCH_DIR synced. The problem is transferring only the newly-created files to $DEST_DIR. $SOURCE_DIR and $DEST_DIR will never match and cannot be synced. 
It's a pain in the ass to install this (you have to install Haskell Platform if you don't have it, get the dependencies for shellcheck, and then compile shellcheck and make sure it's in your path correctly), but once it's done it make shell scripting so much easier. It really brings Sublime Text 3 one step closer to being an "IDE" for shell scripting.
Not exactly a script, but script examples, see the Bash FAQ @ http://mywiki.wooledge.org/BashFAQ It's pretty in depth sometimes, which has made me more able to make more informed choices.
Just got this setup on my MacBook and have run a few of my scripts through it. Very nice. Was a bit of a pain to install but nothing too crazy. I was using TextMate 2 before and all I really write is bash scripts so the jump to Sublime shouldn't be too much of an issue. Thanks for the link!
Yeah, not a problem! I'd also recommend the commercial SFTP package if you're going to be working on anything remotely (it's $20 for a license, or you can just use the unlimited trial), and also the File System Autocompletion package is quite nice.
Sounds like you could read the first line of the file and delete it continuously until there are no more lines? Is this what you want to do?
Use a while loop. while read url do # things with $url done &lt; todo.txt
I don't think I'm understanding what you are trying to do at all. Are you trying to build a recursive web scraper? If so, `wget` can actually do that for you. Check out its man page.
I am! I do know wget, I'm using it in my script as well. I didn't know about that function though. I'm trying several projects to learn more about bash. The web scraper is one of them. It's more about learning bash then to actually create something usefull
So you want to basically do this: Read file (list of URLS) do something for each of the URLS inside file read file again do something for each of the URLS inside file And just keep looping that infinitely?
Omit the "remove downloaded url" step and see how it goes.
This got me thinking into the right direction. It was a slight variation but I did make this mistake. I removed/altered that line and got the script working now! Thanks
How are you calling the script? Are you using ./my_script.sh or are you in another directory and using /path/to/my_script.sh? Are the files you're working with in the same directory as your script, or some place else? Assuming your script is run in bash, what happens when you run it in verbose mode (just run bash -x ./my_script.sh)?
It turned out to be something after the 7zip command that was hanging and not echoing any errors, but thanks anyway.
Always remember to put debug mode on as it can help easily identify stuff like this. "set -x" will enable it. 
so just put 'set -x' in my .bashrc?
I wouldn't want it on all the time, I put it at the top of my script and to disable debugging change the - to a +. 
cool, thanks, that'll come in handy.
I'm not a bash expert, but I think this is a good start : # find -mtime +7 | xargs mv [destination]
Fyi, when using find with xargs, you should use find's -print0 and xargs's -0 options to safely pass file names with spaces through intact. The alternative method to avoid wordsplitting issues with file names, is to use find's builtin -exec find . -mtime +7 -exec mv {} /new/path/here/ \; the {} represents the match and the escaped semi-colon closes the command. 
Both of the above recommendations are right on. They are easily the most straight forward ways to tackle the problem. I wanted to throw this technique out there because it will help you if your time frame is not as simple as days. For instance, you want to go back 3 hours, 25 minutes and 32 seconds. You can use the command: "date +%s" command that will give you the epoch time in seconds. You can count backwards from epoch time to the point that you want to affect something. You can also feed this command a timestamp if you need to convert it. This technique gives you a fully qualified time stamp essentially.
Note also that using `mv` will flatten your directory structure; something like find ... -print0 | cpio -pd0 /destination/ will rebuild a corresponding directory tree.
`mv` flattens the directory structure, and without `-type f` you move entire directories just because one file or directory inside was added or renamed.
As a note, there are two exec forms: `find ... -exec ... \;` invokes once for each file, whereas `find ... -exec ... +` will batch execute with as many files as possible.
Good point :)
Thanks, makes sense that it does quoting though :)
You can drop/remove (links to) crontab files in '/etc/cron.d'. 'cron' notices changes in that directory.
delrinian's recommendation for case is a good idea. i would also make a few changes here: function playagain() { read -n 1 -p "Play again y/n? " playagainanswer if [ $playagainanswer == [Yy] ] then start elif [ $playagainanswer == [Nn] ] -n 1 in read will automatically return after a single character, and [Nn] is basic regex to match on N or n
My general rule is if it requires more than 1 else then it should probably be a case.
 function start() you can do `function start` or `start()` (preferred), you can do both but it's unnecesary (this works different if you're writing POSIX but it's not the case here). if [ "$choice" == 'play' ] in bash is better to use `[[` instead of `[` or `test` since it's safer, easier and more powerful (also faster). also, you can use `=` instead of `==` which I believe it's more compatible (not a concern here but good to know anyway). you should indent every if clause and every function body, it's just aesthetics but it helps readability. cat highscores$board.txt quote eery single variable expansion, specially when dealing with paths and filenames. start I see you're recursively calling the same function as a way to keep showing the menu, I think it'll be best to create a loop on the outside and use the returncode to determine when to stop. read -p "Choose an upper value to guess between. " upper ? (( answer = RANDOM % $upper + 1 )) ah now I get it, you've got a global `$upper` and you change it there, then use it on `playgame()` ok. in general it's best to avoid using globals and shared state, that's something that applies to programming in general but it works anyway so, ok. now regarding the assignment, it is ok the way you wrote it but a more natural way to do it would be: answer=$((RANDOM % upper + 1 )) again, it's ok the way it is. while (( input != answer )) be careful with this as I don't see that you're resetting the variable before getting here, it works the first time because the var is unset but if you play again it may happen that you win automatically if the previous answer is the same as the current one (chances increase when you choose a small `$upper` value). guesses=$((guesses+1)) this is also ok and just a matter of taste but I like to do `let "guesses++"` or you can also do `((guesses++))` but nothing wrong with the way you choosed here. pretty good in general :)
I can't quite remember how % fits into the order of arithmetic operations. Is $(( $RANDOM % $upper + 1)) equivalent to $(( ($RANDOM % $upper) + 1 )), or to $(( $RANDOM % ($upper + 1) ))? As an addition to your excellent analysis above, I would consider writing this statement exactly as intended, and make the ordering explicit.
I *believe* it's equivalent to the first form but I should check to be sure, which proves your point :)
A few comments regarding the logic of your program. 1) in your playgame function, it might be better to replace while (( input != answer )) with a while true statement, and explicitly break from the loop on correct input. 2) you don't perform any input validation. When guessing, if I enter an 'a' instead of an integer, would I be told that my guess was higher or lower than the solution, or would I fall through to your else case? Wrapping your check in a statement like if [[ "$input" =~ ^[0-9]+$ ]]; then statements else echo "invalid input" fi would solve this problem, and eliminate that enigmatic else clause.
This works fine on Linux and OS X Bash. Suspect it may be something with cygwin.
It might be that your 'echo' command accepts options. The usual way to tell a command that the rest of the line aren't his options, although they look like that, is using '--'. Funny thing that the last few characters of 'password' is shown... Terminal settings, CR/LF funk, although not probable. Redirect the output of your script into a file and have a look for funny characters. Try these on the command line: # the line you expected: does it work without the variables? echo --user=root --password=password --host=127.0.0.1 # the same line but with '--' to stop 'echo' from fetching the options echo -- --user=root --password=password --host=127.0.0.1 # same thing, but with adding variables echo -- --user=${MYSQLUSER} echo -- --password=${MYSQLPASS} echo -- --user=${MYSQLUSER} --password=${MYSQLPASS} etc. If your script is going to run on a unix-like server, then please install virtualbox and the server OS and develop on that. Train as you fight.
This is because you've written this script with a windows editor and saved the file with "windows line-endings" (\r\n) instead of "unix line-endings" (\n). To bash, a CR (\r) is not special, it's a regular character that becomes part of the values of your variables. To the terminal \r means "move the cursor back to the start of the line". See http://mywiki.wooledge.org/BashFAQ/052 for more on that. Secondly. Your script is using bad practices. 1. Don't use uppercase variable names. In some languages (e.g. C, python, java, ...) it makes sense to use uppercase for "constants", but not in bash, because in bash the variables share the namespace with environment variables (unlike in C, python, java, ...). So to avoid accidentally overriding environment variables or special shell variables (which are also all uppercase), use lower_case or camelCase. 2. Don't put multiple arguments into a single string. Bash has arrays, so use them. &gt; #!/usr/bin/env bash mysqlopts=( --user=root --password=change_me --host=localhost ) mysql "${mysqlopts[@]}" dbname For learning bash, I recommend this (and only this) guide: http://mywiki.wooledge.org/BashGuide
Will give unnecessary error message if the user accidentally hits space, tab or enter... Quotes. They're bloody darn important. http://mywiki.wooledge.org/Quotes EDIT: and the [ command does not do pattern matching
Wouldn't mind hearing the explanation of a windows line ending is actually causing what OP sees? Assuming line 4 of posted code is all on one line, that'd be a hard explain...
Why bother? Alias is already that command.
I'd put it into it's own file `~/.bash_aliases` which I'd then source in `~/.bashrc` maybe adding a function named re_rc to do the following re_rc(){ source ${HOME}/.bashrc } Then add your alias, you can even name it `alias`: alias(){ echo "$@" &gt;&gt; ${HOME}/.bash_aliases re_rc } Another option is to use the `builtin` command (to improve above example): alias(){ local a for a; do builtin alias "$a" &amp;&amp; builtin alias "${a%%=*}" &gt;&gt; ~/.bashrc done } 
I'm a noob, but... wouldn't something along the lines of alias words="thedictionary" &gt;&gt; bash.rc work? *I have no pipe symbol on my phone sorry
That first alias function won't work...
No, because alias name="code" doesn't output anything, so there's no output to append to .bashrc
% has the same precedence as * and / (same as in C), so $(( RANDOM % upper + 1 )) is equivalent to $(( (RANDOM % upper) + 1 )).
Dude, awesome! I'll give that a try. I'll try adding a way to re-source the .bashrc as part of the function.
Why re-source .bashrc? it already sets the alias...
Great - thanks so much for the explanation, it was doing my head in trying to figure out what the problem was. As you suggested, converting to Unix format end of lines gave me the expected output. Thanks for the notes on good practice and the link as well. I'll start working through that as all of the servers I'll have to deal with in the near future are Linux based. 
I want to be able to use it right away. I thought Bash doesn't pick up changes unless you source it.
I didn't know this existed since it's an introductory class, but I will definitely start using this. Thanks! EDIT: Also, what is the purpose of the double semi-colon? just to ensure the end of that statement?
&gt; quote eery single variable expansion, specially when dealing with paths and filenames. What do you mean by this? &gt; in general it's best to avoid using globals and shared state Why is that? and what is shared state? &gt; be careful with this as I don't see that you're resetting the variable before getting here, it works the first time because the var is unset but if you play again it may happen that you win automatically if the previous answer is the same as the current one (chances increase when you choose a small $upper value). I didn't consider that, thanks for pointing that out!
From my understanding, entering a letter or letters, it converts it into a value. So unless that value is the target number it would just say it's wrong. Is this not the case? EDIT: I did this and if you type the name of the variable it crashes. Is there a way to prevent that? I tried giving it an elif statement for it but it still crashes saying "expression recursion level exceeded". 
The double semi-colon is part of the case syntax. Run help case
it doesn't really "wipe stuff out" it just moves the cursor to the beginning of the line. (you could say it's the next variable that "wipes stuff out" but this is a display-only concern, no data is lost.) If you run it through a suitable raw dump utility you'll see everything, including the &lt;CR&gt;s: beltorak @ corp-lappy [~/win-home/Documents/tmp] $ bash crs.sh --host=127.0.0.1ord beltorak @ corp-lappy [~/win-home/Documents/tmp] $ bash crs.sh | hexdump.exe -C 00000000 2d 2d 75 73 65 72 3d 72 6f 6f 74 0d 20 2d 2d 70 |--user=root. --p| 00000010 61 73 73 77 6f 72 64 3d 70 61 73 73 77 6f 72 64 |assword=password| 00000020 0d 20 2d 2d 68 6f 73 74 3d 31 32 37 2e 30 2e 30 |. --host=127.0.0| 00000030 2e 31 0d 0d 0d 0a |.1....| 00000036 beltorak @ corp-lappy [~/win-home/Documents/tmp] 
Well, you're letting the user write code, while your intention was to just read a number. The user can easily cheat; all the user needs to know is the variable his input gets compared with. Always sanitize user input.
How would they be able to cheat using letters if the letters just convert to a value? EDIT: Fgured it out. But it's only if they know the name of the variable. Correct?
Yes, but the name of the variable is out in the open, since the script must be readable for the user to run it. Make sure the input is a valid number before you compare it with the answer. It's an important lesson most bash scripters never seem to learn; don't treat data as code.
My edit above: EDIT: I did this and if you type the name of the variable it crashes. Is there a way to prevent that? I tried giving it an elif statement for it but it still crashes saying "expression recursion level exceeded".
/u/cannonicalForm showed you one way. See http://mywiki.wooledge.org/BashFAQ/054 for other ways.
Let me give this a whack # Usage # First positional parameter is the alias_command you want to alias # Any parameters afterwards will be mapped to that command and placed in your $HOME/.bashrc file # Example: # alias_rc "echo -e hello world" foo bar baz # # alias foo='echo -e hello world' # alias bar='echo -e hello world' # alias baz='echo -e hello world' # # Make sure to encapsulate your first positional parameter in quotation marks alias_rc() { # Check Positional Parameters if (($# &lt; 2)); then { printf "alias_rc requires at least 2 parameters\n"; printf "%s parameter[s] were given\n" "$#"; } &gt;&amp;2 return 1 fi local alias_command="$1"; shift local alias_names=($@) for name in ${alias_names[@]}; do printf "alias %s=\'%s\'\n" "${name}" "${alias_command}" &gt;&gt; "$HOME/.bashrc" done } 
ah right, I though that was OSX. I'll edit my comment thanks!
&gt; &gt; quote eery single variable expansion, specially when dealing with paths and filenames. &gt; &gt; What do you mean by this? I mean that things like: cat highscores$upper.txt should be written as: cat "highscores$upper.txt" or: cat highscores"$upper".txt the reason is that when the variable contains whitespace (spaces, tabs, newlines, etc.) it'll be split into several *separate* arguments by bash unless it's quoted, in that case bash treats it as a unit, including the spaces it may have. in your case it's supposed to be a number but it doesn't matter, to be safe you should quote everything and don't think about whether the contents of the var are problematic or not, think of it as a "good practice". there's some special situations where the previous rule doesn't apply but again, it won't hurt to quote even when it's not strictly necessary so quote everything and be safe. the point about filenames and paths is that those allow whitespace (even newlines!) and while it's pretty evil to name files like this, it's possible so your code should be ready to handle that situation. &gt; &gt; in general it's best to avoid using globals and shared state &gt; &gt; Why is that? and what is shared state? heh, grab some coffee... a global variable is one that exists and is accessible from any part of your program, the variables you're using (and probably the only ones you know about) are globals, notice that for instance the `$upper` variable is first created at the top of the script but then `playgame()` reads it and `start()` updates its value, this is shared state. ideally functions should be *self-contained* which means that they shouldn't access variables that exist outside their own code nor create new ones that would "leak" to the outside after they return, in bash all variables are global by default but inside a function you can use the `local` keyword to create variables that exist only there and not in the outside. that solves the last half of our predicament, which is to not "leak" data to the outside world, the other half is solved pretty easily, instead of reading global variables use *parameters* when calling the function. with this two things we can have a self-contained function, that is one that does not depend on the *state* of the external world to properly function, this has several advantages: **your functions become more reusable and more robust** the problem with the globals is that your function depends on the existing conditions of the environment to work properly, imagine you had a typo and instead of "upper=100" it was "uper=100" now calling `start()` would break, because it expects the variable `$upper` to exist and have a numeric value. of course if you're using parameters and don't pass anything it'll also break and in the same way as before, however when you debug and find that the problem is that `$upper` is missing you'll be able to trace the function call and know exactly at which point the call being made does not include the desired argument. if you have the same situation but with a global varaible instead, not only you have to scan the whole program for the definition of the variable (since it could be ANYWHERE) but you also have to trace the program execution from the start up to the point where the missing variable is required and this because even if the varaible would be correctly defined where you expect it, there could be another point in your program (again ANYWHERE) where it's being assigned a wrong value or deleted entirely, always using parameters helps you with this because it creates a traceable path for you to verify and locate the error. that's for robustness, now your parameter-based functions are also more reusable because *-when called appropiately-* they'll have everything they need to operate regardless of which part of the code or even which program they're being executed from! that's what self-contained means here, the function can operate as long as it's called appropiately, it does not depend on certain environmental conditions to work. **your code is more readable** when I was studying your script I got confused at some point because the `start()` function and the `playgame()` function communicate *implicitly* thru the `$upper` variable. the thing is, when you have shared state the code of a function is not just the body of it, in order to understand it you need to understand everything that is happening on the outside and this is a really big problem, imagine this same situation but in a large-scale system with hundreds of files and thousands of lines of code. on the other hand when you have a function that is self-contained you just need to focus on that to understant what it does, you won't know what it's used for or where but you'll be able to tell *what* it does because everything the function needs is right there so it's easier for you to follow and understand it and being able to break large problems into smaller pieces is key, that's how we humans can build projects and work on systems *so complex* that no single person would be able to comprehend entirely on its own, "divide and conquer" it works.
Oh I see. I really like this. It helps a lot. Now when I do a command that I never want to type it out again I can just do al !! and I'll never have to.
Links to? Symbolic links? It's not letting me drop anything there. Looks like PHP is running as www-data and would likely need root access.
Since php is running as www-data, it won't let me write to file using crontab -l &gt; cronfile. What is the best practice way of handling this? 
Yes, you have to have root access. You didn't say anything about not having it.
 
Reading user input is done with the read builtin, and to output stuff, use the printf builtin. $ help read | head -n 2 read: read [-ers] [-a array] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...] Read a line from the standard input and split it into fields. $ help printf | head -n 2 printf: printf [-v var] format [arguments] Formats and prints ARGUMENTS under control of the FORMAT. Example read -rp "How many? " number printf 'You answered %s\n' "$number" To pick random files from a directory, see [How can I randomize (shuffle) the order of lines in a file? (Or select a random line from a file, or select a random file from a directory.)](http://mywiki.wooledge.org/BashFAQ/026) ([mirrored copy](http://bash.cumulonim.biz/BashFAQ%282f%29026.html)) For learning bash in general, read http://mywiki.wooledge.org/BashGuide ([mirrored copy](http://bash.cumulonim.biz/BashGuide.html)) (also available as [pdf](http://folk.ntnu.no/geirha/bashguide.pdf)) EDIT: wooledge.org appears to be down at the moment, so added mirrors
Can anyone explain how the arguments get passed to this function? I thought positional parameters were required.
 $ help for for: for NAME [in WORDS ... ] ; do COMMANDS; done Execute commands for each member in a list. The `for' loop executes a sequence of commands for each member in a list of items. If `in WORDS ...;' is not present, then `in "$@"' is assumed. For each element in WORDS, NAME is set to that element, and the COMMANDS are executed. Exit Status: Returns the status of the last command executed. 
I'm on my mobile right now, so I may be a bit off. If I remember correctly, you need to surround your escape sequences with \\[ and \\]. I had this exact same thing happen to me when I was setting up mine. Edit: Now that I'm on my computer, you're probably looking at something like this: `export PS1='\u@[\[\e[${COLOR}m\]]${COMP_NAME} [\[\e[m\]]\w$(__git_ps1 "(%s)")\$ '`
indeed, [BashFAQ/053](http://mywiki.wooledge.org/BashFAQ/053).
ooh, very interesting! Thanks for this dude...
thanks
`for i in $(command)` iterates through the output of `command`, splitting on whitespace. Try using grep 1041 "$passfile" | while read i; do instead.
You can save yourself a ton of processes from all of that cut and echo work by using `read -a` to read a line into an array: IFS=: # read splits words on $IFS grep 1041 /etc/passwd | while read -a fields; do account="${fields[0]}" name="${fields[4]}" userid="${fields[2]}" lastdate="$(last "$account" | head -n1 | cut -c 40-55)" printf "Full name: %s\tUser Account: %s\tUser ID: %s\tLast time logged in: %s\n" "$name" "$account" "$userid" "$lastdate" done IFS=
This worked perfectly, thank you for your feedback! I had some sort of idea that for would split up whitespace due to my results but I had no idea how correct that. Thanks again =)
awesome, I was wondering if there was a way to direct wget to stdout. Thanks.
Can you post the code you used to generate `$CY` so that we have an easier time recreating your case?
Hooray for pipes. =)
Doesn't quite work for me because `CY[09]` gets interpreted as an invalid octal number. Is there a reason you absolutely need the leading zero? If not, the following should work: declare -a CY let CY[1]=CY[3]=CY[5]=CY[7]=CY[8]=CY[10]=CY[12]=31 let CY[4]=CY[6]=CY[9]=CY[11]=30 CY[2]=28
Thanks for that! It'll definitely make declaring the values more efficient. And yeah, my purpose in posting this was to find out how it sorts them the way it does, why, and how to change it to actual numerical sorting (with leading zeroes included) if possible... e: i apologize - i completely disregarded your first statement. You're right, this doesn't work. So i'm using what /u/qupada42 proposed: &gt;`CY=('' 31 28 31 30 31 30 31 31 30 31 30 31)` &gt;`unset CY[0]` i'll post an update edit to my initial post to show where i'm at... thanks for all the help so far
This would be the most efficient, but it does leave you with a 0-indexed array (à la javascript): CY=(31 28 31 30 31 30 31 31 30 31 30 31) $ echo ${!CY[@]} 0 1 2 3 4 5 6 7 8 9 10 11 $ echo ${CY[@]} 31 28 31 30 31 30 31 31 30 31 30 31 You can work around this simply enough: echo ${CY[m-1]} # where m = 1-12 value If you really need it to be 1-indexed, perhaps CY=('' 31 28 31 30 31 30 31 31 30 31 30 31) unset CY[0] # or don't, if the presence of the 0th element isn't an issue. When you say "associative array", did you use "declare -a" or "declare -A"? The latter should let you use 0-prefixed months as in your example since -A uses string keys not integer, but you probably need ${CY["08"]} and not ${CY[08]} to reference them, as /u/Molozonide says 08 and 09 aren't valid octal.
this is perfect, thanks a ton! i used: `declare -a CY` didn't know there was a difference between the `-a` switch and the `-A` one. what's "valid octal"? I'm going through the sidebars Advanced Guide to Bash but i haven't hit that yet.
Numeric input in the form 0[0-7]+ is treated as base 8, so "07" = 7, "010" = 8, etc. There are no digits 8 and 9 in octal, so "08" and "09" are invalid values). (this has long been an issue when parsing dates, as you might expect). Similarly, hexadecimal literals take the form 0x[0-9a-f]+. For example: Octal: $ echo $((07)) 7 $ echo $((08)) bash: 08: value too great for base (error token is "08") $ echo $((010)) 8 Hex: $ echo $((0x9)) 9 $ echo $((0xf)) 15 $ echo $((0x10)) 16 
fascinating... seriously thanks for the quick lesson. excuse me while i wander down this rabbit hole...
 $ echo $((2#111)) $((3#111)) $((4#111)) 7 13 21
 [BASE]#[NUMBER] $ echo $((2#000)) $((2#001)) $((2#010)) $((2#011)) $((2#100)) 0 1 2 3 4
this is really cool. quick follow up... is there an easy way to find out when to use the different "limiters" (probably not the right word)? ${} vs [[ ]] vs $(( )) etc...
i'm trying not to use any calendar specific library or program... sed/ed/awk are ok, but i can't use cal :( i just did added an edit with an update of where im at. hopefully it makes sense... thanks for trying to help me
sweet, thanks dude. you are the man. And this isn't for a professor, just for my own personal knowledge... i got this premise from a friend and i wanted to see if i could do it. it's proven challenging.
no content
Sigh "wipe stuff out" I am smart, when I put my brain into gear... I should have looked at the output more closely. Thanks for the explain though :)
keep reading...
It's hard to follow along or to provide meaningful commentary when the output is often impossible given the command. For example, after this command: tidy &lt;first-fetch 2&gt;/dev/null | grep label | sed 's/^.*\/label&gt;//' | sed 's/\\nt//g' | head -n 3 not only does he end up with more than three lines, but several lines don't contain the string "label", even though he used `grep label` and those sed expressions don't add newlines: &lt;div&gt;&lt;label&gt;&lt;label&gt;Exchange &lt;/label&gt;&lt;/label&gt; &lt;div&gt;&lt;label&gt;SMTP 1921 2250 2375 2500 MB&lt;/label&gt;&lt;/div&gt; &lt;label&gt;&lt;label&gt; &lt;img alt="" src="&lt;br" /&gt;&lt;img alt="" src="&lt;br" /&gt;... He says "Removing the trailing div and MB's we finally have sanitised data:", but there's no difference between the "before" command and the "after" command, but the command output changes to match his comment.
Hi /u/r3j, I appreciate the feedback! Still new to writing posts, so I'm aware I made some mistakes. You're correct that the "head -n 3" does not make sense, and I've removed it. &gt; not only does he end up with more than three lines, but several lines don't contain the string "label", even though he used grep label and those sed expressions don't add newlines: The reason several lines to not contain 'label' after a 'grep label' is because the sed expression removes everything up to the string '/label&gt;' &gt; He says "Removing the trailing div and MB's we finally have sanitised data:", but there's no difference between the "before" command and the "after" command, but the command output changes to match his comment Oops! let me fix that, simple copy and paste error I assure you.
Thanks!
There is an off-by-one error in your code. num=${#values[@]} gives you the number of elements, you iterate until i &lt; num but access the array at j=i+1 which is out of bounds during the last iteration and the variable expands to an empty string giving you the error you see (`test` complains about a missing operand because of the expansion to an empty string, e.g. `[ -le 5]`, like /u/cooper12 pointed out). EDIT: Also, `j=i+1` should be a `let j=i+1` (or `((j=i+1))`, or ...), otherwise you are assigning the literal string `i+1` to `j`. It still "works" because the expansion takes place when you use `$j` as an array index, but it wouldn't work in other places (do an `echo "${j}"` inside your loop and you will see) so that typo is a sneaky bug :-)
Good point! I mean, when using `[]` it is necessary because it doesn't seem to be able to handle null or empty strings well, but otherwise yes, very valid point.
Well now that I am completely awake, number comparison with `[]` does not just only dislike missing/empty operands but expects integers, so just quoting doesn't even help in this case: $ i=1; j=2; [ "$i" -le "$j" ] &amp;amp;&amp;amp; echo true true $ i=; j=2; [ "$i" -le "$j" ] &amp;amp;&amp;amp; echo true -bash: [: : integer expression expected I would not consider this to be expected behaviour: $ i=; j=2; [[ $i -le $j ]] &amp;amp;&amp;amp; echo true true I think that's stupid, that's why I usually stick to single bracket notation unless I need a specific feature, e.g. wildcard expansion or regular expressions in string comparisons.
Awesome! I knew it had to do with that 'j=i+1' line, but I didn't follow it all the way to its conclusion. 
Added a conditional to catch that out of bounds condition. Works like a charm. Thanks again for the help!!! Added the quotes as well for good measure ;)
&gt; The reason several lines to not contain 'label' after a 'grep label' is because the sed expression removes everything up to the string '/label&gt;' That makes sense. I wasn't sure if the more than three lines was due to you accidentally including "head -n 3" in the command, or if it really was three lines but lines were wrapping or if some of the HTML tags in the output weren't escaped and were interfering with the output. &gt; Oops! let me fix that, simple copy and paste error I assure you. I wasn't accusing you of bad faith or anything, just that it was hard to follow. It's hard to tell without more sample input, but it might have been easier to start with: tidy &lt; fetched 2&gt;/dev/null | tr -d '\n' | grep -o '&lt;label&gt;[^&lt;]*&lt;/label&gt;[^&lt;]*&lt;/div&gt;'
Thanks, that does look a lot more clever! I'm going to add an addendum of improvements I've learned from the feedback, and will be sure to name you there.
It's not really a one-liner, right, with all those semicolons? It would be easier to read with more lines. Second general note: don't use backticks. Use `$(...)` instead. Now the first field of `date` (that you extract with awk) is the short day name, right? So to save one process, you can do day=$(date +%a) See `man date` for more date formatting stuff. I think it's kind of questionable to grep for "permission denied" from your logfile, then echo the entire grep output into the awk things. I'd prefer to do it with a single read loop. Another thing is that cut and awk and friends are often overkill when you're working on a single string. [There are good parameter expansions that can save you a bunch of processes](http://mywiki.wooledge.org/BashFAQ/073). Maybe (untested) grep "permission denied" /var/lib/...log | while read line; do IFS=, read -a fields &lt;&lt;&lt;"$line" # split line into fields with , as separator user="${fields[1]#*=}" db="${fields[2]#*=}" db="${db%%:*}" db="${db//ERROR/}" read -a fields &lt;&lt;&lt;"$line" # split on whitespace this time table="${fields[7]}" psql ... done
For some reason I was having difficulty with $() so I opted to go with `` instead. I really like the 'while read line,' that's how I handle loops in Python. I guess for bash I've always learned stuff piecemeal and just 'made it work.' I'm going to look at how you did this and play around a bit, but I still like the ability to run it on a single line in one command which is why I did some of the things the way I did. :)
Ability to copy/paste it quickly between multiple servers, ability to put it in a bashrc which you slurp into a server when you ssh into it... there are a lot of things going for one-liners.
Personally, I'm a fan of this method over scp'ing a script for a single execution: ssh &lt;host&gt; "bash -s" &lt; script.sh I actually tend to use ssh/bash -s with heredocs fairly often as well, it's handy.
Thanks /u/badsuperblock helps in a way but seems to print big blocks of data rather than one password at a time, ill give your link a read when i head home.
It's a UUOC! while read line; do ... done &lt;passwords.txt
It would be helpful if you included sample input in the description; it's hard to tell what it looks like based on your code alone.
It sounds like you want to loop through a file of passwords feeding them to the _encfs_ command as you go, and then ... I'm not sure where you're going with _df_ and _grep_'ing for a deep folder. Shouldn't each filesystem have its own password? It seems like a file with this format would be better: [password] [encrypted dir] [mount point] Or do you have a list of passwords and a list of filesystems but you don't know how they match up? You should describe and copy/paste a single iteration of the process you're going through; that will tell us what you're trying to automate exactly.
Or '`tr -d \\n`' (unless you were banking on IFS whitespace trimming and `read`'s ridiculous backslash interpretation rules), but that doesn't look anything like what OP asked for, not to mention that your while loop isn't modifying the same `passwords` variable because it's in a pipe...
2014-02-13 14:46:17 GMT,user=db_user,db=db_testingERROR: permission denied for sequence name_of_seq 2014-02-13 14:50:16 GMT,user=db_user,db=db_testingERROR: permission denied for sequence name_of_seq 2014-02-13 14:52:57 GMT,user=db_user,db=db_testingERROR: permission denied for sequence name_of_seq 2014-02-13 15:38:09 GMT,user=db_user,db=db_testingERROR: permission denied for sequence name_of_seq This would be 4 different users/databases/sequences though.
Sorry i was a bit off on explaining 'df | awk '{print $1}' | grep (mount point)' is better Im running this on android and when the command is ran itll show a full path if it exists(meaning password is correct) and empty if it doesnt, thus if it exists then the password used was successful in mounting the encfs filesystem and that is the password i need to print to the screen. I have a ton of possible passwords im running through and only have gotten 100 and some in. This script would make it really easy. Just that im new to scripting in bash
I believe this gives the correct result: awk '/permission denied/ {print $3 $8}' sample-$(date +%a).log | sed 's/\w\+=//g;s/ERROR:/,/' | while IFS=, read _ user db seq; do echo "psql -Upostgres -d$db -c\"GRANT SELECT ON $seq TO $user\"" done Output: psql -Upostgres -ddb_testing -c"GRANT SELECT ON name_of_seq TO db_user" psql -Upostgres -ddb_testing -c"GRANT SELECT ON name_of_seq TO db_user" psql -Upostgres -ddb_testing -c"GRANT SELECT ON name_of_seq TO db_user" psql -Upostgres -ddb_testing -c"GRANT SELECT ON name_of_seq TO db_user" Note that I just have it _echo_'ing the commands so you'll need to remove the _echo_, outer quotes and backslashes to have it run the _psql_ lines directly. It also works fine when collapsed on to a single line.
I would think that _encfs_ properly sets its exit code to indicate success/failure, so you shouldn't need to worry about what it outputs to stdout. I believe this should work: while read -r pass; do if encfs -S /locked/path /mount/point &lt;&lt;&lt;"$pass"; then echo "$pass" break fi done &lt;passwords.txt
really neat! i'm a big fan of awk/sed in combination. could you break it down at all? I'm really interested in the "IFS=," portion.
 sample-$(date +%a).log _awk_ can read files directly, so there's no sense in adding using _cat_, a _while_ loop, etc. awk '/permission denied/ {print $3 $8}' When the string _permission denied_ is found, print the 3rd and 8th fields that are delineated by whitespace, so: 2014-02-13 14:46:17 GMT,user=db_user,db=db_testingERROR: permission denied for sequence name_of_seq becomes: GMT,user=db_user,db=db_testingERROR:name_of_seq and is sent to the next step of the pipeline. sed 's/\w\+=//g;s/ERROR:/,/' The first search/replace (There's two - note the semicolon separating them) removes the user=, db=, etc. so the line becomes: GMT,db_user,db_testingERROR:name_of_seq Since the fields you want are pretty much already neatly separated by commas which is easy to parse, the _ERROR:_ string just needs to be replaced by a comma which is what the second replacement does: GMT,db_user,db_testing,name_of_seq There's a vestigial _GMT_ hanging on to the beginning of the line which you could replace via another _s///_ command in the _sed_ line, but it's easy enough to just ignore at the next step: while IFS=, read _ user db seq; do ...; done The _while_ command is structured like this: while [shell code that receives line on stdin and exits with a value of 0/true] So for each line that it receives from the pipeline it's running this code: IFS=, read _ user db seq The _read_ command uses the _IFS_ environment variable (IFS standing for [Internal Field Separator](http://en.wikipedia.org/wiki/Internal_field_separator)) to determine which characters to use to split on. (If you're familiar with _awk_ it's the equivalent of its _-F_ argument) By default _IFS_ is set to whitespace, so I instead set it to comma so _read_ can parse the data in to the specified variables. Note that I ignore the vestigial _GMT_ by putting it into the underscore variable which is commonly used for throwaway values. After that it's just a matter of using the variables in the body of the _while_ loop.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Internal field separator**](http://en.wikipedia.org/wiki/Internal%20field%20separator): [](#sfw) --- &gt; &gt;For many command line interpreters (“shell”) of **[Unix](http://en.wikipedia.org/wiki/Unix)** [operating systems](http://en.wikipedia.org/wiki/Operating_system), the **internal field separator** (abbreviated **IFS**) refers to a variable which defines the [character](http://en.wikipedia.org/wiki/Character_(computing\)) or characters used to separate a pattern into tokens for some operations. &gt;IFS typically includes the [space](http://en.wikipedia.org/wiki/Space_character), [tab](http://en.wikipedia.org/wiki/Tab_character), and the [newline](http://en.wikipedia.org/wiki/Newline). &gt;From the bash man page: &gt; --- ^Interesting: [^Space ^\(punctuation)](http://en.wikipedia.org/wiki/Space_\(punctuation\)) ^| [^Shebang ^\(Unix)](http://en.wikipedia.org/wiki/Shebang_\(Unix\)) ^| [^Lithium-ion ^battery](http://en.wikipedia.org/wiki/Lithium-ion_battery) *^\/u/iscfrc ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cffds67) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cffds67)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less.* ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 57074:%0Ahttp://www.reddit.com/r/bash/comments/1xta3f/improving_my_nasty_oneliner/cffdrob)
In your function, it's a bit silly to do `cwd=$(pwd)` since the current working directory is already in a variable, namely `PWD`, so cds() { if (( $# != 2 )); then printf &gt;&amp;2 'Usage: cds from to\n' return 1 fi cd "${PWD/"$1"/$2}"; } Also, if you do need to set variables inside a function, keep in mind that they are global by default, so declare them local unless you really want to modify global variables. As for the programmable completion, here's my first attempt: _cds_completion() { local p d if [[ $3 = "$1" ]]; then while IFS= read -rd/ p; do [[ $p = "$2"* ]] &amp;&amp; COMPREPLY+=("$p") done &lt;&lt;&lt; "${PWD#/}/" else p=$PWD while [[ $p &amp;&amp; $p != */"$3" ]]; do p=${p%/*} done for d in "${p%/*}/$2"*; do [[ -d ${PWD/$3/${d##*/}} ]] &amp;&amp; COMPREPLY+=("${d##*/}") done fi } complete -F _cds_completion cds 
Can you give me a break down of what that does?
honest question; why not use git? (or any number of other version control tools)
you are asking how to switch between a "pristine" and (by implication) "not pristine" copies of the same deeply nested folder structure. it looks like you are switching between different branches of a versioned history. I'm curious as to what you are actually doing using version control is the wrong tool for the job.
I'm using perforce, for what it's worth. I don't have a choice in that matter. I'm still not sure how git would help me here though. What I was doing today that necessitated this was that I had a working version of a project and a pristine version. The project was a web server, and I wanted to be able to jump between my copy and a pure copy so that I could run the server, load a page, then compare it to how it worked originally. Jumping between branches is easy, but this is part of a large monolithic repo, so I have to navigate to my specific project each time I jump from one project to another. I'd prefer to have a way to just stay in the current sub dir relative to the branch when jumping between branches. 
If you only have one type of error (or four), it would be much clearer to use a specific regex than to hack it up by cutting on commas and equals signs and using arbitrary field indices. sed -n 's/[[:digit:]: -]* GMT,user=\([^,]*\),db=\(.*\)ERROR: permission denied for sequence \(.*\)/psql -Upostgres -d\2 -c"GRANT SELECT ON \3 TO \1"/p'
Maybe you would like something like [this](https://github.com/resultsreturned/to): it's quite neat because you can bookmark all the directories you would like to access easily, and all you'd have to do is just `to _directory_name` upon bookmarking it. 
You "echo it out"? if so, that's likely what's wrong. echo $(some|pipe|line) runs `some|pipe|line`, `$()` captures the output and replaces itself with the output. The output is then word split, that is all whitespace is removed, and the resulting words then become the arguments to `echo`, which prints each word separated by a single space. some|pipe|line runs the pipe line without capturing or modifying the output. See http://mywiki.wooledge.org/Arguments for more about word splitting. It's the most important topic in bash scripting, and what most scripters fail at. That said, you could further improve that pipeline by reading `ps`'s manual page. With the right options, you can make it output just the columns you need, so you won't have to cut afterwards.
Try this. In ~/.inputrc, add: "\e[5~": history-search-backward "\e[6~": history-search-forward "\e[5A": history-search-backward "\e[5B": history-search-forward The reload with bind -f ~/.inputrc It's different from what you asked, but say know what you want to run again. Instead of a list, 'vi &lt;Pg-Up&gt;' cycles through the history for that command, printing it as though you typed it. Enter will execute. 
The top answers [here](http://unix.stackexchange.com/questions/1288/preserve-bash-history-in-multiple-terminal-windows) all suggest the following: export PROMPT_COMMAND="history -a; history -c; history -r; $PROMPT_COMMAND" `reverse-i-search` and event designators like `!!` use the same history list, so it's not possible to have one include other sessions' histories and the other not. However, since history will not synchronize until the next prompt, you will still be able to use both `reverse-i-search` and event designators without other sessions' histories for the current command. Note: `export HISTCONTROL=ignoredups:erasedups` only removes consecutive duplicates, and using the `PROMPT_COMMAND` above will clobber your history with duplicates under certain circumstances. I add [proper deduplication](https://github.com/nkouevda/dotfiles/blob/51d2ec1e3d1cb4a3bba878a9b45fd257cafd5d25/.bashrc#L31-L51) in order to fix this.
I never noticed that it only fixed consecutive duplicates. What do you do in regards to timestamps? Also, I used `tac` instead of `reverse`, since that doesn't seem to be installed by default. My history file looks like: #1392604993 fortune -o zippy -s -a edit: I'm re-evaluating how useful it is to have command timestamps. I'm sure I could make a complex solution to cope with them, but...
Thanks, I've added this to my bashrc (switching `reverse` out for `rev` since that's what it is on my system). Is it possible to instead add the history from all sessions to some other file, and then search that instead? Ideally I'd like to be able to search both through a single session history and 'global' history.
I alias `reverse` [depending on the system](https://github.com/nkouevda/dotfiles/blob/51d2ec1e3d1cb4a3bba878a9b45fd257cafd5d25/.bashrc#L22-L29): Linux has `tac` but doesn't support `tail -r`, while OS X has `tail -r` but not `tac`. If you only need to work with one system, then that's of course unnecessary.
Touché, I should probably look at your `.bashrc` more closely for other tips n' tricks.
I thought about doing a player selection during the setup procedure, I'll be sure to implement that as it's a big configuration change lacking and one that's fairly easy to implement. As for the config info, I made effort to use the script's directory for storing the files, I don't like the idea of the script "making itself at home" at directories of its choice. I might consider reworking it to not read 'saved' from series dir, but from a common directory, per your suggestion, although that would be quite a big refacturing. It does seem a lot cleaner though. Although unless I add a menu for navigating episodes (which seems a bit overkill), having the saved file in the folder is really convenient for writing directly, with echo 1 &gt; saved. Thanks for the input, nice ideas being put out there :)
Sorry, I meant that both event designators and things like `reverse-i-search` will use the history list and not a file.
Include some sample lines from latest.log, or upload it somewhere. You should consider using another language for this; the danger that you make a small mistake and allow users to run arbitrary commands on your server is pretty high with shell scripts.
Just curious, why not use the backticks? Is it a general advice? It's what I use currently in my scripts. var=`echo "$var2"`
http://mywiki.wooledge.org/BashFAQ/082
Thanks! In short, backticks are a relic from a non-POSIX compliant past. I vow to change my ways (and scripts) accordingly.
Holy god in heaven there's a lot of work to be done to clean this up. Just a random nit to pick, why `kill -SIGINT $$` instead of `exit 1`? Quick re-write of `populateList`: function populateList { # Add new formats here. formats='mkv\|mpe?g\|avi\|ts\|mp4' find . -iregex ".*\.\($formats\)" | grep -vi sample | sort }
I don't know if this does exactly what you want, but I had some fun re-writing it. There were several things about your initial script that are 'bad'; first and foremost; reading the script into `~/.bashrc` so it can be called as an alias is just ... REALLY bad. What happens if you run `./series.sh --install` twice? Add something like the following to your `~/.bashrc`, create `~/bin` and put your scripts there. for bin in tools bin; do if [ -d $HOME/$bin ]; then PATH=$HOME/$bin:$PATH fi done I apologize if I seem like a dick or something. I'm not trying to be, you have a bit to learn about scripting, and I'd be happy to give you pointers, and explain what my re-write does, if you're interested. Anyhow, here's my over-haul, which may or may not work as your version does, I had a very hard time following your flow of function calls and determining exactly how you wanted it to work: https://github.com/cpbills/series/blob/master/series.sh
First off, some nitpicks: Use four spaces before a sentence so reddit recognized it as code and doesnt jumble it up. Backticks (\`\`) are deprecated; enclosing commands in `$()` is preferred as it makes nested commands easier to read. Be consistent. You quoted the string in `grep "still"`, /home isn't quoted in `grep /home`. The echo part might be a bit more readable if you wrote it like this (but that's just my opinion. Others might view my way as more repetitive): echo "There are currently ${users} users logged at ${date}" echo echo "They are:" echo echo "Login Name Login Time" echo "${loginname} ${fullname} ${logintime}" --- Now for the answer. It seems you need to get the login time for the users. The key for that is in the `last` command. You can use `cat /etc/passwd` like you did before to get the uid of a user. Then you can use `last $UID`, where $UID is each users specific userid to get a specific users login time. Let's look at a typical /etc/passwd line so you can understand better: `root:*:0:0:System Administrator:/var/root:/bin/sh` The first field is the loginname, as you discerned, and the fifth is the full username. The 3rd number is the UID (user id) and the 4th is the GID (group id). The way I would go about doing this is using a `for` loop, iterating over each user. Like so: echo "Login Name Login Time" loginname=`cat /etc/passwd | grep /home | cut -d: -f1` for user in "$loginname" ; do #grep the fullname #grep the UID #use UID to get login time #print everythig for the specific user done
&gt; This function depends on /proc being supported (most OSes with Mac OS X notably missing). No, it depends on a /proc that exposes processes' environment, which is only linux as far as I know. Anyway, apart from that the code also contains some bugs, e.g. with this line: local VAR=$(tr "\0" "\n" &lt; /proc/$1/environ | grep "$2=") if `$2` is `TMUX` as in the example, it won't only show the value of the TMUX environment variable. It will also match any part of any environment variable that happens to contain the substring `TMUX=`. For an exact match, I'd use bash instead of grep. local key value while IFS='=' read -rd '' key value; do if [[ $key = "$2" ]]; then printf '%s\n' "$value" break fi done &lt; "/proc/$1/environ"
He put "homework" in the title and said "ANY help would be much appreciated". What's the problem? &gt; to the first sentence of your script's output. What does that even mean? What's the first sentence of the output of `ls`? The post you link to has `users="$(who | wc -l)"`, which counts the same user multiple times if they're logged in more than once. OP already has a better version of it that uses `sort | uniq` to count users correctly.
You should read the book you were assigned in class, and hopefully its a helpful one. Those are usually really helpful for beginners. Also, don't expect yourself to become an expert overnight. These things take time. Just keep at it. A very helpful resource for me was http://tldp.org/LDP/Bash-Beginners-Guide. A hepful tip is to start off with pseudocode. Think of how you'd approach the problem first, write a blueprint for it, then write it, instead of jumping right in. About the loop, the reason we're using it is because we need to perform the action for each member of the list of current users. One way that might work is to grep the UIDs and save them to a file. Then use the '-f' flag of grep to use each line of that file as a search term for the output of `last` that you saved in the variable earlier. Also use the '-w' flag to get exact matches. Read `man grep` for more details. Edit: Thanks for the gold!
Well, you need to run last for each UID. So `last 0` on OSX gave me: wtmp begins Mon Feb 11 16:58 You could use either | cut -d ' ' -f '5' or | awk '{ print $5 }' Getting the login time from "still logged in" might actually be a better idea like you said since the order of users might match the order from /etc/passwd. Then you don't need any of what I said before, if that's true.
Which user name are you trying to pull from /etc/passwd? `grep /home` might return many lines; how do you know which ones you want? Why don't you add comments to your script indicating what you think the lines are doing, and how they are supposed to output the information you want. Something like: # for each login entry produced by who... who | while read user x; do # print the user name echo "$user" # find them in /etc/passwd, but I don't know how to extract # their full name grep "$user" /etc/passwd done There are many ways to get the information out of "who", but you should mention which topics you've covered (i.e. what commands you're allowed to use).
I think your post is legitimate, and I was defending you. Maybe you misread my post.
see my other comment. You could cut the login time of the users logged in, but the only way you could use this is if those results match order of the usernames you got from /etc/passwd Sounds like you're juggling a lot. Good luck.