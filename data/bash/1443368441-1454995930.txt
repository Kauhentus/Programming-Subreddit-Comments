OK, maybe not good, but better than this :) Not to defend that site, but a few points. The unquoted args is probably an oversight, since right above it, it is quoted. For the FAQ#4 post, I actually don't like ${#f[@]} because for newbies, it's better / more important to understand pipes and different commands than specialized bash tricks etc. I'd say average isn't garbage, it's average. Average's not bad at all. It's better than bad. Like at work, if I'm asked for specific advice on 'average' code, i'll try and advise it to be better -- but if not asked, I'll just accept it. Bad code, however, is unforgivable.
Note that the first one also forks a process which is quite expensive (though you wouldn't use a Bash script where performance is critical).
They both do ...
Yeah. That's not really an option. oh well. 
I'm actually calling `svn co` a half-dozen times to clone a bunch of repositories and I'm considering throwing this into a library to re-use anywhere we have to checkout an `svn` repo, so the auth string is pre-populated and can be used in each call. the script that I'm modifying (that I didn't write) actually prompts the user each time it clones and each clone operation takes between 1 minute and 20 minutes, so you can't just walk away from the machine, which is why I'm adding the explicit authentication options to the `svn co` calls. the `echo` statement that you suggested doesn't bring anything to the table since that's at the point where I'm using it and calling `$AUTH_OPTS` bare like I have it is fine. My questions is whether there's a better way of doing the assignment since `\` escaping the spaces feels cumbersome. I feel in my gut that there's got to be a better way.
When it's an option, I have adopted an [xkcd-inspired](https://xkcd.com/936/) approach to passwords. I don't regret it and don't expect to.
Apparently so is mv, because i had to do some magic to get everything to work lol. My home laptop is OS X, so it doesnt play well with all linux syntax haha
I have the opposite problem: I write plenty of small one-off shell scripts to do useful things, but trying to read someone else's scripts is a nightmare. Did you know `which` is a shell script? It's indented by a single space and has no comments. https://github.com/sbc100/naclports/blob/master/ports/devenv/which I can tell that it's looping through the directories in `$PATH` looking for executables, but don't ask me to debug it.
Yeah... I imagine OS X/Unix mv is a bit different than linux mv.
It's dirty in that there's a lot of calling external programs to do stuff instead of using builtins to do string replacement, etc on the code. It would've been easier if it was ```show.season.episode.*.filetype```, but alas. With that, I could've just set IFS and then split the filename into an array. Then idx 0 = show, idx 1 = season, idx 2 = episode, and idx[@] = filetype. :/ edit: http://i.imgur.com/M5wl14r.png
Fun fact. On Solaris, `which` is a csh script that reads in and parses aliases from your `~/.cshrc`. It also has different exit status and output from the `which` you typically find on linux (so any scripts that use `which`, and work on linux, will fail on solaris and vice versa). `which` is really useless and redundant. `type` is the standard and portable way to test if a command exists.
[Never use `ls` to iterate files](http://mywiki.wooledge.org/ParsingLs), and use `$PWD` instead of `$(pwd)`. And when referencing files in the current directory, it's a good idea to prepend `./` to them in case any of them happen to start with `-` (which would cause many commands to treat the filename as options instead). #!/bin/bash for file in ./*.flac; do opusenc --bitrate 128 "$file" "${file%.flac}.opus" done dir=$PWD-opus mkdir -p "$dir" || exit cp ./*.png ./*.jpg "$dir" mv ./*.opus "$dir"
Before you've written anything you have infinite possibilities of where you can go and doing anything will immediately limit where you can do next. :P I feel that every time I start a new project (bash or otherwise) and it's an awful paralysis! The thing that's helped me most is to *just start somewhere* and not to focus on getting it right. Think of your first version as a sketch for a better version that you'll ultimately make later after you scrap the first one.
The use of a pipe via cut to strip the extension isn't a "bad" idea, but there's a "better" solution IMHO: #!/usr/bin/env bash # We're gonna use this more than once, so let's store it dest="$(pwd)-opus" # Make sure destination exists, if not, create it :) if [ ! -d "${dest}" ]; then mkdir "${dest}" fi # Show me all the "flac" files. When "ls" is piped, it defaults to the single # column output, so no need to explicitly declare it. ls *.flac | while read line do # ${var%%foo} means strip "foo" from the end of ${var} # Let's use that to replace the extension - oh, and output to the ${dest} dir. opusenc --bitrate 128 "${line}" "${dest}/${line%%.flac}.opus" done cp *.png "${dest}" cp *.jpg "${dest}" You could do more with checking return values (ie "$?") to make sure they're zero which means "success", non-zero means "something went wrong". Basically, you now have a script that outputs the opus files to a destination dir, then copies all the images there too. So you'll have the current dir with the flac+graphics, and destination with opus+graphics. Do some reading on bash variable substitution, substring handling and test cases. These basic skills avoid a lot of redundant pipes to sed/awk/grep/cut etc. Sometimes, you will need the pipes, and that's cool too, but I see so much of this sorta crap: cat file | grep foo | wc -l Which spawn 2 subshells, 3 processes and whole heap of resources to do what can be done with this: grep -c foo file Bash is like any of the command line tools; there's HEAPS of stuff baked into these and knowing how to use them, and when to use them will separate your script-fu from the run-of-the-mill script-jackasses I deal with on a daily basis! Edit: Formatting
This is also how I start my projects. The funny thing is: After a while you realize that it's impossible to get something right the first time because at the beginning you most certainly don't understand the problem you are trying to solve. So you throw away your first version and start over but this time with much more knowledge about your problem.
By the way, there is a third (and often easier) construct: xargs. xargs takes a set of lines from standard input and applies a command (or series of commands) to those lines. It has options that make it fairly flexible to use. For example, say you had a list of numbers (in file "nums") and wanted to increment all those numbers by one, and write the result to file "numsplus": `cat nums | xargs -n 1 -J "%" echo "%" "+1" | bc -l &gt; numsplus` Or, for a less contrived example, I recently realized that I had ten billion instances of Google Chrome running and it was eating up 10 gigs of RAM. So I ran: `ps ax | grep "Google Chrome" | cut -f 1 -d " " | xargs -J "%" -n 1 kill -9 "%"` which searches the current process list for "Google Chrome", cuts out the first column of the results (ie, the Google Chrome process ids), and then uses xargs to run "kill -9" on each process id. The -J xargs options lets you specify a string that is the placeholder in the command you run for the list you are reading from. I find the above command much easier than running a while loop to do the same thing, and conceptually easier as well, because it allows me to think of it as running a single command on multiple inputs, as opposed to running one command a bunch of times on a bunch of inputs. 
I will look into these and report back! A few questions for curiosity: 1. why shouldn't I use uppercase variables? 
Still returning false.
adb is probably outputting with CRLF line endings. Run the script with `bash -x ./scriptname` to see if that's the case. See [FAQ 52](http://mywiki.wooledge.org/BashFAQ/052) for more on that. On a side note, don't use uppercase variable names.
I think you might be in the wrong field for you. 
&gt; Then, the touch command gives me access to the directory's images and modification times. What do you mean, “gives you access”? `touch` *updates* the access and modification times of its arguments, which means that running `touch "$dir"/*` is an extremely bad idea if you want to sort by modification time – you just overwrote that info! Remove that command, you don’t need it. Instead, read the `ls` manpage and search for “sort”. `ls` can already sort for you, even by modification time.
Exactly, except for the subdirs problem. But that’s something you can fix later, try doing the image appending now…
 q3_image_sorter.sh: 4: q3_image_sorter.sh: /home/2015/User/Downloads/Problem3/SimpleTest/: Permission denied dir2 dir4 dir1 dir3 convert.im6: unable to open image `dir1.jpg': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: unable to open image `dir2.jpg': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: unable to open image `dir3.jpg': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: unable to open image `dir4.jpg': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: no images defined `sorted.jpg' @ error/convert.c/ConvertImageCommand/3044. But the reasons I can't open dir1234 is because they are subdirectories in which the .jpgs are found. When putting in the .jpgs directly into the working directory, I get the same result. 
Hm, looks like we do need to fix subdirectory traversal first. `ls` has a `-R` option, for **r**ecursive listing, but I don’t think it’s useful for us, since it separates its output into sections, and doesn’t just give one long list of files (all sorted correctly). Instead, I think you have to *find* all files, *list* each of them with modification date (with a *time style* that supports sorting), then *sort* that list by the right column (which you have to *cut* out of the full `ls` output). I don’t want to just give you the command, so can you try putting that together?
So if I did dir=$1 find dir -name "*.jpg" That would essentially be the same thing?
So whenever I call a variable in bash, I need to have that $ with it? Also, could I do this? find $dir -name "*.jpg" | ls -ltr 
&gt; So whenever I call a variable in bash, I need to have that $ with it? Yes, and usually, you should also quote it: `find "$dir" -name "*.jpg"`. Otherwise, if `$dir` happens to contain a space, that’ll be turned into two arguments to `find`, which you don’t want. &gt; Also, could I do this? No, `ls` doesn’t read arguments from stdin. Instead, use `find -exec`: find -exec command --option=1 {} \; runs `command --option=1 X` for each file, replacing `X` with the file name. (The command ends with `\;`.)
&gt; However, that doesn’t give you files in subdirectories. Not sure what to do about that… `find` will take `-ls` but now you have to strip away a good deal of cruft. And pipe the whole thing into a `sort`.
More accurately: "Why is a [ being added to my prompt after upgrading to OS X 10.11?", to which the [answer](http://stackoverflow.com/a/32889827/656912) is: Uncheck Edit &gt; Marks &gt; "Automatically Mark Prompt Lines" in the Terminal app.
What if I did: find "$dir" -name "*.jpg" - exec ls -l --time-style=+%s {} + | cut -d' ' -f6,7 | sort -n | cut -d' ' -f2 And only find the files that are .jpgs? 
would it be possible to take the list of images and save them all individually as variables and then print them after 'convert'?
I'm getting a lot of errors. This is what I get when i just run the find ... command bash q3_image_sorter.sh ~/Downloads/Problem3/SimpleTest 1442961378 1442961450 /home/2015/user/Downloads/Problem3/SimpleTest/dir1/fee.jpg /home/2015/user/Downloads/Problem3/SimpleTest/dir4/foe.jpg When there should be dir2/fum.jpg and dir3/fie.jpg there as well. Running it with -append yields: convert.im6: unable to open image `image.jpg': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: unable to open image `1442961378': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: no decode delegate for this image format `1442961378' @ error/constitute.c/ReadImage/544. convert.im6: unable to open image `1442961450': No such file or directory @ error/blob.c/OpenBlob/2638. convert.im6: no decode delegate for this image format `1442961450' @ error/constitute.c/ReadImage/544. and no image
Hm, looks like something’s wrong about the columns, and for `dir2` and `dir3` it printed the modification timestamp instead of the filename. What’s the output of `find "$dir" -name "*.jpg" -exec ls -l --time-style=+%s {} +`? (That’s just the `find` part of `find …` without the rest of the pipeline.) (Also, the `image.jpg` is just an example; I don’t know what image you want to append to.)
Full output, please? It should be a table, not just the paths.
Arrgh. nogroup 8693 There’s two spaces in there – for padding – and stupid `cut` interprets that as an empty field :( There are two solutions to this: 1. Fix multiple spaces by inserting this command in the pipeline: sed 's/ */ /g' I. e., `find … | sed … | cut … | sort … | cut …`. This replaces all runs of one or more spaces with exactly one space. 2. Use `awk`, which doesn’t care about multiple spaces, instead of `cut`: find … | awk '{ print $6 $7 }' | sort … | awk '{ print $2 }'
Putting in the 'awk' edits, I now have nothing printed when I run the script. Putting in the sed, it works correctly! I now have a list of .jpgs in order, but I'm still not sure how to pass that to convert -append... :/ Would it be possible to save the file names that are now ordered into a temporary .txt file, and then use that .txt file as what is used for all the .jpg names to be put into convert?
That's pretty close. I'm getting an error because it's failing to find the mp3 file since it's looking for videofile.mp400.mp3. Need to strip the .mp4 out of the mp3 file EDIT - I got it from there. Thanks for your help. You were spot on. I just had to adjust something on my end.
Oh right. From line 267975 to end, not the last 267975 lines. My bad.
There seems to be a bit of a minute/second shift in the file names. 
There's a reason for that: It's not generating the list on the fly, it's generating it in memory and then calling "printf" only after that's done. You are generating `60*12*31*24*60*60 = 1928448000` strings. This is going to take a long time. Each string is 51 characters long and will need a null terminator (→52 chars), so you will end up making `1928448000*52 = 100279296000` bytes worth of strings at minimum before the printf command starts. This will make bash use at least 93 gigabytes of RAM before the command starts. Your output file will be that large in the end too, and the total bandwidth incurred by your HTTP requests alone (excluding responses) will be even larger than that. I recommend you find another way to go about that. If you can spare the 0.1TB bandwidth in requests then go ahead and pipe the URLs to your command\*, but if not you will need to think of another way to do this. \* generating them on the fly: # something lazy like this # there are better ways, but they are a lot more effort to write. gen_urls() { for dontknow in {00..59}; do for month in {01.12}; do for day in {01..31}; do for hour in {00..23}; do for minute in {00..59}; do for second in {00..59}; do printf 'http://cameraurl.com//2015%s%s%s%s%s%s.jpg\n' "$dontknow" "$month" "$day" "$hour" "$minute" "$second" done done done done done done } gen_urls | parallel --tag --bar -q wget -nc {} 
see my other comment. 
If its find you're using, wouldn't exec work? (Or maybe I've read into your example too deeply) find . -name "somefile" -exec vim {} \;
Thanks, I really appreciate your guidance on this.. I just had a breakthrough. There's a thumbnail preview page that I can put the date range I want, open chrome inspector, copy all the HAR data from the network page.. copy that into a text editor and extract all the file paths.. now I can chuck this list at wget!! 
You may want to edit your question to remove references to capturing the output of a command (which is what my answer addresses). That doesn't seem to be what you're after, but rather the actual command line.
Multiplexors like tmux or screen have copy mode, and screen comes pre instilled on many distributions. Not sure it is what you want but its all I got for you.
 for protocol in ssh ftp vnc; do chmod +x "script_$protocol.sh" &amp;&amp; ln -s "$PWD/script_$protocol.sh" "/usr/local/bin/company-name-$protocol" done
I'm doing it ad-hoc. Sometimes I realize I want to do further processing of output (vim, grep, echo, etc...) that has already been generated (via the find command here). I'd like to process it without regenerating the output, but most importantly, without many keystrokes. 
Actually, I am looking to capture output of the previous command like my title says, except I want to do it ad-hoc (capture the output of the command after it has just ran) 
Wow! So simple. So exactly what I need. And all that in 4 lines of code. Thanks a lot /u/KnowsBash :) *edit* I don't even need to write a script for that. I can just type it in one line.. awesome.
That probably has to do with the color mode and env variables. At risk of giving unsolicited advice tmux -2 runs with what most 'modern' she'll emulators need.
just fyi, `!!` does regenerate the output, just like hitting up arrow and adding `vim` to that line.
 ^restart^stop
The funny thing is, your post title already contains the answer :D you can do this with the command `cut`. echo "$var" | cut -d' ' -f2 This cuts the **2**nd **f**ield out of a list **d**elimited by **spaces**.
Or you could use `tput`…
okay, then regex is probably the best solution. $ echo hey how ARE you doing | sed 's/^.*how \(.*\) you.*$/\1/' ARE
Your ~~ps~~ grep has colors. Try this sed command: `sed 's/.*--title[^ ]* \(.*\) --demuxer.*/\1/'`
I have left wifi event that trig an sms, also a join wifi event that trig another sms. when wifi connection is lost for few second, it trig sms for nothing, so I use activatecommand and tell it to sleep 10 sec and check if wifi connection. then if connection is lost, it check after 10 sec and if connection is ok I can know that the wifi is not left. The problem is when it join the wifi after that connection lost. No way to verify it... so with a variable, when join wifi if temp=1 then do nothing if not set it to 1 and trig sms.... Hope it clear enough and thanks a lot!
May I ask what status bar that is?
Ok awesome. I am used to xmonad and 
 echo 1-2-3-4 | awk -F- 'BEGIN{ORS="-"}END{for(i=1;i&lt;=NF;i++){print $i+$i}}' | sed 's/-$/\n/' 2-4-6-8
the bash way would be to use `read` if the words and placement are constant: var="how ARE you" read _ match _ &lt;&lt;&lt;"$var" echo "$match" or an array: var="how ARE you" words=($var) echo "${words[1]}" (careful with the quoting there) or else use parameter expansion if the string varies: var="hey how ARE you doing" var=${var##*how } var=${var%% you*} echo "$var" 
great thanks a lot, so it said "list.csv: text/plain; charset=iso-8859-1" I guess that iso encoding is different than UTF-8? Any chance you know how to save it as UTF-8? If not thanks anyway :)
Terrible typo, I knew that as well, but now I get nothing...
.......or just use printf "."
The only way this works right now in that case is if .old exists. Otherwise you have no logic in the 'if file exists' part where a .old file doesn't. 
I assumed you always wanted the backup file. If you want to be prompted you need another if check and a prompt to ask if it should be created
Install sl and run it. You'll be pleasantly amused next time you typo ls.
ok, I made some changes so that I can see where it is exiting on, and I think simplified things a bit #!/bin/bash file="$1" if [[ -f "$file" ]]; then if [[ -f "$file.old" ]]; then read -r -p "$file.old exists. Overwrite? (y/n)" echo if [[ $REPLY =~ ^[Yy]$ ]] then echo "you chose yes to creating backup"; cp $1{,.old} &amp;&amp; vi $1 else # if the reply isn't yes echo "you chose no to backup"; vi $1 fi fi else read -r -p "File does not exist. Create? (y/n)" echo if [[ $REPLY =~ ^[Yy]$ ]] then echo "you chose yes to creating $1"; vi $1 else echo "you chose no to creating $1"; exit 1 fi fi problem is, if the file exists and the file.old doesn't exist, it does nothing, just ends. Every other condition works. I'm not sure why. I feel like i'm so close, yet missing something huge.
You're welcome. You've certainly advanced my understanding so I thought I'd show my appreciation. I'd love to have a hint on why the nested is bad though - I really chose it just because I know no other way :D
Also this thread (https://discussions.apple.com/thread/1947083?start=0&amp;tstart=0) seems to say the same thing as that other thread I posted, that the file name path needs to be broken up &lt;like&gt; &lt;this&gt; however I tried and kept getting syntax errors
Ok thanks for your quick reply and that link. I checked it out, still not sure what I may be doing wrong. I cd to the folder that has the pictures and list in it, then copy and paste each line of your code but still no luck. If you have any other ideas to try I would really appreciate it, sorry for being such a noob! Here is what happened when I tried this time: &gt;UFO:pics john$ while IFS=';' read -r old_name new_name _; do &gt; set -x &gt; printf 'Renaming "%s" to "%s"\n' "$old_name" "$new_name" &gt; mv "$old_name" "$new_name" &gt; done &lt; list.csv &gt; + IFS=';' &gt; + read -r old_name new_name _ &gt; ++ update_terminal_cwd &gt; ++ local 'SEARCH= ' &gt; ++ local REPLACE=%20 &gt; ++ local PWD_URL=file://UFO.local/Users/john/desktop/pics &gt; ++ printf '\e]7;%s\a' file://UFO.local/Users/john/desktop/pics &gt; UFO:pics john$ 
Yeah I know about the backup option, but I was more interested in being able to choose what gets backed up and keeping it in directory with the other files. Most of this stuff I'm doing is dealing with .conf files for system administration, so I actually changed the format of my script to $1.$date (with the date format being defined earlier in the script), and I think this may end up being the most useful for me. I'm so new, I'm just sort of struggling to remember that ifs can just end if criteria isn't met - that certainly allows for some improvements on my side. I'm going to see what I can do to clean up mine without blatantly copying yours.
 #!/bin/bash DOT="." for VAR in {1..5} do printf "${DOT}\r" DOT="${DOT}." sleep 1 done
Not quite, really... Virtualenvs concern is on packages, binaries and application context. My issue is on working with multiple projects. Virtualenv has actually helped me comming up with this, as I had to work with 6 different projects at a time and it's quite easy to mess up with different projects sourced venvs, so, as you may find out, the `at` command (renamed to `sd` now) deactivates former contexts venv to activate a new venv on target project. Also, it's a handy way to go to the project from wherever you are, so, instead of `cd ~/projects/nested-folder/awesome-project` you can `sd awesome-project` and you're there. Edit: I made some changes to documentation to allow easier undestanding of what the scripts really do.
Yes, it seems we're doing similar stuff. I liked the idea behind `prm` and `swissknife` kind of goes to the same way (for terminal project management), but `swissknife`is intended to be a toolset of small helper scripts in general, for helping development as a whole.. I'm thinking about introducing other scripts such as `http` for testing, for example.
It totally is. Fuck passwords. 
After checking with the team here, we do allow SSH keys, but they have to be passworded. Would this put is back to square one again?
No. All that means is that when you go to use the ssh key you need to unlock it with a password. You only need to unlock it once at the beginning of your session. You wont need to enter it again unless you reboot the machine. ssh-agent is the man page you want to read. 
thanks, this may be workable then. Thanks very much. 
OK, that s ok ! I find a way! Just with activator profile! Just take the time to program my phone and I will be ok to tell if it work fine!
Yeah, we do use some ansible here - this was just for one one-off personal stuff i'm experimenting with - I think I got it all sorted now :)
sshpass can solve this. you can prompt for password once then use that variable for the rest of the script
Thanks, I actually came across that a few minutes ago. Interesting stuff. I actually went ahead and found a solution to this whole thing, so I'm done with this script for now :D
Looks like you should be able to click on the link to download the file and save in which ever directory you want to run this from. If it is embedded in an html page or something you should just be able to copy all the code into your text editor and then save that. 
Unrelated to your problem, but [don’t read lines with `` for f in `cat slist` ``](http://mywiki.wooledge.org/DontReadLinesWithFor). Instead: while read -r server &lt; slist; do ssh "$user@$server" ls | grep test; done
If you have ssh, you should be able to scp the file from your local drive to the remote system. An easier method may be to just paste it into a new vim file (or whichever editor you use). 
If your $DOT doesn't have any special characters, no need for the -e option to echo. All you need is the -n, which makes echo not end with a newline character. I highly recommend reading the bash man page since echo is a bash builtin. It's a lot to read, but so worth it.
Shift + Insert to paste into the terminal. Paste the script in Nano, vi, Vim, emacs, or whatever your editor of choice is. Save the file, and run it with "python script_name_here"
Thank you very much for your comments. I must admit I was always just "hacking" in shell without deeper understanding. If it worked - it worked, hence most of the mistakes you pointed out. I fixed some of them, some of them I will have to think through. As for the variable names: local function vars I've named in lowercase. Only globals are in uppercase. I thought that is the standard? As for the underscores - this script is being sourced and it poisons the local scope, thats why I've decided to go with _. -edit- - My doubts about using tput were concerning its "atomicity". I move the cursor, then print, then move it back. If someone tries to print something during that time my moving cursor will break. - Maybe you know why executing "trap - DEBUG" from inside my script doesn't actually remove the trap? I am unable to find any info on this, and just typing that into my terminal works fine.
&gt; My doubts about using tput were concerning its "atomicity". I move the cursor, then print, then move it back. If someone tries to print something during that time my moving cursor will break. I just thought of this possibility: # Print text in the top-right corner. _cmdt_print () { local sc="$(tput sc)" rc="$(tput rc)" local cup="$(tput cup 0 $(($(tput cols) - ${#1} + 15)))" echo -ne "$sc$cup $1 $rc" } Only the final `echo` command actually prints anything to the terminal, so this might be atomic.
Yes, that worked perfectly, thanks. Hope you don't mind I included you in the commit message :)
You can simply write to /dev/tcp/$ip/$port: { somecommand someothercommand } &gt; /dev/tcp/$ip/$port You can also explicitly open a file descriptor so you can redirect several commands to it: # redirect file descriptor 3 to your IP/port exec 3&gt; /dev/tcp/$ip/$port # write something to it (redirecting 1 [stdout] to 3) ls 1&gt;&amp;3 # close file descriptor when you're done exec 3&gt;&amp;- Note that /dev/tcp/* is not a real directory but a bash-specific internal alias, so be sure to start your script with `#!/bin/bash` and not `#!/bin/sh`. 
4\. You can also use service names instead of port numbers (`/dev/tcp/google.com/http`), they will be looked up.
Just keep track of the `nc` commands you run, and use the shell to control them, no `killall`.
I’d like to add that `"-p$db_pass"` is a bad idea, since it allows anyone else on the system to read the password from the command line. See [MySQL manual](https://dev.mysql.com/doc/refman/5.6/en/password-security-user.html) for details and alternative solutions.
yup pgrep to get the PID
That's probably as good as you can do. The output you see is the command that was executed, after the shell did its substitutions 
Try using `ps` to get the command line and process ID . I just made a short test script that uses a for loop on the output of `ps` to extract the process id and command line for matching lines, print the process id and cmdline to the terminal, and then kill the process with the id. I can post code if you want, but it sounds like an educational thing so you probably want to do it yourself.
Why doesn't bash's job control expand variables? Or rather why does the `jobs` shell built-in not expand variables present in the command? Is it not capable of knowing the value of them, or does it just not expand them when jobs are created? Just curious if you know.
Thank you. You explained this perfectly. I learned a lot from this. Also pid `82158`? What's the uptime on this machine man? ;)
 cat "$@" | ...
I doubt even the external GNU time command can do arithmetic evaluation on the result, so you'll have to pass it to some command that can; like `bc`. Personally, I'd go with the `time` keyword rather than the external command: TIMEFORMAT='%U+%S' # Or to output all three values in addition to the sum: # TIMEFORMAT=$'%R\n%U\n%S\n%U+%S' { time ./E.out e_in.txt &gt; e_out.txt 2&gt;&amp;1; } 2&gt; &gt;(bc -l) See [BashFAQ 32](http://mywiki.wooledge.org/BashFAQ/032) for more on capturing time output.
 awk '{sum+=$NF} END {print sum}' /path/to/logfile should do the trick. edit: example https://labs.neilhanlon.com/ss/2015-10-13_1015.png edit2: Oops. I forgot you wanted the average of the averages. awk '{sum+=$NF; count+=1} END {print sum/count}' /path/to/logfile
Also see [FAQ 96](http://mywiki.wooledge.org/BashFAQ/096).
Sweet! Thank you sir. Another quick question... Could you post your bash prompt? It looks pretty flippin sweet. Thanks again.
https://gist.github.com/NeilHanlon/c2c2e260eca4afc35a65
Found it: https://unix.stackexchange.com/questions/209833/what-does-a-dollar-sign-followed-by-a-square-bracket-mean-in-bash TL;DR: Alternate syntax, same thing, deprecated. ~~Man page used to say “will be removed”, now doesn’t mention at all.~~ EDIT: mailing list thread suggests the “will be removed” in the man page might have been added by distros.
Haha, you made my day! :D On a side note, is it possible to limit the scope of functions such that they can only be called from certain functions? Eg. If I have two functions called DoStuff() and DoStuffPart1(). Is it possible to declare DoStuffPart1 such that it can only be called from DoStuff()? 
Yes, same thing. `$[ ]` is an obsolete bash-specific syntax for shell arithmetic, dating from before POSIX [standardised](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_04) on `$(( ))`.
am new to reddit so i don't know how to format codes properly :P I had the same problem few minutes back, but i solved it, just do this declare -A dlpath for i in a b c d do dlpath[$i]="$i $RANDOM" done echo ${dlpath[a]} ###which will be a including any random number echo ${dlpath[b]} ####which will be b including any random number echo ${dlpath[c]} ####which will be c including any random number note: dlpath is the array you just declared . $i will be whatever stuff you wish to load into the array. 
From the [XRAT](http://pubs.opengroup.org/onlinepubs/9699919799/xrat/V4_xcu_chap02.html#tag_23_02_06_04): &gt; In early proposals, a form $[expression] was used. It was functionally equivalent to the "$(())" of the current text, but objections were lodged that the 1988 KornShell had already implemented "$(())" and there was no compelling reason to invent yet another syntax. Furthermore, the "$[]" syntax had a minor incompatibility involving the patterns in case statements. So bash implemented it in preparation for the POSIX release, then promptly obsoleted it again when `$(())` was chosen instead.
Thanks for your comments. I'm new in shell programming and I need to improve my code. I have some questions about your response. What's the correct way to initialize the arrays? If I use internal functions inside PM main function, why they littering the shell with global variables? Thanks!
&gt; What's the correct way to initialize the arrays? To populate an array, you can use a loop, `read`, or `mapfile`, E.g. $ str=foo,bar,baz $ IFS=, read -ra arr &lt;&lt;&lt; "$str," $ declare -p arr declare -a arr='([0]="foo" [1]="bar" [2]="baz")' -- $ mapfile -t -n 5 words &lt; /usr/share/dict/words $ declare -p words declare -a words='([0]="A" [1]="a" [2]="aa" [3]="aal" [4]="aalii")' See [BashFAQ 5](http://mywiki.wooledge.org/BashFAQ/005) for more on arrays. &gt; If I use internal functions inside PM main function, why they littering the shell with global variables? Variables are global by default. You should declare variables as local unless you specifically want them global. $ f() { x=1; printf 'inside: %d\n' "$x"; } $ x=0; f; printf 'outside: %d\n' "$x" inside: 1 outside: 1 -- $ f() { local x=1; printf 'inside: %d\n' "$x"; } $ x=0; f; printf 'outside: %d\n' "$x" inside: 1 outside: 0 
&gt; Declare -A dlPath uploadPath `declare`, not `Declare`. It's case-sensitive. Also be aware that the arrays will be local to that function
This is exactly what SSH's [ProxyCommand](https://wiki.gentoo.org/wiki/SSH_jump_host) is for.
You need to be more specific. What do you mean by "detect"? If you want to list them, you could run the following command (assuming the list of files in stored in the file "titles"): `cat titles | tr ":" "\n" | tail +2` This will list the file names and parse out the "header".
Yes, you are right! I had that in there from a previous iteration of the puzzle and forgot to remove it. 
oh, what I'm trying to do is to make files/directories with the names of these "files" that I'm trying to parse through. Eventually I would want to create a text file with the name of these. would using &gt;&gt; accomplish this?
Hangs up your call. Makes you have to dial back in while punching your chair. 
Thanks for the suggestion. I will take into account :)
Yes, quotes are vital if you want your script to work reliably, but that's not related to the error messages you posted.
Ok. First off, what exactly does that error refer to? I tried googling it, and couldn't come up with a comprehensive explaination. Secondly, when I was getting that message, I would have my command running as such: 1) skicka download '${uploadPath[bar]}' '${dlPath[foo]} Than I tried changing around the AA: uploadPath=( [foo]='/Documents' ... ) 1) skicka download ${uploadPath[bar]} ${dlPath[foo]} This was how I was getting that error... 
try using single quotes. echo '$1'
 echo '$1' echo "$1" echo $1 echo {$1} echo {'$1'} echo {"$1"} echo -e '$1' echo -e "$1" echo -e $1 echo -e {$1} echo -e {'$1'} echo -e {"$1"} None of those work.
That would prevent it from expanding. The issue is probably when passing the string into his bash function it's being expanded and not treated as a literal, you're doing something like this: root@tinydeb:~/code# cat test.sh #!/bin/bash echo $1 Which nets you: root@tinydeb:~/code# ./test.sh blah\blah blahblah When you want: root@tinydeb:~/code# ./test.sh "blah\blah" blah\blah Even if you're passing something inside a script into a bash function, you still need to quote it appropriately (call the function with the path quoted). edit: formatting is hard, okay?
&gt; you still need to quote it appropriately (call the function with the path quoted). Darn. Is there any way around that? I don't really want to have to put quotes around it
Ah I am doing echo "$1" | sed 's/\\/\//g' | cd with no luck Trying to `cd` into that directory (basically I'm using git bash on Windows. Copying path from Intellij - need to cd to that path in bash)
Well, I don't run bash on Windows, but I don't think cd accepts input on the pipeline like that. You could do things like cd $1, or cd ${1//\//\\} (which would do the / replacement for you). edit: formatting on the replacement got mangled, fixed :)
That depends on various things. * If the array is properly declared as associative, the only way (that I can think of) to get that error messages is by using an empty string as key, but neither `foo` or `bar` are empty strings, so that can't be it. If it was `${uploadPath[$var]}` instead, and `var` was empty, you'd get that message: $ declare -A assoc=(); $ var="" $ printf '%s\n' "${assoc[$var]}" bash: assoc: bad array subscript * If the array is not declared as associative, it depends on what version of bash you are using, because the behavior has changed a bit. Prior to bash 4.2, if you provided a subscript that evaluated to a negative number, you'd get that error: $ printf '%s\n' "$BASH_VERSION" 4.1.17(1)-release $ array=( one two three ) $ foo=1-2 $ printf '%s\n' "${array[foo]}" bash: array: bad array subscript -- $ printf '%s\n' "$BASH_VERSION" 4.2.53(1)-release $ array=( one two three ) $ foo=1-2 $ printf '%s\n' "${array[foo]}" three And even with bash 4.2, you may get that error if you use an index with a very large negative number. There are probably also other ways to provoke this error message.
Yes I mean that running 1 process after another is the same speed as running 5 (supposedly) concurrently. I supposedly b/c I am not sure why there is 0 speed improvement at all. I would think spreading the tasks among 8 cores would yeild some improvement even if it is very minor. this test was done with only 2 of the 5 processes in both case though (as all 5 take around 30 min). 
If you have one 5 Usain Bolts running the 100m dash, vs 1 usain bolt running the 100m dash, who will win? The 5 usain bolts? The one usain bolt? Or will they all pretty much tie? That's why I said, you could see performance improvements in your script if you parallelize tasks *within the script* (preferably the cpu-intensive ones, if you can identify those) Simply running your script (again without seeing it) 5 times in parallel is *not* going to make any of them finish faster.
More specifically, there may be tasks WITHIN your Python scripts you can parallelize. Check out Python's multiprocessing library for ways to parse out tasks to different CPUs within a script: https://docs.python.org/dev/library/multiprocessing.html#module-multiprocessing
thanks for the reply!
Hmm yeah, I didn't think about that - thanks!
I’ve never heard of whiptail before, but Bash also has built-in support for simple selection menus, so I’d try something like this: select kind in switch router "linux server" "windows server"; do case "$kind" in switch) ./switch_script; break ;; router) ./router_script; break ;; "linux server") ./linux_script; break ;; "windows server") ./windows_script; break ;; *) echo wrong &gt;&amp;1; false ;; esac done
 printf "a\ta\nb\tc\nc\te\nd\taa\ne\tf\n" | awk ' {i++; Left[i]=$1; Right[i]=$2} END { for(leftE in Left) { j=0 for (rightE in Right) { if (Right[rightE] == Left[leftE]) { print Left[leftE], Right[rightE]; matches=1; delete Left[leftE]; delete Right[rightE]; }; } } for (leftE in Left) { print Left[leftE], " " } for (rightE in Right) { print " ", Right[rightE]; } } ' I'm not sure if it's the best way to do it.. but this is how you'd do it. https://gist.github.com/NeilHanlon/4b62c3d5bad5d1a83b7c Essentially, what this does is create two arrays, Left, and Right, which contain the left and right elements of the list. Then, we loop through the left array, and inside, through the right. If the left elem matches the right, print the pair, and delete them from the array. Finally, we loop through the left, and through the right, and print them. Since the matches have been removed.
Well, you’ve basically got what `foo &lt;(bar)` does. Creates a fifo, pipes `bar` into it, hands it to `foo`. There’s also `foo &gt;(bar)` which does the same in the other direction. (`foo | tee &gt;(wc -l)` is occasionally useful – print output, then number of lines of output.) On my system, I think that the difference between `vim &lt;(ls ~)` and `view &lt;(ls ~)` is that `vi`, `view` and `vedit` are all provided by the `ex` executable (via symlink), while `vim` is a separate program. I guess that you have the same setup, whereas on your friend’s system, `view` is provided by / symlinked to `vim`. It seems that Vim reads fifos and Vi/Ex doesn’t.
Pipes (fifos) don't support `lseek`--many programs assume that they can use seek to jump around in files that are given to them on the command line. Many "heavyweight" utilities (like GNU ones) check if a file they want to seek around in is a pipe, and if so, create a temporary file and buffer the pipe into it so that they can; but many utilities don't. It could be that `vim` is designed to not need to seek, or that it detects if the file is a pipe, and then buffers it. As for `vim` and `view` being the same program: `ex`, `edit`, `vi`, `vedit`, and `view` are all the same program (via symlinks)--it chooses which to run based on argv[0]. Now, `vim` is but one implementation of `vi`; on some systems `vi` &amp; friends may be `vim`, on other systems `vi` &amp; friends may be a different implementation, and have `vim` as a separate program. I know that on Arch Linux and Parabola GNU/Linux-libre, `vi` and friends are the *original* `vi` written by Bill Joy for BSD, not `vim`.
The only variation in &lt;(...) between different OSes is when they support /dev/fd (in one case it makes anonymous pipes, and in the second case it makes named pipes and removes them later). Why it works in your friend's OS and not in yours depends on what vim/view are doing with pipes there. If you want a temp file (a regular file) without having to clean up, you can do: dualbus@yaqui:~$ { echo hi &gt; /dev/fd/3; cat /dev/fd/3; ls -l /dev/fd/3 ;} 3&lt;&lt;&lt;'' hi lr-x------ 1 dualbus dualbus 64 Oct 17 05:38 /dev/fd/3 -&gt; /tmp/sh-thd-201919303 (deleted) Which abuses that bash creates a temp file for the heredoc, and it deletes it immediately after.
You could pipe to `head -1`. 
I think OP wants the *first match from each line*, which is not what grep's `--max-count` option does. Here's a solution I came up with using Perl (I used a more lenient regex than OP's just to keep it easy to read): $ printf "1.1.1.1 2.2.2.2\n1.1.1.1 2.2.2.2\nthis does not match\n" | perl -pe '/((\d+\.){3}\d+)/; if ($1) {print "$1\n"}; $_ = ""' 1.1.1.1 1.1.1.1 This was my first attempt to actually use Perl for something, so I have no idea if this is a good approach. It seems to work, though.
awk can print the **first** match: awk 'match($0, /regex/) { print substr($0, RSTART, RLENGTH) }' sed the **last**: sed '/.*\(regex\).*/!d;s//\1/' Note that standard sed uses different [regular expressions](http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). An example for the **second**: $ echo 123 456 789 | awk -v "regex=[0-9]+" -v "m=2" '{ a = 0; while (match($0, regex) &amp;&amp; ++a &lt; m) $0 = substr($0, RSTART + RLENGTH); if (a == m) print substr($0, RSTART, RLENGTH) }' 456 Some comments: # set m to the desired match (first, second, third, ...) awk -v "regex=some regex" \ -v "m=3" ' { a = 0 # as long as there is still a match on the current # line and there weren't more than desired while (match($0, regex) &amp;&amp; ++a &lt; m) # cut the earlier matches off $0 = substr($0, RSTART + RLENGTH) # if as many matches as desired were found if (a == m) # print the last one print substr($0, RSTART, RLENGTH) }' As an aside, your regular expression does not only match IP addresses (it for example matches 1.1.1111), also the dotted-decimal notation is not the only representation for IP addresses.
Ah, I figured they just had issues with markdown eating the newline in their example. In that case I would use cut. Assuming there's a space between each IP address you could do: `$ grep -oE '((1?[0-9][0-9]?|2[0-4][0-9]|25[0-5]).){3}(1?[0-9][0-9]?|2[0-4][0-9]|25[0-5])' | cut -d ' ' -f 1` to get the first IP on the line `$ grep -oE '((1?[0-9][0-9]?|2[0-4][0-9]|25[0-5]).){3}(1?[0-9][0-9]?|2[0-4][0-9]|25[0-5])' | cut -d ' ' -f 2` to get the 2nd IP on the line.
Awk and sed are complete mysteries to me. 
Should it work recursively?
cat ... | grep ... | head -1
Feel free to pm..
1. always (with double quotes, single quotes make it literal) quote your variables. Here, it's why $1 breaks. This is especially necessary when working with windows filesystems. But, never mind, just always do it. It's a good habit. In this case, your input must be quoted as well, as it contains special shell characters. 2. you don't need sed or tr, you can do this with bash variable substitution. simply do: dir="C:\windows\crap_directory" cd "${dir//\\//}" No extra processes needed. For great fun: try using "c:\not a file\number 6\nothing\no one\to victory" as an unquoted argument to your command. 
I was taking about my comment. At the time it was in the negative. On this sub that's usually because the code you suggested has some negative side effect. I just wanted to make sure that wasn't the case. 
I still haven't the faintest idea what you want to do, but you should be able to make use of something like the following, where "${file}" is your input file #!/bin/bash umask 022 myroot=/tmp while IFS=: read title dir1 dir2 ; do printf "%s\n" "would make directory ${myroot}/${title} and \"${dir1}\" and \"${dir2}\" under it ..." mkdir -p "${myroot}"/"${title}"/{"$dir1","$dir2"} done &lt; "${file}" If there are spaces or other junk in your fields this should accommodate. Only one process per line, no pipes. * always set umask when creating files. This is lax, giving user writeable and world readable result. This is writing to /tmp, so it's moot, but always do it! * "while" is generally best for looping over lines, especially lines with several fields because - * setting IFS to ":" allows "read" to assign variable names to each field, even with spaces in them * mkdir is pretty explanatory, the expansion { $dir1,$dir2 } just creates both directories at once
also: # Use the first arg if available, or $PWD (BASH internal variable, the same as $(pwd)). # Uses -maxdepth 1 to stop recursion. find "${1:-$PWD}" -maxdepth 1 -type f | wc -l One of shellcheck's lints is ["Use `find` instead of `ls` to better handle non-alphanumeric file names."](https://github.com/koalaman/shellcheck/wiki/SC2012)
Can you be more specific? Because you're looking at scripting this, is this for use across multiple hosts? Are you wanting user interaction or for the script to just make a bunch of predetermined changes? What changes do you want to make?
So I want to set PermitEmptyPasswords no PermitRootLogin no Protocol 2 UsePAM yes X11Forwarding no Without user interaction
sed can do all of this. 
Ok, so how I'd approach it would be to test each and for any that fail, use sed to correct e.g. if ! grep "PermitRootLogin no" /etc/ssh/sshd_config &amp;&gt;/dev/null; then sed -i '/^PermitRootLogin/c\PermitRootLogin no' /etc/ssh/sshd_config elif ... sed -i ... Something like that...
I've never used it myself, but [augeas](http://augeas.net) sounds like a possible solution. I've been meaning to look into it closer for a while now.
&gt; Have a look at Google's Shell Style Guide That link is a solid wall of text without line breaks. 
Not sure if I'm understanding the question exactly, but... ~/code$ cat rando.csv blah,32177583483,skookum blah,312773483,skookum blah,312773483,skookum blah,312772483,skookum blah,312774483,skookum blah,312773483,skookum blah,312773483,skookum blah,32177585555,skookum blah,32177586666,skookum ~/code$ awk -F, '{if ($2 ~ /^312/) print $2}' rando.csv | sort | uniq -c | sort -rn 4 312773483 1 312774483 1 312772483 Like so? (Note: obviously not the best way to deal with a csv, for rogue commas and the like, and you could do the counting in awk as well, but quick and works) :)
Yeah, looks fine in Chrome, just not Firefox. Ah... it was my NoScript extension. 
Totally agree. Quick one-time shit gets thrown into a long pipeline command and stderr gets sent to /dev/null ;) Anything that's going to be a script that runs, or if I really care about the output (for something other than quick data)--I use something else.
`grep -c 321 file.csv`.
&gt;Currently I have it so output from whiptail goes into a temp file called "pkgch." From there it gets formatted to print on 1 line w/o "" and separated by spaces (like you use in sudo apt-get install package1 package2 ect). Out of curiosity, how are you getting your formatting to this point? &gt;I've tried reading the input from pkgch with cat How did you use `cat`? What was the specific command you ran? &gt;and eval Repeat after me: "eval is evil." Generally speaking, you should avoid `eval` unless you know what you're doing. What specific error(s) are you getting out of `apt`? http://www.catb.org/esr/faqs/smart-questions.html &lt;-- read that. It will help people to better help you in the future. All of that said, try: sudo apt-get install $(&lt;pkgch)"
Hah, of course it does. Whoops. 
If you want to keep things more organized, you could still keep your 4 scripts seperated in 4 files and "source" (include) them in your "menu script" when they are needed. http://bash.cyberciti.biz/guide/Source_command 
Thanks!
Couple of issues; For your ur ls commands you need to add -1 to get a single file per line, ls by default is columns. Validate these at the cli and build up the line when unsure. You don't set your directory any where, so the for loop is looking in current directory which probably isn't the one you are scanning files in. don't need a ls command for the for loop, you can just say the search pattern. May want to consider understanding the find command, plus there a lots of grep options to help simplify probably. Can you explain what you are actually trying to validate or achieve? 
Can you explain the difference between what I have and what you have / when to use $()
So basically there is a file on the desktop that contains &lt;Type Answer Here&gt; 3 times. I need this program to run only if it appears twice. **BUT** there could be multiple kinds of these files (File 1, 2, 3, etc), so I need &lt;Type Answer Here&gt; to only show up 2 * the number of files (The files start with Forensic)
Even Google can't write a textual document without having to run a special-purpose program on the user's computer. What on earth is the world coming to?
You're not wrong in any way but it's sort of like you kicked a puppy. 
Hey there, Thank you for the reply and suggestion. I was about to try something like what you suggested. However, I was able to get my script to work with /u/whetu's help. The main reason I wanted it this particular way was so that I could easily add or remove packages from my script with minimal editing to the code. However, your example may come in handy in the future so I thank you again for the help. Take care!
You offered a thorough response to someone who seems to be in the process of learning to use BASH, when leaving a little bit to OP to figure out maybe could've helped him grow as a script-writer. OP may be learning the wrong lesson, by having the question answered so well. At least, that's my take on it. 
Nah don't be so harsh! It was an excellent post! It's just maybe a little less than appropriate, given the circumstance (which I'm not even sure of). 
&gt; a newline is a valid character in a filename, I tend to ignore this particular fact for the sake of practicality. The ability to generate correct NUL-delimited lists of files is relatively rare (`find` and `grep`, offhand), and consuming them even more rare (`xargs`, offhand). So in practice even if you do all the right things, there is no guarantee that a subordinate program won't mess it up. It's easier to just deem newlines in filenames evil and disallow them. Use `xargs -0` and `find -print0` whenever possible, but assume that when a newline actually comes along, something down the line is likely to flub it. By all means take ordinary measures to avoid common problems, like filenames with spaces wrongly getting split. But trying to cope with newlines in filenames completely really takes extraordinary measures. &gt; Don't try * at home. Use ./* instead, because it ensures that you will never trip up on filenames which start with a dash You can usually use `--` instead -- `ls -- *` will never trip up on leading dashes, because you've explicitly said that the arguments that follow are not options. It's the most reliable choice.
&gt; You can usually use `--` instead Yep. I do like this approach, but some programs (looking at you Zopflipng and python's argparse) either don't support it or have broken support for it. That and a co-worker once poked me about a script I wrote and asked "why are there so many double dashes, what does that mean?" I will use it where it works though, which is thankfully a large body of programs.
&gt; some programs (looking at you Zopflipng and python's argparse) either don't support it or have broken support for it. Python's argparse seems to work for me; eg `cd /tmp;touch -- -foor -roof; pep8 -v -- -foor -roof` returns the correct result (both files are checked, no problem is found. result is identical whether running pep8 (python3.5) or pep8-python2 (python2.7.10)). Research suggests that [-- has worked since 2.7.5](http://stackoverflow.com/questions/14693718/how-to-parse-positional-arguments-with-leading-minus-sign-negative-numbers-usi). I double-checked it by running `pep8 -foor -roof`, which returns as expected 'no such option -f' (EDIT: actually, looks like pep8 uses optparse instead. However, I tested also with my program `refcodes`, which DOES use argparse) I did find in the process that mcedit has broken -- support, though. EDIT2: Here's a proper minimal testcase, which works for me: #!/usr/bin/env python3 import argparse parser = argparse.ArgumentParser('test') parser.add_argument('-f', '--foo', help='defloof', type=str, default='') parser.add_argument('-o', '--owl', default=False, action='store_true') parser.add_argument('filename', nargs='+') args = parser.parse_args() print(args) Steps to test: * save above script somewhere * `cd /tmp/; touch -- --owl --foo -f -o` * `python3 PATH_TO_ABOVE_SCRIPT -o fo -- --foo` Namespace(filename=['fo', '--foo'], foo='', owl=True) * `python3 PATH_TO_ABOVE_SCRIPT -- --foo --owl -o -f` Namespace(filename=['--foo', '--owl', '-o', '-f'], foo='', owl=False) (python2 also works)
Argparse has broken support in a different way. I was using a text scrolling program someone wrote in python and came across this bug when I tried to pass it something to the effect of `-s --`. It ignored `--` as the argument to `-s` and assumed it meant the end of options, causing it to error out. It looks like it's searching for the `--` option first, without considering where it is in the argument list. Optparse can do this correctly but for some reason (Murphy's law?) it's deprecated.
This may be of some help: https://shreevatsa.files.wordpress.com/2008/03/bashstartupfiles1.png
`sed -i "s/mirror=/mirrors=/g"`
Shouldn't you use single quotes for sed? I know it doesn't matter in this case, but if there is an asterisk (pretty common in regular expressions), it might be expanded into something if you use double quotes instead of single quotes.
Asterisks are only expanded outside of quotes by POSIX-y shells. Personally, I almost always use double quotes. When it matters, I switch to single quotes.
I can't get it to work...are my variables correct (quotewise or other)?
You can put a single quote in a single-quoted string with the slightly akward ``'\''`` foo='text text '\'' text' Essentially you're ending one quoted string, adding a single escaped quote then starting another quoted string, but there's no extra character required for the string concatenation in shell (cf. Perl/PHP ``.``, java ``+``, etc). I don't think this is actually the entire problem you're having though, if you do this, the ``*`` will still be expanded, but during the ``echo``, not the assignment: foo='/* Here'\''s an example */' echo $foo But if you quote the argument to ``echo`` it will work as expected (double quotes only here, or the ``$foo`` won't be expanded): echo "$foo" You should also be fine to use this version, the single quote inside the double-quoted string is fair game: foo="/* Here's an example */" echo "$foo" For what it's worth, what you've encountered is actually the trick people will often use on a machine with a hosed filesystem, since ``echo`` is a shell builtin - ``echo /*`` will provide a workable analogue to ``ls`` on a machine where ``/bin/ls`` is otherwise unusable.
Try `-i''` or `-i ''`. Some systems, e.g. OS X, don't allow in-place sed replacements.
You need to show us the sample input from /etc/apt-fast.conf. I suspect you need to change the first line only to mirror="#MIRRORS=( 'none' )"
There are at least two problems with your code. &gt; mirror='#MIRRORS=( 'none' )' How do you expect the shell to know the second and third `'` should be literal quotes instead of syntactical? Change the outer quotes. mirror="#MIRRORS=( 'none' )" -- &gt; sed -i "s/$mirror/$mirrors/g" /etc/apt-fast.conf problem is that `mirrors` contain `/` characters, which you use as separators for the `s` command. So sed is seeing `s/blah/blah/blah/blah/blah/g`. How is sed supposed to know which `/` characters should be literal and which should separate the pattern from the replacement? If you know a character that won't be in either the pattern or the replacement, you can use that as the separator instead. E.g. `"s|$mirror|$mirrors|"`. It will also fail if either of the variables at some point may contain characters special in the pattern or replacement part. That's always an issue you have to consider when injecting data into code. Think Bobby Tables.
Don't forget that you can always use another program to generate the special characters in a string - including non-printing characters. Here are two examples, using 'tr' and 'printf' #!/bin/bash a="abc" b=$( echo "$a" | tr '[abc]' '[/*\047]') echo $b b=$( printf "/*\047" ) echo $b 
I would like something that people can just download and use, is there a way to load jq dependency? How would I do this in python? Any links to get started? 
/r/LearnProgramming may be a good start. I've enjoyed the [Learn &lt;language&gt; the hard way](http://learncodethehardway.org/) series. Then look up some guides and examples for using the python JSON library. I think you could embed a statically compiled jq in a shell script, but that extends beyond my level of wizardry.
Thank you so much, this worked!
anytime, glad to help
Would you please explain to me how this works ? How do you find out looking at the log a message with a faked from ? Thank you
You don't seem to have a sed substitution there. I think you probably want your sed command to be sed -i 's_# B3LYP/6-31G(d,p)_# B3LYP/6-31G(d,p) pop=NBO_' which will use the `s` substitute command, then use `_` as the delimiter between the search pattern and the replacement pattern. People often use `/` for that, but you have `/` in your patterns, so it's easier to switch than to try to escape them. So all together you could do it as find . -type f -exec sed -i 's_# B3LYP/6-31G(d,p)_# B3LYP/6-31G(d,p) pop=NBO_' {} \;
Sorry, that sed doesn't look like it would work on one file. Please post proof, e.g. $ cat foo.gjf foo bar # B3LYP/6-31G(d,p) bar xyzzy $ sed -i 's:# B3LYP/6-31G(d,p):&amp; pop=NBO:g' foo.gjf $ cat foo.gjf foo bar # B3LYP/6-31G(d,p) pop=NBO bar xyzzy $ Oh, and perhaps your formatting is hiding stuff? Indent "code" lines.
Don't use '/' as a sed command separator when you have a '/' in the string you are changing I would first make sure all of the files have well-behaved filenames. If you control this, then you can use a loop find . -name \*.gif | while read filename do sed -i 's:# B3LYP/6-31G(d,p):# B3LYP/6-31G(d,p) pop=NBO:g' "$filename" done 
I use emacs a lot, and I run a shell window inside of emacs. I can easily cut/paste past commands into a script, or keep notes of command output, etc. And or course I can search for specific commands, output, etc. You can always "echo !! &gt;newscript" You can also have a large bash history and a large scrollback in your terminal.
&gt; For your ur ls commands you need to add -1 to get a single file per line, ls by default is columns. AFAIK, "ls" uses columns when the output is a terminal. If it's a pipe, it outputs in a single column. 
Try this #/bin/bash for addr in "$@" do printf "========== $addr ========\nWHOIS Information:\n\n"; whois "$addr" | grep "NetRange\|CIDR\|NetName\|NetHandle\|Parent\|NetType\|OriginAS\|Organization\|OrgName\|OrgID\|Address\|City\|State\|Postal\|Country\|inetnum\|netname\|descr\:\|description\:\|country\:\|person\:\|address\:\|route\:\|origin\:\|source\:\|Domain Name\|Registrant Name\|Registrant Org\|Registrant City\|Registrant Country\|Name Server" printf "\nName Server Information: \n" nslookup "$addr" | grep -v \# done 
That hit the nail on the head, thank you! I was forgetting about doing a loop like that with $@. I appreciate everyone's input, I'm continuing to add on more functionality to it. :) 
Why not use a function? pathex() { cd e/"$example"/f ; }
Hey, I found your commands first but I don't know enough if stripping out the calls for user interaction would work. And I don't know if you can do share settings through terminal. Any insight? 
There is a fairly (at least as much as this sub I bet) sub at /r/osx that might help.
Woah! Thanks!
excellent. Thankyou
This was perfect! Thanks
[Here]( https://github.com/xiongchiamiov/osx-useradd)'s another one that mimics `useradd`.
The place to set MIN_UID and MAX_UID is /etc/login.defs. Oh, and you should check whether the user and home directory exists before attempting to create it.
http://linux.die.net/man/3/optind In practice, most people use ```shift $OPTIND``` to move $1 to after the arguments in the script they're making. For example here: http://neilhanlon.me/2015/05/20/parsing-apache-log-files-to-get-ip-addresses-of-high-request-users/ I use it to make it so I don't have to guess (or calculate) what position my positional arguments are at. I just move it to after the last option
 if [[ "$TORRENT_LABEL" == "Movie" ]]; then ut_label="movie" elif [[ "$TORRENT_LABEL" == "TV" ]]; then ut_label="tv" fi ... --def ut_label="$ut_label"
I'm not entirely sure if this is what you want. (I didn't really understand the purpose of line 22, 23). Let me know if this is missing something. #!/bin/bash export LANG=en_US.UTF-8 export LANGUAGE=en_US.UTF-8 export LC_CTYPE="en_US.UTF-8" TORRENT_PATH=$1 TORRENT_NAME=$2 TORRENT_LABEL=$3 # Subtitle language SUBLANG="en" SKIP_EXTRACT="n" MUSIC="y" # Only execute the script if we receive three arguments if (($# != 3)); then echo "This script must be given three arguments" exit fi # Apply TV or Movie label if [[ $TORRENT_LABEL == "TV" ]]; then CHOSEN_LABEL="tv" elif [[ $TORRENT_LABEL == "Movie" ]]; then CHOSEN_LABEL="movie" fi "$HOME"/filebot/filebot.sh -script "$HOME"/filebot/amc.groovy --def music=$MUSIC \ --def skipExtract=$SKIP_EXTRACT -non-strict --encoding utf8 --def ut_kind=multi \ --log all --log-file amc.log --def ut_dir="$TORRENT_PATH" ut_title="$TORRENT_NAME" \ --output "$HOME"/private/media --action symlink --conflict override --def subtitles=$SUBLANG \ --def artwork=false --def extractfolder="$HOME"/private/rtorrent/extracted --def ut_label=$CHOSEN_LABEL 
If you're going to be pedantic, _none_ of the double quotes in that snippet are required. Whether or not they are required doesn't make a difference to me. I also find it ironic that you're complaining about redundant quotes when you quoted `"TV"` and `"Movie"` unnecessarily in your code. 
I worked on this for a while last night. You don't seem to have set "cpuspeed" or "cputemp" as variables, so echo is just showing them as blank. Put a /n between both variables being called to have them print on separate lines. I wasn't able to get it to work properly, but this should at least be a start. I'll work on it some more after work if you don't have a solution posted by then.
&gt; It's also easier for most people to quote everything rather than risking unwanted whitespace-tokenization since a lot of people don't have an interest in learning all of Bash's quirks. That's a good point. Thanks for bringing that up. I'll follow that approach from now on.
 # Apply TV or Movie label if [[ $TORRENT_LABEL == "TV" ]]; then CHOSEN_LABEL="tv" elif [[ $TORRENT_LABEL == "Movie" ]]; then CHOSEN_LABEL="movie" else echo "Unsupported label" exit fi 
s/bash/GNU/
It works well with a 2D array (in bash, an associative array with a separator, like array[$i-$j]), but the diagonal stuff didn't come out very elegant. You can also do `tee &gt;(rev)`. #! /bin/bash dictionary=dictionary.txt puzzle='j s e t f l a l s f e l g a a n p l e p f d p k r e g e l a f n e t e n' #turn it into an 2D array declare -A parray IFS=$'\n' row=0 for line in $puzzle; do IFS=' ' read -a linearray &lt;&lt;&lt; "$line" for col in {0..5}; do parray[$row-$col]=${linearray[$col]} done ((row++)) done { #horizontal for ((i=0;i&lt;6;i++)); do for ((j=0;j&lt;6;j++)); do echo -n ${parray[$i-$j]} done echo done #vertical for ((j=0;j&lt;6;j++)); do for ((i=0;i&lt;6;i++)); do echo -n ${parray[$i-$j]} done echo done #diagonal from TR to TL for ((d=0;d&lt;6;d++)); do i=0 j=$d while [[ ${parray[$i-$j]} ]]; do echo -n "${parray[$i-$j]}" ((i++)) ((j--)) done echo done #diagonal from TR to BR for ((d=0;d&lt;6;d++)); do i=$d j=5 while [[ ${parray[$i-$j]} ]]; do echo -n "${parray[$i-$j]}" ((i--)) ((j--)) done echo done #diagonal from BR to BL minus the big diagonal already done above for ((d=5;d&gt;=1;d--)); do i=5 j=$d while [[ ${parray[$i-$j]} ]]; do echo -n "${parray[$i-$j]}" ((i--)) ((j++)) done echo done #diagonal from BL to TL minus the big diagonal already done above for ((d=5;d&gt;=1;d--)); do i=$d j=0 while [[ ${parray[$i-$j]} ]]; do echo -n "${parray[$i-$j]}" ((i++)) ((j++)) done echo done } | tee &gt;(rev) | grep -f $dictionary -o | sort -u Result: af als de eten fase gaan ge jager kan la lef leger na regel slapen te 
Mastering UNIX Shell Scripting by Randal K. Michael, published by Wiley.
$ HOME=/home/username java -jar buildtools.jar try it
I have been racking my head on this all weekend! Thank you! I had thought about making a home variable, but wasn't sure where to point it to, but that did the job!
What is the input? Are these 4 different times that are supplied from another source like a log file, or is this more like a homework assignment where you're given 4 times to loop through to get the for converted dates? Since this reads a bit like a homework assignment, I'll give you a hint of how I would do it, but not the answer. As with anything in bash you can do this a number of different ways, but I would suggest that you look into using a "for loop" to iterate through the UNIX timestamps, and insert them as variables in your date command. If you supply 4 time stamps the loop will go through 4 iterations of the command, until it has looped once for each supplied input. 
This is kind of a homework assignment. And thanks for helping out. So far I've come up with this: startdate=$(date -d @1325358001 +"%A %B %d, %T %Y %z" ) next=3600 for (( i=startdate; i &lt; startdate + 4*next; i+=next )); do date -d @1325358001 +"%A %B %d, %T %Y %z" done Although it is incorrect, I believe I'm going somewhere. I'm using a for loop with the hope of increments being displayed. I want the timezone offset and hours to be displayed from the epoch input of 1325358001 EDIT: Forgot to put date on the DO command
&gt;bash: ((: Sunday January 01, 08:00:01 2012 +1300: syntax error in expression (error token is "January 01, 08:00:01 2012 +1300") Do you want to run the same command 4 times or change the epoch time input each time? It's hard to tell what you actually want to do.
Recent bash versions can format dates with printf's `%()T` specifier, an example: $ epochtime=1446624201 $ printf '%(%A %B %d, %T %Y %z)T\n' "$epochtime" Wednesday November 04, 08:03:21 2015 +0000 Note that date -d '@1234...' works only in a few variants of date (GNU, busybox).
I will answer your questions though - the ls thing, I originally had some issues using this glob: for f in *.pdf; do pdftotext -layout $f; done I didn't figure out why at the time and realise now, hehe. So that's just me being dense. Loops - I read loops can be used for this kind of purpose if needed [see 11-11](http://tldp.org/LDP/abs/html/loops1.html), 11-18 also gives an example of a loop being used with cat. The alternative was use find or something interesting, I knew none of the files would have spaces so this was a bit of an easier way of doing something, know I know the glob is possible, seems a better idea. The farming for links thing, something I've never ever done before, so that was some use although I prefer the other way (listed in a response to this post). The extract thing, well that's a little more interesting, mostly I did it my way because I was lazy, but also because one of the files had tabs instead of spaces (at least I think this is the problem): sed -rn 's/There.*([0-9]+ in [0-9]+\.[0-9]+).*/\1/p' beginnersluck.pdf~2.txt cat beginnersluck.pdf~2.txt | grep "There" | grep '[0-9]\.[0-9]' | awk '{ print $4,$5,$6 }' 1 in 4.56 Although I'm sure this could be easily resolved with some better sed.
&gt; Loops - I read loops can be used for this kind of purpose if needed see 11-11, 11-18 also gives an example of a loop being used with cat. The alternative was use find or something interesting, I knew none of the files would have spaces so this was a bit of an easier way of doing something, know I know the glob is possible, seems a better idea. I don't really care what tldp says, they perpetuate plenty of questionable practices there and by doing so they cause a lot of pain and misery among people who don't yet understand how the shell works. Shell syntax is ancient and many good ideas implemented in it proved to not be so awesome in the long run. Now we know better and one of the lessons is don't parse goddamned LS or any other command in for loops. That you technically can, doesn't mean you should. It's simple - don't treat plaintext (which is what a command output or contents of a single variable are) as a ghetto array, especially in bash that supports legit arrays (posix sh is unusable trash imo). Whitespace splitting was a pants-on-head retarded idea that is responsible for countless shell scripting disasters. And if you really want to work on a per line basis, use while read while read -r shit; do ...; done &lt; file while read -r shit; do ...; done &lt; &lt;( some command | some other command ) while read -r shit; do ...; done &lt;&lt;&lt; "herestring" you can also use readarray to create a legit array using the same redirections &lt;, &lt; &lt;(), &lt;&lt;&lt; readarray -t my_arr &lt; file for item in "${my_arr[@]}"; do ...; done &gt; The farming for links thing, something I've never ever done before, so that was some use although I prefer the other way (listed in a response to this post). Well, i didn't either, i just took your idea and ran with it in a more kosher way. The reply was taking advantage of a shitload of fancy switches which made the direct cooperation with wget possible, I just wrapped your data source in a generic `while read` which is how you are supposed to read shit on a per line basis. 
 ping -c 3 Checking the man page doesn't hurt.
Thankyou!
FWIW; `split --help` or `man split` would answer this question.
Look into `read` for grabbing the user input, and then `case` to match the input to the proper to decide which response gets echoed. 
```nmap```
Quite a few things that I think could fail or that I would do differently, but which I will skip for now since I'm on my mobile. Assuming Reddit formatting didn't strip the characters in the formatting, I think your `ping` variable is going to be interpreted as a string rather than executing as a command in the line just after you define the ping variable. You may also want to check your variable definition for ping - it may be ok but it looks to me like it could use a set of quotes to lock that all together. 
I did add in the text that the help page was confusing me.
 input = $1 [ "${input}" == a ] &amp;&amp; \ (code) [ "${input}" == b ] &amp;&amp; \ (code) [ "${input}" == c ] &amp;&amp; \ (code) [ "${input}" == d ] &amp;&amp; \ (code) etc. I will probably be both downvoted and severely reprimanded/abused for showing you this method, because it is very old and is considered non-standard, and it also requires use of the test command; which is symlinked as "[" in the above code. I use it because of how simple I find it, however. This is a nearly extinct method as well, unfortunately; you probably will not be able to find a lot of other examples of it online. You can try looking up use of the "test" statement, however, because again, "[" is a symlink to "test" in most cases. Ideally you want to have your code for each case, either declared in a function before the series of tests, or in another script that you can simply call in place of where I have written "(code)". You can write your code there directly, however; but if you do, then instead of writing "fi" at the end of a block as you would for an "if" statement, you have to precede every command with "&amp;&amp;" which is actually a logical AND statement. The other reason why I prefer this method, is because the if and loop constructs in bash and other languages obscure and hide several parts of what the code is doing. Using logical operators directly, however, is much more similar to physical electronics. Because you are specifying everything that your code does so directly, for me it is much easier to debug, and I also get far fewer syntax errors, because I am not bound by built-in constructs whose code I am not familiar with, and which I don't completely understand the workings of. Understand; as long as you have the logical operators, a way of storing a number in a variable, and a way of incrementing that variable, then you are Turing complete and you can do whatever you want with that. Literally everything else that modern languages add is unnecessary in my opinion, and causes more problems than it solves, for the most part. As long as I have binaries or functions which are connected to the hardware or programs I need, and those are accessible from bash, then I can do whatever I want from bash itself. If I have a CGI interface, I can do Web scripting directly in bash as well, although I will be told not to do this because of "security." Security and the fear of terrorism are used as a false rationale for why we should not do a large number of potentially beneficial things. That is how we were originally intended to do things. The reason why it has changed is because Capitalism makes money from problems, not solutions. More recent methods are a degeneration, not a genuine step forward. Bash's only real drawbacks are its' slow execution speed, (which does not matter in most cases on modern hardware anyway) and the fact that it doesn't have a proper include statement. You also have to be very careful regarding how you partition things and split them up, but truthfully you should be doing that anyway. I am a functional programmer, and I truthfully do not like the object oriented paradigm at all. It creates abstractions that are not real, and covers everything with layers of confusion. I need things kept simple.
What was confusing about it? I can try to help.
man useradd man passwd man usermod
Usually you'd be ~~told~~ encouraged to look at the man page, but you've stated that you don't currently have access to a linux machine. Fortunately, the internet provides: http://linux.die.net/man/8/useradd Also, this could have been added to your original thread here: https://www.reddit.com/r/bash/comments/3rnx3y/script_to_automate_adding_users/
Wow thanks, I didn't realise there was a man online! Yeah I probably could've put it in that post.
might want to pipe the password into openssl to prevent errors with /etc/shadow and logging in later unless you are already encrypting the password in your script.
 adduser() { name="$1" shift sudo useradd "$name" &amp;&amp; sudo passwd "$name" &amp;&amp; sudo usermod -a -G "$@" "$name" } # usage: adduser username groupx groupy groupz Untested, off the top of my head on mobile.. It would prompt you for the password, so if you want something that will automatically generate a random password that'll make things a lot trickier..
I'm new to bash scripting. What does the `shift` line do? and what do the `&amp;&amp;`'s do? Also what is the `"$@"` do? How does this prompt me for an input (the `"$@"` maybe?). As I read this I take it to mean that is: * You pass in the line adduser, the username * It generates a user from the given name * Password is the same as their name * The "$@" gets input (maybe) for the group Correct? 
[This will explain shift better than me](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_07.html) $@ reads whatever is typed by the user. [Further reading on this and similar variables](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_02.html) (specifically section 3.2.5). The &amp;&amp; stands for "and, if previous command completed successfully." [Further reading command lists](http://www.gnu.org/software/bash/manual/bashref.html#Lists). The $1 is a positional variable which you can read about [here](http://tldp.org/LDP/abs/html/othertypesv.html) (positional parameter).
I've been trying to install cygwin for a while but my internet is t the best and always drops out. Is virtual box better?
Hi /u/geirha, thanks for your comments. &gt; Never use ps in a script to test if a process is running. Would you explain like I'm five? Also, I tried your replacement and it spits out the error `No such process', though the file output is good. &gt; I recommend not having your script rely on .bash_profile. Agreed. Thanks. &gt;It's misleading. How so?
&gt; &gt; Never use ps in a script to test if a process is running. &gt; Would you explain l like I'm five? Also, I tried your replacement and it spits out the error `No such process', though the file output is good. You run a command that reads and prints out a human readable version of the process table and look for processes that has a pid that is either equal to "$pid", or has "$pid" as a substring. But, even fixing that, you don't need an external command to output data about the process. You have the pid, you want to check if the process is still running, the shell provides. kill -0 "$pid" From the manual of the [kill(2) system call on linux](http://man7.org/linux/man-pages/man2/kill.2.html): If sig is 0, then no signal is sent, but error checking is still performed; this can be used to check for the existence of a process ID or process group ID. bash's kill builtin will output an error message saying No such process when the process is no longer there, so just add `2&gt;/dev/null` to ignore that message. See [Process Management](http://mywiki.wooledge.org/ProcessManagement) for a more thorough explanation of why using `ps` in scripts are generally a bad idea. &gt; It's misleading. How so? `.sh` extension is misleading since it suggests that the script is an sh scirpt, while it's really a bash script. I generally recommend not having an extension on a script at all, since it is redundant anyway, but if you insist, make it `.bash` instead.
If you want something interactive like that, then you will need to use the "read" command to get your input; but aside from that, yes that is correct. The "read" command basically sets up a loop where the program waits until you type something. I am truthfully not so familiar with that, because usually I will just run something from the command line, so for me it would look like:- ./runprogram.sh a With that, the letter "a" would get stored by default in the "${1}" variable. But in your case, the "read" command works. echo -n "Do you A. walk forward B. leave C. Turn left D. Turn right?" read input [ "${input}" == a ] &amp;&amp; \ echo "You die." I can't completely remember why echo needs the "-n" flag, but I'm pretty sure it does. I would also encourage you to get in the habit of surrounding variable names in braces and quotation marks, as I have above. &gt; The last time I coded was when i was 7, and I was using BASIC. I had an Apple IIc then. I think you will find Bash fairly similar to BASIC, although it is a little more fluid and less stultified. You don't need to prefix every line with a number here, as you would have back then. More generally, I would also strongly recommend purchasing the computer game *[Minecraft](https://minecraft.net/)* if you do not currently have it, as well; because you can learn to use the redstone circuits in said game, as a means of getting familiar with [logic gates](https://en.wikipedia.org/wiki/Logic_gate#Symbols) and the [truth table](https://en.wikipedia.org/wiki/Truth_table), which are the two main things that you will need to understand, as a programmer. If you want to potentially become employable as a computer programmer, then unfortunately you will probably need to learn the object oriented stuff as well; but if you know the above, and learn functional programming as well as you can, then that will make you a better object oriented programmer as well.
 paste -d" " host.txt src.txt backup.txt will give you the format you need for your rsync. It will combine your three files line by line with space as delimiter between the fields. You could write the result to a temporary file and work with it or do it directly in your backup script.
Yeah I can use sed to do the same thing but that isn't really what I wanted. 
Depending on the version of openssl, it may be limited to only md5. If you want to crank that up to SHA256 or SHA512, you need perl or python
Oh cool, so that would be more like what I want then. I'll look into it. However, I think I might just install Cygwin because I'm probably just going to setup a dual boot on my machine fairly soon for linux. Cygwin will do in the meantime.
This looks like a good use case for [GNU Parallel](https://www.gnu.org/software/parallel/). Look at the [tutorial](https://www.gnu.org/software/parallel/parallel_tutorial.html), specifically the -a or :::: syntax for getting arguments from files and -xapply option to get one argument from each input source in addition to 'Positional replacement strings'. I know this is a Bash subreddit but I thought it may be relevant to your use case. Apologies if this is the wrong place for it.
Hmm. What is the issue with my new update? I am open to all ideas and love to learn. Mind educating me? 
I ended up doing this before you posted I think. But my question now is this. I combined the three files and separate them using |. So ifs reads the file separate by that symbol and iterates each separation into its own variable. Why is that a bad solution? 
Don't put `.sh` extension on a bash script. It is misleading. Also, don't use `echo` with options, and don't use `[` in bash, read -rp 'Do you A. walk forward, B. leave, C. turn left, D. turn right? ' input case $input in [Aa]) printf 'You die.\n' ;; [Bb]) ... ;; esac
Teaching beginners to use broken code like `for i in $( ls )` and `for file in $FILES` does not fall under my definition of "contributing".
Hi /u/LanMalkieri If I understand correctly, something like this.. [ -t 0 ] || [ -t 1 ] is preferable to this [[ -t 0 || -t 1 ]] Is that what you mean? I was reading [this](http://mywiki.wooledge.org/BashPitfalls#if_.5B_.5B_a_.3D_b_.5D_.26.26_.5B_c_.3D_d_.5D_.5D.3B_then_...) article, though I'm not really sure what the older(?) way is preferable. Is this even what you were referring to?
While yes, you are technically right, it sounded like OP was wanting to share his code to ALL systems, in which case if it is a bash absent system [[ does not work. 
I was referring to [[ being a bash specific syntax and [ being a builtin linux program. 
&gt;#!/bin/bash sourcePath=$1 destPath=$2 numberArguments=$# Filedoc=$"*.doc" Filepdf=$"*.pdf" FilePDF=$"*.PDF" if [ $numberArguments -ne 2 ]; then echo "Usage ; dar doc_path archive_path" exit 1 fi if [ ! -d Redditdoc ] then echo Directory does not exist fi if [ ! -p Redditarc ] then mkdir /root/Redditdoc/Redditarc fi find "${sourcePath}" -name "$Filedoc" -exec cp -r {} "${destPath}" \; find "${sourcePath}" -name "$Filepdf" -exec cp -r {} "${destPath}" \; find "${sourcePath}" -name "$FilePDF" -exec cp -r {} "${destPath}" \;
I have not directly taught OP to do either of those things. In PM's, I have been guiding OP towards his goal without outright doing OP's homework. I'm happy for OP to forward our PM history to you for a second opinion though, because yours is one I respect.
Ah thanks. I was originally only going to only worry about it working with Bash on OSX, but, since part of me wants it to be more widely compatible, it would be better to take now. Advice heeded!
While acknowledging that you have the right to your positions here, I respectfully disagree. I base my own conclusions on what, in my own experience, is demonstrably valuable; not on conventions given to me by a third party. I would encourage the OP to perform his own testing, and reach his own conclusions.
No. If your shebang line says bash (e.g. `#!/bin/bash`), use bash syntax, use `[[`. If your shebang line says sh (e.g. `#!/bin/sh`), use sh syntax, don't use `[[`. Avoiding bash-specific syntax in a bash script is like avoiding C++-specific syntax in a C++ program; just in case you change your mind and want to use C instead...?
I escape anything that isn't standard alphanumeric. And if you're doing a new line inside another command such as sed or awk make sure you escape the backslash twice. Example: Say the file stuff has two lines in it with the words: One Two `cat stuff | awk '{print "echo -e \"line\\\n\"$1"}' | bash` That will output line one line two Certain things can't be escaped though. For example a regex string with negative lookup has an exclamation point in it '!' No matter how hard you try you can't escape that (I'm referring to a one liner, I haven't tried in a script). It's quite annoying. 
Perfectly sensible. Thanks.
A simple bash script I wrote for toggeling service daeomons (especially hostapd and shairport). Useful when using a webserver interface. Feel free to critic, I'm not used to submit my scripts for public use.
Not bad - I like your scripting style; it's nicely verbose (without obscure bash-isms and shorthand) which makes it easy to follow. This is a good habit to get into regardless of the language you use. I have perl code I wrote many years ago but I have NFI how it works, and right now, I'm too afraid to ask ;) However, if this (service toggling) is something you do regularly, I would've approached it differently: 1. Allow sudo to execute service commands without a password (man 5 sudoers) 2. Setup an alias in .bash_profile to save some typing (help alias) 3. Beer No scripting required except for an alias definition. Like most things in Linux...there's more than one way to skin a cat, and no one solution is necessarily perfect, but glad you shared your solution.
In that case, you should something like this cat datafile | awk '{ print "var 1",$1,"var 2",$2}' | sudo tee -a file_im_adding_to You don't want to use '&gt;&gt;' in these scenarios, as it can be unreliable. `tee` will add text to a file just like '&gt; file' and `tee -a` appends text to a file like '&gt;&gt;'. Also if you need to use sudo you can't use '&gt;&gt;' that's when using 'tee' comes in handy again. 
Quoting in bash is pretty horrible, but `echo` is your friend, and unless I'm mistaken, you can just use double quotes: export X=foo echo $(echo "${X}") To check that the double quotes are working right: echo -e 'has a b\nnot a.b' &gt; /tmp/foo echo $(grep "${X}" /tmp/foo) If you leave out the quotes echo $(grep ${X} /tmp/foo) `grep` thinks that 'b' is a file it's supposed to look at and will complain that there is not such file as b, and will also match both lines in /tmp/foo.
To my knowledge, you can't. Read: http://wiki.bash-hackers.org/syntax/quoting#strong_quoting Double quotes will do what you want, even with spaces in the string. if [ "$APPLICATION_ENV" = "testing" ]; then kill $(ps aux | grep "$PHPINI" | awk '{print $2}') else kill $(ps aux | grep "$PHPINI" | awk '{print $2}') fi Please notice though that you're grepping for the same string unless you define a different variable for the second grep. I know you didn't ask about this, but I'll offer some advice. Avoid using all uppercase variable names. All bash reserved variable names are uppercase. Unless you want to learn what all of those are, it's recommended that you use lowercase (or really anything but all uppercase) for variable names to avoid conflict. http://wiki.bash-hackers.org/scripting/style#variable_names I'm not entirely sure, but you may also run into a race condition if you don't remove your grep command from the grep output. Written more cleanly... if [ "$application_env" = "testing" ]; then kill $(ps aux | grep "$phpini_test" | grep -v grep | awk '{print $2}') else kill $(ps aux | grep "$phpini" | grep -v grep | awk '{print $2}') fi One last thing. What are the chances that multiple instances of php.bin or php are running? If there's only one instance, consider using pkill instead. if [ "$application_env" = "testing" ]; then pkill php.bin else pkill php fi EDIT: I lost a word.
The big problem you've got here is the exec on the find command won't report an error if it fails, so you've got no way of renaming the files. My usual technique would be if there's an issue with the copy, then do something different, but yeah - no way of detecting if everything worked ok - means a different approach is needed - have you been taught variables? Exit codes?
Say you have a file that is structured this way Alias1##commands here | more commands Alias2##another set of commands And so on You can do this: cat alias_file | awk -F'##' '{print "alias",$1"=\""$2"\""}' | sudo tee -a ~/.zshrc Would add them all to the bottom of your zshrc file 
In my script there are about 40 CLI PHP scripts that need to run as separate background scripts... is there an easy way for me to run and kill all those scripts easily? I have something right now using eval but because eval is a little evil, there has to be a better way haha
Didn't use the word scripts enough.. maybe you know of some scripts to help me with my scripts
The thing with pgrep/pkill is that it'll return more than one pid if there are multiple processes that match what you're looking for. You can add the `-f` option (I'll let you look that up in `man pgrep` yourself) to reduce it down a bit, but you should be careful &amp; make sure you match only one process. Alternatively (and safer too really), make your script drop [pid files](https://stackoverflow.com/questions/8296170/what-is-a-pid-file-and-what-does-it-contain) (as stated by /u/mfvo) within the application so you know exactly which one you're killing. Just make sure no one can overwrite the pid files. That said, if you want to test pkill &amp; what it'd actually kill, just run the same command with pgrep instead. That'll output the matching pids.
&gt; sed -i '/^#/!{/regex/d}' /etc/sudoers That looks like it will remove all...something from the lines, but I don't want to do that. I still want to iterate over. (I may just be reading that wrong)
no, that's what it would do, you read it correctly. It would go through every line and return files that aren't commented out and from those it would delete whatever matches the regex. 
grep -v ^[^#] /etc/sudoers
I don't think sed modifies the file, but you could append &gt; tempfile.txt just to be safe.
``egrep -v '^#' /etc/sudoers``
sed -i does actually modify the file.
That leaves in blank lines. If there are a lot of blank lines, I like to hide those as well with an extra grep: grep -v '\^#' /etc/sudoers | grep .
thanks for this, I have always piped to grep -v '^$' but your way is less typing
The first two function declarations are fine, the third one isn't. Use either of the first two. The unexpected end of file error is coming because of "else if" in bash, use "elif" instead. That should be enough to get you running!
&gt; line 42: syntax error: unexpected end of file This is the real error, and has nothing to do with your function. You used `else if`(which requires an extra `fi`) instead of `elif`. Run `help if` to get a quick reminder of the syntax. Also, don't name your script `sbox.sh`. It's a bash script, not an sh script.
Couple of problems with this segment of code: if [[ $1 == "-y" ]] ; then sboxON; else if [[ $1 == "-n" ]] ; then sboxOFF; else echo 'usage: sbox [ option ] (-y [on] OR -n [off])'; echo; fi I'll rewrite it with inline comments below: if [[ "${1}" == "-y" ]] ; then # Always put quotes around variables; if they're empty or # undefined, you'll get a syntax error if you don't # do this: # if [[ == "-y" ]] is not valid, whereas # if [[ "" == "-y" ]] is. # Also, I am an evangelist of the habit of always using # curly braces for variable references; it makes it more # easy to modify the script if you need to do anything # like array references ( ${ARRAY[4]} ) and the like sboxON # no semicolon needed here elif [[ "${1}" == "-n" ]] ; then # else if is a new statement; use elif here, otherwise # you'll need another fi sboxOFF # No semicolon here else echo 'usage: sbox [ option ] (-y [on] OR -n [off])' # no ; echo # no ; fi Also, in the functions themselves, rather than doing echo "line" &gt; ${file} echo "line2" &gt;&gt; ${file} it's much more readable and understandable to do something like cat &gt; filename &lt;&lt; EOF Line 1 Line 2 EOF
Thank you for taking the time to share the additional tips. I used every one and it works!
Happy to have helped!
I'd go for this one: awk '!/^#/ &amp;&amp; !/^$/' /etc/sudoers will skip empty and commented out lines
 PS3="Your choice? &gt;" select choice in "Do A" "Do B" "Do C" "Do D"; do case ${choice} in "Do A") echo "You die." exit 1 ;; "Do B") echo "You go on to live another day." break ;; "Do C") echo "You die." exit 1 ;; "Do D") echo "You die." exit 1 ;; esac done
It looks like you should be able to do this: save bclock into bclock.pl and make executable. Substitute bclock.pl for date +%r in your rclock script.
OK, so read the script and break it down. Look at bclock -- what is it, it's a for loop (in perl) that prints the time in binary , then sleeps 1 second. So just strip the print out, you get: perl -e '@d=split("",`date +%H%M%S`);print"\r";for(0..5){printf" %.4b ",$d[$_]}' You can also store it into a variable then print it, just so you see how that works: perl -e '@x="";@d=split("",`date +%H%M%S`);for(0..6){@x=sprintf("%s%.4b ",@x,$d[$_])};print @x' OK, so now just throw that into the part of rclock that does the date print. So instead of echoing the date (formatted with some color codes, etc) you'd run the perl command. while sleep 1;do tput sc;tput cup 0 $(($(tput cols)-44));perl -e '@d=split("",`date +%H%M%S`);for(0..5){printf" %.4b ",$d[$_]};';tput rc;done I took out the &amp; (background) just to toy with it, so obviously throw that back in and whatever else you'd like. Note I had to change the number of columns shift to fit my terminal, so you may have to adjust that. Also, what I would suggest is probably to make an alias to do just the print, (no loop), for whatever formats you like. Then you can in another alias that runs the loop run all/any/either of those other aliases. And so OOP begins...
Definitely - but take out the indefinite for loop/sleep, if one were to do it that way.
This is really neat. Is there a way to do this withought `youtube-dl`? Like with say… wget or curl? 
And four spaces before lines of code like this
I made a prettier binary clock that prints a vertical column of 4 characters to represent the number (2 columns per number like you did) and changing color as it prints hour/minute/second: date +%H@%M@%S | sed 's/@/[@]n/g;s/[0-9]/[0000]n&amp;n[ ]n/g;s/^/2o/' | dc | sed 's/1/█/g;s/0/░/g;s/^/\x1b[31m/;s/@/\x1b[32m/;s/@/\x1b[33m/;s/░*\([█░]\)\([█░]\)\([█░]\)\([█░]\) /\1\x1b[B\x1b[D\2\x1b[B\x1b[D\3\x1b[B\x1b[D\4\x1b[3A/g' It's quite clunky moving the cursor a lot, and it doesn't quite fit in the top right corner, seeming to print too much (edit: in a new terminal it doesn't print too much, but it does need to be 1 distance away from the right edge): while sleep 1; do tput sc; tput cup 0 $(($(tput cols)-7));date +%H@%M@%S | sed 's/@/[@]n/g;s/[0-9]/[0000]n&amp;n[ ]n/g;s/^/2o/' | dc | sed 's/1/█/g;s/0/░/g;s/^/\x1b[31m/;s/@/\x1b[32m/;s/@/\x1b[33m/;s/░*\([█░]\)\([█░]\)\([█░]\)\([█░]\) /\1\x1b[B\x1b[D\2\x1b[B\x1b[D\3\x1b[B\x1b[D\4\x1b[3A/g'; tput rc; done
I tend to like the Advanced Bash Scripting Guide, but people should be aware that the examples have really bad practices sometimes. Take what you learn from there with a grain of salt!
&gt; Advanced Bash Scripting Guide. Thank you very much, this is exactly what I needed. 
Which distro are you on? They do networking in slightly different ways, but most/all should ask for DHCP re-lease after waking from sleep.
examples?
Use of test, for starters; or the test alias. In Bash, I like doing things such as:- count = 3 function next() { count = $(($count+1)) [ $count -lt 4 ] &amp;&amp; \ main() [ $count -eq 4 ] &amp;&amp; \ exit 0 } function main() { [ $count = 1 ] &amp;&amp; \ func1() # Will be a function or script defined elsewhere [ $count = 2 ] &amp;&amp; \ func2() [ $count = 3 ] &amp;&amp; \ func3() next() } You can probably see how clean this looks; but the other reason why I like it, is because what I realised is that pre-written if and loop constructs aren't actually necessary. This is similar to how things are done in assembly, as well. As long as I have the logical operators, a way to store a variable, and a counter to increment said variable, then I don't actually need pre-written control structures, and I've found that I can avoid syntax errors and other problems due to using pre-written statements whose code I can't see. This can be done in C. My anti-OOP fanaticism won't really allow me to look into Java, but assuming it still has support for basic functions, then I could do it there as well. I've noticed that programmers here HATE anything simple, though. Anything non-object oriented, or anything without loads of edge casing or templating shit everywhere, and people react as though they've seen a ghost. It honestly terrifies people if I show them things like this. We live in a fear-based society at this point, so there is endless obsession with coding "defensively," when a lot of the time it is just edge casing crap, which doesn't accomplish anything more than making code look unreadable. If I am afraid of anything, it is how people will react when I write posts like this. I don't like technical debt. I avoid "frameworks" like the plague that they are; because we're talking about a giant pile of bugs on top of bugs on top of bugs, where no one cares how badly anything gets written, or how painful it is to write, as long as it superficially appears to function, and the psychopathic, demonic ***suits*** that are issuing orders are satisfied. I will never get a job as a programmer, because commercial programming is never about what genuinely works well. It's about making middle management happy. I also detest object oriented programming in general, and especially the fact that OOP programmers seem to be desperate to completely exterminate any other paradigm. I've never seen OOP used anywhere, where it didn't make things more complex than they needed to be. Commercial programmers are trained not to care about underlying complexity, as long as they've got the equivalent of lego blocks at the level they have to deal with; again, they don't care about the mountains of poorly written shit that might be buried underneath. Then you get security exploits, and viruses etc...it's a mess.
Physical and remote access security as entertainment. I'd rather learn this way than something more malicious. 
thanks for the quick reply....we are able to use ssh keys... basically i would like the user to be able to stop it whenever they want to. would that be possible? or would there have to be a separate script to do that?
Hmm needs more context I think. Are you trying to give a user the ability to login and do a tcpdump only, for as long as they need to? Are you really after a restricted login that allows a user to do only what you select and nothing else?
Not completely sure what your goal is, but perhaps this can lead you down the path. This script will log into both two defined servers, launch tcpdump and you press CTRL-C to stop it running on both servers: #!/bin/bash # # # function to stop the tcpdump process on both servers stopDump() { echo echo "Stopping TCPDump processes." ssh user@host1 'pkill tcpdump' ssh user@host2 'pkill tcpdump' } # set trap for CTRL-C to stop tcpdump trap stopDump INT # launch tcpdump on both servers using subshells and wait for the # CTRL-C to stop echo "Starting TCPDump processes. Press CTRL-C to stop." while :; do (ssh user@host1 'tpdump -n dst host ip.address.here.00 or dst host another.ip.address.here -w namefile.pcap' &amp;&gt;/dev/null) &amp; (ssh user@host2 'tpdump -n dst host ip.address.here.00 or dst host another.ip.address.here -w namefile.pcap' &amp;&gt;/dev/null) &amp; wait break done # done exit 0 
thanks! so by Ctrl+c ending this script, that will also end the tcpdump on the 2 servers? sorry I told you I am a noob.
I work for a hosting company and for obvious reasons your laptop has to be locked if you're not within visual range. New starters sometimes take a while to get used to this so they quite often find that their volume changes randomly, or the cd drive occasionally ejects itself. They soon learn. 
I often apply wallpaper that's likely to be disliked. 
Satan.
amateur!
One way, tested briefly: #!/bin/bash INPUT=test.csv OLDIFS=$IFS IFS=, [ ! -f $INPUT ] &amp;&amp; { echo "$INPUT file not found"; exit 99; } while read LINE do ARIN=(${LINE//;/ }) dfs=""; long=""; vel=""; pop=""; mlo=""; for i in "${ARIN[@]}" do if [[ "$i" == *"date_first_seen"* ]] then dfs="${i#*:}" fi if [[ "$i" == *"longevity"* ]] then long="${i#*:}"; fi if [[ "$i" == *"velocity"* ]] then vel="${i#*:}"; fi if [[ "$i" == *"popularity"* ]] then pop="${i#*:}"; fi if [[ "$i" == *"month_last_open"* ]] then mlo="${i#*:}"; fi done echo "$dfs ,$long ,$vel ,$pop ,$mlo" done &lt; $INPUT IFS=$OLDIFS
This might get you on the right track: I put the data into date.txt awk -F[:,] '{ print $(NF-4), $(NF-2), $NF}' data.txt "2011-03-20" 3 4 "2011-03-20" 3 5 "2012-05-19" 3 2 "now" 0 0 
This was a quick pass in python3 usage = """parser.py inputfile.csv""" import csv from argparse import ArgumentParser class Fields: date_seen = "date_first_seen" longevity = "Longevity" velocity = "velocity" popularity = "popularity" last_open = "month_last_open" __all__ = [date_seen,longevity,velocity,popularity,last_open] def main(): parser = ArgumentParser(usage = usage) parser.add_argument(dest = 'fin') results = parser.parse_args() for field in Fields.__all__: print(field,end=', ') print() with open(results.fin) as fin: reader = csv.reader(fin) for row in reader: d = dict((field,'') for field in Fields.__all__) for elt in row: if elt: field,value = elt.split(':') d[field] = value for field in Fields.__all__: print(d[field],end=', ') print() if __name__ == """__main__""": main() It doesn't do any form of input validation (although that could be easily added) and expects a file input, although, it could pretty easily be changed to receive input on stdin. 
Bash Floating Numbers Comparison
Oh, no worries. Copy the above file to `/etc/systemd/system/i3lock.service` and replace the `ExecStart` contents with `/usr/bin/i3lock` and `User` with your user. Then enable with: `sudo systemctl enable i3lock`. The trick is `Before=sleep.target`. That ensures this unit runs every time `sleep.target` is invoked, i.e. when the machine is put to sleep. Hope this works! :)
I will give you a non-standard answer, which someone else will probably try and tell you not to listen to; but I would encourage you to test it yourself before discarding it. I would save each entry in its' own file, with the date first seen as the filename:- date-first-seen+2011-03-20+.txt The plus sign is used as a field seperator, with the minus sign used as a space substitute, or seperator within each field. The file extension is also in its' own field. This lets me manipulate filenames themselves with either cut or IFS if I need to. Don't have quotation marks anywhere in data itself, because they are used around variables. Then inside the file itself, you have:- longevity+3 velocity+0 popularity+4 month-last-open+2015-05 Then, to generate my report from each file:- for i in $(ls *+.txt) do date = $(echo "${i}" | \ cut -d'+' -f2) long = $(sed -n "/longevity/p" | \ cut -d'+' -f2) vel = $(sed -n "/velocity/p" | \ cut -d'+' -f2) pop = $(sed -n "/popularity/p" | \ cut -d'+' -f2) mlo = $(sed -n "/month-last-open/p" | \ cut -d'+' -f2) cat &lt;&lt;"EOF" &gt;&gt; report+.txt Report for object first seen on "${date}". Longevity for this object was "${long}". Velocity for this object was "${vel}". Popularity of this object was "${pop}". This object's file was last open on "${mlo}". EOF done I haven't tested this specific script, although I know it will work for the most part. There might be a syntax bug relating to the quoting of EOF in particular, but I think it's right.
what bad practices ? If by recursion you mean the find drilling down, then yes as the supplied directories are top level and there are a couple of levels deep before the 'targets' are hit. The full path generally looks like /mnt/winshare/imagestore/SCANS/SECTION1030/blahblah12345. The loop extracts the section name and the idnumber for each location
What is a file? Where is the name of a file stored? What is a directory? Where is a directory stored? Does it actually take up space on disk? what does "ls" or any other utility that calculates file size actually measure? Google 'inode'
&gt; what bad practices ? Mainly * arguments for simple commands containing unquoted parameter expansions, like `$1` and `$adsid` * treating pathnames as lines * embedding data into `printf`'s format string. &gt; If by recursion you mean the find drilling down, then yes as the supplied directories are top level and there are a couple of levels deep before the 'targets' are hit. The full path generally looks like /mnt/winshare/imagestore/SCANS/SECTION1030/blahblah12345. The loop extracts the section name and the idnumber for each location Ok, so do you really need recursion? it looks like they'll always be at the same level, in which case some for-loops may do. E.g.: cd /mnt/winshare/imagestore/job/SCANS/ &amp;&amp; for section in SECTION1[0-9][0-9][0-9]; do [[ -d $section ]] || continue printf 'section: %s\n' "$section" for dir in "$section"/*1[0-9][0-9][0-9][0-9]; do [[ -d $dir ]] || continue printf 'adsid: %s\n' "${dir:(-5)}" done done
From 'man bash': until list-1; do list-2; done The while command continuously executes the list list-2 as long as the last command in the list list-1 returns an exit status of zero. The until command is identical to the while command, except that the test is negated; list-2 is executed as long as the last command in list-1 returns a non-zero exit status. The exit status of the while and until commands is the exit status of the last command executed in list-2, or zero if none was executed.
filename is given by the filesystem.
ok thanks. 
Its by the command ls -al
&gt;What is a file? A file in unix is just a stream of bytes on a disk. However, those bytes are just the contents of the file. As far as info about the file - like its owner, permissions, and other info) - There are blocks in the filesystem dedicated to storing that info, called *inodes*. They're created when the filesystem is created, and ls -l doesn't count them when it reports file size. Incidentally, the name of the file isn't stored in an inode. The parent directory's file - which again, is just a stream of bytes on disk - is a list of files in the directory. So yeah, you can have an empty file occupying no blocks, that takes up no space on disk because utils like df and ls don't count inodes. 
Linuxacademy.com is pretty good for bash.
Please give us more detail when posting. We are not here to do the detective work or heavy lifting for you (those of us who aren't dolts anyway). If you want some help you'll have to answer some questions so we have more information: * What were you able to learn in a few hours? * What exactly about the script do you not understand? &gt; i'm still confused as to what a lot of the lines do This tells me nothing *and leaves everything open to interpretation*. This is what I hear when I read these kind of posts: &gt;"*I spent 3 hours looking at this rather simple script and ran out of energy searching on Google for answers. Please do my work for me and explain what this script is doing.*" Looks like this is a job for RTFM. 
this metadata does take up space in the file system, but it's not part of the file itself. you can stat the file to find out some things. # create empty file : &gt; zero $ stat zero File: ‘zero’ Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: 805h/2053d Inode: 129368327 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/ badcode) Gid: ( 1000/ badcode) Access: 2015-11-19 22:38:27.056736374 -0600 Modify: 2015-11-19 22:41:04.835465792 -0600 Change: 2015-11-19 22:41:04.835465792 -0600 Birth: - it does exist in the file system as an inode, where that inode points to happens to be nowhere yet
Probably the most useless builtin in Bash. 
another way to fix the issue of progs gobbling the stdin and breaking the loop is to use a nonstandard file descriptor to drive the loop while read -u3 x; do echo "$x"; done 3&lt; file here loop gets its own fd #3 which doesn't collide with the default stdin fd 
I'm not entirely sure what you mean for problem 1. Should it just be: "while getopts tdoh opt;"? Also, does getting rid of "-" apply for t, o and h as well?
Something like this PROMPT_COMMAND='echo -ne "\033]0;${USER}@${HOSTNAME}: ${PWD}\007"' Where specifically would I put this. In bashrc on my laptop, or on the jump host? 
Scripting languages **are** programming languages. What are you *actually* trying to figure out?
am not trying to start a fight. I already looked at google, some people are saying it's not a programming language 
`bash` is a shell and can be used as a user interface $ echo "bash is a shell" But it's also a shell scripting language: #!/bin/bash echo "shell script is a legit programming language" You can even run `bash` as a shell from your current `bash` shell. Just type `bash` at the command prompt.
Let's go back to what programming actually means for computers. It's literally giving the computer a set of instructions for it to carry out a task. So, Bash is a programming language. The only difference is whether you use an interpreter or a compiler, but it's not like writing software in VB and compiling it is somehow nobler than using Bash and your friend probably considers that a programming language. It's all syntax man. 
There are also many ways to [set up bookmarks](https://github.com/mcstafford-git/bm). I have many names set by symbolic link and they use tab completion.
It shouldn't be too different than updating a database with a network based API; not much harder than any other computers project. Seriously, you need to include more details. How's the API work? Is it HTTP/REST based; or is it an RPC mechanism over HTTP\*? What do you want the interface to look like? Is this a tool for automation, or a tool that you'll use at the CLI? *What do you want it to do?* I can tell you that you'll probably want something like xmlstarlet, and curl/wget/netcat/socat/`/dev/tcp`, but nothing meaningful without more details. \* IMO, a silly idea because HTTP is already an RPC mechanism, but it's popular anyway.
I am assuming the xml based api exists. Just use something like curl to hit the API with the information you want.
Or just use aliases in your .profile alias proj="cd ~/MyProject" The you can just: $ proj
I wrote up something similar a few weeks ago: https://gist.github.com/shingkai/3a790850cf5af3b5df70 It supports marking, jumping, listing bookmarks, saving bookmarks, loading bookmarks, clearing saved bookmarks, and tab completion. Saving and loading bookmarks are useful for multiple sessions at once. Some parts are kinda hacky, so input would be greatly appreciated :)
Practical example based on the butt fetishist toker's assumptions + the IMNSHO best practice of keeping xml data in a separate file: curl -v -X POST \ -H "Content-Type: text/xml;charset=UTF-8" \ -d @/path/to_my/file.xml \ http://the_website/the_api_path 
Where did you read anything about a null operation needing 4 spaces anywhere? EOF isn't required and it's called a limit string. It can be any string as long as it doesn't occur in the command list (e.g. comments).
Use the editor: - Open a new editor tab. - Select all the lines you want to comment out. Cut and paste to the new tab. - Replace '\n' by '\n#' in the new tab. - Cut and paste back to the original document. Note: It is only useful if you need to comment out many lines.
Use an editor would be my suggestion as well, but not with this method. Every modern editor has a comment-out feature and it works on multiple lines.
I've often seen: if false ; then commented out code spanning several lines fi You may want to leave a comment explaining why this was done, as it could confuse people.
I generally use this tool: https://github.com/huyng/bashmarks Very short commands, easy to create new "bashmarks".
I believe it is RPC mechanism and it is cloud based. 
Yes, you could use curl or wget to pull a file/data from another source, and then post that back with curl. If you can get the payload from the api in format you need for the second request, just stick it in a variable and send it back over.
Well I'll be damned, thx. Was half asleep didn't even realize what you were talking about. Feel like a dick now my bad.
If awk is a programing language, bash is for sure. Take a look at todo.txt, bashblog, and bashpod.
If you happen to use vim, you can use the [vim-commentary](https://github.com/tpope/vim-commentary) plugin. I use it almost daily to comment and uncomment out multiple lines at once. It takes a motion, or a visual selection, then will add the correct comment markers (or remove them) depending on the file type. Super fast and easy to use...if you are somewhat comfortable in vim.
I `did not know that` about the backticks. `that will come in handy`.
Thanks for your appreciation..
http://blog.sanctum.geek.nz/bash-process-substitution/ See the 5th example, which uses tee to send command output to multiple outputs. One to stdout and one that greps for the date perhaps. Alternatively: $ awk '{ print $2 &gt; "somefile" ; print $1 &gt; "somefile2" }' inputfile echo "$cmdBattery" | awk -v date="$(date +%Y-%m-%d' '%H:%M:%S)" ' /"CycleCount"/ { cycleCount = $3 } /"CurrentCapacity"/ { currentCapacity = $3 } /"MaxCapacity"/ { maxCapacity = $3 } /"DesignCapacity"/ { designCapacity = $3 } /"ExternalConnected"/ { externalConnected = $3 } END { print "Cycles: " cycleCount " | Charging: " externalConnected "\nCapacity: " currentCapacity " / " maxCapacity " / " designCapacity " mAh" ; OFS=","; print date, cycleCount, designCapacity, maxCapacity, currentCapacity, externalConnected &gt; "Files/battery-log.csv" ; };
How could the code you presented be applied to the code in the OP?
Bash has no notion of floats. If you have to deal with floats use a proper language. Clarification: These things seem easy to do at first but will only work for the most trivial examples. .23 would not be recognized 10e3 would not be recognized and so on. If you really have to do computations in Bash use ints and use printf for outputting the value as a float.
What about something like this? It works perfectly as expected, but would it be more inefficient than the other ideas presented here? cmdBattery=$(ioreg -n AppleSmartBattery -r) date=$(date +%Y-%m-%d' '%H:%M:%S) cycleCount=$(echo "$cmdBattery" | awk '/"CycleCount"/ {print $3}') currentCapacity=$(echo "$cmdBattery" | awk '/"CurrentCapacity"/ {print $3}') maxCapacity=$(echo "$cmdBattery" | awk '/"MaxCapacity"/ {print $3}') designCapacity=$(echo "$cmdBattery" | awk '/"DesignCapacity"/ {print $3}') externalConnected=$(echo "$cmdBattery" | awk '/"ExternalConnected"/ {print $3}') printf "Cycles: %s | Charging: %s\nCapacity: %s / %s / %s mAh" \ "$cycleCount" \ "$externalConnected" \ "$currentCapacity" \ "$maxCapacity" \ "$designCapacity" printf "%s,%s,%s,%s,%s,%s\n" \ "$date" \ "$cycleCount" \ "$designCapacity" \ "$maxCapacity" \ "$currentCapacity" \ "$externalConnected" \ &gt;&gt; ~/Documents/Other\ Files/battery-log.csv Paging previous posters: /u/annoyed_freelancer, /u/turnipsoup
What's wrong with using shell for time-sensitive work? And well, I found this code online and I'm just trying to make it fit my needs. I guess 40ms vs 56ms isn't distinguishable, but I'd like to have faster execution time if I can't have the cleanest possible code.
Thanks for your time sir. In case anyone really wants to do real maths things in linux, bash is not right choice. we can easily use Python (even more easy than bash). This is only for simple and easy approaches that most of time we like to resolve. --Thanks ..
I'm afraid I don't understand this. What's this supposed to be?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/awk] [Is there a way to not manual ctrl+D after the input? : repost \/r\/bash](https://np.reddit.com/r/awk/comments/3tx9ph/is_there_a_way_to_not_manual_ctrld_after_the/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
but it was me... i already have had voted by default when reposting to see if i could also get help from there.... oh buckets
Showing the syntax for doing both a print and an output to file, of two differing strings. i.e; there's three lines in the file as indicated in my first example. The second example, first matches against /something/ (the first line) and prints it. It then matches against /some tasty/ and outputs the matched line to 'somefile'. The third example is me cat'ing the file created by '/some tasty/ { print &gt; "somefile" }'. In short, you can do what you're after with a single awk command by doing: echo $yourvar | awk '/yourmatches/ { yourvar=$3 } /morematches/ { anothervar=$3 } END { print youroutput } { print your-other-output &gt; "somefile" }' If you can pastebin me the raw output of ioreg you're using to make the variable, I'd be happy to fix your original awk into a single line.
So I put the contents into a file, batteryinputfile - but your method with the variable will still work. :~$ awk -v date="$(date +%Y-%m-%d' '%H:%M:%S)" ' /"CycleCount"/ { cycleCount = $3 } /"CurrentCapacity"/ { currentCapacity = $3 } /"MaxCapacity"/ { maxCapacity = $3 } /"DesignCapacity"/ { designCapacity = $3 } /"ExternalConnected"/ { externalConnected = $3 } END { print "Cycles: " cycleCount " | Charging: " externalConnected "\nCapacity: " currentCapacity " / " maxCapacity " / " designCapacity " mAh" } END { OFS=","; print date, cycleCount, designCapacity, maxCapacity, currentCapacity, externalConnected &gt; "outputfile" }' batteryinputfile Cycles: 211 | Charging: No Capacity: 3907 / 6839 / 7150 mAh :~$ cat outputfile 2015-11-23 10:56:00,211,7150,6839,3907,No Seems it also needed two END statements, one for each output. Not sure why the original version didn't work. 
Nice. Seconds ago I just figured out the same thing about the syntax for having multiple END statements. It's ridiculous though, I've been Googling high and low and I've never come across any documentation or example that showed this exactly—I've only read statements like "Multiple END blocks are allowed". And another complicating matter is my use of `~` for signifying the Home folder, which it seems like `awk` doesn't understand. This was the kind of answer I was expecting to get from here. Well, thanks for helping and seeing things through!
that returned the reddit.com/subreddits page, when a name was entered it opened the name into a seperate tab, but i did alil reading and actually don't need awk, figured out how to use read to do the same thing, and read will take one line and roll with it _________ #!/bin/bash read sub echo "firefox reddit.com/r/$sub &amp;" | tee subscript.sh chmod 777 subscript.sh ; ./subscript.sh ; sleep 5 ; rm ./subscript.sh _______ it does something
Aliases cannot use the positional parameters (`$1` etc.). You want a shell function instead. r() { firefox "https://www.reddit.com/r/$1" &amp; } *edit:* Don't forget to unalias your alias if you've already set it or it'll override the function. 
&gt; r() { firefox "https://www.reddit.com/r/$1" &amp; } that doesnt take input though EDIT: how'd i get downvoated, it doesn't take input. like i ran this, it openned reddit.com/subreddits/ and that's not what i was going for... nevermind me, atleast i figured out how to do it
It is an XML based API to confirm
Yeah, you're right. Shouldn't smash out a solution before bed!
Purely in BASH, unfortunately not. If you would like the script to connect and authenticate your easiest bet is to set up SSH key authentication. You can read how to set it up here: https://www.debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password Alternatively, you can use Expect (/usr/bin/expect) so your script will wait for the "Password:" prompt and enter the pass, and then execute your command. You can read on how to do this here: http://stackoverflow.com/questions/4780893/use-expect-in-bash-script-to-provide-password-to-ssh-command The drawback of the second method is that you will have your password unencrypted in the file.
Try this: `$ ssh -t yourserver "$(&lt;your_script)"` or this: `ssh -t host4 bash ./run_audit.sh`
Perhaps a here document can work here. ssh user@host2.domain.edu &lt;&lt; _EOF_ #do things here exit _EOF_ 
I use something very similar with telnet. I should have thought to paste it here! :D
i'm gonna start using that, it's a good way to do nothing unsuccessfully..
it's because of the ' right? That's the part that would have unpaired quotes and cause things to go all crazy, if i get what you're saying right. i used it in one i was just righting like this if false ; then echo $t echo $sm echo $d fi and i guess i could have just uncommented but i'm not going to be editting it and i figure it'll be in there if i ever remember to use it. but i can imagin it would have helped last night while i was commenting and uncommenting things, if you change it to true it reads the lines right?
I had these bookmarked. http://www.tldp.org/LDP/abs/html/refcards.html http://www.davidpashley.com/articles/writing-robust-shell-scripts/
You're probably getting a newline or whitespace in there. You can error check using awk and sed. Here's an example of a script that does that: http://pastebin.com/c6hUFpDi
Ima retard. The variable LASTIP was calling a command and I was using single quotes instead of backward ticks.
Cleaned up: #!/usr/bin/env bash # check and send ip address to email MYIP=$(wget "http://ipinfo.io/ip" -qO -); TIME=$(date); LASTIPFILE="/home/name/bin/.last_ip_addr"; LASTIP=$(cat "${LASTIPFILE}"); if [[ "${MYIP}" != "${LASTIP}" ]]; then echo "New IP = ${MYIP}" echo "sending email.." echo -e "Hello\n\nTimestamp=${TIME}\nIP=${MYIP}\n\nBye" | \ /usr/bin/mail -s "[INFO] New IP" name@icloud.com echo "${MYIP}" &gt; "${LASTIPFILE}" else echo "no IP change!" fi 
Thanks!
Much better. I'd also replace `echo` with `printf`, and a heredoc for the mail, and lowercase all the variables. #!/usr/bin/env bash # check and send ip address to email myip=$(wget "http://ipinfo.io/ip" -qO -); lastipfile=~/bin/.last_ip_addr read -r lastip &lt; "$lastipfile" if [[ "$myip" = "$lastip" ]]; then printf 'no IP change\n' exit fi printf 'New IP = %s\n' "$myip" printf 'Sending email...\n' mail -s '[INFO] New IP' name@icloud.com &lt;&lt; EOF Hello Timestamp=$(date) IP=$myip Bye EOF printf '%s\n' "$myip" &gt; "$lastipfile" `~/bin` is an unconventional place for a state file though.
Yes it's a programming language. While used mostly as a glue language (piping together external programs often also referred to as 'scripting') its possible to write scripts in Bash using only builtins (which are similar to external programs but built into the Bash interpreter and which don't require forking a process - which is pretty expensive). 
Oh, your reply actually helped me to realize that I'm dumb. I made the list using windows and it had DOS line endings. Thanks!
You can deal with DOS line endings by including the CR (`\r`) character in the internal field separators (IFS): while IFS=$'\r\n' read -r line; do if [[ ! -f $line ]]; then echo "$line" fi done &lt;"$1" &gt;missingfiles.txt The above also fixes a few other things: * the `-r` flag to `read` disables backslash parsing (in case any of your filenames contain backslashes) * fix quoting and use `[[` instead of `[` to deal correctly with spaces in filenames * more efficient output redirection by redirecting the loop as a whole instead of each `echo` separately 
You could convert it to text and then check that output,using pdf2txt
I've used this one for awhile: #!/bin/sh #AccuWeather (r) RSS weather tool for conky #USAGE: weather.sh &lt;locationcode&gt; #(c) Michael Seiler 2007 METRIC=1 #Should be 0 or 1; 0 for F, 1 for C if [ -z $1 ]; then echo echo "USAGE: weather.sh &lt;locationcode&gt;" echo exit 0; fi curl --connect-timeout 30 -s http://rss.accuweather.com/rss/liveweather_rss.asp\?metric\=${FAHRENHEIT}\&amp;locCode\=$1 | perl -ne 'if (/Currently/) {chomp;/\&lt;title\&gt;Currently: (.*)?\&lt;\/title\&gt;/; print "$1"; }' So for my area I can just run: ~&gt; weather sna ~&gt; Partly Sunny: 60F It says for conky, but it just spits it out on the command line. You can use zip code instead of airport code and it works the same.
Thank you for your suggestion. I did some testing and for some reason the metadata comes as output on those pdfs that don't have text. I'll try to work on it and will post the result later
nice. i has my correct temperature, but it's displayed as 40 C (when it's 40F outside). Overall, it's a nice script!
 awk 'BEGIN { print "a b c" } $1 == "c" { print sum["a"], sum["b"], $2; sum["a"] = sum["b"] = 0 } $2 ~ /^[[:digit:]]+$/ { sum[$1] += $2 }' file
 awk 'BEGIN { print "a b c" checking=1 a=0 b=0 c=0 } { if (checking == 1) { if ($1 == "a") { a+=$2 } if ($1 == "b") { b+=$2 } if ($1 == "c") { print a " " b " " $2 checking=1 a=0 b=0 c=0 } } if ($1 == "file") { checking=1 } }' file 
Thank you for your answer! I think this would be too slow for me. Also, I would rather have the original text from the pdfs which were built using Word or Writer, rather than the ocr'd version of their images. However, your code will be of use in the future =) 
I suspect you're looking for IFS http://bash.cyberciti.biz/guide/$IFS I'm not at the terminal right now, or I'd give a more specific example.
This will work but is a very bad way to solve OP's problem. 
You don't want to use a for loop here. 1) It's an unnecessary use of `cat` which isn't nearly the big deal people make it out to be but: 2) `for` loops are pretty much always worse than `while` loops. As you see, they do splitting on the words, not the lines but worse, in the syntax you use it creates an entire sub shell that has to finish running before the loops starts. So let's say the file was 500m, you would be dumping all that into memory, doing splitting per word,and _then_ doing your loop. Using a while loops you just read in one line at a time and do your loop in real time. Here's a simple way to do what you're looking to do: ProBoxAlpha:~ hegemon$ while read line;do echo ${line};done &lt; /tmp/redhelp john smith victory christopher favour williams I'm actually super tired, so I'm not sure the above made as much sense as I'd like. If not, just say so and I'll try to explain better later.
For iterates over words, which is where your problem is coming in. For this, I would use a while read loop: while read -r line; do echo "$line" done &lt; names.txt See this page: http://mywiki.wooledge.org/BashFAQ/001 Also see this one on how your current solution is flawed: http://mywiki.wooledge.org/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29
Your point about how this is loaded into memory is really important - I've bumped into this a number of times when iterating throwing huge files. My question for you: what if your desired result is to actually iterate through each word in the line (as opposed to the whole line)? Particularly with the consideration of how large files would be processed, what do you think is the most efficient way to achieve the "for" loop behavior?
Ty for the education on "while". I've been using "for" since forever, because it works, but I understand the benefits of while for big datasets. TIL! 
This is the best solution possible. Its reasonably fast because it only uses builtins and performs no unnecessary forks and handles whitespace correctly.
all your solutions worked very well, but i used IFS=$'\n' with the for loop and it still worked. Thanks for the quick response :)
I wish I learned about that 20 years ago. Never knew bash had a debugger! Thanks!
&gt; #!/bin/bash sum=0 for i in `seq 1 200` do echo "SUM ($sum) + I ($i) = " sum=`expr $sum + $i` echo "$sum" done Yikes, that's horrible. Your professor is teaching you to write antiquated garbage, not bash. Here's a modern version: #!/bin/bash sum=0 for (( i = 1; i &lt;= 200; ++i )); do printf 'SUM (%5d) + I (%3d) = %5d\n' "$sum" "$i" "$((sum += i))" done expr and seq do not belong in a bash script. -- Anyway, as to the problem description &gt; "Write a program that will list a family tree from a file. It should keep the following data from everyone: birthplace, date of birth, date of death (if there is one), address, mother's, father's, children's name, spouse's name. Everyone person should be assigned an unique ID. Display information about one person at once. When starting the program, it should display the first person recorded in the file. Pressing different buttons will navigate to different people (as an example, pressing "m" will navigate to the currently displayed person's mother, "f" will navigate to their father, "1" will navigate to their first child, "s" to their spouse, etc.)" So you should have a csv file containing the data described. To parse this you can use a while read loop as explained in [BashFAQ 1](http://mywiki.wooledge.org/BashFAQ/001). To read the commands from the user, in the form of single characters, you can use read with the -n option. Run ``help read`` to see what read can do (though the full documentation for the read builtin is in bash's manual). Writing this in bash though ... it's the wrong tool for the job. Should use a general purpose language for this.
&gt; So you should have a csv file containing the data described. Not exactly. We have to make said file, too. &gt; Writing this in bash though ... it's the wrong tool for the job. Should use a general purpose language for this. Yep, I know. If it weren't for bash, I could have done this in a minute, but... Right now, I've been collecting my classmates' sh files, and by this I mean literally copying their stuff over to my folder. Some of them had the luck to know people who know bash, and I'm scrimming through their files to see examples of syntaxing and stuff. Fortunately for me, my uni for some reason allows us to look into eachother userfolders, and, well, most of them have no idea how to use the chmod command. &amp;nbsp; Anyway. I'll have a look into the BashFAQ you linked.
This is tricky and I once tried to make pstree in bash. A reduced problem. No extra information, doesn't assign a unique ID but uses the name. famtree.txt #Name Mother Father Children(comma separated) Spouse Bernu No record No record John Bert Bert No record No record John Bernu John Bernu Bert Charlie,Cath Trudy Trudy No record No record Charlie,Cath John Charlie John Trudy No record Loraine Cath John Trudy No record Pete Loraine Larissa Lance No record Charlie Larissa No record No record Loraine Lance Lance No record No record Loraine Larissa Pete No record No record No record Cath script.bash #! /bin/bash file=famtree.txt IFS=$'\n\t' declare -A tree while read name mother father children spouse; do [[ $name == \#* ]] &amp;&amp; continue [[ $currentperson ]] || currentperson=$name tree[$name-name]=$name tree[$name-mother]=$mother tree[$name-father]=$father tree[$name-spouse]=$spouse IFS=, childcount=1 for child in $children; do if [[ "$child" != "No record" ]]; then tree[$name-child-$childcount]=$child ((childcount++)) fi done IFS=$'\n\t' tree[$name-childcount]=$((--childcount)) done &lt; "$file" while true; do echo "+-----------" for item in name mother father spouse; do echo "| $item: ${tree[$currentperson-$item]}" done for ((c=1;c&lt;=${tree[$currentperson-childcount]};c++)); do echo "| child $c: ${tree[$currentperson-child-$c]}" done echo "+-----------" while true; do echo -n "Show record for (m)other/(f)ather/(s)pouse/(0-9)child 0-9 or (q)uit: " read -n1 key echo case $key in q) exit;; m) newcurrentperson=${tree[$currentperson-mother]};; f) newcurrentperson=${tree[$currentperson-father]};; s) newcurrentperson=${tree[$currentperson-spouse]};; [0-9]) newcurrentperson=${tree[$currentperson-child-$key]};; esac if [[ "$newcurrentperson" != "$currentperson" &amp;&amp; -n "${tree[$newcurrentperson-name]}" ]]; then currentperson=$newcurrentperson break else echo "Not a valid key or no valid record." fi done done 
Eh, 13 years in you get used to the Hungarian "Forward even if without our students" style of education. Case in point: this homework is for the lesson named "Computer Architecture". According to the Uni's demands, we started the year with creating a webpage in HTML, went onto bash halfway through the half-year, and finally started gutting the professor's own computer. &amp;nbsp; And in the end, the prof said that he would've modified the homework if we found them too hard. He did in the end for me because I forgot to include the date of death, whoops. Still got it accepted, so whatever.
Try `pdftotext` (from the `poppler` package). $ pdftotext somefile.pdf - | wc -l 0 The `-` means: send output to standard output instead of a file; this is then piped into `wc -l` which counts the lines. If it counts zero lines, there is no text. You can use this in a conditional statement with command substitution, for example: if (( $(pdftotext "$pdffile" - | wc -l) &gt; 0 )); then # $pdffile has text else # $pdffile has no text fi 
Yeah I know but haven't had the chance to do so. Didn't release it as 1.0.0 since I'm not sure it's stable, but will have the chance to test soon. Then again, this isn't a big thing right, just something I cooked up in a couple of weeks work.
&gt; What's up with your indentation? It's ginormous haha. yes, it's ridiculous! For the life of me, I cannot get gist tab settings to save. I go to edit, alter the tab settings to something sensible, save, and then it just reverts back. Perhaps a bug :/ Anyway, I wrote them just now, so I'll report back if they reply. &gt;Also, you're assuming a specific IP in there correct? It would probably be better to use a variable. Yeah, this is a good catch. I read somewhere that `10.10.10.10` is a good test address to use with `route`, but I've yet to find a compelling reason. If you come up with something better. Please do share.
Just make that a variable, that way whomever uses your script can change it easily. It just feels odd to me to use an RFC1918 address, when tons of people aren't using a 10/8 address. Makes more sense to me to use an external IP anyhow. I could be wrong so someone please correct me if there is a good reason to do it this way.
No worries, it's not the end of the world just looked a bit goofy.
Well, Im currently on a Windoze computer... So I wanted to show the OS what an actual functioning construct looked like ^.^
I was looking for text editors more along the lines of Sublime Text or Notepad++. But I'll try this out, thanks. 
Vim can do this too with &lt;C-x&gt;&lt;C-f&gt;, but if anything it's less like Notepad++ than Emacs.
this is awesome, I was thinking of doing a project like this but haven't had time. I'll try it later :D
Thanks man, this is more like what I was looking for! 
Vim with YouCompleteMe works well.
Did you want help? Otherwise this probably belongs in /r/scriptswap. Some feedback though: use meaningful loop variable names (`for i in` is a bad habit, `for month in`, as an example, is better practice), and because you're using bash, you can avoid the useless use of echo's and either use here variables or go a step further and use built-in variable substring replacement, which will get rid of all the instances of sed. echo $var | sed... Becomes sed... &lt;&lt;&lt; $var Becomes "${var//find/replace}" Read more here to see if it's appropriate for you http://tldp.org/LDP/abs/html/parameter-substitution.html
backticks are deprecated, use `$()` instead bash has basic math capability, your use of `bc` is unnecessary Use meaningful variable names Use `printf` instead of `echo` ([some light reading](http://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo)) When posting code, indent it by four spaces so that it appears correctly. Alternatively, select your code and press the code button in your editor (this is probably a RES enhancement that I'm just used to) These are off the top of my head - paste your code into http://shellcheck.net to see if it finds anything else.
 thanks for the advice, I'll start using $( ) instead of backticks, it should get picked up by the editor better. I couldn't get bash to deal with floating point numbers which is why i had to bc, the partner script i wrote for this is actually more needing bc but i figure i can leave it for the off chance it's going to be used more precisely... In the future I will try to remember to make up better variable names, I just used what I had written down. I'll read into the printf over using echo, it worked for some parts of the script and others returned an error, but i'll read into it and probably figure out why. I never noticed RES has a code button, that'll also be very helpful, thank you again for you advice and for the links, i'll start running everything through shellcheck and see what i get.
Not available on the system and it's too damn hard to get it put in to help a single customer.
If you already have access to the system you could download a binary and run it locally out of the same folder as the script. 
You wrote `if [ -f "goog_dr_index_nu" ]` instead of `if [ -f "goog_dr_index_nu.txt" ]`
uhh.. heh, oops. that was the only thing keeping it from working properly. thanks :)
assuming availability of associative arrays #!/bin/bash declare -A month declare -A weekday month=( [janv]=Jan [févr]=Feb [mars]=Mar [avril]=Apr [mai]=May [juin]=Jun [juil]=Jul [août]=Aug [sept]=Sep [oct]=Oct [nov]=Nov [déc]=Dec ) weekday=( [dim]=Sun [lun]=Mon [mar]=Tue [mer]=Wed [jeu]=Thu [ven]=Fri [sam]=Sat ) # create dummy test files for m in "${!month[@]}" do touch "userful__${m}_2015.tar.gz" "rawdata__${m}._2015.tar.gz" done for d in "${!weekday[@]}" do touch "some_stuff__${d}__.tar.gz" done # run conversion for m in "${!month[@]}" do for f in userful*"$m"*tar.gz do mv -vf "$f" "${f/$m/${month[$m]}}" done for f in rawdata*"$m".*tar.gz do mv -vf "$f" "${f/$m./${month[$m]}}" done done for d in "${!weekday[@]}" do for f in *"$d"* do mv -vf "$f" "${f/$d/${weekday[$d]}}" done done output given the generated test files $ ./date_fr2en.bash „userful__févr_2015.tar.gz” -&gt; „userful__Feb_2015.tar.gz” „rawdata__févr._2015.tar.gz” -&gt; „rawdata__Feb_2015.tar.gz” „userful__oct_2015.tar.gz” -&gt; „userful__Oct_2015.tar.gz” „rawdata__oct._2015.tar.gz” -&gt; „rawdata__Oct_2015.tar.gz” „userful__janv_2015.tar.gz” -&gt; „userful__Jan_2015.tar.gz” „rawdata__janv._2015.tar.gz” -&gt; „rawdata__Jan_2015.tar.gz” „userful__déc_2015.tar.gz” -&gt; „userful__Dec_2015.tar.gz” „rawdata__déc._2015.tar.gz” -&gt; „rawdata__Dec_2015.tar.gz” „userful__mars_2015.tar.gz” -&gt; „userful__Mar_2015.tar.gz” „rawdata__mars._2015.tar.gz” -&gt; „rawdata__Mar_2015.tar.gz” „userful__juil_2015.tar.gz” -&gt; „userful__Jul_2015.tar.gz” „rawdata__juil._2015.tar.gz” -&gt; „rawdata__Jul_2015.tar.gz” „userful__juin_2015.tar.gz” -&gt; „userful__Jun_2015.tar.gz” „rawdata__juin._2015.tar.gz” -&gt; „rawdata__Jun_2015.tar.gz” „userful__sept_2015.tar.gz” -&gt; „userful__Sep_2015.tar.gz” „rawdata__sept._2015.tar.gz” -&gt; „rawdata__Sep_2015.tar.gz” „userful__nov_2015.tar.gz” -&gt; „userful__Nov_2015.tar.gz” „rawdata__nov._2015.tar.gz” -&gt; „rawdata__Nov_2015.tar.gz” „userful__mai_2015.tar.gz” -&gt; „userful__May_2015.tar.gz” „rawdata__mai._2015.tar.gz” -&gt; „rawdata__May_2015.tar.gz” „userful__avril_2015.tar.gz” -&gt; „userful__Apr_2015.tar.gz” „rawdata__avril._2015.tar.gz” -&gt; „rawdata__Apr_2015.tar.gz” „userful__août_2015.tar.gz” -&gt; „userful__Aug_2015.tar.gz” „rawdata__août._2015.tar.gz” -&gt; „rawdata__Aug_2015.tar.gz” „some_stuff__mer__.tar.gz” -&gt; „some_stuff__Wed__.tar.gz” „some_stuff__ven__.tar.gz” -&gt; „some_stuff__Fri__.tar.gz” „some_stuff__lun__.tar.gz” -&gt; „some_stuff__Mon__.tar.gz” „some_stuff__jeu__.tar.gz” -&gt; „some_stuff__Thu__.tar.gz” „some_stuff__sam__.tar.gz” -&gt; „some_stuff__Sat__.tar.gz” „some_stuff__mar__.tar.gz” -&gt; „some_stuff__Tue__.tar.gz” „some_stuff__dim__.tar.gz” -&gt; „some_stuff__Sun__.tar.gz” 
It's heterocentric.
I actually considered making it gender neutral, ill do that so as to not imply preference.
You could do something like this: # Read input and assign it to a variable read -r passtest # Make comparisons against variable e.g. if [[ "${#passtest}" &lt;= "7" ]]; then # ...
I was looking to compare fail count to locked status. for i in `cat /etc/passwd |cut -d: -f1`; do passwd -S $i; done | grep locked Gets me the locked status I was looking for nicely. for i in $(cut -d: -f1 /etc/passwd); do pam_tally2 -u "$i"; done However the cumbersome output from above is rather messy. Which is why I tried my non-kosher command. This brought a much better output. As far as `chage` goes it not give the type of information I'm looking for. We have a customer that is complaining that they can't access the server and swears they aren't fat fingering their password constantly. I've already provided a report showing the failed attempted with `utmpdump /var/log/wtmp`. However the more I dig the more I believe the number of failed attempts for the individual in question is a failure on their part but the failed access or timeout they've described is from multiple brute-force attempts saturating the open connection count.
that's because you aren't returning anything from the function. you should do something like: function testraid { lspci | grep MegaRAID } _raidtest=$(testraid) think about functions as little programs.
It's always good practice to check for errors or unexpected input/return codes and fail gracefully. 
Yes, the program quit because grep returned non-zero exit status. In most cases one should [avoid `set -e`](http://mywiki.wooledge.org/BashFAQ/105).
&gt; for i in `cat /etc/passwd |cut -d: -f1`; do passwd -S $i; done | grep locked Couple of things: * Use meaningful loop variable names. `for i in` is bad practice, you can see in my previous post that I used `for user in`, for example. * Backticks are deprecated. You should only entertain them if you're coding on something ancient and decrepit. `$()` is what you should be using. * Useless Use of Cat. `cat` is for con`cat`enating, not for dumping file contents into a pipe. Ultimately you can take any approach you like to formatting the output of the loop, you can reformat it to whatever layout you please and blend in the `passwd -S` results too. Probably the simplest approach though is to just print out the header line and then run the loop and filter the header, like this: printf "%s\n" "Login Failures Latest failure From" \ "$(cut -d: -f1 /etc/passwd | while read -r user; do pam_tally2 -u "${user}"; done | grep -v ^Login | sort)"
Old habits I guess hah, I'm currently using /bin/bash. It's declared right before an if statement. Thanks for the insight.
What are the exact requirements of the assignment? Post the text. It looks like the auto-grader has very strict expectations for the room and log.
Alright, I fixed that. Now what? Thanks!
unfortunately, it is not possible to do this. the closest you can get to this is adding functions inside a script and shelling out to it, however, the script won't have access to modification of the calling environment (exporting variables or changing the working directory, for example).
[Here's a link to the assignment](http://cs.umw.edu/~finlayson/class/cpsc225/assignments/04-chat.html). Thanks!
As a quick glance `tail -$chatDiff` looks wrong. I don't think that `-` needs to be there.
I'd also say that `chmod 111 \temp\$roomName` is probably not what you want. I'm pretty sure that that the command you wrote gives the owner, group and world execute privileges, but nobody has read or write privileges. Since all users of the chatroom need to be able to read from and write to the chatroom file, this could be an issue.
Declaring a function inside a function is possible: score@kirisame ~ $ x() { y() { echo hi; }; } score@kirisame ~ $ y bash: y: command not found score@kirisame ~ $ x score@kirisame ~ $ y hi However it's a silly thing to do. Since bash has no scope, and functions aren't closures, there's almost no good reason to do something like this. Is there some feature you want from this? If so I might be able to help you find the equivalent to that feature. I often write "callbacks" in bash just by shoving an `eval` in the right place in my code, e.g.: each_image_in() { for f in "$1"/*; do case $(file - &lt;"$f") in (*' image '*) eval "$2"' "$f"';; esac done } each_image_in ~ 'gm identify' Output: /home/score/2015-11-04-193733912157294.png PNG 861x243+0+0 DirectClass 8-bit 30.0Ki 0.000u 0:01 /home/score/2015-11-07-011112112712403.png PNG 565x173+0+0 DirectClass 8-bit 115.2Ki 0.000u 0:01 /home/score/2015-11-11-021008741084464.png PNG 1106x821+0+0 DirectClass 8-bit 264.6Ki 0.000u 0:01 Of course as any good programmer will tell you, be careful around `eval`.
Or possibly a colon ;)
Fair enough there. I've edited a little extra into my post now if it interests you.
`tail -$x` is usually used as a shorthand for `tail -n $x` - that is, take the last `x` lines
When reading input you can use the `-p` flag to specify the prompt, rather than having to echo it before the read command. You have a capital `Sleep` there. That won't work. Your `echo | bc` lines are poor, I'll count you the problems in the second one: sm=`echo $ss*60 |bc` 1. Backticks (you knew that already) 2. `bc`rather than `$((...))` (you knew that already too) 3. Unquoted use of a variable not intended to be split or expanded 4. The `*` acts as a glob, which means the success of the calculation is now entirely dependent on what files are in the current directory! For simplicity let's say you should always enter 2 strokes per second. Try your script and note its output. Then, create a text file called 20318483960 in the current directory. Try your script again. To rewrite the main body of code with the above and the first post in mind: read -p "Stroke Length: " sl read -p "Strokes per Second: " ss clear ;echo "Computing"; sleep 2;clear; echo "Computing ." ; sleep 1; clear;echo "Computing .."; sleep 1;clear;echo "Computing ..."; sleep 1; clear; t=$((63660/sl)) sm=$((ss*60)) d=$((t/sm)) The sleeps will get old fast though. 
To simulate callbacks you can declare a function and pass its name to another function as argument. _() { printf 'Number %d\n' "$1" } times() { for ((i = 0; i &lt; $1; i++)); do "$2" "$1" done } times 3 _ # executes _ 3 times
On top of what /u/raumaankidwai said, let me start by saying that your indenting style is... interesting. But I'll offer the following constructive feedback: You can avoid using an external command `id -u` by using the bash `$UID` or `$EUID` variables. e.g. if (( EUID != 0 )); then Next, `read` should almost always be used with `-r`. Double quote your variables For portability, you should avoid using the `-p` option of `useradd`. You should probably use `echo '$username:$pass' | chpasswd -e` instead. You can avoid directly handling exit codes like this: egrep "^$username" /etc/passwd &gt;/dev/null if [ $? -eq 0 ]; then By rewriting like so: if egrep "^$username:" /etc/passwd &amp;&gt;/dev/null; then Finally, once all is said and done, you should probably put some sanity checking at the start of the script (just after the root check) to ensure that the commands you need are available if ! command -v chpasswd &amp;&gt;/dev/null; then echo "uh-oh, chpasswd not found" exit 1 Copy and paste your code into http://shellcheck.net and see if I've missed anything.
Interesting and well done! Storing in a matrix is an interesting workaround. You are right `perl` is probably the way to go. I was curious though to see if `sed` could do and in fact someone finally found [a solution](http://stackoverflow.com/a/34080390/1983854): sed -r 's#(([^0-9][0-9]{0,9})*)(\b[0-9]{10}\b)(([0-9]{0,9}[^0-9])*)#printf "%s%s%s" "\1" $(date -d@\3 "+%Y") "\4";#ge' file
Thanks for your comments SIR...
I believe you're looking for `script.sh &gt; output.txt` You can then open `output.txt` in Word.
dont suppose you know how to add a date and time with the log file? I tried it but it dosn't update with every log file that is saved 
First of all, try `man date` in a shell session. Put `echo $(date)` into your script somewhere, optionally with parameters after `date` inside the parenthesis. If you'll be creating scripts often, it would be a good idea to read some of the links in the sidebar. 
`a -le b` performs a **test** on the integers `a` and `b` and **exits** with a certain **status** depending on the result of the test. If `a` is less than or equal to `b`, then it exits with status **0**, which basically means "no error", and is a bit like "true" here; and if `a` is greater than `b`, then it exits with status **1**, which is a bit like "error"/"false". You can play around by running `test` in a terminal: $ test 3 -le 5 $ echo $? # print last exit status 0 $ test 5 -le 3 $ echo $? # print last exit status 1 `(( ... ))` performs an arithmetic **operation** and returns a **result**. For example, `(( 3 + 2 ))` returns `5`, because 3 + 2 = 5. The operation here is that of addition. Similarly, `(( 3 &lt;= 5 ))` returns `1`, which here stands for "true", because 3 &lt;= 5 is a true statement; and (( 5 &lt;= 3 )) returns `0`, which here stands for "false", because 5 &lt;= 3 is a false statement. Again, you can play around in the terminal as follows: $ echo $(( 3 + 2 )) # print the result of the operation (( 3 + 2 )) 5 $ echo $(( 3 &lt;= 5 )) # print the result of the operation (( 3 &lt;= 5 )) 1 $ echo $(( 5 &lt;= 3 )) # print the result of the operation (( 5 &lt;= 3 )) 0 The confusing thing is that, when it comes to **exit statuses**, **0** is like "true" (no error), while **1** is like "false" (error). But when it comes to **arithmetic results**, **0** is "false", while **1** is "true". All that being said, when it comes to `if` statements, you actually don't have to worry because, `if` handles things automatically behind the scenes, as you can see here. $ if [ 3 -le 5 ]; then echo "yes"; else echo "no"; fi yes $ if [ 5 -le 3 ]; then echo "yes"; else echo "no"; fi no $ if (( 3 &lt;= 5 )); then echo "yes"; else echo "no"; fi yes $ if (( 5 &lt;= 3 )); then echo "yes"; else echo "no"; fi no 
&gt;when it comes to if statements, you actually don't have to worry because, if handles things automatically behind the scenes Do note that it will work everywhere. `(( ... ))` also uses exit statuses with the gotcha that you said (arithmetic result of 0 gives an exit status of 1). $ (( 4 + 4 )) $ echo $? 0 $ (( 4 - 4 )) $ echo $? # The arithmetic result is 0, the exit status is 1. 1 $ echo $((1 &lt; 3)) 1 $ (( 1 &lt; 3 )) $ echo $? 0
The `[` and `test` commands are required by POSIX and are the ones you use to test strings, files and integers in sh scripts. Bash has more powerful syntax for such tests, namely the shell keyword `[[` to do the same as `[` but in addition it can do pattern matching and regular expression matching. For integers, it has the arithmetic command `(( ... ))`. In a bash script, prefer `(( ... ))` for testing integers, and `[[ ... ]]` for testing strings and files. Avoid using `[` and `test` in bash. # sh [ "$a" -le "$b" ] # bash (( a &lt;= b )) # yes, without $'s See http://mywiki.wooledge.org/tests
exit codes aren't binarym 0/1 true/false. An exit code of zero means no exit code, your command was successful (or the last function in a script was). A non zero exit code can be anything from 1-255 http://tldp.org/LDP/abs/html/exitcodes.html
http://www.geekpills.com/operating-system/linux/bash-conditional-expression-examples-and-operators-part-1/1819/ http://www.geekpills.com/operating-system/linux/bash-conditional-expression-examples-and-operators-part-2/1825/
You need to learn about what's doing what to your command line and it what order. $() are done by the shell before find gets run. And so on. Rather than go through what the problems are, it may be easier to have you look into why this works. find . -name 'foo*' ! -name '*.gz' ! -exec lsof {} \; -exec gzip {} \;
From `man find`: &gt; The expression is made up of options (which affect overall operation rather than the processing of a specific file, and always return true), tests (which return a true or false value), and actions (which have side effects and return a true or false value), all separated by operators. `-and` is assumed where the operator is omitted. So, the expression in the above command means: `-name 'foo*'` AND `-not -name '*.gz'` AND `-not -exec lsof {} \;` AND `-exec gzip {} \;`. Like in most programming languages, if any of these evaluates to false, the rest won't be evaluated. `-not -exec lsof {} \;` will only be true if the file is not open.
Your problem is that all the apks are named base.apk, it's their directory that has their name. You can use find's exec to invoke a shell, provide it all the results as arguments which can then be looped through.. We can then use parameter expansion to copy the files using the directory name as the apk name: find /data/app -iname '*.apk' -mtime 1 -exec sh -c 'for arg do path=${arg%/base.apk}; name=${path##*/}; echo cp "$arg" "/storage/emulated/0/test/$name.apk"; done' _ {} + This one is a "dry run" version with an echo before the cp. Run it and see if it returns like you want it to, if it does you can remove the echo. It ran properly on my Android device. Edit: Some explanations: for arg do this is short-hand for 'for arg in "$@"; do' which just means to loop through all of the positional parameters (aka the arguments passed to the shell). Note that we use a _ after the shell, this is used as $0 which is not looped through. The + at the end tells find to provide all results to the shell at once, as opposed to ';' or \; which tells it to run the shell once for each result, one at a time. path=${arg%/base.apk} This uses the current $arg in the loop and strips '/base.apk' from it, leaving us with just the path name. name=${path##*/} This strips everything before the final '/' in the path, leaving us with only the parent directory of the apk, which is named what we want (or at least something identifiable). You can learn more about parameter expansion here: http://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion
In addition to the builtin, a system will typically have an external binary of it too, yes: $ type -a [ test [[ [ is a shell builtin [ is /usr/bin/[ test is a shell builtin test is /usr/bin/test [[ is a shell keyword 
 if [ -n "$(find /folder/logfile.log -mmin +2)" ]; then service restart servicename fi From Bash manpage CONDITIONAL EXPRESSIONS ..... -n string True if the length of string is non-zero. But to be honest, you'd probably be better off figuring out why the service is not working properly, rather than using the above, which is a blunt instrument
Why don't you symlink your dotfiles? It's better than copying them each time...
Thanks!! This is what I was looking for :)
Thank you so very much for the detailed explanation. I wasn't aware we could do all this with shell scripting. I'm going to learn how to use all this in my other scripts as well. Now, I tried running your script, it doesn't work. I mean it's not echoing any output at all. I did check to see if the find command is working on its own and it is. It's returning about a dozen files but none of them seem to be getting processed. Any ideas? And I thank you again for taking the time to do this. 
Hm, strange. You can try something like this to see if it's getting any arguments: find ...other stuff here... -exec sh -c 'for arg in "$@"; do echo "$arg"; done' _ {} + Just to see if the echos come through. If that works, you can adapt my original line from 'for arg do' to 'for arg in "$@"; do' Also try removing the _ to see if you can get output without that. I specifically needed it on my device, I got 0 output without it, but I do have a custom ROM so maybe I have a more modern shell.
file directory/* |grep filetype file /bin/*|grep "shell script" /bin/bzdiff: POSIX shell script, ASCII text executable /bin/bzexe: POSIX shell script, ASCII text executable /bin/bzgrep: POSIX shell script, ASCII text executable /bin/bzmore: POSIX shell script, ASCII text executable
Neither of the solutions worked :( Something's probably wrong at my end. Any other ideas? Or a workaround? I friend of mine asked me to use rsync instead of cp. It's not available on BusyBox but I can pull the binary and compile it. What do you think? 
Useless use of subshell (and deprecated backtics) is useless. "throwing stuff together until it works" isn't really a great way to learn programming/scripting... And it definitely teaches bad patterns and habits. OP, I'd use the ```file``` command to check the file type.
check if this works `ls Paris02_{003..020}`
it worked in my bash terminal though `GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)` &gt;ls list_{01..03} &gt;ls: cannot access list_01: No such file or directory &gt;ls: cannot access list_02: No such file or directory &gt;ls: cannot access list_03: No such file or directory 
cheers, I think OP should be able to work out from here...
I started working on this and then saw there's a much easier way after I finished. Oh well, I'm posting it anyway in case someone else finds it useful. I learned how to suppress the octal interpretation of numbers starting with 0, so that's neat. lower=003 upper=020 length=$(( $(wc -c &lt;&lt;&lt; $lower) - 1 )) result= for (( i=$((10#$lower)); i &lt;= $((10#$upper)); i++ )); do result="$result $(ls *_$(printf "%0${length}d" $i) 2&gt;/dev/null)" done echo $result
bash doesn't support variable expansion try this: $(eval echo zip "$1" *_{"$2".."$3"}.png) edit: relevant link: http://stackoverflow.com/questions/19432753/brace-expansion-with-variable
Rough draft. Do not use. Use as roadmap. #!/bin/bash # Takes first arg f_low=$1 # Takes second arg f_high=$2 f_array=(${1} ) low_char=${#f_low} high_char=${#f_high} # seq will strip leading zeros for ZFILE in $(seq ${f_low} ${f_high});do f_array+=( "Paris02_${ZFILE}" ) done # I haven't tested whether the printf subshell # needs to be quoted or not. # It may depend on your IFS or version of BASH. /path/to/zip ParisDayTwo.zip "$(printf ${f_array[@]})"
This here works like you intended: #!/bin/bash echo "Hello" "$1" for i in "$1"/*; do if [[ "$i" =~ \.mp4$ ]] then echo "yup:" "$i" else echo "nope:" "$i" fi done That `$($1)` you wrote originally, that `$()` should treat the contents of `$1` as a command line and run it, then insert the output of that into that spot. When you did something like `./test.sh ~/Documents`, what happened was it tried to run a command line `~/Documents` and you get an error message: `bash: /home/Azwraith42/Documents: Is a directory` When you did something like `./test.sh a b c`, what happened was it tried to run a command line `a` and you get an error message: `bash: a: command not found` 
Good catch! I also moved the trap above the while loop and set a $$ sig but still not working how I would expect : /
It would teach bad habits if I said that was the correct way to do it. I was just minimally modifying OPs original script so that it did what he wanted. `file` command is the way I'd go as well.
Type `man bash` and look it up there or google for Bash completion
What /u/oweiler said. `man bash` has TONS of useful information. You can skip right to the bit about shell builtins by searching for `^SHELL BUILT` and you can search man pages (in case you are unaware) with `/` for forward search and `?` for backward search. https://wiki.gentoo.org/wiki/Man_page/Navigate for more information about man pages and how to navigate them.
Thanks, Updated .. 
I see the same in the zsh I have installed here. I got the idea to add a garbage character on both sides of the `=`, the same as what you do when you expect empty variables. I mean something like this: [ _"$x" = _ ] This seems to make things work for me. See here: $ echo $ZSH_VERSION 5.2 $ x="(" $ [ z"$x" = z"(" ] &amp;&amp; echo t t $ [ z"$x" = z")" ] &amp;&amp; echo t $ x=")" $ [ z"$x" = z")" ] &amp;&amp; echo t t $ [ z"$x" = z"(" ] &amp;&amp; echo t $ x="(" $ [ z"$x" = z"(" ] &amp;&amp; echo t t 
I guess that is a step up from hacks involving `grep`.. I think I'll go with [[ if I need to handle these cases though (since it's more clear what is going on) For empty variables, I had always found quoting them to be adequate, eg `[ -z "$somevar" ]`. Is there a case in which this doesn't work correctly?
37 comments on [shellcheck](http://www.shellcheck.net/#). Apart from that, the only critiques I would have so far would be style choices, which is completely subjective.
 compgen [option] [word] Generate possible completion matches for word according to the options, which may be any option accepted by the complete builtin with the exception of −p and −r, and write the matches to the standard output. When using the −F or −C options, the various shell variables set by the programmable completion facilities, while available, will not have useful values. The matches will be generated in the same way as if the programmable completion code had generated them directly from a completion specification with the same flags. If word is specified, only those completions matching word will be displayed. 
am just a bash noob . what do you mean by style choices :) 
Article TL;DR "Scrambled Bash code is bad, m'kay?" ---- I'm going to write a response to the author here so that maybe other users of /r/bash will notice what a bad article looks like. Let's begin. The author's first point is &gt;All variable should be local. So what's the reasoning for this? &gt;All variable should be local. OK, why? What is the reasoning behind this claim? What are any possible drawbacks? What are the overall benefits? How does this fit in with the narrative of "defensive BASH programming"? With some scripts I write there are variables that, upon set, need to be called by other functions. It would not only be redundant to keep setting that value (i.e. an IP address of a NIC or a size of a directory) in each function that needs that value, but it also compounds on resource usage (CPU, RAM). &amp;nbsp; &gt;Everything is a function *Why!?* One of the great advantages to Bash scripting, is you can implement really quick scripts without having to adhere to some other arbitrary coding style. If I need to punch out a quick script in 5 minutes, why on earth would I invest the time to write, not only everything in a function, but wrap the whole thing around a main() function at the end? You need to state the *why* when you are making these claims. So let's move onto your examples: &amp;nbsp; local files=$(ls /tmp | grep pid | grep -v daemon ---- temporary_files() { local dir=$1 ls $dir \ | grep pid \ | grep -v daemon } &gt; Second example is much better ***WHY!?*** You took a completely simple single-line command and unnecessarily broke it up. Now if this one-liner had, say, over 10 pipes and had long complicated arguments, then you might have a point. But you didn't make that point. You really didn't make any strong points. Beginners to Bash are going to read your article and think "I guess I need to arbitrarily break up every line of code... because *reasons?*" &gt;Code clarity Don't you mean "Forcing code complexity"? You took basic test operators and made functions out of them. You don't even justify *why* you think this is necessary. You just say "Let your code speak". What does that even mean? Are you saying it will help other people decipher what your program does? Are you saying that someone, who already has knowledge about Bash, wouldn't know what your script is doing, unless you wrote **simple test operations as English stated functions**? If the purpose of this article was to educate people on what "defensive programming" means and the benefits of maintaining structured code, then I'll say that you really came up short. This article wasn't very thought out.
Yeah, this article is pure posturing. The author happily uses terrible practices like: for i in $files do chown $user:$group $i done which will fail as soon as any filename contains a space or other strange character. So much for "defensive". 
Let's take a sample function: func.PrintUser.GID() { if awk -F : '{print $0 }' /etc/passwd | grep ^$username 1&gt;/dev/null then gid=$( grep ^$username /etc/passwd | awk -F : '{print $4}' ) echo $gid else func.Erruser fi } I would format it more like this: func.PrintUser.GID() { if awk -F : '{print $0}' /etc/passwd | grep ^"${username}" 1&gt;/dev/null; then gid=$( grep ^"${username}" /etc/passwd | awk -F : '{print $4}' ) printf "%s\n" "${gid}" else func.Erruser fi } The function curly braces could go either way, but most style guides I've seen use two spaces for indenting and they put then/do on the same line. Note how with the 'then' tucked out of the way and the code indented, you can see clearly what will happen if the if condition is met. The other thing is you really should use `printf` instead of `echo`. I've seen significantly worse scripts from self-proclaimed noobs, I've seen massively worse scripts written by professionals (I spend way too much time cleaning up their mess), but I think this one is actually fairly neat and consistent, and it avoids a lot of the "bash noob" pitfalls.
&gt; You just say "Let your code speak". What does that even mean? Yeah.. I don't let my code speak, I let my code do its fucking job. I let my ***comments*** speak for the code.
grep + awk combos are less than awesome. Once you pull out awk, grep is superfluous for example: want gid of a given user? $ awk -F: -v user=pulse '$1 ~ user { print $4; }' /etc/passwd 122 
Yeah, that's because `[[` is a keyword with it's own parsing, and `[` is just a normal binary. Confusing tbh. 
Eh, I did check for its presence in bash before posting it here.. But apparently I checked wrongly, as I can't reproduce it in bash now. Thanks for reporting it, in any case. (edit: I just found another zsh bug. Consider the output of `v=$(printf foo\\0bar);expr length "$v";expr length $v`. semi-correct output is 6 6 This output is produced by bash and dash. (completely correct output would be 7 7.) zsh, however, produces 3 3 , clearly stopping at the NULL. v has not been truncated, because `echo $v` prints `foobar`, not just `foo`, and ${#v} equals 7. EDIT2: apparently that's a limitation of C's argument passing (exec() family of functions, argv+argc interface for main, etc).. which is.. kind of sad, in 2015. )
thanks for the example, am really happy :) . I will format the whole code properly :) 
i don't really know awk totally, but i will test this example 
awk is simple, in classic cases it's a series of `condition { to_do }` `grep word` is equivalent to `awk '/word/ { print }'` Imo it's well worth it to spend an hour or 2 to learn its basics. It's a ghetto perl, more powerful than sed when it comes to text mangling.
Avoid using the `[` and `test` commands in bash. Use `[[ ... ]]` to test strings and files, `(( ... ))` to test numbers, and `if ...` to test commands.
Using `readonly` is also a terrible idea. Once a variable has been declared readonly, there's no way to "undeclare" it or unset it, and in a function, you cannot use a local variable if that variable name happens to be a global readonly variable: $ bash -c 'readonly foo; unset foo' bash: line 0: unset: foo: cannot unset: readonly variable $ bash -c 'readonly foo; f() { local foo=$1; }; f' bash: line 0: local: foo: readonly variable -- And this... &gt; readonly ARGS="$@" this is just plain horrible ... -- Why can't people bother to learn bash before writing these "bash tutorials"
Don't use `.` in function names. It's an undocumented feature.
I prefer bash getopts for argument parsing, thanks. 
&gt; print_mapped_external.sh: &gt; #!/usr/bin/env bash &gt; # NOTE! Requires Bash v4.x &gt; coproc cat ./external_list.txt mapfile -u ${COPROC[0]} mapped_list &gt; for name in ${mapped_list[@]}; do echo "(${#name}) $name" done This is a weird case for coproc. Why bother to run cat in the background when mapfile -t mapped_list &lt; ./external_list.txt will achieve the same? Also, you forgot quotes. `${mapped_list[@]}` is very wrong. It should be `"${mapped_list[@]}"`. Lastly, don't put `.sh` extension on a bash script. A bash script is not an sh script, so it's misleading.
&gt; This is a weird case for coproc. Ha, good point. Useless use of `cat`. I really just wanted a simple/easy-to-digest example for using both `coproc` *and* `mapfile`. &gt; Also, you forgot quotes. ${mapped_list[@]} is very wrong. It should be "${mapped_list[@]}". Whoops! Yea, I had changed that locally. Forgot to push. Fixed. &gt; Lastly, don't put .sh extension on a bash script. A bash script is not an sh script, so it's misleading. Yea, bad old-school unix habit/hangover.
A bit hacky, but this should work: `start=$(grep -n "@Start" file.txt | tail -n1 | cut -f1 -d:)` `end=$(grep -n "@End" file.txt | tail -n1 | cut -f1 -d:)` `((start+=1))` `((end-=1))` `sed -n "${start},${end}p" file.txt`
I'll play around with this and see if I can get it working, thank you! So it looks like you are taking the file, and splitting it by start and end separately. I'm not familiar with the += operation is this situation, is it a way to get the last thing? Or something with string manipulation?
 awk '{a[NR]=$0}/^@Start/{b=NR}/^@End/{c=NR}END{for(d=b;d&lt;=c;++d)print a[d]}' file.txt
What are labels? are they a term that describes something specific to sed?
/u/ray_gun is just talking about _your_ labels (ie, @Start@ and @End@). Pretty advanced solution that works great.
Do you have to use bash? Seems like this could be done very easily in python.
That's pretty cool. What's with the `out`? I entered the `dialog` command without `out` and it ran just fine. What does the `out` do? 
That makes sense. 
spaces around [ and ] are relevant, because it's not a funky parenthesis, it's command called [ with params $#, -eq, 1 and the closing token ] either way for numbers you can use imo more readable and convenient (()) if (( $# == 1 )) ...
you can use bc echo "5+2 | bc"
huh?
you can use the expr built-in: $ expr 1 + 1 2 or arithmetic expansion: $ echo $((1 + 1)) 2 or bc as someone else already mentioned
Type `bc -l` at the command line (assuming you have `bc` installed). Hit `&lt;Ctrl-D&gt;` or type `quit` to exit. You can also just use the interactive interpreter of a common programming language like Python, R, Haskell, etc.
&gt; You're eliminating the separation between the parameters by combining them all in one variable, *separated by the first character in $IFS (normally a space)*. That last part only applies to $*, while $@ is always a space. Here are some examples: $ set -- foo bar foo\ bar $ echo -n "$IFS" | od -A none -t x1 20 09 0a $ echo "$*" foo bar foo bar $ echo "$@" foo bar foo bar $ IFS=$'\n' $ var=$* $ echo "$var" foo bar foo bar $ var=$@ $ echo "$var" foo bar foo bar 
If you're into reverse polish notation (RPN) like HP calculators, go with dc (desktop calculator) $ dc 2 2 + p 4
In your post here, you're saying: if ( condition ) It should be: if [ condition ] (Note the square brackets) ... Or even better: if [[ condition ]] Hope that helps! 
Nice!
Check the sidebar, noob. :) Plenty of entry-level guides to Bash. 
Make it into an array. `daysoftheweek=( Su Mo Tu We Th Fr Sa ); read -p 'Which day (0-6)? '; echo "${daysoftheweek[$REPLY]}"`
Have you tried the [BashGuide](http://mywiki.wooledge.org/BashGuide)?
To generate an HTML file for reading the answers, you can run a command like this: curl -s 'https://api.stackexchange.com/2.2/users/22565/answers?site=unix&amp;page=1&amp;pagesize=100&amp;filter=!-.mgWMP7sO7J'|gzip -d|jq -r '.items[]|"&lt;h2&gt;&lt;a href="+.link+"&gt;"+.title+"&lt;/a&gt;&lt;/h2&gt;"+.body'&gt;/tmp/a.html 
Pretty sure you just did his homework. 
Now I need `zshzle` version of that.
Yes, you are totally right
I think ideally your variable names and everything alleviate the need for comments. I didn't read the article but I'm assuming that's what he would talk be talking about.
For easy backup you can read up on the command rsync (remote sync) For instance here: http://linux.die.net/man/1/rsync
Worth noting that a lot of this, and more, can be found in `man bash`. For the `set` commands, search `man bash` (using `/`) for `Readline Variables`, and `Readline Command Names` for information about commands you can bind to key-presses.
I voted for #2, but I wish the font was thinner like #3. I also don't like the tiny text below BASH. Shrink the logo enough and that text is just a horizontal line. It should look good at every size.
I voted for 1 because I never have just the dollar sign for my prompt and I liked the dark box. 
Why does bash need a logo?
http://lists.gnu.org/archive/html/bug-bash/2015-12/msg00116.html
Hard to tell whether or not this is fishy. Since when does GNU use google products for any official business?
&gt; The guide seems to suggest that pipes can process information in "flows". So in a sense the first function does not have to complete for the other to start using it's output for computation. Right A | B causes A and B to be started simultaneously, with `A`'s stdout connected to the writing end of the pipe, and `B`'s stdin connected to the reading end of the pipe. &gt; But how is that possible? i.e. if I had a program that converted .html to .pdf - how can it start converting without having the full .html file?. Depends on the case. If `B` takes html data on standard in, and produces pdf data on standard out, then it will likely just suck up all the data it recieves, storing it either in memory or in a temporary file, until `A` closes the pipe, in which case it can start processing the html data. If `B` instead is something like `sed s/foo/bar/`, there's no need to wait for the entire input to be sent. `sed` can start processesing as soon as it has received a complete line, and that's exactly what `sed` does. 
Yes. http://tiswww.case.edu/php/chet/img/bash-org.jpg Chet talks about it in the vote announcement. https://lists.gnu.org/archive/html/bug-bash/2015-12/msg00116.html
Voted #2 and was not surprised to view results with that being the overwhelming majority of votes. Looks clean.
Atom has a plugin wisely called autocomplete-paths, which does just that.
I don't know if fishy is the right word or not - or official for that matter. It's just a tool to do an informal vote on something of little consequence. 
Point taken. Seems pretty unofficial for something so official (e.g. Non-trivial). Either way, can't blame them for using the simplest tool, just curious. Nice to see GNU exclusive tools.
maybe... unzip -p file.zip filename.txt | wc
&gt; If B instead is something like sed s/foo/bar/, there's no need to wait for the entire input to be sent. sed can start processesing as soon as it has received a complete line, and that's exactly what sed does. Thank you for the answer. I thought it will be something like that. So in the end it depends on how the program you are executing is dealing with the input - does it read it by lines or waits for EOF. Is it possible to construct some example, where you can enter text line by line in stdin and see other process executing something line by line (maybe printing the word counts in your line as you type them to stdin)? I tried something simple like `sed s/A/B/ &gt; test.txt`. But when typing lines in stdin the test.txt remained empty until I pressed &lt;ctrl-d&gt;
Sorry if my formatting is poor, but what i posted does NOT work as I would hope. I am expecting OUTPUT to be "/newdir/subdir/subdir/ more text", instead it is simply " more text". I have used the += to do this exact thing in other cases, for whatever reason it doesn't seem to be working in this instance though. I have found a workaround and have my code working properly now, I was just wondering if I was doing something incorrectly which was causing me to get the unexpected output I got.
Oops, I meant that when I run it it doesn't say " more text", but "/newetc/ more text". You need to run readlink on a symlink though.
&gt; I tried something simple like sed s/A/B/ &gt; test.txt. But when typing lines in stdin the test.txt remained empty until I pressed &lt;ctrl-d&gt; That is due to [buffering](http://mywiki.wooledge.org/BashFAQ/009). Most programs check whether their stdout is a tty or not. If it is a tty, they'll output as soon as possible. It it's not a tty, they will hold output in a buffer until a certain amount (like 4KiB) has been accumulated, then flush that large chunk in one go. Especially when writing to a file on disk, this makes sense. Writing the data in two large chunks should be more efficient than 100 small writes to the disk.
Homework question? 
I think that needed the option to vote on the text and images separately. I like the $ cube from 2 but the thinner font from 3.
To append "moretext" to the variable OUTPUT: OUTPUT="${OUTPUT}moretext" You need ${...} because otherwise the name of the variable would be $OUTPUTmoretext
For future reference: http://www.fakenamegenerator.com/
You need to turn the color back to default using "\033[m".
Right now you are using `\e[31m${HOSTNAME}` and you want to change it to `\e[31m${HOSTNAME}\e[m` You can do it with variables just like you did with the `${R}`, or you can just put that `\e[m` there. I think what's confusing you is that you are thinking about this as if it's something that's a feature of either bash or printf, so you feel packaging this into one parameter with `"` should somehow encapsulate it and you feel that there are variables has a meaning, but bash and printf actually don't know anything about colors or underline or bold or the cursor position etc. They hand those kinds of escape codes over to the terminal just like any other character you print. It's just normal text from their point of view. The terminal switches to a different color when it sees the `\e[31m`. Now the terminal does not know anything about what you are doing in your script. It does not know that there were `"` surrounding a parameter for printf and that there was a bash variable. The terminal only sits there and waits to receive characters and works on those.
The `+=` is something special for variables that are an "array". An array, you create like this: your_variable=(something another_thing "more text") In your concrete case, you would have done this: OUTPUT=($(readlink /dir/subdir/subdir)) You can then add something to the array with the `+=` like this: OUTPUT+=("more text") You would then access the contents of the array like this: $ echo ${OUTPUT[0]} /newdir/subdir/subdir $ echo ${OUTPUT[1]} more text $ echo ${OUTPUT[@]} /newdir/subdir/subdir more text Here's another thing you need to know because of file names with spaces: $ for x in ${OUTPUT[@]}; do echo $x; done /newdir/subdir/subdir more text $ for x in "${OUTPUT[@]}"; do echo $x; done /newdir/subdir/subdir more text The length of the array you get like this: $ echo ${#OUTPUT[@]} 2
Oh, I saw that file name being put into the variable and started thinking about collecting file names with it, and completely didn't understand that the variable was probably about building a string for output.
Check your version of bash. For that matter, double check that you are actually *running* bash. (Are you at a command prompt doing this interactively? In a script? If script, what shell is the first line specifying?) The commands you give should work as expected. However, I don't know what version of bash the append operator (+=) was added for strings. From a Solaris server I administer: &lt;dba@server ~&gt; OUTPUT=abc &lt;dba@server ~&gt; echo $OUTPUT abc &lt;dba@server ~&gt; OUTPUT+=" xyz" &lt;dba@server ~&gt; echo $OUTPUT abc xyz &lt;dba@server ~&gt; bash --version GNU bash, version 4.1.17(1)-release (sparc-sun-solaris2.11) ...
I agree printf and echo are both bash built-ins, I was wondering if the use of printf and %b was confusing the OP about what was handling the colour change. Using tput, storing the output if it's going to be used many times, has the advantage of not hard-coding the escape sequence, or assuming the terminal supports the operation.
Hope I'm not too late asking, but what is the difference in using one and two pairs of hard brackets? I saw some sort of explanation on tldp, but it isn't seem very clear. 
Thanks everyone for the inputs. I was more confused by the fact the quotes around "${R}${HOSTNAME}" did not "enclose" the variable. I used printf cause I thought the formatting was easier with this rather than echo. I'll drop an eye at tput. Thanks again guys.
Could you explain what the `&lt;&lt;&lt;` does? Also, you might want to enclose the code within backticks for readability 
It's in a code block already. `&lt;&lt;&lt;` will give a command a string as stdin. Compare to `&lt;&lt;` which is a here-document and `&lt;` which is to provide a file as stdin.
Ah, TIL about `&lt;&lt;` and `&lt;&lt;&lt;`. What should I search for to read more about them? Searching for redirection only takes me to pages that talk about `0`, `1`, `2`, `&gt;`, `&lt;` and `&gt;&gt;`
Oh, I didn't understand that, sorry /u/Siladrakona. Doing that with bash can be done like this: $ x=aaaaabbbbbccccddddeeee $ echo ${x/#+(${x:0:1})/} bbbbbccccddddeeee $ echo ${x/%+(${x:(-1)})/} aaaaabbbbbccccdddd It uses a feature `${x/pattern/replacement}` that searches for "pattern" and replaces what matches with "replacement". The replacement part is empty here, so it will delete the pattern that it looks for. In the patterns that are used here, the `#` means to match at the beginning of the variable's contents. The `%` means to match at the end. The patterns that are searched for are then these two here: +(${x:0:1}) and +(${x:(-1)}) In those two patterns, the `${x:0:1}` is the first character of the contents of variable `$x`. The `${x:(-1)}` is the last character. This means the patterns that are searched for are the same as: +(a) and +(e) Those `${x:0:1}` and `${x:(-1)}` are a feature `${x:position:length}` that targets a part of a text in the variable `$x`. The first case uses it from spot zero in the text and for length of one character, so this gets the first character `a`. The second case uses it with a minus-one as the position, which makes it target the back of the text where there's an `e`. The length can be left off at the end. The `${x:(-1)}` needs those `()` because there's another bash feature `${x:-default}` and bash would get confused seeing `${x:-1}`. **EDIT:** I found out how to get it to work with the `${x#pattern}` from my first post. It works like this: $ x=aaaabbbbcccc $ echo ${x##+(${x:0:1})} bbbbcccc $ echo ${x%%+(${x:(-1)})} aaaabbbb It needs `##` instead of `#` to make it look for the longest possible pattern. With just a single `#` it will only cut off a single `a`.
These chapters of tldp: http://www.tldp.org/LDP/abs/html/here-docs.html http://tldp.org/LDP/abs/html/x17837.html
Sweet, thanks a lot. I've used here-docs before without knowing what it was and didn't know what to search for. It feels nice to finally be able to scratch that itch. :)
The `+(a)` part doesn't work for me. Is it a different shell?
This is Bash-only. Use `echo -n |` instead to be compatible with other shells.
 #! /bin/bash # (( UID != 0 )) &amp;&amp; { echo "Error: needs root"; exit 1; } #You could use this to check for root cd ~/Drivers wget -nc "http://www.tbsdtv.com/$(wget -q -O - 'http://www.tbsdtv.com/download/' | sed -n 's@.*href="\([^"]*common/tbs-linux-drivers_v[0-9]*.zip\)".*@\1@; T; p; q')" #The $() part returns only the first occurrence of "common/tbs-linux-drivers_v[0-9]*.zip". Downloading it with -nc (no clobber) will not overwrite a currently existing archive. for latest in tbs-linux-drivers_v*zip; do :; done #This loops over all versions and $latest will become the alphanumerically last one. if unzip "$latest"; then tar xjvf linux-tbs-drivers.tar.bz2 else echo "Error: Unzip failed" exit 1 fi cd linux-tbs-drivers sudo ./v4l/tbs-x86_64.sh || { echo "Error running ./v4l/tbs-x86_64.sh"; exit 1; } sudo make || { echo "Error running make"; exit 1; } sudo make install || { echo "Error running make install"; exit 1; } #I think only this step needs sudo, by the way. read -p "Reboot (y/N)? " [[ "$REPLY" == y ]] &amp;&amp; sudo reboot
Then be practical and use a for loop with seq to repeat the easy part for i in `seq ($number)` Who cares about elegance anyway 
You could extract to a new directory each time. If the unzip overwrites well then I think you just need to `sudo make clean` before `sudo make`.
Not sure I follow. If I run whiptail --title "Some title" --inputbox "Some text" 8 78 "Some String" The input box is pre-populated with `Some String`. Isn't that what you mean by default value?
The title reads like a riddle.
 IFS=$'\n' oldzipfiles= while true; do zipfiles=$(find "$1/temp" -iname '*.zip') #zipfiles now contains filenames of all zip files separated by newlines. newzipfiles=$(echo "$zipfiles" | grep -Fxv "$oldzipfiles") #Take off the zip files we already extracted. [[ $newzipfiles ]] || break #If no files are left, leave the loop for file in $newzipfiles; do #This for loop works because we set IFS to only the newline and we don't expect newlines in the filenames. unzip -q "$file" -d "$1/temp" #Extract to the $1/temp folder where the original zip file was extracted. done oldzipfiles=$zipfiles #All the zip files found at the beginning of this iteration have been extracted. done
I actually did try that and it didn't help. Deleting the linux-tbs-drivers directory was the only thing that forced make to do a full recompile (or whatever make does).
WHAT is your quest? 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/free_satellite_tv] [TBS tuner driver reinstallation script thread from r\/bash](https://np.reddit.com/r/Free_Satellite_TV/comments/3xro62/tbs_tuner_driver_reinstallation_script_thread/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
There's [`r.zip` on this page](http://research.swtch.com/zip) that'l endlessly unzip itself
You could use tee and make it print to the stderr file descriptor: `ifconfig | tee /dev/stderr | grep -q wlan1` or use sed: `ifconfig | sed '/wlan1/{h}; ${p; x; s/^$//; t err; Q 0; : err; Q 1}'` or awk: `ifconfig | awk 'BEGIN {status=1} {print} ($0 ~ "wlan1") {status=0} END {exit status}'`
You can also check with bash of course: `found=0; while read; do echo "$REPLY"; [[ "$REPLY" == *wlan1* ]] &amp;&amp; found=1; done &lt;&lt;&lt; "$(ifconfig)"; [[ $found == 1 ]] &amp;&amp; iwconfig || echo "wlan1 not present"` edit: added ` &lt;&lt;&lt; "$(ifconfig)"` after the loop
For the bash one I forgot to add the ifconfig command, it's updated now.
okay i will keep that in mind .....! thanks!
i do not mean to offend you but have you tried it out since you are confident ?
Get started by finding something you do frequently enough you would benefit from having a script do it for you. When you learn programming it's always seems better to have a solid idea of what you want to accomplish, a simple script to change your background every 5 minutes, or tell you if a certain set of applications are running. Or even just figuring out what distribution / version is currently running will give you problems that you can use bash / Google to solve.
Nice! Would be cool if it saved them to a particular directory and then scanned said directory every iteration and deleted walls older than 24hr or what have you. Gives the opportunity to save ones that you like without killing space. Just a thought. Also, support for other formats would be excellent. 
Also must read: http://serverfault.com/questions/633087/where-is-the-statement-of-deprecation-of-ifconfig-on-linux
There are plenty of resources linked in the sidebar to get you started. Another user said most bash guides on the web are junk. Don't let that discourage you from reading online sources and making distinctions for yourself. Since you're already 1 month into Python scripting, you should have no problem discerning data types, variables, functions, and loops. Most of what's left is becoming familiar with common system tools and capturing standard output data (AKA "STDOUT").
The other advice in this thread is great. I just wanted to chime in and ask what makes you want to learn bash scripting? I say this because you can do much of the same things with Python, unless you're looking into system administration it would be sort of redundant to know both. Since you're a beginner with Python I would suggest continuing to learn that, then possibly learning bash scripting later. Bash's syntax is different, and you'd likely be better off solidifying the fundamental concepts in the one you know best before trying a new language. Not to discourage you, just wanted to provide a counterpoint. 
hey thanks . i actually want to get into system administration(by that i understand it is related to database management , security systems ), that is why i started out with linux and hence bash otherwise i would have continued with windows. you have said actually what i wanted to do, should i continue this ? its also because i find to find out how systems and networks work very cool :P
actually what you are saying is pretty nicely written and i get what you say and yes i was confused between what i should do.learn python or go on to learning bash. your reply seems to make it clear that learning python at the moment would be a better choice rather than jumping ship . i have fallen into this trap before haha , i started out with c++ thinking i would become the most skilled programmer there ever was by watching series of 80-90 tutorials . but i pretty soon got hit in the head with a brick (thankfull, im speaking metamorphically) and realised that at the moment , apart from basic C++ , the language really wasnt doing me any good so then i moved to java thinking ANDROID! same thing happened. then i found out about python , when i found it i was like programming is too hard i cant do it, this just isnt for me even though i have been playing around with computers ever since i could remember . i was always able to work with hardware but i never even looked into programming (such a fool). but when i found python and started working with it(still a beginner , been only 2 months) , im starting to think i really could do something with programming and your answer just makes me realise that feeling of safety and trust that python gave me just when i started working with it for the first time. Thank You , it is always nice to see people helping! Good luck to you too!
I can't even... 
yes i do use checkio.org . somebody here told me about it. Its awesome! i do not know a lot but i will use that i actually am working on scripts that i came to know about through tutorials ...
thanks Unixtreme, I'm not the OP, but this was just what I was looking for. In my case it was openssl passwd -crypt -quiet somePassword. Very much appreciate it.
I know it's JS, but this is pretty neat for this kinda thing http://chancejs.com/ (and I'm assuming there are bash-y alternatives?) Also, two thumbs up to you sir. You are the kind of parent I want to be!
happy to help!
I just tried it. but it overwrites whats in the file. how can i append to the file ? 
That `echo file[12]` behaves the same in bash for me here and prints the file names if they exist.
You're right, I removed it.
Put double quotes around your variables to prevent expanding, etc. 
Bash has autocompletion that's provided by a plugin-type system. For example, on Debian you can get auto completion plugins for a number of common applications by installing [bash-completion](https://packages.debian.org/sid/bash-completion).
tee -a
Why would auto completion be default? I would rather have a slim shell that i can add features to when/if i want. theres a plugin called bash-completion that adds autocomplete to it if you want. 
I've test it but is not the same, and what about the autosugestions in fish is there any bash plugin?
No clue. I don't use Fish (although I tried it) so I don't know what "autosuggestions" is, and I actually don't use anything other than basic path completion for Bash on my own machines. I tend to find arbitrary autocompletion distracting. I know zsh has a huge community and fancy autocompletion. [Oh My Zsh](https://github.com/robbyrussell/oh-my-zsh) is probably the most popular set of packages.
thanks :D
thx for the feedback, can you give me some references where i can read and understand more about what you said?
[From the Bash guide linked in the sidebar](http://mywiki.wooledge.org/BashGuide/SpecialCharacters): &gt;Double quotes — protect the text inside them from being split into multiple words or arguments, yet allow substitutions to occur; the meaning of most other special characters is usually prevented.
tq
Wtf? Are you retarded? 
Why is the file called ".bash" and the shebang is a sh script?
Yeah, I have already been working through that.
Since I can't comment on the article itself I'll post my reply here: This is nice and everything, but the article was poorly written. It never states what a "GUI option" means or why it would benefit the reader. For example: &gt;Here are 1 line examples to get you started. xmessage: out=$(xmessage "Are you sure? " -buttons yes,no -print); echo $out How exactly does this get me started? If I'm a noob to shell scripting, how would I use this in my script to prompt a user for, let's say their eye color. What is *your interpretation* of the benefit with using a GUI prompt and not a prompt at the shell? What would be some real-world examples of using GUI prompts and how they might benefit me, the noob-ish reader? Like I said, it's nice to know there are options like "dialogue" and "zenity" available, and I'm glad you shared this to /r/bash, but the article barely explains anything to your average user. Thanks.
Then you're off to a great start. Are you searching file content, or file names, for a specific marker? What exactly are you targeting?
Greetings and thank you for the reply! The article does state that &gt; A few options for having a GUI from Bash are dialog, xmessage and zenity. I think that expands on the title of the page/article. This does bring my attention to a problem with the title however. Commands in Linux take **options** and arguments so I should use another word. Perhaps I should change the title to Bash GUI Capabilities or maybe Programs. The small examples show how data is retrieved from each program. They are also meant to whet the intellectual appetite and send one scurrying off to the man pages to dream up potential uses for the new found GUI capabilities. I think the calendar example at the very bottom serves as a good example of when you would benefit from Zenity over a simple prompt. Another great idea is the slider widget that comes with Zenity. The info on my page should give you an idea of how to send that slider output to your volume/audio manager and create a lightweight Zenity volume control shell script.
“Unless I see in his hands the mark of the nails, and place my finger into the mark of the nails, and place my hand into his side, I will never believe.” 
Echo sucks. A lot. Printf allows you to print *formatted* text. Which, especially in this case, allows you to make those like 5 echos for line breaks into... One line in printf. Sure, you can do echo -e (?) to make it parse newline characters... But printf is twenty times better. Also, re: ping... You need to shorten the wait, not the ttl. I forget the option but I think it was like -W
Probably not the answer your looking for, but why don't you use ssh keys instead? More secure, no stored passwords anywhere, and it works. 
See [BashFAQ 69](http://mywiki.wooledge.org/BashFAQ/069).
That "guide" teaches you bugs. See the guide in the sidebar instead. 
&gt; if $(zenity --question --text "Are you sure?"); then echo "true"; else echo "false"; fi This is silly. Remove the pointless command substitution if zenity --question --text "Are you sure?"; then printf 'true\n' else printf 'false\n' fi It also fails at quoting. `echo $out` is very different from `echo "$out"`. You always want the latter.
`seq` is a non-standard command that is best avoided. Bash can do fine without it. for i in {0..100}; do ... # or for (( i = 0; i &lt;= 100; i++ )); do ...
Hmm. There is room for improvement here: * Instead of using `find` and `read` to find the files in a single directory, it's much faster and more robust to use a for loop: `for LOG in wget*.log; do`. The only problem is that there will be one loop iteration for the unresolved glob pattern if no files match it, but this can easily be tested for. * bash offers built-in facilities for regex comparison and variable editing using parameter substitution. * Also, the `printf` builtin is better at doing formatting and its bash version can write directly to a variable. * your output redirection on `wget` is wrong; just `2&gt;&amp;1` is what you want. Try this: #!/bin/bash echo "What is the address?" read varname for LOG in wget*.log; do [[ -f $LOG ]] || break if [[ $LOG =~ wget[0-9]+\.log ]]; then NUM=${LOG#wget}; NUM=${NUM%.log} else NUM=0 fi printf -v NEXT_NUM '%02d' $((NUM + 1)) mv "$LOG" "wget${NEXT_NUM}.log.new" done for LOG_NEW in *.log.new; do [[ -f $LOG_NEW ]] || break LOG=${LOG_NEW%.new} mv "$LOG_NEW" "$LOG" done wget -p "$varname" 2&gt;&amp;1 | tee wget.log | tail -n 10 | mail -s "wget download complete" email@address.com
Filenames
I think what you want is to master [`find`](http://www.gnu.org/software/findutils/manual/html_mono/find.html#Invoking-find). No scripting required.
Gotta love this community! Thank you for your interest &amp; indeed this works as well (take your word on why it's better :)).
Thanks
1. I don't know what you mean by "simplify the directory search." It sounds like you need to recursively search a directory, that's all. 2. Using `find` should do what you want, but since you weren't more specific it's hard to tell. If your task is simple enough you may not even need `find`, but just wildcards. 3. Take a tutorial to learn the basics. There's really no shortcut here (except to have someone else write the script for you, which you've already said you don't want).
While I agree that one should not overdo how many different languages they know (after a while you're just learning different syntax), I think it's fine to learn both Python and Bash scripting in parallel. They're sufficiently different, moreover "70% perfect in python" is a meaningless statement anyway.
This would be my solution based on your request: #!/bin/bash read -p "What is the address" address email="ex@amp.le" logname="wget" [[ -f "${logname}.log" ]] &amp;&amp; { logs_array=( "${logname}"* ) logname="${logname}${#logs_array[@]}" # Note this assumes zero indexing, add one otherwise } || : logfile="${logname}.log" sudo wget -p "$address" -o "$logfile" &amp;&amp; \ tail "$logfile" | mail -s "wget download complete" "$email" edit: Removed `ls` for array via /u/KnowsBash
These answers are a Google search away. This should get you started: https://www.google.com/search?q=how+to+run+a+script+as+root https://www.google.com/search?q=how+to+use+username+and+password+in+bash Also, see if the VPN client has a command line interface (CLI). Look up the man page for that tool (e.g. "man openvpn") and see if you can insert the password in the same line. If not you'll need to use a special Bash tool to wait for a password prompt to enter it for you. 
 autoexpect sudo openvpn blah Log in as normal. Ctrl-c. mv -v script.exp connectvpn.exp chmod -c +x connectvpn.exp ./connectvpn.exp 
&gt; readarray logs_array &lt; &lt;( /bin/ls -1 "${logname}"*) Never parse filenames out of `ls`. logs_array=( "$logname"* )
Cheers
Your use of sudo may point to missing data in your post, perhaps a Ubuntu-like distro. I don't use no-root user distros, but in the case of some distros I use, I just add openvpn to the init system (rc-update?) and place the config (.ovpn) where apropiate. My fw is a Gentoo, I place 9 openvpn config files in /etc/openvpn, and make the corresponding simlinks (one for each config, making names match) in /etc/init.d, and add that ones to init. That's a setup for a server, but I have linux clients a well, and quite sure, you can add the username and password to that config files, so it's automated on every reboot, if the machine has a working connection after a normal boot.
Firstly, you are quoting the wrong things. Secondly, The `[` command does not do pattern matching at all. You want the `[[` keyword. read -rp 'Enter name: ' name while [[ $name = *[![:alpha:]]* ]]; do printf 'Your input contains at least one non-alphabetical character. Please try again.\n' read -rp 'Enter name: ' name done See [FAQ 54](http://mywiki.wooledge.org/BashFAQ/054) and [FAQ 31](http://mywiki.wooledge.org/BashFAQ/031) for more.
Almost certainly because he doesn't know you can chain commands together. Not everyone can memorize the details of every Unix utility.
You want to use the [`select`](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_06.html) command to prompt the user to make a selection. Use `head` to display the first three lines of a file.
Thank you for this. :)
`select` is a loop command, like `for` and `while`. Run `help select` to read about it, or look it up in the manual. select file in *.csv; do [[ $file ]] || continue # no file selected yet mapfile -t -n 3 lines &lt; "$file" # read three lines from "$file" select line in "${lines[@]}"; do [[ $line ]] || continue printf 'Doing something with: %s\n' "$line" break done break done
Awesome! That worked perfectly &amp; even lighten up my script. I have some difficulties to understand how the terminal automatically knows to output the error if the variable MAC is wrong tho. Still fairly new to bash haha. I used uppercase just to have a better visibility in my code. It's not advised? :/
&gt; I have some difficulties to understand how the terminal automatically knows to output the error if the variable MAC is wrong tho. The terminal has nothing to do with it. All commands return an exit status to indicate success or failure. 0 means success, 1-255 means various types of failure (what each failure code means is up to each command). When `if` runs a command, it checks the exit status. If it's 0, it runs the `then` block, if it's != 0, it skips to `elif` or `else`, if any. In the code above, `if` is testing the exit status of GOODMAC. GOODMAC is a function. The exit status of a function (or script) is the exit status of the last command executed within. In the case of GOODMAC, that command is grep. grep returns 0 (success) if it finds at least one line matching the given pattern, otherwise it returns failure. &gt; I used uppercase just to have a better visibility in my code. It's not advised? :/ Environment variables and special shell variables are all uppercase, so by using uppercase variables for other purposes you risk overriding environment variables or special shell variables. So yes, it's ill advised to use uppercase variable names. Functions in bash define new commands to run, and commands have a separate namespace. There isn't any danger in having uppercase command names; it just looks weird.
It sounds like you would be better off just scheduling your current script to run periodically using cron or a systemd timer.
Hopefully, your OS uses something better than init scripts for booting, something like upstart or systemd. In which case, you don't have to do much in the script. Just have it run an infinite loop, and don't background it. Write an upstart or systemd job for it.
What kind of system are you on? You can run the script as a cron job or set it up as a Launch Daemon.
Thanks for the input guys, I'm still in the process of writing it but all the suggestions are great.
I wrote this bash script: BackgroundProcess --help 'BackgroundProcess' can be used to launch/control a process running in background. Usage: 'BackgroundProcess -c Command [Param [Param...]]'. Launches 'Command [Param [Param...]]' in background. Outputs background process Id. Exitcode 0 if successful. Usage: 'BackgroundProcess ProcessPid'. Outputs background process' info (See below). Exitcode 0 if process is valid and no longer running (Info valid). Exitcode 1 if process valid and still running (Info not valid). Exitcode non 0 if not a valid process or any other problems (Info not valid). Usage: 'BackgroundProcess -f ProcessPid'. Checks if process with 'ProcessPid' Pid has finished running. Exitcode 0 if process is valid and no longer running Exitcode 1 if process valid and still running. Exitcode non 0 if not a valid process or any other problems. Does not output any info. Usage: 'BackgroundProcess -k ProcessPid'. Kills process with 'ProcessPid' Pid (Previously launched by 'BackgroundProcess'). Outputs background process' info (See below). Exitcode 0 if valid process and killed successfully (Or had exited already) and info read successfully. Background process' info: Text that when sourced (with 'eval') gives values to following variables: 'CommandLine': Command and parameters of process when launched. 'ElapsedTime_mS': Time (in MilliSeconds) the process was running. 'ExitCode': Exit code of process. 'StandardOut': Output generated by process while running. 'StandardError': Error output generated by process while running. Note: To avoid mismatch in the number of ",',`,´ (which would cause 'eval' to fail) each of them can be replaced by \" in 'StandardOut' &amp; 'StandardError' (Some commands output: `Error message') by using function 'CleanText' (See $COMMON_BIN_DIR/BashCodeRepository). Note: Normal usage sequence: Launch a process in background and get its Pid. Do other tasks. Check if backgroung process has finished running using 'BackgroundProcess' and Pid. Read info when backgroung process has finished. Source background process' info and use Variables' contents. If background process has not finished in the allotted time kill it with 'BackgroundProcess' (And get info of killed process). Please read $COMMON_BIN_DIR/BashCodeRepository for examples of usage. Here are a [couple of processes](http://imgur.com/tsdqwux) launched with the above script.
The `${#fileName}` is a mistake because of this here: $ test=(aa bbb cccc) $ echo $test aa $ echo ${#test} 2 What happens for you is `${#fileName}` is the number of characters in the first entry in the array. What you want is `${#fileName[@]}`, see here: $ echo ${test[@]} aa bbb cccc $ echo ${#test[@]} 3 In your `case`, I think you are making it dangerous when you write something like `[a-A]`. It should be `[aA]` without the `-` because the `-` is a range of characters, see here: $ if [[ b = [a-c] ]]; then echo true; else echo false; fi true $ if [[ d = [a-c] ]]; then echo true; else echo false; fi false The `loop--` has to be `(( loop-- ))`. Those `((` and `))` are to enable calculations. You did `rm $fileName` instead of `rm ${fileName[$loop]}` by mistake, same where you do `mv ...`. Where you use `|`, you really want `||`. That's why some things didn't work. Also, when the exit will start working, it won't print your message after it, so in that line you need to first print the message and then exit by doing `echo ... ; exit`. I would change things and try something like this, using another loop for reading input and the `break` keyword to get to the next file: for fileName in *.csv do while true do echo \"$fileName\": head -n 3 "$fileName" echo echo press ... echo press ... read -n 1 option case "$option" in [dD]) # delete "$fileName" break ;; [iI]) # ignore ... break ;; *) echo "Incorrect selection ..." ;; esac done done 
To post something as code, indent everything by four space characters in your editor before copying it to here.
`comm` can do that. $ comm -3 1.txt 2.txt 3 5 6 7 8 9 10 See [BashFAQ 36](http://mywiki.wooledge.org/BashFAQ/036)
The output for `diff -a -d 1.txt 2.txt` looks different for me. Something might be adding parameters to your diff call. Perhaps there's an alias in your bash config? When you use `-c` or `-u` with diff, you can add a number for lines of context. By default it's 3. You need to set it to 0, so `-c0` or `-u0` or `-C 0`, `-U 0`.
 diff is a tracked alias for /usr/bin/diff Sorry for asking such stupid questions :-)
It is probably a different version of "diff" than the ones usually used on Linux on PC. The machine using "ash" is important when you write (complicated) scripts. To translate bash scripts, there's this page here: http://mywiki.wooledge.org/Bashism?highlight=%28bashism%29 That "ash" has the same rules as what's mentioned in the "dash" column in those tables on that page. You could cut away the '@' lines with a second tool. You could use `grep -v "^@@"` to do it: diff -U 0 1.txt 2.txt | grep -v "^@@" The `^` means "start of a line", the two `@` are those characters you don't like, and the `-v` parameter reverses grep's normal behavior and deletes all lines that match the search pattern.
Oh, ok. I understand. Will have a look at that! Works like a charm - and thank you very much for the explanation!
Let's break this down line by line: **Line 1** lastFile=`ls $1 | tail -n 1` Don't use backticks. Use $() to call a subshell command. If your result is expected to be a string, and if it might have spaces, use quotes to capture the returned value. lastFile="$(ls $1 | tail -n 1)" **Line 2** echo $lastFile Use double quotes around variables. This prevents expansion and possible globbing. I also prefer to use curly braces around my variables. echo "${lastFile}" **Line 3** mv $1'/!('$lastFile')' $2/ Don't use single quotes around variables. Depending on your version of Bash, [single quotes sometimes treat the dollar sign like a literal character instead of a variable](http://www.howtogeek.com/howto/29980/whats-the-difference-between-single-and-double-quotes-in-the-bash-shell/). For portability, use double quotes like this: mv "$1"/!("${lastFile}") "$2"/ Lastly, you need to enable the exglob option in Bash at the top of your script ([reference #1](http://stackoverflow.com/questions/216995/how-can-i-use-inverse-or-negative-wildcards-when-pattern-matching-in-a-unix-linu)) if you are trying to use special inverse characters to negate strings from a list ([reference #2](http://stackoverflow.com/questions/670460/move-all-files-except-one)). shopt -u extglob The above references were found by doing two Google searches. I recommend doing some searching next time you come across a problem. 
The ordering is the same as ls in this case; lexicographical order, so this is the (so far only) right answer. A slight variation you can do, since bash 4.3, is to unset the last element with `unset 'files[-1]'`. files=( "$1"/* ) unset 'files[-1]' mv "${files[@]}" "$2"
Hm, that's intriguing. I'm sorry but I don't have an answer for you. I would think that because it's running in a subshell, the command isn't taking any primary bash ops? I dunno man. You got me stumped. If it were me I'd move those files using 'find' and 'xargs'. That's just me.
&gt; Don't use backticks. Use $() to call a subshell command. Unless they're nested, backticks and `$()` are completely synonymous and equivalent. &gt; If your result is expected to be a string, and if it might have spaces, use quotes to capture the returned value. Does not apply to assignments. You're assigning a single scalar value, so field splitting and globbing on expansions are impossible. &gt; Depending on your version of Bash, single quotes sometimes treat the dollar sign like a literal character instead of a variable[1] . Single quotes *always* treat the dollar sign (and every other character) like a literal character. This goes back all the way to the original Bourne shell. 
You make a good point. To further explain my point, since backticks have inconsistent behaviors regarding nested command substitutions and are harder to read within a larger script, I personally view them as inferior, and therefor deprecated. Many other people share this opinion, but that doesn't necessarily make my argument correct. It just means I'm a part of a different tribe.
I actually agree with you, `$(`…`)` is much better. I just don't view it as a reason to tell people off for using backticks, that's all :) 
You bet. Take 'er easy.
sdiff -s file1 file2 As an experienced UNIX admin. Learn also diff and patch :)
 grep -f fileA.txt fileB.txt Though note that this treats all lines of fileA.txt as patterns. If you really want to look for substrings instead, then include `-F` as well. And also `-x` if you want the lines to match exactly. As for reading a file line by line, you need a while loop. See [FAQ 1](http://mywiki.wooledge.org/BashFAQ/001).
this also works: while read title; do grep ${title} fileA.txt ; done &lt; fileB.txt 
Why need -r? seems OK in this instance without it.
Without `-r`, it will eat backslashes. $ read -r var &lt;&lt;&lt; '\f\o\o'; printf '%s\n' "$var" \f\o\o $ read var &lt;&lt;&lt; '\f\o\o'; printf '%s\n' "$var" foo 
If I understand you correctly, you want to print the lines that are common to both files. This kind of thing is what `comm` was made for. If both the files are sorted, `comm -12 fileA.txt fileB.txt` will do the job. If neither are, `comm -12 &lt;(sort fileA.txt) &lt;(sort fileB.txt)` should do the trick.
To answer your question, the script will exit leaving the screen session running and detached. I take it since you haven't posted any code that you haven't started yet. Give it a shot, then come back with any specific questions.
Thanks mate, will give it a shot tonight!
Consider reading the man page of `screen`. You can start a detached screen and run a shell script (or command or whatever) in there. [Here's](http://unix.stackexchange.com/questions/162133/run-script-in-a-screen) a helpful post to get you started. Good luck!
Why don't you lauch a new screen, run your script and detach the screen manually? Usually I keep my scripts running like this
Amazing... thanks!
Try adding the `-F` flag to avoid the lines in fileA.txt to be treated as patterns.
Tried using "grep -Fivf FileA.txt FileB.txt &gt; newresults" but to no avail! 
Make sure fileA does not contain any "hidden" characters, such as leading or trailing whitespace, or maybe it has CRLF line endings. This should help spot if this is the case: sed -n l fileA.txt
OP needs the replacement only on the "local" block. I forgive you, take a nap :)
This worked out perfectly! Thank you so much! I have never used awk before. I hate to bother you anymore but, if it is not too much work, would you mind trying to teach me what is happening here in this script? I actually have to replace the name, user, password and host of the local section. It would be nice for me to learn to modify the script to do that.
`-v awkvar=value` assigns an awkvar with the given value, and you can have multiple -v options to set multiple variables. You can use this to assign awk variables the value of shell variables. E.g. `awk -v user="$username" -v name="$name" ...` As for the script. The script consists of `condition { action }` pairs. Three of them in this case. When awk processes the input file, it applies all conditions for each line in turn. If a condition yields true, the corresponding `{ action }` is run. * `/^[^[:blank:]]/` this condition is true if the line starts (`^`) with a non-blank (`[^[:blank:]]`) character. I.e. an unindented line. The corresponding action assigns the first word (`$1`) to the variable named `section`. * `$1 == "password:" &amp;&amp; section == "local:"` this condition is true if the first word of the line is `password:`, and the section variable, which was assigned from a previous line, equals `local:`. The action does a substitution with the sub function (`sub(/pattern/, "replacement")`), which is similar to the substitution command in sed (`s/pattern/replacement/`) * The last pair is missing a condition, it only has the action `{ print }`. With a missing condition, the action is applied to all lines, as if there was a condition that was always true.
This link might help. http://www.thegeekstuff.com/2009/05/zcat-zless-zgrep-zdiff-zcmp-zmore-gzip-file-operations-on-the-compressed-files/
I'm getting the same results as geirha. Maybe post your input files if possible?
Wow you did it with both! That's is awesome! Like I said to the other person here that also did in awk, I have never used awk so thats really hard for me to grasp right away. But you were able to do it in sed which is great for me! Thanks you so much!
Personally, I'd use node.js, plus a little library like [bottleneck!](https://www.npmjs.com/package/bottleneck). That way you can use async to handle all the hard threading issues for you. The simplest approach in bash would be to use split to split your url list, and start 5 sub-processes. 
This is the right tool in my opinion
parallel is great for running things in, well.. parallel. The syntax can be a little tricky sometimes. It will run the number of jobs specified at the same time, and once one finishes, it starts a new job once one finishes, so it will always have the number of jobs specified running. Just specify the amount of simultaneous jobs with the -j flag: parallel -j 5 'curl -sO {}' ::: $(cat input.file) Give that a go
I can't for the life of me make it work. I posted the files in another response to geirha.
I just tested the awk example you gave me, it presonaly seems more natural and works perfectly. I assume I can just add "&gt; testfile" on the end to write my changes on the file. Thank you so much for your speedy reply!
&gt; sed "/^END:/q" &lt; testfile Note: this will just print that part of the file to standard output. If you want to *edit* the file, it gets a bit trickier. With GNU sed, you can do: sed -i '^/END:/q' testfile `-i` instructs `sed` to edit the file *__i__n place*. If you don’t have GNU sed, you’ll need a temporary file: sed '^/END:/q' testfile &gt; testfile.tmp &amp;&amp; mv testfile.tmp testfile --- Another possible solution could be to use `ed`, the UNIX **ed**itor, which allows us to operate on the text more freely than just in a line-based fashion. ed testfile &lt;&lt; 'EOF' /END:/+1,$ d w EOF This **d**eletes all lines between the first line after the one that matches `END:` and last line, and then **w**rites the result out.
&gt; I assume I can just add "&gt; testfile" on the end to write my changes on the file. Unfortunately not. Here’s what will happen. - The shell reads the redirections `&gt; testfile`. Since it’s a `&gt;`, not a `&gt;&gt;`, it means *overwrite* the file. To achieve this, the file is immediately *truncated*. - The shell reads the redirection `&lt; testfile`. - The shell starts the command (`awk`). Its standard input is set up to read from `testfile`, and its standard output is set up to write from `testfile`. Unfortunately, the file was already truncated, so there’s nothing to read; `awk` immediately exits, and the file remains empty. TL;DR: you can’t `&lt; file` and `&gt; file` the same file.
This is /r/bash, so here’s a more Bash-specific solution :) uses no external commands except for `curl`. #!/bin/bash declare -a urls1 urls2 urls3 urls4 urls5 while true; do read -r url1 || break urls1+=("$url1") read -r url2 || break urls2+=("$url2") read -r url3 || break urls3+=("$url3") read -r url4 || break urls4+=("$url4") read -r url5 || break urls5+=("$url5") done &lt; urllist for url1 in "${urls1[@]}"; do curl "$url1"; done &amp; for url2 in "${urls2[@]}"; do curl "$url2"; done &amp; for url3 in "${urls3[@]}"; do curl "$url3"; done &amp; for url4 in "${urls4[@]}"; do curl "$url4"; done &amp; for url5 in "${urls5[@]}"; do curl "$url5"; done &amp; wait I haven’t tested this, since I don’t have a list of 5000 URLs, but I think it should work. It first reads the URLs into five arrays, and then starts five jobs to download the URLs in those arrays. EDIT: You’ll probably want to either add a `-o` to `curl` or use `wget`…
&gt;Is there a easy/quick way where I could make a loop statement to run through and just append the "LPARNAME" to the end of the command line? Yes, using a for loop, and assuming you actually want the chsysstate command to run: for lparname in $(&lt;input); do chsysstate -r lpar -m PSERVER -o on -f normal -n "${lparname}"; done or formatted for use in a script: for lparname in $(&lt;input); do chsysstate -r lpar -m PSERVER -o on -f normal -n "${lparname}" done Alternatively you can use a while loop while read -r lparname; do chsysstate -r lpar -m PSERVER -o on -f normal -n "${lparname}"; done &lt; input or formatted for use in a script: while read -r lparname; do chsysstate -r lpar -m PSERVER -o on -f normal -n "${lparname}" done &lt; input Whether you use a for or while loop is up to you, some stricter bashers will tell you to use the while loop and give you a few good reasons why, but I'm of a softer opinion on this one - if it's a quick one-off and it gets the result, a for loop is fine. If you're going to script it, then use the while loop.
Besides http://www.catb.org/jargon/html/U/UUOC.html change "cat file|" at the beginning, to "&lt; file" at the end. Later don't edit the command of the result seems good, just type "^ echo" at the next prompt 
That is just.. puzzling to me.
Note: instead of adding [Help] to the title, you can click the “flair” button after you’ve submitted your post to add flair. [Screenshot](https://i.imgur.com/h5pXE4r.png)
Thank you for mentioning that.
*sigh* yes, the `while` loop is preferable. Nevermind the reasons, you’re presumably aware – but does the `for` loop have any advantages? Why do you recommend it over `while` in this case?
Is this correct: you have paragraphs separated by empty lines. In the paragraph where the first line contains "Topic I want", you want the last number of the line that starts with "Important"? If so: $ sed -n '/Topic I want/,/^$/ { /^Important/s/.* \(.*\)$/\1/p }' infile 17.00 What this does * Don't print anything (`-n`) * For the lines between the one containing "Topic I want" and an empty line (address range `/Topic I want/,/^$/`, do this: * If a line starts with "Important" (line address `/^Important/`), then do a substition of the whole line with what's after the last space (`s/.* \(.*\)$/\1/` command), then print it (`p` flag). If you know that there will be just one occurrence of what you want, you can quit after printing: $ sed -n '/Topic I want/,/^$/ { /^Important/ { s/.* \(.*\)$/\1/p q } }' infile 
OK, that works really well. I super appreciate the explanation of what is going on. My use of cli goes back a long time, but never very deep. Thanks again.
/r/DailyProgrammer’s Easy challenges can often be solved with Bash, though it gets trickier for the Intermediate and Hard ones.
I used unix.stackexchange as a great resource. You can start by reading answers to other people's questions, and when someone uses a command/operation that you don't know, you can learn about it, and from there you will often be exposed to even more new stuff. Whenever you learn a new concept, make sure to actually play around with it in a shell. Once you get a good grasp of the fundamentals, you can start practicing daily by actually answering people's questions.
[Advent of Code](http://adventofcode.com/) &amp;ndash; do these in Bash! Two more sites with challenges that support Bash: [HackerRank](https://www.hackerrank.com/) and [CodinGame](https://www.codingame.com) These are all general, algorithm oriented and not shell specific. You do learn a big deal about Bash by (ab)using it for these challenges, though.
Do these site provide bash solution for reference?
Also check rsync 
Using crontab seems to fix out a lot of the issues. However is it possible to do `echo "my password" | crontab command` instead of using a password-less ssh key?
No you can't pipe a password to ssh like that. You can use sshpass.
using ssh keys is the correct way of doing it. hardcoding your password is bad practice. 
I've always used cron as a regular user and it works fine. Just run crontab -e as the regular user and it will use that account. 
Ahh true.
So, uh.. friday, I *COULD NOT* make this work. I tried all kinds of different flags and it just wouldn't work. I show up at work today, monday and just to make sure I start fresh I run grep -Fivf fileA.txt FileB.txt &gt; fileC.txt and this time it just works. The computer hasn't even been restarted since I last tried.
No, because it tests if `wc -l` failed, not `test` which you intended.
Thank you for the help. As you can tell, I'm a beginner with bash scripting. I'm a bit confused by the regex for the first two examples. Would you be willing to help explain? I understand the ^ and $ anchors, as well as the curly braces stating to match preceding character $1 times, but all of the backslashes are honestly throwing me for a loop. Any help would be appreciated.
That `ls "$files" -1` you have there currently, do it like this: `ls -1q -- "$files"`. That hopefully takes care about everything that might go wrong with broken filenames. The `--` makes it so the `ls` can work on names like `-test`, the `-q` makes it so the directory listing won't print any characters that could confuse `wc -l`.
 grep -E "^.{${1?}}\$" awk "length == $1"
The version of `grep` you are using most likely does not support \\w, so to use the \\w atom, `egrep` must be used instead.
It does work: $ grep -x "\w\{6\}" &lt;&lt;&lt; "123456" 123456 $ grep -x "\w\{6\}" &lt;&lt;&lt; "1234567" #Fails $ grep -x "\w\{6\}" &lt;&lt;&lt; "123 56" #Fails
If you don't want to reinvent the wheel checkout [Scapy](http://www.secdev.org/projects/scapy/). And after you get familiar using it in interactive mode, you can [import](http://stackoverflow.com/questions/23269226/scapy-in-a-script) it on a script and create anything you want with it.
I'm currently using tgn, but for some reason it isn't showing up in wire shark or anything.
Try nc (netcat), aka "The TCP/IP Swiss Army": https://en.wikipedia.org/wiki/Netcat
If you’ve got root permissions, you can do this with flood ping: sudo ping -f -s 10000 -l 10000 -c 10000 1.2.3.4 This sends 10000 packets (`-c`), each 10000 bytes long (`-s`), without waiting for a reply (`-l`), without any interval between packets (`-f`).
So this could technically be used to simulate a busy network environment? 
Not really. This is more like an Mac Truck driving the wrong way on your car's lane; where you're looking to simulate for a busy, yet active full road-grid. [Simulating a busy network](https://en.wikipedia.org/wiki/Network_simulation) is a difficult proposition if you want anything near reality, but those wheels have been invented, as well. Edit: a word.
I need to be able to compare both a congested network with a none congest network, whilst holding a voIP call.
Bash can do only basic arithmetic operations like +, -, /, *. What is left in Heron's formula is square root. You need to use some kind of external program to calculate that. Some options are bc (man bc(1)), pearl, ruby, python or even C. These can be easily included in your bash script. This should get you started.
See [BashFAQ 22](http://mywiki.wooledge.org/BashFAQ/022).
Oh thanks completly forgout about this
Thanks for the input. I'll start looking into that immediately.
thanks.. am okay with some other short-cut too, and am familiar with vim as well so, `ctrl+w` works well for the example I gave :) but it is not equivalent of `Esc+Backspace` in tcsh.. for example: ls abc_xyz/ijk_123 `ctrl+w` will delete `abc_xyz/ijk_123` while I want only `ijk_123` to be deleted...
I have this in my `~/.inputrc` file to change ctrl-w to delete words only. # Kill the last word (emacs binding) "\C-w": backward-kill-word # Delete the whole line, not just from where the cursor is "\C-u": kill-whole-line If you want to make it `Esc+Backspace` you can more than likely doing that within `~/.inputrc` as well. `man readline` seems to think that would be `\e\b`, so something like `"\e-\b": backward-kill-word` might do what you want, but I haven't tested it.
There's a standard for the basic "/bin/sh" shell and it is only required to have that `[` you are trying to use. That `[` and the `test` program are the same program and documentation is in `man test`. In bash, there's a `[[` that can do a little more than `[`. That `&amp;&amp;` you are trying to use is in `[[` but isn't in `[`. You get documentation with `help [[` or somewhere inside `man bash`. Bash also has a `((` for working with numbers. I don't really know if you can mix `[[` and `((` like you are trying to do, but what you might want to know is that inside `((` you have operators for doing comparisons, not just calculations. You could completely replace the use of `[[` and just use `((`. Inside `((`, you can use `&lt; &gt; &lt;= &gt;= == != &amp;&amp; ||` like in C, Java, etc. You don't have to use an `$` on variable names. I think this might work (did not try it): if (( LastShrink &gt; (CurrentSize / 2) &amp;&amp; (CurrentSize + ExpectedSize) &gt;= FreeSpaceMB )); then echo "critical" else echo "ok" fi Inside `((`, you can also use `=` and set variables. You can do something like this: a=3 b=4 (( x = a + b )) (( x++ )) echo $x # this should print "8" You can't do `0.5 * something` in bash. It can only work with integers. You need a separate program "bc" for floating point stuff: x=2 y=$(echo "scale=5; sqrt($x)" | bc) Something else: if you want to have parts of your post look like code here, you need to add four space characters in front of each line of your code. If you want a bit of `code` inside a normal sentence, you surround it by backticks like `` `code` ``.
ropid, thanks for the info! it is much appreciated. i will give this a try right now. 
You're calling `ping` within your script. It's like pointing two mirrors at each other, in a way. It's called [recursion](https://www.google.com/?#q=recursion).
You have to reload the init file, you can do it either by logging out and back in, or restarting, or if you're lucky (I can't get this to work for me...) pressing C-x, letting go, then C-r. (Try C-xC-r, without letting go of control, if that doesn't work?) http://superuser.com/questions/241187/how-do-i-reload-inputrc
You are basically "fork-bombing" yourself. See this stackexchange page for a pretty good description of your problem and solution. http://stackoverflow.com/questions/19617161/bash-ping-function-looping short version, name your function something other than 'ping' *or* use the fully qualified path to the "ping" command. as is, your back-quoted command is calling the function, not the executable you expect.
You are most welcome.
I thought you were saying it was killing the whole line, not just a word, for whatever definition of word you want to use. I'm pretty sure you can define the word boundary characters, somehow, so it would not stop at `_`. (insert some light internet searching) Nevermind, looks like you might want the action `unix-filename-rubout` instead of `backward-kill-word`.
You probably mean `grep -q 5\.` (or `grep -Fq 5.`), but unless the release file starts looking weird, it probably doesn’t matter.
Your use of `tr` is wrong, and the parsing you are doing doesn't match up with the format you described.
The default readline keybindings are emacs-based; if you `set -o vi` you'll enter the wonder that is vi-based keybindings.
how am i using `tr` wrong?
here is an examples of the output `tr` is working with: LogFile 2: //SQL_PHOENIX/C$/Program Files/Microsoft SQL Server/MSSQL11.MSSQLSERVER/MSSQL/DATA/templog.ldf, 0.500000, 2097152, 0.0500000000, 96.101562 
 tr -d Free= ^ that removes all `F`, `r`, `e`, and `=` characters from the input. Your usage however, suggests that you are trying to remove the *word* `Free=`.
CDPATH belongs in ~/.bashrc, not ~/.bash_profile. And make sure you don't export it.
EDIT: I see `setup.sh` got updated while I was writing this. To avoid confusion, I was looking at setup.sh in commit [53946f5](https://github.com/Altoidnerd/gipaw-seed/blob/53946f5c48556a55848dfdd0a1198efa5873f034/setup.sh) while writing this. --- Ok, first things that strike me looking at your `setup.sh`: 1. Don't put `.sh` extension on a bash script. It is misleading since bash is not sh. 1. Don't use sed to edit the output of `date`, instead tell `date` to format it the way you want. E.g. `datestr=$(date +%Y-%m-%d_%H_%M_%S)` 1. Don't use uppercase variable names for internal purposes. You risk overriding special shell variables or environment variables 1. In bash, use `[[` to test strings or files, `((` to test numbers (ints), and `if ..` to test commands. Only use `[` in sh. 1. Never use `cd` in a script without testing if it failed. If `cd` fails, you'll usually want to abort the script, because all the remaining commands will be run in the wrong directory. in this case: `cd "../$dest_dir" || exit`. You might want to test some of the other commands as well. 1. Quote variable expansions and command substitution when using them in simple commands. Otherwise, word-splitting and pathname expansion will be attempted on the result. Bad: `mkdir ../$dest_dir`, Good: `mkdir "../$dest_dir"`. Missing quotes might be the reason why you thought you had to use `eval`. 1. Don't use `which`. Ever. It is always the wrong command to use. It's an external command, and non-standard. It behaves very differently on different systems. In this case you want to know where some executable exists in PATH, use the `type` builtin. `pwscf_path=$(type -P pw.x)`. 1. Don't use `echo` in new scripts. Especially not with options. Consider it deprecated and use `printf` instead. `printf 'Host:\t%s\n' "$HOSTNAME"` 
`eval` looks entirely unnecessary here. dateStr=$(date| sed 's/ /_/g') Or one step better, as geirha posted: datestr=$(date +%Y-%m-%d_%H_%M_%S) On top of what else has been said, you don't need to end each line with a semi-colon. 
Thanks, this is excellent feedback. I appreciate it. 
 if [[ !$(which gipaw.x) ]] then error_msg="\nERROR:\ngipaw.x is not in your path. Navigate to your QE distribution and \n\n\tmake gipaw\n\nto compile gipaw.\nAborting setup." $x $error_msg $x $error_msg &gt;&gt; meta.txt exit 1; else $x "gipaw.x:\t"$gipawPath &gt;&gt; meta.txt; gipaw.x | head | grep v --color="never" &gt;&gt; meta.txt fi Another style suggestion - and you'll find this is many style guideline docs - put your "do"'s and "then"'s on the same line: if [[ !$(which gipaw.x) ]]; then ... else ... fi See how the `if`, `else` and `fi` line up with the then out of the way? Next suggestion, for your test, you can use `type` as geirha suggested, personally I use `command` like this: if ! command -v gipaw.x &amp;&gt;/dev/null; then Finally, post your code into shellcheck.net and follow its suggestions.
 if ! command -v gipaw.x &amp;&gt;/dev/null; then I like this way, thanks.
Good to know. I'll have to change where mine is. 
See http://mywiki.wooledge.org/DotFiles
This looks awesome!
Thanks!
Looks fantastic. Clearly you put a lot of time and effort into this and I commend you for it. The only criticism I have is you use the prefix "\_\_crt__" for both variables and functions. I'm assuming you used two underline characters on both sides of "crt" so you can easily pick out certain variables within your script. If it were me I would replace "crt" with categorical codes or simple initials to denote type. __f__ExitNoStdout () "${__v__VerboseStatus}" But that's just me. Kudos!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
i guess not
Ah, that does make sense as well. Thanks!
So, four hours later, what did you figure out for yourself?
 #!/bin/bash myVar1=0 myVar2=0 myVar1=$(cat$1) myVar2=$(rev$1) rev $1 &gt; Revbases.txt echo echo "Bases are: " $myVar1 echo "Reverse is: " $myVar2 if [$myVar1!=$myVar2]; then echo yeaha! Palindromuh! echo else echo boo! No Palindrome; echo fi; That's your formatting fixed up... put four spaces in front of each line. If you have Reddit Enhancement Suite, there's a code button as well that will code format a block of selected text (obviously this won't help you if you're redditing via mobile phone).
Thanks guys! that helped! My mistake. In the API I was referencing XLS instead of "Excel" ..no wonder my format was jacked.
Something like this: #in calling script change #childProcess('bash test.sh' childProcess('bash test.sh databasename username password' #in bash script change: #mysql -uroot -p***** &lt;&lt;EOF mysql -u"$2" -p"$3" &lt;&lt;EOF #and the same for databasename
Can you explain what `parseFile` is supposed to do? **EDIT:** I'm guessing you forgot a `$1` there in `parseFile`, like so: function parseFile () { cat effects.txt | head -"$1" | tail -1 } The rest of the script might do what you were after like this: cat -n effects.txt LENGTH=`cat effects.txt | wc -l` # Choose a line while true; do read -p "What effect would you like? " num if [[ $num -ge 1 &amp;&amp; $num -le $LENGTH ]]; then echo "You choose:" parseFile "$num" else echo "Please answer a valid number" fi done
I had to read your post a few times, but now I think I understand what's confusing you. I think you were hoping to use the `for` loop to create multiple conditions in the `case` statement, like this: case $num in 1 ) echo "You choose:" parseFile 1 ;; 2 ) echo "You choose:" parseFile 2 ;; ... * ) echo "Please answer a valid number";; esac Well, that's not going to work. Along similar lines, though, you can loop over each number and check whether the input is equal to that number using just a normal `if`: for i in `seq $LENGTH`; do if (( $num == $i )); then echo "You choose:" parseFile $i fi done But that's a lot of extra work. What you should do instead is just verify that `$num` is an integer and in the correct range, and then you can pass `$num` directly to `parseFile`. while true; do read -p "What effect would you like? " num if [[ $num =~ "^[0-9]+$" ]] &amp;&amp; (( $num &gt;= 1 )) &amp;&amp; (( $num &lt;= $LENGTH )); then echo "You choose:" parseFile $num else echo "Please answer a valid number" fi done 
It works when you surround everything with `'` or `"` so that the `&gt;&gt;` is worked on by the shell that's started by `watch`: $ watch -n1 "./executable argument | tee -a logfile" Or you could do: $ while true; do ./executable argument | tee -a logfile; sleep 1; done The `| tee -a` is so that you can see the output printed on screen. A `&gt;&gt;` would hide it.
Because if you export it, any bash/ksh/sh scripts you run will inherit it, and a `cd foo` in those scripts could take them to a completely different directory than expected, and/or it creates unexpected output. In a script, you can guard against it by doing `cd ./foo` instead, but do you think every scripter out there has thought about that scenario?
 &gt;I can now delay further research in to ruby and python. Nooo
&gt;but if i set #!/usr/bin/expect I won't be able to use bash commands, am I right? That's right. expect is a different language and syntax... it's TCL, essentially. You need to look up some beginner expect scripts, don't just hack away blindly at this. You need to learn the syntax and understand how expect works. First link from google for "ssh login with expect" http://stackoverflow.com/questions/2823007/ssh-login-with-expect1-how-to-exit-expect-and-remain-in-ssh First link from google for "expect scripts for beginners" http://www.thegeekstuff.com/2010/10/expect-examples/ Go forth, learn, and always set a default action.
Thank you! Yes, you understood correctly what I was trying to do. I may have worded what I wanted to accomplish a bit sloppy but it was a really late hour :P With your and /u/ropid's help I now have a working script to control my leds. Thanks guys!
Thanks for the help! Yeah, I didn't notice that reddit fucked up the code somehow; in the file the function parseFile already had the $1, but reddit didn't want any of that... Thanks anyway, I'm now able to control my leds with this bash script!
Expect is usually not the best option. But if you want to use it, make a script like this: #! /usr/bin/env expect set host [lindex $argv 0]; set user [lindex $argv 1]; set passwd [lindex $argv 2]; set timeout 5 spawn ssh $user@$host expect { "yes/no" { send "yes\r" exp_continue } "password:" { send "$passwd\r" exp_continue } "$user@$host:~? " { exit 0 } timeout { exit 2 } } exit 1 replace "exit 0" with anything you want to do when logged in. Execute the file by running it with *./filename hostname username password*. Spawn is the command for running another process.
I would even save the second lines assignment by doing `lines=${1:-50}` in the first declaration. An early exit would result in no issue with invalid user value.
You don't actually need to have a list of the files, if all you are using it for is to make copies. Try this: find folder/ -name *.png -execdir cp {} /full/path/to/destination/folder/ \; 
sure they're different? they seem to do the same.
The external one doesn't have the unary operators `-o`, `-v` and `-R` for instance.
Yes, they are listed in `help test`.
Cool! Thanks for letting me know!
You can even look for all the file types you want at once too find folder/ -iname "*.jpg" -o -iname "*.png" -execdir cp {} /full/path/to/destination/folder/ \; The iname in there is also helpful since it will do a case insensitive search (just in case you had pic01.JPG, pic01.jpg, pic01.Jpg)
Nope, you don't have to type a password as long as you don't set a passphrase when you generate your keys. 
It would be nice if it didn't append that newline, but this feature was added about 15 years ago, so it's a bit too late to "fix" it now. One alternative is to use process substitution. wc -c &lt; &lt;(printf hi) 
the variable behaves like stdin not like a file
In UNIX-style command line programs, the convention is that every line ends with `\n`, rather than `\n` being used to *separate* lines. Given this convention, it makes sense for `&lt;&lt;&lt;` to add a newline to produce what is considered a line of text by most other tools. `wc -l` is a particularly good illustration of this: $ printf 'hi' | wc -l 0 $ wc -l &lt;&lt;&lt; 'hi' 1 &gt; Shouldn't &lt;&lt;&lt; behave exactly like printf? I don't see why it should. If you want `printf`'s behavior, just use `printf`.
You're right! stdin ends with a newline as well &amp;ndash; maybe that's why here strings do?
But echo isn't POSIX, printf is. EDIT: The bash implementation of echo isn't POSIX. POSIX echo works very differently. See POSIX echo "foo\tbar" vs bash echo "foo\tbar" vs bash echo -e "foo\tbar"
&gt; $ printf 'hi' | wc -l &gt; 0 Ok, that's a really good example, and it drives me insane. So printf 'hi' | wc -c will output an intuitive result, but printf 'hi' | wc -l will output an unintuitive result. Whereas echo 'hi' | wc -c will output unintuitive result, but echo 'hi' | wc -l will output an intuitive result. http://i.imgur.com/u3lWw.gif
Turns out I don't know why there is a newline in here strings.
Um, what? Both echo and printf are in POSIX.
I should say bash echo isn't POSIX, but then bash itself isn't either so vOv
You can either create 2 scripts and then call the expect script from your shell script passing whatever arguements, or redirect commands to expect from your shell script. script 1, shell: #!/bin/bash user="me" pass="1234" host="my.box.com" #call expect script ./expectssh.exp $user $pass $host exit script 2, expect: #!/usr/bin/expect set user [lindex $argv 0] set pass [lindex $argv 1] set host [lindex $argv 2] spawn ssh $user@$host expect "Password:" send "$pass\r" expect "#" or, shell script with expect redirect: #!/bin/bash user="me" pass="1234" host="my.box.com" #pass vars to expect /usr/bin/expect &lt;&lt; EOF spawn ssh $user@$host expect "Password:" send "$pass" expect "#" EOF exit As others have suggested, setting up keypairs really makes expect unnecessary so if you have the power to do so, then you should pursue that route as it is the simplest. However, there are occasions where you have no control over the host and for those scenarios I find expect fits the bill quite well. I alternate between the 2 scenarios I outlined above, depending on the complexity of the expect script. If it's a simple connect, drop a file, disconnect I prefer to just redirect to expect from my shell script. If I have a more complex scenario, then I will separate the two so that debugging is a bit easier. 
You've found two shells where `echo` behave differently. That doesn't mean bash's echo is not POSIX. Though POSIX recommends against using echo for this very reason.
Open a new file, type "hi" and save it. If you run `wc`, it'll also show 3 characters. &gt; &lt;&lt;&lt; was essentially passing a string to a command Not exactly. &lt;&lt;&lt; is a "here-string". The string is treated as file and then passed to commands. $ wc "This is a bare string. This won't work because wc expects a file" wc: This is a bare string. This won't work because wc expects a file: No such file or directory $ wc &lt;&lt;&lt; "This is a here-string; it is treated as a file." 1 10 48 Hope this helps explain why a new-line is inserted. Check the [man page](http://www.tldp.org/LDP/abs/html/x17837.html) for more info.
You're kind of right. `&lt;&lt;&lt;` is a [here-string](http://www.tldp.org/LDP/abs/html/here-docs.html). The string is treated as a file.
bash’s `printf` isn’t POSIX either: $ /bin/printf -v -v$ printf -v bash: printf: -v: option requires an argument printf: usage: printf [-v var] format [arguments] $ 
Can you give an example of such a case? AFAIK, if a command takes a file argument like so `command file` then a here-string should also work `command &lt;&lt;&lt; "string"` because a here-string is equivalent to a file containing that string. echo "abcde" | wc wc &lt;&lt;&lt; "abcde" str="abcde" echo $str | wc wc &lt;&lt;&lt; "$str" echo "abcde" &gt; file wc file Eg. all of these produce `1 1 6` as the output as `wc &lt;&lt;&lt; "$str"` is identical to `echo $str &gt; file; wc file` I'd like to know if this is not the case.
Are you sure? My `man 1p printf` says: &gt; **OPTIONS** &gt; None.
Your examples works for all kinds of input because `wc` reads standard input when there is no file, see [man page](http://man7.org/linux/man-pages/man1/wc.1.html): &gt; With no FILE, or when FILE is -, read standard input. If we have an executable that does not read from standard input, such as this dumb version of `cat`: #!/bin/bash while read line; do echo "$line"; done &lt; "$1" it will complain if fed from standard input: $ ./fileonly &lt;&lt;&lt; "abcde" ./fileonly: line 2: : No such file or directory but will work with process substitution: $ ./fileonly &lt;(echo "abcde") abcde The vast majority of unix utils (`cat`, `grep`, `sed`, `awk`, `wc`, ...) read from standard input if there is no input file specified, so in most cases, both work &amp;ndash; but often behave slightly differently, as seen in your `wc` example. Another example of a small difference: if you pipe multiple files to `grep` as in `cat * | grep 'pattern'`, matching lines are shown without the filename (of course, because grep doesn't get that information, it only gets a stream of lines), but if you specify multiple input files as in `grep 'pattern' *`, by default grep will prepend matching lines with the file name.
This is tangential to your point, but I know a certain popular disk operating system which doesn't end files (nor command output) in newlines, so compensates by printing a newline before each prompt. Though, it leads to hell when you have to think about your prints in your program, and not print a newline if it's going to be the last print you make, or prepend every print with a newline so long as it's not the first, etc. And it goes without saying that if lines are delimited with a newline character, that makes parsing simpler too.
Specifically it's treated as an already-open file on file descriptor zero (i.e. stdin). The same as if you'd used a pipe or another type of input redirection.
But are you going to implement it?
That means printf is not required to have any options.
But wouldn’t potential option support require some sentence like &gt; The read utility shall conform to the Base Definitions volume of POSIX.1‐2008, Section 12.2, Utility Syntax Guidelines. ?
See the description for *OPTIONS* under [1.4 Utility Description Defaults](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap01.html#tag_17_04): &gt; **Default Behavior**: When this section is listed as "None.", it means that the implementation need not support any options. Standard utilities that do not accept options, but that do accept operands, shall recognize "--" as a first argument to be discarded. 
After hours of doing something new, you can become impaired ... I was struggling with the date formatting as well as printf which is why I reverted to hackery. Feeling fresh today though ... I thought about it some more and am ready to roll. I really respect your precision. I got fed up with it that and took a break to think about it, but I'm going to roll these out right now. You're a great help, this is just what I needed. 
Just do nest three while read loops, and run printf from the inner loop..? http://mywiki.wooledge.org/BashFAQ/001
Here's something that works with any number of files: #!/bin/bash combine () { # $1 &lt;- accumulated names # $2 &lt;- file for a set of names to add # $3,$4,... &lt;- rest of the files local text="$1" local file="$2" if [[ -z $file ]]; then echo "${text#; }" # there's a ";" at the front that has to be cut away else while read name; do combine "$text; $name" ${@:3} done &lt; "$file" fi } combine "" Red Green Blue | shuf That "shuf" program that's called at the end randomizes the lines in a file. If it's a fixed set of just those three files "Red", "Green", "Blue", you can do this: while read red; do while read green; do while read blue; do echo "$red, $green, $blue" done &lt; Blue done &lt; Green done &lt; Red | shuf The first method with that `combine()` function does something very similar, except it builds up those nested loops over the different files through calling itself. With each call, one file name is dropped from the list of file names, so the loops stop getting nested when there's no more file names.
Not a complete solution, but instinctively I think of {} Try "echo {a,b,c}{foo,bar}", the result might interest you.
Sure enough, that works! Thank you so much! I'd love to hear the explanation when you have the chance! The memory overflow is quite baffling.
You've aliased `ls` to be `__ls`. So when you use `ls` inside of the function, it expands to `__ls`. Recursion! I prefer to use `command ls` instead of `/bin/ls`. `command` is a bash built-in that's meant for basically this exact scenario. Try `help command` for details.
[GIFV link](https://i.imgur.com/fnOYxNL.gifv)
You forgot parenthesis: find folder/ \( -iname "*.jpg" -o -iname "*.png" \) -execdir cp {} /full/path/to/destination/folder/ \; Otherwise it will only copy png files.
I don't rly code in bash much but I noticed u used an assignment = when you maybe meant a comparison == Maybe that's legit and u know what ur doing but just in case
In `[ ... ]`, both `=` and `==` are comparison operators in bash. There's no difference. In POSIX sh, only `=` is valid of the two.
An old-school option would be dc (desk calculator). It uses a very terse syntax called reverse polish notation (RPN), which is strange at first, but can be very fast to type once you learn it. s = (a + b + c) / 2 translated to RPN would be, a b + c + 2 / and area = sqrt(s(s - a)(s - b)(s - c)) would be, s s a - * s b - * s c - * sqrt So in with dc, #!/bin/sh a=$1; b=$2; c=$3 dc &lt;&lt;EOF 4k # number of decimal points $a $b + $c + 2 / ss # calc s ls ls $a - * ls $b - * ls $c - * v p # calc area EOF
If you go to the trouble of putting the commands in separate files, then you can just add that directory to $PATH.
And make them scripts with error handling and -h support! /s
Are most of the files in `~/.bash_aliases/` single lines?
&gt; POSIX complaint environment Seems about right. 
This is a known issue currently being discussed on the hasl mailing list. It should be fixed by version 0.13.0-1
The fact is that this still uses bash' `alias` internally, so this does not have the call-time overhead of spinning up another bash instance. It only reads from the file system when `load_aliases` is called. For actual scripts that are more complicated than a one-liner and that I might want to call from a non-interactive session, I have my `.bin` directory, whis is in my `$PATH`
This would not work in the current version of the code. But I did think about it. Either I pipe the output through `sed` so that it removes comment lines and adds semicolons at the end of normal lines (could be tedious and bug-ridden) Or I could only support single line scripts with comments before them and read them with `tail -n1` See [this](http://www.reddit.com/r/bash/comments/42twac/everything_is_a_file_for_aliases/czdkose)
You could load them as functions instead of aliases.
It seems something like `function $variable() { echo foobar; }` doesn't work. You'd need `eval` which brings a lot of problems with it.
Actually, it might work with source. First to remove all functions like you did in your code: `unset -f $(compgen -A function)` Then to source files like functions: for f in ~/.bash_aliases/*; do . &lt;(echo "function $(basename $f)() {" cat "$f" echo '}') done
So, if I have an alias `alias l='ls --color=always -FLH'`, I'll create a file called `l` and the contents of the file will be `ls --color=always -FLH`. Is that correct? If yes, then I don't understand what this accomplishes? Why not have all aliases in a single file? What is the advantage of doing it like this? If I have about 100 aliases, what's the benefit of having 100 different files instead of a single one? P.S. No flaming intended, I'm genuinely curious.
Your reasoning is correct. I did this because I use a rather small amount of aliases, but also quite a lot of shellscripts in a PATH folder. I wanted to clean up my dotfiles and not have to tinker with my main `.bashrc` files when changing aliases. Also, I liked doing it just for fun. I don't know if I'll keep this setup, but I did it because I can.
So ... instead of having a section of `~/.bashrc` dedicated to aliases, that can support comments (if needed), or a single file that is sourced by `~/.bashrc`, you want to create a complex 'solution' that only supports (at the moment) single-line files in a directory? Why overcomplicate this? What is the win, in your mind?
How about something like checking if `~/bin` is in your PATH, and if not, add it... something like: if [[ $PATH != ?(*:)$HOME/bin?(:*) ]]; then PATH=$PATH:$HOME/bin export PATH fi Parts of the code you want to split out should be able to be self-contained scripts, right? So split them out into `~/bin` and call them as you would any other program. Usually with functions, converting them over is as easy as s/return/exit/g Aliases and handy tidbits can either stay in your `.bash_profile` or they can be split out into individual directories as discussed elsewhere in this thread, probably kolme's post is the one I agree with most. Then if your code is on github, it's just a `git pull` away.
I do mine like this. In bashrc is the following snippet. this allows me to customize it for different environments and machines. run_scripts() { for script in $1/*; do # skip non-executable snippets [ -x "$script" ] || continue # execute $script in the context of the current shell . $script done } run_scripts $HOME/bashrc/bashrc.d
Now that I think of it... None. I should really reconsider this. 
Hmm, I've never had issues without them, but probably safer to just use them. Thanks!
I do something similar to this. I keep several files in ~/.etc/bash: bashaliasesrc bashoptionsrc bashrc environmentrc functionsrc greprc hyperjumprc keybindingsrc lessrc lsrc todo_completion Then, in ~/.bashrc I have a simple for loop: for i in ~/.etc/bash/*; do source "$i" done I like your function better because it allows for non-executable files in the same directory without introducing errors and lets you have several directories with related scripts grouped together. 
[THIS](http://s23.postimg.org/ga0zw3zln/Screen_Shot_2016_01_27_at_9_09_00_PM.png) happens when I try
 pwd cd ~/Desktop 
You're using MacOS, which uses a braindead filesystem called HFS+ and often causes Bash and other BSD programs to behave in very quirky ways. (This was one of the reasons I switched to Linux). HFS+ is not case sensitive, whereas BSD software like Bash **is** case sensitive. It might be that you have a directory called `desktop` with a lower-case D. Try `ls -dl ~/desktop ~/Desktop` to check if either exist. Your may need to rename the directory twice to fix it: mv desktop desk; mv desk Desktop; 
I don't understand what you're trying to do. As far as I know, `unset` doesn't output anything, so piping it to `cut` doesn't make sense. It might help if you also showed the other things you tried that didn't work (what exactly did you try wrapping with `$()`?).
Right. I'm trying to send the whole output of `$line | cut -f1 -d"="` to unset. This is what I've tried: unset $($line | cut -f1 -d"=") 
Ah, you're just missing an `echo` in there. `echo $line`.
Ugh. I'm an idiot. Thank you so much! I feel like I'll never learn all the weird little nuances of BASH.
Thanks for the explanation :D. i just learn something new 
thanks! 
&gt; Bash is also not a BSD program It is a GNU utility, but Mac OS X uses the port of Bash taken from Darwin, which is BSD UNIX, so yes it is a BSD, and the fact that it comes from the GNU project is irrelevant here because this person is obviously a beginner, so why would I bother explaining that? &gt; You just need to change the option at disk format time. Too bad that isn't the factory default. &gt; it doesn't matter whether Bash is case sensitive here Except wrong, it **does** matter because when you execute `mkdir Desktop` with an upper-case D, the argument you pass from Bash is case sensitive, but when it checks if the directory entry already exists and if the filesystem is case insensitive, the operating system will report that the directory entry already exists if there is a directory called `directory` with a lower-case D, and will therefore fail with an error message. &gt; Please don't offer irrelevant advice or commentary that would only confuse people. You are the one being pedantic here, not me. I have been bit by this problem myself, I am speaking from experience. I simply assumed the user here had not made such a simple mistake as to be in the wrong directory.
&gt;It is a GNU utility, but Mac OS X uses the port of Bash taken from Darwin, which is BSD UNIX, so yes it is a BSD, By that logic, would you consider GCC to be "a BSD program"? Of course not. That's absurd. Bash is not and has never been considered part of the BSD userland. Period. A simple blanket branding of "UNIX programs" would cover things from both the GNU and BSD userlands very nicely and would also match Apple's official branding of said software. &gt; and the fact that it comes from the GNU project is irrelevant here because this person is obviously a beginner, so why would I bother explaining that? Gee, that's a pretty good question. It probably has the same answer as, "Why would we bother explaining to a beginner what a BSD program is?" In other words, it's no more or less relevant to this situation than the fact that Bash came to Mac OS X via Darwin. Far more relevant to OP's interests would be the fact that Mac OS X's Bash is ancient. It's not really relevant here either, though. &gt;Except wrong, it **does** matter because when you execute `mkdir Desktop` with an upper-case D, the argument you pass from Bash is case sensitive, What does this even mean? The `directory` argument by itself is case sensitive? What? That statement has no meaning on its own. You could have dropped &gt; the argument you pass from Bash is case sensitive, entirely and your comment would have made more sense. &gt; but when it checks if the directory entry already exists and if the filesystem is case insensitive, the operating system will report that the directory entry already exists if there is a directory called `directory` with a lower-case D, and will therefore fail with an error message. This is true but is completely irrelevant here. Let's imagine for a second that OP wasn't in the wrong directory. Here's what happens on a case-insensitive file system: [op@op-mac ~]$ mkdir ~/Desktop # works fine--directory already exists mkdir: /Users/op/Desktop: File exists [op@op-mac ~]$ mkdir ~/desktop # same mkdir: /Users/op/desktop: File exists [op@op-mac ~]$ cd desktop # works fine [op@op-mac ~/desktop]$ cd .. [op@op-mac ~]$ cd Desktop # also works fine [op@op-mac ~/Desktop]$ cd .. (Actually, the same thing would happen on a case-sensitive filesystem. Since, you know, Bash and `mkdir` (if you are not using Bash's builtin `mkdir`) are smart enough to just...ask the OS to handle the file I/O for them.) A directory exists under either the name `Desktop` or `desktop` and prevents OP from creating another one with the same name, as you said. But the practical consequence of this on using `cd` to enter the directory is none, since that information is largely ignored and/or hidden by Bash. Bash doesn't care about case sensitivity of the underlying filesystem. That is why both of your comments and your entire line of thinking is wrong. The OS handles the case conversion, if necessary. Bash doesn't care. At all. OP's problem was obviously _not_ being caused by the case insensitive filesystem, as my example also demonstrates: so long as either `~/Desktop` or `~/desktop` exist, as evidenced by `mkdir` refusing to re-create them, it is possible to `cd` to either `~/Desktop` or `~/desktop` and wind up in the same directory. That is why your comment is irrelevant. Generally, there are 2 major classes of issues running on a case-insensitive file system: 1. The software author anticipates a case-sensitive filesystem, and generates filenames in some manner, e.g. randomly, that result in unexpected name collisions on a case-insensitive filesystem. This is obviously not the case here, since we don't want to create both `~/Desktop` and `~/desktop`. So, again, bringing up case sensitivity is irrelevant. 2. The software author anticipates a case-insensitive filesystem (since that is the majority on this OS) and writes software that fails to find files on a case-sensitive variant of the same filesystem. As I recall, this happened to Steam. We don't really know what OP has here, but if you had assumed that OP was in `~` the whole time, then the sequence of commands and their outputs as executed still would not have occurred on either a case-insensitive or a case-sensitive filesystem. Both `mkdir` and `cd` use the same BSD system call `stat(2)` to verify the existence of a directory before attempting any operation on it so there would be no reason to expect, under normal circumstances, that `mkdir ~/Desktop` followed by `cd Desktop` from within `~` would have inconsistent results from one another (abnormal circumstances would be bugs in the software or someone deleting `~/Desktop` after it was created). So, again, bringing up case sensitivity is irrelevant. I don't have a lot of insight into how HFS+ is implemented but the fact that it supports case-insensitivity as well as it does is, frankly, an achievement on its own. There's a lot more to it than CS101's "subtract or add the right number to the ASCII characters and get the other case". You have to support tons of locales and scripts, too, or you'll get a situation where, in the eyes of the user, some files are case-insensitively-named and some aren't. And since it does have a case-sensitive option, the fact that it can be case insensitive seems to be a really poor justification for calling it "braindead". Calling Apple brain-dead for making case insensitive the default would make a lot more sense.
There is a lot of bugs and bad practices in that script. You are lucky it "worked" on one system alone. For debugging, try copy/pasting one and one line from the script until it doesn't produce the result you intended.
you might be interested in having vi keybindings in the shell, for that just put the lines "set editing-mode vi" and "set keymap vi-command" to your ~/.inputrc. btw, you can also use "cd !$" for your first example, and &lt;ctrl-w&gt; for the second one (independently of whether you have vi- or normal readline bindings).
Since the typical "Windows keyboard" does not have a Meta key, Alt is usually mapped as Meta instead, but as you found out, Alt-key combinations are often "eaten" by the window manager, so bash/readline never gets them.
I'm by no means saying that I'm good at writing scripts in bash, rather the opposite since I just started doing it. However I think you're thinking about the HTML stuff reddit added once I posted - they're definitely not in my script file. Of course you could be seeing other bugs, and for them I can only explain with I'm bad at bash and searched the web for something that eventually worked. The Mac successfully runs the scrip, writes all the color names to color.txt and then sends the right name to the remote machine. It must be something with line endings or something similar I figured, because the hyperion-remote answer something like blue != blue ... Thanks for answering though!
This could be it. On OSX OP may need to install the GNU version of some binaries, generally prefixed by 'g', e.g. gsed for the gnu version of sed. Or maybe the command line arguments use different flags on BSD.
&gt;However, while executing exactly the same command and script on the Mac, the script sends the same commands to hyperion-remote on the server, but it responds with something along the lines "not a valid color". ... &gt;It must be something with line endings or something similar I figured, because the hyperion-remote answer something like blue != blue ... What would be useful is if you could copy and paste the actual command and error message back to us here, rather than paraphrasing it. Also, try: set -x [your script] set +x That will give you some debug output that may reveal something. And again, copy and paste the output to us here. ~~Based on what you have given us, though, I'm going to make a wild guess that it may be in this line:~~ ~~&gt;$SSH $REMOTE -c $COLOR -p $PRI~~ ~~That fleshes out to:~~ ssh root@192.168.0.120 -p 22 -t hyperion-remote -c black -p 0 ~~OSX, being based in part on BSD, unsurprisingly uses a lot of the BSD toolset. BSD commands are less finicky than their GNU counterparts about where you put args, so I wonder if `ssh` on OSX continues that behaviour, and is trying to interpret the -c and -p that you've intended for hyperion-remote? Try restructuring it more like this:~~ ssh -p 22 -t root@192.168.0.120 "hyperion-remote -c black -p 0" ~~Run each manually and see if there's any difference.~~ /edit: fix was accepted [here](https://www.reddit.com/r/bash/comments/432owa/bash_function_works_on_windows_cygwin_but_not_on/czf1crn) while I was typing this up With regards to your noobie habits, let's take this one line as an example: &gt;LENGTH=\`cat $file.txt | wc -l\` * don't use uppercase variables unless you mean to. Uppercase is used for globals/environment variables, which you risk clobbering. Use TitleCase, camelCase or Snake_Case instead. * Double quote your variables to avoid globbing and word splitting. I tend to use {} around my variables too. The reason being that people seem to use UPPERCASE as a visual aid, when you stop using uppercase variables (as you should) and you still want a visual aid, "${Variable}" works nicely. * While on the topic of visual aids, some people like to prepend the names on their variables, functions and arrays e.g. "${v_SomeVar}", f_SomeFunction(), "${a_SomeArray[@]}". * don't use backtick command substitution. Backticks are effectively deprecated, they belong in 1982, and are only really acceptable practice for SVR4 scripts. If you're not writing a preinstall script for a Solaris pkg, you really have no reason for using backticks. Use $() instead e.g. `$(cat $file.txt | wc -l)` * That's a [Useless Use of Cat](http://porkmail.org/era/unix/award.html). `wc -l &lt; "${file}.txt"` does the same thing.
&gt;Since the typical "Windows keyboard" does not have a Meta key, Alt is usually mapped as Meta instead Where'd you get that? The meta key == the windows key. 
You do `[ -n $key ]` in your script, and that does not work. It always returns "true", no matter if the variable is set or empty. You need to do `[ -n "$key" ]` or `[[ -n $key ]]`. See here: $ echo "&gt;${testvar}&lt;" &gt;&lt; $ if [[ -n $testvar ]]; then echo true; else echo false; fi false $ if [ -n $testvar ]; then echo true; else echo false; fi true $ if [ -n "$testvar" ]; then echo true; else echo false; fi false The `[[` command is only in bash, not basic "sh". It protects you against weird errors like this that you can do when using `[`.
Yeah their invocations tend to be different ( what the flags do and how it expects the arguments to be formatted). 
The mysql terminal client allows me to designate where to store the log, but it's an interactive session, so I don't want to just pipe stdout to gzip. So, I've got a script that creates a fifo, tells gzip to compress anything directed there, and then tells mysql to use that same location for is log. I have logs that are compressed on the fly.
That is very clever. Nice!
where did $line come from and what is the effect you want to achieve? 
Seems what ever combination I do with alt is not working since the window for my ssh client steals it. Esc always works, so does that mean that Esc is my Meta ? 
&gt; What does this even mean? The directory argument by itself is case sensitive? What? It means Bash does not perform normalization (like case conversion) on the argument, it passes the string directly to the operating system call as it is. Contrast this with an SQL interpreter. It hadn't occurred to me that `cd` should have worked if a directory of a different name existed, at first glance OP seemed to be having a problem that I experienced a little while back where `mkdir` failed to create a directory, but `cd` also failed, because a file with an uppercase version of the name I was trying to `mkdir` already existed. Not that it matters to OP, but you should know that several of the Bash scripts I wrote for Linux have failed to work, or behave in very unexpected ways, when I try to run them on Mac OS X, partly because of HFS+ being braindead, and partly because Darwin uses older versions of the GNU userland utilities. So I have come to distrust and despise Bash on Mac OS X.
Ctrl+C should cause all processes in the process group to receive SIGINT, including the shell itself, sudo, openvpn and the seds. So what you really want is for sed to ignore SIGINT, so that it instead exits normally when openvpn closes its end of the pipe. Simplify the problem by combining the two seds to a single sed. EDIT: try filter () { trap '' INT sed -r -e "s/([[:xdigit:]]{2}[:|-]){5}[[:xdigit:]]{2}/${__esc}[1m&amp;${__esc}[0m/g" \ -e "s/((1?[0-9][0-9]?|2[0-4][0-9]|25[0-5])\.){3}(1?[0-9][0-9]?|2[0-4][0-9]|25[0-5])((\/[0-9]{1,2})|(:[0-9]*)){,1}/${__esc}[1;92m&amp;${__esc}[0m/g" } ... sudo openvpn ... | filter 
What if instead of directly piping openvpn to sed, you pipe it to a `while` loop inside your bash script that reads and prints line by line. I mean like this: sudo openvpn ... | while read -r line; do sed ... &lt;&lt;&lt;"$line" done This way, there's the process of your bash script between openvpn and sed, so `trap` should work like you want it to. **EDIT:** I just noticed I had overlooked the post from Mr. KnowsBash. That's a similar idea, except his way feels neater. It's one single call to sed there, instead of a lot of calls in the `while read` thingy I suggested.
&gt; It means Bash does not perform normalization (like case conversion) on the argument, it passes the string directly to the operating system call as it is. Contrast this with an SQL interpreter. &gt; Yes, therefore, Bash inherently has no case sensitivity. It doesn't care about case at all. It just uses `stat` and `open` and so on and that's it. It is the OS and filesystem that care. &gt;`mkdir` failed to create a directory, but `cd` also failed, because a file with an uppercase version of the name I was trying to `mkdir` already existed. That is because you cannot `cd` or `mkdir` if a file exists by the same name. "Same name" is filesystem-defined; if you are using a case-insensitive filesystem then case can't differentiate between names. This would be true on a case-sensitive filesystem as well, except there, case CAN differentiate between names. &gt;Not that it matters to OP, but you should know that several of the Bash scripts I wrote for Linux have failed to work, or behave in very unexpected ways, when I try to run them on Mac OS X, partly because of HFS+ being braindead, and partly because Darwin uses older versions of the GNU userland utilities. So I have come to distrust and despise Bash on Mac OS X. I'm not trying to claim nobody has ever had problems that couldn't be traced to the case-insensitive default on Mac OS X. I'm saying that your criticism is unjustified and more importantly irrelevant in this situation.
It is mainly developers that only know how to maintain their application and don't know how or trust modifying a minimal centos install in any way for "stability" reasons. So I believe the reasoning is, "we" can write something better. I don't really try to argue. I have bigger fires to put out. 
Thanks, really appreciate the help. For the most part, it makes sense. So here is my current script, although I'm not convinced I'm doing this in the best way. It works, but do you have any suggestions? #!/bin/bash comstring=DevEng host=172.16.1.254 time=4 snmpstring="snmpget -v2c -c ${comstring} ${host}" while IFS=' ' read -r id _ _ status; do if [[ $status = "up(1)" ]]; then out=$(${snmpstring} ifOutOctets.${id##*.} | awk '{print $4}') in=$(${snmpstring} ifInOctets.${id##*.} | awk '{print $4}') ifdesc=$(${snmpstring} ifDescr.${id##*.} | awk '{print $4}') sleep $time out2=$(${snmpstring} ifOutOctets.${id##*.} | awk '{print $4}') in2=$(${snmpstring} ifInOctets.${id##*.} | awk '{print $4}') deltaout=$(( $out2 - $out)) deltain=$(( $in2 - $in)) inbw=$(((($deltain)/$time)*8)) outbw=$(((($deltaout)/$time)*8)) date=` date ` inbps=inbps="${inbw}" outbps=outbps="${outbw}" interface=interface="${ifdesc}" echo "${date}, ${interface}, ${inbps}, ${outbps}" fi done &lt; &lt;(snmpwalk -v2c -c $comstring $host ifAdminStatus) Again, it works and gives me the data in the way I want, but i don't like how it has to `sleep 4` before each interface, which is currently 3. So that's a total of 12 seconds just for `sleep`, and another 1-2 for running. Any suggestion on how I can make them happen concurrently, or more efficiently? EDIT: This is the output I get with the above script, which is exactly what I expected: [root@syslog ~]# ./test.sh Fri Jan 29 19:57:37 EST 2016, interface=lo, inbps=0, outbps=0 Fri Jan 29 19:57:42 EST 2016, interface=eth7, inbps=246776, outbps=7799584 Fri Jan 29 19:57:46 EST 2016, interface=eth0, inbps=4849896, outbps=137712 [root@syslog ~]#
no and no. sorry dude.
Ok, great thanks! Something to look into in the future
Honestly? Never crossed my mind. I took it as a learning experience to advance my knowledge of bash scripting. That is why I came here asking for critique on my script, to learn. I wrote it in a way that satisfied the requirements given to me. All of the results from your suggestion do not, but thanks anyway. I'll keep that in mind when I am forced to reinvent the wheel once again. 
I don't think of rsync as a coming of bash. You might have better luck in the SysAdmin or Linux subs.
`--fuzzy` may help with the second of your two options. I've never used it.
Try searching for 'signal trap bash', to learn how to catch signals. (CTRL-C ~~is a signal~~generates a keyboard interrupt (SIGINT) signal)
 #simple timer script #!/bin/bash PID=$! DATE=`date +%Y-%m-%d:%H:%M:%S` printf "$DATE \n\n" BEGIN=$(date +%s) echo Starting Stopwatch... while true; do NOW=$(date +%s) let DIFF=$(($NOW - $BEGIN)) let MINS=$(($DIFF / 60)) let SECS=$(($DIFF % 60)) let HOURS=$(($DIFF / 3600)) let DAYS=$(($DIFF / 86400)) # \r is a "carriage return" - returns cursor to start of line printf "\r%3d Days, %02d:%02d:%02d" $DAYS $HOURS $MINS $SECS sleep 0.25 # trap ctrl-c and call ctrl_c() trap ctrl_c INT function ctrl_c() { echo "** Trapped CTRL-C" kill $PID #also tried kill -s SIGINT, kill -s $PID etc. } done this currently. i have tried with it outside the loop and still the same
Ok thanks, I wasn't entirely sure but I found this sub and subscribed anyway :)
[removed]
You're doing things in weird ways here, but the main problem is this part: &gt; dir_in="$dir_in/*" for file in $dir_in; do This is where you fail to quote the input directory. The correct way to do that loop is: for file in "$dir_in"/*; do `$dir_in` inside quotes to avoid word-splitting and pathname expansion on the result of that expansion, and `/*` outside quotes, to allow pathname expansion to occur with that path component.
You can check the output of `ls` for mp3 or mkv extensions. The below script will move if there is even one file of that type. By the way, `mv` doesn't need -r. var=$(ls "$HOME/directory/folder") if [[ "$var" == *.mp3* ]]; then # it needs a * at the end as well, because the variable contains many lines mv -v "$HOME/directory/folder" "$HOME/music" elif echo "$var" | grep -qF .mkv; then # alternative way to check mv -v "$HOME/directory/folder" "$HOME/videos" fi
Use find! Example for MP3 `find $SourceDIR -type f -iname "*.mp3" -exec mv {} $TargetDir \;` 
Thanks! This is very helpful. I also wouldn't have thought that you would need a * after the .mp3, so I appreciate the comment. One thing I'm still struggling with though, is that I can't seem to work out how to get it to move the folder containing the files. For example, there could be a folder containing an album with mp3 files, there can also be a folder containing videos as well in mkv. So if I ls $HOME/directory/folder the output is Ghost in the shell # a movie aphex twin - druqs # an album These are the folders I would like to move, depending on the files inside them. So ideally, Ghost in the shell gets moved to Videos, and Aphex Twin - Druqs gets moved to Music. I have tried adding a "*" after folder, so it becomes $HOME/directory/folder/* However, it does not seem to work. I also tried making what comes after the $HOME/directory/folder a variable like this, var1=$HOME/directory/folder/* and then made changes to the move parts, although this didn't work either. I'm quite stumped at what i should do unfortunately. I have been using shellcheck too, just in case I made any minor mistakes when making changes, and it comes back okay. I'm really unsure as to how I can accomplish this. 
Oh sorry I included the type f which means file 
Do this then `find $SourceDir -type f -iname "*.mp3" | rev | cut -d '/' -f 2- | rev | sort | uniq | awk -v tardir=$targetDir -F'\n' '{print "sudo mv \""$1"\",tardir}' | bash` 
Don't parse ls output. See [FAQ 4](http://mywiki.wooledge.org/BashFAQ/004) on how to do it properly.
Ah, so the folder inside the directory isn't the one that contains mp3 files, but the subfolders inside that folder do. Then you can just loop over everything in that folder: # See my other reply for a non-ls solution. for subdir in "$HOME/directory/folder"/*; do if [[ -d "$subdir" ]]; then var=$(ls "$subdir") if [[ "$var" == *.mp3* ]]; then mv -v "$subdir" "$HOME/music" elif echo "$var" | grep -qF .mkv; then mv -v "$subdir" "$HOME/videos" fi fi done
A file named `.mp3-playlists` will cause your test to claim the directory contains mp3 files, so it's inaccurate and lazy. 
So using ls here can give bad behavior. This should be better: for subdir in "$HOME/directory/folder"/*; do if [[ -d "$subdir" ]]; then if [[ -n "$(find "$subdir" -iname '*.mp3' -print -quit)" ]]; then mv -v "$subdir" "$HOME/music" elif [[ -n "$(find "$subdir" -iname '*.mkv' -print -quit)" ]]; then mv -v "$subdir" "$HOME/videos" fi fi done
Heh, yes, an unfortunate name collision there. That's how I found this game, googling for the shellshock bash bug before it had lots of Google hits. According to the changelog, it was created in 2012, two years before the infamous bug.
Can I have your awesome config? 
&gt; awk 'NR==FNR{a[$2]=++i;next} { if ( $2 in a) {print $0}}' file1 file2 &gt; final Remove the `i` in there, it serves no purpose. Otherwise, to get the opposite test, do `!($2 in a)`. awk 'NR == FNR { a[$2]++; next } !($2 in a)' file1 file2
You can use `grep -Fx "$1" file1` in that case, but it's important to note that the files have millions of lines, so that solution will run grep and read the entire file (from cache probably, but still) millions of times.
Is this what you're suggesting? grep -f &lt;(comm &lt;(cut -d " " -f2 file1 | sort) &lt;(cut -d " " -f2 file2 | sort) -23) file1 &gt; Don't iterate command substitutions with `for` What do you mean by this? And why not?
With `for var in $(cmd)`, cmd's output will first be split into words based on the characters in the special `IFS` variable. After that, pathname expansion will go through each of the resulting words and look for, and replace with, matching files if they contain glob characters. Instead, use `read`, `mapfile` or `while`+`read` to parse command output. See [DontReadLinesWithFor](http://mywiki.wooledge.org/DontReadLinesWithFor) and [FAQ 1](http://mywiki.wooledge.org/BashFAQ/001) for more on this.
Ah, ok. So something like this then - while read; do grep $REPLY file1 done &lt; &lt;(comm &lt;(cut -d " " -f2 file1 | sort) &lt;(cut -d " " -f2 file2 | sort) -23) instead of the for?
Yes, but don't forget quotes, and you should always include `-r` on `read` unless you really want `read` to remove backslashes. while IFS= read -r line; do grep "$line" file1 done &lt; &lt;(comm...)
Hmm, then are you suggesting to run grep individually like suggested [here](https://www.reddit.com/r/bash/comments/43tjgn/delete_lines_in_file1_containing_pattern_from/czl1bfu)?
The `&lt;` operator does not take two files. `while read` will then only get input from the last command. Example: $ cat &lt; &lt;(echo foo) &lt;(echo bar) bar
It would need to look like this: sort -k2,2 file1 file2 | uniq -u -f1 -w5 The problem is is that you don't know that `-w5` is correct. At the start of the input the numbers are smaller than 5 characters (so uniq might report duplicate entries as unique) and at the end they might be longer than 5 characters. Another problem is that lines in file2 that are not in file1 should not get reported.
I'm on my mobile right now. It was just a wild guess. It's hard to give a proper solution without knowing how the real data looks like.
 grep -vxf file2 file1 Short and sweet. Use file2 as a pattern source, use -x to make those line regexps, and use -v to invert the matches. $ cat file1 sometext 11 sometext sometext 12 sometext sometext 13 sometext sometext 14 sometext sometext 15 sometext $ cat file2 sometext 11 sometext sometext 12 sometext sometext 14 sometext sometext 16 sometext $ grep -vxf file2 file1 sometext 13 sometext sometext 15 sometext However you want to grep on the second column and don't say if the first or third are also static. The 're' in 'grep' stands for 'regular expression' which is where it gets a bit more fun. Sadly it's 5am here and I'm on a call-out, so I can't commit too much brain time to it. Alternatively, a workaround would be to `cut` or `awk` the second column and use that as a filter, assuming the column data is unique enough to not match against the other columns. Similar to what kshenoy42 and knowsbash have discussed below $ grep -vf &lt;(cut -d' ' -f2 file2) file1 sometext 13 sometext sometext 15 sometext 
It's unfortunate `uniq` isn't as versatile as `sort` yet (`sort`'s ability to use e.g. -k2,2 is really useful). You can remedy the second problem by comparing it a second time with file1 it was said above.
If there are millions of lines to be checked, I don't recommend that.
I really can't give you real data, but the example is close enough. Solution in awk is up in the comments, and it really really fast, even without sorting the files. Thanks for comments anyway, I'm here to learn so everything helps.
logic behind that was i needed to be able to store the old IP between cycles for comparison. it's possible i was doing something wrong, but it appeared as though the variables were only stored while the script was running. after it completes or exits, those stored values are erased and not available anywhere else on the system or during the subsequent cycle. so the logic was to make sure that after the first cycle, there would always be an old value to compare to which needed to be stored between cycles. i'll probably change the new ip to how you have it in this example instead of first outputting it to file and then reading from file. i was just having it output to file for troubleshooting and making sure there was valid data being input into the new ip variable. i see what you're saying with the different formats. i think i was missing something during the development of the script and decided to try the source method instead of reading from the file as i initially thought the problem was with how i was pulling the old IP info. that was before i realized that unless i saved the old IP between cycles, there was nothing to read in the variable on the subsequent cycle.
is uppercase reserved...maybe reserved isn't the right word...for system variables?
Environment variables (EDITOR, HOME, PATH, ...) and special shell variables (RANDOM, SECONDS, IFS, ...) are all uppercase. By using uppercase variable names for internal purposes you risk overriding environment variables and special shell variables. But more importantly, it just looks ugly.
This is true, but to be honest I was pretty close to solution. And I am very very grateful for every comment here. I am still learning, but some day I will have a chance to help someone else in this sub. 
Dear god... How does one delete a "~" named folder, anyway?
`~` only expands to your homedir when unquoted, so any one of these will do: rmdir '~' rmdir "~" rmdir \~
And this, boys, shows you the true importance of constantly backing up. 
Thanks =). For files beginning with a dash you can also do: rm -- -r where "--" acts as a separator between the command options and the name of the file.
The second variation is preferable. Use read to split a file/string by a token (normally \n).
You're using an infinite loop as well. A tip, since `ping` has multiple lines of output, `column` would make the output look really good (date on the left, ping on the right). Try this (pings thrice): for ((i=1;i&lt;=3;i++)); do tput setaf $i; { date | tr -d '\n'; ping -c1 google.com | sed 's/^/@/'; } | column -ts@ ; done
Here's a script I wrote where I tried to solve a similar problem for me: http://pastebin.com/UgTFXD8P This script is just controlling fans. The interesting part for you could be how it deals with errors. The script traps signals and errors to reset the fan settings to the original values when the script is killed or when there's a problem with a command. It behaves well when the machine was put into standby and resumed. Coming out of standby resets the `pwm*_enable` settings on this motherboard here and that has to be detected and fixed by the script. It uses the arithmetic stuff that's in bash instead of calling bc. It works in thousands when doing calculations so that floating point and bc isn't needed. The script checks temperatures every three seconds. It records the readings into an array. It uses the average of the last two minutes to set the actual fan speed. Instead of moving everything in the array like you seem to do, I use an index to the spot with the oldest reading and overwrite it with the newest reading. About those crazy long two minutes, the idea was to try to be smart about avoiding noise and guess when a heat-sink is hot and fans most useful. I used this script here to track temperature changes while researching: http://pastebin.com/Ab0JJ16m I tried to find out how many minutes it takes for the CPU temperatures to top out for certain long-running jobs and how different fan speeds influence that. And I tried to see how many minutes it takes for the idle temperatures to settle at their lowest possible value after stress stops, and how keeping fans running for a while helps with that. This here is a version for tracking the temperature of an nvidia GPU: http://pastebin.com/2xBcc6Hb
 $ help exec exec: exec [-cl] [-a name] [command [arguments ...]] [redirection ...] Replace the shell with the given command. Execute COMMAND, replacing this shell with the specified program. ARGUMENTS become the arguments to COMMAND. If COMMAND is not specified, any redirections take effect in the current shell. Options: -a name pass NAME as the zeroth argument to COMMAND -c execute COMMAND with an empty environment -l place a dash in the zeroth argument to COMMAND If the command cannot be executed, a non-interactive shell exits, unless the shell option `execfail' is set. Exit Status: Returns success unless COMMAND is not found or a redirection error occurs.
Take a look at `help set`
I did start reading: https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html I'm not much wiser and a bit intimidated by: &gt; [The Set] builtin is so complicated that it deserves its own section.
In addition to what /u/KnowsBash and /u/galaktos said: If you want to be super careful, you can do it by [inode number](https://en.wikipedia.org/wiki/Inode). As an example: $ mkdir /tmp/~ $ ls -il /tmp | head -2 total 112 70319845 drwxr-x--- 2 whetu whetu 6 Feb 4 09:30 ~ $ find /tmp -inum 70319845 -exec rmdir "{}" \; This is ultimately an overglorified way of running `rmdir "/tmp/~"` (or `rm -rf` if it's non-empty)
When you do `help set` at the command line, you'll see a short description of each possible parameter for `set`. Here's the lines that describe what `set --` does: -- Assign any remaining arguments to the positional parameters. If there are no remaining arguments, the positional parameters are unset. What they want to say with that sentence is, `set -- $args` is overwriting `$1`, `$2`, `$3`, etc., and they are being set to the contents of `$args`. Now for the `for i; do` line, look at what help text you get when you do `help for`. There's one sentence that explains that if you do `for i`, then that's the same as doing `for i in $1 $2 $3 ...`. So this `set -- $args; for i` is basically a weird way of doing `for i in $args`, just like you suspected. I also don't understand why they do this. They might be wanting to rearrange `$@` into something processed and sorted, but then why first save it in `$args` and not just do `set -- $(getopt cdt $*)`?
The code is broken. It's garbage. The point of getopt is to "normalize" the arguments to your script, e.g. by changing `-cd` to `-c -d`, allowing you to parse options out of them. Except `getopt` is broken. For one, tt can't handle empty arguments. The code you found is worse, since it uses `$*` instead of `"$@"`, and `$args` instead of `"$args"`, breaking it even further by allowing word-splitting and pathname expansion to modify the data, both before passing it to getopt, and after reading it from getopt. If it was meant to be run with sudo by accounts that had otherwise no sudo access, it would be even worse, since it allows arbitrary code execution. The only safe way to use getopt is if you can be sure you have GNU getopt, and use it in a GNU specific way. If you cannot know for certain that all target systems have the GNU implementation of getopt, then don't use getopt. See [BashFAQ 35](http://mywiki.wooledge.org/BashFAQ/035)
If you prefer regex there is a cleaner way to do it: for file in *.{avi,mp4,mkv}; do [[ "$file" =~ ^(.*)\ \(([0-9]*)\)\.(...)$ ]] &amp;&amp; echo mv "$file" "${BASH_REMATCH[2]} ${BASH_REMATCH[1]}.${BASH_REMATCH[3]}" done
`-delete` is not always present, or as ubiquitous as `-exec` I didn't offer `-print0 | xargs -0 rm` for the same reason.
correct. thank you. so let me rephrase: when using emacs what should I do in order to switch between buffers when ctr -x -o does not work? 
Um...C-x C-c then open `vim` instead? I don't really know why it wouldn't work. Are you using it correctly? Emacs key combos are difficult to understand for people who come from a Windows world.
Noted. Scrap `getopt` for `getopts` or roll my own. ~~But I still have gotten _no_ signal of what `set` would be for in that supermegacrazy scenario where `getopt`could be considered usable.~~ ~~No one knows?~~ /u/ropid provided explanation [below](https://www.reddit.com/r/bash/comments/441ihi/please_explain_set_args/czmq1zr)
You are my hero! You actually answered my question! `getopt` mangles all the arguments to something that's more predictable, but to make the code behave in the same predictable way further down the `set`is used to push the re-arranging back to `$1 $2 $3 ...`. That actually makes sense to me. As to why you don't do `set -- $(getopt cdt $*)` I actually read the explanation yesterday. You wan't to do `getopt`separately so that you can have a look at `$?` before doing `set`.
It just means that you can wipe the positional parameters by doing `set --`. The sentence is there because `set -` is different. Check this out: $ echo $@ a b c d e $ set - f g h $ echo $@ f g h $ set - $ echo $@ f g h So using `set -`, you can set new parameters just like with `set --`. But when you do just `set -` without new parameters, then it won't delete the old parameters. About the context here, I also don't understand why they use `set` in their script.
`open` is not a standard command. This will likely only work on OSX.
You should use a browser (or browser extension) that lets you open a text field with your editor of choice, e.g. vim, that way you can write in markdown with syntax highlighting, paste arbitrary text (logs, etc.) into the current file (`:read` in vim), use your editor's commands to easily indent code (e.g. `&gt;ip` in vim), and so on. Also, why does your script open `$dest_file`? Why don't you insert the contents into the clipboard so you can paste them into the text field in your browser? Maybe I'm misunderstanding what you're trying to accomplish here. 
I'm still not quite sure what you're trying to do. Do you just want a way to add 4 spaces to the beginning of every line in a file? For example: 1. Save log to `foo.log`. 2. Run `add4spaces.sh foo.log` to add 4 spaces to the beginning of every line in `foo.log` and copy those contents into the clipboard. 3. Paste contents of clipboard into text fiield in browser. Something like that? If so, here are some suggestions/questions: - What is the point of `echo $1 | sed 's/\\ / /g'`? I doubt you have filenames that look like `foo\ bar`. Note that, for a file with a space like `foo bar`, even though in the shell you have to type `foo\ bar` to escape the space, the filename is still just `foo bar`. - What is the point of echoing "Markdown copied to clipboard"? An exit status of `0` (or the lack of any error from sed) should suffice to let you know everything worked. The message may look cool, but it's totally unnecessary imo. Here's what I'd do: #!/bin/bash sed -e 's/^/ /' "$1" | pbcopy You can put `$1` into a variable if you like, but imo it's unnecessary: this one-liner is short and sweet, easy to understand, no additional lines required. 
It's fun to play around with echoing info, etc., and it's an important step in learning how to efficiently code/script (sometimes it's even useful, e.g. when debugging a script), but usually you'll find that shorter &amp; simpler is better. Here's a (moderately complex) exercise for you: modify the script so that it can also add 4 spaces to whatever is in the current clipboard, so that, for example, you can "cut" code from a text field that you're writing in, run your script, then paste it back into the text field with the 4 spaces added, all without having to save text to a file. Hint: There are now two cases: 1. `add4space.sh` receives 1 argument (a file) and adds 4 spaces to each line in the file. 2. `add4spaces.sh` receives no argument and adds 4 spaces to each line of whatever is stored in the clipboard. (For completeness, you would also want to handle the case where the script receives &gt;1 argument, e.g. echo a warning and exit 1 without doing anything.)
Pretty close, but you should - use the `[[ ... ]]` construct for testing stuff, and - use `$#`, which is a variable that expands to the number of arguments given to the script. So for example: if [[ "$#" -gt 1 ]]; echo "Error: Too many arguments." exit 1 elif ... ... fi 
Very nice!
What about `git add -p`?
Thanks for the help However the issue im having now is that it doesnt seperate the information on each line into different columns. For instance "Application Name" and "Application status" are supposed to be in different columns "Application 1" and "Open" are supposed to be in dfferent columns "Application 2" and "Closed" are suppsed to be different columns This has just but any information on a given line into the same column.
Thanks, these will be useful
Most of the time I want to just add all changes, but I want to look at them first. I don't want to have to go about confirming each chunk. And also, I wanted to learn how to programmatically edit bash command lines in-line.
Yes, you're right, I'm not checking for `flock`'s exit status. I'm reading up on what I can find about this, hopefully I can fix it. (you're welcome to offer alternatives)
&gt; But I’m not sure why you need locks at all. Can’t you just write jobs to the fifo, and each worker reads one line at a time and processes it? ^^ That's what I'd like to do as well, it would be much easier and simpler. However, it seems that the 4 consumers are causing the producer to generate errors like these(not always, just for some of the writes): &gt; ./producer.sh: line 10: echo: write error: Broken pipe 
Wait isn't Vagrant a VM manager? Are you managing a manager?
With the --joblog and --resume flags you can also get check pointing, there's even --resume-failures which will just rerun failed tasks. 
I'm not sure what that means unfortunately, but maybe a clearer example will explain better. Given this input: keyword foo keyword bar keyword baz otherkeyword_bar otherkeyword_something otherkeyword_foo otherkeyword_baz otherkeyword_bop I want this output: keyword foo keyword bar keyword baz otherkeyword_something otherkeyword_bop But I'm getting this output: keyword foo keyword bar keyword baz otherkeyword_bar otherkeyword_something otherkeyword_baz otherkeyword_bop Basically, if the word followed by the `keyword` phrase is found following the `otherkeyword_` phrase I want to remove that line. I'm doing that by putting them in separate groups and removing the last group. I'm starting to think this just has something to do with recursive regular expressions and I might be out of luck.
How about perl -ne 'print if ! /^otherkeyword.*(foo|bar|baz)$/' input If the underscore is always there replace the .* with it for better performance and if there's more text after remove the $ anchor.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux] [Simple lockless job queue in Bash using a FIFO (x-post \/r\/bash)](https://np.reddit.com/r/linux/comments/44g1if/simple_lockless_job_queue_in_bash_using_a_fifo/) - [/r/programming] [Simple lockless job queue in Bash using a FIFO (x-post \/r\/bash)](https://np.reddit.com/r/programming/comments/44g1k3/simple_lockless_job_queue_in_bash_using_a_fifo/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
This is cool that it was done in bash but it's less than practical for many types of jobs. There's much better tools (GNU parallel and xargs) that can do this without the caveats. They can also trivially run one job per line. GNU parallel also has the --joblog flag which will keep track of progress of long running jobs. If you need to return to a job list it can resume and/or resume only failed jobs. Having either tool read from a FIFO is easy and let's you build an appendable job queue.
I recognize the effort you've put in here; isn't this built into screen and tmux?
As said, I don't want to use X or screen. Just bash ;)
If youre willing to use tmux (which is amaizing and cmd line) check out [this video](https://www.youtube.com/watch?v=s0T82aJn1DQ) which does exactly what you want
I told you it's stupid ;) It contains scripts that will provision the VM and it contains an interface to invoke commands in the VM from outside. I wanted to be able to have a common interface in the guest and the host. Now I can create plugins that performs a task locally (if guest) or through SSH (if called from the guest)
Thought to post it here first, then remembered /r/perl and posted it there too. My bad
$USER gets set to root, so you want $SUDO_USER: $ man sudo SUDO_USER Set to the login name of the user who invoked sudo.
I think `$SUDO_UID` would be even better – otherwise you probably get a problem if the user name happens to be numeric (which, as far as I can tell, is strongly discouraged, but not outright impossible). Also, there’s probably no guarantee that the group `$USER` will exist, so better use `$SUDO_UID:$SUDO_GID` or simply `$SUDO_UID:`: &gt; Group is […] changed to login group if implied by a ':' following a symbolic OWNER.
I'd add using install, in place of calling two external binaries, mkdir and chmod
Good point, I hadn’t looked at the rest of the script.
thanks for advice! i got the PIDds to background and reduced bc use. i think the error handling is up next.
This is shameless self-promotion, but you might for example include http://2048.fi/shellalt.txt under Bash or https://www.reddit.com/r/commandline/comments/42bfbp/single_line_awk_commands_that_are_not_in_awk1line/ under Awk. 
I'm getting an error with this test though... #!/bin/bash # Only run if sudo or root if [ "$(whoami)" != 'root' ]; then echo -e $"You don't have permission to run $0 as non-root user. \nUse sudo" exit 1; fi domain="Test" cat &lt;&lt;EOF &lt;VirtualHost *:80&gt; # Some sample text ServerName $domain ServerAlias www.$domain #LogLevel info ssl:warn ErrorLog \${APACHE_LOG_DIR}/error.log &lt;/VirtualHost&gt; EOF &gt; $domain'.txt' Error: test.sh: line 19: warning: here-document at line 11 delimited by end-of-file (wanted `EOF') Which I understand that EOF needs to terminate the line, but how do I send the output to a file? **Edit:** Oh sorry, just realised I would need to do this then... &gt; $domain'.txt' cat &lt;&lt;EOF &lt;VirtualHost *:80&gt; # Some sample text ServerName $domain ServerAlias www.$domain #LogLevel info ssl:warn ErrorLog \${APACHE_LOG_DIR}/error.log &lt;/VirtualHost&gt; EOF Noob mistake :P
The closing `EOF` must be on a line by itself. Not even whitespace before or after. The redirection of fd 1 must be on the cat line. cat &lt;&lt; EOF &gt; "$domain.txt" &lt;VirtualHost *:80&gt; ... EOF Also, prefer printf over echo, test `USER` or `UID` rather than running `whoami`, and don't put .sh extension on a bash script. if (( UID != 0 )); then printf &gt;&amp;2 'You should run this as root\n' exit 1 fi
Does it make a differrence if I write it like...? cat &lt;&lt; EOF &gt; "$domain.txt" or &gt; "$domain.txt" cat &lt;&lt;EOF 
&gt; cat &lt;&lt; EOF &gt; "$domain.txt" This redirects fd 0 first, then fd 1 &gt; &gt; "$domain.txt" cat &lt;&lt;EOF This redirects fd 1 first, then fd 0 Since it doesn't matter which you redirect first in this case, both will achieve the same.
Ok thank you.
Ok. thanks bud. 
The history-search commands take what is on the commandline up to the cursor and use that to search for entries that start with that text. So moving the cursor to the end of the line makes you unable to search back further than 1 entry (usually). You could try choosing the history entry you want using dmenu and then moving the cursor to the end. `cut -d\ -f3-` might need adjusting, as well as what dmenu you use. bind -x '"\C-r": READLINE_LINE=$(history | cut -d\ -f3- | tac | dmenu); READLINE_POINT=10000'
Do your script does something else in that file? Do you really need the global modifier? Another option: sed -ri 's/^#(interface=)$/\1wlan0/'
It gets input and then lower-cases it. 
Bash supports changing case of ASCII strings natively, and that could be written as `read i; i=${i,,}`.