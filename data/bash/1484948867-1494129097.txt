Are you talking about your code syntax when coding? Or the output when you run your script to have color for your output that's in quotes.
wouldn't that mean 111112 and 211111 are the same? and that from 111111 to 666666 there should then be a lot of holes?
 c=1 b=6 f=111111 r="" while read -r line; do printf '%s %s\n' \ "$(while ((c != 0)); do r="$((c % b))$r" ((c /= b)) done printf '%06d' "$(( "${r:-0}" + f ))")" \ "$line" ((c++)) done &lt; "$file" Probably something wrong with this.
Nope! There are 6^6 combinations so no holes and order matters.
`printf "%06d\n" $(($(bc &lt;&lt;&lt; "obase=6;ibase=10;$linenum")+111111))` #%06 spacing not really needed linenum=0 111111 linenum=46655 (6^6)-1 666666 
If I understand you correctly it should be fairly trivial with sed: $ sed '/^[@+]/s/\.1 / /' input @SRR867742.1 HWUSI-EAS1696_100401:3:1:7:1601 length=120 TTTNTGCATAAGTTTTAAATTAAGAAAAGAGACTCCAAGAGACTACACCATTCTCAAAGAGAAGTCCAATAGTAAATAAATAAGCATTTGTGGTACATGCAGAG +SRR867742.1 HWUSI-EAS1696_100401:3:1:7:1601 length=120 553&amp;445554:DCG;@A=5?;DD=&lt;GEBBGGE=GFCF9C&gt;&lt;8/=&lt;&gt;FA&gt;F4;;56@D&gt;FFGFGCC4,A-+66443GCC594CCFCGGCF&lt;580/95&lt;957??:@ @SRR867742.2 HWUSI-EAS1696_100401:3:1:7:1582 length=120 GAGNCAGTGCTCCTCCTGTTGCTATTGTCCTATTGGCTTTGTCCTGCTGCCACTGCATAAGCAAA +SRR867742.2 HWUSI-EAS1696_100401:3:1:7:1582 length=120 554&amp;5==:&lt;8&gt;&gt;??ADDD9ADDA:&lt;-444258834DF&gt;9FF&gt;F:F51@A&gt;@96:9&lt;1-54?)&gt;:A Breakdown: /^[@+]/ # Address, match lines stating with @ or + s/\.1 / / # Replace literal .1&lt;space&gt; with a &lt;space&gt; One might be able to use `s/\.1//2` instead which will remove the second occurrence of `.1` 
If that means the file has exactly 6^6 lines, you could "cheat" and do printf '%s\n' {1..6}{1..6}{1..6}{1..6}{1..6}{1..6} | paste - file &gt; file.numbered
Very curious what this data represents or is used for! 
My guess would be DNA analysis or something similar. From wiki page on DNA: &gt; Each nucleotide is composed of one of four nitrogen-containing nucleobases—either cytosine (C), guanine (G), adenine (A), or thymine (T)
Yup! Chicken genome downloaded from sra 
Thank you! 
Bash can handle integers just fine, floating point it can't do without calling some other command like bc. This is doable, I'm thinking he would have to use sed however and send the filters output to another text file.
[removed]
[removed]
man basename #!/bin/bash for word in /bin/*; do echo $(basename "${word}") done Not sure if speed or performance is a big consideration in your use case but this is much faster by not invoking another command: #!/bin/bash cd /bin for word in *; do echo "${word}" done 
You _might_ be able to do this in bash but that doesn't make it a good idea. The code will be much more readable and maintainable in a dedicated scripting language. 
The first echo is not needed. cd /bin for word in *; do echo $(basename "$word") done 
getopt is the work of the devil. repent and accept getopts as your one true god. #!/bin/bash usage () { echo "--help: Display this message --profile: AWS Profile --region: AWS Region --file: JSON file for configuration" &gt;&amp;2 } while getopts hp:r:-: OPT do if [[ $OPT == - ]] then OPT=${OPTARG%%=*} if [[ "$OPT" == "$OPTARG" ]] then OPTARG=${*:OPTIND:1} let OPTIND=$OPTIND+1 else OPTARG=${OPTARG#*=} fi fi case $OPT in h|help) usage; exit;; p|profile) AWS_PROFILE=$OPTARG;; r|region) AWS_REGION=$OPTARG;; ?) exit 1;; *) echo "$0: illegal option -- $OPT"; exit 1;; esac done shift $((OPTIND-1)) echo "$AWS_PROFILE" 
I can agree that there are other scripting languages out there that would make this task more portable. In bash really all you're doing is stringing commands together from either builtins or other outside utilities. For what qwer777 is using this for ( simple text file manipulation ), this is possible. If the op really wants to use bash script there are utilities - sed ( stream editor ) and awk - that can do this. I'm not trying to shoot down your argument, I'm saying the op asked how they could do this in *bash* . You're not helping them by just giving the op a 2 sentence answer based off of your opinion.
Maybe try `expect`?
Okay, fine, use `printf` instead: printf '%s\n' "${word##*/}" Your version breaks on strange filenames, by the way – it would be better to use `find -exec`: find /bin -type f -exec basename -- {} +
TIL what an accession code is. Cool.
These are good. I needed to conduct a nested loop. I'm not sure where I'm going wrong. #!/bin/bash space=" " for letter in {a..z} do declare temp=0 for word in $(/bin) do if [ "$word{:0:1}" == $letter ] temp+=1 done echo "$letter$space$temp" done
That's hardly a real problem, though.
`yes command | script.sh`
Well it has nothing to do with `echo` - the `"${word##*/}"` operation is bash. (Parameter Expansion) `/bin/*` would report `/bin/*` if `/bin/` was "empty" i.e. the glob did not match. There is `shopt -s nullglob` to change that behaviour if desired.
echo is a builtin. it works the same on every copy of bash, no matter the operating system. It works differently on other shells such as dash, ksh, zsh, etc. but bash is bash.
I know this is /r/bash but zparseopts for zsh really made this easy for me. https://pinboard.in/u:dza/t:zparseopts
use this article from wooledge wiki [mywiki.wooledge.com!](http://mywiki.wooledge.org/ComplexOptionParsing?highlight=%28getopts%29)
use this article from wooledge wiki [mywiki.wooledge.com!](http://mywiki.wooledge.org/ComplexOptionParsing?highlight=%28getopts%29)
&gt; mywiki.wooledge.com! That looks complicated. And it requires bash 4 (won't work on most mac computers).
Hey! Sorry about the delay in replying. Great idea. Checking out some videos on setting up an apache web-server now. Did you have much IT related experience before doing your RHCSA and dabbling in Apache? Is securing an apache server much harder than locking down SSH access (no root access, etc.)? I'm fairly security conscious. 
The DigitalOcean tutorials have been my goto recently. They probably have one exactly for Apache and CentOS 7. I'd spent a couple of years with Linux, setting up mini-networks and web servers, especially a few on AWS, before taking the test. It's a good goal because it requires a basic grasp of the main parts of the OS (setting up users, mounting drives, etc). It's not too difficult to password protect a website with apache. You could also look into only serving it on your loopback ip (127.0.0.1) and using ssh to gain access (ssh -L 8080:localhost:80 you@centos). Another very useful thing to know is setting up the firewall. Firewalld is the name of the main one used with CentOS/RedHat. Overall I just tried to do something like setup a websever and then be willing to go down all of the side avenues along the way like setting up a firewall, virtual hosts, maybe change to nginx, try out mysql, setup a wordpress site, switch back to apache, look into selinux, add letsencrypt and an ssl cert, realize it's been a year and your site isn't even up yet but look at how many new things you know!? 
 $ echo -e '25,B,27,D,8\n23,A,49,B,2,C' | sed -E 's/([^,]+)(,([^,]+))?/\3,\1/g' B,25,D,27,,8 A,23,B,49,C,2
This is what I went with. Thanks!
That seems a poor way. It's fixing up the output rather than making the input be a standard format. I'd have thought if there's an odd number of fields then append a comma to the line.
I agree, it's not elegant. But it's pragmatic and that keeps me from lying awake at night.
 if [[ -f $cert ]] &amp;&amp; openssl x509 -checkend 604800 -noout -in "$cert" http://mywiki.wooledge.org/BashGuide/TestsAndConditionals
 sed -r 's/([^,]*),?([^,]*)(,?)/\2,\1\3/g' Works fine with all kinds of inputs, including empty data cells. The only minor kinda like a problem is with an input like this 1,2,3,4,,5 you get 2,1,4,3,5, but it seems a logical output to me, meaning that there are even number of items and the last one has null value.
[removed]
Really appreciate your insight. So, here's the modified version of the script: #!/bin/bash -x # Script to check running processes for current user # across all login nodes # Ensure login_nodes.txt contains the list of login nodes, # one per line, and the file exists within your home/bin before # running this script. # Assuming you named this script check_self.sh and you # had login_nodes.txt in your home directory &amp; chmod +x check_self.sh, here's how you'd run # this: ~./check_self.sh login_nodes.txt while read line do ssh -n $USER@"$line" "hostname; ps -ef | grep $USER" done &lt; "$1" This works, but the output is pretty ugly looking: http://pastebin.com/m9yg3t5W While I don't think it's possible, perhaps someone can offer insight on somehow cutting the warning banner out of the output? Also, the typical "no running processes before this script was ran" output from all nodes except 13 might be misinterpreted from the user community. I'm guessing there's no easy way to parse that content out or perhaps replace it with "Nothing to see here / No running processes to worry about on this host" sort of message? 
One possibility for getting rid of the warning is to write your stuff to `stderr` and discard `stdout` (assuming that banner is on `stdout` so: ssh -n "$line" "hostname &gt;&amp;2; ps -ef | grep $USER &gt;&amp;2" &gt;/dev/null note that `$USER` should be the default user you try to connect with to the remote system, so you probably don't need to specify it in the `ssh` command
Generally no. CLI tools should generally minimize output or display none at all without any special flags. It is only noise after the second run anyway after everything is working as expected. Only exception is for error messages which should be redirected to stderr 
Protips: - screenshots of code *suck*. - nobody knows what your problem is, apart from that it's "wrong".
Interesting... I hadn't seen that. Although, I get (arg: 200) even when pasting a single character. The escape code for bracketed paste is: ^[[200~pasted text^[[201~ I think the 200 is coming from the escape code. It seems like bracketed-paste may be accidentally triggering the normal mode argument count. I think this is more of a readline issue than a bash issue. Or perhaps it's my terminal... It's good to know what the (arg: n) is about anyway :-) Thanks for your help! 
I get “page not found” – is your gist public or private?
It is public. I recopied the link into the description and opened it in an incognito window and seems to be working.
[removed]
Only do progress bars with useful information. A spinner in a terminal does not tell me more than just seeing that the execution did not return the prompt. In the end if you only display information like "running step x" or "done" is like debugging noise. I only care if the return value is not 0 abd then I would only care about error messages. If you do interactive scripts then you obviously want to guide every step and display relevant info. In the end it depends more who else is going to use your script and how often you will want to reuse it for different situations(ie shell or as a service). So it is more your decision as to what you think is more important for your use case.
&gt; Say my script starts off by making an object: &gt; &gt; l = ls /bin `l = ls /bin` calls a function / command called `l` with the arguments: `=`, `ls` and `/bin`. If you want to assign the value, from the command `ls /bin`, to a variable - which you rarely would, more possible would be to assign the output of another command. The syntax is: my_var=$(my_command arg1 arg2 "arg3 with spaces") The syntax is as above and is called a command substitution. Assigning a variable requires the name and the equal sign as well as the value to be all tight together: my_var=a # Assign the value `a' to `my_var' my_var= a # Call the command `a' making the variable `my_var' accasible from # only within `a'. my_var =a # Call the command `my_var' with one argument: `=a' my_var = a # Call the command `my_var' with two arguments `=' and `a' &gt; Does the for loop just assume I am referring to the only array object that has been made? A for loop will iterate over all the provided arguments, in your case those are `{a..z}` which will expand to: `a` `b` `c` `d` `e` `f` `g` `h` `i` `j` `k` `l` `m` `n` `o` `p` `q` `r` `s` `t` `u` `v` `w` `x` `y` `z`. Try for your self: $ echo {a..z} a b c d e f g h i j k l m n o p q r s t u v w x y z The only catch with the for loop is that if no arguments is provided, as in `for x;` then each provided arguments - to the script - will be iterated, i.e: $ cat ./my_script.bash for x; do echo "$x" done $ bash ./my_script.bash a b 'hello world' c a b hello world c It is exactly the same as writing: `for x in "$@"`. &gt; Is there a better way to only sort through the alphabetized listings only in the /bin directory? The most commen approch to iterate files in a directory is to use pathname expansion (globs): for x in /bin/[a-z]* do echo "$x" done The pattern `/bin/[a-z]*` expands to all files / directories which starts with `a..z` and are within the `/bin` directory. If you want to match files that only contains `a..z`, i.e. to filter out `python2`, then you need to enable extglob, with: `shopt -s extglob`: shopt -s extglob for x in /bin/+([a-z]) do echo "$x" done Both will expand directories as well, take a look at `man 1p test` / `man 1 test` to see how to filter these out. See Greg's Wiki for an overall good reference on bash: &lt;http://mywiki.wooledge.org/glob#extglob&gt; &lt;http://mywiki.wooledge.org/BashGuide/TestsAndConditionals#Conditional_Blocks_.28if.2C_test_and_.5B.5B.29&gt; &gt; Noob question about arrays There is usually no need for arrays in shell scripts, actually a POSIX shell doesn't support arrays, bash does, but is rarely ever needed. I havn't mentioned anything about arrays[0] in the write up above as their isn't any need for arrays. [0] One could argue that `$@` is an array.
Thank you for the detailed reply!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You need to quote the `*` when running the script: script.sh test/ '*' Otherwise, the `*` will be expanded to the names of all entries in the current directory before your script is called. You should also quote `$path` in the `find` invocation: find "$path" -name "$name" -printf "%f\n" (You don’t actually need to quote `$1` and `$2` in the `path` and `name` assignments – variable assignment doesn’t do word splitting. But it doesn’t hurt, either.)
See this for more info: https://stackoverflow.com/questions/11456403/stop-shell-wildcard-character-expansion
already been their, tried all the things nothing, i think i really did a big search for this
Other posts have already told you why it isn't working. I would suggest, as a solution, making `*` a default behaviour. In other words: if `$2` is unset or blank, assume and use `*` Basic demonstration, I've saved this code to `/tmp/findscript`: #!/bin/bash path="$1" name="${2:-*}" find "$path" -name "$name" -printf "%f\n" Right, let's create a temporary directory and some test files, then demonstrate the script: # mkdir /tmp/testdir # touch /tmp/testdir/test{1..10} # cd /tmp # ./findscript testdir/ test4 test4 # ./findscript testdir/ testdir/ test1 test2 test3 test4 test5 test6 test7 test8 test9 test10 Obviously you'll probably want to tweak and expand on that. 
not the think i really want but is close to wt i want, the thing i really want doesnt exist so i think i will stick with this thx
Assuming you are allowed to build or install packages, you could also use [macchanger](http://www.pkill.info/linux/man/1-macchanger/).
I have `macchanger ` but i was wondering how does it work then i wrote this script. i know that there is many like `macchanger` but, i want to improve my bash skill this is why i wrote this script!
&gt; Variable [Capitalized] assignment doesn’t do word splitting Nor filename generation (globbing) which is in question here.
&gt; Also, `-iname` is case insensitive and is usually what you'll want to use. It's not POSIX though, so should be used with caution. See `man 1p find` for POSIX options.
Which is why you probably shouldn't do it, and should just make sure you quote stuff. Of course, those pretty shell scripts will be running in their own processes, and shouldn't inherit the noglob setting from your interactive session.
[removed]
I would check the status of the commands that actually change the Mac address, and also when you confirm the new Mac address I would parse the IP a output so you confirm it's actually changed. Also running as root means you don't require sudo
 Assuming you want just numbers in your file names; this bash script will do what you want for the entire directory. #!/bin/bash for file in * do extension=.${file##*.} newfile=${file//[!0-9]/}$extension mv $file $newfile done Or a oneliner to use in the terminal: for file in * ; do extension=.${file##*.}; newfile=${file//[!0-9]/}$extension; mv $file $newfile; done
Does that get rid of the underscores? ls * | while read x; do echo mv -v "$x" "$(echo $x | sed 's/_[^.]*//g')"; done 
If your objective is to learn Regex, you can post on [r/regex](https://www.reddit.com/r/regex). If however you're like me and try to avoid Regex when possible, you can do this easily with bash and awk... for i in * do newname=$(printf "$i" | awk -F'[__.]' '{print $1 "." $(NF)}') mv $i $newname done
Mine gets rid of everything except for the extension and the numerals.
Did you copy my script exactly? Because this is pretty weird, and it doesn't happen to me. Made a bunch of different test files.
This being taken from command line history. for file in * ; do extension=.${file##*.}; newfile=${file//[!0-9]/}; mv $file $newfile; done Putting a new copy of your command into a text file produces the same hash as if i take the one from history and do the same thing.
Thanks for the detailed answer, Will go over it in full when i get home later as swapping between a terminal and reddit is fun and a web browser is not that fun compared to a triple monitor setup :) One thing though, Is when i try and run your example command for file in *; do mv "${file}" "${file_*\./.}"; done It gives the following error ${file_*\./.}: bad substitution However the previous parts appear to work as expected. Once again thanks for giving more detail on how it all works! Will help with learning how this all goes together.
Ah I see, it should be: for file in * ; do extension=.${file##*.}; newfile=${file//[!0-9]/}$extension; mv $file $newfile; done Which is the same as the shell script, but compressed with ; such that it can be done on one line in the shell.
Ah shit. Sorry... it should be: for file in *; do mv "${file}" "${file/_*\./.}"; done
`prename` is also a handy tool for this type of thing. $ ls 382047__various_unimportant_text_here.png foo__bar_omg.jpg $ prename -n 's/_.*\././' ./* ./382047__various_unimportant_text_here.png renamed as ./382047.png ./foo__bar_omg.jpg renamed as ./foo.jpg `-n` is a "dry-run" - you can remove it to have it actually perform the renaming.
Yep. The colour of your prompt is controlled by your shell, which is most likely `bash`. There are many articles [like this one](https://www.linux.com/learn/how-make-fancy-and-useful-bash-prompt-linux) showing different prompt possibilities.
Try using the program Variety to cycle your wallpapers instead, it lets you select multiple sources for images (both local filesystems and online). If you must do it this way, you'll probably end up creating symlinks to all your wallpapers; I can write a command to do it automatically in a bit.
[removed]
Something like: for i in $(cat test.txt); do [[ -z "$min" ]] || (( i &lt; min)) &amp;&amp; min=$i done
 awk 'min == "" || $1 &lt; min { min = $1 } END { print min }' test.txt
Oh missed the "no" in the title. So homework then. See [BashFAQ 001](http://mywiki.wooledge.org/BashFAQ/001) on how to read a file line by line.
 $ echo '70 5 69 44 36 21' | tr ' ' '\n' | sort -nr | head -1 70 Should be pretty self explanatory. If the input is multiline one might to omit the `tr` part.
Why not use 'while read line' instead? 
&gt; you should be using a `while read` loop instead It's not gonna help much if the numbers are all on one line though.
A couple of golden rules: 1. [don't parse ls](http://mywiki.wooledge.org/ParsingLs) 2. You should try to use `printf` instead of `echo` &gt;`items+=($item)` Stylistically this is dangerous as the two are confusingly close. I like to put the word "array" into my array names and either "Fn" or "Func" into my functions, this way they're visually separable from variables. See one of my previous posts where I used `nameArray` for example. Now, to put all of the files in a directory into an array, you can just use globbing e.g. fileArray=( /path/to/files/* ) In this example I have intentionally put spaces into the test file names: $ mkdir /tmp/testdir1 $ cd /tmp/testdir1 $ touch "test file{01..10}" $ ls test file01 test file02 test file03 test file04 test file05 test file06 test file07 test file08 test file09 test file10 $ fileArray=( /tmp/testdir1/* ) $ printf "%s\n" "${fileArray[@]}" /tmp/testdir1/test file01 /tmp/testdir1/test file02 /tmp/testdir1/test file03 /tmp/testdir1/test file04 /tmp/testdir1/test file05 /tmp/testdir1/test file06 /tmp/testdir1/test file07 /tmp/testdir1/test file08 /tmp/testdir1/test file09 /tmp/testdir1/test file10 Or if you want to partially match the filenames, you can do that too. Say you want an array of files with the .txt extension: fileArray=( /path/to/files/*.txt ) Example: $ touch test{1..4}.txt $ fileArray=( /tmp/testdir1/*.txt ) $ printf "%s\n" "${fileArray[@]}" /tmp/testdir1/test1.txt /tmp/testdir1/test2.txt /tmp/testdir1/test3.txt /tmp/testdir1/test4.txt Obviously if you don't want the paths, you could do something like: cd /path/to/files || exit 1 fileArray=( * ) Or something more elegant. Example: $ fileArray=( *.txt ) $ printf "%s\n" "${fileArray[@]}" test1.txt test2.txt test3.txt test4.txt You may also like to look into [nullglob](http://mywiki.wooledge.org/glob#nullglob) and why you might want to set it. Finally, be aware of `array[*]` vs `array[@]` - [Further discussion on that.](http://stackoverflow.com/questions/3348443/a-confusion-about-array-versus-array-in-the-context-of-a-bash-comple)
Because `n` isn’t defined anymore. Add this: n=${#arr[@]} (or, I suppose: `n=$#`)
/u/whetu I sent you a private message. But thanks a lot for this explanation. I'm not working with sub directories. 
You are the man/woman! Such a stupid mistake I couldn't figure out!
no problem, you’re welcome :)
This feels less clunky than specifying the file to read from outside the loop, as I usually do. while read line; do stuff; done &lt; filename.txt
[removed]
You think typing upp&lt;Enter&gt; is going to be faster than typing the up arrow twice? Anyway, if you want to generate keyboard events, look at `xdotool`. Personally, when I go into an edit-compile-test loop, I usually go single command line like so: $ vi blabla.c ; cc blabla.c -o blabla -lm &amp;&amp; ./blabla A repeat is just &lt;UpArrow&gt;&lt;Enter&gt; away.
That's also a nice way to do it. Thanks for the link. 
So they have the exact same filename apart from the extension? If so you could do something like user@host $ ls a.mp3 b.m4a c.m4a c.mp3 d.m4a d.mp3 user@host $ for f in ./*.{mp3,m4a} &gt; do &gt; base=${f%.*} ext=${f##*.} &gt; [[ $ext = mp3 ]] &amp;&amp; dup=m4a || dup=mp3 &gt; [[ -e $base.$dup ]] &amp;&amp; echo mv "$base.$ext" "$base.$dup" /path/to/dupes/ &gt; done mv ./c.mp3 ./c.m4a /path/to/dupes/ mv ./d.mp3 ./d.m4a /path/to/dupes/ mv ./c.m4a ./c.mp3 /path/to/dupes/ mv ./d.m4a ./d.mp3 /path/to/dupes/ The output is duplicated because we've not actually moved the files here so the exists test succeeds for the duplicated file too. If you remove the `echo` to actually execute the `mv` commands the file will not exist anymore so the test will not be true. It would make sense to have a backup of your music before testing out any scripts.
Thank you! Are the letter names file names that I need to supply, or just functional in the script?
Well i'm not 100% sure I follow you fully - but it sounds like you want something like this... **urls.txt** user@host $ cat urls.txt https://i.imgur.com/episode001.jpg https://i.imgur.com/episode002.jpg **build-automod-config script** user@host $ cat build-automod-config urls=urls.txt while read -r url do printf -v ep %03d $((++i)) cat &lt;&lt;. --- type: comment body: "#E$ep" comment: You mentioned [E$ep]($url), which is not welcomed on this sub. Please consider this in when you are commenting the next time. is_moderator: true . done &lt; "$urls" **sample run** user@host $ bash build-automod-config --- type: comment body: "#E001" comment: You mentioned [E001](https://i.imgur.com/episode001.jpg), which is not welcomed on this sub. Please consider this in when you are commenting the next time. is_moderator: true --- type: comment body: "#E002" comment: You mentioned [E002](https://i.imgur.com/episode002.jpg), which is not welcomed on this sub. Please consider this in when you are commenting the next time. is_moderator: true I've just used 2 urls to keep the output brief. Not sure why you get a syntax error with your example code - it should work fine. Also you probably want to remove the `in` from Please consider this in when you are commenting the next time.
For your first problem, you can use either `find` or `locate`. `locate` is faster, but it depends on a database that is updated once a day, so it may be out of accuracy by several hours. If that's acceptable, it's the way to go. Otherwise, you can use `find`. locate "(1)" find ~/ -name "*(1)*" For your second problem, string manipulation to the rescue: $ testString="~/Downloads/583152.D.0(1).pdf" $ echo "${testString/(1)}" ~/Downloads/583152.D.0.pdf You can work with that to test for the existence of the file without the "(1)" Note: You may need to also cater for unwanted spaces. You've got a series of `if`'s that can be put into a structure: read list of files with (1) in the name, use a for or while loop for each file, remove the "(1)" ... I've shown you this bit already # Test that the filename without (1) exists if file exists; then # If so, compare the modification times # On a Linux system, you can get modification times with 'stat' # In this example, we get that time expressed as seconds since Epoch: # First, the file with "(1)" in it modFile1=$(stat -c %Y ...) # Now the file without "(1)" in it modFile=$(stat -c %Y ...) # Now see if they're the same if (( modFile1 == modFile )); then rm modFile1 else #more tests to figure out which is older/newer and action as required fi # Otherwise, if the file without "(1)" doesn't exist, we rename else mv "${File1}" "${File}" fi done I'm obviously not going to do everything for you, but you've got more than enough to go on.
Are you sure you don't just want to run [fdupes](https://linux.die.net/man/1/fdupes)?
Note that the `stat` command is completely different on Mac and Linux, so any script using `stat` won't work on both.
I've been playing around with this and...so far so good! I also created a modified version to handle directories that have the same issue. Do you see anything in this code that might come back to haunt me? Thanks again. #! /bin/bash # start from current working directory (.) find . -type d -name '* (1)*' -print | while IFS='' read -r file; do dir=$(dirname "$file") fnam=$(basename "$file") onam=${fnam// (1)/} # original name ( # cd within subshell so it doesn't stick cd "$dir" if [[ $fnam -nt $onam ]] # yields true even if $onam doesn't exist then rm -rf "$onam" mv "$fnam" "$onam" else rm -rf "$fnam" fi ) done 
[removed]
Hey now...ip forwarding is pretty common.
+1 to /u/Vitrivius's comment However, if you wanted to pipe them all to one another, you would have to rewrite the perl scripts to write to standard out instead of a file and read from standard in. The point of a lot of Unix programs is that their output by default is standard out and the resulting stream can be redirected and used with other programs (compositional programming). You would have to assess whether or not you gain any utility in refactoring the scripts to do that.
OP here: Sorry, I didn't make this clear in the original post. I only know the extensions for these "intermediate" files, e.g. `outputs1`, `outputs2`. I do not know the actual names of these output files. 
Seems like [process substitution](http://vincebuffalo.org/blog/2013/08/08/using-names-pipes-and-process-substitution-in-bioinformatics.html) is what you are looking for. Here is how I would do it: ``` perl script3.pl --i &lt;(perl script2.pl --i &lt;(perl script1.pl --i input1.tsv)) ``` This assumes you can get them to output to stdout. I think the right way would be to have the scripts take their input from stdin and output to stdout so you could just do a normal pipe, but this works.
Got it. Thanks!
Can you predict the file names somehow? Or do the scripts have some flag that makes them use stdout / stdin instead of reading from and writing to specific files? It's hard to suggest a specific solution without knowing more about these scripts. If you don't know the file names, you could try to just use the most recent file in the current directory. #!/bin/bash perl script1.pl --i $1 outputs1=$(ls -t1 | head -n1) perl script2.pl --i $outputs1 outputs2=$(ls -t1 | head -n1) perl script3.pl --i $outputs2 rm $outputs1 $outputs2
I don't think that'd work. `&lt;(cmd)` takes the output of `cmd` and treats it as a file. Since in this case, `perl script1.pl --i input1.tsv` doesn't output anything (at least nothing relevant to STDOUT), `perl script2.pl --i` will get passed an empty file.
[removed]
/u/kshenoy42 I want to remove the $num and not just shit it out. The reason is I will be doing a for loop until I find the smallest number. The only thing is that I can't use the sort function. Maybe I can remove it with array?
&gt; I tried doing what you suggested: &gt; [code left out] &gt; Thinking I could pipe in that result into sed, but it doesn't appear to work. I just get an empty file. That's not what I suggested. Use the command substitution and put your chain of chaos in there: my_variable=$(a | b | c | d | e | ... | madness) sed 's/'"$my_variable"'/John/' input_file.txt
It's much easier to just use find for this task: find /var/log/* -maxdepth 0 -iname "*.gz" &gt; /tmp/myfilelist.txt 
[removed]
The new code still isn't changing `d`. It's passing d-1 as a parameter to a new recursive call of depth where `$2` is assigned to a new local `d`; `d` in the caller is unchanged. Your long explanation of what you think happens leaves me confused. Is the code working and you just want to understand why?
Like /u/commandlineluser noted, I mixed up echo and parameter expansion.
bash can do some arithmetic, yes: `echo "$((2 * 3 + 5))" "$((1024 * 1024 * 1024))"`
So I would get the available sectors and then just multiply by something like 0.9? SIZE='sectors*0.9' EDIT: using SIZE=((number of available cylinders* 0.9)) and then using $SIZE as a cylinder count for sfdisk seems to work EDIT2: used *100/90 because bash doesn't like 0.9
I'm not sure about the rest, but your solution will probably include the "comm" command.
Apache NiFi can orchestrate custom processes like the one you're describing. The files are encapsulated in a structure called a flowfile, and moves through a system of custom tasks that get you closer to your final product. [NiFi tutorial](https://youtu.be/ou2JWhPvWps) 
 grep -Z -l -R --include 'first*.txt' '&lt;date&gt;01/23' | sed 's/first_/second_/g' | xargs -0 -I NAME find -name NAME -exec sed -n 's/.*upc&gt;\(.*\)&lt;\/upc.*/\1/p' {} +
[removed]
I think this is what you're asking? http://ss64.com/bash/read.html
Wouldn't it be smarter to get the size of the file and determine piece-size automatically? #!/bin/sh for i in "$@"; do [[ ! -f "$i" ]] &amp;&amp; continue # not a file, skip i_bytesize=$( stat --printf="%s" "$i" ) #... case statement with -l values in bytes # cases give a PSIZE="level" declaration mktorrent -l "$PSIZE" -v -p -a announce.url/here -o "$i.torrent" "$i" done Fill in the case statement with your table and test by adding echo before mktorrent command. Or, use read to input manually: read -p "Piece size level: " PSIZE case "$PSIZE" in 19 | 2[0-4] ) # your mktorrent command .... ;; * ) exit 1 ;; esac Instead of exit 1 you could set a default PSIZE, or put the entire read+case in a while true loop so that only a correct value will progress. On the phone so it's hard to type nicely :p
Try while read line; do rsync /data/nfs/hosted_documents/47/$line /home/somebody/recover_47/; done &lt;/home/somebody/something_2.txt
The bash man page is really good. Start by skiming, then revisit it now and again. 
Hi, I am also a noob coming to bash, but here is how i started learning it. * I started taking Bash scripting from Linuxacademy. (They have a good one for starters with couple of exercises.) * Started repeat the script twice so, i can remember syntax on fly. * Now go ahead and find quizz and challenges and first write how you want to proceed to solving it, like here i need to use repetition of a condition should i use a for loop or while. which one is effective. Can't say i became an expert but i am comfortable with scripting with what i learnt until now. P.S : I am still in 2-3 steps, and still in completing the course.
First of all, the syntax here is definitely bash and not sh so you really should use `#!/bin/bash`. I know it doesn't matter in most of the Linux distors today but it's just good practice. Second, do you own homework. It honestly looks like you just pasted in the template from the assignment with zero effort to figure it out yourself. If you look at the `echo ${quiz[*]}` line, it should give you a pretty big clue of some things to try. Even if you don't see that, why not start with google search.
Use `exit 0`. `exit=0` just sets a value for `$exit` to 0.
I guess I was putting too many keywords in while searching! Thank you, that's what I needed. Apologies for the dumb question!
Well that suggests you have carriage returns in your data. Do you see any `\r` in the output of sed -n l /home/somebody/something_2.txt If so you could strip them using `tr` tr -d \\r &lt; /home/somebody/something_2.txt | while read -r line; do ... Or you could overwrite the original file (e.g. `perl -pi -e 's/\r$//' file`) and use `rsync`'s `--files-from` option
Hey Thanks for the reply. Is linuxacademy a free course or does it cost to join just that class? I'm definitely trying to save some money right now, and won't be able to afford a lot of classes right now (hopefully in a few months I'll have my savings rebuilt).
You're talking about the manual from like the Terminal, yes?
perfect, thanks!
I think the only pre-req is to decide what you want to do with bash scripting. Bash scripts are good for automation of operating system tasks and basic text/data manipulation which mostly depends on 3rd party utilities which differs in GNU, BSD or commercial unix worlds. When you need to process some more advanced datastructures, like n-dimensional data arrays, or hashmap of arrays, etc. or you need to process JSON or XML data or your script should be widely portable, then it is better to use something else, like Python, Ruby or perl. I made some Python/bash guide for admins, take some inspiration: http://bruxy.regnet.cz/web/programming/EN/english-workshop-bash-python/ 
Please check also this: http://gnulinux.guru/ But it is for GNU/Linux. Also MacOSX uses 3.x version of bash because of licensing and there are so many nice features in 4.x (like hash arrays and networking).
It's free for a week and its cost around 29$ for a month. I suggest trying it free and see whether its for you or not. if you like go ahead.
Well, since `exit` is never assigned (once the `exit=0` is fixed), that would be incorrect. I assume the intention is an infinite loop, i. e. `while true`.
im no guru but here's some tips from when I learned.. Practice if [[ ]] statements and types of comparisons you can do == =~ -eq etc . Then move on to for / while loops. Combine the two and maybe throw some string manipulation with sed and awk. Pass data around with | pipes and &gt; redirection and you'll have a pretty good working knowledge how to script in bash. You should also know to run your scripts in debug mode with "bash -x script.sh" to see what's going and what's going wrong occasionally. Only other thing I can think of is practice knowing the difference between double " and single quotes ' and learn all the environmental variables available to you like $* $0 $1 $? $!. You can see them by typing "set" and "export". lastly, if you get stuck first google it then goto #bash on freenode irc if you are still having trouble. Hope this helped
Assigning ($@) is the cleanest and most common way. Stick to that.
Nice, that is useful. Cat is useless in terms of? On another not, will this work to print numbers in that way as well? 
&gt;Cat is useless in terms of? Read the link I supplied :) &gt;On another not, will this work to print numbers in that way as well? Yes. This seems like homework, so I'll leave it to you to figure out how.
this actually did the trick, thankyou, i have never encountered this before but now i am intrigued, how did these "carriage returns" get in the data?, and why are they so hidden? the output was generated with just a simple command ls -alh and afterwards everything but the file name was stripped out
 ${@:INDEX:1}
Well in your other comment you mentioned being on Windows and CR LF &gt; i gave this a try, the formatting was done in notepad++ Windows (CR LF), changed this to the Unix(LF) mode, saved and ran the script again, but it still produced the same error So perhaps you didn't save the file with Unix line endings as you thought? Because the carriage returns (CR) were still in your file.
check your env when run by cron. my guess would be PATH is different and some binaries are not found. also check your stderr output which might be mailed by cron.
&gt; Also, the script uses lots of tput commands to do text coloring. You may want to place a 'TERM=xterm' line in the crontab file to see if that helps. The script starts with `export TERM=xterm-256color`.
Not quite, you want to quote the `$@`: set -- "this is a single argument" goodArgs=("$@") badArgs=($@) printf 'There are %d good args and %d bad args.\n' "${#goodArgs[@]}" "${#badArgs[@]}"
If you're going with the df command, i'd suggest using the -h switch to output the space info in (h)uman readable format. ie 50GB instead of 53687091200 bytes.
Perhaps you're looking for `--files-from` which allows you to specify a file that contains the list of names
Always always always set your PATH. Even when not running through cron. I caused a 6 (maybe even 7) figure downtime last month by neglecting to include a PATH in a script run on 50,000 systems.
Awesome, I removed the `fi` before the `else` statement. I thought `getline`would use the first line in my input file but I don't think that's correct so I've removed it. Unfortunately the code now returns `syntax error near unexpected token `;'`. Do you see the issue? I've updated the code in my original post too.
Thanks, I've added the closing `'` but I'm still receiving a syntax error. Do you see the problem?
That looks cool, and I'll remember it for later, but it might be overkill for my little logging improvement. Thanks anyway!
probably the `;` after `then`, i didn't see it first and it should definitely not be there
Differences between versions are described here: http://tldp.org/LDP/abs/html/bash2.html It is quite difficult for me to explain benefits of hash arrays, but if you used them in a different programming language you will know :) Networking in bash can be done by 3rd party programs like wget, curl (for HTTP) or nmap, netcat, but there is also a built-in support for generic TCP/UDP communication.
&gt; I caused a 6 (maybe even 7) figure downtime last month by neglecting to include a PATH in a script run on 50,000 systems. .... Obvious question: was this script put through appropriate dev/test/uat (and/or peer review) or was it just blindly thrown into prod?
How do i go about doing this?
Thank you!
Make sure to set your path to include `pihole`.
Thanks for the post, I was able to make this little test script, and it works, it successfully changed the value of the QoS downlink. Now just got to figure out the rest... hehe #!/usr/bin/expect spawn telnet 192.168.1.1 expect "*WRT*" send "root\r" expect "Password:" send "MyPassowrd\r" expect "*root*" send "nvram set wshaper_downlink=15000\r" Edit: I got the script to subtract 1000 from the speed successfully #!/usr/bin/expect spawn telnet 192.168.1.1 expect "*WRT*" send "root\n" expect "Password*" send "my password D:&lt;\n" expect "*root*" send "nvram get wshaper_downlink\n" expect "*root*" puts "$expect_out(buffer)" set num 0 regexp "\n(\[0-9]+)" $expect_out(buffer) - num puts $num set speed [expr {$num - 1000}] puts $speed if {$speed &gt; 4000} { puts "setting speed" send "nvram set wshaper_downlink=$speed\n" } I sleep now, but now I just need to figure out how to trigger this script when a ping tests over 100ms.
Get rid of the first $ in the line where you set appleurl.
also there is an inconsistency between $serialnum and $serial_num
Yes, you can use a single mount point assuming you never have more than one device mounted at a time. In fact, most linux distributions come with one on the filesystem by default. Check out `/mnt`. For your test, use `findmnt`. Below is a simple proof of concept. user@host:~$ sudo mount /dev/sdc1 /mnt user@host:~$ if [[ $(findmnt /mnt) ]];then echo "It's mounted.";else echo "Nothing mounted.";fi It's mounted. user@host:~$ sudo umount /mnt user@host:~$ if [[ $(findmnt /mnt) ]];then echo "It's mounted.";else echo "Nothing mounted.";fi Nothing mounted.
Yep, once you got me pointed in the right direction, I was able to figure it out. i run this every minute between the hours of noon and midnight #!/bin/bash OUTPUT=$(ping -c 4 www.google.com | tail -1| awk '{print $4}' | cut -d '/' -f 2) PING=${OUTPUT/.*} if [ $PING -gt 100 ] then expect -f /home/pi/expect.sh fi This is the script that actually changes the settings #!/usr/bin/expect spawn telnet 192.168.1.1 expect "*WRT*" send "root\n" expect "Password*" send "password\n" expect "*root*" send "nvram get wshaper_downlink\n" expect "*root*" set num 0 regexp "\n(\[0-9]+)" $expect_out(buffer) - num set speed [expr {$num - 1000}] puts $speed if {$speed &gt; 4000} { send "nvram set wshaper_downlink=$speed\n" expect "*root*" send "nvram commit" } this one just runs daily at midnight to reset the speed back to 18000 #!/usr/bin/expect spawn telnet 192.168.1.1 expect "*WRT*" send "root\n" expect "Password*" send "passwordHEHEHE\n" expect "*root*" send "nvram set wshaper_downlink=18000\n" expect "*root*" send "nvram commit" I added them to the crontab today, and this afternoon the speed settled at 9000 as i watched some youtube. 
by using what now? if you have to remember one thing I said, just remember that ......
I think /u/unixtreme meant to say "by using UUID". Devices are assigned the numbers sdb1, sdc1 etc, based on order of mounting (i.e. whichever is mounted first gets assigned /dev/sdb1). However, the UUID is constant. So using that will be more reliable than just sda1 and such.
I don't really see where cat would come into play here. rsync, whose name stands for something like remote sync, is probably a very good choice for this case. At least as long as you just have to copy the files wholly. If you have to update one servers users and groups with the others while keeping users that might be on server one but not on server two, it might become a bit more difficult.
[removed]
Well variables wont expand inside single quotes. You could double quote the whole thing e.g. "import first; first.func(${i})" Another way you may see it written is to `'quoted'"$shellvar"'more quoted stuff'` This would be so you didn't have to escape any characters you didn't want the shell interpreting. python -c'import first; first.func('"$i"')' You could let python do the replacement by using import `sys` and passing it along as an argument e.g. python -c'import first, sys; first.func(sys.argv[1])' "$i" You may need an `int()` as `sys.argv` is a list of "strings"
Yet another way is to use the environment X=$i python -c 'import first, os; first.func(os.environ.get("X"))'
Well, OP wants to unmount the old directory if it exists, not just do nothing… so: if mountpoint -q /media/all; then umount /media/all fi mount "$1" /media/all
What was the actual use case for you? I mean you are opening 4 processes instead of just scp? I think it's a bit inefficient but I don't want to nag, I'm genuinely curious. :) 
As a follow up, the first.func() returns either 1 or 0 currently I have res=$(python -c "import first; first.func($i)") echo $res but the echo statement doesnt print a 1 or 0 so I am guessing the func return value isnt being stored the way I have everything set up. any tips? I know the function is being called because I have a print statement in there.
If you don't use the function's return value, it is simply ignored. Maybe you want to have `print` print the return value, or you could have the python script exit with 0 or 1 based on the function's return value.
You haven't mentioned what the variable $FIC_REF_EAN13_SRT contains. You should also show an example of the two files contents, so we have both the input and the output. Also, probably not the issue, but make sure you don't have a space after the \
&gt; while read $logins `read` expects a variable name. `$logins` is not a variable name, it's an expansion. Change it to while read -r login &gt; if [ `ssh -qn "$logins" "hostname; ps -ef | pgrep $USER" | wc -l"` -gt 0 ] `pgrep` doesn't read stdin like grep does. It only reads the process table, so piping `ps` to `pgrep` makes no sense. You also can't use the output of that ssh command in an arithmetic comparison, because the output includes the hostname, which is not a number. To get just the count, you can do: count=$(ssh -n "$login" 'pgrep -cu "$USER"') I can't really make sense of what the script is trying to do because the `kill $PID` further down makes no sense.
yeah, i am using since there is only 1 .py file in there, i will change it eventually but im still building my script/project so i will change it later, thank you :) 
thank you! this works perfectly :)
Thanks for the feedback. Basically, I'm trying to verify all running processes across a set of login nodes in our environment. Then, store that result somewhere long enough to give the user that list and, based on that process list (I'm guessing a case/esac?), kill them or leave them be.
Okay, I think I’ve misrepresented it a bit – I definitely recall the using the right-hand side of the pipeline (`ssh server 'cat &gt; filename'`), but I think the left-hand side is usually a pipeline :) That said… I still wouldn’t use `scp` in all cases. For copying many files, I prefer `tar` over `ssh` instead of `scp -r`, because `scp` has a reputation for being slow. tar cz foo/ bar/ | ssh newserver 'tar xz -C /srv' You can throw a `dd status=progress` (or `pv` if you have it installed) in that pipeline to see it working. I tried it out just now, on a moderately-sized git repository (which apparently hadn’t been packed in a while), i. e. lots of small files: - `tar`: 24 seconds - `scp`: 20 minutes - `scp -C`: 17 minutes I think `scp` has a lot of overhead per file (new connection for each file?), which really shows here.
You could check the variables $USER (==root)or $UID (==0), or check if `/` is writable (`test -w /`). But really, those are hacks avoiding your real issue; which is that you really should only have it run when you want; the fact that it is running an extra time when you switch to root is just one symptom. You didn't actually explain how you've set it up; but I get the feeling you are sticking a snippet in `/etc/profile`? Perhaps you would be happier configuring `PrintMotd` in `sshd_config`; or adding `pam_motd.so` (or `pam_exec.so` if you really want it to be a script) in `/etc/pam.d/sshd`. PS: You should probably be using `sudo -i` rather than `sudo su`.
Thanks Luke.. you are dealing with a bash nooblet here so you kinda lost on your opening sentence. From my googling I found out that you can toss in a motd.sh in to /etc/profile.d which is cool and its working great, but annoying when it runs after each sudo su. The sh calls cowsay.. which typing that into here, to me, relates to saying it out loud which sounds childish. But nevertheless its something that I want implement on my servers and finding out how to block root will only help me to learn more about my OS.
Works fine here. Perhaps there are some non-printing characters in one of your files? try `sed -n l $FIC_REF_EAN13_SRT` you may have some carriage returns in there `\r` - if so you could `perl -pi -e 's/\r$//' filename` to get rid
That's actually pretty cool. Thanks for teaching me something new. :) 
Your command looks right based on your input files. You should check like /u/commandlineluser said for nonprinting chars. Does every single output line have that newline in it?
Yep. The link/example was just to get them started
To do exactly what you are asking could be implemented as if [[ $UID =0 ]]; return fi cowsay ... But really, if your goal is to just have it run cowsay when ssh'ing in, then I think you're better served by sticking this at the end of `/etc/pam.d/sshd`: session optional pam_exec.so stdout /etc/motd.sh 
Yup man! That was my problem! Thank you very much! 
Thank you!!! That did the job. 
Oops, didn't know I needed that. Thanks for the help.
Yeah, it would. But I can imagine this being an easy way of prototyping for someone who isn't familiar with python or ruby.
NO! YOU BROKE RULE 1! HOW CAN YOU EXPECT ME TO RESPECT YOU NOW.
[du](http://djangstorm.com/Content/du.html) and [df](http://djangstorm.com/Content/df.html) will help in this case.
It does, but it's `-e`, not `-c`
thanks works
I'm glad you got it working! Sorry I hadn't responded earlier; got caught up with work. 
No worries! I did too. You got me on the right path, anyway.
Awk can also refer to the last field as $NF, second last as $(NF-1) etc. - that way you don't need to care about spaces in earlier "fields" causing awk to think there are more fields. In this case the last two fields don't have spaces so you can do it this way: DriveMountPoint=$( lsblk -f | awk -v uuid=$DriveUuid '$(NF-1) == uuid { print $NF }' )
Yes, by all means, use UUID. Also verify that the drive is mounted- a full root directory is no fun. The following code will put the mount point in the DriveMountPoint variable. If DriveMountPoint is null, the drive is not mounted. (Note $DriveUuid should be set to your drive's UUID) DriveMountPoint=$(lsblk --output MOUNTPOINT --noheadings /dev/disk/by-uuid/$DriveUuid)
Its bash that uses `-c` and reads in commands after the first non-option argument.
This is a bit of spammage... it's 99% python and you've linked this in /r/Python, /r/Database, /r/Ransomware, /r/linuxdev, /r/linuxadmin, /r/linux, /r/mongodb, /r/computerforensics, /r/netsec, /r/Pentesting, /r/InformationSecurity and here in /r/bash... Hand over heart: Do you think this belongs in all of these subreddits? Here's my one piece of constructive feedback: Get a clever url for it. Like http://mysqltuner.pl... it's not from Poland, but how can you forget that url?
Thanks for your feedback. There's https://mongoaudit.com too :)
I don’t think it’s Bash-specific. It’s a bit convoluted, but: - The install script (the one you’re supposed to `curl | bash`) uses some Bash features, but could be rewritten to POSIX sh. - The actual program is written in Python, so it could run on any platform supported by Python. - The binaries included in the 0.0.1 release are for Linux x86-64 and macOS x86_64 (no 32-bit versions). The Linux version happens to also run on Windows with the Linux subsystem. Bash only plays a marginal role here, in my opinion.
I think that your title is a bit misleading, you're replacing the whole file, not just a line. For that you'd need to use something like sed. Of course there's a million ways you could code this, for me this is simple and easy to read so I'd call it "optimal". And bash comments are done with a # not //.
&gt; head -1 file Note that this is obsolete/deprecated syntax for this command and may not be supported in the future (and is probably not supported on all systems now). Use the `-n` option instead, like `head -n 1 file`. See `man head` and the [POSIX spec](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/head.html#tag_20_57) for more info. &gt; echo $line &gt; /path/to/file.txt You want to quote the `$line` expansion with double quote marks, otherwise (unless you've disabled these) it is subject to field splitting and globbing -- meaning whitespace will be garbled and any glob patterns (like `*`) matching file names in the current directory will be replaced by those file names. 
In addition to the other corrections... this should also work. I like shortcutting in bash. file=“/path/to/file.txt” line=$(head -n 1 “$file”) (( line &gt; 3 )) &amp;&amp; (( line++ )) || line=1 echo “$line” &gt; “$file” 
Could you explain the logic in your line:3 above? I am having a hard time wrapping my head around it. 
Actually, that gives me an idea: `sed Q $file` (GNU sed) is a minimal test if a file is readable or not :) shorter than `[ -r $file ]` or `test -r $file`!
&gt; (( line &gt; 3 )) &amp;&amp; (( line++ )) || line=1 Another way to express this would be: # Test if $line is &gt; 3 if (( line &gt; 3 )); then # If so, increment (( line++ )) else # Otherwise, set $line to 1 line=1 fi Did that clear it up or did it just raise more questions? [Possibly relevant reference reading](http://mywiki.wooledge.org/BashGuide/TestsAndConditionals#Control_Operators_.28.26.26_and_.7C.7C.29)
I think I'd have to stick with test unless you can get rid of the exception... $ sed Q temp.txt &amp;&amp; echo "yes" || echo "no" sed: can't read temp.txt: Permission denied no $ sed Q temp2.txt &amp;&amp; echo "yes" || echo "no" yes $ test -r temp.txt &amp;&amp; echo "yes" || echo "no" no 
Well, yes, I’m clearly not being entirely serious ;) you can add `2&gt;/dev/null`, but then it’s no longer shorter.
Did someone say shortcutting? file="/path/to/file.txt" line=$(head -n1 -- "$file") printf '%d\n' "$((++line&gt;3?1:line))" &gt;| "$file"
gotcha, I had to try it
Not that type of bash!
I am new to writing Shell scripts so feedback on bugs and how to expand this script are welcome. Thank you.
Rather nice, but lots of repetition. open_in_term() { gnome-terminal -x sh -c \'"$@" \; exec ${SHELL:-bash}\' # you should test it } open_in_term aireplay-ng -1 0 -a "$targetBSSID" "$interfaceName" # I'm not sure this works, please test [Y tho?](https://github.com/nhooper/basicRecon/blob/a8273a493c643fab2ab4997259279d58d7b08ecc/ClassCopy#L21) In the [switch case](https://github.com/nhooper/basicRecon/blob/a8273a493c643fab2ab4997259279d58d7b08ecc/ClassCopy#L133), I'd use real words instead of a numbered list (to avoid unecessary word -&gt; number -&gt; action processing): case "$attackType" in Airo*) ;; #Airodump code F*) ;; # FakeAuth code [...] Finally, give http://shellcheck.net a shot, to correct remaining artefacts/bad practices. You should also use consistent indentation... :P
I used this pattern, when I used screen (since moved on to `tmux`): if screen -list | grep -q &lt;session_name&gt;; then screen -dr &lt;session_name&gt; else screen -S &lt;session_name&gt; screen &lt;initial_command&gt; fi In tmux (for giggles): if ! tmux list-sessions | grep -q ^&lt;session_name&gt;; then tmux new-session -d -s &lt;session_name&gt; tmux send-keys &lt;initial_command&gt; Enter fi tmux attach-session -d -t &lt;session_name&gt;
Thank you. I will give this a try. 
Both helpful suggestions. Thank you for looking at the code.
I have never used /dev/tty before and do not understand your comment about using it to edit data. Could you please point me to further documentation and/or examples? I tried googling and looked at "&gt;man 4 tty". Your comment about a pipeline suggests that you were mainly interested in "multiple select" menu functionality. As part of work on a possible rewrite of the project I came up with a "multi select" menu utility that would work as part of a pipeline. At the top of the file for the [selected](https://github.com/ronaldxs/withsome/blob/bash-rewrite/experimental/selected) utility there is a description in POD documentation. If I misunderstood your comment about "something that could sit in a pipeline" example(s) would seem to me to be helpful if you have time.
With `foo | choose | bar`, choose's stdin and stdout are the pipes' file descriptors. Its stderr has been left alone. It could open the file /dev/tty to gain access to its "controlling terminal", as tty(4) says. If that succeeds then it's a way of doing I/O with the terminal, and me, the user, to choose from the lines on its stdin and have it copy the selection to stdout. A crude example. $ seq 42 | &gt; (l=; while read n &lt;/dev/tty; do l="$l ${n}p;"; done; sed -n "$l") | &gt; fmt 13 31 7 3 7 ^D 3 7 7 13 31 $
Personally, one of the most important things about CLI utilities is being able to script them, as well as the Unix KISS philosophy where one tool does something simple and well. With this, unless I'm misunderstanding it, scripting would be difficult or impossible (is there any predictability as to what choice would be 1? 2? 4?) and it does duplicate `find` functionality. I think /u/odokemono put it best — if it helps you that's great, but I don't see it being particularly useful to others. (ninja edit: every *nix guy makes tools that are useful to him and not to others)
No, it isn't really intended for embedding in other scripts. The intent was, as [/u/odokemono](https://www.reddit.com/user/odokemono) suggested, "useful, and friendly" convenience. The example in the OP is a common use where you want to vi or grep a source file but don't quite remember its name. Similarly it can be used to pick out and run reports/sql files. There is some predictability in the order of choices which are sorted by directory depth (nearest matching files first) and then alphabetically on the assumption that more commonly used files tend to have shorter directory paths. I understand the concerns and thank you for feedback.
 if [[ $(bc &lt;&lt;&lt; "$var1 &gt; 0.02) == 1 ]] || [[ $(bc &lt;&lt;&lt; "$var2 &lt; 0.09) == 1 ]]; then
Because Bash only supports integer arithmetic, no floating-point numbers, that’s why.
That would definitely put a damper on the use-case then! Thanks for the info -- I would've just scratched my head as to why my script sucked.
If you type: This is text this is code (4 spaces before) `this is inline code` You see: This is text this is code (4 spaces before) `this is inline code`
[removed]
 stat -c %y ths hows the last time it was modified, you can strip it to get to the minutes. I am not on my computer but I was working on something similar just not too long ago
There shouldn't be a problem if you use pipeline between these scripts, contents of pipeline are not altered by bash
Ah OK, that sounds good then! I was reading how bash removes newline characters when assigning a value to a variable, so I wasn't sure if they would get removed in a pipe too, but I guess not. Thanks!
You can also use "tr" and convert \n to something else if you really have to. there was a very obscure use case I had years ago that I can't remember haha
 # stat --printf %Z testfile stat: illegal option -- - usage: stat [-FlLnqrsx] [-f format] [-t timefmt] [file ...] Looks like a GNUism (the above version of `stat` is on FreeBSD, so a similar error should apply for OSX as well). Maybe the `stat` command could be something more like: stat -c "%Y" "${file}" &gt;/dev/null 2&gt;&amp;1 || stat -f "%m" "${file}"
I just hammered this out, it's not pretty and not 100% accurate as I've intentionally left some issues in there. You'll also have to convert NZ's tax brackets to yours... you know, I don't want to do all of your homework for you... http://pastebin.com/ndH7b8p3
Also `-name -maxdepth 0 '*.jpg'` in your `find` command is incorrect. I'm assuming you mean `-name '*.jpg'` As for `-maxdepth 0` that makes `find` process only the start directories given on the command line e.g. `.` in this case. Perhaps you mean `-maxdepth 1` i.e. you don't want to recurse? If so you could just do it with a loop e.g. start=5676; for old in ./*.jpg; do printf -v new %04d.jpg $((start++)); echo mv "$old" "$new"; done You can remove the `echo` to have the `mv` commands actually execute.
I don't know anything about zsh.
Well, if you want this to be persistent mutable state of the system, it belongs in `/etc`.
Subshell output should also be captured in this case, unless the script does some weird stuff like writing directly to `/dev/tty`…
Need more info to help you. Likely one of the files might notexist.
Hey, thanks for all your points. I mostly do PHP so this was like adventure to the new world. I'll fix those. :)
This is perfect. Beautiful. :)
I guess it's doing something weird, then, cause it's not captured. 
It looks like your problem isn't the Bash script but Raspberry Pi. That error code comes from RetroPi - https://github.com/RetroPie/RetroPie-Setup/issues/182
The script is: sudo do-release-upgrade 2&gt;&amp;1|tee logfile
No, the `do-release-upgrade` script… or is that a compiled binary?
The do-release-upgrade script is here: https://file.io/mhpO1X , but it's a general problem, though, to capture output if the command launches a subshell
ok, I can accept that;), but why isn't my command working, then? If it's just launching subshells, it should capture it, but what else can it be doing?
When I run the command, the output is like hundreds of lines, but this is all I get in the logfile: Ser etter ny utgivelse av Ubuntu ^M0% [Arbeider] ^M0% [Kobler til archive.ubuntu.com] ^M0% [Kobler til archive.ubuntu.com (2001:67c:1560:8001::11)] \ ^M0% [Venter på hoder] ^MHent:1 Signatur for oppgraderingsverktøyet [836 B] ^M99% [1 836 B/836 B 100%] ^M99% [Venter på hoder] ^MHent:2 Oppgraderingsverktøy [1 265 kB] \ ^M0% [2 1 183 B/1 265 kB 0%] ^M18% [2 234 kB/1 265 kB 18%] ^M41% [2 518 kB/1 265 kB 40%] \ ^M61% [2 782 kB/1 265 kB 61%] ^M85% [2 1 075 kB/1 265 kB 84%] ^M100% [Arbeider] \ logfile (END) 
Hmm, if I'm being hypercritical having a pattern for each case would be more resource intensive as opposed to doing it one time. I'd probably opt for the way you demonstrated. 
Well you need to get rid of the outer set of quotes HandBrakeCLI -i "$filename" -o "$newfilename" --preset "Android 720p30" --optimize --all-subtitles
Well the file has lots of blank lines in it - so you would need to filter those out - otherwise you wouldn't be reading the correct lines. Also you're using `${basename#*=}` which will remove up to the first `=` in the line - you probably want to be using `:` instead of `=` then another to remove the first leading space Also some of the fields contain `,` so you would want to quote those (or all) fields as not to create "broken" csv. There are a variety of ways you could do this - one could be something like user@host$ while read -r line; do [[ $line ]] &amp;&amp; ((i++ &lt; 15)) &amp;&amp; line=${line#*:} row+=("\"${line# }\""); done &lt; fanfiction.txt user@host$ ( IFS=,; echo "${row[*]}" ) "Little Helper","by Sheryl Nantus","X-Files","English","Completed","1999-03-16","1999-03-16","2017-02-18 01:44:55","K+","1","1,030","www.fanfiction.net","https://www.fanfiction.net/s/4/1/","https://www.fanfiction.net/u/3284/Sheryl-Nantus","Sometimes the worst person you want to see when sick is the best..." You could also do it using external tools e.g. grep . fanfiction.txt | sed 's/[^:]*: */"/; s/$/"/; 15q' | paste -d , -s -
Yeah sorry it wasn't basename specifically but the `#*=` expansion - you were performing that on everything: ${title#*=},${author#*=},...
[removed]
Well you can ignore the external tool version. `IFS` is character based you cannot set it to `: ` to do what you want - you would have to use 2 expansions First you remove up to the `:` then you remove the leading space (which may not be there but that's okay) user@host$ line='Category: X-Files' user@host$ echo "${line#*:}" X-Files user@host$ line=${line#*:} line=${line# }; echo "$line" X-Files user@host$ line='Category:X-Files' user@host$ echo "${line#*:}" X-Files user@host$ line=${line#*:} line=${line# }; echo "$line" X-Files The main problem is the looping though - you have a while loop to read the file line by line but then you have all the read commands inside the loop too. So `read -r name` will read the first line of the file then `read -r title` will read the second, `read -r author` the third, ..., etc. So instead of this approach you could strip out the blank lines then you can read the 15 lines as they will all be in the correct order. That's what the first while loop example did except it used a counter to know when to stop and read them all into an array. 
`echo` outputs the string you pass as argument followed by a newline. If you pass it multiple arguments, it outputs them separated by a space, and again, ending with a newline. If `echo $ENV` outputs an empty line, it means ENV is unset, empty, or contains only blanks, so it's not very useful. To actually tell if it is set or not, and what exactly it contains, use `declare -p`. declare -p ENV
http://paste.ubuntu.com/24033062/ Like that? I'm seriously confused at this point.
 for file in ./*.txt do i=0 row=() while read -r line do [[ $line ]] &amp;&amp; ((i++ &lt; 15)) &amp;&amp; line=${line#*:} row+=("\"${line# }\"") done &lt; "$file" ( IFS=,; echo "${row[*]}" ) done &gt; output.csv
Well, that looks roughly like `apt` output, with some [escape sequences](https://en.wikipedia.org/wiki/ANSI_escape_code) for cursor movement.
yeah, I know that, but the actual output of the command is like hundreds of lines. 
Are you making a csv, or an sqlite database?
Csv , which I understand sql can import.
`echo ${ENV?}`
1+
 export i This will make the variable `i` available to all processes spawned by the shell, instead of just the shell itself.
Thank you ! Near the end of the letsencrypt generation, there's a text block that starts with "IMPORTANT NOTES". It works, without error now.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
http://mywiki.wooledge.org/Quotes is a reasonably good description of how quotes work in bash. There are more links at the bottom. They don't operate like any other programming language, so if you've got any programming experience you need to forget all of that while writing bash. As an aside, this sort of thing would be much, *much* easier to do with Ansible and its expect module, because then you don't have to deal with all this nonsense.
I'd recommend reading the EXPANSION and QUOTING sections of the bash manpage. If you don't care about portability to other shells you can use arrays to preserve the number of arguments in a command: function printargs { for i in "$@"; do echo "$i" done } cmd=(printargs one "all part of \"two\"") "${cmd[@]}" 
See [FAQ 50](http://mywiki.wooledge.org/BashFAQ/050) for an explanation, and examples of how to do it correctly.
Have you considered python?
[removed]
Huh, I didn’t know that – apparently it’s part of [ye olde Markdown spec](https://daringfireball.net/projects/markdown/syntax#precode), but [CommonMark](http://spec.commonmark.org/0.27/#indented-chunk) doesn’t allow it. Anyways – tabs are difficult to type in the browser (unless, as you said, you paste them in), so I think I’ll keep the message the way it is. (In case you’re curious, the trigger is just “line starts directly with `#!`”. I don’t see any such line in the current version of the post, so I assume you edited and removed it?)
I don’t think that’s possible – functions are a shell-only construct and not available to any other process. You can still keep the function in a separate file, though: #!/bin/bash function myfun { printf "myfun: called with %d arguments" "$#" printf ' %s' "$@" printf '\n' } if ! caller 0 &amp;&gt;/dev/null; then # running as standalone script myfun "$@" fi Here, the script will call `myfun` if executed directly, otherwise it does nothing by itself except defining the function. In other words, `expect` can call this script as an external command, but your script can also source the file and then simply use the function. (Edit: spelling)
I don't think I had a `#!` in there, but it's possible. I do remember making a minor edit with a back-tick that didn't have a mate for in-line code. I didn't notice the bot comment until after I fixed that error though, but maybe that was it. It's unfortunate that reddit doesn't seem to have a preview option available prior to submitting. 
Psst – [RES](https://redditenhancementsuite.com/) has that. It’s amazing. Install it and you’ll never want to use Reddit without it again.
I know I should use keys, and was wondering if someone was going to mention it, but I'm constantly re-imaging new computers, security isn't a concern, and I'm lazy. I even wrote a keygen script, but it fell out of use. (which... also... has the password coded in.... There's a head scratcher for you.) tradeKeys: #!/bin/bash PASSWORD="adms\r" function generateKey() { local HOST=${1} expect -c "spawn ssh ${HOST} ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''; expect assword ; send $PASSWORD; interact" } function tradeKey() { local FROM=${1} local TO=${2} expect -c "spawn ssh ${FROM} ssh-copy-id ${TO}; expect assword ; send $PASSWORD; interact" } #Generate Key ssh-keygen -f ~/.ssh/id_rsa -t rsa -N '' for i in `seq 1 4`; do { #Pass key to everyone else ssh-copy-id debian${i} #for j in `seq 1 4`; do #{ # tradeKey debian${1} debian${2} #} #done } done I'm really curious about this though: ssh debian "DISPLAY=:0 zenity --info --window-icon='info' --text='System update complete'" You don't need anything between the variable set and the zenity call? I mean... I tried it, and it works, but I don't know why it works. I usually get bad results if I don't have a `;` or a `&amp;&amp;` between commands. In this case, adding those seem to break it.
&gt; I don't understand the point of encrypting your private ssh key with a password, if you are going to use a script that has your password in plain text to do it, Exactly. Makes no sense. That's why I quit bothering. (Edit: Actually, it's not encrypted with a password. It's passwordless. That's what the `-N ''` does. The password just gets me into the other computers intitally) I have a set of computer that all need to ssh to eachother. I can have up to 15 computers. To initially create the private keys, I need to first ssh into them and I didn't want to type the password 14 times. From there, they all need to be able to ssh to eachother so I needed to copy all the public keys from each of them to each of them so they all had all of eachothers public keys. I realize that having the password stored in plain-text is REALLY REALLY bad from a security standpoint, but I didn't know an efficient way to get it done. I might look back into it again. Since usernames and passwords are the same on all machines, I could probably re-write it so that it prompts me one time for a password and remembers/reuses that password until the script exits. ( I think I tried this and failed... but I've learned more since then, so I can try again) The password we use is very insecure anyway, (literally 4 lowercase characters) and it's not up to me to change it.
&gt; From there, they all need to be able to ssh to each other Why? The more we know about what you are trying to do, the more we can help you perhaps improve your methods. There are likely better ways to do what you're doing, perhaps leveraging something like `ansible` or `salt`, depending on what your goals are.
Primarily, they are all connected to work together to run an application. Any one of them can launch the initial application and act as the controller that sends commands to the others to act as slaves. So to start the system we have to start several processes on each machine from whatever computer launched the application. We also might jump on any one of them an run some scripts that gather system information from the rest. The system is connected via a switch which is not connected to an external network, except briefly for scheduled updates. It will then be monitored and connected to the network while the update is performed, then again physically unplugged from the network.
let me know sample on perl or python doing same work. I want to sesrch post function putput as well if it shows project commpleted 100 % otherwise i am thinking of giving sleep command for 10 min before next iteration happen. how about threads . How to give that ion shell.
Is this homebrew? It sounds a lot like what various HPC type solutions aim to do. In HPC-land you would have your login node(s) that you submit jobs on and those interact with the rest of the compute nodes. As for setting up SSH keys and such, you could look into doing SSH certificates, which are public keys signed by a certificate authority (usually self-signed) to allow users with those signed keys access to systems. Also, you could look into 'host based authentication' which can also make things easier for you to manage your 'cluster'.
&gt; username ALL=(ALL) NOPASSWD:ALL http://i.imgur.com/WVn7B8O.jpg
If your goal is to maintain consistency across the multiple computers, configuration management is _definitely_ something you would be interested in. Stuff like salt, ansible, puppet, chef, etc.
Glad my side projects can be of use to someone other than myself. If its the master script that launches all your most used commands that your getting an idea of its totally worth taking some time to write something up, i wasn't joking when i said i saved so much time now just jumping between screens and not having to retype common commands or remember the names bunches of little scripts i made to do them all. Now all the subscripts are in the /opt/masterscripts folder and the master is in the home. Then on connection all you have to do is launch ./master.sh and away you go. Although with my current setup i have to launch screen.sh and then once connected to the right screen run the master but hopefully someone will have an idea of how to make that work as one. 
Probably Ansible will be easiest to start with IMHO. Which is actually best is an exercise for the individual.
You missed the whole [shellshock](https://en.wikipedia.org/wiki/Shellshock_%28software_bug%29) debacle?
The variables of your second script will not be available to the main command, so you might want to combine into a single script as a new function, or call the source builtin to source the functions from a secondary file.
[removed]
No, I didn’t, but I didn’t know *why* Bash scanned environment variables for function definitions and didn’t care enough about it to figure out why.
Thanks for the advice, i am planning on combining into a single script to cut down on clutter and try to keep everything as self contained as possible. 
Im sorry, i've never really ventured into python. Is it simpler or harder to learn then doing it this way?
You need to quote your variables. Paste your script into shellcheck.net and it'll show you where you need to do this. `printf` instead of `echo` And you should avoid uppercase variables unless you know why you need them. And you could consider making the watering time a random figure. There is a script [here](https://github.com/rawiriblundell/scripts/blob/master/rand) with some ideas for you on how to do that.
Any reason for printf over echo?
I use this from time to time: while sleep $((20 * 60)); do notify-send "Small Break!"; done
I would recommend you using a different argument parsing mechanism (getopt), or if you want to stick with this one it's slightly better style to use case than nested ifs. Also, I would recommend you using $() instead of `` as described in this article: http://mywiki.wooledge.org/BashFAQ/082.
It would be much more direct to just write to sqlite, rather than writing to an intermediary format (especially since csv is rather tricky to get right). "Real" programming languages will make your life much easier when you're doing things like this. Python is a personal recommendation, but Ruby, Perl, etc. will all take much less time and have better error handling than trying to do this in bash. It's useful to have multiple tools in your belt.
FYI, if you want you can add a wait so the script then waits for all the child processes it spawned to finish.
Why not use the "exit" command?
[we need to go deeper](https://raw.githubusercontent.com/nicholas-leonard/slides/master/we-need-to-go-deeper.jpg) 
[removed]
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Not sure what you mean by: &gt; I tried adding the whole /media//bit/MyBook/in.txt to the line but nothing Post the script you are actually trying to run (properly formatted) to get better advice. Based on this post, it's hard to tell what your script actually looks like. I think you are saying you changed the `mv` line to have a destination of `"/media//bit/MyBook/in.txt"` instead of `"in.txt"`? Did you also change the source of the second `mv` command to `"/media//bit/MyBook/out.txt"`? Does "/media/bit/MyBook/Out/" exist? Do you get any errors?
You can use find arguments -print0 to output the processed files, and -exec to perform an action on each file that matches. Then you can redirect that into a text file like so (untested): find /media/bit/MyBook/In -print0 -maxdepth 1 -type f -name "*txt" -exec mv {} /media/bit/MyBook/Out/ &gt; /media/bit/MyBook/Out/out.csv I don't know what your .jar file does, so unsure if this is what you needed.
`declare -a` declares an indexed array, you need `declare -A`. declare -A array=() (( array['$name1'] += size1 )) (( array['$name2'] += size2 )) (( array['$name3'] += size3 ))
I like the disclaimer
This seems good but it gets complex when we don't know when the names repeat for an arbitrary number of names.
Not my method. That is how you wrote your example in your original post. I'm just correcting the syntax.
Ah! Oh Okay.
Good catch. corrected.
[removed]
I like these kinds of posts because you see some technique or syntax you've never seen before and have to go play with it and learn something new. But my brain definitely blew a few cells reading through this. 
Ah i got it
`man inotify` &gt; Inotify monitoring of directories is not recursive: to monitor subdi‐ &gt; rectories under a directory, additional watches must be created. This &gt; can take a significant amount time for large directory trees. You may also like to investigate `incrond` rather than scripting this, or if you're on a `systemd` based distro, you can investigate how `systemd` can do this for you as well.
Ignore inotifywait's output and just do `rsync -a` every time there's an event. rsync will compare modification time and size on the files locally and remotely and then only copy the files that have changed. Also, the events you want to listen to is `moved_to` and `close_write`. Not `create` and `modify`. 
Thanks, I will look into that
Is there any reason this must be performed on an in-memory array? Because the easiest way to do what you are trying to do is just write your key-value pairs into a CSV file, with keys and values separated by tabs, then use the `sort` command: printf '\s\t\i\n' "$data1" "$size1" &gt;&gt;array printf '\s\t\i\n' "$data1" "$size2" &gt;&gt;array sort -o array array You can use `grep` to lookup key-values.
 # bash 4.4 and a sort(1) with -z mapfile -d '' sorted &lt; &lt;(printf '%s\0' "${!arr[@]}" | sort -z) # bash 4.0 and a sort(1) with -z sorted=() while IFS= read -rd '' key; do sorted+=( "$key" ) done &lt; &lt;(printf '%s\0' "${!arr[@]}" | sort -z) And then using it for key in "${sorted[@]}"; do printf '%s matches with %2d\n' "$key" "${arr[$key]}" done Bash could really use a builtin command for sorting arrays.
`"${array[@]}"` expands the values of the array, while `"${!array[@]}"` expands the indexes (if it's an indexed array), or keys (if it's an associative array).
[removed]
It's funny because KnowsBash's username is very applicable to their comment.
Lock files are probably what you're looking for. I recommend looking up flock as it's probably the most robust and safe option out there. 
[BashFAQ 45](http://mywiki.wooledge.org/BashFAQ/045) has examples on how to do it by using mkdir or flock for locking. It also explains why using "flag files", as you put it, for locking is wrong.
 shopt -s nullglob from='/volume1/7. DOWNLOADS/1. TV TO SORT' to='/volume1/4. TV' while IFS=\| read -r pattern dir do files=( "$from/$pattern"* ) (( ${#files[@]} )) &amp;&amp; { echo mkdir -p "$to/$dir/" echo mv "$from/$pattern"* "$to/$dir/" } done &lt;&lt;'.' bbc|BBC Documentaries ch4|Channel 4 stng|Star Trek . You can remove the `echo` to run the commands. Sample output: user@host $ bash sort-tv mkdir -p /volume1/4. TV/BBC Documentaries/ mv ./bbc.filename.mp4 /volume1/4. TV/BBC Documentaries/ mkdir -p /volume1/4. TV/Channel 4/ mv ./ch4.filename.mkv /volume1/4. TV/Channel 4/ You can remove the `mkdir` line if the destination directories already exist.
Yep, this. Or these, as it were. The other option that comes to mind is syslogging auth information and checking on your syslog host(s).
 man bash
Is it a queue? If `foo 0` is running, and I then start `foo 1` and `foo 2`, I'd assume either of the last two might continue with the flock first.
No no, that'd be useful. Not at all what I'm after.
Ah, I get you.
Related: https://www.vanheusden.com/bsod/
if you use a tiling window manager you could make it spawn a new terminal window, which will have the advantage of automatically taking up the largest, most central piece of screen, and moving all your other stuff around. I use xmonad and I have to say that it is incomparably good at hiding all my shit.
Use xcowsay which allows you to customize the picture so you can use an image of clippy. I would also create a custom fortune file of random clippy style advice. xcowsay allows you to set the location of the image on the screen, but I am not sure how to set it for a specific terminal window.
It's because `date +%D` includes `/` characters, which separates pathnames. Use `date +%Y-%m-%d` instead.
`date -I` has the same effect and is slightly shorter (but GNU specific).
(just in case it’s not clear: I’m joking)
It's for my own personal amusement.
Thanks!
Could you possibly set up a mailbox rule to forward emails (say, from a certain sender) from the organizational accounts to the individual accounts?
That was my first thought, but I realized that in reality they would reply to messages from their personal mail accounts and we'd lose the thread, so to speak. As different volunteers take on positions from year to year we really need to retain the corporate memory, so I want them to actually work from the organizational mailboxes so we can look back on the emails from the past. Some like me are very comfortable setting up their multiple email accounts on their smartphones or whatever, but I can't count on everyone to be able to do that. And for some of these volunteers their role is only active for a short period of time in the year, so they may go months without checking their organizational email accounts. All that to say I'm really hoping to be able to send a "You've got mail" to their daily email accounts when something comes in so they will go check their org mail account. It's been suggested to me that a bash or perl script might do the trick, but I have no experience in PERL and *very* limited experience in bash, so I came looking for advice.
What kind of mail server is it?
`bash` might not be the right solution to this, or rather it will be but one component of your solution. As best I can tell, you're dealing with `dovecot` under the hood, so fingers crossed you'll have `procmail` available. And that's something for you to google on: `.procmailrc`
You don't have the ability to set up even one additional box on the network? Ansible is agentless, it just needs python installed on the remote system. Shit, if it were me I'd bring a personal laptop with Ansible installed on it if they were going to be such sticklers. Doing it in a script really isn't all that daunting though. If you know the list of hosts to ssh into and you know the list of packages that should be on the system (therefore also knowing the packages that shouldn't be on the system), it's doable. Take the list of packages that should be installed (`yum list installed` on the example system), feed it into `yum install` on the new system. Now take the `yum list installed`output of the new machine, concatenate that list with pristine list, and `uniq -u` to get the ones that should be removed. To set up the debugging repo, you don't even need to modify the files, you can just overwrite them with a pre-written configuration: cat &lt; EOF &gt; /etc/yum.repos.d/reponamehere.repo [base-debuginfo] name=CentOS-6 - Debuginfo baseurl=http://debuginfo.centos.org/6/$basearch/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-Debug-6 enabled=1 EOF And then do your yum clean &amp;&amp; yum install.
&gt;Process are all the activities on the os everything witch runs on a Os is a process. Two Words: Kernel Mode - not everything that executes is a process, such as anything that execs in kernel mode: drivers, ipc, filesystems, etc.. Processes are the distinct work contexts scheduled by the kernel under typical multitasking OSes. Also known as user mode programs. 
Hmm. On my system (CentOS 7.3, kernel version 3.10.0-514.2.2.el7.x86_64), flock(2) says: &gt; flock() does not lock files over NFS. Use fcntl(2) instead: that does work over NFS, given a sufficiently recent version of Linux and a server which supports locking. In my experience, NFS is flaky enough even without using cutting-edge features. I absolutely love the idea of flock working over NFS. But I'd prefer to let the hypothetical other guy test this for a couple of years first. I've been bitten by NFS-related bugs in production systems so many times I just don't trust it. I would assume the command-line utility is a thin wrapper around the system call, so if the system calls supports it, the command-line tool does too.
xeyes offers zero advice, but does meet the bulbous eyeballs requirement
[removed]
I'd recommend either some magic with tmux, or write a little curses interface. 
Create a database/array of your non-helpful hints, then combine cowsay with them. Next, crest a cron job / launch daemon that runs the script and opens a new terminal window at random (may need to use another language) intervals. 
&gt; `sed 's/,/,"/4;s/(,[^,]*,[^,]*)$/"\1/' file` Well you're close - the problem is that it's `\( \)` not `()` for sed (basic regular expressions) - however you can enable extended regular expressions with `-r` / `-E` in which `()` is the valid way to do capture groups sed -E 's/,/,"/4;s/(,[^,]*,[^,]*)$/"\1/' The command is still slightly off though - you're one too late on the first substitution and a few too early on the second. sed -E 's/,/,"/3;s/(,[^,]*,[^,]*,[^,]*,[^,]*)$/"\1/'
Not really. Regexr uses a very different regex dialect.
not sure about other distros, but i know in Ubuntu, the saved info is in /etc/NetworkManager/system-connections/
Thanks! For that matter, I'd just copy over the CentOS-Debuginfo.repo file over to the other blades' /etc/yum.repos.d/ directory and call it a day, no? I do enjoy seeing HERE scripts though and this is quite handy. They may end up going this route. &gt; Take the list of packages that should be installed (yum list installed on the example system), feed it into yum install on the new system. Now take the yum list installedoutput of the new machine, concatenate that list with pristine list, and uniq -u to get the ones that should be removed. So, apparently, there's enough of a difference between the pristine machine and the rest of the blades that the yum install just errors out saying no package available. This is why I went with the RPM manifest approach: rpm -qa --qf="%{NAME}.%{ARCH}\n" | sort &gt; $HOSTNAME.`date +%Y-%m-%d`.RPM_Manifest.txt This strips out the stuff that **yum install $(cat results.from.yumlistinstalled)** had issues with. Since I'm under the gun getting this out, I **really** don't have the luxury of Ansible and its learning curve, even though admittedly it doesn't look too tough. I just have bigger fish to fry at the moment. Thanks again for your reply. I really appreciate it.
So, this might not directly answer your question but since you mentioned your starting out I have some general comments as I don't believe this copypasta you found is a good example to be followed. First, with have this infinite loop while var1 != 'end'. If you want an infinite loop while [ true ]; is sufficient and communicates a little better that's the intent. I don't see var1 ever being assigned a value, let alone any path that would assign it a value of 'end' to trigger ending the loop. In bash if you're trying to read a variable that hasn't been obviously set that generally means you're expecting it to be an environment variable like $HOME, $PATH etc. The pattern of russian doll nesting of if else tests very quickly devloves in to unmaintainable and fragile code. The general bashim for testing if a tool succeeded or failed was to examine the $? variable immediately after execution. 0 means success, non zero means failure. It's worth noting though that even if you get 100% packet loss with ping -w 1 that return code would be 0 (success) because the tools job ran correctly and gave you the report that there was packet loss. However the way this is set to pipe through grep, that will return an exit code as 1 as grep is designed to report exit code 1 if no matched are made. The comments say this script is designed to test internet connectivity. I argue it does no actual thing, it tests if you can get ICMP packets to and from two arbitrarily picked IP addresses. Two IP addresses does not the internet make. I don't care if 8.8.8.8 is google's famous DNS server, when it does go down you don't care if you're still able to get to your real destination (in this case apt-get servers). If your goal is to do a single update jamming it in this infinite loop doesn't sound like what you want either. At the end of the day you have a specific problem you want to solve: automatic updates. If your purpose of wanting to roll your own as a learning excercise that's fine. However there are already more baked public solutions and utilities to help you if you really just care about that task completing. Ubuntu has well documented info on their built in way to just "turn this on" for you https://help.ubuntu.com/lts/serverguide/automatic-updates.html
1\. OSX (or rather MacOS), uses `\n` as line breaks like the rest of the *nix world. 2\. sed 's/,/,"/3; s/\(,[^,]*\)\{106\}$/"&amp;/'
As a simple test for connectivity, I changed the above script to this: https://paste.ubuntu.com/24096650/ Since it returns the results of the ping to me, as well as a connected message, it does that part of it all with relative speed and ease. It also just seems a bit more clean than my previous by a long shot. That being said, the question remains if this should execute by itself and I let the unattended update handle the rest, or if (given my shitty internet connection) I try to pass the result of this script on to have apt-get run if it's successful.
If the source of the common problem is your main exit point I would take a look at how the outputs of ifconfig and route look when you're in a problem state. That might be more zeroed in monitoring. Using ping for example doesn't tell you the problem is w/ your isp, it tells you there was some problem between you and all the other hops along the way. In this sense that could be a false positive if you're trying to test about being able to just get your foot out the door to begin with.
[removed]
`while true`, not `while [ true ]`
See [http://mywiki.wooledge.org/Quotes](http://mywiki.wooledge.org/Quotes#Prevent_field_splitting_and_ignore_glob_pattern_characters)
TLDR: Wrap all parameter expansions in double quotes: maim --select --format=png "$HOME/screenshots/${name}.png" 
Truthfully it's because I was just hacking around with the script in the OP and it seemed like the path of least resistance as that structure was already kind of there (although I did rename "pingtime" because I didn't like it as a name). That being said, my bash-fu is still pretty weak and so I would probably do it the same way again because it makes a little more intuitive sense to me (quite similar in a way to testing eg a fortran allocate, which I'm much more familiar with) and it'd help if I wanted to maintain/read it in future.
Great point, I was kind of dissatisfied with the robustness/lack of defensive programming in that part, but didn't have the time right then to work out and implement a solution. Will definitely think about lockfiles for things starting automatically behind the scenes in future.
`[ 1==1 ]` is nonsense. It will just confuse beginners. Use `true` or `:` when you need a command that is always true. Further, instead of testing if the dir exist and then run mkdir, just use mkdir -p which does both in one. mkdir -p .update_logs || exit And to test if a command succeeded or not, don't test $? after running it, just test the command directly. if ping ... &gt;/dev/null 2&gt;&amp;1; then break fi Lastly, avoid using `` `...` `` for command substitution. The newer `$(...)` syntax is saner and should always be preferred.
Well one would assume you could extract it from the github api https://developer.github.com/v3/repos/ However.. https://api.github.com/repos/TYPO3/TYPO3.CMS/releases is empty meaning we can't use `/repos/:owner/:repo/releases/latest` Perhaps it has something to do with it being a read-only mirror of their own repo - not entirely sure. https://api.github.com/repos/TYPO3/TYPO3.CMS/tags could be another option but doesn't contain only the versioned releases. Perhaps it is one of the responses somewhere - failing that you could just do it the "wrong" way and extract it from the HTML on the release page https://github.com/TYPO3/TYPO3.CMS/releases e.g. user@host $ curl -s https://github.com/TYPO3/TYPO3.CMS/releases | grep -P -m 1 -o '&lt;span class="tag-name"&gt;\K[^&lt;]+' 8.6.1 If your `grep` doesn't support `-P` you could use `sed` or another approach sed -n '/ *&lt;span class="tag-name"&gt;/{ s///; s/&lt;.*//; p; q; }'
&gt; check again - repeat endlessly I'm going to suggest a cron job over this &gt;and adds it to a list with a time stamp And maybe a sqlite db to make it easier edit: added stuff 
I think he is miss using the work "duplicate." He wants a script that will tell him if he got the same IP address he had in the last 60 min when reconnecting to the VPN.
thank you for your response! I was just working on solution on the download page https://typo3.org/download/. Maybe I can grep the part behind the "Latest" part. Thank you again!
Correct. 
You'll need to enable extended globbing for that I think. I expect what you have will be a problem if you don't do that, and then your parens have different meanings than what you want. That would be the problem with the syntax error. Try: shopt -s extglob ls *:[2-8]*([0-9]).t which will match any start followed by `:` then a digit from 2 to 8, then `*([0-9])` will match 0 or more digits in 0 to 9, then a `.t`
A couple of nitpicks: `ls` doesn't support anything here, the shell handles all the globs and `ls` gets the result, never knowing that globbing came into play Also, for the shell given that this is posted to r/bash I think we can make a guess about the shell in question. bash has supported extended globs for about 20 years, so it's a safe bet that they'd be available here too (see my comment for how to use them for this problem). Your grep wouldn't actually match the described requirements, though it could be made to do so of course. Depending on what you want to do with the files that match, `ls | grep` would be unsafe, but you could use much the same approach with `find -regex` if your `find` supports it. It's not POSIX, but GNU and FreeBSD versions support it
Thank you so much!!!!!!!! :) :)
Thank you!!! :)
I would recommend you ask your instructor for clarification.
So, I tried my hand at this using some pipe commands, and I think that I've come up with a solution to list the command itself after running the command. ls plane[0-9][0-9][0-9].flight &amp;&amp; history | tail -n 1 | cut -c 8- This command should perform the regex ls, then run history to list all recent commands, the output from history will be piped to tail and cut down to only the most recent command, this will be piped to cut and have the line number removed.
Assuming those are the exact file names you're looking for (and not just examples of file names), I'd probably go with: `ls plane7{0,6,3,4}7.flight` And then (assuming he prof wants you to echo that command so it prints to stdout for showing what you wrote): `echo 'ls plane7{0,6,3,4}7.flight'` I would probably just ask for clarification, but if i was going to interpret the assignment without asking, that is the most literal execution of it, I think. 
I see what you mean. I have a big issue with people who try to crowdsource their homework. I wish the sample text within the submission box said "Did you try reading the fucking manual?" but I don't want to create a hostile environment. Learning Bash isn't the same as it was for me back in the 90's and 00's when scripting guru's were a small collective "bad-asses" who refused to answer basic questions that could be answered by, you guessed it, reading the manual. For the past year, /r/bash has been, or has strongly resembled, a hodgepodge of "*Check out my script on GitHub*" and "*how do i create an array?*" type questions. Basically, what I'm saying is, noobs are more than welcome here; and I feel that this should be a sub where we exchange ideas and help each-other with Bash, but there is an entry-level price noobs gotta pay: and that's what I lovingly refer to as RTFM. So please. Pretty fucking please, with a cherry on top, RTFM.
Honestly, I haven't seen anything for the past few months on r/bash that wasn't clearly explained in the bash manual at gnu.org.
I think you got it right, but the easiest thing to do is ask for clarification. IRL you would ask a client to clarify ambiguous requirements. But then again if u/tjdwns is anything like I was in school, the assignment is probably due 8AM tomorrow.
That works but then I get this: TERM environment variable not set. 04/03/2017 04:35:54 TERM environment variable not set. 04/03/2017 04:35:55 adding `export TERM=${TERM:-dumb}` got rid of the extra text Thanks for your help, marking as solved.
very nice! That is exactly what I needed! Thank you
I was paraphrasing him. :)
This an excellent start for to learn from. THANK you, Nascentes. Would you be open to me asking you line-by-line question about what things are doing? I would really like to learn this. 
Wow, even more for me to sink my newb mind into. Thank you so much. I'll ask you the same thing I asked Nascentes above. Would you be open to me asking you line-by-line question about what things are doing? I would really like to learn this. Thanks. 
Sure, go for it.
&gt; &gt; if grep -q ${pub_ip} ${ip_tracker} &gt; I understand what grep does, and I'm assuming this checks the file for the existence of the current I ip, but what I don't know is what the IF was testing for. Is it just if the grep finds a match that the result is true? &amp;nbsp; You're right about the if statement there. "If this command returns positively do this, otherwise..." In this particular instance, we are checking to see if it returns true. eg. &amp;nbsp; t@UD4H:/mnt/c/Users/t$ echo "cat" &gt; test.txt t@UD4H:/mnt/c/Users/t$ grep -q cat test.txt t@UD4H:/mnt/c/Users/t$ echo $? # exit status of previous command. zero = good/success/allow; non-zero = bad/failed/deny (in this context any way) 0 t@UD4H:/mnt/c/Users/t$ grep -q dog test.txt t@UD4H:/mnt/c/Users/t$ echo $? 1 t@UD4H:/mnt/c/Users/t$ &amp;nbsp; &gt; &gt; $(awk -F " " '{print $2}' $ip_tracker) \nNew: ${pub_ip}". &gt; I also don't know what awk does. I suppose I need to learn awk too ;) &amp;nbsp; Don't feel too bad, I barely understand it myself :) To be honest, the above tidbit is pretty much the only thing I do with awk outside of some more complicated parsing for multiple fields/columns. My advice is to read up and get a base understanding of what awk *can* be used for. Then when you feel like it might be the tool for the job, Google something along the lines of "thing_to_do awk one liner". Maybe silly and maybe something you've already got wayy up your sleeve, but I do this all the time. Also great for sed, regex, loops, etc... etc.. &amp;nbsp; &gt; Also I am getting this above the expected output and I can't figure out where it's coming from: &gt; &gt; % Total % Received % Xferd Average Speed Time Time Time Current &gt; &gt; Dload Upload Total Spent Left Speed &gt; &gt; 100 15 0 15 0 0 22 0 --:--:-- --:--:-- --:--:-- 22 &amp;nbsp; Oops! That one's on me. I just plugged that curl statement in after doing the testing bit by just creating a static IP assignment in the script. Just add a '-s' switch to curl in the 'pub_ip' assignment at the top. That will suppress the output. If it doesn't work on your distribution of curl, you should be able to just redirect stderr to /dev/null. eg. &amp;nbsp; pub_ip=$(curl ipecho.net/plain 2&gt;/dev/null) &gt; How do I get carriage returns to not get ignored in the reddit editor? I don't know what the *correct* way to do it, but supposedly you can just double up on newlines, though that doesn't work for me all the time. You can use the `&amp;nbsp;` thing if you want. I added some in this comment so you can see what I mean (look at the source). 
That's zsh only, no?
&gt; vared PATH Yep, running BASH here: $ vared PATH -bash: vared: command not found
figured it out myself ;P ln -s /usr/local/opt/readline/lib/libreadline.dylib /usr/local/opt/readline/lib/libreadline.6.dylib &amp;&amp; ln -s /usr/local/opt/readline/lib/libhistory.dylib /usr/local/opt/readline/lib/libhistory.6.dylib will create the necessary links
If you think bash is not powerful enough, please consider investing your time in contributing to Next Generation Shell, which uses domain-specific language.
What's your use case? I'm curios because I had relatively few situations that are close to needing something like this but I handled it with forks.
You could grep it, save it to a temporary file and source it?
Would awk not work here? Using the print function? 
Why use awk if grep is enough?
Edited!
Hey, me again! When I enter your command, nothing shows up in my terminal (on OSX btw). Is it because of the -P? I'm not very good with regular expressions I'm afraid. also, is it possible to assign a variable in some bash script with the version number? e.g. 7.6.16?
Well if your grep didn't have `-P` you should get an error? You can replace it with `perl` or something else e.g. perl -nle 'print for m{https://get.typo3.org/\K[^"/]+(?=")}g'
Try this: while read ip; do grep -l ${ip} *.iplists &gt; ${ip}.txt done &lt; dictionary.txt This assumes that all the IP addresses in dictionary.txt are unique (it should work for non-unique ones as well but it'll be less efficient). In case of non-unique IPs in dictionary.txt, replace the last line with `done &lt; &lt;(sort -u dictionary.txt)`
I am now using $(curl -s https://typo3.org/download/ | grep -o 'https://get.typo3.org/7.[0-9]\{1\}.[10-99]\{2\}' | head -1) to assign it to a variable in a script. It works for now but I don't know if it's reliable in the future....
everything will be done by the user of `async-bash` in the reject function
You are amazing. Thank you so much!
[removed]
I would not need a for loop?
Sort of unrelated but does anyone know why bash (and every other shell I'm aware of) doesn't search the $PATH directories recursively? This would allow one to organize their binaries into folders
I think gpg-agent is the answer here. 
while :; do break; done
I have read about SHC while ago, tried it, and realized that a code of "compiled" script ca be seen even with: ps xaf (so no memory dump needed).
:O It would help if I re-read the website I linked to too.
'm sorry, what do you mean when you say i need to escape the trailing ] ? 
Putting a backslash in front of it, like what was done with the opening square bracket. It will tell the shell that the next character needs to be taken literally, not processed. Like so: \\].
He/she means you should put a \ before the closing square brackets ] in your fg variable to "escape" the bracket each time. It is escaping the opening square bracket, but not the closing. It should look similar to your nofg varaible which has the closing square bracket escaped. 
cheers, makes sense
As an aside, that's an interesting approach - loading your colours into an array and selecting them the way you are. Did you think about using that to randomise your colours? testArray=(a b c d e f) randElement=$((RANDOM%${#testArray[@]})) printf "%s\n" "${testArray[randElement]}" c
I can't count on all my users being able to receive an SMS. These are volunteers, and current or future volunteers may not have cellphones or may have, gasp, flip-phones. ;) 
I see the logic.. Just use SHC to encrypt the script which contains the user/password. However, the script calling the user/password is a python one and I don't think it is possible for SHC to convert that to binary. 
Granted, it may be more secure over transit if I use SSH keys but then won't that just sit on the server also, vulnerable to use if the server is compromised in the same way a plaintext password could be. 
May be good to know what all of these people share in order to be able to answer your question. Else its pretty impossible to help you.
&gt; if [[ $FILESIZE &lt; 512500000 ]] ;then That does string comparison, not numeric comparison. You want shell arithmetic instead of a string operation: if ((FILESIZE &lt; 512500000)); then 
Well the issue is that you don't define the variable `f`
I do have f defined somedirectory="/some/directory" for f in $somedirectory do code above done just pipe is not working 
it works fine to files unseder 512 MB .. but anything above it all falls under 50m ... and I can't figure out why tried both ways elif (( FILESIZE &gt; 512500000 || FILESIZE &lt; 1025000000 )) ;then and elif (( FILESIZE &gt; 512500000)) || ((FILESIZE &lt; 1025000000 )) ;then still no go
The logic is also wrong. `(( FILESIZE &gt; 512500000 || FILESIZE &lt; 1025000000 ))` will be true for filesize greater than 1G, so the last elif will never be reached. Change it to something like: size=$(wc -c &lt; "$f") if (( size &lt; 2**29 )); then ... elif (( size &lt; 2**30 )); then ... elif (( size &lt; 2**31 )); then ... fi
still 50m for everything ... thanks for the help tho.. I give up tomorrow is a new day 
&gt; Pretty fucking please, with a cherry on top, RTFM. As a newcomer to bash, learning something every day, what is "the manual" to you? If `man`, I admit to finding it extraordinarily frustrating sometimes to find what I really need. As an example, take the man page for *tar*, which tells me: -C, --directory DIR change to directory DIR but what it really means is dump the archived files in that directory, a thing I only realised today. I feel a good deal of frustration at the people who are documenting many of these commands, in the same way I view road signs that are unhelpful to someone without an overall knowledge of an area's geography. I really wish they'd spend more time putting themselves in the shoes of a first-time user. That said, I agree that too many people run to ask questions rather than get their hands inky on the documentation! I'd really love a good, well-written guide myself. &amp;nbsp; *Edit:* a letter; Im learning to tough-type.
Were it not for Google I'd have never even started!
It's just emails. They're all volunteers with the charity. The charity spends the year planning for a big one day event. Volunteers who have a formal position, about 15 of us, have emails. And they are a mix of people, from seniors without a smartphone to younger people very comfortable with social media. Because some may not check their email frequently, particularly at times when their particular role is not busy, I need a way to have the server send an email notification to their personal day-to-day email to let them know they should go to their organizational email account and check their mail. I don't want to simply forward the emails because then they'd reply from their day to day email and we wouldn't have the correspondence captured within our organizational email system. 
Hmm... `exec` seems prevent the execution path from returning and printing the "On exit" echo once for every time the file changed. It doesn't even print it once at the end, which I find surprising. What is `exec` doing? Is it killing the stack below it? ~~The script should spend most of it's time in `foo` right before the md5sum check, so I would think any race condition that exists would bias toward printing once at the end over not printing ever.~~ Edit: Nevermind. The behavior I'm seeing makes sense. 
Ok, thank you! I just figured out why it doesn't print the "On exit" ever with this version. I didn't step though the logic slowly enough. Probability dictates that when the file gets updated, it will be inside the 'foo' function. Therefore it fails the md5sum check and calls exec a final time. The new change to the file `while ( false )` ensures that the last call to `exec` never gets into that loop at all.
yeah, the thing is when you source you keep adding nested layers of code to the running script, with exec you turn it into a loop that can run forever (but of course you need some modifications). another crazy idea would be to setup a watcher on the filesystem with some tool and have it reload based on signals instead of a loop (that might be quite intensive on the CPU).
&gt; Is there a utility that generates passwords with these requirements in mind? Well, there's `pwgen`. For example: $ pwgen -sy1 10 10 - `-s`: (secure) generate completely random, hard-to-memorize passwords - `-y`: use at least one symbol - `1`: print the passwords one per line - `10`: generate passwords 10 characters long - `10`: generate 10 passwords in total But as far as I know, you can't specify exactly which special symbols `pwgen` should pick from, or exclude particular ones. You could, of course, run `pwgen` in an `until` loop until it generates only acceptable passwords, though.
I understand your concern. Man pages often don't have a lot of examples. Gnu tar is one of the exceptions to the rule, though. There is a specific call out for the -C switch. What version of tar do you have installed? But your point is taken. A limited question like that is not "do my homework" worthy. Although, this isn't the *best* place for it, it is likely to get answered. Also, that is not a bash specific issue. tar is not a built-in command but a separate utility.
This helped me. Thanks! 
My point is that while we may claim that there's good documentation, all too often it's not always clear. I've been messing with computers for maybe thirty years, consider myself fairly bright, and yet wading through the `man` pages is often more frustrating than clarifying. By the way, I chose my example carefully; tar actually has better-than-average documentation, though the language is occasionally dense for many people.y, I feel that we're moving away from the main point though, which was to discuss the *doing of homework*. As someone who is learning to code in Python (and touch-type), I often find myself in the position of needing help. Thankfully, I'm mostly capable of worrying through the problem and finding a solution, and if I do ask for help, it's usually for hints rather than solutions. Perhaps this is because above all, I like to know *how* things work rather than just have them solved for me. Anyway, I realise I'm starting to ramble. But I don't disagrree with you. Thanks for responding.
yeah if you omit the `source` it creates a new instance and it gets nested like a function call
I've found this to work pretty universally without needing extra packages. I keep it in a text expansion shortcut (I generate a lot of passwords in my job...) it could be simplified if you are not on a Mac and don't want it to be copied to your clipboard: pw="$(LC_CTYPE=C &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c${1:-16})" echo -n "$pw" | pbcopy | pbpaste
Add this to your `.bashrc` and update it to your heart's content. http://pastebin.com/zcGFxHh9 It's not my usual style, I was forced to write it that way because of `bash` 2.04 on Solaris 8... 2.04 is really bad. So it's long overdue for some updates now that I no longer have to cater for such a limited version of `bash`... so feel free to share any improvements. ... Or at the very least you can get some ideas from it. Like the more portable `| fold -w 10 | head -n 1` style rather than the less portable `| head -c |` (My `.bashrc` is my 'digital backpack' so to speak. It's easier for me to have this function than go through a client's change control process and having to fight bureaucratic paperwork for 2-3 weeks just to get `pwgen` or `apg` installed. I can just deploy my `.bashrc` across a fleet of servers in a few seconds and go on with my life...) Demo, first let's generate 10 passwords of 10 chars requiring a Digit, Uppercase, Lowercase and Special Character (`-Y` was a nod to `pwgen`'s `-y`): $ genpasswd -n 10 -DULY 9s1AmKeM~j QhCWUB9BN? ezf7Bmm(uP 52O$oyoExC PEo4_ZDApk UK~i9P4BfH 7EmJVn7&lt;IO 2rE3J@fRzS uI89~Ud7y7 NXuw3CJz&gt;G Same again, but let's SHA512 them as well (this calls a separate function which essentially tries `python crypt.crypt` and fails over to `perl`, I need to update it to handle passlib/hashlib): $ genpasswd -n 10 -DULY -k6 Original: ?2WFHnzt7I Crypted: $6$7kbQXIIE$8tkGX5Uzk1q/8pjacxTmqAUCmILnUQNERkwZM8ft2IqRn3F5Hcr67uuZil0qkmeQRSvs2mE1Rh/8yephWod2u. Original: #LNoc00Zt0 Crypted: $6$cYImU3ny$.qLseG8jYJy8mrPH0OsCtoJh68drQp/IvqBSJOMMH.haCxDppnbE8nTFYvKb9JD9QAT8adhR379vxwF2WBlqr/ Original: UXsDN!7AwJ Crypted: $6$fY6y4CSW$e3GL8718.1H85Hk2Eesol7toZW3zU89ndMLRK55xSpe9ksf3KIGO50oERMO6uJBBa/HA.kq7MwwERfQ653p6m1 Original: 8Ho@FsOseG Crypted: $6$i4YgC9nj$0fS9pIrwWQidn8vWMYwwktDnS/1mwer13c/sVHGQQqK7a63msGkf9bQk9v4TQFTvV2q03G407UQ4o98NmD9lg1 Original: KEG7&gt;NGWSU Crypted: $6$hP9Mf2CC$YzC/O0skGvVXC/eJHLx7/spmomis5udsnYIBGoUTb4.EdI.CHDaD.2y0GHCZksjfF1RGbhBkujV5nofijayTG0 Original: 1FTsmJFL_N Crypted: $6$Dfltruzu$nDGmFR93cu74/D9ru60qEfNPSJ5xYVKfMDuXBzhq9ZCtxGnHLigiHbhBIh8GD236UfNkLFuUAvGYvudEpaHhf0 Original: nC#xw2TmJu Crypted: $6$k20Iimb3$1wohDyNL4BPEvm3wfiLPDtWc.mzZz2D9y6tjzJ6MfXf5D/1jO2mwakfNfb4WjGLktikngjtJWXd1G5jIxfGmA. Original: n7AMe^jkCU Crypted: $6$ptjQGhxp$/0R6J6UMp9riLrY2tfJh4LRO6.U0Ce6FKC1iXfwAFbMfG2HUbvGPfOQt9BgwPd5WbFEFJvgeH/p2v97ASlldF0 Original: Bl2Guge_Zc Crypted: $6$GldqukuM$zPgoyAeekbUINQTPC9lw5pBAWFxniJIun1egxkodwfCS7q8k.CvqNcuPihmdBjJIInFfZ1QZh.WQ/W19eoHs91 Original: K(ZYl39ZjY Crypted: $6$UKRaYcdE$LsI2kgZADGoRco4RFes4TKeJErMQhGrjry5zaF0e8hjGbO/ePqNLqd5R6l5XYLvX0R0FBFMXXv4vVbSlO/1vG1
&gt; My first question is this Hmmm... when you say it doesn't seem to always work properly - what exactly happens? user@host $ startn=1 endn=4 user@host $ touch file_{1..5}.out user@host $ ls file_[$startn-$endn].out file_1.out file_2.out file_3.out file_4.out I'm not really sure I understand the 2nd part - you want to make sure it begins with 0 but you said it already begins with 0?
 awk '{ getline $2 &lt; "file2.txt"; print }' file1.txt
http://mywiki.wooledge.org/BashFAQ/045
Use flock, it is awesome. I personally started it last year for all such cron Jobs and it is really easy and powerful. 
&gt; I'm trying to build a list of files/folders Build out of what? Where are they originating from? &gt; creating a sym link in their place A symlink to where? Can you describe exactly what you want to do, maybe with an minimal example?
When assigning (using either plain `=` or the additive `+=`), use the variable name without the `$`. Use `$` only where you want the variable name to be replaced by the contents of the variable. Also, using `echo` in a `$(`command substitution`)` is pointless. You should just use the value directly. So instead of $test_name+=$(echo "_log") $test_name=$test_log_loc${test_name}$(echo ".${DATE}.log") do test_name+="_log" test_name="$test_log_loc$test_name.$DATE.log" or (since this all occurs in a shell assignment context where quoting is harmless but unnecessary), even test_name+=_log test_name=$test_log_loc$test_name.$DATE.log Finally, make sure your script is actually executed by bash and not by /bin/sh which does not support additive assignment using `+=`. 
Thanks for the tip! See the edit in the OP
Cool. Glad to help, but if this is for generating personal passwords for online accounts (etc.), I'd strongly recommend you consider using [pass](https://www.passwordstore.org/). Not only does it generate passwords for you... $ pass generate myaccount 12 (create a new pass file `myaccount` and generate a secure password of length 12 for it, or generate a new such password if `myaccount` already exists) ...but it also encrypts everything with GPG, prints password file info to your screen, copies passwords to your clipboard, has file completion, and other goodies, like `git` integration for version control.
Thanks, that worked perfectly.
Neat little program. I've already written a php script which reads lines from a file based on a long master password I've memorized. I just need to enter a line number and the master password and it returns the password on that line.
Sure. I'll do my best.
you could use id_rsa.pub key to do that without password
That part makes sense. It's the whole section of coding where we use the service account to connect to the boxes via ssh, run the add user script, logout, then go to the next Linux box that I'm having trouble with. 
If the script is named `foo-wrapper`, `pgrep foo` will include its pid in its output, so that script will effectively kill itself every time you run it.
Wow, I feel really dumb. Thanks for explaining things to me. Love the name btw!
Technically, you're right (the best kind), but I was using `foo` and `foo-wrapper` as sort of metavariables over script names. Most people wouldn't *actually* call their wrapper script that way, e.g. they'd have a script `backup-movies` that wraps `rsync` or whatever, in which case `pgrep`-ing `rsync` should be fine. Still, you're right that if the wrapper script name does happen to include the other name, a simple `pgrep` will lead to trouble. Same goes for if any other process name contains it, e.g. if there's another instance of `rsync` (or whatever) running, or an instance of `bazrsyncfoo`.
Consider using a tool like Puppet to do this for you.
Good to see you over here in /r/bash (everyone else: original post [here](https://www.reddit.com/r/linuxquestions/comments/5ya8vb/bash_code_failing_to_execute/)). Is `bash` a 100% strict requirement? Because `expect` is a better match for some, if not all of this.
These passwords are intended for use on websites. Often times they require alphanumeric + special chars (some rejected). The goal is to ultimately have a page nested somewhere on one of my websites that takes a line number (1-1000) and a master password. I'll associate each line number with an account in a separate location.
The guy who wants this wants to be able to read it and should I leave he wants someone to be able to come into my spot and be able to run this no problem without having to learn a new language. So it's really a .sh script but could be a .bat if it were to run off a windows machine to connect to the Linux boxes. 
`expect` is fairly easy to read and understand if, like any other language, it is commented appropriately. How is the service account setup on the remote hosts? Your first task is deploying the ssh keys for the service account, it might make sense to do that during the setup of that account on the remote hosts, as it will save you a step. Otherwise, you could run `ssh-copy-id` using a heredoc. In my experience it's not as reliable as an `expect` script though.
Your code fixed up for reddit formatting: #! /bin/bash echo -n "Enter username: " read -r username adduser "$username" echo -n "Enter password: " echo "${username}:${password}" | passwd -e touch /etc/sudoers.d/sugroup echo "$username ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers.d/sugroup
you should use a proper password manager like password store. just saying.
Thanks. I typed this out on my phone so on the site it didn't convert right. 
You are using `shift 2` after setting `ip` and after setting `max`. That will get rid of positions `$1` and `$2`. Only call `shift`. You are also calling shift after the case statement, which isn't needed.
Not quite. I was hinting towards the deployment of the ssh keys when I said "for some". Most of that process can be done with `ssh-copy-id` or a `cat | ssh cat` but you still need to handle the password prompts. As I said in [this post here](https://www.reddit.com/r/bash/comments/5yhjwt/help_bash_scripting_nightmare/deqdisy/), it's my experience that `expect` is more reliable at this than heredoc'ing. Other options for this foundational step include `sshpass` and `pssh -A` Once the ssh keys are deployed then yeah, everything can be done directly via `ssh`, but there are still tasks that `expect` is just better at - portably changing a password, for example.
Not sure what it does without a :. But if you add a : in there it will define the first positional parameter passed to the script or function if it's undefined. See http://stackoverflow.com/questions/2013547/assigning-default-values-to-shell-variables-with-a-single-command-in-bash
Well you could use something like http://fixer.io/ which gives you back JSON - you could then use a tool like `jq` for parsing out the information. user@host $ curl 'http://api.fixer.io/latest?base=USD&amp;symbols=GBP' {"base":"USD","date":"2017-03-09","rates":{"GBP":0.82128}} user@host $ curl 'http://api.fixer.io/latest?base=USD&amp;symbols=GBP' | jq .rates.GBP 0.82128 There's also doing it the "wrong" way user@host $ curl -s 'http://www.xe.com/currencyconverter/convert/?Amount=100&amp;From=USD&amp;To=GBP' | grep -P -o "class='uccResultAmount'&gt;\K[^&lt;]+" 82.1889 
&gt; max=$2 shift; You've removed the semicolon after `max=$2` here Also as you've shifted it will be in `$1` 
Don't use grep to filter find output. Just use find. find . -mtime +600 -type f ! -name "*.tgz" ! -name "*.tar.gz"
I am planning to setup the ssh keys for the service account before deploying this so you are right it will save me a step. But what I have to do in the script is call those SSH keys on the remote servers before it runs the loop of the add user script. 
I still need a regex because it's not always at the end of the file... I have some encrypted files that are like : filename.gz.enc for example. And I have even more odd ones like : filename.TGZ20160123 because at some point someone decided to add a timestamp at the end of the files, for some reason :D But thanks for the find parameters, I am terrible at those things...
find . -mtime +100 -type f ! -iname "*.tgz*" ! -iname "*.gz*" So I can double up the iname command provided I want two "extension" to be handled, right ? No better way to write it ?
How will it break sudo if the user is being put into sudoers.d ?
If the ssh keys are installed properly into the service account's `~/.ssh/authorized_keys`, then you shouldn't have to "call" them at all. It should theoretically be as simple as `ssh serviceaccount@host`. Maybe you'll have to specify the key with `-i` After that is where it gets interesting. Here's a sanitised version of an email I sent around to my team way back before things like Ansible existed. Mixed RHEL and Solaris 8/9/10 environment: &gt; Hey guys, &gt; &gt; [junior colleague] just asked me if I had a way to run a command across multiple Solaris servers, I figured this might be worth sharing, especially in the case of Solaris boxes where remote Satellite commands aren’t an option in our environment. &gt; &gt; You need: &gt; &gt; * An ssh config file to bounce ssh connections to the DMZ's through the Satellite proxies. The ssh config file can be grabbed from /path/to/config, place it in your ~/.ssh directory (recommended) &gt; &gt; * ssh keys, current passwords sync'd and sudo to all the hosts (required) &gt; &gt; * a list of hosts you want to run your command against. One host per line. [redacted bit about a script I wrote to auto-generate such a list from our nagios system] (required) &gt; &gt; * If you’re using sudo, you will need a file with your password in plaintext. Ensure you have ownership of this file and that it is chmod’d appropriately (i.e. 400). Make sure you delete it when you’re done. (recommended) &gt; &gt; To run a non-sudo command against all servers: &gt; &gt; for host in $(&lt;/path/to/serverlist); do echo "${host}" &amp;&amp; ssh "${host}" /some/path/to/command; done &gt; &gt; To run a command against a specific subset of servers: &gt; &gt; for host in server1 server2 server3 server..n; do &gt; &gt; To run a `sudo` command is a bit trickier, thanks to Solaris being a pain in the ass. So we split it out to Linux and Solaris tasks: &gt; &gt; Sudo commands against all Linux servers: &gt; &gt; for host in $([commands that identify linux hosts from the serverlist]); do echo "${host}" &amp;&amp; ssh -t "${host}" "echo $(&lt;pwstore) | sudo -S /some/path/to/command"; done &gt; &gt; &gt; [Redacted sentence. Essentially we're able to easily split Linux and Solaris hosts from a single serverlist as they have different naming schemes]. We echo out the hostname so that we can see in the output where we are, then we ssh with an allocated ptty, and echo our password into sudo. Sudo is expecting the password on stdin (-S). The password must be followed by a newline, so DON’T do something like “echo -n $(&lt;pwstore)” &gt; &gt; Sudo commands against all Solaris servers is a slight change of the above command: &gt; &gt; for host in $([commands that identify Solaris hosts from the serverlist]); do echo "${host}" &amp;&amp; ssh -t "${host}" "echo $(&lt;pwstore) | /usr/local/bin/sudo -S /some/path/to/command"; done &gt; &gt; I know it’s Friday and your attention span is minimal, so let’s have a break and look at this pigeon having a shower: &gt; &gt; http://i.imgur.com/DPMxPqI.jpg &gt; &gt; Right, so here’s an example. &gt; &gt; Yesterday I had the workflow task for removing [guy-who-was-a-dickhead]. I created a list of hosts that his account is on quite simply like so: &gt; &gt; cd /path/to/our/centralised/config/files &gt; grep "^dickhead:" *passwd | cut -d'_' -f1 | tail -n +2 | tr -d "./" | sort | uniq &gt; ~/dickheadlist &gt; &gt; And then removed him like so: &gt; &gt; for host in $(grep [linux-identifier] ~/dickheadlist); do echo "${host}" &amp;&amp; ssh -t "${host}" "echo $(&lt;~/pwstore) | sudo -S /usr/sbin/userdel -r dickhead"; done &gt; &gt; for host in $(grep -v [linux-identifier] ~/dickheadlist); do echo "${host}" &amp;&amp; ssh -t "${host}" "echo $(&lt;~/pwstore) | /usr/local/bin/sudo -S /usr/sbin/userdel -r dickhead"; done &gt; &gt; One day I’ll probably build some functions for this. Probably. &gt; &gt; Finally, when you’re sudoing like this, take extra care with your commands! I did wind up building functions for these kinds of administration activities. Grepping for a user in our collected config files was an easy win. Then I moved on to sorting out the pathing for `sudo` on the Solaris hosts so we could treat them the same as the RHEL hosts. Ultimately instead of going through all of the above, we were able to run `fleetuserdel user`, which means that obviously there were commands like `fleetuseradd user` and `fleetuserpasswd user` All of that is gone now, replaced by Ansible :)
Would you please try [Google](https://www.google.com/search?q=how+to+run+a+bash+program+from+within+a+bash+program)? Using the terms of your question, I was able to find multiple sources that led me to your answer, literally within seconds. I know you're new to Bash, but a lot of us appreciate it when users at least do some searching before asking basic support questions. Thanks! (Edit: Seriously? Fuck me for trying to encourage someone to find the answer for themselves)
Could I do something like: touch /etc/sudoers.d/sugroup child 0440 /etc/sudoers.d/sugroup To correct it?
A use case might be something like a CLI tool that looks for a configuration file in the user's home directory. When they get the config file path so they can parse it, they _could_ do something like: DEFAULT="$HOME/.someconfig" CONFIGFILE_TO_LOAD=${1-$DEFAULT} Which would allow the user to type `cmd /some/other.config` to load the tool using a different configuration file. _Note that this is a highly contrived example use case, and in practice the acceptance of various options should be taken as CLI arguments._
you're appending it to a file called 1, not a variable. Try instead: curl --data "$(cat awk.txt)" $SERVER 
Instead of parsing the data with sed in order to generate bash code to be evaled by bash in order to generate a list of numbers, why not just use bash to parse and generate the list of numbers? a=10-20,50,100-200,500,600 array=() while IFS=- read -rd, from to; do to=${to:-from} # if to is empty, set it to from for (( i = from; i &lt;= to; i++ )); do array+=( "$i" ) done done &lt;&lt;&lt; "$a," # read is delimiting on ',' instead of newline printf '%s\n' "${array[*]}" -- $ fmt &lt;&lt;&lt; "${array[*]}" 10 11 12 13 14 15 16 17 18 19 20 50 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 500 600
`-d @filename` will tell curl to send the content of `filename`. You may also need to set a Content-Type header so the http server knows what type of content to expect. https://ec.haxx.se/http-post.html
How are you running the script? It works for me if I run it from a terminal, eg: $ cat &gt; newexec #!/bin/bash touch $1 chmod +x $1 vim $1 $ chmod +x newexec $ ./newexec # a vim appears Running vim from a subshell isn't a problem: $ (vim) # works as you'd expect
Yup, sorry I had a typo. Blah. Thanks!
Nice solution.
Trial and error. It's always trial and error.
Nesting loops isn't that bad, but your current approach is wrong. It only works if the last octet changes. If you try to generate the range from 192.168.0.0 to 192.168.1.0 for instance, you'll only get the two IPs generated, missing the 255 IPs in between. A better solution would be to convert the IPs to their integer form, iterate between those integers using a C-style for-loop, then convert them back from integer form to get it back to the typical representation. This can all be done with bash alone.
You didn't check if configure succeeded, so maybe that failed? Check its output.
you can use unstable, but then you have the moral obligation to report bugs you encounter to the package developers. 'unstable' means 'not quite ready, may break while we are fixing stuff'.
...which means, on a productive machine it is a no-go. leaving that OpenSSH part then.
Substring expansion works on arrays too array=( {a..z} ) n=${#array[@]} for (( i = 0; i &lt; n; i += 10 )); do printf '%s ' "${array[@]:i:10}" printf '\n' done -- Otuput: a b c d e f g h i j k l m n o p q r s t u v w x y z https://tiswww.case.edu/php/chet/bash/bashref.html#Shell-Parameter-Expansion 
Okay, to be honest, I read your comment yesterday just after you posted, but I couldn't accept it. I wen through quite a few phases: * **denial**:This can't be true. So I thought about it and even tested it, but of course you were right. * **acceptance**: No problem, it's a bashscript just for me! I now know it, so I can work around it, and the impact is not too bad! I'll just always set 255 as the last octett! * **anger**: How could I not think of that? I'm stupid! So, now that I run into this problem, I tried to understand what you posted as a solution. I don't fully understand all of the parts, and maybe you can explain a bit about it. I'd do this: * `cut` the IPs where the dot's are minA=$(echo $IP | cut -d. -f1) minB=$(echo $IP | cut -d. -f2) minC=$(echo $IP | cut -d. -f3) minD=$(echo $IP | cut -d. -f4) maxA=$(echo $IP | cut -d. -f1) maxB=$(echo $IP | cut -d. -f2) maxC=$(echo $IP | cut -d. -f3) maxD=$(echo $IP | cut -d. -f4) * Integers: Why do I have to declare the variable as an integer explicitely? * C-Style for loop: I don't understand that part, I thought I used a c-style for loop in my code already...or is C-style the for (( i=$A ; i&lt;255 ; i++ )); do do something done * convert them back? now I'm very confused, I thought I can work with the integer now... or .. I don't know. ~~Why can't I do this:~~ ~~arrayA=[{minA..maxA} (192 193 194 195...243 244 245.. 254 255)~~ ~~arrayB=[{minB..maxB}~~ ~~... ~~for~~ I wish I was this sophisticated in bash, but sadly I can't figure out how to build what you suggest. Thank you for your help so far!
Thank you for your post, I did that and did a better formatting of the code! &gt; Nested loops are no harm; tho the general rule is that if you have to use eval you're doing something wrong. You can replace the use of {start..stop} with $(seq ${start} ${stop}) for counting without using {X..Y}. I found out this becomes a problem anyway, another person who replied found that the iteration does not work if the last octett of the IP remains 0, or anything below 255, which is not what I desire. Thank you! 
Thanks, will give it a try. Pretty new at bash so it's been a learning curve. Just need to do more research into multi threaded vs parallel processing now.
You might be interested to learn about `xargs` or GNU `parallel`. https://www.gnu.org/software/parallel/man.html http://www.thegeekstuff.com/2013/12/xargs-examples
[removed]
The utilities in FreeBSD have a different heritage and features than the GNU ones typically found with Linux. GNU utilities typically have many extensions. [POSIX](https://en.wikipedia.org/wiki/POSIX) [expr](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/expr.html) doesn't have a `length` operator, but `sh` has a [string length expansion](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02): $ s=word $ echo ${#s} 4
Which would be fine except this is part of a larger script I've been writing. 
then ask /r/CentOS what tools you can use
What would you suggest?
That doesn't work to remove them from the file. 
Thanks that's really helpful! Will this work in CentOS?
Yeah, `userdel` deletes an account from the system which is obviously not what you want to do. I don't know your system, but assuming that `/etc/sudoers.d/sugroup` is a text file with one name per line, your problem can be defined as deleting a line of text containing a specific string (i.e. your username) from that text file. 
Also, are you able to share your other script for critique and feedback? Chuck it into pastebin.com and post a link here...
bash-4.3 introduced a `-n` option to `wait`, which causes it to wait for the next child to exit. This can be used to keep 10 processes running, and start a new one as soon as one of those 10 ends. i=0 for file in "${files[@]}"; do if (( i++ &gt;= 10 )); then wait -n fi printf 'Starting %2d: %s\n' "$i" "$file" sort_app "$file" &amp; done 
It is done by rsync itself, not by xargs. See http://serverfault.com/questions/460423/why-does-rsync-forks-itself-and-why-one-such-forked-process-is-almost-kinda-idl
Looks like you've found a poor guide to learn bash from. You can enable the dotglob shell option to have globs include files that start with `.`. cd || exit shopt -s dotglob for file in ./*rc ./*profile tmux.conf; do [[ -e $file ]] || continue cp "$file" /dest/ done There is also nullglob which will cause unmatched globs to expand to nothing instead of the glob itself shopt -s dotglob nullglob cp ./*rc ./*profile tmux.conf /dest/
It's very easy to mess up globbing, especially for dotfiles. David Wheeler gives a comprehensive treatment of handling filenames in shell, including globbing, in [Filenames and Pathnames in Shell: How to do it Correctly](https://www.dwheeler.com/essays/filenames-in-shell.html). I'm not saying it's exemplary code and I haven't looked much at how other people do it, but here's how I install my dotfiles: [install.sh](https://github.com/torbiak/dotfiles/blob/master/install.sh). I make symlinks for *nix but I have a different install method for cygwin (`cp-to-home` and `cp-to-repo`) since non-cygwin programs like gvim don't follow cygwin symlinks. It's annoying but seems better than forking `.vimrc`.
I agree with most of this, but: - &gt; If Steam had used ShellCheck in 2015, [this line would never have made it to production](https://linux.slashdot.org/story/15/01/16/1429201/steam-for-linux-bug-wipes-out-all-of-a-users-files): &gt; rm -rf "$STEAMROOT/"* &gt; This code violates the [SC2115 rule](https://github.com/koalaman/shellcheck/wiki/SC2115) from ShellCheck. You’ll notice that the rule’s wiki page quotes exactly the `$STEAMROOT` example, even in its first version. I haven’t checked the ShellCheck repository’s history, but it seems that this rule was added *after* Steam’s accident… so it couldn’t have prevented it. - &gt; Use Bash unofficial strict mode No. *Please* no. The “Bash unofficial strict mode” needs to die. We’ve [discussed](https://www.reddit.com/r/programming/comments/4daos8/good_practices_for_writing_shell_scripts/d1pgv4p/) this [before](https://www.reddit.com/r/bash/comments/5ddvd2/til_you_can_turn_tracing_x_on_and_off_dynamically/da3xjkk/). - &gt; Log what your script is doing This directly runs counter to one rule of the Unix philosophy, called the “rule of clarity” in [*The Art of Unix Programming*](http://www.catb.org/~esr/writings/taoup/html/ch01s06.html) and “Don’t be chatty” in *Classic Shell Scripting*. That’s not to say this rule is set in stone, but I expect you to be aware of it, and counter it with something better than “running a script is not supposed to feel like magic”. The article seems to imply a dichotomy between silent shell scripts and shell scripts that log every step, which I believe is a false dichotomy: a well written shell script should detect when something went wrong (you *are* doing error handling, right?) and *then* log output, but there’s no reason to clutter the console with “executing this and that” constantly. I’d also personally advocate for using `logger --journald`, which allows you to attach more information to an entry, but that depends of course on how portable you want your script to be :) - &gt; Document your scripts I agree with the premise, but that’s a really terrible way to implement `usage` in my opinion. Parsing the script itself for appropriate comment lines is cute, but doesn’t work if the script wasn’t invoked with a valid path (e. g., because it’s in the `$PATH` and was invoked by basename). And `expr "$*" : ".*--help" &gt; /dev/null` has got to be the ugliest way to check for an option I’ve ever seen. It will have false positives (`sendmail --subject "TOTD: most commands support --help for documentation"`), and it uses `expr` instead of shell mechanisms (and yet, just below: “Use Bashisms”). - &gt; Stay informed Doesn’t mention /r/bash! Heresy! ;) But just so this comment isn’t all negative, I’ll point out some highlights to agree with too: - Yes, please version control your scripts, for all the usual version control reasons. - Yes, use ShellCheck, it’s worth it. - Yes, use Bashishms all the way (`[[`, `((`, arrays, advanced parameter expansions, etc.). - Yes, quote your variables. *Always* quote your variables. (ShellCheck will haunt you if you don’t!) - Yes, do use a template – just not this one, for the reasons belabored above ;)
Shameless: or stop suffering and help build saner alternative https://github.com/ilyash/ngs
bats?
Yep! I'm using stow with my dotfiles and love it. The script Im writing is for setting up a new system or when I'm hopping inside of a vm and need to do some work. 
I use `grep -q` in conditionals a lot. Example: test if a drive is mounted: dest="/path/to/mountpoint" drive_test() { if ! grep -qs "$dest" /proc/mounts; then echo "Error: Drive not mounted!" exit 1 fi }
You forgot command help Also, regarding: &gt; man bash # search for "\^SHELL BUILTIN" to find information about builtin commands. I used to do this too, but that's just a more complicated way to do help command 
https://github.com/sstephenson/bats http://blog.spike.cx/post/60548255435/testing-bash-scripts-with-bats https://blog.engineyard.com/2014/bats-test-command-line-tools 
You already have the correct code to reset the color in there: `$(tput sgr0)`. Also, it’s best to use `tput` for the other effects as well: `tput setaf 1` to **set** the **A**SCII **f**oreground color to color #1 (red), `tput bold` to switch to bold mode, etc.
If you want the command to be coloured while the output of said command stays default colour, you may be looking for this http://stackoverflow.com/q/4585397 . I haven't tested this personally, just googled a bit around. Also it looks quite ugly. 
The shell doesn't need an external utility for this. Even the original Bourne shell can do this by itself just fine (and it's roughly 100-1000 times as fast): case $STRING in *qwertyuiop* ) echo "String found" ;; esac or (bash-ism) if [[ $STRING == *qwertyuiop* ]]; then echo "String found" fi (note that, confusingly, `==` within `[[` matches glob patterns, it is not an "equals" operator) 
The major barrier to man pages seems to be learning how to navigate them. But then again, I'm not familiar with modern desktop linux systems; perhaps the default pager for man is more new-user friendly. And barring that, there's online man pages, and most people can use their web browser pretty well. :D
they're still pretty dense. it doesn't matter how well formatted they are, when you look at the plethora of options in `curl`, for example, you have to read for a long time to get all that info. If you know what you're looking for, you can search, but if you're just trying to learn what it can do, it can be quite daunting. like, a friend of mine today just discovered the `-1` flag to `ls`. That manpage isn't even that long, but it's got a significant list of options. Lots of people don't know about that flag and I've seen workarounds like `ls | cat` or the less elegant `ls | awk '{ print $1 }'`.
 ./login.sh "${user}" "${ip}" "${password}" set user [lindex $argv 0] set ip [lindex $argv 1] set password [lindex $argv 2] You really shouldn't pass the password around like this though, as it will show up in `ps` listings in plaintext. To add a very tiny extra level of security, write the password to a file, `chown` it, `chmod 400` it, and read it into your expect script. As an example: # Check for the .pwdfiler file if { [file exists .pwdfiler] == 1 } { # Reads admin password from file '.pwdfiler' which is set to chmod 400 # Slightly more secure this way set filer [open "/home/$whoami/bin/.pwdfiler" r] set adminpass [read $filer] } else { send_user -- "I could not find the required file .pwdfiler\n" exit 1 }
&gt; I've still had odd, quirky portability problems with the xpg binaries. Are you sure you weren't just trying to use GNU-isms? Did you run into anything not conformant with the [POSIX spec](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/contents.html)? 
As another poster pointed out, apparently it isn't exactly the same text. He gave as an example `help shopt` not showing all the flags. But for example I always use `help test` and its output is substantially the same as what's in the bash manpage. 
Hi whetu, thanks very much! I was able to get it to accept the inputs for the ssh command no problem :) Thanks for the tips on the password as well - I'm not *so* concerned with it at the moment, as I want to get the script working first, but I'll definitely keep it in mind once everything is working as expected! I am having a bit of an issue now though, using the same formatting as before, in getting this to work with scp. With ssh, I was able to login to a remote machine using this script with no problems at all. But when I change it to scp I am getting this error: send: spawn id exp6 not open while executing "send "$password\r"" And I can't really work out why. When I put the scp command directly into bash, it follows the principles of the script - but the script is complaining about the password.
Or just use built in glob matching if [[ "$str" = "startswiththis"* ]] ...
There's a difference between finding information to get something done _right now_ and finding information to build your skills. The one thing I love about man pages is that, even if I am looking for something _right now_, just in the periphery I am likely to pick up additional tips for later use. It's generally recommended to not use `ls` for getting file names. Two alternatives are to use globbing and `find`. For example: # all files/dirs for file in /some/path/*; do echo "$file" done # just directories for dir in /some/path/*/; do echo "$dir" done # Use find: find /some/path -maxdepth 1 -print0 | while read -rd $'\0' file; do echo "$file" done The reason for `-print0` (find's args) and `-d $'\0'` (read's args) is to split entries on a null byte; this allows easier processing of "bad" filenames (spaces, tabs, newlines, non-printing chars, etc).
 &gt; spawn ssh $user@$ip &gt; /dev/null &gt; expect "password" &gt; send "$password\r" &gt; interact That's not how I'd write `expect` code myself. But I guess the obvious question needs to be asked: why aren't you using ssh keys? You can use `expect` to automate their deployment with `ssh-copy-id` and then not need `expect` again except for certain tasks/situations. Here's the thing about good `expect` scripting: you have to think and test for every possible scenario and then put in appropriate blocks to cater for each. It's good habit to always give a default action as a catch-all, and don't use `interact` unless you know why you need it. For that matter, you should also think about setting a default timeout value, which you literally do after the shebang: #!/usr/bin/expect -f set timeout 10 Here's an example of a default action from an old expect script of mine, that is still used to this day in fact for changing passwords across a mixed server fleet. It has run so reliably that we haven't exactly been in a rush to migrate this task to our `ansible` infrastructure (though I've written a playbook that we're now slowly moving to). # Open the connection and invoke sudo passwd spawn ssh -q -o StrictHostKeyChecking=no -t $server $sudo passwd $user expect { # Set the default action to catch any unexpected responses default { send_user "\nERROR: I was unable to connect to $server for some reason. Please try manually.\n" exit 1 } # Pre-check for a server that we might not have an sshkey on yet -re "$whoami@$server's \[pP\]assword:" { send_log -- "\nWARN: $whoami doesn't appear to have an sshkey setup for $server. You might want to resolve that.\n" send -- "$adminpass\r" exp_continue } So you can see that straight off the bat I'm starting to cater for potential edge cases. So that's a few things to keep in mind. Can you try running your expect script with `-d`? If you're still calling your `expect` script from your shell script (and by the way, you should rename its extension from .sh to .exp, and not even use .sh at all for your shell scripts), then instead of this: ./login.sh "${user}" "${ip}" "${password}" Do this: expect -d login.exp "${user}" "${ip}" "${password}" Otherwise if you're calling it directly from the cli, essentially do the same. Copy and paste (and optionally sanitise) the output back here.
you overwrote the file by using the same filename as input and output. You need to use different files. Try that first, although tail may complain about that many lines. But try it. Other things to consider ... is this an active log file, i.e., is it constantly being written? How will you stop the writing while you are tidying it? Another option is that there are log rotate processes, where it will rename a few prior log generations to generation+1; rename the current log to generation0; and create a new current log. You can then just delete the oldest by generation number, e.g., rm mylog.old.5 . 
I realized shortly after the tail command, that I had overwritten it, lol. I hadn't thought about rotating, as I wasn't really intending to keep many... But that sounds like it would be easiest. Thanks!
Thanks for the link, that looks like it'll do for a quick &amp; easy fix!
you want all rows, which are not unique in their first column?
Correct. In any batch of job applicant records in a file there may be a few that match in column 1, their applicant tracking number. Those are the ones I want to pull out. I need to know why we have two applications with the same number. The rest of the columns contain information that isn't unique.
&gt; awk -F, 'seen[$1]++ == 1 { print p } { p = $0 }' file.csv Excellent! Thanks! Got the rock, now I need a roll. One of the pair prints, how do I capture them both.
Don't use grep on find or ls output. Let find itself do the filtering. find "$1" -type f -name "*[[:space:]ÆæØøÅå]*" -print See http://mywiki.wooledge.org/UsingFind
Thanks! : )
I have no doubt theres better ways to do it but awk 'BEGIN{e=0};{if ($0 ~ /BEGIN RSA PRIVATE KEY/) e=1}; {if ($0 ~ /BEGIN RSA PRIVATE KEY/) next};{if ($0 ~ /END RSA PRIVATE KEY/) e=0}; { if (e==1) print}' id_rsa Looks like it does the job
Use flags: http://stackoverflow.com/a/17988834 $ awk '/BEGIN/{flag=1;next}/END/{flag=0}flag' file
So you suggest not to try building it? Because it might not get traction?
Man how many of these do people write, it's like the first bash project for a majority of people (even I wrote one). Simply grab from the many freely available ad server lists and add to the hosts file. There must be thousands of versions on github.com. 
If you just want to throw away the errors entirely, do `exec 2&gt;/dev/null`. Alternatively, if you might want to look at them later but don't want to worry about deleting them, consider something like tmpfile=$(mktemp -t myscript.XXX) echo "Errors will go to $tmpfile" exec 2&gt;"$tmpfile"
Ah! Thank you! This is exactly what I wanted 😄
There are around 25 repositories listed on GitHub with the search term "hosts ad block". Cheers!!!
hBlock is more feature rich, pmiab is easier to use. Cheers!!!
Mine's private :) But sorry, didn't mean to downplay your work. Good job on the repo, it's always good to see new takes on the same concept! 
tl;dr: Don't.
Thanks for the ideas! It'll really help me not only with this script but the future bash scripts I might write! Cheers!!!
Hey, Come-on mate! No sorry please! And it will be cool if you make your script public :) Cheers!!! 
If I understand your problem, you are missing 'wait' after the *for* cycle. But maybe, instead you want this ? parallel --jobs 10 SORT_APP '{}' ::: $HOME/my_scripts/directoryfilesarestoredin/*"file-im-looking-for"*
A couple of other options: root@proxmox:~# cat /etc/ssl/certs/thawte_Primary_Root_CA.pem -----BEGIN CERTIFICATE----- MIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9C/ON9srbTANBgkqhkiG9w0BAQUFADCB qTELMAkGA1UEBhMCVVMxFTATBgNVBAoTDHRoYXd0ZSwgSW5jLjEoMCYGA1UECxMf ... /qxAeeWsEG89jxt5dovEN7MhGITlNgDrYyCZuen+MwS7QcjBAvlEYyCegc5C09Y/ LHbTY5xZ3Y+m4Q6gLkH3LpVHz7z9M/P2C2F+fpErgUfCJzDupxBdN49cOSvkBPB7 jVaMaA== -----END CERTIFICATE----- root@proxmox:~# grep -v '\-\-\-\-\-' /etc/ssl/certs/thawte_Primary_Root_CA.pem MIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9C/ON9srbTANBgkqhkiG9w0BAQUFADCB qTELMAkGA1UEBhMCVVMxFTATBgNVBAoTDHRoYXd0ZSwgSW5jLjEoMCYGA1UECxMf ... /qxAeeWsEG89jxt5dovEN7MhGITlNgDrYyCZuen+MwS7QcjBAvlEYyCegc5C09Y/ LHbTY5xZ3Y+m4Q6gLkH3LpVHz7z9M/P2C2F+fpErgUfCJzDupxBdN49cOSvkBPB7 jVaMaA== I also have these lines of `sed` that you can use as an example: LinuxFiles=$(sed -n -e '/^\[LINUX\]$/,/^\[\/LINUX\]$/{ /^\[LINUX\]$/d; /^\[\/LINUX\]$/d; p; }' filelist) SolarisFiles=$(sed -n -e '/^\[SOLARIS\]$/,/^\[\/SOLARIS\]$/{ /^\[SOLARIS\]$/d; /^\[\/SOLARIS\]$/d; p; }' filelist) The file 'filelist' is formatted kind of like this: [LINUX] /etc/somefile.conf ... [/LINUX] [SOLARIS] /etc/someother.conf ... [/SOLARIS] The `sed` commands shown extract everything between those blocks. Using the above as an example, feel free to have a go at adjusting to suit your needs. 
Hey there, thank you so very much for the long explanation. I must say that a few bits went over my head. Hope you do not mind a few followup questions: &gt; barry$y[$z] (the part before =) is NOT a name Yes, I get this, it looks at it and says this is a command to be executed, substitution comes in and then barry1[0]=1 is executed. I think I understand that this looks like an assignment of variable to my untrained eye, but because it was not one to start with (due to the variables in the name), it is considered something to be run/executed. This fails because it is not an executable piece of script. I guess my confusion comes from barry1[0]=1 is a valid assignment in and of itself, but the way it is arrived to basically has to determine what to do with it, before the full string is formed, thus initially does not understand it's an assignment (because of my bad syntax), goes on to say, well let's try to run this thing, and never looks back after substitution. &amp;nbsp; I think my issues stem from not really understanding how bash processes commands and string logic, when substitution comes in and what happens first. I think you have helped me understand a bit of it today, sincerely many thanks for that! &amp;nbsp; I'll be a total user and ask for more now, so sorry in advance. I did make the change, and now the inner loops works fine, it properly assigns variables to the array. The second problem, the outer loop not looping, is still present. I played with the inner loop's "done" statement, but if I remove it, it breaks the inner loop (as I expected). I don't see anything wrong with the outer loop. I'm beginning to think that there is a problem with incrementing variable "y" in the outer loop and it is not being caught by the inner loop. So that the loops are properly executed, but since variable "y" never changes in the inner loop, the procedure is always done on the same original array instead of on subsequent arrays. This is the only reason I can think of why barry doesn't have values but in barry1[0-9]. Is there anything special that needs to be done to use the variable "y" with updated value inside the inner loop? Thanks! 
Would be nice to be able to delete items by index, and manage by categories.
Cool, will check it out soon. I've been distracted by life. I really appreciate you taking another look! :) 
I don't have time at this very moment in life to look through this but I am super appreciative of the time you took to do this and will definitely get back to it. Thanks! :)
This is working program: #!/bin/bash # Preparation for i in {a..k}; do array1+=( " $i: $i" ) array2+=( " $i: $i$i" ) done typeset -p array1 array2 the_number=2 # Your routine y=1 while [ $y -le $the_number ]; do z=0 while [ $z -le 9 ]; do echo "y=$y z=$z" temp="array$y[$z]" temp2=$(echo ${!temp} | sed 's/.*: //' | sed -e 's/^[ \t]*//') declare "barry$y[$z]=$temp2" z=$(($z+1)) done y=$(($y+1)) done typeset -p barry1 barry2 I had to * add the_number definition * move *z=0* into the first loop * add *character ;* into the while ... *;* do Apart from that it is fine. There is something you are not showing us. My suggestion is to add *set -x* at the top and observe line after line whether your script does what you expect. Output declare -a array1='([0]=" a: a" [1]=" b: b" [2]=" c: c" [3]=" d: d" [4]=" e: e" [5]=" f: f" [6]=" g: g" [7]=" h: h" [8]=" i: i" [9]=" j: j" [10]=" k: k")' declare -a array2='([0]=" a: aa" [1]=" b: bb" [2]=" c: cc" [3]=" d: dd" [4]=" e: ee" [5]=" f: ff" [6]=" g: gg" [7]=" h: hh" [8]=" i: ii" [9]=" j: jj" [10]=" k: kk")' y=1 z=0 y=1 z=1 y=1 z=2 y=1 z=3 y=1 z=4 y=1 z=5 y=1 z=6 y=1 z=7 y=1 z=8 y=1 z=9 y=2 z=0 y=2 z=1 y=2 z=2 y=2 z=3 y=2 z=4 y=2 z=5 y=2 z=6 y=2 z=7 y=2 z=8 y=2 z=9 declare -a barry1='([0]="a" [1]="b" [2]="c" [3]="d" [4]="e" [5]="f" [6]="g" [7]="h" [8]="i" [9]="j")' declare -a barry2='([0]="aa" [1]="bb" [2]="cc" [3]="dd" [4]="ee" [5]="ff" [6]="gg" [7]="hh" [8]="ii" [9]="jj")' 
I mean it's pretty nifty but https://xkcd.com/1205/
&gt; move z=0 into the first loop That was the key. It clicked when I read it at first even before trying. I looked at my loop and i never was resetting the "z" value back to 0, hence the inner loop was never being run again since z=10 at the end of the first go around. When declaring z=0 in the first one, then it gets reset each time the outer loop runs. &amp;nbsp; Thank you so very much man! Let me know what I can do for you to repay for all this help!
&gt; Thank you so very much man! Let me know what I can do for you to repay for all this help! You are welcome. Help others wen you can, be good :)
I do that on the daily at work, and outside of it :D
 I will look into it some more, I Tried the wait, And it ended up running 1 sort_arp function, waiting more, running the next, instead of doing the 10 at once. And still sent me an email after 10 ran so idk yet. 
Well you've defined `m` as a "string" when it looks like you want it as an array. manufacturers=( Acura Alfa\ Romeo Aston\ Martin ) Notice that you'll need to escape the space in the name to have it treated as a single "item". You could also use quotes e.g. `manufacturers=( Acura 'Alfa Romeo' 'Aston Martin' )` user@host $ for m in "${manufacturers[@]}"; do for year in {1996..2030}; do mkdir -p -v "$year/$m"; done; done mkdir: created directory ‘1996’ mkdir: created directory ‘1996/Acura’ mkdir: created directory ‘1997’ ... mkdir: created directory ‘1996/Alfa Romeo’ mkdir: created directory ‘1997/Alfa Romeo’ mkdir: created directory ‘1998/Alfa Romeo’ ... You can however remove the inner for loop and just do for m in "${manufacturers[@]}"; do mkdir -p -v {1996..2030}/"$m"; done And on that note you can actually do it without a for loop entirely mkdir -p -v {1996..2030}/{Acura,Alfa\ Romeo,Aston\ Martin}
Uppercase variable names could clash with existing environment variables.
I’m not /u/DoesntSmellRight, but here goes: &gt; http://redsymbol.net/articles/unofficial-bash-strict-mode/ The “unofficial bash strict mode” needs to die. I don’t want to repeat what’s been [written](https://www.reddit.com/r/programming/comments/4daos8/good_practices_for_writing_shell_scripts/d1pgv4p/) [before](https://www.reddit.com/r/bash/comments/5ddvd2/til_you_can_turn_tracing_x_on_and_off_dynamically/da3xjkk/), but - `set -e` is difficult to use, explicit error handling is preferable. - Removing space from IFS (not part of the quote, but you included it as well) only hides quoting errors from your script a little longer (instead of breaking on `'this input'`, it will now break on `$'this\ninput'`). - `set -u` is fine, but shouldn’t be necessary if you use shellcheck, which can also detect a lot of other errors. &gt; I've read style guides - they often require constants to be all uppercase. I should hope not, or they’re bad style guides :) they might require all *external* variables to be uppercase, which is conventional: `SUDO_EDITOR`, `LESSOPEN`, `SYSTEMD_PAGER` etc. – but for internal variables (like your `WORKING_PATH`, `GITIGNORE`), this isn’t needed, and using uppercase variables only risks an unintended collision with external variables of *other* programs (Bash alone has almost a hundred of them, many of which permanently lose their special meaning if they’re ever assigned). &gt; https://dev.to/thiht/shell-scripts-matter Yes, the `--help` implementation [was](https://www.reddit.com/r/programming/comments/4daos8/good_practices_for_writing_shell_scripts/d1pgv4p/) my least favorite part of that article :) One, you shouldn’t use `expr` for that (Bash can do substring testing on its own), but two, just concatenating all arguments and looking for `--help` or `-h` anywhere in them is a *terrible* idea anyways (what if your name is *greg-koah__-h__artman*?).
Yeah, that style guide distinguishes between “variable names” and “constants and environment variable names”… not sure what to make of it yet.
Thanks I'll try this tonight
You might instead look into the `cookiecutter` package available through `pip`, to avoid recreating the wheel.
[removed]
blackflag:tmp user$ cat testing123.txt G11S T306A I2682M P99A blackflag:tmp user$ sed -i.bak 's/\\([0-9]\\{1,3\\}\\)/_/g' testing123.txt blackflag:tmp user$ cat testing123.txt G\_S T\_\_A I\_\_M P\_A (sed -i.bak) the -i.bak says to edit the file in place, backup with .bak extension the next part is an expression - 's/ begins the search pattern, \\( and \\) enclose the items you want to capture, [0-9] says any number 0-9 and \\{2\\} means between 1 and 3 of the preceding items (in this case, one two three 0-9 digits, and /_/g means to globally replace the 12 or whatever with _. EDIT: Lots of edits for formatting... EDIT2: blackflag:tmp user$ sed -i.bak 's/\\([0-9]\\)\\{1,3\\}/_/' testing123.txt &lt;--- this will replace one to three instances of 0-9 with only one _ blackflag:tmp user$ cat testing123.txt G_S T_A I_2M P_A EDIT3: Sorry, slow tonight, long day and it's late and I've been drinking.... blackflag:tmp user$ sed -i.bak 's/\\([A-Z]\\)\\([0-9]*\\)\\([A-Z]\\)/\\1_\\3/' abc.txt blackflag:tmp user$ cat abc.txt G_S T_A I_M P_A Basically this says to capture the first letter (first set of parenthesis), the \\1 tells sed to print the first capture group, the second capture group we don't print out (and the * says to capture zero or more 0-9 digits) and just print a \_, and then print the 3rd capture group.
Just saw your reply, check my edit
Also.. there's a difference in formatting between desktop and mobile I just noticed... should only be one instance of backslashes...
The following should take care of both commas and spaces, as well as any consecutive occurrences of them (`-s`): $ echo '1,2, 3 4 5 6,,7 8 9' | tr -s ', ' '\n' 1 2 3 4 5 6 7 8 9
Put inline code between backticks -- \`code\` -&gt; `code` -- and indent your code block by four spaces: code
What does the output look like?
&gt; &gt; injecting user provided data directly into scripts &gt; In this script, why is that bad? What injection attack can you forsee? If any of the data the user inputs happen to contain characters special to sed, sed will not do what it was intended to do. If the user is really unlucky with his/her project name choice, it could even cause arbitrary commands to be run. Better to treat data as data, not as code. -- &gt; &gt; doesn't handle utf-8 &gt; It might do in the future, but I don't see any immediate need for the MVP. Is that bad practice? It's just that it may look silly when you lowercase all ascii letters while leaving non-ascii letters unchanged. -- /u/galaktos covered the other points, so I won't repeat them here.
Thank you!
You can also use pure bash to do it while read line;do printf "%s\n" "${line//[[:digit:]]*[[:digit:]]/_}" done &lt; loopStr.txt ** the `while` ** loop , loops through the entire file contents. When a line of text is read `${line/[[:digit:]]*[[:digit:]]/_}` substitutes all digits with `_` symbol. You can change the `_` symbol to anything you want. **Note: ** It replaces all digit . for example `G1ABC333F` replaces all numbers **Note: ** `//` means replace all occurrence of the specified format which is `[[:digit:]]`. Using only `/` still works. i.e `${line/[[:digit:]]*[[:digit:]]/_}`
This bit of feedback is useless for the OP's question.
ifconfig | grep -E '(string1|string2)'
You might look into using other commands such as "ip addr". Sometimes it's much more simple to use a different program completely
I know this may be already solved, but let me add another option: If there wasn't such "between letters" rule, you could do: `echo 'G11S T306A I2682M P99A' | tr -s '[:digit:]' '_'` `tr` translates the characters in the first set (any digit) to the second (underscore), then squeezes the multiple consecutive translation occurrences (-s). Again, sorry because it doesn't check if they are between letters, but I think it's​ a simple option to take. However, I liked more the `sed` one as a robust solution.
Thank you, kind sir. 
I've used shunit2 for a couple of years now and find that it compares nicely with JUnit I'm used to from the Java world. Bats has a completely different syntax and not something that clicked with me, I prefer xUnit style `assertX [&lt;msg&gt;] &lt;expected&gt; &lt;actual&gt;`. It's nevertheless cool that multiple test frameworks exist. ## Rant: I try to do TDD as much as possible. I believe it makes code less coupled, it allows me to re-factor the code without worrying that I'll break something, it's an excellent way to document how my code is intended to work and lastly, it gives a nice rhythm: there is a bug -&gt; create test that proves it's a bug -&gt; make changes to your program until the test passes. When going to a Java conference, everyone agrees with this and people try to do this in their jobs - to a higher or lesser extent. However, the moment they create a BASH program, TDD - or unit testing in general - flies out the window. Why should BASH programs be any different? They're never "just 4 lines" and they're critical pieces in every organisation I've worked with/at: without BASH, pension payment reports are not posted, images from the picture agency are not imported, PDFs are not uploaded to the FTP server, 3rd party translations are not merged into the git tree, Jenkins cannot build new releases of the node app and so on. Everyone has important BASH programs in their company, if you took them away, the business would suffer big time, but few have unit tests for them. Odd, eh?
The pattern substitution syntax uses globs, not regular expressions.
**Defaulted to one day.** I will be messaging you on [**2017-03-23 16:06:07 UTC**](http://www.wolframalpha.com/input/?i=2017-03-23 16:06:07 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/bash/comments/60fpzl/nested_loop_with_fixed_list_of_subdirectories/df9kkkp) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/bash/comments/60fpzl/nested_loop_with_fixed_list_of_subdirectories/df9kkkp]%0A%0ARemindMe! Thank you! I'm very ecstatic you were there to answer my questions when I didn't know which questions to ask. ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! df9km84) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
RemindMe! 4 Weeks "Give this guy gold."
I’m sorry, I don’t see how this is very Bash related, or even just shell scripting related… can you explain? I think it’s more suitable for some other subreddits (and you’ve already crossposted it to several, I see).
Okay so first let's look at some of the output of the command - we will pipe it to `cat -vet` to show us "non-printing" characters. $ /sbin/ifconfig | cat -vet lo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384$ ^Iinet6 ::1 prefixlen 128 $ ^Iinet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 $ ^Iinet 127.0.0.1 netmask 0xff000000 $ The `$` here specifies the "end of the line" and the `^I` here is an actual tab character. So in `awk` you use `/.../` to match lines it's like performing a "grep". `/^[^\t]+:/` this means match lines that start with "not a tab 1 or more times followed by `:`" `^` matches the start of the line `[]` is called a character class. `[abc]` would match a single character that is either `a`, `b`, or `c`. If the first character inside `[]` is `^` it will "negate" the class. So `[^\t]` matches any character that is not a tab. `+` means the previous "thing" 1 or more times. `awk` deals with "fields" (or "columns") e.g. $ echo one two three four | awk '{ print $1 }' one $ echo one two three four | awk '{ print $3 }' three So field 1 is accessed via `$1`, field 2 via `$2` etc. $ /sbin/ifconfig | awk '/^[^\t]+:/{ print $1 }' lo0: The first pattern matches the interface name lines which we save in the `iface` variable with `iface = $1` The "details" lines start with a tab as we saw above so we use `/^\tinet/` to match the lines with the address $ /sbin/ifconfig | awk '/^\tinet/{ print $2 }' ::1 127.0.0.1 And the `status` lines have `status:` as the first column so we can just do a straight string comparison to match those lines $1 == "status:" So you want to print out the "summary" of each device - however in my output not every device had a status line. If you only wanted devices that had a status line the command could have been simplified to awk '/^[^\t]+:/{ iface = $1 } /^\tinet/ { addr = $2 } $1 == "status:" { status = $2; print iface, addr, status }' So match the interface line, match the address line, match the status line then print all the values. To get all devices even if they had a status line or not you would print the summary when you match an interface line. Doing it this way though you wouldn't want to print the first time you see an interface name because you wont have read its details. Also, when you've processed all the input you will not have printed the last interface's details. This is why we have the check if (iface) print iface, addr, status; An empty string is false in `awk` so `if (iface)` will be false if `iface` is empty - this is one way to avoiding printing the summary when we first match an interace name line. And the `END {}` block runs after all the input is processed in which we print out the final interface summary.
It gives you a bash environment on windows, which is the primary use case for this project. It does so with the xfce terminal. hopefully it will attract some users to open-source operating systems, or give developers forced to use Windows a nice scripting environment.
I got the impression that the Xfce4 desktop was the main goal… the Bash on Windows component of your work is just Cygwin, isn’t it?
This does not use the Windows subsystem for Linux version of bash. It is all Cygwin, including the bash shell. Sure, Xfce is a major component, but it is expected that most time would be spent using xfce4-terminal to run bash, ssh, git and Xfce is leveraged to do key management and graphical app services.
I agree with "General rule is to avoid shell script encryption and use better solutions for your problem." (from the website linked). While I appreciate the author's effort. It would have been cooler if he would have open-sourced the code. Unless you are using this for personal reasons, no organization today will touch this. Primarily because it will be unsupported.
`(( TEST_PROGRESS++ ))` *fails* (returns a non-zero exit code) if TEST_PROGRESS was 0, so `&amp;&amp; continue` does not run in that very first case. This is because `(( ... ))` succeeds when the contained expression has a non-zero value, but the value of `foo++` is `foo`'s value *before incrementing*. Instead you could use `++foo`, which uses `foo`'s value *after incrementing*. Or you could do `[ "$TEST_0" -eq 0 ] &amp;&amp; { (( TEST_PROGRESS++ )); continue; }` so that you don't depend on the result of the `(( ... ))`.
Thanks! This code works as expected: case "$TEST_PROGRESS" in 0 ) [ "$TEST_0" -eq 0 ] &amp;&amp; (( ++TEST_PROGRESS )) &amp;&amp; continue ;; * ) : ;; # something else esac I had already found a workaround in method 1, but this is close to my liking:) I did not know about ++foo and that there could be a difference between foo++ and ++foo. I was wondering why the first case (when TEST_PROGRESS is 0) would fail (but apply the increment).
Well, I already removed it when I posted my first comment (sorry if that wasn’t clear), there’s nothing more for me to do now :)
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Add ++ after an argument, when that argument is being used for some action, means " do this action first, then increment my variable. Regardless of what that action may be, even if it fails, your variable still gets incremented. If you add ++ before your variable it means increment my variable first then use it in my expression. 
You are creating a directory named whatever `$CLIENT_BUILD_DIR` expands to, but your mysql line is trying to write in a completely different directory (one that starts at `/usr/local/test` and has today's day-of-month included as a suffix) read -r day time &lt; &lt;(date '+%d %H%M') destdir=$CLIENT_BUILD_DIR$day mkdir -p "$destdir" &amp;&amp; mysql -u debian-sys-maint -p';)reddit' -e "show full processlist" &gt; "$destdir/$time-fullprocesslist.log" Here at least, `mkdir` and the following command use the same dir; `$destdir`.
Thanks mate much appreciated 
I am sorry but the command works only partially. I thought if I could completely understand how this command came to being I'd be able to solve it, but I am unable to do so. This is the output of ifconfig: ix0: flags=8943 metric 0 mtu 1500 options=e407bb ether 0c:c4:7a:c2:f9:0c inet6 fe80::ec4:7aff:fec2:f90c%ix0 prefixlen 64 scopeid 0x1 inet 192.168.230.106 netmask 0xfffffc00 broadcast 192.168.231.255 inet 192.168.230.100 netmask 0xfffffc00 broadcast 192.168.231.255 vhid 100 nd6 options=21 media: Ethernet autoselect (1000baseT ) status: active carp: MASTER vhid 100 advbase 1 advskew 1 ix1: flags=8943 metric 0 mtu 1500 options=e407bb ether 0c:c4:7a:c2:f9:0d inet6 fe80::ec4:7aff:fec2:f90d%ix1 prefixlen 64 scopeid 0x2 inet 192.168.1.5 netmask 0xffffff00 broadcast 192.168.1.255 inet 192.168.1.2 netmask 0xffffff00 broadcast 192.168.1.255 vhid 2 nd6 options=21 media: Ethernet autoselect (1000baseT ) status: active carp: MASTER vhid 2 advbase 1 advskew 1 igb0: flags=8843 metric 0 mtu 1500 options=407bb ether 0c:c4:7a:c2:f7:90 inet6 fe80::ec4:7aff:fec2:f790%igb0 prefixlen 64 scopeid 0x3 inet 172.16.1.2 netmask 0xffffff00 broadcast 172.16.1.255 nd6 options=21 media: Ethernet autoselect (1000baseT ) status: active igb1: flags=8c02 metric 0 mtu 1500 options=403bb ether 0c:c4:7a:c2:f7:91 nd6 options=21 media: Ethernet autoselect status: no carrier pflog0: flags=100 metric 0 mtu 33160 pfsync0: flags=41 metric 0 mtu 1500 pfsync: syncdev: ix0 syncpeer: 172.16.1.3 maxupd: 128 defer: off syncok: 1 enc0: flags=0&lt;&gt; metric 0 mtu 1536 nd6 options=21 lo0: flags=8049 metric 0 mtu 16384 options=600003 inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x8 nd6 options=21 And this is the output of the command $ /sbin/ifconfig | awk '/^[^\t]+:/{ if (iface) print iface, addr, status; iface = $1 } /^\tinet/ { addr = $2 } $1 == "status:" { status = $2 } END { print iface, addr, status }' ix0: 192.168.230.100 active ix1: 192.168.1.2 active igb0: 172.16.1.2 active igb1: 172.16.1.2 no pflog0: 172.16.1.2 no pfsync0: 172.16.1.2 no enc0: 172.16.1.2 no lo0: fe80::1%lo0 no The problems: 1. I have two IP's per interface with the only difference being an additional "vhid 2" in the second IP address line. I need both the IP addresses. In ix0, the first ip after inet refers to WAN. And the second one refers to WAN CARP address. I think the script assigns the last IP address it finds after "inet" into the variable. How would I get both the IP addresses? 2. You are right, I don't want to show interfaces which don't have a status in it. So when I tried the second script as you mentioned the same problem in 1 arrives. 
Hey there. Yes you're exactly right it only assigns the last one - I was just using my own sample out and didn't realize you had mutliple inet addresses, oops. I also notice it produces `igb1: 172.16.1.2 no` for you which is wrong as `igb1` has no inet address - I guess I should have reset the variables after printing a result to avoid this. I suppose the simplest thing to do would be just to append the addresses e.g. awk '/^[^\t]+:/{ iface = $1; addr = "" } /^\tinet / { addr = addr ? addr" "$2 : $2 } $1 == "status:" { status = $2; print iface, addr, status }' This produces ix0: 192.168.230.106 192.168.230.100 active ix1: 192.168.1.5 192.168.1.2 active igb0: 172.16.1.2 active igb1: no Note that it now correctly gives no address for `igb1`. You could test if `addr` had a value before printing if you also wanted to skip devices without it e.g. if (addr) print ...
Thank you! I've switched out the first two pipes as you suggested Yes, I do expect $1 to be exact, the user is instructed to copy and paste the package id My friend said that the awk command will break the file into however many files based on how many blank lines there are. The NR part, I'm now guessing is what's responsible for adding a number to the end of each of these files. 
It has a few redundancies and inneficiencies but yeah NR prints the current number of record as you guessed
I disagree. When I say mid-sized script I'm taking about 10+ scripts with around a 100 lines of code each. There's just to much going on to do manual verification/testing. 
&gt; Rant: Agreed. Even the 10 lines code I wrote for the example tests have already caught an error that would not have been noticed until other things started to fail. I've already broken even on the 2 hours I spent writing the tests. 
Something like this? http://askubuntu.com/questions/46627/how-can-i-make-a-script-that-opens-terminal-windows-and-executes-commands-in-the
Well it wasn’t clear that you wanted to do this from a script, so it sounded like a rather stupid question, sorry… I’m afraid the answer is still “it depends on your system”, or more accurately, on your terminal emulator. With GNOME Terminal: gnome-terminal -e 'python 2.py' (with the correct path to `2.py`, and perhaps `python2` or `python3` as appropriate.)
Your parameter expansions don't exactly do what you want, but on the question on what's best practice of cd-ing to the dir first, or use absolute path in the for-loop. Either are perfectly fine. Pick whichever's easiest for the particular case. To avoid having to figure out how to cd back, you can run the cd and loop in a subshell, provided you don't need any of the variables set in the loop to carry through. In either case, do not forget to quote, and to check if cd fails. cd "$HOME" || exit for f in profile-*; do [[ -d $f ]] || continue name=${f#profile-} printf 'Do something with %s\n' "$name" done -- for f in "$HOME"/profile-*; do [[ -d $f ]] || continue name=${f##*profile-} printf 'Do something with %s\n' "$name" done 
There's nothing wrong with using `cd` in a script and you don't even need to restore it at the end of your program. The current working directory is not system-wide. Each process has its own. Your script's current working directory can't influence that of any other process. The only exception to that is if you source a dot script (using `. script.sh` or `source script.sh`) into your current script or shell environment. That will cause it to run in the same process, sharing everything with it, including the current working directory. A common idiom to temporarily `cd` into a directory without the need to restore it is by running a block of code in a subshell. A subshell is a completely new process that is forked from the main shell, so it inherits everything (functions, variables, etc. whether exported or not), but shares nothing. You can any block of code in a subshell by quite simply putting the whole block in (parentheses).
&gt; lowercase=$(echo $word | tr 'A-Z' 'a-z' | tr -d '[:punct:]') if your using character classes at least be consistent with tr '[:upper:]' '[:lower:]' :)
Did you forget to shebang in your .py file?
&gt; anal impaler &gt; cockmuncher And then there's this: &gt; 50 yard cunt punt
Where is the script that inserts missing profanity?
A little bash script I wrote for connecting to windows clients, nothing fancy or hard to do, but as a guy who mainly uses Linux and still has to jump to windows machines, I got annoyed by gui programs or typing the whole command all the time. Uses rdesktop and has domain integration. If you have anything to add / would like to see for future updates, just tell me!
I think I won’t remove this comment, but if everyone else might please kindly refrain from quoting any more from that document… :/
Right, it has to be a variable: `${VAR/pattern/substitution}` -&gt; in the variable `$VAR`, change `pattern` to `substitution`.
Grouping by similarity is difficult. You'll need to add a few constraints to the problem to make the problem easier. The constraints can be exact file extension, owner name, or even the time window to be considered part of an aggregate. The linux 'find' command has a user option that can get you started. 
&gt; `"$home"/profile-*` and `"$home/profile-"*` will be treated the same But the latter doesn't apply the properties of putting double quotes around a variable, since it doesn't explicitly have double quotes on just the variable (the variable `$home` is just a substring of the string inside the entire double quotes), right? E.g. something like shell-splitting will still apply to the latter, since the variable `$home` itself is not quoted. That's why I was wondering if it's necessary to have quotes on the variable itself `"$home"` and then quotes on the following path. And I realize that in the example provided, there's no reason to have quotes on that particular path because it contains no special characters, but my logic is this: since we use double quotes on variables even when it's not necessary (because it most likely wanted), we should do the same for paths *in general*, even if they aren't needed 99% of the time. Or am I still wrong? Thanks.
What do you mean all uppercases are transformed? How would this be a problem. Will fix, but just not sure I follow what you mean.
Oh, my apologies. I thought for some reason that the variable `lowercase` was being echoed if it wasn't a profanity. I didn't see it very clearly. I'll edit my comment.
I'll give this a try tomorrow. Thanks for sharing. 
Word-splitting only happens on unquoted parameter expansions, command substitutions and arithmetic expansions, it does not occur static strings, so there's no point in quoting paths in general. Doesn't hurt though.
Thanks for sharing! I'll be trying it tomorrow! 
Sooo... I get that this decreases the time needed typing out the command line variables... but it's not exactly a complex process to rdesktop 
I was actually hoping I could point it at my source code and it would insert the necessary profanity for me 
For this case, you should try to follow the [Google Shell Style Guide](https://google.github.io/styleguide/shell.xml) linked in the sidebar.
This is a personal choice thing, so the short version is that it's up to you - provided you're consistent and it passes shellcheck. I prefer to use `{}` notation to compensate the correct use of not-uppercase variables. One advantage of UPPERCASE variables is they have a visual impact, like the word VARIABLE sticks out from the rest of this paragraph. But you shouldn't, as a general rule, use uppercase variables unless you know why you need to use them. That drops us down to various lowercase options such as stupidhashtagcase, PascalCase, camelCase or snake_case. Personally I like camelCase. You may feel differently, in your given examples you're using snake_case. As an interesting aside; There is an accessibility argument for snake_case i.e. text to speech for the visually impaired. So now we have an issue where $variables can be lost in a large enough script unless you help to make them ${moreVisuallyApparent} to the reader. But for the umpteenth time, it's personal preference. One thing I will suggest though: For your datestamping, you really should try towards the ISO standard and use YYYYMMDD (with or without separators). This format is readily sortable, not to mention immediately understandable by those of us in the world who don't use [weird date formats](https://i.stack.imgur.com/bd99S.png). 
There are no files in "/home/Troberts261"? Does "ls -all" show you "." and ".."? "ls /" is a listing of the files in root, not the current directory. 
I would even suggest using the [ISO 8601 date format](https://en.wikipedia.org/wiki/ISO_8601). It won't be much more complicated and possibly help with later integration with other tools.
 log="${log_dir}-$(date +%Y-%m-%dT%H:%M:%ST%z).log" # UNIX log="${log_dir}-$(date -I seconds).log" # GNU
Note that, by design, base utils are quiet. They won't output anything when not necessary (unlike some other utils, like `dir`). * No error message means *no error occurred*. * No output means *nothing to say*. * Error code `0` means *OK* (`ls; echo $?`). Anything else means there was an error, and is usually detailed in the manpage (`man 1 man`, `man 1 ls` for example).
Love this sub--I always get thorough answers to problems I didn't even know I had.
This is what I came up with as well. Reasoning, you want to test the output of your grep|awk command, hence the $(give_me_the_output), to see if it is equal to your $input2.
&gt; I'm not sure why Google prefers always using braces even when it might be strictly unnecessary, whereas for quotes they don't use it when it's strictly unnecessary. Maybe just because curly braces around variables make them stand out visually? To stand out visually (which I explained in my other post), and to provide some extra consistency. You have to brace arrays and you have to brace variables when you're doing things like (sub)string manipulation, so why not brace variables by default for consistency? They do, rightly, point out to not brace single character variables because that shit can get a bit unreadable, but they also encourage the use of meaningful variable names i.e. not single character variables.
Yeah, sorry for the copy paste, i was at work and didn't noticed the formatting in reddit. (Also the comments are in bold) Your code was very helpfull, it worked like a charm ^^
 #!/bin/bash V() { let h=$((($2/78)+1)); [ $1 = x ] &amp;&amp; let h+=12 [ $1 = y ] &amp;&amp; let h+=18; [ $1 = z ] &amp;&amp; let h+=6; printf "\033[${h};$(($2%78))H$3"; usleep $delay let $1[$2]=$3; } C(){ for((i=0;i&lt;=n;i++));do V $1 $i $(($2[i])); done } S(){ for((i=1;i&lt;=n;i++)); do V $1 $i 0; done; V $1 0 $2; } M(){ c=0;for((i=n;i&gt;=0;i--));do let r=$1[i]*${2}+c c=r/b r%=b;V $1 $i $r;done } D(){ c=0;for((i=0;i&lt;=n;i++));do let r=$1[i]+c*b c=r%$2 r/=$2;V $1 $i $r;done } N(){ for((i=0;i&lt;=n;i++));do [ $(($1[i])) != 0 ] &amp;&amp; return; done; false; } P(){ for((i=n;i&gt;=0;i--));do let $1[i]+=$2[i]; if [ $(($1[i])) -ge $b ]; then V $1 $i $(($1[i]-b)); V $1 $((i-1)) $(($1[i-1]+1)); fi; done; } L(){ for((i=n;i&gt;=0;i--));do let $1[i]-=$2[i]; if [ $(($1[i])) -lt 0 ]; then V $1 $i $(($1[i]+b)); V $1 $((i-1)) $(($1[i-1]-1)); fi; done; } A(){ k=1; S $1 1; D $1 $4; C $2 $1; S $3 1; while N $3; do D $2 $4; D $2 $4; C $3 $2; D $3 $((2*k+1)) if [ $((k++%2)) = 0 ];then P $1 $3; else L $1 $3; fi; done; } clear; n=$((${1:-30}+4)); delay=${2:-1000} b=10;A w x y 5;A z x y 239;M w 4;L w z;M w 4; printf "\033[19;1H3." for((i=1;i&lt;n-3;i++));do printf ${w[i]};done;printf "\033[K\n" 
 $ f='One.Big.Test.File.2016.1080p.BluRay.DTS.x264-LEGiON.mkv' $ [[ $f =~ ^(.*)[.]([0-9]{4})[.].*[.]([a-zA-Z0-9]+)$ ]] $ printf '%s\n' "${BASH_REMATCH[@]:1}" One.Big.Test.File 2016 mkv $ printf -v f2 '%s (%s).%s' "${BASH_REMATCH[@]:1}" $ echo "$f2" One.Big.Test.File (2016).mkv 
Why not simply use the built in functionality of the command? You could always check the man page for vpnc (type "man vpnc" at the prompt), as it appears to accept switches to take care of some of that. Try something like this... sudo vpnc --gateway &lt;IPSec gateway address&gt; --id &lt;IPSec ID&gt; --username &lt;your user name&gt; If you reeeealy want to do the IPSec secret, you're going to need to use the configfile approach, which you'll have to read about in the man page.
&gt; 7 [..] This script is called Shell-bomb Funny story to go with this one. One of my friends was doing a presentation on lxc and its many uses, and one of the things he wanted to demonstrate was using a fork bomb. He however, during his live demo, accidentally pasted it into his terminal..not his lxc-attach'd terminal. Took about 15 seconds after he tried killing the container to notice his mistake. It was glorious. 
 command &lt;&lt;&lt; $variable
Thanks, I might have to. I was hoping there was a way in pure bash
The last thread you posted about this, you linked to a [stackoverflow thread](http://stackoverflow.com/questions/1055671/how-can-i-get-the-behavior-of-gnus-readlink-f-on-a-mac/1116890#1116890) which has multiple answers including POSIX friendly shell. Have you tried all of the answers there?
You can actually skip putting it into a variable first with process redirection : done &lt;(wget -qO- http://URL)
[Process substitution](http://mywiki.wooledge.org/ProcessSubstitution) has proven to be super handy for me in more instances than I can count.
&gt; [[ ("$1" != "a") &amp;&amp; ("$1" != "b") &amp;&amp; ("$1" != "c") ]] If it's really just one character you're looking for, then you can shorten it to [[ $1 != [abc] ]] If the actual strings are longer, you can use an extended glob [[ $1 != @(a|b|c) ]] And as /u/creepyMaintenanceGuy already mentioned, a `case` is also a good option case $1 in a|b|c) : ;; *) printf &gt;&amp;2 'Not a or b or c\n'; exit 1 ;; esac See [BashFAQ 35](http://mywiki.wooledge.org/BashFAQ/035) on how to do option parsing. As for heredocs, there is an indented heredoc syntax, `&lt;&lt;- EOF`, but it only considers TAB as indentation, no spaces.
Thank you! Will try.
Thank you! Will try.
I have got once a question what is :(){:|:&amp;};&amp; during an interview, I knew it so I have passed the question. Later that day, I wrote an email to my friend, also with a small report from that interview and I also mentioned :(){:|:&amp;};&amp;. Few hours later, I have got an email when he is asking what exactly is :(){:|:&amp;};&amp; doing, because he run it in shell (as root) on some HP-UX server and needed to restart it and he was quite pissed, because it was running some many-hours simulations and he lost everything...
Thanks for the help. That is very similar to what I came up with. I will give it a shot and see what shakes out.
The problem is that those variables are already quoted, so wrapping them inside another ones literally unquotes them. If you don't get further help by the time I get into my computer, I will try to solve the problem (right now I don't see a fuck here, despite I understand the problem). EDIT: typo(s)
Very unlikely 
First question: adding arguments works fine. I have an alias that looks like this: alias ll='ls -hal` and it works fine. You would presumably have to do something like alias gga='gg a' alias ggb='gg b' alias ggc='gg c' although if you had more arguments that you wanted to create aliases for (`ggd`), then it might be possible to declare the aliases dynamically in a loop to avoid having to do quite so much typing. Second question: "bash completion" is the thing to search; you'll need to create a completion file (I think it goes somewhere like `/etc/bash-completion.d/`) that tells bash what it should offer to complete to. There are probably loads of examples on the internet.
The [full script](https://pastebin.com/raw/VKiAi3jP).
Oh, that would cause a problem. Duh! OP, this is because the quotes prevent **word splitting** - normally, bash decides where words start and end based on whitespace, but by quoting an argument you say to bash "this is all one word". Each argument (e.g. `--rotate` or `1440x900`) is meant to be passed to xrandr as a separate word, so it tries to interpret `--output HDMI-1 --mode 1440x900 --rotate normal` as a single parameter; it doesn't understand a parameter that looks like that, so it doesn't work. Edit: words
As I replied to /u/anowlcalledjosh, the problem with quoting is that, when quoting... - **in the variable assignment**, you're really terminating the quotes earlier, so any space inside your other variable will cut the assignment and the rest will be parsed as a command. - **in the case statement**, you're giving that bunch of options as only one argument with spaces to `xrandr`, so probably it will not parse them right. So, to avoid those problems, your best solution is to **use arrays**. Declare your freshly-made arrays as this: left_monitor_h=("--output" "${left_monitor}" "--mode" "1440x900" "--rotate" "normal") (you can of course unquote things. The arguments in arrays are parsed the same as arguments in an utility) Then, your variables in the `case` statement should look like this: xrandr "${primary_monitor_h[@]}" --pos 1440x0 "${left_monitor_h[@]}" --pos 0x120 ;; (that will give to `xrandr` the arguments with the proper quoting for each argument)
note, though, that SUID root scripts are _extremely dangerous_. any kind of command injection vulnerability in the script will result in privesc. [here's an example](https://security.stackexchange.com/questions/146095/suid-scripts-vulnerability). OP, i'd also avoid your current strategy - since you're echoing the password in plain text, it'll show up (briefly) in `ps`.
[This](https://pastebin.com/raw/7dmn027X) is the one that doesn't work--I simply added double quotes to all variables. Sorry for the confusion.
Start with the first two [pitfalls](http://mywiki.wooledge.org/BashPitfalls). [Shellcheck](http://shellcheck.net) should also catch most of the bugs.
Thanks. So the reason this doesn't work is that you quoted `"${primary_monitor_h}"` and `"${left_monitor_h}"` in your `xrandr` command. That means that those are *not* split into separate arguments by whitespace, so `xrandr` receives them as single arguments and obviously doesn't know how to deal with that. The problem had nothing to do with the quotes around `${left_monitor}`, `${primary_monitor}`, etc., like I explained in my original comment.
Which kind of `login`? Look up the `/etc/init.d` folder (Google it). It executes daemons and such as root at startup.
so even if a command in the script requires sudo privileges, the script will run as root when the daemon calls it?
Yes, because the script runs under the caller's identity, but beware! Security flaws everywhere about that. If you put your daemon inside init.d, when it calls your script, it will call at a script in the path specified, so, if you​ let anyone modify the script or replace it, someone could immediately escalate privileges. I recommend you to let only the root user to modify the script (`sudo chown root script.sh`, then `sudo chmod 0755 script.sh`), and set the sticky bit to its container folder or letting it only be modifiable by root (`chmod +t folder/`, or `sudo chown root folder` then `sudo chmod 1755 folder/`). You know, just in case.
Right on. Thank you so much for the information!
 set -e [Don't depend on this.](http://mywiki.wooledge.org/BashFAQ/105) [Use `printf` over `echo`](http://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo) if ! test -f "$book_path"; then Some might consider `test` to be outdated and that you should be using its counterpart `[` instead for consistency. If you're 100% sure that you'll only ever be using `bash`, then `[[` is what you want. I personally don't care that much, but FYI. abs_path=$(readlink -f $book_path) `readlink -f` is a GNU-ism. You should probably put a comment about that at the start of your script to declare that it's non-portable. for f in $(ls -r *); do Try not to use vague variables, [don't parse ls.](http://mywiki.wooledge.org/ParsingLs) function print_stats { Non-portable function declaration. Sure, it's legit in `bash` (and `zsh` IIRC), but you may as well use the POSIX portable `print_stats() {` style instead. Some people moan about this: "oh boohoo, but it's not clear that it's a function!" Well, you can either use a comment: # Function to do the thing thething() { Or apply a naming scheme e.g. `func_thething() {` As has been said elsewhere, shellcheck will definitely pick up more.
Same thing.
&gt; Your article Not my article. It also links to another wiki page that talks about globbing, and that covers nullglob here: http://mywiki.wooledge.org/glob#nullglob
Does it still happen if you add a task abcabc? Or only with abc abc?
`-F` appears to have no effect on existing script's behavior. With and without it, "abcabc" is treated as separate from "abc" (good), while "abc abc" matches (bad).
Maybe with single quotes inside: grep -q "'^${enter}\$'" "${todo}" 
And what should it look like for filenames containing `"` characters? What are you escaping them for?
Thanks for the tip. I've updated the blog post accordingly. Any other things you see?
You could workaround by passing those as environment variables while executing, if you don't want to hardcode them into your script: ``` OS_USERNAME=username OS_PASSWORD=password bash export-zone.sh ```
#!/bin/bash # This script creates an array of images # and writes them to a config file used # by the MDM mytheme # The config file mentioned above stored as a variable FILE=/home/sysadmin/MDM/mytheme/config.js # Save all images in bgs directory as an array images=(~/MDM/mytheme/bgs/*) # Now calculate the number of images held in this array ((n_images=${#images[@]}, max_index=n_images - 1)) # Iterate through each item and rewrite it for use in a javascript array for ((i = 0; i &lt;= max_index; i++)); do images[$i]="'${images[i]}'," done # Write the updated array elements to a javascript file echo "var imgArray = [ ${images[@]} ]" &gt; $FILE This is the script I have so far. The script is aimed at updating a config file for a login/MDM theme I am making. The config file lists the images as an array that is then used by a javascript slideshow. I use the bash script as a means of updating the config file so I don't have to do it manually every time I add a new image to the directory. The file names with be in a standard format that doesn't include non alphanumeric characters (I will write a script for this too at some point). I know there are other ways of doing this such as with Ajax but I am using it as a means of learning a bit about bash scripting.
It is essentially to write the array to a javascript file. The filenames will only ever contain alphanumeric characters. I have no idea if this is the most elegant solution with regards to bash but I am not keen on doing it with Ajax/json as I think this is more specific to my system and simpler. I think?!
&gt; Late night with rm -rf Sounds dangerous
add **echo count** into your loop. Also you should run one more **ls** after your script to show if files was moved/renamed.
Yes, unless you have a directory named `https:` with a directory named `i.imgur.com` inside it, that should give you `No such file or directory` errors. If you want to use everything after the last `/` as the destination filename: for file in filename[0-9].txt; do read -u3 -r url &amp;&amp; mv -i "$file" "${url##*/}" done 3&lt; ~/Desktop/test/output.txt
What does `exit` return without arguments? And is `some_command || exit 1` strictly better than if [ $? != 0 ]; then exit 1 fi ?
`exit` without arguments equals to `exit $?`. $ help exit exit: exit [n] Exit the shell with a status of N. If N is omitted, the exit status is that of the last command executed. The `||` thing determines an OR operator. From `man bash`: An OR list has the form command1 || command2 command2 is executed if and only if command1 returns a non-zero exit status. The return status of AND and OR lists is the exit status of the last command executed in the list. So the `if` thing and the `||` one are roughly the same. Keep the one which will grant you the best readability in your script. For instance, if you wanted to cleanup things on an non-zero exit status, go for the `if` statement.
I read `man exit` before I asked but it says (at least on Arch Linux): &gt; This manual page is part of the POSIX Programmer's Manual. The Linux implementation of this interface may differ (consult the corresponding Linux manual page for details of Linux behavior), or the interface may not be implemented on Linux. The exit utility shall cause the shell to exit with the exit status specified by the unsigned decimal integer n. If n is specified, but its value is not between 0 and 255 inclusively, the exit status is undefined. and `help exit` yields `command not found`, not sure why. But I'm using zsh shell (I use only bash for scripts and zsh for everything else). Thanks for the info. 
Also i am open to suggestions. If you think that there's something about bash that someone should must know about. Then please suggest. 
Don't use `which` in bash. Use `type` instead.
Huh... TIL that this is a difference between bash and zsh: Bash: $ type type type is a shell builtin $ type which which is /usr/bin/which Zsh: % type type type is a shell builtin % type which which is a shell builtin I know your name is "KnowsBash", but do you know if the same caveat (use `type`, not `which`) applies to zsh? Or are they the same?
Exactly what I need. Thanks. 
Everytime I see someone speaking about tmux I wonder "why not byobu"
More complicated than you think. What if the file is created, immediately removed, then a new file with the same name is created 29 seconds later?
For your two specific questions, in the first case the you see the output without any extra space because of word splitting. Word splitting will split the input up on any character in the `IFS` variable, then after each part is handled it will be presented as an argument to the command being run (which is the first word in the command line). So in this case you are executing `echo` with 3 arguments, `a_dir`, `a_file`, and `test.sh`. So echo prints each of its arguments with a single space between them. The second, they're on separate lines because the double quotes prevent word splitting, so the single string with the newlines in it are presented as a single argument to `echo` and it dutifully prints it. You can read about word splitting [here](http://mywiki.wooledge.org/WordSplitting) and I recommend reading everything in Greg's wiki there. It's all great and has lots of really really good information.
Switched it in my Notepad++ settings, works great now. I love you. Thanks so much!
What about checking the `ctime`s of each .txt file in that folder every second? If it is at least 30 seconds ago, bye bye .txt!
:( In fact I'd prefer to "mv" the file than "delete" but still same problem. Even with this problem, it might come in handy though. 
Thank you!
You really don't want to be writing a `bash` script to completely handle this. Or some low-fi solution like a cronjob every minute that tests all the files in a directory in some way... that would be fairly simple but won't be too robust and won't scale. You want to google `incrond`, `inotify`and `systemd.path`. Once you have those figured out, select the best fit for your needs. Then simply have it trigger something like `sleep 30; [whatever other action you want done]`
If sudo isn't in there, include that as well. I'd suggest showing an example of mixing together all this stuff. Maybe under the ps command show how you could pipe ps output to a grep command like 'ps -ef | grep $process_name' 
setuid returns "command not found"
Thanks mate, setuid didn't work but I found the solution and updated my original post and marked it as solved. Thanks again for your help, glad to know there are good resources to learn from here.
If it is a builtin, it is probably useful since it can know about aliases, functions, builtins. An issue you sometimes hit in bash, if you use `which` instead of `type`, is when you install a new implementation of a command, somewhere earlier in PATH than the existing one. `which foo` will show the new path of `foo`, but trying to run `foo` may still run the previous one, or give command not found if the older implementation was uninstalled, causing much confusion. `type foo` would tell you that the old path to foo is hashed.
That doesn't prevent form word-splitting, does it? I know the variable will expand but I want to use quotes on variable or command whenever possible, like how `"${var}"` is preferred over `$var` or `${var}`. (Unless it's 100% redundant, but I read that command substitution benefits from quoting).
There's thousands of bash cheat sheets online. [Here's a pretty good one](https://github.com/Idnan/bash-guide/blob/master/README.md) that I saw somewhere on r/bash/ today.
The first format specifier (e.g. %s) get replaced with the first argument after the format. The second format specifier get replaced with the second argument after the format and so on. Don't try to embed variables or command substitutions inside the format string itself. $ printf 'BEGIN: %s\n' "$(date +%Y-%m-%d_%H:%M:%S)" BEGIN: 2017-04-03_12:52:39 And then adding extra stuff before and after $ printf '##\nBEGIN: %s\n##\n' "$(date +%Y-%m-%d_%H:%M:%S)" ## BEGIN: 2017-04-03_12:52:39 ## I'd probably just use three printf's, or put the ### stuff in a variable: $ hr=######################################## $ printf '%.20s\nBEGIN: %s\n%.20s\n' "$hr" "$(date +%Y-%m-%d_%H:%M:%S)" "$hr" #################### BEGIN: 2017-04-03_12:52:39 #################### %.20s means replace with the 20 first characters of the corresponding argument
I'm using Flycheck in Emacs and the reported error for case 1 is "The double quotes around this do nothing. Remove or escape them. [[SC2140](https://github.com/koalaman/shellcheck/wiki/SC2140)]"
Maybe the actual code uses `(( count++ ))` instead of `count=$((count+1))`? because that would trigger errexit in some bash versions. Regardless, don't rely on set -e.
That is great, thanks a lot, could you explain the : at the end?
Ill give it a go, thanks! EDIT: This works great, and i believe the reason is what /u/themattrix explained.
No problem! The `:` is exactly equivalent to `true`, just a bit shorter.
Use `case` for this. The following will loop through every file, so the "package 1, 2, 3..." thingy won't apply here. However, tell me if you explicitly want it: #!/bin/bash for file in ~/packages/*; do case "${file}" in *.deb ) dpkg -i "${file}" ;; *.sh ) chmod +x "${file}" "${file}" ;; esac done 
If you're going to be installing them a lot, you should just convert them all to .deb packages. 
One of the many reasons [not to use `set -e`](http://mywiki.wooledge.org/BashFAQ/105). It's flawed by design.
In my terminal, if I type the example for the first question I had, I get a different output than the output of the script (at least on my zsh terminal): $ b="$(ls)" $ echo $b testdir testfile test.sh ztest.sh I mention this because I don't think it's zsh-related--I saved ran the same script with `#!/usr/bin/env zsh` and its output is exactly the same as running it with `#!/usr/bin/env bash`. Do you know why and do you get the same output as above when running the commands manually instead of through the script?
I think it's better to still use it and be mindfully of its issues. 
Holy shit this is exactly what i needed. Cheers.
How are you running the bash script? I see your output in zsh, but not in bash. This is an implementation choice that zsh made. They don't word split variables when passed to commands or used in loops. See [here](http://zsh.sourceforge.net/FAQ/zshfaq03.html) for their discussion of it
Is there a way to enter your own passwords for saving without using pwgen? Optional dependencies make for easier adoption.
The default answer here is almost always GNU parallel.
Nobody's going to do your homework for you. `help read`
Does the delete_password step actually work? It looks to me like it will delete the whole DB every time. Also, even if it does work it would delete any line that contains the given key as a substring anywhere on the line.
Another note re: security. You note that if your box is hacked the attacker could get your private key, but would still need your password to decrypt the DB. They could also race your code to access the DB while it's on disk unencrypted. Anyone who can become you or root would be able to read it unencrypted. Similarly, if they are able to undelete that file, they'd have it unencrypted. Securely deleting files is a very hard problem (especially with journaling file systems).
Regarding the code itself: nicely done, good comments and neatly organized.
The renaming between +INSTALL and INSTALL is pointless. Just omit that. The only reason sed choked on the + before was because you fed it `+INSTALL` as the *sed script* rather than as a *filename* to read. Are you sure you want ``re_name=`olive` ``? You have a command named `olive`? Also, if you only need it to work on Mac, you can use the empty string as backup suffix. `sed -i '' 'script' file`
&gt; as the sed script rather than as a filename to read Don't really understand what you mean, there.
&gt; As for another of your questions that I didn't answer before: &gt; &gt; Are you sure you want ``re_name=`olive` ``? You have a command named `olive`? &gt; Yes, renaming Ok, just checking, since initially you wanted `re_name='olive'` which assigns a simple string instead of running a command named olive and capturing its output. A rather big difference.
Yeah, caught that a bit after posting all this.
`ls` actually tries to be very smart about its output. It will format things differently when it is writing to a terminal or not. So when it's writing to a terminal and things fit it will print them side-by-side. When, instead, it's writing to a pipe or file or similar (saving to a variable in this case), it will print things one per line (well, if there's a new line in a filename it would be less than one per line I guess). You can see this by doing, say $ ls file1 file2 file3 $ ls | cat file1 file2 file3 or even more thoroughly: $ ls | od -c 0000000 f i l e 1 \n f i l e 2 \n f i l e 0000020 3 \n 0000022 where you can clearly see the `\n` characters 
Can you explain a bit?
All files matching the glob `./*/` is assigned to the array named `dirs`. `./*/` matches all directories in the current directory (except the ones that start with `.`). `"${dirs[@]}"` expands all the elements of the array as separate arguments.
Well spotted /u/beatle42 ... I fixed that this morning.
...which is great. If you want to continue developing your own project, you could use `pass` as inspiration (e.g. for adding new features or re-implementing old features).
Thanks. I'm on mobile too, to at least I got that going for me /s Edit: autocorrect
If you try: echo ./*/ You will see it only get directories, I think because of "*/". I did not know this nice trick for expansion of just directories, I will do something like dirs=( $( ls -d ./*/ ) )
The `-d` flag for `ls` just means "list directories themselves, not their contents", not "list only directories, not files". `ls -d` will/can still list non-directory files. For example, I have `alias l.='ls -d .*'`, which is useful for listing all hidden directories *and* files. Without the `-d` flag, `ls` would recurse into the hidden directories as well. For listing all directories (including hidden), I have `alias ld='ls -d .*/ */'`.
thanks! 
This sounds like homework, is it homework? What have you tried so far?
Do you have a suggestion for implementation?
Read -p "Enter String" string If [[ $string == $string ]]; then echo $string:if Else Echo $string. I am new to scripting just trying to figure out the best way to approach the promblem. I can't get it it to display the scripts I entered
Is something like this what you're looking for? ===&gt; https://paste.intergen.online/view/b6aadc78
Yes I need help with scripting lessons 
Best resource I found when starting out w/ bash: ===&gt; https://linuxconfig.org/bash-scripting-tutorial Additional resources: ===&gt; http://www.tldp.org/LDP/abs/html/index.html ===&gt; http://www.gnu.org/software/bash/manual/bashref.html Also ask questions on IRC ===&gt; http://irc.netsplit.de/channels/details.php?room=%23bash&amp;net=freenode
You'll probably want `find` with multiple `-size` clauses $ ls -l -rw-r--r-- 1 user group 4 8 Apr 03:52 a -rw-r--r-- 1 user group 6 8 Apr 03:52 b -rw-r--r-- 1 user group 10 8 Apr 03:53 c $ find . -size 6c -type f -ls 15612827 8 -rw-r--r-- 1 user group 6 8 Apr 03:52 ./b $ find . -size +5c -type f -ls 15612827 8 -rw-r--r-- 1 user group 6 8 Apr 03:52 ./b 15612838 8 -rw-r--r-- 1 user group 10 8 Apr 03:53 ./c So you can use `-size +Nc` for greater than `N` bytes and `-size -Nc` for less than `N` bytes to create a range.
Found your answer by searching Google: https://www.google.com/search?q=bash+print+fixed+width It was the 3rd link down. Please search next time before posting. 
It would be nice if the author included HISTTIMEFORMAT, which is super useful.
Not familiar with MacOS, but the touch seems unnecessary and the function only saves them from having to specify the .docx extension.
Glad you asked. Yes! Use "awk".
 echo "If your lines are static, then you can just do something like this" | fold -w30 gives output: &gt;If your lines are static, then &gt; you can just do something lik &gt;e this Or break with spaces: echo "If your lines are static, then you can just do something like this" | fold -s30 &gt;If your lines are static, &gt;then you can just do &gt;something like this I assume it breaks at the closest space to the given number of bytes.
&gt; I did an echo of the command of the second output and it's exactly the same as the first one, so this has to be a quoting problem. Well yes, when it is quoted it is treated as a single "word" $ cmd='date +%s' $ set -x $ $cmd + date +%s 1491761356 $ "$cmd" + 'date +%s' bash: date +%s: command not found For storing commands in a variable the simplest way is to use an array $ cmd=( date +%s ) $ "${cmd[@]}" 1491761442 As for your second question I'm not sure what the problem is exactly? rsync ... \ ... if [[ $? -eq 0 ]]; then ... ... ... fi Why does the length of the previous command matter? Anyways to use &amp;&amp; you would need to use `{}` to group the commands e.g. rsync ... \ ... \ ... &amp;&amp; { ... ... } You may or may not consider that "more readable" though
You can't. Variable names can only contain alphanumeric characters and underscores. To use the terminology from the manpage, a variable is a parameter denoted by a name, and a name is a word consisting only of alphanumeric characters and underscores, and beginning with an alphabetic character or an underscore. See the `PARAMETERS` and `DEFINITIONS` sections of the manpage for details.
&gt;P.S. What is the best way to check if previous command was successful and do something only if it is, if the previous command is quite long? use `&amp;&amp;` (and you probably want to know about `||` too.) true &amp;&amp; echo "&lt;--- this command was successful" likewise: false || echo "the prior command exited with a status other than 0" also: false &amp;&amp; echo "this message will not be printed b/c the prior command exited false"
&gt; Everything inside double quotes is protected against word splitting. Oh I see, so it's as if the variable itself is also double-quoted. I assumed word-splitting is not applied to the variable simply because there are no explicit quotes around just the variable, so the double quotes only apply to everything that is not a variable and you would need to somehow still double quote it separately. Seemed more intuitive to me, but the actual behavior is certainly more convenient.
shellcheck.net finds a number of issues. See what you think of their suggestions 
 man rsync Yes, I'm being serious.
&gt; I'm not sure if the find command removes empty directories only from the source path specified. Well the command is `find . -depth -type d -empty -delete` meaning that it will search `.` i.e. the current working directory as opposed to the source (or dest) path specified. This would appear to be "broken" behaviour. Also, using a glob on the rsync command may not be the best idea. You can use the `-n` or `--dry-run` option to have `rsync` print out what it would do without actually doing it. (`-P` is also `--progress`) $ mkdir -p old/a/b/c $ touch old/a/omgfile $ rsync -n --remove-source-files -avh -P old new sending incremental file list created directory new old/ old/a/ old/a/omgfile old/a/b/ old/a/b/c/ sent 191 bytes received 61 bytes 504.00 bytes/sec total size is 0 speedup is 0.00 (DRY RUN) $ rsync -n --remove-source-files -avh -P old/ new/ sending incremental file list created directory new ./ a/ a/omgfile a/b/ a/b/c/ sent 183 bytes received 60 bytes 486.00 bytes/sec total size is 0 speedup is 0.00 (DRY RUN) Notice the difference when you supply the trailing `/` to the source path. You say you want to mimic the behaviour of `mv` in which case it sounds as if you would want to strip any trailing slashes from the source path given. You could do that using `extglob` e.g. shopt -s extglob src=${1%%+(/)} If it was a single trailing slash you could just use `${1%/}` but then if you added an extra one by mistake you would get unintended results. You should create some test files/directories to experiment with.
Use a function. wp_cmd() { sudo -u "$user" /usr/local/bin/wp --path="$www_path" "$@" } wp_cmd core download See [BashFAQ 50](http://mywiki.wooledge.org/BashFAQ/050) on why your approach failed.
 WWW_PATH="/opt/web/users/$USER/www/" WP_CMD="sudo -u $USER -s /bin/bash -c '/usr/local/bin/wp --path=$WWW_PATH'" echo "$WP_CMD core download" eval "$WP_CMD core download" What do you see when you run above? is "core download" a part of the wp command you're calling from /bin/bash? if so then you should add the "core download" parameters to the substring after "--path=$WWW_PATH" 
That's really smart. Thank you!
I don't know. I decided to go with a function for this. Thank you anyway!
There's a few ways you could do it. Find into a while loop may be easiest to deal with: find . -name "*.mkv" -print0 | while read -d'' -r file do ffmpeg -i "$file" -vcode copy -acodec copy \ -movflags +faststart -acodec copy \ "${file%.mkv}.mp4" done
It should work if you do this: command \ "argument"\ " continuation of argument" Note that you cannot indent the line after the backslash at all, as that will become two arguments. Shellscript has no operator for string concatenation - some languages do ``"string"+"string"``, others ``"string"."string"`` but shell is just ``"string""string"``, by having `"string"\\n"string"`` it's still treated as a single literal.
You could store it in an array output=( '/tmp/%(title)s -' '%(uploader)s' '(%(upload_date)s).%(ext)s' ) Then use `--output "${output[*]}"` to pass it along as a single string 
I often build up all the arguments first and assign them into variables.
Thank you - that works just perfectly.
One could indent by abusing backticks. $ prargv a b\ &gt; ` `b 0 '/home/ralph/bin/prargv' 1 'a' 2 'bb' $
You could delete the soft link every time, using -f to silence any "file does not exisit errors", then recreate the link: rm -f $HOME/downloads ln -sv /hdd/downloads $HOME/downloads
A simple if statement in your script should work: if [[ ! -L "$HOME/downloads" ]]; then ln -sv "/hdd/downloads" "$HOME/downloads" fi See Also: http://tldp.org/LDP/abs/html/fto.html 
I have wget -r --no-parent -l5 -A ocr.txt http://chroniclingamerica.loc.gov/lccn/sn83045211/1916-01-03/ed-1/ but I'd really like to download everything from 1916. Any ideas?
Same as the solution by Sepheus only one line: [[ -L "$HOME/downloads" ]] || ln -sv "/hdd/downloads" "$HOME/downloads" 
Is TARGET ever going to be something other than /hdd/something and is LINK_NAME ever going to be something other than $HOME/something? If it's not then all you need is one array for $something If it is going to change then that adds a higher complication to it. I'm no expert by any means so I'd probably have to play around with it a bit before coming up with some possible solutions.
This is better
Similarly, backticks can provide inline comments echo foo bar `#comment` baz and line comments in multi-line commands long command line \ `# explains this part` \ this part \ `# explains this other part` \ this other part
This is a really decent use case for GNU Parallel. 
This is a really decent use case for GNU Parallel. 
&gt; if ["$HOSTNAME" = $PROD_HOSTNAME]; then You need spaces inside the brackets: `[ "$HOSTNAME" = $PROD_HOSTNAME ]`. Without those, if `$HOSTNAME` is `foo`, then you're trying to run a command called `[foo` with the arguments `=` and `$PROD_HOSTNAME]`. Also, always run your code through [shellcheck](http://www.shellcheck.net/). It will point out errors like this.
Thanks, I'll check it out. Understood zsh has some exciting features like in tab completion. But for some reason my main shell is Bash.
Thanks for your quick reply. That is because $dropboxdir is empty while the dropbox daemon isn't running. Starting the dropbox client "creates" the dropbox-directory in my home-directory. It's not there otherwise. 
Also look at pushed/popd/dirs.
maybe you can use [curl with --max-time](https://curl.haxx.se/docs/manpage.html#-m) option instead of wget?
This, or use the timeout command to time it out although it's ugly and I like curl better 
What happens if you need to use single quotes in the command? Since I think the set of single quotes can't be escaped within another set of single quotes.
&gt;for i in Don't use meaningless variables. You want to process PID's? `for pid in` or `while read -r pid` &gt;$(ps -ef | grep testuser &amp;&amp; grep pts/0); One useful thing you can do when debugging a loop like this is to manually step through each of its components. So if you run that command by itself, the behaviour you should see is the output of the first `grep` followed by what appears to be a hang. Thats' the second `grep` waiting for input... You can type in "pts/0" and it will echo it back as a match and continue waiting for input. What's happening is `ps -ef | grep testuser` either completes successfully or it doesn't. If it does complete successfully `&amp;&amp;` (i.e. AND) allows the next command to continue. So `grep pts/0` runs. Imagine running `grep pts/0` by itself. You haven't given it a filename, so what's it `grep`ping? Nothing. So it waits for input. What you want to do is pipe the output of your first `grep` into the second `grep` i.e. this is closer to what you intend to use: $(ps -ef | grep testuser | grep pts/0); See the logical operators section [here](http://tldp.org/LDP/abs/html/ops.html) for more, as well as casting your eye [over this](http://wiki.bash-hackers.org/commands/classictest#and_and_or). &gt;do kill $i; done Don't use meaningless variables. All that aside, have you considered this? pkill -t pts/0 Might save you from reinventing the wheel?
Much easier to use `awk` rather than grepping multiple times: kill $(ps -ef | awk '$1=="testuser" &amp;&amp; $6~/\/0$/{ print $2 }')
An idea: sudo cat /dev/input/event17 | head -c1 &gt; /dev/null This command terminates as soon as any key is pressed; you can chain your mouse-restoring command after it. Caveats: - Raw device access needs root permissions (for good reason!). - Discovering the right device ID is finicky. I used [this command](https://github.com/kernc/logkeys/blob/8d5b63bafdae8b004396069fa133a26d258da07f/src/logkeys.cc#L334-L336), which I found in [this SO answer](http://stackoverflow.com/a/3384496/1420237). - For some reason, using `read -N1` or `head -c1` on the file directly both fail with “invalid argument”, hence the `cat | head` above. This seems to be related to the count passed to *read(2)*; does anyone know a good way to perform exactly one *read(2)* with large buffer size from a shell script?
Does `mutt` accept input over pipes? My guess is it doesn't and the log file you're trying to pipe is 60 lines long. Try: mutt -s "subject" otherargs ... &lt; "${log}"
https://unix.stackexchange.com/a/149991
Yes I am. My ~/.muttrc is here https://pastebin.com/nyiwHk5x
That is exactly what the issue was. I appreciate everyones help. 
short answer: If you want to do integer arithmetic in shell you can use plain old built-in: $ echo $(( 3*(8-5) )) 9 longer answer: Don't use `expr` or `eval` for this. Your working example won't even work in a directory containing files, as the `*` will expand to all filenames: $ ls $ expr 3 * 4 12 $ touch file01 file02 $ expr 3 * 4 expr: syntax error $ set -x $ expr 3 * 4 + expr 3 file01 file02 4 expr: syntax error bash (sh, bash, zsh, *sh) will first expand all wildcards and substitute variables before executing the command, so the moment you press enter, the wildcard will be replaced with all filenames the shell can find, resulting in a syntax error for `expr`. You'd have to do all kinds of escaping with `\` to prevent the shell from interfering. If you want to do floating point stuff, use `bc`: $ echo '3*(8-5)'|bc 9 ninja edit: a sentence
How do I find what bash version I have?
I'm pretty sure POSIX specifies globbing. If there aren't any matching files in the directory, `*` will expand to a literal `*` - could that be the problem?
I figured as much, but I wasn't sure enough to correct you. Glad to hear it's working.
For others who haven't seen it yet, this was discussed recently over in /r/linux: https://www.reddit.com/r/linux/comments/6553bz/a_script_that_backs_up_your_bash_history_into_an/
Hi, you need to do a loop within a loop, or I think they're sometimes known as nested loops. Make one loop change the filename, and another loop to change the dir name. You need to use 2 vars. Also using 'curl' allows use of number sequences without the need for a 'for loop, for example: curl https://www.example.com/segment-[1-22].mp4 Wget probably has similar functionality, it may be worth looking into. Good luck.
Does curl increment each variable at the same time, or will it behave like wget and just pound away at the server?
Hmm, according to the man page..... Nested sequences are not supported, but you can use several ones next to each other: So, maybe not?
Here's another version you can have some fun trying to dissect: printf 'http://www.podtrac.com/pts/redirect.mp3/cdn.twit.tv/audio/ww/%s/%s.mp3\n' ww00{01..10}{,}| wget -i-
I know you got this working, but I've recently made a simple script that uses this fantastic script "dropbox_uploader.sh"(source: https://github.com/andreafabrizi/Dropbox-Uploader) and it's been great for easily backing up to Drpobox. I'd recommend checking it out. It's great not having to sync any files to my server.
Ahh, I get it now. Was not aware of that HISTFILE variable. This is amazing, thank you! Going to do this on all my VMs right now. 
Your bash version is too old. http://mywiki.wooledge.org/BashFAQ/061
Certainly not a bug. Probably not an oversight. I don't think there is much of a demand for that feature. 
Exactly! I wonder why I didn't find this myself. Thanks!
Ah I see. Thank you.
I’m not sure what the point of the `unset`+`unalias` dance at the beginning of the script is – perhaps someone can enlighten me? It seems I can easily circumvent it with an alias on `unset`, since the first `unset` isn’t backslashed: # kill unset alias unset=: # kill unalias unalias() { :; } # trap command alias command='echo /evil; :' If I run the prolog after this, `$PATH` begins with `/evil:`, a path under my control. Of course, none of this is possible if the script is started as a new process (via the shebang), but in that case why do the `unset`+`unalias` at all?
I was confused by the title; to me "prologue" is the thing you put before something else, "prolog" is a programming language. The dictionary tells me that "prolog" is also correct in the United States (where I am), but "English (United States)" spellcheck disagrees.
I didn't understand why you don't want to use sudo. If it's some external condition, my answer is irrelevant. Otherwise you should reconsider it. Advantages : - simple and flexible configuration (which even can be stored to ldap) - no need to share common password (so to revoke access you don't need to change it to everyone) - auditing (who ran which command and when) - the privileges can be granted to a specific group instead of single person An example : bob server=(alice) NOPASSWD: /usr/bin/blah That says that Bob can run 'blah' on machine called 'server'. (Using ALL instead of 'server' says 'everywhere') You can even specify which arguments can be passed to 'blah' and what checksum the file /usr/bin/blah must have. Bob will invoke the command via sudo - u alice /usr/bin/blah The disadvantage of doing this is that the command won't have the environment set up as Alice would have - most notably $PATH and $HOME. There are other options to do that besides sudo (su, suid executable) but if you are asking this kind of question, they will be more pain. Forget about writing it in C unless the goal is to learn stuff (like signal handling). Last thing, if you want to run shell script as another user, probably you don't need whole script run as alice. Chances are that you need to run only some command from the script as her. In such case make sudo recognize only that single command and let Bob run the original script as himself. 
wasn't trying to be condescending but ok whatever i guess
However, line numbering isn't a feature of any script or text file of any kind. How are you submitting the script to him?
 dir=$(date +%A) mkdir -p ~/backups/$dir cp ~/desktop/* ~/backups/$dir However, I suggest to use some advanced timestamp scheme, like: date +"%Y%m%d"
Two terrible ideas: 1. `chronic` will run a program and buffer its output, and then only print the output if the program exited nonzero (i. e., it failed). So if you can find a filter that unconditionally prints all input to output but exits according to whether a match was found, you can stick that into `chronic`. I don’t know of any filter that does this, but I decided I wanted to try my idea out anyways, so I implemented it in Node (only environment with a `process.exitCode`-like facility that I know of): git status | chronic node -e 'process.stdin.resume();process.stdin.setEncoding("utf8");process.stdin.on("data",function(data){if(data.match(/nothing to commit, working tree clean/))process.exitCode=1;process.stdout.write(data);});' 2. If you know that your output won’t be too long, you can abuse `grep`’s context facility: git status | grep -C1000 'nothing to commit, working tree clean'
grep is all you need: git status | grep -C 10 "nothing to commit, working tree clean"
Thanks!
`sudo su user` isn't the appropriate command here. `su user` or `su - user` is more appropriate `su www-data -c "php artisan tinker"` root$ pwd /var/www/dev-develop root$ su www-data -c "php artisan tinker" Psy Shell v0.7.2 (PHP 7.1.4-1+deb.sury.org~trusty+1 — cli) by Justin Hileman &gt;&gt;&gt; exec('whoami') =&gt; "www-data" &gt;&gt;&gt; exec('pwd') =&gt; "/var/www/dev-develop" Note that root's $PATH will be preserved in this case, but $USER, $SHELL, $HOME won't be. If you do need the path for the user as well, you can use `su - www-data -c "cd /path/to/project &amp;&amp; php artisan tinker"`. You have to specify the path to the project because su - will change your working directory to the home directory of the user you've switched to, whereas su will switch to the user at the same working directory. The script fails because a) wrong switch user command, b) $@ has a specific logic to it, that is mostly broken when it's being used in something like `bash -c "$@"` when more than 1 parameter is being passed. if you add `www_data() { su www-data -c "$@"; }` in your root user's .bashrc, it'll run the command `php artisan tinker` as user www-data **when you double quote it**: `www_data "php artisan tinker"` but will simply run the command `php` as www-data if not quoted: `www_data php artisan tinker` 
To further the point of u/kalgynirae I would also recommend using double brackets anytime you're using bash. if [[ $var == $var1 ]]; then do stuff fi
Sweet thanks! 
No problem, I take it I can flair this “solved” then? :)
Yes please :)
&gt; if [[ $operator1 != "+" ]] || [[ $operator1 != "-" ]]; then That is always true. What you meant here, was the opposite of "if operator is + or -", but you forgot to apply [De Morgan's laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws) when you negated it. if [[ $operator != + &amp;&amp; $operator != - ]]; then but since the right side of = is a pattern, you can shorten that to if [[ $operator != [+-] ]]; then 
Oh, come on, I'm jealous. Put that shit on github!
You mean like [this](https://ptpb.pw/ALtJulrWTr9pamwfxtM_l0wy_9YY.png)? Doesn't seem to work (I'm getting string errors). Command works if I run it normally on the terminal. On vim, it shows that there's no trailing whitespace so that's not the issue. Any idea what's wrong?
A trick that came to my mind reading your title is to ... cheat! You could spawn a new bash that calls the script again, change in there and then via exit you'd be back to where you started. Have it check for a command line argument signalling if it's the first or second time the script is called, aka "script 2" so it knows that it's being called by itself. Then go from there. Or, the more boring way, you save the current directory in a variable. The command to read the current directory is 'pwd', which doesn't mean password ... i keep forgetting that.
I do have a small critique. Instead of having so many comment characters, #, simply use linebreaks. The # add nothing substancial. They're lines the brain actively has to ignore. (Ignoring is an active process). It's better for oversight and concentration to instead use something the brain does not have to process, yet allows easy seperation of codeblocks. Paragraphs. Like in books. It works in books, therefore it works in every written text. :) Three lines of are clearly visible as seperation, save you time typing and avoid the unnecessary clutter that's actually counterproductive for visibility.
Yep, the script will execute in a subshell. What you probably want to do is inside your script wrap it in a function definition, source it as part of your rc and then you can call it by executing the name of the function.
Yes, I use youtube-dl. I have it in a script that detects the domain of the URL (GFYcat, imgur, instagram, youtube, nhl.com, etc) and then uses the appropriate too download that media. `youtube-dl` is the tool used for downloading YouTube content. When that function is called, a `read` command is executed to ask me whether or not I want to download the MP3 version, or a high quality copy of the video (depending on if I want to download a song off of YouTube or a video). Here's the general process of how the script works: * The full URL is copied and pasted to the terminal as the 1st argument, e.g. `~/bin/script.sh http://youtube.com/?v=theBigButtGolferkadsfk` * The script analyzes the URL and extracts the domain and the video ID. The video ID is cut out of the URL via a regex grep. * Depending on the media, the script either uses `curl` or `youtube-dl` to download the media. * If I have specified a "save as" name in the quoted 2nd argument, the script will save the file as whatever I've specified. * The script then checks to see if the file downloaded correctly and prints out a message. I hope this helps guide you down the correct path to learning how to script `youtube-dl`.
Thirded!
I've already tried that. the entire command is supposed to be: youtube-dl --output "/wd500/extracted-media/%(title)s - %(uploader)s (%(upload_date)s).%(ext)s" "$@" I included a single white space because there is supposed to be a whitespace at the position where the string argument splits. I've added this single white space to either the end of the 2nd line or beginning of the 3rd line inside the quotes. I also completely removed the whitespace but the error is the same: ERROR: '(%(upload_date)s).%(ext)s' is not a valid URL. Hoping you can see this for yourself if you use youtube-dl.
No, `source` (or `.`) does not cause the script to execute in a subshell, the whole point of it is that it's incorporated into the main shell as if it were part of the script. Simply running a script like any other external command doesn't cause it to execute in a subshell either. It executes in an entirely new shell, [which is something different](http://mywiki.wooledge.org/SubShell).
Are you simply looking for BashScript.sh ; cd /path/to/target
 $ echo "`printf '%s' \ &gt; foo \ &gt; bar \ &gt; xyzzy \ &gt; `" foobarxyzzy $
That appears to work: youtube-dl --output \ "/tmp/%(title)s - %(uploader)s ` `(%(upload_date)s).%(ext)s" "$@" But I've never come across it before. what is this backtick usage called? I've only ever seen backtick used as a old alternative to `$()` for command substitution. How does it compare with using `\` and is this feature considered deprecated like when used for command substitution? Much appreciated. EDIT: Also, I realized that I get an error if I have a commented line like so: youtube-dl --output \ #"/wd500/extracted-media/%(title)s - %(uploader)s ` "/tmp/%(title)s - %(uploader)s ` `(%(upload_date)s).%(ext)s" "$@" I get the error: `youtube-dl: error: --output option requires 1 argument`, which leads me to belive perhaps backticks is more of a hack in this scenario.
A couple of tips: I have a script with yotube-dl that simply reads URLs from a text file. There are options for that and to also keep a file up to date with what videos have been ever downloaded. So you could have something watching the clipboard and writing to a text file. Then another script on an infinite loop moving that file and running yotube-dl on it. File handling left as an exercise​
http://babun.github.io/
Hmm. So apart from shells that nobody cares about like `tcsh`, strictish C-style syntax hasn't been relevant (let's be honest) in shells for a long time... Let's generously say mid 90's, but likely earlier. The new fangled syntax used in `bash` and its bourne-alike kin is a kind of C-abstracted-through-algol dialect, in fact you can Google "bournegol" or "bash algol" and similar and you'll probably find what you're after. FWIW Bourne set aside `||` and `&amp;&amp;` as macros for ORF and ANDF respectively. In v7 unix `sh`. Which was released in 1979. `|` has obvious other usage in shells, so we can deduce that somebody came up with `\` at some point. So you're looking for `csh`, `tcsh` and other C-style shells, or you're looking for anything pre-Bourne. /edit: potentially useful, if not interesting: http://hyperpolyglot.org/unix-shells http://oldhome.schmorp.de/marc/bournegol.html https://research.swtch.com/shmacro https://www.in-ulm.de/~mascheck/bourne/ 
If you just want to be able to keep using the same terminal while it's running: java -jar "foo.jar" &amp; If you want to stop it outputting to your terminal as well: java -jar "foo.jar" &gt;/dev/null 2&gt;&amp;1 &amp; There's no easy way to make it run in a new terminal window. Probably what you'll want to do is open a new terminal window using whatever your terminal emulator's keyboard shortcut is (probably ctrl-T or ctrl-N), then run `java -jar foo.jar`.
I don't know what that is
oh. my plan was to run multiple jar files on multiple terminals. I have a python script that goes like: java -jar "1.jar" java -jar "2.jar" etc etc etc, like 15 of those. the 2nd java -jar doesnt execute unless I exit the first jar. Also I want them on separate terminals so I can see their "logs" or whatever you call them, individually. sorry, am a linux noob.
Depending which terminal emulator you're using, you might be able to do something like `gnome-terminal -e 'java -jar "foo.jar"'`.
He's writing this on macOS. There's no udev to be had. 
Sorry for a late reply. Thank you @Xenther and @z0rberg. @Xenther Now passing with "flying colors": &gt;$ shellcheck myscript &gt;No issues detected! @z0rberg Yes, you do make a very strong argument. This was expunged.
&gt; This behavior is part of readline, and as far as I know is not configurable, unfortunately. That's strange, because I've never seen it happen anywhere else. Do other programs use readline? Do they all get this wrong in the same way? &gt; Do you really find that key combo to be faster for you than just holding down ALT and typing b five times? The most common use is that in emacs I use ctrl-U all the time, and so I unmap it from kill in the tty driver, because otherwise I end up killing what I'm typing by mistake. So it occurred to me I could map ctrl-U to "universal-argument," and then the bash command line would act like emacs. But when I hit ctrl-U, the whole line changes, and that makes it completely unusable.
I've been through a similar exercise myself so I have many thoughts about it. Short version - You said it yourself: # It would be better to have a reliable hardware RNG or install haveged Here's a [decent HWRNG](http://onerng.info/) and I've been happy with `haveged` as a pragmatic solution for VM's and headless servers. It steps in only when it's needed, so I simply install it everywhere. `haveged` alone will dominate anything your script generates, both in terms of randomness quality and speed. An even more direct option is to simply `rmnod /dev/random` and `mknod -c /dev/random 1 9` (this is off the top of my head, so double check that!). What this does is removes the blocking character device `/dev/random` and replaces it with the not-blocking character device `/dev/urandom`. It's a cleaner solution to just plain old removing `/dev/random` and `ln`'ing. Don't do this and use `haveged`, because it will max out an entire CPU core. The main problem with the Linux CSPRNG is that `/dev/random` blocks, which is IMHO shitty behaviour - others take a saner approach of blocking-at/from-boot until sufficient entropy is built up and then never blocking. Couple it with developers who think that "`/dev/random` = secure, `/dev/urandom` = not secure" or that these CSPRNG's are sources of fast entropy (they aren't) and you get software incorrectly pointing at `/dev/random` that unreasonably blocks, or is held up performance-wise for no actual security gain. If your software genuinely needs true(r) and/or faster entropy, it will require an HWRNG. Let's set some time aside now to read this: https://www.2uo.de/myths-about-urandom/ More direct thoughts about the script itself: I don't like the use of uppercase variables (IMHO only use uppercase variables when you know you need to), I don't like the /full/pathing/to/every/binary (this destroys any chance of portability - set PATH right at the start instead only if you need to) and using `shuf` definitely destroys any chance of portability. Unless you add a `shuf` function. /usr/bin/shasum -a 512 | /usr/bin/awk '{print $1}') # get only shasum from column 1 Is repeated multiple times, violating Don't Repeat Yourself (DRY), make it a function and refactor. I'm not a fan of in-line comments. The `awk` to get just the sum serves little purpose - I get that it sanitises the sum ready for splitting and hex'ing, but feeding the extra two characters into the pool will serve no harm. Don't bother either with selectively choosing whether or not to gather "entropy" from your sources - you simply want a bunch of fast, dirty, unpredictable characters - so just dump everything. Here's some of the code from the approach I took: https://pastebin.com/jZKCiXCm There'd be little extra work involved with putting something like that into a loop and having `rngd` sip away at it. In my use case I was creating a script to generate random integers as portably/POSIX-ly as possible (hence the lack of bashisms), and I've since replaced all that pseudo-entropy handling with a linear congruential generator (based very loosely on [this](https://rosettacode.org/wiki/Linear_congruential_generator#UNIX_Shell)). There's an interesting alternative here that uses wifi as a noise source: https://calomel.org/entropy_random_number_generators.html What would be genuinely interesting would be something along the lines of [pollen and pollinate](https://github.com/dustinkirkland) 
thanks
Why not just use `xbacklight`? In fact, according to `man xrandr`: &gt; `--brightness brightness` &gt; &gt; Multiply the gamma values on the crtc currently attached to the output to specified floating value. Useful for overly bright or overly dim outputs. *However, this is a software only modification, if your hardware has support to actually change the brightness, you will probably prefer to use `xbacklight`.* (emphasis added)
Thanks for the feedback! I had `$DEFAULT_OUTPUT` outside of the function for other reasons and completely forgot to plop it inside. Also, `local` is a new thing to me -- thanks! As for `bc`: I will add the comment; do you know if there is any other floating point arithmetic tool that ships with most/all distros?
&gt; do you know if there is any other floating point arithmetic tool that ships with most/all distros? No, I think `bc` is your best bet (unless you're using `zsh`, as I noted above), since `bc` is a POSIX standard. Arch just may be an exception. (Actually, I don't know why Arch doesn't come with at least POSIX `bc`; instead, you have to install GNU `bc` from `extra`.)
As your script is a bash-script, the better syntax to use is: if [[ $uptime1 != "$uptime0" ]]; The difference being that `[` is another name for the `test` command, while `[[` is a bash inbuilt "new test". The new test has many improvements, for example LHS variables don't need to be quoted as no globbing or wordsplitting will happen within `[[]]` (but patter matching will happen on the RHS, so you need to quote that unless wanted). Read more from here: http://mywiki.wooledge.org/BashFAQ/031 Btw. if you're not checking strings, you should use the arithmetic comparison `((` instead. edit. Added note of RHS pattern matching as per comment below. 
`awk` is usually a good bet as a `bc` alternative. You might also like to consider `python` or `perl`
This is every annoying comment I can come up with: About that DEFAULT_OUTPUT variable name you use, upper-case names are popular for things in the global environment. I think this isn't a problem in this script here, but if you use lower-case in your scripts, you increase the chances that your name will be unique. Something that might be interesting to know: awk has grep basically built in. These two lines do the same: xrandr -q | grep " connected primary" | awk '{print $1;}' xrandr -q | awk '/ connected primary/ {print $1;}' When you compare numbers with `if [ $# -gt 0 ]`, that's the way to do it to be compatible with the rules of a basic /bin/sh program like what's used in Debian and Ubuntu. If you know you only write code for bash, it has neat extra features you can use. For numbers, there's `((` and you can do `if (( $# &gt; 0 ))`, or just `if (( $# ))` as it's using C rules where a zero means 'false' and anything else means 'true'. You get documentation with `help '(('` and `help let`. Another thing to know if you write just for bash: you will always want to use `[[` over `[` for tests. You can use `&amp;&amp;` and `||` inside it, and it protects you against mistakes with missing `"` around variable names. The documentation is in `help '[['` which adds on top of what you read for `help test` (which is the rules for `[`). A different way to get text sent into bc is with `&lt;&lt;&lt;`. These two lines do the same: echo "0.75 + (0.25 / $x)" | bc -l bc -l &lt;&lt;&lt; "0.75 + (0.25 / $x)" 
Return 9
You can work with floating point numbers with the external math program named "bc". You pipe your math as text into 'bc', and it will output the result as text. This bc program also has comparison operators where the result will be 1 for 'true' and 0 for 'false'. Using it looks like this in practice: line=$(wc -l &lt; "~/.bashrc") if (( $(bc -l &lt;&lt;&lt; "x = $line / 9; x &gt; 8 &amp;&amp; x &lt; 9") )); then echo ... fi Instead of doing this, you might want to rethink your code to be able to work with integers. This is because floating point can be annoying when you ask things like "x &lt;= 9.0" because you often compute results that should be "9" but the variable gets something like "9.0000000000001" written to it because of how binary floating point math works. What I mean with rethink your stuff is something like this: x / 9 &gt; 8 =&gt; (x * 9) / 9 &gt; 8 * 9 =&gt; x &gt; 72 This means, instead of dividing by 9 and then comparing with 8, you could instead not divide at all and compare with 72.
Is this an acceptable way to keep the int? PLIT_NUMBER=`expr ${FILE_COUNT} / ${DIVIDE_NUMBER}` if ((${SPLIT_NUMBER} % 8 == 0)) then ${SPLIT_NUMBER}= ${SPLIT_NUMBER} + 2 else ${SPLIT_NUMBER}= ${SPLIT_NUMBER} + 0 fi
You need to add four space characters in front of every line that you want reddit to show as code, and there needs to be an empty line before and after the code. I don't understand what you are doing with the division and that modulo test. I guess I'll just talk about bash in general because bash will complain a lot about your code: The `((` in bash can replace the expr program. Your code could look like this if you use `((` everywhere: (( SPLIT_NUMBER = FILE_COUNT / DIVIDE_NUMBER )) if (( SPLIT_NUMBER % 8 == 0 )) then (( SPLIT_NUMBER = SPLIT_NUMBER + 2 )) else (( SPLIT_NUMBER = SPLIT_NUMBER + 0 )) fi You can also do things like this with `((`: (( x += 2 )) (( ++x )) A line like this does not work: ${SPLIT_NUMBER}= ${SPLIT_NUMBER} + 2 First, bash will translate all those `$...` into the contents of the variable. If you for example do this here: x=4 $x=$x+2 What bash sees is you typing: x=4 4=4+2 If you try this right now at a bash command line, you'll see it complain like this: $ x=4 $ $x=$x+2 bash: 4=4+2: command not found The name on the left of a `=` has to be without a `$`, like so: x=$x+2 This is now still wrong, because bash will not see it as math. It will run this as if it's text: x="4+2" It needs to look like this: x=$(( x + 2 )) Or this: (( x = x + 2 )) Space characters are important on lines with `name=`. If you look closely at your lines previously, you did something like this: x= $x + 2 Those spaces will confuse bash and you'll get strange error messages, see here: $ x=aaa $ x= $x bbb ccc bash: aaa: command not found $ x= "$x bbb ccc" bash: aaa bbb ccc: command not found It only works if there are no spaces: $ x="$x bbb ccc" $ echo "$x" aaa bbb ccc You can make bash print the lines of code it executes when it runs a script. This might help to think through what's happening. You start the script with `bash -x scriptname.sh`, or you add `set -x` to the beginning of the script (you can also surround just a part of your code with a line `set -x` at the start and `set +x` at the end). I think I remember this `set -x` has a downside with code that's using `((` where it won't print the contents of variables like it does with normal code, but you can add this back by keeping `$` for names, I mean `x=$(( $x + 2 ))` instead of `x=$(( x + 2 ))`. You might want to use lower-case names for all variables you use inside your script, so for example "file_count" instead of "FILE_COUNT". This is so nothing unexpected happens when there's an environment variable on the system having the same name, and those environment variables are nearly all upper-case.
For Firefox, there's an addon named "Open With" that can add menu entries for programs to the context menu for links. The programs get started with that particular URL as a parameter. You could use that to get URLs into a script and wouldn't have to do things to the clipboard.
Thanks for the rundown
The script https://pastebin.com/YeYztsKN Example txt file https://pastebin.com/E8s19QU1 What it can do: Switch between foreign - native ↔ native - foreign Add/delete new phrases Randomly presents you with a phrase Can read the foreign part using espeak Install: Put the script and the language file in the same folder inside $PATH Edit the script at line #20 so it knows your txt file. edit: More about me: I am a scripting noob. This whole thing took me 4 days, mostly Googling for stuff. 
I experimented on the command line, trying to write a function that follows what you explain: foo() { if [[ -z "$1" || -z "$2" ]]; then echo "wrong parameters"; else dir="$1 - $2"; if [[ -d "$dir" ]]; then echo "directory '$dir' exists"; elif [[ -a "$dir" ]]; then echo "name '$dir' exists"; else mkdir "$dir"; fi; fi; } The same looks like this with line-breaks: foo() { if [[ -z "$1" || -z "$2" ]]; then echo "wrong parameters" else dir="$1 - $2" if [[ -d "$dir" ]]; then echo "directory '$dir' exists" elif [[ -a "$dir" ]]; then echo "name '$dir' exists" else mkdir "$dir" fi fi } Here's how testing this in an empty folder looks like: $ foo() { if [[ -z "$1" || -z "$2" ]]; then echo "wrong parameters"; else dir="$1 - $2"; if [[ -d "$dir" ]]; then echo "directory '$dir' exists"; elif [[ -a "$dir" ]]; then echo "name '$dir' exists"; else mkdir "$dir"; fi; fi; } $ foo wrong parameters $ foo a b $ ls a - b $ foo a b directory 'a - b' exists $ touch 'c - d' $ foo c d name 'c - d' exists 
Why do you need to check if the folder already exists? You're already using `mkdir -p` which will prevent any errors if it does.
"env | grep &lt;stuff&gt; | wc -l"?
It works when you order it like this: program &gt; filename 2&gt;&amp;1 I can't explain it. Bash has a `&gt;&amp;` and a `|&amp;` that you can use instead, and then it looks like this: program &gt;&amp; filename program |&amp; program This won't work with the sh used by Debian/Ubuntu/BSD, and will only work with bash, and I guess that's why you always see that `&gt; file 2&gt;&amp;1` stuff in guides and not that simpler `&gt;&amp; file`.
The file descriptors are evaluated from left to the right. So: program 2&gt;&amp;1 &gt; filename means run program, direct stderr into stdout, direct stdout to filename. The problem with that construction is that stdout redirect to file has not yet been set when the stderr redirect to stdout is evaluated. If you run that syntax on the command line, stderr will appear where stdout normally appears if not redirected, in the console.
What happens when you run `which env`? It usually resides at `/usr/bin/env` so you could try running that too... It might help if we had some context about what you're actually wanting to achieve.
try printenv 
This is a modification to a script I've used in the past to do daily incremental snapshots with `rsync`, which has the benefit of not duplicating files when they exist unmodified in a previous backup (via hardlinks) #!/bin/bash today=$(date -I) yesterday=$(date -I -d "1 day ago") week_ago=$(date -I -d "7 days ago") source_dir="~/desktop/" target_dir="~/backups/${today}" link_dir="~/backups/${yesterday}" rsync -avh --delete --link-dest="$link_dir" "$source_dir" "$target_dir" rm -rf "~/backups/${week_ago}" 
No worries I would get into the habit of keeping safeguards in your scripts. It's just good practice, and the while IFS= read syntax is pretty much the de facto usage. A big area where this can be applied is with double quoting variables. Feel free to join #bash in the [macadmins slack](https://macadmins.herokuapp.com)
Thanks! this worked like a charm 
PowerShell. You just described PowerShell. Go try it. 
Phew! that's a lot of stuff! Thanks for the feedback. It was encouraging. I am glad you didn't see my script for backups :D That is a total disaster but works. Getting the number of lines was problematic for me. I tried several ideas but none worked. One of the ideas was something like: lines=wc something-something random_line=shuf -i 2000-$lines -n 1 So I did want to use `shuf` but I got only errors. Probably some obvious mistake. I was struggling with every part of the script. All I can do is to look something up and reuse it. You didn't mention how I managed to do the switching. Lines 124 and 132 where the options hash one another. I don't like it because it edits the script causing (IMO) unnecessary writes. But I had no other idea. I am going to use your suggestions some time. Probably in the next script. I plan to make an interactive simulator to help me understand some things at work, how goods move from warehouse to the shop and stuff. 
A newly built Windows 10 PC
What are you using Bash with then? Sys32 or whatever it's called?
I think the comma is only special inside an arithmetic expression (ie. `((`, see ARITHMETIC EXPANSION in the manpage). For example: $ echo $((1,2)) 2 $ declare -A a $ a=([a]=1 [1,a]=2) $ echo ${a[a]} 1 $ echo ${a[1,a]} 2 Do you have a reference for the comma's special behaviour in an array parameter expansion, /u/ropid?
For the subscript of an associative array, comma is just a character like any other. Nothing special. 
If -n can check for a variable to be set. Conversely -z returns true if it's not set or empty. 
Oh for the love of... I absolutely knew this at some point. There must be a word for forgetting and only remembering because someone explained it to you all over again. Thank you.
Arrays can't be exported. The environment only knows `KEY=value`, and there's no way to serialize a bash array into the environment without potentially causing a "non-array" environment variable being treated as a bash array.
If you have questions seek help at /r/PowerShell! 
Yes, a lot of options: add some compression, or incremental backup, rsync to some remote place or with a cloud drive, or something more like code repository, :) However you did not provide more details or you exact needs to bring some better idea.
**Here's a sneak peek of [/r/commandline](https://np.reddit.com/r/commandline) using the [top posts](https://np.reddit.com/r/commandline/top/?sort=top&amp;t=year) of the year!** \#1: [Cool, but obscure unix tools](https://kkovacs.eu/cool-but-obscure-unix-tools) | [51 comments](https://np.reddit.com/r/commandline/comments/5k2d0z/cool_but_obscure_unix_tools/) \#2: [CLI geeks like sudoku ?](https://i.redd.it/bziwltfccary.gif) | [22 comments](https://np.reddit.com/r/commandline/comments/654cfq/cli_geeks_like_sudoku/) \#3: [TIL vi is 40 years old... initially released 1976. I knew it was old... but not that old. Pretty impressive considering it is still standard issue today.](https://en.wikipedia.org/wiki/Comparison_of_text_editors) | [35 comments](https://np.reddit.com/r/commandline/comments/5ammom/til_vi_is_40_years_old_initially_released_1976_i/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
Thanks.
Dude :) Hash ==== $ declare -A HASH $ HASH[one]=1 $ HASH[two three]='2 3' # Values $ echo "${HASH[@]}" 1 2 3 # Keys $ echo "${!HASH[@]}" one two three Utility function used later =========================== upvar() { if unset -v "$1"; then # Unset &amp; validate varname if (( $# == 2 )); then eval $1=\"\$2\" # Return single value else eval $1=\(\"\${@:2}\"\) # Return array fi fi } Function returning more elements ================================ # $1 - input number # $2 - variable to return x^2 # $3 - variable to return x^3 math() { upvar $2 $(( $1 ** 2 )) upvar $3 $(( $1 ** 3 )) } $ math 5 A B $ echo $A 25 $ echo $B 125 Higher order functions ====================== add_three() { echo $(( $1 + 3 )) } twice() { local func=$1 $func $( $func "$2" ) } $ twice add_three 5 11 Multi dimensional arrays ======================== Well, there are multiple approaches, all ugly. I wanted to present my solution but instead I found bug in bash so I'll just shut up now :) 
I tried this just now, and most of the time disappears into the "convert", so that's where you'd look. When I check this: time scrot /tmp/screen.png That completes in 0.1 sec. It runs terrible when I test both scrot and convert like this: time { scrot $t; convert $t -blur 0x7 $i -gravity center -composite -matte $t; } I guess you need to replace 'convert' with a completely different program that does its stuff faster, or try different filters and filter settings for convert to see if you can find something that runs faster.
I too would be interested in something like this. I routinely train total newbies how to work with bash and it would be nice to see what other people have put together. 
Perl works in bash :) And on all Linux distros :) In all seriousness, use a tool like perl or ruby for complex and serious parsing. Awk, sed, and grep all have their roles and neat little niches. But they all do one thing and them well. Perl can do them all and do them well. 
&gt; grep/sed/awk are not Don't they all have a POSIX set of options that is... standard?
The only really good example I remember is that sed has annoying differences in its `-i` parameter for editing files. I think it might not be possible to build a command line that makes all versions happy at the same time. Here's a something where people talk about this: https://unix.stackexchange.com/questions/92895/how-to-achieve-portability-with-sed-i-in-place-editing The thing about perl is then, if it's just about editing a file with sed's s/// command, that looks really similar when translated into a perl one-liner so you don't have to learn anything new: perl -i -pe 's/abc/def/' filename It gets complicated when trying to translate more complex sed scripts. There's a book about perl one-liners that's pretty much just a list of a lot of examples and is easy to steal from without having to think about perl a lot.
I have no special tool to advise, but i think is should be easily achievable with the use of "sleep" command and the "&amp;" char at the end of your command to not lock your script. Something like this (pseudo code): seconds=0 while(true) if (seconds == 0) your command &amp; commandPid = $! fi sleep 1 seconds++ if (seconds == 20) kill commandPid wait = 0 while (wait &lt; 5) sleep 1 wait++ endwhile seconds = 0 fi endwhile The "&amp;" char at the end of your command prevents it to lock your script. The "$!" allows you to get the PID of your command in order to kill it once the 20 seconds are reached. The sleep command pause your script 1 second. It mays not be very accurate. If you need presise timing you'll have to play with dates or timestamps instead. 
The examples are quite thorough and helpful. Thank you for this.
May be you need a * or + (\* or \+) after ]
Yes, would remove the complete content of your partition including every additional partitions mounted at the time. Though to fully work it would need to be called with `sudo` before it to gain admin rights since a normal user wouldn't have the permissions to remove the systems folders. (Not exactly sure if without sudo it would stop right away with an error or still kill at least those folders the user has permissions for). In any case don't use it.
&gt;Im not sure if this is the right place to ask this. /r/linux4noobs might be a better fit.
Has anyone actually ever done this because they needed to? What happened?
sorry i didn't ask well my question so i will repeat my question. i'am searchin for a string that composed of word and a `regex` # Word('regex') i'am using grep for my search and it look like this: # grep -o 'Word(''[^a-zA-Z0-9]' text.txt and it show me only this Word(' #Exemple of the string that i'am searching: Word('MTA')
that's it it worked thank's for you'r help appreciate 
Then keep that shit as blackmail ^^
No no.. Its: man &amp; rm - rf /*
Concatenation in bash is quite straightforward. foo="Hello " bar="World" concat=$foo$bar 
I guess you'd only run it for fun. On Unix you can delete files that are in use by running programs. The files will disappear from the directories, but the actual file contents will still be kept on disk as long as the files are kept open by programs. You might want to see what happens on a desktop, see what still keeps working and what's broken. I don't know if it ever really makes sense to wipe stuff with 'rm -r'. The command has to go through all directories and remove each entry individually. If you instead use mkfs to overwrite the file-system with a new, empty one, that's done instantly so I'd always want to do that instead.
base="sudo useradd" addcomment=" -c $comment" printdirect=" $user &gt; /dev/null" If you really want to just add to the same variable, the syntax would be something like this: var="Hello" var=$var" World" echo $var command=$base$addcomment$printdirect
I did it once in my early days of terminal - following a tutorial that was otherwise just fine. Luckily, I had a recent backup -- also part of the tutorial now that I think of it. Lesson learned. 
Solid advice. I endorse this response. 
What about this: while :; do ./some_prog &amp; proc=$! sleep 20 kill $proc sleep 5 done 
Oh but there is, I'm having trouble finding it right now but there is a write up done by someone who was able to "come back from the dead" after doing this. 
You will run into problems if you need spaces in one of your arguments. For example that `"$comment"` earlier, if that text has spaces, it will get split into several arguments when you finally run your command with `$command`. Concretely, you can't package something like this in a single variable: sudo useradd -c "John Doe" john You need to keep that comment in a separate variable and then write your script something like this: if [[ -n "$comment" ]]; then $command -c "$comment" $user else $command $user fi Or another way to work around the problem would be using an array. This would look like this: command=(sudo useradd) command+=(-c "$comment") command+=("$user") Then run the program like this: "${command[@]}" That `&gt; /dev/null` you want on the command line will always be a problem. The `&gt;` and the `/dev/null` will get treated as text and sent as arguments to the program. Bash will not react to it and will not redirect output to /dev/null. You need to keep it outside of your variables, so like this: "${command[@]}" &gt; /dev/null To help understand what's going on if something doesn't work, you can run your script with `bash -x scriptname` to make bash print the lines how it actually sees them after variables get replaced with their contents.
By the way, what does `-n` mean in that if statement? EDIT: Okay, I guess it checks if the string is not null? Correct me if I'm wrong. 
The "-n" tests if the "$comment" variable is set or not. To not have to remember all of the possible tests (there's a lot of them), just remember that you can do `help test` on the command line to make bash print a list. There's also `help '[['` that adds a bit to what you can read in `help test`.
I would use [parameter expansion](http://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion), make the search condition $1 the mandatory argument and $2 the path, using period if $2 is not set... find ${2-.} 2&gt;&amp;1 -name "$1" 
&gt; However, the actual command has `&lt;path&gt;` as optional (if you don't specify the path, `find` will use the current directory by default. I just want to point out that this is a GNU findutils extension, and should not be used in scripts intended to be portable.
The Unix recovery legend http://www.ee.ryerson.ca/~elf/hack/recovery.html There's also the Toy Story 2 one: https://www.quora.com/Did-Pixar-accidentally-delete-Toy-Story-2-during-production/answer/Oren-Jacob?share=23eba63a&amp;srid=aTaF
Thanks /u/ernesthutchinson and /u/ralfwolf--I was hoping to somehow preserve $1 as path only and $2 as search condition to replicate how `find` has path as $1 and $2 as search condition when both are specified (this is important to me because it preserves the usage of `find` if I were to use the actual command in other ways, but it seems that that requires a significantly trickier implementation. So I might just stick with my current alias which preserves the argument usage if $1 as path and $2 as search condition even if a path is mandatory for the alias.
You don't need the `eval` or the double quotes around the whole command. Instead, put double quotes around each argument that contains `$filename`.
Ah, I see. Thanks!
Write a script to generate a .ftprc file and then call ftp. 
To get exactly what you want, you could split your call to 'find' into two different lines that get chosen depending on the number of arguments: findname() { if (( $# &gt; 1 )) ; then find "$1" -name "$2" else find -name "$1" fi |&amp; grep -v ': Permission denied$' } That `(( ))` is the math mode in bash. I don't understand what you are doing with all of those redirections and grep and that `$? -eq 1` test. In this example here, I just made grep remove the "permission denied" errors using the `|&amp;` that bash has, which is a short version of `2&gt;&amp;1 |`. You can experiment with all that stuff on the command line before you put it into your .bashrc. This looks like this: $ f() { if (( $# &gt; 1 )); then find "$1" -name "$2"; else find -name "$1"; fi |&amp; grep -v 'Permission denied$'; } $ f test $ f /usr/lib test If you want to experiment with using `$1` etc. a bit on the command line, for example to test a line that you would use inside a function, you can set those `$1` etc. variables with `set --` like this: $ set -- one two $ echo $2 two While at the command line, the bash `type` command can print how it sees your function that you've typed on one line. You can use that to get something to copy'n'paste into your .bashrc. Check this out: $ f() { if (( $# &gt; 1 )); then find "$1" -name "$2"; else find -name "$1"; fi |&amp; grep -v 'Permission denied$'; } $ type f f is a function f () { if (( $# &gt; 1 )); then find "$1" -name "$2"; else find -name "$1"; fi 2&gt;&amp;1 | grep --color=auto -v 'Permission denied$' } 
Once I learned sed was for more than just the a command, it changed my life! Still not an expert, but this site has helped much: http://www.grymoire.com/Unix/Sed.html
There is about a million ways to do that. I would do it with a shell script that deletes the stuff or whatever, then call that script daily with crontab 
Don't be a dick, be a dude! 
Two alternative suggestions I have for you: 1. Have a look at an application called motion (should be able to get it via apt on Debian or Ubuntu and presumably similar on other distros). It can manage webcams and ip cameras for security and can automate pretty much everything including cameras with pan &amp; tilt etc. I use this with a pi and a webcam and find it very good. 2. Consider using expect scripts, it can type and respond to pretty much anything you can do by hand on the command line. If you can type it, it can imitate it. Again very handy, i have some I use for connecting to a router over telnet because the router is locked down embedded system so I can't script in there easily. But i can easily script from the client side. 
Could you elaborate on your last point? I see that the ...&amp;&amp;..|| does the same thing, but I'd like to understand why it's better.
&gt; Just use something &amp;&amp; whatever || something else . https://github.com/koalaman/shellcheck/wiki/SC2015
Notice that `head -6` style is discouraged in favour of `head -n 6`, as the former isn't POSIX conformant. The GNU `head` manual says: &gt; For compatibility `head` also supports an obsolete option syntax `-[num]` [...] If your script must also run on hosts that support only the obsolete syntax, it is usually simpler to avoid head, e.g., by using `sed 5q` instead of `head -5`. And the same goes for `tail`.
I think it sounds pretty straightforward. But your description is just a little ambiguous. Can you provide the code so I can see what it's doing?
Wow butthurt much? youCanNameYourVariablesHoweverYouWantGuy
Excellent examples. Thank you. I had actually thought the shorthand would be viewed as sloppy. Singe line if statements in C style languages like "if( condition ) doSomething();" seem to be frowned upon, so I had thought the same might apply to bash. Also definitely good to know about string vs. integer comparisons. I've read this before, but seeing it reaffirmed is always a good thing.
My final solution is as follows: mountspace=( $(df --output=avail ${mountname[@]} | tail --lines=+2) ) which creates an array *mountspace* which contains the current available free space for each of the mount points listed in *mountname*. The rest of my script just does a comparison for the minimum amount of free space and then sends me a notification via Pushover if below the limit.
I don't get it. I guess I'd try making the script print messages into a log file in the hope to learn why it hangs, something like this in front of the tar command: echo "$(date) -- $0: Compress world" &gt;&gt; /tmp/"$0".log and in front of the find command: echo "$(date) -- $0: Delete backup files" &gt;&gt; /tmp/"$0".log Messages will be saved in a file in /tmp under the script's name. I don't know what happens if several scripts add to the end of the same log file at the same time. A thing to try is, you could connect to one of those processes with `strace -p &lt;pid&gt;` and look at the system calls it is doing. You might be able to guess what's going wrong if you see an interesting directory name or similar in there. If you have "htop" installed on the machine, it has a hotkey `s` that starts strace on your selection (needs root permissions, so start it with `sudo htop`).
Ah, that must be it. It does appear that the google drive directory is hanging. Can't even ls it, thanks for the assist! 
Using [Redirections:](https://tiswww.case.edu/php/chet/bash/bashref.html#Redirections) { youtube-dl ... 2&gt;&amp;1 1&gt;&amp;3 | tee ~/youtube-dl.log ;} 3&gt;&amp;1 Using [Process Substitution:](https://tiswww.case.edu/php/chet/bash/bashref.html#Process-Substitution) youtube-dl ... 2&gt; &gt;(tee ~/youtube-dl.log)
!gcc That should be what you want, this will run the last command that started with gcc Source: http://samrowe.com/wordpress/advancing-in-the-bash-shell/
This is EXACTLY what I was looking for thank you so much!
Hmm, I tried that, but when I use the function it doesn't stop running: fname() { if (( $# &gt; 1 )); then find "$1" -name "$2" else find -name "$1" fi 2&gt; &gt;(grep -v 'Permission denied' &gt;&amp;2) }
I didn't know you can do that, it's awesome, thank you :)
 grep -q -e "$value1" -e "$value3"
Yes. If you have a lot of patterns, you can put them in an array and do patterns=( foo bar baz ) grep -q "${patterns[@]/#/-e}" or put them in a multiline variable, since every line in a pattern argument will be treated as separate patterns patterns=$'foo\nbar\nbaz' grep -q -e "$patterns" or put them in a file, one pattern per line, and use -f. grep -q -f pattern-file
If you use the shell in vi mode (set -o vi) then you can search your history with - esc k / SearchWord enter. I find it extremely useful! Reference: https://linuxacademy.com/howtoguides/posts/show/topic/16618-vi-and-the-command-line
Don't need the 'k'. ESC/ works fine.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You don't need brackets to call a bash function.
You don't need the "()" to call the functions in the main loop: while true do clear Show_Menu #Displays the menu Read_Input #waits for user input done Also, in your "Read_Input" funtion, you need to fix the read statement to use lower case: Read -p "Enter your choice [ 1-4 ]" choice _should be_ (note the lower case "read") read -p "Enter your choice [ 1-4 ]" choice On a side note, you probably want to be quoting all your inputs when you use them to handle files with spaces in them etc. eg, mv $f1 $f2 becomes... mv "$f1" "$f2" Lastly, I don't know what your assignment requirements are, but the destination in a move statement doesn't _have_ to be a directory. Move a file to another name is how we rename files in *nix.
Ahh old habit !
Think twice before you add a progress bar. Most of the time, it's a feature you don't want, because it clobbers the output. What happens when you pipe the output of your script to something else? etc.
If you use !gcc, it'll execute the last command that started with "gcc". If you use !gcc:p, it will print the last command that started with "gcc" to stdout; useful if you want to verify what the last command starting with "gcc" is before actually executing it blindly, as azzid mentions.
One possible syntax is also this one: grep -q -E "$value1|$value2" &lt;&lt;&lt;"$valuelist" Where the patterns are separated with a `|` and you'll enable this regex-syntax with `-E` (or use `egrep` if available). Also note that you should always quote your vars (`$valuelist` in this case).
Thanks, i'm going to plug these into my test script to play with them. See what makes the most sense for the logic i'll need.
I considered playing around with `pv`, and I'll give those suggestions a try. 
`$upspeed` is not the same as `$upSpeed` (capital S), so that explains why that prints nothing. As for grep matching everything, that is because you're losing the newlines when piping the `$testResults` to `grep`, try to double quote instead: `upSpeed=$(echo "$testResults" | grep ...)`. 
An argument containing `?`, `*`, or `[` is treated as a pattern. `.[].Env.Config[]` is a pattern starting with a dot, followed by any of the characters `].EnvCofig[`, which matches the parent-directory entry `..`. From [the manual](https://www.gnu.org/software/bash/manual/bash.html#Pattern-Matching): "A ‘]’ may be matched by including it as the first character in the set." Turning on `nullglob` (`shopt -s nullglob`) might help while you're playing around with this.
Thank you so much! This seems much more obvious now. My brain was grouping each pair of `[]` so I assumed that at worst, it was matching nothing. I didn't realize that the globbing engine would get greedy and slurp up all that. I actually had to re-read your comment 3-4x before it finally clicked about what you were saying (I'm not sure how you could have said it any more clearly).
I forgot that `pv` is not installed be default on the distributions I am testing this function on. That's sort of a deal-breaker for my purposes. If anyone is interested, this is how `pv` could work within this framework, to at least show that the process is running: function disk_space(){ local largestfiles=$(find / -type f -exec du --separate-dirs --human-readable {} + 2&gt;/dev/null) # find largest files by disk space; output background noise to /dev/null # write_header "Disk Usage" df --human-readable --total | awk 'NR==1; END{print}' # echo "Retrieving largest files..." echo "------------------------------" echo " Top 10 Disk Eating Files " echo "------------------------------" echo "${largestfiles}" | pv | sort --reverse --human | head --lines=10 } 
 alias dev="cd /Users/&lt;myname*&gt;/Desktop/all/dev" Try alias dev="cd $HOME/Desktop/all/dev" I have all sorts in [my .bashrc](https://raw.githubusercontent.com/rawiriblundell/dotfiles/master/.bashrc), have at it. Note: a lot of that was written with Solaris in mind, and some of it down to Solaris 8 (i.e bash 2.04... ugh), so certain decisions were forced into play. I'm slowly modernising it in areas.
Great topic! I've recently started to clean up my .bashrc by having a .bashrc.aliases and a .bashrc.functions and then source these from my .bashrc. It's not needed of course, but it does make things more neat. As for favourite things, here are two of mine: Showing tab-separated files neatly on the command-line: function coltab { columnt -t -s $'\t' $1 | less -S } And for quickly copying the absolute path of a file: function r { realpath $1 realpath $1 | tr -d "\n" | xclip -i -selection c -f | xclip -i -selection primary } 
Good point! At the top of the script I have trap control_c SIGINT Then I have a function below as follows: control_c() { echo -en "\n *** CTRL+C caught - Trying to cleanup before exiting ***\n" if [ $cleanup_possible -ne 0 ]; then echo "Cleanup possible..." rm -rf $folder; echo "Bye Bye" sleep 1; exit 1; else echo "Cleanup not possible - Folder already contains data" echo "Bye Bye" sleep 1; fi exit 1; }
[My bashrc](https://github.com/BlitzKraft/dotfiles/blob/master/bashrc). My fav part is the `extract` function. I don't know its original author, but I am eternally grateful. It extracts almost any kind of archive and no need to mess with various tar flags. 
 alias listfun="cat ~/.bashrc | grep function | cut -c 9- " Oh that's asking for some reinvention, if you're open to feedback of course. Here's a completely overplumbed one-liner for you: while read -r func; do if egrep "^${func} ?()|^function ${func}" ~/.bashrc &gt;/dev/null 2&gt;&amp;1; then printf "%s\n" "${func}"; fi; done &lt; &lt;(compgen -A function) So it removes the UUOC, caters for `function functionName`, `functionName()` and `functionName ()` declarations, and tries to minimise duplication and spurious matches. At the absolute least, yours could be rewritten as `grep "^function" ~/.bashrc | cut -c 9-` &gt;My fav part is the `extract` function. I don't know its original author, but I am eternally grateful. It extracts almost any kind of archive and no need to mess with various tar flags. Don't forget its friend, `compress`, here's a basic version I found that could probably be expanded: # Provide a function to compress common compressed Filetypes compress() { File=$1 shift case "${File}" in (*.tar.bz2) tar cjf "${File}" "$@" ;; (*.tar.gz) tar czf "${File}" "$@" ;; (*.tgz) tar czf "${File}" "$@" ;; (*.zip) zip "${File}" "$@" ;; (*.rar) rar "${File}" "$@" ;; (*) echo "Filetype not recognized" ;; esac }
I tend to use functions over aliases most of the time these days. I have been using a very simple [.bashrc](https://github.com/jnalley/dotfiles/blob/master/bashrc) that loads in additional functionality for the commands I want to "enhance" and fails or degrades gracefully if the commands aren't present. I have a directory ([~/.bash.d/hooks](https://github.com/jnalley/dotfiles/tree/master/bash.d/hooks)) that contains small shell snippets per command. My .bashrc iterates that directory and sources the "hook". One example is my [grep](https://github.com/jnalley/dotfiles/blob/master/bash.d/hooks/grep.sh) hook. I want to have a 'g' command that recursively greps the current directory. I prefer 'ripgrep' or 'the silver searcher' to standard grep - so my hook creates the correct function or alias depending on what command is available on the system. Also rather than having aliases to cd to particular directories you can do the following: CDPATH=.:~/Projects:~/Other/directories shopt -s autocd Then you can just type the name of the directory to cd there.
Why are all caps variables considered bad practice?
&gt; I had the compress function, but I didn't use it at all. So, I removed at one point. [`tar(1)` syntax is soooooo easy!](https://xkcd.com/1168/) $ tar cvf - folder | xz -9 - &gt; archive.txz # POSIX and portable!
[Image](https://imgs.xkcd.com/comics/tar.png) [Mobile](https://m.xkcd.com/1168/) **Title:** tar **Title-text:** I don't know what's worse\-\-the fact that after 15 years of using tar I still can't keep the flags straight, or that after 15 years of technological advancement I'm still mucking with tar flags that were 15 years old when I started\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/1168#Explanation) **Stats:** This comic has been referenced 201 times, representing 0.1282% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_dh3b3fj)
That is an extreme edge case, but the author implies otherwise. If you close stdout, don't `echo` to stdout.
A perl one-liner instead of grep would look like this: perl -ne 'print if /substring/ and /substring2/ and /substring3/' indexed_files The same with sed: sed -r '/substring/!d; /substring2/!d; /substring3/!d' indexed_files Using ls like you do seems bad. I fear its output is not really standardized, but I don't really know. Check out the `-printf` parameter in find's man-page. You can use that to make find output things in a format you decide yourself, instead of building your stuff around whatever you see ls printing. Also, about those examples earlier, you might want to cut away everything that's not a file name before you search for your substrings. In the sed example, making it cut away the first seven columns looks like this: sed -r 's/^(\S+\s+){7}//; /substring/!d; /substring2/!d; /substring3/!d' indexed_files and in the perl one: perl -ne 's/^(\S+\s+){7}//; print if /substring/ and /substring2/ and /substring3/' indexed_files
I don't think `grep` has support for `AND`ing patterns. `grep` is written around the idea of matching a line to a regular expression; regular expressions in general aren't conceptually compatible with `AND`ing. According to this [Stack Exchange Post](https://unix.stackexchange.com/questions/55359/how-to-run-grep-with-multiple-and-patterns) there are `grep` variants that do allow you to `AND` patterns together. However, I think a better suggestion from that post is to use `awk`. It seems like a more natural fit. The core paradigm of `awk` is to take actions on lines matching patterns. Those patterns can be `AND`ed together as part of the basic `awk` syntax. I would also reconsider your aversion to using `find` on the live file system. If you are just matching on the file names, it's might be about as efficient as searching a text file. There's also the issue that when you are `grep`ing through your file listing, you're not just searching through the file names, you're also matching on the rest of the line line. For instance, if one of your patterns was "May", you'd find any files with "May" in the name, but you'd also find any files last modified in May, since the long listing will display the timestamp that way. Here's a completely untested stab at a function to do what you're suggesting. I fully expect that I'm probably screwing something up with the wildcard characters and quoting, but you should get the idea: andfind () { local SrchRoot="$1" shift local p="" local -a Switches=() for p in "$@" ; do Switches+=("-name" "*$p*") done find "$SrchRoot" "${Switches[@]}" -exec ls -lhd {} + } Which would be invoked like this: `andfind /etc substring1 substring2 ... substringN` 
There's `awk` as has been mentioned e.g. `awk '/foo/ &amp;&amp; /bar/'` To do case-insensitive matching however you would need to add `tolower()` e.g. $ echo 'FOO bar' | awk '{ line = $0; $0 = tolower($0) } /foo/ &amp;&amp; /bar/ { print line }' FOO bar You'd need to store the original `$0` to keep the initial *"casing"*. You can do `AND` with regex but you need lookaheads which you have if your `grep` has `-P` (or you can just use `perl` directly if it doesn't.) $ printf %s\\n 'foo foo' 'FOO bar' 'foo bar BAZ' | grep -P -i '(?=.*foo)(?=.*bar)' FOO bar foo bar BAZ 
Try this. grep "pattern1\|pattern2" That's a backslash and a pipe.
Use `if [ $percent -gt $threshold ]` instead, IIRC (although someone'll probably come and tell me why this is the wrong way to do it...)
What's your end goal? Why not use [locate](https://linux.die.net/man/1/locate)?
Some thoughts (though again, creating an MCVE that replicates the problem would help immensely - it's also a good troubleshooting tactic): * Have you confirmed that the trap function is the problem (i.e. commenting out the `trap` statement avoids the problem)? * What sort of work is going on when you kill the script? Could you be interrupting it at a bad time? * Your conditional seems error-prone - if your intent is to delete the folder if it's empty, check for that inline, rather than some previously-set variable that might be out of date. * I can't recall offhand if this would be a *problem*, but I think you want to `return`, not `exit` from the function. 
I don't understand why your script works. Did that paste.ubuntu.com website remove backticks (`` ` ``) that are surrounding your `df | ...` lines? I mean this line here: result=df -kh |grep -v "Filesystem" | awk '{ print $5 }' | sed 's/%//g' That is probably supposed to look like this, or with `` `...` `` instead of `$(...)`: result=$(df -kh |grep -v "Filesystem" | awk '{ print $5 }' | sed 's/%//g') And same with the "partition=" line: partition=$(df -kh | head -$i | tail -1| awk '{print $1}')
Thanks a lot. How do you compare /u/X700's redirection with yours? &gt; `{ youtube-dl ... 2&gt;&amp;1 1&gt;&amp;3 | tee ~/youtube-dl.log ;} 3&gt;&amp;1` I'm having trouble understanding his example out of curiosity. As for the process substitution, is my understanding correct (referring to the example `youtube-dl ... 2&gt; &gt;(tee ~/youtube-dl.log)`): `youtube-dl ... ` outputs stdout to terminal like it normally does, then `2&gt;` redirects stderr to something that tee can take as stdin, which will print the stderr to stout (console) and also write it to the log file. ? Does this respect the order in which youtube-dl normally prints both stdout and stderr to console? Or does this command print all stdout first and then all stderr after? Do applications print all stdout and then stderror to console anyway or is it completely up to the application itself? I'm not sure if youtube-dl does it. And is [this](http://burgerbum.com/stderr_pipe.html) still relevant or can process substitution achieve the same thing more intuitively/easier?
You would need to loop. While there are still arguments left and `$1` does not start with `-` append it e.g. -n | --name ) shift projectName=$1 shift while [[ $# -gt 0 &amp;&amp; $1 != -* ]] do projectName+=$1 shift done But that would mean you cannot have any "non-option arguments" after it.
I like using the `history-search-backward` and `history-search-forward` functions, which scroll through the history of commands starting with whatever is currently entered. So for your example you'd press x and then ctrl-p to see the previous command that started with x. One wrinkle, though: those function aren't bound to any keys by default. Add the following to your bashrc and source it: bind C-p:history-search-backward bind C-n:history-search-forward If nothing is currently on the command-line `history-search-backward` works the same as function bound by default to ctrl-p, `previous-history`, and likewise for going forward.
Try this approach: https://gist.github.com/superDross/7e997d5cda5686312066b035cc48b37d I've yet to use getopts but I'm pretty sure it is commonly used. Anyone - What would the advantage of using getopts be over using my approach?
Cool! Glad I could help!
Yep, copied and pasted that one from... somewhere... without fixing it up. Thanks for giving me a nudge to look at fixing it
Udev rules
Does this Ubuntu version use systemd? You can add something like this in /etc/fstab: /dev/sdc /mnt/media auto defaults,auto,nofail You then do `sudo systemctl daemon-reload` for systemd to read the config changes. The important parts are those `...,auto,nofail` mount options. After this, it will now automatically mount things to /mnt/media the moment /dev/sdc shows up. If you already have a disc in the drive, you first have to remove it and put it in again for this to trigger. Then in a next step, to make systemd activate your script after things are mounted, you write a service file that gets triggered after mounting /mnt/media. The thing to know about this is, systemd will have translated your /etc/fstab entry into a unit named `mnt-media.mount`. This is the name you will use in the rules for your service. The name for the unit is built out of the path you used in /etc/fstab. This name here is what happens for that /mnt/media path in the previous /etc/fstab example. Your service file has to be something along these lines: [Unit] Description=do something with /mnt/media Requires=mnt-media.mount After=mnt-media.mount [Install] WantedBy=mnt-media.mount [Service] ExecStart=/usr/local/bin/my-media-script.sh Type=oneshot This file goes into `/etc/systemd/system`. The important parts are the Install section, and what you do in the Requires= and After= lines. I don't know if there's a short guide somewhere on how to use systemd. Here's a bunch of stuff: * `systemctl daemon-reload`: needs to be done after every edit to a config * `systemctl cat name`: prints the service file that systemd sees * `systemctl enable --now name`: enable and start a service * `systemctl status name`: look up its errors * `journalctl -u name`: look it up in the log (will have all of your script's output) **EDIT:** I tested this stuff right now, and it seems to work. I used a USB stick, not a disc. **EDIT2:** Also, to make this trigger for just the right device and not just anything that will show up as /dev/sdc randomly, you should look at what interesting things you can find in /dev/disk. You can address things through the exact hardware controller and ATA port in there, or exactly one certain USB port. There's things like this in there: /dev/disk/by-path/pci-0000:00:14.0-usb-0:1:1.0-scsi-0:0:0:0 /dev/disk/by-path/pci-0000:00:1f.2-ata-3
Oooh, awesome! This will absolutely get me started, thank you :)
Try changing `mv -- $sourc $dest` to `mv -- "$sourc" "$dest"` Edit: just tested it. Works for me. Adding quotes should do the trick $&gt; mkdir test1 test2 &amp;&amp; touch test1/a\ hi $&gt; ./move.sh SOURCE test1/a hi Destination test2 $&gt; ls test2 'a hi'
My source folder is FILES 3 Destination folder is: MOVE HERE mv: rename /Users/james/Desktop/FILES\ 3 to /Users/james/Desktop/MOVE\ HERE: No such file or directory Edit: It works if I replace the $sourc and $dest in the script and write out the actual paths in there. But trying to replace it with a variable thru input doesn't seem to want to accept it. Am I doing something wrong?
Hmm, it's working for me #!/bin/bash echo "SOURCE" read -r sourc echo "Destination" read -r dest set -- * [ "$#" -le 10 ] || shift "$(($# - 10))" mv -- "$sourc" "$dest" And when I run it... $&gt; mkdir test\ 1 &amp;&amp; mkdir test\ 2 &amp;&amp; touch test\ 1/a $&gt; ls test\ 1 a $&gt; ./move.sh SOURCE test 1/a Destination test 2 $&gt; ls test\ 2 a So yes, you're doing something wrong...but I don't know what (*scratches head*) Edit: check that you're not including backslashes when your script prompts you for the folder names. You don't need it there
The backslashes were the issue. Not typing them in the prompt fixed it. Although its moving the entire folder and not the files inside but I'll tinker around to figure it out. Thank you so much!