Cool! Yeah.. I learned a bit about ordered pairs in Python, but was struggling to figure it out in bash. I'll play around with this and see if I can figure out a nice way to use it. Thanks!
You might find nmap's greppable output format (-oG) useful. If you give it a list of addresses to check you'll get something like this back: $ nmap -sn -oG - slashdot.org google.com # Nmap 6.47 scan initiated Mon May 23 14:20:28 2016 as: nmap -sn -oG - slashdot.org google.com Host: 216.34.181.45 (slashdot.org) Status: Up Host: 216.58.194.110 (dfw06s48-in-f14.1e100.net) Status: Up # Nmap done at Mon May 23 14:20:29 2016 -- 2 IP addresses (2 hosts up) scanned in 0.46 seconds You can also scan a subnet instead of checking specific addresses (though of course that will take somewhat longer): $ nmap -sn -oG - 10.0.1.0/24
http://wiki.bash-hackers.org/scripting/posparams should get you started. 
Oh, that's cool, I didn't know about that 
&gt;Let's say my script is named "testy", located in /usr/bin (with chmod +x), so i can execute it by just writing "testy" in my terminal. Let's say "testy" an installer script like apt-get, yaourt, pacman, yum, etc... . I want to write: &gt;$ testy [program i want installed] Hey, on that part you might like to do a wee google search for something like "reddit bashrc functions aliases", those threads are a goldmine and I've seen a basic version of the exact thing you're after. If you can find it, tie it in with your positional parameter and come back here with your code, we can help you bump it up to the next level.
Nice, but: * paste your code into http://www.shellcheck.net (the site changed and now sucks or what?... still gives some tips) * the good shebang is `#!/usr/bin/env bash` * don't use `[` in bash, use `[[` instead * don't use ALL_CAPS_VARIABLES those are usually reserved for global use.
Bash actually has a `coproc` keyword since version 4. The examples in README.md could be implemented like this: # Example 1 coproc a while sleep 1; do cat /proc/loadavg; done coproc b while sleep 1; do date; done paste &lt;(cat &lt;&amp;$a) &lt;(cat &lt;&amp;$b) # Example 2 coproc bc_proc { bc; } stdin=${bc_proc[1]} stdout=${bc_proc[0]} echo '1+1' &gt;&amp;$stdin read -u $stdout result echo $result 
A couple of ways using printf: printf %s "${array[0]}" printf ', %s' "${array[@]:1}" printf '\n' -- printf -v tmp '%s, ' "${array[@]}" printf '%s\n' "${tmp%, }"
Any other simpler methods? This seems a bit literal.
Well, you can always write a function: join() { # usage: join ', ' "${array[@]}" local sep=$1 arg printf %s "$2" shift 2 for arg do printf %s%s "$sep" "$arg" done printf '\n' }
&gt; If I understand correctly, the first line (`local sep=$1 arg`) assigns the local variable `sep` to the first argument `$1`, or `', '`—but what's the `arg` doing beside the `$1`? variables are global unless you declare them local to the function, so since I use arg for the `for`-loop later on, I declare it as local so it won't litter the global scope. $ foo() { var=inside; }; var=outside; foo; printf '%s\n' "$var" inside $ foo() { local var; var=inside; }; var=outside; foo; printf '%s\n' "$var" outside &gt; The second line prints out the second argument `$2`, the array—but why? join ', ' "${array[@]}" expands to: join ',' "a" "b" "c" "d" So, `$2` is the `a`. &gt; I don't really understand what shift does. The for loop seems to be printing the array literally. `shift 2` shifts away the first two arguments # before: $1 = ', ' $2 = a $3 = b $4 = c $5 = d shift 2 # after: $1 = b $2 = c $3 = d so the for loop iterates those last three values, printing each one with `$sep` added in front. If you run `set -x` you'll get some debugging output that shows what's happening. `set +x` to turn it off again.
Imagine the 'arg' standing next to the 'sep' name, not the '$1', so imagine it like this: local sep=$1 local arg The 'arg' is later used in the 'for' loop. It's added to the 'local' line so that a similarly named variable on the outside of the function will be protected. The 'for' loop would overwrite an 'arg' that exists outside without the 'local'. The second argument will be the first element in the array. It will not be the whole array. The `"` work differently when used with an array. When you do this: `"$@"`, then bash reads that as `"$1" "$2" "$3" ...`, not `"$1 $2 $3 ..."`.
Something else: You might want to use `"` around `$dirName`, or it won't work with names that have spaces. I mean, it should look like this: chmod 'g+w' '/var/www/html/'"$dirName"'/wp-content' or like this: chmod 'g+w' "/var/www/html/$dirName/wp-content" 
Thanks, but I need a way to do it without "local - n" as I'm on bash 4.2. I'll keep that method in mind if I can upgrade in the future. Do you know of another way to do it? 
Works well enough, but it's still a bit literal—using regex to replace the space.
Your second example actually does work when elements contain spaces.
Not a pure bash solution, but this can be done easily enough with `awk`: echo $array | awk -v OFS=", " '{$1=$1;print}'
You're probably better off discarding the `sudo` handling and forcing the script to be run only by someone with appropriate `sudo` or root permissions. This is pretty easy - right at the very top of the script you put something like if [[ ! -w / ]]; then printf "%s\n" "ERROR: This script must be run as root or with appropriate sudo rights" exit 1 fi
I don't really know. It seems you can do this here: $ a=(hi hello yes no what) $ x=a[@] $ echo ${!x} hi hello yes no what So this means you could use this to somehow read from an array. I don't quite know how you are supposed to write. This here seems to work: $ x=a[2] $ declare $x=YES $ echo ${a[@]} hi hello YES no what It needs the 'declare'. Without it, I get this error: $ $x=YES bash: a[2]=YES: command not found I think it should be possible to translate that earlier example into something that uses these tricks here, but I feel a bit overwhelmed. I'd probably rethink the whole script you are working on right now just to try to avoid this.
https://www.youtube.com/watch?v=G2y8Sx4B2Sk
&gt;[**You keep using that word. [0:07]**](http://youtu.be/G2y8Sx4B2Sk) &gt;&gt;"You keep using that word. I do not think it means what you think it means." &gt; [*^bagheadinc*](https://www.youtube.com/channel/UCNtnXVqtRcbUYSotB0ED09A) ^in ^Comedy &gt;*^1,766,460 ^views ^since ^Feb ^2007* [^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)
It's not exactly what you're asking for, but you should probably check out getopts for parsing the arguments. http://wiki.bash-hackers.org/howto/getopts_tutorial
I think this one is the winner
Ooh looks interesting
Yes of course, that was rude of me. I was just making sure I was in the right place since Expect is a slight off shoot. My code looks pretty close to this: (taking out the sensitive bits of course) !/bin/expect -f set timeout 30 log_user 0 puts stderr "Connecting to VPN *__Servername__*" spawn env "/root/bin/forticlientsslvpn/64bit/forticlientsslvpn_cli --server *__serverIP__* --vpnuser *__Username__* --keepalive expect "Password for VPN" send "*__password__* interact
Simple, there is \r at the end...
can you paste your full script and the bug on pastebin? Sanitize it of course. Pasting it here sucks for formating. FOr the $ being in there just in close it like this {pa$sword} the {} escapes the entire string. also expect isn't an off shoot of bash. :) 
&gt; spawn env "/root/bin/forticlientsslvpn/64bit/forticlientsslvpncli --server *serverIP* --vpnuser *Username_* --keepalive Possibly it's the missing double quote on the end, but you shouldn't need to put `env` into the mix at all, just do this: spawn /root/bin/forticlientsslvpn/64bit/forticlientsslvpncli --server *serverIP* --vpnuser *Username_* --keepalive 
yeah i was try to recall this operator. Thanks.
&gt; You should probably have a look at: http://tldp.org/LDP/abs/html/string-manipulation.html Preferably don't have a look at that garbage. Try [FAQ 100](http://mywiki.wooledge.org/BashFAQ/100) instead.
He can use dos2unix to remove those for text-files.
&gt; toUpper: Use `tr '[a-z]' '[A-Z]'` or any of the other existing solutions. &gt; &gt; toLower: Swap the args for `tr` above. That’s actually worse than OP’s function, which uses the native Bash features `${variable^^}` / `${variable,,}` and works for more characters than just A-Z. (Though I agree that, as it’s already a native Bash feature, it’s not really necessary to make a function for it.)
The start of the script is incorrect. Use \#!/usr/bin/expect -f the #! is called a shabang. http://tldp.org/LDP/abs/html/sha-bang.html It tells the OS where your script interupter lives. Can you run the script and paste the output you are getting along with errors now? 
You also still have a `"` on line 6 which you need to remove
**Always** [double-quote expansions](http://wiki.bash-hackers.org/syntax/quoting) (eg `echo "$sudo_password"`) if you don't know for sure that they don't contain spaces or other special characters. Unless you actually want argument splitting and wildcard expansion to happen to the result. (nothing personal, it's just that this mistake is so damn prevalent and it will fuck you up when you least expect it)
As mentioned you're running `cut` on the whole file date=$(cut -d " " -f1 /var/log/bind/query.log) Run the `cut` command in your shell on it's own and look at the output. You could write it without `cut` by using `-a` to store the `read` into an array instead of using `cut` to split it up into "words" while read -r -a line do line[4]=${line[4]%%#*} echo "${line[0]} ${line[4]} ${line[7]}" done &lt; "$1" Another option would be to pass multiple names to read and discard the "words" you don't want into a dummy variable name (`_` is commonly used for this) while read -r date _ _ _ ip _ _ url _ do echo "$date ${ip%%#*} $url" done &lt; "$1" You may also want to look at `awk` awk '{ sub (/#.*/, "", $8); print $1, $5, $8 }' "$1"
&gt;echo 'PATH="HOME/bin:$PATH"' &gt;&gt; "$HOME/.bashrc" You forgot a `$` before the first `HOME` `sudo my-script` will work too then.
Thanks, that solved first problem but not the second problem (sudo my-script)
I started the script using awk but decided to use cut as I'm not very familiar with awk. I should probable get familiar though. 
nothing happened when I wrote that, what was it supposed to do? Also, tried **sudo my-script** after but still says **command not found**
I suppose PATH is already marked as export elsewhere. Anyway just to be sure, `my-script` works? And `sudo echi foobar` works?
Yep, my-script works without sudo and **sudo echo foobar** works.
How about these: `sudo -E my-script` `sudo "PATH=$PATH" my-script`
Wow the array version seems to run much smoother. Thank you. Would you mind explaining the "%%#*" in the array declaration. 
I removed the "%%#*" and ran the script. It looks like it removed the "#" after the ip address. Not sure how though. 
&gt; Alternately, you can add the fully qualified path into the root user's .bashrc PATH environment. This sounds like what I want to do.. I guess it's done something like: sudo echo 'PATH="$~livirkillerfetish/bin:$PATH"' &gt;&gt; "$HOME/.bashrc" Can you confirm/correct this, please?
Yeah. `%%` removes the longest matching pattern from the end (a single `%` removes the shortest). The pattern is `#*` the `*` here is a glob character which matches "anything" (if you're familiar with regex it's like `.*`) user@host $ s='foo#bar#baz' user@host $ echo "${s%#*}" foo#bar user@host $ echo "${s%%#*}" foo There is also `##` and `#` from removing from the start. It's called **Parameter Expansion** there are some more examples here: http://mywiki.wooledge.org/BashFAQ/073
If you want the command to be available to sudo then you should locate it in a standard system path directory. /usr/local/bin should be the right place.
The redirect will happen in your current user shell not inside sudo. I'd suggest using sed to append the line like this: sed -i '$a PATH="~livirkillerfetish/bin:$PATH"' /root/.bashrc Honestly, I don't typically like adding directories that are not owned, and write restricted, by root into the root path because it could lead to trivial trojan attacks.
That's good content, but it it might better received as an article or blog.
If your issue is that the ABS needs to be fixed, why don't you help fix it, instead of blanketly trashing it. Yeah, it may not be the 'best' resource, but it is *a* resource. Otherwise, come up with a shorter summary than what you have above, and something a little more informative than 'it's garbage'. Otherwise people like me will get annoyed with you for shitting on 90% good advice. edit: Also, google hits and google rankings, _are_ important. Most people looking for advice will only find the stuff you consider beneath you. They aren't going to find your bash wiki, because noone is willing to look that far, when something that 'mostly' works is readily accessible. So hate google all you want, but you have to do some SEO optimization for the bash wiki if you want more people to find it and reference it. edit edit: Thank you for the insights and explanation, I do appreciate you taking the time. edit edit edit: You also criticize my use of `expr`, but there is no equivalent for getting the index / position of a character in a string in your wiki. If you have an alternative, I'd recommend adding it to the string manipulation page you linked to. edit edit edit edit: &gt; An advanced bash-scripting guide should focus on explaining bash features. Yeah, because using only pure bash will solve all problems. ABS goes over `sed` and `awk` as well, are you going to criticize it for teaching people advanced scripting, in addition to advanced bash scripting? &gt; This works ok if `$IFS` is unchanged Yeah, but a lot of the older guides and old mentality was: Don't fuck with `$IFS` unless you *absolutely* need to, to get the job done. I see `$IFS` jiggering all too common in a lot of 'modern' scripts, and usually where it is not necessary. Not quoting things is bad, but blaming not quoting on improper use of `$IFS` is looking for bad code in (probably) bad code. Not quoting strings is bad for a number of other reasons, most importantly mishandled user input resulting in opportunities to exploit the script. ABS may not be completely up-to-date on best practices (though it could be, if they allow(ed) revisions), but it provides a solid footing for learning enough to learn that you might want to seek out better practices. That's why stuff like your Bash Pitfalls page is very useful, and hopefully advanced advanced [sic] bash scripters will find it, once they've gotten off the ground learning the advanced basics. I learned how to script with the docs at TLDP and I didn't stop there. That's why I'm /reasonably/ good at writing shell scripts.
I originally didn't know what I was going to do: an article or a program. As you can see, the supporting script itself is actually too long to just dump inline into an article: https://github.com/eriksank/altcalconv/blob/master/altcalconv.sh. So, that is the reason why I have pushed it to github. At the same time, the README is indeed not really just a set of neutral installation instructions. It ended up turning into an opinion. I originally didn't know that I would end up doing that. I think that it may indeed make sense to republish at some point, if there is demand for this. Otherwise, if there isn't really, it can just keep sitting in that github repo while I just move on to other things. ;-)
I want the archive to look like this: file1.txt file1a.txt file1b.txt But I also want folders to ignore their parent folders. The `-j` flag doesn't seem to allow this for directories. So let's say I have these files: /Users/Username/Desktop/file1.txt /Users/Username/Downloads/folder1 /Users/Username/Music/song1.mp3 And `folder1` has other subfolders and files. I want to be able to select them and zip them up, and when you unzip the archive, you'll see this: file1.txt folder1 song1.mp3 Which is how Finder on Mac compresses files by default, and probably also how Windows compresses files. I want to be able to take any file from any folder as the parameter. So methods that `cd` into a directory and refer to basenames will not work (or at least, I think).
The `-i` parameter is changing an actual file on disk. It needs a file name mentioned on the 'sed' command line, meaning it has to be a command line like this: sed -i "s/string/replacement/g" "/path/to/document.txt" You can feed 'sed' more than one rule on its command line. You don't have to split your task over three calls of 'sed'. Choose one of these options here to do this, whatever you like best: sed -i \ -e "s/string1/$replacement_String_1/g" \ -e "s/string2/$replacement_String_2/g" \ -e "s/string3/$replacement_String_3/g" \ document.txt or sed -i "s/string1/$replacement_String_1/g; s/string2/$replacement_String_2/g; s/string3/$replacement_String_3/g" document.txt or sed -i " s/string1/$replacement_String_1/g s/string2/$replacement_String_2/g s/string3/$replacement_String_3/g " document.txt
sed -i 's/string1/${replacement_string_1}/g; s/string2/${replacement_string_2}/g; s/string3/${replacement_string_3}/g' document.txt
Apart from `ln -s --target-directory=DIRECTORY FILE1 FILE2 FILE3 ...`, `ln` can only do one operation at a time. I'm not sure where you're getting your file list, but `find` might be useful, for example to run a command on all the files in the current directory and all subdirectories: find . -type f -exec chflags uchg {} + #this will run a single chflags with all the files as arguments find . -type f -exec chflags uchg {} \; #this runs chflags for each file one at a time. Not preferred. With your array example: a=(file1 file2 file3) chflags uchg "${a[@]}" #all at once for i in "${a[@]}"; do ln -s "$i" "/path/to/symlink/$i" #one at a time done &gt;What about in cases wherein I need to know how many files encountered an error Just look at stderr, right?
Nice mission. What if tar nor rsync is installed? Unlikely, I know :) It's probably best to check each separately though, using the output of "which" or "type" instead of verifying the hardcoded full paths. Gives better results and more specific feedback. Good luck developing this further.
Thanks, very enlightening to read for a lazy scripter like me :)
Good point, I was thinking of that earlier, and tar is definitely an optional dependency in this case. I'll look in to a better way to check for dependancies too, thanks for the help!
 $ printf '' | awk 'END{print index("test","s")}' 3 And no, I think it’s simply not required to be implemented at all. EDIT: I looked into the PWB/UNIX tarball, and its `expr` manpage indicates that it already supported `index` – so it’s not, as I had suspected, a GNUism. Not sure why it’s unspecified, then. (The point still stands that you can’t rely on it being supported.)
&gt; If your issue is that the ABS needs to be fixed, why don't you help fix it, instead of blanketly trashing it. Only the author can fix it, and he won't. That's the reason the [BashGuide](http://mywiki.wooledge.org/BashGuide) was written in the first place; to offset the mis-teachings of the ABS. Besides, I find it more worthwhile to work on a guide and FAQ that's geared towards UNIX and UNIX-like systems in general, rather than focused on GNU/linux systems. &gt; Otherwise, come up with a shorter summary than what you have above, and something a little more informative than 'it's garbage'. Otherwise people like me will get annoyed with you for shitting on 90% good advice. That was just a few points of many. I certainly don't agree with 90% good advice. Will have to go below 50. &gt; Also, google hits and google rankings, are important. Most people looking for advice will only find the stuff you consider beneath you. They aren't going to find your bash wiki, because noone is willing to look that far, when something that 'mostly' works is readily accessible. &gt; So hate google all you want, but you have to do some SEO optimization for the bash wiki if you want more people to find it and reference it. My one mention of google was to point out that "google hits" and "quality" are not related. Somehow this means I hate google? come on... anyway, you further established the point by mentioning SEO matter more than quality. &gt; You also criticize my use of `expr`, but there is no equivalent for getting the index / position of a character in a string in your wiki. If you have an alternative, I'd recommend adding it to the string manipulation page you linked to. I pointed out `expr` is redundant in bash scripts, so indirectly I criticized your code yes. It can be done with multiple parameter expansions. Cumbersome, but mostly faster than forking a subshell to run `expr`. However, finding the index of a substring is usually a pointless task in bash. For instance, in some languages you might use an `indexOf` function to find the offset for the first `=` in a string, and then use that to split the string into a key and a value. In bash you'd just use the `read` command to achieve the same, so finding the offset would be a pointless step. If you can think of a realistic scenario for where it's needed, I'll be happy to provide one or more solutions using only bash. On a side note, your `expr` command is wrong btw. If you try it, you'll see. At least assuming it's meant for GNU expr. &gt; &gt; An advanced bash-scripting guide should focus on explaining bash features. &gt; Yeah, because using only pure bash will solve all problems. ABS goes over `sed` and `awk` as well, are you going to criticize it for teaching people advanced scripting, in addition to advanced bash scripting? Again you twist my words. I never said pure bash will solve all problems. Including examples of using standard utilities (like `cp`, `mkdir`, `sed`, `grep`, `awk`) in bash are of course useful in a bash guide. `expr` is not particularly useful, other than the reason I've already stated. &gt; I learned how to script with the docs at TLDP and I didn't stop there. That's why I'm /reasonably/ good at writing shell scripts. So did I.
It's a shame that TLDP.org doesn't, seemingly, allow revisions of the documents. There's a reason there's a 6th edition of Learning Perl, etc. It might be worthwhile to work together, as a community, to bring editions to ABS and other 'core' documents from the hay days. Not to be nit picky, but I'm not sure what you mean about my example of `expr` not working as expected; $ poop="test"; expr index $poop es 2 My points about Google are that by and large, people are lazy, and will find and reference the first thing that 'works'. If you want to improve the quality of what people find with Google, then you need to play the SEO game. Otherwise you have to be satisfied knowing most people will have to seek your attempts at documenting on their own, without the assistance of search engines. I also agree with the silliness / futility of `indexOf`, but I was not judging their choice of functions, just attempting to provide some simpler examples hoping to educate them on the lack of a need to maintain a large garbage bash library.
&gt; $ poop="test"; expr index $poop es 2 poop="test"; expr index "$poop" se 2 
&gt; note it may require sudo mode to install global node package Just NO!
It gave the same output for both `es` and `se`. Meaning it doesn't actually find the index of a substring
Thanks for the feedback, you have good points. Will update post later &amp;&amp; checkout provided subreddits
I don't understand the code used for this solution. Can you please explain?
 find -type f #is all the files in the current directory and all subdirectories. find -type f -exec echo {} \; #from where you ran find, this will run the command echo on each file with full path find -type f -execdir echo {} \; #for each file, cd into the directory the file is in and runs echo find -type f -execdir ln {} {}\ Link \; #Watch out, don't put a space before \ Link. This runs the command ln -s file file\ Link for each file inside each file's directory. Creating a 'file Link' symbolic link that points to 'file' in the same directory.
Whenever I type `find -type f`, I get this error: find: illegal option -- t usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression] find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression] Also, it seems like I wasn't clear. The collection of files I'm interested is just a subset of the files on the disk. So when I say: folder1/file1.txt folder2/file2.txt folder3/file3.txt folder3/folder3a/file3a.txt I'm only interested in symlinking those 4 files. But in reality, there're other files in folders 1, 2, and 3, like so: folder1 abc.pdf def.sh file1.txt ... folder2 file2.txt file22222.txt ... folder3 folder3a folder3a.txt aaa.txt ... folder3b folder3b.txt bbb.txt ... text.txt ... folder4 ... ... 
What's `shopt -s globstar` do?
&gt;however I'm hitting a few issues with it, specifically around the MySQL section By all means, don't post any error messages. Wouldn't want to help you or anything. :P
D'oh! It is complaining that I can not login as either the root or cowrie users. It is meant to login as root with a previously entered pasword, create the cowrie user and Database, then logout as root and in as the cowrie user and setup the db as per the .sql file on the cowrie github site.
Why is it that there seems to be a lot of interest in creating BASH libraries when we recently switched to systemd from sysv in part _because_ maintaining BASH libraries is difficult and lead to poorly maintained / difficult to maintain startup scripts? If people love creating BASH libraries so much, you'd think that would be a solved problem. But the reality is, it's neat for a PoC and personal project, but not at all useful to the rest of the world. Sorry to shit on your stuff, I think it's neat that you can do this, I think it's great that you learned how to build this, but if I ever need libraries and complex functions to write something in BASH, I am going to use a better more actively developed and extensible language, like perl.
If I understood it I wouldn't have had to copy it from stackoverflow. :)
You need to wrap all non-printing characters in your PS1 in `\[\]`, like this: PS1='${debian_chroot:+($debian_chroot)}\[\e[0;34m\]\u\e\[[0;35m\]@\[\e[0;92m\]\h:\[\e[0;96m\]\w\[\e[0;31m\]\$\[\e[0m\]'
Thanks for support. I was googling it for little while before I came here. It was my first time making and working with a .bash_profile and getting it to do stuff. It can be tricky googling a problem when your not really sure what you're doing in the first place let alone what the actually problem is. Prompt generators are cool and all but not always so helpful in explaining what was wrong in the first place. 
Neat - though i use a vim template file for it.
Not sure if you are looking for comments but first thing that jumps out is that there is no protection for overwriting existing files. Also, it's be useful to add basic variables like: myname=`basename "$0"` mypath=`dirname "$0"` and maybe even a basic getopts block.
You run `read` as the conditional command, and also run `read` inside the body of the loop. Also, use an array rather than a string to store multiple values, and use `read`'s `-p` to print the prompt, instead of `echo -n`. #!/bin/bash all_hosts=() while read -ep 'Enter hostname&gt; ' hostname &amp;&amp; [[ -n $hostname ]]; do all_hosts+=( "$hostname" ) done printf 'The hosts will be:\n' printf ' %s\n' "${all_hosts[@]}"
Thanks! That does seem to capture them all much better, but it makes my second part weird. When it processes this part: for i in $ALL_HOSTS do ping -c 1 $i done It does the first host, and then quits. No matter how many hosts are in there it only does the first one. This is exrta confusing as when it printf's all hosts earlier in the script, they are all there.
Good idea! I've not thought about that or really tried to use them! This is mainly why I'll be tossing things on here is to start conversations and see what people have to say or different ideas! 
It's an array, so you must [treat it as an array](http://mywiki.wooledge.org/BashFAQ/005). for hostname in "${all_hosts[@]}"; do ping -c 1 "$hostname" done Including all of the `"` quotes. If `var` is an array, `"$var"` is the same as `"${var[0]}"`. That's why you only iterated the first value.
http://shellcheck.net it's in the sidebar that way --&gt;
Oh man that is awesome! Fuck that's cool. Thanks man! 
Does it matter if variables are defined multiple times? ed wordpress-config.php &lt;&lt; EOF ?define('NONCE_SALT' .r salt wq EOF
See my post on this in the other thread [here](https://www.reddit.com/r/bash/comments/4m1vgm/not_100_this_is_the_right_sub_but_my_custom/d3sarp2).
http://bashdb.sourceforge.net/ comes closest to what you are looking for.
Not GPG but something like this maybe? for file in /files/*; do cat $file | openssl aes-256-cbc -pass pass:SOMEPASSWORD &gt; $file.encrypted ; done
Thanks for the tip! Might I ask why it's the preferred choice?
Will check that out! Thanks! 
Better not say the passphrase via arguments. If you don't, gpg will prompt for it via stderr, and you'll feed it in via stdin. That I think is a better way to go, because it won't show in the shell history, or ps like you said.
Grab the second word from the title of the current CNN home page and convert it to upper case. Create a file in /tmp with that value as its file name, and the contents of that file being the current date in YYYY-MM-DD format. 
Hi, you could search the issues tagged for "first-timers" or "up-for grabs" (not much in that case though, but still worth a look): http://up-for-grabs.net/#/tags/bash
You forgot quotes (`"$file"`, not `$file`) and I'd also drop the pointless cat. for file in /files/*; do openssl aes-256-cbc -pass pass:SOMEPASSWORD &lt; "$file" &gt; "$file.encrypted" done
Here's with the expr on OSX: $ expr index test s expr: syntax error
Google for gpg-agent
Thank you. The second one worked the best for me!
Why not use `tar` to group the files into a single tarball, then gpg encrypt that?
I'm doing them individually, not as a whole folder of encrypted files as one.
This is going to be pretty tricky to do with sed or awk - you're probably going to have to use perl, python, ruby, etc. user@host $ cat salt define('AUTH_KEY', '1jl/vqfs&lt;XhdXoAPz9c_j{iwqD^&lt;+c9.k&lt;J@4H'); define('SECURE_AUTH_KEY', 'E2N-h2]Dcvp+aS/p7X{Ka(f;rv?Pxf})CgLi-3'); define('LOGGED_IN_KEY', 'W(50,{W^,OPB%PB&lt;JF2;y&amp;,2m%3]R6DUth[;88'); define('NONCE_KEY', 'll,4UC)7ua+8&lt;!4VM+#`DXF+[$atzM7 o^-C7g'); define('AUTH_SALT', 'koMrurzOA+|L_lG}kf07VC*Lj*lD&amp;?3w!BT#-'); define('SECURE_AUTH_SALT', 'p32*p,]z%LZ+pAu:VYC-?y+K0DK_+F|0h{!_xY'); define('LOGGED_IN_SALT', 'i^/G2W7!-1H2OQ+t$3t6**bRVFSD[Hi])-qS`|'); define('NONCE_SALT', 'Q6]U:K?j4L%Z]}h^q71% ^qUswWgn+6&amp;xqHN&amp;%'); user@host $ cat wordpress-config.cfg begin wordpress config AUTH_KEY define('AUTH_KEY', 'put your unique phrase here'); define('SECURE_AUTH_KEY', 'put your unique phrase here'); define('LOGGED_IN_KEY', 'put your unique phrase here'); define('NONCE_KEY', 'put your unique phrase here'); define('AUTH_SALT', 'put your unique phrase here'); define('SECURE_AUTH_SALT', 'put your unique phrase here'); define('LOGGED_IN_SALT', 'put your unique phrase here'); define('NONCE_SALT', 'put your unique phrase here'); end wordpress config NONCE_SALT Here is an example using Perl user@host $ cat update-wordpress-config.pl #!perl my %config; BEGIN { warn "Usage: $0 salt-file wordpress-config\n" and exit 1 unless @ARGV == 2; open my $fh, '&lt;', shift; my $salt = do { local $/; readline $fh }; %config = $salt =~ /^define\('([^']+)',\s*'([^']+)'/mg; } while (readline) { for my $key (keys %config) { s/^define\('$key',\s*'\K[^']+/$config{$key}/ } print } user@host $ perl update-wordpress-config.pl salt wordpress-config.cfg begin wordpress config AUTH_KEY define('AUTH_KEY', '1jl/vqfs&lt;XhdXoAPz9c_j{iwqD^&lt;+c9.k&lt;J@4H'); define('SECURE_AUTH_KEY', 'E2N-h2]Dcvp+aS/p7X{Ka(f;rv?Pxf})CgLi-3'); define('LOGGED_IN_KEY', 'W(50,{W^,OPB%PB&lt;JF2;y&amp;,2m%3]R6DUth[;88'); define('NONCE_KEY', 'll,4UC)7ua+8&lt;!4VM+#`DXF+[$atzM7 o^-C7g'); define('AUTH_SALT', 'koMrurzOA+|L_lG}kf07VC*Lj*lD&amp;?3w!BT#-'); define('SECURE_AUTH_SALT', 'p32*p,]z%LZ+pAu:VYC-?y+K0DK_+F|0h{!_xY'); define('LOGGED_IN_SALT', 'i^/G2W7!-1H2OQ+t$3t6**bRVFSD[Hi])-qS`|'); define('NONCE_SALT', 'Q6]U:K?j4L%Z]}h^q71% ^qUswWgn+6&amp;xqHN&amp;%'); end wordpress config NONCE_SALT You can use perl's `-i` switch if you want to overwrite the original file. (be careful use `-i .bak` to create a backup when testing). So it takes the first file and builds a mapping of the variables =&gt; values then in the 2nd file it loops through the mapping making the replacements. Feel free to ask any questions, hope it helps.
&gt; "^[A-Z]{2}$" Nope. This tells: Begin String; One Capital Letter; One Capital Letter; Stop not what he's looking for
&gt;~~But does anyone know how to remove the lines that are only composed up of two letters~~ Ah, I see what OP means. Edited above. 
I think OP likely meant "any number of instances of only two particular characters." ...which may be a better job for `awk` than `grep`.
I don't know awk, so I did it with grep instead: grep -vE '^([A-Z])\1*([A-Z])?\2*(\1|\2)*$'
I don't think "grep -vE -B1" will do what you want. By the time the second line is evaluated the previous line is already written to output buffer.
Would you have an IMAP service I could use to test this out? The one I normally use kind of yelled at me for logging in via a non-traditional route. I think I might be able to do this in a single line, assuming I have something to test it against.
Thank you very much! I will be looking through this.
II was on mobile so didn't feel like trying to type this out.. Here's a solution using gawk since I don't think grep will work even though the /u/moviuro is correct. Unfortunately, awk patterns don't seem to support inline use of captured match strings so I implemented using a awk's associative array. gawk '/^&gt;/ {id=$0}; /^[A-Z]/ {delete ar; n=split($0,ch,""); for (i=1;i&lt;=n;i++) {if (!(ch[i] in ar)) {ar[ch[i]]=1} } if (length(ar) &gt; 2 &amp;&amp; length() &gt; 4) { print id; print } }' &lt;Name of file to process&gt; Sorry if this is hard to read but I thought you might want it as a one liner.
Ahh, no problem. I was a little curious as to how this could help someone. I gathered this could have been for some strange statistical analysis. This makes a lot more sense. Thank you for your challenge :-) I really appreciate it.
&gt; But does anyone know how to remove the lines that are only composed up of two letters? I assume you mean something like a line filled with only 2 of A or C or G or T The problem with using `grep` and `-B1` in this manner: grep -Pv '^([AG]*|[AC]*|[AT]*|[CG]*|[CT]*|[GT]*|[ACTG]{1,4})$' -B1 &lt;filename&gt; let's say we have two sequences, 1. Only A's &amp; G's (let's call this pattern 'p1') and 2. Only A's and T's (let's call this pattern 'p2') In case 1. we'll match your line 2, and because of `-B1` and the `-v` flag we ignore these two lines and print everything else out even those that should get failed by pattern two. i.e. You want to print something out when it's something like `!p1 &amp;&amp; !p2` but using `grep -v -B1` will give you `!p1 || !p2` Instead, can you try the following: perl -0777 -pe 's/^&gt;.*\n([AG]*|[AC]*|[AT]*|[CG]*|[CT]*|[GT]*|[ACTG]{1,4})\n//mg' &lt;filename&gt; The same thing can be achieved with sed: sed '/^&gt;/{N;/\n\([AG]*$\|[AC]*$\|[AT]*$\|[CG]*$\|[CT]*$\|[GT]*$\|[ACTG]\{1,4\}$/d}' &lt;filename&gt;
Agreed. Suckless' ii fits well as it outputs to a file by design.
Hi ;-) Sorry, I'm in Europe and went to bed right after posting. The shebang is sh, but the styles are very similar, so on IRC for example, we do both on #bash. This script collects lots of advertisement, tracking, malware server names and puts them in a unique file that can be used in a variety of DNS servers, such as unbound(8) or bind(8). This protects your users on your network and the devices that can't run uBlock (like an Android phone).
The google one? Or any other imap service. What is the traditional route then? If you use the imap specification it should be find.
I think you got me wrong. basically it's not a client, it's just a listener. and this is actually pretty simple by using telnet. Check this out: NICK=$1 #MYPASSWORD=$2 IRC_SERVER=$2 IRC_PORT=$3 CHANNEL=$4 ( echo NICK $NICK echo USER $NICK 8 * : $NICK sleep 5 echo "JOIN $CHANNEL" sleep 2 echo "PRIVMSG $CHANNEL" ":in" sleep 30 echo "PRIVMSG $CHANNEL" ":out" echo QUIT) | nc $IRC_SERVER $IRC_PORT that's it! ( for making one that posts two messages, the listener later greps for something and stuff.)
Note that this style enforces many more things that can't really be enforced in a sane and consistent way, like the 80-column limit or the need for a "main" function in long scripts. As to tabs v spaces, I really don't care either way. I went for tabs simply because of what is explained in the README - by objectively looking at the spec. I'm familiar with Google's style, but tabs are clearly preferred over spaces in the spec, even if just because of `&lt;&lt;-`. As said elsewhere, I'm of course not demanding that everyone use this style.
Did you write this knowing what web browser the OP is using? The OP doesn't name a web browser. You don't name a web browser. Does every web browser have "Dev Tools" and "Copy as cURL" that you suggest using? 
If you are using Firefox, install this extension https://addons.mozilla.org/en-Gb/firefox/addon/export-cookies/ Use it to export your cookies to a file. The page I linked to tells you what the wget option to load the file is, I'm pretty sure curl has an equivalent but not on a Linux box to check right now. curl man page or Google will tell you. I suggest that Firefox extension having used it myself several times to get a cookie file to use with wget. 
That's an odd thing to call out as making the Bourne shell odd. It's like saying that C is an odd language because the compiler outsources core things to libc. In the shell language, calling an external program is just calling a library function in another language.
If so, not by much. In UNIX V6 (the Bourne shell and `test` appeared in UNIX V7), `test` wasn't a separate program; `if` was a separate program (similar to how it is done in the `rc` shell), and `test` was effectively part of it.
Well, I don’t think there are many languages that outsource integer comparison and string tests to a library. (The file-related features are of course a different story.)
You may be right, found this and V7 is the earliest listed revision: http://www.in-ulm.de/~mascheck/various/test/ 
I checked snapshots of the V6 and V7 releases before posting :) Digging deeper into Dr. Spinellis' archives, based on the timestamps of the files in the first snapshots they appear in `test` predates `sh` by about 2 minutes :)
No. The shell literally has the sole purpose of running external programs. Anything more is extra. The V1 shell read a line, split on whitespace, and executed the command from /bin. You could change the current directory from in the shell, that was it.
Well, clearly the shell authors decided that’s actually too limited, and that `if`, for example, should be part of the shell syntax (rather than an external program). And then it indeed seems odd that `if` is built in but even the simplest conditions for it aren’t. (Bash, for example, supports `if ((arithmetic))`.)
`if` wasn't a built-in until V7. But that's a good example. `if` is part of the compiler/jvm, not part of `rt.jar`. So you are supporting my point.
I tried using Google, but that didn't help. Creating accounts on Yahoo and Hotmail have both become very difficult. I would try via a hosted domain, but I do not have one that I can currently test with unfortunately.
Why are you zipping movies? The codec is already making it as small as (conveniently) possible. Example, I just zipped a 210674 byte .jpg file and the resulting .zip is 210825 bytes. If you want to collect them in a single file for some reason, use tar (which has an -X switch to exclude from a file and various operations like --no-wildcards).
I don't like IDEs, but this doesn't make a terminal into an IDE. Click-bait title.
I don't think modify is the best event; `close_write` means I can `cat &gt;foo` and the command only runs on EOF. inotifywait -qme close_write foo | xargs -I_ sha1sum foo
That's some neat functionality in Firefox that you've made me discover. :)
I'm guessing the problem is somewhere else, somewhere in that "chromium-browser" script/program.
At least it's not this line of bash. Chromium seems to fail fetching the home directory, see [here](https://bbs.archlinux.org/viewtopic.php?id=184371) (Arch Linux forum, but relevant either way).
&gt; it doesn't require you to run every test in its own subprocess How do you make sure tests are not influencing each other?
You should make a function to handle the locking that write the current script pid inside the file and that is signal aware (trap is your friend). 
&gt; I'm all ears about how I might achieve that same effect using only bash builtins - any ideas? You can use bash's printf ... $ mtime=1464739384 $ printf '%(%Y%m%d%H%M.%S)T\n' "$mtime" 201606010203.04 &gt; As for getting rid of the other external commands... dd is essential. You can probably get it done without `dd`, but the code would be ugly, and probably slow. Still, as long as external tools are required (you can't get around mkdir and chmod for instance), I wouldn't call it a pure bash implementation. Not even "almost". 
The cleanest way I've found to loop through files with bad filenames is: find /path -type f -print0 | while read -rd $'\0' filename; do stuff_with "$filename" done `-print0` separates the file listings using a null character, and `read -rd $'\0'` tells `read` to use the null character as delimiter.
That's great! I didn't know about %(datefmt)T in printf. I'd be happy to include your name in the credits - how do you want to be remembered? I could just put in "Thanks to /u/geirha on reddit..." Edit: and how would you get rid of dd? The basic need is to read an arbitrary number of bytes at the specified offset into a variable.... and or into a file
Depening on `stuff_with` you could use `xargs -0 -n 1` instead of your own loop, though yours will work with aliases while the xargs one wouldn't
Keep in mind aliases do not work in scripts. If what you're doing is one command on the filename, then yes, `xargs -0` is a good option. However I tend to do many things with the filename, and tend to prefer the control a loop provides.
&gt; That's great! I didn't know about %(datefmt)T in printf. I'd be happy to include your name in the credits - how do you want to be remembered? *Shrug*, I have the same handle on github if you insist. &gt; Edit: and how would you get rid of dd? The basic need is to read an arbitrary number of bytes at the specified offset into a variable.... and or into a file By using `read`'s `-n` and `-d` options, and some math. The tricky part is that bash can't seek in a file, and it can't store binary data, but if you set locale to `C`/`POSIX` (and take other necessary precautions to avoid `read` modifying the data), you can store any bytes except NUL. You can however detect NUL bytes with `read`, and print them with `printf`. So with some long and cumbersome loop you can read up to *N* bytes to skip, then use another loop to read and print up to *M* bytes. As I said, it'll be ugly, and probably slow.
Yes it can be done with a for loop: IFS=$'\n' for x in $(find /some/path -name '*.vbox'); do VBoxManage registervm "$x" done But hacking $IFS is kind of fugly. _Edit:_ This breaks on filenames with newlines, but if you have those, you have bigger problems. :-)
I've updated it - date is gone, and at someone else's suggestion I quoted a bunch more variables that I missed on my first round through, and replaced \`foo\` with $( foo ). As for dd... I'm still trying to wrap my head around this. I try this: (copies $1 to stdout usage: ./try infile &gt; outfile) #!/bin/bash LC_ALL=C c="" off=0 IFS='' while read -r -N 1 c ; do if [ ${#c} -lt 1 ]; then # would only print when in the right range printf "\0" else # would only print when in the right range printf "%c" "$c" fi off=$(( $off + 1 )) done &lt; $1 which gets me ALMOST what I want. The problem is that there are two cases where read -N1 gives me 0 bytes, if the byte is 0, and if the byte is FF. I can't see any way to differentiate between these two cases. Ideas? And yes, you are right, this is probably 3 orders of magnitude slower than dd. I have dd in my target environment, but I always like a challenge. To read/copy a 17 MB rpm, this took more than 10 minutes.
Awesome - I thought it could be done but I ran out of mental steam by the time I got to that todo. I've incorporated your code. Thanks :) It is true that I have read many configure scripts in my day... but I changed mine to -z and -n - thanks for the suggestion. 
Incidentally, I can see how to set normal permissions on a file I create by creative umask setting, but bash's builtin umask wont allow creating suid/sgid/sticky-bit files... e.g. I can umask 0111 echo &gt; newfile ls -l newfile -rw-rw-rw- 1 foo sfoo 0 Jun 6 15:32 newfile but the leading 0 MUST be 0 so no suid/sgid extraction this way. I guess I stick with chmod. 
bash supports globstar `**` that matches path of any depth, which means it can replace the usual find lookup in most basic cases. for vm in /absolute/path/to/vms/**/*.vbox do VBoxManage registervm "$vm" done I think it works in interactive mode right off the bat and can be enabled in scripts with `shopt -s globstar` The advantage is that globs are first class and are not affected by IFS shenanigans used to parse shit out of plaintext at all, which means that you never hit the whitespace problems with them. `while read -rd $'\0'`/`find -print0` loop is the kosher backup.
Use the -n1 trick there as well, I guess. #!/usr/bin/env bash LC_ALL=C skip=$1 count=$2 while IFS= read -rd '' -n1 &amp;&amp; (( --skip )); do : done while IFS= read -rd '' -n1 c &amp;&amp; (( --count )); do if [[ $c ]]; then printf %s "$c" else printf '\0' fi done if [[ $c ]]; then printf %s "$c" else printf '\0' fi
nor can you set execute bit that way
I don't understand what you want to do. Perhaps put an "echo" at the start of the "ffmpeg ..." line while you are working on the script so that it will just print instead of running the command.
yeah I will leave IFS alone for now ;)
The things are all different. There's a bunch of them: &lt; &lt;&lt;some-end-marker ... ... some-end-marker &lt;&lt;&lt; &lt;( ) The `&lt;` redirects a file into the input of a program. The `&lt;&lt;end` ... `end` puts a piece of text into the input of a program. Bash will read several lines of text until it finds a line with that "end" on it, will feed all of those lines into the program's input. The `&lt;&lt;&lt;` is related to the `&lt;&lt;...`. It's a version of it without that end-marker thingy. You can for example do this here to put a line of text into the "bc" calculator program: bc &lt;&lt;&lt; "3 + 4" With the previous `&lt;&lt;`, the same could look like this: bc &lt;&lt; EOF 3 + 4 EOF The `&lt;( ... )` wraps around a program's output and wires it into a fake file. That fake file will be something like "/dev/fd/52". When you see `&lt; &lt;( ... )`, that's a program's output wired to that fake file, then that file redirected into another program.
Yes, personally I can't remember even one instance where hacking $IFS seemed like a nice approach.
Hi devosion, Well the script is going to do other things. Here's the nutshell version of what i'm hoping I can get it to do... 1) Connect to ssh 2) Enter password 3) Take input from user (string) 4) Based on that string, navigate to a particular folder 5) Grep files in that folder for that string 
You might want to ditch the password solution for key-based authentication (`ssh-keygen(1)`). Then you can have something like: #!/bin/sh ssh login@serv:port sh -c "my_command () { # your script goes here } my_command \"$1\""
You could also use sshpass. [](https://sourceforge.net/projects/sshpass/)
Hehe, that's wayyyyyy beyond what I need here. Have no idea what that even means. Just trying to write a little nothing script that the support techs at my company can run to save them a little time. 
GNU sleep does, as well as `.1s`, `2m` or `3.5h`.
You can't easily structure shell scripts to ssh in to another host and then run a series of commands like that; when scripting ssh, you generally run a single command through an ssh connection. Also, you're not currently using bash (you're using expect), and you should never ever be entering passwords into ssh like that; using keys is safer and easier. I would adjust your outline to instead be: 1. Prompt user for input 2. Use that input to construct a command to run on the remote server 3. Run the command on the remote server via ssh.
that helps, thanks
Who controls access to the server? I'd strongly recommend talking to them and asking for help/approval. There are so many ways this could go wrong that you won't have even thought of, and you could be fired for negligence if the servers are hacked because you didn't take the time to understand security. Edit: Downvote is for irrelevant/offensive comments, not things you disagree with. Why are you guys downvoting op when his comments are directly relevant to the problem at hand? 
I don't think so. Did you try it?
If it happens to be on a raspberry pi, GPIO no longer requires sudo if you're running Raspbian Jessie (thanks to `/dev/gpiomem`). 
`rm -f`
You may find this stackexchange discussion to be useful reading: http://unix.stackexchange.com/questions/26284/how-can-i-use-sed-to-replace-a-multi-line-string
Thanks, that fixed it, such a simple fix, just add one letter :P Do you know where I can learn about this stuff? A reference page for these **//** grep-like expressions and not for the regex inbetween.
O'Reilly's Sed and Awk book is THE book on sed. It's old, but still relevant.
See `man bash`: BUGS There may be only one active coprocess at a time. 
Well `//` is the same as the pattern of the `s` command - it just allows you to filter out lines to perform commands on. Like you can write `s/foo/bar/` as `/foo/s/foo/bar/` if you wanted to (you could also write it as `/foo/s//bar/` if you use an empty pattern it uses the last matched and since `/foo/` matched it's equivalent to `/foo/s/foo/bar/`) There is also the `/start/,/end/` syntax which allows you to filter/select a range of lines e.g. user@host $ echo $'foo1\nstart foo2\nfoo3\nend\nfoo4' foo1 start foo2 foo3 end foo4 So if you wanted to "grep" out a certain range you can do user@host $ echo $'foo1\nstart foo2\nfoo3\nend\nfoo4' | sed -n '/start/,/end/p' start foo2 foo3 end ... and if you wanted to perform an `s` command on that range only you can do user@host $ echo $'foo1\nstart foo2\nfoo3\nend\nfoo4' | sed '/start/,/end/s/foo/bar/' foo1 start bar2 bar3 end foo4 Notice how the `foo` outside the range remain unchanged. You can also filter lines out by number e.g. `sed '2s/foo/bar/'` only perform the `s` command on line 2 of input and you could also combine that with a `//` pattern match if you wished sed '3,/foo/{ s/^/newstuff/; s/$/moar new stuff/; }' Which would mean from line 3 until a line that matches the pattern `/foo/` perform these 2 s commands. You can use `{}` to group commands and `;` to separate commands. Grymoire has some good stuff regarding sed, awk, etc http://www.grymoire.com/Unix/Sed.html Eric Pement hosts the sedfaq and his infamous sed one-liners which may also be beneficial to your learning. http://www.pement.org/sed/sedfaq.html http://www.pement.org/sed/sed1line.txt
So, it's the reason why I implemented warningless coprocesses. Also, I dislike return values via global variables, like NAME_PID.
I'm not sure about your syntaxe... But that is another question. So about the question to add float... for me, you could not do it directly using bash arithmetic. Rather you should use some others programs/utilities, like for example "bc". I give some example : echo "1.5 + 0.3" | bc 1.8 So probably you shoud do something like : total= $( echo "$total + $read" | bc ) 
Instead of `MondayJune6.txt` you can use `current=$(date +%A%B%d.txt)` and then `$current`. When it's about many inputs I think you should consider a template with a file editor, it's easier in case you mess something up: current=$(date '+%A%B%d.txt') { echo 'Year: ' echo 'Make: ' echo 'Model: ' echo 'Job: ' echo 'Time: ' echo '(separate times by spaces)' echo } &gt;&gt; "$current" ${EDITOR:-vim} "$current" echo Each jobs time today: sed -n '/^Time/{ s/^Time.*: *//;s/ *$//; s/ /+/g;p}' "$current" | bc echo Total of all times today: sed -n '/^Time/{ s/^Time.*: *//;s/ *$//; s/ /+/g;H};${x;s/^\n//;s/\n/+/g;p}' "$current" | bc
Could you explain why [[]] is better than []? Or give a link please?
thanks!
&gt; EDIT: Welllll, it might not reach that point. This test would fail: [ -f /usr/lib/systemd/$1.service ] &gt; Also, run your stuff through shellcheck.net, there are some serious issues. There's a straight up syntax error, and you need to quote your variables to start with. Running this function with no parameter **will** work resulting in removing /etc/, so please don't do it. the reason is this: || [ $1 != "getty" ] basically as long as $1 doesnt equal getty it will run. the logic OR should be put inside the systemd evaluation. also, your example here is also dangerous because there's several inputs that would end up deleting /etc/ e.g.: / rm -rf /etc// I would do a full review but sadly I need to go now however if I remember when I come back I add more information :)
Oh sheeeyit, good point! I'm going to edit my post right now.
ls the dir and post here
Use quotation marks on both sides, or neither.
Are you expecting to be in that directory after the script exits? That won't work, since the script runs in its own sub she'll. 
What shell is your script running? Also, try cd $HOME/Documents
You shouldn't quote a tilda expansion. This won't work: cd "~/Documents" but these should work: cd ~/Documents cd ~/"My Documents" Inside a shell, you should use `$HOME` instead of `~`. If it's being passed in as an argument, it should arrive already expanded.
*tilde
Correct, this doesn't work for non-FHS filesystems. 
What...why would you do that instead of just using `$HOME`? That doesn't even make any sense.
What is the error? Also, you can do what this script does with the find command. 
Looks like /* just keeps getting appended to PARENT and FULLNAME forever. ETA: Which I fixed by adding: if [[ -d $x ]]; then to check if its actually a directory first. ETA2: Which has now broken it for the mv command. I might just redesign it now that I know whats going on better.
You might be thinking of Ctrl+Z, which will suspend the process and return control to the shell. But that won’t really help you checking the program’s status – it just allows you to resume the process (`fg` or `bg`). To check what the program is doing, `strace` or `lsof`, as suggested by /u/innodb, are probably more useful.
Now that I think about it, I might be thinking of an SSH command, which outputs the PID, elapsed time, etc. ....
Guess we should post shellcheck next ! :)
Removed, because it’s already in the sidebar and the poster hasn’t given any context for posting this link. (For example, I would welcome some discussion about whether this advice is good or bad, but that doesn’t seem to be OP’s intention.)
`*` is a *glob pattern* – it expands to all files in the current directory; `*.pdf` expands to all PDF files, `IMG*.JPG` to files like `IMG001.JPG`, `IMG002.JPG` etc. This makes it very easy to specify file names on the command line, but in your case that’s not what you want – you have to *escape* the glob with quoting (`"*"`, `'*'`) or backslashes (`\*`).
yes I use \* inside the script to do the actual calculation. But is there no way to make it so the positional parameter converts $2 to a string? So even if you do 4 * 2 it will not view the star as a pattern but as "*"
You have to escape the * either by \ * or '*'
You should use find command. I wrote a script that did what you are asking for, but i later deleted it, it took me days though, but so many anti-paterns
I know people are saying it's impossible but all you have to do is turn glob off before running the script. Create an alias like this: alias calculator_script='(set -o noglob; \calculator_script)' Now inside the script, the * could still be interpreted as a glob pattern so you'll need to put: set -o noglob Inside your script before you use the positional parameters. If you want to re-enable globbing then do: set +o noglob 
this works! However through set i could technically just reset the conditional paramters no? Something like: set $1 "*" $3 I thought this would work but it doesnt. Your solution works great though thanks! 
You might like to try something different like using (and/or building in optional support for) alphabetical arithmetic operators e.g. 1 a 1 10 d 2 3 s 1 5 m 3 addition, division, subtraction, multiplication. Or, very simply, swap '*' with 'x'
I think you'er right, bullet dodged. That's how my scripting works. Encounter a tedious/slow/repetitive problem, research intently for a few days writing snippets of code, debug, combine snippets, debug more, test with process, debug some more, test, put into limited production. Completely forget everything you've learned in the last process and start again with new problem. Keep applying!
So I put something together and this is the result, but how do I add in a 'switch' type of function? Where I can select the type of file it is and then it'll compile accordingly? #!/bin/bash echo "Please enter a filename" read FILENAME javac $FILENAME.java java $FILENAME
You know what they say, hiring a "lazy" programmer can be good, because they will have found the most efficient way to do everything.
You are typing the string's content into a script and simply want to make it easy for yourself regarding escaping? Or the content could be unknown and coming from an external source?
Common is `'string'` and if the strings contains a `'` then use `'\''` because backslash are always literal in single quotes, e.g. `'don'\''t'`. Control characters need `$'...'`, e.g. `echo $'ttring\rs'`. If you need to convert to an escaped string from another command, you could use printf's `%q` like this: $ touch a\ b "ab'c" $ printf '%q\n' * a\ b ab\'c 
Think you might be looking for a function. Can be added to `.bashrc` for persistence. `comprun () { javac $1.java; java $1 }` Then you should be able to do `comprun filename`. EDIT: oh, and if you only wanna run on successful compile you probably wanna do `comprun () { javac $1.java &amp;&amp; java $1 }` instead.
Thanks so much!
You shouldn't rely on locale-specific formats such as `%c` if you want to save a date and parse it later. As [the manual](https://www.gnu.org/software/coreutils/manual/html_node/General-date-syntax.html#General-date-syntax) notes, &gt; The output of the date command is not always acceptable as a date string, not only because of the language problem, but also because there is no standard meaning for time zone items like ‘IST’. When using date to generate a date string intended to be parsed later, specify a date format that is independent of language and that does not use time zone items other than ‘UTC’ and ‘Z’. [More specifically](https://www.gnu.org/software/coreutils/manual/html_node/Time-zone-items.html#Time-zone-items), &gt; Time zone items other than ‘UTC’ and ‘Z’ are obsolescent and are not recommended, because they are ambiguous; for example, ‘EST’ has a different meaning in Australia than in the United States. Instead you should store and parse the output of an unambiguous format, such as `date +@%s` or, if you _must_ store &amp; parse a human-readable string, `date --iso=sec`. You can also set `date_end` without reading the system clock twice (it's unlikely, but your script might not be scheduled for a measurable period of time) like this: date_now=$(date --iso=sec) date_end=$(date --iso=sec -d "$date_now 60 seconds") or date_now=$(date +@%s) date_end=@$((60 + ${date_now#@}))
I've found for most circumstances storing my date variables tend to fit into one of 3 buckets: -Things that should be stored as a unix-date -Things that should be stored as SAVED_DAY/SAVED_YEAR/SAVED_HOUR/SAVED_MIN vars -Things that are for humans to read, not computers /u/yrro's post below answers your question specifically, but it looks like the issue you were trying to solve that led to the question fits into the first bucket which can be unwrapped for human display with date -d @${MY_UNIXTIME} info date contains some pretty useful info on how/why this is the case. 
Thanks! I'll check it out. Sorry for not knowing the right thing -- I'm new to Linux coming from Windows. I just thought Bash would be easy to implement something to automate compilation and running.
Because I had simply used chmod +x scriptname.sh ./scriptname.sh And each script -- there are three -- would compile and run each program once given a name. I figured it'd be easier than me typing that over and over again. I'll definitely check out the makefiles! Are you well versed enough that I could go to you if I have problems? Ideally, I'd like it to be one script or makefile, where I type in the filename, it prompts me with something like "What is the file extension?" and then I can enter c, .c, C, or .java, or Java, or cpp and then it picks the appropriate file commands so to speak. For instance, it'd pick javac filename.java and then java filename if it was a Java file.
/u/DrFrix Thanks for the reply! Most of our information is confidential so Google is not an option due to their privacy stances. I admire many of their products for consumer use; however, we need the privacy and file permissions that Dropbox for Business (or similar) provides.
/u/dtbrough iCloud was definitely our first thought. Ironically, Apple suggested that we use another service since it's not really designed for business use (no ability to manage employee permissions). Thanks for the suggestions!
1) As someone who has a lot of pleasant memories of wordstar, and then wordperfect, i am happy to know that it's still has users. It was a really good editor, the last time i used it - which was quite a few years ago. 2) Git is a free, open-source distributed versioning system. This was initially developed by Linus Torvalds. The same chap who wrote the first version of the Linux Operating System. It is the de facto standard among programmers - especially those who work with open source code. Even companies like Microsoft and others are moving towards supporting Git in a major way. Though i don't work at dropbox, i would make a fair bet that Dropbox development team uses Git. :-) The only problem with Git is that it is meant for programmers who track changes to files across the world. What dropbox offers, in comparision, is a very simplified facade on top of such versioning. ( They probably use Git internally though ) As far as your user base is concerned, i would suggest Google docs - the only trouble with that is 1) Moving existing documents from dropbox to google. 2) Learning a completely new system for your user base. They already are going to be a bit busy learning how to use the Mac OS, though :-) Hope this helps.
You're just reinventing the wheel here. You'd probably be better off to use an established tool that is specifically for building Java projects, such as Maven or Gradle. Someone else already mentioned Make, but it's a more general tool that's more tailored to C/C++. Make probably won't offer much benefit over a plain shell script since it is not specific to Java; it won't do things like manage the downloading and installation of dependencies or automatically build Java specific things like jar files like Maven/Gradle will.
I understand completely. Best of luck!
I'd be curious to know what privacy and file permissions are available in Dropbox that are not there in Google (I've never used Dropbox for Business, legitimately curious).
/u/m1ss1ontomars2k4 thank you so much for your reply. You seem to have much more experience in this area than I do. I definitely value your opinion. As far as the privacy concerns: My understanding is that Google's income is mainly generated by the collection of customer data for the purpose of selling advertisements. Is this correct? You make a very valid point. Neither Google nor Apple products are widely used in business right now. Fair is fair. My apologies. Thank you for mentioning Google Apps for Work. I was unfamiliar with their business-class products. I watched a few videos about the admin console and it seems very comparable to Dropbox for business. My mistake. As I mentioned, the goal is to move away from Microsoft products. As a non-techie, I find it a bit insulting to feel "forced" into Windows 10. I don't like the way they treat their customers and would prefer to avoid 'locking in' for another 6-7 years. Thank you very much for your recommendations, though. I'm sure the only reason WordPerfect works is because it has these features built into its core software. When we moved to Dropbox from our file server, we were very (pleasantly) surprised that this feature still worked like usual. You make a very strong case towards this being a foolish idea. As you say, it may very well be "doomed to failure." :-( Thank you for explaining some of the complexities so patiently. I certainly do not hold myself as an expert. I apologize if I came across as arrogant. Mea culpa!
Yeah: $ grep -a -o -e 'GIF8' -e $'\xFF\xD8\xFF' -e $'\x89PNG' &lt;(echo RANDOM DATA; cat 000001.png; echo RANDOM DATA) PNG 
That thing between the `&lt;&lt;EOF ... EOF` lines is like a second script, running in a separate bash process. The variable you set there is existing only inside that second bash and is gone after it quits. It never exists in your first script. You could make that second bash process output text instead of setting a variable, then catch that output with `$( ... )` into a variable in your first script. Building on what you have now, it could be changed into this: external_IP2=$(sudo -u user2 bash &lt;&lt; EOF dig +short myip.opendns.com @resolver1.opendns.com EOF ) Or probably better this here: external_IP2=$(sudo -u user2 bash -c 'dig +short myip.opendns.com @resolver1.opendns.com') Or this here, without a second bash process: external_IP2=$(sudo -u user2 dig +short myip.opendns.com @resolver1.opendns.com)
There are five matches, 4 blank lines and a single a, because the extended regex `^a?$` (in words: match lines that from beginning to end that contain 0 or 1 `a`) is the equivalent of a combination of `^a$` and `^$`
I ended up using a variant of your script, along with jpegoptim, and I've already saved close to 200GB of space, which is more space than is allotted to some of our servers. Thanks again very much for your help and everyone else's help! 
Everything within the backticks \`...\` will get executed in a sub-shell and the output will be placed into RESPONSE. The backticks are an older syntax and should be replaced with $(...)
That's because the script is using command substitution. In Bash, this can be done by either doing `$(command)` or by using backticks \`command\`, as is shown in the aforementioned example. `curl` is executed with all of the arguments and its standard output (better known as `stdout`) is stored in `$RESPONSE`.
He isn't talking about a compressed archive, but concatenated data.
I don't quite get the point of pushing your IP address to your phone :&gt; You could use stuff like DynDNS, no?
I did not mean that. Your regex breaks down like this: ^ -- Start of Line a -- The letter "a" ? -- 0 or 1 previous character ("a") $ -- The end of the string You are anchoring with `^` and `$`, so it would not match a line has anything more than one "a". 
&gt;unary operator expected errors on line 28 &gt;`28. if [ $I -gt $HALFTEST ]` `$I` or `$HALFTEST` is empty. It should be useful to put `set -x` before the trouble lines and `set +x` after.
care to describe what the file is and the algo in a few words? I can't be bothered to reverse engineer the idea behind the code. and why are you using oldschool single [] instead of (( )) for tests and $(()) in case of integer related math? (( )) makes math readable and bearable. It becomes almost C. $ n=33; d=11; $ outcome=$(( n%d==0 )) $ echo $outcome 1 $ (( n++ )) $ echo $n 34 same deal with the inconsistent use of \`\` and $() 
yeh i'm working on cleaning it up. Some people here a while back fixed some things for me and they didn't use bash formatting sooooo I just haven't gotten around to fixing it and cleaning it up yet.
Yup, you could. But I think the script is clunky. those 3 variables (constants) aren't even needed. prime_test should be split up, some recursion added and the temp file dropped. Less code, more elegance. I could do that, just for fun, but I'm going to watch some netflix. edit: tyop
What would be cool is if you could type something in English and it translates it into cron. e.g. "Thursdays at 8 am, 2 pm, and 11 pm but only on even months and odd days of the month.
I want Android app of it 
As far as detox is concerned the string `ugyldig_koding` does not use characters with an illegal encoding. So just rename it without that string: `mv '07._Untitled_Instrumental-Rza.mp3-ugyldig_koding-' '07._Untitled_Instrumental-Rza.mp3'`
Not bash, but if you can install zsh: $ zmv '(*)\.mp3(*)' '$1.mp3'
I don't want an additional step. I want bash to not append anything to the filename.
This removes everything after mp3 for all files in the current directory: for file in *.mp3*; do mv -i "$file" "${file%.mp3*}.mp3" done
&gt; It's bash that "appends" this "illegal filename" string. Bash wouldn't do that. Maybe the mp3 tagging program?
 ~/wu/prod/etc/taste_the_pain$ ls -l totalt 42316 -rw-rw-rw- 1 root 1337 5097646 2000-08-08 00:56 01. Wu-World Order - Rza feat. LA the Darkman.mp3 -rw-rw-rw- 1 root 1337 6606804 2000-08-08 01:41 02. Rumble - U-God, INS and Meth feat. LethaFace.mp3 -rw-rw-rw- 1 root 1337 4443476 2000-08-08 01:14 04._Take_It_Back-Called_Back_To_36-Rza-Tekitha.mp3 -rw-rw-rw- 1 root 1337 3966464 2000-08-08 01:19 05.Wu-World_Order-Instrumental-Rza.mp3 -rw-rw-rw- 1 root 1337 4022178 2000-08-08 01:23 06. Rumble - Instrumental.mp3 -rw-rw-rw- 1 root 1337 2923548 2000-08-08 01:27 07._Untitled_Instrumental-Rza.mp3 -rw-rw-rw- 1 root 1337 2721976 2000-08-08 01:30 08. Untitled Instrumental - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 2581752 2000-08-08 01:33 09. Untitled Instrumental - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 552886 2000-08-08 00:56 10. Untitled Chamber - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 1041792 2000-08-08 00:58 11. Untitled Chamber - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 967298 2000-08-08 00:59 12. Untitled BreakBeat - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 510944 2000-08-08 01:00 13. Wu-World Order Acappella Snippet - Rza feat. LA The Darkman.mp3 -rw-rw-rw- 1 root 1337 4362722 2000-08-08 01:05 14. Untitled Instrumental - Rza.mp3 (ugyldig koding) -rw-rw-rw- 1 root 1337 3506354 2000-08-08 01:09 shaolin_temple__prod_rza__masta.mp3 I got these all over my filesystem. Don't know how it happened, but these files are really old; back in the days when I used Windows. Hmm, it does look like this is the actual filename, so somehow it was appended when I copied it sometime. Nautilus lists is as music files; does it use file to detect file type? I don't think so. Look here: http://ctrlv.in/774387 Here, I click rename inside nautilus and it somehow knows what's the actual filename. 
Please quote all parameter expansions as they will undergo word splitting: IFS="1" ./the_script.bash This will make the `$URL` among other split into two arguments for `curl`: curl ... '--message=...' 'https://api.pushover.net/' '/messages.json'
I never said that you _should_ do this kind of stuff, only that you _could_, and I stand by my edit: if you have filenames with newlines in them (or wildcard characters, or null bytes, fuck you Unix), you have way bigger problems than scripts breaking left and right.
Thank you guys. I think some of this I will have to rep out. 
Have you considered modding your nas e.g. https://aaronparecki.com/2011/07/10/1/enabling-ssh-on-the-seagate-blackarmor-nas-220
Take a look at the request that is sent with your browser's code inspector, such as Chrome's Inspect Element. You'll need to see what is going on in the POST request and try to duplicate it as much as is needed. If you could show a screenshot of Chrome's Network Tab for the request, we can get a better understanding of what is being sent in the request and understand how to duplicate it.
That looks like a "busy" indicator rather than a progress indicator.
I saw that, but decided to stick with using the web interface.
For java I recommend using ant. You can make targets to clean, build, deploy, run, and even generate documentation. While you can accomplish what you want with bash Ant was designed to do this.
http://stackoverflow.com/questions/2613662/display-directories-with-a-bold-font-how-to-enable-with-bash-profile
Thanks for the link but I have already seen that post. The top response seemed to answer the OP's and my question. However it did not work for me. I am on mac and when I run: which dircolors I get no output. I am thinking that is why the solution that you linked did not work for me. I don't have the coreutils on my system. I found these links which I will try later: http://www.conrad.id.au/2013/07/making-mac-os-x-usable-part-1-terminal.html http://hocuspokus.net/2008/01/a-better-ls-for-mac-os-x/ 
FYI, this is not a bash question, but a BSD `ls` question and/or a Mac OS X question.
Oh I'm sorry. I guess I thought it was possible to write a bash script to do it. I will look elsewhere. Thanks for the heads up
What resources did you use to come up with your guidelines? I've found the google shell guide useful and mostly stuff I agree with. There are a number of other bash boilerplate projects as well, although I'm less familiar with their content: https://github.com/alphabetum/bash-boilerplate#related-projects In particular I find it more useful to use bash's double braces for testing since they automatically prevent word splitting, so you can dispense with quotes (unless you're on the right-hand-side of an operator which looks for regexes). I also did the "always brace your variables" rule for a long time until I realized that it's more readable if you follow this slightly-more-complicated formula, always quoting the variable of course: - never brace the single-character builtins like $@ and $1 (always brace the multicharacter ones like ${10} though) - enclose the variable name in quotes directly, even if it's part of a larger (unquoted) string. the quotes play the role that the braces would have, effectively doing double duty since you need them anyway - only quote the entire string instead of just the variable if either: - the string has spaces in it or - there are multiple variable references in the string - if putting variable(s) in such a longer quoted string, only use braces if the character immediately following the variable name is also a valid identifier character, namely 0-9, a-Z or underscore These rules are safe and result in less visual noise. Also, you don't need to "export -f my_script" in order to run that as a command line function when you source your script, so the logic on that could be simpler.
Figured it out: `complete -f -X '!*.t' -o plusdirs prove` The `-X` means "exclude files matching this" so `-X "!*.t"` is "exclude any non-*.t file". 
Hm, I didn’t know that. The point still stands, though, since only `[[` gets special treatment for quotes: $ a="x y"; [ "$a" = $a ] &amp;&amp; echo ok || echo oops -bash: [: too many arguments oops $ [[ "$a" = $a ]] &amp;&amp; echo ok || echo oops ok
You should read about [word splitting](http://mywiki.wooledge.org/WordSplitting) and [quotes in generel](http://mywiki.wooledge.org/Quotes#When_Should_You_Quote.3F). Also consider reading `man read` especially the part about `-r` :-)
Also worth mentioning is [shellcheck.net](http://www.shellcheck.net/), which is a great linter for shell code.
 #!/usr/bin/awk -f /abc/ { if ( $1 == "location" ) { print } else { printf("%s\n%s\n", $0, "}") } } /^#/ { print } sed script /location \/def/{ N N d } 
If you can accept upper and lower case responses, you can make this change to ray_gun's code. [[ "${REPLY}" =~ ^[YyNn]$ ]]
There's nothing wrong with bracing all the variables. I do it. I don't find it visually noisy at all, in fact it makes the variables that much easier to spot for me when scanning. Quoting everything does add a lot of noise and is tedious, but we have to live with it.
I guess mine is a little wordy... function ask() { read -p "Yes or no?(y/n) " ANS case "$ANS" in [yY]*) yesfunction ;; [nN]*) nofunction ;; *) ask ;; esac } 
 confirm() { local ans IFS= while read -rp "$1" -n1 ans; do printf '\n' case $ans in [Yy]) return 0 ;; [Nn]) return 1 ;; esac done } if confirm "Are you sure? (y/n) "; then ... fi
Some notes from my first peek at this thing * enabling errexit and nounset is generally not a good idea; makes you write hacks to avoid the script unexpectedly failing. Especially if you want the script portable between 3.x and 4.x. * yet it doesn't enable useful options, like extglob * `.sh` extension on a bash script is misleading; sh != bash * Using `eval` on user input without validation * Using `[` rather than the more powerful `[[` for strings and files, and `((..))` for numbers * Using `read` without `-r` mangles the data * Using long pipelines like `echo ... | awk ... | sed ...` for simple string manipulation that bash can already do ...
I use logger too, but in accordance with twelve factor, we recommend apps of any kind (so scripts in this case) to use streams for logging and let the caller decide where those streams should go. In your case that would mean adding |logger to the production execution, but just looking at the stdio as you're still developing.
"Magic Variables" __os="Linux" Wait, what? You do realize there is `uname`, right?...
Indeed! We're testing against the different platforms automatically via Travis CI and it seems we can get away without running any commands, which we felt was better. Are there clear disadvantages to the chosen route you feel? 
I totally agree here. Besides `set -e` has some really nasty pitfalls because it also takes it's surrounding context into account when deciding if it should exit the script or not.
If you're writing code complex enough for it to nullify the advantage of exiting on routine errors, you should be using a better scripting language than bash. For nominal uses of bash, using errexit is fine so long as you test your code reasonably.
Not sure if you are just looking at examples to improve your bash or interface. We use zenity and/or dialog for a nice dialog box options.
I've not messed much with dialog and have yet to hear of zenity. I've mainly tinkered with whiptail which doesn't have that great of documentation, well not that I've found at least. Most of what I've been doing has been basic user input without going down the dialog box options, mainly due to lack of knowledge. I do intend to get there though, it would make things a lot more streamlined I imagine. Will be checking out zenity this morning and taking another look at dialog itself. 
So if I write some "simple" code, and decide to refactor it a bit by moving some parts of the code into separate functions, this task should be considered too complex for bash?
I use this as I like to be able to use a simple exit code sometimes. It's wordy but works well. ask-yes-no() { tty -s || { printf %s "shell must be interactive, bye" ; return 1 ; } # usage: $ ask-yes-no "Are you sure?" | do something with true|false # or: ask-yes-no "Are you sure?" &amp;&amp; command local msg="${1}" local answer read -p "${msg}: (YES or NO)" answer until false ; do if [ "${answer}" = "YES" ] ; then printf %s true return 0 elif [ "${answer}" = "NO" ] ; then printf %s false return 1 else printf "%s\n" "Please answer \"YES\" or \"NO\"" "All caps, then press ENTER" read -p "${msg} (YES or NO)" answer fi done }
There are some nice things I like about this one! It is a bit verbose but one of the things I like is this part. local msg="${1}" The customization for the question makes it a bit more versatile I think. I'm not quite sure what this bit is doing, care to elaborate? I get that it's checking for an interactive shell, I'm just not sure how or why? tty -s || { printf %s "shell must be interactive, bye" ; return 1 ; }
He probably sources this function in multiple places, and does work with non-interactive shells. He doesn't want to run this in a non-interactive shell during a script, or otherwise, so added this check. tty prints the file name of the terminal connected to stdin; tty -s makes it return an exit status only, either true or false, which evaluates based on whether or not a terminal is connected. So if it's true, he knows there is an interactive shell. Otherwise he bombs out and returns 1 from the function.
I'm not entirely sure what is being done with the IFS here. I'm vaguely familiar with how it's used but in this case I'm a bit lost. Any explanation?
It's not the `printf` that's failing it's how you are printing the `$BEGIN` and `$END` variables. I'm guessing you are using: echo BEGIN is now \"$BEGIN\" Instead try echo "BEGIN is now \"$BEGIN\"" 
Without quotes you are actually passing in 5 separate commandline args so unquoted version looks like this post variable substitution: echo BEGIN is now \" HELLO\" Note that the first escaped quote is actually the forth positional argument to echo and the HELLO\" is the fifth. Bash command line ignores multiple spaces between positional arguments. When echo gets the 5 arguments it prints out each argument and puts a space between each of the arguments so you end up with the output: BEGIN is now " HELLO" When you do this instead: echo "BEGIN is now \"$BEGIN\"" you are passing echo a single argument that has all spaces protected within the one argument.
An artifact of a particular use case by me. Probably should have removed it, but on the other hand, it's only one line that prevents it from being used incorrectly, and would cause a script to hang infinitely were it executed. "Impossible" is better than "not likely" and it doesn't hurt anything.
Kind of. $ type echo echo is a shell builtin But it behaves as if it were a normal executable (though iirc, there are some subtle differences in how GNU coreutils `echo` and GNU Bash builtin `echo` deal with flags and escapes).
Checking if stdin is a tty can also be done by bash (and even sh): [[ -t 0 ]] || { printf &gt;&amp;2 'stdin is not a tty\n'; exit 1; }
for `&lt;&lt;&lt;` http://askubuntu.com/questions/678915/whats-the-difference-between-and-in-bash shell redirections in general: https://github.com/learnbyexample/Linux_command_line/blob/master/Shell.md#redirection
Thanks for the great explanation! I had seen on stackoverflow that this method was better, but didn't get why until reading your explanation.
A colored prompt serves that purpose well enough, I think.
Don't put commands in variables. Either write a function, or put the arguments in an array. Using function: dump() { mysqldump --verbose --single-transaction --add-drop-database "$@" } dump "$db" | gzip &gt; "$db.sql.gz" Using array: args=( --verbose --single-transaction --add-drop-database ) mysqldump "${args[@]}" "$db" | gzip &gt; "$db.sql.gz" Oh and don't use uppercase variable names for internal purposes. You risk overriding special shell variables and environment variables.
 PS1="$ $(tput setf 1)"; trap "tput sgr0" DEBUG Or, more generally, append `$(tput setf 1)` (set foreground to color 1 – red) to your prompt (`PS1`), and add a trap that resets the terminal ([Select Graphic Rendition 0](https://en.wikipedia.org/wiki/ANSI_escape_code#graphics)) before each program executes.
I find it hard to trust a document that pipes ls into anything
can you please elaborate on that? will be useful to know there are two cases, can you give alternate to them as well.. * `ls -l | grep '^d'` * `ls -t *.txt | head -1`
Because Make is very old and static. I always write bash build scripts over Make scripts. Make does nothing special that you can't do with anything else
ls is a tool meant for the user to see what files exist. It is not meant for parsing. http://mywiki.wooledge.org/ParsingLs For the first: for file in ./*; do if [[ -d "$file" ]]; then printf "%s\n" "$file"; fi; done The second, you can do something like: for file in *.txt; do printf "%s\n" "$file"; done | head -1 # theres probably an even better way than this, but I have to leave soon # and this is better than ls anyways
sorry but neither does what was intended... * the first one just prints the directory name, which could as well be done using `find . -maxdepth 1 -type d`.. it won't show me long listing format * the second as well just does `head -1` on list of `*.txt` files, doesn't actually get the latest modified file as `ls -t *.txt | head -1` does from the link you gave, I get the fallacies.. for example, after I do `touch $'a\nnewline.txt'` , `ls -t *.txt | head -1` fails..
yes, as said, I had to leave soon ;) also, that is highly dependent on your OS/ls edit: on my system it does exactly as intended
 for file in *.txt; do [[ -e $file ]] &amp;&amp; printf '%s\n' "$file"; break; done files=(*.txt); [[ -e ${files[0]} ]] &amp;&amp; printf '%s\n' "${files[0]}" 
Hm, looks like I need to do less late night math :D but then, why does the `cat` version still only have a difference of 2?
&gt; also, that is highly dependent on your OS/ls Not really. POSIX actually specifies the output format of `ls -l` and the meaning of the `-t` option, so both of the examples should work on any compliant system (except that it should be `head -n 1` for portability).
I think he's more referring to things like GNU tar being "gtar" on MacOS or Solaris and "tar" on linux. Still, though, a function is better.
Better to avoid `set -u`. It's an inheritance from the old bourne shell, and doesn't really play that well with the newer shell features.
Try this way: if [[ "${1:-nope}" != "nope" ]] then ... fi set -u is great to avoid stupid mistakes, especially when your scripts become quite large. Search Unofficial bash strict mode 
[This page](http://wiki.bash-hackers.org/scripting/obsolete) has a good explanation of why 'set -u' and other commonly used bash-isms are bad/obsolete.
Commenting so I can come back to this later to improve one of my scripts 
Possibly recursion? Page a links to page b and page b to page a. You might want to skip already existing pages.
You're probably being blocked - you could try using `--random--wait` instead of `--wait=1` - not sure if it would make much of a difference though.
Instead of set -u if [[ -z $1 ]]; then $1="test" fi I'd suggest giving your variable a real name to make the code more readable and just assign a default value if ${1} is empty like this: set -o nounset variable=${1:-test} 
You could just use the [PDF version](https://upload.wikimedia.org/wikipedia/commons/c/c1/BlenderDocumentation.pdf)…
There may be better options, but you could always put the results in a temp file and use wc tot count the lines of the file
How about uniq... grep "hello" filename | sort | uniq -c If there is a date at the beginning then just awk the columns you want to search for... grep "hello" filename | awk '$1=$2=$3=""; {print}' | sort | uniq -c
If you're going to do that, there's no need to pipe into a temp file when you can pipe it / tee it / whatever. OP, I would probably do something like "grep foo files* | wc -l" there's no reason to grep again the 2nd time because you know all those lines are a match. you just need to count how many of them there are.
`grep [options] [pattern] [filenames] | tee outfile.txt` Tee copies the input it gets tot both stdout (the display) and a file `cat outfile.txt | wc` I'm not sure if wc needs an option tot display line count 
By default, `wc` prints line count, word count, byte count. `wc -l` for just the lines.
It's incomplete. That's why I took the website instead.
This is the only answer without a pipe, which I would consider a cleaner solution than any other answer. I'm stealing this.
What I like about this post is the fact that it highlights that you can do the same thing with various tools. The question that should be really be asked is, to phrase the question better. It's somewhat vague in the requirements. That said, I see people adding examples of extra pipes or commands that are totally unnecessary. For example, what's the point of "sort" if you're just going to "uniq" anyway when you want a count? Of course it'd give an alphabetized list, but if that's not a requirement that's a lot of wasted cycles. You can also sort after the uniq, to get the list sorted by the frequency. In short, there are plenty of ways (I like the awk comment -- but it is likely way overkill for the job... even though it does appear to be faster than the grep + wc method, since it's a single loop rather than having to re-iterate through the output.), and as with any other tools, learn them all, and use whichever is easiest for the task at hand, even if it may be a little less optimal. Worry about optimization later when you realize that &lt;THIS&gt; is a bottleneck and not something else that takes 100x more time.
`sort` comes before `uniq` because `uniq` only detects adjacent repeated lines. The man page also suggests `sort -u` as an alternative to `sort | uniq`.
Are you suggesting: if echo `ifconfig` | grep "^ppp.\+" | grep -q "Link encap:Point-to-Point Protocol" ? Tested it and no, it doesnt work. 
Here's a runnable example that you can copy and paste into your shell: if date | grep '^Wed.\+' ; then echo true ; else echo false ; fi It will display something like this: Wednesday 29 June 00:33:40 AEST 2016 true Use 'grep -q' to hide the date command's output.
But it cannot be applied to the code I have...
So.... if ifconfig | grep "^ppp.\+" | grep -q "Link encap:Point-to-Point Protocol" 
Yup, this seems to be working fine :)
Similar to another I comment I just posted, how is this &gt; if ifconfig | grep "^ppp.\+" | grep -q "Link encap:Point-to-Point Protocol" "working fine" ? If I run that bash sits waiting for further input until I press ctrl-c.
I only meant it may be overkill to learn awk for things that can be solved with "simpler" to understand methods. Of course awk is an awesome tool, it just has a steeper learning curve.
Maybe. But that would mean the OP has been posting single lines of their code which don't work on their own and expecting people to realise that's what they're doing and assume that they have some more liens which make it work. Which would be a fucking stupid thing if you're asking people for help. 
Mate, if you don’t recognize that simple problem or can’t fix it yourself, I wouldn’t count much on your help :P calm down, and be glad that OP got their problem solved :)
function d2ts { typeset d=$(echo "$@" | tr -d ':- ' | sed 's/..$/.&amp;amp;/') typeset t=$(mktemp) || return -1 typeset s=$(touch -t $d $t 2&gt;&amp;amp;1) || { rm $t ; return -1 ; } [ -n "$s" ] &amp;amp;&amp;amp; { rm $t ; return -1 ; } truss -f -v 'lstat,lstat64' ls -d $t 2&gt;&amp;amp;1 | nawk '/mt =/ {printf "%d\n",$10}' rm $t } Found that on stack overflow. Have not tested it myself. From various sources, though, it looks like +%s is not posix standard and why many UNIX variants do not include it. You have to write something in perl/Python/c/bash (per above) to do the conversion to seconds for you.
If your version of Solaris is newer, (/edit: quick test shows that Solaris 11 has this, Solaris 10 doesn't appear to) check to see if `find` (maybe the xpg4 version, or gfind if you're lucky) has the `-mmin` option. If so, use that. if find /path/to/files -type f -mmin -120 &gt;/dev/null 2&gt;&amp;1; then ... fi If you really must do date calcs, use epoch time, pretty much always. Date calcs suck in shell. You can use `truss` or `perl` or similar to get an epoch time for a file's last modified time, and you can refer to my post [here](https://www.reddit.com/r/linuxadmin/comments/4ozute/noob_bash_scripting_questiontime_difference/d4h40nh) for a variety of ways to get the current epoch time. Then do a simple bit of math and you're there.
Its pretty obvious there is more code below...Im not sure where the confusion is.
Another search so Im going to post is here since it is a grep search. ifconfig gives this output: eth0 Link encap:Ethernet HWaddr 00:11:22:33:44:55 inet addr:192.168.1.21 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::211:32ff:fe3b:b37b/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:967958 errors:0 dropped:0 overruns:0 frame:0 TX packets:665999 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:532 RX bytes:195697888 (186.6 MiB) TX bytes:168994702 (161.1 MiB) Interrupt:8 eth1 Link encap:Ethernet HWaddr 55:44:33:22:11:00 inet addr:169.254.37.113 Bcast:169.254.255.255 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:532 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) Interrupt:10 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:310320 errors:0 dropped:0 overruns:0 frame:0 TX packets:310320 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:37486970 (35.7 MiB) TX bytes:37486970 (35.7 MiB) ppp300 Link encap:Point-to-Point Protocol inet addr:192.168.100.214 P-t-P:192.168.100.163 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1400 Metric:1 RX packets:58298 errors:0 dropped:0 overruns:0 frame:0 TX packets:57585 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:3 RX bytes:3541998 (3.3 MiB) TX bytes:3488144 (3.3 MiB) Which is fine But I can do: if ifconfig | grep "^ppp.\+" | grep -q "inet addr:192.168.100.214" I want to make sure the address is indeed 192.168.100.214 for the ppp* interface 
Ok, awk then. if ifconfig | awk -v RS= '/^ppp/ &amp;&amp; index($0, " addr:192.168.100.214 ") { f=1;exit } END { exit(!f) }'; then printf 'There is a ppp interface with ip 192.168.100.214\n' else printf 'Something is wrong\n' fi
It can't be done with one grep alone since the two patterns are on separate lines. `grep` selects lines based on patterns. It can't do logic based on what surrounding lines contain.
Understood. So the code I posted above, based on what you posted, works fine?
Well no, you truncated the awk script. You need the whole thing. If you feel it's too long, you can split it on multiple lines, and/or put it in a function. I don't see the point in adding those redundant { ... } inside the then and else blocks.
So... if ifconfig | grep -A 1 "^ppp.\+" | grep -q "inet addr:192.168.100.214" ?
&gt; Well no, you truncated the awk script. You need the whole thing. If you feel it's too long, you can split it on multiple lines, and/or put it in a function. Lost me there. awk executes C code (AFAIK).... Ill give it a try at least.... if ifconfig | awk -v RS= '/^ppp/ &amp;&amp; index($0, " addr:192.168.100.214 ") { printf "I found 214"; } END { printf "I did not find 214"; }'; then { printf 'its up' } else printf 'its down' fi Is this correct?
yeah if second grep fails, `echo $?` should give you exit status of `1`
Something I have used: pattern-1.*\n +pattern-2 This looks for pattern-1, then continues to the next line jumps past the spaces and look for pattern-2
The `-A` option to grep is not specified in by POSIX but is supported by GNU and BSD versions of grep. As an alternative you can use awk: if ifconfig | awk '/^ppp.+/ { getline; exit !/inet addr:192.168.100.214/ }' This is not a drop-in replacement for the grep solution. This will also return true when there's no ppp interface reported by ifconfig.
I hope for you and your future co-workers, you pick up some better practices over time.
Thanks so much! I'm digging in.
Ah perhaps egrep? I use it inside a text editor but I am pretty sure I have used it on the command line as well. Found this: pcregrep -M 'abc.*(\n|.)*efg' test.txt (source: http://stackoverflow.com/questions/2686147/how-to-find-patterns-across-multiple-lines-using-grep) 
sorry there's one more line: a script called lssec that does the above output, something like: echo '########## LSSECFIXES ##########';cd /tmp/lssecfixes; tar -zxvf *; sh lssecfixes that is all what is outputed to the tee file; DUE DATE DUE SEV ADVISORY DESCRIPTION 
Several points; some of which have already been made: * When using read, always use read -r * Variables are for arguments. Command outputs go in arrays. * Find has a -exec flag to avoid doing what you're doing with $(). ( find . -type f -name somefile -exec gpg -c {} \; ) * You're not checking if the command was successful before removing the original * "${quote}" "${every}" "${expansion}" * You are doing a find for the file anywhere on the filesystem - but then doing an rm without specifying the path. Also your find could potentially end up encrypting any file sharing that name from / upwards. If you have to use find, do 'find .' for current folder instead of 'find /'. If you give more detail on what you're trying to achieve, I'd be happy to help more. At the very least, make sure you're quoting *EVERYTHING*. Your file variable, should be "${file}" as otherwise 'my file' gets split into two files, 'my' and 'file' and will break shit. 
You could also use a simple command group: { echo '########## DATA ##########'; date # etc. } | tee outputfile.txt It’s *slightly* more efficient because it doesn’t fork.
This is similar, but will return false either if the address is wrong or if there is no ppp interface, else true: if ifconfig | awk '/^[^ ]/ { i=$1 } i ~ /^foo/ &amp;&amp; $1=="inet" { a=$2 } END { exit a!="addr:10.18.2.14" }'
Is there a way to do this with symmetric encryption so that it loops through an entire directory for *.tgz files or from a variable. So I have a ton of .tgz files I want to encrypt every day, but I only want one 1 tar in one gpg file. So for example - How could I loop this so it just does all .tgz files in a directory. 25 input files, 25 outputs. Is that possible? gpg --batch --yes --passphrase 1234 --symmetric backup-2016-06-28.tgz
AH I GOT IT! encrypt2(){ for file in $folder/*.tgz do gpg --symmetric --passphrase your-passphrase "$file" done } Now - how can I do that from a variable which is full of file paths?
First, use "ifconfig ppp0", not "ifconfig" if you are concerned with just the ppp0 interface. You don't need grep -A/B ifconfig ppp300 | grep -q "inet addr:192.168.100.214" || do something 
download the GNU versions and install them so that you have them to run scripts which depend on them http://sunfreeware.com/
Mobile, but I suspect your issue is the shell scrip's [ ] tests comparing what you think are numbers but are be evaluated as hex or octal.
Yea, I think so too. So the zfs is failing to create snapshots I suppose. So what is the command in the script that create zfs snapshot? I want to try to run that command manually and read the errors. Also, why would it suddenly fail? It was running fine for the past 300 days and suddenly it fails? It doesn't make sense. 
As I am not a sysadmin nor a Zfs guru, I suggest you try /r/sysadmin 
Thanks, new lead and the right direction now. Much appreciated. 
Consider making this a script and not a 'one-liner'... #!/bin/bash declare -A data WRAP="##########" OUTFILE="outputfile.txt" data["DATE"]="date" data["HOSTNAME"]="hostname" data["IPs"]="ifconfig -a | grep -i 'inet addr' | cut -d: -f2 | cut -d' ' -f1" data["RHEL_RELEASE"]="cat /etc/redhat-release" data["ORACLE_RELEASE"]="cat /etc/oracle-release" data["LSB_RELEASE"]="lsb_release -a" data["KERNEL_VERSION"]="uname -a" data["LSSECFIXES"]="(cd /tmp/lssecfixes; tar -zxvf *; sh lssecfixes)" echo "$WRAP DATA $WRAP" | tee $OUTFILE for key in ${!data[@]}; do { echo "$WRAP ${key} $WRAP" eval ${data[${key}]} } | tee -a $OUTFILE done 
that would be nice if i was allowed :( to implement it!
interesting; thks
i will think of it...maybe it's a good idea for my case....several different customers with lots of machines....
Drop the ego and accept that you can learn better practices. I'm happy for you that it works as a duct tape solution.
 [[ -z $TMUX &amp;&amp; -s $HOME/.bashrc ]] &amp;&amp; foo Double square brackets are a bash thing. If you use other shells YMMV. I think i t might look more like: [ -z $TMUX ] &amp;&amp; [ -s $HOME/.bashrc ] But I'm not sure.
After looking at the issues filed against the project for this reason and the various solutions, I settled on adding this to my .tmux.conf: set -g default-command "exec bash" It replaces the default "bash -l" that tmux hardcodes to load the profile. You can see my shamelessly-plagiarized config here: https://github.com/binaryphile/dotfiles/blob/master/tmux.conf 
I use only bash. Thanks
ok, so as someone who looks after a shell based system information gathering framework, my advice is to try any way possible to not do it through shell. If you're not allowed to implement facter, try Ansible facts. If you're not allowed to do that, try cfg2html + grep. You have 7 checks only in yours so far, mine is at nearly 49k lines of code, and it probably needs to almost double. I don't say that to brag - I say that as a warning from a position of experience; to point out that while gathering system information in a script sounds simple, the reality is that it's anything but. If you can find a way to not do it, then don't do it!
Great blog post! It sent me on a goosechase to figure out what jstat and jstack did along with learning how the gc works. Eden, old, and permanent areas of the jvm. I'm sure I'll use that info in future!
what do You mean by portable syntax?
Syntax defined by POSIX, for cases where sh or other posix shells sources it.
You can put it in your ~/.profile or ~/.bash_profile. Basically, you replace that line you have that edits $PATH with the functions and then a line that runs `path_append folder1 folder2`. I have things organized differently and do most stuff in ~/.bashrc, and ~/.bash_profile has a line that loads ~/.bashrc like so: [[ -f ~/.bashrc ]] &amp;&amp; . ~/.bashrc Everything about when a certain file gets loaded is pretty confusing and I don't fully understand what happens with profile and bashrc.
I see, thanks, it works.
That also removes spaces, because `${array[@]}` writes spaces as delimiter. You can solve that with `[*]`, an empty `$IFS` and a quoted `${array[*]}` (also inside the `${...}`): $ url='http://www.foo.com/bar-baz/boo gie' $ remove=("-" ":" "-" "/" ".") $ IFS='' $ echo "${url//["${remove[*]}"]/}" httpwwwfoocombarbazboo gie $ echo "${url//[${remove[*]}]/}" #this removes spaces as well httpwwwfoocombarbazboogie
&gt;When you type in emulationstation on the command line and press enter you get an error "X is running. Please shut down X in order to mitigate problems with loosing keyboard input. For example,logout from LXDE". I'd just run retroarch in X. Anyway, instead of closing X, you could just go to the second virtual console with CTRL+ALT+F2. You can have systemd have tty2 automatically be logged in (change tty1 to tty2): https://wiki.archlinux.org/index.php/automatic_login_to_virtual_console And in .bashrc do something like: if [[ "$(tty)" == /dev/tty2 ]]; then read -p "Launch emulationstation? (y/n)" [[ "$REPLY" == [Yy] ]] &amp;&amp; emulationstation fi
&gt; Is this something that's possible in bash? Sure - you could use a for loop and Parameter Expansion. user@host $ ls folder1 file1 file2 file3 file4 user@host $ ls folder2 file1 file2 file3 file4 user@host $ i=-1; for f in folder1/*; do echo mv "$f" "${f%/*}/$((i+=2))"; done mv folder1/file1 folder1/1 mv folder1/file2 folder1/3 mv folder1/file3 folder1/5 mv folder1/file4 folder1/7 user@host $ i=-2; for f in folder2/*; do echo mv "$f" "${f%/*}/$((i+=2))"; done mv folder2/file1 folder2/0 mv folder2/file2 folder2/2 mv folder2/file3 folder2/4 mv folder2/file4 folder2/6 You can remove the `echo` if you want to actually run the `mv` commands. If you have the perl version of `rename` I suppose you would have to do something like: user@host $ rename -n 'no strict; s/[^\/]+$/($i+=2)-1/e' folder1/* folder1/file1 renamed as folder1/1 folder1/file2 renamed as folder1/3 folder1/file3 renamed as folder1/5 folder1/file4 renamed as folder1/7 user@host $ rename -n 'no strict; s/[^\/]+$/($i+=2)-2/e' folder2/* folder2/file1 renamed as folder2/0 folder2/file2 renamed as folder2/2 folder2/file3 renamed as folder2/4 folder2/file4 renamed as folder2/6 `-n` prints what would happen you can remove it to actually have it rename the files.
Why not rename, the Perl rename, so `Folder 1`'s (please avoid spaces in file name, or `rename` them away) names come before 2's for the same filename. `rename 's/$/a/' *` in 1's directory and similarly but for b in 2's giving `File 1a` and `File 1b`. They can then be put in one directory and `ls -v` gives them in order. A further rename could then make them page{1..42}.
Thanks I can give this a try. Looks more complex than I thought! Also, the second half of your comment displays wierdly in Reddit sync, but shows normal when I go to reply. The pearl section looks like it's on multiple lines.
Thanks for the reply! I will definitely not use spaces in the folder names, I've been learning since I've had so much time spent in terminal. Could you break down what's happening when you use 's/$/a' *? I know * is the wild card for which files we are looking at.
I dont know what is after ppp
Shit I thought of one even better.... ifconfig $(cd /sys/class/net; ls -d ppp* | head -1) | grep -q "addr:192.168.100.214" I think this would be even better and would give me 1 (or 0) results which is what I want, without any condition that would give me more than 1 result. 
Found a better workaround: ifconfig $(cd /sys/class/net; ls -d ppp* | head -1) | grep -q "addr:192.168.100.214"
I edited it and removed the command substitution. 
I’m tentatively flairing this as “help”, but OP, if you don’t explain what this post is about, I’m going to delete it soon, because I have no idea what this is doing here. EDIT: removed – OP, feel free to appeal by replying to this comment (or via modmail) if you disagree.
404
sed, awk, and bash could all do that, but it depends on what you need to do with the lines. 
I'm starting to wonder if we should add "Garbage" flairs for posts like this.
&gt; lines=(); for i in {1..10}; do read -r line; lines+=("$line"); done That part can be rewritten as mapfile -t -n10 lines
`echo` prints text and adds a newline to the end. You are printing some text that includes a newline, so echo is adding a second newline Either take out your newline echo -e $DATE &gt;&gt; /var/www/cam/list.txt or tell echo not to include it's default newline (-n) echo -ne $DATE'\n' &gt;&gt; /var/www/cam/list.txt
thanks brother, worked like a charm.
PLEASE SEE MY VIDEO
iz deletd
&gt; $ cat input &gt; JUNK JUNK &gt; Delimiter ... That input shows varying amounts of lines between each delimiter, not 10 lines like you specified earlier. Don't lie like that. People trying to help will quickly lose interest if every time they provide a solution, you go "that doesn't work because my input *actually* looks more like this". The solution requires a different approach now. An algortihm like this: For every line, 1. if line contains thing you need, set a variable, `found`, to 1 1. if line is a delimiter line, then * if `found` flag is true, run function on `lines` array * reset `lines` and `found` * `continue` to next iteration 1. append current line to `lines` array 
Don't pipe into `while`. You should use ShellCheck in the side bar which will tell you the same `Modification of xml_str is local (to subshell caused by pipeline).`. #Replace this line: echo "$lsof_str" | awk '{if (NR!=1) print $2,$3}' | while read -r pid user #with while read -r pid user #Then replace this line: done #with done &lt;&lt;&lt; "$(echo "$lsof_str" | awk '{if (NR!=1) print $2,$3}')"
Thanks, that solved my problem.
Apologies - no context given! Basically, I was showing how you could easily download the entire Chilcot report (very big news in the UK at the moment regarding the Iraq war and its aftermath) with just one simple command in your terminal.
Oops! I've given a bit of context, now, in the comments.
Ah, yeah, the URL wasn't supposed to be clicked on - I could have done this instead `wget -r http://www.iraqinquiry.org.uk/media/*` And then explained that the above code is to be run in a terminal (if you fancy downloading 2000 PDFs about the 2003 Iraq war!)
Hm. I still don’t see how that’s particularly relevant to /r/bash, and I personally don’t find it interesting… but we’ve allowed less relevant content than this before, and this is no longer spam. If you want to, you can submit this as a new thread, and I won’t delete it :)
&gt; &lt;&lt;&lt; What's this do exactly? 
Nice, thanks for sharing.
Don’t hard code terminal escape sequences – use `tput`, which reads the terminfo database.
Hi there, Author of the linked scripts here. Thanks for the tip re: `printf`. I'll see about replacing the `echo` statements soon (got a lot going on right now). While doing some research I found [this StackExchange answer](https://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo) particularly insightful. Can you elaborate on your views re: `set -e` and `set -u`? I do agree they make writing scripts potentially more challenging, but I'm of the view it's worth it. They effectively force you to code defensively and handle error conditions that otherwise could crop up with unpleasant results. Thanks for toning down annoyance factor. I saw your original reply and wasn't sure what to make of it (and wasn't going to reply). The scripts are only an attempt to provide a useful guide and/or starting point for those writing new scripts. I suspect I know more than most but (a lot) less than many, so suggestions are definitely appreciated to make them as "correct" as possible. Thanks!
Bash has `getopts` to handle options and option arguments. Here's a nice [overview](http://wiki.bash-hackers.org/howto/getopts_tutorial) Further down in the comments on that page is a link to an example on how to use [long option names](http://stackoverflow.com/questions/402377/using-getopts-in-bash-shell-script-to-get-long-and-short-command-line-options/7680682#7680682)
&gt; Can you elaborate on your views re: set -e and set -u? I do agree they make writing scripts potentially more challenging, but I'm of the view it's worth it. `set -e` will cause your script to abort on **some** failures. Some failures get ignored, some failures shouldn't have been considered failures. The rules for what is considered a failure changes very much depending on the context the code is in. If you write some code that works as "expected" with `set -e`, you can't just cut and paste part of that code into a function (in order to reuse that code multiple places) without also having to modify the code, because `set -e` may now trigger and not trigger on different things. Even worse is that these rules can change between bash versions as well. If you write a script that works in bash 3.2, it is likely to behave differently in 4.x when `set -e` is in effect. As for `set -u`. Why would treating an empty array as an error be a good idea? `set -u` was probably intended as a way to detect typos, but it doesn't do a very good job of it in modern shells. See [BashFAQ 105](http://mywiki.wooledge.org/BashFAQ/105) and [BashFAQ 112](http://mywiki.wooledge.org/BashFAQ/112) for more on `-e` and `-u`. &gt; They effectively force you to code defensively and handle error conditions that otherwise could crop up with unpleasant results. Do they? You'll really have fewer unpleasant results if you handle errors manually. Instead of `set -e`, test your scripts better. Instead of using `set -u` to "catch typos", use shellcheck. It does a much better job. &gt; Thanks for toning down annoyance factor. I saw your original reply and wasn't sure what to make of it (and wasn't going to reply). Yeah, sorry about that. I get annoyed by all these recent "templates" and "boiler plates" that follow the usual anti-patterns, but letting the annoyance end up in the discussion isn't very constructive. &gt; The scripts are only an attempt to provide a useful guide and/or starting point for those writing new scripts. I suspect I know more than most but (a lot) less than many, so suggestions are definitely appreciated to make them as "correct" as possible. I'd also recommend not using global readonly variables. They are not a good substitute for `const`/`final` from other languages. In bash, readonly variables disallows using those variable names even in local context. Try the following: readonly somevar=foo f() { local somevar=$1; } f bar Also, size. Bash is not a general purpose programming language. It shouldn't be used for everything. If a script reaches 200 lines, you likely should've switched to a more suitable language 100 lines ago. So a 200+ line template is something I'd never consider using. TL;DR: don't try to apply the paradigms you are used to from other languages. They likely don't work that well in bash.
just a fun fact. Architect linux, basically an installer for Arch Linux, is written in bash. It has roughly 3000 loc. This may not be best practice, but looking at the source code helped me learn a few things in bash 
Great reply. I'll properly parse the suggestions tomorrow when I'm actually awake. W.r.t. your last point on large 200+ line scripts, I'm completely sympathetic to this, but don't have a good solution. My issue here is if the bulk of the script is essentially calling system executables for management tasks, past experience is (re-)writing it in a programming language rarely results in a more concise implementation, and there's also the potential portability issues. For example, Python's pretty pervasive and in my view both expressive and easy to write, but you'll just end up with a ton of `subprocess` module usage, and many Linux distros still won't have it installed by default (or will have an incompatible version, or lack the `subprocess` module, and so on). I have a fair bit of code that's either doing initial provisioning work (e.g. to bootstrap something more full-fledged like Salt/Ansible/Chef/Puppet/whatever), or automating a Git workflow (i.e. lots of calls to Git). Re-writing this in something other than shell script doesn't seem likely to be an improvement. So I'm stuck with shell scripts while trying to make them as durable as possible. I guess there's Perl, but not sure if that's really an improvement. On a lighter note, the catalyst for me publishing this repository was cleaning-up some provisioning scripts for Salt Minions on our infrastructure. The official [salt-bootstrap script](https://github.com/saltstack/salt-bootstrap/blob/develop/bootstrap-salt.sh) is a ~6,400 line pure `sh` behemoth. There's no way I trust that thing enough to run it as `root` on anything important. I've already seen enough bugs in Salt's Python...
&gt; $ curl -sLo- http://get.bpkg.io | bash And some bad practices in that "setup script" to boot. No thanks.
I feel like you’re submitting this a bit prematurely if the [package guidelines](http://www.bpkg.io/guidelines/) are empty and there isn’t even a link for “How to create your own bpkg package from scratch”. On the other hand – one, I’m not sure who I’m addressing here (’cuz /u/speckz just posts stuff all over, I’m pretty sure they didn’t write this), and two, this was already submitted [a year ago](https://www.reddit.com/r/LinuxActionShow/comments/2h8cvm/bpkg_bash_package_manager/) to /r/LinuxActionShow, so it’s clearly not actually a new project.
Why bash?
Assuming that a majority of people either use the Downloads folder or use the desktop as their default save location, can we check for errors with this command sudo installer -pkg $(pwd)/Downloads/lpmacosx.pkg -tgt / and if that fails to execute, go to sudo installer -pkg $(pwd)/Desktop/lpmacosx.pkg -tgt / 
`pwd` is the command for `print the current working directory`. If the script is already in ~/Downloads/, your edit would translate to /Users/user/Downloads/Downloads/lpmacosx.pkg Do you understand? You could use the `find` command and execute this in one shot: find /Users -maxdepth 3 -name "lpmacosx.pkg" -exec sudo installer -pkg {} -tgt / \; However, if you have multiple files named "lpmacosx.pkg", find will run the `installer` command on all instances.
&gt; This does the job. Can Permission denied errors be suppressed? sudo find /Users -maxdepth 3 -name "lpmacosx.pkg" -exec installer -pkg {} -tgt / \;
You're welcome!
I have only experiences with bash but python/Perl is OK too.
That is not a good solution because it treats filenames as code. See [Bash pitfall 1](http://mywiki.wooledge.org/BashPitfalls#pf1).
This looks good. Oh yeah, hold on... I forgot to specify another condition. Okay, so the structure `/Users/user/Music/artist/album/song` holds for only 95% of the time. For the others, the structure might be `/Users/user/Music/artist/album/cd/song` or something like [ `/Users/user/Music/artist/album/song` + `/Users/user/Music/artist/album/folder/file`].
The first one should be fixed by changing the `/*/` in the outer loop to `/**/`. Not sure about the second one.
I'm actually as much as a gnu parallel geek as I am a fan of bash, so how about this: find / -type d | parallel /usr/local/bin/is_empty.sh This would be the script called: #!/bin/bash if [[ -z $(ls "$@"/*.mp3 "$@"/*.mp4)]]; then echo "$@ is empty."; fi 
It's called a here-string. Similar to a here-doc. It supplies a process data on stdin without using a pipe. 
Call your script with something like this; `/path/to/script; at now + 5 hours &lt;&lt;&lt; /path/to/script` e.g. Use the `at` command.
You can use cron to run the script at bootup if you start the line for it with @reboot
Your request implies you only want to run the script exactly twice on every boot. Is that true? If you want to run continuously put a "4. Go to step 2" in your pseudo code. 
Is there any reason that `sleep 18000` and a loop wouldn't solve this problem?
I assume the old filename should occur somewhere, probably like this: tablist "Aardvark_v1.0c_truth_des_rotated.${arr[i]}.fit [col PX; PY; PZ]" &gt;ixyz_$i.txt
Oh, I thought perhaps they wanted to make the numbers consecutive. So use the array value for the original name and then the index for the new name.
[removed]
In addition, the `-n` must be removed from the ssh command, since it tells ssh to ignore the input.
Oh, good call. Weird that OpenSSH doesn’t warn you more explicitly if you use `-n` and `-t` simultaneously – those two clearly contradict each other.
Unfortunately I was unable to get the script working with the original suggestion. How ever I was able to get it fully functional with making the change below for output in `cat ${serverlist}.txt`; do echo "$output" ssh -t -t -oBatchMode=yes -oConnectTimeout=1 "$output" "echo $pass |sudo -S $cmd" done
Err wait, it's `read` that should read the serverlist, not ssh.
Yes, that’s what my version does, right? `read` gets the serverlist, `ssh` gets original stdin.
Oh, so I read it right the first time, then the second time I read `done 3&lt; serverlist`, so yeah, nevermind. I usually do it the other way around; change `read`'s input.
I'm new to this stuff, and since I don't know php at all I figured doing a cron / curl script might be easier. Any suggestions?
This is the answer right here. Use python. It's got a requests module and a json module that will parse the json for you. Alternatively , if you really really want to try to parse JSON in bash, you could use something like jsawk: https://github.com/micha/jsawk
Gratz on the WA :)
If maintaining queue order is critical, I'd suggest you need a shared, persistent data structure that you're managing yourself. If you truly need something to indefinitely queue, you probably want a central management script monitoring the queue. If you have tons of scripts checking for their turn, you lose scalability. Have your work packages submitted by what you originally envisioned as the script doing the work and waiting, immediately terminating after submission (or block and structure your main handler to terminate on completion, if blocking is necessary.) When the main process has completed the item on the top of the queue, it clears it, and moves onto the next.
This is not a robust solution since it doesn't take into account comments and &lt; and &gt; characters inside quotes. It also wouldn't hurt to use more good practices, like quoting parameter expansions, using redirection instead of `cat`-ing a single file, using -r on `read` to avoid it eating backslashes, not use misleading extensions for commands, and perhaps most importantly, use the right shell in the shebang line.
Just a basic `flock` with a path to a global file will work fine.
If you want to write your own RSS reader in bash, then check out xml2: http://www.ofb.net/~egnor/xml2/
it's a string substitution. `${string##substring}` deletes longest match of `$substring` from front of `$string` So if `$user_displayN` is `"abc, fgh"`, `$user_Fname` will get the value `"fgh"`
Yes, this is exactly what happens. You can experiment with that stuff on the command line, for example: $ x='abc, def, ghi' $ echo ${x##*, } ghi
i was wondering if i could also trouble you asto what does the curly brackets do in this case? if [ "${user_displayN}" = "" ]; then
As /u/paypaypayme said, python. That's the language I'd use (mostly because it's the scripting language I know that's not BASH). Also, the thing you're trying to do seems pretty standard for Python so you should have a pretty easy time finding sample code online that you just need to modify.
Awesome, thanks so much!
This is definitely one of the best subs I've been on. People are super helpful.
Maybe don't try to block. Just run one instance of the worker and let it read tasks from a queue. You can imitate a FIFO queue with a combination of timestamps and file name convention. enqueue.sh - arguments passed to enqueue.sh are written to a file: test -d queue || mkdir queue ARGS=$* TIME=$(date "+%s%N") TNAME=.${TIME}.args FNAME=queue/${TIME}.args echo $ARGS &gt; $TNAME mv $TNAME $FNAME dequeue.sh - the worker does an ls to get the oldest task: while true; do DEQ=$(ls queue | head -n 1) test -z $DEQ || { # work to be done - in this case cat cat queue/${DEQ} &amp;&amp; rm queue/${DEQ}; } sleep 1 done 
Depends on your router model.
hmm probably?
Well, you can probably enable telnet or SSH access to the router and then reboot it via that connection, but you’ll just have to google for how to do that on your particular model.
yeah....provide more details of COURSE. 
Possibly a job for expect? Write yourself a little script that logs in and reboots it? But this smells. Why do you want to reboot your router from a script? If one of my coworkers checked in a script that did that I would have a ton of questions for that code review. 
I did something similar to this recently on my laptop running FreeBSD. On there I used devd to define an action that would happen when the lid state reported closed and another when it reported open via acpi. Under Linux it will depend on your platform. In Ubuntu and Fedora (that I've checked) you can add a script to /etc/pm/sleep.d/ similar to the following example (replacing wlan0 with your wireless device name if it's different): #!/bin/bash case "$1" in suspend) ifdown wlan0 ;; resume) ifup wlan0 ;; *) ;; esac I hope that helps. **EDIT**: I just re-read your post and realized your wording was "disable networking". If you meant "disable your wireless" the above example will work. If you need something more "complete", you can replace `ifdown wlan0` with whatever command you're running to disable networking manually and then `ifup wlan0` with the inverse command. Sorry for the confusion. If that's still not exactly what you're looking for, provide some additional detail and maybe we can find something that works.
thanks dude. I'll look it up. I am clearly doing something wrong though otherwise I wouldn't get the "syntax error near unexpected token `else'" error on line 37 when trying to run it 
Hi! I took the liberty of cloning your code and making some modifications (tidying it up, etc.) and noticed you are missing "done" lines after your "do" to finish off the block. I have also changed your "else" then "if" lines into an "elif" which does the same functionality (minus needing to have an extra line or two) in the code. Let me know if you have any other questions about it. Pastebin: http://pastebin.com/eE4vr5LQ
thank you. I think I found out how to remove a whole bunch more
the `\b` won't work as `.` is also a word boundary like space from your example, one way to solve is to use: sed -i 's/ bob/ wew/g' also, you can use sed -i.bak 's/ bob/ wew/g' to create backup of original files, incase something went wrong and you need to get back original copy or just to compare original and modified files
Did you have a command you were running manually to disable the network before suspending? The first command that comes to mind would be `systemctl stop networking` and `systemctl start networking` but on newer systems, particularly Ubuntu, I believe this is no longer considered the proper way to stop and start networking.
Question: isn't it better to use $UID instead of `id -u`? https://github.com/jgmdev/stopspam/blob/master/src/stopspam.sh#L54-L56 (bash noob here)
speckz just spams reddit with (mostly low quality) links, I've never seen him actually participate in a discussion on one of his postings.
$ is probably what's fucking you up here. 
Interesting idea. Admittedly I have not read all of your code but based on the description in github, I'm not sure how effective your firewall rule will be against most MTAs. Yes it will block incoming connections for a short period of time, but it won't do much to impact how much spam you receive. Here is why. 1. Our MTA views a non-response from the recipient ISP (in this case *you*) as a deferral, so the email we're trying to deliver will be queued and tried again later. In our logs it just shows "unable to connect". This can be due to *so* many different reasons that it isn't practical to take any action on the MTA level. 1. Our deferral queue waits longer and longer before retrying, so the first few retries will hit within that first 10 minutes but after that our wait time will exceed your timeout, so you'll still see retries from our MTA for 2 days. 1. Our MTA has multiple IPs (some clients have hundreds of IPs that they rotate through) that can span multiple subnets. There is a good chance that blocking on IP alone will only block a small portion of their IP space. 1. Once the email finally does expire after being blocked by your firewall for 2 days it will bounce in our system, and but again it won't have a meaningful message tied to it, so we (as the MTA administrators) don't have any reason to take any action to remedy the issue. It looks like your MTA is offline... not our problem. I know that this is 100% different direction, and is not a solution that covers multiple MTAs, but for longer-term management I think it would be better to build a tool that rejected on the SMTP level, and provided the remote MTA with a rejection response. As an administrator of an MTA this kind of data in the SMTP transaction gives me an actionable response, and puts the responsibility back on me. Not everyone will do something with this, but those of us who will can use this data to know whether one of my client is violating an AUP and gives me grounds to shut them down, and for those who don't take any action based on the SMTP response aren't going to fix it (and will presumably remain blocked so they won't be able to deliver mail to your MTA anyhow). It would also be more effective to look at more than just the IP address (which are very easily changed). Looking at the source domains (in the PTR of the IP, in the `HELO`, `Return-Path` header, DKIM, and From Address Domain) and making sure that they all match, and by checking the content, DKIM and SPF authentication (as well as DMARC alignment) -- blocking / allowing based on any or all of those will give you a much more comprehensive protection against spammers. TL;DR -- simply blocking at the firewall does very little to help admins solve problems on their network. Instead, consider rejecting during the SMTP transaction with a meaningful response, and base the rejection on more than just the IP. 
https://github.com/koalaman/shellcheck/wiki/SC2006 Don't use uppercase vars unless you know what you're doing Use printf instead of echo Use [[]] instead of [] I feel like we need a bot to keep saying these things...
So check what port the server uses, I bet it is not using 21.
 grep -il first * | xargs grep -il last
Either of these should work: grep -rF 'YourFirstName' | grep -F 'YourLastName' grep -r 'YourFirstName.*YourLastName\|YourLastName.*YourFirstName'
I didn't know that bash initialized this variable, I would say we never stop learning, we are all noobs :)
I thought about that as an after thought, honestly. Oh well, at least it's in a functional state. Also, could further be reduced by removing the "echo \`command\`" and just putting "command"
It IS a specially reserved character in bash
Ha! Well, maybe it is telling of the industry I work in when I read "spammer" and immediately think email. I can't really speak to the forum spam block, but we have effectively deployed fail2ban across our entire hosted network to help manage those script / bots trying to brute force their way in. If you haven't checked that out before it is well worth looking into. Good luck!!
Thanks for the reply! Where do I put the path to the directory I want to search?
It'll be &gt; grep -i 'firstname' /path/to/file | grep -i 'lastname'
How about something like this? function mysplit() { local max= if [ "$1" == "-C" ]; then max="$2" shift 2 fi local pattern="$1" local file="$2" grep -A1 "&gt;$pattern" "$file" | { if [ -n "$max" ]; then ( head -c "$max" ; echo ; ) | grep -v "^$" else cat fi } } $ mysplit sequence1 data.txt &gt;sequence1 GATTACAetc $ mysplit sequence2 data.txt &gt;sequence2 ACGGTCCAGTalienswerehereACCGT $ mysplit -C 20 sequence2 data.txt &gt;sequence2 ACGGTCCAG It could use better argument parsing and validation, but is that what you're looking for?
&gt;ACGGTCCAGTalienswerehereACCGT Goddamned Engineers... `wc -c` is probably the test you want to use. I thought about `head -c` but `wc -c` should be more portable. Example: # cat gattaca &gt;sequence1 GATTACAetc &gt;sequence2 ACGGTCCAGTalienswerehereACCGT # grep -A 1 "&gt;sequence1" gattaca &gt;sequence1 GATTACAetc # grep -A 1 "&gt;sequence1" gattaca | wc -c 22 Obviously `grep -A` won't be portable either, replace with `sed` or `awk` to suit if portability is a goal. I think possibly the intermediary store may wind up being required, but if you use an array instead of a file, it should be fairly quick. The other option is to just blindly split everything out and then if the option you want to provide has been set, you could run a cursory `find /path/to/split/files -maxdepth 1 -type f -size +"${bytesMax}"c -exec rm {} \;` Something like that. I wouldn't go for the `find` option, myself. But if you get absolutely stuck for options, it's there...
Wow, this is mad cool.
from the [github](https://github.com/taviso/ctypes.sh) &gt; Here is what people have been saying about ctypes.sh: &gt; &gt; * "that's disgusting" &gt; * "this has got to stop" &gt; * "you've gone too far with this" &gt; * "is this a joke?" &gt; * "I never knew the c could stand for Cthulu." &gt; I was just remarking the other day to a friend of mine that PowerShell beats unix shell scripting hands-down in 2 areas, one of them is that you can use any .NET library function or object from the script. This narrows the gap down considerably. I'm considering reworking some of my scripts that "interpret" complex text structures from command line tools to using this. I mean seriously, the only way I can get the IP address is to regex the heck out of the output of `ip addr`/`ifconfig`??
afaik FASTA files are not necessarily two-line sets, I've seen sequences wrapped to many more lines for readability.
Looks like a bot to me.
Very nice! You could eliminate all references to suffix by changing the last line to: printf("&gt;%s",$0) &gt;&gt; FILENAME "." int(size/max) }' dna.txt
&gt; I'm open to feedback... I feel I've used double quotes too much, I fear that I've stuck to a PHP style instead of Bash, and I'm not sure about my decision to move the plugins to their own files. There's a few places where you've quoted things that don't need quoting, but that doesn't really hurt, but you haven't used double quotes too much. You need more of them. e.g. with `main $@`. Never use `$*` or `$@` without them being enclosed in double quotes. &gt; read -p "$@ [y/n]: " response This only works if your function gets 0 or 1 or arguments. You do not want to expand the arguments as multiple words here. Either use `$1` or `$*` instead. &gt; Also, I'm pretty sure I need to add .sh suffix to the files. No. Don't use extensions for commands. And especially not .sh when the script is not an sh script. If you insist on an extension, use .bash since that isn't misleading at least. Some other notes about your code: * New scripts should not use `echo`. Use `printf` instead. * Don't use the `[` command in bash. Use `[[ ... ]]` to test strings and files, and `((...))` to test numbers. * Many functions use variables that should've been declared local * `currpath="$(pwd)"` You already have the current working directory in the `PWD` variable. No point in running the `pwd` command there. * `currpath="$(cd $currpath ; cd .. ; pwd)"` - never use `cd` in a script without checking its exit status. You don't know what directory you ended up with now. `currpath=$(cd "$currpath/.." &amp;&amp; pwd) || exit`. There's more, but I don't have time to point them all out right now
Try http://shellcheck.net. It's good at finding bugs like that.
Use functions and for the love of God, check your return codes.
If you have bash 4.3, you can use the new `wait -n`, which waits for any process to exit. #!/usr/bin/env bash if ! (( BASH_VERSINFO[0] &gt; 4 || BASH_VERSINFO[0] == 4 &amp;&amp; BASH_VERSINFO[1] &gt;= 3 )); then printf &gt;&amp;2 'Sorry, this script requires bash 4.3 or newer\n' exit 1 fi num_procs=10 # number of process to run in parallel avg_88() { local n=$1 dir=$2 printf &gt;&amp;2 'Job %03d processing %s\n' "$n" "$dir" python avg_88.py "$dir" "$n" printf &gt;&amp;2 'Job %03d done. Exit status:%3d\n' "$n" "$?" } i=0 for dir in /project/s625/xxxxxx/*/; do [[ -e $dir &amp;&amp; $dir != */@(foo|bar)/ ]] || continue if (( ++i &gt; num_procs )); then wait -n # Wait for any job to complete fi avg_88 "$i" "$dir" &amp; done # wait for the remaining processes wait See the [process management](http://mywiki.wooledge.org/ProcessManagement) page at the wooledge wiki for more on the subject.
 sed -n 's/Squid-\([^-]*\).*/\1/p' Are you sure there isn't a way to get rpm to output just the version?
Use cut with "-" as delimiter... See "man cut". Without looking at manpages; cut -d" - " -f2
Then you'll have to choose between one of the other suggestions at that page. [I want to process a bunch of files in parallel...](http://mywiki.wooledge.org/ProcessManagement#I_want_to_process_a_bunch_of_files_in_parallel.2C_and_when_one_finishes.2C_I_want_to_start_the_next._And_I_want_to_make_sure_there_are_exactly_5_jobs_running_at_a_time.)
/u/geirha is right: Rpm maintains a database that has all the information you want, and then some. This should do the trick: rpm -q --queryformat '%{VERSION}\n' squid Obviously I should've replied with an RTFM or a LMGTFY, but it's too hot today. Here you can find quite a bit on rpm: http://www.rpm.org/max-rpm/s1-rpm-query-parts.html
That worked perfect! Thanks!
Ok I managed to install parallel and tried to use the sem suggestion, here is the code #!/bin/bash i=0 for dir in /xxxxxx/*/ do ( echo "$i" ) name=$(basename $dir) [[ $name =~ ^(xxxx|xxxx)$ ]] &amp;&amp; continue sem -j10 python avg_88.py $dir $i ((i++)) done sem --wait It doesnt seem to do what I intended, the python script isnt being passed 1 then 2 then 3 ... 10, running it for those 10 then doing 10-20.
Already did. It showed nothing.
Yes it does, it shows a lot of bugs with that code. The one that bites you is that the heredoc you start on line 25 is never closed, because line 37, where you intended to close it, is indented. If you had fixed all the issues shellcheck.net mentioned, your "missing `fi`" error would go away too.
Get rid of the quotation marks. 
Try a sleep command in between each command
Something I found recently is the "subprocess" $( &lt;code goes here&gt; ) &amp; The code in that section is run independently in the background. Example: https://github.com/mridlen/wakeup-watcher/blob/master/wakeup.sh Hope that helps!
This is what I did originally but it didn't seem to work.
I usually use xargs - this does mean writing two scripts but other than that it's very versatile. script1.sh while [ ${#} -ge 1 ] do echo "working with ${1}" python avg_88.py ${1} shift done script 2 / manual execution: find /xxxx/ -type d|xargs -P 8 -n 1 /path/to/script1.sh This would execute /path/to/script1.sh in 8 parallel threads as long as enough input is available. -P is the number of threads and -n is the number of paramters per execution. This would perform avg_88.py on all directories in /xxxx/ - I'm not quite sure why you would need the number of iterations to be passed to the python script - if that is a required option that obviously wound't work for the above example, however the xargs method is fast and easy and available on most if not all unix systems you may encounter.
You should probably share the particular errors. You can run bash in [debug mode](http://stackoverflow.com/questions/951336/how-to-debug-a-bash-script#951352) to get a better sense of what the script is doing, but since you're just calling external commands you should probably try to isolate the errors instead. Comment out all the lines, then "activate" them one by one and see what throws which error.
If we're playing golf: find / -xdev -regex '.*/\(tcpdump\|etheral\|wireshark\)' -exec ... {} +
However if you leave out `function` it'll work with any POSIX compliant `/bin/sh` — assuming you've managed to avoid other bashisms.
I knew there was a reason I preferred the `mysplit()` form. Thanks for reminding me. :D
Hello everyone! LagDrop is a router script to block potentially laggy peers for console games. Largely inspired by Smash Bros for Wii U, just assign your console to a static IP address, and run. Follow instructions to adjust settings. Looking for feedback and testers. Questions welcomed! Thanks! 
You probably want to do the validation first, before checking if it is over or under 18. You could nest the over/under 18 logic in a block after validating that the input is an integer. You could also just have a single if statement before the over/logic logic and "exit 1" if validation fails. With the 'test' command, which is what is used with single brackets, "-eq" tests whether two integers are equal so it will fail for non integers. so you could validate with: if [ $age -eq $age ]; then # over/under logic else # not valid fi
You probably want to hide the error message though: if [ $age -eq $age ] 2&gt;/dev/null; then # over/under logic else # not valid fi
Awesome! Thank you!
IMO, using `[ $age -eq $age ]` is not ideal for all scripts, because it relies on a syntax error. Here's what I use, taken from [How do I test if a variable is a number in bash?](http://stackoverflow.com/questions/806906/how-do-i-test-if-a-variable-is-a-number-in-bash) on StackOverflow: if [[ $age =~ ^[0-9]+$ ]]; then # over/under logic else # not valid fi `[[` allows for regex matching using the `=~` operator. The regex is very simple, just "start of line, digit 0-9, one or more of the proceeding, end of line".
Even better use `${*:-home city}` so you can type `weather new york`
Just use [gnu parallel](http://www.gnu.org/software/parallel/). #!/bin/bash main_loop() { dir="$1" name=$(basename "$dir") [[ $name =~ ^(xxx|xxx)$ ]] &amp;&amp; continue || : python avg_88.py "$dir" } export -f main_loop parallel main_loop ::: /project/s625/xxxxx/*/ I'm not sure why you need the counter `$i` if the code can be written this way. However, if it is necessary, you can map the directories to a global array (exported), and then pass to parallel the indices so that `dir="$1"` would become i="$1" dir=${global_dir_array[$i]} 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
This is 100% how I'd do it. It's better to actually validate the variable than to just try it and see if it throws an error imo.
Any feedback is welcome. About the simplicity: the aim behind the project was to create a notifier for my workflow. If I receive enough requests to implement configurable durations or add aliases then I'd gladly do so. :)
Seems reasonable, so I've implemented the changes! Thanks!
Have you tried: ./script static "${array[1]} ${array[2]} ${array[3]}" ? Assuming that through the use of the different numbers you're wanting to select different array elements? To dump the whole array, use `"${array[@]}"`... It's sad that you can't export an array AFAIK, because that would be the cleanest thing to do.
The safer route is to pass the static var and the three array elements as four arguments; `./script static arr0 arr1 arr2`. In which case you do static1=$1 shift static2=( "$@" ) inside the script. If you mash the elements into a single argument, you need to use a delimiter that cannot occur within any of the elements. So if we say comma is such a safe delimiter for your data (`./script static "arr0,arr1,arr2"`), use `read` inside the script to split the argument into an array. static1=$1 IFS=, read -ra static2 &lt;&lt;&lt; "$2" # also assumes they contain no newlines
Well, nothing major to contribute -- but some minor niggling points. /me puts on his Captain Pedantic cap * Use curly braces around variables when manipulating them with text. This helps avoid confusion for bash, yourself, and any future reviewers of your code. * You don't need to **touch** the *$newfile* because your output redirection from **echo** will do it for you. * You should break the habit of using **echo** and start using **printf** when working with bash. **echo** may not act is expected when the value contains certain flags or parameters or is not properly quoted. **printf** behaves better in many of these cases. Ideally you could do it like this: (or anyone one of a hundred other ways, too.) #!/bin/bash filename=$1 newfile=${filename}.sh /usr/bin/printf '#!/bin/bash'"\n" &gt;$newfile /usr/bin/chmod 0755 $newfile /usr/bin/vim $newfile * I prefer to include the full path to executables because one never knows where-from (or who-from(?)) a script may be executed. YMMV, points for style, if it works your way no sweat, whateverfloatsyourpoint. You could even trim it down even further like this: #!/bin/bash newfile="${1}.sh" /usr/bin/printf ... ^EDIT: ^because ^typos ^abound ^EDIT2: ^newline ^added ^to ^printf ^... ^&gt;$newfile
&gt; Your number (.*) has been matched! :P (Also, you’re missing a `do`/`done` for the loop, and probably want to `break ` after the `printf`.)
I haven't seen these mentioned elsewhere, so I will include it here. Apologies if this is already understood. * In general testing an expression works better with **[[...]]** than with **[...]** or **test ...** http://wiki.bash-hackers.org/scripting/obsolete * The error tells you right there that an integer was expected. When you feed it text, it barfs. * As [/u/settingorange](https://www.reddit.com/user/settingorange) already said (and I will *liberally* paraphrase), why validate input after you've already let them overflow the buffer? In general you seem to have included the necessary pieces, but the order of checks and operations needs to be reviewed. You might find insight here; I certainly do: http://wiki.bash-hackers.org/syntax/ccmd/conditional_expression
&gt; * Use curly braces around variables when manipulating them with text. Absolutely unnecessary in this case and just makes the code more tedious to read IMO. &gt; * You don't need to **touch** the *$newfile* because your output redirection from **echo** will do it for you. Won’t work with the order the commands are currently in… they’d have to move the `echo` before the `chmod`. &gt; Ideally you could do it like this: […] Why on earth would you hard-code absolute paths to programs into your script? The hell? Also, you’re missing a newline in the `printf`. Recommending `printf` over `echo` is fine, but not if you introduce bugs in the process…
Yes, the order of the OP commands won't work without touch. But I do go on to demonstrate in two different ways how it can be done... As for full paths to programs...well I do indicate it's a preference. Portability, pedantry, etc. And yes, you are correct that I missed the \n for printf. Normally I would shrug it off, but when wearing the Captain Pedantic cap I supposed I can't get away with that crap. Thanks.
Not sure exit inside a function meant to be used in an interactive shell is a good idea. Adding error messages in addition to the error messages that will already be printed seems overkill as well. newscript() { (set -C &amp;&amp; printf '#!/bin/bash\n\n' &gt; "$1") &amp;&amp; chmod +x "$1" &amp;&amp; vim "$1" }
Woops. Too used to 'while read' loops, and it's only meant to be called once.
Good point. Do you just echo/printf the return code for functions? It wasn't meant to be too serious, but I should probably correct the issues it has. edit: `return`; d'uh. I'm smrt.
&gt; is "$1" by default equal to "/"? No, but `$1/*` ⇒ `/*` ⇒ `/bin /boot /dev /etc …`
aah yes, it makes sense now. If I specify a directory, it will use that. But when I don't specify an argument, the $1 doesn't even get processed and I'm basically looking at /* Thank you for that clarification
lol true, but I just hit TAB after typing the first 3 letters
Neat! Couple things.. I would filename=${1:?'Missing argument: filename'} I would also not add the `.sh` automatically. I know I'd probably end up with this half the time. my_script.sh.sh No need for newfile, just do it all on filename? filename=${1:?'Missing argument: filename'}.sh If all you want is to enable execute then change chmod to chmod +x Are you really using `vi` or are you using `vim` with an alias? If I were to write it it might look something like.. #!/usr/bin/env bash filename=${1:?'Missing argument: filename'}.sh echo '#!/usr/bin/env bash' &gt; $filename vim $filename chmod +x $filename Things I still might consider adding, don't overwrite if `$filename` exists. I haven't tested this so bugs may be abound!
 sed -i 's/^plugins(git toast)$/plugins(git toasting composer)/' test.txt See `info sed` for more information.
I'm not an expert, but if the script's not going to be used on any other systems, it doesn't make a difference. Even if used in another system, bash is at `/bin/bash` on Mac, Ubuntu, and I assume most other *nix systems. For a simple script like this, the shebang shouldn't matter as long as it hits bash.
Yes, but `sed -i` chooses a temp file more cleverly than this. There’s a `tmpfile` C function in POSIX, but no shell program, so I don’t see a good way for a script to do this safely, without potentially overwriting an existing file. (You can `set -C` to make `test.txt &gt; test.txt.tmp` fail if `test.txt.tmp` exists, but then you’ll still have to find another filename.)
Sure, but that's not really a problem. You don't need to find a unique name.
Ok, if there exists, or may exist, files that end in `.tmp` that are not temporary, you need to be careful. But I wager in most cases, that's a non-issue. If the `.tmp` file already existed, that's likely just from a failed earlier attempt, so it doesn't matter if you overwrite it again.
 grep test file.txt | grep -v ^# 
 grep '^[^#].*test' beginning of line, not #, anything, test.
Add four spaces in front of each line to get a code block. You can also change the `.*` to `[^#]*`. grep '^[^#]*test' file.txt
Just to present some other options: sed -n -e '/^#/d' -e '/test/p' file.txt or awk '/^#/{next} /test/' file.txt EDIT: s/prevent/present, wow that was a stupid typo
Thanks, it doesn't show in sync (the code block), but hopefully it's correct now. With grep '^[^#]*test' you only get lines with no # chars in front of the test, might not be the intended behaviour.
Subtle, but this breaks when the first word on the line is "test" since the "not #" character class has a nonzero match size. Could be tweaked with: egrep '^(test|[^#].*test)' file.txt
Alternatively, `^[^#]?.*test` would be a better solution since then there's no need to duplicate the target pattern.
That doesn't properly filter [leading #'s](http://i.imgur.com/eevRES6.jpg). Since ? allows a size zero match, if it finds one it skips that bit and moves on to matching ".*testing"
While your solution certainly solves your problem, I wanted to suggest an alternative (simpler?) approach. Your "generate_script.sh" could just contain: #!/bin/bash and as long as it's CHMODed to have execute permissions, you could start your new scripts by simply copying "generate_script.sh" to a new file by doing: cp generate_script.sh new_script.sh vi new_script.sh The copied file will have the same permissions as the original file.
http://shellcheck.org *please* use it. #!/bin/bash Won't work. No quotes around `$1`?.. oh dear. Also, correct me if I'm wrong, this should do: #!/bin/sh if [ -z "$1" ]; then printf "%s\n" "No filename, aborting" &gt;&amp;2 exit 2 fi umask 022 printf "%s\n" "#!/usr/bin/env bash" &gt; "$1".sh if [ -n "$EDITOR" ]; then $EDITOR "$1".sh else vi "$1".sh fi
Yeah, well \*BSD aren't actually second-class citizen, and they won't merge `/usr/local/bin` to `/bin` anytime soon, for good reasons ;)
 declare -A array array[x1,y1]=100 array[x1,y2]=200 array[x2,y1]=300 array[x2,y2]=400 #alternative 1, extract all the main keys with sort for key in $(printf '%s\n' "${!array[@]}" | sed 's/,.*//' | sort -u); do #alternative 2, keeping track of the main keys keys=(x1 x2) for key in "${keys[@]}"; do #the loop contents are the same echo "$key : y1 = ${array[$key,y1]}" echo "$key : y2 = ${array[$key,y2]}" done 
What was the ^ for? I'm new to bash, and I'm seeing that character is what you use when you're trying to exclude something. How does that make sense here?
In a regular expression, that character can have two meanings: - Inside brackets, it inverts the character group: `[abc]` stands for the character `a`, `b`, or `c`, while `[^abc]` stands for any character *except* `a`, `b`, or `c`. - Outside them, it matches the beginning of the text. `^test` will match `test string`, but not `this is a test string`, because in the second one the `test` isn’t at the start of the string. (Similarly, `$` matches the end of the text.)
Makes sense. Thank you.
Your script runs in a child shell and a child shell can't change the environment of the parent. Your only option is to source the shell which means you run the commands of your script inside your current shell. This is what the `. scriptname` does in sh and bash. Beware however that everything you do in that script will run as if you typed it into the current shell so it can change any number of environment settings. 
the shell script runs in another instance of your shell. the shell serving you your prompt has no knowledge of what the spawned shell did, minus its exit code. if you want to run your script in your current shell: source scriptname.sh or alternately: . scriptname.sh This will maintain any environment changes the script made. it is usually used to load environment variables into your current shell but it also has the side effect of keeping any directory changes. 
Thank you!
In some consoles you can still use copy/paste but instead of ctrl+v you use ctrl+shift+v and ctrl+shift+c
You can run the first command and have it's output inserted in the second command. ls -l $(echo *.txt) A silly example.
http://stackoverflow.com/questions/1923435/how-do-i-echo-stars-when-reading-password-with-read
NOTE. this will actually run the previous command again to get its result, so it's not a practical solution for long running commands.
That just grabs the last argument of the previous command. OP was looking for a way to insert the *output* of the last command into the current command.
So many convoluted answers... here's the simple one: If you are using a console on Xorg (the GUI), you can select text with your mouse and press the middle clic button of your mouse to paste the text. Yup, copy-pasting has never been easier.
Gah, don't use the non-standard, external command named `which`. Use the standard, builtin `type` command. Unlike `which`, it actually tells you what you need to know.
Am I'm the only one who is slightly anoyed by this: $ cat /usr/share/dict/words | head -n $RANDOM | tail -n 1 I mean useless use of cat, could have been: $ head -n ... /usr/share/... | tail -n 1 `head -n $RANDOM` is an unquoted parameter expansion, which would will fail if `IFS` contains numbers. The fact that `$RANDOM` gives a number between 0 and 32767 and that `/usr/share/dict/words` contains about 100,000 lines: $ wc -l /usr/share/dict/words 119095 /usr/share/dict/words Piping `head` to `tail` could be avoided if `sed` is used: $ sed '3q;d' file The above will print the third line from `file` and it will exit immediately after printing it. In total the command could look like, still assuming 0-32767 is enough: $ sed "$RANDOM"'q;d' /usr/share/dict/words
&gt; The fact that $RANDOM gives a number between 0 and 32767 and that /usr/share/dict/words contains about 100,000 lines: shuf solves this really well: `shuf -n1 /usr/share/dict/words`
If you are in Linux (using X), you can use the xclip command. See this stackoverflow: http://stackoverflow.com/questions/749544/pipe-to-from-clipboard If you are using Cygwin in Windows, you can probably use the clip command. If you are in OSX, I am not sure what you can use. But knowing about "clip" and "xclip" can get you started on your Google-foo journey.
Amazing comment, thank you!
&gt; &gt; `shuf` is part of GNU coreutils right? Does FreeBSD etc. have something similar? &gt; &gt;No shuf on OS X at least. OS X uses tools from FreeBSD so I guess that confirms that
I am not entirely sure that I understand the question you are asking. I have several directories that I use a lot and have simply created aliases for them to make it easier. Example from my .bashrc export DL="$HOME/Downloads" export WF="$HOME/Sync/Watch" And then if I want to move files from Downloads to Watch I can just type: mv $DL/*.pdf $WF And the variables are expanded for the paths.
Usage for pushd would be something like: pwd /home/myuser/dev/projects/website/template/main/ pushd /etc/ngninx/ {do some work in this directory} popd pwd /home/myuser/dev/projects/website/template/main/ It is just an easy way to leave your current directory and then get right back to it.
Are you talking about in scripts or in the shell? If you are in the shell, I would say that you should just be using tab complete to stop the typos.
If you want to move a file from the current directory to the previous directory you were in, `~-` expands to the previous directory (as does `"$OLDPWD"`): mv file ~-/file
omfg yes this is exactly what I was looking for, thanks.
Wow. I've been using bash for almost 20 years and never saw this before. Thanks.
I agree with /u/n30h80r in that tab-completion is your best friend in the shell. e.g. If I have a file in /usr/local/repo/projectA/bin/executable That i want to move to /usr/local/bin/executable I type mv /us&lt;tab&gt;lo&lt;tab&gt;re&lt;tab&gt;pr&lt;tab&gt;A/b&lt;tab&gt;ex&lt;tab&gt; /us&lt;tab&gt;lo&lt;tab&gt;bi&lt;tab&gt;`
Ping immediately exits if it can’t build an initial connection, or (doesn’t apply here) if it can’t look up the host name. But I don’t know of any condition, apart from Ctrl+C or kill, that could make a running ping exit.
Ah, thanks. Yes. I misunderstood what `$(!!)` did.
Why not use -o (do not overwrite) instead? Otherwise, provide the full path to the -x argument.
This doesn't seem right. Are you telling us the whole problem? For ex, are you supposed to do this all in the original ping script? Or are you supposed to write another script? Are you allowed to leverage existing solutions? Or does it need to be from scratch in your own script(s)? Here's a random link that shows some high level thoughts on how to accomplish this from a single script. http://askubuntu.com/questions/356800/how-to-completely-restart-script-from-inside-the-script-itself It would be very unorthodox to actually monitor a script from the script itself. If that's what you are bring asked to do, then this is obviously a learning exercise, and is almost certainly something you'll never actually do in the real world. You'll frequently see people use existing solutions like monit, upstart, or systemd to accomplish this. Another more hackey solution is a cron job that runs pgrep on a proc, and then issues a start command if it's not found. I'm personally leveraging Chef, Sensu, and a touch of upstart to accomplish proc babysitting. I won't go into how it works as you are a newbie, but just thought I'd throw it out there that there are many approaches.
I want to be able to move the extracted files after. How exactly do I add the full path? Sorry, I'm very new at scripting.
hi, I was just using ping as an example, its the first thing that came into my head.
Yes. Screen is a console tool. Works well on tty and over ssh. 
No luck. It's like it doesn't even look at the already_extracted.txt.
http://www.cyberciti.biz/tips/howto-linux-unix-find-move-all-mp3-file.html
You can split the filename on `-`, then reassemble after for file in *"$today"*; do IFS=- read -r name_year mon day hour min sec_rest &lt;&lt;&lt; "$file" newname=$name_year$mon${day}_$hour$min$sec_rest mv "$file" "$newname" done
 -exec mv -b --suffix=bac ../new_folder {} + results in find running mv -b --suffix=bac ../new_folder ./file1 ./file2 ./file3 ... as many files as will fit. And it's the last filename argument that mv uses as the destination, and that must be a directory if you are moving more than one file at a time. You have two options, either use the `-exec ... \;` version, which runs the command once per filename: -exec mv -b --suffix=bac {} ../new_folder \; or, since you clearly have GNU mv, use its -t option to specify the target directory before the files to move: -exec mv -b --suffix=bac -t ../new_folder {} + 
That worked, thank you. Please can you explain what this bit means? IFS=- read -r name_year mon day hour min sec_rest &lt;&lt;&lt; "$file" newname=$name_year$mon${day}_$hour$min$sec_rest 
The exclude/x for unrar is for the name of the files *in the .rar* to not unrar, **not** the .rar to not unrar. To accomplish what you want I would use a for loop which loops over the files... for f in *.rar do file=$(basename "$f") filecheck=$(grep -c $file already_extracted.txt) if [ $filecheck -eq 0 ] then unrar e -r -o- $f /media/Media/Extracted fi done 
I tried this and it would still extract files listed in already_extracted.txt. Maybe the list is formatted incorrectly?
 var=hello cmd will set the environment variable `var` for the command `cmd` only. In the above case its `IFS="-" read ...`. `read` will be default interpreted backslashes, this can be disabled with `-r`. When multiply variable names are provided then `read` will look at the environment variable `IFS` and split on it accordingly. If the input is `hello-world-john-doe` and the read command is: IFS=- read -r a b c then `hello` will be stored in `a`, `world` in `b` and the rest in `c` (`john-doe`). Finally `&lt;&lt;&lt; "..."` is a herestring. It's a bash syntax - among other shells and is used there to avoid running `read` in a sub shell, as it would have happen, if a pipe was to be used: # The below will NOT work: printf "%s\n" "$file" | read -r ... # But this DOES read -r ... &lt;&lt;&lt; "$line"
Also /u/geirha fixed your code in such a way that word splitting and pathname expansion will be avoided when your parameter expansions is happening. You should really consider reading about said topics: http://mywiki.wooledge.org/WordSplitting http://mywiki.wooledge.org/Quotes#Effects_of_Quoting
I don’t think so, the example is just grepping the file for the name, try adding debug to the shell script... #!/bin/bash set -x or add some echo's to see what's up... echo $f echo $file 
Thank you very much. That works for me.
Try quoting "a{2}" so the command reads `grep -E 'a{2}' ...`.
Bit unconvinced by this one as I'm unclear what the benefit is? The choice of a `while` loop and `case` statement was originally made from past experience with `getopts` having a lack of support for long option names. Clearly it can be made to work, but it looks like it's if anything more complicated?
Just my two cents: why not use the real iso format for your dates? IIRC yyyy-mm-ddThh:mm:ssZ or with timezone information instead of Z. This could play nice with some other programs you might end up using.
Thank you! Why did ${day} need curly brackets though and the other variables didnt? newname=$name_year$mon${day}_$hour$min$sec_rest
&gt; Thank you! Why did ${day} need curly brackets though and the other variables didnt? &gt; &gt; newname=$name_year$mon${day}_$hour$min$sec_rest Using curly brackets around the parameter you want to expand can always be used, eg these are the same: var=1 echo "$var" echo "${var}" But what happens when you want to print an `a` after the expansion? var=1 echo "$vara" # This expands to nothing, as there is no parameter called vara echo "${var}a" # This expands to 1 and the a is added. Prints 1a In the above case it's to avoid expanding the parameter `day_` as `_` is a valid part of a parameter name. 
Use `true` or `:` or `sleep` as the conditional command for an infinite loop. Don't do silly concoctions like `[[ TRUE ]]` or `[[ 1 ]]`. This suggests that `[[ FALSE ]]` or `[[ 0 ]]` would have the opposite truth value, but it doesn't. Anyway, see [BashFAQ 36](http://mywiki.wooledge.org/BashFAQ/036) for more on doing set operations using standard tools. 
 alias cpass='pass -c $@' Aliases do not have `$@`. When you do use `$@` (for example in functions) you should always quote it to prevent pathname expansion and word splitting: `"$@"`
&gt; [[ TRUE ]] The above is the same as writing: [[ -n "TRUE" ]] Which property illustrates it better why it shouldn't be used :-)
I think a self-contained version of this would be something like: err() { while IFS= read -r line; do echo "err prefix $line" &gt;4 done } std() { while IFS= read -r line; do echo "std prefix $line" &gt;3 done } exec 3&gt;&amp;1 1&gt; &gt;(std) exec 4&gt;&amp;2 2&gt; &gt;(err) echo "line1" &gt;&amp;1 echo "line2" &gt;&amp;2 echo "line3" &gt;&amp;2 echo "line4" &gt;&amp;1 echo "line5" &gt;&amp;2 echo "line6" &gt;&amp;1
Don't know how it handles buffering, but have you tried `ts` from `moreutils`? e.g. exec 1&gt; &gt;(ts '&lt;STD&gt;%Y-%m-%d!%H:%M:%S') exec 2&gt; &gt;(ts '&lt;ERR&gt;%Y-%m-%d!%H:%M:%S') 
I haven't, I'll look into this though. Thanks!
The eval commands processing the dates as well as the additional pipe outputting to a file. Since all of those things can add buffer delays, I didn't think it made sense to me to remove them given the exact issue I'm having. 
I updated my post on StackExchange and I believe the issue is resolved by using unbuffer with all of my echo commands OUTSIDE my redirect functions. I'll have to do some more thorough testing though. Thanks everyone for feedback!
Yes, i was able to reproduce it with your output as well. I was also playing with your code when I tried the unbuffer command on each echo at the bottom of your command which actually seems to resolve the issue. I was always trying to unbuffer my output in the redirection functions themselves!
On mobile and on holiday, so can't easily read what you've tried so far, but be aware that awk can emulate tee's behavior and you can use fflush to get around buffering issues. Edit; fflush is an awk function. Awk '{print ; fflush();}' 
Thanks, i'll keep this in my back pocket when testing.
If you can't tell from the "action" whether output 1 or output 2 is to be displayed, then you need a variable that says which output the program is currently printing. In the example, if stdin ends in a 0, the output_selector is toggled. Then the corresponding line is printed (OUTPUT 1 for selector 0, and OUTPUT 2 for selector 1). seq 100 | while read; do if [[ "$REPLY" == *0 ]]; then if [[ "$output_selector" == 1 ]]; then output_selector=0 else output_selector=1 fi fi if [[ "$output_selector" == 1 ]]; then echo "OUTPUT 2" else echo "OUTPUT 1" fi done
my 2 cents: use the [real ISO format for dates](https://en.wikipedia.org/wiki/ISO_8601), it could help later on.
That doesn't quite address the usecase of mine, although it does give me some good ideas. The action itself is predefined, external to bash, but executed in the same script. Basically what I need to do is run a function to generate formatted output which toggles this format based on a given state. This function would be called within the script in real time and piped to $interpreter. I just can't figure out the necessary logic to store that state, and toggle it with every call.
Why not use a status file ? ```CHECK="/opt/prog_status" if [ "$(cat $CHECK)" == "0" ]; then run progaction; echo 1 | sudo tee $CHECK ; elif [ "$(cat $CHECK)" == "1"]; then run otherprog; echo 0 | sudo tee $CHECK; else echo "First Run"; run progaction; echo 1 | sudo tee $CHECK ; fi```
&gt;Why not use a status file ? Because that is *soooo* 2003. But actually it's because it's just one more part to keep track of. Additional clarity has been added to OP. 
Hmm, i was just briefly looking on Wikipedia but that seems less useful than the examples I was seeing. 
Use the '[-s silent]' flag in conjunction with the 'read' command 'read -s -p "Type your password: &gt;&gt;&gt; " PASSWD' (assuming that's the way you'd be receiving the password input). Should get the job done assuming I understand the question correctly
Thanks for all the info. I really enjoy writing bash scripts, probably because I'm comfortable at the command prompt. Can you suggest a language that would better suit general system administration?
Brilliant, thanks! I love dissecting scripts like this. (seriously, not sarcastically)
You can just use a `while read` loop instead of a `for` loop. (You don't need to use `awk`.) user@host $ cat example.conf datasetA1 datasetB1 datasetC1 datasetA2 datasetB2 datasetC1 datasetA3 datasetB3 datasetC1 user@host $ while read -r x y _; do echo "x: $x"; echo "y: $y"; done &lt; example.conf x: datasetA1 y: datasetB1 x: datasetA2 y: datasetB2 x: datasetA3 y: datasetB3 You'll notice we read data into the variables `x y _` - we're just using `_` here as a placeholder to discard the rest of the line. (Otherwise `y` would contain `datasetB1 datasetC1`) `read` also has an `-a` option which will populate an array (and there is `mapfile`/`readarray` on newer versions of bash) user@host $ read -r -a F &lt;&lt;&lt; 'datasetA1 datasetB1 datasetC1' user@host $ declare -p F declare -a F='([0]="datasetA1" [1]="datasetB1" [2]="datasetC1")' user@host $ echo "${F[1]}" datasetB1
Well in this instance I guess it would be depending on the args
Do you have an example of the exact use case?
Sure. Say I want to clear the bash history after the "cd Desktop" command is run. Cd-ing to the desktop would be the trigger that would then execute the clearing of the history.
No that doesn't work. I tried that. With a dataset of 3 rows if I do it with one loop it is outputted 3 times. But with a nested loop it is outputted 9 times even though I only have two columns (two variables) of 3 rows. I'd want it to spit out 3 outputs only. 
[removed]
Use perl. If you don't know it, now is a good time to learn. The native language supported by irssi is perl, and has all sorts of extensibility. If you don't want to write perl, get weechat, it supports plenty of other languages. https://weechat.org/files/doc/stable/weechat_scripting.en.html But, while we're at it, a couple tips: Don't do `cat &lt;file&gt; | grep pattern`, instead do `grep pattern &lt;file&gt;`. # instead of: cat $reqfile| sed -e "s/$ID//g" |sed ':a;N;$!ba;s/\n//g' # try this: sed -e "s/$id//g" -e ':a;N;$!ba;s/\n//g' "$reqfile" Don't use backticks to capture output for variables, use $() instead; e.g.: # Not this: ID=`grep $request $reqfile` # This instead: id=$(grep $request $reqfile) Don't use ALL CAPS variable names; it is possible you will accidentally clobber existing environment variables. Don't use a fixed filename for a temporary file. That may introduce a race condition, since you are pairing this with irssi and event triggered execution. Use `mktemp` to make (and return the name of) a temporary file to use. tmp_file=$(mktemp) sed -e "s/$id//g" -e ':a;N;$!ba;s/\n//g' "$reqfile" &gt; "$tmp_file" 
Hey man, awesome tips. Will adapt my script. This is why i subbed to this subreddit. Always eager to learn more bash, and the community is always eager to teach ;) 
&gt; only two Perl statements required. Care to point into the right direction? Never really fiddled with perl before :)
bash 4.3 introduced nameref variables. declare -n ref=AAAA_Array for elem in "${ref[@]}"; do ... done For older versions, there's variable indirection which is a bit more cumbersome. See [BashFAQ 6](http://mywiki.wooledge.org/BashFAQ/006). A better approach may be to use associative arrays instead and/or rethinking your design, but that depends on the actual goal you're trying to solve.
Search the documentation of flac how to open STDIN rather than the file audio.wav. After you do this, you can pipe the results of ffmpeg the usual way: ffmpeg -i /... | flac -8 ... --the-option-in-question Ive never worked with those, so I don't know the option. Usually, it is -i in other binaries, but it's not "standard" edit: There's another way though, using the file itself as an intermediary: ffmpeg -i /BDMV/STREAM/00004.m2ts -map 0:1 -acodec pcm_s24le audio.wav &amp;&amp; flac -8 audio.wav This will create the file and if that is successful, the next command will open and play the file. No pipes, just an intermediary file. edit2: ugh! I must be tired... you also have to checkout the documentation of ffmpeg how to force its bytes onto the STDOUT (rather than the audio.wav file). Just check both binaries with man for STDOUT and STDIN for ffmpeg and flac respectively 
Also note that one would properly want to wrap the expansion in double quotes: `for OPT in "${OPTIONS[@]}"` or the output would undergo word splitting and pathname expansion. Funny enough one of the few places this can be safely omitted is in `case $arg in`.
thank you. I'm looking into associative arrays at the moment. But I'll be reading up on nameref variables. My goal is really to be able to take a term, $keyword, and and see if it exists in the array for that keyword.
You may wish to drop using bash here. Build a package for your Linux distro and distribute and install this package using your package manager: this will save you a headache later on.
You could use `agrep` (in Arch Linux in package `tre`) In the example, option `-s` shows scores. `-k` matches literal strings (not regex). `-E 10` allows 10 errors (you should set this to `-E 100` or even higher, it doesn't matter if it's too high in this case). $ seq 10 | agrep -skE 10 50000 | sort -n -t: -k1,1 4:10 4:5 5:1 5:2 5:3 5:4 5:6 5:7 5:8 5:9 Here, 10 and 5 match closest to 50000.
Thanks. I'll look into it
Don't use `for a in $(cat b)` for iteration over files (assuming you want to iterate over the lines), use `while` instead: while IFS= read -r host; do if ssh -q -n "$host" 'ps -ef | grep -qi "WEBLOGIC"'; then printf "%s have WEBLOGIC runninn\n" "$host" done &lt; serverlist Also wrap double quotes around parameter expansions (`$host`) to prevent it from ungergoing [word splitting](http://mywiki.wooledge.org/WordSplitting) and [pathname expansion](http://wiki.bash-hackers.org/syntax/expansion/globs). I'm not completely sure what you want but the above will print all servers from `serverlist` that have a process running matching `WEBLOGIC`. `&lt; file` is a input redirection making the while-read loop iterate over each line in `file`.
You could just dispense with the loop and do this: awk '{ print $1" "$2 }' example.conf 
Almost. I have a file and and a directory both similarly named. I want to move the file into the directory. The directory is one of many at the tree level. 
Not sure if there is a nice solution for such a task - here are some possible solutions however sed: sed 's/\t\([^|]*|\)*\([^|]*\)|\t/\t\2\t/' awk: awk -F\\t -v OFS=\\t '{ sub(/\|$/, "", $2); sub(/.+\|/, "", $2); print }' perl: perl -pe 's/\t\K.+?([^|]+)\|(?=\t)/$1/'
http://stackoverflow.com/questions/893585/how-to-parse-xml-in-bash
Awesome, thanks! But i actually ended up using remote.pl - basically a fifo pipeline into irssi, which enables me to just echo commands into the fifo from my original script. 
Thank you all for the great info... I wrote a script that works for what I need to do. I'm only starting out with scripting so I'm kinda proud of it. Feel free to knock me down a peg. Any advise is welcome. #/bin/bash ps1='ps -ef | grep -v grep |egrep -iq "WEBLOGIC"' ps2='ps -ef | grep -v grep |egrep -iq "TOMCAT"' ps3='ps -ef | grep -v grep |egrep -iq "ACTIVEMQ"' found="" eval $ps1 rc=$? if [ "$rc" == "0" ] ; then echo "WEBLOGIC FOUND on $HOSTNAME" found=y fi eval $ps2 rc=$? if [ "$rc" == "0" ] ; then echo "TOMCAT found on $HOSTNAME" found=y fi eval $ps3 rc=$? if [ "$rc" == "0" ] ; then echo "ACTIVEMQ found on $HOSTNAME" found=y fi if [ -z "$found" ] ; then echo "nothing found on $HOSTNAME" fi Then, I run a for loop from a mgt server: for HOST in $(cat serverlist) ; do cat chkapp.sh | ssh -q $HOST '/bin/bash' &gt;&gt; /tmp/test.txt ; done 
The other poster has xml covered. Technically all valid json is valid yaml. So if you could control the input, you could use jq, which can take some getting used to but is very handy. But I'm guessing you can't control the input. In that case, yaml is a very tricky language to even imagine writing a shell parser for. You have to cover both the character-delimited (json) and space-delimited styles. But, amazingly, someone has written a parser with sed and awk that can do yaml nested up to two deep. That horror, as well as other less insane options, are in here, which was the first link on google for "shell yaml parser" : http://stackoverflow.com/questions/5014632/how-can-i-parse-a-yaml-file-from-a-linux-shell-script Also, bash has its place, and it isn't this. 
https://github.com/zsh-users/zsh-syntax-highlighting and http://fishshell.com/ boom also related: https://www.reddit.com/r/linux/comments/2frvjg/awesome_tool_that_highlights_your_terminal/
I suppose it's possible to build this as a `.bashrc` function probably using `tput` and the output of `compgen -c | grep -x $1` or something to that effect... I personally can't see the need for this kind of thing, as you'll get an error anyway if something's wrong... What would actually make this useful would be maybe an error that points to where in a chain of piped commands things actually broke e.g. grep word somefile | tr "\n" "," | cut -d":" -f5- | akw '{print $3}' ^---Shit broke here, yo. shellcheck does that already, but I mean in terms of direct interactive feedback from your shell. You may like the classic "changed PS1 based on the last command's exit code" thing as an alternative. I had a quick google for that and wouldn't you know, [the first link](https://krash.be/node/25) is a blog with a picture that's familiar to me - a former colleague of mine. Small world!
Correct... similar but not the same. /r/fishshell might be up your alley.
[ShellCheck](https://www.shellcheck.net) might help you debug it
This is neat that it's all done on one line, but something with this many moving parts may be better organized in a bash script. For simplicity's sake (and readability), here's your script split to a more readable format so we can go over each command you're running. Note that this is just split to be readable, so consider this pseudocode which won't execute if you copy-paste: imanuel@orangepipc:~$ find /media/et10000 -name '*' -exec file {} \; | grep -o -P '^.+: \w+ image' | cut -d':' -f1 | xargs -I {} sh -c "fname="{}"; echo 'filename:'; echo $fname; filebase=$(basename $fname); filenew=${fname:14}; cp -dv $fname /media/newhdd/photos$fnamenew; ln -s /media/newhdd/photos/year/$(date +%Y)/$fnamebase /media/newhdd/photos$fnamenew" A couple of things I took from this that I noticed right off the bat: xargs invokes sh, which is implicitly bash but may not actually be bash, so double-check that sh = bash. To that end, I don't believe sh can chop up parts of a variable, at least not in my environment (ubuntu 14.04, zsh -&gt; sh): ➜ Pictures x="blahblahblah" ➜ Pictures echo ${x:5} lahblah ➜ Pictures sh $ x="blahblahblah" $ echo ${x:5} sh: 2: Bad substitution $ Also I noticed you're improperly escaping your double quotes in your xargs statement, specifically: xargs -I {} sh -c "fname="{}"; ... I believe also that specifically "{}" is a placeholder for returned results in only the "find" binary, so I don't think it's interchangeable along the course of your command. That far down the command string you're only communicating through STDOUT/STDERR and your environment, which further complicates your issue. Another argument for writing a bash script. You can put the results of find in a variable and work with it that way. I'm thinking in your case you may want to consider using a for loop here, but I'm just spitballing.
Wow thanks! This is a very helpful site. :) Using this tool I learned that I needed to escape all $'s in my command.
This is the bash script I wrote in the end. Does anyone have an idea on how to split it across multiple lines? Anyway, I'm happy that it finally works now. :) #!/bin/sh find "$1" -name '*' -exec file {} \; | grep -o -P '^.+: \w+ image' | cut -d':' -f1 | xargs -I {} sh -c "fname={}; filebase=\$(basename \$fname); fnamenew=\${fname#$1}; mkdir --parents /media/newhdd/photos\$(dirname \$fnamenew); cp -dv \$fname /media/newhdd/photos\$fnamenew; mkdir --parents /media/newhdd/photos/year/\$(date +%Y); ln -fs /media/newhdd/photos\$fnamenew /media/newhdd/photos/year/\$(date +%Y)/\$fnamebase; mkdir --parents /media/newhdd/photos/year/\$(date +%Y/%m-%B); ln -fs /media/newhdd/photos\$fnamenew /media/newhdd/photos/year/\$(date +%Y/%m-%B)/\$fnamebase;"
Makes sense. Thank you!
Yea I was thinking of going Python at this point.
Awesome. That's a good option.
I've updated the script to have selectable mode for testing: Ping, traceroute, ping OR TraceRoute, Ping AND TraceRoute, plus other improvements. I'd really like some feedback. 
 printf ':%s, ' "$@" | sed 's/, $/\n/'
When I ran it, I got a trailing comma (e.g. "1, 2, 3, 4,"). Do you know how I would do it without a trailing comma?
First, `"$@"` expands to all of the positional parameters as separate words. So if you called this script with `script arg1 arg2 arg3`, `"$@"` expands to be `"arg1" "arg2" "arg3"`. Next, `printf` takes a format string followed by any number of parameters, and it formats each parameter into the format string and prints those. So our format string is `:%s, `, and the `%s` gets replaced by each parameter, and those get printed one after the other. Next, the output of the `printf` command is piped (`... | ...`) into the `sed` command. Finally, the `sed s/.../.../` command looks for `, $` (this is a regular expression, so `$` is special and means "end-of-line") and replaces it with `\n` (a new line). This effectively removes the trailing comma and adds a trailing newline (in the command-line world we nearly always expect output to end with a newline).
The file the process substitution expands to is not a regular file, so don't treat it as such. In particular, you can't read it twice like you do, since a pipe can only be read from start to end once. No rewinds.
I thought that might be where the problem was. The only alternative I can think of for the template scripts is creating temporary files. Appreciate the input.
Slightly simpler, and replaces the trailing comma: for arg; do echo -n ":$arg, " done | sed 's/, $/\n/'
I use this function a lot: #!/bin/bash function __join { local delim=$1 ; shift printf "$1" ; shift printf "%s" "${@/#/$delim}" } ARRY[${#ARRY[*]}]='one' ARRY[${#ARRY[*]}]='two' ARRY[${#ARRY[*]}]='three' ARRY[${#ARRY[*]}]='four' DELIM=' | ' line=$(__join "$DELIM" "${ARRY[@]}") echo $line This will print one | two | three | four &gt; local delim=$1 ; shift Takes the first item and assigns it to the delim variable, then shifts it off the argument list. &gt; echo -n "$1 ; shift prints the first element of the array without a newline, then shifts it off the argument list (in this case the first element of the array). &gt; printf "%s" "${@/#/$delim}" This is processed right to left ... the line noise (${@...}) just adds the value of $delim to the beginning of each element of the array and feeds it to printf. Edit: I do know my right from my left ... Edit2: Change echo to printf and get rid of extraneous spaces.
That's a cool use of substitution, using just the # as your pattern. I didn't know you could do that. Cool function, though I would change your `echo -n "$1"` to a `printf %s "$1"`. `echo` should always be avoided in something that's meant to be reused.
You could pipe the loop to `sed 's/, $//'`, but the top comment already gives a better answer.
just for the lols, in vim you can start from this text arg1 arg2 arg3 arg4 and do this ggqqi:&lt;ESC&gt;ea,&lt;ESC&gt;wq3@qxZZ
PM sent.
I think this calls for a for loop. That is, I can't figure out a reasonable way to do this all within a find command, at this time of night at least. for datfile in $(find $DIR -type f -name "*.dat1" ) ; do echo process -i $datfile -o /tmp/$(basename $datfile) done 
It can be simplest to have a little temporary script that takes `{}` as `$1` and does all the work in there to call `process` having produced the arguments rather than work out the correct escaping within multiple levels of quotes for a `-exec bash`. Then it's the much simpler `-exec ./foo {} \;`.
Why do you have hundreds of spaces after the shebang, and why did you misspell "array"?
I copypasted from my terminal and sometimes whatever mechanism is used to do that includes all the space in my terminal window. I just missed that line when cleaning it up. I didn't misspell array, any more than I did delimiter.
Yeah, but do remember that in awk `condition` is the same as `{if (condition) {print $0}}`.
Hmm, any chance you have `\r`/`^M` characters at the end of the lines in the source file? That might be the source of the problem here if so
aaah I see. Works now. Thank you!
Also fstrcmp, a fuzzy text search designed for this sort of thing. http://freecode.com/projects/fstrcmp
What do you mean? Also, this is using readline, isn't it?
Awesome. Thank you. The script works exactly as intended now. Didn't know about character classes, and I definitely didn't know that you can use the **//** like that to remove something from a variable. Thanks again!
Eh.... Why don't you use just "grep -i" ?
Or if you want those line numbers and messages, use awk.
This sounds like a job for $PATH. Alternatively: function sudo() { ... su -c "cd ...; $@" ... } 
Thanks, that works really well for commands without arguments. However if I try to run "sudo cat testfile" su tells me that user testfile can not be found. EDIT: My mistake, it doesn't work. I guess it is because $@ is not defined inside su. EDIT2: Thanks! I got it to work with this function: sudo() { CURDIR=$(pwd) su root -c "cd $CURDIR; $(echo $@)" }
Btw, does bash allow you to combine character classes? I tried adding [[:upper:]] to your example, but I guess you can't add more character classes when deleting with **//**. I'm asking because I couldn't find any documentation on such a thing. Here are the combinations I tried: nospace=${line//[[:space:]][[:upper:]]} nospace=${line//[[:space:upper:]]} nospace=${line//[[:space::upper:]]} 
Nice! Thank you
whenever you are using shopt -s nocasematch , you should use case statement 
Uh, `sed -i` is a GNU-ism, so I'm surprised that works if you don't have GNU, I'm also surprised that works since the `$` in the double quotes should try to expand a variable, and do be careful about `$suffix` having any strings that are important to `sed` that could be interpreted as commands, also, good habit would be to quote `$members` and `$src_file` in case any word-splitting characters show up in them. Also, looking at that command, it seems I misunderstood your need. What I said will append the `$suffix` to the end of each line in the file, if you just want to add it to the end of the file, you don't need `awk` or `sed` or anything, just do printf '%s\n' "$suffix" &gt;&gt; "$members" with the double redirect so it will append to the file (a single `&gt;` will overwrite the file, so do be careful there) Edit: to clarify, `-i` with no arguments is a GNU-ism, I believe the BSD version requires an argument (which is optional with GNU) which is the extension of the backup file to make when doing the in-place edits
Can you explain why out of curiosity?
It puts the current directory to the front of PATH, so *any* command can be overridden by a script in the working directory. If I have an account on your system, I can create an `ls` script that roots your box and then calls the real `ls`. If I can get `root` to run `ls` in that folder, the box is mine. Depending on `root`'s dotfiles, it may only be necessary for `root` to `cd` to that directory.
I don't get the joke. Even if I did, you made me highlight a URL from a hypertext link to go look at the shirt because you uploaded a low-res version. Not sure if you're a dick or if you did that on purpose to make me follow your link to avoid Reddit's tracking/spam algorithm. I'm going to say both.
You don't need find for what you're doing, really. Something like this should work well; for file in *; do if [[ -d "$file" ]]; then tar cvf "${file}.tar" "$file" fi done
Even more simply: for dir in */; do tar cvf "$dir.tar" -- "$dir" done
&gt; we are migrating from old Ubuntu server to Solaris 11. I found your first problem... Anyway, you can install GNU coreutils on Solaris, and then you just call a different find. https://www.opencsw.org/packages/CSWcoreutils/ 
For starters, the values you're piping to bc are not variables. 
Could you explain to me what they are?
Don't glob in shell scripts. Ever.
you shouldn't post dogmatic rules without explaining your reasoning. It doesn't help anyone.
Well you're on Solaris with access to ZFS right? Why not drop the Ubuntu limitations and simply use ZFS snapshots? And never glob in a shell script. Ever. #!/usr/bin/env bash set -euf -o pipefail fun safe stuff goes here
Would you please elaborate on this?
You’ve done nothing more in this thread than give the exact same piece of bad advice thrice, with no explanation whatsoever, and post a script that doesn’t actually do anything. Be warned – that constitutes spam and is not tolerated.
Huh, TIL how file I/O works in awk – `beam.in` stays open until you explicitly close it? Interesting.
I think they refer to the fact that globs are stored in memory and can cause all sort of problems when consuming huge amounts of memory. In the code posted there is no problem however as it is fair to assume one directory can be handled by any decent computer. It's mostly related to not quoting parameter expansions containing user input. As leading out the quotes will cause word splitting and pathname expansion (globbing) to the content of the variable: ~ $ a='*' ~ $ echo $a Desktop Documents Downloads ... One can construct a glob that is pretty harmful: echo /*/*/*/* The above will expand to the first four levels in the directory tree, combined with `../` it get out of hand: echo /*/../* The above will expand to the first level of the directory tree, but this time each item will be represented multiply times, try it yourself.
I'll take my downvotes away if you provide a reason why one should never glob in a shell script "ever."
Would you care to elaborate on my thrice advice as being bad? If my thrice advise on avoiding globing is bad, then you must know * more than I do. Get it?
&gt; Get it? I don’t. If “you must know * more than I do” is supposed to be an explanation, then you’ll have to try again. Or is this about how globs expand to their literal value if they match zero files? That’s easily fixed with `shopt -s nullglob`.
We'll never actually know, you can guess _forever_.
You must've misread the BashPitfalls page, because it certainly does not back your "never glob in a shell script" claim. The other one recommends using shell options that should not be enabled in new scripts. `set -e` and `set -u` don't play well with modern shell features, and their behaviour differ between bash versions. You'll have less buggy scripts without those options. Do the error handling manually.
Oh thanks for the reply, but how would I run this script from some other folder, meaning in case the script won't reside in the actual folder that needs to be tarred? I've tried with /var/www/ instead of * at the very first line and it archives all of the sites in one tar (www.tar) instead of site1.tar, site2.tar, site3.tar etc...
You can break the line after a pipe, `|`. The xargs could be replaced with a while read -r f; do ... done so all those commands aren't a double-quoted string. Present a working version, that still doesn't handle spaces, like that and it will be easier for others to help improve.
Yes, look in your `.bashrc` file where it presumably has code to source that bash_aliases file. You can add more like that with whatever names you want.
You can source as much files as you want in your `.bashrc` (or similar file). source ~/bash_aliases_1 source ~/bash_aliases_2 source ~/some_functions source /other/directory/elswhere/file_with_things
In bash, `source` and `.` are the same command, yes.
Thanks! Using your tip, I made my script work! :)
That user is banned here now, if you want an answer (I don’t think you’ll get one beyond [this](https://www.reddit.com/r/bash/comments/4ykp9t/workaround_for_find_maxdepth_attribute/d6pnmvw)) you’ll have to ask via PM.
All else considered equal, err toward readability. 
horaay it looks perfect! What is did was http://pastebin.com/X27WRLEB but yours looks better
Hi all, thanks very much for the ideas, very helpful. I discovered that `paste` has some special behavior: paste -d $'001' - - will take `STDIN` and paste every pair of lines into one line, separated by `\001`. Similarly `paste - - - -` will do the same to very 4 lines. Weird, but cool. So I'm planning on using this `paste` behavior together with some mechanism (`tr`?) to replace the `\001` with newlines in the split files. Thanks all!
Ah you're right, I had to delete the "[" character in: for cmd in zpool awk [ Upvotes for all, thanks fellas.
&gt; It checks the return value from calling which on the commands zpool and awk, As posted, that's not what it does. Note the \`backticks\`; they do command substitution. Meaning, for each `cmd`, it takes the *output* of `which $cmd`, executes that output as a command (with no arguments), and checks its exit status. In other words, the script as posted is wrong and if it happens to work that's pure coincidence. To fix the bug, simply remove the backticks.
got it, Thanks a lot~
Maybe use tail instead of cat: tail -n+X filename where X is the number of lines you want to skip at the top. 
Thank you so much
No VM or Live CD necessary on Windows 10 v1607. `Control Panel &gt; Programs and Features &gt; Turn Windows features on or off &gt; Enable "Windows Subsystem for Linux (Beta)"`
Linux sysadmin here, you're talking to the wrong guy buddy. 😂
OK, so by "traditional" I'm going back further than you. Would it help if I said "The pre-tail(1), Bell Labs, long-standing method..."?
I gave the game a go to see what it was about. You can log in to the same server by ssh-ing to localhost. E.g.: ssh -l bandit1 localhost # or ssh bandit1@localhost (The game didn't allow using `su` for that)
Hmm, not sure how that quote got there, I use VIM and Notepad++, neither were showing the color change for an extra ", and a review of the hard copies of the script isnt showing it either. Ive edited the OP to reflect
&gt; cp -p /usr/local/logserver/log.\[1-${LOGNUM}\].gz $LOCATIONDIR/_logs/$NOW.$TICKETNUM/logserverlogs.$(TIME); You're escaping the `[` and `]` shell glob characters with backslashes, removing the special effect you're relying on. Remove those backslashes.
Yes, it uses readline, but I'd argue that learning bash's readline keybindings allow you to use the use the plain command line in more situations without any other helpers. For instance, you can easily get the last command with Ctrl-p and remove the last word with Ctrl-w, which is pretty much what a REPL utility like the original posts' does, and it's only 3 keys (and it works in any readline REPL!) Another example: many commands take options first, but operate mainly on their last operational parameter. Bash's yank-last-arg (alt-.) pastes the last commands' last argument, which means you can do: ls some_dir du -sm &lt;alt-.&gt; cd &lt;alt-.&gt; 
Sorry, I don't understand. You say there are two rows, and show two lines, but then say "Every 120 lines or so".
Assuming "FP1 amp" as indicator for repetition and spaces (not tabs) as fields separators: #! /usr/bin/awk -f BEGIN { getline head # read head of table line getline content # read table content line # searching for "FP1 amp" except at the beginning while (match(head, " FP1 amp")) { # print heading and content each up to the found FP1 amp field printf("%s\n%s\n", substr(head, 1, RSTART), substr(content, 1, RSTART)) # remove the printed parts head = substr(head, RSTART + 1) content = substr(content, RSTART + 1) } # print the remaining part printf("%s\n%s\n", head, content) } Example data: FP1 amp FP1 lat FP2 amp FP2 lat FP1 amp FP1 lat FP2 amp FP2 lat FP1 amp FP1 lat FP2 amp FP2 lat 1.1 2.2 3.3 4.4 5.5 6.6 7.7 8.8 9.9 10.10 11.11 12.12 Output of an example run: FP1 amp FP1 lat FP2 amp FP2 lat 1.1 2.2 3.3 4.4 FP1 amp FP1 lat FP2 amp FP2 lat 5.5 6.6 7.7 8.8 FP1 amp FP1 lat FP2 amp FP2 lat 9.9 10.10 11.11 12.12 
There's also something built into the newer versions of bash. You use a path `**` and bash will go search recursively. In your example, you would do this: You can check if it finds everything by doing: echo ~/sourcedir/**/*.jpg The copy command would look like this: cp ~/sourcedir/**/*.jpg ~/destdir/ You need to have the shell-option "globstar" enabled for this to work. You can check if it's on or off with `shopt`. To enable it, you do `shopt -s globstar` (that's what you'd put in your .bashrc).
You can open Reddit in your default browser by running the following command: xdg-open https://reddit.com/ If you're running Gnome as your GUI, you need to run the following instead: gvfs-open https://reddit.com/ If you're looking to open Reddit in your terminal instead of a browser, you can "sudo apt-get install lynx" (Ubuntus), or "sudo yum install lynx" (RHEL), and then run this: lynx https://reddit.com/ You can save any of the above commands in a .sh file, and then run it with "sh filename.sh". If you want to run it with a dot like this "./filename.sh" you need to add the shebang which tells the OS what binary should be used. So you can save the file like this: #!/bin/bash xdg-open https://reddit.com/ If you want, you can edit your .bashrc file (your BASH profile customization file), and add an alias which will let you type "reddit" and run that script: alias reddit="xdg-open https://reddit.com/" Then you can restart your terminal so the change will take effect, or you can type "source ~/.bashrc" which will refresh the config file without closing the terminal. Hope this is clear enough, feel free to let me know if there's something you don't particularly understand :)
Or on mac: `open https://reddit.com/`
You don't have to run the script to run the commands. Troubleshoot by running a grep /cut command on the file from the command line. Observe output, adjust cut - see 'man cut' from command line - observe output. Repeat until you get result you like then change the script to match. 
A few things: Simplify `cat temp2datafile | grep "02-CPU 1"` into `grep "02-CPU 1" temp2datafile`. For indentation, you have tabs in some places and spaces in others. I recommend you switch all the tabs to spaces, because tabs don't work with terminals that do tab autocompletion. It's not common practice to indent `then` and `else`, but that's nbd. You desperately need to learn arrays and `for` loops. It's much easier to declare an array at the top of a script and iterate through its items than to write out every single iteration yourself.
And on cygwin: cygstart.exe https://reddit.com/
This is a bit more efficient: find ./originals -name '*.jpg' -exec mv -t /path/to/destination {} + This will result in a lot less `mv` processes. (Warning: `mv -t` is a GNU extension.)
&gt; If you're running Gnome as your GUI, you need to run the following instead: `xdg-open` works on my Gnome too…
Why not just use a language where these paradigms are catered for, rather than hacking it up in bash using undocumented features?
Grep, cut etc are all fine and dandy, but if you know the format will always look like your original with white space after the "|" (bars), you can use a simple "awk" command: awk '{ printf "%d %s %s", int($2), $4, $5 }' datafile Done. If you need to allow for the possibility of no white space before/after the bars, then you can use the bars as field delimiters instead of the default which is white space: awk -F\| ' { printf "%d %s", int($2), $3 }' datafile Keep in mind, this latter form includes any/all white space between the bars as it considers everything not a bar part of the field. There's ways to implement functions in awk to strip the white space, but you can look into that. Hope that helps.
I'm using 4.3.46 , so I will give what you very kindly wrote a whirl once I'm at my PC. Thanks so much really appreciate it! 
I went ahead and tested it with live data and [it returned this](http://pastebin.com/mLv5Zj9Z) , I tried pasting it as a code block but had no luck. I am going to tinker around and see whats up.
&gt; caveats would you elaborate on the "potential caveats"
here is the current state of get_ipmi_data ; http://pastebin.com/akDfEPxC , I did change the file it uses to match what was originally there, and the file it generates is typically created and removed in each script 
Wow, didn't even see that, nice catch. Saw the obvious one and assumed that must be the problem :)
AWESOME! So this time [it did pull good data](http://pastebin.com/rzm6zWM4) , ~~but I dont think it likes the curl lines `curl -i -XPOST http://10.1.10.13:8086/write?db=home --data-binary health_data,host=esxi1,sensor=cpu1temp value=40` , specifically the --data binary line , as none of the values show up in InfluxDB~~ Scratch that, I looked at the original script that works and its the same deal, the only thing I can think of as to why it wouldnt write is because the values from the Supermicro is [temp] [degrees C] where as the script you so kindly provided outputs value as [40] (for example) , I think it needs the degrees part and the % , not just the value...Any insights into that? After looking at it and tinkering, is it possible to do something similar to ` val=${values[$sensor]} unit=${unit[$sensor]} data="health_data,host=esxi1,sensor=$name value=${val}${unit}"` I did poke around and try several variants of the above, none worked, and a few returned a error for me being an idiot and putting something where it doesnt below. 
I am such an idiot, I thought the echo curl was echoing AND writing :/ *facepalm* thank you so much! 
to answer your question about the dir tree... I am making a setup script for this https://github.com/kaldi-asr/kaldi in the "egs" folder will there be an extra folder named "setup_base_files", which the name states will have the bash script, and data files nessesary to create a new workspace. here is the dir tree of the folder "setup_base_folder" - http://pastebin.com/cQBPuPn2
can't be bothered to check in the script, on which level is the data supposed to be split? 89 directories or 1082 files?
84 directories.. Only the directories in the database -&gt; an4 -&gt;data.. 
Yeah, nah.
Could you explain why? There really is no reason to use such a script outside of my house other than to just be a dick. Regardless of that, there are plenty of working scripts with available source code as it is. I simply wanted to make my first script that wasn't the generic echo Hello World! that every other person makes.
I think that it is great you are wanting to create a script. But why not create something useful. What purpose does this serve for anyone? Create a script that maps your network and moves items to csv format? Create a script to determine hosts that have common ports open so that you can check for any vulnerabilities you might want to close. But this is just silly. Just my one cent. 
&gt;Create a script that maps your network and moves items to csv format? Create a script to determine hosts that have common ports open so that you can check for any vulnerabilities you might want to close. I dont know what any of that means (and something tells me you already knew that) and therefore would have no idea how to approach that. Nor do I have any worries about my little home network's vulnerabilities. &gt;This is just silly. Did I mention this is my first script? Is the level of silly too high for my *first* script? If you don't want to help, fine, but I don't see how telling a new user who wants to learn how to write scripts that his is too silly is in any way helping that user learn. 
Thanks man! I really appreciate it. [I actually made a working script using aircrack-ng after my post.](https://gist.github.com/gibbyb/18ab697c003d16daec109950c2803355) But honestly the important thing is the information you gave me because that is *exactly* what I needed. Information used to help simply make my code look more sophisticated is very helpful. The script won't run mostly because of arpspoof or my lack of knowledge regarding arpspoof at the very least. 
Process substitution is basically just syntactic sugar for creating named pipes, so managing the named pipes yourself is a solution here. for file in a b c d; do mkfifo "fifo_$file" head "$file" &gt; "fifo_$file" &amp; done ./command fifo_* wait
From one line to six. I guess I'm a stickler for simplicity, even if it looks repetitive.
More than six really, since you'd also want to clean up the fifos when it's done. And it can always be written as a new command, of course.
 while read line ; do arr=(${line}) ; echo "mv foo_2_a1_${arr[1]} newfile" ; done &lt; txt EDIT: You didn't really say what the criteria of the dest directory would be, but this should be a start.
Extracting a particular column of output or from a file can be managed with `cut` or `awk`, I find `awk` to be the more robust option though. So for that you'd do something like this: awk '{print $2}' filename To wrap it up in a one-liner, you could use a loop e.g. for file in $(awk '{print $2}' filename); do mv foo_2_a1_"${file}".h3 /path/to/destination/; done Or using a while loop along the lines of while IFS= read -r file; do mv foo_2_a1_"${file}".h3 /path/to/destination/; done &lt; &lt;(awk '{print $2}' filename) It should also be possible to do this without a manually constructed loop by using `xargs`, but for the most part you'll find a `for` or `while` loop will be the standard answer to your problem.
Thank you a lot for the suggestions. These are the reasons we should not hesitate to help people, one gets chance to improve his own skills too. I preferred backticks because it was easier to type, but if $() is indeed better, I will be using it from now on. About sed, I know it is kind of a really bad habit. Similar when doing `cat file | grep search_string` Trying to improve this habit a lot. Thank you once again! 
 mv foo* /path/to/dest/
If there’s at most one link per line, this could do the trick: for file in *.html; do sed -i '/ref="noopener noreferrer"/! s/target="_blank"/&amp; ref="noopener noreferrer"/' -- "$file" done The sed script starts with an address: a regex matching lines that already have those attributes. The `!` inverts it: the following command will only be run on those lines that *don’t* match the regexp. The `s` command itself is essentially the same as yours. This won’t work if - there are multiple links in one line, and some but not all of them have those attributes, or - some links already have those attributes, but not quite in that same literal way (e. g. `ref="noreferrer noopener"`), or - your HTML, for whatever reason, contains `target="_blank"` or `ref="noopener noreferrer"` outside of HTML attributes, or - anything else I didn’t think of. The first three points are true for your version as well, I believe (you can fix the first one easily with a `/g` flag). If you need this to work on arbitrary input, it’s probably time to use a language that offers proper HTML parsing.
Thanks! I was never good with awk, do you have any examples you could point me to?
Sure, so you can take the active part of the loop: &gt; `mv foo_2_a1_"${file}".h3 /path/to/destination/` and change it up as you see fit e.g. mv foo*"${file}"* /path/to/destination/
You could try the manual it ships with :) the overall index looks a bit disorganized, but the intro looks decent enough: info '(gawk) Getting Started' Note: I haven’t actually read all of it, so I don’t know how good it really is. But I’ve had good experiences with GNU software’s info manuals in the past.
Use tab completion, or escape the space by prepending with \ if you insist on typing it all in by hand.
What, if anything, have you tried so far?
Reddit homework club? 
have fun!
Seriously?
OP didn't specify an upward bound. There are an infinite number of primes, we'll never be able to solve OP's question this way because we won't have enough space to store all the primes. We also would need them to be provided before hand. Interesting solution to part of the question though! 
You forgot a then after your if statement. There are a lie more stylistic problems, but I'm on mobile. 
You need to iterate over the array elements, use "${ronly[@]}" i.e.: for nfs in "${ronly[@]}"; do ...; done
hi again /u/thestoicattack i have it working now for the most part only issue is it only ever returns the first mount not all nfs mounts on the system. i guess my loop isn't looping #!/bin/bash readarray -t ronly &lt; &lt;( grep -P "\snfs[\s,]" /proc/mounts | awk '{print $2}') for nfs in "$ronly" do read -r mount &lt;&lt;&lt; "$nfs" touch "$mount/ReadWrite.check" if [ $? -eq 0 ] then printf "statistic: 0\nMessage: Success\n" else printf "statistic: 1\nMessage: Failure\n" fi echo $mount done 
/u/commandlineluser perfect, thanks so much for your help its well appreciated :-)
I think you are confusing `date` format codes with `printf` format codes. They are not the same. Also, your `awk` won't do what you think. If your goal is to print the date of your last successful backup, I'd suggest something like this: # This will give you the date and time /Applications/Carbon\ Copy\ Cloner.app/Contents/MacOS/ccc -h |grep "Success" | tail -1 | cut -d '|' -f 4 # This will give just the date /Applications/Carbon\ Copy\ Cloner.app/Contents/MacOS/ccc -h |grep "Success" | tail -1 | cut -d '|' -f 4 | sed 's/,.*//' 
Type this: man mail
What are you specifying as your shebang in the Shell script? Bash? If I had to guess I'd say this is something to do with the format of the windows filesystem
At first I wasn't specifying anything. Tried #!/bin/sh - same result. Log files are dropped to one of the linux partitions, windows is only accessing them through samba. Peculiar thing is if the command is run from the actual command line # filename is displayed correctly in windows and linux. If run from shell script Windows sees gibberish, shows up fine on linux side.
Inside `[` and `[[`, `-gt` is for numbers, while `&gt;` is for strings, or you can use: if (( D &gt; P )); then See `man test` or `man bash` INTEGER1 -gt INTEGER2 INTEGER1 is greater than INTEGER2 string1 &gt; string2 True if string1 sorts after string2 lexicographically. arg1 OP arg2 OP is one of -eq, -ne, -lt, -le, -gt, or -ge. These arithmetic binary operators return true if arg1 is equal to, not equal to, less than, less than or equal to, greater than, or greater than or equal to arg2, respectively. Arg1 and arg2 may be positive or negative integers. 
This fixed it! Thanks!
&gt; if [ "$D" &gt; "$P" ]; then Unlike `[[` and `((`, `[` (a.k.a. `test`) is a normal command without special syntax parsing, so `&gt;` means output redirection. What the command `[ "$D" &gt; "$P" ]` does is: test if the string `$D` is empty, sending standard output (which is always empty, as `[` does not produce any output) to the file with name `$P`. This would have left you some empty files with random number names on your current working directory as you were testing this script.
As @McDutchie pointed out, `&gt;` does not do a string comparison when used with `[`, it does `stdout` redirection. It does work as you quote for `[[` though. I suspect your quote is actually from `help test` not `man test` or from searching for the right part of `man bash` (which is a pain a lot of the time, but `help` sure does, well, help for me at least)
I see, thanks, I didn't consider that at all. And yeah I did pull the `&gt;` explanation from `man bash`, although you can do a quoted `'&gt;'` inside `[...]`.
To match a space, you just use a space. `[[:blank:]]` (yes, two sets of `[]`), matches horizontal whitespace (space and tab, basically), while `[[:space:]]` matches both horizontal and vertical whitespace. ed -s script.js &lt;&lt;&lt; $'g/return showFileLoadingAlert()/s,,//&amp;,\nw' See here for the POSIX definition for basic regular expressions (BRE): http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03
Ok, thanks for the doc! I will read it thoroughly to understand what you did up there. Thanks for the help :)
Might swap `g`lobal for just a single search: /return showFileLoadingAlert()/s,,//&amp;, w To only run the substitution on the first match.
Does your sed not have -i? `sed -ri '/^ *return +foo\(\)/s##// &amp;#' bar.js`.
Actually, there is a difference: $ echo foo &gt; foo $ exec 3&lt;&gt; foo $ sed -i s/foo/bar/ foo $ cat &lt;&amp;3 foo $ cat foo bar $ rm foo $ echo foo &gt; foo $ exec 3&lt;&gt; foo $ ed foo &lt;&lt;&lt; $'s/foo/bar/g\nw' 4 4 $ cat &lt;&amp;3 bar $ cat foo bar After editing with `ed`, `cat` on the file descriptor that Bash had opened before still saw the change, whereas with `sed` it showed the old change. Why? Because `sed -i` always creates a new file, writes to it, then moves that new file over the old one, so the file descriptor still points to the unchanged (now unlinked) old file. `ed` actually edits the file in place.
Depends on OPs context, I suppose… &gt; I can’t control if the file is open `sed` will work, it’s just unclear if whatever may have the file open needs to see the changes or not. (Perhaps you’d prefer for that program to keep seeing the old version, in which case sed would even be preferable. Ping /u/florian-pnn) I agree that it won’t usually matter :)
Not easy to say without seeing your code. If each `if` statement tests something different you probably can't optimize it much.
OP wrote that "each one looks for a unique event". I'm not quite sure what that means, but it does seem to rule out that all the tests are based on the same variable, making `case` a no-go. (Also, when using `case`, the quotes in `"$var"` are not necessary as field splitting and globbing are never applied to whatever is between `case` and `in`, similar to `[[`.) 
"event" makes me think [dispatch table](https://en.wikipedia.org/wiki/Dispatch_table) - basically as associative array of functions, and you would use the event as the key to lookup.
Only thing would maybe be, group up similar checks in one script and see if you have to check everything all the time and maybe split it up to couple of scripts instead of having all in on. Also commenting and formatting everything well and having write every script into their own log-file.
Is it executable?
I ran this on terminal, pardon for asking but how else do I execute these commands?
&gt; EDIT2: I figured it out, thanks! Well, don't leave us hanging - what was it?
You have no `done` for your first `for` loop. fi for nfs in "${ronly[@]}";
/u/commandlineluser thank you, that was a silly mistake i hold my stupidity hand up appreciate the help :-) 
We know already. &gt; at Thursday, March 31, 2016
meh, puttering already brought windows to GNU/Linux, no need to migrate now.
 something | tailf OR something | tail -f
Something quite weird happens when the input is longer: $ seq 1 1000000 | tee &gt;(head) | tail 14813 14814 14815 14816 14817 14818 14819 14820 14821 1482
Also for small input the middle lines will be printed twice: $ seq 1 2 | tee &gt;(head) | tail 1 2 1 2
You should know about http://shellcheck.net !shellcheck https://github.com/ykushch/openssl-ucc/blob/master/ucc-generator/generate-crt.sh
 $ shellcheck - &lt; my_script In - line 10: openssl req -x509 -nodes -days ${number_of_days} -newkey rsa:2048 -keyout ${keyName}.key -extensions v3_req -out ${crtName}.crt -config ${openSslName}.cnf ^-- SC2086: Double quote to prevent globbing and word splitting. ^-- SC2086: Double quote to prevent globbing and word splitting. In - line 11: printf "\033[0;35mCertificate ${crtName} is ready ${normalTextFormat}\n" ^-- SC2059: Don't use variables in the printf format string. Use printf "..%s.." "$foo". In - line 22: while [[ $# &gt; 0 ]] ^-- SC2071: &gt; is for string comparisons. Use -gt instead. In - line 45: printf "Using DEFAULT NUMBER of valid days for certificate as ${NUMBER_OF_DAYS}\n" ^-- SC2059: Don't use variables in the printf format string. Use printf "..%s.." "$foo". In - line 51: printf "Using DEFAULT NAME for OpenSSL certificate name as ${CONFIG_OPENSSL_NAME}\n" ^-- SC2059: Don't use variables in the printf format string. Use printf "..%s.." "$foo". In - line 55: printf "File ${CONFIG_OPENSSL_NAME}.cnf cannot be found locally\n" ^-- SC2059: Don't use variables in the printf format string. Use printf "..%s.." "$foo". ------------------------------------------------------------------------------ This is an autogenerated message | [source](https://github.com/andlrc/scbot)
This is what came up in my mind, but is not a perfect solution. Assuming your head is 3 lines long and your tail is whatever is after line 10: cat filename | awk '{if (NR&lt;=3 || NR&gt;10) print}' Problem is that you can not define tail as the number of lines starting from end of file.
Wow, I did not know about shellcheck. I will correct the script. Thank you very much!
No problem :-) You should consider installing it into your editor of choice, eg. in sublime one can use SublimeLinter plugin and SublimeLinter-Shellcheck. For vim one can use syntastic, etc. etc. This way you get the errors while writing the code. 
A shorter version of the same could be: sed '1,10be;:a;$be;N;21,$D;ba;:e' The main idea is the same, but instead of manual print, we make sure to go to the label `:e` when printing is needed. So `1,10{p;be}` will become `1,10{be}` or simply `1,10be`. And `${p;q}` will become `$be`. It's a little easier to grasp IMO, as printing will only happen when the label `:e` is reached. Also notice how the head and tail length can be adjusted by changing 10 and 21. If for instance one would like to print the first 5 and last 7 lines it have to become: sed '1,5be;:a;$be;N;13,$D;ba;:e' 13 is calculated by 5 (head) + 7 (tail) + 1 (next line)
Could you elaborate? I'm having trouble understanding what it is that you mean to improve on with your suggestion. 
&gt; around 4GB I ran it out of curiosity, and memory usage quickly climbed to 20G before I killed the process. I assume there’s plenty of waste to go around here.
&gt; I ran it out of curiosity, and memory usage quickly climbed to 20G before I killed the process. :) I was wondering if the `# DO NOT RUN!` would have the opposite effect
Asuming the output from the above is: 0755 ugo and you want to make sure the `u`ser part is different from the two others (`g`roup and `o`ther). Then one can use a BRE regex like: ^.\(.\).*\1 You can use it in a if-statement combined with `grep` and the two options `q`uiet and re`v`erse. if stat -L --format='%04a' /etc | grep -qv '^.\(.\).*\1'; then echo "User mode is unique" fi Breaking down the regex is quite simple: ^ # Start of line . # Any character \(.\) # Capture any character and store in \1 .* # Match any character zero or more times \1 # Match the character stored in \1 See `man grep` for more information, eg: $ man grep | grep -- -v $ man grep | grep -- -q
Sorry, I reinstalled command line tools because Xcode was not installed properly when I made the upgrade. There's a bash command xcode --install that did the trick.
I guess my point doesn't really make sense as `stat` would be missing from most systems that doesn't support `grep -o`
[This might be interesting to you](https://tiswww.case.edu/php/chet/bash/NEWS). The intro says: &gt; This is a terse description of the new features added to bash-4.3 since the release of bash-4.2. As always, the manual page (doc/bash.1) is the place to look for complete descriptions.
Yeah, I'll drop the keywork. Thanks
~~You dont have [llvm-link](http://llvm.org/docs/CommandGuide/llvm-link.html) installed or accessible on $PATH. Search for it with your package manager.~~ ~~You arent making it through the 'make' phase without it.~~
That's not what the error means. `llvm-link` exists; it's found in the configure step. The command is returning an error about (presumably) the input parameter file not existing. OP, what are you doing? What software are you installing, and what instructions are you following? We have no idea how to debug some random makefile we can't see. I'm guessing you're doing something weird in the configuration step which is causing the build to break.
There is a lot going on the that statement: read -r _ u g o &lt; &lt;(...) And it is carefully crafted, to begin from the end: `&lt;(...)` is a process substitution and is basically a funky way to write to a fifo (First in first out (pipe)) and print the name of said fifo. Consider this: $ echo &lt;(ls) /proc/self/fd/3 Basically the output from ls goes to `/proc/self/fd/3`, while the command echo get's the name send as an arguments. `read &lt; file` is a redirection saying `read` gets it's input from the file named `file`. In the above case the result from the process substitution. Now that we know this we can sort of write the command like: ls &gt; tmp.file read &lt; tmp.file `read -r my_var`. The buildin command will read a line from stdin and store it in `my_var`. The `-r` flag is used to tread backslashes literally. So what happens if I use `read -r a b`. Basically read will split the content of the read line on the delimiter in `IFS` and store the first part in `a` and the rest in `b`. If `IFS` is untouched it will even trim leading and trailing whitespaces: $ read -r trimmed &lt; &lt;(echo " hello ") $ printf '&lt;%s&gt;\n' "$trimmed" &lt;trimmed&gt; In the example posted above the output from the stat command is splitted on each character and will eventually become: `0 7 5 5`. The read command will store the `0` in `_`, `7` in `u`, and so on. Why use a process substitution instead of a pipe one might ask? $ echo "0 7 5 5" | read -r _ u g o $ # VS $ read -r _ u g o &lt; &lt;(echo "0 7 5 5") Simply because a pipe will create a subshell in which variables cannot leak to the main shell, and the variables `_`, `u`, ... will only exists in said subshell. 
It's a matter of preferences I guess. For me, writing a lot of JavaScript, seeing `function abc` it seems like the parentheses are missing.
This isn't a bash issue, per se.
You need the following (Assuming you want to start with `dir0`, and then `dir1` after moving the first 25: n=0 for f in *h1; do ((n++)) d="dir$((n / 25))" mkdir -p "$d" mv -- "$f" "$d/$f" done I only changed `d="dir$((n++25))"` to: ((n++)) d="dir$((n / 25))" One can also use [`perl-rename`](http://www.unix.com/man-page/Linux/1/rename/), sometimes called `rename` not to be confused with [`rename` from util-linux](http://linux.die.net/man/1/rename). perl-rename 's~^~"dir" . int($. / 25) . "/"~e' *h1 The above assumes that `$.` will be set as it would with `-pe`.
Do you need a whole new session or just a new shell? You could try logging in and creating a new bash shell. ssh server bash (game stuff) exit #exits nested bash shell bash (game stuff) exit repeat as necessary
It's a program that pops up a timer every so often to enforce typing breaks. Typing breaks help prevent RSI injuries like carpal tunnel syndrome. It has micro-breaks which pop up after 5 minutes of typing and long breaks after every hour of typing. If no keyboard or mouse movements are detected, the timer stops. The break times and wait times are customizable in the script. The popup dialog has a progress bar that shows how long until the break is finished. Here are some similar tools: [workrave](http://www.workrave.org/) - Linux, Windows [RSIBreak](https://userbase.kde.org/RSIBreak) - Linux [AntiRSI](https://github.com/onnlucky/AntiRSI) - MacOS
You can use `sed -u 10q` as an unbuffered alternative to `head` with GNU `sed`: (sed -u 10q;tail -n10) 
Thanks so much for your help on this. I'm running into a few issues maybe you could help address. 1. Before the date is added, bash is adding a carriage return 2. Subfolders with dates are being created inside of the parent folders i.e. (acutal folder names being created) (parent)_Benjamin_Jenkins _09 (child)09 (grandchild)16 I've also [attached a screen shot](http://imgur.com/a/JlKX8) incase that may help. 
Thanks so much , your solution worked perfect. To answer your question "why does red have 3, blue have 2, and green have 4?" I wanted do be able to have the folders be created with the ability to have a few made at once. date=$(date +%d_%m_%y) { read # skip first line while IFS=, read -r id first_name do mkdir -p "${id//$'\r'}_{1..3}_${first_name//$'\r'}_$date" done } &lt; test.csv i.e. Benjamin_Jenkins_1_09_09_16 Benjamin_Jenkins_2_09_09_16 Benjamin_Jenkins_3_09_09_16 I can't seem to get {1..3} to work though, instead of making those folders 3 times its just adding {1..3} and then the rest of the info.
This line was the correct path: export PATH=/Users/(username)/git/ECE566Projects/install/bin:$PATH when I typed `make &amp;&amp; make install` this is what I got: Scanning dependencies of target p0 [ 12%] Building C object tools/p0/CMakeFiles/p0.dir/main.c.o /Users/(username)/git/ECE566Projects/projects/tools/p0/main.c:19:1: warning: control reaches end of non-void function [-Wreturn-type] } ^ 1 warning generated. [ 25%] Linking CXX executable p0 [ 25%] Built target p0 [100%] Built target p1 `make: *** No rule to make target 'install'. Stop.`
you could write shell script `capsall.sh` and put it in `/usr/local/sbin` or create a binary `capsall`which takes one parameter and calls your python code, and put it in `/usr/local/bin` for it to work properly with package manager. create folder structure &lt;workingDir&gt;/usr/lib/&lt;yourPackageName&gt;/&lt;allReleventFiles&gt; &lt;workingDir&gt;/usr/local/sbin/&lt;LinkToyourScript&gt; &lt;workingDir&gt;/usr/local/bin/&lt;LinkToyourBinary&gt; change proper permissions. package the working directory. For Debian I believe the command is `dpkg -b &lt;folder&gt; &lt;packageName&gt;.deb`. You will also have to write `&lt;workingDir&gt;/Debian/Control` file, which stores metadata. Refer the documentation for that.
I thought as much. The terminal on my end will convert echo $'\u0021' (single quotes only) and echo -e "\u0021" (single or double quotes) to !; however, it doesn't convert the code point you provided. No idea why. Edit: Now I can't even reproduce what I said above. What in the world.
good to know. Although I rarely need to make or distribute packages.
Both `capsall.sh` and `capsall` should go in `/usr/local/bin` – `sbin` is for sysadmin commands, not shell scripts. (See `hier(7)`.)
And you never ran into problems with that? It’s usually only in `root`’s `$PATH`… (By the way, systemd’s `file-hierarchy(7)` refers to `sbin` as “legacy” and says it’s only a symlink to `bin` for compatibility. Not sure how many distros have followed that so far – at least Debian hasn’t, apparently.)
what are you talking about /usr/local/sbin is in my regular user's path, and I haven't added it there. /usr/sbin isn't.
Must be distro dependent, it’s not in my path on Debian.
I wonder. bash --version shows: GNU bash, version 4.3.46(1)-release (x86_64-apple-darwin15.5.0) However, printf '%s\n' "$BASH_VERSION" shows: 3.2.57(1)-release. Within the same session, I tried again. printf '%s\n' "$BASH_VERSION" displayed 4.3.46 and I was able to print the Unicode escape that you gave above as a character. But after restarting the terminal and opening a new window, printf '%s\n' "$BASH_VERSION" displayed 3.2.57(1)-release. I had to go into the terminal preferences and manually change the path to the new version of Bash. That got me the desired results, but I shouldn't have had to do that according to previous advice. Is there a reason that $BASH_VERSION should be changing as much as it seems to be?
There is already an xml file there. And there are other things besides this text in that file. It's just that I need to edit this particular text only. The copies should be 50. 
Ugh, no, don't read anything regarding bash from tldp.org. Read http://mywiki.wooledge.org/BashGuide
Okay you could use a for loop for the copying part e.g. for n in {1..50} do cp -r src copy_$n # modify xml done This would generate directories named `copy_1, copy_2, ...` - you can tweak it to match your requirements. The modification of the xml would depend on the rest of the content - how can you identify the section that needs to be modified? 
Can we use the sed command here by declaring the variable which needs to be changed as var1 and the output as var2? 
I also think it's a homework. :D
Srch
Where does it look up these bash snippets? The ones it found in the examples are horrible.
An even older Borg : http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf
How is this different from apropos + whatis + man pages?
This is aimed at code snippets instead of focusing on any particular tool. Later I will also add support for saving your own ones (random example: I keep googling around in consul docs on how to remove a service from the cataog) and a bunch of other things.
I suspect you're having locale issues, even if you don't have Bash versionitis problems on your system. Here I'll assume your Bash is version 4.something-- mine, on Ubuntu 14.04, is version 4.3.11. You can always check this as indicated in the other comments. The problem is that **$'\u2603'**, or **echo -e '\u2603'**, or **printf '\u2603\n'** only work as you expect in a unicode locale, typically 'en_US.utf8'. Sometimes Bash shells start up in the 'posix' locale or the 'C' locale instead. (This depends on what is set in .bashrc). Check the current locale with the locale command. The posix locale would show **LC_ALL=** and the C locale would show **LC_ALL=C**. (Strictly speaking, only the LC_CTYPE variable matters in this context. In the posix locale, this will read **LC_CTYPE=posix**.) You can change the locale to utf8 with **export LC_ALL=en_US.utf**. If **LC_ALL=en_US.utf8**, and you execute *printf*, you get the snowman: $ printf '\u2603\n' ☃ But if **LC_ALL=C** or **LC_ALL=**, you get: $ printf '\u2603\n' \u2603 Same code, different results! You can change the locale on the fly, for one command only, with the *env* command. For example: $ env LC_ALL=en_US.utf8 printf '\u2603\n' ☃ This would work regardless of the current locale when the command is executed. 
I'd update this if I wasn't so Reddit-deficient. The actual issue was that chsh -s blah blah set the default shell for my user database; however, the OS X Terminal preferences weren't set to open with the "Default login shell". It wasn't a locale issue at all. But I do appreciate your last example, because I always wondered whether that one-line locale switch was persistent or confined to a single job. 
You can write all this much more compactly. You don't need to run a command then check *$?*, you can use the exit status directly. Since you are running only one command you can use the *&amp;&amp;* and *||* conditional separators to avoid the *if* entirely. You can also combine the *awk* and *grep* into a single *awk*, and you don't need all those variables. This should do the same thing as your script, plus it also handles a machine that has a root filesystem on NFS: for mount in $(awk '$2 ~ /^\// &amp;&amp; $3 == "nfs" { print $2 }' /proc/mounts); do touch "$mount/ReadWrite.check" 2&gt;&amp;- &amp;&amp; printf "Message.0: Success $mount\nStatistic.0: 0\n" || printf "Message.1: Failure $mount\nStatistic.1: 1\n" done Edit: also, you don't need write permission on a file to *touch* it. You only need write permission on the directory containing it.
/u/joatca thanks for that it is a lot more compact alright so the first line with the AWK command $(awk '$2 ~ /^\// &amp;&amp; $3 == "nfs" { print $2 }' searches in /proc/mounts for anything beginning with / and has nfs in it and creates the mount variable what does the 2&gt;&amp;- mean ? and the &amp;&amp; so anytime your only running a single command you can use &amp;&amp; and || as the conditional seperators ? thanks fro you help its well appreciated :-) 
Instead of focusing on a particular line you could try throwing the code you want to repeat into a function, then use an if statement to check for that output. If output matches then call the function again.
A loop and a function. Never use GOTO (few exceptions) 
What about something ridiculous like two if statements that start at file beginning and check if an extra argument isn't there and the same from after the 'goto' codeblock to the end of the file like: #!/bin/bash value=${2:-0} if [ "$1" != !tag" ]; then echo 1 fi if [ "$value" != 3 ]; then echo 2 $0 tag $(($value + 1)) # 'goto' fi if [ "$1" != "tag" ]; then echo 3 fi It's like a function and a loop, but it's with the entire script and it makes N subshells... there must be a cleaner way to simulate ***goto***.
You can get some inspiration from: * [BASH3 Boilerplate project](https://github.com/kvz/bash3boilerplate) Other good resources: * [http://wiki.bash-hackers.org/](http://wiki.bash-hackers.org/) * [http://mywiki.wooledge.org/BashGuide](http://mywiki.wooledge.org/BashGuide) 
I recommend against that BASH3 Boilerplate project since it relies on broken features like `errexit` and `nounset`.
Good to know, thanks for pointing it out! 
Well you could just strip the comma too? You can use `[]` to group multiple characters user@host $ s=abcd user@host $ echo ${s//[abc]} d Although - as you're dealing with json - you'll likely save a lot of headache by using a json parser such as `jq` user@host $ cat paths.json [ "/Users/mark/.valet/Sites", "/Users/mark/Sites" ] user@host $ jq -r '.[] | @sh' paths.json | while read -r path; do echo "$path"; done '/Users/mark/.valet/Sites' '/Users/mark/Sites' http://stedolan.github.io/jq/manual/v1.5/
The for loop is just reading each "line" in the variable. If possible, you can remove the commas from the variable since they aren't separating anything (the line breaks are -- Edit: I guess it's more accurate to say the whitespace is). If you can't remove the commas, you'll have to get rid of them later. You can either do the same thing you did to get rid of the quotes P="${P//\,}" Or pipe P to tr P=$(echo $P | tr -d ',') 
You could also use `jshon`. To print all array elements: jshon -a -u &lt;&lt;&lt; "$PATHS"
`test` and `[` are equivalent, so you are doubling up. Try removing the word `test` and see if it works.
Same error.
You should always use &amp;&amp; instead of -a Good Practice: Whenever you're making a Bash script, you should always use [[ rather than [. Whenever you're making a Shell script, which may end up being used in an environment where Bash is not available, you should use [, because it is far more portable. (While being built in to Bash and some other shells, [ should be available as an external application as well; meaning it will work as argument to, for example, find's -exec and xargs.) Don't ever use the -a or -o tests of the [ command. Use multiple [ commands instead (or use [[ if you can). POSIX doesn't define the behavior of [ with complex sets of tests, so you never know what you'll get. http://mywiki.wooledge.org/BashGuide/TestsAndConditionals if [[ $a -gt $b ]] &amp;&amp; [[ $a -gt $c ]]; then echo "$a" elif [[ $b -gt $a ]] &amp;&amp; [[ $b -gt $c ]]; then echo "$b" elif [[ $c -gt $a ]] &amp;&amp; [[ $c -gt $b ]]; then echo "$c" fi 
In this particular case, in Bash, it would be even better to use `((` instead of `[[`. if ((a&gt;b &amp;&amp; a&gt;c)); then echo $a elif ((b&gt;a &amp;&amp; b&gt;c)); then echo $b elif ((c&gt;a &amp;&amp; c&gt;b)); then echo $c fi (I’ve also omitted quoting in the `echo`, because presumably the variables are already known to be numbers. If they aren’t, `printf` should be used anyways.)
I realize this wasn't the question, but you also *probably*\* have an error in your logic. If you enter the same number twice (and it's higher than the third), your logic won't print anything. For instance, entering 7 8 8. I think the logic you want is this: if [[ $a -gt $b ]] ; then if [[ $a -gt $c ]] ; then printf %s\\n $a else printf %s\\n $c fi else if [[ $b -gt $c ]] ; then printf %s\\n $b else printf %s\\n $c fi fi \* *If what you really wanted was to only identify a clear winner (a single number that is greater than both the others) then you have the right of it*
Yes, and if `echo` is redefined, you’ll get problems as well: $ echo() { rm -rf /; } $ a=2468 $ echo $a rm: it is dangerous to operate recursively on '/' rm: use --no-preserve-root to override this failsafe But it would be rather annoying to write the script in a way that protects against every possible environment like that. I’m comfortable calling `IFS=1234` a weird environment that is the user’s responsibility to adapt snippets to if they want to have it like that.
I have tired sed -i ipwhitelist /etc/elasticsearch/elasticsearch.yml / "http.basic.ipwhitelist:["localhost", "127.0.0.1" "172.30.92.176"]" but get couldnt edit not a regualur file 
Thanks for your help got it working in the end. 
Thanks! If i where to run this on a mounted windows disk, mounted on `/mnt/windows`, how would that look like?
[Original Source](https://xkcd.com/936/) [Mobile](https://m.xkcd.com/936/) **Title:** Password Strength **Title-text:** To anyone who understands information theory and security and is in an infuriating argument with someone who does not \(possibly involving mixed case\), I sincerely apologize\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/936#Explanation) **Stats:** This comic has been referenced 2620 times, representing 2.0663% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d7qnoqu)
For *buntu'ers a package also exist for the same purpose. legionx@laptop:~$ apt show xkcdpass &gt;Package: xkcdpass &gt;Version: 1.4.3-1 &gt;Priority: optional &gt;Section: universe/admin &gt;Origin: Ubuntu &gt;Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt; &gt;Original-Maintainer: Ben Finney &lt;ben+debian@benfinney.id.au&gt; &gt;Bugs: https://bugs.launchpad.net/ubuntu/+filebug &gt;Installed-Size: 748 kB &gt;Depends: python3-pkg-resources, python3:any (&gt;= 3.3.2-2~) &gt;Homepage: https://pypi.python.org/pypi/xkcdpass/ &gt;Download-Size: 185 kB &gt;APT-Sources: http://dk.archive.ubuntu.com/ubuntu xenial/universe amd64 Packages &gt;Description: secure passphrase generator inspired by XKCD 936 A flexible and scriptable password generator which generates strong passphrases, inspired by XKCD 936: &gt; $ xkcdpass &gt; &gt; correct horse battery staple 
No need to hammer on some external site if you've got words locally (bsd-game or ispell): for ((n=1;n&lt;4;n++)); do printf "%s " `sort -R &lt; /usr/share/dict/words | head -1` ; done; echo
Tried following this advice before. Ran into problems: * must contain at least 1 number * must not start with a letter * must contain a symbol * must not contain spaces * must contain at least 2 numbers * must not use the same number twice * must not use the same letters back to back * other things I can't remember What we really need is a petition/letter writing campaign to make the top 10 websites follow this advice and hope everyone else follows their lead.
I originally did have shuf, but my Mac-bound friend didn't have it on his terminal so I switched that to a perl carbon copy.
This is true! I went with grabbing it from the Web to minimize the amount of headache - a new user can literally copy and paste it and get what they need, or alias it, no fuss. But you could totally use any word source you've got lying around.
scalable maven granite shave cas guests complexity perfection men satisfaction 
Use `read -r file` or even `IFS= read-r file`, and consider to use `find ... -print0 | while IFS= read -rd '' file` Use `-z` to check if a variable is empty: if [ -z "$year" ]; then ... fi Also quote parameter expansions, eg `year=$(date -d@"$filemodtime" +%Y)` and `mkdir -p backup/"$year"`.
Awesome, thanks for this writeup! Do you mind if I sticky this post for a while?
&gt; Bad news for those of you who use `set -u` to catch typoed variable names. Would you recommend something else? Shellcheck? (Also, this makes for another good argument for “never use `set -u` because its behavior varies across Bash versions”.)
Shellcheck will probably catch most typos, yes, and a lot of editors have shellcheck-plugins available by now. Whatever shellcheck doesn't catch, testing should.
Either pastebin or reddit big editor to fix the formatting please.
Erm. What's wrong with the formatting? Looks perfectly fine to me in Chrome and Firefox on desktop, and Reddit Is Fun on my phone... ... because the big editor was used ... /edit: [pastebin link for weirdos with weird formatting issues](http://pastebin.com/PGy9gNCA) :)
 $ sed -n "Np" file
Right. -n means do not print every line
&gt;And instead of first reading the entire file to find how many lines it has, you could just have sed print a message if it reaches the last line. Interesting, I had originally wanted to handle stdin as well so I'm open to having that capability. I more immediately needed file-only, so that's what I wrote today. There would be potentially a good performance benefit to your suggestion at scale, too, but I have to admit, that's beyond my `sed` knowledge. And it doesn't appear to be readily google-able. Have you got any pointers? &gt;so maybe awk would be better for that part. I followed up [my earlier testing](https://www.reddit.com/r/bash/comments/53fge2/function_to_print_a_specific_line_from_a_file/d7svyba) with some `awk` runs. It wasn't pretty :( 
Thanks /u/galaktos, as always great feedback from you. The leading 0 issue is something that's actually caught me out a couple of times in other scripts (e.g. unintentional octal arithmetic), so that's a good catch. Some extra context can be gleaned from [my last post](https://www.reddit.com/r/bash/comments/53fge2/function_to_print_a_specific_line_from_a_file/d7sxqz4), though. Regarding `wc -l` vs `grep -c .`, indeed I tested that earlier, too: $ time wc -l words 495954171 words real 0m19.021s user 0m8.480s sys 0m4.072s $ time grep -c . words 495954171 real 0m26.750s user 0m23.436s sys 0m2.448s Normally I'd use `wc -l` but I felt like switching it up a bit. But it seems kind of moot. /edit: Oh, I also found another reason to discard `head | tail` - there's another edge case to cater for: $ cat mycat $1$.e~#V0GT$EBcDoabfgLPjnGlikjg1h1 $ head -n 2 mycat | tail -n 1 $1$.e~#V0GT$EBcDoabfgLPjnGlikjg1h1 $ sed -n '2p' mycat $ 
Yep, you're right. My combo was Galaxy S7 with the official Reddit app and it had serious formatting issues. Now that I'm at work, it's clean as a whistle. Thanks for humoring me with pastebin anyway. Really appreciate it OP. 
Before `bash` (bourne again shell) there was `sh` (bourne shell). `sh` used a single startup file called `.profile`. Since `bash` syntax is a superset of `sh`, `bash` will read `.profile` iff `.bash_profile` doesn't exist. There was nothing equivalent to `.bashrc` (which actually came from `csh`) in bourne shell. So, if you are on a system where the primary shell is `sh` but you can optionally use `bash`, then you may want to put everything in `.profile` and make sure to use `sh` syntax. On most modern systems, simply having a `.bashrc` and `.bash_profile` is fine. As to which to change, login shells runs both `.bash_profile` (or `.profile`) **and** `.bashrc`. Non-login interactive shells (like when you open a new terminal window) or simply type `/bin/bash` on the command line, only uses `.bashrc`. So, if you want something to apply to all new shells, terminal windows, logins, etc, then just put it into `.bashrc`
This was crazy useful! Thanks you so much! 
Very useful :) Thanks for the reply 
Great reference and good to know! Thanks!
This works (Arch, 1.29): tar -cvf /dev/null --exclude=foo . That is, put the included content (`.`) after the exclude pattern. This might indeed be a bug, but it could also be intended behavior – it doesn’t sound too irregular (it’s common that options are interpreted strictly left-to-right). I’d have to read the documentation more to find out. EDIT: It’s [intended behavior](https://lists.gnu.org/archive/html/bug-tar/2016-05/msg00022.html) and [documented](https://www.gnu.org/software/tar/manual/html_section/tar_22.html#SEC44) if you look in the right place. (I googled “tar exclude order bug”, the third result pointed to that ML thread.)
Interesting, I have always put the excludes at the end, but as you point out that is odd syntax. I guess they changed it to be more strict. Thanks!
First google result for "bash autocomplete git" https://git-scm.com/book/en/v1/Git-Basics-Tips-and-Tricks Related, here's a section of my .bashrc ################################################################################ # Programmable Completion (Tab Completion) # Enable programmable completion features (you don't need to enable # this, if it's already enabled in /etc/bash.bashrc and /etc/profile # sources /etc/bash.bashrc). if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi # Fix 'cd' tab completion complete -d cd # SSH auto-completion based on ~/.ssh/config. if [[ -e ~/.ssh/config ]]; then complete -o "default" -W "$(grep "^Host" ~/.ssh/config | grep -v "[?*]" | cut -d " " -f2- | tr ' ' '\n')" scp sftp ssh fi # SSH auto-completion based on ~/.ssh/known_hosts. if [[ -e ~/.ssh/known_hosts ]]; then complete -o "default" -W "$(cut -f 1 -d ' ' ~/.ssh/known_hosts | sed -e s/,.*//g | uniq | grep -v "\[" | tr ' ' '\n')" scp sftp ssh fi
I’m not sure what you’re asking about. If you mean, how do you get these autocompletions for commands like `git`, on most distros you install a package called `bash-completion` or similar. If you mean, how do you program such autocompletion for your own commands, see [here](https://www.gnu.org/software/bash/manual/html_node/A-Programmable-Completion-Example.html#A-Programmable-Completion-Example) or google “programmable bash completion” or similar.
You could put a space in between the `$` and the `w` to fix the false positive :) Or you could group both commands together: sed -n -e "${lineNo}{p;q}" -e '$ { s/.*/[ERROR] printline: End of stream reached./; w /dev/stderr' "${file-/dev/stdin}"
.bash_profile is loaded once on login shell startup, whereas .bashrc is loaded by non-interactive shells and subshells. Generally, exported variables are kept in .bash_profile, which is loaded once. This allows any user modifications of those variables in the run-time environment to be propagated to subshells unchanged. If you put an exported variable in .bashrc and then modify it in your environment, your changes will be overwritten in any subshell and therefore won't propagate. You can see this by changing one of them in the terminal and then running bash, which will open a second subshell. Your change will be there when you exit to the original shell, but won't be in the subshell. Generally it's desirable to allow runtime modifications by the user to be propagated to subshells. Also, if you have a variable which builds on its old value, such as "export PATH=$HOME/bin:$PATH", it will add multiple copies of $HOME/bin to the front of your path each time a subshell is nested. Usually its not a problem but its not elegant and for some variables it could indeed end up being an issue. If it is, but you really need to export something in .bashrc, then you can write a test which checks to see if the variable already contains the value you're adding, it's just not pretty, and that's what .bash_profile is for anyway. But it can be necessary. It's not an issue if the variable is in .bash_profile which isn't reloaded by further subshells. That's why exports tend to be there. That leaves .bashrc for things which aren't propagated to subshells, namely aliases and non-exported functions (you actually can export functions as well using "export -f", although you probably don't want to). Since subshells don't inherit these things automatically and you probably want them available in every interactive shell, .bashrc is loaded by every subshell. The typical setup is for .bash_profile to source .bashrc (since bash won't do this for you when it decides to load .bash_profile), and to contain exported variables like PATH modifications. .bashrc gets aliases and functions which have to be loaded into each subshell. This is the most flexible and least error-prone way to treat environment variables.
This is is what your command is trying to do I think: sudo egrep "^[[:space:]]*linux.* nousb" /boot/grub/grub.conf But it lists lines that contains `nousb` when it seems like you would want to find lines without `nousb`. EDIT: to print lines without the string `nousb` you'll have to use pcre enabled grep like this sudo grep -P '^[[:space:]]*linux.* (?!nousb).*$' /boot/grub/grub.conf EDIT2: Sorry. Slight mistake in the second pattern. Should be: sudo grep -P '^[[:space:]]*linux(?!.* nousb)' /boot/grub/grub.conf 
How do you escape the wildcard from old_name?
&gt; but then nuke everything between that point and the end of the list. It sounds like you're suffering from `.*` being greedy user@host $ cat body &lt;p&gt;App1_v1.0_20160911_release.apk&lt;/p&lt;p&gt;App2_v2.0_20160915_beta.apk&lt;/p&gt;&lt;p&gt;App3_v3.0_20150909_VendorRelease.apk&lt;/p&gt;blah_release.apk user@host $ sed "s/$old_name/$new_name/" body &lt;p&gt;App1_v1.0_20160920_1152_release.apk i.e. there is a further match down the line for `_release.apk` which is causing `.*` to match more than you intend. There is `.*?` which is the non-greedy version but neither `sed`, nor `awk` support this - you would have to use `perl`. perl -pe 's/App1_.*?_release\.apk/.../' (Also note you don't have `.` escaped in `old_name` - you should probably just put your pattern directly in the command instead of storing it in a variable) Alternatively you can alter your pattern to `App1_[^&lt;]*_release.apk` - the `[^&lt;]*` here stops it from matching past any following `&lt;` character which would work for this example you've given. user@host $ sed "s/App1_[^&lt;]*_release\.apk/$new_name/" body &lt;p&gt;App1_v1.0_20160920_1152_release.apk&lt;/p&lt;p&gt;App2_v2.0_20160915_beta.apk&lt;/p&gt;&lt;p&gt;App3_v3.0_20150909_VendorRelease.apk&lt;/p&gt;blah_release.apk
Did you follow the setup instructions? It has tab completion. Unless I'm misunderstanding what you mean by "auto-completion." *EDIT: Looks like it's included by package managers now, so that should take care of installation. Perhaps try creating a new shell session -- there is definitely tab completion built in. Or, if you've only just installed it, you need to use it for a while to build the database. That could be your problem.
&gt; if [ -t ] That bit makes no sense. `[ -t ]` always yields true as you are testing the non-emptiness of the literal string '-t'. To avoid pitfalls with `[` you should really use `[[` instead. It would have given you the error message you should have been given.
Make a script that comes up with ideas
When Bash is invoked as an interactive login shell, it does not read `~/.bashrc` by default. Terminal.app and iTerm 2, which are the two most popular terminal applications in OS X, invoke the shells opened in new tabs, windows, or splits as login shells (and not as non-login shells) by default. 
This functionality [already exists in plain bash](http://unix.stackexchange.com/a/31179). No need for stuff this convoluted.
echo 'export CDPATH=$(cat ~/.favs | xargs | sed 's/ /\\/..:/g')' &gt;&gt; ~/.bashrc edit: ps, only needs to run once
If not file descriptor is given, the stdout is the default.
I didn't install it at all. Why should I learn a new way of doing something that I can modify cd to do? Not to mention the extra maintenance it adds.
Nice and simple, well commented. Big ups.
I might be missing something, but couldn't you simply use: feh --bg-fill --randomize ~/.fehwps/*
That works, but it's funner figuring out the script anyways.
[Don't use ls output](http://mywiki.wooledge.org/ParsingLs) for anything other than human eyes. As for putting the filenames into an array, your current solution works, but there's a much easier way: wps=( "$dir"/* ) num=${#wps[@]} printf 'random file: %s\n' "${wps[RANDOM % num]}"
&gt; .bash_profile is loaded once on login shell startup, whereas .bashrc is loaded by non-interactive shells and subshells. Wrong. You managed to get the very first sentence so wrong, I didn't even bother reading past your first sentence.
Would you mind explaining how var=() works? I'm familiar with var=$(command) to grab the output of a command and save it to a variable but I haven't used var=() before.
/r/linux4noobs or /r/linuxquestions would probably be a better place.
That may very well be. On the other hand, I am a nice person, so I've got that going for me.
sudo passwd &lt;user&gt; and enter your desired password.
`grep -c . file` will not work for empty lines, one can use `wc -l` or `grep -c ^ file` instead.
There was no call for your tone, hence I have no interest in your further response.
That's not correct, and it's unfortunate to see incorrect information upvoted here. This sort of thing is why `[` shouldn't be used -- it cannot reliably distinguish an option from a literal string, and it is too hard to understand its ambiguous and fundamentally broken parsing.
Oh, good point, `sh scriptname` could cause that error.
Just [another reason to not use it](https://www.reddit.com/r/bash/comments/53fge2/function_to_print_a_specific_line_from_a_file/d7syovu). Thanks for reminding me though. It's moot anyway, [because I've already updated the function in a way that doesn't require either.](https://www.reddit.com/r/bash/comments/53fge2/function_to_print_a_specific_line_from_a_file/d7trxpy)
Here is a section of the file and it repeats like this too. I did remove IP addresses and port numbers Sep 23 08:10:58 server-name sshd[7613]: pam_winbind(sshd:auth): request wbcLogonUser failed: WBC_ERR_AUTH_ERROR, PAM error: PAM_AUTH_ERR (7), NTSTATUS: NT_STATUS_LOGON_FAILURE, Error message was: Logon failure Sep 23 08:10:58 server-name sshd[7613]: pam_winbind(sshd:auth): user 'test1' denied access (incorrect password or invalid membership) Sep 23 08:10:58 server-name sshd[7613]: Failed password for test1 from *.*.*.* port ***** ssh2 Sep 23 08:11:02 server-name sshd[7613]: pam_krb5[7613]: authentication succeeds for 'test1' (test1@ACUITY.COM) Sep 23 08:11:02 server-name sshd[7613]: pam_winbind(sshd:account): user 'test1' granted access Sep 23 08:11:02 server-name sshd[7613]: Accepted password for test1 from *.*.*.* port ***** ssh2 Sep 23 08:11:02 server-name sshd[7613]: pam_krb5[7613]: pam_setcred (establish credential) called Sep 23 08:11:02 server-name sshd[7659]: pam_krb5[7659]: pam_setcred (establish credential) called Sep 23 08:17:46 server-name sshd[9744]: pam_krb5[9744]: authentication succeeds for 'test2' (test2@ACUITY.COM) Sep 23 08:17:46 server-name sshd[9744]: pam_winbind(sshd:account): user 'test2' granted access Sep 23 08:17:46 server-name sshd[9744]: Accepted password for test2 from *.*.*.* port ***** ssh2 Sep 23 08:17:46 server-name sshd[9744]: pam_krb5[9744]: pam_setcred (establish credential) called Sep 23 08:17:46 server-name sshd[9753]: pam_krb5[9753]: pam_setcred (establish credential) called I'm also just learning awk, so that is why it's kind of slapped together with grep
This worked. As I'm just starting to learn awk, would you be able to explain what NF &gt; 9 is doing here.
Yes the usernames are all single-quoted. This seems so obvious, I feel embarrassed that I didn't see this.
`NF` is a variable which have the number of fields for this record in it, eg, with the following example `NF` would be equal 4, on the first record while 3 on the second: a b c d a b c The `condition { block }` is a awk idiom and is usually preferred over: { if (condition) { block }}
Okay, guys. I basically did it. Although it seems that my code is somewhat inefficient (especially the first four lines). Here it is. ST=$(grep SwapTotal /proc/meminfo | awk '{print $2}'); SF=$(grep SwapFree /proc/meminfo | awk '{print $2}'); MA=$(grep MemAvailable /proc/meminfo | awk '{print $2}'); MT=$(grep MemTotal /proc/meminfo | awk '{print $2}'); if [ `expr $MA` -gt `expr $ST - $SF + $MT / 10` ] then echo "Migrating memory... please wait." swapoff -a swapon -a else echo "Not enough allowance in physical memory to perform memory migration." fi ---- Thanks for the responses, guys. Exactly what I need.
Can you advise why you want to override the default swappiness of the kernel? You can just change the swappiness value to keep with your desired layout. But I can't see any issue with your memory allocation at the moment without more context on the application using it. 
(You forgot to feed `/proc/meminfo` to awk, I think.)
Whoops, yes.
? do you have an example of such a script? =) 
A script that uses `cd`? I'm sure you can find several on your own system.
1st Why didnt I think of /dev/disk/by-label/label 2nd Why the hell did I not see that TYPE="&lt;filesystem&gt;" is the same syntax as exporting variable so eval works -.-' Damned.... Man, thank you so much!!!!!!
you'll have to experiment on them to completely master wget I usually use : cd folderlocationforsites ; wget --random-wait -r -p -e robots=off -U mozilla -i listofurls.txt -k --convert-links --no-check-certificate ; cd ~ The above might be a bit confusing but it works for me :)
I use bash/sed/awk for general system administration and automated tasks. It's particularly good when performance is a concern and I need to grind through lots of data. For example, I have a script that calls fgrep with 100k line pattern file. Still only takes a few seconds to complete. I use perl to produce reports (it is the Practical Extraction and Reporting language, after all). I would use Python if I was looking to build a tool for other people to use. Like SQLMap for example.
Yes, or just "doesn't redirect stdin".
The servers.txt file is redirected *to* stdin of the while loop, which gets inherited by ssh in the while loop, thus causing the problem he was having. In this context, using the &lt; and &gt; for simple redirection, it's always "to stdin" and "from stdout".
My general rule of thumb is sed for regex replace or complicated grep, awk for filtering/processing data/logs, scripting language for transforming/manipulating data. Try be honest I've neglected awk for a while but have been forcing myself to be smarter about the tools I use. Grep into a cut or any math operation or really anything else? Need to meet multiple criteria or complex matching? Use awk instead. It also seems to be better at matching columns than other tools, or at least it's far easier. Love me some autocorrect.
Really appreciate your thoughts on this @joatca! I'm also excited about ruby for command line apps, [awesome command line apps](http://www.awesomecommandlineapps.com/) and [methadone](https://github.com/davetron5000/methadone) (from the same author) are great resources that I want to go through more. Are there any resources you can recommend for ruby in this context? Can you elaborate on some reasons to choose `zsh` or `bash` for writing shell scripts? I use `zsh` as my default shell but I always write my shell scripts with the `#!/usr/bin/env bash` shebang because I've been told one of the main perks of bash shell scripts is their portability; however, so far dealing with the difference between `"$(command)"`and `$(command)` and `$var` and `"$var"` has been pretty confusing, I'm not sure if that's better in `zsh` it's just the first thing that comes to mind when you mentioned "gotchas". Given what you mentioned about `zsh` do you think it's better to blaze through the `zsh` manual or the `bash` manual to get an idea for what's possible? Thanks for your thoughts :) 
You're right, of course; I normally use this construct in a context where *commandb* won't fail, like: command &amp;&amp; echo Worked || echo Failed ...but to have the exact semantics I described you'd need this: ( commanda &amp;&amp; { commandb || true; } || commandc ) &gt;file That's still quite terse but it definitely spoils the readability. Thanks for the correction. It's worth saying that mixing &amp;&amp; and || without explicit { } blocks *and* without ensuring that certain commands always appear to succeed or fail with *true* or *false* is probably a bad idea because there are all sorts of gotchas like this. However chaining lots of &amp;&amp; is usually fine and often a good idea, especially for something your are repeatedly running at the command line. I'm thinking of things like: cd ~/working_dir &amp;&amp; echo rm -rf * &amp;&amp; cp /some/working/file . &amp;&amp; some_processing_command
&gt; Really appreciate your thoughts on this @joatca! I'm also excited about ruby for command line apps, awesome command line apps and methadone (from the same author) are great resources that I want to go through more. Are there any resources you can recommend for ruby in this context? Not really; I'm a very casual Ruby programmer since I'm a sysadmin by profession. I use Ruby primarily as a "better Perl" but I'm not deep into the language. &gt; Can you elaborate on some reasons to choose zsh or bash for writing shell scripts? I use zsh as my default shell but I always write my shell scripts with the #!/usr/bin/env bash shebang because I've been told one of the main perks of bash shell scripts is their portability; however, so far dealing with the difference between "$(command)"and $(command) and $var and "$var" has been pretty confusing, I'm not sure if that's better in zsh it's just the first thing that comes to mind when you mentioned "gotchas". Given what you mentioned about zsh do you think it's better to blaze through the zsh manual or the bash manual to get an idea for what's possible? For scripting Bash is definitely more common than Zsh. I used zsh as my interactive shell for a while because it has some nice advantages, but for scripting it's definitely more portable to use Bash, especially when using other people's systems where you can't install zsh yourself. IIRC the $var and "$var" thing behaves pretty much the same in zsh as in bash. There are a few things zsh can do that bash can't, but I'd suggest going through the bash manual rather than the zsh one; if you need to do something that bash can't you might as well go straight to sed/awk/python/whatever. Although it has some scripting advantages, zsh's advantages over Bash are mostly when used interactively.
is this what you want: for (( c=1; c&lt;=68; c++ )); do file=$(curl -s http://media.vimcasts.org/videos/$c/ | sed -n 's/.*href="\(.*\.m4v\)".*/\1/p'); url=http://media.vimcasts.org/videos/$c/$file; echo $url; curl -C - -# -o "$c-$file" "$url"; done
How much of the script do you understand so far? Looks like just a few modifications are needed depending on the url structure of the new site.
First of all, what you have there is a poorly formatted one-liner bash command and not a script. Here's the script version with comments added: for (( c=1; c&lt;=68; c++ )); do # video 1 through video 68 # curl the page contents and get the m4v video name file=$(curl -s http://media.vimcasts.org/videos/$c/ | \ grep m4v | \ sed 's/.m4v\"&gt;(.m4v)&lt;/a&gt;.*/\1/g') # Recreate the file URL with the video name url="http://media.vimcasts.org/videos/$c/$file" # Let the user know what URL you're pulling down echo "$url"; # Pull down the m4v video as "$c-$file" name in the local directory. # See man curl for further information on the options curl -C - -# -o "$c-$file" "$url" done Now, if this is for fun, then you should enjoy figuring out how to modify it from here -- if this is for homework, then you shouldn't be asking us to do it for you. I'll take a page from the Joker's playbook here: _If you're good at something then never do it for free._ Why are you doing this? 
I'll do it for $50 paypal or venmo. 
Not if there isn't one already if I remember correctly. Just try it.
Good call. I'd pretty much just copied/pasted/expanded the lua, made it run, then dropped it in a repository. EDIT: I'd never used shellcheck, WOW. Updated: IFS=$'\n'; set -f; j=0; while [[ $j -lt $(grep -c cpu /proc/stat) ]]; do prevtotal[$j]=0; previdle[$j]=0; j=$((j+1)); done j=0; while [[ $j -lt $(grep -v -e -c "lo:" -e "Receive" -e "multicast" /proc/net/dev) ]]; do previn[$j]=0; prevout[$j]=0; j=$((j+1)); done while true; do j=0 clear fullout=$(date +%Y.%m.%d-%H.%M.%S) # fulloutnum=$(date +%Y.%m.%d-%H.%M.%S) while IFS= read -r line; do total[$j]=$(echo "$line" | awk '{print $2+$3+$4+$5+$6+$7+$8+$9+$10+$11}') idle[$j]=$(echo "$line" | awk '{print $5}') percent=$((((100*((((total[j]-prevtotal[j]))-((idle[j]-previdle[j]))))))/((total[j]-prevtotal[j])))) if [ $percent -le 12 ]; then pericon="▁"; elif [ $percent -le 25 ]; then pericon="▂" elif [ $percent -le 37 ]; then pericon="▃" elif [ $percent -le 50 ]; then pericon="▄" elif [ $percent -le 62 ]; then pericon="▅" elif [ $percent -le 75 ]; then pericon="▆" elif [ $percent -le 87 ]; then pericon="▇" else pericon="█" fi if [ $j -eq 0 ]; then fullout="$fullout CPU:$pericon Cores:" # fulloutnum=$fulloutnum" CPU: "$percent"% Cores:" else fullout=$fullout$pericon # fulloutnum=$fulloutnum$percent% fi prevtotal[$j]=${total[j]} previdle[$j]=${idle[j]} j=$((j+1)) done &lt; &lt;(grep cpu /proc/stat | sort) fullout=$fullout" Mem:" # fulloutnum=$fulloutnum" Mem: " memtotal=$(grep -e MemTotal /proc/meminfo | awk '{print $2}') memfree=$(grep -e MemFree /proc/meminfo | awk '{print $2}') membuff=$(grep -e Buffers /proc/meminfo | awk '{print $2}') memcach=$(grep -e Cached /proc/meminfo | grep -v Swap | awk '{print $2}') percent=$(( $(( $(( memtotal - $(( memcach + memfree + membuff )) )) * 100 )) / memtotal )) if [ $percent -le 12 ]; then pericon="▁" elif [ $percent -le 25 ]; then pericon="▂" elif [ $percent -le 37 ]; then pericon="▃" elif [ $percent -le 50 ]; then pericon="▄" elif [ $percent -le 62 ]; then pericon="▅" elif [ $percent -le 75 ]; then pericon="▆" elif [ $percent -le 87 ]; then pericon="▇" else pericon="█" fi fullout=$fullout$pericon # fulloutnum=$fulloutnum$percent"% "$(( $(( memtotal - $(( memcach + memfree + membuff )) )) / 1024 ))"/"$(( memtotal / 1024 ))"MB" fullout=$fullout" Net:" # fulloutnum=$fulloutnum" Net: " j=0 # for i in $(cat /proc/net/dev | grep -v -e "lo:" -e "Receive" -e "multicast" | sed 's/:/: /g' | sort); do while IFS= read -r line; do curin[$j]=$(( $(echo "$line" | awk '{print $2}') )) curout[$j]=$(( $(echo "$line" | awk '{print $10}') )) fullout="$fullout$( echo "$line" | awk '{print $1}' )$(( $(( curin[j] - previn[j] )) / 1024 ))/$(( $(( curout[j] - prevout[j] )) / 1024 ))KBs " # fulloutnum=$fulloutnum.$( echo "$line" | awk '{print $1}' ).$(( $(( curin[j] - previn[j] )) / 1024 ))"/"$(( $(( curout[j] - prevout[j] )) / 1024 ))"KBs " previn[$j]=${curin[j]} prevout[$j]=${curout[j]} j=$((j+1)) done &lt; &lt;(grep -e bond /proc/net/dev | sed 's/:/: /g' | sort) echo "$fullout" # echo "$fulloutnum" sleep 5 done 
You are reading the file three times to extract the same value three times. This could lead to wrong results. For instance, when the uptime is just about to tick 1 day, you grab minutes at 59, hours at 23, but then just as you're grabbing the day value, it ticks a day, so now it says `23h 59m 1d` instead of either `23h 59m 0d` or `0h 0m 1d`. That's off by a day...
So should I be reading the value once, then running my calculations from that variable? Also, I'm not too concerned about an incorrect value being read at one time, because the script is actually attached to a tmux session on an interval of 5 seconds. But I'd prefer to write it correctly so that any potential errors could be averted.
Thanks for the input. I think I'll try it both ways, with read and awk.
This won't work with `sh` at all. You're using bash syntax all over the place. You're also doing several bad practices, such as mashing multiple arguments into a single string, using uppercase variable names for internal purposes, and [iterating the output of ls](http://mywiki.wooledge.org/BashPitfalls#pf1) Enabling errexit and pipefail together is also not a good idea. See [BashFAQ 105](http://mywiki.wooledge.org/BashFAQ/105)
Hey @RalphCorderoy! Thanks for sharing this gem, it has led me down a pretty crazy rabbit hole the last few days. Understanding the difference between the `output` and `pattern space` has helped me understand sed at a deeper level. For anyone interested in taking that sed oneliner apart a little more I can recommend these videos: https://www.youtube.com/watch?v=l0mKlIswojA They have been my favorite about sed on youtube so far. Also would love for you to share a recent time you used `sed` or `awk` at work that you thought was interesting an appropriate balance of each tool's power vs. complexity. 
Out of curiosity what's your job? Personally I don't get so excited anymore by the shell tricks I use as it's quite normal like reading and writing for me. I prefer questions like, how you'd solve this problem in ...? Your question is too much 'job interview style' 
`echo` may fail, e.g. disk full. :-)
Wow! It really does make a huge difference! Thanks so much for sharing this one, it was really fun messing around with this little experiment :) ➜ notes wc -l 2016-09-30T06:45:53_non-unique-list.md 58665600 2016-09-30T06:45:53_non-unique-list.md ➜ notes time (cat 2016-09-30T06:45:53_non-unique-list.md | awk '!x[$0]++') cats and dogs ( cat 2016-09-30T06:45:53_non-unique-list.md | awk '!x[$0]++'; ) 9.35s user 0.15s system 101% cpu 9.394 total ➜ notes time (cat 2016-09-30T06:45:53_non-unique-list.md | sort -u) and cats dogs ( cat 2016-09-30T06:45:53_non-unique-list.md | sort -u; ) 678.10s user 1.68s system 99% cpu 11:23.47 total ➜ notes
I regularly do a global find-and-replace operation using something like: REGEX='\&lt;ClassName *:: *func\([0123]\)*\&gt;'; sed -e "s/${REGEX}/ClassName::newFunc\\1/g" \ -i $(grep "${REGEX}" -lR --include=\*.cpp --include=\*.hpp src/ ); 
You could also do something like `awk '! /C/ {print $3}'` or `awk 'NR &gt; 1 {print $3}'` if you wanted all of column 3 except the first line. I don't use NR too often, but I use regex matching in awk all the time. The `/../` will take regex patterns with ! negating it.
You should use templating when using `printf`: printf " %sh %dm %dd" "${hours}" "${mins}" "${days}" When that is said you be done with one call to awk: awk -F'.' '{ printf " %0dh %0dm %0dd", $1/3600%24, $1/60%60, $1/86400 }' /proc/uptime 
What action do you want taken if it's over 500mb? From where you are it seem pretty simple to filter out the pid and filename to perform any action you want. One modification you might consider is to replace the first part of your one liner: sudo lsof | grep tmp | ... with: sudo lsof /tmp | ... I think it will run faster and more efficiently.
I'd like it to report the pid and filename at least I"m building this as part of a nagios plugin so i need to be able to dump exit 0 and exit 2 which tell nagios how to report in it's dashboard. I'll make that change thanks.
I recently wanted to get every "nth" line in a pipeline, to help monitor some busy log files. Ended up going with awk 'NR == 1 || NR % 100 == 0' in the pipeline.
For me lsof outputs the following lsof COMMAND PID TID USER FD TYPE DEVICE SIZE/OFF NODE NAME systemd 1 root cwd DIR 8,1 4096 2 / systemd 1 root rtd DIR 8,1 4096 2 / If I'm understanding what you're trying to do, I think you missed the empty "TID" column, and just need to change your 7's to 8's? Also, it probably doesn't make a difference, but I think the default number of lines for *head* is 10 so -n10 isn't necessary.
Why not just use Ruby? something like: require 'find' home = ENV['HOME'] gid = File.stat(home).gid puts Find.find(home).all? { |path| File.stat(path).gid == gid } You may need a `File.exists?(path)` test in there if you have broken symlinks.
does a solution like that work in the event the user belongs to more than one group?
I don’t know `sshpass`, but most programs allow you to separate arguments from flags with `--`. Does this work? sshpass -f password.txt -- ssh -n -o StrictHostKeyChecking=no $line
No worries I'm stuck at what I need to use to say 'ok i've output a line that meets my criteria report on this line' I thought an if statement would be good but i'm not sure how to tell bash if any result from the previous command has a line that meets my desired settings tell me which one that is.
&gt; while read $line; do Variable names don't have $ in them. Also, you should add `-r` on read by default. while read -r line; do
Nope wget just works fine If anyone found it hard here's a brief intro : The default format of a wget command is : (ignore the $ that is your prompt) $ wget [options] [url] url is the one you want to download, don't download the entire internet though. Now coming to the options (important ones) : -r this tells wget to follow links in pages and download them too, by default wget reaches 5 layers deep. so problem here is that the whole internet begins to get downloaded to fix that : --no-parent this option limits our search to the url given ie : if our url is http://www.xyz.com/vader only webpages that are under the hierarchy will get downloaded -w 10 this sets a 10 sec wait between requests so that the web server doesn't think it's under an attack --limit-rate 20 maximum download speed is limited to 20K/s out of politeness This was my basic intro to wget for you. Happy web scraping !! 
Alright Alright I'll try cliget !
This can probably be done using find and searching for the gid(s) of the user. On mobile, but `man find`
If you just want the exit code try this: sudo lsof /tmp | grep frmweb | sort -n -r -k 7 | head -n 10 | awk '{if ($7 &gt;=200000) print $7, $9, $10, "Critical"}' | egrep "." [ $? -eq 0 ] &amp;&amp; exit 2 exit 0 
Thanks! I didn't know youtube-dl had a search function like this...will be using it.
You're welcome! You can just try and pipe the contents of your text file into the `youtube-dl` command. Ask away if you have any more questions.
You'd need `xbindkeys` for that. In `~/.xbindkeysrc`, to bind the key Shift + F1: "winid=$(xdotool search "application name here" | head -n1); xdotool keydown --window $winid v; xdotool keyup --window $winid v" Shift + F1 You may also need `--clearmodifiers` after `keydown` and `keyup`, but I'm not sure. By the way, `xdotool` uses a window stack, so a single command is possible too, I think: "xdotool search "application name" key --clearmodifiers v" Shift + F1
(You’d need to escape the quotes around “application name here”, right?)
Huh, alright :D
&gt;I want it to spam a button Do you mean a mouse button? That's `xdotool click 1` for the left mouse button. "xdotool search "application name" click 1" v
No, use the first one with `key`: `~/.xbindkeysrc` "xdotool search "application name" key --clearmodifiers x" v
Yeah, that's a viable way to do it. If you want it shorter, you can do something like this: `cat songs | xargs -d'\\n' youtube-dl -x --default-search "ytsearch"` `xargs` executes the given command for each line you feed it. ogg and opus are actually totally fine formats (they are free, as opposed to mp3), but you can specify an audio format with the `-f` flag, such as `-f mp3`. You can also convert the output audio files to mp3 manually, e.g. with `ffmpeg`.
I put that in and I'm pressing v but nothing is happening, it just doesn't put the v. On the terminal it makes the cursor change to the one it uses when it isn't the focused window.
run this on the command line and see what it prints out: ifconfig | grep -A 1 'enp2s0'| tail -1 | cut -d ':' -f 2 I'm betting that `enp2s0` is an interface on your desktop but the rpi uses `eth0` or `wlan0` instead. You'll need to change your `grep` command. Also, your `cut` command doesn't return a clean IP. Once you fix that, then surround `$ip` with double quotes like so `"$ip"`. 
Simply put: Its not easily possible. Whatever is the focused window will receive mouse and keyboard events. Depending on which wm you use and which window server (X or Wayland) there may be hacks to make it work. Why do you need that though?