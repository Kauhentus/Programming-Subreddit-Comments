Yes. `using` is used in my last example (its abbreviation `u` works too). :)
This script creates p1-p3 for the first 500 hits for p, then it uses p1 and p2. It also creates a `data.formatted.dat` file for gnuplot containing Title : value. If you want to drop p values for the first 500 lines altogether, just change: if [ "$P" -le "2" ] ; then ((P++)) ; else ((P--)) ; ((P--)) ; fi to ((COUNTER++)) ; continue Here is the entire script (condensed for space): #!/bin/bash DATA_IN="data.dat" DATA_OUT="data.formatted.dat" COUNTER="1" ; P="1" while read -r ; do for item in "Urelx" "Urely" "Urelz" "omega" "k" "p" ; do if ! printf "%s" "$REPLY" | grep -q "Solving for $item" ; then continue ; fi data=$( printf "%s" "$REPLY" | grep -o -E "$item, Initial residual = (.*)," | sed -e "s|$item, Initial residual = ||g" | awk -F, '{print $1}' ) if [[ "$item" = "p" ]] ; then item+="$P" if [ "$COUNTER" -le "500" ] ; then if [ "$P" -le "2" ] ; then ((P++)) ; else ((P--)) ; ((P--)) ; fi else if [ "$P" -eq "1" ] ; then ((P++)) ; else ((P--)) ; fi ; fi fi printf "%s\n" "$item $data" &gt;&gt; "$DATA_OUT" ((COUNTER++)) done done &lt; "$DATA_IN" 
```kunst``` is a deamon that extracts the album art from the songs playing in ```mpd``` and displays them in the a little window. It doesn't loop on a timer, instead it waits for ```mpd``` to send a ```player``` event. When it receives a ```player``` event, it wakes up and extracts the album art of the current playing track. This makes ```kunst```really lightweight and makes it idle at ```~0%``` CPU usage. **If there no embbeded album art, it will try to fetch the album art from the internet.** Github repo - https://github.com/sdushantha/kunst AUR - https://aur.archlinux.org/packages/kunst-git/ *(need to be updated with the new feature)*
Thanks for taking the time! I'm new to this kind of scripting, so I have a couple of possibly quite obvious questions. I've switched over to not using cat like you recommended, although what is the benefit of this? Should I try to minimise piping in general? In terms of setting my x range, I do want to start at x=0, but for 2 of my 5 variables there is no data for x&lt;2001, so by setting the axis like this I would lose those initial data points for the 3 variables that dostart at 0 I think? I will also post over a r/gnuplot, thanks again :)
Change the 'printf' line to this: printf "%-20s : %s\n" "$1" "$description" This will use a 20 character wide column for the "$1" value. Using "-20" instead of "20" makes the column left-justified, by default it's right-justified.
Just the thing i needed, thanks 
If you don't mind (seriously, only if you really don't mind!) could you comment that for me please? Theres a lot going on there and I'm unfamiliar with a lot of the syntax, trying to work it out but it's taking me a while.
This is quite nice, I did something similar a while ago. If I can suggest a couple of improvements: * to see if you are connected to the internet why don't you ping api.deezer.com instead of 8.8.8.8. Apart from not pinging google it will behave as expected if deezer servers are down. * I think you should build up a cache for the covers that were downloaded online. If I am playing an entire album there is no reason to download the same album art on each song. If you save the covers with a file name like "artist\_album.png" then you can quickly see if it has been downloaded already and just use it if so.
Impossible to say without seeing the content of your `.bashrc`.
Do you have git installed? Also, post the line that contains the alias with the git command. Come on, try just a little bit harder to help us help you. 
Yes, of course. I assume the data file has a format of Time = tstamp data lines data lines blah bla Time = tstamp 2 etc. The `while read -r ; do &lt;stuff&gt; ; done &lt; "$DATA_IN"` loop reads the data file line-by line. If it contains "Solving for item" where item is one of "Urely" "Urelz" "omega" "k" "p", it will grab the data between the string "item, Initial residual = " until a comma, then it will print that into a 2nd file (DATA_OUT) preceeded by the line count and the item name. For p I do a bit of juggling to read values into p1..p3 (for lines below or equal to 500), and values p1..p2 for lines further down than line 500.
&gt; Come on, try just a little bit harder to help us help you. Lol. Okay. Yes, I have git installed. But, I'm executing the command from my home directory and it is not a git repo. These are the contents of my .bashrc and my .bash_aliases: test -s ~/.alias &amp;&amp; . ~/.alias || true if [ -f ~/.bash_aliases ]; then . ~/.bash_aliases fi export PATH=$PATH:/opt/jdk-10.0.1/bin export PS1="\[$(tput bold)$(tput setab 0)$(tput setaf 1)\]\u@\h:\w # \ [$(tput sgr0)\]" alias just=sudo #alias ls='ls --color=auto -l -a' alias mv='mv -i' alias rm='rm -i' alias git log='git log --oneline --decorate --graph --all' Cheers,
 cat FILE | grep "expr" | etc.. spews out the entire file and greps all instances of "expr". grep "expr" FILE only spews out those instances. For small files, it doesn't really matter. But when you have millions of lines, it does. Also, you _could_ have separate lines for the first 500 and next lines segments, e.g. plot "file" u x:y:z [0:500],\ "" u x:y [501:*] And whatnot.. Depends on what you need. I don't know what you're trying to accomplish and how the data looks, so I can't really say what's better :)
&gt;Come on, try just a little bit harder to help us help you. Every help post in a programming sub ever :D
The space between git and log is most likely the culprit... how come a space within a symbol did’t give you twitches?!
If you really want the space, you can add the following to your `~/.gitconfig`: [alias] log = 'git log --oneline --decorate --graph --all' &amp;#x200B;
Also, git has it's own mechanism for [aliases](https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases).
&gt; Yes, I have git installed. But, I'm executing the command from my home directory and it is not a git repo. I think that's why you're getting that error. There's no .git directory in that directory, right?
Thank you for the feedback. I will do this :)
* Do not store password in plaintext. * Do not abuse cat e.g. use for line in $(cat filename) ; ` * Use $(...) instead of `...` Now to the problem solving: add `set -ex` on the second line. That will show you what the file is doing. Also show us the file kernel_hosts. Is it a long list of hosts? Because I would use passwordless login.
 myUser@$filelines Likely wants to be "myUser@$line"
No, there's not a .git directory in my home directory. 
You might be running in a different terminal. Do you have ZSH installed? 
Hey, It was indeed the culprit. Once I removed this alias (which did not work btw, I just discovered), I could source .bashrc. Why that interfered I dont understand however. Tnx for your reply!
Wrapper calls API on haveibeenpwned.com...
Hey, thanks this was indeed the fix. But can you or /u/houghi explain what you mean by "`Do not abuse cat e.g. use for line in $(cat filename) ; \``"?
That's why you're getting the error. Try that command in a dir where there's a .git directory.
https://mywiki.wooledge.org/BashFAQ/001
Because you can’t alias subcommands with the built in aliasing mechanism, you can however alias the log subcommand within git via .gitconfig as pointed out by another user
&gt; `...` It may not be quite clear because of the formatting (backticks are for inline code). I assume you mean \`...\`, which refers to using backticks in the code around expressions.
Just don't do that at all. It doesn't accomplish anything. Just try to hit the API, and if that fails then handle it. Whether you can ping something isn't directly related to whether you're online. Maybe the user is behind a proxy. Maybe the network filters ICMP. Adding a ping test only creates a new way for your script to fail. 
&gt; `sshpass -p $password` https://media.giphy.com/media/2UCt7zbmsLoCXybx6t/giphy.gif
[This answer](https://stackoverflow.com/a/42168237) on SO explains that NPM ignores the environment that the NPM script is executed in, and *does* in fact use `cmd.exe` to execute the script. It seems the solution to my problem is configuring the shell environment that NPM uses :/
On top of what else has been said... Usually a `while` loop is superior to a `for` loop for this kind of task. e.g. https://github.com/koalaman/shellcheck/wiki/SC2013 So a rewrite of your loop might look something like this: while read -r remoteHost; do ssh "${remoteHost}" 'yum list kernel' done &lt; "${filelines}" Notice how the `ssh` call is cleaned up? That's because you're going to sort out your keys and maybe `~/.ssh/config` first. The other change I've made is using a meaningful variable name. Now, an alternative way to do this might look something like this: ansible all -i filelines -m shell -a "yum list kernel" -o
on second thought. No no you shouldn't. 
Use prefix/suffix removal: for f in apple.*; do mv "$f" "orange.${f#apple}" done
It's customary to have required arguments specified using positional parameters, as in `mv SRC DEST`. `[ -z r ]` just tests if the string "r" is nonempty, you need to read in all your options into variables before passing them on. #!/usr/bin/env bash while getopts r:c: o; do case "$o" in r) rval="$OPTARG" ;; c) cval="$OPTARG" ;; *) echo &gt;&amp;2 "Invalid flag" esac done if [ -z ${rval+x} ] || [ -z ${cval+x} ]; then echo &gt;&amp;2 "Missing requried flag" exit 1 fi echo "r: $rval, c: $cval"
Thanks for the quick response! Just curious, why did you add a +x with the r and the c vals? what does that do in the argument? Also, I noticed that you put a ; at the end of the if condition. I don't put that in my code but it works just as fine? 
The `${var+string}` construct will expand to nothing if var is unset, and `$string` if var is set.
Nitpick: I strongly prefer if [ -z "$rval" ] || [ -z "$cval" ]; then over the unobvious: if [ -z ${rval+x} ] || [ -z ${cval+x} ]; then Aside from saving two measly characters, the former clearly does what it says: It tests that each variable expansion is null. The latter resorts instead to a "trick evaluation": When a variable is unset, that test expression resolves to `[ -z ]`, which actually evaluates as "is `-z` *not* null?" Not only does it *not* test what you'd expect, it also inverts the sense of the original test. Great for demonstrating bash guruhood, not so great as a regular coding practice.
Or you could actually use Bash syntax and drop the quoting: if [[ -z $rval || -z $cval ]]; then I tend to use: if [[ -z ${rval+set} || -z ${cval+set} ]]; then though since I almost always use `set -o nounset`.
Ah, I think what I meant to do was `[ -z "${rval+x}" ]` and I forgot the quotes... Usually I want to treat empty and unset variables the same way, as in your preference. But since unset was asked for, I answered as such. I don't do it often in my scripts since (as you said) it's not obvious. I like the zsh method of `$+var`, which evaluates to 1 if set and 0 if unset. It's pretty clean, and being a single bit makes operations testing a lot of variables quick using bitwise operations: (( $+SSH_CONNECTION &amp; ~$+TMUX &amp; ~$+SCREEN &amp; ~$+DTACH )) I use this to test if I'm in an ssh session but don't have a detachable/multiplexed session.
Are you asking Reddit to do your homework?
https://mywiki.wooledge.org/CatEchoLs This is regarding their use in scripts and 'one-liners' - but you will also come across `UUOC` - [Useless Use Of Cat](http://porkmail.org/era/unix/award.html) - The following lines produce the *same* output... cat ./in-this-file | grep "find this text" grep "find this text" ./in-this-file Why would you want to type the extra 6 characters needed?
We help people who try. `&lt;WIN+SHIFT+S&gt;` is not trying. Post your code.
This is a form of [Parameter Expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html) (search for `parameter#`) Basically, `#` is to the "left" of `$`. `%` is to the right. `${f#apple}` means "the string stored in variable f, minus the first, left-most occurrence of the [glob](https://wiki-dev.bash-hackers.org/dict/terms/globbing) `apple`." Note that this wouldn't match a file named crabapple.txt, but `${f#*apple}` would.
Hey. Have a look at [this](https://github.com/sakishrist/sgo). It might be useful.
Thats great, thanks for the explanation! As you said, it did take quite some time to write it all to the formatted data file, but now that it has, plotting is much easier. Thanks again for your help :) 
If there is too much data, a while read loop is not the right solution. This was just an example :) For bigger files use e.g. (grep and) awk (with awk counting) to format the data.
So what have you now? Easier toi fill gaps when we know where the gaps really are.
As per your recommendation I've checked the "LOOP find" and I don't see why would I choose this ``` . modernish use safe use var/loop LOOP find --glob lsProg in /*bin /*/*bin -type f -name ls* DO putln "This command may list something: $lsProg" DONE ``` over this `find /*bin /*/*bin -type f -name 'ls* ....'`? Also I don't understand what exactly can not be done in any shell with the LOOP find? While I admire you effort (3 years, whow!) - same question like others - why would I want to use this library? I know from experience that shell (bash) scripts CAN be written safely and robustly. Maybe the use of this lib could be in portability, however again - this can be achieved by either defensive scripting and/or leveraging other tools like perl/python that guarantee more portability over platforms? &gt; If none of those advantages ever mattered, why not use python or perl to begin with? Conversely, if they do matter, why not try to enhance the shell language? Again, from experience - usually when I reach limits of what bash script (or some part of it) can do - for example lots of psql/curls start to be messy - usually I just rewrite only the complex part of the script and this new script is then invoked from shell.
Assuming you use urxvt #!/bin/bash urxvt -e bash -c "nvim ~/path/to/file" &amp; urxvt -e bash -c "nvim ~/path/to/otherfile" &amp; The `-e bash` is to prevent the terminal from closing if you exit nvim. The `-c` is the command. 
Xterm can do the same exact thing if he's not using urxvt, so very solid suggestion here.
Thank you! This helps a lot, adding this to my arsenal of references.
&gt; alias git log='git log --oneline --decorate --graph --all' The key here is that multiple arguments/actions can be given to a single alias command. Arguments are separated by unquoted whitespace. So what happened here is that you gave two arguments to `alias`. The first argument, `git`, means `print the existing alias called git`. That is why it was complaining about `git not found`: not such alias had been set. The second argument sets a new alias called `log`. 
Can you show an example of the input string, and output string you desire? &amp;#x200B; This is most likely going to be a for-loop of some sort.
Yes I have a string that is Someparam[0:0] and I want to assign a new value then go to the next [0:1] all the way to [18:15] . The value I'll be setting is the same each time and that won't be hard currently its Assign SomeParam[0:0] SomeValue Assign SomeParam[0:1] SomeValue Assign SomeParam[0:2] SomeValue ..... Assign SomeParam[18:15] SomeValue I know theres a way i just need shoved in the right direction. I'm super intermediate level with bash but know enough to really mess things up. 
You could have a look at [for loops](https://ma.ttias.be/bash-loop-first-step-automation-linux/) and [nested loops](https://www.tldp.org/LDP/abs/html/nestedloops.html).... Or there is brace expansion 
That's low effort. How do you intend to loop? What does "working" even mean? Do you know ping(1)? curl(1)? man(1)? Google?...
Curl probably. Thanks.
you can use netcat to see if a port is open and a connection happens: `nc -zv www.google.com.sg 80 ` something like this might work `#!/bin/bash` ` for i in ($cat domains.txt) ` do ` `nc -zv -w 3 $i 80 ` `done that `-w 3` is a three second time out if it doesn't connect. 
Simplest example I can drum up ... partially understanding the scenario that is ... &amp;#x200B; `#!/bin/bash` `myvalue='This is my repeating value!'` `for f in \`seq -w 0 99\`;` `do` `for l in \`seq -w 0 99\`;` `do` `echo "SomeParam[${f}:${l}] is $myvalue"` `done` `done` &amp;#x200B; &amp;#x200B; ... god this new input format for code on reddit blows.
You are awesome . This is exactly what I was looking for! Thank you 
u/WeranioRacker, since you have thousands of hosts to scan, you're probably best served with a tool that's designed for such scanning: [nmap](https://nmap.org/). Its automatic dynamic parallelism lets you go as fast as possible without blowing out your network connection and annoying your coworkers. In a nutshell: $ nmap -p80 -sT -iL domains.txt -oG - | awk '/Status: Up/ {gsub(/[()]/, "") ; print $3}' &gt; working.txt does an `nmap` probe: * `-p80`: on port 80 * `-sT`: using a TCP connect scan * `-iL domains.txt`: on all the domains listed in `domains.txt` * `-oG -`: and producing greppable output `awk` then massages the output to your spec. Sidenote: The "greppable output" of nmap is technically deprecated, but I doubt it'll disappear any time soon, and it's easier to deal with for most folks than the recommended XML. For purists, here's the XML version using [XMLStarlet](http://xmlstar.sourceforge.net/): $ nmap -p80 -sT -iL domains.txt -oG - | xmlstarlet sel -t -v '/nmaprun/host[ports/*/state/@state="open"]/hostnames/hostname/@name' &gt; working.txt
Oh thanks. Perfect.
A problem with `nmap`'s "greppable" output is that it returns the hosts after (re-)resolution, so that they don't match what you gave it. For example, if your input is `google.com` your output will be `172.217.4.238 (ord30s31-in-f14.1e100.net)`. That makes the whole of filtering the hosts out a lossy one. I don't know if that's important to you /u/WeranioRacker, but if it is you can get the original host back if you use the XML output (note `-oX`, not `-oG`): nmap -p80 -sT -iL domains.txt -oX - | xmlstarlet sel -t -v '/nmaprun/host[ports/*/state/@state="open"]/hostnames/hostname[@type="user"]/@name' Also, it's probably much faster to use `-sn` instead of port scanning, if you can get away with it. *Also*, note that if any of these hosts are public Web servers, a lot of them will probably appear "up" even though there's nothing there, because they redirect to their DNS/hosting providers' squatting servers. You'd need a blacklist or something to deal with that.
It is important to remember that most of what you're doing in bash when doing this sort of thing is run a system command. So on my Ubuntu system the date program can do this. On other platforms maybe not. If you're on Linux check out the date manpage for more details but the following returns the last month as a number: date --date="last month" +%m You'd need to store this into a variable with command substitution.
&gt; It is important to remember that most of what you're doing in bash when doing this sort of thing is run a system command. FWIW, this could be done with Bash builtins only: printf '%02d' $(( ($(printf '%(%m)T') + 11) % 12 )) Not exactly the clearest, though.
Darn kids with your fancy new bash 4 features! Just kidding. Definitely a TIL moment. I'm not sure I'd use that over just using date for clarity sake. But is that still using a subshell? I don't really know if builtins actually run externally on substitution.
Bash doesn't fork to execute a builtin. However, I am using a command substitution for the inner `printf`, and that's run in a subshell.
Assuming you have GNU date on your system. (Almost all linux distress will): $ date -d 'last month' "+%b"
This is a new project. I'm been dogfooding it for a bit. But needs more eyes on it. Appreciate any and all feedback. Thanks!
FYI, `for` has a built-in count; you don't need external commands. `for ((i=0; i&lt;99; i++)); do` Or you could use Brace Expansion on more recent versions `for i in {0..99}; do`
gnome-terminal as well.
Echo $? to get the exit code of the last command? That isn’t a trick, that’s an extremely basic and fundamental component of bash.
It's nice to know though. I usually do if command ; then whatever ; fi but I often check exit code manually using `echo $?` if I haven't read the man page (or it doesn't say). Some programs (like `ftp`) have terrible exit code designs.
Much better. How would you precede the single digits with zero? &amp;#x200B; I'm a bit old school with this still ... bad habits. &amp;#x200B;
[Pure Bash Bible](https://github.com/dylanaraps/pure-bash-bible), offers lots of tricks, including ways to do things in pure bash which are typically done with external tools.
Sometimes it's good to have a reminder. 
I went with the seq method omitted the -w option as that made it 00. Worked like a charm! Now I've got that trick in my toolbox I'm super dangerous 
Thank you so much! This worked!! 
Sure, but it’s still not a “trick”
It certainly is good to know, I just don’t think it’s a trick.
No, it doesn't. `find` will execute `rm` directly, and `{}` is always substituted with a single argument. No shell is involved.
Thank you. So to be sure I'm reading you correctly, I don't have to worry about sanitization?
Side-note: why are you using `-exec rm` instead of just `-delete`. That would allow find itself to do the job and would spare you a fork per file. And it would allow you to not even think about escaping. 
You do not need to worry about it.
Oh hey didn’t know about `-delete`, thanks!
Dude are you still posting your homework here? 
Use -delete or print0 \| xargs -0 rm
 for ((x=0; x&lt;=10; x++)); do printf '%02d\n' $x; done for y in {00..15}; do printf '%s\n' $y; done
Wouldn't this find folders though? Granted it would fail as you aren't running "rm -r", but you could add in "-type f" to specifically find files reducing the risk (or add in recursion if you are looking for folders). I'm not sure whether the -delete command would remove files and folders so be cautious! The -delete command is better as it doesn't spawn a process; but each to their own :) 
Watched the video presentation, so you need to use a Mac and use a version of emacs made for the mac. No thanks
this is great!
Yes, it's not really a trick but I don't want to be to strict with the naming of the repository. It is intended to be a little reminder with some commands and usage tips :)
find is mucho dangereaux and often nonportable. Better to write a dedicated application, e.g. in Rust, Go, Ruby, Python, C/C++
Thanks for the tip, I added -type f.
Find is wrote in C and has been in unix and Linux kernels since the 80s, I’m yet to use a kernel without it actually... people don’t use it much for some reason (to be honest the syntax can become confusing) but it’s a very flexible powerful tool... for example let’s say you have a folder with a billion subfolders and a billion files; you can type ls for each folder which would be slow and tedious or you can just type find which would list everything quickly. It’s also good for handling long filenames. Never underrated always under appreciated. 
Find’s syntax is not portable across unixen. A shell script invoking find on GNU/Linux often presents errors, or worse, different behavior, when run on macOS, MirOS, Haiku, and others.
Ok, not sure I agree with much/any of that - but fine. I think find is a fantastic tool.
If you type this here at the command line: test5 test It does not matter if you add spaces at the end. Bash will do "word splitting" for you on that command line and will remove all spaces. The argument that will be sent into 'test5' will be exactly `test`. If you want to have a space character inside the argument, you will have to type one of the following command lines: test5 "test " test5 'test ' To check if that kind of `'test '` argument was passed to the function, I came up with this here right now while experimenting at the command line: $ last_arg() { local arg="${@:$#}"; echo "last argument: &lt;$arg&gt;"; if [[ $arg == *' ' ]]; then echo "space = true"; else echo "space = false"; fi; } $ last_arg a b c last argument: &lt;c&gt; space = false $ last_arg a b 'c ' last argument: &lt;c &gt; space = true With line-breaks added to that 'last_arg' function, it looks like this: last_arg() { local arg="${@:$#}" echo "last argument: &lt;$arg&gt;" if [[ $arg == *' ' ]]; then echo "space = true" else echo "space = false" fi }
Space is an argument separator. You could identify the space if you had `"test "` as an argument on the command line with literal quotes typed by the user.
no idea why you'd want to do this - and bash will give you noting to indicate if there's any trailing white space on the line invoking the function however, you can get the line of the caller - and then scan the script, eg. here with sed... #!/bin/bash FILE="$_" test5() { sed -n " ${BASH_LINENO}{ / $/q0 q1 } " "$FILE" } test5 test echo $? # on next line, put whitespace after the last argument.... test5 test echo $? and testing neil@tvpc:~$ ./test 1 0 &amp;#x200B; &amp;#x200B;
&gt; echo "bypas.... where is your closing quote?
Thanks for this. I will see if this solution works for my project. To give more background, I created an alias organization and autocomplete project called Nama. It's a node project that lets you organize aliases into namespaces and run them. It also has autocomplete for both the namespace name as well the commands within the namespace. To do the command autocomplete lookup, my script has to detect if there is a space after the first namespace, indicating that a new bash argument is present to the autocomplete script: https://github.com/alecdibble/nama/blob/master/bin/nama#L16-L37 Originally this worked as far as detecting the fifth argument after a space using tabcomplete, but I realized that I couldn't do things like `cd` with my implementation. I changed the way the script was called to this: `na() { if [[ $1 == "completion" ]]; then nama $@; else . nama $@; fi }"` and put it in the ~/.bashrc. (https://github.com/alecdibble/nama/blob/master/src/install.js#L29) However, it looks like bash function arguments work differently where $# doesn't get incremented when there is a space after the last argument during tabbing. Your solution will hopefully allow me to detect that space. Currently, autocomplete will work on the command level if the first letter of the command is present. However, I would like any tab automcompletes after a valid namespace with a space after it. Please let me know if you have any questions. I really appreciate your time.
Unfortunately, I can't think of a good way of requiring quotes as it would degrade the user experience of my project. Here is a link to my comment with more info: [https://www.reddit.com/r/bash/comments/atzn1d/is\_it\_possible\_to\_detect\_a\_space\_at\_the\_end\_of\_an/eh4n60h](https://www.reddit.com/r/bash/comments/atzn1d/is_it_possible_to_detect_a_space_at_the_end_of_an/eh4n60h)
It's there. It got lost in "masking" my actual variable names. Oops. Edited to fix.
&gt; echo "using What about that one - that should be there since you haven't got any excuse like 'masking'.... ?
&gt; It got lost in "masking" my actual variable names. Ah, a fully paid up member of the tin-foil-hat brigade... Tip; use variable names that you can post while troubleshooting - don't change them for something else, since if the trouble is with some dumb-ass variable name you chose then it'll work on everyone but yours due to something that you are keeping hush-its-a-secret-cos-the-feds-are-listening-to-my-leet-haxor-skillz .... 
You really should have mentioned that you are working on completion and not on normal stuff... It I'm not sure I understand what you want to do. Is it something like the following? $ btrfs &lt;TAB&gt; balance filesystem property receive restore subvolume check help qgroup replace scrub version device inspect-internal quota rescue send $ btrfs scrub &lt;TAB&gt; cancel resume start status $ btrfs scrub sta&lt;TAB&gt; start status I don't understand how that 'complete' command and such works so I can't help at all. Perhaps try to look at what other people are doing that have things set up for their program like what you want to happen for yours. The completion scripts for different programs are here on my distro: /usr/share/bash-completion/completions/
This is sort of "cheating", but something like this might work alias foo='getLiteralIn #' getLiteralIn() { echo "$(history 1)" | sed -E s/'^[ \t][0-9]*[ \t]*foo (.*)$'/'\1'/ } This works because bash records the command as entered, then substitutes any aliases, then executes the line (and does word splitting and all that good stuff). This means that when someone calls `foo &lt;...&gt;` whatever they typed dgoes into the bash history, then foo gets transformed to `getLiteralIn #`, which comments out whatever the user inputt and calls `getLiteralIn`, which in turn reads the last entry from the history file, chops off the initial `[#] foo ` part, and returns (in theory) exactly whatever the user input. 
 $ array=(one two three "four " "five ") $ for index in "${!array[@]}"; do case "${array[index]}" in *\ * ) echo "${array[index]}" has a trailing space ;; *) echo "${array[index]}" has no trailing space ;; esac done one has no trailing space two has no trailing space three has no trailing space four has a trailing space five has a trailing space $
A lot of these are just [Parameter Expansion](https://wiki.bash-hackers.org/syntax/pe) wrappers. No hate, but learn to use the actual expansions.
How did you come to this conclusion
You can invoke grep. I like to stick to builtin commands, POSIX sh when possible. `case` blocks can do basic string matching like that.
Its worth quickly noting that `##` and `%%` do the same but make the globs greedy. i.e. f='a.b.c.d' ${f%.*} --&gt; a.b.c ${f#*.} --&gt; b.c.d ${f%%.*} --&gt; a ${f##*.} --&gt; d
I watched the video
I've also watched it but haven't seen anything about Mac or emacs for Mac
I really love this book, has tons of great info in it. Might be able to find it online somewhere. [https://www.amazon.com/Linux-Command-Shell-Scripting-Bible/dp/111898384X/ref=pd\_lpo\_sbs\_14\_t\_0?\_encoding=UTF8&amp;psc=1&amp;refRID=KDKCH1GWS30M5R9G2Z3A](https://www.amazon.com/Linux-Command-Shell-Scripting-Bible/dp/111898384X/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=KDKCH1GWS30M5R9G2Z3A) Otherwise, come up with a simple task you want to accomplish, and try and break it down into small steps. Then do some reading on basic bash commands and try and piece it together from there!
The best advice I can give to beginners is to make something that *you personally desire*. This way, you have a persistent motivation to finish. Write a script which organizes your mp3 files, or perhaps something which rotates wonky images. It could be anything. If you don't have some personal motivation, you will never get your foot into the door to really learn the fundamentals properly.
Does it have to be bash? Bash is pretty kooky as a first language. If you use bash at work, then I think it's appropriate. If not, I encourage you to look at Python or Perl. Perl uses some shell syntax so, if you want to get to bash eventually, Perl would be a good place to start. If you are processing structured data (i.e. delimited or fixed-with files), look at awk. I use awk frequently to create reports and summarize detail files that I download from third parties. If you go the Perl route, I suggest Modern Perl by chromatic. If awk, check out Effective Awk Programming by Robbins. In either case, I think you will be up and running in a short period of time. If you are on a Windows machine at work, installing git bash will give you a minimal GNU environment with bash, Perl, awk, and a few other utilities. It shares your Windows filesystem, so working with other applications is pretty easy.
[http://shop.oreilly.com/product/9780596009656.do](http://shop.oreilly.com/product/9780596009656.do) [https://covers.oreillystatic.com/images/9780596009656/lrg.jpg](https://covers.oreillystatic.com/images/9780596009656/lrg.jpg) &amp;#x200B; # Learning the bash Shell, 3rd Edition ### Unix Shell Programming By [Cameron Newham](http://www.oreillynet.com/pub/au/304) &amp;#x200B; **Publisher:** [O'Reilly Media](https://www.safaribooksonline.com/library/publisher/oreilly-media-inc/?utm_medium=referral&amp;utm_campaign=publisher&amp;utm_source=oreilly&amp;utm_content=catalog&amp;utm_content=catalog) **Release Date:** February 2009 **Pages:** 360
I would recommend starting with Python. It’s a lot more user friendly and will give you a good foundation for when you do learn bash
One thing you can do is remove the first two arguments after you are done with processing them, like this: shift 2 This "shift" command will make $3, $4, $5, etc. move up to $1, $2, $3, etc. You can then afterwards write your for loop using "$@" like normal: for file; do if [[ -f $file ]]; then echo "'$file' is a file." else echo "'$file' is not a file." fi done Or, something else that's possible is writing `"${@:3}". This will make bash create a list of `"$@"` where the first two elements are skipped. The 'for' loop would then look like this: for file in "${@:3}"; do if [[ -f $file ]]; then echo "'$file' is a file." else echo "'$file' is not a file." fi done Here's an experiment at the command line showing how that `"${@:3}"` behaves: $ set -- a b c d e f g $ echo "${@}" a b c d e f g $ echo "${@:2}" b c d e f g $ echo "${@:3}" c d e f g 
THis work perfectly! thank you so much 
Perl and awk for a beginner?
Everyone starts somewhere.
Check out a helpful tool named "shellcheck". It tries to hunt down mistakes that are easy to make in bash scripts. You can try it online at www.shellcheck.net, and your distro likely has a package for it. In a #!/bin/bash script, you will always want to use `[[ ... ]]` instead of `[ ... ]`. The `[[` is part of bash's scripting language, while `[` behaves like an external program. There's a mistake that can you do with `[`, which won't happen with `[[`: $ x="hello world" $ [ $x = hi ] bash: [: too many arguments $ [[ $x = hi ]] # &lt;-- no error here
Nice. Thanks for the heads up. Will run it through there. Didn’t even think about running it through a linter which I’m guessing would help check things out as well. 
xq is the tool for the job, usually used for XMLs, but I can't think of a reason why it won't be 100% compatible with HTML. Now, pup is specialised for HTML, but I've only used it to parse it, check this one firșt to see if it can generate it.
Hmm. Do you need a colon after the 'p' in the argument to getopts, i.e. "getopts r:c:dp:" ?
Can you describe in more detail how the "formatted text file" is formatted? 1. How each header is formatted. Is it in all caps? Is it preceded and followed by, say, === or ---? Etc. 2. How is each paragraph formatted? Is the beginning of the paragraph indented? Is the entire paragraph on one line without any included newlines? Etc.
Suppose this: # Header H1 Paragraph ## Header h2 Paragraph ### Header h3 Paragraph 
For H1 (I'm assuming that the double quotes are literal): sed -E 's/^"# (.*)"$/&lt;h1&gt;\1&lt;\/h1&gt;/' For paragraph ... a little tricky, since the paragraph may start with double quotes.
what if assume paragraph has nothing but letters and numbers?
That would make it a required parameter right? I thought about it but I was thinking if no path was specified it would run a default path (I didn't code this in yet but didn't think that would matter atm) &amp;#x200B;
Specifying the colon makes the argument after the option required, but the option itself is still, well, optional. So, if you use 'r:c:dp:', then you could do: mycommand -d -p my/python/path or just: mycommand -d You could initialize path to some default before the while getopt loop. The other thing is, if you don't specify the colon after the 'p', $OPTARG ends up being unset in the 'p' case. That's why your path is missing.
You could probably add an ACK... ......PS1='\[\e[34m\w\] \[\e[33m\]\[\e[1m\]$\[\033[00m\] \006' But I've not tested it.
OHHHHH. Okay that makes more sense now. I thought that putting a : means the script would always expect a value to those parameters. Oh wow thank you so much for your help. 
This seems to happen specifically at the \`sudo su\` command. So my guess is, that line in the history file includes a special character (probably entered by mistake). So you could try opening the bash history file with nano (for example) and delete that line. I believe that's the reason because it does not happen with the line \`6cord\`
Does the new readline library now support multiline editing, or how do I have to understand this feature here: &amp;#x200B; &gt;b. There are new \`next-screen-line' and \`previous-screen-line' bindable commands, which move the cursor to the same column in the next, or previous, physical line, respectively. &amp;#x200B;
Or use bash-it with bash Or oh-my-zsh if you are using zsh And you will have a lot of options to choose from, fast, easy and safe
I think It's not properly escaped. Try this `PS1="\[\e[34m\]\w\[\e[m\] \[\e[37m\]\\$\[\e[m\] "` &amp;#x200B;
u/bobbathtub, based on one of your comments, you're basically trying to convert a Markdown subset to HTML. Is there a reason you need to use sed instead of a proper conversion tool? There are literally dozens of them out there, from the progenitor [markdown](https://daringfireball.net/projects/markdown/) (written by Markdown's creator John Gruber) to the all-singing all-dancing [pandoc](https://pandoc.org/) document format converter, as well as libraries for various languages like [Python](https://pypi.org/project/Markdown/) and [JavaScript](https://github.com/evilstreak/markdown-js). sed is a particularly poor choice for doing this.
u/FrontGovernment, the problem is here: PS1='\[\e[34m\w\] \[\e[33m\]\[\e[1m\]$\[\033[00m\] ' ^^^^ You just told bash that the current working directory (`\w`) that's included in your prompt is *non-printing*, hence readline won't take its length into account when it handles your interactive command prompt. You'll see an even more dramatic difference when you `cd` outside your home directory and Up-Arrow through your command history. Changing it to: PS1='\[\e[34m\]\w \[\e[33m\]\[\e[1m\]$\[\e[00m\] ' should fix that nicely. (I also tidied up a stray `\033` in your prompt for consistency.)
I suggest that you learn Python. 
You are missing a `$`: if [ -n "$p" ]; then Note the quotes, that ensures that an empty p is expanded to an empty string rather than no string. In general, you should always quote your variable expansions unless you want word splitting. (So I'd add quotes in the body of the conditional too.) [ -n ] : True [ -n "" ] : False
ayy cool this works 
bind your shortcut to insert text into clipboard then immediately paste from clipboard? That would probably be desktop environment / clipboard manager features. for `xclip` it would probably be something like `xclip /path/to/template/file &amp;&amp; xclip -o`
Do you know about Autokey? It pastes text on a keystroke or a typed word. I use it it all the time, it's great. It will also let you run python-scripts.
Hmm, I know it, yes. But isn't autokey windows only?
You're thinking of "AutoHotKey" with regards to Windows.
Are you talking about [this section](https://github.com/cooperhammond/irs#spotify-tokens)? You don't need to do anything to your PATH. In fact, the way you should do this is use it's config file. Take [this file](https://github.com/cooperhammond/irs/blob/master/irs/config_preset.py), save it as `~/.irs/config_.py`, and replace the variables inside with your secrets.
The `PATH` is a list of directories that will be search when looking for an executable file. I don't know about Spotify but it seems this is something else. Ok, I went to the page you linked to and what you need to do is write in the console. export SPOTIFY_CLIENT_ID=clientidyougot export SPOTIFY_CLIENT_SECRET=clientsecretyougot irs -h 
Oh so that definitely worked but now there's another issue which says that I need a chrome extension installed to be able to get past a Captcha. &amp;#x200B; selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see [https://sites.google.com/a/chromium.org/chromedriver/home](https://sites.google.com/a/chromium.org/chromedriver/home) &amp;#x200B; that is the output code. Sorry if this is more involved than expected! I'm not quite as knowledgeable as I'd like to be. &amp;#x200B;
So I did that and it solved that problem but now it leaves me with [this](https://imgur.com/JG2Sujw) error. Would this be solvable through an apt install command?
I'm no expert, but when I see "eval" and "input" on the same line I tend to cringe.
To me, code that’s easier to read is easier to maintain. By myself or others later on. 
Some ideas, while I'm not trying to understand your different 'lineLengthSort*' functions because they look scary and complicated: You have this example about your eval idea: # map each line in input to field in an array mapfile -t inArray &lt; &lt;(echo "${input}) # loop over the array, and apply some function `foo` to eaach line for nn in "${inArray}"; do foo "${nn}" done You could write that as follows: while read -r line; do foo "$line" done &lt;&lt;&lt; "$input" About sorting those text lines you have by line length, that would look like this with a Perl one-liner: perl -n0777E 'say join "\n", sort { length($a) &lt;=&gt; length($b) } split /\n/' Performance of this one-liner is really good. I tested it against your lineLengthSort2() function and with your $bb variable that I both copy+paste'd into a terminal window. I mean, I did the following to set up the test: $ x=$(echo {a,bb,ccc,dddd,eeeee,ffffff,ggggggg,hhhhhhh,iiiiiiiii,jjjjjjjjjj,kkkkkkkkkk,llllllllllll,mmmmmmmmmmmmm}{0..49}{50..10..2000}| sed -zE s/' '/'\n'/g | sort --random-sort) $ echo "$x" ... ffffff1550 hhhhhhh1550 $ lineLengthSort2() { &gt; # loop sort &gt; ... That lineLengthSort2() performs like this here for me: $ time echo "$x" | lineLengthSort2 &gt; /dev/null real 0m1.853s user 0m2.022s sys 0m0.579s The Perl one-liner does this: $ time echo "$x" | perl -n0777E 'say join "\n", sort { length($a) &lt;=&gt; length($b) } split /\n/' &gt; /dev/null real 0m0.003s user 0m0.001s sys 0m0.002s I also thought about how I'd solve this sorting problem in bash. I came up with this: lineLengthSort4() { while read -r line; do echo "$(wc -m &lt;&lt;&lt; "$line") $line" done | sort -g | cut -d' ' -f2- } Here's how it performs for me: $ time echo "$x" | lineLengthSort4 &gt; /dev/null real 0m1.813s user 0m1.412s sys 0m0.675s And here's that eval-using lineLengthSort1() for me: $ time echo "$x" | lineLengthSort1 &gt; /dev/null real 0m1.843s user 0m1.869s sys 0m1.155s Now looking at this again here, I notice I'm not seeing those differences in performance between lineLengthSort1() and lineLengthSort2() that you are getting in your results. Not sure what might be going on there. The amount of threads the hardware can do might matter when using pipes. My CPU here is an i5-3570k (four cores, and just one thread per core).
It isn't the loop that you're saving on, it's the `inMod="$( )"` capturing that's expensive. Just pipe the loop as is, don't capture: for nn in "${inStr[@]}"; do local -a inStrCur mapfile -t inStrCur &lt; &lt;(echo "${nn}") for mm in "${inStrCur[@]}"; do echo -n "${mm}" | wc -m | sed -zE s/'\n$'// &amp;&amp; echo " ${mm}" done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ unset inStrCur done The above gets me within a margin of error of your eval monstrosity. --- Of course, the unrolling speed increases are dwarfed by making proper use of builtins instead of calling external programs thousands of times (context switches are expensive!): lineLengthSort4() { # (alt-alt) loop sort (( $# == 0 )) &amp;&amp; set -- "$(cat &lt;&amp;0)" for nn in "$@"; do local -a current mapfile -t current &lt;&lt;&lt; "$nn" for line in "${current[@]}"; do echo "${#line} $line" done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ unset inStrCur done } It makes quite the difference: eval sort, random initial sort real 0m3.405s ... (alt2) loop sort, random initial sort real 0m0.029s 
I'm happy I'm not the only one who wasted time deconstructing this. Unlike you, I did confirm his performance differences. If you're curious what is *actually* causing the speedup, it's that they're capturing whole loop into a variable before piping. mapfile can be really quick when dealing with large files, but it is usually negligible. (Especially compared to making a tight loop like this run with builtins instead of external programs.)
Good catch on using `${#...}` instead of `wc -m`. Using read -r also helps. Adding in these things almost entirely closes the gap for me. Eval is still a bit faster, but only by 5-10%, which probably isnt worth it for most scenarios. foo() { echo "${#*} ${*}" } test0() { eval $(echo "${*}"/ | sed -E s/'(.*)$'/'foo "\1"; '/) | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test1() { eval $(echo "${*}" | sed -E s/'(['"'"'])'/'\\\1'/ | sed -E s/'(.*)$'/'foo '"'"'\1'"'"'; '/) | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test2() { # map each line in input to field in an array local -a inArray mapfile -t inArray &lt; &lt;(echo "${*}") # loop over the array, and apply some function to each line echo "$(for nn in "${inArray[@]}"; do foo "${nn}"; done)" | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test3() { # map each line in input to field in an array local -a inArray mapfile -t inArray &lt; &lt;(echo "${*}") # loop over the array, and apply some function to each line for nn in "${inArray[@]}"; do foo "${nn}"; done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test4() { local nn="" while read -r nn; do foo "${nn}"; done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ }
It does seem that the loop overhead is much smaller than my initial results suggested. I redid the tests with somem modifications and , while eval was consistently a bit faster, it was on the order or 5-10%, which isnt worth it most most use cases. As a side note, i tested it and just capturing the loop output isnt the problem, its continuously updating the variable that is expensive. See test2/test3 below. And for sure using natively parallelized/vectorized builtins are always the better way. This was more intended for scenarios where there just isnt a builtin that does what you need (or your dont know what it is at any rate). At the least i came up with this idea when trying to figure out that type of problem. foo() { echo "${#*} ${*}" } test0() { eval $(echo "${*}"/ | sed -E s/'(.*)$'/'foo "\1"; '/) | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test1() { eval $(echo "${*}" | sed -E s/'(['"'"'])'/'\\\1'/ | sed -E s/'(.*)$'/'foo '"'"'\1'"'"'; '/) | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test2() { # map each line in input to field in an array local -a inArray mapfile -t inArray &lt; &lt;(echo "${*}") # loop over the array, and apply some function to each line echo "$(for nn in "${inArray[@]}"; do foo "${nn}"; done)" | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test3() { # map each line in input to field in an array local -a inArray mapfile -t inArray &lt; &lt;(echo "${*}") # loop over the array, and apply some function to each line for nn in "${inArray[@]}"; do foo "${nn}"; done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ } test4() { local nn="" while read -r nn; do foo "${nn}"; done | sort -g | sed -E s/'^[0-9]+ (.*)$'/'\1'/ }
A) If you're going to make some claim about performance two competing codes, present a minimum working example, not the actual code you were working on. Not only it confuses people who may be interested it can also lead to the so called [XY](http://xyproblem.info/) problem. B) Just because there are lots of cool features in bash, it does not mean you have to use ALL of them in a particular problem. Code readability trumps everything else. C) Your main claim that whether running a command over a loop not as efficient is true particularly for text. See this [answer](https://unix.stackexchange.com/a/169765) for more details. That is why in bash we use specialized tools developed for the purpose. Using `eval` and a really complex `sed` expression is not just one of them. Not only it is hard to debug but it will likely break at any deviation from the problem you're trying to solve. D) I believe you can do what you're trying to achieve using `xargs`, no need to reinvent the wheel. Just create the string of arguments and pipe it like `string | xargs foo`. There are many useful options to `xargs` like `-I` which allows you to replace strings.
You can do this task with two disk operations and one process. This is not tested; please let me know if you're still having issues. &lt; list.txt while read name; do awk -v n="$name" 'NR==1; $0~n' proteins.txt done &gt; families.txt
Hard to say without knowing the structure of your files. `while read` should be used for files with values separated by line breaks. Note you are starting a new process and disk operation for every "name". Here's a method that accounts for multiple lines of input as well as multiple space-separated names per line: while read -a Line; do Names+=("${Line[@]}"); done &lt; proteins.txt regex=$(printf %s $Names; printf '|%s' "${Names[@]:1}") awk -v names="$regex" 'NR==1; $0~names' list.txt &gt; families.txt 
As HenryDavidCursory pointed out, it's difficult to answer the question without knowing the contents of either of the input files. Perhaps there are some metacharacters (e.g. `*`) in `LyticProteinIDs_mod.txt`? Is `list_0.001_30_0.7_appearance2` or `LyticProteinIDs_mod.txt` empty? But there's an opportunity to simplify here. If there's one pattern per line in `LyticProteinIds_mod.txt`, you can simply do the following: head -n 1 list_0.001_30_0.7_appearance2 &gt; LyticGeneFamilies grep -f LyticProteinIDs_mod.txt list_0.001_30_0.7_appearance2 &gt;&gt; LyticGeneFamilies `grep`'s `-f` flag will take patterns from the given file, one pattern per line. If your patterns have metacharacters that you want taken literally, add the `-F` flag to `grep`, which will interpret the pattern as a string and not a regex. You can probably further collapse these two lines into just a single invocation of `awk`, but that's left as an exercise.
Is that... compiz?
There's an open issue about that: https://github.com/cooperhammond/irs/issues/52 Looks like installing chromedriver as a workaround resolved it for someone.
No its compiz ... -reloaded
I'm a huge advocate of [tmux](https://github.com/tmux/tmux/wiki). It's high energy to start, but entirely customizable and so bonkers powerful that it makes window management optional. I refuse to work without access to the combination of tmux and vim.
Why not put these constructs in run control?
You don't, really, but you can wrap the command to make it look a little nicer. exec mlterm -g 83x33+400+0 --name float \ -e sh -c "(cat ~/.cache/wal/sequences &amp;);\ vim -i NONE '+autocmd BufUnload * \ execute \":!vimcmd \" . expand(\"%:p\") \ . line(\".\") . \"$group\"' +$page \ '+normal mm' \"$1\"" Being able to see it all without scrolling will make a huge difference.
Thank you, it does make a big difference!
With this awk command, where would I place ProteinID file?
I would be very curious to know what kind of program it is that requires you to adopt this very brutish method
`cd /; for FILE in **/*; do [ $(program "$FILE") != 'wrong file type command failed?' ] &amp;&amp; break; done`
If the command returns a non-zero exit code for the wrong file and a zero exit code for the correct file you can use: find / -type f -exec command {}\; -print 2&gt;/dev/null The exec flag will filter the output results and the 2&gt;/dev/null will clean up the output. 
I suspect wannabe hacker searching for a password file.
That sweater tho...
seputil, an iphone program, trying to search for a correct input file
Actually, I need a little more help. What if "wrong file type command failed" is only PART of the output?
where do I put the command?
You would replace "command" in the find command with the command you want to run. You can run your command with a bad file and then `echo $?` to see what the resulting exit code was. If it was not 0, great! You can then run your command with a sample good file and , again, run `echo $?`. If it's 0, then you're good to go!
it was 5.
find / -type f -exec ./seputil --infile {} --pair STCK. \; -print 2&gt;/dev/null Would this work? After reading online, the {} is where the filename goes to while the \; tells find to pass along one filename at a time but. Bash isn't my main language and I don't have access to a iOS device to test this on.. Out of curiosity I'd love to hear what you're trying to accomplish with this. 
1)If you want to know what I am trying to accomplish, I am trying to subvert apple's monopoly on pairing home buttons. If you break your home button you can not substitute in a different one, because they are paired to each device from the factory. Only apple can pair a new home button. I am sick of this crap. 2) How can I find the name of the one file that works? Because that is the goal of this, finding out which one is good so I don't have to go through this again.
Do what it says in the message. Download chromedriver and move it to one of the directories in your PATH. I suggest `/usr/local/bin` that should be in the path.
I'm not 100% with this answer as that file may be overwritten when updating.
/u/23-15-12-06 has it. But I think you don't want the period after STCK: find / -type f -exec ./seputil --infile {} --pair STCK \; -print 2&gt;/dev/null The find command filters each item it encounters based on flags. So the first flag is, "is this a file" (yes), and the second flag is, "Does this command, when we run it with this file (replacing {} with the full file name) exit 0?" (hopefully), and the final filter is "Print the file name" So when you run this, it should only print out one filename.
That's a lot better explained than I could have done. Lol. Have an up vote my friend. 
If you get it working, you should post a tutorial for others. I used to have a jailbroken iPhone back in the day but I couldn't stand default iOS so I hopped ship, went to Android and never looked back. Apple's money grabbing and methodical locking down of users hardware pissed me off too much. Lol. 
I sure as hell will 👌
thanks! Lol your usernames are so similar
that `^M` is likely a Windows carriage return - try running the file through `dos2unix`
Backup everything then `dos2unix *`. Seems that you have MS-DOS/Windows style line endings - it's that `^M` character.
That works! Thanks so much for the help!
I did that and now it works! Thank you for all the help.
Scripts written in go and documentation that recommends `zsh`... just how badly do you want to be burned at the stake?
Everyone who replied here is correct. But to be more specific, the error appears to be in the first line of the script. !#/bin/bash appears to have \`\^M\` at the end of it. Since this is (apparently) running on a Linux box, it's literally looking for a program named \`bash\^M\` when what's there is \`bash\`.
That's exactly what I did and the new error said (unknown error: DevToolsActivePort file doesn't exist). I'll see if there is any documentation online about this. I'm not sure if this complicates things but I am using WSL which does not have any GUI capabilities. Thanks for the help!
That's what I've been looking at. Nothing has worked so far, however.
Please use more descriptive titles in the future.
&gt; lst = [1, file1, 1, file2, **2, file3**, 2, file4, **3, file5**, 3, file6, 3, file7] has groups of 2,2,3, not 3,2,2 as you suggest ... Can you be consistent in your question, or if I am incorrect, explain it so people cannot misinterpret it?
I used placeholder names. Make the following changes to my script: * `proteins.txt` -&gt; `LyticProteinIDs_mod.txt` * `list.txt` -&gt; `list_0.001_30_0.7_appearance2` * `families.txt` -&gt; `LyticGeneFamilies` I highly recommend organizing your file naming scheme, but whatever.
This works as expected on the command line too!
It's running. How long do you think this should take?
Is that variable one big string or does it contain an array?
Back up... where are you getting this value, and what are you trying to do with it?
at a guess 'seasons' and 'episodes', but last (deleted) Q here was ambiguous too
I'm late to the discussion, but you should probably pass `-xdev` as an option to find, to avoid scanning `/sys` and `/proc` directories which will take forever and not give you any result anyway. The `-xdev` option will ensure that the search will all happen in the same filesystem as `/`. (If you have separate disks/partitions where your contents could be found, either search in all of them separately or use the `-regex` option to limit the directories; this is more involved).
That probably won't work, unless you're running this in a very very small environment, as `**/*` will expand to something massive, and bash will complain about too many arguments. The `find` approach in this thread will scale properly to large filesystems.
This will print two array elements per line -- separated with a tab. &amp;#x200B; &amp;#x200B; #/bin/bash x=$(echo ${#lst[@]}) # set variable x to equal length of the array count=0 # initialize counter variable which will be used to output array elements during the loop # print elements while count is less than x while [ $count -lt $x ]; do echo -n "${lst[$count]}" (( count ++ )) echo -e "\t${lst[$count]}" (( count ++ )) done exit &amp;#x200B;
Possibly true, however, the `find` approach does not terminate once the file has been found. I'm quite sure there is no way to break out of the `-exec` loop in a `find` command.
I have used dos2unix on my script and am now getting output, but the outputted file only contains the header of my target file. For some reason my script is not properly iterating through my protein ID file and grepping my target files lines into a new file.
Whatever helps you remember is good.
It could take a long time depending on how many files you have and how long it takes for the command to run. If you wanted to log as it goes, there's two approaches, either: ``` find / -type f -exec ./seputil --infile {} --pair STCK \; -print 2&gt;/dev/null | tee -a /foundfiles.log ``` or, if you're extra paranoid, you can write each file you test to a file and then, if the exec causes it to crash, you at least have a record of the last file you tested: ``` find / -type f -exec bash -c 'echo {} &gt; ~/logfile.log' \; -exec ./seputil --infile {} --pair STCK \; -print 2&gt;/dev/null &gt;/tmp/successfiles.log ``` But that will slow it down even more as now you're doing 2 execs...
sorry about this, but I removed that dev null bit, ran it again, and saw tons of errors. SMH I'm such an idiot the fault is mine, it turns out the command I need to run is ACTUALLY ./seputil --outfile STCK --pair (filename generated by -find goes here) Will this code work? find / -type f -exec ./seputil --outfile STCK --pair {} \; 
Maybe all your protein ID files have carriage returns as well; try running `dos2unix` on everything.
Ah. I just learned something new! I’ll be remembering ^U from now on. Thanks!
 Pure bash (version 4+): #!/usr/bin/env bash # # Get the previous month. months=( January February March April May June July August September October November December ) # Get the current month index. printf -v month '%(%m)T' # Use the current month as a key in the list above. prev_month=${months[month-2]} # Print it (for the example). printf '%s\n' "$prev_month" 
Things like these are reasons I keep thinking about switching to vi-mode. When programming most of the time is spent navigating and editing. vi-like modal editing is superior (at least imho) to emacs mode less with chords. vi has a fairly simple "syntax" in normal mode. When executing commands in a terminal, most of the time is spend typing, not editing (well, perhaps searching history). Emacs mode is perfect for this. Modal style (vi mode) doesn't fit as well. My god is it hard to remember the "shortcuts". I have memorised a few into mussle memory, but they don't make logical sense (except `^N` and `^P` for next and previous). I guess if you're an active emacs user, then these are the same or similar (minus a mod key). Vi mode uses normal mode syntax, but that quite inconvenient for quickly writing a few commands. Such a hard toss up. 
I agree; vi-mode doesn't have Vim's text objects, which are a huge part of Vim's superiority. If a single command gets complex enough to warrant Vim, I just use `fc` to bring up the full editor. Protip if you start using `fc` with Vim: `:cq` forces Vim to quit in error, thus *not* running whatever you've been editing. 
`&lt;Ctrl+W&gt;` and `&lt;Alt+D&gt;` are really important to me
Why do you need it to terminate? Sorry on mobile on a crowded bus. Maybe I can help tho? Is it so that you can identify the file?
The first thing I would do is try manually processing this, just to make sure the commands you are using are actually doing what you want. Secondly, use standard debugging techniques, which in this case may be as simple as printing out intermediate results. Check that "$name" is what you think it is.
Don't forget C-w, C-y, ALT-f, ALT-b
oh: ALT-delete
&gt; I just use fc to bring up the full editor. Isn't it simpler to use `Ctrl-x,Ctrl-e`?
You have surrounded the `\w` with `\[ \]` by mistake. This makes bash miscalculate the length of the text and draw things wrong.
Reverse? So if someone typed in 996 it would print 996, but if someone typed in 997 it would print out 799?
Sorry if I wasnt clear enough. Someone inputs 596 It would print 695 but of those numbers, only the numbers that are prime numbers.
If you're in bash (not posix sh), you can use a lot of familiar math syntax with arithmetic expansion: `(( ))`. So divisibility could be tested with something like if (( a % b )); then printf '%s is not disvisible by %s\n' "$a" "$b" fi [Keep the reference manual close at hand (this links to the arithmetic section)](https://www.gnu.org/software/bash/manual/bash.html#Shell-Arithmetic).
How much of the different required steps can you already do with bash? What they probably want you to learn is that bash is used best as the glue between different command line tools. There are a lot of small tools on Unix that take some text input and do something interesting with it and then print output. Printing text reversed, there's a tool `rev` for that. Making bash send a stream of text into it looks like this: $ echo "hello" | rev olleh or like this: $ rev &lt;&lt;&lt; "hello" olleh There is a tool `factor` that's interesting for checking if something is a prime number: $ factor 12 12: 2 2 3 
The prime *digits*? - 123456789 ⇒ 7532?
`set -o vi`
Unless I'm missing something it's the same thing. Same number of keystrokes and all.
Not helping any Islamic State healthcare major.
Pure Bash: reverseComposite(){ for argument; do result= for ((i=${#argument}; i&gt;0; i--)); do character=${argument:i-1:1} [[ $character =~ [1468] ]] || result+="$character" done; printf '%s\n' "$result" done } Output: $ reverseComposite 1234567890 0987654321 "1jif2sdf3 4sd56 7vfjiv89" 7532 2357 7532 
&gt; Someone inputs 596 &gt; &gt; It would print 695 but of those numbers, only the numbers that are prime numbers. Damn college CS professors just can't help making a joke of themselves. They are so detached from professional problems in the real world, they can't begin to imagine what one looks like and make up silly math puzzles instead. 
&gt; I have to write a shell script that takes an integer that someone types in and prints the reverse of that number but ONLY the prime numbers. &gt; &gt; Someone inputs 596 &gt; &gt; It would print 695 but of those numbers, only the numbers that are prime numbers. Damn college CS professors just can't help making a joke of themselves. They are so detached from professional problems in the real world, they can't begin to imagine what one looks like and make up silly math puzzles instead. Sure, they're all math puzzles at their core, but there are plenty of less contrived exercises (like figuring out the boundaries of a classless netmask, calculating the difference between timestamps, etc) that are equally challenging and not obviously useless...
Write the code that reverses, factors, and filters in Java/C/python/whatever and read its input from standard in Use the shell script to pipe input from the to your program. Professor can't claim you didn't use a shell script
&gt; If a single command gets complex enough to warrant Vim, I just use fc to bring up the full editor. So you put in three steps: 1. You type the complex command 2. You put that command in your history 3. You use `fc` to go back to that command and edit it in vim. Whereas, 1. You realize you are typing a complex command, you type `Ctrl-x,Ctrl-e`
`$?` is the exit status of the last command, not the error message generated. It's always an integer: anything other than 0 is failure for whatever reason. `[ $? = 'alphabetic string' ]` will never be true. Here's my untested approach to your extremely concerning problem: for brute_force in {a..z}{a..z}{a..z}{a..z}; do ((counter++)) if openssl \ enc -d -aes-128-ecb -base64 \ -pass pass:$brute_force \ -in flag.txt.enc -out flag.txt.dec then break else echo $counter $brute_force fi done GLHF; don't go to prison 
Read can assign multiple variables: while IFS='\?\+' read -r a b c d ; do printf '%s@%s@%s@%s' $a $b $c $d done
Why no regex?
because this is an example my school gave me. In fact I can do it with awk, but I want to try without it.
Cool, I am just always curious about requirements and what drives them, thanks.
What you're proposing essentially; won't work - you cannot guarantee alignment of tabs. Instead; I would recommend you look at the shell [width and height](https://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window). Then calculate the length of the text and the amount of spaces required to line up with this. 
Thanks! This is pretty much what I was looking for.
The shell is not really required here. I open the Text with Sublime, so I don't have a shell. Sublime just opens the file. I need the same amount of tabs to line up in sublime.
I'd try using the 'column' command to do the alignment. The following will make it align things to get output that looks like a table: column -t -s$'\t' The `-s$'\t'` argument makes it look for TAB characters to understand where a new column should start in the table. This means the text you send into the tool should be something like this: { echo -e "first\tsecond" echo -e "really long text here\thello" echo -e "x\ty" } | column -t -s$'\t' This will produce the following output: first second really long text here hello x y The 'column' command aligns things through space characters, so this is not the TAB characters you asked for. If you want to have TAB characters, I'd just add the 'unexpand' command on top of this: { echo ...; echo ...; } | column -t -s$'\t' | unexpand -a
Tabs alignment aren't guarenteed. You can control the length by the `tabs` command, but in general you should use spaces to align. Anyway, `column` is built for things like this: #!/usr/bin/env bash column -s : -t &lt;&lt; 'EOF' smart_sdc_Z4D2NRSE.result: Seagate Enterprise NAS HDD ST6000VN0001-1SF17Z (compatible): PowerOnHours: 31030 smart_sdd_S3Z2NB1KB56665K.result: Samsung based SSDs Samsung SSD 860 EVO 500GB (compatible): PowerOnHours: 882 EOF Output: smart_sdc_Z4D2NRSE.result Seagate Enterprise NAS HDD ST6000VN0001-1SF17Z (compatible) PowerOnHours 31030 smart_sdd_S3Z2NB1KB56665K.result Samsung based SSDs Samsung SSD 860 EVO 500GB (compatible) PowerOnHours 882 
I'd like to try this solution. The Problem is the whole thing runs in a loop. the loop is simplified like this with column added. How do I write it correctly with my loop? When it is implemented like this, it only adds four spaces, no matter how long the text before was: for file in folder do [[ -e "$file" ]] || break #no files hddname=$(basename -- "$file") echo -e "$hddname: $hddname2 $HDDComp: \tPowerOnHours: ${PoH}" | column -t -s$'\t' &gt;&gt; "$sm_file" echo -n "more echoes here" &gt;&gt; "$sm" done
The trick is to put the pipe after the `done` word, like this: for file in folder do [[ -e "$file" ]] || break #no files hddname=$(basename -- "$file") echo -e "$hddname: $hddname2 $HDDComp: \tPowerOnHours: ${PoH}" done | column -t -s$'\t' &gt;&gt; "$sm_file" The output of the whole loop will get sent into the 'column' program as one thing. If you want to send something additional into the same pipe, not just your loop, then surround everything with `{ ... }` and put the pipe after that. For example, if you'd like a sort of table header added to your columns and an empty line after the table, then do this: { echo -e "HDD name\tPower on hours" for file in folder do [[ -e "$file" ]] || break #no files hddname=$(basename -- "$file") echo -e "$hddname: $hddname2 $HDDComp: \t${PoH}" done echo # an empty line; this needs a -L parameter added to 'column' } | column -t -L -s$'\t' &gt;&gt; $sm_file
A 4-character password with AES encryption? That's kind of incongruent. Like trying to build a go-kart with a Lamborghini engine. For your next experiment, I suggest trying 50-character passwords with MD5 encryption. &amp;#x200B; (Just to be clear, this post is tongue in cheek. There's no useful information here.)
What was your solution?
It turns out that there is an open bug with the script itself. So after editing `~/.irs/config_.py` (successfully!), I found another error with ChromeDriver. The bug ([https://github.com/cooperhammond/irs/issues/52](https://github.com/cooperhammond/irs/issues/52)) has not been fixed, so I decided to use another downloader until it is fixed. I ended up installing SMLoadr ([https://git.fuwafuwa.moe/SMLoadrDev/SMLoadr](https://git.fuwafuwa.moe/SMLoadrDev/SMLoadr)) which took me a while to setup but now works just as well. Not the best outcome but since I haven't taken enough Python classes it's a bit over my head.
You should probably look into `printf` and fixed field widths. Have a read through this: https://unix.stackexchange.com/questions/396223/bash-shell-script-output-alignment
If you want to duplicate a drive, just dd it over and expand the filesystem (if you need). There are so many things to check, timestamps, attrs, permissions, it's a mess with rsync unless you know exactly what you want. This isn't really a bash question.
The third thing you tried should work. I have a function "args" in my ~/.bashrc to experiment with what bash is actually sending into a program as the argument, and it says this about the third you tried: $ args -E 's;["'\'']&lt;insert long regex statement here&gt;["'\''];..\1;g' 2 args: &lt;-E&gt; &lt;s;["']&lt;insert long regex statement here&gt;["'];..\1;g&gt; This seems like it should be what you want? That 'args' function I used looks like this: args() { printf "%d args:" $# printf " &lt;%s&gt;" "$@" echo } 
Backslashes in single-quotes are literal, so `'\''` becomes: beginning quote, backslash, ending quote, beginning quote, leaving an unterminated string. Backslashes in double-quotes are not literal, so `"\""` becomes a string containing only `"`. However, now you have to escape your backslashes in bash so they are passed to sed (for example, `\\1` for your match group). So: `sed -E "s:['\"]&lt;long_regex&gt;['\"]:..\\1:g"`. Again, make sure to escape all your backslashes in your `&lt;long_regex&gt;`. 
I would try to repeat the rsync command line you used, but add a `-n` = `--dry-run` parameter so that it doesn't actually copy, just check if there's something new to copy. When doing this, I think you need to experiment with `-v` and `-i` parameters to get it to output some info about what it would like to copy. There is also a parameter `-c` = `--checksum` that makes rsync compare the contents of the files, perhaps add that as well to really check if everything got copied correctly. This will take a long time if there is a lot of data to compare.
You can probably only do a clean rsync if the OS is not running on either drive at the time. IE, use a livecd/usb. Using dd (or ddrescue) is more reliable for 1:1 copies if disks are the same size or new disk is larger. 
`args` returned `&lt;s;......1;g&gt;` isn't `sed` expecting `&lt;`s;......1;g`&gt;`
Assuming that you have all day... dd if=/dev/filesystem1 | md5sum dd if=/dev/filesystem2 | md5sum
No, the quotes are for bash. They get removed by it after it is done working through what was typed on the command line. The program will not see those quotes. For example, the following are all the same word for bash: $ echo aaabbbccc aaabbbccc $ echo "aaabbbccc" aaabbbccc $ echo 'aaabbbccc' aaabbbccc $ echo "aaa""bbb""ccc" aaabbbccc $ echo aaa'bbb'"ccc" aaabbbccc 
I was just told to do it this way, as I have little to no knowledge outside of googling and the man. Thanks for the reply!
Yeah its several TB. It's going to be a long wait, but it has to be done unfortunately, I'll give what you said a shot thanks!
I did not know that. Unfortunately I'm on a mac ecosystem primarily and everything is 3x more annoying to do...
Which will almost certainly not match for a new user using rsync.
Sure thing. If you have any reason to believe the source drive is problematic, you might look into ddrescue. Arch has a decent guide for both here: https://wiki.archlinux.org/index.php/disk_cloning
Try this aa=&lt;whateverYourePipingIntoSed&gt; bb="$(cat &lt;&lt; EOF s;["\']&lt;insert long regex statement here&gt;["\'];..\1;g EOF )" eval echo 'echo "${aa}" | sed -E '"'${bb//"'"/"'"\""'"\""'"}'" The idea is the following: * the `cat &lt;&lt; EOF` bit reads the original regex into a variable (`bb`) * the `${bb//.../...}` bit turns each `'` into `'"'"'`. running `echo "'${bb//"'"/"'"\""'"\""'"}'" should *always* return the original `$bb`, so this adds a quoting layer so that after sed strips it off it gets the regex you actually want it to evaluate * the `eval echo '...'` bit uses echo to construct the command such that it is the same as if you had directly typed it into a terminal, and then evaluates the expression using eval. 
&gt; The program will not see those quotes. journalctl -f | sed 's;.*;Ahh... I see ....;g' &amp; ps aux | grep pts ; kill %+ Doh! &gt; are all the same word for bash Surely you mean they are all the same word to `echo`.... `bash` would handle those lines differently if there was a `$` in front of those which (OK, I was wrong last time, but what the heck, I'll stick it out again) I think infers that `bash` doesn't see them as all `aaabbbccc`.
Yeah, the quotes are there to get bash to ignore things like `$`, and to help build arguments that have spaces in it. And yeah, I guess it's not really bash itself seeing things the same. It first has to work through the parts of the text that has a special meaning for bash itself. Then afterwards the words will be the same from the point of view of the commands and programs.
With `sed`, my rules are as follows: 1. Single-quote the overall regex (which you're already doing). This minimizes the backslashing madness. 2. When a single quote or a bash-level substitution is needed, close the single quote, append the necessary stuff in double-quotes, then add a single quote and continue. So I'd formulate your `sed` command as follows: sed -E 's;["'"'"']&lt;long regex with '"${HOME}"'&gt;["'"'"'];..\1;g' It looks noisy as hell, but with practice, you should eventually get the hang of correctly parsing the final expression by eye.
&gt;I'm on a mac ecosystem If you'd led with that, I would've recommended using the native Disk Utility instead, which Does The Right Thing by unmounting both source and destination drives before making a clean block-level copy. In the case where the source is your boot drive, simply reboot from your Recovery HD volume; Disk Utility is available there too. Read [Use Disk Utility to Clone a Mac's Drive](https://www.lifewire.com/use-disk-utility-to-clone-macs-drive-4042367) for details.
Ya holy shit powershell is complete garbage and based off of .net which was written by toddlers. 
I would create an argument array for your command in yt.sh, and then either use an array or `set` to build your command after parsing all the command line arguments you need. Since your output directory is an optional argument, it is more idiomatic to specify that with a flag. You also probably want to allow downloading any format to /tmp too, so let `-t` be a mutually exclusive flag. Since a lot of your `if [ -d "$2" ]; then` blocks only differ in one argument, you can replace them with `"${directory:-"$fallback"}"`. Anyway, [here's the result of my tinkering (WARNING: untested)](http://ix.io/1CiA). Some of this is the way *I* prefer to write, not the way you necessarily should. I went with the idea of building up a command with `set`, since that seems to match your wrapper here.
Give yourself a simple problem, and step through the process of automating it. 
This seems like a weird resource to give a beginner, but it was the most important single document in my learning experience. Don't worry about what you don't understand, just read it once a month and notice how everything starts to connect. Knowing what not to do will save you a *lot* of time. https://mywiki.wooledge.org/BashPitfalls
There are resources linked in the side bar.
Thx a lot I read it and I will to understand it
I can not do it like that. Like this my echoes are no longer working as expected. Is there some way to do this without the redirect for everything at done | column -t -s$'\t' &gt;&gt; "$sm_file" ?
Do a thing then automate the thing. 
Reading a sidebar is too much work, how do I become a computer master?
The following idea is perhaps dumb: In monitor.sh, you could move that code from your 'case' construct into different functions "single", "vertical", "laptop", and then change the 'case' to this: case "$1" in single|vertical|laptop) $1 ;; # ... esac That `$1` line will make bash run the function with the exact name as what's the content of the $1 variable. That's the part that is perhaps dumb to do, I have a feeling something might go wrong in the future, though I'm not seeing what exactly. You could also add that `"")` case to this scheme as well, first create a function named "default" and then change 'case' to this: case "$1" in ""|single|vertical|laptop) ${1:-default} ;; *) usage; exit 1 ;; esac 
Use $() instead of ` 
I would personally write this kind of script with an optional flag instead of prompting the user for installation directory.
Curious, just for readabilities sake?
You should probably quote your variables. Also, you might want to check out the tool [shellcheck](https://www.shellcheck.net/).
No, they support nesting, which can be useful in future if you add more features to this script
readability and nesting
Ah, yeah, makes sense! Thanks
Thanks!
Small side note: \`\[ $?='string' \]\` is a test of a string, not a variable assignment. Also, it's much safer to quote those variables than to leave them unquoted.
To parse JSON there are specific tools like `jq`. I assume there are XML parsers, too, although I never needed one.
Please never ever use bash/grep/awk/sed to grab values from JSON/XML/YAML and similar. I've seen it far too many times to make my eyes bleed every time I see something like this. Python json, ElementTree modules, jq or xmllint / xml2? Or google "command line json/xml parser" and pick the best tool for you.
Well I know that python or other languages might be better for this, but this is a one time thing that is sort of time sensitive, so I figured it would be much better to do it in a language that I already know, than to learn python from scratch.
Assuming everything is in KM, no float numbers and format is same for all files, there are no extra whitespace and nothing unexpected happens: ``` for file in *.xml do echo $file - $( grep '&lt;text&gt;.*km&lt;/text&gt;' $file | sed -r 's#&lt;text&gt;([0-9]+) km&lt;/text&gt;#\1#' ) done ``` should give you for every xml file: file.xml - 205 Now please let me be ashamed of myself while I silently vomit into the corner and wash my hands with bleach. ;)
:D I looked into the jq and its actually pretty easy to do what I need. Can you explain why it is so bad to use the regular command line tools for this kind of thing?
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
In "languages" like XML, the context of things matters which is a problem for "regex" patterns. "Regex" are those search patterns used in all the normal tools like 'awk', 'grep', 'sed'. What I mean with context is, when you look at the example files you showed earlier, there are keys that are named "value" and "text". Those keys show up in two different places of the file. In the two spots, they mean something different. In the first spot the context is "distance" and in the other it's "duration". This kind of thing is exactly what "regex" = "regular expressions" can't work with, they have no feature to deal with context. To successfully use regex, you would first have to translate the file into a different "language" that regex can deal with. If the file would look like this for example, then regex would work fine: rows/elements/distance/value=205012 rows/elements/distance/text="205 km" rows/elements/duration/value=7515 rows/elements/duration/text="2 hours 5 mins" rows/elements/status="OK" 
- IMO you can add some error checking especially when calling `keytool` command. - Maybe you can say innthe prompt that the name is also the directory name. - Also it's a small script here but I think it's a good habit to have. I think it increases readability
To be honest no, not going to help. If you're using programs like aircrack for illegal purposes it's highly advised that you learn yourself or rephrase the question. If you don't know what you're doing with a program built for illegal activities you're on your own.
Well Aircrack grabs MAC from the air, I want to build something that tracks and logs all MAC that come by my house, with no one around for miles, that should only be me and my family, this is a easy way to see if anyone has been snooping around my house with a MAC I don’t recognize 
Well Aircrack grabs MAC from the air, I want to build something that tracks and logs all MAC that come by my house, with no one around for miles, that should only be me and my family, this is a easy way to see if anyone has been snooping around my house with a MAC I don’t recognize. So yes Aircrack can be used for illegal purposes but I truly believe my use for it is legitimate 
some more: - The chmod may be too much: when doing this you potentially remove *group* and *other* rights on this file. You can do something like `chmod u+x` (not sure plz check man page) - When searching for the keytool *executable* you should use `find` instead of shell expansion.
so the step of the script IMO is to get only what you want from input file : grep -Eio '^[a-z0-9:]{8}' "${input}" &gt; /tmp/whatev.lol # then use grep -f grep --col -f /tmp/whatev.lol "${OUIFILE}" Both files need to have only one mac by line hf and format your code ffs
`jq` doesn't rely on regular expressions, it's a full-fledged json parser and generator.
You're using `#!/bin/sh`, but Bash features such as arrays; you should use `#!/bin/bash` (or `#!/usr/bin/env bash`) instead.
jq is so good that it does NOT work with reddit's json files. 
&gt; can you explain why it is so bad to use the regular command line tools for this kind of thing? because bash nazis, see also 'soup nazi, Seinfeld'.
Dude, sweet.
hey how i can acess sidebar &amp;#x200B;
Looks neat. Since you're looking for feedback: - If you think about it, that first comment wouldn't be necessary if you named the variable, `default_installation_directory` or some shorter but just as descriptive variant. If you can avoid repeating yourself (even between code and comments) it pays off in the long run. And if doing this helps you avoid comments, it's actually a good thing. - It's awesome the way you prompted the user with the defaults in square brackets. This is very good form when prompting a user for an optional overriding value. - Note that your `#!` line claims that `/bin/sh` should be your interpreter. Note that `/bin/sh` is allowed to be a barebones [POSIX compliant](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) shell and could be missing a few features: - `read` may not support the `-p` option. - POSIX compliant shells don't need to support arrays (`files=( $pattern )`) - Consider changing that `#!` line to `#!/bin/bash` as it looks like you're depending on those sweet sweet GNU extensions. - You absolutely need to enclose variable expansions within command lines in double quotes. This includes the the last argument to `chmod` and the expansions of the `$keytool` variable when you're actually running the tool. For example, if there is a space in the expansion of the glob, `$pattern`, it would cause `chmod` to error out in a strange way. - Back at the beginning of the script ur use of an array to hold the expansions of `$pattern` was 100% on point. This helps allow you to, later on, ensure that no matter what whitespace characters were in the glob-expansions of `$pattern`, you'd be able to grab the first one and assign it to `keytool`. However, the approach you chose to store the aliases for your imported certificates could be prone to dreaded word-splitting-after-variable-expansion during your `for` loop. Consider using a while-read loop to iterate over each alias like this: ``` "$keytool" -list -v -keystore "$install_dir/shared/certificates/default.ks" -storepass '' | \ sed -n -e 's/^Alias name: //p' | while read item; do "$keytool" -exportcert -keystore "$install_dir/shared/certificates/default.ks" -alias "$item" -rfc -storepass ''" done ``` This way, you can rest assured that each iteration of the loop will have a full alias name in the variable, `item`, even if an alias name happened to have an embedded space. And note that a tonne of advice on shell scripting surrounds variable expansions and word splitting. It's not because folks assume that whitespace may appear in a variable; usually there's a pretty low chance of this happening. It's more because where such a scenario comes up, it's extremely difficult to debug in a moderate sized shell script: Commands just won't work properly and it'll take a long time to figure out why. Use quotes to clearly mark each argument to each command you run and you'll spend less time debugging. - Just a note that buffering and storing the output of the `$keytool` command in `cert`, then immediately emitting it isn't really necessary. Just running the `$keytool` command will have the same result, will seem a bit faster, and will consume a bit less memory. - Where you need to use backticks, consider using their equivalent, `$(...)`. Believe it or not, backticks can be nested, but believe it or not, it's pretty much impossible for humans to visually match backticks when they're nested. You'll be doing maintainers a favour by (1) avoiding backticks when possible, and (2) where you have to use them, using the `$(...)` form. This is all super minor stuff, but hope it's helpful.
When it comes to writing software, it's all about solving problems...that start simple and get a bit more complicated over time. Usually, when you start with a scrappy solution to a problem, and the problem gets a bit more complicated, the solution evolves with the problem and remains nice and maintainable. When it comes to a problem that involves parsing json/html, though, if you start by applying regex to lines in a shell script, as the problem gets more complicated, the solution never quite moves in the direction of _a json/html parser_. It more often turns into a Rube Goldberg dumpster fire that you grow dangerously attached to because it took so long to debug and get sorta right. The folks who say _just use `jq`_ or _use a more complete language with a reliable json parser_ aren't (always) being jerks. They've just been down that road enough times to steer clear of it, and like focusing on new problems instead of the solved-problem that is parsing a standard data format.
If you're on desktop, you shouldn't have to do anything. It should already be there. If you're on mobile, it will depend on which Reddit client you're using.
I used desktop through web but it was not showing so u went to settings and tried but unable to do that 
Does it show on old.reddit.com/r/bash?
When y'all leave school you might learn you don't always have access to the right tool. Don't @ me. while IFS=\&gt; read -d\&lt; tag data; do [[ $context == "distance" &amp;&amp; $tag == "value" ]] &amp;&amp; printf %s "$data" context="$tag" done &lt;file.xml
for a real world problem like parsing a reddit json file, jq appears to fail. 
I would extract the distance - value part from the above json with jq and afterwards divide by 100 with bc. This gives you the original value from the original source.
Could you give an example of jq's failure with the reddit json?
It really rustles my jimmies when folks sit on principle. When y'all leave school you might learn that you won't always have access to the correct tool. ( while IFS=\&gt; read -d\&lt; tags data; do case $tags in *distance) ((c^=1)) ;; value) ((c==1)) &amp;&amp; printf %s $data ;; esac;done &lt;file.xml )
It really rustles my jimmies when folks sit on principle. When y'all leave school you might learn that you won't always have access to the correct tool. ( while IFS=\&gt; read -d\&lt; tags data; do case $tags in *distance) ((c^=1)) ;; value) ((c==1)) &amp;&amp; printf %s $data ;; esac;done &lt;file.xml ) Technically no regex lmao don't @ me
I always use catfish search file to find all locations where espeak is located at. Just make sure your searching the whole system /. &amp;#x200B; [http://espeak.sourceforge.net/voices.html](http://espeak.sourceforge.net/voices.html) &amp;#x200B; [http://espeak.sourceforge.net/mbrola.html](http://espeak.sourceforge.net/mbrola.html) &amp;#x200B; [http://espeak.sourceforge.net/add\_language.html](http://espeak.sourceforge.net/add_language.html) &amp;#x200B; [https://askubuntu.com/questions/554747/how-to-install-more-voices-to-espeak](https://askubuntu.com/questions/554747/how-to-install-more-voices-to-espeak) &amp;#x200B; [https://superuser.com/questions/518448/realistic-voice-for-text-to-speech](https://superuser.com/questions/518448/realistic-voice-for-text-to-speech)
add .json to this post's URL, and parse the fifth comment, using jq. 
After seeing the .json and reading a few comments about the code quality of Reddit especially in regards to json, I am not surprised. `jq` balks already at pretty-printing the resulting code. This is terrible, terrible json. If I would have to work with it, I would use workarounds with sed/grep/awk as well.
Yeah, that should do the trick. I put the `2&gt;/dev/null` in there so that STDERR will be redirect to /dev/null. When you do a `find` on the root dir you get caught up in /proc or /dev and those throw a bunch of errors so I was just blocking them out.
Use the package manager of your distro to list the file contents of the espeak package.
Even better: Use a tool like clonezilla, which takes care of cloning in a sensible manner. Doesn't copy too much, just the used clusters, and avoids other pitfalls when cloning (identical UUID etc).
printf is probably your friend, you can use a format string like `%010d`; $ printf "%010d\n" "1234" 0000001234 
Oh great ! Actually, I am quite a beginner on that one. Seems that I am not that successful. I have tried: #!/bin/bash for i in \seq 0900000000 0900000020`` do echo printf "%010d\n" "$i"; done But it outputs as: printf %010d\n 9e+08 Perhaps another method is needed ?
 echo printf "%010d\n" "$i"; must be printf "%010d\n" "$i"; &amp;#x200B;
Could you try showing an example of what you're trying to set?
Thanks. Seems to work well. I am super bad with awk and unix scripting in general. I do have another use case. How would I do it if I have variable number of columns I need to match against my dict? Earlier I was always matching column5, but now lets say my keys that I extracted from file 1 can occur in any column after column 6: [So in a particular line you can have et222, et2 and et1 and this column should be matched as well since et1 is in the key we stored from file1] &gt;&gt;1 2 3 4 5 typez et1 et222 et2 &gt;&gt;5 5 5 5 5 typez et222 et2 et1 &gt;&gt;3 3 3 3 3 typef et4 problem is there maybe variable number of columns for these keys but they always occur after column 6[ which is the type column]
Is this what you want? $ { echo 'Num;Content'; for (( x = 0; x &lt;= 999999999; x++ )); do printf '%010d;"&lt;!--{""id"":""text-2""} /--&gt;"\n' "$x"; done; } | head Num;Content 0000000000;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000001;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000002;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000003;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000004;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000005;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000006;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000007;"&lt;!--{""id"":""text-2""} /--&gt;" 0000000008;"&lt;!--{""id"":""text-2""} /--&gt;" Remove the `... | head` at the end to make it continue counting to 999,999,999. For redirecting into a file, add a `&gt; filename` to the end after the `}`. It seems bash is a bit slow and completing this will take quite a while. The output will be several gigabytes large. Using a 'for' loop and `seq 1 999999999` or `{1..999999999}` is not possible, it's too many numbers and you will run out of RAM. You need to use the "C"-style loop like I showed here.
here an example: et3mHsgzu6kF5hE9^W?j+^XFHw^rrkmSTm^_K!BbHAd$3SR3UtmhRWWfh=_fUX3s8Pc&amp;UYRVCAX3eNnP+?_e2H$KksUbMM2vfkzW%MFN6vGBUYGMWPN@ZecMVVwm2YdRWKz$qqmQ%e?pVf25ThB2+x79sd2&amp;W58$fz*VMUT$2cr4SkDYhk4*Q6vtt9B*uCfKxvb&amp;HTsc6zP6EU3a#Ub3^yFr*FMpU65b2WS8*aZXz-NpL9*-X?TA$m#jcaDZaga*
Can you show the exact command you tried?
yeup, the command is: `set myVar=value`
might be better to use a file in this case...
`set myVar='et3mH...aga*'` should work. There are some special characters in that string that you shell would be trying to interpret.
Did you remember to wrap that string in single-quote ' chars to Renee a string literal?
Two things: * The command should be just `myVar=value`. * The string contains, among many other special characters, `&amp;`, which would normally run the command up to that point in the background and start a new command. So the string should be quoted. Furthermore, it contains dollar signs, which normally introduce variable expansions even inside double quotes. So the quotes need to be single quotes, which do not treat any characters specially other than `'` itself So, for example: myPassword='et&amp;blah$stuff;magic' The only character that would be a problem then are single quotes themselves, which would have to be encoded as `'\''` instead (end the quotes, escape a literal single quote, then start the quotes again).
yes but impossible to run a command inside my redis-cli I have experienced, maybe summon the file content by other mean? &amp;#x200B;
works but seems my pass isn't available in the redis-cli to auth, maybe you have a clue about how summon my variable inside an another scope? 
Do you need to export it instead of just setting it? `export myVar='...'` This will put it in the 'global' scope, so any other called scripts or functions will have access to it
yes that was export indeed, Awesome,maybe I'm forgetting something cause I'm trying but redis returns me: 127.0.0.1:6379&gt; echo ${myVar} "${myVar}" 
you can drop the echo command, printf will print it to the terminal. The `\n` at the end is a newline so it prints a full line. There are other format strings you could use for example `%s` which expects a string; printf "%010d:%s" "1234" "this is a string" 
sounds like it's now a redis problem. maybe `print` instead of `echo`, but i don't know anything about redis
okay thanks for your assist 
Well show me the whole command you're trying when you set it. I'm curious what you're typing.
of course, here: export myVar="$6nw8ZUuAB9BScNnbj?XJTn5+^9TxJTgCpx@LrBaN7+nh+ZfDh7B3%kjEB428UB3Dn9Zhk+V6mxmGgbABrTW@7+Nan^-53mGupRvBs!#6FPm^$HhpT#H?s!nn_-8g!SzQLs=csVWqp26qd++BqS8%?gSw=V45D_^M_ybQhaJbp=TfhAb&amp;g%4M629Y&amp;-A48TxpCmpsy&amp;%uWktTUMwx28@fu*XsUJD*B3nN9dAqRMjqe?W2^_Q@@eC8V+BwzTpxPr@"
redis-cli is not bash, you can't just type bash commands into redis. Yes, they both have "echo", but that's about all they have in common. Why aren't you using the `redis-cli -a &lt;password&gt;` argument? Have you ever noticed how most people asking for help online start by explaining what they're trying to do? Do you think you saved time by making people ask six different questions to drag this information out of you instead of saying that in the first place?
Now that you say it...
Came to comment about shellcheck, great tool! 
`for file in *.flac; do ffmpeg -i “$a” -qscale:a 0 “${a[@]/%flac/mp3}”; done`
Using double quotes here allows the shell to interpret the $ and the ! -- have you tried single quotes? 
There's a dedicated r/awk subreddit for your future AWK questions, but to close this particular loop and made the script more readable: # First file contains the lookup table ARGIND == 1 &amp;&amp; $2 == "abc" { want[$1] = 1 print "Want: " $1 } # Second file contains the actual data ARGIND == 2 &amp;&amp; $6 == "typez" { for (i = 7; i &lt;= NF; ++i) { if ($i in want) { tl += $5 print "Tallied: " $i " (" $5 ") on column " i # ASSUMPTION: We're done on first match found. # If $5 should be tallied N times for N matches, remove the "next" statement next } } END { print "Total: " tl }
It's because you have curly quotes, I think. They'll just be treated as part of the filename; looks like it's complaining that the output name is ``.mp3”``, and it doesn't know what format that is. You want ``"$a"``, not ``“$a”``, and likewise for the output variable.
Wow, thank you. &amp;#x200B; What is it about **./** before \*.flac that causes a problem? Or is it something else in my script? &amp;#x200B; In your script (thanks for sharing!) does the %%.flac strip everything from the back of the filename to just before the first instance of a space? What does mv change in the file name at the end with ## and the regex?
u/matfus, if that was copy-pasted directly from your script, and not typed into Reddit by hand, then I'm surprised it worked at all for *any* files, because you're using the wrong kind of double quotes: “ - U+201C : LEFT DOUBLE QUOTATION MARK {double turned comma quotation mark} ” - U+201D : RIGHT DOUBLE QUOTATION MARK {double comma quotation mark} bash instead understands and properly handles: " - U+0022 : QUOTATION MARK With the former, even a non-spaced file named `test.flac` would be passed to ffmpeg as the literal `./“test.flac”`, which doesn't exist. If you write your scripts in a word processor, or any editor with "smart quotes" and other code-incompatible substitutions automatically enabled, you really want to switch to a proper code editor. This script should work fine for all your FLACs, spaces or no: #!/bin/bash for a in ./*.flac; do ffmpeg -nostdin -i "$a" -qscale:a 0 "${a/%flac/mp3}" done I've also replaced your `&lt; /dev/null` with ffmpeg's `-nostdin` flag, and removed the unnecessary array deferencing at the end.
Hmm, yeah thanks for this. I'm just using a plain text editor to whip this up, and it might be changing it to the other quote type. &amp;#x200B; Strange thing is, this script works fine if I manually remove the spaces in the file names first and then run it to transcode. 
Thank you for this! I just checked my script in nano from the command line and it looks like my double quotes are the straight kind. Sorry for the confusion, but thanks for pointing this out. So I tested your recommendation and it worked great with filename spaces. I added the \[@\] back in just for kicks, and this is what causes the invalid argument error when a space exists. &amp;#x200B; &amp;#x200B;
For parsing XML , you can use `hxselect`; # cat myfile.xml | hxselect -c -s '\n' distance text 205 km Get `hxselect` at [https://www.w3.org/Tools/HTML-XML-utils/](https://www.w3.org/Tools/HTML-XML-utils/)
Works great. Thanks for the all the help.. And final question related to this, how would I go about getting the negation of this? I want to sum column 5 only if all the columns from 7-&gt;NF don't have a key in want.
Alright, so I don't like the ./ because it isn't strictly the file name. I change directory to the one I want to operate on and then run through all the files. You could write some tests or do \*.flac if you prefer, depending on your needs. &amp;#x200B; As for what is wrong in your script, I don't honestly know for sure. I'm wondering if it is the output redirection you're using. You see how I'm just using the file as an argument in quotes so that spaces aren't an issue. That's how I would do it, I don't even know why you used the redirection to /dev/null. &amp;#x200B; So the `${file%%.flac}.mp3` strips off the .flac and then writes a .mp3 to the file, since I'm converting from flac to mp3, as I don't need the lossless format on my phone for music, I prefer to have the space and 320 CBR is more than enough quality for me. &amp;#x200B; That bottom part I should actually fix and implement a test before running it, although I don't think it hurts to have there anyways. What I do there is check each file to see if it has a naming convention like: 01. OR 01 - Song Name then I remove that part so each file is just the song name without any numbers. I use the ID3 tags to keep track of all that information and I like my file names clean. Hopefully some of this helped :).
its 15.6 display approx 1920*1200 px
and i tried old version its not showing can u pls share the resourses through other way 
On mobile now, so I’ll just say to do your tallying outside the “for” loop. The “if ... next” statement inside the loop forces awk to immediately move on to the next input line when the “if” condition is true. 
I guess you could do something like: &amp;#x200B; file_count=0 for file in *; do if (($file_count&lt;=1000)) then mv "$file" "dir1/" elif (($file_count&gt;1000 &amp;&amp; $file_count&lt;=2000)) then mv "$file" "dir2/" elif (($file_count&gt;2000 &amp;&amp; $file_count&lt;=3000)) then mv "$file" "dir3/" else echo "Done!" &amp;#x200B; I imagine something like that would work. I'm sure there is a quicker way and probably a better route to take, but that should do the trick if you feel like finishing that script.
Perhaps Perl could do this in fewer lines? Perl developers, please pass by :-) I am curious to read your solution.
Then the sidebar should show just fine.
This will create a unique directory for each of the different first letter of each existing file, and move all files that start with the character to the corresponding directory. &amp;#x200B; `for x in \`find | cut -c 1\`; do` `mkdir $x` `mv ${x}* $x/` `done` &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
This will create a unique directory for each of the different first letter of each existing file, and move all files that start with the character to the corresponding directory. for x in `find | cut -c 1`; do mkdir $x mv ${x}* $x/ done 
I'm torn between chastising you for not posting any code and rushing to win 5 karma for my flexible solution. In the future, please make an attempt and show your approach. splitDir()( # [target] [sub_dir0] ... [sub_dirN] # Default behavior: split working directory in twain count=; target="${1:-.}"; shift # Create "Dirs" array to hold list of subdirectories (($#)) &amp;&amp; { Dirs=("$@"); } || { Dirs=("${target}0" "${target}1"); } # Create subdirectories and iterate over target glob for sub in "${Dirs[@]}"; do mkdir -p "$target/$sub"; done for file in "$target"/*; do # Only operate on regular files [[ -f "$file" ]] &amp;&amp; { # Cycle through Dirs array sub_index=$((count++ % ${#Dirs[@]})) # Move files one at a time mv -v "$file" "$target/${Dirs[sub_index]}" };done) Usage: $ ls -aF test test.bk test: ./ ../ 0 1 2 3 4 5 6 7 8/ 9/ test.bk: ./ ../ 0 1 2 3 4 5 6 7 8/ 9/ $ splitDir test 'test/0' -&gt; 'test/test0/0' 'test/1' -&gt; 'test/test1/1' 'test/2' -&gt; 'test/test0/2' 'test/3' -&gt; 'test/test1/3' 'test/4' -&gt; 'test/test0/4' 'test/5' -&gt; 'test/test1/5' 'test/6' -&gt; 'test/test0/6' 'test/7' -&gt; 'test/test1/7' $ splitDir test.bk foobar "with spaces" 'test.bk/0' -&gt; 'test.bk/foobar/0' 'test.bk/1' -&gt; 'test.bk/with spaces/1' 'test.bk/2' -&gt; 'test.bk/foobar/2' 'test.bk/3' -&gt; 'test.bk/with spaces/3' 'test.bk/4' -&gt; 'test.bk/foobar/4' 'test.bk/5' -&gt; 'test.bk/with spaces/5' 'test.bk/6' -&gt; 'test.bk/foobar/6' 'test.bk/7' -&gt; 'test.bk/with spaces/7' 
Hey you didn't even try
All I could do was mkdir multiple directories, and do various horrible if statements to move the files in the directories created; it was a terrible approach, so please do not be so uneducated in replying; if you think my question was not valid, why answering it?
Please post your code next time.
did you read my comment? &amp;#x200B; &amp;#x200B;
I'm sure they did and I did as well. You sound quite ungrateful for someone who just did free work for you. You should have posted what you have attempted thus far so the community here could better help you with finding and understanding a solution. What you've done is just ask for someone to write you a script which has been done. You did it again later in this same post asking for a Perl dev to write more code for you. The /"uneducated" solution /u/HenryDavidCursory provided should work for you until you can make/find someone to make you something else. 
 -bash: ppwd: command not found I assume this is supposed to be a command that returns the parent directory of the current working directory, but sharing a prompt with a custom command without sharing that command isn’t really useful.
I use IFS=" " at the beginning of my script so that spaces in file names are ignored. 
 x=$((37/7))
 $ mkdir $(seq 0 1 20) $ for i in $(find . -type f -print) do (( FOO = $RANDOM % 20 )) mv $i $FOO/. done 
say for example that `x` here is the file named "foo". You can use the first character this way: for x in * do mkdir -p "${x:0:1}" mv "$x" "${x:0:1}/" done
I think the top answer is great already, but here you go: use warnings; use strict; use autodie; use Cwd 'abs_path'; use File::Copy 'cp'; my $source="source"; my $dest = "dest"; my $numdirs = 5; my $numfiles = 1000; my $filesperdir = int($numfiles/$numdirs); for my $x (1 .. $numdirs) { mkdir $dest . $x; } open(my $cmd, '-|', 'find', $source, '-type', 'f'); my $i = 0; while ( my $file = &lt;$cmd&gt; ) { chomp $file; cp $file, abs_path($dest . (int($i++/$filesperdir) + 1)) || die "$!"; } 
I just went for the [ignore option](https://www.reddit.com/r/apple/comments/aukgsw/is_apple_overpriced_ios_development_costs/)....
I think you're in the wrong thread my dude
&gt; I'm torn between chastising you for not posting any code and rushing to win 5 karma for my flexible solution. Nah - I skipped answering after reading OP's delightful comments to those who responded to their question over there....
Thanks for the chuckle
Sorry to be that guy, but: * You can simplify the conditions by removing the `&gt;` subclauses. For example, if it gets to `(($file_count&gt;1000 &amp;&amp; $file_count&lt;=2000))`, that means `file_count` was not `&lt;= 1000`, which means it was `&gt; 1000`, so explicitly stating that is redundant. * You don't need the `$` for variables inside an arithmetic context (`(( ))`). * You should put `./` in front of the `*` glob so that files with leading dashes don't give you [unexpected results](https://mywiki.wooledge.org/BashPitfalls#pf3). * `"Done!"` [may or may not work](https://mywiki.wooledge.org/BashPitfalls#echo_.22Hello_World.21.22). file_count=1 for file in ./* do (( file_count++ )) if ((file_count&lt;=1000)) then mv "$file" "dir1/" elif ((file_count&lt;=2000)) then mv "$file" "dir2/" elif ((file_count&lt;=3000)) then mv "$file" "dir3/" else echo 'Done!' fi done 
Wow, doing real live maths in bash huh? Super cool. You tried using go or python to maks your life a little easier?
Every time I try to pick a "real" language to git gud at, I find myself struggling to replicate the GNU/Linux environment control at which Bash excels. It's so bonkers powerful that I'm becoming more and more aggravated by the prevailing philosophy that the shell should be abandoned at the first sign of trouble. Scalability is important, and shell programming is not a performance game, but it's more potent than the rap it gets from people who were educated to approach everything from the perspective of OOP. 
u/Serpens-Caput, there are several important questions that no one's yet asked: 1. Is there a pattern to your filenames? 2. How are the filenames distributed within that pattern? 3. How do you access those files (direct access by filename, all files at one go, etc.)? For instance, if the answers are "there isn't one", "there's no distribution", and "direct access by filename" respectively, then leaving them all in a single directory [may actually be *more* performant](https://medium.com/@hartator/benchmark-deep-directory-structure-vs-flat-directory-structure-to-store-millions-of-files-on-ext4-cac1000ca28) than trying to construct an artificial hierarchy. Ditto if you always "sweep" through your files (e.g. `grep -r ... myfiles/`): The performance gain you get from building a hierarchy will likely be slim. It could even be *negative* if it was a "fast-succeed/fail" op like `grep -qr`; an artificial hierarchy may force you to examine more files on average than a flat namespace. One scenario where you *will* get a performance boost: 1. There's a clear pattern to the filenames. 2. The filenames are (more or less) evenly distributed within that pattern. 3. You access specific files by name, rather than sweeping through your collection. For example, I'm building my own media library, with *millions* of documents, images, audio/video clips, etc. that I collected over the years. I'm looking at indexing them all in an SQLite DB, while renaming the files to their SHA256 checksums and storing them in a hierarchy based on the first byte of that checksum. In bash, it'd look something like this: find "$srcdir" -type f -print0 | xargs -r0 sha256sum | while read sha256 f; do # We'll build a shallow hierarchy based on SHA256 1st byte d="$destdir/${sha256:0:2}" mkdir -p "$d" &amp;&amp; mv "$f" "$d/$sha256" done So when I need to find a particular item, or set of items, I simply execute the appropriate SQL query, get the SHA256 values of those items, then pull them from my hierarchy accordingly. Some content-hosting sites likely use a variant on this algorithm to organize *billions* of user-submitted files for public access.
I'm not familiar with VSCode. Does it work in other editors such as `nano` and `vim`?
I don't know. I've never tried nano :(
Well, it's free.
Problem solved. I edited the file with nano. Thanks :)
nano /path/to/file/filename Or vim /path/to/file/filename 
Regardless of popular opinion, I think bash is a real language and as a system administrator who doesnt always have go or python: I appreciate what you do. It's really nice to have these tools available just with bash ready to deploy anywhere. Cheers!
I appreciate that. Thanks for your feedback.
I find zsh to be useful for small bits math, since it has native floating point support. I wrote a script to wrap bertini (a numerical solver), using zsh floating point arithmetic to detect how far my solutions had moved in between runs.
You said that `&lt;` doesn't work... But that's really the only way I could see to do this. Does this work?: read -res -t 60 -p "Enter password for '$nn': " tempPass &lt; /dev/tty
Bash I awesome, I love using it when I can over other languages
What the heck are you rambling about? It was not homework nor work for me, it was a workplace issue. It was a problem I faced when I wanted to check my personal mp3 files. So, perhaps it is a problem that other people have and maybe now it can be solved. What is exactly your mental issue? I did not have to demonstrate anything to anyone. This is not a college exam. It is a (mostly Q&amp;A) subreddit.
Yeah, you will have to do that thing in your second idea, but you made a mistake there. The following part of your idea will not work: # this won't work! someFunctionThatGivesVariableNumberOfOutputLines | while read -r nn; do tempArray[$kk]="${nn}" ((kk++)) done This will not set those variables for the rest of your script. The right side of a `|` pipe is running in a "sub-shell" in a different process. That different process has its own set of variables and any changes made will not be seen by the main process of your script. You have to swap things around and move your 'while' loop from the right to the left side. You can do that using `... &lt; &lt;( ... )`, like this: while read -r nn; do tempArray[$kk]="${nn}" ((kk++)) done &lt; &lt;( someFunctionThatGivesVariableNumberOfOutputLines ) On that note, you don't have to manage an index yourself. You can add to the array's end like this: while read -r nn; do tempArray+=( "$nn" ) done &lt; &lt;( someFunctionThatGivesVariableNumberOfOutputLines ) There's also a command named `mapfile` in current bash versions that you could use instead of your own loop (it's missing in very old bash versions). It would look like this: mapfile -t tempArray &lt; &lt;( someFunctionThatGivesVariableNumberOfOutputLines ) And another mistake, getting a list of array entries is done with `"${name[@]}"`: for nn in "${tempArray[@]}"; do ...; done If you write just `"${name}"` that would only be the first entry of the array.
Right. Good luck 👍
u/jkool702, the combination of: * "getting run during the boot sequence", * "`mapfile -t array &lt; &lt;(someFunction)` doesnt work", and * "`/bin/bash -c 'mapfile -t array &lt; &lt;(someFunction)'` runs" strongly suggests that your code is being run by a "neutered" shell like `dash`; Ubuntu, for instance, symlinks `/bin/sh` to it. (Incidentally, even `dash` does `&lt;` correctly; you're probably misinterpreting the lack of *process substitution support* a.k.a. `&lt;(someFunction)`.) If you absolutely must use bashisms in your code, you'll have to isolate it in a separate script with a `#!/bin/bash` shebang header, and double-check to make sure that it's being *executed* by the relevant startup scripts, rather than being *sourced* (else the new shebang would make no difference). This means your script now has to pass information back to the caller via some other channel than setting global variables. Options include: * print the necessary info to its stdin for the caller to consume * print shell code like `myVar="new value"` to a temp file, for the caller to source Now, to interactively prompt your user for a password, I'd always redirect input from `/dev/tty`, both to be absolutely sure where it's coming from, and neatly dealing with the issue you faced (multiple conflicting `read`s on the same stdin). Finally, your pipeline runs your `while` loop in a separate subshell from the main code, so any variable settings it does within will disappear when the loop ends. I wrote about this on Reddit recently: see [The `lastpipe` Maneuver](https://www.reddit.com/r/bash/comments/aqg8x9/tip_the_lastpipe_maneuver/) for a simple solution to this, if you really need to preserve that state for later code. Taken together, the following script makes the minimum changes necessary to (hopefully) get your code working: #!/bin/bash # Force the while loop to run in the main shell shopt -s lastpipe someFunctionThatGivesVariableNumberOfOutputLines | while read -r nn; do if [[ $(someCondition "${nn}" ]]; then # Your read can fail (e.g. timeout), so you need to test for that... if read -res -t 60 -p "Enter password for '$nn': " tempPass &lt;/dev/tty; then someUnlockingFunction &lt;&lt;&lt;"$tempPass" else echo "Couldn't get password from user (status code: $?)" &gt;&amp;2 fi else doSomethingElse "${nn}" fi done # Misc. other code...
`x=$(echo "37/7"| bc -l)`
Yeah, but OP wasn't asking to prove rsync (although that is what he is doing) he asked to see if a drive was *duplicated properly*. We all know that rsync will do a good enough job, but won't make it "the same", so I think the answer sticks anyways. OP will not make a perfect copy.
&gt; x=$(echo "37/7"| bc -l) for f in {1..1000} ; do { time x=$(echo "37/7"| bc -l); } 2&gt;&amp;1 ; done | \ sed -n 's;^r.*.\(...\)s;\1;g p' | \ awk '{ total += $1; count++ } END { print total/count }' for f in {1..1000} ; do { time x=$(bc-l &lt;&lt;&lt; 37/7); } 2&gt;&amp;1 ; done | \ sed -n 's;^r.*.\(...\)s;\1;g p' | \ awk '{ total += $1; count++ } END { print total/count }' 
CSV sucks in so many ways. There I said it. If you have control of the CSV file, then you can at least be sure there are no escaped commas or other oddities. I'd seriously consider tab delimited files, but comma can work as well. Also be mindful of global vars that affect bash function, such as IFS. Below I set it to nothing so there is no word splitting on space. ~]$ ~]$ IFS=; for f2 in $(cut -d',' -f2 Reader/sampleData/example1.csv); do echo "Field2 = $f2"; done Field2 = Last Name Field2 = Baker Field2 = Baker Bash old and quirky, but I love it.
I'd be tempted to write it in python. That has CSV parsing included. 
Thanks @bigfig
I am tempted but I have never even looked at python so I thought that this would be faster.... maybe not. &amp;#x200B;
What does this output: echo "$HOSTNAME,$IP,$UPTIME,$VERSION,$UTILIZATION" 
Yeah, the general rule I use is if the script (mostly) controls other scripts, I use bash, if it manipulates data directly, I run to perl or ruby.
\`echo -en "$HOSTNAME, $IP, $UPTIME, $VERSION, $UTILIZATION\\n"\`
&gt; echo "$HOSTNAME,$IP,$UPTIME,$VERSION,$UTILIZATION" RUSL-C18-S12 RUSL-C18-S8 RUSL-C18-S11,10.10.46.52 10.10.46.48 10.10.46.51,183 days 263 days 263 days,8.0.40.0 8.0.30.0 8.0.30.0,10% 10% 9% 
&gt; echo -en "$HOSTNAME, $IP, $UPTIME, $VERSION, $UTILIZATION\n" here's my output: RUSL-C18-S12 RUSL-C18-S8 RUSL-C18-S11, 10.10.46.52 10.10.46.48 10.10.46.51, 183 days 263 days 263 days, 8.0.40.0 8.0.30.0 8.0.30.0, 10% 10% 9% I think because the variables rotate for each find in the grep, i get new lines.
Maybe use python? The standard lib is present on most mint linux boxes. 
For this kind of problems where the input file has mixed text it is best to preprocess the file first. Here you can use `sed` to get the text in the form below $ sed -n 's/^\([.0-9]\+\)\:.*Kernel build id \([.0-9]\+\).*/\1\n\2/p ; &gt; s/.*Hostname \([-A-Z0-9]\+\), uptime is \([0-9]\+\) days.*/\1\n\2/p ; &gt; s/.*Utilization *\([0-9]\+\)\%.*/\1/p ; &gt; /=\+/p' test.txt ============================================================ 10.10.46.52 8.0.40.0 RUSL-C18-S12 183 10 ============================================================ 10.10.46.48 8.0.30.0 RUSL-C18-S8 263 10 ============================================================ 10.10.46.51 8.0.30.0 RUSL-C18-S11 263 9 ============================================================ &amp;#x200B; Then you can use have many options that can parse this file using the line separators. An example `awk` application is shown $ awk 'BEGIN {RS = "=+" ; OFS = ","} ; NR &gt; 1 { print $1,$2,$3,$4,$5 }' test2.txt 10.10.46.52,8.0.40.0,RUSL-C18-S12,183,10 10.10.46.48,8.0.30.0,RUSL-C18-S8,263,10 10.10.46.51,8.0.30.0,RUSL-C18-S11,263,9 The order is not exactly same but you can edit this quite easily. &amp;#x200B;
I don't know python, do you have an example or something i can search? 
Yeah, there's a lot to get through here. Don't put spaces around '=' in your variable assignments: oraclepw = ORACLEPWREPLACE oracleun = ORACLEUNREPLACE Should be: oraclepw=ORACLEPWREPLACE oracleun=ORACLEUNREPLACE Moving on Oracle11s=Serverinfo/oracle11info.csv For Oracle11 in $Oracle11s; do Oracle11Name = $($Oracle11.hostname) Oracle11Service = $($Oracle11.Service) ./Assessor-CLI/Assessor-CLI.sh -b benchmarks/CIS_Oracle_Database_11g_R2_Benchmark_v2.2.0-xccdf.xml -p "Level 1 - RDBMS using Traditional Auditing" -D xccdf_org.cisecurity_value_jdbc.url=jdbc:oracle:thin:$oracleun/$oraclepw@//$Oracle10Name:0000/$oracle10service -D xccdf_org.cisecurity_value_listener.ora={env:ORACLE_HOME}/network/admin/listener.ora -csv -html -ui -u https://0.0.0.0/thing/me.pl done Ok, it looks like you're trying to suck up the csv contents and split it. In `bash`, `for` is lowercase, and you need to explicitly read the file. You'll often see this badly expressed as: for Oracle11 in `cat $Oracle11s`; do A better alternative is to use `bash`'s built in capabilities, drop the backticks because it's not the mid 80's, and in `bash` you should almost always quote your variables, so you might get something like this: for Oracle11 in $(&lt;"${Oracle11s}"); do Even better is to use a `while read` loop: while read -r line; do # do things with "${line}" done &lt; Serverinfo/oracle11info.csv (I've skipped assigning the file to a variable here, it seems kind of pointless in your use case) The upside to a `while read` loop is that you can use `IFS` to assign your variables in one hit. As an example, I'll assign some csv data to a variable, then use `IFS` and `read` to split it to individual named vars: ▓▒░$ var='a,b,c,d' ▓▒░$ IFS=, read -r char1 char2 char3 char4 &lt;&lt;&lt; "${var}" ▓▒░$ printf '%s\n' "char1: ${char1}" "char2: ${char2}" "char3: ${char3}" "char4: ${char4}" char1: a char2: b char3: c char4: d So in the context of a csv file, with columns like "hostname,service,warning threshold,critical threshold" you'd do something like: while IFS=, read -r hostname service warnthres critthres; do # Do things with those vars done &lt; mydata.csv (Note that it's important to have `IFS` inside the `while` loop to contain it there) Now, this isn't exactly dict/list/array splitting like you're probably thinking of, for that you'll need to use `python`. Loosely speaking, `bash` is based on text streams and powershell is more object based. `python` is a better corollary for powershell. But if you're looking at using `python`, you may as well use Ansible. Ansible is seriously the better way to run CIS audits. As a *nix sysadmin who does CIS audits all the time and has debugged both `bash` and `python` CIS auditing scripts, you can trust me on that. If you do go ahead with `bash` for this, after you've updated your code, throw it into shellcheck.net and fix everything that it suggests. Good luck!
I think /u/random_cynic just provided a good solution. My first hunch was awk as well, but I didn't know how to properly transpose. 
Have you already tried printf? Something roughly like this: printf '%s,%s,%s,%s,%s\n' $HOSTNAME $IP $UPTIME $VERSION $UTILIZATION
 ##Break file into variables HOSTNAME=`egrep 'Kernel build id|Hostname|Utilization' $FILE | grep Hostname | cut -d ',' -f 1 | cut -d ' ' -f 3` IP=`egrep 'Kernel build id|Hostname|Utilization' $FILE | grep Utilization | cut -d ':' -f 1` UPTIME=`egrep 'Kernel build id|Hostname|Utilization' $FILE | grep Hostname | cut -d ',' -f 2 | cut -d ' ' -f 4,5` VERSION=`egrep 'Kernel build id|Hostname|Utilization' $FILE | grep Kernel | cut -d ',' -f 1 | cut -d ' ' -f 5` UTILIZATION=`egrep 'Kernel build id|Hostname|Utilization' $FILE | grep Utilization | cut -d ' ' -f 11` Don't use UPPERCASE variables. And this is a good example as to why - `$HOSTNAME` is often set as a default environment variable in `bash`. Don't use backticks. They were superseded in the early 80's. `egrep` is non-standard and deprecated, use `grep -E` instead (often, `egrep` is now setup as an alias to `grep -E`, but `grep -E` is the more portable option) Work your way through your code command by command: ▓▒░$ grep -E 'Kernel build id|Hostname|Utilization' RUSL | grep Hostname | cut -d ',' -f 1 | cut -d ' ' -f 3 RUSL-C18-S12 RUSL-C18-S8 RUSL-C18-S11 So your first variable assignment sets `$HOSTNAME` to those three lines. So when you run `echo $HOSTNAME,$IP,$UPTIME,$VERSION,$UTILIZATION`, that's why your output is behaving the way it is. What you can do as one approach to this is to treat this output as list of devices to work through e.g. ▓▒░$ awk '/Hostname/{print $3}' RUSL RUSL-C18-S12, RUSL-C18-S8, RUSL-C18-S11, (You can use `awk`'s `substr` to strip that trailing comma, I'll show two methods to do this below) So you might get something like this: for netDevice in $(awk '/Hostname/{print $3}' RUSL); do deviceName="${netDevice/,/}" deviceIP=$(awk -F ':' "/${deviceName}/{print \$1; exit}" RUSL) deviceUptime=$(grep "${deviceIP}.*uptime" RUSL | cut -d ',' -f 2 | cut -d ' ' -f 4,5) deviceVersion=$(grep "${deviceIP}: Kernel" RUSL | awk '{print substr($5, 1, length($5)-1)}') deviceUtilization=$(grep "${deviceIP}:.*Utilization" RUSL | awk '{print $3}') printf -- '%s\n' \ "${deviceName:-null},${deviceIP:-null},${deviceUptime:-null},${deviceVersion:-null},${deviceUtilization:-null}" done Which gives output like this: RUSL-C18-S12,10.10.46.52,183 days,8.0.40.0,10% RUSL-C18-S8,10.10.46.48,263 days,8.0.30.0,10% RUSL-C18-S11,10.10.46.51,263 days,8.0.30.0,9% Now, this could definitely be made a bit more efficient (`grep | awk`'s should normally be reduced to single `awk` calls, for example), but I didn't want to crank it up to that level for you. There's enough there for now for you to wrap your head around.
I think this is the best solution, I'm just working through each regex to make sure I understand it. I'll respond again once I figured it out. Thanks very much!
The regex is fairly simplistic here. Feel free to replace these with more robust versions. I have provided a brief explanation below: s/^\([.0-9]\+\)\:.*Kernel build id \([.0-9]\+\).*/\1\n\2/p This command gets the IP using the regex for numbers and periods `^\([.0-9]\+\)\:` and this is ended with a colon. It uses the same regex to get kernel id. These are captured using `sed` capture expression `\(..\)` and used in the replacement where they are placed in different lines using `\n` (this probably won't work with BSD `sed`) s/.*Hostname \([-A-Z0-9]\+\), uptime is \([0-9]\+\) days.* s/.*Utilization *\([0-9]\+\)\%.*/\1/p This gets `hostname` by capturing one or more combinations of hyphens, upper class characters and numbers and uptime using just numbers. The same thing is used to get the utilization percentage part from a trailing `%` sign which needs to be quoted. Finally, s/=\+//p just replaces lines containing multiple = with a blank line which is the line separator here. In awk, `RS` is the record separator which when set to blank character `""` makes awk separate by blank lines instead of newlines. Then the field separator automatically becomes newline. &amp;#x200B;
At a basic level, if you want to manipulate files on a remote server, you're going to need to have access to them somehow. The basic ways to get that access are through a remote shell (ssh / scp) or a networked file share. If we assume that you don't want to (or can't) use NFS or similar, you should use ssh. But that's not too say you "have to ssh all over the place". If you have ssh access to the second server, presumably you could place a script on there too do all the work, and just execute it via ssh once? ssh server2 "/usr/bin/scp_files_back.sh file1 file2" If you don't want to put anything on the remote host, but still want to do it in one command, do: ssh server2 "gzip -c file1 &gt; /tmp/file1.gz &amp;&amp; scp /tmp/fil1.gz server1:/mydir &amp;&amp; rm /tmp/fil1.gz" Both of these assume you have scp set up to work both ways.
that's quite a one liner. I would prefer to wrap it in a function where I do, 1) ssh server2. 2) cd into directory, 3)g-zip on files 1,2,and3, 4)scp onto server and location where script is 5)remove extra gz files. This would be ok right if I wrapped it in a function? I would actually want to move them all to a 3rd server, how would that work exactly? sort of like an edge server, but it wouldn't have the script or the unzipped files until I move them there. I don't want to make another script either.
Then you do the SCP as he showed. SCP works both ways, you can push and pull files. Why are you wanting to set it up in a function? Unless it's part of some bigger bash script just use the oneliner. Doing a function requires you to have it consistently declared, so it could be called again. If that's what you want, then yeah by all means. Then you could even try and implement some parameter handling. In which case you'd just turn it into a regular script in your path that you'd call.
Have you considered using `rsync -z --remove-source-files` instead? This will move the files in compressed form and delete them from the source host in one operation via ssh. The files will be stored uncompressed, but you'll still get the bandwidth savings during the transfer.
Use `grep`. See the manual with `man grep`. IIRC, there's an option to search recursively, another to search through specific lines, and some arguments to match strings as well as exclude others. Sorry I don't have much time rn.
[Gotta be done](https://xkcd.com/327/)
Are you missing an argument to `-n`? shopt -s globstar # enable ** n=12 # or however many lines you want to match for f in dir/**/*; do # match any file at any depth if head -n "$n" "$f" | grep ':' &amp;&amp; head -n "$n" "$f" | grep -q -v '[ @]'; then echo "$f" fi done &gt; matches.txt
u/springs87, generic SQL allows you to escape any embedded quotes in your string literals, by simply doubling them: ``` "INSERT INTO database.table (item) VALUES ('${value//\'/\'\'}')" ``` Note that I've also removed unnecessary double-quotes from your query. That said, if you're doing this on the command line, you might also consider using MySQL's _prepared statements_ facility to do your `INSERT`s. You'll still have to double-up on your single-quotes, but prepared statements avoid issues like SQL injections that can ruin your whole day...[and your DB](https://www.xkcd.com/327/). See [the MySQL docs](https://dev.mysql.com/doc/refman/8.0/en/sql-syntax-prepared-statements.html) on how to use them.
u/Cwesterfield, each of your variables contains multiple items separated by newlines. The easiest way to join them all together into a CSV is with the `paste` command: $ paste -d, &lt;(echo "$hostname") &lt;(echo "$ip") &lt;(echo "$uptime") &lt;(echo "$version") &lt;(echo "$utilization") RUSL-C18-S12,10.10.46.52,183 days,8.0.40.0,10% RUSL-C18-S8,10.10.46.48,263 days,8.0.30.0,10% RUSL-C18-S11,10.10.46.51,263 days,8.0.30.0,9% And if you don't actually need to process any of the variables' contents later in your script, you can dispense with them entirely: $ paste -d, \ &lt;(grep Hostname $file | cut -d ',' -f 1 | cut -d ' ' -f 3) \ &lt;(grep Utilization $file | cut -d ':' -f 1) \ &lt;(grep Hostname $file | cut -d ',' -f 2 | cut -d ' ' -f 4,5) \ &lt;(grep 'Kernel build id' $file | cut -d ',' -f 1 | cut -d ' ' -f 5) \ &lt;(grep Utilization $file | cut -d ' ' -f 11) Note that I've also simplified your pointless double-`grep`s.
This seems to work: path="." n=100 find "$path" -type f -readable | while read -r file; do if head -n"$n" "$file" | grep -q ':' &amp;&amp; ! head -n"$n" "$file" | grep -q -E '[@ ]' then echo "$file" fi done
If you don't care about encryption you can pipe the output of a gzip to netcat in server mode and then use netcat on the client side to connect and pipe the output to gunzip.
This is a one liner. grep -R /path/to/files | grep -v " " | grep -v "@"
Whetu, Wow, this is insightful and honestly exactly what I needed to kick me over the hump. I will put on the headphones and get to finishing this project! you sir are awesome! Thank you kind stranger!
Thanks, that's seems to have done it.
I don’t have access to my laptop right now but I like to use the network inspect feature in chrome, you can copy your get and put requests as curl commands. I use that as a starting point when writing web scraping scripts. 
Idk if you meant: * for **each** line of the first **n** lines of file, * have neither " " nor "**@**" * and have "**:**" &lt;or&gt; * for the **whole chunk** of text, * for all **n** lines of file, * have neither " " nor "**@**" * and have "**:**" I'm not even sure if what i just said made any sense.. but this should find all files that have at least 1 line with ":" but without " " or "@" on it within n lines.. * run from the top dir with all files below * replace **n** in the **NR&lt;=n** part to define number of lines &amp;#x200B; &gt;find ./ -type f -exec awk 'NR&lt;=**n** &amp;&amp; !/\[ @\]/ &amp;&amp; /:/ {print FILENAME}' {} + | sort -u &gt; ../PATH/TO/OUTPUT/FILE &amp;#x200B; If you had some example input/output data, i could do better :/
Do the criteria apply to the 5 lines as a whole? Should this file match (for 3 lines): abc: def Ghj Should this file fail? abc: d f Ghi
Here's what I came up with using awk. This follows the rule: "find every file in the current directory and below where there is a colon (:) anywhere on the first 3 lines and there is neither a space ( ) or an at symbol (@) on any of the first 3 lines" find./ -type f -exec awk 'BEGIN{ foundColon = 1; }{ if ( $0 ~ /[ @]/){ exit 1; }; if ( $0 ~ /:/){ foundColon = 0; }; if ( NR == 3 ){ exit foundColon; }; }' {} \; -print Change the number 3 in `NR == 3` to your target number of lines and let 'er rip I'm on mobile so there might be a typo... 
Why not just cat the file on the remote side and compress it on the local side? You said want to do this, but it doesn't work: gzip -c theweebles@sersver2/local/bin/test/file1 &gt; ${PWD} But you can simply do this: ssh theweebles@sersver2 "cat /local/bin/test/file1" | gzip -c &gt; ${PWD}
Wouldn't it be easier and also more accurate just to do the time once around a subshell? time (for f in {1..1000} ; do x=$(...) ; done)
You're requesting the wrong url, it'll be one of these. https://portal.gdc.cancer.gov/auth/api/v0/graphql/CancerDistributionSsmBarChart https://portal.gdc.cancer.gov/auth/api/v0/graphql/CancerDistributionSsmTable You also need to craft a post request to get the results you want, I suggest putting this through a proxy to view the requests.
I see, could I do this: 1)ssh theweebles@serser2 "cat /local/bin/test/file1" | gzip -c &gt; file1.gz this above one liner ssh's me to server 2 and gzip's file1, so could I do: 2)gzip -c file2 &gt; file2.gz 3)gzip -c file3 &gt; file3.gz I don't have to ssh to server3 here do I? 4)scp file1.gz server2 server3 5)scp file2.gz server2 server3 6)scp file3.gz server2 server3 Cleanup: 7)rm file1.gz; rm file2.gz; rm file3.gz ssh back to server1 This doesn't seem clean though even if it did work
Thanks for the answer. How do I use those two url? They both lead to a "url not found" message. I have tried using the API of their website like this curl "https://api.gdc.cancer.gov/ssms/${SsmID}?pretty=true&amp;fields=${keywords} But I could not find the keywords for the exact things I was looking for ("Project" and "# SSM Affected Cases"). 
Thanks for the answer. I found the text I want [under layers of "&lt;div&gt;"](https://i.imgur.com/lUbfJoz.png). How should I write the curl command to get the text?
May I ask why do you want to gzip the files? Why not simply *scp -C* them to any server you like?
You are accessing a U.S. Government web site which may contain information that must be protected under the U. S. Privacy Act or other sensitive information and is intended for Government authorized use only. Unauthorized attempts to upload information, change information, or use of this web site may result in disciplinary action, civil, and/or criminal penalties. Unauthorized users of this web site should have no expectation of privacy regarding any communications or data processed by this web site. Anyone accessing this web site expressly consents to monitoring of their actions and all communication or data transiting or stored on or related to this web site and is advised that if such monitoring reveals possible evidence of criminal activity, NIH may provide that evidence to law enforcement officials. Please be advised that some features may not work with higher privacy settings, such as disabling cookies. **WARNING**: Data in the GDC is considered provisional as the GDC applies state-of-the art analysis pipelines which evolve over time. Please read the [GDC Data Release Notes](https://docs.gdc.cancer.gov/Data/Release_Notes/Data_Release_Notes) prior to accessing this web site as the Release Notes provide details about data updates, known issues and workarounds. Contact [GDC Support](https://gdc.cancer.gov/support#gdc-help-desk) for more information.
Thanks for the answer. Press "Accept" and you can get past that.
Then what? Break the Law?
How exactly am I breaking the law?
Upvotes for rsync! I don't know what I would do without this program. Waste time, probably.
adding nohup works
If you want to `time` the `for` loop as well.
They are really large files, a few gb in size
Good luck in what? Good luck to you also, nonetheless.
Remove that one `=` character you have on the `for` line.
Here's the thing though: `time` doesn't have very much accuracy (somewhere between 0.01 and 0.1 seconds). The longer something runs, the more accurate the reading. Setting a variable is just noise, so it won't provide a meaningful result to add up the measurements of a whole bunch of very tiny`time` results. Also, the overhead of the `for` loop here is negligible — on the order of a ten-thousandth of second probably in total time.
Maybe /r/linuxmasterrace would like this glorious RGB in the terminal.
scp itself can use compression without invoking any other tools. See Compression at https://linux.die.net/man/5/ssh_config
I'd go with `awk` for this one: #!/usr/bin/env -S awk -f 1 { print $0 } /switchport mode access/ { print "switchport port-security maximum 3" print "switchport port-security violation protect" print "ip arp inspection limit rate 100" print "auto qos voip cisco-phone" print "storm-control broadcast level bps 4m 3m" print "storm-control multicast level bps 5m 3m" print "no shutdown" } Run with `./foo.awk file.txt`
When a blade doe not respond, I get two lines of ='s. This makes the sed mad, is there a command I should run first to get rid of double = lines? 10.10.12.145: Manufactured by OCCM on 9/10/2008 10.10.12.145: 10.10.12.145: PLD revision 0.2 10.10.12.145: FPGA revision FPGA_0: 000.006 10.10.12.145: 10.10.12.145: Chassis B6-012 10.10.12.145: Hostname MLST-C1-S5, uptime is 199 days, 5:24, load average 4.21, 4.93, 5.03 10.10.12.145: MLST-C1-S5#show bridge summary^M 10.10.12.145: Bridge Summary 10.10.12.145: Total MACs 1291 10.10.12.145: Macs Avail. 16384 10.10.12.145: Utilization 7% 10.10.12.145: MLST-C1-S5# 10.10.12.145: ============================================================ ============================================================ 10.10.12.152: 10.10.12.152: MLST-C1-S12&gt;^M 10.10.12.152: MLST-C1-S12#show version^M 10.10.12.152: B6 System 8.0 for B6-256 [shelf:b17, slot:12] 10.10.12.152: Kernel build id 8.0.30.0, made Thu Aug 27 18:10:01 CST 2015 10.10.12.152: Processor: 405EX Rev. D at 533.333333 MHz, revision 20.115 (pvr 1291 1473) 10.10.12.152: 10.10.12.152: Board assembly number is 400-00394, revision number is 22, type is 1 10.10.12.152: Top-level assembly is 0320710 10.10.12.152: Serial number is 211604800353, MAC address is 00:02:86:30:65:12 10.10.12.152: Manufactured by Calix on 04-16-2016 10.10.12.152: 10.10.12.152: PLD revision 1.2 10.10.12.152: FPGA revision FPGA_0: 0.9 10.10.12.152: 10.10.12.152: Chassis B6-012 10.10.12.152: Hostname MLST-C1-S12, uptime is 5:05, load average 3.38, 3.39, 3.27 10.10.12.152: MLST-C1-S12#show bridge summary^M 10.10.12.152: Bridge Summary 10.10.12.152: Total MACs 1776 10.10.12.152: Macs Avail. 32767 10.10.12.152: Utilization 5% 10.10.12.152: MLST-C1-S12# 10.10.12.152: ============================================================ 
I'd do it like this: sed '/^ *switchport mode access *$/ a\ switchport port-security maximum 3\ switchport port-security violation protect\ ip arp inspection limit rate 100\ auto qos voip cisco-phone\ storm-control broadcast level bps 4m 3m\ storm-control multicast level bps 5m 3m\ no shutdown ' testfile If you want to edit the file instead of just print it, change the command from `sed` to `sed -i`.
Not entirely sure. Something something expansion. Try echo ${HOSTS}. Also it's supposedly not so good to create an array with cat. There are better ways to read into an array. Check Google.
If you precede the block of text with four spaces, Reddit will format it as a code block. Reddit's "Fancy Pants" editor should make it easier.
The issue you’re getting isn’t on the echo line. It’s when you assign host. You’ve got a command substitution going on (`$()`) when you dump the Mac addresses which is correct(ish), but then you’re passing the result of that into `$(())` which tries to treat the contents as an arithmetic expression and assign the result to the `HOST` variable. What you probably want to do is just wrap the command substitution in a single set of parens: HOST=( $( cat /home/stamour547/storage/wireless/mac_inventory ) ) Bash will use word splitting and create an array of strings split on white space, which is what I think you expect. Depending on the format of that file (if it’s one item per line) you can use the mapfile function: mapfile -t HOST &lt; &lt;( cat /home/stamour547/storage/wireless/mac_inventory ) Which will be a little more predictable. 
You have to use a multiline version of `sed` command. To only remove duplicate lines containing `=` use: sed '$!N; /^\(=\+\)\n\1$/!P; D' &lt;file&gt; The `$!N` command appends a newline to pattern space for all lines except the last. The next command searches for two consecutive lines that has one or more = and replaces them with a single line. Finally `D` deletes and starts a new line. You can also use an awk command like that does the same thing: awk '{if ($0 ~ /=+/ &amp;&amp; !a[$0]++) next; else print}' &lt;file&gt; &amp;#x200B;
excellent! Unfortunately, that was not the thing making it mad. I have some swithec that have less than one day of uptime, and thats whats breaking it. &gt; uptime is 5:05 Debug to the max! :) Working on that now.
That's what *-C* is for, every file is compressed on the fly at source before transfer and decompressed at destination. No need for gzip. Try it and you'll notice that the transfer speed is significantly greater as without it.
Since the output of the uptime can vary in format your best bet is to **capture everything** between `uptime` and `load average` like below and use `awk` to output the whole field within double quotes. Then you have to make sure that your downstream program can process that time stamp in a flexible way when it processes the csv. Here is the modified program: $ sed -n 's/^\([.0-9]\+\)\:.*Kernel build id \([.0-9]\+\).*/\1\n\2/p ; &gt; s/.*Hostname \([-A-Z0-9]\+\), uptime is \([ :,a-z0-9]\+\), load average.*/\1\n\2/p ; &gt; s/.*Utilization *\([0-9]\+\)\%.*/\1/p ; &gt; s/=\+//p ' test.txt &gt; test4.txt $ cat test4.txt 10.10.46.52 8.0.40.0 RUSL-C18-S12 183 days, 10:56 10 10.10.46.48 8.0.30.0 RUSL-C18-S8 263 days, 2:18 10 10.10.46.51 8.0.30.0 RUSL-C18-S11 263 days, 37 min 9 The `awk` command changes a little bit to be explicit about newline as separator and include double quotes for the csv output. $ awk 'BEGIN {RS = "" ; FS = "\n"; OFS = ","} ; { print $3,$1,"\""$4"\"",$2,$5 }' test4.txt RUSL-C18-S12,10.10.46.52,"183 days, 10:56",8.0.40.0,10 RUSL-C18-S8,10.10.46.48,"263 days, 2:18",8.0.30.0,10 RUSL-C18-S11,10.10.46.51,"263 days, 37 min",8.0.30.0,9 &amp;#x200B;
If your target text is in `target.txt` and text to be inserted is in file `insert.txt` you can simply use `awk` as $ awk '/switchport mode access/ { print $0 ; while(getline &lt; "insert.txt") {print}}1' target.txt Another option is to use the `ed` editor (can also do with `sed` but it is more complicated to insert a line below a pattern). $ ed -s target.txt &lt;&lt;EOF &gt; /switchport mode access/-r insert.txt &gt; ,p &gt; q &gt; EOF &amp;#x200B;
If your target file is `target.txt` and the text to be inserted is in `insert.txt` then you can use awk as: $ awk '// ; /switchport mode access/ {while(getline &lt; "insert.txt"){print} ; close("insert.txt")}' target.txt The `ed` editor can also do the job: $ ed -s target.txt &lt;&lt;EOF &gt; g/switchport mode access/+-r insert.txt &gt; ,p &gt; q &gt; EOF &amp;#x200B;
I got it all to work out the way I like it. There one entry that does not get the utilization, but I have no idea why. It's just one so I'm not going to worry about it too much. Thanks very much! I learned a bunch. 
UwU what's this? `$!/usr/bin/env -S`
You can use the following method if you want to actually declare an array and add to it by reading from another file line by line. &amp;#x200B; Assuming the filename is `list.txt` and is in the same directory as the script: &amp;#x200B; #!/usr/bin/env bash # declaring and initializing variables file=list.txt slot=0 # declare array (completely optional but I did this for clarity) declare -a macaddress # populating array from file while read line; do macaddress[$slot]=$line ((slot++)) done &lt; $file # looping and echoing through array to check values were added for i in ${macaddress[@]}; do echo $i done &amp;#x200B;
Why all the hard work, guys? mapfile HOST &lt; /home/stamour547/storage/wireless/mac_inventory
whoops i switched a "+" with a "\\;". (was trying to make it faster and couldn't quite remember the difference between the two &gt;.&gt;) &amp;#x200B; it should work now, but it doesn't check that ALL lines match. Only if ANY of the **n** lines match Idk if the picture is going to be helpful, or just more confusing.. but the top 2 boxes are the test files, the bottom box is the command being ran and then the cat'd out output file for the command &amp;#x200B; &amp;#x200B;
Where `firstfile` is the text from your existing configuration, and `secondfile` is going to be the new configuration. while read line; do echo -e "$line" &gt;&gt; secondfile if [[ "$line" =~ "switchport mode access" ]] then echo -e "switchport port-security maximum 3" &gt;&gt; secondfile echo -e "switchport port-security violation protect" &gt;&gt; secondfile echo -e "ip arp inspection limit rate 100" &gt;&gt; secondfile echo -e "auto qos voip cisco-phone" &gt;&gt; secondfile echo -e "storm-control broadcast level bps 4m 3m" &gt;&gt; secondfile echo -e "storm-control multicast level bps 5m 3m" &gt;&gt; secondfile echo -e "no shutdown" &gt;&gt; secondfile fi done &lt; firstfile Like others have said, `sed` and `awk` are cleaner ways to do this, but this *is* a bash forum. Tested briefly, returns output like the following: interface g3/0/44 description STAFF switchport access vlan 308 switchport mode access switchport port-security maximum 3 switchport port-security violation protect ip arp inspection limit rate 100 auto qos voip cisco-phone storm-control broadcast level bps 4m 3m storm-control multicast level bps 5m 3m no shutdown interface g3/0/45 description STAFF switchport access vlan 308 switchport mode access switchport port-security maximum 3 switchport port-security violation protect ip arp inspection limit rate 100 auto qos voip cisco-phone storm-control broadcast level bps 4m 3m storm-control multicast level bps 5m 3m no shutdown interface g3/0/46 description II/156 STAFF switchport access vlan 308 switchport mode access switchport port-security maximum 3 switchport port-security violation protect ip arp inspection limit rate 100 auto qos voip cisco-phone storm-control broadcast level bps 4m 3m storm-control multicast level bps 5m 3m no shutdown interface g3/0/47 description STAFF switchport access vlan 308 switchport mode access switchport port-security maximum 3 switchport port-security violation protect ip arp inspection limit rate 100 auto qos voip cisco-phone storm-control broadcast level bps 4m 3m storm-control multicast level bps 5m 3m no shutdown
First time I've seen `mapfile`. Thanks. Hopefully I'll remember it long enough to use it.
From my quick reference: -C callback command -c callback count (0) -d delimiter (newline) -n count (0) -O origin (0) -s discard (0) -t no trailing delimiter -u file descriptor (0) 
/r/unixgore 
Thank you. Just out of curiosity, why do you recommend camelCase for variables? I've moved to lowercase whole words separated by underscores for readability.
 $ grep Name docs/style variable_name ArrayName ENV_NAME functionName filename.ext 
If I'm reading this right, it's a method you use in order to make locating variables easier?
Right. This way I can infer type by name alone.
The problem you're running in to is that GNU/Linux and other UNIX like or compatible environments are not Windows. WSL is Microsoft's stab at providing a UNIX like environment. I would recommend learning how to use `vim` or `emacs`. Both are way more powerful than `nano`, and will give you a better idea about the UNIX philosophy that Microsoft's WSL so poorly communicates. 
Got it. Thanks for entertaining me.
 trap "echo HUP&gt;&gt;/tmp/err" SIGHUP trap "echo QUIT&gt;&gt;/tmp/err" SIGQUIT trap "echo INT&gt;&gt;/tmp/err" SIGINT trap "echo TERM &gt;&gt;/tmp/err" SIGTERM Reveals a SIGINT being sent on exit. trap solves the issue.
You can avoid all that `echo` and redundant disk operation: while read line; do printf '%s\n' "$line" [[ $line =~ "switchport mode" ]] &amp;&amp; cat &lt;&lt;-EOF switchport port-security maximum 3 switchport port-security violation protect ip arp inspection limit rate 100 auto qos voip cisco-phone storm-control broadcast level bps 4m 3m storm-control multicast level bps 5m 3m no shutdown EOF done &lt;input.txt &gt;output.txt Note the dash in the heredoc `&lt;&lt;-EOF` will ignore leading tabs but not spaces. In my file, there are two tabs and one space leading every inserted config line, preserving the single space indentation OP wants.
I've never heard of camelcase/PascalCase. I have always used all caps to make it easier to quickly find a variable in a script. I guess I should break that habit. &amp;#x200B;
Thank you. I'll try that and see if it works for me. 
[removed]
That works great. Much appreciated.
Thanks! Useful stuff. I didn't preserve leading spaces, relying on the switch's configuration mode to insert the leading white space. I guess that was an assumption on my part.
So I ultimately ended up using `systemd-ask-password` instead, though what you suggest wouldnt have worked. The issue, as /u/anthropoid guessed, seems to be that the commands are either being run through a limited bash shell or perhaps not using bash at all. The act of using a `&lt;` in the command line throws an error, much like a randomly/improperly placed `!` or `$` would. In whatever shell this actually, is using, `&lt;` simply doesnt seem to mean "input from file". encompassing everything (either a set of commands or the command to run/source the script) inside of `/bin/bash -c '&lt;...&gt;'` does make this work, however.
took me a while but done.
Appreciate your taking the time to respond. &gt; This will not set those variables for the rest of your script. The right side of a | pipe is running in a "sub-shell" in a different process. SO, at the time i posted I didnt realize this, though i sort of figured this out (or suspected it at any rate) bu trial-and-error as ive gone through troubleshooting on various parts of the code. side note: Id always sort of wondered what the practical differences between `A | B` and `B &lt; &lt;(A)` were (other than "one uses a pipe, one uses a temp-file as an intermediary"), and it would seem the answer is "one runs `B` in a subshell, one doesnt. &gt; `... &lt; &lt;( ... )` problem with this is `&lt;` doesnt work without being wrapped in a `/bin/bash -c` call, since the shell in the initrd doesnt seem to use bash by default (even though it says its shell is bash). I later figured out I could mostly get around this by wrapping the actual function call rather than parts of the script - i.e., run `/bin/bash -c 'script.sh'` or `/bin/bash -c 'source script.sh &amp;&amp; function ${args}'`. Before figuring this out though, I resorted to relying on word splitting and doing something like tempArray=($(someFunctionThatGivesVariableNumberOfOutputLines | while read -r nn; do [[ $(cond1 "${nn}") == "true" ]] &amp;&amp; echo "$(modifier1 "${nn}")" || \ [[ $(cond2 "${nn}") == "true" ]] &amp;&amp; echo "$(modifier2 "${nn}")" || \ [[ $(cond3 "${nn}") == "true" ]] &amp;&amp; echo "$(modifier3 "${nn}")" done)) for nn in "${tempArray[@]}"; do ... Im 99% sure my array variables will always be space separated, so, despite being not quite as robust, it should be fine. &gt; `for nn in "${tempArray[@]}"; do ...; done` This i actually did know. just forgot to put it in when i typed up the sample code.
Take a look at this [Stack Overflow](https://stackoverflow.com/questions/18128573/bash-if-time-between-8am-and-1pm-do-esle-do-specifying-time-variables-an) question, hopefully it helps
Please be more specific about what you are trying to do. Is the subject directory the place where the files you want to copy live or the place where you are trying to copy files?
Perhaps like this: weekday=$(date +%w) # day of week (0..6); 0 is Sunday time=$(date +%H%M) # hour (00..23), minute (00..59) # IMPORTANT: do not type "0900", because with that zero # at the beginning, bash thinks this is "octal" and not a # normal decimal number. You either get the wrong result # or an error message. if (( weekday &gt;= 1 &amp;&amp; weekday &lt;= 4 )); then echo "monday through thursday" if (( time &gt;= 900 &amp;&amp; time &lt;= 1800 )); then echo "do something here" fi fi if (( weekday == 5 )); then echo "it is friday" if (( time &gt;= 800 &amp;&amp; time &lt;= 1500 )); then echo "do something here" fi fi if (( weekday == 0 || weekday == 6 )); then echo "it's the weekend" fi 
 isopen() { if [ $1 -gt 5 ] ; then echo logic 2 ; else if [ $1 -lt 5 ] ; then if [ $2 -gt 8 ] &amp;&amp; [ $2 -lt 18 ] ; then echo logic 1 ; fi ; else if [ $2 -gt 7 ] &amp;&amp; [ $2 -lt 15 ] ; then echo logic 1 ; fi ; fi ; fi ; } ; isopen $(date +%u" "%H) but good luck explaining that to your teacher if it's homework.... 
Setup crontab entries 
Don't need to cd to directories, cp /path/to/file /path/to/directory. It depends what files you want though
I know you said you want to use if else, but a switch statement was made for this kind of stuff
This is the kind of stuff I'm happy to come across.. Brilliant..
You can use `read` with a timeout, like `IFS=" " read -t 0.001 -d "" -ra stdin`.
`[[ ! -t 0 ]]` checks if file descriptor 0 (stdin) is not a terminal. I regularly use the following construct to collect objects from arguments and stdin. Flag processing is a whole other game. Obj=("$@"); [[ ! -t 0 ]] &amp;&amp; while read -a Line; do Obj+=("${Line[@]}")
You don't have to be in a directory to do stuff there. For example, the following will iterate over a list of paths, check if those paths lead to "regular files", then copy them to your home directory. for path in /foo/bar/file /foo/baz/file2 "/path/with spaces"; do [[ -f $path ]] &amp;&amp; cp "$path" "$HOME/${path##*/}"; done * `for ...; do ...; done` iterates over a one-line list * `[[ -f ]]` tests for regular files (not directories, links, etc.) * `&amp;&amp;` does the thing on the right if the thing on the left is true * `${path##*/}` removes everything to the left of the last forward slash, leaving the filename.
Your code, as is, does nothing, because you need to create and declare the isopen function which on top of that in one call you don't pass parameters and another you do. Might want to look in the mirror 
For the weekday, a switch could work but the hours still have to use a if I think Good suggestion through
Very nicely typed and understandable thanks. Also thanks for the protip of not adding a extra 0. Did not know that.
&gt; /u/ta4bash [S] 1 point 16 minutes ago &gt; Your code, as is, does nothing, because you need to create and declare the isopen function &gt; which on top of that in one call you don't pass parameters and another you do. &gt; Might want to look in the mirror It might not do anything because outside the hours of 9am to 6pm on a Monday to Thursday, between 8am and 3pm on a Friday, and all day on both Saturday and Sunday, by definition it is supposed to do nothing. And it is not yet 9am on a Thursday here
tyvm. (I was chuckling when I realised I could use one `if..else` as per OP's question)
Isopen() does nothing because it isn't declared anywhere. Try harder.
&gt; Isopen() does nothing because it isn't declared anywhere. &gt; &gt; Try harder. Erm, the whole of the line before the last semicolon *is* the definition....
&gt;EDIT: Damn it I for about I need special dates too... &gt; &gt; specialdates=("2019-03-07" "2019-03-08") &gt; todayisspecialdate=0 &gt; currentdate=$(date) &gt; for date in "${dates[@]}" &gt; do &gt; if ($date -in $specialdates) &gt; $todayisspecialdate=1 &gt; break &gt; fi &gt; done &gt; &gt; &gt;Would that work? Err, no.
No, you are not stupid.
Seems like it when it comes to scripting, etc. 
That's quite a brief `Cheat-sheet`.... was expecting something a bit bigger for a `gnu` program.... wonder how many options there really are... &lt;quick search&gt; [omigawd](https://www.gnu.org/software/parallel/man.html) &gt; #EXIT STATUS &gt; &gt; ... &gt; ... &gt; 101 &gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; More than 100 jobs failed. 
Haha!
I played with Parallel a bit a few years ago. I was wondering if anyone here would share what they have found to be their best use cases for it and maybe some examples of how they implemented it. I feel like I never used it to it's full potential.
Does look well handy for batching up jobs and managing resources while doing jobs that you don't want to have a noticeable effect on the system while they are going on (e.g. triggering the cooling fan speed I term as 'lift off')
You might start by scrolling through the examples in the man page: https://www.gnu.org/software/parallel/man.html#EXAMPLE:-Working-as-xargs--n1.-Argument-appending
You can also use it for po' man's distributed parallel processing. Nifty if, for instance, you have a bunch of boxes and want utilize the resources to re-encode large (or lots) of video files.
Aye - I had gone back to the bit that said STOP! after my curiosity was piqued re: just *how short* the scroll bar at the side of my screen was.... ... and noticed that OPs ID was mentioned in a `lulu` URL... 
&gt; (Incidentally, even dash does &lt; correctly; you're probably misinterpreting the lack of process substitution support a.k.a. &lt;(someFunction).) I went back and checked, and you were dead on the money with this. It was throwing a `syntax error at \`&gt;'` error and I had assumed it meant the 1st `&lt;` (or both of them, but i think the 1st throwing an error means the 2nd never gets executed). If i change the `&lt; &lt;(...)` to a `&lt; $(file)` it doesnt throw a syntax error though. This makes me wonder if, alternately, something like this would work [[ -d /tmp ]] || mkdir /tmp [[ -n "$(cat /proc/self/mounts | grep '/tmp ')" ]] || mount -t tmpfs tmpfs /tmp exec 3&lt;&gt;/tmp/in.tmp someFunction $args &gt;&amp;3 mapfile -t someArray &lt;&amp;3 &gt; This means your script now has to pass information back to the caller via some other channel than setting global variables...print the necessary info to its stdin for the caller to consume This is what i ended up doing. It uses word splitting, but im 99% sure the array elements will always be space-separated anyways, so this should be OK (the array elements are ZFS pool/dataset names) For the actual password prompt part, i ended up using `systemd-ask-pass` (or, well, the native zfs encryption key loading functions, but this is just a lightly wrapped call to `systemd-ask-pass`). Im troubleshooting rather complicated startup sequence (UEFI stub loader boot --&gt; encrypted LUKS --&gt; encrypted ZFS on root, with 1 encryption key provided via user password prompt and the other provided by the systems TPM2 using `clevis`) . I had briefly thought the issue might be related to the password input, but this isnt it. Its still poorly implemented - in particular it doesnt check if the encryption key was loaded using alternate means, so if the keys already loaded and it runs it will always error out with a "the key is already loaded" error (even if you input a valid password), which fails `zfs-mount.service`, which (via failed dependency) fails `sysroot.mount`, which fails basically everything else and throws the boot sequence into an emergency shell. &gt; lastpipe Ill have to look into this more, it looks useful. In this case I can get away with echoing the output out of the subshell since word splitting is OK, but `lastpipe` seems like the better option. side notes: I figured out a few alternate solutions too. if you call the function like `/bin/bash -c 'script.sh'` or `/bin/bash -c 'source script.sh &amp;&amp; function'` then everything int he script gets executed by a full-featured bash shell and works as expected. Alternately, you can ad the `/bin.bash -c` call directly into the function doing something like this (ps wrote this in the reddit prompt, so its not tested) wrapFunctionInBash() { # get function content and save in variable funC="$(declare -f "${*}")" # strip off name funC="$(echo "${funC}" | grep -E '^.+$')" funCName="$(echo "${funC}" | head -n 1)" funC="$(echo "${funC}" | tail -n -1)" # transform `local` into `declare` funC="${funC//'local '/'declare '}" # escape single quotes ( ' --&gt; '"'"' ) funC="${funC/"'"/"'"\""'"\""'"}" # add /bin/bash -c and add in function name funC="$(cat &lt;(echo -e "${funCName} \n") &lt;(echo '/bin/bash -c '"'"'') &lt;(echo -e "\n${funC} \n'"))" # export echo "${funC}" | tee "${funCName%%(*}_bash" }
You still a ass? Err, yes.
This sounds suspiciously like a homework assignment. Also, you probably don't mean plain text or plan text, you probably mean "characters from the extended ASCII set". Having said that, converting the zip to base64 is probably your best bet.
It's not a homework I finish university a few years ago. Yes I mean " extended ASCII set" I'll see if I can do it. Thanks
Use od to convert it to, say, octal and send that to your corresspondent.
zcat file.zip or cat file.zip | gunzip could bei useful. I dont understand exactly what youbare trying to doch though... 
Iteresting option use od to do it but first I 'll try to use base32 
On second thoughts, use xxd # xxd infile.zip &gt; outfile.xxd # xxd -r outfile,xxd infile-2.zip
&gt; This sounds suspiciously like a homework assignment. What is up with this sub and homework assignments? 1: Noone teaches bash lol 2: /r/Powershell does not have this attitude at all. GTFO if you dont want to lend a hand to OP.
&gt; I 'll try to use base32 [`man base32`](http://man7.org/linux/man-pages/man1/base32.1.html)
This is what `uuencode` was built for, although Wiki says it's largely been replaced with MIME which uses base64 encoding, so why not try that? You say you'll try base32, maybe that will work just as well.
Except that he did help. base64 is the best answer.
&gt;GTFO if you dont want to lend a hand to OP. Did you not see the rest of my comment?
Parallel would completely change my game if I had access to it at work, but it's not included by default in any distro I'm aware of, and I don't have the pull to install it across the servers I work on. 
Well if its homework you don't learn anything by having people tell you how to do it. But some people learn by doing their own research others learn by being told. So whatevs.
This is the most universal solution; on some systems base64 or b64encode aren’t part of the base system. `xxd` always is, like `strings`. Unless Windows, but then I assume none of those are preinstalled anyway ¯\_(ツ)_/¯ 
Note that there's at least one other `parallel`s in the wild that does `xargs`-like things: Tollef Fog Heen's contribution to Joey Hess' [moreutils](http://joeyh.name/code/moreutils/) package, which is available in many Linux distros and possibly in other Unix-like OSes too. It's basically "`xargs` with system load awareness", and may be installed by default in certain distros, since it comes with many other useful utilities in the same package. GNU parallel, in contrast, seems to be an optional install in most cases. So if your `parallel` doesn't quite work the way you expected, check to see what you're actually running (`parallel --help` should tell you in no uncertain terms). On Debian/Ubuntu, both packages can coexist, but the "lesser" `parallel` gets renamed to `parallel.moreutils`.
getopts requires a -whatever or it acts weird so &gt;v p 1111111 #is wrong
I like it :)
if you don't reset OPTIND you will get very weird behavior http://pix.toile-libre.org/upload/original/1552023698.png
TIL, thank you.
u/veekm, while your ongoing documentation of `getopts`\-discovery is mildly entertaining, you're probably better served actually reading a tutorial on how to use it, like [this one](https://sookocheff.com/post/bash/parsing-bash-script-arguments-with-shopts/) by Kevin Sookocheff. Also, as the StackOverflow link documents, you're much better off declaring `OPTIND` (and the other variables as well) as `local`, instead of `unset`ing them, so as not to accidentally reset any `getopts` parsing that you're doing at the toplevel: local myarg OPTIND OPTARG
Any screenshots available to see it in action?
It's nothing unique; think `dmenu` in the terminal. But sure, I'll take one.
you should put a couple of example pipe commands in the readme
Nevermind. I'm an idiot. Solved with an escape character.
Where are you setting `host`?
Not sure if that needs to be a global. [https://stackoverflow.com/questions/50441717/nginx-proxy-pass-use-proxy-host-as-the-request-header-host](https://stackoverflow.com/questions/50441717/nginx-proxy-pass-use-proxy-host-as-the-request-header-host) "Now keep in mind that `$host` is specifically the first `server_name` that is defined in the current server block." 
Seems like `smenu`. Will give it a try asap.
I don't think xxd is standard anywhere where vim isn't the default vi.
*Wooo* It's your **6th Cakeday** Dylan112! ^(hug)
I've never heard of `dmenu`. What would be an example use case for this or `dmenu`?
I've seen your screencast and read your README, but can you explain what's the used cases ? &amp;#x200B;
Check out "dateutils" if you dont mind downloading something.
It seems nice &amp; useful, but I can’t imagine a real world use case for it. Can you give an example of the kind that compelled you to build this? I saw the asciicast but I would imagine picking a number from a seq-generated stream wasn’t one of those use cases :P
I was simply inspired by `dmenu` to make this. This isn't as useful, as it's already run from a terminal, but it can be used for some random things, one of them actually being that; you could bind a key to a script that brings it up with numbers from 1 to 100 and sets the volume to the output number. And one advantage of it is that since it's written in Bash you can easily include a modified version of it in your script, for example, one that sets the initial selection to the current volume. You can find a lot of possible use cases here: https://bbs.archlinux.org/viewtopic.php?id=80145.
You would use sed or perl to alter the stream. cat sar-file | grep -e "00:00:01\|Average" | perl -pe 's/^/\n/'
u/notthatbigbrother, taking your requirements *literally* (adding a newline before `00:00:01` *but not* `Average`, and *wherever it appears on the line*, not just at the beginning), `sed` can do it all at one go: sed -n '/00:00:01\|Average/{s/\(00:00:01\)/\n\1/;p}' sar-file In order, `sed` will: * `-n` \- *not* output lines by default * `/00:00:01\|Average/` \- capture lines that match that regex * `s/\(00:00:01\)/\n\1/` \- add a newline before `00:00:01`, if present * `p` \- print the (possibly modified) capture
This worked like a charm (and was very easy to integrate into my codes), Much appreciated.
Pipe to sed replacing ^ with \n. ^ is line start i.e. add | sed 's,^,\n,g'
Nice and elegant ! Now I'm ashamed that I made a python script to do this. I will not even post it after your answer. Also OP there is no need to pipe a cat to a grep, that's called an useless cat and there is always some rude dude that will point it to you ! (sorry) &amp;#x200B; FYI and amusement: [http://porkmail.org/era/unix/award.html](http://porkmail.org/era/unix/award.html) &amp;#x200B;
This is exactly what I needed. Thank you! There is a lot to digest/learn from this command you wrote. Thanks for taking the time to help!
Thanks for the insight on the cat command being useless. A little new to working with bash. There is still a lot left for me to learn.
&gt; * s/\(00:00:01\)/\n\1/ - add a newline before 00:00:01, if present You can simplify this to `s/00:00:01/\n&amp;/`. `&amp;` in the replacement pattern will be replaced with the string that was matched.
Thanks for the follow-up. It means a lot.
Your comment helps nothing. 0. At best a suggestion...
BTW, thats a function, not a script.
Got everything to work :) Special thanks to /u/ropid I would love to post the code for others to enjoy but sadly some users act like complete asses and want to stroke their ego and not help out. So, if anyone wants the code, you can PM me and Ill send it. If you dont need special dates, /u/ropid basically has the code
if you are planning to extract and analyze logs often Perl is a better choice than Python or anything else really
I've added a [patch](https://github.com/Crestwave/shmenu/blob/master/patches/volume.patch) for an example script; does that help?
People do teach bash in class. At university for a BSCS we had a required second year data structures course where we were taught C and bash on linux systems.
Alternatively, [quote your initial heredoc marker](https://mywiki.wooledge.org/HereDocument) so that substitutions won't occur. cat &gt; /home/user/output.conf &lt;&lt;'EOL' ... location / { proxy_pass_header Authorization; proxy_pass http://$upstream_ipport; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; proxy_set_header Connection “”; proxy_buffering off; client_max_body_size 0; proxy_read_timeout 36000s; proxy_redirect off; } ... EOL
The shell doesn't do tilde expansion inside quotes, and it doesn't get recursively expanded from an unquoted variable expansion, so you get a literal `~`. Tilde expansion _does_ occur after the first `=` in a variable assignment if unquoted, so `blah=~/blah` will work, but you're better off using `$HOME` because it's more predictable. [Tilde expansion on the Bash Hackers Wiki.](https://wiki.bash-hackers.org/syntax/expansion/tilde)
‘~’ is a part of shell expansion so it only works when unquoted. 
There's a bunch of strange behavior with `~`. It's not behaving like a normal bash variable. You should always use `$HOME` in scripts and such to be safe against running into one of the special rules of `~`. In the example you showed here, you used `'` quotes. Those quotes will make bash also not touch a `$HOME` text, it would also not be translated into your actual home when you set the variable. You need to use `"` quotes. You should also use `"` quotes when you later access the variable and run `touch "$blah"` because then your script will be able to have space characters in file names. Now about `~`, it's a special thing and bash is not using the same text replacement rules that it uses for variables. The `~` will not work inside `"` quotes. Try for example these command lines here: echo ~ echo "~" echo "$HOME" You'll see the `"~"` line won't work but the other two will. The following will work if you want to assign something that needs quotes to a variable: $ blah=~/'some file name with spaces' $ echo "$blah" /home/ropid/some file name with spaces This here worked because of bash using special rules when it see the `~` text next to other characters. There's more rules about `~`'s behavior that depend on what characters exactly are next to it. It might disable itself or start behaving differently. If there's characters behind it, it will look for the home of another user name, for example: $ echo ~mail /var/spool/mail $ echo ~root /root This doesn't happen for all characters, for example `:` still works: $ echo ~:hello /home/ropid:hello $ echo ~: /home/ropid: But touching `~` with `:` from the front does not work: $ echo hello:~ hello:~ $ echo :~ :~ Now check out what happens with a `=` character: $ echo ~= ~= $ echo =~ =~ $ echo hello=~ hello=/home/deep There seems to be a special behavior to make it possible to assign a variable but it's not the `=` alone triggering it, unlike with the `:` character. I'd say this whole behavior makes no sense, it would be best to just ignore it and not try to learn it. Use `$HOME` instead.
General guidline: `~` expansion is fine for getting to your home directory quickly when hacking away at the command line. It has no business being in scripts or applications.
You can always use eval eval echo $blah
Not really an option as I do need to replace some of the vars in that block, like \`$domain\` and \`$upstream\_ipport\`. 
I guess you are right sir but I'm really used to python and I rely to him when I don't know how to do something in bash.
 xrandr | grep -q "${laptop_monitor} connected.*[0-9]"
I know this is a bash sub, but can you use a perl script? Even command line argument?
Not sure exactly what you want, but in my case, the output of xrandar is; Screen 0: minimum 320 x 200, current 1680 x 1050, maximum 8192 x 8192 VGA-0 connected primary 1680x1050+0+0 (normal left inverted right x axis y axis) 474mm x 296mm 1680x1050 59.95*+ So I would use something like; # xrandr | grep VGA-0 | grep ' connected ' 
Throwing eval at an expansion problem is like summoning Mephistopheles to help you with your homework. Dangerous overkill.
The problem with simply grepping for `connected` is that it doesn't mean the display is on or off--just that it's connected via the HDMI cable. If the display is off, it still says it's connected, but the difference is there's no numbers found on that line (whereas if the display is on, it shows the output like your example). Here's my output for xrandr for a connected monitor that's off, for example: Screen 0: minimum 320 x 200, current 1920 x 1080, maximum 8192 x 8192 eDP-1 connected (normal left inverted right x axis y axis) And if it's on: Screen 0: minimum 320 x 200, current 1920 x 1080, maximum 8192 x 8192 eDP-1 connected 1920x1080+0+0 (normal left inverted right x axis y axis) 294mm x 165mm So far the solution mentioned by another person seems to be working.
I just thought it'd be educational to know it works... My bad folks.
bash &gt; awk &gt; Perl. Python has no use at all if you master these three in a Unix shell.
Thanks man. Yeah I noticed 'HOME' working.
Thanks, but here's why this is dangerous. Let's say I am using this in a script, where a folder is configured in a file. The process that runs pulling data from the file can be running under a different user, even root. If eval were used, then I can run anything I want as that user if I could write to the config. E.g. echo eval "`rm -rf somethigbad`" 
There's one more case: $ echo hello=hi:~ hello=hi:/home/ropid The idea here is that `=` is the separator character for flags and variable assignments, and `:` is the path separator (e.g. in `PATH`). `~` will expand after these characters because if you were thinking in terms of paths, you'd expect it to in these places. 
$(which progname) Assuming progname is in the path of the user executing the script.
Use `command` which is more portable and POSIX compliant. In bash specific cases use `hash` or `type`. Following are some example tests you can setup. Using a built-in is better than calling an external program like which [ -x "$(command -v program)" ] [ $(type -t program) = "" ] The last type command also returns what program is (alias, built-in, function etc).
You could use a test on $?, like this output=$(prog with args) &amp;&amp; echo $output That will hide any error that happens executing "prog," including prog not being found.
university != class C? In 2019? 
It doesn't work... Where there used to be the banner command I put: [ -x "$(command -v banner (what the banner actually says is top secret until i finish it))" ] And it just doesn't do it anymore.
That's because the square brackets notation is just a test. If you want to actually perform the command after it, put `&amp;&amp; banner` behind it.
...Yeah? How can I use that to make the script act accordingly?
That’s not how it works. `command` tells you if the given program *exists* and what it aliases to/what the path is/etc. It doesn’t *run* the program. So you would need: ``` if [ -x “$(command -v banner)” ]; then banner “top secret text” fi ``` 
Ohhhhhhhhhhh... Well I feel dumb now. But hey, it works. Thanks!
You can define a custom banner command if it's not found by putting something like this at the beginning of the script if ! type banner &gt;/dev/null 2&gt;&amp;1; then banner() { local IFS printf '%s\n' "$*" } fi So now you can unconditionally use the banner command later in the script, and it will use the external banner command if it exists, else it just prints the text with printf instead.
which banner || echo "banner is not installed"
My approach would be essentially the same as geirha's except, I have the following in my `.bashrc`, and because it's small it's re-usable in other scripts: # Functionalise 'command -v' to allow 'if exists [command]' idiom exists() { command -v "${1}" &amp;&gt;/dev/null; } alias is_command='exists' You've seen `which` and `type` thrown about, in my experience `command -v` is the most reliable and portable choice. There are edge scenarios where `type -p` might be a better choice, but those scenarios are rare. So, exactly as it says on the tin: if exists banner; then banner() { printf -- '%s\n' "PANTS!" } fi And, likewise: if ! exists banner; then banner() { printf -- '%s\n' "PANTS!" } fi Or, equally: if ! is_command banner; then banner() { printf -- '%s\n' "PANTS!" } fi
while i wholeheartedly agree that imagine a drive with dd is far more reliable, if the isnt possible (say, because you are cloning it to a different file system) ive found that the following works well sudo rsync -alvAXWEH --xattrs --super /source/dir/ /target/dir/ this *should* grab files, preserve links (hard+soft), keep xattrs+permissions+ACLRs+times+SELinux context. (note: the ending `/`'s are important for the paths)
2018 at NJIT. CS288 Intensive Programming in Linux. A required course in the computer science curriculum and still is required in 2019, just taught by a different professor. You learn to use C and Bash, which taught us the basics of using a unix terminal. I don't know why this would be a bad thing? We learned how things are stored in memory, mainly. Using C to teach students about memory helped me learn/retain a great majority of the information. We were also exposed to inline assembly, which I thought was very interesting. The course was difficult and weeded out a lot of people who did not know what they were getting into. Many people go into CS and have never even touched a unix system. This course gave everyone a crash course into unix. I feel like this course helped a ton of people, including me. I had personally not used linux that much and barely knew anything about bash/C. Linux/bash is used a ton after university, in my experience. You would be surprised how many people graduate with a CS degree and have zero experience using a unix system. Why is it surprising that we would get taught C or bash at university? 
rtfm 
I once moved to a place, due to studies, where i didnt have internet, and had Linux installed, sooo i started writing scripts for fun. Learned the basics. My latest project, which im sure is more suitable in other language like perl or python, is a bot that Powers a plex request function through Facebook messenger. You can search, request, refresh (do another search for subtitles), find missing episodes and some other bot-y stuff. It is a complex system that makes plex, couchpotato, Medusa (sickrage), thethvdb and imdb talk togheter in ordner to make it all work. You can search for "star", and the bot will reply with what tv shows or Movies are available for being added/downloaded to plex based on imdb or thethvdb respectively. When you have requested a movie, the bot will invoke couchpotato, which in turn sends it to rtorrent which in turn invokes the bot again. It will notify you privately as well as in the group chat. So during the night when New episodes are added, you can see what episodes have been added in the group chat. It also serves as a game enforcer for when we play Tag in febuary. Once you've given away a tag, you tell the bot (.tag), and the bot notifies the players that a tag has been given away. This also gives me the ability to graph the game statistics over time, which is kinda neat. So yeah, bash is a pretty cool and versatile language, and ive learned a Great deal about Linux while goofing around. Dont know any other languages though. 
I'm a self taught programmer, who has been learning programming and Linux for just over 4 years now. I find some bash scripts to be super easy to read. Some are crazy complicated. My most complicated shell scripts involve for loops. I don't do anything too crazy in bash. Bash can do a lot though, and i see people that put all kinds of funky syntax in their scripts. I just skip right over those. I'll learn that stuff when I need to know it. &amp;#x200B; You won't need to unlearn bash to learn any "real" programming languages. Your knowledge of bash just may not help you too much. Other than declaring a variable, and using a for loop, there's nothing in my bash scripts, that really resemble any of the code I put in a Java program. At this point, I think you've got to decide what you want to do. Why do you want to get better with bash? or why do you want to learn a "real" programming language? I think you need the answer to that to get any farther. Because the answer, kind of determines where you should spend your time. I will say that once you learn a "real" language, the concepts all pretty much carry over, so it's hard to pick a first "real" language. But I'd say go with python. I've combined python with bash to do some great stuff. Like I once had a few hundred little PNG's with white backgrounds. I wrote a python script that takes in an image, and converts white to transparent. Then I wrote a quick bash script, that would take in a folder of images, and call that python script, on each image individually.
Would something like `sxhkd` do the trick?
It is said that what you can do in a language, you can recreate in any other. As proof of this, check out bash-oo-framework which implements some concepts from other languages like objects and exceptions. If you end up believing that, and I do believe that about today's languages, the choice stops being about what you like writing in and more about what is apropriate for a given problem. Go is good for concurrency so it's good as a web backend launching goroutines for each request. Rust is good for safefy so it's good for operating șystems, drivers and embedded. C++ is good for extreme performance. D is a good general default language all around. Bash is great at introspection like calling a function with the name of a variable that you have at runtime. However, as a hobbyist, you probably don't have a narrow list of problems that you have to solve, so Bash or any other single mature language would do just fine. I also wouldn't put a barrier between scripting and more sophisticated types of programming, they are more similar than different.
Triggering actions with keystrokes is bound to fail spectacularly. I wrote, in bash, a general purpose daemon that runs in a loop checking conditions (bash scripts) and running actions (bash scripts). Every user + pseudo user 'common' who generously provides all other users with environment, services, software and data, can start an instance. [The instance, running for 'common' starts after boot](https://i.imgur.com/35lrk6Z.png). Here is an example of a condition script ('PrinterNeedsPower'): #!/bin/bash if $Int_SystemShuttingDown || $Int_SystemStartingUp then exit 1 fi if $IntNewTime2Seconds then if [ -n "$(2&gt;/dev/null lpstat -o)" ] then exit 0 fi exit 1 fi if $PrinterNeedsPower then exit 0 fi exit 1 
Link ? 
This is just my opinion as a Unix-like OS user, but one detail I think would be different is that when I'm making a shell script, I usually jerry-rig something together to just barely work without any elegance or fluff at all. AFAIK a "real programmer" would have more elegance and recursion (function that calls another function that calls another) and various ways of input and error handling, etc. This complexity is lacking on my end because I can just get away with something very simple and specific to my usage without much flexibility.
I havent really published the code, werent really planning on it either. Its really specific to my setup and has too much information on my system for me to be comfortable with sharing it. I'm planning on rebuilding it from scratch and will likely be publish it on github or something, but its a hassle and will probably take a while. 
&gt; recursion (function that calls another function that calls another) Recursion refers to a function calling itself, not another function. Just, FYI.
Thanks, the proper word for what I meant was "nested" or "nesting" I think.
Maybe encapsulation 
I think you would find a simple, modern, rapid development language like Python or Ruby really useful. Go might fall in the same category, but I'm not too familiar with it. The reason for this is that they can essentially be used as Bash extensions to enhance, complement and leverage what you already know and do. For example, you might have a shell script that takes a CSV spreadsheet full of a party RSVPs with names and emails, and you want to send them each an email. Your `cat file.csv | while IFS=',' read name email; do ..` loops works fine most of the time for e.g. `Alice,alice@example.com`, but it fails for several lines where people wrote a comma in the name field: `"Bob, Carol and Dave",dave@example.com`. Instead of trying to awkwardly hack your Bash script into figuring out which commas are or aren't inside quotes, you can now write a simple Python script to make it semicolon separated and save it as `comma2semi.py`: ``` #!/usr/bin/python import csv import sys rows = csv.reader(sys.stdin) for row in rows: print(";".join(row)) ``` You can then simply insert that into your loop and carry on with your script: `cat file.csv | comma2semi.py | while IFS=';' read name email; do ..` This works great to take the pressure off of things Bash are bad at, like parsing, computing with decimals, using nested data arrays, handling binary contents, anything asynchronous, or anything that benefits from two-way communication (like automating interactive tools that don't always ask the same questions in the same order). Since it integrates nicely, it doesn't detract from any of the things Bash is good at, like running specific programs with specific inputs or arguments, looping and branching based on the result success, and otherwise setting up and gluing tools together with simple control flow, pipelines and redirection.
I second interest in it. I'd like to see how you implement these functions. No pressure on use of pure bash. I just want to see how you did it.
tl;dr - the shell programs you're reading that seem overly complicated are running into shell's inability to allow a programmer to express their intent as they solve larger problems with code. You're right that depending on the problem, it helps to have another language in the toolbelt. Python and Ruby are great choices. Instead of needing to unlearn shell, you should find that either of these provides a straightforward transition from shell, and will grow with you as you learn more about programming. I think that when it comes to writing code, the goal should always be to express the solution to some non-trivial problem to a computer and more importantly to someone else...even if it's future you. Granted; programs will be run by machines, but humans have to maintain them. For these reasons, it's always best to choose the most expressive language you know for each problem u try to solve. Shell is most expressive when the problems are solved by running other programs. For example, automating an installation makes sense in shell because the steps to install something are usually documented as a series of commands, each followed by decisions on whether to proceed. When coming in from this mindset, shell just makes sense. In contrast, reasoning with a solution written in C will almost certainly carry a higher cognitive load: in order to reliably run an external program in C, you just gotta do more things that aren't directly related to solving the problem. You need to potentially allocate pipes, fork, worry about resource leaks, exec, handle errors and ensure to always clean up defunct child processes. All the while, you need to deliberately manage memory. Here, shell is the language of choice, because the solution to the problem is most expressive in shell. The expression to noise ratio is highest. As you may be hinting at, when a problem's solution isn't a matter of running external programs, a lot of the benefit of shell is lost. For example, a program that ingests user provided data and provides user defined views on that data is kinda tricky to get right in shell. Here, a shell script will be dominated by creativity and defensiveness. Shell doesn't provide a lot of tools for structuring data, so the solution will almost certainly be either difficult to read or will require documentation in addition to the code as you try to encode the data into strings and shallow arrays and dictionaries. Also, shell has a lot of gotchas when it comes to handling user provided data, so the expressed solution will be dominated by careful quoting or other convensions which may turn into a minefield for inexperienced maintainers...or you a few months from now when you're trying to remember what you were trying to do. Finally, in addition to being a difficult language for structuring data, shell doesn't really offer a lot of tools for structuring code. It provides functions, which sorta work like they do in other languages, but for the purpose of breaking a program into independently testable units **that pass around data,** they leave a lot to be desired. For this reason, you may end up verifying that your program works by simply running the entire program on different inputs. These tests are a missed opportunity to provide examples to maintainers (or future you) on how the bits and pieces of your program should be used, should it be to be enhanced in the future. So yeah. Shell isn't the language of choice for all problems. Heck, it's not even the language of choice for problems outside of a very narrow niche. This is why it's a good idea to try to get comfy with at least one higher level language. Of the ones our there, you'll get the most bang for your time from Python or Ruby. They're as ubiquitous as shell and provide syntax or built in functions that map to what you already know. This could give you the stepping stone you need to level up the design of whatever you're writing such that your solutions are more expressive. Also, these languages have great tools for testing and debugging programs, which is a real step up from shell. As far as needing unlearn shell, you definitely shouldn't. Just try to learn how best to express your intentions in whatever new language you pick up. You can do this by reading books on the language or by taking a peek at popular open source programs in the language. Find one with a lot of stars on Github, read the readme, find the entrypoint (the first function run in the program) and try to understand as much of what's happening as you can. Then try to figure out why you understand it. Is it because of comments? Is it because of a certain naming convention? Is it using functions or other language features for organizing code? Do the tests help you understand how the parts of the program work together? Try applying those patterns to a solution you noticed was very cumbersome in shell and see what happens. As far as compiled languages like C, C++, Go, ... , you'll know when you need them. They're very useful and lead to the most expressive programs when you want to control more precisely what your program is doing at all times, or if you need to 💯 talk directly to the operating system with no intermediary. Again, you'll know when u need these. Anyways, hope this helps. You're spot on on your views on programming languages and shell. When you're thinking of why a shell script or any program is difficult to understand, though, think of expressiveness and whether there language allows the programmer's intentions to be clearly expressed. Hopefully you find the right companion tool in your tool belt as you solve slightly beefier problems and good luck.
The original Borne shell was developed concurrently with C and UNIX at Bell Labs. They work well together. If you can program bash you can program any procedural language. The biggest change would be learning an object oriented language like python.
Thanks a lot. I'd be really interested. I think it's really cool. 
if you use \`NR == 3\` won't it derp out on files with less than 3(n) lines?
The name is confusing, as people will think about webbrowser stuff when the see "cookie" not about templates.
They can finish reading the headline, it’s not really a problem. Name overloading happens all the time.
In all seriousness, what is the point of this versus something like ansible?
I’m curious as well. It seems like the template module + j2 would accomplish the same thing in a way that’s easier to onboard. I’m guessing this is just a limited alternative. 
You don't even need ansible. Just a very short sh or bash script would accomplish the same. Template can be its own script.
&gt;most difficult for me to decipher, being written by "real" programmers that are proficient in several different languages Funny, my experience differs quite a lot from yours. In the professional settings the bash scripts Ive seen that were written by the "real" programmers have mostly been utter garbage. They've been the most simple and straightforward scripts I've seen but still full of errors. 
You might want to format your code blocks in proper markdown. This is not github.
TIL that Desktop reddit and Mobile reddit uses different markdown
[https://www.semicomplete.com/projects/xdotool/](https://www.semicomplete.com/projects/xdotool/)
I think my difficulties in reading some of the more complex (to me) scripts is due to my lack of experience and general knowledge. With that said, I've had a real programmer tell me that one of my scripts was hard to follow. I believe a different perspective has to be responsible. When writing scripts, I tend to start by writing code as if I was typing on the command line. I don't really have the ability to set down and "plan" or outline how I will structure the script. Order and structure usually come to me after getting started. If unsure of anything, after a bit of reading I usually run those parts of code in "bash -x" to see how it's working and running as intended. As far as garbage code or errors, I'm still at the "if it works in testing, all is good" stage. If the script misbehaves in use, I've been able to go back in and fix things so far. I see "elegance" mentioned in scripting / programming, which is honestly a foreign concept to me. Same with other concepts. ie: algorithms, etc... 
&gt; I'm still at the "if it works in testing, all is good" stage. To get to the next "stage" I would recommend using `shellcheck` to check your scripts and possibly reading the list of [bash pitfalls](http://mywiki.wooledge.org/BashPitfalls). I checked the aurt and it seemed pretty decent. Sure shellcheck found some things but most of them were minor. Things like SC2143 I personally just ignore for example.
Thanks for this.
This feels very close to mustache, which is multilingual including a bash implementation. 
fantastic ..thanks for sharing
 : ${1:?"Missing side 1"} : ${2:?"Missing side 2"} : ${3:?"Missing side 3"} can obvs be replaced with just : ${3:?" Missing side "$(($#+1))}
 How can I test if a file exists on another machine? if [ -e /path/to/mountpoint/and/file ] Although you probably want to check your mount is there first
Technically correct fun fact: quoting this can make it a billion times faster in its worst case. It's unlikely to matter in practice, but if you pass `'/*/*/*/*/*/*/*'` as the third argument, the simple null check goes from 1 microsecond to 15+ minutes, and you can add as many space separated globs as you want to consume almost arbitrary amounts of memory. This is not a problem with `: "${1:?"Missing side 1"}"`, which is always a microsecond. 
The script is on a linux machine. The file I am checking to exist is on a Windows machine share I am looking at the smb share with caja (mate desktop) How do I find where it is mounted? Is it mounted?
First I'd mount folder on linux using standard `mount` command (see /etc/fstab also) or using [autofs](https://wiki.archlinux.org/index.php/autofs) daemon. Then I'd just simply check file for existence as it would be local folder using bash script 
Was briefly thrown by your solution using a modified line 1 testing for the first argument and commenting regarding mine and my lazy checking for the 3 arguments but calculating the missing one with `$#+1` .. thinking that my addition had been the cause. Then reread it, and obvs - the argument was being expanded in the expansion of the unquoted parameter... so for the original post, no matter in which (or all) position(s) your "string of strain" was, it would have been expanded as opposed to mine which was only vulnerable should the payload have been in the third position... Right? : "${3?" Missing side "$(($#+1))}" 
 mount should show you 
Exactly right, you didn't introduce this. I just picked \`$3\` so it would manifest in both the original article's code and your improvement. 
&gt; you didn't introduce this. but I didn't spot and catch it... :-/ 
OK, I do see something on /run/user/1000/gvfs/ like this 'smb-share:server=192.168.1.10,share=public,user=myusername' The problem here is that the mounter creates the directory name, as a string of information and I would have to guess what it is.
&gt; OK, I do see something on /run/user/1000/gvfs/ it is always under `/run/user/`***`UID`***`/gvfs/...` where UID is Linux ID of the user that the mount was created for &gt; I would have to guess what it is Isn't that what wildcards are invented for?
I tried mounting using -t cifs sudo mount -t cifs //192.168.1.10/Public /home/myusername/mymountdirectory -o rw,user,user=myusername It mounts but everything mounted becomes owned by root
You can read about the available tests inside `[` by doing: help test That `&lt;&gt;` you are currently trying to use does not exist, it is a mistake. If you want to test text for "equal" you use `=` or `==`, and "not equal" is `!=`. The `|` you are trying to use does exit in bash so you don't get an error message, but it is something different and not what you want. The "or" operation is `||`. Use `[[` instead of `[` in a bash script. They both are very similar, but `[[` enables some extra features. When you use `[[`, you can put `||` into the `[[` brackets, for example when you have this: [ a = b ] | [ c = d ] you can instead write: [[ a = b || c = d ]] The evil thing about bash is that you can make a lot of mistakes where you won't get an error message, instead things will not work correctly. Check out a super helpful tool named `shellcheck`. It tries to hunt for all kinds of mistakes that are possible to make in bash. Your distro will likely have a package for it and you can also try it online at www.shellcheck.net without having to install it.
They're not equivalent, but you have to be doing something unusual to get into the differing case. The original will catch arguments that are empty strings, whereas the suggested replacement only catches missing arguments. That is, for `./is-equilateral 3 '' 5`, the original will error and the replacement will pass. This can happen if you're using variables to fill in the arguments and you misspell it or otherwise mishandle the data.
&gt; I don't understand what you are trying to do in the second line He's setting `marap` to the characters from the tenth to the nineteenth of `$@`; a pure Bash alternative would be `marap=${@:9:10}`.
In one of your libs that you’re including, you’re setting `errexit` which is a shell option that will cause your script to end immediately on any line that returns a non-zero status code. The sub shell where you’re calling MySQL (and assigning the output to the DBDIST variable) is probably failing and killing the program. Here is where it’s being set: https://gitlab.com/gwinans/dba-tools/snippets/1834942#L25 Here is documentation on that setting: https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html Err exit is generally considered to be a best practice since it prevents runaway scripts that may do unexpected things after an error. Pardon formatting. On mobile. 
Line 28 of the verbose output shows you have a trap to capture errors... `trap check_for_locks SIGTERM SIGINT ERR` When `DBDIST=$( mysql --version | grep MariaDB )` is run, it is considered an error which triggers your check\_for\_locks function to run and exit. &amp;#x200B; This script shows it a bit easier... &amp;#x200B; `#!/bin/bash` `err_report() {` `echo "Error on line $1"` `exit` `}` `trap 'err_report $LINENO' SIGTERM SIGINT ERR` `echo hello | grep foo` `echo "won't run because the trap exits"`
1. I think /u/ropid knew what that does, he was saying that he doesn't see what the OP is trying to accomplish by doing that. 2. No, you're wrong about a "pure Bash alternative." `$@` is an array, not a string; `${@:9:10}` will evaluate to the 9th-19th items in the array. The pure Bash alternative would have to be a 2-step process marap=$* marap=${marap:9:10}
You have several mistakes that are probably coming together to cause a lot of confusion. ## Background The core data-structures in Bash are "strings" and "lists of strings" (there are also associative-lists/maps, but we won't talk about those here). There are 3 things that happen in order when Bash builds a simple command: 1. variable/subshell expansion 2. field separation 3. glob expansion Single-quotes `'` turn off all 3, double-quotes `"` turn of field separation and glob-expansion, but still enable variable/subshell expansion. ## Specifics ### 1 marap=$(cut -c 10-19 &lt;&lt;&lt;"$@") ^ When expanding an array `${array[@]}` (for regular arrays) or `$@` (for the special args array) expand to a *list* of strings. `${array[*]}` (for regular arrays) or `$*` (for the special args array) expand to a *single* string, where each item is separated by the first character in the special variable `$IFS`. Because the default value of `IFS` is ` \t\n` (space, tab, newline), that usually means separated by a space. Because `&lt;&lt;&lt;` takes a single string, you probably meant to use `$*` here. I'm not sure what you're expecting to accomplish with `cut`. What do you expect the arguments to look like? ### 2 [ $marap &lt;&gt; hevc?????? ] ^^ I assume you think that `&lt;&gt;` is an inequality operator. It is not, it performs I/O redirection; the above is equivalent to [ $marap ] &lt;&gt;hevc?????? Which runs the command `[ $marap ]` with standard-input redirected to a file matching the glob `hevc??????` (or `hevc??????` if no other file matching that already exists), and has the file open for both reading and writing. Even if you did use the string comparison operators (`==` or `!=`), the `[` command doesn't do glob matching. You can do glob matching using the similar `[[ … ]]` construct (which is a syntactic feature, `[[` is *not* an ordinary command). Because `$marap` is not quoted it undergoes field separation and glob expansion, neither of which you want. Assuming that `$marap` doesn't start with a `-`, then `[ $marap ]` is just checking that `$marap` is non-empty. I mentioned the special command `[[` a bit ago, you could write the command like: [[ $marap == hevc?????? ]] ^ ^ 1 2 1: Because [[ is a special syntax feature, not a regular command, you don't need to quote a variable that is the left-hand-side of the comparison (thought there's no harm in wrapping it in double-quotes) 2: Because [[ is a special syntax feature, if you don't quote the right-hand-side of a string comparison, it will treate it as a glob and match the left-hand-side against it. It does not try to match this glob against files or do glob expansion. ### 3 if [ $marap &lt;&gt; hevc?????? ] | [ $marap &lt;&gt; h264?????? ] | [ $marap &lt;&gt; mpeg1????? ] | [ $marap &lt;&gt; mpeg2video ] | [ $marap &lt;&gt; vc1??????? ] | [ $marap &lt;&gt; vp8??????? ] | [ $marap &lt;&gt; vp9??????? ]; then ^ A single `|` combines multiple commands in to a *pipeline*; it takes the standard-output of the command on the left-hand-side and feeds it as standard-input to the command on the right-hand-side. Do do a boolean "or" operation on the exit statusus of the commands, use `||`. ### 4 You can do glob matching in a "switch/case" statement, which probably makes the code more workable: #!/bin/bash case "$(cut -c 10-19 &lt;&lt;&lt;"$*")" in hevc??????|h264??????|mpeg1?????|mpeg2video|vc1???????vp8???????|vp9???????) exec '/usr/lib/plexmediaserver/Plex Transcoder2' -hwaccel nvdec "$@";; *) exec '/usr/lib/plexmediaserver/Plex Transcoder2' "$@";; esac In most languages you end a case by writing `break`, but in Bash you end it by writing `;;`.
Indeed, that was it. Forgot that grep returns an exit code of 1 if nothing is found. Now I've got an issue with bash adding single-quotes around ${DB_OPTS} This: ${BACKUPBIN} "${DB_OPTS}" --backup --target-dir="${BACKUPDIR}"/backup_"${DATE}" 2&gt; /tmp/xtrabackup.log Is turning into ... xtrabackup '-uroot -pSomePass' --backup --target-dir=/backups/backup_2019-03-12 I cannot find anything in the bash docs, let alone the lovely StackExchange about this... result. Any thoughts, offhand?
Becomes root because you're mounting folder using `sudo` or created local folder under user root with root ownership? Your user should have read/write access to local folder "/home/myusername/mymountdirectory". You may try: `sudo chown -R youruser:yourusergroup /home/myusername/mymountdirectory` `sudo chmod -R 777 /home/myusername/mymountdirectory` `mount -t cifs //192.168.1.10/Public /home/myusername/mymountdirectory -o rw,user,user=myusername` Then you may check if it's mounted correctly with just executing `mount` command
I should have provided more information, sorry. So here is the code I have got off the Plex forums: `#!/bin/bash marap=$(cut -c 10-14 &lt;&lt;&lt;“$@“) if [ $marap == “mpeg4” ]; then exec /usr/lib/plexmediaserver/Plex\ Transcoder2 “$@“ else exec /usr/lib/plexmediaserver/Plex\ Transcoder2 -hwaccel nvdec “$@“ fi` It takes the FFMPEG command and adds nvdec to everything except for mpeg4. This script is working. Instead of blacklisting mpeg4, I want to whitelist all compatible codecs: MPEG-2, VC-1, H.264 (AVCHD), H.265 (HEVC), VP8, VP9. 
My bad, I quickly typed it up just before leaving. Is it really an array, though? My understanding was that it's a special parameter and that behavior is special to it.
Thank you for your help, please see my comment above to get more background info.
I bought a network security bundle and the quality was off the charts horrible. The fonts were rendering wrong on both my Kobo and on my Linux NB, I had to scroll halfway through the book to get past the preface and nonsensical discussion of the importance of the topic.
My first suggestion is to provide absolute paths in the script as opposed to `./script`. This will specifically tell the system where it should look for the executable. I’m assuming these files are in your users home folder. So, in the script replace the ./ with `/home/user/script`. 
`&lt;(...)` is a bash syntax for a process substitution. It basically creates a named pipe that lets the "outer" process treat the "inner" process as though it was a file. You can also do `&gt;(...)` to have the "outer" process write to the "inner" process as though it were a file. That syntax doesn't exist in POSIX, which is why `/bin/sh` complains.
It's special, but I think it's fair to call it an array. Or rather, Bash arrays are a generalization of Bourne-shell's special arguments handling. `$@` is equivalent to `${array[@]}`, `$*` is equivalent to `${array[*]}`, `${@:2} is equivalent to `${array[@]:3}`. The only real difference is that you can only set the arguments array using the `set` command, and that the arguments array starts at `1`, where other arrays start at `0`.
Awesome that you figured that first part out. Fror `DB_OPTS`, bash isn't actually "adding" anything. The `-x` output puts them there to show you how your code is being interpreted so that you can troubleshoot issues like this. I'm not sure how familiar you are with programming as a whole, so I won't make too many assumptions. When a program is executed, it receives an array (a list) of arguments so that it can access them positionally, by number. When the program is runing, whether it's a shell language like bash, a scripting language like perl, ruby or javascript or a compiled language like golang, rust or C, this is all the same and the program can't tell how the quoting was done when someone invoked it from the commandline. It's up to the user's interactive shell to handle this free-form string that was typed, parse it out, make any substitutions (variables and special characters like `~` and globs like `*`) and turn it into this array of strings that will get passed to the new process. Ok, so now that that part is out of the way... you've got your command, with variables: ${BACKUPBIN} "${DB_OPTS}" --backup --target-dir="${BACKUPDIR}"/backup_"${DATE}" 2&gt; /tmp/xtrabackup.log the line is first going to be parsed by the bash interpreter and variables are going to be replaced, which in this case, will turn it into this list of commands: 1. `xtrabackup` 2. `-uroot -pSomePass` 3. `--backup` 4. `--target-dir=/backups/backup_2019-03-12` &gt; I'm omitting the redirection part since that's not getting passed to the process and is kinda a separate thing A naive workaround would be to remove the quotes around `${DB_OPTS}`. In many cases this will "fix it" but will expose you to another issue: word splitting. What this means is that the variable will be expanded in-place and bash will try to figure out how to group the arguments, rather than being more explicit as you were when you provided quotes. in this case, if you removed the quotes, that would leave you with: 1. `xtrabackup` 2. `-uroot` 3. `-pSomePass` 4. `--backup` 5. `--target-dir=/backups/backup_2019-03-12` Which looks fine, right? But let's say you had a space in your password, it would be come out like: 1. `xtrabackup` 2. `-uroot` 3. `-pSome` 4. `Pass` 5. `--backup` 6. `--target-dir=/backups/backup_2019-03-12` and the `xtrabackup` command would probably be like "wtf are is this `Pass` argument?" and error out. Similar things could happen with other special characters like `~` or `*` or even `&amp;`. So the more fool-proof solution is to use an array variable for your `DB_OPTS`: DB_OPTS=( "-u$USERNAME" "-p$PASSWORD" ) if you want to append new arguments to it you can do: DB_OPTS+=( "another argument" ) DB_OPTS+=( this 'time' "three" ) # three arguments, quoting works same as in shell, word-splitting is same In this case, you'd have to change how you refer to the `DB_OPTS` variable: `"${DB_OPTS[@]}"` -- you need the curly braces and the `[@]` part, which means "give me every element" and you want to double-quote it since that will preserve the elements when it gets expanded tot he commandline. Assuming you used the version with the username and password, this line: ${BACKUPBIN} "${DB_OPTS[@]}" --backup --target-dir="${BACKUPDIR}"/backup_"${DATE}" 2&gt; /tmp/xtrabackup.log would expand to: 1. `xtrabackup` 2. `-uroot` 3. `-pSomePass` 4. `--backup` 5. `--target-dir=/backups/backup_2019-03-12` which is exactly what you want. many consider bash arrays to be more advanced, but for situations where you're constructing commandline arguments with variable numbers of arguments where you may not control the contents of them, it's really the only way to deal with it. On a side-note, you should make sure you always quote your variables, since it will protect you from these word-splitting problems and unexpected results. so quote `$BACKUP_PLAN` and you can even quote the entirety of your other arguments so you have less visual noise on the line, so either: --target-dir="${BACKUPDIR}/backup_${DATE}" or "--target-dir=${BACKUPDIR}/backup_${DATE}" whichever suits your tastes better (I like the first, personally) I hope this helps! Sorry it was so long.
Nit: It's just a regular pipe, not a "named pipe". A "named pipe" is a pipe that is bound to a name in the filesystem (other than `/dev/fd/` or `/proc/$pid/fd/` that all opened files get); you can create one with the `mkfifo` command. For a concrete example of how process substitution works: Bash calls the `pipe(2)` function, which creates a pipe that is opened to the Bash process, returns a *file descriptor number* for that pipe. On most unix-ish systems (unfortunately, it isn't in POSIX), all files that a process has open can be accessed through the special files `/dev/fd/$n` where `$n` is the file descriptor number. On the Linux kernel, `/dev/fd/` is implemented as a symlink to `/proc/self/fd/`; you can use `/proc/$pid/fd/$n` to refer to the open files of a different process. Anyway, Bash has that open file descriptor for the pipe, and needs to pass it as a file name to the command. So, it uses `/dev/fd/$n` to turn that number in to a path: $ echo &lt;(ls) &lt;(ls) /dev/fd/63 /dev/fd/62 /dev/fd/63
Responding to your nit, here's a quote from the GNU documentation about process substitution: Process substitution is supported on systems that support named pipes (FIFOs) or the /dev/fd method of naming open files. and looking in the source for bash 4.3.30 (the only one I have already around) we can see in `subst.c` on line 9759 if the system doesn't support the `/dev/fd/` method the pathname is generated by: pathname = make_named_pipe (); so I feel pretty confident that it is in fact using named pipes, at least when it's not using `/dev/fd`
That makes sense that it uses named pipes on systems that don't support `/dev/fd/` (since named pipes are POSIX but `/dev/fd/` isn't). I would struggle to name a system that doesn't support `/dev/fd/`, though.
Yeah, the overlap between systems without `/dev/fd` but with a bash that is new enough to support process substitution is probably pretty small, though apparently the GNU people felt the need to handle it I guess.
Hi. Thanks for the reply! Yes that seems fine, the thing I'm really struggling with however is this line `pgrep -f "monitor" &gt; /dev/null;` which is meant to check if "monitor" has detected any new video files being dropped into the dropbox folder, and if so, do this: `echo "New video has been uploaded..."` `echo "Restarting OMXPlayer..."` `killall omxplayer` `echo "Starting BASH3..."` `./BASH3.sh` However I don't think I'm going about it the right way
The manual link is already given but it is very terse and doesn't really explain very much or shows the power of process substitution. Here are some examples to give you an idea of what it is and why it is "cool". In essence process substitution creates a special temporary file containing the result of a command. This can be seen if you run for example $ cat &lt;(date) Wed Mar 13 10:43:46 EDT 2019 The result of the date command is stored in a temporary file that cat reads. The name of the file and type can be seen with any usual command. $ ls -lt &lt;(date) lr-x------ 1 random_cynic random_cynic 64 Mar 13 10:44 /dev/fd/63 -&gt; pipe:[132591] $ stat &lt;(date) File: ‘/dev/fd/63’ -&gt; ‘pipe:[133514]’ Size: 64 Blocks: 0 IO Block: 1024 symbolic link The above shows that it is a special temporary file called named pipe. That's great but why it is cool. That's because since the output is stored in a file it means that you can use them with commands that expects files like diff, sort etc. $ ls test* test: a.txt b.txt c.txt test2: a.txt c.txt e.txt $ diff &lt;(ls test) &lt;(ls test2) #compare files in directory 2d1 &lt; b.txt 3a3 &gt; e.txt $ sort -r &lt;(ls test) &lt;(ls test2) | uniq #Create a unique list in reverse order e.txt c.txt b.txt a.txt In the above example we showed the difference in file contents between directories and also showed the unique files in both directories in reverse sorted order. This would be much more complicated without process substitution.
just use some arrays. In this example you can see that if there is no value yet the if is true. awk ' { quantity[$1]=quantity[$1]+1 total[$1]=total[$1]+$2 if($2&gt;max[$1]){ max[$1]=$2 } } END{ for(i in total){ avg=total[i]/quantity[i] print i " Maximum:" max[i] " Average:" avg } } ' inputfile 
&gt;Since named pipes are POSIX but \`/dev/fd/\` is not, why do you think they've implemented it with named pipes as fallback and not the other way around? Couldn't they offer process substitution to non-POSIX shells like \`sh\` by implementing it first with named pipes? &amp;#x200B;
Very informative and complete answer. Thank you!
Here is example that I've mounted Windows network share on Manjaro Linux: List shared folders list on fileserver: `smbclient -L fileserver -U=user -W=domainname` &amp;#x200B; Create folder share with 777 permission: `mkdir /mnt/share` `chmod -R 777 /mnt/share` &amp;#x200B; Mount manually with manual password authentication from terminal: `sudo mount -t cifs -o rw,username=user //fileserver/ict$ /mnt/share/` &amp;#x200B; Mount manually with automatic password authentication from terminal: `sudo mount -t cifs -o rw,credentials=~/fileserver-samba //fileserver/ict$ /mnt/share/` &amp;#x200B; Contents of file fileserver-samba: \[manjaro@manjaro \~\]$ `cat ~/fileserver-samba` username=youruser password=userpassword domain=domainname
Since they fallback to named pipes which are POSIX, why is process substitution not available in POSIX shell implementations like `dash`?
What have you tried so far? What is wrong with your results?
POSIX defines its own standards. It's not necessarily about whether or not something could be implemented, it's whether something must be implemented. I honestly don't know if the POSIX standards people have been discussing including it in the next standard or not.
The most expensive part about it is launching the program itself. If you can find an option or an alternative which will simply print new statuses to stdout, that will be much more efficient. For example, instead of while sleep 1; do date "+%a %b %d %T"; done ...you can [run this instead](/r/i3wm/comments/aumd1a/is_it_computationally_expensive_to_call_the_date/ehutsko/). When launching `date`, it will have to load its shared libraries, and parse the command line arguments before it tells you the time. Not to mention that bash/whatever your shell is is parsing the loop, the return codes for everything, etc. You can do a similar thing for other programs
It would be best if the monitors you run did not spawn a new shell or even multiple shells. Sometimes programs like dashboards will execute a shell, then the program you run is a shell, which then runs a binary or set of binaries. "Soon, you are talking about real money". So if possible, run one program that collects and parses the data that you need from the files in the /proc filesystem. That way no additional binaries are run. If the menubar can poll the files in /proc itself, then so much the better. If you need to create a temp file for the menubar to parse, put the temp file in /dev/shm.
Thank you so much for this. While I do have a long and storied history with programming, it never extended beyond being a basic hobby. First and foremost, I'm a DBA. This is, amusingly, the most complicated BASH script I've written in well over a decade. I do most of my system management scripting in Python. Again, thank you. I'm stuck with the sloppy seconds of a poorly designed environment.. doing my best to make it suck slightly less.
Thanks for the very detailed response. Very informative. So in thinking about what I have used shell for, it seems so far, I really have no requirements beyond shell. ie: automating tasks by running other programs (at this point). With that said, perhaps then expanding my knowledge of shell (really need to learn regex, ext regex, expand on awk, etc). Beyond that it sounds like a scripting language would definitely be better suited for what I may be doing in the future. I've read a tiny bit about both ruby and python and it seems they are at opposite ends of the spectrum as far as: * rigidly one way to do things: python * various ways to get things done: ruby. I like the ruby way if I'm getting it right, but perhaps I **need** the structure required by python to become better at scripting? Also seems like I see more python used around the Arch Linux ecosystem. Also need to follow up with your explanation of expressiveness to get my head around it. Currently the biggest stumbling block I run into with shell is just figuring out how to precisely extract the bits of what I need from commands. Not really knowing regex I believe, is the cause.
I have pondered that if one was proficient in C and bash, anything would be possible in the Linux ecosystem.... &gt;procedural language &gt; The biggest change would be learning an object oriented language I've seen these terms mentioned, but the meaning escapes me at this point. Good to know that bash falls into the procedural language category. Also, no interest in making money with my hobby. Besides, it took my 10 years to get to this stage, so actually mastering bash, and then SQL, I'd estimate another 50 years, lol. I'm not smart enough to actually learn any of this, just tenacious and OCD enough with my hobby to pick it up.
u/Trickmanpat, a [direct link to the bundle](https://www.humblebundle.com/books/linux-wiley-books) would've been more useful. Mini-Review: The general audience for this bundle is typified by the *Dummies* book in it. Also, three of the books (the two Wrox books and the assembly language one) date from the last **decade**. Wrox imprint books from that era are particularly easy to spot by their red/B&amp;W-author-photo covers and bold typeface (modern Wrox books use a skinnier font and non-author cover art). For folks above "rank beginner": the Linux Server Security book could be useful if you're an intermediate Linux sysadmin, and the Linux+/LPIC study guides might be good revision texts if you're going the certification route. Otherwise, spend your money on this bundle either to support the Freedom to Read Foundation, or to gift the lot to a Linux newbie.
Wow, I'd have thought what you're describing would be beyond what a shell language was capable. Interesting...
&gt; as a hobbyist, you probably don't have a narrow list of problems that you have to solve, so Bash or any other single mature language would do just fine. Thanks! Is D used much within the Unix / Linux ecosystem? I've heard of it, but that's about it... Interesting
Most Unices outside the Linux/BSD universe didn't support anything like `/dev/fd` when process substitution first appeared in bash (I think it was 1994, and bash compiled on Solaris/AIX/etc. just fine). I suspect most of those still don't.
It's not used at all, but it's on my list of languages with which you can do anything efficiently and it's in the default language spot. It's a C++ done right without the backwards compatibility with C which brings it down so much. It's also made by my conational Andrei Alexandrescu so I might be biased.
&gt; you would find a simple, modern, rapid development language like Python or Ruby really useful &gt;Instead of trying to awkwardly hack your Bash script into figuring out which commas are or aren't inside quotes Already experience issues with using awk in bash, trying to figure out how to work around awk's single quote requirements getting in the way of shell... &gt; you could just integrate it into the bash script to remove a pain point &gt;It still lets you leverage everything that makes Bash great as a glue language, you've just enhanced your ability to write your own pieces to glue together with it. Very encouraging and motivates me towards Python or Ruby.
Thanks, I do *try* to use shellcheck, have it installed. Lately though, with no one other than me using my scripts, I'm guilty of bypassing that step prior to uploading stuff to github Using shellcheck often proves **very** time consuming for me to use and correct stuff, even with the info they provide. I've also noticed scripts out there that have comments about disabled shellscript errors. I really need to get to the point to where I understand how to properly correct it.
Want to thank everyone with all the info provided in your replies! I could spend the next weeks or more going back over this to further study a lot of what's been provided. So far, I think based on whats being said, sounds like it's between python and ruby for me to invest time to learn. Now I need to decide which one....
&gt; I'm a self taught programmer, who has been learning programming and Linux for just over 4 years now. Nice. Four years, you're obviously smarter than me to be able to do this in such a short time frame! And another one for python. Thanks for sharing.
It's been a while since I had to touch it, but I'm pretty sure SunOS 5.10 has/had `/dev/fd`.
Yeah its powerful. I once made an imgur clone, you could upload pictures to the website, and it would automatically convert the image with some neat frames that made it suitable for use on forums etc. You could log in, change a few setting, upload stuff and view/admin your upload, Even cookies was all bash. The only thing i couldnt make work was how to handle the upload itself. Never figured that out, so I ended up doing the upload code in php. But honestly, bash isnt really made for this type of stuff. Other languages are more suitable,but it is possible to do lots of weird stuff in bash.
Aye - I missed out the check for empty parameters in my later version to save getting a `Missing side 4` if there *were* 3 arguments but the third was empty.. Looks like I'm not gonna save a lot of space checking each argument for nulls as well ... for E in 1 2 3; do : "${!E:?Missing side $E}"; done Ach well, back to the day job....
&gt; For example, you can script something that runs every 3 seconds (presumably via a loop with sleep 3) There's a bad assumption hiding in that sentence that will some day come back to bite you. Using `sleep 3` will not have the job run every 3 seconds, it will have the job run every X+3 seconds where X is how long the operations inside the loop take. For example, if your operations take a whole second every time, using "sleep 3" means your job runs every four seconds. If the time of your operations varies a lot, it gets hard to predict exactly how many times your job will run within a given timeframe. 
Love me some Awk Golf: awk '{Data[$1]+=$2;c[$1]++} END{for(i in Data)print i, Data[i], Data[i]/c[i]}' input
&gt; For example, if your operations take a whole second every time, using "sleep 3" means your job runs every four seconds. If the time of your operations varies a lot, it gets hard to predict exactly how many times your job will run within a given timeframe. As always, there's a horrible workaround for that if you don't mind a couple of extra processes to calculate a sleep delay for you. while sleep $(dc -e "60 $(date +%s.%N) 60 % - p"); do : #something done Will run at as near as possible to the top of every minute (tested on my laptop just now, about 7-9ms delay), adjust the two "60" values to your choice of time interval. ``ps`` output running the above: USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND xxxxx 198744 0.0 0.0 6180 772 pts/2 S+ 10:24 0:00 | \_ sleep 59.988702868 Now how accurate ``sleep 59.988702868`` is in reality I couldn't tell you, but it seems to serve the purpose well enough. Close enough for government work, certainly.
C is taught at a very entry level and basic....It all become OOP, mostly C++ C is a good concept language but is poor in 2019.
so this isn't the answer, what about searching your history for your commands? I'm on linux, but I'm gonna take a leap and presume these commands will work the same way * `history` it will cat your history file to stdout * `history | grep "search term"` this will send the lines where you used the search term to stdout * `history | grep "search term" | tail -n 2` this is like above, but will limit it to the last two commands &amp;#x200B;
Thank you for your answer, nonetheless. This is probably the standard/ a good way to do it. I just feel that there should be a way to remove the stuff you don't want from the terminal buffer. I wonder if it could be coded, somehow.
oh... and I should mention if you do want to be Homer Simpson and continually enter y's into your program, your command would be `yes | command`
Not an answer for your question, but if you want to jump to previous commands, your terminal emulator probably has a way to search for some text. You could search for some unique text from your prompt; each result will take you to each instance of your prompt in the scrollback, allowing you to skip the `y`s.
Usually, the terminal buffer is "read only". In quotes, because there are other commands and libraries like ncurses that can work with redrawing the display. To have the terminal work the way you want, you may need to use a different shell, or a different terminal application, or both. 
The only thing you can do is look into what kind of features the terminal emulator you are using has. The terminal emulator is the program where the scroll buffer is living and that interprets things like Shift+PageUp keys or mouse wheel events to show different stuff in its window. I think for example urxvt has a feature where it can be extended by Perl scripts. Maybe it would be possible that a script like that has access to the content of the scroll buffer and tries to understand where command prompts are in the scroll buffer, and then delete the lines between two certain prompts. The script would look for certain codes you print with bash and printing those would be how you could try to tell it to do something.
&gt;the Linux Server Security book Do you mean [this one](https://www.goodreads.com/book/show/864310.Linux_Server_Security)? Thanks.
&gt;trying to figure out how to work around awk's single quote requirements getting in the way of shell If it gets annoying to work with complex awk snippets due to having to shell quote the whole thing, you can put these in a separate file just like you can with Python and Ruby: $ cat foo.awk #!/usr/bin/awk -f BEGIN { print "String with 'single' and \"double\" quotes" } $ chmod +x foo.awk &amp;&amp; ./foo.awk String with 'single' and "double" quotes
This is a *Linux by* ***Wiley*** bundle, so I mean [this one](https://www.goodreads.com/book/show/28767498-linux-server-security).
I see. Correct me if I'm wrong, but it sounds like that this is something that would need to be coded in something other than bash.
While I also typically search history with a combination of `history` and `grep`, bash supports a history search feature via ctrl-r and ctrl-s. Dunno if that helps you or not.
Thank you.
That's correct. There is a terminal, that was electron based, that partially addresses your problem. The way it does is when long output is detected, it just decreases the font size (of the output) so the previous command and current cursor stay in the viewport. That way the amount of scrolling back is reduced. While still keeping the text content. It can be zoomed in when you need to look through it or search for text. I can't remember the name of the application. I'll update when I find it.
u/Send_me_Your_Code, you'd have gotten an answer almost instantly if you'd posted to any of the *Mac-specific subreddits* like r/macos. Just sayin'. In any case, u/ropid's on the right track, in that it really depends on your terminal emulator, not bash. Specifically, the stock macOS Terminal has a feature called *Marks* that, you guessed it, marks specific lines in its scrollback log, and there are keyboard shortcuts to jump to each mark in the scrollback log...or clear them. Look under Terminal's **Edit** menu for all the good stuff, and search its help for "Use marks and bookmarks" to see what it all means. Since you seem not to know about this feature, I'm betting that you'll find **Edit &gt; Marks &gt; Automatically Mark Prompt Lines** already checked, so Terminal automatically lays down a mark *at every bash prompt*. I'm also assuming that you haven't changed any of its keyboard shortcuts, so to clear the output of the just-executed `yes` command without affecting anything else, simply hit `Cmd+L`.
Thank you for your response. Perhaps I don't see "Marks" because I'm using OS X 10.7 (Lion)? 
Thank you!
Interesting, thank you!
It should be noted that ctrl-s also pauses flow control, which will result in a terminal which appears to be unresponsive. Use ctrl-q if that happens.
Ah. I believe Terminal marks appeared in El Capitan (10.11), so you're out of luck.
I don't even have the `Cmd+L` functionality that you mention. However, I do have `Ctrl+L` functionality, which seems to work like executing `clear`.
ctrl R is a reverse search try it! &amp;#x200B;
Well, `Cmd+L` is Marks territory, so there's no surprise there. As for `Ctrl+L`, that's actually handled by your shell, which consumes it and spits out the appropriate sequence of bytes to clear your terminal, pretty much exactly what `clear` does. For instance, search the [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html) for the **clear-screen** readline command.
An easier workaround: at the beginning of each loop iteration, launch `sleep` in the background like sleep 3 &amp; (which has negligible overhead) and at the end of the iteration, wait until 'sleep' has finished: wait 
This works provided your something runs quicker than the interval. For example, if something took 65 seconds to run, it wouldn't run every minute. While you would need a quite long running program to have that be a problem, it becomes more likely for the OP who wants to run every second.
You're absolutely right, need to add a `END{ exit $foundColon; }` to the end of the awk script. On mobile or I'd post the whole thing. 
&gt; sleep is negligible in bash, it's a builtin Note that it's a loadable builtin, though; it isn't enabled by default. You can usually (the path might vary) enable it with `enable -f /usr/lib/bash/sleep sleep`.
The below soln is what I came up with for finding avg and maximum . Wanted to know if they could be nested &amp;#x200B; awk '$2 &gt; max\[$1\] { max\[$1\]=$2 ; maxline\[$1\]=$0 } END { for (i in max) print maxline\[i\] }' awk '{ sum\[$1\] += $2 ; count\[$1\] += 1 } END { for (i in count) { print i, sum\[i\] / count\[i\] } }' &amp;#x200B; &amp;#x200B;
Thanks a lot ...Cheers
Like this: awk '{$2 &gt; Max[$1] &amp;&amp; Max[$1] = $2; Avg[$1] = ($2 + Avg[$1] * C[$1]++) / C[$1]} END{for(n in Avg) print n, "Max:" Max[n], "Avg:" Avg[n]}' file
Thanks mate
You're welcome. Feel free to pm/tag me with any Awk questions; I appreciate the workout.
can you narrow it down to a single line?
I did, though I guess I could clarify. I said the verify section, which would be... `verify="$(zenity --entry --no-wrap --title="ffmpeg" --text="This is the current command that will execute.\n$cmd\nAre you sure you want to continue? (yes/no)")"`
I've never used `zenity`, but from looking at your script and trying it on my machine, it works if you remove the `--no-wrap` option. I tried that since that was the only difference between the verify call and the previous calls.
Thank you. That makes sense now, I don't know why I didn't think about that. The verify call was originally meant to just let you know what the command was, then I figured it would be better if it was a question for in case the command wasn't what was wanted. I don't get why it said `--(null)` though. Usually when you mess up a flag it'll specify which flag is wrong and why. Maybe a bug on zenity's part? Also, I don't usually use zenity either. I'm really only using it because I made a systray icon in Python, and I was going to make an entry that calls this script. Anyways, thank you again. No idea why I didn't think about that.
python
why ??? can u give me more information ???
its good !!!
I also say python. Its a rather natural progression from bash, and has a just-working-ness about it.
ohhh ok thats a good explain but there any other language ?? except python
It depends, what do you want to do with it? More scripting? Python is a great general purpose language, especially for scripting, but also pretty much anything else you might want to do.
Python all the way :D
It really depends on what you want to do: * Python: For ease of use and scripting. * JavaScript, CSS, and HTML: If you want to do webdev. * C: If you want to make your own UNIX applications and learn about pointers. * JAVA or C#: If you want to learn a big Object-Oriented language. * Haskell or SML: If you want to learn functional programming Python is a good start if you want a beginner friendly and powerful starter language. 
I guess you want to do dis: '"${variable}"' ↑ `$variable` is in bash env but not in awk env so I # terminated the first quote # put var enclosed inside double " (habit nice to have) # re-opened quote to concat the rest of awk command hth 
`a=()` is an array assignment, while `a=b` is a variable assignment. So the former sets each element (determined by `IFS`) to an array indice while the latter basically sets the first element to it all.
Thank you! Is there a particular way to know when bash will interpret it as a subshell vs an array assignment?
&gt; I know that () creates a subshell not always.... run echo "myArray=($(cat file))" and see what your command is expanded to... 
 if ($i = $variable) Need == here for comparison
&gt; Thank you! Is there a particular way to know when bash will interpret it as a subshell vs an array assignment? This is hard to answer both accurately and precisely. The first thing to remember is that `(` and `)` are metacharacters, so their interpretation _depends upon their context_. In particular: * If `(` is encountered at the point at which a command would be parsed, it is treated as the `( LIST )` compound command, where `LIST` is executed within a subshell. * If `(` is encountered immediately after the `=` in a _variable assignment_, it is treated as the beginning of an array. Variable assignments are permitted in the following contexts: * Preceding a command: a=foo b=(1 2 3) some command * Without any command: a=foo b=(1 2 3) * As an argument to the `export`, `local`, `readonly` or `typeset` builtins: local a=foo b=(1 2 3) These are the _only_ places in which `(` would be interpreted as the beginning of an array. `(` has several other users &amp;mdash; again, dependent upon its context &amp;mdash; but in most other places it is a syntax error. You cannot do this, for instance: some command a=foo b=(1 2 3) 
This is absolutely the kind of thing I was hoping to see. Thank you!
What programming language to use is VERY subjective to your experience and frame of reference. Another thing to consider is what your goal is with the finished projects, ie scientific research, system administration etc. One thing you should always keep in the back of your mind no matter what you do: KISS (Keep It Simple Stupid). As in, don't overthink things and use the least cumbersome and the most time-efficient way to do things. While I personally know quite a few programming languages, the most useful to me for what I do are: shell scripting (bash), Python and Rust.
You stunning beacon of light among humanity. Thank you, thank you, thank you. 
So I did. Thank you!!!!!!
I did shell scripting and some perl earlier in my career and then moved to python and use it for most everything now. I only use bash for tiny little scripts now. I like the syntax, the available libraries, and its depth of capabilities. Plus its not a bad language to know since it is pretty popular, and so there are plenty of jobs out there for it. But as others have said, it really depends on how you want to use it. Are you doing sysadmin type work and living on the commandline, then python is a simple fit since it is widely used in that space. If you are just a hobbiest, and want to get move into programming, then you have to figure out what you want to build next and what language would be most appropriate. 
&gt; urxvt I think this is the workaround. Some piece of code/logic would need to differentiate between one command prompt and another and remove previous lines. Do you have any idea how I could attempt this? Or what I could search online?
Go back to that other thing about Apple's default terminal program and "marks" that's not working on your OS X version: I think there's a third-party terminal program for OS X that also has the same feature. I think it was this here: https://www.iterm2.com/
To be safe, I'm using cp (and linking if they're on the same fs): shopt -s globstar for f in dir/**; do [[ -f "$f" ]] &amp;&amp; cp --reflink=auto -n "$f" "$(sha256sum "$f").${f##*.}" done
Jesus Christ. Thanks. Does this move it out to the working directory?
It will copy it out, but will use a hardlink if possible (don't move data, simply add another pointer to it) 
Thanks. I have backups and a massive amount of time.
If you know where the files are going, add the path in the cp command: cp --reflink=auto -n "$f" "/path/to/dest/$(sha256sum "$f).${f#*.}" I also added the `-n` option (no-clobber), so it won't overwrite files with the same name. Use that option if it makes sense for your usecase.
Thanks. You're like 100 times better at this than me.
np, flair as solved if everything looks good
Okay. I'll get to it in like a half hour.
I just remembered something: If you have any hidden files, add `shopt -s dotglob` as well.
It's not working. How am I supposed to run this?
Oh, I got around to a terminal, and I realize now that sha256sum outputs the filenames as well. shopt -s globstar dotglob for f in /source/dir/**; do [[ -f "$f" ]] || continue new="$(sha256sum "$f")" new="${new%% *}.${f##*.}" cp --reflink=auto -n "$f" "/dest/dir/$new" done 
It's still not running. How do I run this? I'm just appending #!/bin/bash
`bash script.sh` Or, `chmod +x script.sh`, then run it directly with `./script.sh`
Oh, i'm dumb, I forgot I had to change the path. But now it's saying it can't append the file. &amp;#x200B; cp: cannot create regular file '\~/New/\[checksum\].markdown': No such file or directory &amp;#x200B;
`mkdir ~/New`?
It's there... IDK... &amp;#x200B; Thanks again for all your help. Your script works perfectly otherwise.
Is the `~` in quotes? it will be used as a literal tilde, rather than be expanded to /home/user. Use `"$HOME/New/$new"` instead.
YES!!!
NICE!
Replace ` new="${new%% *}.${f##*.}"` with: ext="${f##*.}" new="${new%% *}.${ext,,}"
Thanks so much. You're amazing.
If you want to see why I used what I used, check out the bash reference manual. If you're on Linux, iputting `file:///usr/share/doc/bash/bashref.html` into your browser should get you there. These two sections I always keep handy, they are incredibly useful: - **3.5.8 Filename Expansion**: `globstar`, `dotglob`, `**` - **3.5.3 Shell Parameter Expansion**: `${foo##bar}`, `${foo%%bar}`, `${foo,,}`. 
That still won't work. That's gonna try to give you `$17`, e.g., positional parameter number 17.
This is what you need: awk -v variable= 17.75157738 '{ for (i=1; i&lt;=NF; ++i) { if ($i == variable) print i } }' $file
Aw, thanks! Make sure to check out the reference manual I linked, there's a lot of good things there. Also, this [pure bash bible](https://github.com/dylanaraps/pure-bash-bible).
Perl5 is pretty easy to learn if you already know Bash. It's very powerful and expressive. But there are dark corners to beware of in terms of syntax.
You probably know this, but `${!Array[@]}` lists the indexes of an array. I often iterate over the indexes of an associative array with regex comparison `=~` to get information about "sub-arrays". If you have access to GNU Awk, it's got true arrays of arrays baked in.
IMO bash scripting should be kept relatively simple. If you’re needing to do stuff with multidimensional arrays it’s time to consider shifting your script to something like Python...
&gt;What problem are you solving? boring with repetive code /s I do things in bash that i should do in python instead, but the fact is that i dont know python nor any uninterpreted programming language. Also I like challenge of doing such things though unencouraged. Right now i'm trying to take some advantage of the wikidata lexicographical service and try to "guess" wd:queries from natural language. I'm on the "guess how many values in this dimension" part, but im befriending ksh! ksh has all the multidimensional functionality i need (or that I think).
yay 😄
I see this answer plastered all over this sub and I find it very frustrating. How can we evolve if we don't push the envelope?
If you post your existing shell I will attempt it in Bash.
&gt;existing shell what? I dont understan that part. do you mean my actual shell? is bash but im on the way to migrate to ksh. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Post your code, please.
As it is right now it does gammatical analysis of every word of the input: #!/bin/ksh #. /opt/bin/extArrays.sh #array lexeme args=0 for x in $*; do #salida de prueba #feo salto de linea echo $x lexeme[$args][0][0][0]=$x args=$(($args+1)) ###Análisis gramatical #BUSCAR ITEMS CUYO LEMA COINCIDE CON CADA PALABRA DENTRO DE LA CADENA DE LA ENTRADA #Idioma Español Q1321 ### No hay muchas entradas #Inglés 1860 #parser SPARQ echo 'SELECT ?desc ?lLemma ?featuresLabel ?formLabel ?categoryLabel WHERE { ?l dct:language wd:Q1860; ontolex:lexicalForm ?form . ?l wikibase:lemma ?lLemma . ?l wikibase:lexicalCategory ?category . ?form ontolex:representation "'$x'"@en . ?form wikibase:grammaticalFeature ?features . SERVICE wikibase:label { bd:serviceParam wikibase:language "en". } } limit 100 :'|qserv 2&gt;/dev/null |tail -n+3&gt;/tmp/info"$args" lexeme[$args][1][0][0]="/tmp/info"$args"" #Resultados de consulta por palabra. isense=0 for y in $(cat "${lexeme[$args][1][0][0]}"|cut -d, -f4|uniq); do # 100 lexeme[$args][2][$isense][0]=$y #| Divide la consulta por formas #Asocia una o mas formas a una única palabra #Ej, ver y visto son formas distintas del verbo ser lexeme[$args][2][$isense][1]=$( grep $y $( echo ${lexeme[$args][1][0][0]})| #Concatena los aspectos gramaticales cut -d, -f3 #de cada forma, ej el AG de la FG, visto es pretérito ) #lexeme [$args][2][$isense][0] # Una prueba echo -n ${lexeme[$args][2][$isense][1]} // isense=$((isense+1)) done ######Salto de carro: echo done &amp;#x200B;
look upwards.
Wow. Challenge accepted; don't hold your breath.
AFAIK bash does not support multidimensional arrays. How are we supposed to answer a question like this?
like you said i just want to start with simple and easy to use language and by time ill try something else
i just want to learn it maybe will help e in the future thats it and i feel kind of bored of bash and want to try and nice programing language
Bash is inherently not as full featured or easy to use *as a programming language* as Python or other common scripting languages, but equally I wouldn’t want to try and use valid Python as my shell, for example. We use plenty of bash as glue to join multiple other small specific Python, Ruby, Node, Go etc scripts together. Some pretty simple things are written purely in bash. But anything where I can write some thing clear and simple in a few lines of Python, or a bash equivalent which is more error prone, harder to test, harder to understand, probably longer? I’ll go to the Python every time. Anything much longer and more complex where I want to have tests etc, Python is easier every time. If more functionality is built into bash such that I can do something just as quickly and easily using that then sure, I’ll start doing those things in bash. But I’m not going to tie myself in knots and make it harder for the people coming after me to understand WTF is going on just so I can say “Look aren’t I clever, isn’t bash powerful, I managed to do all *this* purely in the shell”. I *could* hammer in a nail with a pair of pliers, but that doesn’t mean it’s the easiest or best tool for the job. Half the time my issue is that it’s really easy to install a scripting language and a library and write code to do what I want using that lib. Finding a CLI utility that allows the same functionality is a massive pain in the neck, if not sometimes impossible. As a very basic example parsing JSON to a Python map and then pulling stuff out of it, manipulating it, writing it back out. Trying to do anything remotely complex like that using `jq` or similar is an exercise in frustration. If I’ve got a bash script that gets longer than a single terminal page, requires anything more than basic vars, single-dimension arrays, and maybe one loop I tend to rewrite it in Python and it’s always clearer and simpler, easier to extend and enhance etc, even if the bash version *worked*.
At some point, you have to acknowledge that you've reached a language's limits. Bash does many things great, but it doesn't do this. So what do you do? Either redefine the question (i.e. solve the problem while working within the limitations of the language) or move to a language that does what you need it to do. In this case, the OP may have envisioned a solution that seems optimal, but if he's unwilling to learn a language where his solution can be realized, he'll have to come up with another way of getting where we wants to go. To answer your "how can we evolve" question: by being flexible. You can't have rigidity and evolve. To say we have to solve problem X by using only one tool and we can only implement that tool using one language isn't very flexible. 
Read this very carefully: https://www.reddit.com/r/bash/comments/aziuff/_/ei9b2tb No matter what the language, when you're programming for someone else (even if that someone else is you in 3 months after you forgot what you were thinking when you wrote a program) the name of the game is clearly expressing your intentions with code that a computer could also run. The computer will understand any valid program, no matter how cryptic out clear. But if the result isn't clearly expressed, you'll quickly find that **pushing the envelope** will lead to a solution that you'll be forced to rewrite instead of maintain and reuse over the long run. This is why it's best to choose a language that helps you best describe the problem you're trying to solve. For example, Bash is exceptional where the solution to your problem requires you run a bunch of external programs. Where the solution requires a bunch of in-memory processing of structured data, there are a bunch of other languages where that can be much more clearly expressed, giving you an easier go in 3 months should you need to fix a bug or have someone else help maintain it.
English. First learn that. Then maybe Python..
Evolve bicycles to be tractor trailers? If you have an existing script, and you *really* need to do this to fix it, sure, go ahead. But you evolve to use Bash for what it was meant for, job control. This means using temporary files with trap EXIT signals for cleanup, using flock to prevent process contention, or launching future one time running processes using at/batch. If you need a lot of data structure heavy work, you look to other languages like Perl, Python or my favorite, Ruby. Bash is *intended* to launch other specialized processes anyhow.
haha thank you so much for your replay 
Then I think python could work for you. It's dynamic like bash, and is a pretty nice language to work with. 
I feel like this line of thinking only applies if you feel that bash *should be* a highly featured language. Personally I think shells are and should be little more than user interfaces. Outside of string manipulation, basic arithmetic, evaluating conditions and loops, trying to do or add anything else to a shell feels very square-peg-round-hole.
Python is probably your best bet then. If you're looking for a resource, I really like 'Automate the Boring Stuff with Python' https://automatetheboringstuff.com/ It's well organized and gives lots of interesting example projects. 
thank you so much you help me a lot 
thank you so much i think im going with python 
ok thnak you so much you help a lot 
hahah that what other guys say
cheers - best of luck. check out /r/learnpython and r/python as well. 
Here's a sneak peek of /r/learnpython using the [top posts](https://np.reddit.com/r/learnpython/top/?sort=top&amp;t=year) of the year! \#1: [Why study programming when you can just play an RPG?](https://np.reddit.com/r/learnpython/comments/aw0au0/why_study_programming_when_you_can_just_play_an/) \#2: [Al Sweigart, author of the legendary 'Automate The Boring Stuff' book (and many others) now streams beginner-friendly live coding on twitch!](https://np.reddit.com/r/learnpython/comments/ao3zq2/al_sweigart_author_of_the_legendary_automate_the/) \#3: [Just spent 2 hours to automate joining csv files saving hundreds of hours a year](https://np.reddit.com/r/learnpython/comments/a085dx/just_spent_2_hours_to_automate_joining_csv_files/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
Try grep(ing) lines that start whith "Port" or "a number", then use sed or tr to translate spaces and "|" by a comma.
And this answer [https://www.reddit.com/r/perl/comments/b1hgme/expect\_script\_regex\_how\_to\_format\_cli\_output/eilow44/](https://www.reddit.com/r/perl/comments/b1hgme/expect_script_regex_how_to_format_cli_output/eilow44/) is very simple to understand how Perl regexes work, neat.
cli_output | sed -n '/^M/d;/Port Type/p;/[0-9+] 100\/1000T/p' | awk '{ print $1 "," $2 "," $4 "," $5 "," $6 "," $7 "," $8 "," $9 "," $10 "\n"}' | head -n -1 
@notthatbigbrother - Great cheatsheet. Since this is an expect script, how do I get it working in my current script? How do I call it? Here is an example that isn't working, yet. ?? &amp;#x200B; `#!/usr/bin/expect -f` `set timeout 20` `spawn ssh -l manager` [`192.168.1.10`](https://192.168.1.10) `expect` [`"manager@192.168.1.10`](mailto:"manager@192.168.1.10)`'s password:"` `send "admin\r"` `expect "Press any key to continue"` `send "j"` `expect "sw**"` `send "conf\r"` `expect "sw*"` `send "no page\r"` `expect "sw*"` `send "exit\r"` `expect "sw*"` `log_file cli.log` `send "show interface brief\r"` `expect "sw*"` `log_file` &amp;#x200B; `set cli_output cli.log` `cli_output | sed -n '/^M/d;/Port Type/p;/[0-9+] 100\/1000T/p' | awk '{ print $1 "," $2 "," $4 "," $5 "," $6 "," $7 "," $8 "," $9 "," $10 "\n"}' | head -n -1`
thank you so much
Sorry, but I don't have any experience with except scripts. Kind of new to bash. But a quick google search gave me this. https://unix.stackexchange.com/questions/275458/unable-to-execute-bash-script-inside-expect-script Maybe putting this in a bash script and then calling it from the except script will work. Good luck.
No problems. I'm glad I could help. Your desire to learn more about regex sure implies you're doing a little more than just running programs ;). When I mentioned that Bash is great for running other programs, I literally mean that that's the point beyond which the value of Bash drops off...and I mean a severe drop-off. When it comes to manipulating data, things can get murky very quickly in a shell script. And although awk is pretty handy, there are more modern and just as ubiquitous options. And it all leads into that topic of expressiveness and code clarity. Maybe it would help to give you an example. This one doesn't involve `awk`, but bare with me: Let's say the file, `file`, contained the following numbers: ``` 10 9 8 7 6 5 4 3 2 1 ``` Without looking at a manpage or Bash reference; and without running it, start a countdown timer for 2 minutes, and in that time, lemme know what this program does: ``` #!/bin/bash sort -u &lt; file | sed -ne '10 p' ``` Also without a Ruby reference, give yourself another 2 minutes to lemme know what this program does: ``` #!/usr/bin/env ruby lines = IO.read('file').lines numbers = lines.map do |line| line.to_i end tenth_greatest_number = numbers.sort.uniq[9] puts tenth_greatest_number ``` Although you may not be comfortable with Ruby, I have a feeling you could wrap your head around what the second program was doing by just reading it. Try to think about why (or why not) and let me know your thoughts. If my guess was right, the reasons you provided are parts of the definition of expressiveness/code clarity. And one of the reasons why the value of Bash really drops off when it comes to data manipulation is the fact that that first script contains a bug. Can you find it? A real benefit of clearly expressing your intent when writing a program is that bugs become obvious. Since the shell script is kinda cryptic to begin with, the missing `-n` option to `sort` just blends in and can be easily missed...even while actively debugging! When a shell script needs to be maintained by people with various experience levels, nuances like this lead to misunderstandings that could lead to latent bugs that are pretty difficult to get to the bottom of. And when I say "maintained by people with various experience levels", I could mean you and a team of developers, or you right now and you a few weeks after being immersed enough in Bash to bang out your script. Hopefully you have a little better idea what I mean by expressiveness and code clarity. Oh, and hopefully you find that next language for you toolbelt. Try not to think about which language will restrict you into a certain track of learning. Instead, try and think of the language that can help you more easily express solutions to your problems; and the language that can help you solve larger problems than you can now with Bash. Life's too short to give yourself artificial restrictions for the sake of learning a certain style of programming, so choose the language you'll feel most comfortable with. Wow. Another monster comment. Hope it helps, and if you have any other questions, just lemme know.
You absolutely do not need multidimensional arrays for this. I'm almost done; could you please show an example of desired output?
As the layman I am, I don't understand much of the explanations given here. Can someone please show me a working solution for my problem? Thanks all!
Don't use regex to parse HTML (or XML) https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags
You could use something like this to process the xml using syntax like JQ. https://github.com/kislyuk/yq This avoids the trap that is processing xml with regex 
Use xmlstarlet to process xml files.
On the other hand, if you really realy want to "break the rules" and use regex because you know that the xml is formatted EXACTLY as you expect and you're only going to do it *just this once*, then something like this will do the trick: BEGIN { id = name = ""; } { if ( $0 ~ /^\s*&lt;entry/ ) { id = name = ""; } else if ( $0 ~ /^\s*&lt;name/ ) { split( $0, a, "[&gt;&lt;]" ); name = a[3]; } else if ( $0 ~ /^\s*&lt;id/ ) { split( $0, a, "[&gt;&lt;]" ); id = a[3]; } else if ( $0 ~ /^\s*&lt;\/entry/ ) { printf "%s:%s\n", id, name; id = name = ""; } } I printed out the results separated by a colon so that it's easier to parse in another pipeline. Also, you can test that it did what you thought it would do by grep'ing for lines that start or end with a colon (e.g., no name or id)
Okay, so I tested this out. If you have an XML document like this: &lt;xml&gt; &lt;entry&gt; &lt;name&gt;A&lt;/name&gt; &lt;id&gt;1&lt;/id&gt; &lt;/entry&gt; &lt;entry&gt; &lt;name&gt;B&lt;/name&gt; &lt;id&gt;2&lt;/id&gt; &lt;/entry&gt; &lt;entry&gt; &lt;name&gt;C&lt;/name&gt; &lt;id&gt;3&lt;/id&gt; &lt;/entry&gt; &lt;/xml&gt; Saved to `/tmp/test.xml`, then this command: xq -r '.xml.entry[] | "\(.id):\(.name)"' /tmp/test.xml Will Output the id and name fields colon-separated. Not that I had to use ".xml.entry" at the start of the xq command because my entire document is wrapped in `&lt;xml&gt;` tags. If your document is different then you'll need to adjust that. You can use `xq . /tmp/test.xml` to see the full output. Though for a huge file it might take a while.
Know your tools, it is the most important part of being a programmer. sed and grep regex might work occasionally for small files that have xml in a proper format. It is much safer to use a proper xml parser. Python has an excellent one called `ElementTree`. With that you can quite easily achieve what you want (check the [tutorial](https://docs.python.org/3.7/library/xml.etree.elementtree.html#tutorial)).
I get "out of memory" issues with large XML files 
`xq` may not work for you after all. I ran it against a 130M test XML file and it consumed nearly a gig of RAM. It also took 2 minutes to run compared to a mere 37 seconds for the awk script. You can generate a test XML using this: { tab=" "; id=0; echo "&lt;xml&gt;"; for i in {0..10}{0..100}{a..z}{a..z}{a..z}; do echo "$tab&lt;entry&gt;"; echo "$tab$tab&lt;id&gt;$id&lt;/id&gt;"; echo "$tab$tab&lt;name&gt;$i&lt;/name&gt;"; echo "$tab&lt;/entry&gt;"; id=$(( id + 1 )); done; echo "&lt;/xml&gt;"; } &gt; /tmp/test.xml
Thanks! I'm actually suprised sed or grep couldn't handle this Is it because multi-line matches are a big deal with large files? &amp;#x200B;
Someone already posted a link to a famous StackOverflow question. I suggest you take a look at some of the top answers there. In short regex grammar is less complex than xml/html grammar so it cannot be used to parse XML. That being said people have come up with incredibly complex regexes (see [this](https://stackoverflow.com/a/1736801/2116081) and [this](https://stackoverflow.com/a/5233151/2116081) answer) that may work quite well in some scenarios and as I said for some small well structured files when you just to scrape some data regex might be easier to setup. But for larger files which have highly heterogeneous formatting it is always better to use a parser designed for the task. 
thanks! you guys are awesome - i'll check out the parser really blown away by the quality of answers here on r/bash
This is an error from `bc` which means in the last statement for some cases $count stays as zero (probably because for loop is not running). Post a sample of `result1.txt` when you get this error and also what you are trying to do here and we can take a look and suggest some options.
This line: `grep -i processed result1.txt | awk '{printf "%s\n", $6}' &gt; result1.txt` Is both reading from result1.txt and writing to result1.txt. When you do that with an overwrite stream redirection operator (`&gt;`), you wipe out the file before you read it. I would instead do: `grep -i processed result1.txt | awk '{printf "%s\n", $6}' &gt; result_nums.txt`
&gt;grep -i processed result1.txt | awk '{printf "%s\\n", $6}' &gt; result\_nums.txt That solved the issue, i couldn't thanks very much for that. 
Have you tried xmllint? (I never tried it on large files.)
optimization: change `sum=$(echo $sum+$i | bc )` in `let sum+=i` &amp;#x200B; ...and avoid the division if `$count -eq 0` &amp;#x200B;
From [http://tldp.org/LDP/abs/html/arrays.html](http://tldp.org/LDP/abs/html/arrays.html) (that is a suggested reading): &gt;Bash supports only one-dimensional arrays, though a little trickery permits simulating multi-dimensional ones. &amp;#x200B;
korn shell 93 supports multidimensional arrays very well though. Solved.
Seems good, I will simplify it soon need to finish the rest of the script before I get into that. 
Probably not enough allocatable ram to parse the file.
Yes i'm running it on 2gb virtual machine - it can't work with a 1gb file??
It probably can’t because the program has to parse then lex which converts the tags and things into token which take up RAM, if you had like 5GB of ram then it may work.
You might find that SAX is better suited to your very large files than DOM. Python (and probably other modern languages) has a SAX parser. Take a look at [https://www.tutorialspoint.com/python3/python\_xml\_processing.htm](https://www.tutorialspoint.com/python3/python_xml_processing.htm) or try googling 'python sax'.
the term 'SAX' and 'ElementTree' came up before - but i'm new to Python The article you posted is looks very helpful How do I decide between SAX and ElementTRee?
Yes exactly
I'm posting this as a curiousity ;) I think yq or python would be advisable, but I've used sed/awk/grep for massive text files before. I saved the sample from here ([https://www.reddit.com/r/bash/comments/b1spf5/1gb\_xml\_help\_with\_sed\_or\_grep\_on\_ubuntu/eio1lid/](https://www.reddit.com/r/bash/comments/b1spf5/1gb_xml_help_with_sed_or_grep_on_ubuntu/eio1lid/)) to test.txt &amp;#x200B; `cat test.txt | tr -d '\n' | sed 's_/entry_\n_g' | while read line; do name=\`echo $line | sed 's_^.*&lt;name&gt;__g;s_&lt;/name.*$__g'\`; id=\`echo $line | sed 's_^.*&lt;id&gt;__g;s_&lt;/id.*$__g'\`; echo "${name},${id}"; done` `A,1` `B,2` `C,3` &amp;#x200B; This would probably be slow, but I'd be curious if it works at all. you can try it with a portion of your file - e.g. replace "cat test.txt" with "head -200 test.txt" to see how it would work initially.
Try with more ram and it should work.
Thanks for replying! 1. First removing the \n then ... 2. adding a \n ... (what do the underscores do?) then .. 3. looks like a loop stars, setting "line" to the entire line 4. now you're matching name, and id 5. now printing name and id What are the underscores and trailing _g? How do I write bash across multiple lines for readability?
Thanks for trying?
Thanks! Why do I see a lot of documentation not using the perl regex style? \[\[:space:\]\] instead of \\s Is it tradition or is perl regex style not compatible or problematic? 
Like everybody else said, you can't use regexp to match general xml. If you're certain your input is in a specific simple format, like in your sample, you can process it with stream tools. Try something like this (processes a ~800M file repeating your sample in about 30s): cat b.xml | sed 's/^&lt;entry&gt;/%&lt;entry&gt;/g' | tr -d '\n' | tr '%' '\n' | sed 's/^&lt;entry&gt;&lt;id&gt;//g; s#&lt;/id&gt;&lt;name&gt;# #g; s#&lt;/name&gt;&lt;/entry&gt;##g' &gt; b.txt 
sed 's\_blah\_qapla\_g' * s for substitute * \_=first char after the s is the delimiter * blah=regex or string to replace * qapla=what to replace it with * g=global, all instances of it &amp;#x200B; Putting multiple lines in a file or editor is probably the best way to go. if you're writing at the prompt, then having an open loop, or quote, pressing enter will just take you to the next line anyway. &amp;#x200B; cat test.txt | tr -d '\\n' | sed 's\_/entry\_\\n\_g' | while read line; do name=\`echo $line | sed 's\_\^.\*&lt;name&gt;\_\_g;s\_&lt;/name.\*$\_\_g'\` id=\`echo $line | sed 's\_\^.\*&lt;id&gt;\_\_g;s\_&lt;/id.\*$\_\_g'\` echo \^C{name},${id}" done &amp;#x200B; &amp;#x200B;
Wow, thanks! Why not just use the perl regex style?
Thank you for the example - I'm starting to see why using the command line is actually less than ideal
Np. I'm just used to it. Are you referring to -r for extended regex, or the more common use of '\\' as a delimiter, or using perl -pe 's/a/b/g' ? I tend to use sed often in areas where '\\' or '/' is common, so I hate using a slash delimiter. You end up with nasty confusing picket fences :P .... /\\\\\\/\\//\\\\/\\//\\/\\/ ..... ¯\\\_(ツ)\_/¯
Give [structured grep](https://www.cs.helsinki.fi/u/jjaakkol/sgrep.html) a try. It uses memory mapped io -- so it is not exactly streaming (like SAX), but tens of times better than tools that parse XML into DOM first, because all implementations of DOM are memory-inefficient due to excessive storage of additional metadata with the nodes. So you need to have a contiguous gig of free ram (that includes virtual memory, so if you have a large swap file -- it increases your chances). Also, if at first try it doesn't fit -- try rebooting your machine and run it again. Contiguous blocks of memory are much more available right after reboot, when the memory hasn't been fragmented yet by applications. 
Here's a Perl one-liner that should work on that large file of yours: $ perl -nE 'BEGIN{ $/ = "&lt;/entry&gt;" } for (m{&lt;entry&gt;(.*?)&lt;/entry&gt;}sg) { ($name) = m{&lt;name&gt;(.*?)&lt;/name&gt;}s; ($id) = m{&lt;id&gt;(.*?)&lt;/id&gt;}s; say "$id $name"; }' testfile 1 A 2 B 3 C With line-breaks added to it, it looks like this: perl -nE ' BEGIN{ $/ = "&lt;/entry&gt;" } for (m{&lt;entry&gt;(.*?)&lt;/entry&gt;}sg) { ($name) = m{&lt;name&gt;(.*?)&lt;/name&gt;}s; ($id) = m{&lt;id&gt;(.*?)&lt;/id&gt;}s; say "$id $name"; } ' testfile The example file I used was the one someone posted here in this thread. With this type of code you see in the 'for' loop, I usually let perl read the whole file into memory as one large multi-line string. For this problem you have here, I tried to make it read it in chunks. That's done by this part here: BEGIN{ $/ = "&lt;/entry&gt;" } The `$/` is the "input record separator" that controls how Perl reads from a file. It will read multi-line strings with the file split by where there's `&lt;/entry&gt;` text in the file. About the code that's then run on each chunk from the file: The 'for' loop searches for the content of `&lt;entry&gt;` tags: for (m{&lt;entry&gt;(.*?)&lt;/entry&gt;}sg) { ... } Inside the `(.*?)` part of the search result, it looks for `&lt;id&gt;` and `&lt;name&gt;` tags. The `(.*?)` part of pattern is saved in variables: ($name) = m{&lt;name&gt;(.*?)&lt;/name&gt;}s; ($id) = m{&lt;id&gt;(.*?)&lt;/id&gt;}s; Then it prints those variables: say "$id $name";
/r/bash is amazing, thank you guys! These are all very interesting answers
Are you processing this on a regular basis? If not just upsize the VPS...
I could - but i'm thinking I could run into problems later if i'm running into memory issues with a 1gb file
What about provisioning a large swap? Does the time that it processes matter? 
&gt;swap No it doesn't - are you suggesting using the SSD as RAM drive? (sorry - it's all i can think of)
[https://askubuntu.com/questions/1104017/how-can-i-sent-sms-mobile-text-message-from-bash](https://askubuntu.com/questions/1104017/how-can-i-sent-sms-mobile-text-message-from-bash) &amp;#x200B; [https://www.textmagic.com/docs/api/shell/](https://www.textmagic.com/docs/api/shell/) &amp;#x200B; [https://www.quora.com/How-does-one-send-a-text-message-to-a-cell-phone-from-the-Linux-command-line](https://www.quora.com/How-does-one-send-a-text-message-to-a-cell-phone-from-the-Linux-command-line) &amp;#x200B; [https://www.twilio.com/labs/bash/sms](https://www.twilio.com/labs/bash/sms) &amp;#x200B; [https://www.omgubuntu.co.uk/2017/04/indicator-kde-connect-send-sms-autocomplete](https://www.omgubuntu.co.uk/2017/04/indicator-kde-connect-send-sms-autocomplete) &amp;#x200B; [http://www.sms4mail.com/smsmail/smscmd2.htm](http://www.sms4mail.com/smsmail/smscmd2.htm) &amp;#x200B; [https://www.developershome.com/sms/gnokiiIntro.asp](https://www.developershome.com/sms/gnokiiIntro.asp) &amp;#x200B; [https://www.linuxjournal.com/content/system-status-sms-text-messages](https://www.linuxjournal.com/content/system-status-sms-text-messages) &amp;#x200B; &amp;#x200B;
In very short terms: ~ refers to your home directory Every ~/.bashrc is only used for that used, usually to load PATHs and environment variables when they login /etc/bashrc is global and applies to every user that logs in :)
A bit like why yesterday's newspaper doesn't automatically disappear after you go to the shop and get today's.... you have to throw it away - by `unset`ting it or by exiting your shell
Are you talking about `/etc/bash.bashrc`? Bash can be compiled with an option where this file is sourced just before `~/.bashrc` would be. Your operating system must have Bash configured in this way.
&gt; Every ~/.bashrc is only used for that used, usually to load PATHs and environment variables when they login "Environmental" settings are best applied in `~/.bash_profile` (or `~/.profile` if you want to be compatible with non-Bash POSIX shells), since it is only loaded in a login shell. One typically does _not_ want to modify the environment in a non-login shell.
[removed]
If you are in the same shell, then there is no way to know that you don't want a particular variable until you issue those instructions. The settings file you sourced is no different than a set of commands typed in by you. A="hello there" B="the time is" Later you decide that you don't want to use `B` A="HI" And you omit changing or referring to `B`, but the shell is not psychic. It doesn't *know* that you no longer want it around. You need to `unset B`. 
Rocket scientist
Rockentist. *** ^(Bleep-bloop, I'm a bot. This )^[portmanteau](https://en.wikipedia.org/wiki/Portmanteau) ^( was created from the phrase 'Rocket scientist' | )^[FAQs](https://www.reddit.com/axl72o) ^(|) ^[Feedback](https://www.reddit.com/message/compose?to=jamcowl&amp;subject=PORTMANTEAU-BOT+feedback) ^(|) ^[Opt-out](https://www.reddit.com/message/compose?to=PORTMANTEAU-BOT&amp;subject=OPTOUTREQUEST)
Boring bot
Linux Admin :)
There is a whole lot of funny in the top comment. FYI.
Others have already mentioned the startup files for bash but I'd like to point out that `/etc/bash.bashrc` is specific for Debian and Ubuntu systems. The equivalent file from other systems is for example `/etc/bashrc` for Red Hat. It is read before `~/.bashrc` for an interactive non-login shell.
Linux Admin/Engineer
Linux Engineer
Essentially, it’s slower, but if you don’t care it’s a considerably cheaper solution to running out of memory. 
No I don't care if 5 minutes becomes 10 minutes Do bash scripts time out the same way web servers do? Or do they keep running forever?
I’m a tools developer for a large tech company, supporting a worldwide team of mostly Frontend web developers, but also some systems people and other misc engineers. I target mostly macOS but probably 90% of my tooling is totally portable and will work on Linux or other posix platforms.
Software Engineer working on Platform as a service stuff. 
Cool! What do you mean by 'tools developer' ? Do you set up tools and services for developers that assist developers with their work? Such as a git server?
basically it starts out with a script that sets up their machine if it’s a new employee/vendor, making sure Xcode is installed, creating ash keys and walking them through getting them into github, installing and configuring necessary tools like node and docker and setting up their preferences for things like their email address and where they store their code. Then there’s tooling around checking out repos, adding git hooks for adding issue numbers to commit messages, launching their local preview servers which have very specific, custom configs. Things like that. I can’t go into more detail but there’s a lot of super complex processes around how devs access dependent code in order to work and I simplify that down to a single command. Any time I see a dev do something tedious that I can automate, I work with them and see if there’s something I can build to make their life easier. This has been my role (at least in part) at my last 3 jobs. 
I've no idea. I much prefer \s.. Much easier to write and easy to remember what it means 
Same here - thanks!
That's cool - is this also called "Devops"? 
When I first started, they described my role as “frontend devops” but I used to work in devops and I wouldn’t really describe my role as that. I don’t touch servers or the code deployment layer or any of the traditional devops stuff, but I do interface with their apis. I generally just call myself a “tools developer.” It’s taken me 10 years to realize that that’s my strength.
Thanks for your detailed reply - I have no exposure to your industry, so it's interesting to hear about what you guys do
I have some that run for several days if they are pulling a large dataset. 
I work in computational biology (networks, cancer and evolution) and frequently do computations on supercomputers in High Performance Computing Centers. Linux/bash is almost universally used in these areas. I have been working with Linux and bash for a while and have picked up a few things over the years. 
Wow, it's actually downloading for several days? Or extracting something? Or parsing?
That's cool! Thanks for replying, what's a "super computer" these days? I thought these were replaced by regular computers that are able to share the load with other computers?
Custom Cabinets and other wood work. The best tech I get to use at work is a CNC machine and a TigerStop (a programmable saw stop). I haven't used BASH at work yet, though I have written some Python to convert cut-lists into a format that works for us. Unfortunately, all of our machines use Windows for their controller software, and it's the only OS that anyone else even remotely understands (I mean they know that the mouse does some clicking, and a keyboard is for typing).
That's right, but they do that on Linux machines too
Supercomputers these days are mostly clusters of commodity hardware with a high performance, low latency interconnect. There are also some scale up systems, such as the HPE Superdome Flex. The definition of a Supercomputer isn't clear (I.e. min required size or speed) or when a high performance compute cluster becomes a Supercomputer. I tend to think in terms of a system which is pushed to the limits of the hardware to solve problems which are beyond the capability of a regular workstation or server. 
Bioinformatics and big data stuff
Oh, I work with roughneck carpenters. Most of them don't even own a computer. To answer your question, I would say yes, cabinet design software (we use Mozaik), and the CNC controllers that play nicely with that software, are mostly Windows. We use a rebranded WinCNC to control our ShopSabre 408 (it really is awesome). I talked to the shop owner about Linux one day because Windows updates are a constant headache, among other Windows-related problems, but there are no real Linux alternatives for Mozaik design software. I also have to support anything we use, and I don't want to train the whole company (only 4-5 people, but not tech-literate) to use Linux. I can imagine all of the calls I would get. I'd never get an off-day again. I can dream though, because Linux, BASH, SSH, and everything I love about Linux, would be a huge time-saver for us if it worked. The scripts I could write.. and no, I'm not gonna go down the PowerShell road. Not for this setup anyway.
No prob!
Yeah you're mostly right that the traditional centralized supercomputers have been replaced by more distributed architecture. The computers I run my simulations on are organized on a distributed network (grid) with some special algorithms to efficiently distribute load and use resources. There are large number of other architectures with some massively parallel centralized architecture with interconnects. These have become very much application dependent nowadays.
I do Linux distro development
`"${var//$'\t'/|}"`
Software Engineer in the bay area at a FAANG, 420k TC.
Currently in the financial sector on the service side, but have just applied for a job as a Linux Sysadmin for the IT dept.. Not sure if i get the job since they require i know oracle, and mysql, (my experience is limited to mysql), but hey, wont ever know if I never apply, eh ?
If you want to turn it into an array, just do it from the start: IFS=$'\t\n' arr=($var)
Extracting and writing to csv files from massive multi terabyte audit databases. 
`source` does not mean "reload" or "update" or anything like that. It basically means "evaluate everything in this file as if I wrote it into my shell right now" Obviously, if you write `foo=1` into your shell it will set the variable. However, if you *don't* write it, nothing happens -- no variables are unset through inaction.
I make Kubernetes developer tools at a startup, but I am also a core contributor to a GNU/Linux distro. When interviewing at companies, saying that I am extremely knowledgeable about Bash has generally gotten me a reaction like "that's wonderful, everything is glued together with Bash, but no one we have really feels comfortable with Bash."
&gt;Bash command line is so specialised Is it, though? In my view, Bash is to files and text what Excel is to numbers. 
If the sql tab delimited output is stored in a file called `output.txt` you can just do this to directly get an array while IFS=$'\t' read -r -a Var do #things with array Var done &lt; output.txt Or to read directly from command output use `done &lt; &lt;( command )` for the last line.
I’m a neuroscientist who works in neuro Imaging. We use a lot of tools that are run in high performance clusters, and also many of the major reproducible pipelines are done un bash so it’s kind of a go to in the field. 
Sys admin / consultant (switched to 100% Linux 2 years ago)
Fucking azure / windows admin because the company i interviewed for bait and switched me. Christ i need to go blast out some more resumes. 
GIS Data Analyst
I've had this happen twice. I can't understand why Linux Admin on a jobsheet might mean documentation specialist, programmer, webdev, windows engineer, etc.
Cloud Operations Engineer for a PaaS that is gaining a lot of footprint in the industry. Bash will always be my first love, although I write more python these days. 
linux kernel (storage drivers) support 
Another Linux sysadmin 😁
DevOps / Unix System Engineer
Linux engineer
Just a math GA, actually. I use Linux as my daily driver, and enjoy having that control.
You don't. You could pipe to cut, but that's more expensive than capturing into a variable, then doing parameter expansion. (Sidenote: `echo ${$(lsb_release -d)#*:}` *does* work in zsh.)
Senior network engineer
Thanks. I ended up doing this: db_get_movie_data() { while IFS=$'\t' read -r -a _data; do for i in {0..4}; do data[${movie_data[$i]}]="${_data[$i]}" done done &lt; &lt;(myq plexbot "select id, name, path, request, status from pb_movies where id=$id;") }
High Performance Computing Engineer. I build supercomputers for all kinds of uses. 
Interesting. Do you care to elaborate?
I like KDEConnect: kdeconnect-cli --send-sms "Hey, what's up?" --destination 5555550123 -n 'OnePlus X' I need to write a wrapper which lets me `fzf` through a contact list though. 
Discovering Bash reminds me of discovering cheat codes in my early video game experiences. Only it generates money and it doesn't get old.
I work on the experimental distro based on Arch Linux called (BlueLight)[https://bluelightos.js.org].
I think you'd be better of building your own packages (possibly with package scripts) using something like fpm.
/u/sudo_throw_, please let me know if you have any problems with or questions about the following. (for i in {1..5}; do sudo -u postgres pgbench -Sn -T3 -c100) \ |awk '/processed/~tolower($0){c++; sum+=$6} END{printf("%.2d\n", sum/c)}'
MySql DBA for a local private college
I work sys eng / help desk job in a small team of a transnational non conventional renewable power company, now biggest in my country... But want to move into DevOps, based in Azure
I'm surprised I haven't seen anyone answer that they're a high-end escort.
Another linux admin checking in.
It's possible your service provider is rejecting the email. From a cursory search for your carrier (tmobile) it shows you need to prepend the phone numbers with "1" for emailing. However, some results do say other wise. 
Just tried it and didnt work :,( thank you for the efforts though
Have you ever had it work before? In my country, you have to pay extra for this service (and on business accounts)
LITERALLY JUST GOT IT TO WORK. At first when I was configuring the "mail" command, I couldnt get it to work. But for some reason after I correctly configured "sendmail", the "mail" command started to work. For some reason tmobile doesnt block sms messages when you use the mail command
Software engineer. I got most of experience using Bash when I worked in porting and testing software to small embedded computers, because so much of what I did involved performing a hundred little tasks repetitively. Scripting was my only way to keep up with deadlines. I learned that while you can script most things, some things just cannot be scripted, especially when setting up a test bench for a unique piece of hardware. I still wrote Bash scripts for absolutely anything that I could. I currently work in artificial intelligence research at a start-up company. I don't write scripts so much anymore but my Bash skills still come in handy with day-to-day tasks, since I use the command line for pretty much everything that I do.
Nice work!!
Are you talking about a background process or something running in your shell or in a script?
It probably has less to do with the command and more to do with if a valid "From:" address is sent. If the from address is just "user", many mail servers don't accept that. Has to at least be in the normal format of user@domain.com , even if it's not real. 
In my shell!
Here is one way to do it. ~$sleep 10 &amp; [1] 84746 ~$while kill -s 0 $! 2&gt; /dev/null; do echo "Running" &amp;&amp; sleep 1; done &amp;&amp; echo "Finished" Running Running Running Running Running Running Running Running Running [1]+ Done sleep 10 Finished It's probably better to do something like this (but you don't get the constant status polling). #!/usr/bin/env bash echo "Running command" sleep 5 &amp; # do some other stuff if necessary if wait $!; then echo "Command completed successfully" else echo "Error running command" fi 
Use time or watch command.
If it’s running in the foreground, it won’t return until it’s finished running. So if you write a script, you can put the “echo $?” right after the command.
If you don't see a prompt at the cursor, the process is definitely running. If you want more feed back, you can echo statements at strategic points in the code - something like "checking for files done", "checking if the number is greater than 500", etc., BEFORE the actual bash statement. 
I believe that ElementTree sucks in the whole model into RAM and then processes it. SAX allows you to process the file as a stream so it should prevent your memory overflow problems.
You can use $PS0 to run a command before any other command is run, like this export PS0='$(echo "Running")' And likewise $PROMPT_COMMAND to run a command after any other command is run. PROMPT_COMMAND='echo "Finished" '
Sure I'll try that tonight. I have 2 more sections to finish (benchmarking against connection pooling and replication) once they're working I'll then start optimization. To be fair considering the scale it got to,270 lines, I will need to dial it back a bit 
If I understood correctly, what you want is `echo "Running"; { your command &amp;&amp; echo Finished; }`.
Do not use unquoted variables; that will leave it subject to glob expansion. The correct way would be `IFS=$'\t' read -ra var &lt;&lt;&lt; "$var"`.
&gt; The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. TIL, thanks.
[https://unix.stackexchange.com/questions/207210/make-pwd-result-in-terms-of](https://unix.stackexchange.com/questions/207210/make-pwd-result-in-terms-of)
This works fine, but I had to add something to remove the newline: `alias pwx="dirs +0 | tr -d '\n' | xclip -selection clipboard"`. Thanks!
In bash using prompt expansion: cwd2clip(){ local path='\w' xclip -selection clipboard &lt;&lt;&lt; "${path@P}" } In zsh using prompt expansion: cwd2clip(){ local path='%~' xclip -selection clipboard &lt;&lt;&lt; "${(%)path}" }
Your welcome. I had something else in mine. At least I lead you in the right the direction and your happy with your results. Glad to help out. &amp;#x200B; And I learn something about dirs. It's nice to learn something new as well. &amp;#x200B; [ftp://ftp.gnu.org/old-gnu/Manuals/bash-2.02/html\_node/bashref\_63.html](ftp://ftp.gnu.org/old-gnu/Manuals/bash-2.02/html_node/bashref_63.html) &amp;#x200B; I knew about popd and pushd. But now I have a different prospective of these commands. &amp;#x200B;
This is a cool question! Are you looking for curlresponse.txt to contain just the header information for all these URLs? or the actual HTML response?
Assuming the second column has only 25 or 50 or 100 , is there any way we can count the number of occurences of 25 or 50 or 100 and print output for every line like this Input Data-1 50 Data-2 50 Data-3 25 Data-2 50 Data-1 100 &amp;#x200B; Desired output Number of rows with 25 Number of rows with 50 Number of rows with 100 Data-1 0 1 1 Data-2 0 2 0 Data-3 1 &amp;#x200B;
If your goal is to get the value after Description in the output. $ lsb_release -d Description: Ubuntu 18.04.2 LTS The `-sd` should do it. $ lsb_release -sd Ubuntu 18.04.2 LTS &amp;#x200B;
That is my goal here specifically. Buy I find myself having to do similar things in other scenarios. So while the existence of the -sd option is useful I still would like to know if there is a better way to do: x=$(getstring) &amp;&amp; echo ${do something to x}
it should contain the original url and also the header info like this OriginalUrl HTTP/1.1 200 Server: nginx Date: Mon, 18 Mar 2019 07:35:11 GMT Content-Type: text/html Content-Length: 188 Connection: keep-alive Do you know how to do it ?
You can do something like this: parallel -j 5 'echo {}; curl -s -IL -k {}' &lt; urls.txt &gt; curlresponse.txt Parallel will start bash to run the stuff inside the `'` quotes. It will escape any special characters in the URL with `\` to make sure bash won't get confused. I tried this with imgur.com as one of the URLs, and the output of that looks a bit problematic. It seems curl gets redirected to a second server and will print two blocks of stuff. The URL will only show up at the start in front of the first block, the second block won't have it. You might want to somehow highlight where a call to curl ends or where it starts. You could perhaps do something like this: parallel -j5 ' echo -------------------------------------------------------------------------------- echo {} echo curl -s -IL -k {} ' &lt; urls.txt &gt; curlresponse.txt
has to be with grep? this is very easy to do with awk
I don't know a lot about awk, but I quess that's fine too as long as it works live with tail -f
Here is an option from me: [https://imgur.com/F6C0cf9.png](https://imgur.com/F6C0cf9.png) &amp;#x200B; `tail -f targetfile | while read -r line; do if [ "$line" == "a" ]; then echo "A triggered"; elif [ "$line" == "b" ]; then echo "B triggered"; else echo "Nothing found"; fi; done;`
you can do soemthing like this: tail -f input.txt |awk '$0~/abc/ {system("echo " $0 "&gt;&gt; match_found.txt")}; $0!~/abc/ {system("echo " $0 "&gt;&gt; match_NOT_found.txt")}' Being abc the regex patter you are triying to match. Also normally you would just `print $0 &gt;&gt; "filename"` but probably you don't want to do that in this case because awk is getting buffered input, so for efficiency it would wait until the buffer is full to write.
Here's using Perl: tail -n 0 -f testfile | perl -ne 'BEGIN{ $|++ } print if /hello/; print STDERR if /hey/' &gt;&gt; one 2&gt;&gt; two Here is the same again with line-breaks added: tail -n 0 -f testfile | perl -ne ' BEGIN{ $|++ } # this disables buffering print if /hello/; print STDERR if /hey/; ' &gt;&gt; one 2&gt;&gt; two Where you see `/hello/` and `/hey/`, that's the part that works like grep. You put your search patterns inside the `/.../`. At the end, you see `&gt;&gt; one` and `2&gt;&gt; two`. That's the two filenames where the output gets appended to. The `&gt;&gt;` is where the result of the first search goes to, and the `2&gt;&gt;` is where the second search prints to (the search where you see `print STDERR` in front of it). If you want to search for something that contains `/` characters, then `/.../` is annoying to use. You can then also write the search like this: m{hello} is the same as: /hello/
Looks pretty good to me. A couple ideas that might help you. * I always try to make sure that scripts like this will not duplicate data if they are run twice in the same period. Include period checks in your query or in your script. * Related to the previous point, I typically would not remove the daily files so quickly. At least one day history is useful to hang on to in case you have to troubleshoot issues. One way to do this is create an archive folder to which you move existing \*.tsv files as a first step. * I'm uncomfortable storing passwords in individual shell scripts. Better to have a single well-secured configuration file for this.
``` #!/bin/bash set -eu echo "$0 $@ [$$] START" &gt;&amp;2 if [ "${1:---help}" = '--help' ]; then echo "Usage: $0 src dst" echo "Example: $0 /tmp/12 /tmp/14.gz" echo "Info: copy with archieve" exit 0 fi cmd1 cmd2 cmd3 echo "$0 $@ [$$] SUCCESS" &gt;&amp;2 exit 0 ```
This works, even if it's somewhat artless :-) `while [ "$(ps aux | grep "$PROCESS_NAME" | grep -v grep | wc -l)" -gt 0 ]; do sleep 1; done; echo Finished`
bash is not the tool to solve this problem. Bash is just the shell. pacman is a package manager. apt (apt-get, apt-cache, etc..) is a package manager. Homebrew, MacPorts, or Fink (if that's still around? it's been a decade since I touched a mac) is where you need to look. But again - not bash related.
This works for me with GNU sed: $ tail -f input.txt | sed -n -e '/pattern/w match.txt' -e '//!w nonmatch.txt' Brief Explanation a) `-e` allows one to run multiple commands with sed b) `/pattern/w match.txt` writes the matched `pattern` into `match.txt` c) `//!w nonmatch.txt` writes lines not containing `pattern` (repeats automatically last pattern) into `nonmatch.txt`
`ls | while read line ; do echo $line | grep j &amp;&amp; echo J found: $line || echo no J $line ; done`
May be interesting to have it slightly tweaked along the lines of `[[ "$line" == *"a"* ]]` to include non-exact matches.
 tail -f file | tee &gt;( grep pattern &gt; matches.txt) | grep -v pattern &gt; non-matches.txt &amp;#x200B;
Retired now but 35 years of Unix/Linux, device drivers, crypto programming in C, devops/CI - small cog in very large corporate.
As another commenter has stated bash is a shell. Pacman is a package manager - for Arch Linux. You would typically use APT on a Debian or Ubuntu based system and yum on a red hat based system. You may have used brew on a Mac. All of these systems typically use bash as the shell. I'm afraid that I find your description of what you're trying to achieve quite unclear: what is the system that you are trying to install packages on? What package are you trying to install? I don't understand why you are trying to 'translate' any packages.
Looks promising, I'll try that!
yeap that's all true! this post is the product of my mounting frustration at trying to get my iterm2 emulator to display images in w3m. After changing the config to load up cacaview, cacaview threw the error of not supporting anything but Bitmap. I then searched it up and discovered that cacaview could display other file types and it's just really poorly configured. I then downloaded the sources (think that's what you'd call them) and ran the ./configure with the --enable-imlib2 which is supposed to enable other file types. This library probably isn't linked to my pkg-config so my cacaview was made the same as if I just put brew install libcaca and had some candy. At which point I went back to the web and discovered that "apparently" tycat from the terminology emulator can draw inline images or something. Well that was yesterday so I can't remember why I wanted to do that, but it has a two prerequisite slow dive so I started on that, and for some reason I was compiling it. I think I was compiling it before it wouldn't build terminology with meson build so I got to compiling efl and it was then that I realized that I've been useing brew and it hasn't been linking any of the libraries to pkg-config which is a big problem and is probably the criminal who has been stoping me from getting my beautiful cacaview working properly - and criminals belong in jail, people! I just need to call the export PKG_CONFIG_MANAGER=some/remote/thug/hideout/ and get these "prestoopniks loveted" Yes also my title for this post is misleading. tl;dr mac os x sucks!
https://asciinema.org/a/oNtFM5HBpr4Fi3TrnvAg1Y1v6 that's what I got so far
According to [this StackOverflow answer](https://unix.stackexchange.com/a/21213), you can simplify that to something like this: tail -f input.txt | awk '{ if (/pattern/) { print &gt; "match" } else { print &gt; "nomatch" } }' Taking into account the buffer comment, you can at least use the `if` statement to avoid having to evaluate the regex twice.
i would think would have to use something more capable than grep... with awk below, you can attempt to match against many expressions, one after the other - moving on to next input line as soon as any match - and apply special handling at the end for those that didn't match anything awk ' /pattern1/ { print &gt; "match.txt"; next } /pattern2/ { print &gt; "match.txt"; next } { print &gt; "non-match.txt" } ' file &amp;#x200B;
If the first thing you think of when you encounter a free service is ways to abuse it you're one of the bad guys.
Thanks!
Unfortunately, `grep` can't really produce two output streams from one. If you must use `grep`, that other response that combines `tee` and two instances of `grep` will be your solution...even if it's completely unmaintainable :) A more sane approach is to just use Bash: ``` while read -r line; do if [[ $line =~ match ]]; then echo "$line" &gt; matches.txt else echo "$line" &gt; non-matches.txt fi done ``` But depending on how much data you're consuming, a program written for a runtime with less overhead than a Bash interpreter may be an even better option. For example, Bash will re-compile that regular expression for every line of input, and cannot buffer more than one line of input. Most runtimes allow regular expressions to be compiled once and matched against several strings, and make it easier to buffer input depending on how quickly it can be consumed. Also, more modern languages may model the components of a streaming data a little more cleanly, allowing them to independently tested and maintained completely independently.
I think "brew edit libcaca" might be something to look at but I'm not super familiar with brew. Could always use sips to just convert to png. Make an alias to streamline the process
Those programs are written in C, and are not Bash builtins. They and `getopt` are part of `coreutils`, and as far as I know they do use `getopt`. They're part of POSIX so they should be available on pretty much any Unix-like system; just make sure that you stick to POSIX functionality and it should be portable.