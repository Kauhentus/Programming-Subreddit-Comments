&gt; no, not homework here.. its for work I think what /u/Mr_Queue means is that you are fishing for us to do your work for you because the result you want done requires nothing more than to read a few man pages and read up on the basics of how BASH works. In other words, your request is so elementary it's almost intellectually criminal to ask other people how to accomplish the utmost simple task you want to do because you obviously have not taken the proper steps to educate yourself. &gt; i know how to diff 2 files, grep tail etc.. just don't know how to put it together into a bash &gt;&gt;into a bash You do not put things "into a bash". You use BASH as an interpreter to run an order-based script to interpret commands for you. In this case you want BASH to run a command (diff) and output that command to a file (I/O Redirection). Go on my student. [Google is your friend](http://lmgtfy.com/?q=I%2FO+redirection+in+BASH). Best of luck, friend.
What he said for the most part. `# man ngrep|tail -n5|head -n1|sed -e 's/^[ \t]*//'` There are a few ways to get that... But that is some BASH
WTF is this even... what???
Also look at the `\D` switch as it does stroftime formatting. (instead of subshell for date) 
Why not play PS2 games instead? Time Killers is a good one.
ik.. but this... this... these colors and dis PS1............
Further evidence that I am not you, nor vice versa.
&gt; There is no verbatim text in the string. What do you mean? When posting to reddit, if you encapsulate text / code with back ticks, it won't attempt to turn it into reddit formatting or html or whatever. `Like this`.
Oh, duh. Right.
You can just do a simple [string manipulation](http://tldp.org/LDP/abs/html/string-manipulation.html) if you want - for example: $ temp="some &amp; text &amp; here" $ echo ${temp//&amp;/} some text here You can use this same method to insert an escape if that resolves your issue.
Can you describe your current process a little better? What are you reading through, line-by-line, and how are you utilizing `$line`? E.g. are you firing off `lftp` for each file in the list or building a script file for lftp?
McDutchie, thank you for your infinite patience. If you have yet to notice, this is my first post on Reddit; to have it answered so completely has rendered me speechless. Thank you, MD.
Lol.
See my code above. I'm calling lftp in LFTPDIR() and LFTPFIL()
EDIT: Ok I've figured it out half of it. I'm putting "\" before spaces with my awk pipes, but I should be doing that for '&amp;' as well. I'm just trying to figure out syntax for that now. gsub(/\\&amp;/,"\\\\&amp;") isn't quite right.
OK, so I guess this is harder than I thought. How does one gsub '&amp;' for '\&amp;'?
Your code above doesn't indicate what you are sourcing for your while / read loop. Is it a text file of directories and filenames you want to fetch from a remote host? Also, you need a `done` for your `while &lt;foo&gt;; do`
&gt; if [[ $remote_dir == *'/'$ ]]; then #Checks whether the filename is reffering to dir. &gt; fildir=d &gt; elif [[ $remote_dir == n ]]; then There's your problem. No quotes around $remote_dir, twice. Unquoted variable references can get expanded into multiple arguments or otherwise parsed, depending on their contents. Enclose the variable reference in double quotes, and you should be good. There are other cases, like `cd $remote_dl_dir`, that should be quoted too. If nothing else, it's good security practice. Especially if you read untrusted input from a file, *always quote your variables*. Just make a habit of it (except in cases where you know that the variable contents *should* be expanded into multiple arguments).
Excellent. I caught some of those unquoted variables and fixed those in the process. But after reading your comment, I checked and fixed all of them that may need it. Thanks!
Yep. So... CREATE_LIST () { touch $listfile lftp -p $port -u $username,$password $server &lt;&lt; EOF cd "$remote_dl_dir" cls &gt; $listfile quit EOF sed -i -e 's/^/#/' $listfile } Which creates the list file, populates it with the remote files and subdirectories from the remote location, then comments out all the files. When I run this script as a whole, it calls the editor after populating this list file, and I can uncomment what I want to download. It then downloads each download differently depending on whether its a directory or a single file. Also, made sure done was there. Must have forgot to paste it here.
Long story short, you can quote your strings for bash using `quoted=$(printf '%q' "$variable_to_quote")`.
what you are saying about unquoted vars is true (word splitting is a disaster with a proven track record) and that doublequoting vars is a good habit to instill, the example you cited is wrong. [[ ]] is magical in the sense that word splitting doesn't apply there, unlike inside [ ]. $ x='a lot of spaces '; [[ $x = *spaces* ]] &amp;&amp; echo true true $ x='a lot of spaces '; [ $x = *spaces* ] &amp;&amp; echo true bash: [: too many arguments 
Ah yes, thanks for the correction. To be fair, I generally program cross-platform for the POSIX shell and avoid bashisms, so I tend to forget about stuff like this.
didn't know this. thanks!
I'm grabbing the list of files, commenting them all out, then uncommenting the ones that I want, whilst allowing me to browse and search. (works nicely with vim) I have about 400 files in a directory, and only end up downloading maybe five at a time. Overall is looking to be more efficient than mget. 
gifsicle.
&gt; It cannot disable wordsplitting in the core parts of language by default or luddites will scream bloody murder. zsh seems to have managed. Never mind that disabling word splitting in POSIX is trivial. Use double quotes wherever you need to disable it, which should be assumed by default. It's not rocket science. In bash, it's sometimes needed, and sometimes not. That makes it *more* error prone for novices, not less. ---- &gt; IFS=: read -ra dirs &lt;&lt;&lt; "$PATH" &gt; process_dirs "${dirs[@]}" The POSIX way is just as rock solid and much simpler: ( IFS=':' process_dirs $PATH ) ---- &gt; (( )) for int math That's in POSIX shells too. &gt; and forget that shitty, limited [ ] exists. I can agree on that one. I forgot it existed a long time ago and use 'test' instead. Yes, same thing, but a command masquerading as syntax is evil. I would argue 'test' is not limited enough, though. Much better to use elementary 'test' statements and process them with standard shell grammar, which is more than flexible enough. POSIX agrees and has deprecated boolean logic (-a and -o) in 'test'/'['. Simplification and consistency are the proper way forward. Bash does the opposite. To fix complexity it adds more complexity. To fix inconsistencies it adds more inconsistencies. It's downright byzantine. &gt; I don't consider eval to be something to write home about, it's considered harmful in pretty much any language it exists in [...] ["Considered harmful" is itself considered harmful](http://meyerweb.com/eric/comment/chech.html). I don't care about religious objections. I'd prefer a logical argument. All I'm using eval for is referencing a variable by a name contained in another variable. If that is mixing code and data, then associative arrays themselves are mixing code and data, by definition. Yet, lots of "real" programming languages have them. If the slightly hacky stuff is strictly limited and relegated to a few shell functions, the rest can be "treated as a proper programming language", so the end result is the same. This is not different from any other function library in any other programming language. In the end it's all a matter of sensible coding practice. Meanwhile, my scripts are running a lot faster and in a fraction of the memory.
&gt; zsh seems to have managed. And it's not anywhere near being widespread. It has its fanatics but you don't see zsh scripts on the internet, while bash snippets are everywhere. I am asking for a flag in my shell of choice. &gt; Never mind that disabling word splitting in POSIX is trivial. Use double quotes wherever you need to disable it, which should be assumed by default. It's not rocket science. And look how many people forget such a simple thing, how can it be? I quote that shit like there is no tomorrow, but if in 99.9% cases of naked $vars use the word splitting "feature" was in fact not desired, then maybe just maybe the default behavior is not sane. How many people owned themselves with the default behavior? Why not have an explicit syntax for 'yes, split that shit, i am really serious'? Explicit is better than implicit and bad design is bad design. &gt; I would argue 'test' is not limited enough, though. Much better to use elementary 'test' statements and process them with standard shell grammar, which is more than flexible enough. POSIX agrees and has deprecated boolean logic (-a and -o) in 'test'/'['. regex comparisons with `test`, go. Overhead from spawning external processes to do a little pattern matching can add up. &gt; The POSIX way is just as rock solid and much simpler: ( IFS=':' process_dirs $PATH ) so spawning a subcontext to do a trivial thing. And in case you need to reuse the "list" in several contexts you have to make sure your IFS is set every time. Yes, micromanaging shit like there is no tomorrow is simpler... for certain values of the word "simpler". Face it, a global MASTER_DELIMITER used for every tiny thing is a harebrained idea. Arrays allow for the awesome "populate once, keep stuff in a pristine condition, never give a shit about the potentially destructive and context changing state of $IFS" &gt; All I'm using eval for is referencing a variable by a name contained in another variable. If that is mixing code and data, then associative arrays themselves are mixing code and data, by definition. Yet, lots of "real" programming languages have them. The difference is that key of assocarray/dict is still data not a handle living in the namespace of the environment itself, not to mention the ability to conjure an actually working, arbitrary function from the data. You are not going to escape the sandbox and exploit vulnerabilities by playing with dict keys. Wasn't the whole shellshock fiasco caused by treating data (contents of env variables) as a kinda valid code, sometimes, during subshell spawning? Yes, i know it was in bash in a specific context, either way it shows that blurring the line between code and data is a questionable choice. 
or, even better, skip your `-exec` clause, and just use `delete` instead. example: ``` find &lt;root-dir&gt; -type f -delete ```
Ok, great - thank you!
Hm, interesting. I'll try that, thanks!
Never tried that. Usually use exec rm for the -f option. You taught me something today though.
&gt; And it's not anywhere near being widespread. It has its fanatics but you don't see zsh scripts on the internet, while bash snippets are everywhere. I am asking for a flag in my shell of choice. Agreed. All I'm saying is that the lack of it is not a fatal flaw if you adopt a simple habit. &gt; And look how many people forget such a simple thing, how can it be? Lack of widespread proper coding standards. A broken programming culture around the shell. Utterly wrong tutorials everywhere on the net. A dearth of information that is both correct and easy to access. It's mainly a problem with people. "Real" languages (especially things like C and Perl) all have similar problems if you don't write properly, but the attitudes are different in their communities and proper coding standards are widely followed. You'd think a decent coding culture ought to exist around a language that is the most central after C in every UNIX-like system ever. &gt; Overhead from spawning external processes to do a little pattern matching can add up. True. But in something like ash or dash it needs to be really bad to exceed the overhead you get from using bash in the first place. If you do it smart, you avoid spawning any subprocess within a loop and pipe a bunch of values through grep all at once. This can speed up bash scripts, too, because its loop runs are sluggish. &gt; either way it shows that blurring the line between code and data is a questionable choice. I have to concede your point there. My trick using eval has a shellshock-like code injection vulnerability if untrusted input is used. The input can be sanitized but that involves a performance hit. 
This worked well, thanks.
&gt; Lack of widespread proper coding standards. A broken programming culture around the shell. Utterly wrong tutorials everywhere on the net. A dearth of information that is both correct and easy to access. It's mainly a problem with people. &gt; "Real" languages (especially things like C and Perl) all have similar problems if you don't write properly, but the attitudes are different in their communities and proper coding standards are widely followed. &gt; You'd think a decent coding culture ought to exist around a language that is the most central after C in every UNIX-like system ever. Indeed. But these tutorials didn't fall out of the sky, it's the hardcore unix neckbeards who were supposed to know better who wrote them and now the ball is rolling. Apparently the expectation is simply unreasonable. Ever heard "90% of everything is shit"? It applies to shell scripts too and given the power they wield in the system, i don't think it's a smart idea to design something that is so trivially easy to trip up and potentially destroy things. "Real" languages are replaced more and more by python et consortes, because they are both easy to write and the amount of gotchas to account for is relatively small. I don't think shells are supposed to be C-like when it comes to the difficulty of writing shit correctly. There is a reason newer languages have easy to use data structures. Writing your own is a good compsci exercise for your uni but it simply stands in the way of getting shit done in a robust way. &gt; If you do it smart, you avoid spawning any subprocess within a loop and pipe a bunch of values through grep all at once. This can speed up bash scripts, too, because its loop runs are sluggish. yes, but then the problem space grows by yet another language you need to master, be it sed, awk or whatever. I wrote many nontrivial scripts where the core logic was in pure bash (slightly more powerful param expansion, that regex ability, arrays, etc), which would be flat out impossible in posix sh alone (well, ok... i assume sh is Turing complete so it can do everything, theoretically). I do consider performance implications of what i am writing but the point is that sh is very inconsistent about its scope and the let's call it quality of service. It can do X... but with gotchas, it can do certain flavor of Y but for something slightly different you need external programs. Farming out because you need some simple regex? Globs were sufficient in the 70s but now we know better. Why is not posix meaningfully updated every 10 years? Let's take your example of `( IFS=:, process_shit $PATH )`. What if we are interested in the output of that and need to send it back to the main scope ( `stuff=$(process_shit $PATH)` )? Isn't subshell its own universe? Theoretically syntactically simple but with a huge drawback, a business as usual in sh. There is a reason why in bash you can write `while read; do ...; done &lt; &lt;( stuff )` - people invented process substitution so that things can live in the current scope (and so that you don't have to have a shitton of temp files everywhere) - instead of `stuff | while read; do ...; done`, where the loop body is kinda out of sight. All these limitations are ugly and mean that the posix sh is not transparent as a programming language. 
Move the files in question to a directory in /tmp, and then use rm to very explicitly delete it from there, eg:- rm -rf /tmp/my-directory Using rm in scripts directly is potentially hazardous. To the extent that rm is used, directories must be stated very carefully and explicitly, ideally with files being moved to /tmp first, as mentioned. It's also much safer to never run rm as root; I've deleted system files before.
That's not modifying the script. That's creating an alias in a really obtuse way in your shell. The installer isn't using your environment to run. You don't have an editor installed on that system, I assume? edit: An editor isn't an issue. I got too confused by what you were doing to point out that all you need to do, to accomplish what you're doing is `alias more='ls'`. edit: Which won't work. I was just pointing out that you didn't need to do so much work.
I'd forgotten about `yes`. It's available on Debian, so hopefully the OP can try it. That'd be a lot simpler than writing expect. Not sure how it will / would interact with text being parsed through `more` first, though.
One way would be with a regular expression along the lines of `s/STRING1|[^|]*|STRING2/STRING1|STRING3|STRING2/` Can likely be shortened or golfed a bit. Another way would be with `awk`: `awk -F'|' '{OFS="|"} $3="STRING3"'`
Test it yourself: testalias.sh #!/bin/bash alias ls='echo it worked!' ls ./script_that_calls_ls_alias script_that_calls_ls_alias #!/bin/bash ls Try running it: $ ./testalias.sh script_that_calls_ls_alias testalias.sh script_that_calls_ls_alias testalias.sh Long story short; aliases don't work in scripts.
Stupid long one liner: Let's saf a file contains: Blah blah blah ABC hello blah blah blah bloh bloh bloh DEF Bah bah bah This will give us what between ABC and DEF: $ sed "s/^ABC/+ABC/" &lt;file | sed "s/DEF$/DEF+/" | tr "\n" "~" | tr "+" "\n" | grep "^ABC" | tr "~" "\n" | grep -v ^ABC | grep -v ^DEF hello blah blah blah bloh bloh bloh
This is not complicated, but there are many cases that can be missed if you are not careful. The easiest solution is to use Perl and read the whole file, provided it is not too big: perl -0777 -pe 's/STRING1.*?STRING2/STRING3/gs' file.txt The `-p` flag emulates Awk behavior (loop over lines), the `-0777` makes Perl read the whole file, since the 0777 separator char (in octal) is nowhere to be found. The `s` flag in the substitution command enables dots to match newlines. Also note the non-greedy operator, `*?`. Eg. &gt; The tao that can be told is not the eternal Tao The name that can be named is not the eternal Name. The unnamable is the eternally real. Naming is the origin of all particular things. Free from desire, you realize the mystery. Caught in desire, you see only the manifestations. Yet mystery and manifestations arise from the same source. This source is called darkness. Darkness within darkness. The gateway to all understanding. perl -0777 -pe 's/Th.+?is/This is/gs' tao.txt This gives: &gt; This is not the eternal Tao This is not the eternal Name. This is the eternally real. Naming is the origin of all particular things. Free from desire, you realize the mystery. Caught in desire, you see only the manifestations. Yet mystery and manifestations arise from the same source. This is called darkness. Darkness within darkness. The gateway to all understanding. 
I actually wrote a Perl script with some option handling bells and whistles for exactly this. On mobile, so I can't get the direct link, but it's the script called betwixt in [this]( https://github.com/sluidfoe/scripts) repo. 
&gt; `[[ $( &lt; a ) != 0 ]]` Thanks, that's exactly what I was looking for. `read` isn't much better than `cat` since it still costs another process.
You can use the -s flag of test for this. "-s file True if file exists and has a size greater than zero." [[ -s /some/file ]]
That's different. For example, a file containing just the character 0 would pass -s but fail the test in the OP, while an empty file would fail -s and pass the test in the OP.
This worked. Incidentally, the delimiter '|' did not occur in STRING3 in this case. Thank you, everyone, for all of your help.
You prompt needs to check for emails. [It's the law.](https://en.wikipedia.org/wiki/Jamie_Zawinski#Zawinski.27s_law_of_software_envelopment)
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 3. [**Zawinski's law of software envelopment**](https://en.wikipedia.org/wiki/Jamie_Zawinski#Zawinski.27s_law_of_software_envelopment) of article [**Jamie Zawinski**](https://en.wikipedia.org/wiki/Jamie%20Zawinski): [](#sfw) --- &gt; &gt;*Zawinski's law of software envelopment* (also known as *Zawinski's law*) relates the pressure of popularity to the phenomenon of [software bloat](https://en.wikipedia.org/wiki/Software_bloat): &gt;&gt;"Every program attempts to expand until it can read [mail](https://en.wikipedia.org/wiki/E-mail). Those programs which cannot so expand are replaced by ones which can." &gt;Examples of the law in action include [Emacs](https://en.wikipedia.org/wiki/Emacs), [MATLAB](https://en.wikipedia.org/wiki/MATLAB), [Mozilla](https://en.wikipedia.org/wiki/Mozilla_Application_Suite), [Opera](https://en.wikipedia.org/wiki/Opera_(web_browser\)), [Trillian](https://en.wikipedia.org/wiki/Trillian_(software\)), and [Drupal](https://en.wikipedia.org/wiki/Drupal). &gt; --- ^Interesting: [^Mozilla ^\(mascot)](https://en.wikipedia.org/wiki/Mozilla_\(mascot\)) ^| [^XScreenSaver](https://en.wikipedia.org/wiki/XScreenSaver) ^| [^Identifier](https://en.wikipedia.org/wiki/Identifier) ^| [^SlashNET](https://en.wikipedia.org/wiki/SlashNET) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cqj2q5p) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cqj2q5p)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
i believe the OP was interested in deleting files but not directories. additionally, your `rm /path/*` would not handle recursive removals.
Yes, there's still lots of bash being written. In general, I start all scripts in bash. If it starts to need remotely complex datastructures, I move it into ruby (my Perl), with all of the important bits still residing in shell execs. And if it is more of a "hook these two libraries together" type of thing, or deals with a web api, I write it in python.
I'm currently in an app support role. We have shell scripts all over the place. It's like the glue that holds together all those applications together into a working product. 
I actually have that book on my desk as well. It is an excellent resource. I use primarily bash scripts for anything I do. My environment is mostly linux and AIX, and having bash available in both makes writing scripts that can work in both environments very friendly.
My current set-up has a Monitor (primary screen) and a TV next to it, which we use for watching Netflix/Movies, so I made a script to auto swap between single screen (using headphones) and dual screen (using HDMI for audio). Also just finished writing a script for automatic testing at work, using a Raspberry Pi as a video source.
&gt; "whatsallthisthen" And if one hasn't permission to read the file or directory it says "Nothing to see here, move along.", yes?
"Move along. Nothing to see here, doc."
Perl provides a more natural progression from shell scripting. When you start learning it you will realize how much it borrows and extends the shell syntax you already know. 
Perl used to be my goto for more advanced scripts, but I feel like the community just isn't as prolific as they used to be and it's harder to find help. Also, the object system, for really advanced tasks, feel so bolted on, that I never want to use it. Starting around 2006, I replaced perl with ruby and find that it's better suited for a lot of tasks and retains a lot of perlisms. I'd probably continue to use ruby day to day if it was easier to distribute complicated scripts with a decent version of ruby. At my last job, we'd omnibus the toolset, but because the omnibus tools are in so much flux, every couple months, I'd have to spend a week getting my build scripts up to date to support new features.
I have the script with the two functions to find the minimum or maximum value. Thanks for the clarification!
hmm, what do I have to do to make awk accept the argument as an input file? for example: `awk -F '{blla blla blla}' $1` ?
that should work, check this for more info http://stackoverflow.com/questions/1450390/how-to-pass-the-file-name-dynamically-to-awk-when-awk-is-used-in-command-substit
The script is working now. Thank you for your help!
&gt; My pip install line does not recognize the file, so I'm stuck there. Did you check what is the value of `$BASEDIR`? If you're calling the script from the same directory where it is, `$BASEDIR` should be `.`. Does `pip` take that? (I'd guess, but worth a double check). If you need the absolute path to the dir, you could use `readlink -f $BASEDIR/requirements.pip` or similar (in Debian there's also `realpath`). Another idea: is `pip` in your current `$PATH`? try placing a `which pip` in your script and see what it outputs. And as general recommendation, I would place a `set -e` at the beginning of the script. That way the script will immediately exit if an error occurs, so one error doesn't turn into a cascade of errors. Oh, and as a side note, I use Puppet for provisioning, but over time I realized they're not much cleaner/maintainable than plain shell scripts. YMMV. Best of luck!
&gt;Did you check what is the value of $BASEDIR? If you're calling the script from the same directory where it is, $BASEDIR should be .. Does pip take that? (I'd guess, but worth a double check). If you need the absolute path to the dir, you could use readlink -f $BASEDIR/requirements.pip or similar (in Debian there's also realpath). Originally I had the pip requirements file inside another folder, so maybe I am using $BASEDIR in vain? I will change `dirname $0` to . and let you know what happens later tonight when I get back home. I will likely just try to feed the file to pip like: pip -r ./requirements.pip or even try LINK=readlink -f $BASEDIR/requirements.pip pip install $LINK &gt;Another idea: is pip in your current $PATH? try placing a which pip in your script and see what it outputs. I will try testing this as well. &gt;And as general recommendation, I would place a set -e at the beginning of the script. That way the script will immediately exit if an error occurs, so one error doesn't turn into a cascade of errors. I didn't know this, it sounds really useful, I will add it in. &gt;Oh, and as a side note, I use Puppet for provisioning, but over time I realized they're not much cleaner/maintainable than plain shell scripts. YMMV. My thoughts/suspicion exactly. The trade-off I suppose is a test-backed, constantly maintained system versus a simple script or two to handle your specific needs, mine being very crude at this point. Perhaps later I will use these but * **puts on tin foil hat** * what if any of these services get hacked or go the way of the Dodo? Anyways, thanks for your advice, I'll keep working on this until I get it working. 
You should use git or SVN.
try changing colors of terminal instead (http://stackoverflow.com/questions/3085834/what-are-your-ls-colors) check http://blog.twistedcode.org/2008/04/lscolors-explained.html if you are following the procedure correctly
I'm not sure "instead" of what you mean: that is the change I have mead described in the post itself. I do know how to make the change in 'real' bash - I don't have this problem in RedHat or Debian - just on the mac, and it's the same on iTerm &amp; Terminal. Note that the environment variable LS_COLORS already has the directory color set to cyan (36), yet it still displays as dark blue, and it's the same for my default user &amp; for root, so it's not a local configuration issue.
I don't use it, but I think this is what you are looking for. http://www.marinamele.com/2014/05/customize-colors-of-your-terminal-in-mac-os-x.html
now that's interesting - an OSX environment variable that's just a 'little' different: LSCOLORS instead of LS_COLORS - I think that might be the answer I was looking for! Thank you.
You have to tell bash that you are running a command and not just trying to set the variable to text: bs=$(lsusb | grep Logitech | cut -c5-7) 
Yes. Backticks will also work, but are not nestable and (in my opinion, anyway) not as easy to read in complicated scripts.
Thanks for such a fast reply I did that and I got this Error opening output file: Is a directory + /home/jarmey/scripts/webcamloop.sh /home/jarmey/scripts/webcamloop.sh: line 3: syntax error near unexpected token `|' /home/jarmey/scripts/webcamloop.sh: line 3: `bs=(lsusb | grep Logitech | cut -c5-7)' /home/jarmey/scripts/webcamloop.sh: line 4: syntax error near unexpected token `|' /home/jarmey/scripts/webcamloop.sh: line 4: `dve=(lsusb | grep Logitech | cut -c16-18)' 
strike that... you set me straight Thanks so much. I did not add the "$" the first time. 
I'm just going on my 30th year at this... Building up to it! ;)
You don't need the backticks and you forgot the $ that /u/kittykarlmarx pointed out. That $ is crucial.
For launching Hadoop: https://github.com/apache/hadoop/tree/trunk/hadoop-common-project/hadoop-common/src/main/bin
The better question here is why in the world you're using colons in your directory names.... 
Readabilty isn't?
 #!/bin/bash name=${1##*/} no_ext=${name%.*} filedir=${1%/*} mkdir -p "$2/$filedir" ffmpeg -i "$1" -codec:a libmp3lame -q:a 4 -n:v "$2/$filedir/$no_ext@160k.mp3" If you want to do it in parallel, use `xargs` or `parallel` instead of `-exec` in find.
sorry for the confusing question. edited it!
yes! im trying to make it all of the shell output! yeah. im trying the first link you gave. 
 :~$ while read -r line ; do echo "| ${line}" ; done &lt; test | 1 | 2 | 3 | 4 | 5 | 6 | 789 I typo'd part of it in my sleepyness. 
thank you! had some points to figure this out
this got working for me but for the error output only. based on testing your code above i found the `clear` and some other commands unusable. more testing to do! exec 9&gt;&amp;2 exec 8&gt; &gt;( while IFS='' read -r line || [ -n "$line" ]; do echo -e "| ${line}" done ) function undirect(){ exec 2&gt;&amp;9; } function redirect(){ exec 2&gt;&amp;8; } trap "redirect;" DEBUG PROMPT_COMMAND='undirect;'
Try ctrl-c twice.
You're not in grep. This is bash, saying "that quote does not have an unquote, so please enter more text". IOW, you're in bash's mode for entering multi-line strings. You can get out of it by typing the final quote and then enter, or by typing Ctrl-D which means end of file. 
Just hit `"` then `&lt;ENTER&gt;`. The terminal thinks you started a new string, as a consequence of the failed escapement.
 #!/bin/bash fullPath=$1 fileName=$(basename ${fullPath}) filePath=$(dirname ${fullPath}) # stuff exit 0
Others will tell you to use basename and dirname, and these are simple, common, readable, newbie-friendly ways to achieve this. The problem is that these are external tools, which can in certain cases be problematic (i.e. portability, scale). These aren't issues for you, so go ahead and use basename and dirname. For future reference though, bash has built in string manipulation capability, and instead of using basename and dirname, you'd do this instead: fullPath=$1 fileName="${fullpath##*/}" fileNameWithoutExtension="${filename%.*}" filePath="${fullPath%/*}" Example: $ fullPath="/a/b/c/d/somefile.txt" $ echo $fullPath /a/b/c/d/somefile.txt $ echo ${fullPath##*/} somefile.txt $ echo ${fullPath%.*} /a/b/c/d/somefile $ echo ${fullPath%/*} /a/b/c/d Now, assuming this is a homework task, you can use that to blow your teacher's socks off.
Thanks, I totally forgot about this very helpful feature that is CDPATH. I've immediately incorporated it into my .dotfiles (http://github.com/florianbeer/dotfiles)
Thanks for sharing! I will have to give that a read, though I'm so used to Bash that I find the idea of switching shells a bit daunting.
Yeah. Plus you can have fun stuff made easy like prezto and etc. 
asking a vim guy an emacs question Some people just wan't to watch the world burn... But to answer your question: Yes, they're pretty different in Syntax vim uses it's own Syntax/Language called Vim script, while emacs uses Lisp in a special dialect called emacs lisp.
Thank you! I used your suggestion to make the script. Ive posted an example of it in another comment.
That works well if you know that your path is unique, but what if you are unsure about one of the directories, or if you didn't realize that your path wasn't actually unique enough to define the final destination? Do you have to back out and check each segment of the path or will it give you options for just the part that you didn't identify with enough letters to make it unique?
Just a small thing, but the -name '*' part is unnecessary. If you want it to return all files, you already have that with -type f
Sorry, I don't know anything about using emacs. And I will NOT get into that stupid editor-war ;)
It will auto complete all of the bits it can and put the cursor on the first of the choices with more than one option. Then you work through the options of each. EDIT - But also, it will autocomplete with hints for `cd -` and `cd -2` (etc). So there are other helpful features.
Thanks, mostly just to learn. It started because I could never remember the stupid nuances of the find command and always had to go back and reference the man pages or my bash script I pilfered off the net to change media file name extensions after downloading when trying to find &amp; delete something. So I decided to see if I could make a function that included a rm command. Of course now I've about memorized it so it comes back to mostly just to learn. Cheers though your example helped! function finddel () { sudo \find $1 -iname $2 -exec rm -ri '{}' \+ || echo "Usage is 'finddel &lt;dir&gt; &lt;filename&gt;'" ; } 
The advanced bash-scripting guide (ABS) tend to teach bad practices, pointless code and buggy code more than being a useful resource. I recommend referring to [BashFAQ 100 at the wooledge wiki](http://mywiki.wooledge.org/BashFAQ/100) instead of the ABS's string manipulation page. Also notice that ABS is not listed in this subreddit's sidebar.
I got these problems solved. So printing to the console information like: pwd which pip ls -l helped me see what was going on. Also, using: set -e was great for stopping a failing process. So the MAIN problem was not correctly [setting my shared/synced folders in the Vagrantfile](http://docs.vagrantup.com/v2/synced-folders/basic_usage.html). Without this, simply writing: pip install -r requirements.txt does not work if the synced folder is not set up to import from folder where the Vagrantfile sits on the host machine (where I keep my provisioning scripts/files). Thanks a bunch for your help buddy!
It's a university subject called web systems, we're learning unix at the moment so we do web dev, piping and wildcards and bash scripting
or for giving the answer for a homework problem I myself from Electronics background.. commands, Vim and Perl were picked as when needed, no formal learning... only now I am trying to learn the concepts too... so, trying bash scripting after Perl was troublesome, I particularly hate the white-space issues, like even within [] for condition check.. and hence the hard syntax impression... I told about this to my college juniors like 4-5 times and still many made the white-space mistake, and embarrassingly even I did while typing it out on big screen... 
Check out [shunit2](https://code.google.com/p/shunit2/downloads/list) Also this [article](http://code.tutsplus.com/tutorials/test-driving-shell-scripts--net-31487) does a fairly decent job of applying TDD to shell script development. NOTE: Strictly speaking, it's not really a BDD tool. More of a TDD tool - though i find the difference minimal.
Hm. Could be your `ps` output is localized. If so, this should work: LC_ALL=C ps -eo %cpu=,vsz=,user= | awk '{ cpu[$3]+=$1; vsz[$3]+=$2 } END { for (user in cpu) printf("%-10s : Memory %10.1f KiB, CPU%%: %4.1f\n", user, vsz[user]/1024, cpu[user]); }'
:( Distributor ID: Ubuntu Description: Ubuntu 14.04.2 LTS That's mine. 
Odd. Works here. $ lsb_release -sd Ubuntu 14.04.2 LTS $ LC_ALL=C ps -eo %cpu=,vsz=,user= | awk '{ cpu[$3]+=$1; vsz[$3]+=$2 } END { for (user in cpu) printf("%-10s : Memory %10.1f KiB, CPU%%: %4.1f\n", user, vsz[user]/1024, cpu[user]); }' geirha : Memory 551.3 KiB, CPU%: 0.1 ...
Add `| sort -k4 -rn | column -t` for style and flair! :D
"but what exactly does it do on that function." Launches a subshell (as a fork of this process?), passes the string between the back-ticks to it, with an appropriately chosen shebang line ("#!/bin/bash" is implied in this subshell), launches an interpreter (/bin/bash in this case) to that string (between the ``), and finally pastes the resulting STDOUT as a string to whatever is being formed in the parent shell (another string)? ... If this "formed string" is part of a "X&lt;equals&gt;&lt;rest&gt;" "scheme" where the &lt;rest&gt; part incorporates this/other subshells, it gets "pasted into it" (the &lt;rest&gt;) and a variable name is "set" to the current shell. ... if this "formed string" is part of a "command line expression", again it's used as argv[] expansion to the first whitespace separated string (used as "command name"/"alias")? ... &gt; find . -type d -regex ``./[^.].*'' -empty -exec touch {}"/.gitignore" \; In the end, you are launching in this case an empty shell with an interpreter and nothing to be interpreted for it. What should be the result in this case? An empty string? So, the only trouble I see here is here actually the forward ticks (the single quotes after the asterisk symbol). They bug me for some strange reason (can't think of any other gripes with it right now)
Anything really that was a bad conversion from TeX. I "love" that in the POSIX manpages, all of the `-` in flags are mistakenly converted to em-dashes (`—`) instead of plain dashes.
Put quotes around stored string variables. There's no law. It's just good practice for maintaining and segregating data types. hello="$(echo "hello")"
Sorry I was thinking in bash originally (my shell was bash). That's why the the "?" at the end of my... lines of thinking =)
Weird, works now :S
Thanks!
I agree with /u/koeningyou666 Python and pentesting are a match made in heaven. If you're just getting into it, check out "automating the boring stuff with python" and a handful of other books by the same author (who's a redditor, I'll link to his post when I get to my computer) EDIT: http://inventwithpython.com/blog/2015/04/21/celebrating-the-release-of-automate-the-boring-stuff-with-python-with-discount-codes/ http://www.reddit.com/r/learnpython/comments/33cyhc/my_latest_book_for_total_beginners_automate_the/ /u/AlSweigart
it would be better if you give sample file and sample output expected... grep/sed/awk/perl/python/etc can solve it, I certainly not able to get this... by human readable format, do you mean KB, MB, GB? below command takes a number to convert into KB, MB, etc numfmt --to=iec-i --suffix=B
Hadn't heard of that one. I'll check it out.
Maybe take a look at [ack](http://beyondgrep.com/documentation/) instead.
also been a sysadmin all of a week please go easy lol 
"By forking, the subshell keeps all the variables, functions, aliases etc... set in the main shell" ... and that's why the usage of /usr/bin/env.
From the cpulimit man page: &gt; cpulimit always sends the SIGSTOP and SIGCONT signals to a process, both to verify that it can control it and to limit the average amount of CPU it consumes. This can result in misleading (annoying) job control messages that indicate that the job has been stopped (when actually it was, but immediately restarted). This can also cause issues with interactive shells that detect or otherwise depend on SIGSTOP/SIGCONT. For example, you may place a job in the foreground, only to see it immediately stopped and restarted in the background. 
OK I see what's happening now, but it seems like there should be a way around this like having kdialog run only if ffmpeg stops like an "if then" statement. I'm afraid my bash skills are very basic.
With a % contained in your variable, you are no longer comparing integer vales. Just like if you were comparing $14 -le 10. you need to strip off the symbol because -le actually just wants to compare a number. 
You can also try nice instead
I don't think nice would be the right command in my case. I want to limit the cpu because it's running too hot.
Thanks for that answer, I thought that might be the problem. I will try you suggestion.
OK, thanks, I thought that may be the problem.
 curl http://blog.us.playstation.com/2015/02/12/ps4-ps3-ps-vita-games-in-2015-the-big-list/ | sed -n '/PS4/!{H;x;s/^.*\n\(.*\n.*\)$/\1/;x};/PS4/{x;s/^\([^\n]*\).*$/\1/;p}' | cut -d\&gt; -f2 |cut -d\&lt; -f1 |tr -d \}| sed '/^$/d' | sort -u How 'bout that?
Very good! A learning experience after all. 
How do you put coded text in line like that?? I've only been able to get it on a new line.
BSD sed requires that the closing `}` of a block be preceded by a semicolon or newline, or you can put it in a new script argument (`-e`). So easy fix is to add two semicolons. The sed script does otherwise only use commands and features defined by POSIX, so it should work the same after that fix.
Thank you for clearly pointing it out! I got it myself after rereading the man page, though, and used -e. ;)
Oh wow. Thank you! I was just trying to use the standard utilities, no restriction to `sed`. `sed` just seemed the most robust pattern matching compared to `grep`. But `xmllint`! That seems to be an *amazing* tool. Thanks for showing it to me!
With great power, comes great responsibility. By using sed to parse HTML, you risk summoning [the pony](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454).
Solved on StackOverflow :) http://stackoverflow.com/questions/30153081/arbitrary-command-completion-possible
Yep. You can put a script into `/etc/bash_completion.d` such as: _myscript() { local cur prev opts COMPREPLY=() cur="${COMP_WORDS[COMP_CWORD]}" prev="${COMP_WORDS[COMP_CWORD-1]}" opts="--help -p --X --foo --bar" COMPREPLY=( $(compgen -W $opts -- ${cur}) ) } complete -F _myscript myscript In this case it will allow tab completion of options. For a more complex example to model from, install git using your package manager, and have a look at it's `/etc/bash_completion.d/git`
Yep I sure did. My bad. Looks like you figured it out anyway, though :)
This may not be the best way, but you could use something like `uniq -c` to tell you how many of each line it finds. Lines that only appear once will be preceded by `1`, twice by `2`, and so on. Then it's just a matter of piping to something like `awk '$1 &gt; 1'` and piping the output to a delete command/xargs/etc.
the file path differs on the dupes lines, so uniq won't help me/
Mind you explain the following parts: '*[[:space:]]*' -F '^MD5 [(]|[)] = ' 'seen[$3]++ == 1
Lines 16 and 17 (and several following examples in your script) should be combined into: if grep -i -q 'centos' /etc/os-release; then... Any command that returns an exit code can be used as the condition of an if statement, while loop, or until loop. As a matter of style, your case statements may be easier to maintain if you format them like this: case $opt in 1) # comment about what happens here code for option 1 ;; 2) # comment about option two code for option 2 ;; *) # catch-all option code ... ;; esac 
&gt; `! -path '*[[:space:]]*'` The `md5` command's output is not parser friendly, and cannot be parsed safely if the paths may contain whitespace, therefore I added a filter on paths containing whitespace so the output becomes safe to parse. &gt; `awk -F '^MD5 [(]|[)] = ' 'seen[$3]++ == 1'` `-F '^MD5 [(]|[)] = '` sets the field separator to the parts matching `MD5 (` and `) = ` A typical line of the `md5` command's output looks like this: MD5 (empty.txt) = d41d8cd98f00b204e9800998ecf8427e ^^^^^ ^^^^ The carets mark the two parts that split the line. So $1 is now the empty string before MD5, $2 is `empty.txt`, and $3 is `d41d8cd98f00b204e9800998ecf8427e`. Finally, that makes seen[$3]++ == 1 become seen["d41d8cd98f00b204e9800998ecf8427e"]++ == 1 which only evaluates to true the second time that md5sum is encountered. And if it evaluates to true, the following `{print $2}` is run.
thanks for the input so far, i have tried both advices and it really makes the script look cleaner. I was wondering if i can do anything about the menu to tidy it up a bit. Usually when i want to add a different set of options to a submenu, I create another show.menuoption with it`s suboptions. Other that that is there anything else that I could change and/or improve the script?
Feedback welcome!
If the paths contain whitespace need to enclose in ""
https://www.reddit.com/r/sysadmin/comments/28ph0b/this_weeks_lesson_know_when_your_ssl_certs_expire/cid6h95?context=3 ;)
No, that's not the issue.
Expiry isn't the only thing you should monitor. Don't reinvent the wheel, just use this: https://github.com/ssllabs/ssllabs-scan That's golang, another option would be to possibly grab and scrape their web-based output from here: https://www.ssllabs.com/ssltest/analyze.html?d="${DomainGoesHere}"&amp;hideResults=on But... you wanted feedback, so... for your script, you should avoid using backtick command substitution. That's 1979 stuff. You should use $() instead (e.g. $(cat url-list.txt) ) But that said, this is a useless use of cat (UUOC). Google that. And you should try to avoid vague variables in loops - give your variables a meaningful name. So, something like: for url in $(&lt;url-list.txt); do What happens next really depends on how you want to deliver the information. Warning email? stdout? NRPE? Either way, I'd try to avoid using files as you are. Finally, some people might point out that you should use a while read loop rather than a for loop. Personally I don't have a strong opinion about that - if a for loop works reliably for you, go for it IMHO. /edit: added hideresults to scrape suggestion
You can use `` $(&lt; url-list.txt) `` instead of `` `cat url-list.txt` ``. It’s faster because it doesn’t have to spawn a new process. Alternatively, you could use a `read` loop: while read url; do # ... done &lt; url-list.txt EDIT: …somehow I didn’t see /u/whetu’s comment until right after I posted mine. Enjoy the redundant information I guess…
What's wrong with the link he provided for the CLI tool?
TIL `$()` spawns a new process just for itself. Good to know, thanks…
Your benchmark is a bit off; `read j &lt; /tmp/empty` reads in the first line, and the first line only, where the other two will read in the entire file. Redone, where `head -n 1000 /usr/share/dict/american-english &gt; /tmp/words` #!/bin/bash start=$(date +"%s%N") for i in {1..1000}; do for word in `cat /tmp/words`; do : done done printf '%-10s%10s\n' 'cat' $(($(date +"%s%N") - start)) start=$(date +"%s%N") for i in {1..1000}; do for word in $(&lt; /tmp/words); do : done done printf '%-10s%10s\n' 'sub' $(($(date +"%s%N") - start)) start=$(date +"%s%N") for i in {1..1000}; do while read word; do : done &lt; /tmp/words done printf '%-10s%10s\n' 'read' $(($(date +"%s%N") - start)) $ ./read-bench cat 2358073972 sub 2191931656 read 3736018049
Ugh, wow, that is also a ridiculous amount of code for the task, especially considering the API does the heavy lifting.
When using temp files, use `mktemp`; e.g. `temp_file=$(mktemp)` I don't really see a need for a temp file though, you could do something like this: #!/bin/bash while read host; do expiry=$(curl -k -v -o /dev/null https://$host 2&gt;&amp;1 | \ grep -iE '(expire|expiry)' | grep -m 1 -o -E '[0-9]{4}-[0-9]{2}-[0-9]{2}') printf '%s %s\n' "$expiry" "$host" done $ ./find-expiries &lt; hosts &gt; expiries $ cat expiries 2015-08-04 google.com 2017-04-02 bing.com 2015-09-25 yahoo.com
Righto, had an order of magnitude error somewhere.. seeing 100 lines as a more reasonable rule of thumb for the break-even point now.
From the few times I've used it, it is very good. Give it a shot and see if it meets your needs.
Mainly consistency. For many things, echo and printf can produce the same output, but I prefer to use one of them consistently. Since printf is superior, I tend to consistently use printf.
Alright, thanks!
Really, I wanted to see if the user had more than the typical set of dot files where we'd want to consider whether data needed to be archived before removing them. I'm a little confused as to why the done &lt; /tmp/userlist is at the bottom. If I remember right, that's basically redirecting the userlist into done, but shouldn't the script simply reference the file instead of the file be directed into it? As in, declaring the variable further above and referencing it in the while loop?
So after some days away I was able to really dig into that `sed` line and I have two further questions if you have time to help me understand. I wrote the line out as a script and # lines with their (assumed) function. I have to admit, the hold buffer is a little overwhelming at first and the man for it sucks, but I tinkered and think I figured it out. What remains for me are the two substitute lines. I’m 90% sure I understand the first, but the second is a whopper. sed ‘ /PS4/!{ # if string does not match ‘PS4’ H # append to the hold buffer x # put to the pattern buffer s/^.*\n\(.*\n.*)$/\1/ # del all lines but two before(?) ‘PS4’ x # put remaining lines into hold buffer } /PS4/{ # if string does match ‘PS4’ x # put into hold buffer s/^\([^\n]*\).*$/\1/ # del leading newlines? p # print hold buffer }’ 
Aha! Thank you. So close! 
Be careful to audit your input - service accounts like www-data (in debianish), mysqld, mall, all have nologin as their shell and homes not in /home or /users. It would be disastrous to remove their homes 
Ok thanks!
It would be much better to use getent, instead of trying to guess what a user's home directory is. You can do something like `home=$(getent passwd $user | awk -F: '{print $6}')` to store $user's homedir in $home. Also, `/sbin/nologin` doesn't exist on all platforms, but `/bin/false` (generally) does.
http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_03.html if OP wants to know about the case block
[ಠ_ಠ](http://i.imgur.com/fqWmilY.jpg)
A snippet of the book's contents can be found here: http://assets.booklocker.com/pdfs/7831s.pdf
I think this is what you want to do: for i in {12..14} ; do sed -i "${i}i foo" bar ; done you can use `${}` to isolate the variable name, which is handy when you want to concat a string to your variable without having bash interpret too much of the string as the variable name. alternatively, you can also do something like: for i in {12..14} ; do sed -i "$i"'i foo' bar ; done since butting up the variable against a string will effectively concat the 2 values. edit: spelling (jittery from so much coffee)
why didn't I think of brace expansion!??! gah! thanks :)
 #!/bin/sh count=12 while "${count}" -lt 15 do sed -i "${i}"'i foo' bar count=$((count+1)) done Both the "-i" flag to sed, and the for loop you have used, are GNU extensions and not specification UNIX. They are therefore non-portable outside Linux. GNU extensions also never need to be used; there is always another way, and your study will be rewarded by finding it. Convenience is antithetical to freedom. You may get the immediate job done today, but you will lack knowledge of ***how*** the task was fundamentally performed. As long as you rely on convenience provided by others, you will never be able to surpass the limits set for you by said others. https://www.youtube.com/watch?v=iR3fSL9WMdg
first you need to answer what the conversion is supposed to look like, because json-&gt;xml translation is not unambiguous (which json parts are supposed to be xml tags, which parts are supposed to be tag attributes?) then you whip out eg python, import few modules and with the help of stack overflow write a script. Parsing json is literally 2 lines, constructing customized xml is going to be more but 1 page of code should be enough.
a few cool hacks and a few good rules of thumb, but the dude was a bit too obsessed with doing everything pipeline way for my taste. What if you subscribe to a different school of writing scripts that says oneliners are overrated? Repeatedly he warned against shell variables but there was nothing about how using expansions the variables come bundled with can reduce the overhead of calls to external tools, or that spawning pipes inside the shell loop is a bad bad idea. Yes, shell is not the fastest, but whipping out sed or awk for a trivial task at the drop of the hat inside a loop means paying the cost of spawning shortlived processes and people do this all the time. 
 #!/usr/bin/env python3 import json import sys def dict2xml(d, indent=0): tags = [] pad = ' '*2*indent for k, v in d.items(): if isinstance(v, dict): v = dict2xml(v, indent=indent+1) tag = '{pad}&lt;{k}&gt;\n{v}\n{pad}&lt;/{k}&gt;'.format(pad=pad, k=k, v=v) else: tag = '{pad}&lt;{k}&gt;{v}&lt;/{k}&gt;'.format(pad=pad, k=k, v=v) tags.append(tag) return '\n'.join(tags) f_in = sys.argv[1] with open(f_in) as f: json_data = json.load(f) data = { 'xml': json_data } # add root key print( dict2xml(data) ) quick and dirty py3 script that seems to work with dicts {} and basic data types. Had no idea what to do with lists if they appear (list elements are anonymous in json so what tag to apply?) $ ./json2xml.py test.json &gt; test.xml $ head -5 test.xml &lt;xml&gt; &lt;glossary&gt; &lt;title&gt;example glossary&lt;/title&gt; &lt;GlossDiv&gt; &lt;title&gt;S&lt;/title&gt; 
there is this too, but it tosses syntax errors up on me [json2xml](https://github.com/hay/xml2json)
&gt; Convenience is antithetical to freedom. Are you serious? Follow that logic and you'll be writing assembly for every task. Pretty soon you'll realize that the "convenience" of ASM is holding you back. It isn't low level enough and you'll need to write pure binary. Honestly, if I gave you a task to create an api connector class to pull data from a remote service, are you going to use unix sockets directly? Or do you draw the line arbitrarily at `/bin/sh` as far as high-level abstractions go? A sane person would at least use something like `curl` or a higher level language that utilizes `libcurl` or something to make your life easier. I hope you don't actually believe what you said because it's ridiculous.
Make sure that nobody can access any git metadata or repo information over http! This is generally a boatloat of hurt waiting to happen.
 # BSD (personal preference) sed -i.tmp ’12,14i\;foo’ &lt; bar 
See http://www.shellcheck.net/ for some general examples of things you could improve. Overall at a glance, it seems fine aside from the mentioned items on shellcheck.
&gt; I'd suggest appending .sh to the filename so you have a better idea of what the executable is later. I’ve read people recommend against that. The rationale is that 1. it’s irrelevant – the file name should document what the script does, not how it does it 2. it’s likely to change – if you expand your script, eventually you might find yourself too limited by the shell programming language, and want to switch to something like Perl or Python – but updating the name is risky because you might forget to update it everywhere 3. in this case, it’s wrong – it should be called `.bash` (this point is closely related to 2) I think a much better name for this script would be `git-pull-all-websites`. Alternatively, you could extract the glob (`/var/www/*.com/v/*`) into an argument, and call it `git-pull-all` (and put `/usr/local/git-pull-all /var/www/*.com/v/*` in your `crontab`).
You could use bash arrays: excludes=(.homeBack_Ups .cache .steam) excludesOptions() { for exclude in "${excludes[@]}"; do echo "--exclude=$exclude" done } tar … $(excludes) &gt; It does not exclude any of the given directories as the code stands now. I think you have to put the excludes before the `.`.
General critique of your GitHub, I hope you don’t mind :) * Very important: &gt; #Protected from being copyrighted because it is copylefted # [Saying “copylefted” does not make something copylefted.](https://i.imgur.com/bRtdajD.png) You have to put it under some particular license, and specify that license – usually, by adding a `LICENSE` file to the repository, mentioning it in the `README`, and (for scripts) mentioning it in the script header. The [GPL](https://www.gnu.org/licenses/gpl.html) has more information on “How to Apply These Terms to Your New Programs” (bottom). * You call your README file README**.md**, and yet it’s not markdown at all: 1. This is not how a markdown header looks: &gt; |==========| &gt; |homeBackup| &gt; |==========| This is what you should write: &gt; homeBackup &gt; ========== To get: &gt; homeBackup &gt; ========== 2. Enumerations in Markdown should end in dots: Write this: &gt; 1. Backup everything in home directory into a compressed tar. &gt; 2. Restore every file in a backup (overwrites files). &gt; 3. Delete backups. &gt; 4. Colorz to get a proper HTML list: &gt; 1. Backup everything in home directory into a compressed tar. &gt; 2. Restore every file in a backup (overwrites files). &gt; 3. Delete backups. &gt; 4. Colorz. * Uppercase variable names like VERSION=v1.01 are discouraged because they’re considered reserved for shell variables like `$USER`, `$TERM`, `$SHELL`, which you don’t want to override. Use `version` instead. * Use `tput` instead of hardcoded escape sequences to set colors and other attributes: tput setaf 1 (“tput set ASCII forground 1”) will work on any terminal, even if it uses completely different escape sequences. (You’re not likely to encounter such terminals nowadays, but still.) * Use `$USER` instead of `username=$(whoami)` * Consider removing the `.sh` file name extension. It’s misleading – it suggests that your script is portable across various shells, when in fact it uses highly bash-specific features like arrays – and if you ever want to rewrite your backup script in another language (e. g. Python), you would need to track the rename across all kinds of places where you used the old name (e. g. `crontab`), whereas with `backuphome` you can keep the name as long as you want. * If you’ve already tied yourself to bash (arrays!), you might as well use `[[` instead of `[`.
[Cyberciti.biz bash tutorial is great!](http://bash.cyberciti.biz/guide/Main_Page)
Thanks, I did not do much in a very long time. Its summer time so I want to work on my little programs. I just noticed how bad it is so I am adding more features and will take your tips into consideration. 
You can prepend `--exclude=` to each element while expanding it: excludes=(.homeBack_Ups .cache .steam) tar … "${excludes[@]/#/--exclude=}" … This is the `${parameter/pattern/string}` [syntax](http://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion). * *parameter* is `excludes[@]` * *pattern* is `#`, which matches the empty string at the start of the string * *string* is `--excludes=`
You bash arrays solution worked, but putting it before the . did not work. I read somewhere that if you have GNU Tar it must go after. And I have GNU Tar. Thanks a lot.
bash is my go-to language too when there is an itch to scratch but there is no point fighting uphill battles against its brutal shortcomings. Parsing shit and dealing with multilevel structures (eg xml, json) are things it absolutely sucks ass at and this is where a bit of quick and dirty python comes in handy. Xml modules in python are a bit annoying to work with (because of the value/attribute dichotomy which complicates things i guess) but json is an absolute breeze. 2 lines to get the whole thing parsed into a standard dict, the reverse operation is similarly 1 method call. Can't get any simpler than that.
I've taken a look to it: it seems a good book, but it lacks a list of exercises after every chapter, like the one I linked. I guess I'll try to look for exercises online as I learn bash from this and other books. 
I did try that but I must have done it wrong because it didn't work. Then I retried your suggestion and it worked. I guess that was a stupid question. Thanks
The issue you have is that you're trying to pass shell syntax (like `&amp;&amp;`) to a facility that doesn't support it. If you *weren't* trying to use shell syntax, you could simply do `"${@}"` and it would work like you want: % cmd=( 'echo' 'hello world' ) % "${cmd[@]}" hello world (You could also use `exec` in your case if you put it in a sub-shell, but that doesn't seem useful/necessary.) If you want to execute arbitrary shell "code", the only two options I can think of are: 1. Pass your "callback" as a string to the shell's `-c` option (like just `bash -c "${*}"` in your function) 2. Pass your "callback" through `eval` Neither option is especially secure or robust, but the former seems a little better, so I would do that.
Oh, I see. I thought that \`stuff\` and $(stuff) were capable of handling control operations. Apparently not? Is it because the subshell doesn't have the -c option? Would a subshell from a bash invocation with the -c flag be able to handle them? I'm not sure what exactly is different.
they are able to handle control operations but only explicit ones. In the very first stage of code mangling bash looks for these tokens to establish the shape of general flow/structure (pipes, &amp;&amp;/|| chains, redirections etc), only then it does expansions of $ and shit. Once you stringify shit and pass it as a param or whatever, &amp;&amp; or &gt; are just plain characters without meaning, as if you typed them escaped. The only way to restore that is by using the unsafe practice of treating an arbitrary string as code, either by eval or by farming the job out to bash -c "$*" x=$( some -shit | some &amp;&amp; other shit &gt; out ) # ok string='some -shit | some &amp;&amp; other shit &gt; out' x=$( $string ) # not ok, you get command some with a bunch of params including '&amp;&amp;' and '&gt;'
By the way, yes I know I'm obsessing over something very trivial (a bash prompt), but it's just bothering the hell out of me that I can't figure it out.
Don’t have time to look into your issue right now, sorry, but if &gt; The goal was just to display my git branch and status then I **have** to tell you about `__git_ps1`. The script is in `/usr/lib/git-core/git-sh-prompt` or `/usr/share/git/completion/git-prompt.sh` (depending on distro, possibly elsewhere too). Read its documentation, then use it. [Example usage](https://github.com/lucaswerkmeister/home/blob/master/.bashrc.d/40%20prompt). Will come back tomorrow to look into your problem. Good luck :) EDIT: Looks like /u/geirha [solved it](https://www.reddit.com/r/bash/comments/36k97i/yet_another_bash_prompt_ps1_question/crezdlp).
Actually that does help. That seems a litle heavy-handed for what I want to do, but within it lies the answer I think -- `PROMPT_COMMAND`. I forgot all about that. Thanks.
Well everything else is easy. It's escaping the damn control sequences for color that's throwing me off. I might just give up and steal from one of these people. At this point I don't care so much about the prompt, but how to make what I was trying work, you know?
&gt; __git_ps1 type __git_ps1 Yeah... That's actually really awesome. Ok. I'm definitely using this now and I'm all set. I'm still not sure why I couldn't escape those control sequences properly with my approach, but I see no reason not to just use this instead of hack together my own. Thanks again.
The `\[ \]` only work when they're in the PS1 value as it gets parsed; they won't work if they're the result of an expansion (like `$var` or `$(cmd)`). However, bash translates `\[` and `\]` into byte 1 and 2 respectively, which is what readline really uses to determine the width of the prompt, so you can circumvent that step by using byte 1 and 2 instead of `\[` and `\]`. An example adding `(NNN)`, where NNN is exit status (in red), to the prompt when the previous command fails: red=$(tput setaf 1) reset=$(tput sgr0) exit_status() { local res=$? if (( res != 0 )); then printf ' (\1%s\2%s\1%s\2)' "$red" "$res" "$reset" fi } PS1='\u@\h:\w$(exit_status)\$ '
Do not [reinvent the wheel](https://github.com/magicmonty/bash-git-prompt).
 awk '/string[245]/ {print $3}' input.txt | awk '{getline b; getline c; printf("%s %s %s \n", $0,b,c)}' &gt; final_output.csv don't know to simply further
I get only this when i open the csv file &gt;\#output_id ,\#output_type ,\#addkey edit: it has to do with the way my file is structured. I made my example more simple than it really is, hoping it doesn't matter. it actually goes like this: &gt;RECORD1 &gt;\#input_id &gt;\#output_id &gt;\#input_type &gt;\#something &gt;\#something2 &gt;F string1 value1 &gt;F string2 value2 &gt;F string3 value3 &gt;F string4 value4 &gt;F string5 value5 &gt;F string6 value6 edit: ~~why o why are these words bold and bigger font?~~ smart people use \
I've put the name "string" here because they are rather long, and very different from each other. 
 $ cat input.txt RECORD1 F string1 value1 F string2 value2 F string3 value3 F string4 value4 F string5 value5 F string6 value6 RECORD2 F string1 value7 F string2 value8 F string3 value9 F string4 value10 F string5 value11 F string6 value12 $ awk -v RS= -v FS="\n" '{printf("%s,%s,%s\r\n", $3, $5, $6);}' input.txt F string2 value2,F string4 value4,F string5 value5 F string2 value8,F string4 value10,F string5 value11 
 awk '/string2/ || /string4/ || /string5/ {print $3}'
bash tells me I am mising &gt;&gt;&gt; { &lt;&lt;&lt; somewhere, but I cant find it. I have literally replaced strings with the correct names, no funky characters, only word_word_word But thanks for this, I will figure it out
Use paste (nifty little program I think of too little) and a positive lookbehind, which requires perl-compatible (GNU) grep. grep -Po '(?&lt;=string[245] ).*' input.txt | paste -d, - - - &gt; output.csv If you don't have a grep available with the perl extension, use grep+sed or awk as shown in the other answers. I guess this one wins for conciseness though ;) edit: since you're learning, you might enjoy this as well for adding a header and formatting columns grep -Po '(?&lt;=string[245] ).*' input.txt | { echo 'string2,string4,string5'; paste -d, - - -; } | column -t -s, string2 string4 string5 value2 value4 value5 value8 value10 value11
Thank you, I will try that tomorrow. And yes, I was thinking of adding header, but couldn't do it haha. Thanks for that 
It works now. I have no idea why it didn't work yesterday, maybe I copied some funky character. 
Don't source `.bash_profile` from `.bashrc`. It should be the other way around. See http://mywiki.wooledge.org/DotFiles
I prefer GNU Stow rather than the manual install scripts. It also symlinks and it keeps the directory structure consistent with what ~ actually looks like and makes it super easy to add/change/install stuff. But nice, looks good! See mine here: https://github.com/CodyReichert/dotfiles
try eval $cmd courtesy: http://stackoverflow.com/questions/2355148/run-a-string-as-a-command-within-a-bash-script also, a good link: http://mywiki.wooledge.org/BashFAQ/050
Excellent!
Do you really need this: echo $cmd ? Because that's really the sticking point in your script. Otherwise you wouldn't have cmd as a variable that you're trying to use - you'd just outright run the command and move on. And there's your simplest answer: just replace the line command $cmd with mysql -u'${user}' -p'${pw}' algo --execute="SELECT algo.test();" Another way to do it would be to make cmd a function, something like this (untested, obviously): #!/usr/bin/bash if [[ "$#" -lt 3 ]]; then printf "%s\n" "ERROR: Missing arg(s)." \ "Usage: ./myscript.sh arg username password" exit 1 fi arg=$1 user=$2 pw=$3 cmd() { mysql -u "${user}" -p "${pw}" algo --execute="SELECT algo.test();" } printf "%s\n" "Initiating mysql connection with the following details:" \ "mysql -u ${user} -p ${pw} algo --execute=\"SELECT algo.test();\"" cmd Also, where is ${arg} used?
Don't mash multiple arguments into a single string. See [BashFAQ 50](http://mywiki.wooledge.org/BashFAQ/050). 
&gt; * dashes are special characters No, they're not. -- &gt; http://i.imgur.com/4AcVEOH.png The line should be mysql "-u$user" "-p$pw" -e "SELECT algo.test();" algo or instead of passing the mysql commands via -e/--execute, you can also use stdin: mysql "-u$user" "-p$pw" algo &lt;&lt; 'EOF' SELECT algo.test(); EOF -- If you insist on storing it so you can output it before executing it, then at least use an array: args=( "-u$user" "-p$pw" -e "SELECT algo.test();" algo) printf mysql; printf ' %q' "${args[@]}"; printf '\n' mysql "${args[@]}" 
Here is a method with parameter substitution using glob patterns: # Since filepath comes last in the 'read', this will work even if filepath contains a comma. while IFS=',' read -r filedate filetime filepath do scannerid=${filepath#[!/]*/}; scannerid=${scannerid%%/*} adsvolid=${filepath#[!/]*/}; adsvolid=${adsvolid#[!/]*/}; adsvolid=${adsvolid%%/*} filename=${filepath##*/} echo "$filedate,$filetime,$filepath,$scannerid,$filename,$adsvolid" done &gt; $joblog &lt; $templogimg No external process, so it's quite fast. Also, this doesn't use any bash-isms (neither does the rest of your script). On Debian's dash (their default /bin/sh), it's about five to ten times as fast still. (Note that it's more efficient to redirect the entire loop's output to $joblog rather than append each echo's output separately.) *edit:* Also note that: * `echo` is unportable (some variants interpret backslash escapes) so if you want it to work reliably on any shell, you should use `printf '%s\n'` instead. * Unquoted variables in the arguments to commands are subject to field splitting and globbing, so if you want guaranteed-correct output, they should be enclosed in double quotes. *yet another edit* There is another way to split $filepath, using IFS and the positional parameters ($1, $2, etc.). This is perhaps better, but it requires saving and restoring $IFS if you don't want the rest of your script to break, and it's only good if you don't need to keep your existing parameters. saveIFS=$IFS IFS='/' while IFS=',' read -r filedate filetime filepath do set -- $filepath scannerid=$2 adsvolid=$3 filename=$4 echo "$filedate,$filetime,$filepath,$scannerid,$filename,$adsvolid" done &gt; $joblog &lt; $templogimg IFS=$saveIFS Finally, if you can guarantee that $filedate and $filetime don't contain any slashes, you can make it even simpler by treating the slash and the comma as an equivalent separator. while IFS=',/' read -r filedate filetime pathprefix scannerid adsvolid filename do filepath=$pathprefix/$scannerid/$adsvolid/$filename echo "$filedate,$filetime,$filepath,$scannerid,$filename,$adsvolid" done &gt; $joblog &lt; $templogimg 
sweet reply thanks. There's a bit to take in there, I will check it out and report back.
&lt;esc&gt; follow by a . gives you your last command last argument!
I think you're confusing `!!` with `!$`: * `!$` expands to the last argument to the previous command * `!*` expands to all the arguments to the previous command * `!!` expands to the entirety of the last command And you can mix all kinds of stuff, which makes history expansion super powerful: * `!-2$`: the last argument to the next-to-last command * `!ls:*`: all the arguments to the last command that starts with ls * `!!:gs/foo/bar`: the last command with all instances of `foo` replaced with `bar` See the HISTORY EXPANSION section of bash(1) for all the minutia.
&gt; why bother with saveIFS if you can just add / to read's local IFS? You had better guarantee a locale that doesn't include slashes in the date/time output. &gt; Btw, not using bashisms is nothing to write home about I suppose in /r/bash, that's quite true. Personally I happen to care about cross-platform shell programming. The world is bigger than the GNU universe. But there is no /r/sh, so here I am.
Most common ones I use that people seem to never remember are simple: "Ctrl+a" takes the cursor back to the beginning of a line. Great for when you have a huge command or string of commands . "cd -" takes you to the last directory you were in. Whether one level up or in some other completely different place. "Shift+ZZ" in vi saves and closes the file. Doesn't need to be in command mode.
&gt;"Shift+ZZ" in vi saves and closes the file. Doesn't need to be in command mode. It doesn't work in ~~command~~ insert or command-line mode for me. Only works in ~~insert~~ command mode.
 $ column -s, -t csvfile The maximum line it can handle is 2048 which sucks for me because I frequently work with very wide csv files. 
Just to be complete ! == bang, not exclamation. 😁
That would literally type two capital Z's in insert mode. I guess to be more clear on it, you can use it without a colon first in command mode. There are two modes (command/insert) but I usually and incorrectly refer to it as three modes. vi mode - Starts in this. This is where all the keyboard shortcuts work command mode - Type a colon and then vi will interpret further typed commands insert mode - anything typed is inserted in to the document That's my bad. It's always felt like 3 modes to me, but it is technically two. The Shift+ZZ works in command mode.
I fucked my post up anyway. I meant it only works in command mode (what you originally called *vi mode*). It does not work in command-line mode (what you called *command mode*) or in insert mode (we agree on this one). ;) I think it *is* three modes.
This similar thread from a few years ago has a lot of gold in it to go through: https://np.reddit.com/r/linux/comments/mi80x/give_me_that_one_command_you_wish_you_knew_years/ One of my favourite commands from the above thread was a tip for using `python` to easily make a directory reachable over the network. Very useful for quick-and-dirty transfers: python -m SimpleHTTPServer 8080 The one that I just learned after re-reading it is that you can get yourself an instant ASCII table (from 0-127) using `man ascii`.
One reason you might want `foo &lt; &lt;(bar)` is in the case of a while read loop. If you do `command | while read foo; do ... done` then the pipe causes your loop to be within a subshell, preventing you from updating variables in a more global scope of a script. By doing `while read foo; do ... done | &lt; &lt;(command)` you avoid that.
If you want a laugh in your office (of the cluefull), you can comment that it's secure because it uses HTTPS. ;-)
best part is the SSL cert! 
How else would you do a 'here doc' then?
You should check the $EUID environment variable and if it's 0 (meaning root) then print the message 'Why would you type such a stupid command you idiot!?!!'
 wall &lt;&lt; STR ... STR
To be fair, it's stupid whether you're root or not.
It's a growing trend in method of installing software.
Your whole code is wrong. v1=`echo "hi"` is the same as v1="hi" You're assigning a value to v1, not a command string. Also, you'll need to parse a command string in a more complex way to execute it. Here's a solution: #!/bin/bash v0='echo zero;'; v1='echo one;'; v2='echo two;'; nr="$((RANDOM % 3))"; command="v$nr"; eval "${!command}"; 
Thanks, I'll give it a try!
Thanks a lot. That's exactly the sense I wanted to convey.
Thank you. Think I can improve with: `` replaced by $() Quote the find command But $ for i in "$(find . -type l -print)"; do mv -v $(readlink $i) $i done gives errors.. Ideas?
Just write to a temp file, then move it over the old. PATH=/usr/xpg4/bin:$PATH # makes sure the usable tools are first in PATH - with sed: sed '/^#mirror/c\ mirror=http://example.com/something' pkgutil.conf &gt; pkgutil.conf.tmp &amp;&amp; mv pkgutil.conf.tmp pkgutil.conf (yes, that's `\` followed by a literal newline, so the sed is on two lines) with awk: awk '/^#mirror/{$0="mirror=http://example.com/something"} {print}' pkgutil.conf &gt; pkgutil.conf.tmp &amp;&amp; mv pkgutil.conf.tmp pkgutil.conf Or you can use `ed` to edit the file. ed -s pkgutil.conf &lt;&lt; EOF /^#mirror/c mirror=http://example.com/something . w EOF
That must be GNU sed. BSD sed would throw an error at that.
LOL. Valid point. Also, your edit is great. Should be a required format for this sub. (though my bash man is 5372 :P)
you don't have to use [ $? = 0 ] just plug the command directly (unless you do this for readability/shorter lines, but then you could go with proper ifs instead of boolean shortcircuiting on a single line. `br[0-100]:)` that's not how globs work. They, just like regexes, are about characters and have no idea what a multidigit number range is. That character class says: `a number from 0-1 range OR 0 OR 0` $ [[ x1 = x[0-100] ]] &amp;&amp; echo true || echo false true $ [[ x2 = x[0-100] ]] &amp;&amp; echo true || echo false false once you whip out awk, greps, cuts and what not become superfluous. Also arrays. $ readarray -t devices &lt; &lt;( ip addr show | awk -F'[: ]+' '/^[0-9]+:/{ print $2 }' ) $ for d in "${devices[@]}"; do echo "$d"; done lo eth0 vboxnet0 $ ip addr show eth0 | awk -F'[/ ]+' '/inet/{ print $3 }' 192.168.50.10 
That's odd. I've never seen a BSD util that uses options with optional arguments; that's pretty much a GNU thing. Must be some hybrid conconction to please users coming from "linux". EDIT: Indeed, it's a hybrid. http://src.illumos.org/source/xref/illumos-gate/usr/src/cmd/sed/main.c#163
&gt;that's not how globs work. They, just like regexes, are about characters and have no idea what a multidigit number range is. Thank you. That will probably save me hours down the road. Also appreciate the awk-fu. Thanks for the review!
Just surprised because some parts don't look like something a shell scripting noob would write but then some do :) 
It's hard to say without seeing the code of your nametoID function. I'm noticing one other thing, though: echo $(nametoID "$name") is probably redundant. A $(command substitution) takes what the command writes to standard output and puts it into an argument to another command. But 'echo' then simply writes it to standard output anyway. So you should just do: nametoID "$name" which is also much faster because each command substitution launches a separate process in which to run that command. Only one difference: since you didn't quote the command substitution itself, it would have been subject to field splitting, and multiple blanks in the output (if any) would have been stripped. But that is probably irrelevant in this case. Now if you show your nametoID function, perhaps we can get some clue about why you're getting those errors. 
On mobile so can't provide exact syntax but look at rsync for what you're trying to accomplish 
rsync -Phav /source/ /destination/ Then i would make a cron to create a tar.gz in an archive location
I find it hard to believe that this works, because you're not updating the `count' variable, so you're basically replacing the zeroeth element of the tmparray.
A quick way to deal with CR/LF separated text files is to simply add the CR character ($'\r') to $IFS when you read stuff. That makes the 'read' command ignore it. while IFS=$IFS$'\r' read name; do ... 
Maybe you should take a look on rsnapshot.
When I open the file I get a bunch of garbage. I assumed it was compiled code. Just in case you're interested - When I do type nametoID I get the path (as I should right?) When I do typeset -f nametoID I get no output (btw, what is this command, I've never seen it before)
As many have said, rsync is the way to go. This is my "daily" script: http://pastebin.com/xshX9Jw8 I made a few changes recently (today) and it's not working, but it'll give you the jist of things. I do something similar with my weekly backup. For the monthly backup, I want to compress it so I came up with this script: http://pastebin.com/fw3Dgz6A It displays/e-mails/etc. as well - probably more than you need.
 while read NewVar; do ... done &lt; $directoryVar/file1.txt This iterates through file1.txt line by line, storing each line of the file into `$NewVar` before executing the body of the loop. Assuming you meant `echo $NewVar` and not `Echo NewVar` in your example, it will: 1. Read the first line of file1.txt into NewVar 2. Assign the first field of that line to Field1 3. Assign the second field of that line to Field2 4. Repeat steps 1-3 for the next line of the file until it reaches EOF 
That's a rather poorly written script. And it doesn't help that you've taken the artistic liberty of breaking it further when writing it here on reddit, by writing `Echo` instead of `echo`. What the code should look like, when using good practices, is: while IFS='~' read -r field1 field2 _; do printf 'Some command using %s and %s here\n' "$field1" "$field2" done &lt; "$directoryVar/file1.txt" This is reading file1.txt line-by-line, and for each line, it puts the first field in the variable `field1`, the second field in the variable `field2`, and the rest of the line (if any) in the variable `_`. It splits the line into fields by the characters in the `IFS` variable, which we've set to `~` in this case, since that's what those `cut`s where cutting on. See [BashFAQ 1](http://mywiki.wooledge.org/BashFAQ/001) for more on that. Also, I strongly recommend reading the [BashGuide](http://mywiki.wooledge.org/BashGuide) to learn the basics of bash.
Yes that is what I attempted to type. Hard on mobile with auto correct. I'm slowly getting all of my access and rights set up so I can't see the file this is talking to yet, but aside from my alias question I'm not sure why it's reading the entire file through overwriting those two variables constantly. I've seen something similar where it's SQL looking for the latest date time but that doesn't seem to be the case here. 
I want to make sure I'm clear and understand your reply. I'm not trying to run this process from an HTML form. I have a script /home/me/scripts/symlinks.sh that accepts a single argument of a text file tab delimited. The script parses that text file and creates symlinks from the names provided in the text file. I can run that script all day long from my command line and it works every time. However, instead of me having to request the text files and generate symlinks for others with less familiarity with the cmd line I'm trying to make it web accessible for our tech support reps to be able to do this themselves. What I'm trying to accomplish is a web interface that allows an upload of the text file that can turn to an internal script (running on the same server as apache is) and run the script and display a "Completed" to the user. That way they are hands off the server and I don't need to train them to use the script.
Another way to skin this cat would be: while read NewVar; do Field1="${NewVar%%~*}" Field2="${NewVar#*~}" done &lt; "$directoryVar/file1.txt" But, to answer your question `$()` takes the stdout from whatever is run inside it and allows you to treat it like a variable. `echo $NewVar` simply echoes / prints / displays the contents of the variable `$NewVar`. `|` pipes stdout from the previous `echo` statement to the command `cut`, and `cut -d~ -f1` echos the first field, when using '~' as a field separator. `&lt; $directoryVar/file1.txt` is saying redirect the contents of file `$directoryVar/file1.txt` into the previous command, which, in this case, is a while loop. `read NewVar` reads each line of input (from the redirected file) into a variable `$NewVar`.
actually, /u/geirha figured it out. Thanks for the help though!
Your Google form doesn't offer a choice for Mac OS X Snow Leopard. It only offers Leopard and earlier, or Lion and later.
...I think you're in the wrong spot. 
Have you tried running the script with 'set -x' to see what commands are being executed? http://mywiki.wooledge.org/BashGuide/Practices#Activate_Bash.27s_Debug_Mode
/u/geirha figured it out, thanks for the reply though
Vaphell - You raise a valid point in that I did not effectively make the case about the cost of variables before I gave examples about how to get rid of them. Variables, as you know, can allow you to do things that there is no way to do with pipes. And your point about being careful about what is done in loops is also good to keep in mind. I gave the example about the while loop simply to indicate how to transition between a pipeline structure and a looping/variable structure when such is needful: I am not advocating that breaking into a loop/variable form is a good idea if you do not have to. I am not against the use of loops/variables but rather against the overruse or unnecessary use of them of them because they can be costly in terms of speed. When it is possible to accomplish something with a pipeline, and you throw a loop or shell variable into it unnecessarily, you can incur a speed penalty. To see what kind of speed penalty I mean, try executing time seq 100000|while read n;do factor $n;done|awk 'NF==2{print $2}' and compare that to time seq 100000|factor|awk 'NF==2{print $2}' On my computer, the second approach runs more than 200 times faster than the first. I use loops and shell variables all the time in my own shell scripting because they are useful, but I try to do so while being mindful of what they can cost in terms of overall script execution speed. My book contains literally hundreds of examples of loops and variable-based shell programming simply because they are so useful and powerful. You are quite correct in that there are alternate ways of doing things, and that pipelines are not a universal solution. They are merely one way among others to get something done, one tool in a box of tools. But they do tend to provide some speed advantages. That was the point I was trying to make in the talk despite how clumsy my attempt at making it might have been.
I did think that python script was pretty strange. Next time I'm at the Large Hadron Collider (which will probably be never) I'll let them know. 
&gt; Next time I'm at the Large Hadron Collider (which will be sometime down the road) Fixed that for you. Stay optimistic ;)
Shit, I don't even know where this goes. Some weirdo had a form somewhere. I think it's this: http://www.reddit.com/r/Favors/comments/38pr2e/request_people_with_a_laptop_touchpad_please_tell/
 wc &lt;(cat &lt;&lt;EOF hello how are you it is nice to meet you EOF) This is a variant of the "redundant use of `cat`" disease I haven't seen before. Of course, it's better and more efficient to simply do: wc &lt;&lt;EOF hello how are you it is nice to meet you EOF and cores=`python &lt;&lt;EOF import multiprocessing print (multiprocessing.cpu_count()) EOF ` echo Using $cores cores. 
`num_procs=$(python -c "import multiprocessing; print multiprocessing.cpu_count()")` isn't good enough for you?
Sure, but that is the same thing that you called a horrible misuse of python, just with a different shell construct.
Fair enough. Without seeing the rest of the script, I'd bet it would probably be fine in 'pure' python. The way I proposed is a minor improvement in that it's not multi-line, and it's not trying to feed a 'file' to python, which is just ugly.
&gt;This is a variant of the "redundant use of `cat`" disease I haven't seen before. So, I'm wondering, should I use e.g. printfs instead of cat, when doing: cat &lt;&lt;"EOF" Stuff EOT ?
That is not a redundant use of `cat` because in that case you have nothing else that copies standard input to standard output. So that's fine. If you want to do it without launching any external program, you can do this instead: while IFS='' read -r line; do printf '%s\n' "$line"; done &lt;&lt;EOF Stuff EOF This only works for text but a here-document is text and you don't want to display non-text on the terminal anyway. For short texts, this is considerably faster than `cat`, because launching external processes is quite expensive. (On bash and most other shells except pdksh/mksh, 'printf' is built in to the shell.) You could make a shell function to make this more concise: showtext() { while IFS='' read -r line; do printf '%s\n' "$line" done } showtext &lt;&lt;EOF Stuff EOF 
Because newer versions are under the GPL version 3 which Apple considers poisonous or something. They won't ship anything that's under that license.
Does that mean apple is just going to stop updating bash completely?
[Apple’s great GPL purge](http://meta.ath0.com/2012/02/05/apples-great-gpl-purge/).
This is one of the major reasons why I shredded my Apple Fan Boy membership card. I was a Mac user my whole life (literally, I would mash keys on an Apple IIe when I was barely old enough to walk). About 6 years ago I bought my first non-Apple computer (Dell with Ubuntu pre-installed) and haven't looked back or missed them at all.
They might change it to other shell (maybe the one they could create).
From [Wikipedia](http://en.wikipedia.org/wiki/GNU_General_Public_License#Legal_barrier_to_app_stores): &gt; The GPL License is incompatible with many application-distribution-systems, like the Mac App Store, and certain other software distribution platforms (on smartphones as well as PCs). The problem lies in the right "To make a copy for your neighbour", as this right is violated by the integrated DRM-Systems made to prevent copying of paid software. Even if the application is free-as-in-beer in the App Store in question, it might result in a violation of that app store's terms. ... &gt; In other cases, such as the Ubuntu App Store, proprietary commercial software applications and GPL-licensed applications are both available via the same system; the reason that the Mac App Store (and similar projects) is incompatible with GPL-licensed apps is not inherent in the concept of an app store, but is rather specifically due to Apple's terms-of-use requirement that all apps in the store utilize Apple DRM-restrictions.
No need for downvotes here. I'm pretty much a lifelong Mac user and even I think /u/Ramin_HAL9001 is absolutely right. We have to run something like homebrew or MacPorts to get a halfway decent UNIX-like environment on the command line. Even then, HFS+ really is a very bad filesystem. Never mind slowness and brittleness, it's fundamentally incompatible with the rest of the world. It uses a [bizarre proprietary "decomposed" form of not-quite-UTF-8](http://search.cpan.org/~tomita/Encode-UTF8Mac-0.04/lib/Encode/UTF8Mac.pm) to store its filenames. This bizarre form is *not* converted back to proper UTF-8 when you do something like "set -- *" in bash, so now it's impossible to accurately match filenames that contain non-ASCII characters against proper UTF-8 strings, or measure their length. (It's really funny -- even something like /bin/ls as shipped by Apple doesn't cope with this! For fun, try "touch 'sómè fïleñâme'", then run 'ls' and watch it screw up the column layout. It would be hilarious if it weren't so sad. Now imagine being Japanese or Chinese and trying to use /bin/ls on your Documents folder.)
Thanks for not down-voting me. Yeah, I once had a tar archive of a file tree with lots of symbolic links and mixed language file names from a Linux system, and I tried to untar it on Mac OS X -- boy did that **not** go well. There have been other times when `grep` just behaves very strangely when matching on the output of `find` which I have never figured out why, but I believe has something to do with how OS X normalizes UTF-8 strings, or maybe `grep` flags that work on Linux are no-ops in Mac OS X's default `grep` or something, I really don't know. I just don't mess with it anymore, life is too short.
web server application? wut?
web server application? wut?
If you want to keep the slash: sed 's/\(.*\/\)\(.*\)/\/\2/' If not: sed 's/\(.*\/\)\(.*\)/\2/' Also, the `basename` command does exactly that.
That deletes the last slash as well, as does basename(1).
I was editing as you replied.
A more complete title should have been used for the benefit of all the readers.
 echo "/foo/bar" | cut -d'/' -f3 That is simpler. The only downside is that you need to know which field number you want, in order to use cut.
The description does not seem to be clear. There are several 22s in your data, which of these should be kept, what are beta and dumpel really?
Solved it! sed -n -E 's/(^| - [0-9][0-9]:[0-9][0-9].*[^0-9])([0-9]{1,4})&lt;.*/,\2,/p' tmp.txt &gt; tmp2.txt
What it does is takes inputs via cat'ing files and then uses them as part of generating a simple HTML page. Try it out and you will see. It has a blog feeling in that it has the posted date option and also a last rendered option (although that is more of an extra info feature).
What is the use case for this? I will give it a try.
Well for people who need to get a quick page with info running on their web server that can easily be edited. This is handy, I built it as I.needed it.
Fedora 22 just released a new feature in gnome that auto syncs your Google calender to your desktop. Would this work for you?
It's a way to use command arguments when given, but default to '`.`' (current directory). The `${}` is "parameter substitution". You can google that and find a lot of resources. Put this in a script and run it with some arguments, and then without: echo "${*-.}" It will print the arguments when given, and print '`.`' when they aren't given. Notice I put quotes around it. It's always a good idea to use double quotes when dealing with file names (or else spaces will present a problem). From [tldp.org](http://www.tldp.org/LDP/abs/html/parameter-substitution.html): &gt;"`${parameter-default}` and `${parameter:-default}` are almost equivalent. The extra : makes a difference only when parameter has been declared, but is null." ----- Also: In case it wasn't clear, it's a `bash` thing, not a `find` command thing. 
It is not specific to the `find` command, it is part of the Bash programming language "variable substitution" syntax, so it will work anywhere. You can, for example, do something like `tar czf backup.tgz "${*-.}"` or `grep 'keyword' -R "${*-.}"`. Basically, this variable substitution only works in a shell function or a script. So lets say you have a function like this: new_todo() { echo "$(date) ${*}" &gt;&gt;"$HOME/todo.list"; } Now you can call your function: new_todo pickup items from the shop All tokens after the function `new_todo` will be stored into the `${*}` variable. The tokens will then be substituted into the `echo` command in the function. Then you can `cat "${HOME}/todo.list" and you should see something like this: Tue Jun 16 12:34:56 UTC 2015 pickup items from the shop This will also work when you execute a script, the parameters (such as file names or directories) you pass to the script will be stored in "${*}" as a single text string. You can also iterate over each parameter in a for loop by using "${@}" instead or "${*}". 
So the other answers have already explained about the "default value" parameter expansion. So I'll skip that. &gt; find ${*-.} -type f -print This will fail for pathnames containing whitespace or other special characters. The use of the `*` parameter is completely wrong here. E.g., if you run the script with the two argument `foo bar` and `bar`, it will be equivalent to find foo bar baz -type f -print but you'd want it to be equivalent to find "foo bar" baz -type f -print What it should've been is: find "${@-.}" -type f -printf
`$*` and `$@` are always wrong when not enclosed in double quotes `"$*"` expands to all the arguments joined together into a single string, using the first character of IFS as the separator (space by default): $ set -- "foo bar" baz qux $ printf '&lt;%s&gt;\n' "$*" # same as printf '&lt;%s&gt;\n' "$1 $2 $3" &lt;foo bar baz qux&gt; $ IFS=, $ printf '&lt;%s&gt;\n' "$*" # same as printf '&lt;%s&gt;\n' "$1,$2,$3" &lt;foo bar,baz,qux&gt; `"$@"` is what you usually want. It expands to all arguments as separate words. $ set -- "foo bar" baz qux $ printf '&lt;%s&gt;\n' "$@" # same as printf '&lt;%s&gt;\n' "$1" "$2" "$3" &lt;foo bar&gt; &lt;baz&gt; &lt;qux&gt;
Sorry, I meant ${*}. Oh, I see. Sounds coherent.
what do they mean exactly?
`${x~}` toggles the casing of the first character. `${x~~}` toggles the casing of all characters. $ x='foo BAR' $ echo "${x~}" Foo BAR $ echo "${x~~}" FOO bar 
.bash_profile paste http://pastebin.com/4fw1m3np
kill/ps is personal preference. It's also makes more sense to run a ```ps``` to check it than ```kill -0```ing it in my opinion--we're just checking the exit code after all. Yes--in general it's better to redirect stderr to /dev/null or elsewhere--however it will never cause weird issues like you've shown if you're using it with ps. Your echo example is sort of a fringe case, since the actual error echo is returning is a write error: Bad file descriptor. [root@sandbox /]# if echo &gt; /dev/null; then echo true; else echo false; fi true [root@sandbox /]# if echo &gt;&amp;-; then echo true; else echo false; fi bash: echo: write error: Bad file descriptor false edit: yeah--could just sleep for five seconds or w/e if you want.
Are you running bash or zsh? Those look like zsh errors. ps -p $$
Yeah. for example in my zsh: ╭─&lt;nhanlon@hanlonn-deb1&gt;-&lt;~&gt;-&lt;1:43PM&gt;-◇ ╰─➤ source .bashrc .bashrc:16: command not found: shopt .bashrc:24: command not found: shopt .bashrc:107: command not found: shopt /usr/share/bash-completion/bash_completion:51: command not found: shopt /usr/share/bash-completion/bash_completion:57: command not found: complete /usr/share/bash-completion/bash_completion:62: command not found: complete /usr/share/bash-completion/bash_completion:65: command not found: complete /usr/share/bash-completion/bash_completion:68: command not found: complete /usr/share/bash-completion/bash_completion:71: command not found: complete /usr/share/bash-completion/bash_completion:74: command not found: complete /usr/share/bash-completion/bash_completion:77: command not found: complete /usr/share/bash-completion/bash_completion:80: command not found: complete /usr/share/bash-completion/bash_completion:83: command not found: complete /usr/share/bash-completion/bash_completion:86: command not found: complete /usr/share/bash-completion/bash_completion:89: command not found: complete /usr/share/bash-completion/bash_completion:92: command not found: complete /usr/share/bash-completion/bash_completion:314: parse error near `\n' \[\e]0;\u@\h: \w\a\]\u@\h:\w$ Can always just run a ```chsh -s $(which bash)``` 
`$!` expands to the most recently executed background process, that was used here. `pgrep` and `ps` were used to determine if that process is still alive, but you are right, these are external, the command `kill` is a builtin alternative.
All good, thanks mate! Had my shell set to zsh in my user.
One of the easiest ways is probably using timeout like /u/RalphCorderoy said: #!/bin/bash # kill command after 1 second and set timeout to 1 second timeout -k 1 1 sleep 11 # get the return code of the last command (timeout) r_code="$?" # check the return code of timeout (124 is time out) if [[ $r_code == "124" ]]; then echo "Timeout!" else echo "Success!" fi Syntax of timeout: timeout -k (kill after) 1 (kill after 1 second) 1 (timeout after 1 second) sleep 11 (command)
This works well on Linux but is not available for Solaris. I should have stated that in the post. Solaris has a timeout function that I am reading through. Thanks
Maybe I'm not understanding your problem, it seems like your sample in is already what you want your sample out to be, but either way, (gnu) `sort` has a `-M` option to sort by month: AirBoxOmega:~ d$ cat /tmp/rdt.help /path/to/files/Filename_with_date_like_2015-06-17_0-31.mp3 /path/to/files/Filename_with_date_like_2015-06-14_2-25.mp3 /path/to/files/Filename_with_date_like_2015-06-16_0-28.mp3 AirBoxOmega:~ d$ sort -M /tmp/rdt.help /path/to/files/Filename_with_date_like_2015-06-14_2-25.mp3 /path/to/files/Filename_with_date_like_2015-06-16_0-28.mp3 /path/to/files/Filename_with_date_like_2015-06-17_0-31.mp3 AirBoxOmega:~ d$
It's all for the sake of reducing discrepancies between the output of the script and the actual numbers. If you open the messages.htm HTML file in your favorite text editor, you'll see that there are no newlines and that some deletion is absolutely necessary. For instance, in the HTML file, for every thread, there are headers containing the name of the person(s) you conversed with. If "$2" happens to be a part of a person's name in a header, the name in the header will be included in the word count. Also, there are certain parts in the HTML file that are not relevant to what I am trying to accomplish with the script and would therefore need to be deleted. TL;DR: Your command does not take into account the lack of newlines and unnecessary parts of the file and would therefore cause discrepancies between the output and the actual number.
Fair enough. I'm on mobile so can't type all I want at the moment, but it could use a cleanup and I'll offer one later. 
Are you trying to search for a literal regex? Either way, I know you asked in /r/bash so this is going to be like you asked what type of sandwich to eat and I said "fuck sandwiches, eat pizza", but why not use the facebook api? I've never used it but I'm guessing there are plenty of high-level api client classes in your language of choice, and there is probably a `getMessages($friend = null)` (pseudocode) method that would return nice and clean JSON that you could then parse. It would eliminate the need to strip html tags and such as well as having to first download the file. **EDIT:** Come to think of it, the FB API probably exposes a `searchMessages($string)` type method directly.. 
This is "[process substitution](http://www.gnu.org/software/bash/manual/html_node/Process-Substitution.html#Process-Substitution)". Exactly how it is implemented depends on what the kernel supports. On Linux, it uses `/dev/fd`, as you saw. On a typical GNU/Linux system, if you use `ls -l` on `/dev/fd`, you will see that it is a symbolic link to `/proc/self/fd`. On many unixoid kernels, `/proc` is a special mountpoint that exposes information about running processes; a process' information is in the `/proc/$PID` directory, and `/proc/self` is even more special because it is information about the current process. On some systems, `ps` is implemented by simply looping over the directories in `/proc`. Because the `/proc` filesystem is a special filesystem implemented in the kernel, you can't just edit it in any which way. I'm not sure if you are familiar with C, but unixoid systems, files are implemented as integer file descriptors that are indexes in a big array of file information in the kernel (each process gets its own array). (Even if you do know C, you might not have dealt with that; libc provides a buffered wrapper (`FILE *`) around them; batching I/O turns out to usually be a big speed boost). Well, that table is part of the information provided about a process in `/proc`. You can peek in `/proc/$PID/fd/` to see what files a process has open--the kernel shows you that by showing you a symbolic link named with the integer file descriptor, linking to the file that is open for that descriptor. Well, all "files" are opened as file descriptors, even if they aren't actually a file on a drive. When you pipe two commands together, that pipe is a file. You you looked at `/proc/$PID/fd/1` for a program that you piped to another program you would see that it links to something like `pipe[334649]`, which isn't a real file. But, if you try to open it, the kernel will duplicate the file handle that the process has open (this is a handy trick if you do something bad like delete the database. If the database is still running, and has the file open, you can write a program to open it from the file in `/proc/$PID/fd`, re-link it, and save the day!). Anyway, when you do `&lt;(some command)`, Bash makes a pipe for stdout (file descriptor 1) of the command, just like if you were piping it to another command normally. But instead of connecting the read end of the pipe to a process you piped it to, it gives a path that another program can use to open the pipe. The reason you couldn't edit it in vim is that a pipe has a read-end and a write-end; it will fail when vim tries to open the read-end for writing. You could have gone the other way; `&gt;(some command)` has Bash make a pipe for stdin (file descriptor 0) of the command, and give a path to the write-end (example: `youtube-dl -o &gt;(vlc -) http://example.com`) Some kernels don't implement `/dev/fd` as a link to `/proc/self/fd`, some implement it as it's own special filesystem. That doesn't change the concept though. Some kernels don't support `/dev/fd`, but they do support making named filesystem pipes (you can do this with the `mkfifo` command in the shell). If that's the case, Bash will make a temporary file that is a named pipe (that is, it will make a pipe like normal, but then link it to a real file on the filesystem), and give the path to that.
Are the files in /dev/fd actually files that store the contents? Or are they just empty files with integers as names? 
Ah ok, so no data is stored in these files, in terms of storing data on disk. My understanding is that these files (FIFOS) are simply "names" so programs can treat them as files. 
Pretty much yeah.
&gt; On Linux, the set of file descriptors open in a process can be accessed under the path /proc/PID/fd/, where PID is the process identifier. Sorry, couple other bits that are just nagging me. File descriptors seem to be represented as files too in Linux. Are they themselves considered one of the special "files"? &gt; In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories, block and character devices (also called "special files"), Unix domain sockets, and named pipes. [wikipedia FD] (https://en.wikipedia.org/wiki/File_descriptor) So apparently FD's aren't just represented as files, but can REFER to files somehow. Any idea how this might be done? 
There are two types of file descriptors. For the IO type, they are files that don't behave like a normal file. Whenever you write data to the file descriptor, data is written to the stream/device/network/etc, and when you read from the file descriptor data is read from there too. The second type simply points to other locations on the disk. For instance, a directory points to the locations of the files and directories inside it.
k, thanks for all the help so far
Not gorgeous by any stretch, but my simple hack was to use sed to move the sort-string to the front, mark its position with a null character, sort it, and then move it back where it goes with sed again. This should work reasonably well if your date format is fairly consistent: spdqbr@MyBox 00:17:30 /cygdrive/c/temp 2427 $ cat test.txt s3://ntsprod/podcasts/DJ_Skunkie_2015-06-17_0-31.mp3 s3://ntsprod/podcasts/DJ_Charlie_2015-06-19_0-27.mp3 s3://ntsprod/podcasts/DJ_Skunkie_2015-06-15_0-53.mp3 s3://ntsprod/podcasts/DJ_Ali_2015-06-17_0-58.mp3 s3://ntsprod/podcasts/DJ_Skunkie_2015-06-16_0-31.mp3 spdqbr@MyBox 00:17:40 /cygdrive/c/temp 2426 $ cat test.txt | \ &gt; sed -e 's/^\(.*\)\([0-9]\{4\}-[0-9][0-9]-[0-9][0-9].*\)/\2\x0\1/g' | \ &gt; sort | \ &gt; sed -e 's/^\(.*\)\x0\(.*\)/\2\1/g' s3://ntsprod/podcasts/DJ_Skunkie_2015-06-15_0-53.mp3 s3://ntsprod/podcasts/DJ_Skunkie_2015-06-16_0-31.mp3 s3://ntsprod/podcasts/DJ_Skunkie_2015-06-17_0-31.mp3 s3://ntsprod/podcasts/DJ_Ali_2015-06-17_0-58.mp3 s3://ntsprod/podcasts/DJ_Charlie_2015-06-19_0-27.mp3
A bit shorter with awk and cut awk -F_ '{print $(NF-1)"_"$(NF)"\t"$0}' test.txt | sort -r | cut -f2-
The file and data will be in ram. 
That almost has it perfect! And I think I can finish it from there. Thank you!
Could you use an HTML form and lynx(1), links(1), etc?
I could, but if I was going to go the HTML way, I'd just set it up on my server. It would be easier that way for sure. But I have experience with php&lt;--&gt;postresql forms, reports, etc.. I'm just trying to scratch a different itch.
I updated my code to reflect the changes suggested in this thread. Would you like to take a look at it and see if you had any more suggestions?
If you want to do this in a shell script, you'll probably find the `dialog` program helpful. See the `--form` and `--mixedform` options. You might want to google for code examples since it's kind of hard to wrap your head around with just the manual page.
I would make `someCommand` not need to receive those arguments in that quoted fashion, if that were an option. Other than that, to escape a `"`, you would write it as `\"`. Your other option is to just use `'`, since that will not expand arguments or be 'automatically' quoted any further. You could have found your answer 2+ hours ago if you bothered searching for 'escape quote character bash'.
Sadly, the command needs to receive the arguments as a quoted string...not much I do to change that. I did try the backslash before the quote, but for whatever reason, it didn't work as expected. Let me try it again.
I already know the package mangers for Linux. But I am look to do some automation for software installs
Your sleeps are unnecessary. There's no such thing as "letting things settle"--once a process completes, it is complete, and the next line is executed. You're not forking any processes into the background, so all your sleep lines are doing are making the script take longer to execute. Additionally, you're not really installing the "latest" version of tmux. You're installing a specific version of tmux, and a specific version of libevent. For it to be the latest, you must programmatically determine the latest version of the software you wish to compile. The script is... okay, and certainly a great first attempt. There are a number of things I suggest: 1. variable-ize your variables. In your script, things like the URL to .tar.gz archives, etc, are all variables. Instead of putting them in and making it harder to change in the future, you can put them in variables like ```TMUX_PACKAGE=https://someurl.github.com/some/path/package.tar.gz``` and then use ```$TMUX_PACKAGE``` later in the script to curl it, whatever. 2. Use the ```-O``` option for curl (Oh) to output to the remote name of the file. So for: https://github.com/libevent/libevent/releases/download/release-2.0.22-stable/libevent-2.0.22-stable.tar.gz, it would write "libevent-2.0.22-stable.tar.gz". You can then use something like: ```echo $LIBEVENT_PKG_URL | awk -F/ '{print $NF}'``` to get the name of the file that was written so you don't have to do any globbing to change directories like you are. What happens if someone is running this on a "dirty" directory and has multiple libevent-*'s in their working path. It will mess up the script. 3. Comments. Yes, it's good to comment a script, but don't comment absolutely everything. ```yum install blah``` is self explanatory. You don't need to say #install blah with yum. or anything else like that. If you want to separate out logic for different parts of the script, then use functions. You could have a function that installs ncurses. A function that installs libevent. One that gets your packages. One that installs rvm. Etc. http://tldp.org/LDP/abs/html/functions.html 4. Consistency: Just make sure you're consistent in how you do things in your script. For example. You source things twice in the script, but you do them in different ways. In one spot you do ```. somefile``` and in another, ```source anotherfile```. In BASH, ```source``` is a synonym for ```.```. Personally I like ```source``` as it is more widely used and compatible with other shells like ZSH, but that's just my opinion. 5. Let's talk about this line. ```curl -L get.rvm.io | bash -s stable``` Yes--I know that's how RVM says to install RVM. But it's not safe. If that domain is ever hijacked, changed, whatever--you're sending the output of a potentially malicious script right into a shell without checking it at all. Maybe you want to download it and compare the file against an MD5 Hash to make sure it's what you expect it to be. Just don't ever pipe into bash. It is a really, really bad idea.
Just a few random notes: * Why have those "sleep 5" everywhere? If it's to give you time to abort in case something went wrong, consider just aborting the whole script instead. Couple ideas can be found here - [Stackoverflow: Exit entire bash script](http://stackoverflow.com/questions/1378274/in-a-bash-script-how-can-i-exit-the-entire-script-if-a-certain-condition-occurs) * Create a temporary directory and download the source there (instead of using the current one). mktemp -d is your friend here. Cleanup afterwards. * Instead of modifying the script every time a new version of libevent/tmux is released (I mean, this is why you have a script, right? To not do manual lookups?) use GitHub APIs to get the link to the tarball. [Official documentation](https://developer.github.com/v3/) For example a quick (and ugly) way to get the download link for libevent: curl -s https://api.github.com/repos/libevent/libevent/releases/latest | \ grep -Eo 'https.*stable.tar.gz' | \ head -n 1 * Every time you run your script you add another "export LD_LIBRARY_PATH" entry to your bash_profile... Not really kosher... * Since your script requires root access, you should just check for it at the beginning of the script. But overall, if the script makes your life easier then I'd call it a success :)
I would break the parts of the script into manageable functions. For example: function install_libevent() { yum install -y ncurses-devel yum groupinstall -y 'development tools' } function install_rvm() { curl -L get.rvm.io | bash -s stable source /etc/profile.d/rvm.sh rvm reload rvm install 2.1 yum install rubygems } function install_tmuxinator() { gem install tmuxinator } Finally, at the and of the script I would invoke them with: install_libevent sleep 5 install_rvm sleep 5 install_temuxinator sleep 5 This way you can *name* the various segments of your code, making it easier to digest, potentially reusable, and generally nicer for other humans.
I've never used haskell, but I'll take a look at it. Thanks
&gt; TMUX_PACKAGE=https://someurl.github.com/some/path/package.tar.gz and then use $TMUX_PACKAGE later in the script to curl it, You'll want to do: "$TMUX_PACKAGE"
Check out http://www.shellcheck.net/ - might give you some answers.
It shouldn't be a problem, it uses the utilities pngcrush and optipng to optimize PNG-images filesize-wise. It doesn't do any reprocessing of the image. Basically it just improves the filesize, leaving the image quality as is. 
This is closer to what I am looking for thank you I will work on it and let you know if it work for me. Thank you so much.
Sure thing. One thing I also try to do when I create a script that I plan on using on different systems is check to make sure special tools (like `dialog`) are installed on the system before continuing. You can do this in bash like this: if ! hash dialog 2&gt;/dev/null; then # dialog not installed, attempt to install it printf "attempting to install dialog...\n" apt-get -y -f --force-yes update apt-get -y -f --force-yes install dialog fi Since you plan on using it on both Debian based and Red Hat based systems, you will need to check which package manager is on the current system and use the appropriate package manager commands in the script. if [ -f /etc/debian_version ]; then pman="apt" elif [ -f /etc/redhat-release ]; then pman="yum" fi ... case $choice in 1) printf "Installing Apache2...\n" if [[ "${pman}" -eq "apt" ]]; then apt-get -y -f --force-yes install apache2 elif [[ "${pman}" -eq "yum" ]]; then yum -y install apache2 else printf "unkown package manager...exiting.\n\n" exit 1 fi ;; 
Also: http://www.shellcheck.net/
Thanks for the reply. :-) ...I had been using shellcheck and reading various documentation about loops/patterns. However, I'm still too green to connect the dots between an error message and a solution in this case.
Thanks for everyone's feedback so far :) I just have one question that I'm kind of stuck on. LIBEVENT_PACKAGE=$(curl -s https://api.github.com/repos/libevent/libevent/releases/latest | grep -Eo 'https.*stable.tar.gz' | head -n 1) TMUX_PACKAGE=$(curl -s https://api.github.com/repos/tmux/tmux/releases/latest | grep -Eo 'https.*.tar.gz' | head -n 1) LIBEVENT_PACKAGE_TAR=$(echo $LIBEVENT_PACKAGE | awk -F/ '{print $NF}') TMUX_PACKAGE_TAR=$(echo $TMUX_PACKAGE | awk -F/ '{print $NF}') With the above, I have 4 variables; 2 url's with the latest archive using the API URL and 2 tar.gz filename's. What would be the best way to create 2 new variables e.g. LIBEVENT_PACKAGE_DIR and TMUX_PACKAGE_DIR which would be echoing LIBEVENT_PACKAGE_TAR without the tar.gz ? That way I could then do **cd $TMUX_PACKAGE_DIR** 
https://www.reddit.com/r/bash/comments/3as211/new_at_bash_scripting_and_github_need_opinion/csgjtip
you should use `file` to determine filetype. $ file --mime-type *.png *.jpg nm.png: image/png tard1.png: image/jpeg &lt;- ext not matching the content test_img.png: image/jpeg &lt;- ext not matching the content tard2.jpg: image/jpeg test_img.jpg: image/jpeg called with `-b` (brief) it gives you mimetype alone, so it's easy to compare against 'image/jpeg' or 'image/png' 
The other website you need to bookmark and use is explainshell.com
&gt; #!/bin/sh You're using bash-isms, so this line needs to be: #!/bin/bash Not all Linux systems have bash as their `/bin/sh`. Even for those that do, invoking bash as `/bin/sh` modifies certain behaviour to be more POSIX compatible, which is not what you want if you're directly targeting bash in your coding. ---- &gt; if [[ ! `dpkg -l | grep -w "ii ${DEPENDENCIES[$i]} "` ]]; then Using backticks for command substitution has been deprecated for about two decades; they are hard to read, hard to nest, and really need to die. Use `$( ... )` instead. Even POSIX has now supported that for many years. if [[ ! $(dpkg -l | grep -w "ii ${DEPENDENCIES[$i]} ") ]]; then Since `[[` is capable of matching (extended) regular expressions itself without invoking the external `grep` command, the following would be more efficient. if [[ ! $(dpkg -l) =~ "ii ${DEPENDENCIES[$i]} " ]]; then You're also executing 'dpkg -l' repeatedly in a loop even though it will give the same results on each iteration, so this is needlessly inefficient. Better to assign its output to a variable before entering the loop, e.g.: DPKGL=$(dpkg -l) and then match the variable within the loop: if [[ ! $DPKGL =~ "ii ${DEPENDENCIES[$i]} " ]]; then ---- &gt; if [ "$PKGSTOINSTALL" != "" ]; then Since you're coding for bash, don't use the `[`/`test` command which exist purely for compatibility purposes with POSIX shells. Use the `[[` keyword instead; it's more robust, and it provides protection against shell quoting snags and grammatical ambiguities. if [ $PKGSTOINSTALL != "" ]; then ---- *(edited for typos)* 
you use bash features so your hashbang is wrong DEPENDENCIES=(build-essential ... ) this thing is an array, which is not a feature of sh. But then instead of using another array, you start lame posixy concatenation PKGSTOINSTALL=$PKGSTOINSTALL" "${DEPENDENCIES[$i]} why not PKGSTOINSTALL=() # empty array ... PKGSTOINSTALL+=( "${DEPENDENCIES[$i]}" ) # append to array if (( ${#PKGSTOINSTALL[@]} &gt; 0 )); then # if size &gt; 0 sudo apt-get install "${PKGSTOINSTALL[@]}" # unpack array here btw, dont use backticks to embed a command, use $() instead 
Now check: yes no
'Do *one* thing, ***well***'.
Yeah, the GNU coreutils include all kinds of little programs that work well in shell scripts. It is kind of like a standard library for the Bash programming language. You can see the full list of utility programs and their documentation here: http://www.gnu.org/software/coreutils/manual/html_node/index.html You may just discover a few more commands you didn't know about that may come in handy for shell scripting.
I write my own Bash scripts and am currently working on learning to write kernel drivers in C. I have never come across anyone anywhere who used the 'yes' command in a script or even mentioned it. At first I thought I had a weird mis-typed alias that was somehow fucking up.
Yeah, in almost every circumstance a Bash `for` or `while` loop is better. The only time I have ever used `yes` was when I needed to fill a file with some 100 lines of content and it didn't matter what the content was: yes '99 bottles of beer on the wall. Do NOT take any down or pass them around.' | head -n 99 ..and I had to do it in a script. Usually if I ever need to fill a text file by repeating a string, I would just use Vim in normal mode: `yy99p` 
No, it's not multithreaded - just a very simple stress test.
If you manage your own kernels, then you'll use "yes" fairly often. It's used to automate the process of selecting new kernel config options. User can either manually select things, or just press enter to accept the default. Yes with an empty string emulates just pressing enter The classic process is: * Get new kernel source * Copy existing kernel config to new kernel src * Pipe yes with empty string into make script to update config * build new kernel . emerge gentoo-sources cd /usr/src/linux cp /boot/config-4.0.5-gentoo .config yes '' | make oldconfig make -j15 make -j15 modules make modules_install make install
Cool. I've not come across this. I usually don't manually change the kernel but run different VMs.
I want to print the name of each file in the directory? how is that possible
 printf '%s\n' * or with a loop: for f in *; do echo "$f" done If you want hidden files (name starting with a dot) as well, then use `.* *` instead of `*`.
The first thing I do with any new files I download, is process them to convert spaces into - signs. Most UNIX text utilities do not deal well with spaces, and I don't feel any particular need for spaces, so I just get rid of them. I've been using computers for a long time, so I remember what it was like to have to get by without spaces in DOS anyway.
&gt; word for it Interpolation? 
I always preface for-loops with IFS=$'\n'; So they ignore whitespaces
&gt; ie to always quote your variables I do. That isn't the issue. Spaces in filenames are frivolous, for the most part. Underscores have traditionally been used, because UNIX programs are usually designed to ignore whitespace. Contemporary practice is to assume that data is always messy and unformatted, expend unnecessary complexity on programs that (often unsuccessfully) edge case like crazy in order to get around said mess, and then call said extra complexity modern. I, on the other hand, format my data properly, so I can still use simple programs.
&gt; I realize it's common in Linux but I've never known why. Because underscores are more visible, and because UNIX programs are traditionally designed to either ignore whitespace, or treat it as a default field seperator.
care to show example where a plain quoting is not enough? Also the reality is you often have to interact with stuff not under your control so your assumption is not practical.
&gt; Also the reality is you often have to interact with stuff not under your control so your assumption is not practical. The only reason why this scenario exists, is because of the number of people who are willing to tolerate it, and also to tell themselves and others that it is unavoidable.
That's what I'm saying. It's easier to read a _ than a - in my opinion and it's more compatible across multiple languages and systems. You can use _ in windows / Mac / Unix / Linux / bash / Python / MSSQL / PL/SQL / VB.net etc. etc. but you can't use - in a lot of those without issues. 
You can also use `find` to handle wonky file-names. find '/run/media/quantum' -exec echo {} \;
&gt; IFS=$(echo -en "\n\b") In bash, the command substitution is unnecessary. It's better (and much faster) to use: IFS=$'\n\b' Not sure why you would want `\b` (backspace) in your IFS, though.
 yes | yes
copy paste is an important part of learning process so a lot of code is going to be born that way, plus many people care only about getting shit done using 'good enough' hacks, without knowing well what they are doing. That's not going to change and that's why sane defaults are crucial. Dragging the noobs' asses through the minefield in hope they find the promised land at the end is not how things should be. Also shell is tricky in that a change can have cascading consequences because shit like IFS does too many things. Eg disable word splitting with IFS but then `read` stops working because the damn thing uses the same IFS and now you have to plug local IFS override as a command prefix every single time to restore the default behavior. You have to know shell like the back of your hand to micromanage shit like that.
 for f in $(ls "$fin"); do echo "$f"; done I can see 6 bugs with this already. In increasing order of severity 1. If `$fin` doesn't exist in the filesystem, an error will be printed to the terminal. Don't screenscrape ls. 2. `echo "$f"`: if `$f` begins with `-` the behaviour is difficult to predict. Chances are it won't echo the filename correctly. Use `printf %s\\n "$f"` 3. `ls "$fin"`: If `$fin` begins with `-`, the behaviour will be wrong. Using `ls -- "$fin"` would fix this but ls is the wrong solution 4. It doesn't work with symlinks to directories. Could be solved with `ls "$fin/"` if you are intent on screenscraping ls. 5. It doesn't handle some special characters in filenames. GNU ls replaces some characters with question marks, so it may report some filenames that don't exist at all. The solution to this one again is not to screenscrape ls. 6. `ls` delimits its arguments with spaces which means you need to use word-splitting to distinguish between filenames. This means you can't handle filenames with any kind of whitespace in at all. This one is also solved by not screenscraping ls. McDutchie's solution here: printf '%s\n' * has none of those issues, and can be easily adapted to use `$fin`: printf '%s\n' "$fin"/*
^(Some / a lot of this post has been mentioned in other posts. I'll confess I didn't read them until after writing this. I've kept the duplicate recommendations in there anyway, that way you know more than one person thinks it's a good idea) I can't see any bashisms other than "function" in the script, so you may as well remove the "function" keyword and replace the shebang with `#!/bin/sh`. That said, the script doesn't do anything as the function is never called. If it's not meant to be run as a standalone script, remove the shebang altogether. Shebangs are *only* for things that can be executed directly by the program described in the shebang. It is interpreted by the linux kernel when the script is executed. I'd like to draw attention to this: optipng -o1 --quiet "$img" pngcrush -q "$img" /tmp/$$; mv /tmp/$$ "$img" Is there a reason to use both optipng and pngcrush? For the record [I use](https://github.com/ScoreUnder/scripts-and-dotfiles/blob/master/bin/png-optimize) `zopflipng -m --lossy_8bit --lossy_transparent` when I want maximum compression. Zopflipng has consistently given me better results than any other compressor, despite its silly name ^(and poor command line parsing and lack of man page and unformatted --help splash). Another thing to be mindful of is your use of `/tmp/$$`. [This is a security vulnerability](https://en.wikipedia.org/wiki/Symlink_race), so you should use mktemp along with an appropriate `rm -f -- "$tmpfile"` in an exit trap. A surprising number of images I've seen online (on chans, imgur, and such) have the wrong extension. I would recommend parsing the output of `file` rather than just checking the extension (also recommended by /u/Vaphell in this thread). You can correct improper extensions this way. I use this as an opportunity to normalize "jpeg"/"jpe"/"jfif" to "jpg". On this line: echo "Saved " $(( diff / 1000 )) " kB." Nothing is really wrong with this other than spacing but it looks like you meant: echo "Saved $(( diff / 1000 )) kB." When you move images: mv /tmp/$$ "$img" This is done before the new size is calculated. I would calculate and compare the new size first, in case jpegtran has increased the size of the image (this is not unheard of). ----- Matters of style that may not be to your personal taste: * `for img in "$@"; do` is a verbose way of saying `for img do` * This: if [ ! -f "$img" ]; then continue; fi is a verbose way of saying `[ -f "$img" ] || continue` -- which I would consider idiomatic. I don't know if everyone else feels the same. * `continue;` (used twice) - there is a semicolon at the end of the line here which doesn't do anything * In your block of locals: local input_file_size= local output_file_size= local output local percent local diff=0 local width= local pad= None of the initial values of these are used so you can strip away the equals signs. You are allowed to specify more than one per line so you might as well have this: local input_file_size output_file_size output percent diff width pad ----- Nitpick-grade complaints: * This code if [ ! -f "$img" ]; then continue; fi should also be checking `[ -r "$img" ]` * That same piece of code needs some form of error reporting. I would set a variable to remember to return nonzero from the function, so that the caller can know that at least one image "failed" somehow. * If any argument begins with a dash, a lot of your commands will break. The only workaround I know for this when dealing with programs which don't support `--` is this: `[ "${img#-}" != "$img" ] &amp;&amp; img=./$img` (i.e. if `$img` starts with `-`, then `img=./$img`) * `width=$( tput cols )`; `tput cols` can fail if the `$TERM` isn't properly supported, so you may want `width=$( tput cols 2&gt;/dev/null || echo 80 )` * It's preferable to use `[ condition 1 ] &amp;&amp; [ condition 2 ]` rather than `[ condition 1 -a condition 2 ]` because the latter is harder for /bin/test (or the shell's builtin equivalent) to parse, and depending on input, on some systems it may not work as expected.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Symlink race**](https://en.wikipedia.org/wiki/Symlink%20race): [](#sfw) --- &gt;A __symlink race__ is a kind of [software security vulnerability](https://en.wikipedia.org/wiki/Vulnerability_(computer_science\)) that results from a program creating [files](https://en.wikipedia.org/wiki/Computer_file) in an insecure manner. A malicious user can create a [symbolic link](https://en.wikipedia.org/wiki/Symbolic_link) to a file not otherwise accessible to him or her. When the [privileged](https://en.wikipedia.org/wiki/Setuid) program creates a file of the same name as the symbolic link, it actually creates the linked-to file instead, possibly inserting content desired by the malicious user (see example below), or even provided by the malicious user (as input to the program). &gt; --- ^Relevant: [^Symbolic ^link](https://en.wikipedia.org/wiki/Symbolic_link) ^| [^Linux ^Security ^Modules](https://en.wikipedia.org/wiki/Linux_Security_Modules) ^| [^Time ^of ^check ^to ^time ^of ^use](https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use) ^| [^GoboLinux](https://en.wikipedia.org/wiki/GoboLinux) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csm9dre) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csm9dre)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
I'll walk you through fixing up your original rather than give you another solution. Your original code: array=($(python $string import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) sys.exit(array) )) I'm assuming you wanted to pass the lines after "python" into python's standard input, so use a heredoc to pass it in: array=($(python $string &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) sys.exit(array) EOF )) $string here will be interpreted as a python script filename, so you need to tell python explicitly to get the script from stdin by naming the script `-`: array=($(python - $string &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) sys.exit(array) EOF )) $string here will be subject to [word splitting](http://www.gnu.org/software/bash/manual/html_node/Word-Splitting.html) which means it treats spaces, tabs and newlines as argument separators. That's a bad thing, so add quotes around it to prevent this: array=($(python - "$string" &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) sys.exit(array) EOF )) It still doesn't work. You can only "exit" with a number between 0-255 (and only 0-127 is portable and predictable). To send the array back to bash, you need to print it. array=($(python - "$string" &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) print('\n'.join(array)) EOF )) This leaves us with a problem. Bash is doing word splitting on that too, splitting on every space. So instead of an array of paragraphs, you have an array containing each word. There's no easy way to do this, but you can print null characters to delimit the array and use bash to parse that: array=() while IFS= read -r -d '' item; do array+=("$item") done &lt; &lt;(python - "$string" &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string) print(''.join(x.replace('\0', '') + '\0' for x in array)) EOF ) Note that in the above code I've replaced out all existing instances of \0 in the array. It's unlikely that there were any, but better safe than sorry. I've also added a \0 to the end of each element and joined on an empty string. Not because I'm crazy and like doing things odd ways, but because bash expects each array element to *end* in the delimiter, not just to be separated by the delimiter. The "read" probably came out of the left field if you haven't done this kind of thing before. `IFS=` and `-r` are necessary to stop read from fucking with your input (`-r` (**r**aw) to stop it from parsing backslashes, `IFS=` to stop it from stripping leading spaces). `-d ''` tells it that the elements are **d**elimited by nulls (empty string, because nulls can't be represented in bash strings). The next thing to do is to fix multi-line support. You can do this by adding flags to the regex: array=() while IFS= read -r -d '' item; do array+=("$item") done &lt; &lt;(python - "$string" &lt;&lt; 'EOF' import re, sys string = sys.argv[1] array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string, flags=re.S) print(''.join(x.replace('\0', '') + '\0' for x in array)) EOF ) Here "re.S" is a badly named constant for single-line mode. In short, it means "." matches any character (usually it does NOT match newlines). Now paragraphs with a newline character in them will be matched too. After these changes it works for me. Test case: &lt;p&gt;Testing this is paragraph one&lt;/p&gt;&lt;p&gt;and this is paragraph two&lt;/p&gt; &lt;p&gt;this is paragraph three with weird formatting&lt;/p&gt; &lt;p&gt; paragraph four has leading newlines and spaces&lt;/p&gt; This doesn't cover all cases but it should be good enough to demonstrate. Here's a session demonstrating it: $ string=$(&lt;testcase) $ array=() $ while IFS= read -r -d '' item; do &gt; array+=("$item") &gt; done &lt; &lt;(python - "$string" &lt;&lt; 'EOF' &gt; import re, sys &gt; string = sys.argv[1] &gt; array = re.findall(r'&lt;p&gt;(.*?)&lt;/p&gt;',string, flags=re.S) &gt; print(''.join(x.replace('\0', '') + '\0' for x in array)) &gt; EOF &gt; ) $ echo "${array[0]}" Testing this is paragraph one $ echo "${array[1]}" and this is paragraph two $ echo "${array[2]}" this is paragraph three with weird formatting $ echo "${array[3]}" paragraph four has leading newlines and spaces $ Final disclaimer that this is not actually parsing HTML or anything close to it
Everything is not as it seems. Can you post something as close to the original script as possible? Any commands you run, variables you assign, etc? It could be getting 'lost in translation' anywhere along the way. Feel free to anonymize values and programs run, but if you so much as manually substitute one single variable, you could accidentally be removing the bug in the code you show to us.
You didn't correct it or update it, so to anyone reading it looks as if you stand by your solution. Also, I am willing to bet I mentioned something you didn't think of, at least. ~~edit:~~ ~~\&gt;gets salty and downvotes my posts \&gt;doesn't amend the abomination above it's like you're trying to mislead people~~
This advice is equivalent to throwing a learner swimmer into the ocean. 
[The linux command line](http://linuxcommand.org/tlcl.php) . You can download it [here](https://courseweb.pitt.edu/bbcswebdav/institution/Pitt%20Online/MLIS_Pitt_Online/LIS%202600/Intro%20Module/LIS_2600_%20M1_Shotts%202009.pdf) since the one on the original page links to sourceforge...
http://mywiki.wooledge.org/BashGuide
Someone's probably going to recommend the TLDP "Advanced Bash Scripting"; disregard that, it's terrible and encourages many bad practices. Instead, I think the best resources aren't books. /u/cpbills already recommended the [Wooledge Bash Guide](http://mywiki.wooledge.org/BashGuide), it's good stuff. For something a little shorter, I'm a fan of the [Gentoo developers Bash reference](https://devmanual.gentoo.org/tools-reference/bash/index.html), with the exception that it discourages Bash 3 features--nobody still uses Bash 2! Also, I wrote an [article](https://lukeshu.com/blog/bash-arrays.html) on Bash arrays; other resources often treat them as an advanced topic; I think that's ridiculous. Once you're kinda proficient, the Bash manual is very detailed and good, but it's definitely overwhelming for an absolute beginner. Both the `info` manual and the `man` manual are good; they have slightly different styles of writing, but are both detailed and readable. However, they each use slightly different terminology with regards to the precise syntax! I think I prefer the `man` version.
That's not helpful at all, jerk. Man bash | less
Sink or swim, buddy. It's like reading a hard book with a dictionary. Underline the words you don't know, write them down, research them and continue. At first, there's a bunch of underlining and rereading. But after awhile, not so much. I thought that it was pretty doable to navigate bash and the command line from the internet and man pages once I understood pipes, redirection, regex (fairly well), argument variables, string formating, and a few others. I was always into Sci-fi and movies like the Matrix so I guess I'm bias to wanting to learn how to use command line tools well.
Hmm what system are you running the commands on? For speed's sake you may want to just run the command(s) on the command line without the saving to a file, just so you can view your results without having to open up the pout file. Has your course/semester included use of other utilities such as awk or sed. If so then [geirha's](https://www.reddit.com/r/bash/comments/3bn1do/excluding_from_grep/csnmk06) response is a good shout too! 
That's weird. Seems like it should work. Did you try to reverse the order of the commands like /u/shibo said? 
yep, that was it!
You are saying that this command returned a line with the text "/usr": `grep -v "/usr" /etc/passwd | grep "ksh" &gt; pout` But this did not? `grep "ksh" /etc/passwd | grep -v "/usr" &gt; pout` Can anybody explain that? It seems like they would both return identical sets.
Yeah odd. Either way works fine for me on OS X(BSD) 
did you try egrep?
My best guess is that OP actually ran grep -v "/usr" /etc/passwd | grep "ksh" /etc/passwd &gt; pout Easy mistake to make.
&gt; nobody still uses Bash 2 I have to endure it on some old Solaris boxes. 2.03 on the one remaining Solaris 8 box in my client's fleet is especially annoying. On the other hand, while scripting for Bash 2 on Solaris is less convenient and more convoluted, it does tend to force portability habits.
&gt; Most UNIX text utilities do not deal well with spaces Every UNIX text utility I've ever seen deals fine with spaces. It's bash newbies who don't.
Has awk been covered? It's the tool I thought of as soon as I saw the OP
Good catch! I typically don't think about not messing with IFS; I follow the strategy that if I'm using it, I need to set it explicitly first. I'll update the post.
Aren't all man pages displayed in less?
Thanks a lot, both of you! I just left work, so I won't be testing it until tomorrow!
A short version: #!/bin/bash set -e hash dpkg apt-get sudo DEPS="build-essential cmake cmake-qt-gui python python-matplotlib libtool libcoin80-dev libcoin80-doc libsoqt4-dev libxerces-c-dev libboost-all-dev libqt4-dev libqt4-opengl-dev qt4-dev-tools python-dev python-pyside pyside-tools liboce-ocaf-dev oce-draw libeigen3-dev libqtwebkit-dev libshiboken-dev libpyside-dev libode-dev swig libzipios++-dev libfreetype6-dev libsimage-dev checkinstall python-pivy python-qt4 doxygen graphviz" deps_missing="$(dpkg -l $DEPS 2&gt;&amp;1 | awk -F'[ .]' -v ORS=' ' '/^No package/ {print $5}')" [[ "$deps_missing" ]] &amp;&amp; echo sudo apt-get install $deps_missing Comments on the code, in order: * `set -e` kills the script if a command within it exits with a non-zero status. * `hash` is a BASH built-in that has the property of outputting an error when specified commands aren't found and exiting with a non-zero status. If that happens the script will then exit due to `set -e` - this is desirable since there's no sense in continuing if the commands that make the script work aren't available. * `dpkg -l` doesn't just list all packages - it can list information about specific packages and output ones that aren't installed to stderr. * The `awk` command will look for lines from `dpkg -l` specifying packages that aren't installed, pull out the package name, and output the package names separated by spaces. * If `$deps_missing` is non-empty the packages get installed via `apt-get`. (Note: remove `echo` to make that line functional) If you really want to be prompted about installing the packages I have two suggestions: * Use `read -p` instead of `echo -n` followed by `read`. (`read -p 'Some dependencies ...' sure`) * Test `$sure` via a regex instead of static chars to make handling cases such as "yes", "Yy", etc. easy. (`[[ "$sure" =~ ^[Yy] || "$sure" = "" ]]`)
Could just add that to your bashrc as an alias too. No need for a full on script. 
I'd stick a `-v` in there because of this line in the OP's script: echo "moving $f to $itunesAuto" 
&gt; files=$(shopt -s nullglob dotglob; echo $downloadsDir/*.mp3) It looks like you were about to use this in the "for" loop until you found out that it gave you the wrong results, right? If you do it as an array, it should work: shopt -s nullglob dotglob files=( "$downloadsDir"/*.mp3 ) if (( ${#files[@]} )) then for f in "${files[@]}" #...etc... I've put `$downloadsDir` in quotes, because otherwise it will be subject to the usual word splitting and globbing. Not usually a problem but better safe than sorry. The extra `[@]` after `files` is an array subscript. In this case the special subscript `@` meaning it should fetch all array elements. --- On an unrelated note, since I have a feeling you'll like it: c_yellow='\033[33;1m' c_default='\033[0m' echo -e "Moving $c_yellow$f$c_default to $itunesAuto" --- Edit: As for nifty scripts I use: https://github.com/ScoreUnder/scripts-and-dotfiles/tree/master/bin Some things I think are handy in there are `webmify`, `wallpaper-cycler` (because everyone needs randomized bgs), `grab-ssh-agent` (for when I git push over ssh, lol -- if you abuse it, it's also a nice proof of concept of how agent forwarding is a bad security practice), and `optdl` along with its many helper scripts. It's also got some junk in there from when I was experimenting with various things. Maybe it'll be either interesting or good for a laugh. The poorly named `dims` especially. `STUPID FILE $f!`
Regarding files=$(shopt -s nullglob dotglob; echo $downloadsDir/*.mp3) tbh I just pulled that line from another script I had without thinking but I guess it's not really necessary is it. I could have just done: files=$(echo "$downloadsDir"/*.mp3) and then just checked to see if that was empty or not to know if I had some mp3s to move. And you're right, I love me some colors, I'm gonna give that a try. Thanks for pointing me to that repo, I'll definitely scan through and see if I can steal anything useful for myself :) appreciate the feedback.
 echo "INTERFACES IP ADDRESSES"; /sbin/ip -4 -o addr | awk '{printf "%-17s\t%-15s\n", $2, $4}' | cut -d'/' -f1 | tr ' \t' '. ' The quickest way I could think of. It's not pretty. :P Slightly different solution to what I assume is the problem you're trying to solve here (getting way to big for one line): echo "INTERFACES IP ADDRESSES"; /sbin/ip -4 -o addr | awk 'BEGIN{odd=false} {odd=!odd; if(odd) { printf "\033[48;5;235m" } else { printf "\033[40m" } printf "%-17s %-15s\033[0m\n", $2, $4 }' | cut -d/ -f1
I have this, that I hardly ever use, but somehow made it into my aliases: alias hr="printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' ="
That's pretty sweet! Very succinct. Thanks for sharing!
Somewhat relevant: There's a feature in iTunes that will automatically import any files placed in a folder under the iTunes tree called "Automatically add to iTunes". It's always worked flawlessly for me. I download directly into it. https://support.apple.com/en-ca/HT201970
Yea but sometimes you download things other than mp3s from the internet. Like images, files, videos etc. Having all downloads going into the automatically add to iTunes would be a hassle to organize. 
 ip -o -4 addr | awk -F'[ /]+' ' BEGIN { printf("%-17s %s\n", "INTERFACE", "IP ADDRESS") } { dev = sprintf("%-17s", $2) gsub(/ /, ".", dev) print dev, $4 }'
Thanks /u/UnchainedMundane will take a look at this.
This is exactly the sort of response I was hoping for! I have so may questions! I hope it's not too much trouble. Why are you able to specify colors in all caps and use them as lowercase? I'm sure it's better to use posix, but I really don't know what is/isn't posix. Why are these equivalent? Not following order of operation, but rather a simple left to right? LEN=$(( $(( $(tput cols) - ${#TEXT})) / 2)) =&gt; LEN=$(( $(tput cols) - ${#TEXT} / 2)) What magic is this? ${colors[${1^^}]} Why do you gave a printf with "left" before left is declared? Why does the -v option do to the printf? It's not in the man page. ---- Edit: Ok, I think I see what is going on with my last question. "printf -v" prints to a variable: http://stackoverflow.com/a/30098993 printf -v left '%0*d' "$l_w" #Write a bunch of zeros left=${left//0/&lt;} #Overwrite the zeros with a symbol
See if you can use the [`watch(1)`](https://en.wikipedia.org/wiki/Watch_\(Unix\)) utility. There are a few different variants available (Wikipedia is not correct in calling it a GNU command), all of them should be smart in updating the screen.
You can use control codes to place the cursor at a certain &lt;x,y&gt; location to overwrite old text.
I found that in my google searches, but I'm using cygwin and it's definetly not included by standard and it doesn't look like it's available in the installer either. :/
Oh, this sounds interesting! What do you mean by control codes? I don't think I've come across that terminology in bash before.
Damn, that sound really cool. Also really complicated.. I'll try to read up on it. My sed/awk-fu is very weak.
You shouldn't clear at all with this method.
Oh snap! I'm dumb. Thanks a lot! 
My environment is filthy. When I first learned about aliases/functions I made one for everything. I have ~130 aliases, some I use a lot, some are actually aliases to aliases (!?), some are shortcuts to commands that I now know well enough to not even use an alias, and some I don't ever use (I thought I would, but never did). It's a jungle in there (I run `set` or `env` and look at all the mess). These days I keep everything in a nicely organized `~/scripts` folder and symlink the most used tools to `~/bin`. Every once in a while I will go through some aliases and delete or convert them to standalone scripts. It used to be even worse, but I still have a lot to do. Long story short, I agree with you because I am the shining example of aliases gone wrong. There is also a short delay every time I open my terminal because it has to load/parse all that junk. In fact, this very comment has inspired me to clean this up today.
I don't know what parts specifically you need help with and I don't want to just hand you the completed homework, so I'm going to give a few pointers. Sorry in advance to mobile users, but you will need to hover over any 'spoiler's here to view them. The text is in the tooltip. 6: * `man 1 alias` if you're unsure how to use the command. * Remember to quote things correctly. * To get the current user, [(spoiler)](#s "Use the $USER variable or the whoami command") * The password file is located [(spoiler)](#s "at /etc/passwd") 7: * `man man` -- this tells you how to set the pager. Searching for "pager" should get you there eventually. It doesn't tell you how to un-set it [(spoiler)](#s "but an empty string works, as does cat") 8: I'm not entirely sure how you could test this one either. It seems to me like the correct answer, but I haven't ever used it. 9: * I'm assuming you know what the redirections do here. What the command as a whole is doing is [(spoiler)](#s "reading all of 'mfile1' and writing it into 'aa'") * So in essence it's just [(spoiler)](#s "copying mfile1 to aa") -- you probably know the command for that * Actually to be more faithful to the original you'd have to [(spoiler)](#s "touch the destination file before copying, so that it gets created even if the original doesn't exist") but I think that's an oversight on their part, I don't think they will want you to replicate that behaviour.
&gt; `man 1 alias` if you're unsure how to use the command. alias is a shell builtin, so `man 1 alias` will not give the documentation of bash's alias command. If there is an alias(1), it will more likely describe what POSIX requires of the command instead. Use `help alias` for documentation about bash's alias builtin (and/or look it up in bash's manual).
Add an argument to sweep a directory of folders, burning each to disk and ejecting, then waiting for another disk to be put in and repeating until finished. That'd be neat. 
Use `&amp;&gt;`. Be consistent; stick with `[[` over `[`. Consider *here* files rather than long lists of echo statements. Don't run tput(1) over and over; store the output in a variable. `[+]` as a parameter to echo is an unquoted glob. Commands like pushd and wodim aren't checked for success, instead we plough on and remove files.
I'll skip the error on line 19, with the `ls`, since /u/Vaphell already covered that. 1. The `.sh` extension suggests it is an sh script, while the shebang says otherwise. I recommend not using extension for commands, since they are redundant, but if you insist, name it with `.bash`. 1. You are running `tput` for nearly every line of output. Store the output in variables at the start of the script instead. You should also make sure the output is going to a terminal before writing terminal escapes. If you run `./burn_cd.sh ./ &gt;some.log` with your current script, the file will end up looking weird in a text editor. You can test if stdout is a terminal with a `-t` test. if [[ -t 1 ]]; then red=$(tput setaf 1) green=$(tput setaf 2) reset=$(tput sgr0) fi ... printf '%s[+] Converting files to .wav%s\n' "$green" $reset" This covers the case where you output to stdout at least, but you also output terminal escapes to stderr, so you will need some additional logic to handle that. 1. In bash, consistently use `[[` to test strings and files, and `((` to test numbers. Don't use `[` at all. There's no point in using `[`, since `[[` can already do everything `[` can, and more. And `((` gives a much more readable way of testing numbers. On line 20 you have `if [ -z $total ]`. This is completely wrong. `total` is a number, and you are testing if this number is an empty string or not. Surely you wanted to test if it equals 0 or not. To do that use: `if (( total == 0 ))` 1. On line 18 you run `pushd "$1"`. You never test if it succeeded. If it fails, the script continues on in the wrong directory, which could be disastrous. Always check if `cd` (or `pushd` or `popd`) failed. `cd "$1" || exit` 1. On line 31, you have `for i in *.mp3; do lame --decode "$i" "$(basename "$i" .mp3)".wav; done`. This might fail if any of the mp3 files start with `-`, as lame might try to interpret them as options instead of filenames in that case. The typical way to avoid that is to prepend a path to each filename. `for file in ./*.mp3`. Same goes for the wodim and rm commands on lines 40 and 46. Also, to replace the extension with `.wav`, you can use parameter expansion instead: `lame --decode "$file" "${file%.mp3}.wav"`.
I'd say you already got it.
That's it. Why does it feel hacky? The extglob syntax is a little odd, but other than that?
you could use standard glob for a more familiar look and feel `files=( *.jpg *.png )` or `files=( *.{jpg,png} )` but other than that it's as kosher as it gets.
i cannot think of anything less romantic lol. good luck nevertheless;)
it'll be hard to give ideas without knowing a bit more about your gf's usage patterns and preferences, how's she like? what things she enjoy (in and out of computer), how does she use the terminal? etc.
So what does this script do?
Try it; source it and type some letters on the prompt.
Well, I have ShellCheck installed as a linter (via Vim's Syntastic plugin), and it doesn't seem to like this syntax at all.
If you try pasting it at http://shellcheck.net you will see it does not complain. The plugin is possibly misconfigured. Maybe it assumes you are writing an sh script when it is really a bash script.
if Linux isn't her thing maybe you should try some other approach? I'm not the most qualified person for relationship advice lol but I think it should be something she enjoys not just you. best of luck!
Down a similar path to what's been recomended here, look at tput, specifically tput sc and tput rc. It seems though that you might want to look into auto-scrolling a paginated output instead? Have a look at this thread: https://www.reddit.com/r/commandline/comments/37get5/throttle_stdin_to_stdout_for_readable_tail_f_for/
`ALIAS(1)` is very system specific. I tried `man 1 alias` on three different systems: * On OSX, it gives me `BUILTIN(1)` which just contains a comparison table of what builtins csh and sh have. It doesn't explain what the alias command does, and seems like a rather useless manual page all together. * On Ubuntu I get `No manual entry for alias in section 1`. * On RHEL I get `BASH_BUILTINS(1)` which is a section of the bash manual that redhat has seen fit to separate into its own manual page. Here at least, it explains what bash's alias command does. So suggesting a beginner use `man 1 alias` is probably more confusing than helpful.
 $ cat foo YYYYYYYAACCCCCCCCCCBBZZZZZZZZZ YYYYYYYAACCCSTOPCCCBBZZZZZZZZZ YYYYYYYAAPOTSPOTSPOBBZZZZZZZZZ $ grep -Eo 'AA.{10}BB' foo | grep -v STOP &gt;bar $ cat bar AACCCCCCCCCCBB AAPOTSPOTSPOBB $
The title of your post mentions sorting, but that isn't a requirement in the description?
Ah, my apologies for the wording; I meant that I am trying to sort, or rather copy, just those sequences that fit the criteria into a new (seperate) .txt file.
You need to learn regular expressions. I'm so very sorry. It'll take you about two weeks. Afterwards they will always be in your brain, like little snakes. But hey, you'll be able to look at a sequence like you posted and quickly think of a single-line egrep command, without resorting to piping multiple greps. 
 mapfile -t array &lt; &lt;(printf '%s\n' "${array[@]}" | sort -nr) `sort` sorts lines, not space separated values.
`"${array[@]}"` produces all its values in a single line; `sort` expects input values on multiple lines. This works: $ array=(34 56 12 31) $ for i in "${array[@]}"; do echo "$i"; done | sort -rn 56 34 31 12 Alternatively, you can exploit the fact that `${array[*]}` (note the `*`) expands to a single word, separating the values with the first value of `IFS` (Internal Field Separator): $ array=(34 56 12 31) $ IFS=$'\n \t' # the default is $' \t\n' $ echo $(sort -r -n &lt;&lt;&lt;"${array[*]}") 56 34 31 12 But ~~this breaks if the values contain spaces, and~~ changing `IFS` is generally dangerous and should be avoided. EDIT: Nevermind, I’m talking out of my ass. Of course it works even with spaces in the content. You should still avoid messing with `IFS` though.
And so to re-enter that into a sorted array might be: for x in "${array[@]}" count=$((count+=1)) do echo "$x" | sort -rn &gt;&gt;&gt; sortedarray[$count] done ? I really just wish I could operate on the contents of an array without reading it out.
/u/KnowsBash lives up to his/her name again! :D Thanks, very interesting, and good to know in general.
as it stands, the question doesn't make much sense; what does "keeping their names" mean here? a single file has a single name.
#!/bin/bash immediately=1 response='eww' while on one knee in real life; do echo 'Will you marry me, bitch?' for i in $response; do break done exit $immediately done
#!/bin/bash set -e I've made a huge mistake
BTW, `read` should nearly always have `-r`.
I'm on my phone so I didn't get to test this but I think this may get what you're looking for: find /path/to/the/dir/ -type f -name *.sfx -exec mv {} /backup/dir/ \; -exec cp -a /path/to/new.sfx {} \; 
`man cut` or search the internet for "get field x text linux"
This did it. Thank you, I love you. &gt;cut -c46-90 new.txt &gt; newnew.txt 
Are you open to using python instead of bash? This is a pretty straight forward use case for the Beautiful Soup and Requests libraries. Install the libraries: sudo pip3 install beautifulsoup4 sudo pip3 install requests example script: #!/usr/bin/env python3 import requests import bs4 import os target_site = "https://en.wikipedia.org/wiki/William_Tecumseh_Sherman" #send http request to site, and load into parser r = requests.get(target_site) soup = bs4.BeautifulSoup(r.text) #set column width if displaying to terminal, or defualt to 80 for piping try: ts = os.get_terminal_size() width = ts.columns except: width = 80 print("="*width) #parse contents of a tags: for link in soup.find_all('a', href=True, title=True): print("link: "+link['href']) print("title: "+link['title']) print("="*width) Example output: [http://pastebin.com/GmN4wWx5](http://pastebin.com/GmN4wWx5) 
The shell, like all orthodox UNIX stuff, is a language oriented around lines of text. HTML, by contrast, is a stream-based language – there is no such concept as a line, and there is no guarantee that the data you want is on the same line or on 1000 different lines, nor do you have any idea what other elements or attributes might get inserted there to screw up your ad-hoc attempt at parsing. You need a proper HTML parser to accomplish this job. The `hxpipe` command from the [HTML-XML-Utils](http://www.w3.org/Tools/HTML-XML-utils/) package is specifically designed to convert HTML to a line-oriented format which is easy to parse with something like `awk`, `sed`, or the shell. This is what the output of `hxpipe` looks like on your example data: $ hxpipe &lt; test.html -· Aclass CDATA featured_article_metadata has_been_on_main_page (span Ahref CDATA /wiki/William_Tecumseh_Sherman Atitle CDATA William Tecumseh Sherman (a -William Tecumseh Sherman )a )span - · Ahref CDATA /wiki/Military_service_of_Ian_Smith Atitle CDATA Military service of Ian Smith (a -Military service of Ian Smith )a - · Aclass CDATA featured_article_metadata has_been_on_main_page (span Ahref CDATA /wiki/Issy_Smith Atitle CDATA Issy Smith (a -Issy Smith )a )span - · Ahref CDATA /wiki/Oerip_Soemohardjo Atitle CDATA Oerip Soemohardjo (a -Oerip Soemohardjo )a - · Ahref CDATA /wiki/Myles_Standish Atitle CDATA Myles Standish (a -Myles Standish )a - · Aclass CDATA featured_article_metadata has_been_on_main_page (span Ahref CDATA /wiki/Ronald_Stuart Atitle CDATA Ronald Stuart (a -Ronald Stuart )a )span - · \n So now you can simply do: $ hxpipe &lt; test.html | sed -n '/^Ahref / { s/.*CDATA //; p; }' /wiki/William_Tecumseh_Sherman /wiki/Military_service_of_Ian_Smith /wiki/Issy_Smith /wiki/Oerip_Soemohardjo /wiki/Myles_Standish /wiki/Ronald_Stuart $ hxpipe &lt; test.html | sed -n '/^Atitle / { s/.*CDATA //; p; }' William Tecumseh Sherman Military service of Ian Smith Issy Smith Oerip Soemohardjo Myles Standish Ronald Stuart
Cool. Never heard of hxpipe, very interesting. However, how do you associate the `href` to the corresponding `title`? hxpipe seems to split this into multiple lines.
best I could come up with in shell: #!/bin/bash OLDIFS=$IFS IFS=$'\n' a_tags=$(curl https://en.wikipedia.org/wiki/William_Tecumseh_Sherman | grep -Po '(?s)&lt;a.*href.*title.*&lt;/a&gt;') printf "================================================================================\n" for line in $a_tags; do printf "$line\n" | grep -Po '(?s)href.*title.*"' | awk -F\" '{print "link: " $2"\ntitle: "$4}' printf "================================================================================\n" done IFS=$OLDIFS Example output: [http://pastebin.com/v3bKUFeF](http://pastebin.com/v3bKUFeF)
You could add a prefix like **prefix_** to the files and write the script to sort them in folders as appropriate based on the prefix. Or, if the files have appropriate metadata use mediainfo to parse it then sort.
When those "files" are downloaded there is no metadata. I am trying to use the words inside the filenames as indicators to what subfolders they should be moved to. Has anyone written a script or seen similar tools?
You want to delete everything up to and including the last space on the line, keeping the last word. sed 's/.* //' huge &gt;domains Doesn't require counting column positions, assuming they're consistent across all lines.
I d a similar thing but for TV shows. I'm not at my computer for a bit, so i cant give you a code dump but basically I just grep for particular keywords/titles and move them to a predefined directory. Sadly bash isn't too great with multidimensional arrays but it gets the job done. When I get to a keyboard I can write a snippet for you if you want.
You don't really need a script for that, you could do it with a single command: find dir/to/search -type f -iname '*foo*' -exec mv -t /path/to/destination {} + To break that down a bit, `find` is a program that searches recursively for files and directories based on the parameters given to it. `-type f` means we only want to return actual files and not directories or symlinks. `-iname` means to filter by file names in a case-insensitive way, and `'*foo*'` means to match 'foo' in any part of the file path. `foo` can be a single word or multiple words but keep in mind it will have to match the phrase exactly. `-exec` is a way to run a command against the matches, and the `{} +` part at the end is where `find` puts in the paths to whatever command is given to `-exec`, unfortunately find requires this to be at the end which is why you have to use `mv -t` which lets you put the target (or destination) at the beginning. Most of the time when you run `find` you'll want to run it without `-exec` first to test what it matches. `mv` also has the `-i` switch that will confirm before moving. If you want to search for multiple phrases and prompt before moving you can do find dir/to/search -type f \( -iname '*foo*' -o -iname '*bar*' \) -exec mv -i -t /path/to/destination {} + The difference here is that we added a block of `\(` `\)` and added `-o` in between our `iname`s, basically saying, "give me file names that have either `foo` or `bar` in them". You can repeat that `-o -iname '*search*'` param as many times as you want as long as it stays inside the `\(` `\)` block. `find` is a very powerful command but it's syntax does take some getting used to. Check out this page for more info: http://mywiki.wooledge.org/UsingFind I don't think it would be too hard to turn this into a script, but I'd say getting comfortable with using find is more valuable than the time saved abstracting it away. ^(*edit: slight phrasing, more explanation*)
Thanks! I am familiar with Python, but never used beautiful soup. I am just learning bash and wanted to do it using that. I am going to save these solutions for future use. 
I can understand your concerns. But I don't know python, perl or ruby. Was just asking if other people, had ever created or use some script or software to do the same thing.
Checkout [flexget](http://flexget.com/). More specifically this [sort recipe](http://flexget.com/wiki/Cookbook/Series/Sort). It's extremely configurable and powerful.
I made this https://github.com/abouthillier/NSFW-Wallpaper-Hider
 if (( $# == 1 )) &amp;&amp; [[ $1 == ulog ]]; then sudo -u mqm /usr/bin/less /var/user.log exit fi if ! (( $#==2 || $#==4 )); then HELP fi # Process the command line arguments &lt;etc etc&gt; modify your HELP to include non-switch param.
In case you weren't aware, you might find `pgrep` useful (completely not an answer to your question).
Yea, that looks like it might be the proper tool for what I'm doing. 
I see no evidence that it behaves differently. In all cases issuing -ax as arguments yields that warning.
Exactly. $ ps -ax &gt;/dev/null Warning: bad ps syntax, perhaps a bogus '-'? See http://procps.sf.net/faq.html $
I can't reproduce that effect. Both commands give the same result for me. What is your output of `sed --version`? In any case, what you want can be done in a much simpler way: echo "$string" | sed 's:[&gt;/ -]::g' Note that `"$string"` should be quoted to avoid any unexpected effects of field splitting (a.k.a. word splitting) and pathname expansion (a.k.a. globbing). 
The system has an older version of sed, 4.1.5. Thanks for the new sed statement. 
Your input probably contains carriage returns (`\r`). That, combined with your lack of quoting, causes the same line to be overwritten multiple times when you run `echo $string`. See [FAQ 52](http://mywiki.wooledge.org/BashFAQ/052) on how to get rid of CRs, and [this](http://mywiki.wooledge.org/Arguments) about the importance of quotes. If you want to remove everything but digits and newlines, you can also use `tr`: xmllint --shell file.xml &lt;&lt;&lt; "cat /path/to/data/text()" | tr -cd '0-9\n'
Also, `$command` should be quoted to avoid unexpected field splitting and pathname expansion snags. eval "$command" 
Just in case you're curious, it wasn't a carriage return, but the escape character. `echo "$string" | sed -n l` yielded `\033[?1034h$string"`
Ok. That's even worse.
 porn_root=/Porn/ path_one=/Downloads/ Are these anonymised variables? If you have these in your `/` you are doing something horribly wrong.
Is that another way of saying you take bribes?
Sorry, I was joking
No worries, mate.
&gt; Please note I have not tested it, and I think this is a silly idea edit: Don't get me wrong; if you want to improve on it, or fix it, by all means. Certainly you should probably use $@, but you know that, so I'm not sure why you posed it as a question.
you could say it was rhetorical.
Have you considered join(1)?
It was option 2 I was aiming for... Basically output all lines from file 1, and for each matching date/time append the corresponding metrics from file 2 onto that line Someone has suggested 'join' which looks to be a great starting point, thanks for your help
Interesting! I'm not sure if join will do everything I need yet, but the awk approach definitely appeals, Thanks
That is some odd behaviour. There might be a control character in you MAC variable? I'd suggest, on line 19, taking this expression "echo -n ${ETH/*HWaddr/}" and piping it into "hexdump -C" too see what is there. I'm not optimistic about this but it is the only thing I can think of at the moment. Another thing you could do is run the script with "set -x" to get the debugging output to look at: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html EDIT: when you print just the MAC, echo is adding a \n at the end, so if you have the \r, then you're getting \r\n on those lines. EDIT2: noticed you have a \r in the password variable, and this is used in the command that creates the ETH variable, which is then used in the command that creates the MAC variable... that might be where this is coming from although it doesn't seem likely.
I'm in agreement with the other commenters regarding the echo. On 18, I'd change it to use printf. It's a lot more predictable than echo. You should also insert a statement to handle weird characters there -- you're getting a result from expect, when you don't really know what to ... er, expect. ;) On 21, split the line up to get the variable you want first, then do the if-test against it. This gives you more control; if the variable is garbled or missing you can easily handle it. Otherwise you have no idea what is actually happening in that test. 
TIL, thanks. Up until now I thought c-style \\(escape) expressions were part of the regex syntax.
Yeah I only learned that a couple weeks ago, oddly. They're strictly in the GNU sed, so basically everywhere except like... Macs and bsd-based stuff for the most part. Reading: http://www.gnu.org/software/sed/manual/sed.html#Escapes 
I almost never use a C-style for in bash. There's almost never a need to do it. Interestingly the ```for..in``` style loop is _technically_ faster: pi@print ~/fortest $ ./timeit.sh cstyle.out Samples: 1000 Mean Avg: .71545 pi@print ~/fortest $ ./timeit.sh instyle.out Samples: 1000 Mean Avg: .57276 Though not by much in smaller scale tests (it's negligable) And therefore in my opinion, up to the user's preference. Coming from Python, I like ```for..in``` much more than ```for(lots;of;semicolons)``` Edit: test source https://gist.github.com/NeilHanlon/8a0fdb5ee40ff97271fc
Is this what you want? ─&lt;nhanlon@hanlonn-deb1&gt;-&lt;~&gt;-&lt;12:15PM&gt;-◇ ╰─➤ echo -n "some really cool text" | sha256sum | awk '{print toupper($1) }' | xargs -I{} echo "ibase=16; obase=4; {}" | bc -l 20012122320213023003330222303211310030210232113222102033011201001001\ 103012001300221230313110313002300023212203102200230300131130 There may be a better way, but basically what I did was use xargs to pass the output of evreything before it into the echo as ```{}```, and then pipe it into ```bc -l```
yeah it can be anything you pass into -I.. like ```echo "asdf" | xargs -I A mv A somefolder/```` would do ```mv asdf somefolder/``` I usually use {} because not much is going to use {}
 $ bc &lt;&lt;&lt;"ibase=16; obase=4; $(sha256sum &lt;/dev/null | awk '{print toupper($1)}')" 32032300301010022120333001300110212233233310302021211233232102100213\ 223210013210121021232103103022102111212101231320110223201111 $
I've never encountered dc before.. interesting. Any discernable difference between it and bc?
Both `dc` and `bc` split long numbers over multiple lines, as documented. Nice trick with the read, one could do `IFS= read...` to limit the change of IFS.
I find I don't need to mess with IFS at all? $ hd &lt;&lt;&lt;$'foo\\\nbar' 00000000 66 6f 6f 5c 0a 62 61 72 0a |foo\.bar.| 00000009 $ read s &lt;&lt;&lt;$'foo\\\nbar' $ printf %s "$s" | hd 00000000 66 6f 6f 62 61 72 |foobar| 00000006 $
&gt; Variable expansions should usually be quoted, but it's not needed if the variable can only have certain trusted values and you know it's not going to be split anyway, or if `IFS=''; set -o noglob` are active, or, of course, if you want field splitting (which doesn't apply here). If you want robust code, you quote all parameter expansions and command substitutions where bash will attempt word splitting and pathname expansion. Messing with `IFS` and `set -f` just adds pointless extra code when proper quoting makes it a non-issue. There's never any point in allowing word splitting. Use `read` or `mapfile` if you want to split something into words/lines. &gt; Same with the `-r` option to `read`; you don't know the content of the file and it's not impossible that adding -r would actually screw things up. IFS whitespace removal may be needed here. I know of more cases where not adding `-r` would screw things up. There's very few cases where it makes sense to remove backslashes in the input. &gt; There is nothing actually wrong with using uppercase variable names. This is a matter of opinion more than anything. There is. All special shell variables and environment variables are all uppercase (with `http_proxy` as the sole exception I know of). By using uppercase variables you therefore risk overriding special shell variables and environment variables. Even if you know all the special shell variables that exist today, new ones could be introduced in future versions. It's a bug waiting to happen, so no, it's not simply a matter of opinion.
&gt; Messing with IFS and set -f just adds pointless extra code when proper quoting makes it a non-issue. There's never any point in allowing word splitting. If that's true, then adding quotes to all variable expansions just adds pointless extra code when a single line saying `IFS=''; set -f` makes it a non-issue. &gt; Even if you know all the special shell variables that exist today, new ones could be introduced in future versions. It's a bug waiting to happen, so no, it's not simply a matter of opinion. Your own `http_proxy` example shows there is no guarantee that they won't introduce environment or special variables in lowercase. Names that are already in use need to be avoided in any case and otherwise all you can do is pick sensible names that are specific enough they're unlikely to cause conflicts down the line. 
&gt; If you want robust code, you quote all parameter expansions and command substitutions where bash will attempt word splitting and pathname expansion. I never knew bash did this when the variable is part of a redirection. It does globbing but not word splitting in `POSIXLY_CORRECT` mode, is that a bug? 
Actually join(1) is a bit useless (it will hide lines that do not match, and requires sorted input), and I got so annoyed with it years ago that I wrote a simple replacement: * "merge.pl" -- http://pastebin.com/EuGC2FRH It takes input from one, two, or any number of files. And you don't need to sort your input. I tend to use this script a lot when processing data from different sources. Using my utility, your problem a simple matter of putting in a TAB character after the timestamp, for instance with a shell function like this: tabize() { awk '{out=$1" "$2"\t"; for(i=3; i&lt;=NF; ++i)out=out" "$i; print out}' "$@"; } Then merge your data: tabize file1 file2 | merge.pl And then if you want, you could remove all the extra whitespace: tabize file1 file2 | merge.pl | sed -e 's/\s\+/ /g' -e 's/\s\+$//'
I feel like bash might be what's holding you back. It has a fairly anachronistic approach to lists, and data structures in general. If I were to write a tool for this, my strategy would be: read in the CSV file line by line, extract the id information (and possibility convert it to an int to speed up comparison), search a hash map for the id, and if it was not in the hash map, store the id in the map, and write the whole row to a new file, and if the id were in the hash map, move on. The idea is that lookups in a hash map are O(1) instead of O(n). Edit: there are associative arrays in bash 4 (just looked it up) so the scheme above would work in bash. Use declare -A myarray to create the array.
So you can't just sort the file using sort -u for unique lines? Or is it that the lines aren't fully unique when the id is duplicated?
If you have dupe records, how do you pick which one to keep? If each of the dupes are exactly the same amongst that set of duplicates, you could just 'uniq' your file. If it's a dupe ID, and you maybe just want to retain one entry and to hell with the rest .... mmm.. ok. Try this: https://gist.github.com/unxmaal/1992f34fdcbdb41ec2a6 
Did further research with the parties at hand and I'm OK to drop additional records. So in the end the guts of the script are just doing: sort -n -t, -k1,1 file.csv&gt;out.csv And the rest is fluff to do other housekeeping. Though I learned a ton of other tricks that I had forgotten about in the course of working on this. I'd explain more about it but I legally can't. 
This is a good answer. Also, always do the most readable thing.
how do you know it's done? You might want to add a *wait* at the end: find . -name '*.jpg' | { while read line; do convert -resize 1600x1200 -quality 89 "$line" "$line" &amp; done wait } 
Thanks for the help, but it turns out the real problem was just that the pi was running out of ram. It has 1 gb and no swap, I didn't realize resizing pictures used so much of it. There are 10 files in the directory, around 5 mb each. After I took out the pipe I was able to see the background processes terminating and noticed some of them were being killed. Then I found this in dmesg: [1300317.477243] Out of memory: Kill process 31526 (convert) score 119 or sacrifice child This works like I wanted: #!/bin/bash num=0 for file in *.JPG; do mod=$(($num % 4)) if [[ $mod -eq 0 &amp;&amp; $num -gt 0 ]]; then echo "waiting" wait fi convert -resize 1600x1200 -quality 89 $file $file &amp; num=$(($num+1)) done Watching top I can see the free memory creep down to around 200 mb with 4 simultaneous processes, 5 would always cause it to run out. Which is convenient because my goal is just to utilize all 4 cores. For the record it also works fine piping the ouput from ls to the loop. Thanks again! 
Thanks. See my reply below for the solution.
With bash 4.3 you can have it run a new process as soon as one is done, instead of running 4, waiting for all 4, then running the next 4. #!/bin/bash (( BASH_VERSINFO[0] &gt; 4 || BASH_VERSINFO[0] == 4 &amp;&amp; BASH_VERSINFO[1] &gt;= 3 )) || { printf &gt;&amp;2 'This script requires bash 4.3 or newer.\n' exit 1 } i=0 procs=4 for file in ./*.JPG; do if (( i++ &gt;= procs )); then wait -n # wait for any of the background jobs to complete fi mogrify -resize 1600x1200 -quality 89 "$file" &amp; done wait Don't omit the quotes around $file
If they're in the stdout, then use a pipe to put it into awk, and omit the filename. ```Some command | awk command ``` Edit: and what do you mean by "use the y values in succession"? Like add them up? 
Oops I meant that I have to use all the y values in separate grep commands 
untested: less input | awk -F '[\. ]' '{ system("grep $2 input") }' My gut says this is probably a bad idea, but I'll leave it to somebody better than I to explain why. At the very least, be sure your y values are sane.
Might be worth looking into gnu parallel. parallel --bar 'convert -resize 1600x1200 -quality 89 {} {}' ::: * Gives you a nice progress bar and only spawns a few processes at a time (depending on how many cores it detects).
Since OP says he needs these printed only on lines with `MON` in them, I would update your invocation to: STDIN | awk -F '[\. ]' '/MON/ { print $2, $5 }'
Worked perfectly, thanks!
Bash is bash. You probably just miss some of the tools you are used to on GNU/linux systems. Don't blame it on bash. GNU is not UNIX. AIX is not GNU.
Good catch. I edited my original. I don't *think* it actually matters in this case, but I just forgot to throw them in when I typed it on mobile.
The use of a subshell and mapfile seems a little unnecessary here, no?
Compared to four subshells and no mapfile?
It does matter, because bash attempts word splitting and pathname expansion on the result of an unquoted variable expansion, like `$output`, which means 5 lines may become 1, or 10, depending on the circumstances.
I'm aware of how bash works. Edit: sorry hit the send button early. I'm not sure what made me think it wouldn't need quotes in that reply. In any case, they've been added to my answer. 
I know that but there's some weirdness with how some of the bash built ins work. 
Ok, I'll bite. How would you do that? Your current one: &gt; output=$(playlist.py) lines=$(echo "$output" | wc -l) has four subshells.
I don't consider command substitution to be subshells... Though , yes. It does invoke a subshell. The point is that your example is unnecessarily complex, especially when OP is a beginner. Just because it /can/ be done your way doesn't mean it should. I don't mean to say my way is the best, either. They're two options and there are undoubted many more ways to do it. 
It depends on what the content of the Playlist is. For all we know it might have more than just a song name in it. But yes, an array does seem like it could be useful. However, arrays in bash can be confusing to newbies, and throwing a lot of weird subshells and array stuff is a surefire way to get a case where someone has a script but has no idea what it does. 
I really doubt that. Bash is bash. 
'K Although part of it could be due to some of the security setup. I don't touch that part. 
Apple's version of UNIX is a direct ancestor of BSD but it is not BSD. A descendant is not the same thing as an ancestor any more than you are the same person as your parents.
\&gt;getting downvoted for not being bash-y enough Guys, not everything needs to be an array. If you ignore the one potential nitpick about printf vs echo, there is no problem whatsoever with this solution and I'd argue it's more readable than the other
He has a point. "a\nb\nc\n" (3 lines) → word splitting ("a" "b" "c") → `echo` joining with spaces ("a b c") → `wc -l` (1)
To be fair it's possible to do it with only one fork, while not using any bash-specific features (as NeilHanlon's wasn't). Not pretty, but here it is: output=$(playlist.py) lines=0 while IFS= read x; do lines=$((lines+1)); done &lt;&lt; EOF $output EOF exec dzen2 -fn xft:Droid Sans:size=9 -fg '#F6F9FF' -bg '#404E5E' \ -x 285 -y 25 -w 160 -l "$numlines" \ -e 'onstart=uncollapse;button1=exit;button3=exit' -p 5 -ta l &lt;&lt; EOF $output EOF 
For more flexibility, you can do this directly in shell code (this is not even Bash-specific but will work in any POSIX shell): while IFS=':. ' read -r junk junk key y junk junk z do test "$key" = MON || continue printf 'y=%s; z=%s\n' "$y" "$z" done &lt; filename This way you can do with each of the values what you want in place of the `printf` command.
Could you walk me through understanding it? Especially the first line
Use -eq instead of -gt. Use the below to test it. #!/bin/bash files_logged=0 if [ $files_logged -eq 0 ]; then echo "Working" fi 
is that a function call? I'm pretty sure you don't need the parentheses. Functions take parameters like commands take arguments (delimited by spaces); if your call has no parameters you only need the name of the function
Why does this make better sense than -gt?
Unrelated to the topic at hand (since it's already been solved) but you might be interested by the `trap` command. Example: cleanup() { ret=$? tput cnorm # Show cursor again trap - EXIT exit "$ret" } trap cleanup EXIT INT HUP TERM You can use "trap" to set a hook on various events. Here I'm hooking EXIT (script exit), INT (interrupt, aka Ctrl+C), HUP (hangup, terminal window being closed), TERM (the signal it gets when killed). The reason for it here is so that if you press ctrl+c, or the process terminates for some other reason, you won't be stuck with no terminal cursor. If you use this approach you can remove the "tput cnorm" near the end of the script because it will be done automatically. A cleanup function in a trap is a good place to delete temporary files and such too.
EXIT will trigger also when the script dies because of a signal, so there's no point in trapping any signals. Just EXIT will do cleanup() { tput cnorm } trap cleanup EXIT
Oh, snap, that sounds cool as hell. As you already spotted, my terminal emulator gets all screwed up during testing of scripts because of my heavy tput usage and stuff. But where do I put the trap? Does the entire script run inside of it, like a huge try / catch exception-kinda block?
When you set the trap, it will lie in wait until you either set a different trap on the same signal, or until the script exits (and presumably triggers the trap). So as long as you set the trap before the `tput` which puts the terminal in a state you don't want to keep, it should work. At the start of the script is fine. Related: to unset a trap on a given signal, use `trap - SIGNAL`. This is different to `trap "" SIGNAL`, which sets an empty trap (and thus makes the shell ignore the signal, instead of what it would usually do). You can't ignore an EXIT though.
I forgot to mention that if I put the variables in the actual file, like username="username" password="password" at the top, then that actually works as well with the variables. So it must be the sourcing somehow, right?
I'm confused. My echo command cut out "curl --user \"$username" part from the curl command? Does your username or password contain any characters other than a-z or 0-9? If so, what?
Neither of them contain any special characters. But yeah, it cut it off in a really weird way..
To get the exact command it might be simpler to just put `set -x` before the curl command and `set +x` after. 
An `\r` would cut it off like that. The credentials file might have DOS line breaks.
&gt; GNU bc doesn't do that, sadly... Wait, why 'sadly'? Is there a disadvantage to the way GNU does it?
Good thinking, it could be some weird non-printing character that somehow got inserted in there. Edit: I bet the end of his username has a \r from a Windows copy/paste. 
I think /u/power78 is right. Rewrite your sourced credential file from scratch, don't copy/paste. Make sure you're writing it with a Linux editor, not windows. I'm thinking you have a bogus line ending character at the end of the username. 
Thanks for posting this! Didn't know about the assignable READ_LINE and READ_LINE_POINT variables - wish I'd known earlier!
You know what, I think we might have a winner. I work on Windows through Cygwin and it's entirely possible that I forgot to switch to Unix line endings in Sublime Editor for the tiny .conf file..
Couldn't you just ` zenity --file-selection --multiple | tr '|' "\n" ` and it would work. Or am I just being a complete dumbass when it comes to readline? I wouldn't be surprised if I was honestly.
Eh, I actually can't check until tomorrow, I'm doing this thing at my work computer!
With OP's code, you could type at the prompt: `somecmd` &lt;space&gt;, &lt;Ctrl+g&gt;, select the two files `foo bar.txt` and `bar baz.txt`, and the line would become: user@host:~$ somecmd 'bar baz.txt' 'foo bar.txt' Which will run somecmd with two filenames as arguments if you hit enter. Doing the same with your zenity|tr, it would become: user@host:~$ somecmd bar baz.txt foo bar.txt Which will run somecmd with `bar` and `baz.txt` as arguments, and then try to run the command `foo` with `bar.txt` as argument.
Firstly, bash and tcsh are very very different. This is the wrong subreddit by far (/r/shell or /r/commandline would be more applicable. There's apparently no /r/tcsh). Secondly, when you post it in a more relevant subreddit, explain what what the script should do, and how it is failing (e.g. does it give some error messages? good, show the error messages!).
It wasn't from cut/paste, I work in Sublime Text in Windows through Cygwin, but apparently forgot to set Unix line endings on the small .conf file.
No, don't, use Zsh's excellent filename completion.
Yes, they were quoted in there. It's basically just a file of #!/bin/bash username="username" password="password"
atool
why not? 
You could say the same in response to literally every post on this sub, contributing absolutely nothing. Encourage people to explore their own solutions and maybe, one day, one of them will be able to help you when you fail to find an existing solution to a problem.
True, that is why I said can be used as exercise or improvement. 
does echo "$ASMDISKLINE" also show what you expect to see?
&gt;Well, those are two different outputs from oracleasm I thought so too, initially, but that's not the case. doing: ASMDISKLINE=$(oracleasm querydisk -d ASM_AMIDEVD1) gives this: echo $ASMDISKLINE Disk "ASM_AMIDEVD1" is a valid ASM disk on device /dev/dm-232 /dev/dm-233 /dev/dm-235 but echo "$ASMDISKLINE" Disk "ASM_AMIDEVD1" is a valid ASM disk on device /dev/dm-23[253,23] That's the same variable, two diferent outputs. the [253,23] is doing *something* but I don't know what. 
Your code formatting can be cleaned up by indenting it by four spaces for the left-most margin, then by eight spaces for each additional level of indentation you want to show. Reddit will then format your code properly. You say you are stuck, and want to know what you are missing. Maybe you should state what behavior you expect and what behavior the code is producing. 
Instead of appending to test you overwrite it each time.
Ah yes, the &gt;&gt; operator. How would I implement this though?
Yes. Don't use bash or cgi to do it. Use the proper programming language for the task. 
Thanks. What do you reccomend?
This is awesome!
Sounds like a job for websockets. Not that I know how to do that with CGI. That said, the retro approach of storing a session key and refreshing every now and again to give you an up-to-date view of your session would also work.
Good point, I figured it out (to reorder the code) before you commented. That said, for anyone reading, I highly suggest reordering your code in a logical manner. What I ended up doing was just making a .sh script and having it output to a HTML file (# ./script.sh &gt;&gt; index.html) with the HTML code before and after the loop that generates the line of code. That way, I can just refresh the page when I want to get the latest information.
Fair enough. As a sysadmin in a small/medium sized company I've seen something like this only happen once. On some cheap VPS host one of our servers had a virus in a suspiciously named directory like `/usr/lib/.system` (I don't remember exactly but it was that level of suspicious). This hasn't happened before or since, but it illustrates that full compromises can and do happen.
install clusterssh on a linux desktop (spin up a virtualbox with ubuntu if your on a windows desktop) then you can use that to install the SSH key on all the servers at the same time
Not true, su - requires the root password and sudo su requires your user password
ah true. my bad
Just think of it as a little extra security.
Wow that is a perfect explanation. This hurts my brain a bit. 
Do you mean *which* is useless always or in the case? Also not to be pedantic but, how do you *know* where env is? Is there a reason it cannot be found elsewhere (in a weird distro - and they do happen). About word splitting - is this the behavior we saw above, where bash treats whitespace differently depending on where the "" are? Thats a good resource I didn't know about, thanks homes. 
&gt; When just echoing out on a single line, this is not noticeable, Probably important to note that the only reason it's not noticeable is because `echo` joins its arguments with a space. If your words are all joined with a space in the first place\*, then splitting them by those spaces and joining them back together with a space isn't going to make a difference. \* and don't contain more than one space, or any tabs or newlines
The oneliner in the post is the source. I know people are making github sites for the most trivial little scripts now, but somehow, making a one for a oneliner still seems a little excessive to me.
Fair enough. I'm not I agree that the which command is "useless." It tells you where a binary is located in the path. Why is that useless? 
Geirha mentioned that: `which` isn't guaranteed to be installed on any given system, and nor is it guaranteed to work how you expect. `command -v` does something similar and is a builtin command in zsh, bash, dash, mksh and busybox, so in those shells it works regardless of operating system or binaries present (it'll still work in a minimal chroot). It is useless in many cases because there is often no reason to find the path of a program in a shell script, since things like this work: CC=gcc "$CC" --version # or: eval "$CC --version" No need there to find the path of the program, the shell will find it for you. ---- edit: Furthering the case against `which`: score@kirisame ~ $ some_func() { echo hi; } score@kirisame ~ $ command -v some_func some_func score@kirisame ~ $ command -v fasdf score@kirisame ~ $ which some_func which: no some_func in (/home/score/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/android-sdk/build-tools/19.1:/opt/android-sdk/platform-tools:/opt/android-sdk/tools:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl) score@kirisame ~ $ which fasdf which: no fasdf in (/home/score/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/android-sdk/build-tools/19.1:/opt/android-sdk/platform-tools:/opt/android-sdk/tools:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl) (note, `command -v fasdf` returned a falsy exit code, which can be easily checked with `if`) 
It is and it isn't. I've actually spent a lot of time thinking about what github is , and how I should use it so here I go: For one, github is a place to store all the code I write that keeps it pretty organized for me, let's me get it from anywhere etc. I use a lot of computers all the time - when I want something I usually just get it from my GitHub account since all I have to do is remember what I named it. So I started using github for myself and me and I to store my shit. But then since the code is on a social network where style points do matter, i end up spending a bit more time trying to make my code look hip. Also, I do want to be interacted with, so the thought process goes like "it's a one liner now, but if I do things methodically, maybe someone will fork it and help it grow!" But yeah - that I use github as a portable code container is the biggest reason I put everything up on GitHub. 
The output of `command -v` is arguably more correct there. The `grep` that bash runs when you type `grep` is an alias, rather than the actual command (which only happens to be called by the alias). Also, running in non-interactive mode you won't have any aliases polluting your namespace. But I see your point there. ~~Though it doesn't prove that which isn't useless (for the most part; of course it's not completely useless) -- what use could you have for the full path of a system executable given its name, in a normal shell script?~~ (crossed out because of your other reply)
&gt; there is really no such guarantee ever that something will be available You can generally rely on [shell builtins as mandated by posix](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_09_01_01). If there's something there which does what you want then there's no reason to use an external binary over that. If you're on bash specifically you can of course rely on all the bash builtins on top of that. As for external binaries you can rely on... who knows. I do know that I am surprised every day by how barebones the core utilities on OS X are. &gt; which finds a binary in your path. That's all it does, and does it well Depends on your shell. (Yes I realize this is /r/bash, though people often post here because it's more active than /r/shell) I use zsh for interactive use: score@kirisame ~ % which which which: shell built-in command score@kirisame ~ % which ls ls: aliased to command ls --color=auto I've also seen cases where `which` was aliased to some abomination making use of `--read-alias --read-functions` so that it displayed those when invoked too. &gt; doing more stuff I'd disagree that it does more stuff. Look at `man which` vs `man command` for instance. The default behaviour of `command -v` and `which` (when not shadowed by a shell builtin like zsh's, or aliased to some form of `which --read-alias --read-functions`), is slightly different. `command -v` "indicates the pathname or command that will be used by the shell", while `which` prints "the full path of the executables that would have been executed when this argument had been entered at the shell prompt. It does this by searching for an executable or script in the directories listed in the environment variable PATH using the same algorithm as bash". (The latter sentence being important, because it can't actually determine what the shell will run so it instead does its own directory search)
You want `type`. $ type grep grep is aliased to `grep --color=auto' $ type -a grep grep is aliased to `grep --color=auto' grep is /bin/grep $ type -P grep /bin/grep Run `help type` to see what else it can do.
Thats really cool! I tried to use watch on it, but it garbles the output! What can we do?
I keep receiving ./clockr: line 12: 3000-1N: value too great for base (error token is "1N") Also ... why did you format it like satan?
You're welcome. I'm curious, on what kind of system are you running? Solaris?
This was OS X Yosemite! I love your script man it's incredible. Trying to figure out how it works so I formatted the code... It's in the fruitywatch repo now.
What is this for? Isn't Bash itself fundamentally a task runner? Then why build another one on top of it? I don't get it.
On first glance I noticed this: &gt; if [[ $ColorsSupported &gt; 256 ]] &gt; [[ $ColorsSupported = 88 ]] &gt; while [[ 0 &lt; $# ]] &gt; etc. This is wrong. You're testing whether one string sorts before the other. For instance, 88 would be "greater than" 256. You want shell arithmetic instead: if (( ColorsSupported &gt; 256 )) if (( ColorsSupported == 88 )) while (( 0 &lt; $# )) (note, use double `==` for comparison as the single `=` is an assignment!) 
Alternatively use `-gt` (greater than), `-lt` (less than) and `-eq` (equal)
And that last one flips to the more normal `while (($# &gt; 0))` which is simply `while (($#))`.
Very nice.
if [ expr1 ] || [ expr2 ] ; then echo "Done" fi I'd agree with using a case IF the output is different depending on the valid string. ..also your example shown can just be simplified to: echo $string if you know it will be one of them - you can just check if it's populated
If it's bash and not sh then use `[[` rather than `[` AKA `test`. `case` might be useful, as others have said, but look out for globs in the values of your string variables; see how the `*` in `$foo` matches `xyzzy`. $ foo='bar*' $ case barxyzzy in &gt; abc|$foo|123) echo T;; &gt; *) echo F;; &gt; esac T bash's `==` similarly takes a glob on the right-hand side. It also has a `=~` comparison operator with an extended ergexp on the right. We need descriptions of `string` and `address[ABC...]`'s range of possible values to be more precise. You existing code can appear more brief if you `if [[ ... ]]; then` on one line
Use [`case`](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_09_04_05) (which is not bash-specific, but goes back all the way to the original Bourne shell). case $string in ( "$addressA" ) echo "$addressA" ;; ( "$addressB" ) echo "$addressB" ;; ( "$addressC" ) echo "$addressC" ;; ( "$addressD" ) echo "$addressD" ;; ( "$addressE" ) echo "$addressE" ;; esac case $string in ( "$addressA" | "$addressB" | "$addressC" ) echo "Done!" ;; esac Note: unless you want glob pattern matching, quote the query variables (`"$addressA"`, etc.). However, quoting the variable between `case` and `in` is not needed because (as an exception) it is not subject to field splitting or pathname expansion. It doesn't hurt to quote it, though. 
`case` has already been covered, but you can also use pattern matching with extended globs: shopt -s extglob # enables extended globs (necessary for bash &lt;= 4.0) if [[ $string = @("$addressA"|"$addressB"|"$addressC") ]]; then printf 'Done!\n' fi
Q: Can't this be done just as easily in vanilla sh with no magic? task_one &amp; task_two &amp; main_task wait (If not hopefully this inspires a Q&amp;A entry :D )
Of course! The only limitation is that you never know which command failed, because wait always returns 0. It can return an exit code if you pass a `pid`: task1 &amp; pid1=${!} task2 &amp; pid2=${!} wait ${pid1} || handle_error ${?} wait ${pid2} || handle_error ${?} But the problem I see here, is that waits (and error handling calls) are sequential, so it can act weird, like you handle first error and exit the script, but `${pid2}` might still be running. It just seems simpler to do something like this: runner_parallel task1 task2 || handle_error ${?} Which expands to this: local -a pid local exit_code=0 for task in ${@}; do runner_run_task ${task} &amp; pid+=(${!}) done for pid in ${pid[@]}; do wait ${pid} || exit_code=41 done return ${exit_code} UPD: I need to stop editing my answer that many times. &gt;_&lt; And thanks for this question. You've inspired me to replace `xargs` with some vanilla bash, ~~and `runner_bubble` is ugly, too~~ (now it was officially removed).
&gt; unless you want glob pattern matching I do not want glob pattern matching, so I'll quote my variables. Do you recommend the first or 2nd example you listed?
As seen, I think case is my best bet in the long run, thanks!
Well quite. In theory too it will never be negative. :-)
What does it mean to block an IP-address on Android/iOS? That a TCP-handshake cannot be made (connection timeout)? Would it be sufficient to make the DNS name unresolvable?
If you're trying to get each value to trigger a different action, the first one. If you're trying to get several values to trigger the same action, the second one.
Is there any difference between `"${file%.wav}.mp3"` and `${file/%wav/mp3}`? Is one way of doing it preferable over the other?
Only a difference if the filename doesn't end with `.wav` (which can't happen in this case anyway). $ file=foo.ogg $ printf '%s\n' "${file%.wav}.mp3" "${file/%wav/mp3}" foo.ogg.mp3 foo.ogg And the former syntax is POSIX, while the latter is not.
Cool. Thanks, that works. I guess I'll read up on the flags some more.
&gt; if [ -z $1 ]; then This works more by luck than judgement, but is actually wrong. The `$1` should be quoted; otherwise, if it contains a space or other character in `$IFS`, it split into multiple arguments and the `[` command will throw a usage error. Also, if it is empty and not quoted, it is subject to empty removal, so you are actually executing `[ -z ]` which means "test if the literal string `-z` is empty", which it isn't. So it returns 0 (true/success), which is what you want, but purely by accident. A command `[ -n $1 ]` for an empty `$1` would also return true, which is the opposite of what you want. The above are some of the reasons to use `[[` instead of `[`. `[[` is a shell keyword instead of a regular command, so the arguments within are subject to alternate grammar parsing and you don't get these problems. Also, you should quote variables in other places, such as with `echo`. Otherwise, due to field splitting, any whitespace in them will not be reproduced correctly.
See also: * http://mywiki.wooledge.org/BashPitfalls * http://mywiki.wooledge.org/Quotes
Clever tool!
You could use awk or sed to prepend the hostname to each line of output from the ssh command: #!/bin/bash for host in blade0{11..16}; do ssh -q "$host" "$1" | awk -v "host=$host" '{print host":"$0}' done Don't use uppercase variables for internal purposes, don't put code in variables, and don't use absolute paths to commands. Use PATH instead.
Thank you love 
You inspired me to polish up a nasty one-liner to run things on all Puppeted hosts... #!/bin/bash puppet_master='puppetmaster.widgets.org' # Get a list of hosts Puppet knows about: hosts=$(ssh $puppet_master "puppet cert list --all" | awk -F\" '{print $2}') # Loop through the hosts while read host; do printf 'Running "%s" on %s:\n' "$*" "$host" ssh -n -oBatchMode=yes $host "$*" done &lt;&lt;&lt;"$hosts"
I'd add `-r` on that read to make it not mangle the data it reads. As for `echo` vs `printf`, the danger is not in `$host` expanding to something starting with `-e` or `-n`, it's `$host:$line` expanding to something containing backslashes. $ VAR='foo\tbar' bash -c 'echo "$VAR"' | od -An -c -tx1 f o o \ t b a r \n 66 6f 6f 5c 74 62 61 72 0a $ VAR='foo\tbar' sh -c 'echo "$VAR"' | od -An -c -tx1 f o o \t b a r \n 66 6f 6f 09 62 61 72 0a Notice how bash output a `\` followed by `t`, while sh output a TAB (`\t`) And the same with `printf`: VAR='foo\tbar' bash -c 'printf "%s\n" "$VAR"' | od -An -c -tx1 f o o \ t b a r \n 66 6f 6f 5c 74 62 61 72 0a $ VAR='foo\tbar' sh -c 'printf "%s\n" "$VAR"' | od -An -c -tx1 f o o \ t b a r \n 66 6f 6f 5c 74 62 61 72 0a
Try running `sync` before doing a `du`, so the copy has a chance to complete. Also, you should probably be using `rsync` to copy your files.
One likely scenario is that you have deleted (unlinked) some files under `beds/` that are still open by one or more processes. If that is the case, the disk usage will remain in use until those processes exit. Since those files are unlinked, `cp` will neither see them or bother with them.
`cp` is doing its job just fine. It's your expectation that the disk usage will always be the same after a copy that is wrong.
Please provide some clarification. I may just be misunderstanding everything completely, but it appears to me that people seem to be interpreting your script as non-interactive, but passing ${1+"$@"} to SSH would launch an interactive shell. Someone please explain how this for loop would not launch a single SSH process at a time and exit once each remote login shell exits. Does the -q flag suppress that behavior in this case? If this is a non-interactive script, I would think you could just do $CMD "hostname; ${@}" and get the output you are looking for. If you don't need an interactive shell, you should be able to drop the ${1+"$@"} unless you are targeting non-POSIX Bourne shell compatibility. Like I said, I could just be misinterpreting everything completely and not really understanding what's going on here.
rsync!
username appears to check out!
&gt; I'd add -r on that read to make it not mangle the data it reads. Damn, I always forget that :( &gt; Notice how bash output a `\` followed by `t`, while sh output a TAB (`\t`) I thought `-e` was standard, but it isn’t: &gt; […] if any of the operands contain a &lt;backslash&gt; character, the results are implementation-defined. One more reason to prefer `printf`…
&gt; Please provide some clarification. I may just be misunderstanding everything completely, but it appears to me that people seem to be interpreting your script as non-interactive, but passing ${1+"$@"} to SSH would launch an interactive shell. `${1+"$@"}` is the same as `"$@"`. It's just a workaround for a bug in an old shell, where `"$@"` would expand to empty string instead of nothing when there were no arguments. Nothing that affects bash, so pointless here. As long as at least one argument is provided to the script, ssh will run a non-interactive shell on the remote end, and with no arguments an interactive shell will be run.
Or alternatively xargs awk
How about using something like sed or grep to extract only the dates, use sort to sort by date and pick the last one with tail -n 1? 
First off, I want to outline the distinction between "apparent size" of a directory and the actual "disk usage" of a directory when reporting size statistics. Let's use the concept of sparse files in Linux to illustrate this. Let's set that **beds** wasn't a directory, but a single 900M file. Let's say that **beds** was entirely comprised of binary zeroes. Using the cp command, you could copy this file to a new file called **backupbeds** and if the cp command was implemented with sparse file detection, it wouldn't write a single bit to a data block on disk, it would just mark some metadata in the inode of **backupbeds** and the disk usage would be 0 bytes. The *apparent size* of the file would be 900M because if we told Linux to "fill the holes" of the file in disk, it would spin a kernel thread that actually wrote every single zero to disk and then the *disk usage* would also then be 900M. Using the du command, you could report on the apparent size of the file using the --apparent-size or -b flag. The nuances are important when reporting disk sizes on different file systems which have vastly different disk write behavior and patterns. In the specific case of a distributed filesystem like GPFS, optimizations were built into the filesystem based on the expected workloads that distributed filesystems face that monolithic filesystems like ext4 or xfs do not. GPFS has a management daemon that supervises something called a pagepool that caches sequential files (via indirect blocks). Until the daemon decides to flush those files (these tunables are set by the system administrator based on the unique usage patterns of the clusters they manage) du will report a smaller size until all of the indirect data blocks are flushed. My *guess* is that this is what you're experiencing. *If* the reason you were using du was to confirm that your cp command was successful, I would recommend checking the exit code stored in the $? variable after your copy or using something like md5sum on each file to confirm that the file transfer was successful. 
You've got both the syntax and the concept of `case` wrong. It goes like this: &gt;`case` *word* `in` &gt;`(` *globpattern* `)` *command* `;;` &gt;`(` *globpattern* `)` *command* `;;` &gt;[...etc...] &gt;`esac` Each *globpattern* is a pattern as in filename matching (with wildcards like `*` and `?` and others), but instead of files it is matched against the *word*, which can be anything, including the output of a command (using command substitution, which is done with `$(`*command*`)` or \`*command*\`). So you can do something like: case $(zfs list -t snapshot) in ( *"$yesterday"* ) lastdate=$yesterday ;; ( *"$twodays"* ) lastdate=$twodays ;; ( *"$threedays"* ) lastdate=$threedays ;; esac (Note: the opening parentheses `(` for the glob patterns are optional, but I always include them because I think having unbalanced parentheses is an insane syntactical feature. The variable references within the glob patterns are quoted to keep any wildcard characters in their values from being interpreted as such.) 
OP is purposely wanting to use case.
Thanks for that, I didn't even think to check the sidebar. I'll probably just stick with bash mainly because that what I have on every computer and server (although I do use Bourne again for some of my shell scripts). I know theres not much programming culture around it which is why I originally got into python however, I find that there are a lot of things that I would be better off doing in bash. Thanks again, -yuitimothy
Thank you, that actually explains it very well.
Thank you. I will do it that way as well so I can learn how to use sorting.
Don't use uppercase variable names for internal purposes, and always quote variable expansions when used as arguments. There are other ways to do this as well: extra_dirs=( your/*/glob/*/pattern ) IFS=:; PATH="${extra_dirs[*]}:$PATH" That one has the "drawback" of modifying IFS globally. Another way is using printf -v and some parameter expansion magic. extra_dirs=( your/*/glob/*/pattern ) printf -v PATH %s "${extra_dirs[@]/%/:}" "$PATH" All of them will produce undesirable results if any of the directories matched by the globs contain `:`, of course.
What are you hoping to learn? How to use the shell for everyday tasks, write scripts or something else entirely?
Nope, it's perfectly logical. Your unquoted variable reference `$IFS` is being field-split using itself as the field separators. By definition, that eliminates all characters from the expansion, no matter what IFS contains. As /u/giigu said, quote your variables (unless you turn off both field splitting by making IFS empty and globbing/pathname expansion with `set -u`). Also, to see the complete contents of IFS, including control characters, try feeding it to this `od` command, which will give you both hexadecimal and ASCII representations of each character: printf '%s' "$IFS" | od -v -An -tx1 -c (using `printf` instead of `echo` to avoid the extra linefeed and any interpreting of control characters). 
&gt; because I think having unbalanced parentheses is an insane syntactical feature To be fair: 1) We have it in English 2) Nobody bats an eyelid ;-)
Thank you very much for taking the time to suggest improvements! I've updated my article just now based on your feedback...
Might be easiest to just manually "mark" the hosts that lack this font. I assume you copy your `.bashrc` around to all hosts, so touch a file like `touch ~/.nofancyprompt` whenever you encounter a host that can't display it properly, and have your `.bashrc` decide what prompt to use based on the existance of that file.
Yeah I was thinking about defaulting to the simple prompt and checking a variable to enable the fancy prompt. But I think I like the file approach better as it's easier to make functions to handle enable/disable. Thanks! 
You could filter it out by doing: for n in $(seq -w 01 46 | grep -v 08); do …; done Alternatively, without external programs, you can skip `08` inside the loop: for n in {01..46}; do if [[ $n = 08 ]]; then continue fi cp … done Or, slightly more compact: for n in {01..46}; do [[ $n = 08 ]] &amp;&amp; continue cp … done
You *could* also just literally do two `seq`s: for n in $(seq -w 01 07) $(seq -w 09 46); do … done
Superb! Thank you so much. I've wasted an hour looking for a solution before I figured I could ask for help on reddit!
Here's pseudo code, Function calcDirSize( dirPath ) // calculate size of files in directory using utils like awk Return total size End function Function delOldest( dirPath ) # find and delete the oldest file # log any errors too find $dirPath -blahblah -exec rm {} \; 2 &gt; $logpath/whatever.Log End function # cron Command would look like this (calcDirSize( /x/y/) &gt;= 3gb) &amp;&amp; delOldest( /x/y/ ) I am sure this could be vastly improved. But you should be able to use this plus help on awk and find command to get what you need ;-) Hth. 
 user@server:~/recent$ find -mindepth 1 -maxdepth 1 -type f -printf '%s\t%C@\t%p\n' | awk -F'\t' '{sum+=$1; if (oldest == 0 || $2 &lt; oldest) { oldest=$2; &gt; stuck 
Triple-click to copy an entire line.
ah sry my fault.
As is, for the current working directory, and not subdirectories, it prints the total amount of bytes, the oldest file's time stamp and filename, and if the total amount of bytes is more than 3GB it will stat the filename. You want to change "stat" to "rm" at least, and maybe get rid of the print statements.
Thank you for the suggestions! I'll take a look at that link and try and fix up the script a bit. When you say create a new file system is that equivalent to creating a partition?
Why not give me what i asked about at the first time? :D
If you are open to using python instead of bash: #!/usr/bin/env python2.7 import os start_dir = '/home/derp' size_limit = 3000000000 #in bytes def gen_file_list(start_dir): file_list = [] for subdir, dirs, files in os.walk(start_dir): for f in files: file_list.append(os.path.join(subdir, f)) return file_list def calc_dir_size(start_dir): dir_size = 0 for f in file_list: dir_size += os.path.getsize(f) return dir_size def find_oldest(file_list): date_list = {} for f in file_list: date_list[f] = os.stat(f).st_mtime return min(date_list, key=date_list.get) #remove files by date until directory is under target size file_list = gen_file_list(start_dir) dir_size = calc_dir_size(file_list) while dir_size &gt;= size_limit: old_file = find_oldest(file_list) try: os.remove(old_file) except Exception as e: print e.message, e.args file_list.remove(old_file) dir_size = calc_dir_size(file_list) 
Because handing people poorly vetted one-liners and scripts using `rm` can be irresponsible.
He's not being a dick, running destructive code written by a stranger that you haven't walked completely through and understand could be a VERY bad thing for you. Whatever answer you choose here, you owe it to yourself and your users to go through it line by line and Google the bits you don't understand. Even senior sysadmins/devs do it. 
Here's an update script I put together for my mint/ubuntu boxes. I don't think it's an issue any more, but ubuntu used to leave the old linux-image packages in place after an upgrade, so there's logic in there to remove them. Mostly I wanted to put it here for the "--force-yes" apt-get option, which you might want. Sometimes - rarely - you still get prompted, but I don't know a way around that without piping in output from "yes", which I'm not really comfortable doing. It takes an argument, -r , which will reboot the box upon completion. In the actual script, I use &amp;&amp; instead of newlines - I want to make sure that if something fails, the process halts and doesn't fall all the way through to a reboot. The &amp;&amp; were removed here for readability. #!/bin/bash sudo apt-get update --force-yes -y sudo apt-get upgrade --force-yes -y sudo apt-get dist-upgrade --force-yes -y for version in `dpkg -l linux-image* | grep ii | awk '{ print $2}'`; do current=`uname -r` uninstall="" if [[ $(echo $version|cut -d'-' -f3-5) &lt; $(echo $current | cut -d'-' -f1-2) ]] then uninstall=$uninstall" $version" sudo apt-get purge $uninstall -y fi done sudo update-grub2 sudo apt-get autoremove --force-yes -y sudo apt-get autoclean --force-yes -y sudo apt-get clean --force-yes -y sudo apt-get purge --force-yes -y localepurge if [ "$1" = "-r" ] then sudo reboot exit fi read -n 1 -t 10 -p "Update complete, reboot $HOSTNAME? (Y/N): " RE if [ -z "$RE" ] then RE="N" fi case $RE in "Y" | "y" ) echo reset echo "REBOOTING THE SYSTEM AT YOUR REQUEST" sudo reboot ;; * ) echo echo "Dropping to prompt - reboot manually if you want" ;; esac exit 
Here's another one that might be of interest. #! /bin/sh # lsppa: Script to get all the PPAs installed on a system ready to for reinstall on another system for APT in `find /etc/apt/ -name \*.list`; do grep -o "^deb http://ppa.launchpad.net/[a-z0-9\-]\+/[a-z0-9\-]\+" $APT | while read ENTRY ; do USER=`echo $ENTRY | cut -d/ -f4` PPA=`echo $ENTRY | cut -d/ -f5` echo sudo apt-add-repository -y ppa:$USER/$PPA done done 
Glad I'm not the only one. The only change I make is # when I'm logged in as root. To compensate, I've made my $PROMPT_COMMAND output any non-zero exit codes. I've also experimented with setting the title bar to the PWD and the user@host line, but I never use it and am thinking of changing it. I thought about changing PS1 based on context, so it becomes % or &amp; if something special is happening. But I can't figure out anything that isn't just noise. Maybe if I have an SSH connection. 
Thanks, that second one did it. I've been up since yesterday, and didn't think about that it didn't need anything after the $PWD, so in this case wouldn't be an issue. I was thinking back to one time when I had letters immediately after the variable. (Like if for some reason I needed $PWD**OtherText** ) Thanks for helping my tired mind out!
Yes, `# ` here too for root. I also dabbled with `pwd` in the terminal's title, but ended up preferring then to have familiar names based on their screen location; I tend to know it's that bottom-left window I want to return to. Also, if `pwd` is a common command then `alias .=pwd` can help and is mnemonic; one doesn't need to source often, and `source` is available. Similarly, ~/bin/hn can give hostname.
EDIT: TLDR: I went from this: http://imgur.com/a/rVxMr#1 to this: http://imgur.com/a/rVxMr#0 --- I've been thinking about doing something fancy, and this seems like a good place to ask - how would you embed a conditional in the PS1? Like, say I wanted to display the full pwd unless I was inside a directory that matched a regex, in which case I would just display the match. How would I do this? ~~Actually, I just realized - sed passes through by default, right? So I could just embed the sed command, which would either do the s&amp;r, or not match and display the normal pwd.~~ ~~Thanks guys!~~ Thanks reddit, this is now all I've been working on all day. Here's what I have now: # Echoes the passed in text ($2) with the given 256 color code ($1). Also ends the color code. __echocol() { local col=$1 shift echo -e '\x01\e[38;5;'${col}'m\x02'"$*"'\x01\e[38;5;7m\x02' } # Replacement for the \w in the PS1 __echopwd() { local pwd_intermediate="`pwd | sed 's!.*\(\&lt;\w*\)/\w*/\(\w*companyname.*\)!\1-&gt;\2!g' | sed 's!\(.*\)_companyname[^/]*!\1!g' | sed 's.^'$HOME'.~.g'`" local checkout="`sed -e 's!^\(\w*\)-&gt;.*$!\1!g' &lt;&lt;&lt; $pwd_intermediate`" local branch="`sed -e 's!^.*\w*-&gt;\(\w*\)/.*$!\1!g' &lt;&lt;&lt; $pwd_intermediate`" # echo "pi: $pwd_intermediate, c: $checkout, b: $branch" &gt;&amp;2 if [ "$checkout" == "$pwd_intermediate" ] || [ "$branch" == "$pwd_intermediate" ] then # We aren't in an svn checkout __echocol 63 $pwd_intermediate else # This is an svn checkout echo $pwd_intermediate | sed "s/\($branch\)\(.*\)$/\1`__echocol 63 '\2'`/g" | sed "s/$checkout/`__modcol $checkout`/g" | sed "s/$branch/`__modcol $branch`/g" fi } # Echoes the passed in text ($2), preceeded by an ANSI color code which is # generated based on the hash of the text and the modifier ($1). __hashcolor() { local modifier="a$1" shift local text="$@" local cnf=$(expr `md5sum &lt;&lt;&lt; "$modifier$text" | tr -d [a-z-] | head -c5` % 256) local color_num_fg=$(expr $cnf + 1) __echocol $color_num_fg $text } # Colorizes and echoes the passed-in text randomly, with exceptions __modcol() { local mod="" case $@ in workhorse) mod=3 ;; root) mod=9 ;; # red mrc) mod=10 ;; # green esac if [ -z "$mod" ] then __hashcolor 1 $@ else __echocol $mod $@ fi } PS1='$(__modcol $USER)@$(__modcol $(hostname)) $(__echopwd) $(__echocol 202 \$) ' What this does is give a random yet consistent color to every string passed to __modcol, UNLESS I don't like the random color, in which case I can override by manually specifying the color code in __modcol. I have a lot of svn checkouts that are organized like `$checkout_name/{branches,trunk}/${branchname}_companyname_extra_info`. Say the checkout is named "ness" and the branch is called "coolio", the filesystem looks like `~/code/checkouts/ness/branches/coolio_companyname_2015_01_01/`. That's a lot to have in a pwd when all I care about is that I'm in the coolio branch of the ness checkout. With this new PS1, I went from this: http://imgur.com/a/rVxMr#1 to this: http://imgur.com/a/rVxMr#0 Yay! Thanks for the thread, OP!
 PS1="C:${PWD//\//\\}&gt; "
I need to know what host I'm on. I tend to have a lot of terminal windows open (local and remote), so that's kind of a big deal. It's nice to know who I am and where I am, but not required. 
There is always [bashrcgenerator.com](http://bashrcgenerator.com/). Have fun.
In the case of carriage returns, you can change them to newlines with tr. lame ... | tr '\r' '\n' | ...
Strangely it doesn't work. If i do it without " | grep -Po '\d+%' ", the lines are output below each other, but if I pipe it to grep, the grep waits until the program is finished and then outputs them below each other at once. I have no idea why. If I don't do the tr, the grep waits until the program is finished and only outputs the last line. So something different is happening, but not what I wanted. 
Try ```IFS=$'\r';```
 $ IFS=$'\r'; $ php script.php | grep -P '\d'; ----------5---------- Still does nothing. See my edited submission text if you'd like to test it for yourself. 
Yes, the tr can replace the \r by a \n, but only after the program has finished. You are writing the whole text into a string and then output it at once. If the program would output only a line per second, the tr would wait 5 seconds and then output all at once. But what I'm trying to do is output 1 line every second filtered. Do you know what I mean? Take this for example: function output_test { for i in 1 2 3 4 ; do sleep 1; printf "\r----$i----"; done; } output_test | tr '\r' '\n'; #works output_test | tr '\r' '\n' | grep -P '\d'; #does not work because it waits until the program is finished 
Another option is to use awk instead of GNU grep lame ... | awk -v RS='\r' 'match($0,/[0-9]+%/) { print substr($0, RSTART, RLENGTH) }'
Ah mawk 1.3.3 from 1996, that is an ancient version of mawk, and not POSIX compliant. Why Ubuntu ships with it by default is beyond me as it is useless for most intents and purposes. Install gawk to have a usable and compliant awk, or install a more recent version of mawk.
Doubtful. If you remove the sleep, it works as expected... I think. I'll check in a bit.
Mine is almost exactly the same. Decided to add the functions for getting the Git branch and color for anyone interested. # Get Git branch of current directory git_branch () { if git rev-parse --git-dir &gt;/dev/null 2&gt;&amp;1 then echo -e "" git:\($(git branch 2&gt;/dev/null| sed -n '/^\*/s/^\* //p')\) else echo "" fi } # Set a specific color for the status of the Git repo git_color() { local STATUS=`git status 2&gt;&amp;1` if [[ "$STATUS" == *'Not a git repository'* ]] then echo "" # nothing else if [[ "$STATUS" != *'working directory clean'* ]] then echo -e '\033[0;31m' # red if need to commit else if [[ "$STATUS" == *'Your branch is ahead'* ]] then echo -e '\033[0;33m' # yellow if need to push else echo -e '\033[0;32m' # else green fi fi fi } export PS1='\[\033[0;34m\]\u\[\033[0;35m\] at \[\033[0;34m\]\h\[\033[0;35m\] → \[\033[0;30m\]\e[46m[\w]\e[0m$(git_color)$(git_branch)\n\[\033[0;34m\]\$ '
As someone who loves bash snippets, these threads are quality. Keep it up thx.
What is meant by truncated is a bit vague here. Perhaps show a few lines of output that differ between yum list installed and yum list installed | cat
My `PS1` has the following items: * Different colours for the type of file system that I'm on. * If the function is loaded on a different user/server, that field might be a different colour * If I've SSH'd in, I'll get the address that I've SSH'd in from. This feature is disabled in `tmux` or VNC sessions, since the IP that I'm accessing the session from isn't necessarily the one that started the session. * A function to display basic SVN info (revision number, changes pending flag) if I'm working in a checkout ([Source](https://github.com/regit/subversion-prompt/blob/master/subversion-prompt)). I adjusted the `__svn_stat` function to also check to see if `svn info` succeeded, since not every folder in a checkout will necessarily have a `.svn` folder depending on the SVN client's version. While it is handy, you might find that it backfires on you if you are doing things on a network share that lives at a remote share. I think that making the prompt smart enough to know what constitutes remote would be a bit impractical. If I did, I'd have to resolve the IP of the remote server if I was on a network file system, then see if that address was in the same subnet as any of my interfaces. The easier thing to do would be to make a quick function to toggle the behaviour on or off. * I've been meaning to put a Git version of the SVN prompt functions in as well. I've seen a version or two floating around, it's just a matter of adding it in. Edit: The end result doesn't look much different from the default CentOS prompt. The non-colour changes look like this: [user@server via 10.11.12.13][~/tools][265*]$
I rarely am on another machine, so mine is pretty basic. User and Directory, that's it. I kind of do want to set up some right sided stuff like zsh can do, but I'm too lazy to fool around with it right now. Some time in the next few months I'll probably play around with it.
Yeah. It's something I looked into a while ago, and most people have come up with workarounds, or other ways to use `rpm` to get the information. I couldn't find anything obvious in the yum man page, though it is quite wordy. I was hopeful some sort of buffering magic could fix it, but as I was working out those examples for you, it occurred to me that there's probably some 'internal' column length somewhere that's being set / used with piping or... something. Probably isn't related to buffering at all.
[I prefer to just change the colour.](https://i.imgur.com/sSHn3Yo.png)
How are you liking Colemak with Vim? I normally use Dvorak and have dabbled in Colemak but end up switching back to qwerty for Vim. The arrow keys are in respectable places on Dvorak but all 4 end up being on the forefinger in Colemak. Did you do a bunch of remapping?
I just like the good old `'\s-\v\$ '` from all my time doing reverse shells sh: no job control in this shell stdin: is not a tty sh-3.2# Even though I use zsh, here's my server's PS1s: http://i.imgur.com/BDhgK12.png
Don't worry, there are people who've had decades of experience with Linux, and haven't worked with git. It makes revision control very approachable, and revision control is useful in a lot of ways, so it's worth learning at some point.
I read "chunk" as "drunk." I'm clearly WAY past the Ballmer Peak. Decent explanation though &amp; good research. Well done. Don't take my word on it, though. I've proven not to be trusted at this time of night.
using $* instead?, that will show the entire command line.
- Not. Ever. Using. Eval. - No seriously. Stop using eval. - Just run ```$1``` edit: not $("$1"), just $1
pls explain why?
http://stackoverflow.com/questions/17529220/why-should-eval-be-avoided-in-bash-and-what-should-i-use-instead 
Awesome, didn't know about `stdbuf`.
&gt; Just run $("$1") This doesn't seem to be working. function exec_cmd { echo ${1} $("$1") } $ exec_cmd 'echo hello' $ echo hello $ bash: echo hello: command not found
I pass a string to `exec_cmd`. So it will always have one argument. For example: exec_cmd "echo test" 
Yeah I had that before and for some reason questioned myself.
Just make sure to quote it. eval_line() { printf '%s\n' "$1" eval "$1" } - EDIT: `$1` and `$*` which has been suggested by other comments, are wrong as they will cause unwanted word-splitting and pathname expansion to occur on the command. If you use `"$@"` instead exec_cmd() { printf '%q ' "$@" printf '\n' "$@" } you can run it as exec_cmd echo "this is a test" instead of eval_line 'echo "this is a test"' The `eval_line` command will be more like passing a string to `bash -c` or `su -c` or `ssh`
$ man man
What if I told you... that I wasn't being a dick, and you should probably stop projecting your own issues onto other people.
well it is nice and all but why would you do this?
stop crying like a baby
Yeah, man. Then again, I much prefer the [luxury, simplicity and consistency of my terminal.](http://i.imgur.com/soNemGE.png)
I love the command-line too, but man pages only contain about 5% of what I need to know.
Having just $1 requires that you always quote your text after exec_cmd.
On my ubuntu machine I can sync my fonts (TTF) in my ~/.fonts directory, so that might work on most linux distros
Does something like `awk -F [][] '/error/ {print $2} /mail/log/log.log | awk '!a[$0]++'` work to generate your unsorted uniq PIDs? Use [ or ] as field separators, match lines with pattern 'error', print the 2nd field (the PID), and pipe to awk which prints when a[PID] == 0 (first time it sees it). edit: Using a while loop: #!/bin/bash logfile='/mail/log/log.log' declare -a pids while read pid; do [[ ${pids[$pid]} -eq 1 ]] &amp;&amp; continue printf "%s\n" "$pid" pids[$pid]=1 done &lt; &lt;(awk -F [][] '/error/ {print $2}' "$logfile")
Huh? What do you mean by that? What does that mean? Sorry, I'm extremely new to bash. 
If I'm reading your code right, it looks like you are storing a string in the $selected variable and then trying to use it as a list in your for loop. Assuming your package names have no embedded spaces, you may want to do something like this instead: selected=( $(yad --list --checklist --column "Block" --column "Package" \ --separator "" --print-column=2 &lt;&lt;&lt; "$packages") ) Those outer parens cause what's inside to be treated as a list of items separated by spaces. If they are separated by some other character, you can set $IFS to that character before running the code. Then in your for loop: for i in "${selected[@]}"; do adb shell pm block "$i" done Should give you the results you want. 
 #!/bin/bash packages=$(adb shell pm list packages -e | sed 's/package://' | sort) selected=( $(yad --list --checklist --column "Block" --column "Package" \ --separator "" --print-column=2 &lt;&lt;&lt; "$packages") ) for i in "${selected[@]}"; do adb shell pm block $i echo $i done Same behavior :/ Added in echo to show it's still iterating them the same. [parker@x3720 ~]$ ./adblock (yad:25485): GLib-GObject-WARNING **: The property GtkButton:use-stock is deprecated and shouldn't be used anymo (yad:25485): GLib-GObject-WARNING **: The property GtkSettings:gtk-button-images is deprecated and shouldn't be Gtk-Message: GtkDialog mapped without a transient parent. This is discouraged. new blocked state: true com.moez.QKSMS new blocked state: trueroid com.snapchat.android 
 #!/bin/bash readarray -t packages &lt; &lt;(adb shell pm list packages -e | sed 's/package://' | sort) # printf '[%s]\n' "${packages[@]}" options=() for p in "${packages[@]}"; do options+=( "TRUE" "$p" ); done yad_params=( --list --checklist --column='Block' --column='Package' --print-column=2 --separator='' --width=400 --height=600 ) readarray -t selected &lt; &lt;( yad "${yad_params[@]}" "${options[@]}" ) # printf '[%s]\n' "${selected[@]}" for s in "${selected[@]}" do adb shell pm block "$s" done
Is your file literally called "$1.hh"? If that's the case then you need to escape that dollar sign with a backslash for the assignment to work. You also probably want to surround the assignment expression with $() like so: hffile=$(cat \$1.hh) Note: there is probably a better way to do this.
I'm not sure if its a reddit formatting issue but it looks like you are making the hhfile variable literally "cat $1hh.txt". What happens when you wrap it in a subshell? hhfile=(cat $1hh.txt) 
 ▶▶▶ kalgynirae@hostname:lastdircomponent [0]$ I find that the `▶▶▶` really helps me visually identify the boundaries between commands that output lots of stuff. The rest of the first line is colored based on the hostname and current directory (like, I actually hash username+hostname+directory into an integer and use that to choose the color); that's another visual clue that's helpful because I often have 3 or 4 terminals open on my screen at once, often connected to remote machines. The `0` is the number of background jobs, which is important to me because I background processes all the time. Implementation [on GitHub](https://github.com/kalgynirae/dotfiles/blob/2cdbb60f767f3f0b08b0eb67a3b650da19233d36/bashrc#L72).
You can probably just use ed: ed $1.hh &lt;&lt;&lt; "/class/i $library . wq"
no idea, i just took the code and cleaned the shit out of it using arrays and stuff ;-)
 library=./Point.hh awk -v header="$library" ' /^class/{ f=1; for (i=0;i&lt;n;i++) print lines[i]; printf("#include \"%s\"\n\n", header); } NF { lines[n++]=$0; } f { print } ' test.hh &gt; test.hh.tmp &amp;&amp; mv test.hh.tmp test.hh
Use a `select` loop. It works just like `for`, but prints a menu of selections. The chosen option is stored in the variable you specified, and what the user literally typed in is stored in $REPLY. You can use `break` and `continue` like in `for` and `while` loops. See `help select` for details. A simple example: select textfile in *.txt; do echo "You chose the file $textfile" done 
No, it makes a list of all the *.txt files in the current directory. No file is created. This was just an example, you can put any list of items in its place, like: select item in firstitem seconditem thirditem fourthitem; do echo "You chose item $item" done 
i keep getting select: not found, what is wrong? 
Sure you're using bash? Check with: echo $SHELL echo $BASH_VERSION 
output: user@pc:~$ echo $shell user@pc:~$ echo $bash_version As those were blank i tried: user@pc:~$ bash --version GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software; you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. 
shell variables are case sensitive, you need `$SHELL`, not `$shell`.
oops :) new output user@pc:~$ echo $SHELL /bin/bash user@pc:~$ echo $BASH_VERSION 4.3.11(1)-release
UPPERCASE.
Lastly, make sure you run the script with `./scriptname` and NOT `sh scriptname`
Maybe use an until loop? until [[ ${selection} == 0 ]] ; do echo "" echo "1) selection 1" echo "2) selection 2" echo "3) selection 3" echo "0) press 0 to exit" echo "" echo -n "Enter Selection: " read selection echo "" case ${selection} in 1) do some commands;; 2) do different commands;; 3) do commands for option 3;; *) echo "Please make a valid selection";; 0) exit 0 esac 
Try adding this to the end of the definition of `packages` (before the final \`): | sed s/\\r\$// `adb shell`, for some dumb reason, sends a CR before every LF. It breaks pipes badly. This should fix your issue. --- Also, on an unrelated note, consider pipes and functions rather than variables: get_packages() { adb shell pm list packages -e | sed 's/package://' | sort | sed s/\\r\$//; } select_packages() { yad --list --checklist --column "Block" --column "Package" --separator "" --print-column=2; } disable_packages() { # can be done with xargs if you don't like this way while IFS= read -r package; do adb shell pm block "$package" done } get_packages | select_packages | disable_packages This means you don't need to deal with the potential pitfalls of using `echo`, and `yad` can read in the contents of its dialog while they're still being generated (and it does do this). This also splits on newlines correctly.
1. To read IP addresses separated by a dot instead of spaces, you can set `IFS` (Internal Field Separator) to make `read` split them differently. 2. There’s two parts to printing only the IPs that responded: 1. Hide the output from `ping` itself: output redirection 2. Find out whether `ping` succeeded or not: fortunately, `ping` exits with an error code if the ping didn’t succeed; read up on exit status and conditionals I don’t want to solve the assignment for you, but hopefully this will be enough to get you going…
Here's a hint: pipe the output of the loop through a `grep` command to print only the output you want. Good luck.
I don't think this is the best option. Grep has a lot more overhead than simply redirecting the output to /dev/null and checking the builtin exit code from bash.
So I've tried using IFS, but I can't seem to make it work: Is this wrong? IFS=. read ip1 ip2 ip3 ip4 &lt;&lt;&lt; "$input" (used this site: http://www.catonmat.net/blog/bash-one-liners-explained-part-two/ ctrl-f for "ifs")
Almost there! `command &lt;&lt;&lt; input` is equivalent to `echo input | command`. In other words, what you’ve done here is telling `read` to read its variables from `"$input"`, instead of from user input. You don’t want that, so remove the `&lt;&lt;&lt; "$input"` part.
Oh, and another thing: **Always use `read -r`.** Period. Without `-r`, `read` will do stupid things with backslashes: $ read smiley ¯\_(ツ)_/¯ $ echo "$smiley" ¯_(ツ)_/¯ # he lost his arm :( You never want that, so just get used to never use `read` without `-r`.
Quote your variable expansions?
what? I don't think it is the code's problem because it is the same in every other machine. What's so wrong about putting the script in a puppet-like environment? 
Portability?
I guess we just assume 24 bit mask? &gt;Develop a bash script that takes an IP (IPv4) from a user. Based on this IP, the script should ping all (255) IPs within that range. For example, if the user enters the IP 192.168.1.50, the script should ping every IP from 192.168.1.1 to 192.168.1.255. I think a good tactic here is to assign your IP to a variable, then use parameter expansion to extract the first three octets out of this variable for your network prefix, then use the range expansion you already have to get all 255 addresses in full form. see: [http://www.tldp.org/LDP/LGNET/issue18/bash.html](http://www.tldp.org/LDP/LGNET/issue18/bash.html) &gt;Update the script so that the script only prints the IPs of which there was a reply. Then as others have pointed out, simply use the fact that bash will check successful status codes of a command in an if block. You can use this to print the addresses that responded succesfully. [http://mywiki.wooledge.org/BashPitfalls#cmd.3B_.28.28_.21_.24.3F_.29.29_.7C.7C_die](http://mywiki.wooledge.org/BashPitfalls#cmd.3B_.28.28_.21_.24.3F_.29.29_.7C.7C_die)
Probably a terrible answer but take a separate variable for day month and year and do a loop. Increment day until it hits 31 then set it to 1 and increment month, loop till month hits 12 and day is 31 then increment the year and set moth and day to 1. Just concat all the variables to get the part of the curl request you need. 
Try this in bash (not on a mac, but on CentOS or Debian): i=757400400 while [ $i -le 1439956800 ] do date +%Y%m%d -d @$i i=$(( $i + 86400 )) done As an extra added bonus, you could have some fun with regexps &amp; see how many leap-years happened in that time-period, on one line, with: i=757400400; while [ $i -le 1439956800 ]; do date +%Y%m%d -d @$i; i=$(( $i + 86400 )); done | egrep "0229$" | wc -l
I was thinking something like this: curr_date=19940101 end_date=$(date +%Y%m%d) while [ "$curr_date" != "$end_date" ]; do printf %s\\n "$curr_date" # TODO: Replace that with your curl command # Like this: curl -o "$curr_date" -- "http://example.com/$curr_date" curr_date=$(date +%Y%m%d --date="$curr_date + 1 day") done I put prints there so you can see it in action, rather than actually curling it. Bear in mind that if you download so much in a row you might get banned from the site (depending on how worried they are about bandwidth/bots). It outputs dates correctly. See the boundaries of the months, for example: 20150131 20150201 ...snip... 20150228 20150301 ...snip... 20150331 20150401 
I like some of [these](http://www.askapache.com/linux/bash-power-prompt.html). They are fast, give essential information so I can monitor system usage and let me know at a glance where I am when working in the datacenter. The only thing that I've added is a bit of code specific to git repositories.
Thanks - this is useful.
Instead of epoch seconds, you could also do: n=0; while :; do date -d "19940101 + $n day"; let n++; done 
Through an HTML request? -&gt; CGI script on webserver, curl/get with POST data. Or do you want to write the array's content in your HTML source?
If I used php -f filename.php How would I refer the variable value current?
Is it allowable to have a small window of time where an old date under /quicknfs doesn't exist?
Thanks for all the feedback, I will go through and change with your suggestions later tonight. 
I made a few changes. still much clean up to do.
Something like the following should be fairly portable: if ! type sed; then printf '%s\n' "Please install sed" 1&gt;&amp;2 exit 1 fi
or in case there may be more sets of parenthesis: for file in ./*-??????\(*\).jpg; do mv -i "$file" "${file%(*).jpg}.jpg" done
Use `rename`.
Use `find` to locate /quick day directories due to be demoted. A bash `while` loop to process each of those. `cp -a` to copy it recursively to /big. `mv` the /quick one to a '.old' version, or `rm` it now if it doesn't take long; the window starts here. `ln -s` from /quick to /big, closing the window. Remove the .old if you only moved it out the way. Do all this with a `set -e` script so if any command exits with an error, e.g. cp, you stop rather than trundle on making things worse.
Salary range?
A rough estimate would be $20 to $23 hourly on a W2
You can have me for $150 hourly on a 1099.
 # Replace 'last month' with 1994-01-01 to get the full range of datestamps for (( i=$(date -d 'last month' +%s); i&lt;=$(date -d today +%s); i+=86400 )); do echo "curl -s http://example.com/$(date -d @$i +%Y%m%d)/ &gt;&gt;outfile" done
$23/hour (~$46k / year) for that job description seems like a pretty big ask. Seems more like in the $30/hr - $35/hr range to me... maybe things are different in Lockport tho. 
It is an entry-level role. They are looking to hire new grads.
I think it is because busybox uses the [almquist shell](https://en.wikipedia.org/wiki/Almquist_shell), not bash, and so function declarations [do not use the `function` keyword](http://linux.die.net/man/1/ash): &gt; **Functions** &gt; &gt; The syntax of a function definition is &gt; &gt; `name () command` &gt; &gt; A function definition is an executable statement; when executed it installs a function named name and returns an exit status of zero. The command is normally a list enclosed between ''{'' and ''}''. Try removing those. Additionally, you have what looks like a function declaration (to the shell) on lines 80, 82, and 94. Fixing that let me run it with `busybox sh`. *edit* the function keywords are probably fine; fix the function calls on the above lines.
Could you explain what you mean about the function declarations? The Gentoo wiki was where I got the format, so far as I can tell I'm calling functions on those lines, not declaring them. 
Fixed the things you mentioned, now it's complaining of an unexpected ')' on line 34 (the first function declaration).
Assuming that the important thing is in column 2, e.g. 1,0015000000wtp5VAAQ,2 1,0015000000wtp5VAAQ,2 1,0015000000wtp5VAAQ,2 1,0015000000wtp5VAAQ,2 Python version, takes about 1.5s on a 17M file. #!/usr/bin/env python import sys import csv from itertools import groupby from operator import itemgetter with open(sys.argv[1], 'rb') as data: rows = csv.reader(data) for key, group in groupby(rows, key=itemgetter(1)): g = list(group) cols = len(g[0]) - 1 for row in g: print ','.join(row) for row in g: print ',' * cols Ran as $ python foo.py in.csv &gt; out.csv This is probably doable in awk but it took less time to write the script than to figure out the awk syntax.
That's how my mind was feeling while Googling for something.
Right now, if you have a group of 5, it will print 5 blanks. If you want to print 10 blanks for a group of 5, change for row in g: print ',' * cols to for row in range(len(g)*2): print ',' * cols Or if you mean 5 + 2, for row in range(len(g)+2): print ',' * cols
 xargs -n 1 ping -w 2 &lt; hostlist.txt Or, programmatically for nice output: while read host; do echo -n $host ping -c 1 -w 1 $host &gt;/dev/null 2&gt;&amp;1 if [ $? = 0 ]; then echo " is up." else echo " is DOWN." fi done &lt; hostlist.txt 
It indeed is, you wanna provide a link to the file so i can test my script?
This worked perfectly, thank you! One last question; some of the files also have "_RESIZED" after the parentheses that needs removed, so the original file looks like this filename-YYMMDD(Tag1, Tag2, Tag3)_RESIZED.jpg I need it to come with the same output as above filename-YYMMDD.jpg I'm trying to modify it myself, but if you or someone could chime in, that'd be great. Thanks again!
You can use [extended globs](http://mywiki.wooledge.org/glob#extglob). shopt -s extglob # enable extended globs for file in ./*-??????\(*\)?(_RESIZED).jpg; do ... And the glob in the parameter expansion must be changed accordingly. Oh and thanks for the gold!
Run `alias wget` in a cygwin, and post the output.
Or `bash -x download.sh`
And no other output?
\+ wget $'http://www.wuxiaworld.com/mga-index/mga-chapter-293/\r' --2015-08-24 22:19:25-- http://www.wuxiaworld.com/mga-index/mga-chapter-293/%0D Resolving www.wuxiaworld.com (www.wuxiaworld.com)... (some ip addresses) Connecting to www.wuxiaworld.com (www.wuxiaworld.com)| (some ip addresses)... connected. HTTP request sent, awaiting response... 404 Not Found 2015-08-24 22:19:25 ERROR 404: Not Found. oh man that extra \r at the end of the url is probably the problem... hmm When download.sh has only wget http://www.wuxiaworld.com/mga-index/mga-chapter-293/ I get --2015-08-24 22:22:40-- http://www.wuxiaworld.com/mga-index/mga-chapter-293/%0D Resolving www.wuxiaworld.com (www.wuxiaworld.com)... ip's Connecting to www.wuxiaworld.com (www.wuxiaworld.com)|ip's... connected. HTTP request sent, awaiting response... 404 Not Found 2015-08-24 22:22:40 ERROR 404: Not Found. This time there's a trailing %0D Would you know how get rid of those?
Get an html parser. Doing it with awk or some other line-based, regex-based tool will only bring pain.
I want to pipe multiple html files into a text file. With a script I could automate the whole thing. In my current case there are actually no inner divs, so I'm pretty sure it's not suppose to be difficult, I just don't know which command is used for this
I looked at the options and it looks like --verbose is the default. I also tried adding it but no additional information was printed
I was thinking bash would be more flexible. First I want to download a bunch of html files (wget on a website that has chapters of a book), parse out everything but the chapter text, then append it to a file. If I could just get the command to return the result of a regex (&lt;div class="entry-content"&gt;(everything in between)&lt;/div&gt;) I'd pretty much be set. I could write a Python script, I just really want to do it this way since I remember using this for a school project a few years ago, I just can't remember the command!
this barebone program works with a single file (sys.argv[1]) #!/usr/bin/env python3 import sys from lxml import etree with open(sys.argv[1]) as html_file: html_tree = etree.parse(html_file, etree.HTMLParser()) for node in html_tree.findall('//div[@class="entry-content"]'): print(node.text) It could be trivially extended to manage more and to parametrize the xpath. That said you could also run it with all wgeted files one by one like this for f in *.html; do python3 prog.py "$f"; done &gt; common_output_file result: one big file with contents of all `div.entry-content` nodes.
 #!/bin/bash #funny idea touch pussy.rub; mv pussy.rub pussy; cat pussy; echo 'finger' &gt; pussy; mv pussy pussy.wet; sed -i 's/finger/dick/g' pussy.wet; sed -i '/dick/d' pussy.wet; echo 'dick' &gt; pussy.wet; for i in {1..20}; do sed -i '/dick/d' pussy.wet ; echo 'dick' &gt; pussy.wet ; done end=$((SECONDS+4)); while [ $SECONDS -lt $end ]; do echo 'cum' &gt;&gt; pussy.wet done ; sed -i '/dick/d' pussy.wet; xmessage Thanks 
I didn't learn much about loops, but it was an idea I had and I made it do what I wanted, I guess I could have piped it all out into the terminal screen so people could watch it happen but that wasn't really the point
In another notation Windows line endings are `\r\l`, where Unix only uses `\l` Hence in your file you got `...apter-293/"\r\l` Bash finds the Unix line ending `\l`, so `\r` must belong to the URL. Switch to Unix line endings.
This looks like a csv export of ids from Salesforce. Python and more specifically using [pandas](http://pandas.pydata.org/) makes this sort of data wrangling a lot easier. Its hard to make out what exactly you are trying to do. If you can state your requirements in more detail, I'd be happy to provide some snippets of code to help address what you are facing.
Thanks for the reply. Yeah this is a dump of Salesforce data. I'll PM the details. 
do `which -a wget` and then call the full path in your script
I somewhat-recently set up a web-scraper that downloads files with bash and curl (~= wget) to a file then call a Python script with the Beautiful Soup module to pull the data I want. At first I dumped to a CSV file then later dumped to a JSON file instead. You can try regex, but trust what everyone is saying that you'll save yourself pain using a purpose-build HTML parser to get HTML data.
Hi /u/t13r! I've been working on something like this as well because it seems like I'm always running a few specific commands &amp; I just kinda like the idea of a fancy menu on an otherwise command line only box. I'm really reluctant to post my code here because its rough &amp; I'm worried it will be completely ripped apart, but I'll send it to you via IM if you want. 
You could use sed, something like e.g. sed -n '/&lt;div/,/&lt;\/div/{//!p}' somefile This matches &lt;div as a first string, &lt;/div as a second string, treats it as a range (that's the comma in the middle) and prints everything between them (that's the {//!p} bit) A simpler example would be to print out the lot and grep -v the divs e.g. sed -n '/&lt;div/,/&lt;\/div/p' somefile | egrep -v "&lt;div|&lt;/div" Obviously if your needs get more complex, then it's going to get increasingly more painful, and as others have said - you should look to better suited tools.
Scripts being torn apart and rebuilt is how the world gets better bash scripts, IMO.
Awesome! Thanks! I guess I was over thinking it, aha. Such a simple solution. 
Thanks so much for all that feedback! You're the person I need to review the bash code I write that nobody wants to look at :) I don't have much to respond on the feedback on the code itself, except maybe for the use of `eval`: I'd have thought that using `eval` would actually be worse here (you know… "eval is evil" and all that). I guess not? * For the `list` argument, the point was to have a way of knowing what the task will do. I needed something like that in the script I was writing (the `i18n` example at the beginning of the post) because that script has multiple tasks, so I couldn't just run `some_func` since `some_func` was defined in my `i18n` script (and not sourced, so not callable). I guess that could certainly be refactored in a different way... (like what you suggested, having `run_commands` take `stdin` instead and have my `i18n` script parse `list` rather than passing it to `run_commands`) * Yes I print the non-executed commands on purpose. Arguable certainly, but I wanted to show where the user stood in the task at hand. E.g. if it fails on the last step on something unimportant, you know you don't really have to run the task again after fixing the issue. Or maybe the failure and its fix means you should skip another step. I don't have a good example off-hand but the main idea is to know where you are and what's left to do. * `$skip` here is used to tell `run_commands` which command to start at. It's for when a command fails and you want to pick up where you left off. You *do* want to execute that command but only once. So if one command failed in the middle, you won't want to run the first ones again until you've done them all. I could keep track of that in other ways of course (in a file, a variable…) so that whenever you'd re-run it, it'd know where to start again. But then there were questions on *where* that would live, for how long, how to be able to override it, etc. Using an argument was a bit simpler and lets the user in charge. * Checkmarks. Fair enough. It was just a little bit of useless eye-candy to liven up a command line tool. :) &gt; Last thing I promise: https://i.imgur.com/Q4oBKQD.png Ugh. Not much I can do about that sadly :-/
Like /u/UnchainedMundane said. Look at all the great feedback I got!
Oof, yeah! Sorry, I keep switching back and forth between Bash and C++ about once a week for a class I'm taking over the summer and sometimes I forget all of the little intricacies of each language. Thanks for the heads up!
&gt; I'd have thought that using eval would actually be worse here (you know… "eval is evil" and all that). I guess not? So many people abuse eval that they need to be told "eval is evil". It's evil for most purposes (e.g. parsing JSON using javascript's eval, calculating using python's eval, or doing *anything* with unsanitized input that isn't meant to run it directly), but when your use case is to execute a command from a string, that's a perfect use of it. Regarding printing the commands, I see what you mean there. It seems useful for sharing scripts and such, in that case. `skip` seems useful but I've found that in a lot of scripts and aliases I've used, it isn't necessary (e.g. `git checkout develop` `git merge master` -- if you execute commands like this twice in a row, it's about the same as executing them once). Regardless, it's great for the scripts in which it *is* necessary (if part of your objective is printing the whole script at some point). &gt; Ugh. Not much I can do about that sadly :-/ I thought by the message it might be a mirror of the site or something. It didn't occur to me that you might actually work there, for some reason!
 awk -vcode="$CODE" '{print code}' This worked for me. I didn't use the full awk command you have there but it should still be the same. If I could hazard a guess at where you're going wrong (a complete stab in the dark)... were you trying to use `$` to reference the variable? `$` in awk is an operator which refers to a field whose column number is the expression following the dollar sign. You can use `$ 2` or `$(1+1-1+1)` and these are equivalent to `$2`. The relatively common idiom `$NF` gets the last column because `NF` contains the number of the last column\*, then `$` gets that column given the number. ^(* Pedantry: Actually, the number of fields/columns in the current record. They will always be the same number, it's just that NF is intended to be a cardinal number while calling it the "number of the last column" or using `$` on it is treating it as an ordinal number.)
Hey thanks for the prompt reply! Here's what I've tried awk -v var="$locator" '/var/{print "\t","\t"m "'"$CODE"'"}1' $DHCP_POOL &gt; $Staging where $locator contains my pattern. The end result is that awk just looks for the substring var instead of the variable... Not sure what I'm doing wrong... I even tried letting the shell expand the expression itself by doing this: string="'/$locator/{print \"\\t\",\"\\t\",\"'\"\$CODE\"'\"}1'" awk "$string" $DHCP_POOL &gt; $Staging That threw an error saying invalid expression ''' which I didn't quite understand either... 
&gt; awk -v locator="$locator" -v code="$CODE" '$0 ~ locator { print "\t", "\t", code }1' "$DHCP_POOL" &gt;"$Staging" This worked perfectly and feels a lot cleaner. Thanks a BUNCH! 
Thanks for the help! I used this solution with a slight modification and it worked perfectly. I'm still really new to bash. Been doing nothing but powershell for most of my career so I will make sure to follow your advice about variables and a quotations. It still breaks my brain a little bit that variable declarations don't like spaces like test = "test" needs to be test="test" But I'm learning. Thanks again for the help! You guys got me in the right direction!
Let me know if you guys have any suggestions.
Out of curiosity, why use a "while read" instead of a "for I in $(cat file)" ? 
@parkerlreed based the data you provided to me, it appears that what you want is basically add two rows for each row you have. The additional rows are for 'Medium Duty' and 'Heavy Duty' service categories, right? If my assumption is correct, then building on what [primordia](http://reddit.com/u/primordia) had provided, the following python code should do what you are asking for: #!/usr/bin/env python import sys import csv from itertools import groupby from operator import itemgetter with open(sys.argv[1], 'rUb') as data: rows = csv.reader(data) for key, group in groupby(rows, key=itemgetter(1)): g = list(group) cols = len(g[0]) - 1 for row in g: # print the row 'as-is' from the source csv file print ','.join(row) # print a copy of the row for the 'Medium Duty' Service_Category mdrow = [row[0:5],["Medium Duty"],row[6:]] mdrow = [item for sublist in mdrow for item in sublist] print ','.join(mdrow) # print a copy of the row for the 'Heavy Duty' Service_Category hdrow = [row[0:5],["Heavy Duty"],row[6:]] hdrow = [item for sublist in hdrow for item in sublist] print ','.join(hdrow) Please note that this will print the 'Medium Duty' and 'Heavy Duty' lines right below the 'Light Duty' record. Net effect should be the same as you originally had asked. One note of caution is that you are copying the Id (column 1) down to the new records. This has ramifications from a Salesforce point of view. I presume that you are either going to ignore this column in your Jitterbit or Dataloader mapping file. Lastly, this script will print the header row thrice. I figured that you can manually edit the two redundant rows ;-) Caveat Emptor: I provide this derivative code with no warranties implied or otherwise whatsoever. Hope that helps!
@whetu said will work. I've used this approach for filtering out data in xml files. You can even output more than one match by using a semicolon between the patterns. Example: cat SomeSourceFile.xml | sed -n '/&lt;objectPermissions&gt;/,/&lt;\/objectPermissions&gt;/p;/&lt;fieldPermissions&gt;/,/&lt;\/fieldPermissions&gt;/p' This will print only the &lt;objectPermissions&gt; and &lt;fieldPermissions&gt; elements in the source file. Sorry for the obscure xml element names, it was from a snippet I had saved from a while back.
http://lintut.com/colorize-log-files-on-linux-using-ccze-tool/ edit: I figured I would add a little more, since it may be unclear. It's fine to re-invent a wheel (which is what your script appears to be doing, since there are at least a few tools like ccze that will colorize logs for you), but be aware of what exists already, and learn from what they've done, to extend your features, if you choose to continue down that route.
Put `set -x` on the line before, and run it again. It'll give a better idea on what's going on. To get code-blocks here on reddit, put an empty line before it, and prepend each line with four spaces.
Your first example has quotes but your second example doesn't.
$ sudo apt-get update $ sudo apt-get install multitail $ multitail -c /path/to/file.log
make cool little scripts. i have scripts that automatically connect me via ssh to my server based on whether or not i'm local, copy files, wake it up, sexy prompt, sync my dot files to my server, list all my 256 colors in blocks, op cowsay etc. customize EVERYTHING. anything you can think of. I do use zsh now doe, but I started off in bash, the possibilities are endless. get creative :)
&gt; So are there any online projects Google is your best friend. Countless fun online projects and resources out there. &gt;you guys have any advice on how a home computer user can keep bash fresh in his head ???? Consistently work on little BASH projects to keep the information fresh in yo head. This could be as little as once a month. Always ALWAYS **ALWAYS** RTFM
Go through this list: http://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html These are the 'built-in' commands for bash. Write a script that lets you play with the ones you get excited about. 'while', 'read', do' 'if'.. these are all very powerful and form the basis of a loop; while there are still values to read, read a line from my wordlist, strip the whitespace, and perform a DNS lookup on it, appending '.com'... Another good way is to write a script that generates a bash script stub for you; Shebang, date, favorite functions, that sort of thing, popping the new script open in your favorite editor.
Ok. After editing .bash_profile you need to apply the changes. Open new terminal emulator, or use `source ~/.bash_profile` command, to apply changes to the current terminal.
Ok, but do you know why it worked without the quotes? I managed to change the alias successfully but I want to know why it worked. 
Cause it's a bash assignment feature. ┌[lord] [homesrv:ssh]:~ └&gt;q=testtext ┌[lord] [homesrv:ssh]:~ └&gt;echo ${q} testtext ┌[lord] [homesrv:ssh]:~ └&gt;q=test text -bash: text: command not found ┌[lord] [homesrv:ssh]:~ └&gt;q="test text" ┌[lord] [homesrv:ssh]:~ └&gt;echo ${q} test text
When you followed your friend's suggestion, did you just type the alias command directly into bash or did you add it to the profile? With or without the quotes should work if you just type it directly into the terminal. But it won't be persistent, so the next time you run a terminal, it won't work. Both will work on your profile, but they won't take effect until you start a new shell.
I just typed it directly in bash. 
I got ./backup2.sh: line 2: MdSrc: command not found I'm still pretty new to this, so is a tic mark this key? ` And should I replace all your single quotes with it? I did make a few changes to it because my path has spaces in it, so I have to use "/path/to the/file" instead of /path/to the/file I was sort of anonymizing my own script for the original post for privacy reasons, but I doubt anyone could really harm me with it so I'll just post the full unaltered script that I have with your modifications below. #!/bin/bash cd "/home/eggheaddash/Firefox Backup/sessionstorebackups/" MdSrc = 'md5sum /home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf' MdTgt = `md5sum "/home/eggheaddash/Firefox Backup/sessionstorebackups/*.rdf"` If [[ ${MdSrc} -ne ${MdTgt} ]] cp "/home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf" "/home/eggheaddash/Firefox Backup/sessionstorebackups/$(date +%Y-%m-%d-%H-%M-%S-)session.rdf" (ls -t |head -n 5;ls)|sort|uniq -u|sed -e 's,.*,"&amp;",g'|xargs rm
You want to prevent the copy if the digest matches *any* of the existing files, or any that are the same underlying filename? Otherwise, if foo happens to match an old bar, you won't get a backup of foo, but just have to magically know what version of bar it matched. And what about if foo changes to match an older version at some point, so foo-{1..5} are sitting there and foo now matches foo-3, do you not want foo-6? Only foo-{1..3} are going to be removed when {6..8} get created and then you won't have the version that was between 5 and 6 any more; it was in 3 but that's gone. So do you just want to prevent the copy if the digest matches that of the last backup of the same base filename? Another option is to always do the copy, and then occasionally run something that hard-links identical files in the backup directory. Your dance with ls, head, another ls, sort, and uniq, can be `ls -t | sed 1,5d`.
 $ # Test data. $ grep ^ */* dest/1:a dest/2:b dest/3:c dest/4:d src/5:e $ $ # Digests without filenames. $ digest() { md5sum "$@" | sed 's/ .*//'; } $ $ # Spot a unique src digest. $ comm &lt;(digest src/5) &lt;(digest dest/* | sort) 2cd6ee2c70b0bde53fbe6cac3c8b8bb1 3b5d5c3712955042212316173ccf37be 60b725f10c9c85c70d97880dfe8191b3 9ffbf43126e33be52cd2bf7e01d627f9 e29311f6f1bf1af907f9ef9f44b8328b $ $ # We only want column one. $ comm -23 &lt;(digest src/5) &lt;(digest dest/* | sort) 9ffbf43126e33be52cd2bf7e01d627f9 $ $ # Have grep spot column one isn't empty. $ if comm -23 &lt;(digest src/5) &lt;(digest dest/* | sort) | grep -q ^; then &gt; cp -v src/5 dest &gt; fi `src/5' -&gt; `dest/5' $ $ # Second time, nothing happens. $ if comm -23 &lt;(digest src/5) &lt;(digest dest/* | sort) | grep -q ^; then &gt; cp -v src/5 dest &gt; fi $
I know that this sub is about helping with bash scripting, but it may be useful to mention that the functionality you're looking for already exists in `rsync`, which is a much better tool for backups, anyway.
Start a [bitbucket](https://bitbucket.org/) or [GitHub](https://github.com/) repository so you can organize your files, share them and get feedback on them. Both sites are also killer places to look for interesting projects. Look through this subreddit, and places like stack overflow. Some ideas: **Stack overflow** http://stackoverflow.com/questions/tagged/bash **Github** https://github.com/mathiasbynens/dotfiles/blob/master/.osx His project site one level up has some good config stuff too: https://github.com/mathiasbynens/dotfiles Also other stuff like: https://github.com/alebcay/awesome-shell Just look around, you'll find stuff that strikes you. When I first started I actually turned my laptop into a Ubuntu laptop so I had to use Linux. You can't get around unless you use the Shell a lot. Also READ. Read stuff. I read Sams UNIX in 10 minutes, as it's small and concise. If you want to get more deep, John Musters UNIX made easy. Two things that will make all the difference in the world: 1) Learn [special characters](http://www.tldp.org/LDP/abs/html/special-chars.html) 2) Learn [regular expressions](http://www.tldp.org/LDP/abs/html/x17129.html) Have fun! EDIT: @ahandle had a suggestion of going through all the bash builtins. That's a GREAT suggestion. Knowing all [the commands](http://ss64.com/bash/) that are involved are important, as manipulating input and output tot chain commands together is really when it starts to hit you how powerful the shell really is. That list won't reflect your list of builtins exactly. Mac has a different set, Posix another, so take it with a grain of salt, but if you man bash then you will get a list of all your builtins. I actually went through each one to see what it did, and wrote down a definition to absorb it more. EDIT2: Also apropos is your best friend. That will produce a list of commands based on the words you give it. I.E. :~$ apropos copy bcopy (3) - copy byte sequence copysign (3) - copy sign of a number copysignf (3) - copy sign of a number copysignl (3) - copy sign of a number cp (1) - copy files and directories cpgr (8) - copy with locking the given file to the password or group file cpio (1) - copy files to and from archives cppw (8) - copy with locking the given file to the password or group file dd (1) - convert and copy a file debconf-copydb (1) - copy a debconf database getutmp (3) - copy utmp structure to utmpx, and vice versa ... 
awesome will follow all advice and if anybody else has ideas keep it flowing 
I find both to work just fine for me.
Don't forget Ctrl-r (reverse-i-search), I don't event understand how I used the command line before I learned about Ctrl-r.
:) The animated gif in that article of Scott Hanselman is hilarious, isn't it? :)
Incorporating suggestions from you and others in this thread, here is the current v #!/bin/bash cd "/home/eggheaddash/Firefox Backup/sessionstorebackups/" MdSrc='md5sum /home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf' MdTgt="md5sum 'home/eggheaddash/Firefox Backup/sessionstorebackups/*.rdf" If [[ ${MdSrc} -ne ${MdTgt} ]] cp "/home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf" "/home/eggheaddash/Firefox Backup/sessionstorebackups/$(date +%Y-%m-%d-%H-%M-%S-)session.rdf" ls -t | sed 1,5d Lines 1-4 complete without error, but I get ./backupreddit2.sh: line 5: If: command not found I'm on Linux Mint if for some reason there's some package they don't include that I need.
its just a text file? why would ram matter?
You didn't even do what I suggested... This line of code MdSrc='md5sum /home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf' stores the string "md5sum /home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf" in `MdSrc`. I'm certain that's not what you want to be doing. *** Things are case-sensitive. There is an `if` command. The correct syntax to use it is: if ...; then ... fi *** `-ne` is the not-equal operator for integers. You should only use it if the things you're comparing are integers. In this case, they are not, so you want `!=` instead. But `md5sum "home/eggheaddash/Firefox Backup/sessionstorebackups/*.rdf"` is going to produce multiple lines of output (one per file), so that will *never* be equal to the output of `md5sum /home/eggheaddash/.mozilla/firefox/u3o3lncg.default/session.rdf`. So instead of just checking for non-equality, you're going to have to do something a bit fancier like using `grep` to see whether the hash of the file you want to back up appears in the output of that first `md5sum` command. *** At this point I'm thinking that what your very-broken script *looks* like it's supposed to do is not actually what you *think* it's supposed to do. You should step back for a moment and write out very precisely what you want the script to do. And then start over writing it from scratch, making sure you understand what each piece does. And then ask for help with the specific pieces that you can't figure out.
&gt; mv: cannot stat ‘HM1-2015-08-12\r.png’: No such file or directory Your input file has "windows line-endings". See http://mywiki.wooledge.org/BashFAQ/052 You can use `read` with a CR included in IFS to trim it off: #!/usr/bin/env bash { read -r; IFS=$' \t\r' read -r line; } &lt; g.txt b=HM1-$line.png c=HM2-$line.png convert -density 1000 HeatMap1.pdf "$b" &amp;&amp; mv "$b" HM1/ convert -density 1000 HeatMap2.pdf "$c" &amp;&amp; mv "$c" HM2/ 
You could also use `dos2unix`: a="$(dos2unix &lt; g.txt | head -2 | tail -1)" # or a="$(dos2unix &lt; g.txt | sed -n 2p)"
I learned something today. Thank you. 
I'll give this a try later. Good to have some options. Thanks!
More of a general warning, don't put random code into your terminal if you don't know exactly what it does. 
You'll want `sar` running regularly, via cron. With its output you can see if core is filling and so on. I'd also write something that grabs the process table and writes it to a file... Use cron again and something as simple as `ps wwwwaux &gt; ~/$(date +%y%m%d%h%m%s)-ps` would do. I'd re-write the above to let cron handle the restarts too. I suspect that one or more of what you're trying to kill isn't dying off nicely and so had lost of core, or you've multiple instances of something running. in either case I *think* the above will help you track it down.
This may be a lower-maintenance implementation of your questionable idea. #!/bin/bash myproc() { echo '# kill stuff here' for i in {9..0}; do echo "${i}" sleep 1 done echo '# launch stuff here' for i in {6..1}; do echo $(( 10 * i )) sleep 10 done } while : ; do myproc done 
 if [ "${a_Args[$check]}" \&gt; 999999 ] You don't want `&gt;` here, you want `-gt`. `&gt;` does string ordering, so `6 &gt; 50` is true because `"5"` sorts before `"6"`. `-gt`, on the other hand, is an arithmetic operator. I think that if you use `-gt` and one of the elements is not an integer, the test will fail and bash will print an error. The various operators are listed in the bash man page under CONDITIONAL EXPRESSIONS.
Thanks. Any particular reason not to use expr ? I'm working off an old Unix text book for most of my examples and problem sets and that is the only method it shows. 
&gt; Instead of reading this old UNIX book to learn bash scripting Don't have much of a choice with that one. It's the university's text for the course. I will definitely check the links out though. Thanks. 
No. Because of the double quotes, the `$i` is expanded during the assignment of the test variable. Apparently, `i` has the value 10 at that point, so you end up evaling: for (( i=0 ; i&lt;10 ; i++ )); do echo 10; sleep 1; done
Try this on for size #!/bin/bash #Declare variables procs_to_kill="Gw2.exe q4wine q4wine-helper" doKillProcess() { #Kill processes one at a time kill_pid="" for proc_to_kill in $procs_to_kill; do kill_pid=$(ps aux | grep -i "$proc_to_kill" | grep -v "grep" | awk {'print $2'}) if [ $kill_pid ]; then kill -9 $kill_pid; fi echo "Killing $proc_to_kill" sleep 0.5 done } doCountDown() { #Begin countdown echo "Gw2.exe ${message[0]}" for (( i=$count_time; i&gt;0; i-- )); do echo -n "$i seconds before ${message[1]}" sleep 1 echo -ne "\033[2K\r" done } doRestart() { echo "Gw2.exe relaunching"; echo /usr/bin/q4wine-cli -p "GuildWars2" -i "Gw2" } while true; do doKillProcess message=("killed" "restart") count_time="10" doCountDown doRestart message=("restarted" "killing Gw2.exe") count_time="100" doCountDown done Edit: whoever downvoted me, I'm sure I did something that's not best practice. I'd like feedback. But the script does work.
Sie probably ran for (( i=0; i&lt;10; i++ )); do echo $i; sleep 1; done before trying the eval bit. In which case `i` will have a value of 10.
Which is sort of an interesting point by itself. I feel like we've all learned something today about the order that bash evaluates stuff. 
Apart from the `"$i"` blooper, there's a tolerable (is it because it's old?) [StackOverflow discussion about the difference between `$cmd` and `eval $cmd`.](http://stackoverflow.com/questions/4668640/how-to-execute-command-stored-in-a-variable)
I noticed something really weird when trying cmd="ls -l"; $cmd vs eval "$cmd"... $cmd doesn't generate colored results based on my shell preferences and eval "$cmd" does... NO IDEA
Sorry, no. Thanks for you help, though-- hadn't tried that simplicity.
This is a very common pitfall beginners fall into. **Failing to quote.** --- # WRONG echo $var This tells bash to expand the variable's content, then remove all whitespace (spaces, tabs and newlines, or whatever happens to be in the special `IFS` variable), split it into words, then if any of the words happen to contain glob characters (`*`, `?`, `[...]`), those words are replaced by matching filenames, if any. Lastly, echo prints each of these words separated by spaces, which is why you only get one line. The same applies to command substitutions (`$(cmd)` and `` `cmd` ``). --- # RIGHT echo "$var" # Even better: printf '%s\n' "$var" This expands the variable's content, then echo (or printf) prints it. --- I recommend reading the [Bash Guide](http://mywiki.wooledge.org/BashGuide) to learn the basics of bash. 
How about this: #!/bin/bash echo -e "This is a message\nthat\ncontains\n\nseveral\nlines" 
Ok, so there are two main ways to approach argument handling and that's positional parameters and getopts. There are other ways like getopt (note: no s) but they're rarer examples. With positional parameters, let's take an example like: ./somescript.sh pirate 5000 "pirate" and "5000" are treated as positional parameters, so in the script "pirate" will be $1, "5000" will be $2 and so on. These are basically treated as an array, so you can also use array variables like $* or $@ to manage your parameters. Positional parameters are great for simple scripts like, for example, a script that prints out the 5 biggest files in a path e.g. ./filereport /some/path/to/audit So meeting your first requirement is easy: # Check that we have parameters if [[ $# -eq 0 ]]; then printf "%s\n" "Usage: ${0##*/} [first parameter (required)] [second parameter (optional)]" exit 2 fi This uses the builtin variable $# which provides a count of how many positional parameters are in use. ${0##*/} is a fancy pants way of printing out the script name, you could use `basename` instead. The problem with positional parameters is that as your script gets more complex, the harder the parameters are to control and process. At some point it pays to move up to getopts. Here's an example: # Getopts while getopts ":ahp:S:" Flags; do case "${Flags}" in a) aVar=True ;; h) printf "%s\n" "scriptname - purpose" \ "" "Required arguments:" \ "-p [description of what -p does]" \ "-S [description of what -S does]"\ "" "Optional arguments:" \ "-a [description of what -a does]" \ "-h [Help/Usage]" exit 0 ;; p) pVar="${OPTARG}" ;; S) sVar="${OPTARG}" ;; \?) echo "ERROR: Invalid option: $OPTARG. Try './scriptname -h' for usage." &gt;&amp;2 exit 1;; :) echo "Option '-$OPTARG' requires an argument, e.g. '-$OPTARG 5'." &gt;&amp;2 exit 1;; esac done So you can do either processing per arg directly in there, or you can set a variable and handle it later (as in aVar). We can see that -p and -S require some value assigned to them e.g. `-p 500 -S someserver`. This is set in the while line by putting a colon after them: ":ahp:S:". In this example of getopts I have the help text in getopts, it's better practice IMHO to have that separately as a function to call - so both -h and \? call e.g. Function_Help. Now, you can check for no args with getopts using OPTIND but it's simpler to just use the positional parameter check I posted before. So, ultimately you could do something like * the positional parameter check * getopts, assigning variables to your arg values * a simple check during/after getopts to check the variables That check can come full circle back to handling it as an array as you've asked, or it can be targeted to a specific arg e.g. if [[ "${pVar}" -gt 99999 ]]; then printf "%s\n" "ERROR: -p can not be greater than 99999" exit 1 fi You could even have it as a function and pass each OPTARG through it, within getopts itself. Sorry this is a bit much to absorb, but it may give you some food for thought and some launching points for googling.
You've got an actual interesting question for StackOverflow or SuperUser.
Thanks, that is excellent information. I looked up the getopts and found a few descriptions of it, but they were a little high level and I had a hard time getting it. 
He's not using () like $() in his demo code. He's using it to read the output of `find` into an array. Still needs plenty of quoting, and he should be using $() instead of back ticks, but like you said, most of that can be avoided if you just pipe it to a while loop.
&gt; "newline" is a function that I tried to just insert a newline, but it did not work... it just adds an empty space to the string. How did you implement `newline`? It adds empty space to what string? The output from all the echo commands is being appended to a file. Does `newline` also append things to this file? Or does it just output to the console? &gt; What I get when I output the data is a SINGLE LINE with all of the information on it. I'm not sure what "the data" is. Is it the contents of `$fullfilepath/tilinginfo.dat`? How are you outputting it?
Weird characters meaning spaces, new lines, escape characters. Escape characters in particular aren't common, but they do happen from time to time.
I don't like my explaination will be as "great " as /u/geirha's, but this is how I saw it. You were evaluation `i` in the variable `test`, basically trying to make `test` function as a, well. So when you ran `test`, the computer is evaluating the output of your `for loop` and storing that value in `test`. So when you ran `test`, you get what `test` returned. Edit: Today I learned how to format comments. Hooray! I think.
Does `http://camera.url/` give a listing of the files? Otherwise, if you know it's roughly twenty minutes between them, then find one, and then move twenty minutes ahead, and work outwards from there, both directions, until you find another.
there's a few comments already but I'll add some tips: files_check=(`find $FILEFOLDER -type f -mtime +"$Days" -print`) you don't need the parenthesis there, you can just `files_check=\`find..\`` or preferably: `files_check=$(find..)` if ( [ "${f_c}" == $(`lsof | grep "$f_c"`) ] ) you don't need the parenthesis here either, `if` evaluates a command (and the parenthesis create a subshell in this context but it doesn't affect the result). this seems like a problem: `$(\`lsof | grep "$f_c"\`)` because the inner backticks will run the `lsof` command and replace themselves for its output but then the outer `$()` will take that output and try to execute it again, is this what you meant? you should also use `[[` instead of `[` (assuming you're doing bash and not POSIX). finally `exit -1` is a mistake because error codes go from 0 to 255. the rest is good, keep it up!
Does not work inside a function.
How does it not work, exactly? despite some lacking quotes, that looks like it is working to me.
 $ myfunc() { printf 'multiple\nlines\noutput from a function\n'; } $ myfunc multiple lines output from a function 
Turns out that this DOES work. Now I feel a little stupid. Notepad++ was set to ignore the CR/linefeeds like Notepad for windows does. Output from 'cat' confirms your solution. And probably every other solution. Man, I swear I'm not a n00b. Probably any one of the dozen or dozens of solutions I tried would have worked. Thanks for all of your help.
Not bash, iOS script
Your curl command is outputting two lines, which you're then piping into awk, which processes it for each line. root@drop1:~# curl --silent "http://xml.weather.yahoo.com/forecastrss?p=USNY0200&amp;u=f" | grep -E '(Current Conditions:|F&lt;BR)' | sed -e 's/Current Conditions://' -e 's/&lt;br \/&gt;//' -e 's/&lt;b&gt;//' -e 's/&lt;\/b&gt;//' -e 's/&lt;BR \/&gt;//' -e 's///' -e 's/&lt;\/description&gt;//' + sed -e 's/Current Conditions://' -e 's/&lt;br \/&gt;//' -e 's/&lt;b&gt;//' -e 's/&lt;\/b&gt;//' -e 's/&lt;BR \/&gt;//' -e s/// -e 's/&lt;\/description&gt;//' + grep -E '(Current Conditions:|F&lt;BR)' + curl --silent 'http://xml.weather.yahoo.com/forecastrss?p=USNY0200&amp;u=f' Fair, 73 F If you make your awk line awk 'NF {print $1" With a temperature of " $2 " degrees fair in height"}' I believe that should fix it.
You might want to work less on your prompt and more on doing productive stuff. 
This should move the first 50 files into dir001, next 50 into dir002, etc... files=(./*.wav) n=${#files[@]} step=50 for (( i = 0; i &lt; n; i += step )); do printf -v dirname 'dir%03d' "$(( 1 + i/step ))" mkdir -p "$dirname" &amp;&amp; mv "${files[@]:i:step}" "$dirname" done See also http://mywiki.wooledge.org/BashFAQ/095
That's exactly what I want! Thank you very much. I think you saved me a lot of work.
Might wanna check your locale and make sure you have utf8 selected. locale -a
This is definitely productive, I checked.
Unfortunately, `FIGNORE` matches suffixes, which means *any* file that ends in one of those. So basically, there's no way to get what you want. I've generally found the `FIGNORE` variable to be pretty useless for my own purposes because of this limitation. Sorry.
grep (GNU grep) 2.16 xubuntu Thanx for the reply, same result however, 2090 characters are copied.
I'm too stupid... The paragraph had a line break that i didn't see, and i used this very passage again and again... When copying the file to this account i tried some other word to provide an example, and of course it worked. I'm sorry i wasted your time, and thanks again for your contributions.
Both inside and out of screen its: screen-256color, its set in my .bashrc
LANG=en_GB.utf-8 LC_CTYPE="en_GB.utf-8" LC_NUMERIC="en_GB.utf-8" LC_TIME="en_GB.utf-8" LC_COLLATE="en_GB.utf-8" LC_MONETARY="en_GB.utf-8" LC_MESSAGES="en_GB.utf-8" LC_PAPER="en_GB.utf-8" LC_NAME="en_GB.utf-8" LC_ADDRESS="en_GB.utf-8" LC_TELEPHONE="en_GB.utf-8" LC_MEASUREMENT="en_GB.utf-8" LC_IDENTIFICATION="en_GB.utf-8" LC_ALL=en_GB.utf-8
All above have pointed out most things. Just a small IF statement information... That particular error occurs frequently when using single square brackets to enclose your IF condition in bash. IF conditionals in bash are best enclosed in double square brackets. ... if [[ $a != $b ]]; then ... Using single brackets requires you quote your variables. ... if [ "$a" != "$b" ]; then ... Also your curly braces aren't strictly needed for your variables. So all that said... Here's your snippet. for f_c in ${files_check[@]} do if [[ $f_c == $(lsof | grep $f_c) ]]; then Hope this helps in the future!
You can use 'date' to generate the timestamp. The manpage has the info on formatting it the way you want for your OS (BSD may need 'gdate instead). If I were you I would make an alias, let's take xfer, in .bashrc. alias xfer='TIMESTAMP'='date "+%m-%d-%Y-%H-%M-%S"' &amp;&amp; 'scp $1 me@server:backup/${1%%.*}-${TIMESTAMP}.bib' then, you simply go 'xfer myfile.bib' and the transfer should start. Been out of linux for a while but I'm sure someone will take the mistakes out or come up with something better ;) edit: added /u/spizzike's date formatting code. Thanks, did not have a gnu version of date on hand :)
specifically, the date formatting arguments would be called like: date "+%m-%d-%Y-%H-%M-%S" per OP's requirement.
worked. thanks!!
FWIW, if you timestamp "YY-MM-DD" you get the benefit of lexicographical sort resulting in chronological sort too. US timestamps are evil for sorting. Yes, I know you can do all manner of jiggery-pokery to achieve chronological sort using US timestamps (and I've lived through that hell), but why bother?
&gt; alias xfer='TIMESTAMP'='date "+%m-%d-%Y-%H-%M-%S"' &amp;&amp; 'scp $1 me@server:backup/${1%%.*}-${TIMESTAMP}.bib' This is completely wrong in several ways. It won't work. You can't really use an array for this at all. *For almost every purpose, aliases are superseded by shell functions.* So it should rather be something like: xfer() { scp "$1" "me@server:backup/${1%.*}-$(date +%Y-%m-%d_%H:%M:%S).${1##*.}"; } I recommend against putting month or day first in the date. Put most significant number first. Year, month, day... That way sorting them lexicographically will also sort by date. See [BashFAQ 100](http://mywiki.wooledge.org/BashFAQ/100) for more on doing string manipulation in bash.
Thanks. Never tried it. Will now though. Thanks for saving my fingers.
So your file contains only names, not paths? You could do something like the following: find . -name '*wav' -print0 | grep -zwvf &lt;textfile&gt; | xargs -0 rm -f Though you would probably want to print the list and inspect it before doing the `rm`
So here's the whole explanation of what I was shooting for there: `find` starts in the current directory "." and finds every file that ends in "wav", which it will print as a null terminated string That stream of names will be piped to grep which uses `-z` to know they're null terminated, `-w` to match whole words (in case some filenames were substrings of other ones) `-v` to reverse the match, so we'll only print things that do NOT match anything in our list and `-f` to tell grep to get the list of patterns from a file, which is your list of names (one per line I'm assuming) `grep` will print all the names that are not matched in the file, and they are still null terminated, so we'll use the `-0` flag to `xargs` to make it handle those as well, and have it invoke `rm -f` to forcibly delete each of the names that comes out of `grep`
I like that array idea! You could make it faster by using `-delete` instead of the `-exec`, and they could just leave that off to get the list until they're satisfied. Of course, if the name is too long the command line could become too long. Also, make sure to quote the `{}` if you're not going to use `-delete`
Scroll up?
Are you in a screen? If so here's how to scroll up: * Hit your screen prefix combination (C-a, control+A by default), then hit 'Escape'. * Use the arrow keys to move up/down as needed. * When you're done, hit the 'Return' key twice, you will be back to the end of the scroll buffer.
Probably a mv would be safer to text with and then just delete the entire directory after the move.
Tmux equivalent: * Ctrl+B, PageUp * Scroll with PageUp/PageDn, arrows or hjkl * Return to normal with `q`
I came here to recommend exactly this. Use GNU Screen (or Tmux). You can even issue commands to your current screen via the `screen` command in a shell script, so you could create a script that calls a command like: #!/bin/bash # This is the "fuck" command: screen -X clear #!/bin/bash # This is the "unfuck" command: echo '(Ctrl-B to scroll up, Q to return to command line)' screen -X copy 
Storytime? I'm going to guess you had something on your screen that you wish you hadn't (password typed in the clear, maybe?) and so created your alias.
that gives you commands (input), but not their output. Only half of what gets "cleared"
no, just try different commands and they dont work out... if they fail a few times i get pissed off 
[This](http://stackoverflow.com/questions/1371261/get-current-directory-name-without-full-path-in-bash-script) should help you.
Instead of clear you can do `echo -ne '\e[?1049h\e[1;1H'` (a.k.a. ncurses's altscreen &amp; go to top left) and then you can unclear with `echo -ne '\e[?1049l'`
I just tested that in Linux Mint 17.2 and in Windows 10 Git Bash and it worked fine.
Oh! I think I found the issue! This seems to work: #!/usr/bin/env bash exec {FD}&gt;&amp;1 echo hello &gt;&amp;$FD So the issue is in the interpreter. The system bash in `/bin/bash` seems not to support requesting an open fd, whereas the bash interpreter in PATH does.
That's right. I have a new version of bash from homebrew which I use, but didn't realize that feature is not there in bash 3.2. I wonder if there's a way to get an open file descriptor in bash 3.2. I guess it's not a big deal, because it seems that opening an fd in one script and then opening the same fd in another script the first script calls, works in a nested fasion. so when the subprocess dies the parent process gets its fd attached to what it had attached originally.
&gt; I wonder if there's a way to get an open file descriptor in bash 3.2. In bash 4.0 and older, you must specify the number explicitly, e.g. `exec 9&gt;&amp;1`. &gt; I guess it's not a big deal, because it seems that opening an fd in one script and then opening the same fd in another script the first script calls, works in a nested fasion. so when the subprocess dies the parent process gets its fd attached to what it had attached originally. Not sure I follow. File descriptors are inherited (from parent to child), but a file descriptor opened in a child will not be transfered to the parent when the child process dies.
Was it [this](http://brettterpstra.com/projects/where/)? I dont know..
I mean, with parent: exec 3&gt;&amp;1 bash child echo knock knock &gt;&amp;3 and child: exec 3&gt;log echo friendly &gt;&amp;3 Here, if I run `bash parent`, I see `knock knock` on stdout and `friendly` in the `log` file. In other words, redirecting fd 3 to stdout, then redirecting it to a file, writing to &amp;3, dying, and then writing to &amp;3 one more time, writes to stdout in that second write. It's as if redirecting is nestable. It behaves like a stack: { int a = 0; { int a = 1; } // assert(a == 0) } this is different from, say, assignment: { int a = 0; { a = 1; } // assert(a == 1) } So it seems redirection is not like assignment but more like a stack scope, IIUC.
Yeah. When the child process inherits the parent's file descriptors, that means it gets file descriptors that point to the same things as the parent's file descriptors. So, it has its own names `1`, `2`, etc., but they point at the same things that the parent's `1`, `2`, etc. point at. When you do `3&gt;log` in the child, you point the child's `3` at `log`. But that does not change what the parent's `3` is pointing at.
cools, that works for what i need, thanks... i dont really understand it though. but thanks alot i implemented it as such, for anyone that ever reads this and wants to copy. (in .bashrc) alias fuck="echo -ne '\e[?1049h\e[1;1H'" alias unfuck="echo -ne '\e[?1049l'" 
&gt; I was hoping some one withh more experiance with bash scripting could help me out please? &gt; I am putting together a script that uses urlcrazy to parse a list of domains to find any miss-spellings that could be being squatted or used for nefarious deeds. &gt; As part of that process I am using wkhtmltopddf to go and get a screenshot of all the domains that return A records. &gt; Now everything is working esxcept for when wkhtmltopdf tries to write the pdf file result, I get an error stating: &gt; Exit with code 1, due to unknown error. &gt; I think the issue is to do with the location where the file is to be output. my code for the screenshot is: &gt; for LINE in `cat /tmp/regdom.txt`; do echo "Taking screenshot of "$LINE; wkhtmltopdf -l -q $LINE results/$FOLDER/screenshots/$LINE.pdf; done 1. [Don't read lines with for](http://mywiki.wooledge.org/DontReadLinesWithFor). 1. [Quote](http://mywiki.wooledge.org/Quotes) variable expansions that are used as arguments 1. Don't use uppercase variable names as you risk overriding special shell variables and environment variables. &gt; The $FOLDER variable is assigned at the start and is as follows: &gt; FOLDER=$(date +"%m_%d_%Y"); &gt; mkdir results/$FOLDER; That's a horrible date format. Put the most significant numbers first, that way it sorts correctly. &gt; Now everywhere else in the script is fine with me using &gt; results/$FOLDER/thing.txt &gt; when telling it to where to save the output to except wkhtmltopdf. If I remove the $FOLDER variable it works. &gt; So my question is how to I get mkhtmltopdf to save the pdf output to the results/$FOLDER/screenshots folder and have it name the file the same as the name of the site it captured? You created `results/$FOLDER/`, but not `results/$FOLDER/screenshots/`. &gt; Thanks for your help. Fixed version: #!/usr/bin/env bash dir=results/$(date +%Y-%m-%d)/screenshots mkdir -p "$dir" || exit while read -r domain; do printf 'Taking screenshot of %s\n' "$domain" wkhtmltopdf -l -q "$domain" "$dir/$domain.pdf" done &lt; regdom.txt 
&gt; date +%Y-%m-%d GNU extension: A shortcut for that is date -I or, more verbosely, date -Idate date --iso-8601=date There’s also `-Ihours`, `-Iminutes`, `-Iseconds`, and `-Ins`.
Actually it doesn't work if you do it multiple times. Like if you have screen A, then you `fuck` and go to screen B, you `fuck` again and go to screen C, but because B and C are both an altscreen an `unfuck` would just take you to screen A.
&gt; 32 if [ "$(grep -q -e "$file" "$_temp")" == "$file" ] ; 33 then 34 echo "File is in use, quitting the process" 35 exit 1 36 fi Drop the `[` and `$()`. You want to test grep's return value, not its output. if grep -qFe "$file" "$_temp"; then printf 'File was recently in use, and possibly still in use, but bailing out to be safe\n' exit 1 fi &gt; 19 function clean_up { 20 21 rm "$_temp" 22 exit; 23 } 24 25 trap clean_up SIGHUP SIGINT SIGQUIT SIGTERM You don't have to trap a bunch of signals in order to clean up on exit, just use an EXIT trap: clean_up() { rm -f "$_temp" } trap 'clean_up' EXIT
Thank you.. The only reason to TRAP the signals is lets say if user interrupt the script by pressing Ctrl+c Ctrl+d The script can detect those signal and remove the _temp file from the server rather than leaving it behind....as if the directory is having so many files to go through with. lsof loop then it will take time to reach until the end of it. 
The EXIT trap will also trigger when you interrupt or kill it.
Look at this hexdump to see the escape codes cal uses: `cal --color=always | hexdump -C` So you get this: `cal --color=always | sed -n 's/.*\x1b\[7m\([^\x1b]*\).*/\1/p'`
 rsync -a user@remote:/some/dir /local/dir
Starts the process in the background so you can continue to use the bash prompt.
So, are you actually putting the '$' in your command? I used to not understand this. When you see bash commands and the first thing is $, it means they're running the command as a regular user. When you see it start with '\#', it means they're running it as a superuser.
&gt; mountpoint -q /mnt/nas1 if [ "$?" != "0" ]; then echo &gt;&amp;2 "It looks like the nfs share is not working; giving up." exit 1 fi You are checking the exit code of `[` to see if the exit code of `mountpoint` was not 0. That's a pointless operation. Test the command directly. if ! mountpoint -q /mnt/nas1; then printf &gt;&amp;2 'It looks like the nfs share is not working; giving up.\n' exit 1 fi Another point here is that you might as well get used to using printf rather than echo. printf is superior to echo, so for consistency, you might as well always use printf. --- &gt; if mkdir "$lockdir"; then Here you test the command directly (which is good), why didn't you do that for the first test? --- &gt; if [ $(id -u) != "0" ]; then echo "You must be the superuser to run this script" &gt;&amp;2 exit 1 fi Avoid using `[` in bash (only sh). Use `[[ ... ]]` to test strings and files, and `((...))` to test numbers. Also, bash already has the uid of the user in the special shell variable `UID`, so you don't have to run an external command like `id`. if (( UID != 0 )); then ... --- &gt; /usr/local/do-cool-stuff-here.sh That's an unconventional location for an sh script.
Checking for superuser is most likely cheaper than trying to create a directory, and it’s also good to know for a reader of the script that the script requires superuser rights, so I’d move that check to the beginning of the script.
The top `echo` line has `$` instead of `&amp;`. There are no quotes around `$*`. There is no `;` after your test (`[ $# != 3]`). I like to use double-parentheses when evaluating math expressions. Your method works, but would fail for something like `[[ 22 &lt; 3 ]]` because it's comparing strings. Notice I used `[[ ]]` instead of `[ ]` also. Single-brackets are more compatible with other shells, but double-brackets are more useful when working with `bash`. `[ 22 &lt; 3 ]` is not even valid code (`[ 22 -lt 3 ]` is, and would return the correct result). [More on comparison ops](http://www.tldp.org/LDP/abs/html/comparison-ops.html), I didn't even know `[ 22 \&lt; 3 ]` was a thing. Cleaned up a bit: #!/bin/bash echo "$*" 1&gt;&amp;2 if (( $# != 3 )); then echo error 1&gt;&amp;2 exit else echo "$*" 2&gt;&amp;1 fi Look up '[shellcheck](https://github.com/koalaman/shellcheck)'. It will 'lint' your shell code and show warnings/errors. It found all of these mistakes right away. There are plugins for Atom, Sublime Text, and I'm sure other editors/ides. When I save a script in Sublime Text `shellcheck` will point out my mistakes for me. With Atom, it will show real-time notifications about these things.
Uhm. "Be prepared for spaces in filenames", and then it goes on to recommend `for file in $(find...)`.
You don't want to use grep, right? Then the array[@] construct is probably the best. `array=(); while read -r; do array+=( \! -path "$REPLY" ); done &lt; no.txt` `find . -mindepth 1 -maxdepth 1 -type d "${array[@]}"`
Can you expand on the `( \! -path "$REPLY" )` part? I don't understand how that generates a mutually exclusive list from the folders specified in no.txt?
Oh, I didn't know that $REPLY is the default variable in case one is not specified for the while loop. TIL, thanks. That is very interesting :) Also, what's the role of `-r`?
I hate to be that guy... but... ```man sed``` This seems like a homework assignment, and as such, I'm not really comfortable giving an answer. 1. Write a regex for each problem 2. Use sed with option(s) to do what you want (replace, remove, etc) Is there something specific that you're having trouble with?
Have you though about, you know, actually doing the work(yourself) and working stuff out until it works perfectly so you don't have to hope to barely pass?
&gt;Would it be too slow? Yes. Even native window managers are already too slow, even for me who can't even really notice choppiness unless it's less than 15 fps.
are you using this instead of a DE or WM? How is Sxhkd different from Xbindkeys
thanks, that was brain opening. I will stick with DWM since I have it very usable at the moment.
I think the best solution would be to leave the script itself alone and instead cause it to be called periodically. The guide you linked to recommends to do this with Conky: ${execi 240 ~/bin/changer.sh} Perhaps you could also do it with a cronjob. For instance, to change your wallpaper every 10 minutes, add 0,10,20,30,40,50 * * * * your_user path/to/changer.sh (Conky probably works better, haven’t tried either.)
Thanks, I applied the changes and removed the exstension.
I created a dir called bin inside my home so that anything put there is automatically added to PATH and then moved my script there. I've tried using crontab and installing a new crontab as well as editing the /etc/crontab manually. No luck on either of them. Am i missing something ? /etc/crontab: 10,20,25,30,40,50 * * * * "username" wpchanger 
cron has its own PATH which will not include ~/bin unless you specifically add it in the crontab. Also, don't quote the username. Perhaps add it to your user's own crontab instead. E.g. by running `crontab -e` as your user, and adding: PATH = /home/username/bin:/usr/bin:/bin 10,20,25,30,40,50 * * * * wpchanger 
Just an FYI, to run a cron job every 10 minutes: */10 * * * * your_user /path/to/changer.sh 
alright now im making progress. I'm sorry for the slow updates but life unfolds around me and I can't seem to escape it. So I did as you said meaning i added the correct path to the correct crontab. While waiting for it to trigger i found something interesting that has helped me troubleshoot. /var/spool/mail/username Inside this file an automated message is stored everytime the cron job (doesn't?) run. the output was a bit different for everytime it ran. X-Cron-Env: &lt;PATH=/home/per/bin:/usr/bin:/bin&gt; X-Cron-Env: &lt;SHELL=/bin/sh&gt; X-Cron-Env: &lt;HOME=/home/per&gt; X-Cron-Env: &lt;LOGNAME=per&gt; Message-Id: &lt;E1ZeI1Z-0000Li-K0@Babe&gt; Date: Tue, 22 Sep 2015 09:30:01 +0200 /bin/sh: 1: per: not found user not found ? changed to root instead of per. still the same error. hmmm could it be the shell... as you said, it's a bash script and not a sh script. I added: SHELL=/bin/bash and that gave me: X-Cron-Env: &lt;SHELL=/bin/bash&gt; X-Cron-Env: &lt;PATH=/home/per/bin:/usr/bin:/bin&gt; X-Cron-Env: &lt;HOME=/home/per&gt; X-Cron-Env: &lt;LOGNAME=per&gt; Message-Id: &lt;E1ZeIKv-0000PU-E7@Babe&gt; Date: Tue, 22 Sep 2015 09:50:01 +0200 /bin/bash: root: command not found now i'm frustrated. could it mean that the root is where the command should be? I tried removing the username column and then just adding the path to script. I figured it would be cool since it's already inside the users specified crontab or am i wrong here ? */2 * * * * /home/per/bin/wpchanger that gave me this weird stuff: X-Cron-Env: &lt;SHELL=/bin/bash&gt; X-Cron-Env: &lt;PATH=/home/per/bin:/usr/bin:/bin&gt; X-Cron-Env: &lt;HOME=/home/per&gt; X-Cron-Env: &lt;LOGNAME=per&gt; Message-Id: &lt;E1ZeIWX-0000R8-Vc@Babe&gt; Date: Tue, 22 Sep 2015 10:02:01 +0200 (nitrogen:1681): Gtk-WARNING **: cannot open display: So now i think it's running but something else is not working properly. This just keep getting better and better :D
Thanks, It looks much neater. Have an upvote on me. 
It’s also simply misleading: if you’re using Bash features, it won’t work with any other `sh` (and probably not even with Bash invoked as `sh`), so if anyone tries to run the script as `sh changer.sh`, they’re in for a surprise…
Awesome dude!! 
Doing my usual hole-poking: `set -e` is a nasty crutch IMO. It's inconsistent in its behaviour and encourages sloppy code. It also makes it more awkward to check for different exit statuses in your commands. `set -u` is okay-ish but it doesn't always protect you from the kind of mistakes it's used to defend against, so it should be used as a debugging tool used to find problems in your script (which you should fix ASAP), not a security feature. The find/xargs point is valid but the author has neglected to mention that this is a non-standard GNU extension. The author also fails to mention "-exec", which *is* standard and is just as safe. The traps point is valid but notice how he fails to quote all variables, and even writes the trap in double-quotes, fully expanding `$lockfile` *as shell code* into a trap. Race conditions... why go hacking around with noclobber and such when you can just use `flock(1)`? I think this is absolutely the wrong way to go about it. He also has another race condition in the "fixed" script where if it is interrupted after the first `rm`, it will execute another `rm` -- even if that lock file was recreated by another script! If you use flock, locking and unlocking without these races is child's play. While he makes some good points, I think the author has a lot to learn about robust bash scripts before he lectures others on that topic. ---- As for atomicity, I wonder if you can copy the directory, `cd` a shell into the original directory, bind-mount the copy over the original, perform your `sed`s using the shell which is inside the original (non-mounted) directory, and then umount when you're done. It needs root access but I wonder if that would work and give you a real atomic switch-over of these directories. ---- &gt;You can check for files with files open by using `lsof` Fileception?
&gt; best to always use it in non-interactive settings I think even in interactive settings, backslash parsing may be a little unexpected for the user.
&gt; I hate to be that guy... but... `man sed` I also hate to be that guy… but `info sed`. That’s an actual manual that’s meant to teach you `sed`, not just a short summary. Seriously, the GNU programs have amazing info manuals. (Another great example: `info gawk`.)
Thanks for the advice!! Any particular why it would be best to use [[ ... ]] and (( ... )) versus [ ... ] ? Just looking for some good information :)
This looks pretty interesting. Nice post!
 $ type [ [[ [ is a shell builtin [[ is a shell keyword `[[` is a shell keyword, and more powerful than the `[` builtin; `[[` allows pattern matching and regular expression matching. The `((` syntax makes arithmetic comparisons prettier and easier. E.g. `while (( i++ &lt; n )); do ...; done` vs `while [ "$i" -lt "$n" ]; do i=$((i+1)); ...; done`. See [BashFAQ 31](http://mywiki.wooledge.org/BashFAQ/031).
good! 
That is so much simpler than fiddling with cron or conky. An infinite while loop that sleeps for 10m and then runs again. Why did I not think of this ! Will try this as soon as I have some free time. Thanks for the advice mate. Kudos!
While it's a nice library I beg you to never use something like this in production code. If you need OO support use a proper language (like Python, Ruby, Perl, whatever).
Yeah, definitely anything mission critical I would advise against using this library. There's totally a possibility of object pointer collisions -- but I suppose people also do this in databases with UUIDs. You can totally bump the length of the UUIDs though to reduce the risk of collisions. I am a little in disagreement that this shouldn't be used for ANYTHING in production, because sometimes Bash is the right language for the job and this library can offer a little more structure/maintainability to your code. I have run some pretty extensive tests against this library (There will also be a testing library module in the framework I'm working on, which is not fully baked yet) and am actually amazed that I've ran into no issues yet. But again, I wrote this for fun!
Wow super cool -- I was pretty confident something like this had to be done before. I'll dig into your source later, very interested into seeing how you went about it!
You can use case/esac. * is for everything else.
You can also shorten it with grep. getIds=`ps aux |grep $i | grep -vE '(grep|topView)' | awk '{ print $2 }'` 
Rexify.org Look at this. I'm not sure if you're just playing with some home machines or production. But invest a little time in the above. What you're doing will kinda work given you are using SSH key access and such, but, it's extremely hacky and rex will do what you need and more with very little learning curve. 
nope. work stuff and i'm not an admin
could always grab uptime first, then run reboot command, run a loop to check if the box uptime reset by continuously trying to ssh uptime untill it succeds or X intervals happens and if it doesnt respond log it.
I recommend this guide for Bash: https://github.com/progrium/bashstyle
Bonus question, if you're not admin, why the heck do you have reboot permissions?
What about dsh (dancer's shell) ? https://www.netfort.gr.jp/~dancer/software/dsh.html.en
It's not uncommon to let junior admins (lacking things like install permissions) access to things like shutdown/reboot. You need the on-call people to be able to at least power cycle machines. This often happens in lab environments where scripts update and sanitize workstations. 
You probably just need to use [pdsh](https://code.google.com/p/pdsh/) pdsh -Rssh -w server[001-100] 'uptime' Check out piping that output to [dshbak](http://linux.die.net/man/1/dshbak)
Look at Python with paramiko or fabric which is built off paramiko 
You can also specify the prompt directly: read -p "Do you really want to run '$selected_task'? " confirmation if [[ $confirmation == yes ]]; then # … fi
 if cmd; then something elif cmd2; then something else fi vs if cmd; then something else if cmd2; then something else fi fi Logic is the same. Using elif just makes it shorter and easier to read.
in case of 2 expressions it makes almost no difference but elifs guarantee that sibling conditions are mutually exclusive which improves readability a lot. Let's say you have mutually exclusive set of options ABCD (not a valid bash code below) if A ... elif B ... elif C ... elif D ... else ... using only if else you'd have to nest because if else allows you to process only 2 cases at a time. if A ... else if B ... else if C ... else if D ... else ... 
To be fair couldn't you use something like that ? if A //Code else if B //Code else if C //Code But i guess it would be a mess adding all the "fi"s at the end, so it's there for convenience, as I suspected, thanks!
Ah. I didn't watch the video. :P
* Do we really need a video for this? This trend of videos as opposed to an article / info site is getting really old. I don't need a video of everything, especially since you can't copy/paste from a video. This is not a lecture where some people would learn better by seeing than by reading. * There are already several really good sites with tips on history files [For example](http://www.shellhacks.com/en/7-Tips-Tuning-Command-Line-History-in-Bash) 
So you only record 25k commands and your bash history becomes unwieldy.
My guess is that IFS has been changed previously in the script, causing `` `seq 1 5` `` to not be split into words. Running the script with `bash -x` (or putting `set -x` inside the script) would show if this is what's happening. Relying on word splitting is a bad practice anyway, and `seq` is an external, non-standard command. Use a C-style for-loop instead. for (( i = 0; i &lt; 5; i++ )); do .... done 
&gt; * There are already several really good sites with tips on history files [For example](http://www.shellhacks.com/en/7-Tips-Tuning-Command-Line-History-in-Bash) I wouldn't call that site "good". The one you linked to exports the special shell variables for no good reason. Looking through other pages at that site does not improve my liking. The extract function in [this](http://www.shellhacks.com/en/HowTo-Extract-Archives-targzbz2rarzip7ztbz2tgzZ) for instance, fails to properly quote the arguments it passes to the various commands. And [this](http://www.shellhacks.com/en/HowTo-Count-Number-of-Files-in-a-Directory) recommends using ls and wc to count files. (See [FAQ 4](http://mywiki.wooledge.org/BashFAQ/004) for how to do that). It's rather around the average of what you find on the net. That is, mostly garbage.
I’d have suggested this instead: for i in {1..5}; do … done It’s shorter and perhaps more readable, and also seems to work regardless of IFS. On the other hand, the full C-style loop is more flexible, sure.
&gt; where you pipe data into the while loop, the while loop runs in a subshell A workaround for this, if you really do need to pipe into the loop, is to set the shell option `lastpipe` (`shopt -s lastpipe`); with `lastpipe` active, the last command in a pipeline is run in the current shell. However, in this case, 2) is preferrable IMO (with `&lt; file`).