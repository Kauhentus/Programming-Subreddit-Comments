Not sure why it didn't notify you, but thanks again for your help.
This works like a charm, thanks :) I have one unrelated question: I only need some "echos" for debugging purposes and not in the terminal ouput. If I only wanted to output these echos if i debug and not usually, how do i do this properly? is there something for debugging like stderr &gt;&amp;2, that I should output these commands to? Or will I have to build my own debugging commandline-option somehow?
You already surround most of your terminal sequences with \\\\\[ and \\\\\] to let bash know which parts of your prompt do not move the cursor. However, you do not in your \_\_git\_status function. The solution is a bit tricky, because bash internally replaces the prompting special characters \\\\\[ and \\\\\] with ASCII 1 and 2 *before* your \_\_git\_status function is executed. You can print these instead in your function: Change info=$(\_\_git\_info) &amp;&amp; printf '\\e\[30;43m%s' "$info" to info=$(\_\_git\_info) &amp;&amp; printf '\\1\\e\[30;43m\\2%s' "$info" Please note that only suitable terminals can be used with hard-coded terminal sequences.
- TLDP: Bash Guide for Beginners ([pdf](http://tldp.org/LDP/Bash-Beginners-Guide/Bash-Beginners-Guide.pdf), [online](http://tldp.org/LDP/Bash-Beginners-Guide/html/index.html)) - TLDP: Advanced Bash-Scripting Guide ([pdf](http://tldp.org/LDP/abs/abs-guide.pdf), [online](http://tldp.org/LDP/abs/html/index.html)) - WikiBooks: Bash Shell Scripting https://en.wikibooks.org/wiki/Bash_Shell_Scripting
I really think `find` is the best option here. It is built to recurse through directories, and it provides a means (`-exec`) to execute just about any command you can think of (in this case `ln -s`) on each of the files. I think it is the simplest, most elegant option.
/u/omgmog thanks!
I write a lot of shell scripts and I was reusing a lot of code So I wrote a small library for my own use. Its mainly for coloured text and prompts and such. Thought I share it not sure if there is much of a use case for this sort of thing or whether to expand it further? 
The TLDP bash guides are outdated, teaches bad practices and in some cases they're completely wrong. They are not good sources for learning bash.
If you want physical books, O'Reilly are always a good pick.
Honestly I haven't tried a bunch. The main thing I tried was just to move the stuff in the folder to the parent then delete the folder, that doesn't do anything with renaming. I'm not sure how to rename and care for extensions and spaces. I go into the directory and do the following oneliner "for i in *; do cd $i; mv * ..; cd ..; rm -rv $i; done"
My go to lately has been the bash reference manual. It's very well written. But I have been using it as a means to learn features that I already know about in more depth. When I started I used the bash page on the learnxiny site.
Can you show one example file name, then show what exactly you would type at the command line to manually do the change you want to it?
The below. I would like to to be able to handle spaces Before /data/foo/bar.zip &amp;nbsp; /data/foo folder has been delted After /data/foo.zip
The Free Bash Book (also creative commons): [https://books.goalkicker.com/BashBook/](https://books.goalkicker.com/BashBook/)
[removed]
This book is a compilation from what used to be Stack Overflow Documentation. I contributed to it quite a bit, but I have to say I consider it a dumpster fire and not a good choice to learn from.
I second the [BashGuide](https://mywiki.wooledge.org/BashGuide) recommendation; I did quite like ["The Linux Command Line"](I second the [BashGuide](https://mywiki.wooledge.org/BashGuide) recommendation; I did quite like "The Linux Command Line", too - covers not just Bash/shell, though.), too - covers not just Bash/shell, though.
I personally enjoyed this one: [Advanced Bash Scripting](https://github.com/mfherbst/bash-course/blob/master/bash_course.pdf)
It would help to see a sample of the entire command you want to run. That said you can use awk, here's an example. If you know that the word you want is always in the same place you can print just that field. In your sample 'xxxxx' is in field 7: echo "date word word word word pw: xxxxx" | awk '{print $7}' xxxxx If your word is always the last word in the line, but lines have different word counts you can use $NF. You can also use $(NF-1) to get the second to last field, or use $(NF-2), $(NF-3), etc. echo "date word word word word pw: xxxxx" | awk '{print $NF}' xxxxx Set a variable as as the word: var="$(echo "date word word word word pw: xxxxx" | awk '{print $7}')" Print: echo $var xxxxx
Okay, I use the functions then. Can you explain to me, why it is better in this case to use a function? 
Thank you for your assistance. Essentially what I’m trying to do is pass xxxxx as the password to a program. Something like this: Echo -e (xxxxx)\n | mysql_secure_installation
 var="" cd ~/Desktop/TEST/bash var=$(sed -n 's/.*pw://p' testfile.txt) echo $var &amp;&gt;/dev/null # trim whitespace This will do it just, change paths and filename. using sed command. 
you could also use `read` command.. for ex: $ echo 'foo baz 123' &gt; ip.txt $ # reads only first line from file input $ read -r a b c &lt; ip.txt $ echo "$a" foo See https://mywiki.wooledge.org/BashFAQ/001 for details
You can use var="$(printf "%s\n" "g/pw:/s/.*pw: //p" "Q" | ed -s /path/to/file)" Another user has posted a similar solution with sed which would work but the s in sed is for stream and you are using a file, therefore I would prefer the standard editor instead of a stream editor.
I could be wrong, but I'm not sure this would work for that script. "--password=password, -p password This option is accepted but ignored. Whether or not this option is used, mysql_secure_installation always prompts the user for a password."
In case yum install -y epel-release does not work (as happens on RHEL 7.5 AMI), this works: yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
and another one at line 54
 echo "date word word word word pw: xxxxx [Any text after the passord.]" &gt; pw.txt readarray -t FILE &lt; pw.txt PW=(${FILE[0]##*pw: }) echo ${PW[0]}
whether it is right depends on what you want it to do haha. have you tried testing it? does it do what you expect?
Normally yes, but it's with elif, else etc that I do not know what to put
im no bash whiz but they seem fine. i would however be tempted to use if [ -f ... ] instead of if [ -e ] 
Thank you all for the responses thus far, will be trying them when I get home 
Try throwing the script in [shellcheck.net](https://shellcheck.net). I use it to validate all of my scripts, and any recommendation I could make will be addressed by that. (Except perhaps `NMAP="/usr/bin/nmap"`... I'm not sure why you did that. Just put in your PATH)
For this you use the `read` utility. It reads content from stdin, so you can pipe it the contents of the previous command. For example, the lines `read var &lt;&lt;&lt; "like this"` and `read var &lt;&lt;&lt; "$(echo "like this")"` will both store the string `like this` in the `var` veriable. But beware! The line `echo "like this" | read v` will indeed store the string `like this` in the variable `v`, but as the command `read` is called from a pipe, the command runs inside a subshell, meaning the variable only will be available under its shell, for example, using it like `echo "aloha" | (read v; echo "${v}")`. The last thing about the command: you can use a loop along with `read`, like this: ls | while read text; do echo "object number $((n++)): ${text}" done So to do what you want you'd like it to separate characters instead of lines, right? Try this: echo "dinosaur" |while read -n 1 v; do echo "${v}" done You should be fine with this. Good luck!
I'm a bit unclear, you want to have the script slowly execute a command, and have that command output? You might spawn a coprocess? Is this to simulate a user? You might also look into expect. This should do exactly what you are looking for, but would require some scripting.
Im basically trying to get the script to store the command from the terminal as in the example 'ls' to then put that through a process which then outputs to the terminal through the script. So from: $&gt; ls Folder1 Folder2 Document1 to: $&gt; scifi.sh ls F\_o\_l\_d\_e\_r\_1 F\_o\_l\_d\_e\_r\_2 D\_o\_c\_u\_m\_e\_n\_t\_1 KEY: each underscore '\_' is a 0.2 second wait to 'print' the character. So just like in the old computers era when each character was printed at a time in a 'typing' manor (think war games moive)
Thanks for the reply, I've been experimenting but im not sure if this is what im trying to achieve (quite a beginner at bash scripting (should have said that to begin with)). But I would like to store a terminal command's output in a variable to then print out through the script with the 0.2 second delay on each character as in the code I posted.
Thanks for this, I'll look into it more. Although I only would like it to run when typing a specific command for example if I were to type ls it would perform the command normally, but if I wrote scifi.sh ls it would slowly print out the output in a character by character basis at 0.2 seconds per char. (Think old computers)
In that case, just pipe individual commands into `pv` or `ratelimit`.
I'm guessing he wants a script that would read stdin and echo each character with a delay. Which would be used like `ls | script.sh`. I guess. I don't know if strings are iterable in bash ? Maybe by redefining the IFS ?
 fichier=$HOME/Code/bash/file ndf=file if [ ! -e $fichier ]; then You will be better off using [[ and ]] instead of [ and ]. [[ will handle strings better. echo "Le fichier \"$ndf\"$file n'existe pas" exit 83 fi countlines=$(wc -l $fichier | awk '{print $1}') This line can be simplified: countlines=$(wc -l &lt; "$fichier") Redirecting the file to wc's standard input eliminates the filename output wc normally appends and makes the pipe to awk unnecessary. Also, be sure to quote variable names when dereferencing their values. This ensures that Bash treats them as a single token when there are spaces in them. if [ -e $fichier ] &amp;&amp; [ $countlines == 1 ]; then In the above test you should change the second test to: [[ $countlines -eq 1 ]]; The way you have it, you are testing the equality of two strings rather than the equality of two numbers. echo "Le fichier comporte $countlines ligne" exit 80 echo "Le fichier contient $countlines lignes" fi 
Here's what you want: #!/bin/bash echo "$0 $*" output=$(sh -c "$*") for ((i=0; i &lt; ${#output}; i+=1 )) ; do echo -n "${output:$i:1}" sleep 0.2 done exit 0
Here's what you want: #!/bin/bash echo "$0 $*" output=$(sh -c "$*") for ((i=0; i &lt; ${#output}; i+=1 )) ; do echo -n "${output:$i:1}" sleep 0.2 done echo exit 0
It doesnt work for me, when I remove the quote. I set it to variable. I tried subl=tilix\ -e\ vim\ -p and subl=tilix -e vim -p both times, I get command not found: tilix -e vim -p I call it with "${subl}" "$deriectory" "$file1" "$file2"
When you use a variable for a command with arguments, you can't quote it. subl='urxvt -e vim -p' $subl "$file1" "$file2" etc.
Can you show the script?
Well the whoel Script is too long, but heres the relevant parts, i hope. I set the variable in a different file and then source that configfile. actual_script.sh: #!/bin/bash IFS=$'\n' shopt -s nullglob Script_dir="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )" source "${Script_dir}/files/config.sh" #more things happen here. $subl "$R" "$file1" "$file2" "$file3" "$file4" "$file5" "$file6" "$file7" "$file8":10000 "$file9":100000 "$file10" "$file11":1000000 "${PicArray[@]}" IFS="$OIFS" files/config.sh: #!/bin/bash #set Editor for windows/linux&amp;mac: # subl, vim , etc if grep -qE "(Microsoft|WSL)" /proc/version &amp;&gt; /dev/null ; then subl=/mnt/c/Program\ Files/Sublime\ Text\ 3/subl.exe os=win else #subl=subl #SublimeText 3, this works subl='tilix -e vim -p' #vim #subl='tilix -e vim -g -p' #vim-gnome os=other fi
It's because of IFS=$'\n' Expanding a variable to a command and arguments assumes it can split the words in the variable by spaces. Try removing the IFS line or only setting it when needed and then putting it back to `IFS=$' \t\n'`. Or using newlines in config.sh. Like this works for me as well: IFS=$'\n' subl=$'urxvt\n-e\nvim\n-p' $subl "$file1" "$file2" etc.
And another question: I have this logging function: function log () { #Verbose Logging function, usage: log "Infos" if [[ $verbose -eq 1 ]]; then echo "[verbose] $@" fi } Which I call with log "abc" But how can I put cat output through this function? I tried cat "${package_versions}" | tee | log cat "${package_versions}" | log cat "${package_versions}" &gt; log log $(cat "${package_versions}") But none of these work..
This is perfect! Exactly what I wanted, if possible (or if you have time), could you just provide a quick summary of how it works? If not ill try and figure it out. Thanks!
This is great! Helps me understand and learn about Thai bash script thanks again for being so helpful!
Do the $file1 etc have spaces in the name? If not you could do OpenFiles="$file1 $file2 $file3" $subl $OpenFiles Else you need to use an array: OpenFiles=( "$file1" "$file2" "$file2" ) $subl "${OpenFiles[@]}" 
Of the examples you gave, these should work log $(cat "${package_versions}") log "$(cat "${package_versions}")" The first won't print newlines in the file. The second won't prepend "[verbose]" to each line, but the first.
You can change the log function to accept both arguments (`log “something`) and stdin (`echo “something” | log`). This version will be something like this: function log() { # return early of verbose isn’t set [[ “${verbose}” -ne 1 ]] &amp;&amp; return # here we use `-n` to not print the trailing newline echo -n “[verbose] $@” # we use cat to print everything from stdin cat - # here we very explicitly add the newline we skipped with the first echo # a plain `echo` without arguments works as well echo -en “\n” I hope this helps! 
Don't know C.
Here's another idea, it uses a timeout read to see if there is data on stdin, and if so prepends [verbose], and then regardless prepends [verbose] to all arguments. function log() { [[ "$verbose" != 1 ]] &amp;&amp; return if read -t0.01; then { echo "$REPLY"; cat; } | sed 's/^/[verbose] /' fi for arg in "$@"; do echo "[verbose] $arg" done } $ seq 10 | log hello there [verbose] 1 [verbose] 2 [verbose] 3 [verbose] 4 [verbose] 5 [verbose] 6 [verbose] 7 [verbose] 8 [verbose] 9 [verbose] 10 [verbose] hello [verbose] there 
Yes. Harmless.
Just stumbled on this! https://youtu.be/u8RXKFTekqw
Still, Obligatory don't login as root, sudo things as needed
With a snarky comment like that you need to be linking to the specific source files with at least some kind of line number to start looking at
That's a fair point, sorry about that. 
It has different uses. Variables let you swap them out and change them in scripts in case the path is different through the script. I can only imagine that you'd need the full path in very specific instances. The reasons not to use it id say would be portability, preferring just to use "ffmpeg" rather than stating a full path to the binary. 
I’ll do this with some longer commands YTD for youtube-dl IPT for iptables Stuff like that and then using other variables to set certain options so I can refer to them multiple times in a single script and not have to type out the long string of options over and over. I’m on mobile right now but can share examples if you want to see.
If the exact path to your binaries matters (hint: it normally doesn't), just set `PATH` once at the top of your script. I used to do the whole "absolute path for external commands" thing, until I realised that it's pretty stupid. If somebody wants to run my scripts on an utterly misconfigured system, that's their problem, not mine.
Ever ending installations of Gentoo, because I did that, or other things that I shouldn't care about, I know.
So every time it's better to use [[ test ]] than [ test ], Whatever the situation ? I have a script for the battery state : https://pastebin.com/xuZSzUTt so maybe it's better to change that, in some scripts on the internet i see [ test ] And it's better to use ==, &lt;, &gt;, or the text like -gt, -eq, -lt, or there is no difference ?
Knock yourself out. Like anything, you have to ask what you gain versus the extra complexity. Moreover, if you are going to do that, a function wrapper is generally considered a better option as it allows you to add features to the command without altering the overall script flow. So the `ffmpeg_func` wrapper might first check the file size before running it, or set a timeout to prevent long running process from gumming up a script that really should only take fifteen minutes.
If running Xorg, you could use `xdotool`
Stack overflow has a lot of references to `xprintidle`, various perl, and C solutions. `xprintidle` seems to rely on the X screen saver, so I'd investigate how it does it. `xprintidle` reportedly does not have available source anymore, though the Debian package is supposedly still distributed. I bet you can find the source anyway, if you look hard enough. Maybe in the Debian source packages.
There's no point. Just use `ffmpeg` to run ffmpeg.
[removed]
You need to drop the `$` from the variable assignment on line 3 (the one right at the start of the line).
$(date ..,) Or with backticks `date.... `
That's a nice tutorial video. Overall the program looks quite clean and user friendly. I like that you can partially type out a command and then do ctrl-r and it will show a list of matching commands. Overall however I think most of this functionality wouldn't see much use. Without this program if I want to see all commands that match a certain pattern I can just grep on my .bash_history file. I also don't see the point of bookmarking commands. How is this better than an alias or custom function I have placed in my bashrc?
&gt; I can just grep on my .bash_history file You can, and this is probably what it basically does under the hood, but it will save you the keystrokes of grepping it, or even needing to know that the .bash_history file even exists. 
 date -d '1 month ago' +'%m%d'
In addition to what others said, change this line. `mkdir /var/www/example.com/html/src/$oldfolder` To `mkdir /var/www/example.com/html/src/"${oldfolder}"` OR `mkdir /var/www/example.com/html/src/"$oldfolder"` To prevent globing/spaces, ..etc from causing problems. 
P.S I come from a Java/C/python etc background, never shell.
Since you know C, you probably know that C has the notion of true (1) and false (0), but also the notion of command success (0) and non-success (non-0). In the original Bourne shell language, which bash is based on, there is only the latter: success (0) and non-success (non-0). So the `if` statement uses that too. The basic form of `if` is: if list_of_commands; then list_of_commands else list_of_commands fi The second list of commands is executed if the last command of the first list of commands "succeeds" (returns status 0); otherwise the third list of commands is executed. Of course, every command has its own criteria for whether or not to return status 0. In your example, `ls` will return status 0 if `file_sample.txt` exists, and a non-zero status if it doesn't. Hope this clarifies.
Ah got it. Thinking of every line as its own sub-program works! Cheers
`if` executes a command and tests the command's exit status. If the exit status is 0, the `then` branch is executed; otherwise, the `else` branch is executed (if it exists at all, of course). So in your particular example, the `ls` command's exit status will only be 0 if the file exists. Most programs document their exit statuses in their manpages and other documentation. Note that if you simply want to test whether a file _exists_ or not, a better option might be: if [[ -e file_sample.txt ]]; then ... fi This doesn't need to execute an external command to do the job.
Just to chip in here, and please do tell me if I'm not understanding; how does encasing an if statement within an 'until' or 'while' loop go towards solving the true/false elements mentioned above please?
This works good. But how do I combine it with u/gold_and_seaweed s solution? Because I need to use arguments lie -e for colors. This ouputs the echo, but I see no colors as i do without the logfunction: elif version_compare_gt "$PureVerInstalled" "$PureVerAvailable"; then echo -e "\e[93minstalled Version later than available?!\e[0m" elif [[ "$PureVerInstalled" == "$PureVerAvailable" ]] ; then echo -e "\e[32msame Version, package is up to date!\e[0m" 
M8 this is gr8 I r8 8/8 cheers
Hey, idigress31337, just a quick heads-up: **succesful** is actually spelled **successful**. You can remember it by **two cs, two s’s**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Sure, use lindex and $argv: ``` set arg1 [lindex $argv 0] spawn python ... $arg1 ... ```
For this I would run my expect as: \`expect gfsk\_loop\_start.exp -arg1 value\_of\_arg1\`?
No, $argv just keeps positional arguments.
In the for loop, change this line: echo "[verbose] $arg" to this: echo -e "[verbose] $arg" You can then do: echo -e '\e[31mRED color\e[m' | log '\e[32mGREEN color\e[m'
This is all true, but where do loops come into it?
Ok, but is it also possible to set -n for example when calling the log function? Because sometimes i need it and sometimes not.
What do you consider competently? If you're trying to learn how to navigate the command line and do basic tasks, that can be learned in a day or two, but to get good it will take some practice. Learning scripting in any language takes time. Bash has a strange syntax. (As a programmer in other languages, shell syntax is mildly confusing, however if bash/shell is your only language it shouldn't be an issue.) Learning bash in 48 hours would be take serious dedication, and is likely not possible if you're not already familiar with UNIX/shell scripting in general. The script you have posted is a super simple one. I'll break it down line by line `#!/bin/bash` sets the interpreter `set -x` sets an option to [print all commands run and their arguments](https://ss64.com/bash/set.html). `for file in *.c` starts a for loop over all .c files in the current directory. This leverages globbing and shell expansion '*.c' is essentially a regular expression looking for any string ending in '.c'. The input to this regex is the list of files in the current working directory. `do` syntactic weirdness in shell scripting, similar to brackets `{}` in other languages `gcc $file -o 'echo $file |sed 's/.c$//'` As you posted it, this is a poorly formatted call to `gcc` the gnu c compiler. `$file` is a reference to the variable set in the for loop above. A better way to write the same command is this: `gcc $file -o $(echo $file |sed 's/.c$//)`. This makes it clear that you're invoking a subshell and using the result of the subshell as part of the input to `gcc`, specifically the file where you want gcc to dump the compiled c program. (see `man gcc` for how I knew that) The subshell prints the variable name, which is the name of the file `gcc` is compiling at the moment, then pipes it through `sed` which executes a substitution (the `s/.c$//` bit) to remove the .c extension. `done` ends the for loop, again another syntactic oddity of shell scripting. 
It works as an Array. Theres one last thing I need to do with the Array. I need to add doublequote to each element, when I echo it, how would I do this without manually adding \" like I did before? Before it looked like this: echo "subl \"$DIR\" \"$FILE1\" \"$FILE2\"" when I just echo ${OpenFiles[@]}, it echos it without the doublequotes.. I guess it can be done with sed or Variablesubstitution, but I am not sure how exactly.
 echo -n subl for arg in "${OpenFiles[@]}"; do echo -n "\"$arg\""; done echo
I think the thread title was just a little confused. OP really just wanted to know whether something like `ls` can be used in any kind of conditional, and thought of both "if" and a loop entry test as examples. 
Thank you for the nice summary, feedback and questions! **...if I want to see all commands that match a certain pattern I can just grep...** The main reason why I created HSTR was that I was tired of finding the right command (among similar commands) by moving back and forth using &lt;kbd&gt;Ctrl&lt;/kbd&gt;&lt;kbd&gt;r&lt;/kbd&gt;/&lt;kbd&gt;Ctrl&lt;/kbd&gt;&lt;kbd&gt;s&lt;/kbd&gt; and seeing just one line in Bash. Next stage was grep - exactly as you suggested. But I was too lazy to write long pipes and/or grep and/or expressions. HSTR can search history by finding substrings, regular expressions or **keywords** (you can easily change matching method by pressing a shortcut). Since a HSTR user brought keywords idea I used the last option. By =search by keywords= I mean searching for commands that contain given tokens (in any order) e.g. "wl gr" gives "iwconfig wlan0 | grep -i --color quality" and other matches. Consider either - pressing &lt;kbd&gt;Ctrl&lt;/kbd&gt;&lt;kbd&gt;r&lt;/kbd&gt;, "wl gr", &lt;kbd&gt;ENTER&lt;/kbd&gt; - or - writing a grep... HSTR can be also much **smarter** than grep when ordering matches as grep does **not** know anything about searched source - HSTR does. Matches are ordered by a metric that considers (weighted): * how many times is command in the history (more occurrences is better) * how recent were invocations of that command (recent is better) * how long is the command (longer is better) * ... and more This makes results much more relevant than with plain grep. **How is this better than an alias or function I have placed in my bashrc?** Yes, I went through aliases as well. Similarly as above - they don't scale as the number of favorite commands grows + it's too much work to define them + edit them + recall them. With HSTR I just lookup a command, hit &lt;kbd&gt;Ctrl&lt;/kbd&gt;&lt;kbd&gt;f&lt;/kbd&gt; to define favorite (alias). When I need such command, I lookup it in the same way (I don't have to remember exact alias name - especially when they are similar and/or list aliases). In short, HSTR saves a bit of time - you don't have to waste your time.
**... grep ... and this is probably what it basically does under the hood** HSTR uses a metric for matches ranking which makes it stronger than grep - please check my description above. **One thing I would change if I was the author, would be to look for tokens that start with letters typed, instead of searching for the letter anywhere.** My apologies - demo is four years old and it shows just an initial matching method. Currently HSTR has three methods for searching (can be changed by pressing Ctrl+e): exact, regexp and keywords. I think that you mean **exact** - please give it a try. Thank you for your interest in HSTR and questions!
Just to distinguish Bash's if from some other language's ifs, conditions in Bash if statements can be more than simple tests that evaluate to true (0) or false (any other number). They can be commands that do useful work, such as creating, moving, or deleting files. This can make error checking a bit more efficient.
Do you want to restrict access to only people using said script? I would start by setting up the FTP server (e.g. VSFTPD) and then write the script against that. The script could be a one-liner depending on what is on the FTP server.
If your IP address is dynamically assigned by your ISP, you can use a ddns service to give you a stable, public-facing address. You could optionally register a domain and point it at that address. You could also check your public IP address on ipchicken.com or something like that. 
Why FTP? It's a pretty outdated service. Even when new, it was poorly designed, requiring two separate layer-4 sessions (one for control, one for data) and embedding layer 3 information (addresses and port numbers) in the control channel so that firewalls need to inspect the payload to allow a PASV session. I ran a customer-facing FTP server for a start-up to distribute hotfixes, documentation, etc. Every time we showed users an FTP URL for that server, for the next month we'd average one support ticket per day, where the help desk had to walk them through changing their browser/client from PASV FTP to active FTP. I nearly finished the change management process for turning PASV off on the server (and hoping it didn't have any adverse effect on browsers preferring PASV) before I asked "is there a reason we're not doing this with HTTP?" Only one person had a concern about plaintext transfer, which was moot when I pointed out FTP is also plaintext. If the goal is to distribute files, even very large ones, modern HTTP(s) fits the bill just fine without all the pitfalls of clunky old FTP. 
Honestly. Just don't do that. Why would you want to open your network to the whole wide world. Throw the files in github, then host your script in github. Done. No need for you to go hosting the files. 
Can I curl files from github? Say I create a repository, can I just curl the files and have them download with my script? I know I’m inexperienced, thx for all the help.
&gt; Option #3: Get yourself a virtual server (via Digital Ocean or Linode) and host on that. That will have a static IP. This is what I recommend. If something bad happens with that server, it's probably better than it happening to your local machine. I don't recommend people ever serve stuff on their home connections, which in some cases violates your ISP's TOS, but can also allow risky ingress into your home network. Digital Ocean droplets start at $5 a month. More pros to think about with this option: * Much better bandwidth using the datacenter uplink instead of your home uplink * Valuable practice with administering a machine 100% remotely * Don't have to think about your server's users when you need to work on your home router/modem 
Its part of the arcane ansi escape sequences the [ means the start of a control sequence (CSI) and the m means this control sequence is supposed to change how the text looks (SGR). Wikipedia actually has a pretty good article on them
&gt;the normal 7-bit X3.64 Control Sequence Introducer is the two characters "Escape [" Interesting reading on the subject: - https://www.askapache.com/linux/zen-terminal-escape-codes/ - ftp://ftp.invisible-island.net/shuford/terminal/ansi_x3_64.txt - https://archive.org/details/byte-magazine-1984-04 
 case !? in 1) echo "sucess"; 0) echo "fail"; easc Just gonna leave this here :D 
yes heres an example of the readme from some random git repo https://github.com/samyk/poisontap/blob/master/README.md The above link takes you to the web interface view of that file. if you click "raw" or just copy the link, you'll get https://raw.githubusercontent.com/samyk/poisontap/master/README.md which is just the raw file. So you should be able to first put the files in a repository, then edit the script so it gets things from those links. Then you can even put the script in a git repo and just send people a link to it. 
This is what I’m deciding on doing. Quick question, if for some reason I can’t use https, can I use http? Just replace https with http in the link?
hmm I actually don't think so. I hadn't considered that. seems like going to http just redirects me to https. curl -k should work with certificate issues. You'd have to find somewhere else to host it if you can't do https
Do you know any free hosting sites supporting curl? I’m making a project for ancient hardware and github isn’t curling even with -k. Google.com does tho for example. So far everything I see is paid
As far as I can tell, not really. `man readline`: show-mode-in-prompt (Off) If set to On, add a character to the **beginning** of the prompt indicating the editing mode: emacs, vi command, or vi insertion. The mode strings are user-settable. so you *could* play with the user strings like this (example): set vi-cmd-mode-string \1**\t\t\t**\2:\1**\n**\2 set vi-ins-mode-string \1**\t\t\t**\2+\1**\n**\2 and put the indicator above the prompt string like this https://i.imgur.com/IR246hG.png , which is kinda ridiculous. Sorry for the unhelpful comment.
I've just checked the Bash source code. It looks like it's hard-coded to expand those at the beginning of the prompt (or the beginning of the last line of the prompt, if it is a multiline prompt). There does not seem to be any way around that. There also doesn't seem to be anything you can do with `PS1` to achieve the same thing. `PS1` is expanded before calling into readline, and it's readline that manages those insert and command mode strings.
If you know what options you have could you first detect the OS and platform then run a different variation of a function based off that OS class? 
May be use the version strings of the programs, and compile a list of all the used versions. Then cater to the lowest available version in the list? Make a list of all the utilities, and run `--version` with each of them, record the output. Run it on all the devices you are using. 
That's a good idea actually... now I just need to only allow said check if strings is installed! This entire project is turning into one giant voracious self-devouring chicken/egg dilemma!
why not just $( test here ) e=$? if [ $e -eq 0 ] then $( test here 2&gt;&amp;1 ) | egrep -iq "usage|help|options" [ $? -eq 0 ] &amp;&amp; e=1 fi
Yep, many of us have been there and done that... but usually it's just for one or two commands. Having had a think about it, I suspect that in the case of `busybox`, because it's a monolith, that you may need to use the `command --help | grep ' -option '` approach. This possibly puts you in a position of failing through multiple attempts or doing some sort of detection. `ps -o comm= -p $$` is the usual way to detect your shell, but `busybox` is spesh-ul, so you'd need to use something like `ps -o pid,comm= | grep "^$$ "` I have one foot out the door, but that second command probably needs to be an `awk` command that prints the second field...
It took me some searching but I assume you mean this article: https://en.wikipedia.org/wiki/ANSI_escape_code#Escape_sequences 
I would take a step back and, for each option I think I need to detect, determine whether there's a way to achieve the same effect without relying on the presence of the optional behavior I'm testing for. For example in the case you've presented here- you're looking to see whether ls supports "-f". Presumably, on platforms where ls does _not_ support -f, you're doing the sorting some other way. Why not just sort in that fashion on _every_ platform? You don't need to take advantage of optional behaviors just because they're available, particularly if there's some other option which is actually portable. 
Yeah, I've considered that as well - ls is just an example, I'm using all the typical text-processing tools, as well as most of the sysinfo tools... at least most of the sysinfo tools that are common to any Linux/Unix out there. And yeah, I'm looking at ways to make things "work no matter what". As an example, instead of using regex within egrep, I'm doing multiple sequential greps with less-than-graceful patterns.
I thought [there is a way without unmounting](https://apple.stackexchange.com/questions/99536/changing-creation-date-of-a-file/99599). Try it and tell us how it went.
As far as I know, Bash doesn't support multiple variables in it's loops like Python does. What I would suggest is something like this: for key in $keys; do value=$(echo $json | jq -r .[key]) echo "$key has value $value" done How to actually get the $keys variable though, I'd need some source data to fiddle with. Also, `echo $json` can just be the output of any command that produces json, such as a web request, or whatever.
That's perfect, I can get keys from jq no problem. Thanks for the help! 
then why not just write a function for the (probably) 3-5 “major categories” of machines you have, then detect the machines’ properties and call the right one? that has to be much simpler than a bunch of crazy if-elses (though ymmv)
I don't know what you're doing, and maybe it's out of place, but you could write your script in another language ? Node or Python. Bash is strong when manipulating systems, files, less strong when manipulating data.. But maybe you have your reasons.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sysadmin] [alias + sudo = gotcha](https://www.reddit.com/r/sysadmin/comments/97sexs/alias_sudo_gotcha/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I was just trying to minimize dependancies. I agree, another language would be much better suited. 
`ls` does not only list sub directories but also files. `open` as described is osx only. `rm -r` removes recursively. `rm` doesn't know about "Trash" it's fine to copy a file to a directory and omitting the filename `nano` is not bundles with bash. in fact, its an optional package in most distributions. you do not need to `touch` a file before you open it in an editor, all editors are able to create a empty file. You really should start your shellscripts with a [shebang] (https://en.wikipedia.org/wiki/Shebang_\(Unix\)) 
In that case I'd argue `jq` is a dependancy and chances are all you need would be in Node or Python standard lib... You're the best judge of what's right for you and your systems :)
It is accurately described in the article annotation: *simple, repetitive, and sometimes boring article about Bash.*
Yap, definitely doing that next, I had the alias for a while, though I can't recall if docker back then allows using group in lieu of sudo
The general idea of what you are suggesting works, and is sorta what I am doing, but there are still a couple quirks... * egrep is one of the biggest offenders on my list. It generally doesn't exist at all on the older NIX's, so I'm usually stuck with grep, and sometimes even it is unceremonially trimmed to near-worthlessness on newer BusyBox machines. * I'm having trouble verifying whether $? is just a Bash thing or whether it is GTG for all POSIX terminals... 
*READ THE MAN PAGES FIRST* use `man commandName` to bring up a manual page for that command. To exit hit `q` First Bash is the default shell for OSX and the vast majority of mainstream Linux distros. Second, if you are opening a file in an application you can use the command for that application ex: `gthumb picture.jpg` it opens an image viewer with that photo. Third, file extensions are not necessarily a requirement for Linux and it is most likely the same on a Unix system. If you need to know what kind of file it is then use the "file" command. Fourth and someone else already made this comment. ls will show subdirectories AND files. The only thing that makes a file hidden is a period in front of it which is why the working directory and parent directories only show with the -a flag. Fifth, the -r flag for rm is for recursively removing files and directories NOT MOVING THEM TO THE TRASH so keep in mind data recovery from the rm command can be a real pain depending on your filesystem. Sixth and this is a big one. The command line is case sensitive so if you need to move multiple files use a lower case r with the cp command ex: `cp -r ./somedirectory someotherdirectory/withinadirectory` I'm too tired to finish writing up all of this. Good night internet and remember to read the man pages. I should put that at the top hold on.
The only acceptable alias involving 'sudo' is `alias fucking='sudo'`
*sigh*
Short version: once someone can use docker, they can volume mount `/etc/` and then modify files like `/etc/passwd` and `/etc/shadow`. A lot of people can explain this better than I can, including Docker Inc itself: - https://docs.docker.com/install/linux/linux-postinstall/ - https://www.projectatomic.io/blog/2015/08/why-we-dont-let-non-root-users-run-docker-in-centos-fedora-or-rhel/ - https://www.zopyx.com/andreas-jung/contents/on-docker-security-docker-group-considered-harmful 
Oh, hmmm. And you can't restrict what docker is permitted to mount?
 tmux display-message -p '#{pane_current_path}' | sed "s|$HOME|~|; s|/\(.\)[^/]*|/\1|g" The `sed` command has two parts: one to replace `$HOME` with a tilde (as in your version), and one to replace each slash, following character, and following sequence of non-slash characters with just a slash and the first character.
If you control the docker daemon, you effectively have `root` on the system as you'll be allowed to create privileged containers that can bind mount any path the host filesystem and run processes with an arbitrary UID.
thank you! I tried a lot of patterns with sed using back references but I couldn't come up with yours, it was so simple though! btw I used `sed -E 's|(/.)[^/]*|\1|g'` to keep the slashes in front of initials 
You are right sorry, I looked too quickly at your answer (only the pattern matching and not the subs) 
If you’re looking for a workaround, it’s best to create a password-less service account that is added to the Docker group and instead grant users access to the service account to run Docker commands. This clears up permissions issues since there’s a single user, and any additional security measures only need to be applied to the service account.
Unlink Windows, Linux &amp;mdash; and by extension the sort-of-Linux you get with WSL &amp;mdash; does not try a random assortment of extensions when executing a program.
`while true; do` `read -p "What is your name?" name` `echo "Your name is: $name"` `done`
Yes. I've even renamed them with bizarre names that I knew didn't match any other files on the system, and they still execute, but _only_ when Iinclude the 'bat' extender. 
Well, if that's the case there must be something quirky with the way WSL works. I've never used it, so I can't help you with that. I doubt it's really anything to do with Bash though.
Here's a little additional documentation on 'read': http://wiki.bash-hackers.org/commands/builtin/read
I am not using WSL. I'm just trying to add plugins to Vim, but it has been a pretty nightmarish experience. Thanks for your input!
&gt; I am not using WSL. Fine then. It must be something quirky with the way "the Bash you get with Git-for-Windows" works. Same thing, as far as I'm concerned.
Yep, this is exactly that. BASH wizards are fun.
Can I do something like this? echo Press the enter key to continue running this script read -p echo This script is continuing 
There's another question on SO pertaining to FreeBSD using fsdb but no OSX solution [https://stackoverflow.com/questions/40751290/change-ctime-of-file-under-freebsd-ufs-macos-x-hfs](https://stackoverflow.com/questions/40751290/change-ctime-of-file-under-freebsd-ufs-macos-x-hfs)
Why don't you just try though?
At work right now, far away from anything I can run bash on
Sure. read -p "pres enter to continue" 
Thx! This is my favorite sub 
A really simple way to handle this would be to just use "$@" Script would look like this: \#!/bin/bash mount /dev/sde2 /media/hermes mv /media/hermes/\* "$@" &amp; progress -mp $! umount /media/hermes chmod -R 777 /media rm -R "$@"'$RECYCLE.BIN' rm -R "$@"'System Volume Information' To use it you'd do chmod +x script\_name.sh and then \`./script\_name.sh /media/4tb\_2/\` or whatever you want the destination path to be.
This is the correct answer. 777 is almost always the wrong way to fix permissions. P.S. /u/pingueame, you can format commands by encasing them in back ticks, or adding four spaces before each line.
use read's -a option to read into an array: read -rp "Enter command: " -a command case ${command[0]} in [dD]*) printf 'change to %s\n' "${command[1]}" ;; esac
Came here to suggest rsync solution - beat me to it. Nice :) BTW, I use the rsync ownership/permission switches all the time when deploying websites to different hosting providers. Permissions on my dev box are not always what should be deployed - rsync makes sure the deployed version is “safe”.
Remove space between '-' and 'annotate'.
Changed it to: convert -gravity south -pointsize 36 -annotate "$last_modify_time" $image_file $temporary_file Errors: convert convert: Unrecognized operator (-annotate). mv: cannot stat 'img1.png.tmp.19247': No such file or directory
Are you not passing the entire path of the image? The temporary file is getting created in a different folder if you don't give a path.
Yeap, i know about format. But from mobile is ... not easy. When I reach a stable connection, and if I dont forget, I will fix it ;)
&gt; I pass an image 'img1.png' and when I ./execute.sh it, nothing happens. No, when you execute it, it waits for input because you gave it `-` by itself as an argument. A dash by itself means to take input from stdin and treat it like an input file. You're supposed to put image operators before the image they operate on, and annotate is for output files. Try putting `-annotate [notation]` after the input file. In your case, you probably want all the arguments after the input file: `convert $image_file ...`. If it still doesn't recognize the option, it could be that you're using an extremely outdated version of imagemagick. Also, that `ls ... | cut ... | sed ... | cut ...` sequence looks like a real travesty. Like, why add to the string with `ls -l` and then immediately cut off the fields `ls` added? Why strip leading spaces with sed when the next step is to cut off everything before the first backslash anyway? You'll get better recommendations if you show what the filename looks like and which part you want to extract. 
/u/CaptainDickbag format fixed
rsync is good tool. I love it
 last_modify_time=`ls -l "$image_file" | cut -d'\' -f5- | sed 's/^ *//' | cut -d'\' -f2-4` On top of what has already been said, there are a few golden rules of shell scripting. One of them is: Do NOT parse the output of `ls` You want to use `stat` instead. The exact flags required depends on whether you're using Linux or OSX or something else e.g. GNU `stat`: `stat -c "%y" "$image_file"` OSX `stat`: `stat -f "%m" "$image_file"` 
I like to use `shift` myself in those situations.
Sounds like you're trying to text a list of login info you obtained from a phishing site to see which ones are good. Reported.
no, im testing a list of my old shitty passwords because i forgot them and im a dumb ass and only recently started to use lastpass
i can supply my input file if needs be, none of these old account had anything useful in them
Approved. This seems harmless enough to me, and I don’t think there’s a script solution for this anyways, except for websites that implement HTTP basic authentication.
bash is tricky when working with web requests. but i would use curl for this. basically: for each user, use curl to get the http response code, if code is 200 then the http has worked. that should be enough to get you some direction
thanks galakots, i wanted to see what little kid me did on the old websites i played games on or what i did when i checked out my space.
could you give me an example? this sounds promising either way
given $USER and $PASS, something like curl -I -s -u "$USER:$PASS" https://example.com | grep -s '200 OK' &amp;&amp; echo 'example.com' || echo '' for `curl`, the `-I` option says to only request the headers and not the entire response body, `-s` says to not output as much stuff (i.e., it won't render a progress bar on stderr), `-u "$USER:$PASS"` sends the username and password (stored in `USER` and `PASS`, respectively). The output of `curl` is sent to `grep`, where the `-s` option again says to not output as much (i.e., it won't print anything, and will only communicate based on its exit status), and it will look for `200 OK` (you might need to change this depending on the particular site). 
I recommend: * Bash: [BashGuide](https://mywiki.wooledge.org/BashGuide) * awk: [Effective Awk Programming, 4th Edition](http://shop.oreilly.com/product/0636920033820.do) (also available [for free](https://www.gnu.org/software/gawk/manual/), but why not support the author?) * sed: [sed &amp; awk, 2nd Edition](http://shop.oreilly.com/product/9781565922259.do) * Vim: [Learning the vi and Vim Editors, 7th Edition](http://shop.oreilly.com/product/9780596529833.do) The latter three also cover regular expressions, if I remember correctly. A lot of people like The Grymoire tutorials: * [Shell](http://www.grymoire.com/Unix/Sh.html) * [awk](http://www.grymoire.com/Unix/Awk.html) * [sed](http://www.grymoire.com/Unix/sed.html) * [Regular expressions](http://www.grymoire.com/Unix/Regular.html) but I haven't used them myself.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://www.grymoire.com/Unix/Awk.html) - Previous text "awk" [Here is link number 2](http://www.grymoire.com/Unix/sed.html) - Previous text "sed" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
regarding quoting, I highly recommend [this excellent Q&amp;A on unix.stackexchange](https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters) for sed/awk/etc, if you are using GNU versions, I have a [repo dedicated to cli text processing](https://github.com/learnbyexample/Command-line-text-processing) - plenty of examples and also covers regular expressions from basics.. I also have curated resources lists for [Linux](https://github.com/learnbyexample/scripting_course/blob/master/Linux_curated_resources.md) and [Vim](https://github.com/learnbyexample/scripting_course/blob/master/Vim_curated_resources.md)
I would recommend the bash reference manual to anyone already fairly familiar with bash and wants to learn everything in more detail.
`dd` isn't the right tool for deleting the last 5 lines of a file. Use `head` instead: head -n -5 $outputfile
I see. dd seems to be a vim only command, I probably can’t use it in a bash script then. The head command seems to give me permission denied as well. I think I know the issue. Maybe I don’t have permission to modify any files on my the Hadoop cluster?
You can use this: ed -s "$outputfile" &lt;&lt;&lt; $'-4,$d\nwq'
I have never looked into this functionality before, so I doubt I'll be much help, but out of curiosity, what is the output of: bind -P | grep "C-w" 
 ffmpeg_1 () { ffmpeg -some-parameters $@ } ffmpeg_2 () { ffmpeg_1 -vf scale $1 } that calls ffmpeg_1 from ffmpeg_2, and $@ means all arguments (not only the first, $1)
You can use 'dd' inside vim as a command to delete lines yes, but it's also it's own command - see the man page for [`dd`](https://linux.die.net/man/1/dd) Getting a permission denied error almost certainly points to the wrong permissions for the directory or file you're trying to write in to. Have a read over this page for information on how this works and what to do look for - http://www.tldp.org/LDP/intro-linux/html/sect_03_04.html If the file is stored in HDFS though, then that's a whole other kettle of fish.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://linux.die.net/man/1/dd) - Previous text "dd" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
The way I typically do it, is to set a variable outside of the commands, for example: ffmpeg_opts="-some-parameters" ffmpeg_1() { ffmpeg $ffmpeg_opts $1 } ffmpeg_2() { ffmpeg $ffmpeg_opts -vf scale $1 } This would work when it's all referenced in a single script. Not 100% certain how it'd work if you load it in to something like your .bashrc file though.
just check your terminal history? `$ history` I prefer using: `$ view $HISTFILE` because you can navigate/search more easily.
I remember quite some "cheat" command projects, but don't have a specific one in mind right now.
/r/learnpython/ is a more appropriate subreddit for this question.
This is not even close to being a bash question
I am asking for a bash command. Here, something from wikipedia, you might learn something: "Bash is a command processor that typically runs in a text window where the user types commands that cause actions"
No. You're asking for a shell command. You would have this problem no matter what shell you were using. That you happen to be using bash does not make this a bash question, since there us nothing bash specific about it. Insert snarky comment about Dunning-Kruger effect... 
Who cares if bash is just an implementation of shell, question is still relevant here. Also, no idea why you are wasting your time on this. Go be productive or something.
I thought it might be productive to clue you in that this is off topic here and you would probably get you better help faster if you asked somewhere else, like a reddit about osx or something... But yeah, you're right, this is obviously a waste of time
Not all time commands support the -f/--format flags. To test if it does, I would try something simple like time -f '%e seconds' some_comand and see if it works, or if you get back -f: command not found 
I don't have those commands on my system so I am guessing their output is the one you pasted there awk ' $1~/Size/ {size+=$2" "$3} $1~/Speed/ {speed=$2" "$3} $1~/Type/ {type=$2} END {print size " GB " speed " " type} ' &lt;&lt;&lt; "$(system_profiler SPHardwareDataType; system_profiler SPMemoryDataType)" The size variable gets the sum of all the occurrences where Size is found in the first column (blank separated) of the value of the second and third (which doesn't change, but is cool to know you can do this) (which are $2 and $3). Similar with the other matches and variables, but only the last value is printed (there is no sum). Btw you can use printf too
Bash’s `time` builtin doesn’t support a `-f` option. GNU coreutils’ `time` command does, but since that’s a regular command, not a builtin, you can’t use it to time a compound command (`(x; y)`). The `time` builtin supports reading the format from the `TIMEFORMAT` environment variable, but I’m not sure what syntax it uses: $ TIMEFORMAT='Executiontime: %esec' $ time sleep 1s -bash: TIMEFORMAT: `e': invalid format character
You can see which files and directories the process currently has open: ls -l /proc/$(pidof grep)/fd
If you just want it to be on a single line, remove the newline character (\n) from the first printf. You might want to replace it with a tab character (\t) to space it out a bit on the line
You are asking a Mac question. Asking here is just luck if you run into someone using a Mac that can answer your question. Your chances are better finding something in a Mac sub-reddit instead of a general bash sub-reddit. What you could try that's bash related is, you can browse your bash command history like this: history | less You quit that "less" text browser by typing `q`. You can navigate to the end of the file by typing `G` (Shift-g), and back to the first line by typing `g`. If think you can remember part of the command line you used to change your Mac keyboard setting, you can try to search for it by typing: /some-word-here You can repeat the search by typing `n`, and can search backwards by hitting `N`. You can also filter the file like by typing: &amp;some-word-here To disable the filtering again you type `&amp;` and Enter.
Ok thank you its working reasonably well but its not giving me the 200 OK response with my current login for facebook, it'll work if i set it up to [`https://www.facebook.com/connect/login_success.html`](https://www.facebook.com/connect/login_success.html) because thats where you are meant to redirect logins to for custom facebook login pages, any ideas?
If you take some variables and slap them together, you can use the `bc` command ([man bc](https://linux.die.net/man/1/bc)) to do the math on it. There are also a heap of options for daemonising your script as well and including checking the time of day so it can automatically dim for you. What have you tried so far in writing your script?
Solved with the thanks of bash IRC here is the answer (posting here for future googlers to find): &amp;#x200B; \#!/bin/bash output=$(xrandr --verbose | **awk** **'$1 == "Brightness:" {print $2; exit}'**) new=$(**echo** **$output** \- 0.1 | bc) xrandr --output DP1-1 --brightness **"$new"** exit &amp;#x200B; (change - 0.1) to +0.1 and its the brightness up command :) &amp;#x200B; &amp;#x200B;
With LAT/LNG, you could even adapt brightness during sunrise and sunset: [I do it for screen color temperature](https://gitlab.com/moviuro/moviuro.bin/blob/master/sctw).
u/houghi is right, not all whois outputs have orgname, but I see in your grep you have upper case characters, I checked one of my CentOS servers and the field names in whois output are all lowercase so try changing your grep to be case insensitive by adding -i: `while read ip; do whois $ip; | grep -i OrgName done &lt; file-with-ipnumbers.txt` I have a script that I use to generate a report of IP's brute forcing my internet facing ssh server. Its a bit ugly but it may give you some ideas or tips: `#!/bin/bash` `LASTB=$(lastb | awk '{print $3}' | grep -v "192.168.1" | head -n -2 | sort | uniq -c | sed -e 's/^[ \t]*//')` `# lastb returns failed login attempts` `# print only the 3rd column which has the IP addresses` `# grep out local network entries` `# remove last 2 lines of the output, lasst line is the date and 2nd last is blank` `# sort list for uniq` `# uniq -c displays a count of the uniq entries` `# remove the leading spaces` &amp;#x200B; `echo "${LASTB}" | while read line; do` `# The 2 lines below use parameter expansion, see` [`http://wiki.bash-hackers.org/syntax/pe#substring_removal`](http://wiki.bash-hackers.org/syntax/pe#substring_removal) `# Returns the IP address being the longest part of the string` `IP=${line##* }` `# Returns the number of instances the IP address appeared in lastb` `COUNT=${line% *}` `# Greps the output of whois for the country the IP originates from` `COUNTRY=$(whois ${IP} | grep -i country | awk '{print $2}' | head -n 1)` `# echo the output in a nice format` `echo -ne "${IP}\t"` `echo -ne "${COUNTRY}\t"` `echo "${COUNT}"` `done` &amp;#x200B;
Use jq(1) for parsing json.
Has some nice documentation online too https://stedolan.github.io/jq/manual/
While not bash it is the best solution once you get used to how it works. Will save you hours of hair pulling to figure out another way. Like they said, jus jq.
 jq -r '.[] | "\(.ip) \((.ports[].port|tostring ))/\(.ports[].proto)" ' | \ awk -F' ' -v OFS=',' '{x=$1;$1="";a[x]=a[x]$0}END{for(x in a)print x,a[x]}' | \ sed -e 's/,,/: /'
This is honestly something I'd handle with Python or C, but that's more because I'd want a gradual dimming (though I don't know how repeated calls to xrandr might behave in fairly rapid succession). I know it's the bash sub, but I feel like doing that in bash would be a pain. But yeah, in general you can think about problems that are similar like so: find a way to output the original value, use formatting tools (like awk, scanf, some flavor of regex, etc) to pull out the value, modify, and issue the command. I recently threw together a monitoring thing that's basically a glorified version of this same problem: Get cluster monitor output -&gt; parse based on line delimiting and fields into JSON -&gt; push to MongoDB Boom, beautifully simple way to make important data searchable and displayable. Now meshing that in with the time series data into a dynamic Django view? Yeah I'm gonna be working on this for a while.
Try with that declare VPN_STATE="$(opt/cisco/anyconnect/bin/vpn state)"; &amp;#x200B;
That did it. Thank you so much!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
The short version is don't use break in case. It's not needed - just remove the various break lines and you're done. A case statement will run the relevant commands for the given match and then move onto the next bit of code. There's no need to explicitly tell it to exit after matching.
&gt;wk ' $1\~/Size/ {size+=$2} $1\~/Speed/ {speed=$2" "$3} $1\~/Type/ {type=$2} END {print size " GB " speed " " type} ' &lt;&lt;&lt; "$(system\_profiler SPHardwareDataType; system\_profiler SPMemoryDataType)" That is impressive, and far more efficient than my iteration. If I understand how this works, you are redirecting the output from the \`system\_profiler\` calls to \`awk\`, parsing what is returned, and then printing it in the desired format? 
I thought about this earlier, but I'd like to separate the output AND add a newline at the end. I couldn't figure out how to get \`printf\` to do both. Thanks. &amp;#x200B; &amp;#x200B;
&gt; | \ jq -r '.[] | "\(.ip) \((.ports[].port|tostring ))/\(.ports[].proto)" ' | \ awk -F' ' -v OFS=',' '{x=$1;$1="";a[x]=a[x]$0}END{for(x in a)print x,a[x]}' | \ sed -e 's/,,/: /' it works but its printing ips instead of hostnames.anyway to convert the ips to hostname.
You have a break in the first `if` which is incorrect. How about this? #!/usr/bin/env bash declare -A VERSIONS=([k]="kinetic" [m]="melodic" [i]="indigo" [l]="lunar") function red_msg() { echo -e "\\033[0;31m$*\\033[0m" } function setup_rosrc() { [[ -n "${ROSRC_SETUP}" ]] &amp;&amp; return 0 # already set red_msg "rosrc is not set up! Running setup assistent" echo "Welcome to the rosrc setup assistant" # set shell while :; do # read one character only read -r -n1 -p "Enter shell: [b]ash or [z]sh: " shell &amp;&amp; echo # lowercase shell matches b* or z*? if [[ ${shell,,} != @(b*|z*) ]]; then echo "wrong choice! Options are [b]ash and [z]sh" continue else break fi done # shell should only be a single character but :0:1 makes sure [[ ${shell:0:1} == "z" ]] &amp;&amp; ROS_SHELL="zsh" || ROS_SHELL="bash" echo "Shell set to: ${ROS_SHELL}" export ROS_SHELL while true; do # set ros version echo "Enter ROS version: " for v in "${VERSIONS[@]}"; do echo -e "\\t[${v:0:1}]${v:1}" done read -r -n1 -p "Your choice: " version &amp;&amp; echo version=${version,,} if [[ -z ${VERSIONS[${version:0:1}]} ]]; then echo "Invalid Choice: ${version,,}" continue else break fi done export ROSRC_VERSION=${VERSIONS[${version:0:1}]} echo "Version set to: ${ROSRC_VERSION}" export ROSRC_SETUP=true }
What about in a case where you have empty slots? `awk` would then return `empty; e.g.: Memory: Memory Slots: ECC: Disabled Upgradeable Memory: Yes BANK 0/DIMM0: Size: 4 GB Type: DDR4 Speed: 2400 MHz Status: OK Manufacturer: 0x802C Part Number: 0x344154463531323634485A2D3247334532202020 Serial Number: 0x25213062 BANK 0/DIMM1: Size: Empty Type: Empty Speed: Empty Status: Empty Manufacturer: Empty Part Number: Empty Serial Number: Empty BANK 1/DIMM0: Size: 4 GB Type: DDR4 Speed: 2400 MHz Status: OK Manufacturer: 0x802C Part Number: 0x344154463531323634485A2D3247334532202020 Serial Number: 0x25213024 BANK 1/DIMM1: Size: Empty Type: Empty Speed: Empty Status: Empty Manufacturer: Empty Part Number: Empty Serial Number: Empty\`
Sorry, no hostname in the JSON :)
I’ll comeback to you, once I’m home. I’ll edit the code from there Yeah, I inserted Standard markdown in the editor, and thought it would be converted just fine. Now I know better. 
He has the `while true` to prompt again if they make an invalid choice.
try `DATE=$(date +"%Y/%m/%d/")` note the trailing (suffix) forward slash
just add a check for the column to not be `Empty`, like awk ' $1~/Size/ &amp;&amp; $2!~/Empty/ {size+=$2} $1~/Speed/ &amp;&amp; $2!~/Empty/ {speed=$2" "$3} $1~/Type/ &amp;&amp; $2!~/Empty/ {type=$2} END {print size " GB " speed " " type} ' which returns `8 GB 2400 MHz DDR4`
yes
I use [jshon(1)](http://kmkeen.com/jshon/).
Does ${DATE} work?
Thank you! This is combination with the comment below fixed it. Thank you so much!
Thanks! This with the comment above fixed it. Thanks so much!
Yep no worries - ${VAR} is generally the safe way to call variables in bash. 
If that # character was in the URL, I think it would end the command and make the rest of the line a comment. Unlike wget, curl does not URL-encode such characters for you, that # would need to be replaced with %23. I had similar problems with &amp; and used single quotes to avoid them, but that would prevent variables from working.
Not sure there is any need to use eval, try just removing that Also you can combine your conditions rather than running nested if statements I assume the Nas will wake again when a new request comes in? Seems like a pretty cool idea, you should stick it on GitHub if you get it working
To replace consecutive spaces and leave just one of them, do this: tr -s " " To freaking get rid of all of them, do this: tr -d " "
I can only think of something with steps: sentence='this is a test (a mei zing) one two' sed -r "s,\((.*)\),($(echo $sentence |sed -r 's,.*\((.*)\).*,\1,g'|tr -d ' ')),g" &lt;(echo $sentence) #this is a test (ameizing) one two ​ &amp;#x200B;
maybe `grep -Hrnw --col "reddit" /home/i1/` can suit your needs It display the file name when grep found an occurrence.
Will the beginning/end-marked sequences nest? If so, what behaviour are you after? Will the blacklisted character ever be one of the beginning/end-characters? Will they span newlines or will they only appear on a single line? Depending on the answers, your solution will either be pretty straightforward, or pretty darned complicated.
#!/bin/bash sentence="$1" parens=$(echo "$sentence" | cut -d "(" -f 2 | cut -d ")" -f 1 | sed 's/ //g') output=$(echo "$sentence" | sed "s/\((.*)\)/($parens)/") echo "$output" exit 0
Alright, thank you for the help!
Using Bash regular expressions and no external tools: # Assign string to variable str='This a test sentence (or it could be called a line) to explain what I want to do.' # Regular expression with three capture groups, the second of which is the contents of between the parentheses re='(.*)(\(.*\))(.*)' # Match string with regular expression [[ $str =~ $re ]] # Print all matches; the first field of BASH_REMATCH is the complete match, the other fields are the capture groups printf '%s\n' "${BASH_REMATCH[@]}" # Output: # This a test sentence (or it could be called a line) to explain what I want to do. # This a test sentence # (or it could be called a line) # to explain what I want to do. # Print sentence, use parameter expansion to remove all spaces from the part between the parenstheses printf '%s%s%s\n' "${BASH_REMATCH[1]}" "${BASH_REMATCH[2]// }" "${BASH_REMATCH[3]}" # Output: # This a test sentence (oritcouldbecalledaline) to explain what I want to do. 
Pure Bash: #!/bin/bash SENTANCE="This a test sentence (or it could be called a line) to explain what I want to do." MID="${SENTANCE#*\(}" MID="${MID%\)*}" NEW="${SENTANCE%\(*}(${MID// /})${SENTANCE##*\)}" echo "${NEW}"
yes, exactly. 
That's a little above my bash skills! Thank you! &amp;#x200B; Can I ask you a couple of questions about it? what does \`${version,,}\` do? what's the \`,,\`? what's this "technique" called: \`"\\\\t\[${v:0:1}\]${v:1}"\`. Regex? excuse the nooby question... &amp;#x200B;
Exactly tr or just simply with sed ...
Numeric operators to the `[[` command in bash (`-eq`, etc.) take arithmetic expressions. In arithmetic expressions, dollar signs are not needed. You can even do things like `[[ a+3 -eq 18 ]]`. Though it is probably better and easier to understand to use pure arithmetic syntax instead, using `(( ... ))`, so you can use proper C-style arithmetic operators instead of the archaic `[[` options syntax, e.g. `((a+3 == 18))`. 
Sure! The `${variable,,}` converts the variable to lowercase. You can convert a variable to uppercase using `${variable^^}`. e.g. variable="HeLlO" echo ${variable,,} # hello echo ${variable^^} # HELLO The purpose of converting to lowercase is so that we don't have to worry about whether the user entered a capital letter or lowercase letter when comparing their choice to the valid choices. The `echo -e "\\t[${v:0:1}]${v:1}"` is just outputting the list of version choices (\[k\]inetic, \[i\]ndigo, \[m\]elodic and \[l\]unar) * The `-e` flag to echo enables interpretation of backslash characters. The "\\t" is just a tab. * The `[]` brackets are just duplicating the formatting in your original output e.g. `[k]inetic` * The `${v:0:1}` is string slicing (aka [substring extraction](https://www.tldp.org/LDP/abs/html/string-manipulation.html)) String slicing works like this: variable="hello world" echo ${variable:0:1} # h echo ${variable:0:5} # hello echo ${variable:5} # world echo ${variable:0-1} # d The first number after the `:` is the starting point in the string (from 0). The second number is the end point. If the second number is omitted it goes to the end of the string. The \`:0-n\` syntax starts from the end of the string. Just to cover a couple of other things in the script: The following statement creates an associative array indexed by the first letter of each version choice where the values are the full version name. declare -A VERSIONS=([k]="kinetic" [m]="melodic" [i]="indigo" [l]="lunar") The following test is checking to see if the version selected (`${version}`) is defined in the array. If it is, we know it's a valid choice - so we break out of the loop. (The string slicing here is really unnecessary because `${version}` should be only one character since we passed `-n1` to `read`) [[ -n ${VERSIONS[${version:0:1}]} ]] &amp;&amp; break The `@(string|...|...)` syntax allows you to compare a string against multiple potential matches e.g.: ~$fruit=grape ~$[[ ${fruit} == @(apple|pear|grape) ]] &amp;&amp; echo "Matched!" || echo "No Match!" Matched! ~$fruit=peach ~$[[ ${fruit} == @(apple|pear|grape) ]] &amp;&amp; echo "Matched!" || echo "No Match!" No Match! I highly recommend using [shellcheck](https://github.com/koalaman/shellcheck) for validating your shell scripts. It offers detailed information for each error/warning on the shellcheck [wiki](https://github.com/koalaman/shellcheck/wiki/SC2068). There are instructions for integration with a [variety of editors](https://github.com/koalaman/shellcheck#in-your-editor).
oh thanks, that explains it :)
Try separating the commands like this: https://hastebin.com/atasositiq.bash
Technically speaking, this oneliner do the job done. Unpopular opinion (maybe?) in this sub - why not use ansible for this? Or there are some os specific limitations (like no python) on the servers?
wouldnt that be twice as many connections though? but it is worth a try I guess, thanks! 
Can you provide a prototype of your sw alias/wrapper? Do you pass all the arguments from the wrapper to the original ssh command? It should be something like this if it’s a func (sorry for the formatting, the app is really terrible at it): sw() { ssh -o &lt;whatever=value&gt; “$@“ }
I will try this, thanks!
I have the documentation for it at work, it is pretty bare bones though... I will get back to you
thanks it's for school though so the results are provided Nmap scan report for [192.168.1.206](https://192.168.33.206) Host is up (0.040s latency). Not shown: 982 closed ports PORT STATE SERVICE 21/tcp open ftp 80/tcp open http 135/tcp open msrpc 139/tcp open netbios-ssn 443/tcp open https 445/tcp open microsoft-ds 1030/tcp open iad1 1037/tcp open ams 1038/tcp open mtqp 1042/tcp open afrog 1521/tcp open oracle 2030/tcp open device2 2100/tcp open amiganetfs 3372/tcp open msdtc 3389/tcp open ms-wbt-server 4443/tcp open pharos 7778/tcp open interwise 8080/tcp open http-proxy MAC Address: 00:50:56:AF:78:86 (VMware) &amp;#x200B; I'm supposed to write a bash script to parse through all the data and display it in that specific manner that I put in the post but I'm completely stumped and it's due tomorrow
Ahhh, gotcha! Well your current script is a one-liner, which is probably possible to do... but ugly. You'll want to [iterate](http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-7.html) over the data and store it in an [associative array](http://clubmate.fi/associative-arrays-in-bash/). That first link is full of good bash foo...
awesome!!! thank you!!
okay, i just learned i cant use xmllint or xmlstarlet, i need to use pure bash :( At the moment the script looks like this: cat xml.txt; j=!:1; for i in $( xmlstarlet sel -t -v '/spaces/space/device/lvm/raids/raid/@path' $j ); do echo -e \\n$i $( sed -n 's/[^\"]*\"\([^\"]*\)\"[^\"]*/\1 /gp' $j | cut -d " " -f2); done the output i have now is: /dev/md2 UTF-8 /volume1 X3BAVZ-PqyQ-BNvJ-mYaf-9lAR-Up24-icJSsM 11a49b00:599c89fe:09b6521c:2cea662f /dev/sdma6 /dev/sdmb6 /dev/sdmc6 /dev/sdna6 /dev/sdnb6 /dev/sdnc6 /dev/sdoa6 /dev/sdob6 /dev/sdoc6 /dev/sdpa6 /dev/sdpb6 /dev/sdqb6 /dev/sdqc6 /dev/sdra6 /dev/sdrb6 /dev/sdrc6 /dev/sdsa6 /dev/sdsb6 a11476dd:d11720f9:3ae54a77:a2fdb1eb /dev/sda5 /dev/sdb5 /dev/sdc5 /dev/sdd5 /dev/sde5 /dev/sdf5 /dev/sdg5 /dev/sdh5 /dev/sdi5 /dev/sdj5 /dev/sdma5 /dev/sdmb5 /dev/sdmc5 /dev/sdna5 /dev/sdnb5 /dev/sdnc5 /dev/sdoa5 /dev/sdob5 /dev/sdoc5 /dev/sdpa5 /dev/sdpb5 /dev/sdqb5 /dev/sdqc5 /dev/sdra5 /dev/sdrb5 /dev/sdrc5 /dev/sdsa5 /dev/sdsb5 /dev/vg1000/lv how can i safely remove a11476dd:d11720f9:3ae54a77:a2fdb1eb, a11476dd:d11720f9:3ae54a77:a2fdb1eb and /dev/vg1000/lv ? And how would i replace the first xmlstarlet?
If you delete the directory you are in??? ```bash ~$cd /tmp tmp$mkdir d tmp$cd ./d d$rmdir ../d d$pwd /tmp/d d$echo ${PWD} /tmp/d d$cd .. -bash: cd: ..: No such file or directory d$ls -ald /tmp/d ls: cannot access '/tmp/d': No such file or directory d$[[ -d /tmp/d ]] || echo "MIA!" MIA! d$[[ -d . ]] &amp;&amp; echo "I'm still here!" I'm still here! ``` 
&gt; If you delete the directory you are in??? `pwd` doesn’t fail in that case, though. It’s arguable whether the result is correct or not, but the command itself still succeeds.
&gt; /bin isn't in the PATH variable Doesn’t matter if the command is called by its absolute path (`/bin/pwd`).
The problem is ffmpeg itself is quite fussy with the parameters. Now I realize I have misquoted the example. Because ffmpeg needs something like ```ffmpeg -input_file_parameters -i $INPUT_FILE -output_parameters $OUTPUT_FILE``` so, in the end, I have to break the $@ into $1 and $the_rest
Oof. Yeah. My bad, it's been a long day today. I readit as him asking specifically *that* command not if it was called absolutely. I'll change it now. 
It's contrived because I've been given that question as a part of the studying bash I have to do, as a practice.
I have been given that question as a practice, that's why it's precisely with bin/pwd and why it seems so "unnatural" in general.
Not sure exactly, but are you sure you have the necessary permissions?
yeah i remounted the drive with: mount -o remount,rw /media/root/$drivename
I think we might need more info. What is actually mounted at /media? A CD? USB device? Is "/example/dir" exactly as the find path? At what point in the live boot is this? Have you made the partition table yet? 
Might have to chown -R user:user /whatever/directory Before you can delete it, replace user with your username
So you're asking the internet to do your homework for you...
Is something recreating them?. 
Two usbs, And the windows OS drive. No, its just an example. Booted to desktop and running terminal. No. Thanks for this
Nope
Have you enabled extended globbing?
You should enable the extglob option for more powerful globing (shopt-s extglob / shopt -u extglob, for enabling/disabling respectively) Check man bash for more info on extended globing. Reddit app is really awful in case of giving examples...
Yes. If you have set that then it should work. What do you mean by it doesn't seem to be working?
I moved globbing to enable right before the cp command with the wildcard, and now I'm getting this error: /usr/bin/FlashBack: line 33: syntax error near unexpected token `(' /usr/bin/FlashBack: line 33: ` cp -r -f "/var/mobile/Library/Preferences/"!(com.apple*).plist $backupLocation/Preferences/' Is it formatted wrong? I don't see an issue with it
EDIT: Fixed it. I moved it to enable at the very beginning of my script. Thanks!
You enabled globbing but you changed your command. Use the command then you were originally using, because in this one you have the asterisks wrong.
Sadly doesn't solve my problem
This one actually works, Thank you! I am bit confused how is it different from mine though... :/ The principle behind my script and your script should be the same
I just use one liners with keybindings. volup = "amixer -q set Master 2dB+ unmute" voldown = "amixer -q set Master 2dB- unmute" volfull = "amixer -q set Master 100%" volmute = "amixer -q set Master toggle"
&gt; amixer -q set Master 2dB+ unmute pactl set-sink-volume @DEFAULT\_SINK@ 2dB+ same thing, if you want risk your hearing. Btw. does alsa limit max volume when you increase it that way?
You'll find that Python is very easy to pick up as a scripting language, and if you get the hang of the subprocess module you can do the same things that you do in BASH scripts. Just a thought if you have any free time coming up, I've found learning Python to be very very useful.
`/tmp` is a tmpfs on most distributions these days, I think, so this technically satisfies your requirement: { program-1 &amp; program-2 &amp; program-3 &amp; } &gt; /tmp/output wait cat /tmp/output You can also pipe your programs into `tac | tac`, which doesn’t *look* like it writes anything to disk. (`tac` actually uses a temporary file, but the temporary file usually goes to `/tmp`, so it’s the same as the above version, really.)
yes would want to run the programs in parallel
Would love to write to tmpfs but on the host are no accessible tmpfs mounts for me and I do not have root.
The lines you want to run, are they all the same program, just called with different parameters? If that's the case, check out a tool named `parallel`. It's pretty easy to use and has great documentation. It will take care of capturing and sorting the output of the forked programs.
No. Anything you can download in a browser, you can download with cURL. 
Seriously? Was not aware. Thx.
Instead of dumping the human-readable format into the output file and trying to make it machine-readable, why not choose one of the machine-readable output formats when you generate the file? 
Posted my bash script
http://man7.org/linux/man-pages/man1/lsattr.1.html
so what I'm gonna do with that webpage? there's no immutable bit on that webpage
I do not have root to change /tmp or add a tmpfs myself:D
This works very well, thanks. Also made it a lot simpler
I know that /tmp has sticky bit by default. My first reply with 'I do not have root' I was implying that I do not have root permissions to change the mountpoint into a tmpfs since my goal is to not write to disk and not that I am unable to write to /tmp.
/tmp usually already mounted as tmpfs 
I'm not really available for chat. But lsattr and chattr are the commands to look at to see and or change if the immutability bit is swt. You can Google immutability, but it's a way to set a file to prevent it from being deleted. 
Another way to filter lines by its absolute number is: sed -n '4,103p' file.txt
I like [The Linux Command Line](http://linuxcommand.org/tlcl.php) (more general than just shell scripting) and the [BashGuide](https://mywiki.wooledge.org/BashGuide) (*the* Bash best practices resource, in my opinion).
http://wiki.bash-hackers.org is interesting once you start picking things up.
`$ man bash`
You can assign the output of your programs into a variable, of course. out="{ program-1 &amp; program-2 &amp; program-3 &amp; }" But you probably want output lines to be atomic so text from lines are not mixed together. If you really want to avoid touching a file system (odd requirement), consider moreutils timestamp (ts) command, prepending a high-precision timestamp to each line, than cut(1)ing the timestamps out after a numeric ascending sort. &amp;#x200B; out1=" $(program-1 | ts) &amp;" out2="$(program-2 | ts) &amp;" out3=$(program-3 | ts) &amp;" wait echo $out1 $out2 $out3 | sort -n | cut -d " " &amp;#x200B; I have not tested this idea. &amp;#x200B; &amp;#x200B; &amp;#x200B;
What you posted as your solution looks to work fine, though personally I would run it against the 'mount' command instead of df. mount | grep 'on / ' | awk '{print $1}' Other than that, your command works for me.
I would avoid parsing the output of df. Here's a bash only method to get the info from proc. ```bash #!/usr/bin/env bash while read -r mount; do set -- ${mount} [[ $2 == "/" ]] &amp;&amp; echo $1 &amp;&amp; break done &lt; /proc/mounts ``` 
This is must better way, I'll modify my script to match this. I guess the next question would be is there a better way to find the name of the second partition of the root block device (in my case root is /dev/vda, and second partition is /dev/vda2). There's a possibility the image could be xvda or something, in which case this wouldn't work. 
This is where we start making assumptions about what the OS and FS look like. Given that you're handling LVMs, we can use a different set of commands. `pv` for instance will show us the Physical Volumes configured on the server and we can get the block device from that and it doesn't matter whether it's /dev/sda, /dev/xvdb or w/e. The specifics of what you would search for depend on how many different ways you're likely to configure it so ymmv. But we can do this. In this case it is assumed that your root volume group is `vg0` and your root logical volume is `root`. If all your servers are bootstrapped from the same image, then presumably hardcoding the root LVM isn't an issue. That then gets us this: runcmd: pvdev=$(pvs | awk '/da2 / {print $1}') pvresize $pvdev lvmresize -r $lvmdev -l +100$FREE /dev/mapper/vg0-root
You're right, I appreciate your response by the way! The expectation is that in this cloud-enabled images everything will be done automatically. My first step is attaching simply the root with an expanded size, and having it auto-resize at the time of boot (with LVM of course). Your line pvs | awk '/da2 / {print $1} works great, it returns the PV for /dev/vda2 which is what I want. Now I guess step 2 will be attaching another volume at the same time, and having cloud-init either format this as another partition and attaching it elsewhere, say /mnt/foo or tacking it onto the root. I think putting it on a secondary mount point is what I'm looking to do here. When I issue pvs on my cloud-image there's only 1 PV, can I just filter for that on it's own? 
&gt; When I issue pvs on my cloud-image there's only 1 PV, can I just filter for that on it's own? Not sure what you mean by that past what we already achieved? Also, if you add a secondary disk, it won't necessarily have been formatted as a PV already, so wouldn't show up anyways.
lol
I'm not sure if it is distribution dependant but you can normally do the following : ``` df -P /dev/disk1/|sed '1 d' ``` the `-P` is here to force output to be on one line. Only the line you want will be printed hth
did alot of trying around and could not fix it with regex. But imo the easiest way would be: `for i in $(ls -1|grep -vi apple); do &lt;your operation&gt;;done` &amp;#x200B; Greets
Hey, Trudels42, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Does this here work on CentOS? $ findmnt --nofsroot --noheading --output SOURCE / /dev/sda2 
What about using pssh? Deploy to $n amount of hosts in parallel till the complete host list is processed 
Awesome! Any guidance on getting this installed on MacOS?
Yup. [MacOS instructions](https://github.com/dvorka/hstr/blob/master/INSTALLATION.md#macos) 
Got it. I can relate to being asked to manage a codebase that isn't ideal. I've never built any web apps that used off-site scripts. Does the browser complain when the URL protocols don't match? ( I thought it only did that for resources of the same site )
you should pipe the output into `awk` which will split up your string into something like an array. might look something like this: `&lt;command to produce the output&gt; | awk '{ print $5 }'` Greets
Unless this is being used in a Dockerfile or something I would suggest using [ansible](https://www.ansible.com/) (or another configuration management system). Ansible has the advantage of being idempotent. This allows your playbook to be modified and ansible won't attempt to apply changes that have already been applied. The `\` is unnecessary (but doesn't hurt anything - and I think it improves readability). The following two snippets produce the same output. echo foobar | tr '[:lower:]' '[:upper:]' &amp;&amp; date echo foobar | \ tr '[:lower:]' '[:upper:]' &amp;&amp; \ date
Thank you - appreciated! It's not Docker, and Ansible is on my list to learn over winter when I have the downtime from work -- right now it's a manual copy &amp; paste, which is helping me with visibility and learning the ins-and-outs of server builds.
Holy shit. Are you really asking for help with malware? Bad malware, at that?
Move Ansible up to this week. Seriously. For what you're doing the ramp-up time is measured in hours.
I found it. Non-interactive shells don’t have job control by default. Add `set -m` to the beginning to fix it. (Oddly enough, another workaround I thought about – using `$!` instead of `jobs -x` – does *not* work even when job control is enabled: `$!` always evaluated to the empty string. Not sure why, but I don’t really care, since my problem is solved.)
Alright - I'm convinced. Thanks for the tough love.
If you don't wanna mess with awk, you can use cut. `| cut -d" " -f5`
Looks great! There is also this alternative for zsh users: https://github.com/zdharma/history-search-multi-word
yeah buts its for pentesting. google it
I second Ansible, but also look into "set -e" or "set -o errexit". It causes the script to terminate on an error in any command, which is functionally equivalent to a bunch of &amp;&amp; operators.
To just show a certain amount of lines, pipe the content to `head` or `tail`. To be able to navigate through the output, pipe the content to `less` or `more` (if I remember correctly one of them was an improved version of the other)
Have you heard of /dev/shm ?
You could also try Shift+PgUp, that usually will scroll up a few pages in the buffer. That should work in and out of X.
Better yet just forget about more.
You can press "scroll lock" to enable navigating with the arrow keys. Typing is disabled until you hit "scroll lock" again
&gt; hostname sorry for the ignorance but how do you use this command? i put hostname i get [hint] did you want "--hostname"? i put --hostname i get FAIL: unknown command-line parameter "hostname"
`hostname` is part of the [net-tools suite](http://net-tools.sourceforge.net/). It's already installed on most **Linux** systems. I see you haven't mentioned which OS you're on, and I just assumed you were on Linux. Which OS are you on?
You could tag a -n (no clobber) on the mv, and then check the exit code of that command, if not zero (HTML file preexisting would be one reason), print an error message. I'm not sure how specific mv status codes get. Sorry for lack of formatting, on mobile and in a rush.
This does exactly what you want but the trouble is that the script will stop at the moment when it comes across the file.htm for which file.html already exists, so the files before that will be moved and the files after that will be skipped. If you wanted the move operation to continue after avoiding that file, or if you wanted the move operation to never start if such a case exist then you'll need something like. Maybe you need to describe your requirement in more detail. for file in *.htm; do newfile=`echo $file | sed 's/\(.*\.\)htm/\1html/'` &amp;&amp; [ -f "$newfile" ] &amp;&amp; echo "$newfile already exists" &amp;&amp; exit 1 || mv $file $newfile; done
I've been hacking on bash for fifteen years and I just got some pointers from the "bad one liner dissection" on this site. Thanks for the recommendation!
Why are you using `-E` to turn on extended regex if you don't want to match 1 or more `r`s at the end? Perhaps you really want `-F` to treat them as fixed strings, not as patterns? Also, I'm assuming from this that you're trying to figure out which values of `name` are matched in the file, right?
I can see which values are matched in console: when I use grep -F "${name}" "$FILE" with foo1234bar I machtes foo1234bar and foo1234bar+, but only "foo1234bar" in foo1234bar+ is highlighted, so matched. (no the +) I think I need to match something like "foo1234bar(Any whitespace character here)" to not match foo1234bar+. I thought this is what \S does, but obviously I'm wrong?
I was thrown by not reading carefully enough in the [FreeBSD man page](https://www.freebsd.org/cgi/man.cgi?query=grep&amp;sektion=&amp;n=1) since lists `-P` as a flag for Perl regex, but then notes that FreeBSD does not support that flag (seems like they should just leave it out of the manpage then to me).
Oh, definitely not better in the grand scheme of things. I actually did not know the existence of this one. The only thing that I ~would~ consider better is that it does not copy any files to the remote host. It transfers the function and alias definitions on the fly without using any files on the remote machine. Bottom line though is that I did not hit this one when trying to find such a thing for my personal usage, so I did what was most logical at the time and I wrote my own :)
Can you post the entire script, and where it's being called. Cron, at, etc? What is calling your script to execute.
`grep` prints lines that match a regex. If you want to print single values from a line that matches a regex either pipe to `cut`, or use `awk`, which is probably the better solution in this case. dumpsys telephony.registry | awk '/mServiceState/ {print $?}' # replace ? with a number corresponding to the specific value you want to extract.
The very first thing he wrote was a link to the github project with all the files. 
Damn, you're correct. I completely missed that. Thanks
Thx you :)
Thank you, it works :)
Besides the other answers and depending on your terminal you may be able to simply use the same gestures that scroll your browser.
he could've removed the "" around the URL as well and it should work - like: `curl https://www.metaweather.com/api/location/44418/$DATE/ -o ~/Desktop/wow.txt` 
seems like he is not passing command line options, but adding options after the call over a read 
i'd recommend using Command Line options rather than entering the Option on every run of the Script: usage(){ echo "This is the function you can echo the Help for your script!" exit 1 } # Read the Command Line Options while [ "$1" != "" ]; do case $1 in -s | --server ) shift server=$1 ;; -d | --disksize ) shift disksize=$1 ;; -v | --verbose ) shift verbose=true ;; * ) usage exit 1 esac shift done Since a sample script call would be: `/path/to/script.sh -v -s server1 -d 10MB` the $0 variable always contains the path of the script you called the $1 variable would be the first string you entered after the call of the script - so in the example $1 would contain -v the shift command will "throw away" the actual content in $1 and will fill $2 -&gt; $1 i think my snippet is pretty much self explaining - but ask if you have questions about it
Possibly you need to change this line to read: for user in "$dir"/* Give your code a quick run through shellcheck.net first as well.
The you :)
You'd probably want to curl or wget the page, parse it locally into CSV or Json first, then perform analysis.
I suggest you apply some formatting to your code snippet so that it's easier to see what's going on and indeed if there any syntax errors that's causing your code to misbehave. Also, running this through `shellcheck` catches many subtle bugs. Exactly what's not working? Isn't `counter` increasing? Have you tried `counter=$((counter + 1))`?
Or maybe even (( counter += 1 ))
As another reply said, better formatting of your code would be helpful. Running the following in a terminal runs as expected. COUNTER=1 while [ $COUNTER -le 10 ] do sleep 1 echo $COUNTER ((COUNTER++)) done 
From what I see, user has no value in your script. How are you assigning the user? &amp;#x200B;
Or maybe even `let c++`
dude, this is golden... do you have any other tidbits that you use to debug?
Apparently the rest of us did too, what did they say?
Thx
Thx
Thx
Thank you for the link, I read it. But to do a next line in my code does not work for me. I don't know why.
right now, it's being launched manually via the command line so that I can see the output. Once it's completed I was either going to launch it via KDE at login, or run it in the background automatically (as a service?). As you can probably tell, I'm not very familiar with how linux works (outside of various things I know from trial and error via 2 years or so experience with arch, fedora, and debian flavors)
thanks, I'll implement this tonight
Look into screen for this. You want a terminal multiplexer.
xdotool can be used to automate X. Something like xterm &amp; sleep .2 xdotool type 'make create_server' sleep .2 xdotool key Return ... etc should do the trick
Xterm. -e. Script.sh. &amp;
Happy to help. Also, in case you run into this: use single quote (') if there is a $ in the URL. Otherwise Bash will try expand it as if it was a variable.
You could use "associative" arrays for this problem: declare -A chr chr[1]=yes chr[2]=no chr[X]=yes chr[Y]=no # etc. for a in {1..22} X Y M; do # this "{1..22}" thing actually works, it's not pseudo code if [[ ${chr[$a]} = yes ]]; then echo Processing Chromosome $a... fi done A problem with associative arrays is that they only work with the version of bash that is used on Linux. It won't work with what's installed by default on MacOS for example.
&gt; `for A in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 X Y M; do` No need to feed bash a complete sequence of numbers in order, it knows how to count: $ for a in {1..22} X Y M; do echo -n "$a "; done 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 X Y M
Still struggling to get "correct" uptime. Currently, I'm thinking about creating tmp file at reboot, which will be deleted after 20 mins. While this file is present, cron will not put NAS in hibernation mode. 
run your script through shellcheck (either online or local install) to catch most of the similar problems. 
This did the trick! Thank you very much for your assistance.
I liked your method but u/giigu's way is portable.
Could you [`trap`](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html) the signals and make the script ignore them?
Is it possible to keep the functions and aliases after i have changed to root-user? Also will I be able to run i.e. xmllint, that is not available on the remote machine, but only my host with this?
What If doubling the number results in a 6 Digit number? Should the following Chars Shift to the right in this Case?
I don't understand the purpose of the last line of the function
To be clear, this is code provided by the NVM maintainers and is not produced by me. It should run, and clearly runs for everybody but me it would seem.
If you are doing in-line code you use backticks. For the block of code, you add four spaces before each line (and don’t use backticks).
Did you already verify that the issue doesn't exist if Steam lauches the original Warframe.exe directly? (Without this data point, I'm not convinced that your bash script is the issue.) Why do you need the "Launcher.exe runs Launcher.sh" step? Can you just rename Launcher.sh to Launcher.exe to avoid this step? For step 5, did you try doing just a regular `exec` of Warframe.exe (without any `&amp;&gt;/dev/null` or anything)? If the issue really is Steam being confused about something, it seems like this combined with the previous suggestion would be the safest way to avoid that, because it means that the process that Steam originally forks eventually becomes the real Warframe.exe process.
"Did you already verify that the issue doesn't exist if Steam lauches the original Warframe.exe directly? (Without this data point, I'm not convinced that your bash script is the issue.)" -Steam still closes it even without my bash script (mentioned in original post) "For step 5, did you try doing just a regular exec of Warframe.exe (without any &amp;&gt;/dev/null" yes, my original line was just an exec without &amp; or anything after 
your input does not match your 100-105 character range (it should be around 124-131), I am guessing that was an approximation? I so and the column is the next to a BG or EA number then this should work awk '{ for(i=1;i&lt;NF;i++){ if($i=="EA" || $i=="BG"){ i+=1 a=b=$i; a+=1; if(a-1==b){ #tests if it is a number i+=1 a=b=$i; a+=1; if(a-1==b){ #tests if it is a number gsub($i,$i*2,$0) }else{ i-=2 } }else{ i-=1 } } } print }' The else are just in case you have some around that string group (3rd column)
It looks that the Exonerator page can have parameters passed as part of it's URL, so that makes this pretty straight forward. So you could use a little tool like this: #!/bin/bash ip=$1 date=$2 lang="en" base_url="https://metrics.torproject.org/exonerator.html" if [ "$2" == "" ]; then echo "Usage: $(basename $0) ip_address iso_date" exit fi response=$(curl -Li "$base_url?ip=$ip&amp;timestamp=$date&amp;lang=$lang" 2&gt;/dev/null) status_code=$(echo "$response" | awk '/^HTTP/ {print $2}') if [ "$status_code" == "200" ]; then result=$(echo "$response" | grep 'class="panel-title"' | sed -e 's/ *&lt;[^&gt;]*&gt;//g') exit_code=0 else result="Error" exit_code=1 fi echo "$ip,$date,$result" exit $exit_code This takes two positional arguments, IP Address and ISO standard date. For example: $ ./exonerator_lookup.sh 86.59.21.38 2017-12-31 86.59.21.38,2017-12-31,Result is positive What it does is construct a [`curl`](https://linux.die.net/man/1/curl) request to send a web request to the exonerator server, as if you navigated there in your browser. We provide it with the parameters you want to look up and get the resulting page back, but on the command line. The script then checks that the [HTTP response code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) indicates a successful request and if that is the case, extracts the line from the HTML stored in the `$response` variable that that contains the result from the lookup. All of this then gets printed out at the end in CSV format, ready to be ingested by whatever loop you put around it. Be polite with this though and don't smash the service, otherwise they may choose to block this sort of request in the future. 
Nice tip, thank you. 
Do you ha e spaces in your file path? I'm not in front of my computer right now but I remember something about dirname having issues with spaces 
https://ia802309.us.archive.org/25/items/pdfy-MgN0H1joIoDVoIC7/The_AWK_Programming_Language.pdf
anything in particular?
It's so that the function spits out a return code.
Before I made 4 spaces for each line of code. And I did put 2 spaces at the end of line. Maybe i have to use notepad++ but I'm on Android.
Not to piss in anyone's cereal, but you are aware of tools like Ansible, Puppet, and Chef, right?
Great guess but spacing is not an issue at this point.
Excellent point, I do have an update waiting to install. I'll give that a shot this afternoon to see if it fixes the issue. In the mean time, output of `bash -x:` + NVM_SCRIPT_SOURCE=/bin/bash + '[' -z '' ']' + export NVM_CD_FLAGS= + NVM_CD_FLAGS= + nvm_has unsetopt + type unsetopt + '[' -z /Users/teacher/.nvm ']' + unset NVM_SCRIPT_SOURCE + nvm_process_parameters + local NVM_AUTO_MODE + NVM_AUTO_MODE=use + nvm_supports_source_options ++ nvm_echo '[ $# -gt 0 ] &amp;&amp; nvm_echo $1' ++ command printf '%s\n' '[ $# -gt 0 ] &amp;&amp; nvm_echo $1' ++ . /dev/stdin yes + '[' _yes = _yes ']' + '[' 0 -ne 0 ']' + nvm_auto use + local NVM_MODE + NVM_MODE=use + local VERSION + '[' _use = _install ']' + '[' _use = _use ']' ++ nvm_resolve_local_alias default ++ nvm_echo ++ command printf '%s\n' '' + VERSION= + '[' -n '' ']' + nvm_rc_version &amp;#x200B; &amp;#x200B;
Though I have used those products and they are great they definitely have an overhead that's a bit much for a hobbyist or new programmer to work with. I needed simple deploy/backup scripts that I could read / teach to a new programmer in an afternoon so they could feel like they have some control over their systems, not a product to develop an industry around. Also my OCD about what I am installing goes crazy when I work with those things. Just checking the puppet source code quickly I can\`t even find the entry to the application and RUBY is not a standard Debian include. It was worth the week to make this. I am looking forward to making the video on it use.
&gt;*Also, you might want to look at* &gt; &gt;j*man run-parts* &gt; &gt;*because it looks like you're trying to reinvent it with that* &gt; &gt;*for* &gt; &gt;*loop.* I got the idea from the for loop [here](https://stackoverflow.com/questions/41079143/run-all-shell-scripts-in-folder) &amp;#x200B; they said something about run part not being the most ideal solution due to it not executing scripts ending in .sh (unless I misinterpreted).
&gt; they said something about run part not being the most ideal solution due to it not executing scripts ending in .sh (unless I misinterpreted). I'd say they have it backwards. Putting an .sh extension on your scripts is not an ideal solution because it can break things like run-parts. https://askubuntu.com/questions/503127/should-i-save-my-scripts-with-the-sh-extension
so I added `echo "Called by PID $PPID: $(&lt; /proc/$PPID/cmdline)"` and noticed that both instances were being called by the same script. I then added some echo statements into the monitor\_screen\_locks.sh script and apparently this is the bit of the code that is getting lauched twice: &amp;#x200B; export $(grep -z DBUS_SESSION_BUS_ADDRESS /proc/$(pidof -s dbus-daemon)/environ) expr='type=signal,interface=org.freedesktop.ScreenSaver' # DBus watch expression here dbus-monitor --address $DBUS_SESSION_BUS_ADDRESS "$expr" | \ while read line; do case "$line" in *"boolean true"*) echo "session locked" log session locked SECONDS=0 lock_task ;; *"boolean false"*) log session unlocked echo "session unlocked" echo "$SECONDS"&gt;"$seconds_file" unlock_task ;; esac done &amp;#x200B;
Hmm, if you're going to make it distro-agnostic then you might be better off with the for-loop... the guys in your link did have a good point about run-parts not being available on every distro. 
You do whatever works for you. But it does feel like you are reinventing the wheel.
Hey there CloverboxFox, this is all awesome stuff. These are a great start. You need to understand the basics of how to configure an application, the various bits of lego you need to glue together to produce a useful server as the end result and you can only really get there by going through this sort of thing. However, its unsustainable for a number of reasons: * more than a handful of servers and this becomes impossible to manage * its brittle and will likely break after an update of o/s version change * its brittle and can't be used on another o/s (non-portable) * extending this to cope with new features or configurations might be more effort than its worth Do yourself a favour and use these scripts and your experience to port the logic over to a tool dedicated to doing this exact thing. I'd start with ansible as its reasonably quick to get going with it and requires no infrastructure (agents) to support it. Just your terminal and ssh is all thats required.. after installing ansible of course. It will also ensure that the skills you get are immediately relevant. A script is so 'hardcoded' to a particular set up that you likely will struggle in any new place. It is also now pretty much industry standard to use a tool-set to orchestrate hosts. From ansible or salt for servers to things like xcat for HPC clusters of thousands of machines to kubernetes for larger installations or a more devops approach to the problem. It changes your mindset and the sooner you get it, the better off you will be and the more fun you will have. Also, tar gzips in a git repo make it really hard to quickly evaluate the files. You're on the right track.. jump in and immerse yourself. Splash around in mud with the rest of us.
Seriously. It is maddening. I can feel my fragile sanity collapse just thinking about it.
Take a look at http://mywiki.wooledge.org/BashGuide. Sounds like you've been around the block already.
That the project files were posted at the very beginning of his post.
&gt;export $(grep -z DBUS\_SESSION\_BUS\_ADDRESS /proc/$(pidof -s dbus-daemon)/environ) expr='type=signal,interface=org.freedesktop.ScreenSaver' # DBus watch expression here dbus-monitor --address $DBUS\_SESSION\_BUS\_ADDRESS "$expr" &amp;#x200B; the output of the above command is: signal time=1535764411.025132 sender=org.freedesktop.DBus -&gt; destination=:1.508 serial=2 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameAcquired string ":1.508" signal time=1535764411.025174 sender=org.freedesktop.DBus -&gt; destination=:1.508 serial=4 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameLost string ":1.508"
Shell is designed primarily as glue for other processes, so if you have a script that loads data to a database, there may be associated programs that fetch the data and parse the logs to notify others of performance metrics such as how many records were added, updated etc. That's how I view the strengths of bash. So in cases such as that you might save email addresses in [arrays](http://tldp.org/LDP/abs/html/arrays.html) or you might need to [prevent processes from running concurrently](https://plus.google.com/+RandalLSchwartz/posts/QcrqvT3mUdy). There are tricks for [cleaning up](https://www.linuxjournal.com/content/use-bash-trap-statement-cleanup-temporary-files) of [temporary files](https://www.mktemp.org/manual.html) or [adapting to errors](https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/).
First and foremost learn about the `man` command. It will show a manual for most command line programs. Second, learn which commands are bash-built-ins just so you know what is, and is not Bash `man bash-builtins` These are not going to have man pages unless there is a separate program with the same name. `type -a echo` shows that echo is both a built-in command and a program. Notice some extremely common programs such as `ls` are not actually part of Bash, but of course they are invaluable I find the most common commands/programs I use are: awk df du cat cd chmod chown cp cut diff eval exec fdisk find grep ifconfig (or ip on some newer releases) less mkdir mount mv ping printf ps pwd rm sed sleep sort tail tar test time wget For remote systems: rsh rsync ssh This is my favorite site for understanding bash: http://www.tldp.org/LDP/abs/html/ That site shows you how to do just about anything in bash. Arrays, loops, regex, functions, you name it. If you have a need for speed in your scripts, check out bash-built in ways to do things. Although they will sometimes be more limited than calling an external program such a `cut`, `sed` or `awk`, it can be much faster to do in pure bash wihout launching those external programs. Some common things that can often take place of **simple** calls to `cut`, `sed`, `awk`. https://www.tldp.org/LDP/abs/html/parameter-substitution.html https://www.tldp.org/LDP/abs/html/string-manipulation.html If you have something complex to parse, then those programs are great. Benchmarks are your friend.
|
Just updated to macOS 10.13.6 and still encountering the same bash error : /
Does bash have an implicit return for the last line of a function? Like Ruby or JS arrow notation functions?
I'm not familiar with Ruby/JS arrow notation functions, but to my knowledge `bash` functions have an implicit return that is the exit code for the last executed command within the function. Function return codes can of course be made explicit by using `return`, say for example you wanted to break out of the function on a particular condition, or you want to invert the logic etc Disclaimer: I have not had caffeine yet. I might have donked-up words.
well I've managed to fix the issue after looking at the output for a while(apparently both `path=/ScreenSaver` and `path=/org/freedesktop/ScreenSaver` were returning a boolean value. so I specified the path to be `/org/freedesktop/ScreenSaver` thus fixing that issue. if I changed the specified path to `/ScreenSaver` and changed the interface to `*.ScreenSaver` could I essentially use this with any screen locker? &amp;#x200B; I know gnome's screen locker is similarly named.
That sounds about right. Ruby and JS arrow notation works the same way. Keyword return to have explicit control else the value of the last line of a function is implicitly returned. Thanks for the info! :) Now grab some coffee
`cron` runs with different environment settings. Most problems like this can be resolved by declaring the `$PATH` or full-pathing each command e.g. instead of `rm something`, you need to use `/bin/rm something`
&gt;What kind of maniac puts gzipped tarballs in a git repo? Pull the repo vim edit in a new key and run the SEND install script and its good to go. If the files are not tared all of a sudden you have to go root or have some kind of fakeroot environment to work from. 
I think that this breaking at the points you have mentioned is a sign of good design. &amp;#x200B; This is definitely going to be the reasons to move on page of the instructions.
I found a lot of good looking sports cars for the task but no solid carts. Being minimalist I did not even want to create a new user on a system I was working with su - www-data -c /bin/bash from root brings me into www-data and lets me work without worrying about permissions and things like that or having to remove a user etc. when closing up the server for production. Write some HTML/JS get the first revision up clone the box delete the tools I was using and as long as my base system is secure I know I didnt break/change a single thing.
Thank you, I will give this a go :) 
I used `rm` specifically, because it is not full pathed in the script I glanced at. Looking at it now, `expr`, `grep` and others are not full pathed either: https://github.com/skewballfox/pomodoro_lock/blob/master/monitor_screen_locks.sh
ah, turns out I was misinterpreting you. I thought you meant the initial running of the script in cron. I'll make the necessary changes and post again the problem persists
Use of a user D-Bus instance from a Cron job is dicey at best. There's no guarantee that the user will even have a D-Bus running, or that its socket will be available from outside any of the user's sessions. (As a security measure D-Bus could, for instance, ignore connections from processes not in any of the user's sessions.) And anyway, your script just assumes that there the "first" process named `dbus-daemon` is the right one. That's definitely broken the moment two people are logged in, and I can't see how it would ensure you don't pick the system D-Bus instance. Find some other way to do what you need to do.
what do you think would be best method to go about this?
btw would the issue I'm currently experiencing related specifically to running it via a cronjob? would I still have an issue of the script not staying active if I, for example, ran it from at login via KDE? &amp;#x200B; While that solution, if it works, wouldn't solve an issue with it being something that isn't very reliable in terms of a multiuser environment (something I would want to implement in a future version), it would solve the immediate issue: me needing a screen locker so that I can force myself to step away from the computer in a regular time frame.
If it's run from within the user's KDE session it should work, since that is within a session that has access to the user's D-Bus instance. And better yet, you don't need any crazy logic to determine the D-Bus address... just use `dbus-monitor --session`.
also what resources would you recommend(books or websites) to better understand how to solve this issue? I want to have some distro/user agnostic way of monitoring lock and unlock events (something that runs at startup for that user and launches scripts in relation to the events for that user with the ability to toggle various launch scripts by command). I know my current approach is nowhere near that.
Adding absolute paths to commands just adds more maintenance, and makes it less portable. Better to adjust PATH either in the script, in the cronjob, or in the crontab itself.
If it requires dbus, have it run during login instead of from cron. Look up xdg-autostart
&gt; I'm writing a little node script This is /r/bash. Are you lost?
&gt; My issue is with sshpass and sftp commands. I don't know what other sub would be better, so i'm posting here. To me it sounds like an issue on the Node side. You've already shown that sshpass and sftp work correctly when you run them from a shell.
The `sftp` manpage mentions that `-b` should really only be used with non-interactive authentication. I'm not entirely sure of the reason for that, but it's probably because it does something dumb like simply reopen standard input on that batch file.
I've been working under the assuption that sshpass -p was non interactive but now that you mention that, sshpass does scan for a password prompt and handles that, and it occurs to me that is probably considered interactive. 
It runs its command inside a pseudo-TTY, allowing regular keyboard-interactive authentication to be faked up.
I have a job that uses sshpass and sftp; this is the syntax that works for me: sshpass -p 'hunter2' sftp -oBatchMode=no user@host &lt;&lt;-EOF put localPath remotePath EOF 
Ah, I think I see the problem. I (and probably most others here) are looking at it from a sysadmin "how to deploy this in a secure and reproducible fashion" perspective. You seem to be more of a developer "how to build something" mindset. I like that you are looking into automating tasks and trying to build a basic shell of an application/environment that is reliably standardized and easy to deploy, but you need to take the mindset farther. From a security standpoint mucking around as www-data and then "cleaning up the server for production" is far more worry some than having python installed on a server for ansible. turning your dev environment into your production environment is a recipe for disaster, and having dotfiles in the directory structure where the webserver will be accessing and serving files is a big no-no. Develop your application however you want, but then pack it up cleanly and deploy it to the production environment. P.S. ansible doesn't require any extra users on a server. technically it doesn't even require any packages or software installed on a server, although python does make it more versatile. Just think of it as shell scripting basic tasks with a nice error handling framework and not having to worry about stuff breaking just because you deployed it to Red Hat/CentOS/Fedora/Debian/Ubuntu/Arch/Gentoo/OpenBSD/...
Hi! Could you please remove the second link of this comment? Reddit keeps marking the comment as spam, no matter how many times I approve it, and that seems to be the only way to fix this.
I agree, that's why it was the second thing I said in my first post :)
OP here. I wrote this tutorial a while back and submitted it a couple weeks ago to r/Python and r/Programming. I got a very positive response from r/Python and nobody said much of anything about it in r/Programming (got buried fairly quickly). I thought I'd see what the Bash community thinks about it. I make some pretty harsh criticisms of Bash (mostly concentrated in the beginning of the document), and I'm interested to see how this sits with people and if there's anything I got wrong or could state better. Thanks, Aaron
I like it. Good stuff.
I think your criticisms are relatively reasonable, in terms of the tradeoffs between bash and python. I agree with your main point that bash is good at organizing subprocess and bad at almost everything else, and python is a general-purpose language but the subprocess module is a lot harder to use than bash. The only part I really disagree with is this idea of replacing grep, sed, or awk with python's regex stuff. To me, the work of setting up a script to read line by line, as well as python's regex syntax, take about ten times as long as doing something that those tools are made for. (Fun side note: you mention that awk is turing-complete. So is sed.) Another thing to point out is that most of the coreutils work nicely in pipelines, but python often raises PipeError and dies with a stack trace unless you're careful to avoid it. In general, this seems like a reasonable guide, except for me, the domains of bash and python are different enough that there's not a big set of things I would want to "translate" from one to the other.
Thanks for the input. &gt; The only part I really disagree with is this idea of replacing grep, sed, or awk with python's regex stuff. To me, the work of setting up a script to read line by line, as well as python's regex syntax, take about ten times as long as doing something that those tools are made for. I agree that a oneliner with sed/awk/grep is better expressed in the shell. I was more talking about cases where someone might be doing a more lengthy script and was used to working with sed/awk/grep for text manipulation. I'll try to add a few sentences to make it more clear that using sed/awk/grep is a much better choice where one-liners or very short scripts are concerned. My main thought was to discourage people from shelling out to these programs in the middle of Python scripts. &gt; (Fun side note: you mention that awk is turing-complete. So is sed.) I had no idea! I use sed a bit like ed with super powers. If you have resources with more info about "programming with sed", I'd love to read them. &gt; Another thing to point out is that most of the coreutils work nicely in pipelines, but python often raises PipeError and dies with a stack trace unless you're careful to avoid it. Good point. I will consider how to deal with this in the tutorial. &gt; In general, this seems like a reasonable guide, except for me, the domains of bash and python are different enough that there's not a big set of things I would want to "translate" from one to the other. I kind of know what you mean. For me, Bash was the first programming language I learned (well, when I took programming up again as an adult...), so anything I wanted to do, I was doing in BASH. I realized at some point (with the help of the #bash IRC) that this was not what one should be doing. I've spent a fair amount of time translating bash scripts into python scripts. This may be my own fault. My rule these days is, if a shell script reaches twenty lines, reevaluate your life and probably rewrite in Python. So I guess I agree with your general sentiment, but there are still people out there writing scripts in excess of 100 or even 1000 lines of Bash, and there's no domain in which that is not a recipe for pain, unless I'm mistaken. Thanks again. Great criticisms!
About the sed thing, on Rosetta code, I think I once saw a sed implementation of a sorting algo... Maybe it was another thing.
Hmm interesting, that could work in my case I’ll give it a try. Thank you. 
&gt; Why? Because changing a process's persona has nothing to do with changing its environment. In _most_ cases, actually going through the whole login process (which is what `su -` or `su -l` or `su --login` will do) isn't actually necessary. People tend to use `su -` out of habit and superstition. Of course, something that relies on `$USER` being correct would break if `su -` isn't used. But such code is broken anyway, since `USER` is merely a convention, not a rule. There's nothing stopping `USER` from being unset or changed to a wrong value in any process's environment. &gt; Is there other methods to get the current username? In Bash, the correct way to determine the process's effective persona is to look at `EUID` (or `UID` if you care about its real persona). Other languages will have other means. All of them will be backed onto the `geteuid` or `getuid` syscalls.
A very well put together. I like your down to the basics explanation approach for some libraries. Thanks for sharing. &amp;#x200B;
Well, I used `whoami` instead of `$USER. Problem solved.
Class homework?
Thanks! Was a good read.
Worth reading. As someone conversant in the command line, but far from fluent, I would say I understood about 80% of it. Started skimming when I felt lost, but was able to get my feet back underneath me to understand the conclusion. Hope that's a help to some degree. I deal with the 'curse of knowledge' in other areas of my life. It really is hard to get an accurate read on what your audience doesn't know on a topic that you have a level of expertise on. You assume they know nothing and they feel condescended to, but if you assume they know more than they do, you run the risk of losing them. Anyway, quote a tightrope to walk. You seem to acknowledge that and that puts you pretty far ahead of most. Respect. 
Done.
 #!/usr/bin/env bash # Get the UUID with: cryptsetup luksUUID /dev/sdb1 USB_UUID=xxxx-yyyy-zzzz USB_NAME=crypt_drive USB_MOUNT=/mnt/luksdrive # cryptsetup prompts for the password if [[ -h /dev/disk/by-uuid/${USB_UUID} ]]; then cryptsetup luksOpen UUID=${USB_UUID} "${USB_NAME}" fi # only mount if it successfully decrypted if [[ -h "/dev/mapper/${USB_NAME}" ]]; then mount "/dev/mapper/${USB_NAME}" "${USB_MOUNT}" fi 
&gt;but there are still people out there writing scripts in excess of 100 or even 1000 lines of Bash I feel personally attacked 
http://wiki.bash-hackers.org/howto/getopts_tutorial#using_it
when you use the simple comma you are constructing a string as it is, so it does not interpret $1 as its value (eg just testing) but as literally $1, so to make the shell expand $1 to its value you need to use either $1, ${1} or "$1". Each is different but lets say the best is "$1" (if you want to know more look into word splitting). So in your case `find $mydir -iname '*$1'` should be `find $mydir -iname "*$1"`. The option part can be solved for example just checking the second argument: if [ -n "$2" ]; then #checks if $2 is not null/empty, if it is not then the test is true #TODO whatever you want in that case, like checking if it is -s or -r else #if it is null/empty echo "please give at least one operator; possible operators are\n-s =&gt; search for file\n-r =&gt; play in random order" #or the same with several echos (you can also use printf) #echo "please give at least one operator; possible operators are" #echo "-s =&gt; search for file" #echo "-r =&gt; play in random order" fi Or if what you want is to the user to input or choose one of the options once you have reached a certain point in the script then you need to use read, like: echo -n "Use option -s [y/N]" read REPLY echo "" # (optional) move to a new line if [[ $REPLY =~ ^[Yy]$ ]]; then option=s else #TODO whatever if not answered Y or y, like just pressing enter fi 
Start here https://unix.stackexchange.com/questions/447615/provide-password-to-udisks-to-unlock-luks-encrypted-device
Only do it this way if you want to get it done fast and never look at it again, or if you're willing to throw away all that work once you realize you want to allow any combination of arguments in any order. If you want to half-ass it without using getopts, you're better off with a [while/case/shift loop](https://docstore.mik.ua/orelly/unix3/upt/ch35_22.htm). But just use getopts. I can't count the number of times I've seen someone think "Man, I don't want to spend a half hour learning how getopts works. I've got shit to do." and proceed to spend four hours trying to re-invent an inferior implementation of getopts. 
Hard no. Regexs are expressions written in the Regular language. Globbing, in theory, could use regexs ... but I have yet to see that. All globbing implementations that I know of use `patterns`. They are different languages (albeit with some overlap). With globs, * matches anything. In RegExp, you would use .* With globs, ? matches a single char. In RexExp, ? marks the previous item as optional and . matches a single char.
Can we have a post with less graphics?
I agree, but he also should learn some basics before anything else
&gt; (don't use ~, as touch doesn't like it) What? $ cd / $ touch ~/touch_likes_this $ ls -l ~/touch_likes_this -rw-rw-r-- 1 stu stu 0 Sep 3 11:54 /home/stu/touch_likes_this $ 
I really enjoyed reading your post! Very interesting, thanks!
Usually considered better to use #!/usr/bin/env bash
Replaced ideone link with tutorialspoint link.
Just wondering. Why would it be considered better to use `env`? I can think of at least two downsides (one kinda serious), but I'd love to learn about the upsides if any. Certainly looks cool and new-age. I'll admit that :-)
No dice, it’s still getting flagged. I have to remove your comment or Reddit will never let /r/bash have an empty modqueue again, sorry :(
That ensures the user’s path is taken into account, which in turn allows for using a version of bash different from the one at `/bin/bash`. Some distros (particularly enterprise ones) discourage upgrading the core utilities, so you install new versions in `/usr/local` or `/opt` and leave the normal locations unchanged. Also, some systems aren’t upgraded very often, and local users might install a newer version to their home directory (e.g. `./configure —-prefix=“${HOME}”/.local`). It’s important to use the same bash executable as the user is running, since otherwise you’ll get the frustrating bug of “it works in my shell but breaks when I make a script of it!”
Yeah. I think you're part of the niche for a post like this. Someone who already knows a good 80% of the content and who seeks knowledge. Whenever I go into one of these deep dives into the abyss, my goal is usually two-fold: To **convey a message**, and to **draw the curtain back on a little magic**. The **message** here is that unless you know exactly what you're doing, every shell script that uses additions to the standard should begin with: ``` #!/bin/bash ``` Not `#!/bin/sh` and definitely not a non-shebang line. Otherwise, it only may be be run using Bash, when executed. This is pretty important when running scripts in distributions that only ship with a standard shell, or a default Bourne compatible shell and Bash. Getting this wrong is the source of much frustration when running an invocation wrapper or debugging script for a service that ends up being run in an Alpine Linux container. As for the magic. One of the cool things about Linux is that there's very little magic actually going on. It's just layers of abstraction and components that each have their own responsibilities. Going down a level just requires a little digging, even if it's into areas that aren't within one's comfort zone. Case in point: How `#!` lines are actually used, their strange limitations, and how `bash` is designed to help those who leave them out or get them wrong. Anyways, thanks a tonne for the feedback. I'll try to jump back and forth between Bash and other more involved topics, but I know I can always return here to exchange a little depth for a little confidence boost :-) ❤️
Gotcha. I always thought this pattern made it a just a teensy bit too easy to get trolled. For example, it would require some major neglect for a sysadmin to replace `/bin/bash` with an untrusted shell, but it's pretty easy to convince someone to move a program called `bash` in-scope of their `PATH`. The former cannot be done without root and the latter can be done by anyone. Also, only two words may proceed `#!`: The path to an interpreter and "the rest" which is sent to the interpreter as a single argument. It's a pretty common practice to add `-e` or `-x` to a Bash script's shebang line (or `-w` to a Ruby or Perl script). `env` occupies one of these words. I feel you on distros that don't track modern versions of utilities. One could argue that you're best to use what ships with the system, but I get the desire for users to use newer stuff. I'll still advocate using `#!/bin/bash`, but it's neat to get a more in-depth explanation of this whole `env` business. Thanks 🙏
&gt; It’s pretty easy to convince someone to move a program called `bash` in-scope of their `PATH`. The former cannot be done without root and the latter can be done by anyone. Placing an untrusted executable in your path is always a bad idea (hence the rule that placing `.` in your path is a terrible idea). Generally you should only place a select few directories in your path, and any component *nix user will be careful about adding new entries to directories on the path. &gt; Also, only two words may proceed `#!`: The path to an interpreter and “the rest” which is sent to the interpreter as a single argument. It’s a pretty common practice to add -e or `-x` to a Bash script’s shebang line (or _-w_ to a Ruby or Perl script). _env_ occupies one of these words. For bash (not sure about Ruby), you can do `set -e` or `set -o errexit` as the first line of the script. That’s what I always do. I’ve never seen the shebang contain this option in the wild. &gt; I feel you on distros that don’t track modern versions of utilities. One could argue that you’re best to use what ships with the system, but I get the desire for users to use newer stuff. CentOS, for one example, explicitly condones this behavior. RedHat provides “devtoolsets” to upgrade compilers, debuggers, etc to more modern versions. I believe there are other packages as well. I don’t know how other “enterprise” distros handle this.
Sorry, to expand on that, touch will not evaluate the ~. e.g. $ var="~/file" $ touch $var touch: cannot touch '~/file': No such file or directory Since touch won't evaluate the ~ it's easier just to write the full thing down, put in /home/user/file and it'll avoid the issue. You could also use eval or other tricks but I haven't bothered because I don't migrate usernames very often. It's worth noting this isn't the same everywhere though, for example: $ var="~/file" $ firefox $file $ nano $file $ nemo $file # error $ vim $file # error Firefox and nano will open the file just fine, vim and nemo won't open anything.
I think reddit screwed up your formatting a bit. 
Your "dmidecode.file" is the same as what can be seen when running `sudo dmidecode`? I experimented with the text output of that dmidecode tool and came up with this here: $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ { /^\s*Size:\s*/ { s///; /No Module/! p } }' 8192 MB 8192 MB About how I came up with this, my experiments went like this: First, this here replaces your `grep -A6 ...` command: $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ p' Memory Device Array Handle: 0x0007 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 8192 MB ... The sed rule I used there makes it target the block of lines in the text that start with "Memory Device" and end in an empty line. Then next, you can add a target for the "Size: ..." line to the "p" command: $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ { /^\s*Size:/ p }' Size: 8192 MB Size: 8192 MB Size: No Module Installed Size: No Module Installed Now instead of printing, you can make it cut off the "Size:" part first: $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ { /^\s*Size:\s*/ { s///; p } }' 8192 MB 8192 MB No Module Installed No Module Installed And last, only print if the text is not "No Module": $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ { /^\s*Size:\s*/ { s///; /No Module/! p } }' 8192 MB 8192 MB 
Yes, I already have that output already actually. Now my Problem is, sometimes it's reported in GB like 32 GB. If I have for example 2 x 32 GB, I get the reult 64MB, because my Script can't handle GB or MB yet.
 error(){ #$1 command to test eval "$1" || echo "Command: $1" } You can use eval, so you have the command used saved in a variable. Btw eval prints an error message with the command when it fails.
 usage(){ echo "This is the function you can echo the Help for your script!" exit 1 } # Read the Command Line Options while [ "$1" != "" ]; do case $1 in -s | --server ) shift server=$1 ;; -d | --disksize ) shift disksize=$1 ;; -v | --verbose ) shift verbose=true ;; * ) usage exit 1 esac shift done This is a command line options parser i've written a while ago. &amp;#x200B; should be pretty easy to understand :)
You could do something like this: case $(dmidecode -t 17 | grep -m 1 "Size") in (*MB) dmidecode -t 17 | awk '/Size.*MB/{ s+=$2 } END { print s "MB" }' ;; (*GB) dmidecode -t 17 | awk '/Size.*GB/{ s+=$2 } END { print s "GB" }' ;; esac This also lends itself to the scenario where, say, you want to output MB only, where you could do something like this: case $(dmidecode -t 17 | grep -m 1 "Size") in (*MB) dmidecode -t 17 | awk '/Size.*MB/{ s+=$2 } END { print s "MB" }' ;; (*GB) dmidecode -t 17 | awk '/Size.*GB/{ s+=$2 } END { printf("%0.fMB\n", s/1024) }' ;; esac Or something like that.
Yeah, now I understand why they call it read only language
Okay thanks so far. I need to compare the result of dmidecode against a textfile so see if the installed RAM is bigger than usual. From the Textfile I have the usual RAM in this format: 512 MB or 1 GB How can I convert this format to a comparable format with numfmt?
You could do this: sed -r 's/([A-Z])/ \1B/' This would turn for example `16G` into `16 GB`.
Is there some reason you're doing this instead of 'free -g' or parsing /proc/meminfo?
Interesting read. Thanks for sharing.
As /u/chaspum suggests, you may just have to wrap the commands you're invoking, rather using an `ERR` trap to be notified when a command fails. `ERR` has all the same problems as the `errexit` shell option. I think it would be unwise to rely on this trap being invoked, since it's so easy to refactor the code in such a way that it won't be invoked, even on a failed command. (Just to be clear, I don't think `errexit` is totally useless. I make a habit of using it. I'm just aware that it's a safety-net only, and that a robust script still needs manual error handling.)
Okay, but now I can not compare them, because if [ "$Memory_installed" -gt "$Memory_text" ]; throws the error, that it only expects numbers. The Variables are for example "12 GB". Now if I just removed GB, I would get the result, that for example 1024 MB is bigger than 2 GB, because 1024 &gt; 2...
Yes, because what i am working with is just a systemdump. Also the logs of free are not exact, there are some MBs stripped for the Kernel, which are not reported to free. Also I need to see the size of each installed module, which free does not report. /proc/meminfo is not included in this dump.
Yeah, unless I'm misunderstanding something I think I get that and I'm trying to write all of my code to use proper error handling with tests, conditionals/`||` statements so ideally I'd never see my error function. That said, there are still enough complexities that some errors are difficult for me to anticipate so when I do hit one I'd like to log a more helpful message and then continue or, if interactive, print and prompt user to continue. Depending on the script I might also only enable that in a `--debug` option. Wouldn't that behavior mitigate the disadvantages of `errexit` or an I misunderstanding something? I'm still new to Bash scripting but trying to write as solid and debugable code as possible so I appreciate the feedback!
Interesting, I hadn't thought about using `eval` that way but I have been avoiding using it.
You should probably not use numbers like "16 GB". You should convert everything to bytes so that you can compare things like "8192 MB" to "16 GB" without problems. This means you would first do this with the output of dmidecode: $ sudo dmidecode | sed -nr '/^Memory Device$/,/^$/ { /^\s*Size:\s*/ { s///; /No Module/! { s/ //; s/B//; p } } }' | numfmt --from=iec | awk '{ sum += $1 } END{ print sum }' 17179869184 And you should convert the number from your file to a similar byte number with numfmt. You can do that with `tr -d ' B'` to cut away the space character and the "B", and then `numfmt --from=iec`. Here's an example of what I mean: $ echo "16 GB" | tr -d ' B' | numfmt --from=iec 17179869184 
These are all very cool. I'm looking forward to going through these line by line and learning form them. Thanks!!
&gt; Wouldn't that behavior mitigate the disadvantages of errexit or an I misunderstanding something? I guess I just wanted to point out that even if you _don't_ do proper error handling, at least at the point at which the command failure occurs, it's possible for `errexit` not to trigger or for your `ERR` trap to be called. Take this example: trap 'echo "got ERR trap"' ERR f() { # ... do stuff ... some-failing-command # ... forget to handle its failure, and do other stuff ... } if f; then : fi Since `f` was called from within the test clause of the `if` statement, the `ERR` trap is completely disabled. You would not notice that `some-failing-command` failed and that you hadn't handled its failure properly.
That makes sense, thanks!
Touch will evaluate the tilde. It’s because you have it in quotes, the tilde is passed literally to touch. If you did it without quotes: $ var=~file $ touch $var It would work!
Awesome! Glad you figured it out, good to know the difference in the paths.
*The Linux Command Line* You can get a physical copy from No Starch Press for (iirc) $30, and there's a free and legal pdf floating around online.
Also, https://mywiki.wooledge.org/BashGuide and https://wiki.bash-hackers.org.
From tldp (the linux documentation project), old but gold. [http://www.tldp.org/LDP/Bash-Beginners-Guide/html/](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/)
`kil $pid`
lol typo thats what it is
If `ffmpeg_function` is really just running a single command in it (after building the command--BTW see https://mywiki.wooledge.org/BashFAQ/050) why not just put that command in the background instead of the function? Also, are you sure the command is really running still, perhaps it did in fact exit before you try to `kill` it? Also, why `kill @pid` that won't expand the `$pid` value
@pid was a typo. if I do `ps -e | grep &lt;pid returned&gt;` it says its the name of my script, and if I do `ps -e | grep ffmpeg` I get every ffmpeg I started in the script, (even the ones I wanted to stop). I don't really understand how I would do what you are saying, do you mean just copy paste the contents in the loop? that is very messy and its in a large script
That's because it *is* your original script. You've forked off a child process to execute your `ffmpeg_function` function, and the PID you're capturing is the PID of that process.
Jobs and process groups are not related to each other, at least, they don't have to be. You also don't have to enable job control to put something in the background and use a jobspec with the `kill` command. You only really need job control if you want to interactively (i.e., with `fg` and `bg` move jobs between foreground and background), if you want to use `&amp;` to run something in the background then `kill %1` that works find without `set -m`. Note that the job can survive longer than the process does though, which can really be confusing at times. . . (e.g., see https://stackoverflow.com/questions/31757422/why-do-i-get-different-results-with-kill-0-jobspecpid)
Hmm, I've just done a few experiments, and I think we're both wrong in certain ways. &gt; Jobs and process groups are not related to each other, at least, they don't have to be. That is true, however Bash documents that when job control is enabled, "all processes run in a separate process group". &gt; You also don't have to enable job control to put something in the background and use a jobspec with the kill command. No, but the behaviour of `killl` is quite different. Take this example: set -m ( for i in {1..5}; do sleep 1; echo $i; done &amp; wait ) &amp; sleep 2 kill -TERM %1 wait With job control enabled, the subshell is in its own process group, and the `kill` kills the entire process group. If I remove `set -m` though, the `kill` kills _only_ the immediate child process; the `for` loop continues executing. But this shows that my idea of using `kill -- -$pid` isn't going to work without job control as well.
I learned a lot of bash scripting doing the bandit CTF on overthewire
Change: '%{url_effective}' to: '%{url_effective}\n'
Thanks for the clarification on the relationship between jobs and groups there. I had not appreciated that difference. It can also be highlighted by running `pgrep -l -g 0` before your `sleep 2` in each example so you can see which processes are in the "outer" group in each case. I confess, I'm still really trying to unpack the details of the example you provide. It certainly behaves as you describe it, but it's not quite fitting all together in my mind just what's going on. Without job control enabled it seems the "inner" jobs aren't killed by a signal, but if you trap it and do `kill %1` within the subshell it will (not surprisingly) terminate the job. Of increasing (to me) interest, just highlighting how confusing this stuff is to deeply understand, I think the GNU documentation is wrong. If you comment out the `set -m` in your first example you can still use `kill %1` though as you note it doesn't have the same effect as when job control is enabled. Allegedly, you shouldn't even be able to do that, per the [GNU bash docs](https://www.gnu.org/software/bash/manual/html_node/Job-Control-Builtins.html#Job-Control-Builtins): &gt; When job control is not active, the kill and wait builtins do not accept jobspec arguments. They must be supplied process IDs. but we see they clearly do still accept jobspec arguments. I'm going to have to continuing digging it seems to wrap my head around it better.
&gt; Of increasing (to me) interest, just highlighting how confusing this stuff is to deeply understand, I think the GNU documentation is wrong. The GNU documentation certainly does not match the code. You will see [here](http://git.savannah.gnu.org/cgit/bash.git/tree/builtins/kill.def?h=bash-4.4#n205) that the code has a comment: /* Posix.2 says you can kill without job control active (4.32.4) */ I'm actually have a bit of trouble finding that explicitly mentioned in [the (current) POSIX specification](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/kill.html) though. Further on in that source, you will see the code that's responsible for `kill`'s different behaviour depending on whether job control is enabled: j = get_job_by_jid (job); /* Job spec used. Kill the process group. If the job was started without job control, then its pgrp == shell_pgrp, so we have to be careful. We take the pid of the first job in the pipeline in that case. */ pid = IS_JOBCONTROL (job) ? j-&gt;pgrp : j-&gt;pipe-&gt;pid;
Can’t you just do whereis ?
/* (joking, please don't do this)
&gt; '%{url_effective}\n' thanks
&gt;Touch will evaluate the tilde. It’s because you have it in quotes, the tilde is passed literally to touch. If you did it without quotes: No, touch will not evaluate the tilde, quotation marks only matter when declaring the variable. The tilde is actually expanded out when you declare the variable, e.g. $ var=~/file $ echo $var /home/levi/file . $ var="~/file" $ echo $var ~/file Some programs when they see the tilde will evaluate it, but touch isn't one of them. There's arguments that they should expand it themselves, e.g. you may be reading paths from a file and will have to use "" to escape them properly, or you may be reading input from a user and there's no guarantee they'll escape their spaces correctly. Not using "" is a solution that would work here, for me at least. But if someone else uses a space in there it'll mess up again unless they use '\\ ' instead.
your PROMPT_COMMAND ends with a `;` (`...\007";fi;'`), then when you assign PROMPT_COMMAND again with `PROMPT_COMMAND=${PROMPT_COMMAND:+$PROMPT_COMMAND; }` where you append yet another `;` if PROMPT_COMMAND has a non-empty value, which in turn becomes a syntax error. I suggest the first assignment you do to PROMPT_COMMAND should **overwrite** whatever PROMPT_COMMAND currently contains, if anything. Otherwise you'll just be appending the same commands over and over again each time you source it, which does you no good.
Looking at [what veil is](https://bytesoverbombs.io/unveiling-veil-1efac4e283d5) and this from the readme: &gt; ./config/setup.sh // Setup Files &gt; This file is responsible for installing all the dependences of Veil. This includes all the WINE environment, for the Windows side of things. It will install all the necessary Linux packages and GoLang, as well as Python, Ruby and AutoIT for Windows. In addition, it will also run ./config/update-config.py for your environment. I'm inclined to say that this will have installed all sorts of things everywhere and your best option would be to reinstall your system. Next time you choose to investigate stealthing malicious payloads onto other people's computers you can be more circumspect about safeguarding your own system.
Is your goal to download the mac version from a different computer? In that case why not go here: https://www.hamrick.com/alternate-versions.html ? It needs js to work and the link on the button just takes you to a generic html download page. It seems the OS detection is not particularly accurate. On a regular browser, it detects mine as linux; from elinks, it thinks it's windows. It seems to default to windows. 
Not really sure what you are trying to do here. Are you talking about a function or a script you're trying to write? $1 and $2 generally refer to positional arguments passed to a script or function at the command line.
Thank you buddy, but i resisted a lot for asking help, was uppset and and asked but however i found the solution for my beginner problem. I really thank you that wanted to help a beginner.
That comes from menu.min.js. var ve = "9.6.15"; 
Thank you, I looked at that file but it was all on one line so I can see how I missed it lol.
Thank you, I looked at that file but it was all on one line and therefore I didn’t think it was of much use but I was very wrong. Thanks again! 
Thanks
the (obscure) correct tool is `getopts`
Given the context, you will probably have to look into **PAM** and **su** or **sudo**. Reading an AD password in bash directly is probably a bad idea.
 [[ -d /folderb ]] || cp -r /foldera /folderb
So you say "something like rsync"... why _not_ rsync?
&gt;Scenario: The environment has SSO and Active Directory. To read some attributes, you need to use an administrative account that is different from your regular account to query with. I would like to prompt in a script for the administrative credentials to execute the script. Ok. Do you have any code so far? Is this [BOUOW](/r/bashonubuntuonwindows) `bash` or `bash` on Linux? If it's `bash` on Linux, how are your hosts authenticated against AD? `winbind` or `sssd` or something else?
Read through the examples of the rsync man page, the tool was basically made for this, if you still need help I'm down but check the man page
&gt; ${oplist[*]} Means "everything in oplist" &gt; ${!oplist[*]} Means "everything *not* in oplist"
Try this to find out: oplist=('add' 'subtract' 'multiply' 'divide') echo ${!oplist[*]};
In that spot, the `!` makes bash produce a list of index numbers for that "oplist" array. Here's an experiment at the command to show what bash is doing: $ x=(red green blue) $ echo "${x[@]}" red green blue $ echo "${x[*]}" red green blue $ echo "${!x[*]}" 0 1 2
you can do it in `sed` with one line like so: sed -n 's/.*\({[^}]*WD30EFRX - 68EUZN0[^}]*}\).*/\1/p' inputfile.json
Cheers! Can't believe I dind't think of trying this.
Thanks! I understand it now. Is there any other uses for '!'? I tried "${!x[*]}" but replacing * with 2, 0..2, but nothing seems to do much besides. I guess what I mean is, '!' seems to be completely reliant on also having [*]?
Hey, Thursday\_throwaway2, just a quick heads-up: **occurence** is actually spelled **occurrence**. You can remember it by **two cs, two rs, -ence not -ance**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
&gt; -ence not -ance well how am i gonna remember that? It sounds the same in my head, mr. bot ;)
Hey Thursday_throwaway2, did you know you can spell words correctly by remembering how they are spelled? You can always recognize incorrectly spelled words by looking at the way the words are. Have a day!! `I am a sentient Reddit bot. I need karma to live. Please feed me.`
Cheers! I'll keep that in mind, swapped the *'s for @'s. One think I don't get in your examples, is that $ for y in "${x[*]}"; do echo "word: $y"; done is the same as $ for y in ${x[*]}; do echo "word: $y"; done except for the quotes. Yet they produce different examples. Why is this? I thought quotes were really for if there's e.g spaces etc., but wouldn't be necessary in this example.
&gt; I wonder if he uses the stock version of Bash that ships with OS X. &gt; &gt; That would be version 3.2. &gt; &gt; From 2007. &gt; &gt; I also wonder if RMS weeps whenever he sees a picture of Brian Fox with a Macbook. Perhaps RMS weeps *for* him.
This is JSON, you'd have to be crazy to pipe it through grep, sed or awk. Use [jq](https://stedolan.github.io/jq/). jq 'select(.model_number == "WD30EFRX - 68EUZN0")' your-data.json
This * and @ thing of bash is confusing. The @ has special behavior when you use `"..."`. When you have an array, for example: x=(a b c) And then you write this here: "${x[@]}" Then bash translates that into: "a" "b" "c" Now with that *, when you write: "${x[*]}" Then bash will do: "a b c" If you put this into the 'for' loop, with the @ you will have three words, but with the * you will have just one: for i in "a" "b" "c" for i in "a b c" Using * will be a mistake because the 'for' loop will break. The confusing part about bash is that if do not use the `"..."`, then both will show the same behavior: for i in ${x[@]} for i in ${x[*]} They will both read as: for i in a b c The problem with * shows up if you have spaces in your array elements, for example: x=( "a b c" "d e f" ) When you now do: for i in ${x[*]} You get: for i in a b c d e f # &lt;-- six words And you can't add ": for i in "${x[*]}" Because you will get: for i in "a b c d e f" # &lt;-- one word You need to use @: for i in "${x[@]}" for i in "a b c" "d e f" # &lt;-- two words
This is JSON, you'd have to be crazy to pipe it through grep, sed, cut, or awk. Use [jq](https://stedolan.github.io/jq/). jq 'select(.model_number == "WD30EFRX - 68EUZN0")' your-data.json
This is JSON, you'd have to be crazy to pipe it through grep, sed, cut, or awk. Use [jq](https://stedolan.github.io/jq/). jq 'select(.model_number == "WD30EFRX - 68EUZN0")' your-data.json
Have to 2nd this. While it's certainly possible to do what OP wants, those tools just aren't the right ones for this job. Use jq and save yourself the massive headache.
You're right, after I posted a reply I immediately thought I worded that "you'd have to be crazy" poorly. I pushed a lot of json through sed before I found out about jq. 
About 14 years ago I was working on a project that was trying "parse" HTML with regex. It kinda worked for what we needed... except for all the times it didnt... 
`grep passwd /etc/nsswitch.conf` should be a sufficient way to find out. 
You can convert it to a csv, then use seems and awk
&gt; Is there any other uses for '!'? Yes! There are two other things that an exclamation mark can do in shell parameter expansions. The first thing is what's called "indirect expansion": $ apple=green $ banana=yellow $ cherry=red $ fruit=banana $ echo "${!fruit}" yellow You tried to use it with an array, and it can work there when too when you're picking a _single_ array element: $ fruits=( banana cherry apply ) $ echo "${!fruits[1]}" red The other thing an exclamation mark can do is expand to a list of shell parameter names: $ fruit_apple=green $ fruit_banana=yellow $ fruit_cherry=red $ for i in "${!fruit@}"; do echo "$i"; done fruit_apple fruit_banana fruit_cherry $ for i in "${!fruit*}"; do echo "$i"; done fruit_apple fruit_banana fruit_cherry You'll note it follows the same rules as before: using a `@` inside a double-quoted string produces separate words, using a `*` produces a single word. (This theme is echoed in a few other places in Bash.) You might think, "aha, this looks like a way I can produce fake arrays where the keys are actually _strings_, rather than numbers"... and indeed, once upon a time it was the only way to do this. But a better approach nowadays would be to use _associative arrays_. Bash has had them since v4.0. For more details on all of this, check out the Bash documentation, in particular: * [Shell Parameter Expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html) * [Arrays](https://www.gnu.org/software/bash/manual/html_node/Arrays.html)
Oh, I did not know about this, thanks. It even formats the JSON nicely :) But for some reason it does not work. I tried jq 'select(.model_number == "WD80EFAX - 68KNBN0")' my.json Will I need to find it, tell jq, that there is a "list" all this is is in? The Json looks like this: { "success": 1, "list": [ { "product_name": "myproduct1", "hcl_id": 886, "usage_id": 12, "usage": "-", "recommend": true, "eol": true, "category": "hdds", "status": "Published", "size": "4TB", "class": "Enterprise", "series": "Ultrastar 7K4000", "firmware": "MFAOAA70", "type": "3.5\" SATA HDD", "4k_native_hdd": "no", "brand": "HGST", "model_number": "HUS724040ALA640 - 0F14688", "has_note": true }, { "product_name": "myproduct2", "hcl_id": 15316, "usage_id": 12, "usage": "-", "recommend": true, "eol": false, "category": "hdds", "status": "Published", "size": "8TB", "class": "NAS", "series": "Red", "firmware": "81.00A81", "type": "3.5\" SATA HDD", "4k_native_hdd": "no", "brand": "Western Digital", "model_number": "WD80EFAX - 68KNBN0", "has_note": true } ], "pages": 18 } Or do I need to escape the spaces or something similar? 
&gt;rsync man page I read the page i think its got something to do with the compare dist flag but i cannot seem to get it to work.
Ich use a Makefile for all sorts of build Tasks in my daily Job as web dev, where usually gulp/grunt are used. Works Like a Charm.
Yeah, I did the same for a while and found that it worked well too. I did find that there were some edge cases, like integrating with CI/CD where I wanted to run commands in a docker container locally and on the CI/CD servers to work the same way and this Bash method handled it well. I haven't tried it, but I'd imagine a combination of both would work quite well, Make handling the builds tree and Bash for more environmental things and maybe handling some of the things you'd normally make phony in a Makefile.
im cuntopilis but i forgot my password lol, anyway let me know if this helps mkdir in_dir out_dir tree # . # ├── in_dir # └── out_dir # the {a..z} is a shell expansion like glob but expands to a b c d... etc # also {1..100} works touch in_dir/{a..z} tree # . # ├── in_dir # │ ├── a # │ ├── b # │ ├── c # │ ├── d # │ ├── e # │ ├── f # │ ├── g # │ ├── h # │ ├── etc.... # └── out_dir rsync -av in_dir/{a..i} out_dir/ # sending incremental file list # a # b # c # d # e # f # g # h # i # # sent 504 bytes received 187 bytes 1,382.00 bytes/sec # total size is 0 speedup is 0.00 rsync -av in_dir/ out_dir/ # sending incremental file list # ./ # j # k # l # m # n # o # p # q # r # s # t # u # v # w # x # y # z # # sent 1,043 bytes received 342 bytes 2,770.00 bytes/sec # total size is 0 speedup is 0.00 
This isnt working or i am just not getting it... &amp;#x200B; When i run this rsync -av --dry-run /mnt/user/Data/ZZRandom/out/Movies /mnt/user/Data/Movies/ &gt; include.txt &amp;#x200B; I check include.txt and it will move duplicate movies over. IE "ATM (2012)/ATM (2012) \[DVD 420p\].avi" will get copied over despite the destination having the Folder "ATM (2012)". &amp;#x200B; &amp;#x200B; &amp;#x200B;
there is no pitfall, there's just lack of RTFM.
Lack of RTFM is everywhere, not only in bash. That is a problem, I agree. Still in my view, "if test" is one of the many pitfalls in bash. I don't think it's one vs the other exclusively.
You’re going to to invent a new shell that works when people put syntax errors in their scripts? 
That looks pretty good. I was thinking about writing something similar, but i think I'm going to give this a shot first.
 for i in bin src usr var dir dir dir do cp /mnt/chroot/$i . done
still does not make any change
Try removing the space: `-t$`
I hope it works for you! Please feel free to raise any bugs you discover. Happy to share the specifics of problems I've encountered!
I didn't get it why you need to `-t'$'`? What you want is just sorting the file names, right?
https://www.mankier.com/1/atool
"\$" 
You had me on board up until: &gt; Shell is supposed to do whatever is needed to make my life easier. 
Cool. What's your idea of an ideal shell then? I just don't like the bump when I already wrote something in bash and then it becomes big and I need to switch to a "real" language.
I’m not sure about my view on this, but I think what I want from my shell is simply that it binds together input and output from the tools that I use. And bash does that fine without understanding structured data. But with that understanding bash would be even more useful. So I just disagree with the statement that bash should do whatever you want. That’s asking for unmaintainable code. Have you looked i to adding this feature to bash? 
More precisely, I think that a shell should have fully featured language. (That's what I'm doing with NGS) I discussed exactly that (adding structured data to bash) with a friend some time ago. I couldn't figure out how this could be technically possible. In my mind, structured data handling would require a garbage collection. Bash doesn't have one. In addition it would probably be very hard to find a syntax for the new functionality without breaking someone's production code. 
If you need JSON parsing in bash, you shouldn't be using bash..
I want my shell to be powerful. I need structured data in my shell. Bash can't do it. That's why I'm working on the alternative. Historically shells were used to interact with the system. Today I use my shell to interact with APIs. The reality has changed. Now, it does make sense to me that structured data would be a feature of the shell.
Or I could just use Python
I feel like there is no need to replace the shells in linux since there are already a lot of alternatives. You have a point that there is room for a better scripting language which is able to more easily handle data and work with APIs. But it especially needs to be easier to use than a higher language otherwise everyone would just pick the higher language anyway.
Alternatives like fish and zsh? In my view, they are kind of better bash. I want something else could completely. I think some will prefer to use domain specific language like NGS which has clear advantages for the intended use. If I understand your viewpoint correctly, you might like Elvish much more than NGS. Check it out. https://github.com/elves/elvish 
&gt; I think some will prefer to use domain specific language like NGS which has clear advantages for the intended use. Yes and that should be its goal. I do not require another shell which interacts with my os as bash and all its current alternatives do since they do it just fine already. If your domain specific language is also handling os tasks like bash and the others do then I would expect it to be able to do it well too. If not why should it be implemented or compared to existing languages like bash, fish, zsh? I would just run the domain specific languages shell from inside bash or others. If anything the domain specific language should be easy to script for its domain. It should not be required for every scripting language to have a shell. But if that domain specific language does have a shell it should also work in its domain and not as one is used to it from the existing shells(bash, fish, zsh.. etc). For example: In bash i would cd to directories, create files, call programs, pipe results etc -&gt; everything os related In the domain specific language the shell needs to have domain specific implemented builtin commands(bash has cd, exec, exit, etc... &lt;- os related). So what I would expect in the domain specific language is to have a shell where I would have builtin http calls which makes it easier to work with the http protocol or something else it should be good at.
Add a `--line-buffered` parameter to grep, like this: grep --line-buffered "no answer"
&gt; grep --line-buffered "no answer" it worked, thanks!
Glad you found your answer. But, it does help to view your man pages from time to time. &amp;#x200B; [https://linux.die.net/man/1/grep](https://linux.die.net/man/1/grep) [https://askubuntu.com/questions/562344/what-does-grep-line-buffering-do](https://askubuntu.com/questions/562344/what-does-grep-line-buffering-do) [http://www.pixelbeat.org/programming/stdio\_buffering/](http://www.pixelbeat.org/programming/stdio_buffering/) [https://stackoverflow.com/questions/7161821/how-to-grep-a-continuous-stream](https://stackoverflow.com/questions/7161821/how-to-grep-a-continuous-stream) [https://unix.stackexchange.com/questions/323747/grep-line-buffered-can-i-search-the-same-buffer-with-the-results-of-the-first](https://unix.stackexchange.com/questions/323747/grep-line-buffered-can-i-search-the-same-buffer-with-the-results-of-the-first) &amp;#x200B; Plus, I like the reason how it's working as well.
/u/yaboroda are you on a Linux machine or perhaps a Mac?
Genuinely curious here - how would OP have known what to Google/man search for unless they had heard of line buffering. I hadn't.
&gt;Mainly a shell is and should be the interface to the services(programs, I/O) that the OS provides That's not my use case. I need to talk to APIs. From the shell. That's how I work.
Of which none are domain specific. You can use Java too, right? It just doesn't fit that well. Meaning less productive.
&gt;But everything beyond like leads into some craziness. Because there is no fancy way to serialize complex data structures... Can you elaborate please? I don't understand the problem that you are describing. Maybe example could help.
I was all ready to answer a cake decoration question! 
`ls` output is sorted by default, no need for `| sort`. How about you explain the difference between what you want and what you get from `ls` by itself.
Better to ping it once and test the exit code. if 0, all good. if not, all bad. ping -c 1 google.com &gt; /dev/null if [ $? -eq 0 ]; then echo "all good" else echo "nope" fi 
That is the definition of a shell. If you want to talk to APIs and do stuff with more than just string, well that's why scripting languages like for example ruby exist. 
If you are trying to copy everything from /mnt/chroot to the current directory, you can use cp -R /mnt/chroot/* .
I am not. There are some things I explicitly don't want.
It’s kinda funny, but what you are looking for is a tool called `expect`.
 if [[ ! -z "$var" ]] ; then $var="defaultvalue" else read $var fi
No,I get the grep part. Just not the line buffer part.
thank you!!
autoexpect, also.
If you are writing the script that accepts input, you can skip the prompts entirely and process the data from a pipe. Bash uses a builtin called read, though it's more typical to use bash to run other programs that accept the data and connect to a database etc, using `python`, `ruby`, `perl`, `php` etc.
Nah, i suspect OP is confronted to a script he doesn't own, and I also suspect he asks for a specific solution instead of exposing his problem. We're not on stack overflow though, so he won't end up crucified on the public place ;) 
You should use `;` instead of `&amp;&amp;` to tie those two command together on the same line. If you use `&amp;&amp;`, the exit status from the command can break your code. Or, you could put your `output=$(...)` on its own line in front of your `if`, like so: output=$(candump can0) if [[ -z $output ]]; then # no need for " if you choose to use [[ instead of [ ... fi Do you know if this "candump" program has an exit status? If it does, maybe you don't have to look at its output at all and can just write: if candump can0 &amp;&gt; /dev/null; then ... or if ! candump can0 &amp;&gt; /dev/null; then ... For checking the text output, an alternative to saving it in a variable would be to measure it with "wc", like so: if (( $(candump can0 |&amp; wc -m) == 0 )); then # command had no output else # error fi The `|&amp;` is a special version of `|` that redirects error output as well, not just the normal output like `|` does. `wc -m` counts characters that were printed by the command. The `$(...)` then captures the number that was printed by `wc`, and `(( ... ))` is bash's math mode to work with numbers.
The problem is: when you have some really complex JSON, to be able to work with it, you need to convert it into a memory structure, but bash is not offering you some advanced array of arrays, hashes of hashes, trees, etc. There may be some specific cases when some workaround (little bit more coding) will help you, but nothing general. 
&amp;&amp; is used when you want the comand on the right executed only if the command on the left returns without error. do this AND IF SUCCESS do this.... || is used when you want the command on the right executed only if the command on the left fails or does not return successful. do this OR IF FAILURE do this. So you should be able to piece together the logic and structure from this to get the results you wish.
candump has a return value, but no output is nkt manifested by those. It is still returning 0, when there is no output. Returning 1 happens only during some hardcore problems which I assume wont happen in my case. Thank you for the answer, it helped me a lot. 
That's an article that's somewhat... lacking... IMHO. &gt;The challenge is that there is almost no known method to produce true random numbers, for now. Oh boy. This is one of those [ridiculous academic arguments](https://www.youtube.com/watch?v=ICv6GLwt1gM) about 'where the line is drawn'. We can generate random numbers from radioactive decay (ranging from [smoke alarm hacks](http://www.etoan.com/random-number-generation/index.html) all the way up to detecting and consuming atmospheric radioactivity, and [everything in between](https://www.fourmilab.ch/hotbits/)) and that's often classed as True RNG. There are also non-radioactive, [circuit based HWRNG's](http://onerng.info/) that are also often classified as TRNG's. So where does the author draw the line about what is and isn't a TRNG? &gt;In Bash, you can use environ variable $RANDOM for a random integer between 0 and 2^15-1. Yeah, it would have been more accessible to represent that as "between 0 and 32767 (2^15 - 1)" &gt;Mersenne Twister algorithm is a pseudo-random number generator that produces 53-bit precision floats in a period of 2^19937-1. People like it because it's fast, re-entrant, and efficient. It's also one of the most extensively tested RNGs. The extensive period reduces the probability of causing issues. There's a lot to unpack there, but let's keep it simple: there are things there that need to be defined. What does it mean to be re-entrant? [Ukuleles are re-entrant.](https://en.wikipedia.org/wiki/Reentrant_tuning) What's a period... in the context of RNG's? Why does the extensive period of the Mersenne Twister reduce the probability of issues? And so on. This article really speaks to an audience who already understands the terminology... so it's ultimately preaching to the choir... so it ultimately serves little purpose. &gt;Linear Congruential Generator Fun fact: [$RANDOM is an LCG.](http://git.savannah.gnu.org/cgit/bash.git/tree/variables.c#n1298) &gt;It's recommended to always use OS random number generator, for example /dev/urandom on UNIX platform If you're feeling the urge to trot out "durr /dev/random is more secure", [here's your homework.](https://www.2uo.de/myths-about-urandom) For anyone genuinely interested in diving into this crazy rabbit hole of random numbers and [entropy](https://www.youtube.com/watch?v=5bueZoYhUlg), there's the [PCG RNG](http://www.pcg-random.org/). Its author has some videos up and a fascinating series of blog posts that are all accessible. [This one is great...](http://www.pcg-random.org/posts/visualizing-the-heart-of-some-prngs.html) it has pictures. She's a redditor too, so she'll probably turn up here now that I've mentioned her RNG. Pre-emptive "Hi!", /u/ProfONeill. Sample PCG 8 code, please? Implementing PCG in `bash` might be a fun exercise... or it might cause one to go insane... Rosetta Code is also a good resource: https://rosettacode.org/wiki/Linear_congruential_generator https://rosettacode.org/wiki/Random_number_generator_(included)
IIRC, there’s a maxdepth flag you can use to do this.
I looked at my earlier post again and noticed something: When you use that `output=$(...)` method, you should probably add a `2&gt;&amp;1` to capture both normal and error output. It would look like this: output=$(candump can0 2&gt;&amp;1)
Yes, bash has very limited data structures while that's the functionality that I would like to have in a shell. That's one of the reasons why I am creating NGS. 
Either `cut` or `awk` are handy for taking a line of text, slicing it up into a series of fields (or columns), and let you specify some actions for specific fields. Using your example text, if we used the space character as a delimiter (default delimiter for both `cut` and `awk`), then you'd want fields #2 and #3 to get your timestamps.
TIL |&amp; exists!
 awk '!a[$2$3]++' Brief explanation: concatenate fields 2 and 3 (date and time), use them to index an associative array and increment its current value (defaults to `0` if uninitialized), take the value in the array before the increment, interpret it as a boolean (`0 = false`, `1 = true`) and invert it (`!`), and use it as condition for the default action (`{print}`, print the whole line).
https://stackoverflow.com/questions/1915636/is-there-a-way-to-uniq-by-column
You could do something like: if [[ -z $(candump can0 2&gt;&amp;1) ]]; then foo else bar fi
You sure it's not in the source file? Grep ~~doesn't~~ shouldn't just spit out info it doesn't have. Sort the file and do a unique on that field. Or use awk to extract it. Only other thing I can think of is that the ips appear multiple times on a line, and since your grep is just extracting matches and isn't line aware, it could duplicate if the content matches multiple times. 
So I went back again to the source data and yeah it was the problem. For some reason a whole bunch of entries had been duplicated and once that was dealt with everything was fine. &amp;#x200B; Now I feel stupid :P
It's a bit cumbersome, and may require advanced redirection juggling depending on where you want the commands' outputs to go. This one captures only time's output, and lets the two command's outputs go to the original stdout and stderr. { Extractiontime=$( { time { foo; foo2; } &gt;&amp;3 2&gt;&amp;4; } 2&gt;&amp;1) } 3&gt;&amp;1 4&gt;&amp;2 See [BashFAQ 32](http://mywiki.wooledge.org/BashFAQ/032) for more ways, and more in-depth explanations.
If you only care about wall-clock time, and you're happy to have a resolution of only 1 second, it might be simpler ignoring the `time` builtin and just doing something like: start=$SECONDS # run commands end=$SECONDS printf 'Extraction of files took %d sec' $(( end - start )) 
You will need semicolons immediately before `else`, `fi` and `done`. Check the syntax synopses with: help while and: help if
TIL you can assign to `$SECONDS`. Neat!
 grep '^https?://' "$file.m3u" | while read url; do if curl -s "$url" &gt;/dev/null 2&gt;&amp;1; then echo "Healthy: '$url'" else echo "Unhealthy: '$url'" fi done \`grep '\^https?://' "$file.m3u"\` gets the list of urls inside the file, it is piped \`|\` to a while loop which reads each one as the variable \`url\`. \`curl -s "$url"\` will try to resolve the url silently sending any output or error to \`/dev/null\` by redirecting both stdout and stdin \`2&gt;&amp;1\`. If the exit code of curl isn't 0 the url will be considered "unhealthy".
Sure, I also think it should follow redirects -L :) I just copy pasted OP's script.
As it's already explained, I would like to say that you can also use parallel to make it faster: `grep '^https?://' $file.m3u | parallel \[ \$\(curl -s -o /dev/null -I -w "%{http_code}" "{}"\) == "200" \] \&amp;\&amp; echo "Healthy: {}" \|\| echo "Unhealthy: {}"` It might be possible to avoid escaping too...
I created the .sh script and put few .m3u files in the same directory as the script. How to run the script to check the channels in each .m3u file? I am kind of a noob in bash scripting. 
 #!/usr/bin/env bash if [[ $# -lt 1 ]]; then echo "Give me a file !" exit 1 fi grep '^https?://' "$1" | while read url; do if curl -s "$url" &gt;/dev/null 2&gt;&amp;1; then echo "Healthy: '$url'" else echo "Unhealthy: '$url'" fi done That should be your script file. To make it executable `chmod u+x yourscript`. To execute it either place it in your `$PATH` (somewhere like `$HOME/.local/bin` ?) or execute it directly `./yourscript`. You'd use it like that `yourscript playlist.m3u` (or `./yourscript playlist.m3u`).
you don't have to write a `.sql` file to disk to do this although that may be easier for you depending on how complex your code is and whether you want to audit the sql query (like, if you want to save it to inspect later). but you can do something like: var=$( mysql -u "$username" -p"$password" -h "$host" "$database" &lt;&lt;-SQL START TRANSACTION; select * from users; update users set name = "roger" where id = 12; delete from users where email like "%@gmail.com"; COMMIT; SQL ) (forgive my crappy SQL. it's been quite a few years since I've done anything with mysql. I also omitted the `-E` because I'm not sure what that does. but you may need it. the `-e` is removed because I think that tells `mysql` that you're passing a file containing a query to it, which you don't need if you're passing the query to STDIN). This is utilizing a feature called "heredocuments" or "heredocs". Here is the documentation: http://tldp.org/LDP/abs/html/here-docs.html Basically, you define an arbitrary delimiter (here, I called it `SQL` but it can be literally anything as long as the first and last are the same) and everything that's between the delimiters gets passed to the command via STDIN. This would be similar to: cat myquery.sql | myql -u "$username" ... but with the added benefit of being able to dynamically construct your query and include bash variables (the sql code will parse the string for variables, so you can include that stuff in there). If you need more control over dynamic generation of the query, you can use a "herestring": query="select * from users;" var=$( mysql -u "$username" -p"$password" -h "$host" "$database" &lt;&lt;&lt; "$query" This does the same as the heredoc, except the value of the variable on the right side of the `&lt;&lt;&lt;` is passed to the command via STDIN. Documentation on herestrings: https://www.tldp.org/LDP/abs/html/x17837.html I hope this helps!
It's the stupidest things that get you. I just copy pasted you at first and didn't even look in details. You've got a interrogation mark in your grep match. I didn't even see it, if you remove it, it should work.
Are you serious with that 90s background music? And why don't you put the weather forecast in the prompt also. J/k, nice video. But seriously, less is more.
I am using the script you posted. I don't see interrogation mark in grep section. You mean this symbol '?', correct? 
Yes, a question mark.
The question mark is removed, but the script is not working for me. Maybe i am doing something wrong. 
[https://asciinema.org/a/hPyTcAHwWawg04qy4tfKTXsOk](https://asciinema.org/a/hPyTcAHwWawg04qy4tfKTXsOk)
Can't you choose what plugins to install? So it won't be "bloated"?
With oh my zsh? No.
My confs are posted in this thread if you wanna see a nonbloated zsh config. https://www.reddit.com/r/linux4noobs/comments/9emkvk/comment/e5qsxh0
This is all i do at work. Database processing with the help of bash for automation and such. Utilizing bash variables is a great way to avoid dynamic sql which makes me crazy. I actually wrote a little 4 line bash script that writes the contents of a file into a here doc within a temporary bash script. The temporary script is then run to produce the contents of the input file with all variables interpreted. This way i can store all my sql files with bash variables in them. So something like: `query="$(./interpfile.sh file.sql)"` will store the contents of the file with all the bash variables translated into the query variable.
MariaDB has a cron-like feature called events... I've used it for a handful of things -- it's pretty sweet!
Well actually I also need the milliseconds or at least one digit after the dot. Is this also possible in a similar way? Because I don't really like the other solution with redirecting to 2,3 and 4.
Can I see a few lines of your m3u file ?
Not easily, but we can encapsulate the complexity in a shell function. For instance, consider this: record() { read -r _ "$1" 3&gt;&amp;1 4&gt;&amp;2 &lt; &lt;( time -p ( "${@:2}" 1&gt;&amp;3 2&gt;&amp;4 3&gt;&amp;- 4&gt;&amp;- ) 2&gt;&amp;1 ) } This records the wall-clock of a command into a variable name supplied as the first argument, e.g.: record elapsed perl -MTime::HiRes=sleep -e 'sleep rand 1' echo "Took $elapsed sec" It should work correctly even if the command is a shell builtin or function, and it doesn't clobber redirections of the standard streams applied to `record` itself, e.g.: record elapsed something 2&gt;/dev/null With a bit more effort, it could be made to also not clobber _non_-standard streams (i.e. file descriptors 3 and up), but that would require using an additional helper shell function. Let me know if you need that.
[removed]
You can choose which plugins get loaded with oh-my-zsh, I personally use antigen which makes adding plugins from anywhere relativiely simple usually just one line in my config.
Here is the script (So I guess it is more than 4 lines): #!/bin/bash afile="$1" echo "cat &lt;&lt;AFILE &gt; /tmp/$$_.tmp" &gt; /tmp/$$.tmp { cat "$afile"; echo -e "\nAFILE"; } &gt;&gt; /tmp/$$.tmp chmod 777 /tmp/$$.tmp &gt;/dev/null 2&gt;&amp;1 /tmp/$$.tmp cat /tmp/$$_.tmp This essentially writes a bash script $$.tmp where the contents of the script becomes: cat &lt;&lt;AFILE &gt; /tmp/$$_.tmp # THE CONTENTS OF THE INPUT FILE AFILE Then the script is executed leaving your file with interpreted variables in $$\_.tmp. We output this with cat to print out the contents into stdout for you to do whatever you want with. &amp;#x200B; So if I have a file called file.txt with the contents: I like to say ${var} You could do this: var="Hello World.": export var # Exporting is necessary for this to work ./interpfile.sh file.txt You will see the output: I like to say Hello World. Note: I made this for my personal use and probably isn't very portable. It should probably add a shebang line to the top of the generated script for safety. The script also uses `echo -e "\n..."` to get the heredoc delimiter on the new line. I'm pretty sure that is not a posix use of echo, and your echo may not have that feature. The script would be better if it were converted to use printf.
My mistake. I should have seen that. But when i check a list it show all links as 'unhealthy' even that channels that are online. Any idea why? I am running the script from debian server installed on virtualbox. 
Well, shucks, that's because of the \\r\\n line delimeters. If you replace them with simple \\n it shoud work. &amp;#x200B; [https://asciinema.org/a/97LfV3tG50BIXg5bf7Tb5XQeJ](https://asciinema.org/a/97LfV3tG50BIXg5bf7Tb5XQeJ)
&gt; to remove \r, open the file in vim, type :%s/\r//g When i try to use that i get `to remove \r, open the file in vim, type :%s/\r//g`
I'm not sure if this is what you're looking for since you posted this in /r/bash, but you might want to look into [rundeck](https://www.rundeck.com/open-source).
There are some suggestions here: [https://unix.stackexchange.com/questions/314550/how-to-set-a-script-to-execute-when-a-port-receives-a-message](https://unix.stackexchange.com/questions/314550/how-to-set-a-script-to-execute-when-a-port-receives-a-message) &amp;#x200B; If any of the above are still too permissive for your tastes, I suspect you'd have to look into interfacing with sockets in your language of choice. At $work, we use a docker container that is really just a fancy expect script that listens on a specific port for something like this.
That's some good looking software, but I've already written a plugin for [cockpit](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/getting_started_with_cockpit/index) as my web interface. Besides, I'm more limited by what devices will be connecting as opposed to general use. Most automation devices support basic TCP, not so much REST API.
Try SSH + predefined configured command to run. Any examples on the net. Here's one: https://serverfault.com/questions/407497/how-do-i-configure-sshd-to-permit-a-single-command-without-giving-full-login-ac/407508#407508
That's pretty close to what I am looking for, assuming I can sanitize the inputs to avoid arbitrary execution.
There's an assumption there that the automation devices support SSH, many do not. Especially simple things like keypads or push buttons.
You should look at socat. Socat is netcat on steroids. 
Not Bash, but Golang. I am using [https://github.com/adnanh/webhook](https://github.com/adnanh/webhook) to expose some internal scripts and automatisms securely to the outside world (with ssl infront)
Hey it's 2018 do you know ansible ?
Why TCP? In that case you could even use UDP. You could also digitally sign and/or encrypt your messages. This way, it's not apparent to anyone that a port is even open unless they have the private key. Netcat or socat can do this. Python would be a better choice if you it's available.
Nevermind, i think bash script will not be a good solution for this. I even found another m3u checker script on [github](https://github.com/peterpt/IPTV-CHECK) but he also do not provide good results. Thanks, for the help, anyway!
Maybe can simplify by piping ls *L etc into your logic
&gt; But recently I wanted to limit the number of output files I get That command only produces one output file. &gt; That didn't work. Well, you _are_ quoting the text there, so it's just a literal string, not a glob. It's the same reason: echo '*LLL*' only prints out that single argument, whereas: echo *LLL* prints out the list of matching filenames. &gt; Adding the stars outside of the single quotes doesn't work either. No, because then you're going to have something along the lines of: [ -e aLLLa bLLLb cLLLc ] and the `-e` predicate only takes one operand. &gt; Anybody can hint on how can I encompass what I am trying to do into the condition? It's not exactly clear what you're trying to do. Test for the existence of _any_ files matching that glob? Since this is Bash, we can use Bash features like `nullglob` and arrays: shopt -s nullglob files=( *LLL* ) Now the `files` array contains the matching filenames, and you can decide what you want to do with them. For instance, you can easily get the number of them: printf 'Found %d matching files\n' "${#files}" or test the number of them: if (( ${#files} &gt; 0 )); then echo 'At least one matching file' fi
&gt; piping ls *L [Please don't.](https://mywiki.wooledge.org/ParsingLs)
You could well consider using `find` to, well, find the files you're looking for, if they exist. I'm not sure what your actual commands are, but if you're intending to operate on each file by itself something like find . -maxdepth 1 -iname '*lll*' -exec grep ... {} \; &gt; output.txt which uses `-maxdepth 1` to ensure we don't descend to subdirectories (since your example doesn't seem to). `-iname '*lll*'` uses a case insensitive match so you'll find `*LLL*` and `*lll*`, then `-exec` takes a command to run and puts `{}` as the matching filename in the arguments of the command you're running. `\;` terminates the `-exec` command (or you can use `+` in some versions so it'll execute the command fewer times, like using `xargs`). Of course, you needn't do anything like this in many cases because `grep` can read files, so you might be able to just do `grep pattern *LLL* *lll* &gt; output.txt` unless you really were meaning `cat -n` to turn on line numbers for it. The previous `grep` will just complain about there being no files if the globs fail to expand, so you might want to redirect stderr to `/dev/null` or similar
Thanks for the great response! I have essentially several operations that go under the "then" expression which I didn't want to detail What I am looking for is the following condition: If a file has "LLL" in the filename then {Execute everything under "then"} If there is no filename that has "LLL" in the filename then exit Obviouslt LLL is not the string I am looking for and is just an example of the 3 letters I am looking in the filename Make it LOG if it makes it easier to understand. So anything that has the letters LOG in it
&gt; If a file has "LLL" in the filename then {Execute everything under "then"} Do you want to "execute everything" for _all_ matching files at once, or do you want to do it for _each_ matching file one at a time? &gt; If there is no filename that has "LLL" in the filename then exit So I'd actually turn this around. Rather than putting the meat of your script in the `then` branch, make the `then` branch the exiting case: if (( ${#files[@]} == 0 )); then exit fi Then just follow on after this with the "yes, there actually is at least one file" case. If you wanted to do something for _all_ files at once, you'd just expand the array to a sequence of words: echo "Processing ${files[@]}..." do-something-with "${files[@]}" If you want to do something for _each_ file in that list, you could use a loop: for file in "${files[@]}"; do echo "Processing $file..." do-something-with "$file" done Just for completeness, I should point out that this latter situation could have been written like this: for file in *LLL*; do echo "Processing $file..." do-something-with "$file" done That is, by using the glob directly in the `for` loop. It ends up being the same sequence of words, after all. The `nullglob` shell option is critical, however: it ensures that glob expands to _no_ words when there are no matching filenames. Without this, you get the glob itself as a word, and that is probably undesirable.
I am not a programmer and am doing a very simple script for almost "Home use" really... All of these expressions and variables etc are too complicated for me at the time.... I am not looking to optimize my script at the moment... I just want to make an "IF" statement that will be able to look for files with 3 letters in the file name. If such a file exist - It will execute, and if such a file does not exist then it will not execute.... It already works without the "IF" condition... It is something I wanted to add recently that is all
Use a general purpose language to write your server. Using bash with socat, netcat is just a pain to handle especially if it is supposed to service more than one task.
Those suggestions aren't just "optimizing" they're also avoiding errors. I'd suggest you take this as an opportunity to learn a bit more about what you're doing, that way you'll do it well, know more, and won't run into a lot of problems in the future when something changes unexpectedly (or someone puts a file with a space in it in the directory or something like that).
&gt; If such a file exist - It will execute, and if such a file does not exist then it will not execute.... OK, that's fine. It's just that as I said right at the top, you can't actually _do_ that using `[ -e ... ]` (at least not directly), since `-e` only tests for a _single_ file. Globs implicitly test for existence: a glob can only expand to filenames that exist. So globs can be used to do what you want to do. Howeverr, seeing whether a glob expands to "nothing" or "not nothing" does unfortunately require saving the result of that glob somewhere. Using an array variable is one way. The other is to use the `$@` pseudo-array: shopt -s nullglob set -- *LLL* if (( $# == 0 )); then exit fi I do think having an explicitly-named array is a better approach though.
It's really hard to suggest the right way of doing things without knowing what you're actually trying to do. If you want to do a single task to each file, then `find` is a great way to go (it will automatically do nothing if nothing is found). If you have a bunch of steps to do, well, it all depends then, but aieou has given you some great info for that if you don't want to use `find`
&gt; Will that work? That's only going to run the `then` clause if there is _exactly one_ matching filename. Is that really what you want? &gt; Also can I set both LLL or lll ? I addressed this in an earlier post.
Yup, As of right now (And for my learning process) that's exactly what I needed. Also I've seen that you can set it under "Files" but it's important that it will be either `*LLL*` or `*lll*` And how do I use "Files" under "Set"? =S `set -- files=(*LLL* || *lll*)` &lt; Is that correct?
Um, no. All you need is: set -- *LLL* *lll* Think about what filename expansion actually _does_. It takes a glob (`*LLL*`, say) and expands it to a sequence of words. So if you've just got two globs next to each other, then all you end up with is a longer sequence of words.
Well, I don't have a clue what file expansion actually does (Unfortunately) That's why I've started with something as simple as [ -e And as it could not accomplish what I needed, I've turned here in hopes to find a work around! Thanks so much for all of you who commented (aioeu and beatle42 especially) I'll go test it and let you know if it worked
&gt; Well, I don't have a clue what file expansion actually does (Unfortunately) You probably do, you probably just don't know that that's what it's called. You know how you can do things like: ls *.txt to list all the filenames whose extension is `.txt`? That glob is expanded by the shell into the matching filenames: ls foo.txt bar.txt baz.txt _before_ the `ls` command is invoked. `ls` doesn't even see the glob. This is all that _filename expansion_ is: the shell turning a glob into a list of matching filenames.
 sed -i '4s/^.*$//' FILENAME
`sed -i 4d FILENAME` :)
Do you need to delete the 4th line or insert a blank line without clobbering whatever is already on the 4th line?
 ed -s /path/to/file &lt;&lt;&lt; $'4\nc\n\n.\nw'
i want to delete everything on line 4 but keep line 4 still there just empty
Ok then the other answers ITT ought to do it.
The end marker for the heredoc can be any word: it doesn't have to be `EOF`. So you can use a different marker for the outer versus inner heredocs: cat &lt;&lt; 'ENDSCRIPT' &gt; /script.sh #Beginning of script cat &lt;&lt;EOF &gt;&gt; "$DOCIMAPPENDINGTO" $STUFF $OTHERSTUFF EOF ENDSCRIPT
Oh indeed, not feeling so smart anymore now.. ha!
Thank you!
If your stance is "I can't make this happen with the concepts that I already understand, but I reject your answer because it uses concepts that I don't already understand," then we're really at an impasse here. 
What does the file structure look like inside each of these archives?
You can compile whatever version of ssh you like, the ciphers/macs/kex's it supports are determined by the underlying ssl library, and the library in OEL5 (because I'm familiar with RHEL5), supports two thirds of fuck all. Unfortunately you can't easily update the ssl library. You might like to look at containeris-ing your OEL5 host, or `chroot`ing or similarly sandboxing the underlying app. You may like to investigate using `~/.ssh/config` to use the 7.4 server as a proxy using the `ProxyCommand ssh proxy-host -W %h:%p` directive. Maybe that will work. Alternatively, you could look into sequencing the steps of the transfer i.e. app does whatever -&gt; on success scp the file to the 7.4 box -&gt; on successful transfer the 7.4 box sftp's the file to the vendor You could do that neatly with `cron` and a series of decently written scripts that do things like alert on failed conditions, or you could implement a system like rundeck or awx/ansible tower to automate it 
Thanks for the response. I initially attempted to recompile openssh7.x and backed out because of all the dependencies. For the time being, I have to deal with monolithic apps/servers because .....reasons. I have to solve this problem, and replicate the changes to prod, which is also OEL5. I'd love to containerize this as a proof of concept, but I'm in a bit of a time crunch and am still in the learning phase using docker. The method you describe is what I'm hoping to develop. The steps are: 1. Manual process kicks off the file creation and this script. 2. Script manipulates the file and moves it to a different location on the same server. 3. Script does sftp and error checking. 4. Script deletes original files, sends .out file to signify completion. So I need the script to send #3 to new server. 1. New server checks folder or gets notified that a new file is posted to kick off new script. 2. New server runs sftp and error check / deletes file. 3. New server sends OK to Old server to delete originals and create .out file. 
Not to ask a bunch of redundant questions, but what would be the correct syntax? Are there any modifiers? `sed 's/rights="none"/rights="all"' $HOME/Desktop/policy.xml`?
The `sed` command is also missing a trailing `/`.
I believe it'd be something like sed -e 's/rights=\"none\"/rights=\"all\"/' FILENAME 
Now you're thinking. So you could consider a couple of approaches within your plan... The first is to expect the file to be there by a particular time (i.e. `cron` jobs). Check for its existence, transfer it if it exists, if it doesn't: fail and maybe generate an alert through a monitoring system if you have one... maybe an automatic ticket in a ticketing system if you have one... maybe both if you have both... There's lots of edge cases with this approach. An obvious one that comes to mind is the race condition of: what if the file is still being created when the check for its existence occurs? The second approach is to look into filesystem events, so on OEL5 you're looking at the likes of `incrond` and `inotify`. Essentially they are file/folder watchers that can trigger actions on specific fs events. In your case you want to watch for a completed write of a new file within a directory, and upon closure of the write, kick off a script to transfer the file (i.e. scp using locked down keys). Obviously this lends itself towards the OEL5 host pushing the file to the 7.4 host. Great thing there is you can follow the same logic on the 7.4 host: watch a folder until a new file write is completed, then kick off the sftp script. `incrond` and `inotify` were abandoned IIRC because `systemd` gobbled up their functionality, so you may need to figure out how to do the same thing using `systemd` on your 7.4 host. That means that end to end, the file should transfer pretty much the moment it's created + transfer times. And then you can just have a couple of `cron` jobs to check that a file has been sent in the last 24 hours. How you engineer that is something I'll leave to you. 
Well with the help of the gentlemen in this subreddit I already have accomplished my goal. I understand that learning new things was the way to go, yet not in a way where I've been given tips for somebody who understands everything there's to know. Most of the suggestions here were simply giberish to me =/ and I needed a simple solution (Namely how to make the script find the file with the letters in the filename instead of using a defined string). I doubt anybody here would've wanted to give me an all inclusive bash course and training sessions &gt;_&gt; Either way, thanks for the help. I'll be coming back when I learn new things and need more help, maybe then I can understand you better
My installer has script within it that needed to be written to its own file. The script itself is full of heredocs, and therefore several 'EOF' instances. I did not want to echo every single line, so using a heredoc worked perfectly using the advice from the first comment. Further clarification: This is something that needs to be distributed as a single file, and not a folder or archive.
On a low level, what you want to do is to edit a file. The tool for that is `ed`, but that seems like nobody uses it for some reasons. The easy way is to use `sed`, with the `-i` switch. On the other hand, maybe there is a `lens` for your file, using `augeas`. `augeas` is a generic configuration handler, made by Red Hat, and it is the low level way to set up servers in a consistent way. For instance, `puppet` relies on `augeas`.
Its either a single .iso folder or a set of usually 4-10 folders with a couple files outside those folders. 
Sent you a DM with a pastebin link of the folder structures.
And, since it's macOS, if you don't have GNU sed, `-i` needs an argument (which can be empty): sed -i'' 's/\(rights=\)"none"/\1"all"/' "$HOME"/Desktop/policy.xml I've added a capture group, too.
You don't have to escape `"` within single quotes, and `-e` is just telling sed that what follows is a sed command, but this will still just write to standard output and not change the file directly.
I’m not familiar with `screen`, but one workaround that should work would be to create a script called `homebridge-I` (without a space) in `/usr/local/bin/` or a similar location, with the content #!/bin/sh homebridge -I "$@" and to launch that command from `screen`.
I'm still figuring a lot of stuff out, and that actually sounds like a pretty good solution. Two more questions: 1. Does the file need to have a certain extension? 2. How would I run it, for example if I simply name the file "hbboot" without extension and place it in `/usr/local/bin/` would I just write "hbboot" instead of "homebridge -I" in `rc.local`? I know a little bit, could I also just put "hbboot.sh" in my home folder and use `.~/hbboot.sh`? Not sure if that's the right syntax, I think I remember . making the .sh file run and ~ designating the home directory, but I could be way off.
1. No; Bash can/will attempt to run any file which has its executable bit set. 2. Yes, scripts can be run by name. Usually need to give the system a full path though, such as in part b below. 3. a) `.~` will be invalid because it's just invalid syntax in a script or the terminal itself. `~` expands to `/home/username` (the current users home folder), and `.` is the current location (like `pwd`), so `.~` would be `./home/username` which will not exist under normal circumstances as the root of the drive is `/`. Also, `~` will not expand in a script but `$HOME` will. 3. b) `.` being used to reference the current directory can be used to run a script that's in the current directory like `./script.sh`.
Ah, thank you for the detailed explanation, that’s really helpful. I always thought ./ was some magical way to run bash scripts, but now I realize it’s all about chmodding it to be executable. I’m a webdeveloper and in HTML and PHP I always just use the folder name or filename without a leading period and slash to reference the current folder, that might be why I got confused. Thanks again!
No problem :)
Does printf work on macOS tho? I’m away from it right now so I won’t be able to test it until tonight...
Lol Apple and sane are two words that don’t go together. 
Ok so I did some research but am not sure how to implement this, could you give me an example on how I would go about left-justifying one string, centering another and right-justifying the third on a single string using a fixed amount of 84 columns? I would really appreciate this, I’m pretty good with python but when it comes to bash, I’m pretty bad at it :/
Here's an example. There are a few ways to do this. This is just one way. You should maybe use $COLUMNS instead of a hardcoded value. ``` #!/usr/bin/bash set -x set -e #sample strings lstr="left" rstr="right" cstr="center" #figure out what the buffer would be on either side of the centered string buffer_len=$(( ( 84 - ${#cstr} ) / 2 )) printf "%-${buffer_len}s%s%${buffer_len}s\n" "$lstr" "$cstr" "$rstr" ```
Hmmm, thanks! But I'm unsure that it does what i need... I didn't mention something important: I want to decide as I go ( command 2 will depend on the result of command 1, so I can't type them at once). Is there a way to achieve this?
(command1 &amp;&amp; command2 &amp;&amp; command3) &gt; namedpipe should not do command2 unless return code from command1 is zero
No, what I mean is... I want to decide what command two is, depending on the result of command one. I mean high level decision here, not programmatic; so depending on the result of command 1, I need to choose a command 2 that could be anything 
 &gt;Intuitively, it does seem to me like I'd somehow have to redirect the output of my commands in such a way that they didn't have file terminators at the end. Is this possible? Pipes don't actually shut down when an EOF is indicated. If the process at the read end of the pipe is *expecting* multiple files, it can continue to read after the EOF is indicated and thereby receive multiple files from multiple independent senders. 
Thanks for the help! I really appreciate it!
Pretty sure mac uses zsh. Can't be bothered to Google it though ;). Also no idea how much difference there is between the two
 # Creating the pipe mkfifo mypipe # Reading process cat mypipe # Opening a file descriptor for writing to the pipe exec 3&gt; mypipe # Consecutive commands writing to the pipe command1 &gt;&amp;3 command2 &gt;&amp;3 command3 &gt;&amp;3 # Closing the file descriptor (the reading process receives EOF) exec 3&gt;&amp;- Instead of hard-coding (and possibly overwriting) the file descriptor you can make bash pick one by using the [redirection syntax](https://www.gnu.org/software/bash/manual/html_node/Redirections.html) `{fd}&gt;` and subsequently `&gt;&amp;"$fd"` for writing and `{fd}&gt;&amp;-` for closing.
If the terminal width isn't going to change you could maybe use column. `echo "$text1$text2$text3" | column -t -c &lt;terminal width&gt;`
Please bear with me as I ask a really dumb question. this is a ksh script and not bash and I'm running into a dumb issue. The process preceding this script outputs a file to an adjacent folder $anotherfolder/filename_here_XXXXXXX.txt. My current script is supposed to read the filename, extract the XXXXXXX and pass it along as $request_id . see below: request_id=`echo $1 | cut -f2 -d " " | cut -f2 -d "="` echo Request Id: $request_id For testing I captured the result, but I get nothing due to the undefined variable. So i tried something like this: 1=$anotherfolder/filename_here_*.txt request_id=`echo $1 | cut -f2 -d " " | cut -f2 -d "="` echo Request Id: $request_id and I get a file not found error even though I can ls this file. how do I fix what appears to be a simple definition issue? is there something special about defining variables in ksh that I'm unaware of? BTW, Just so you know I'm not completely brain dead, I can do the transfer part of the script on the other system fine using a cron job and systemd. This part of the script is just for record keeping. 
[removed]
The error message tells you what the problem is. You need to put the path before the first expression which in this case is `-type f`. Also, drop the double quotes around the path. Those are unnecessary and can potentially cause problems when being interpreted multiple times.
Yes, this will work: ssh root@192.168.0.5 find /var/www/html/ -type f &gt; /disktmp/iad3_blogsdir.txt 
Can you give an example of one the chunks?
You got powershell?
When I'm trying to strip out all the HTML markup and process only the content that would be visible to a human using a browser, I usually try the dump mode on a terminal-based browser like lynx or elinks. elinks --dump &lt;url&gt;
Someone know what is the problem ?
&gt; on the end of the sh i put to restart the whole script Can you post the code for that part? Try running your script with `bash -x script.sh`: each command will be printed before it's run, so you can see which command has hung.
That is the whole code but is copied multi time like this \#!/bin/bash export DISPLAY=:0 &amp;#x200B; expressvpn connect "Location" sleep 25 /home/viral/Desktop/firefox/firefox "imacros://run/?m=co1.iim" &amp; sleep 200 killall firefox expressvpn disconnect sleep 10 &amp;#x200B; expressvpn connect "Location" sleep 25 /home/viral/Desktop/firefox/firefox "imacros://run/?m=co1.iim" &amp; sleep 200 killall firefox expressvpn disconnect sleep 10 &amp;#x200B; &amp;#x200B; expressvpn connect "Location" sleep 25 /home/viral/Desktop/firefox/firefox "imacros://run/?m=co1.iim" &amp; sleep 200 killall firefox expressvpn disconnect sleep 10 &amp;#x200B; . . . . &amp;#x200B; ./script.sh and this need to run all the time. I don't have experience with linux so please if you can help me. &amp;#x200B; Thanks &amp;#x200B;
I will try to run like this and wrote what i get as error.
You could just put the main part in a loop like this: #!/bin/bash export DISPLAY=:0 while true; do expressvpn connect "Location" sleep 25 /home/viral/Desktop/firefox/firefox "imacros://run/?m=co1.iim" &amp; sleep 200 killall firefox expressvpn disconnect sleep 10 done This will essentially put your code into an infinite loop. Looks like your call to the script in the end is probably what causes the crash eventually. Its like a recursive script. I have no idea what the outcome of that would be. Again, this is really not a good way to do this. Using a killall after some predetermined length of time is like trying to kill an ant with a shotgun. I highly recommend researching crontab. crontab in linux is sort of liked the task scheduler in Windows. 
If I were you, I'd use XPath in favor of regular expressions. If your output doesn't need to be formatted or aggregated, then `xmllint` can run an XPath query quite simply. For bigger jobs, `xsltproc` can process an XSLT stylesheet containing your query. You'll be able to format your output to your liking. It's really best if you don't think of HTML/XML as text when you want to process it. Think about the data-structures it contains, and use a tool that lets you access those structures. I like to think of XPath as regular expressions for HTML/XML, which makes `xmllint` comparable to `grep` and `xsltproc` comparable to `awk`. 
Sorry, I'm not sure what your goal is, but here is a nice article about keeping scripts running that should give you some options: https://hackaday.com/2017/03/10/linux-fu-keeping-things-running/ One thing that I have done with some success is to have the script create a file with its Process ID (PID) as the contents of the file, and have a cron job that reads the PID out of the file and checks to see if there is a running process with that ID, and if not, restarts the script.
Came here to say use ‘while true’, but ya beat me to it!
`timeout` is a good fit here: https://www.gnu.org/software/coreutils/manual/coreutils.html#timeout-invocation
Or xmlstarlet? Roast me!
That does look like a better fit. I've never used that.
I keep hearing the name here and there, but I just haven't tried it yet. 
There's also the html-xml-utils package. I'm not sure how it compares with XPath.
probably some kind of stack overflow because of the infinitely recursive call, i.e. `./script.sh` at the end of the script. 
I'm really wondering, why exactly does fzf not work?
add while true; do: then rest of the code below it and end it with done? will rerun the code all the time or some other kid of infinite loop?
Of course. For my use case, the development servers I work on most of the time are RHEL5 and we have no permissions to install third party libraries. If I were to ask for something to be installed without a reasonable business case, it would have surely been rejected. So installing Go wouldn't have been feasible for the install. It's a shame, because whilst I do enjoy building things like this, I'm also a lazy programmer and would have liked to have had a tool available immediately for my use!
Also might need double quotes on the function \`bind '"\\C-r": "peco-history-selection"'\`
Yeah, that's not it. That was just me typing here. 
Use `tee`: $ tee --help Usage: tee [OPTION]... [FILE]... Copy standard input to each FILE, and also to standard output. ... $ x | tee file.txt
thanks very much :)
An easy way to deal with creating aliases is to issue history | sort | less and browse the output. It will show you what commands you use often. Those are good candidates for aliases. 
`!!` returns the last command issued. echo 'test' test alias tt="!!" tt test This has a caveat - the type of quotes matters. If you are mindful of that, this will create an alias. However, this will not be written to your bashrc. `alias` command lists all your aliases. You can create a separate file for aliases (e.g. `~/.aliases`) and just pipe your aliases into that file once you are done creating aliases. Just add `source ~/.aliases` at the end of your bashrc. `history` command lists the previous commands used. There is a limit for that (default, I think is 500), which can be edited. You can pipe that to awk/sed/sort etc to get counts. 
An improvement - history | tr -s ' ' | cut -f 3- -d ' ' | sort | uniq -c |sort -nr | head Gets history, removes repeated spaces, strips serial numbers (from history), sorts, deduplicate and keep count of duplicates, sort by numeric value in reverse, and display top ten of that. This whole command can be aliased to something easy to remember/type. 
IMO aliases are for ~~lazy~~ pretty good sysadmins who know what they're doing. Using aliases on a lazy purpose to speed up your work without understanding those commands is like avoid learning stuff . Get lazy when you're ready not before. 
Like this? 0 18:30 ~/ $ for i in /sys/class/input/event{0..17}/device/name; do [[ -r $i ]] &amp;&amp; echo "$( cat $i ) $i"; done Power Button /sys/class/input/event0/device/name Sleep Button /sys/class/input/event1/device/name Lid Switch /sys/class/input/event2/device/name Power Button /sys/class/input/event3/device/name AT Translated Set 2 keyboard /sys/class/input/event4/device/name SynPS/2 Synaptics TouchPad /sys/class/input/event5/device/name Video Bus /sys/class/input/event6/device/name Video Bus /sys/class/input/event7/device/name Dell WMI hotkeys /sys/class/input/event8/device/name HDA Intel PCH Mic /sys/class/input/event9/device/name HDA Intel PCH Headphone /sys/class/input/event10/device/name HDA Intel PCH Headphone /sys/class/input/event11/device/name HDA Intel PCH HDMI/DP,pcm=3 /sys/class/input/event12/device/name Laptop_Integrated_Webcam_2HDM: /sys/class/input/event13/device/name &amp;#x200B;
LOL yeah, Thanks!
Thanks!
I think this is the culprit. The `test` builtin doesn't have a `&gt;=` operator, it uses `-ge` instead. [Shellcheck](https://www.shellcheck.net/) is useful to find these (and many other) kinds of problems. &gt; `elif [ "$queuecount" &gt;= 100 ]; then` 
Use double brackets for all your conditionals.
&gt; queuecount=`echo $queuelength` &gt; if [ "$queuecount" == "" ]; then isn't queuecount setto "echo NUMBER" here? I think what you want is &gt; if [ -z "$queuelength" ]; then ? Meaning if quelelength is unset. Or you want &gt; if [ "$queuelength" -eq 0 ]; then I guess?
Man that is a great site. It's been a while since I had to write a BASH script. I'm going to have to bookmark that. 
I've tried that... It exited with errors.
Yeah. In my initial test it didn't like queuelength so I passed it all into another variable and it seemed to like it. Using the site JP posted, I was able to clean up the code. Seems to work now. 
Yeah that Shellchecker site mentioned that, for which I did change. 
Log story short, my company is acting as a "msp" to this other smaller company. They do not have an SNMP server and my boss doesn't want to go through the hassle of setting it up, etc. On top of that, they will be switching to O365 soon (I've been here three months and that's the official word... "soon") This little script is my work around since. 
It’s also a command line tool. Which I find more useful. 
When programs read stdin. Systemd things don't do that, so...I guess: don't use pipes! When manipulating streams of data, e.g. sorting, then grep, then column, eventually lolcat.
shellcheck is pretty awesome and you can install it in the OS so you can go like shellcheck [script.sh](https://script.sh) and it will tell you whatever the issue is. 
To my mind, aliases should generally be used for setting the desired default behaviour of a command. Most anything beyond that should be a function.
&gt; for i in /sys/class/input/event*/device/name; do echo "$( cat $i ) $i"; done FYI the `cat` here is a [UUOC](http://porkmail.org/era/unix/award.html). You can achieve this simply like so `$(&lt;$i)`. This is also where using meaningful variable names helps e.g. `$(&lt;"${event}")` OP, if you're comfortable with a different format, then this could be simply achieved like this: # grep . /sys/class/input/event*/device/name /sys/class/input/event0/device/name:Power Button /sys/class/input/event10/device/name:HD-Audio Generic Mic /sys/class/input/event11/device/name:HD-Audio Generic Line /sys/class/input/event12/device/name:HD-Audio Generic Line Out /sys/class/input/event13/device/name:HD-Audio Generic Front Headphone /sys/class/input/event14/device/name:HD-Audio Generic Front Headphone /sys/class/input/event1/device/name:Power Button /sys/class/input/event2/device/name:Video Bus /sys/class/input/event3/device/name:Logitech USB Receiver /sys/class/input/event4/device/name:Logitech USB Receiver /sys/class/input/event5/device/name:HP WMI hotkeys /sys/class/input/event6/device/name:HDA ATI HDMI HDMI/DP,pcm=3 /sys/class/input/event7/device/name:HDA ATI HDMI HDMI/DP,pcm=7 /sys/class/input/event8/device/name:HDA ATI HDMI HDMI/DP,pcm=8 /sys/class/input/event9/device/name:HDA ATI HDMI HDMI/DP,pcm=9 And because this output is delimited by colons, you can easily work on it at the field level. For example, we can use `awk` to switch the fields: # grep -H . /sys/class/input/event*/device/name | awk -F ':' '{print $2 " " $1}' Power Button /sys/class/input/event0/device/name HD-Audio Generic Mic /sys/class/input/event10/device/name HD-Audio Generic Line /sys/class/input/event11/device/name HD-Audio Generic Line Out /sys/class/input/event12/device/name HD-Audio Generic Front Headphone /sys/class/input/event13/device/name HD-Audio Generic Front Headphone /sys/class/input/event14/device/name Power Button /sys/class/input/event1/device/name Video Bus /sys/class/input/event2/device/name Logitech USB Receiver /sys/class/input/event3/device/name Logitech USB Receiver /sys/class/input/event4/device/name HP WMI hotkeys /sys/class/input/event5/device/name HDA ATI HDMI HDMI/DP,pcm=3 /sys/class/input/event6/device/name HDA ATI HDMI HDMI/DP,pcm=7 /sys/class/input/event7/device/name HDA ATI HDMI HDMI/DP,pcm=8 /sys/class/input/event8/device/name HDA ATI HDMI HDMI/DP,pcm=9 /sys/class/input/event9/device/name Note that I've added `-H` to `grep` to force it to output the filenames, that way `awk`'s input should be as expected. And most instances of `grep | awk` can (read: *should*) be merged into a single `awk`, this example could probably be written a bit safer but for this use case it's fine as-is: # awk -F ':' '/./{print $1 " " FILENAME}' /sys/class/input/event*/device/name Power Button /sys/class/input/event0/device/name HD-Audio Generic Mic /sys/class/input/event10/device/name HD-Audio Generic Line /sys/class/input/event11/device/name HD-Audio Generic Line Out /sys/class/input/event12/device/name HD-Audio Generic Front Headphone /sys/class/input/event13/device/name HD-Audio Generic Front Headphone /sys/class/input/event14/device/name Power Button /sys/class/input/event1/device/name Video Bus /sys/class/input/event2/device/name Logitech USB Receiver /sys/class/input/event3/device/name Logitech USB Receiver /sys/class/input/event4/device/name HP WMI hotkeys /sys/class/input/event5/device/name HDA ATI HDMI HDMI/DP,pcm=3 /sys/class/input/event6/device/name HDA ATI HDMI HDMI/DP,pcm=7 /sys/class/input/event7/device/name HDA ATI HDMI HDMI/DP,pcm=8 /sys/class/input/event8/device/name HDA ATI HDMI HDMI/DP,pcm=9 /sys/class/input/event9/device/name So many ways to achieve the same thing. Now, some people would say that the `for` loop is fine - and in this case, sure. But it's still often worth exploring different ways to approach your problems. One of the tasks I have performed in my career as a *nix sysadmin is reviewing and fixing scripts that other people have written, usually because they're broken or performing badly. I've easily fixed simple scripts where know-nothing colleagues have bleated about how awful shell scripts are and how we need to switch to `perl`/`python`/`ruby`/`javascript`/`whatever` to replace them. In some cases that may be true, but in many cases it's simply a matter of not being a dumbass. One script I fixed went from nearly 8 hours of processing time down to 5-10 minutes on average!
I've seen the very same error. Here my suggestion: once you manage to identify the line that is generating it, replace ... [ ... ] ... witn ... [[ ... ]] ... 
This is interesting and surprising. I'll be watching this space for enlightenment as it has me bamboozled too!
Can you declare a function and pipe to that?
I wonder if cmd_a and cmd_b are in different sub-shells, or cmd_a is in a sub-shell and cmd_b isn't.
You should be able to put the source statement in a conditional ala ``` if [[ something ]]; then source file fi ```
I've tried -- it causes the same behavior either way, unfortunately.
Yeah, been struggling with it. I've been trying tricks with exec to copy the filedescriptor. Problem is , the first program is going to close the filedes. So there needs to be a program that creates a "tee". I wrote one a long time ago. It would be a simple program to write again. In the meantime, you might checkout the program socat. It has a lot of options for redirection and splitting for networking..and for files. If you are desperate, I can write the program. 
And get rid of all the needless quotes! `[[ ... ]]` doesn't do word-splitting, and anything that can reduce the visual noise in a script should be embraced.
I'll try to help get you started. If you describe for me what you want to pull from your HTML document, I can see if it's simple enough that I can write up a sample query for it.
I appreciate the helpfulness. I think it's mainly because of my lack of HTML knowledge. Example: http://bugmenot.com/view/virustotal.com Intention: Parse Username, Password, Success rate The HTML formatting was like so: article class="account" data-account_id="1903632"&gt;&lt;dl&gt;&lt;dt&gt;Username:&lt;/dt&gt;&lt;dd&gt;&lt;kbd&gt;JohnDoe2000&lt;/kbd&gt;&lt;/dd&gt;&lt;dt&gt;Password:&lt;/dt&gt;&lt;dd&gt;&lt;kbd&gt;Dontbugme&lt;/kbd&gt;&lt;/dd&gt;&lt;dt&gt;Other:&lt;/dt&gt;&lt;dd&gt;&lt;kbd&gt;plzdontbug@mailinator.com&lt;/kbd&gt;&lt;/dd&gt;&lt;dt class="stats"&gt;Stats:&lt;/dt&gt;&lt;dd class="stats"&gt; &lt;ul&gt; &lt;li class="success_rate success_90"&gt;84% success rate&lt;/li&gt; &lt;li&gt;43 votes&lt;/li&gt; &lt;li&gt;6 months old&lt;/li&gt; &lt;/ul&gt;&lt;/dd&gt;&lt;/dl&gt;
This sounds like a problem for [named pipes](https://www.linuxjournal.com/content/using-named-pipes-fifos-bash).
I created an XSLT stylesheet that you can use if you install the app `xsltproc` on your Linux/Unix/etc box. The stylesheet I create is [here on pastebin](https://pastebin.com/5Fyd8Xmf). Download that file to your box and call it `stylesheet01.xsl`. Then you can run this command: curl -sL "http://bugmenot.com/view/virustotal.com" | xsltproc --html stylesheet01.xsl - 2&gt;/dev/null The output is text, fields delimited by tabs and records delimited by new-lines. The first row is column headers. If you use [`cut`](https://www.computerhope.com/unix/ucut.htm) or `awk`, you should be able to work with the text very easily. Let's get back to the stylesheet for just a minute: Within the stylesheet are 3 XPath queries that do most of the work. These queries are responsible for returing the Username, the Password, and the Success Rate from the source HTML. The comment that I made earlier was that you should use XPath instead of regular expressions. I hope you'll take the opportunity to experiment a little and see how XPath works. If you're going to search websites (not some random XML files on your computer) then I'd recommend you use Chrome and pop open the Dev Tools, go to the Elements tab, click on the frame for the source code of the page, then hit CTRL-F on your keyboard. In the bottom of the frame you'll get a search bar. Here you can search by text, by CSS selector, or by XPath query. When I want to find out if my XPath query is correct, this is where I go. I can see the results updated immediately. If you Google you'll find more resources to help you learn XPath and XSLT. I think W3Schools' introduction is a good explanation, and when I simply want a reference of keywords and syntax I go to MDN. I'd be very surprised if this answers all your questions, but hopefully it is enough to get you started. If you've got more questions or something went wrong that prevents you from working with this stylesheet, just let me know.
This is insane! I can not thank you enough for the amount of work you put into helping. Seriously, thank you! I'll get myself acquainted with this syntax because this is what I've needed to be using.
That's what I thought too! But to no avail, it behaves the same as with unnamed pipes.
Yep, I probably do belong in an asylum for considering this a fun activity ;) I hope you can get `xsltproc` installed and working and it all works out for you. Happy to have helped.
/usr/lib/libreoffice/share/gallery/sounds/apert.wav 
Hm I might need to do this too. I have it set up so i get a notification when a process which took longer than 60 seconds is done. Might want to add a oink sound if the exit code is != 0. :D 
Sort of works: yes "I Like Pie" | ( head -5 ; sed '1,10s/^/&gt;/' | head -10 ; hexdump -C ) | less 
I like to use short pulses from that PC speaker everyone hates, with a tool such as [beep](https://packages.ubuntu.com/source/bionic/beep). Main advantage for bypassing the main sound card is that the loudness always remains the same. Something like: `beep -f440 -l20 -D40 -n -f587 -l20` works perfectly for me.
The issue is that A reads lots of data at a time for performance reasons, and since you can't "unget" data from a pipe, A can't put the extra data it read back. For your desired behavior, A has to read exactly the number of bytes it needs, not more, which, for most programs (including line-based programs), means it has to read one byte at a time, e.g. because if it read two bytes and the first were a newline terminating the line it was supposed to stop at, it'd have read one byte too many. No utilities read one byte at a time by default because it's extremely slow, but, as you found, you can change the behavior of the standard library IO functions to read one byte at a time. This won't help you with utilities that don't use standard library for IO or modify its options, like GNU head, and in some cases, a program needs to read [x+y | y&gt;0] bytes in order to figure out that it only needed to handle x bytes, in which case there's nothing you could do. Files are different because you don't need to put the data back, it's already there, and you can seek backwards to where you were supposed to stop. $ trace () { strace -o '| sed -rn "s/^(read|lseek)\(0/&gt; &amp;/p"' "$@"; } $ seq 4 &gt; file $ cat file | { trace sed -n 2q; cat; } &gt; read(0, "1\n2\n3\n4\n", 4096) = 8 &gt; lseek(0, -4, SEEK_CUR) = -1 ESPIPE (Illegal seek) $ { trace sed -n 2q; cat; } &lt; file &gt; read(0, "1\n2\n3\n4\n", 4096) = 8 &gt; lseek(0, -4, SEEK_CUR) = 4 3 4 $ { trace stdbuf -i0 sed -n 2q; cat; } &lt; file &gt; read(0, "1", 1) = 1 &gt; read(0, "\n", 1) = 1 &gt; read(0, "2", 1) = 1 &gt; read(0, "\n", 1) = 1 3 4 Replace sed with head and see how the read size doesn't change when you specify `stdbuf -i0`.
&gt; In what situations would you use command substitution vs. piping? when the command in question (timedatectl in this case) does not support taking arguments from stdin.
Starcraft wav files. &amp;&amp; say construction_complete || say this_is_very_interesting_but_stupid
none. I hate notification noises. 
 beep
Just for educational purposes you can use $(pwd) to grab your current directory. 
Homer Simpson Success: http://www.richmolnar.com/Sounds/Homer%20-%20Woohoo!%20(1).wav Failure: http://www.richmolnar.com/Sounds/Homer%20%20D'oh!%20(1).wav
Just out of seeing this wanted to give my own version here. rm results.txt; head -1 *.txt &gt;&gt; results.txt Should work I believe. I've got no capacity to test it right now however. 
Lol I did mention that, but +1
Yeah sorry my bad. 
No worries. Glad to have a second opinion :)
I had no idea -- that's awesome. Time to read the sed manual...
1st 10 seconds of schatten aus der alexander welt by Bethlehem https://m.youtube.com/watch?v=ZKZfb_3meq0#
OP, if this is yours, please run your code through http://shellcheck.net and fix the mistakes. Also, it's called `line_extractor_from_textfiles.sh`, which is a misleading name with a meaningless extension (i.e. don't use `.sh` or `.bash` for your scripts, only for libraries). It could be called something like `print_first_line`, which now implies that it will take an argument or glob e.g. `print_first_line somefile` or `print_first_line *` As others have pointed out, `head -n 1 *.txt` will also get the job done without the need for a loop, but for a laugh: `grep . -hsI -m 1 *.txt` will do it too (GNU `grep`). For a slightly bigger laugh: for txtFile in *.txt; do &lt;"${txtFile}" read -r printf -- '%s\n' "${REPLY}" done I'll leave `awk`, `sed`, `perl` and other implementations to others to contribute
Something super simple, this here plays the bell, the same sound you hear when you hit backspace at an empty prompt: printf "\a" What's interesting about it is, you might be able to configure your terminal program so that it will tell your desktop that something is "urgent". How this is done depends on the terminal tool. For example, I couldn't find out how to enable it with Gnome's terminal tool, for "termite" it's done in its config file, and for "urxvt" it's done in ~/.Xresources. When a windows is "urgent", your desktop will do something like highlight the window's button in the taskbar. For example with i3, the workspace indicator gets highlighted and the tile's title bar gets highlighted.
this!
My server plays the Imperial March when it's finished booting
So 'sed' is a good player and tries to rewind its input if it reads more than it needs. Interesting! bash's '`read -n 1`' also appears to be a good guy as evidenced by this: seq 4 | { while read -n 1 i; do echo "$i"; [[ "$i" == 2 ]] &amp;&amp; break; done; echo "cat"; cat; } 1 2 cat 3 4 ... perhaps OP can use that somehow - as /u/r3j mentioned, it won't be fast but ...
Maybe get the PIDs of the required processes and see if they're running with '`pkill -0 $PID`' - it returns 0 if the process is still there and 1 otherwise. Or perhaps you should be using the restart facility of systemd or init
pew pew
yes but i have no clue how to do this script :( can anyone do this for me?
lose the cat and the pipe. You are using stdin twice: &lt; Is $1 a filename? The brackets are probably supposed to be double quotes 
$1 is the first input. Didn't realize the cat was redundant. Dont need to double enter now, but there is a blank line below the output when the program runs.
Please describe your program.
its a codebook encryption program. so the first argument is the codebook (basically just a bunch of letters to substitue a-zA-z ) itself and the second is what you want to encrypt. so if you had Hello World or something it would output Oxac Vlawo for example
Thanks, I hate it. But I do appreciate the creativity
Please tell me this comes from the motherboard speaker
Yes. It's a server in a rack, it's not gonna play through my home theatre system.
if you use `&gt; results.txt` in place of `&gt;&gt; results.txt`, won't it simply overwrite the file and remove the need for `rm`? I think OP's use of `&gt;&gt;` is somewhat necessary because it is used inside a loop. But since your method needs no loop, it seems like you could clobber the file instead of appending to it.
If they're going to be run multiple tumes as a command, yes. It's not grep.sh, it's grep. 
I see what you’re saying, but I do think it depends. If it is a utility which is to be used frequently, then yes absolutely either drop the suffix or symlink into $PATH with a proper name. However, I find myself writing one off scripts all the time, and prefer to name them with a suffix. This allows you to easily identify what is a binary executable and what is a script (and thus tinker-able).
ah HA. I didn't think the results appearing in the search. Ok, that makes sense.
You might want to add a shebang as well. 
I'm not actually sure how the shell handles these multiple-matches into a pipe scenarios. I know you can run into some problem with xargs trying to run the same command multiple times with preceding wildcard matches. I'll play around with it tomorrow and report back :)
So I tried `head -n 1 *.txt &gt; results.txt` and the first time it worked perfectly... Because *results.text* didn't exist yet. Then after talking to you I ran it again, and sure enough, *results.txt* appeared in the output.
Yes, it's poor form. For a wide range of reasons, which I will leave to you to lookup, because otherwise I'll be here for hours writing a novel. You'll find the majority opinion at the likes of stackexchange, stackoverflow, superuser, hackernews etc will probably agree with me in some way. The Google Shell Style Guide linked in the sidebar also strongly advocates for no extensions except on libraries. Finally, run this - and note that the `file` command is a better way to identify what a file actually is rather than depending on a suffix which is meaningless on a *nix system: `file $(which $(compgen -c) 2&gt;/dev/null) | grep script` You might need to modify it slightly depending on your version of `which` e.g. `file $(which --skip-alias $(compgen -c) 2&gt;/dev/null) | grep script` The results that look out of place have suffixes. On a test VM here, that's 14 out of 334. Not-suffixes is the standard, and best, practice.
Yup. I'd avise straying away fr using xargs unless forced to mainly because the pipe redirection takes care of that. There's situations where xargs is necessary because you have multiple fields to fill or certain commands don't properly output data and you need xargs to handle it. Also you are correct as stated. &gt; Writes to the file while &gt;&gt; appends. The rm is necessary 
I don't mean to be over critical, but where did you learn to create functions like: unixtime() { date +%s } Traditionally, you might (instead of calling it `$(unixtime)`) do something like `$(date +"%s")` or unixtime=$(date +"%s")` and reference `$unixtime`. What is the gain of making a function to call a single command?
I have a system set up to conveniently play arbitrary sounds. I have just shy of 100 in the list at the moment, but some standouts that might be useful for alarms are: * Various flavours of klaxon: * Star Wars * Submarine * Something from Venture Bros. I've never watched the show, but it's probably the most intrusive of the entire collection. * Metal Gear alarm sound * Zelda item-get/puzzle-solve sounds. * Various Warcraft sounds, including: * Flag cap sounds * WC3's "Work work" and "Job's done" from Peons/Peasants * Dark Souls death I also added network support to the scripts so that remote and/or speakerless systems can still trigger a sound on my current machine (so long as they can reach back to it). On the client end it's now just a matter of exporting an environment variable for a target. For the moment, none of these sounds are actually baked into any of my own scripts to avoid annoying me. When I use them it's usually something like `do-the-thing &amp;&amp; sound-zelda-secret` or doing a `wait-for-pid the-thing &amp;&amp; sound-jobs-done` (`wait-for-pid` being a separate script that waits for a process to end) I haven't published the sound scripts anywhere public so far, but if any of these systems are of interest then I can do so.
Thanks a lot for suggestions. Your solution is of course fine and standard. Here is my reasoning on this: \- Sleep won't help, you need to read and discard inputs. Unless I would parse the dmesg line timestamps, which is too complex. Is there big downside to not closing the stream? \- writing same command with date is duplication and hard to read. \- foreach eval is dangerous, but does the job quick and simple if u r responsible.
The line that you are showing here should work fine. I am guessing the mistake is somewhere else, perhaps in the spot where you set your $DOCKER_HUB_PASSWORD variable. Here's an experiment about this done at the bash prompt: $ bar=hello $ foo='abc$bar' $ echo $foo abc$bar
It's set in the interface of the CI (Bitbucket pipelines). And like I said, I tried single quotes and it didn't work. :( Maybe I'll raise an issue with Bitbucket.
very strange that the 'ou' from ”you" isn't eat by variable substitution. I would have guess It only output "hello" with the variable `$You` replaced by nothing
Agreed.
Ooh, that's a good one. Will try it!
I do mostly agree with you. The only place I differ is if writing a one off script which is not in $PATH, I typically append .sh. But, alas it wouldn't hurt to drop the suffix all together to keep things consistent, where of course there IS inherent value. Tbh I did not know it was a widely adopted practice, thanks for pointing it out!
&gt; Unless I would parse the dmesg line timestamps, which is too complex. Is there big downside to not closing the stream? No, nothing really wrong with keeping the stream open, though it does depend on how the commands in your chain manage memory and other resources. The issue is you're watching for a flag that could be missed, and there might be something in `/proc` or `modinfo psmouse` that you could test for, every 3 seconds. Sleep is nicer to the CPU \*shrug\* &gt; foreach eval is dangerous, but does the job quick and simple if u r responsible. It's better to learn and practice alternatives, so you don't build bad habits when you have to throw something together quickly, and it ends up in production. :)
you could make a template for the output and replace marked keywords with the real info maybe? 
So I'm going to his you've defined those variables earlier in your Makefile: DICKER_HUB_PASSWORD=hello$You What you have to remember is that these are *Make variables*, not shell variables. There are three ways of referencing a Make variable within a Makefile: 1. With parentheses: `$(var)` 2. With braces: `${var}` 3. If and only if it's a single character, bare: `$v` It is strongly recommended you use the first form for your own variables, the third form for automatic variables, and the second down never. So in your Makefile the Truckee stars when you set that Make variable. It contains that this form of variable reference, and presumably you don't actually have a `Y` Make variable defined anywhere. To set it correctly, you would need to use: DOCKER_HUB_PASSWORD=hello$$You `$$` is expanded to `$` at the time the variable is defined. This alone won't solve your problem, however. When this variable is expanded, you'll be left with a single dollar sign, and you need to make sure this isn't treated as a shell metacharacter. You should therefore enclose the word in single-quotes: docker login --username '$(DOCKER_HUB_USERNAME)' --password '$(DOCKER_HUB_PASSWORD)'
Thanks, but it doesnt work this is my bash file: `#!/bin/bash` `SCREEN -dmS Xbot_instance_1 php core.php -i 1 &amp; EXPORT instance_1_PID=$!` &amp;#x200B; `if ps aux | grep $instance_1_PID &gt;&gt;/dev/null ; then exit 0 ; else cd /root/xbot/ %% ./restart.sh ; fi` and i got `root:~/xbot# ./script.sh` `-bash: ./script.sh: /bin/bash^M: bad interpreter: No such file or directory` &amp;#x200B;
I had not heard of it prior to your mention of it. Thanks. 
What would a template look like? I considered working with \`column\` but that seems like too much overhead for this use case. 
&gt;&gt; foreach eval is dangerous, but does the job quick and simple if u r responsible. &gt; It's better to learn and practice alternatives, so you don't build bad habits when you have to throw something together quickly, and it ends up in production. :) Let's not forget that code explicitly shared publicly eventually becomes "role-model" code for others who are learning to code. There's a really huge difference between writing a script to run privately and sharing a script publicly with a blog post and social media. 
I call xxd a lot for terminal passwords; this is lifted from a proxy\_on function (at work): echo -n "Password: "; read -s PASSWORD; PASSWORD=$(printf $PASSWORD | xxd -plain | tr -d '\n' | sed 's/\(..\)/%\1/g'); It really just turns it into hex value, which is useful for special characters (@#$, etc) more here: [https://linux.die.net/man/1/xxd](https://linux.die.net/man/1/xxd) &amp;#x200B;
something like hostname..........: &lt;&lt;HOSTNAME&gt;&gt; uptime...........: &lt;&lt;UPTIME&gt;&gt; where you can replace with sed `sed "s/&lt;&lt;HOSTNAME&gt;&gt;/$HOSTNAME/" &lt; template`
&amp;#x200B; &amp;#x200B; [Bash-Beginners-Guide](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/Bash-Beginners-Guide.html) [seq command](https://www.howtoforge.com/linux-seq-command/) [Bash arrays](http://www.tldp.org/LDP/abs/html/arrays.html) &amp;#x200B; &amp;#x200B;
a bit clumsy but i had a go... &amp;#x200B; \#! /bin/bash &amp;#x200B; \# Take the limits of a sequence by ardgument then pass sequence to array &amp;#x200B; array\_x=( $(seq -s' ' $1 $2) ) &amp;#x200B; echo Elements in Array: ${#array\_x\[\*\]} &amp;#x200B; for i in $(seq 0 $(expr ${#array\_x\[\*\]} - 1)) do echo Array\\\[$i\\\] = ${array\_x\[i\]} done &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
- Thanks for the modinfo tips. It is very possible there is a better way for this. - Sure. Perhaps &gt; alias foreach-do=while read -r line; do It would be used as: &gt; foreach-do echo "$line"; done;
 $ for i in $(seq 1 12); do a[$i]=$((i*i)); done $ for i in $(seq 1 12); do echo "a[$i] = ${a[$i]}"; done a[1] = 1 a[2] = 4 a[3] = 9 a[4] = 16 a[5] = 25 a[6] = 36 a[7] = 49 a[8] = 64 a[9] = 81 a[10] = 100 a[11] = 121 a[12] = 144 
 $ echo {1..12} 1 2 3 4 5 6 7 8 9 10 11 12
 &gt; a='1-12' &gt; a=$(seq -s , ${a%-*} ${a#*-}) &gt; echo a=$a a=1,2,3,4,5,6,7,8,9,10,11,12
/[\d]+$/
How would you use this in a find command? I'm not getting any results find . -regex '/[\d]+$/'
Server1\.log[0-9]{5}
The beginning of the file name changes. Any idea how to use the [0-9]{5} syntax with the find command?
[This page](https://www.computerhope.com/unix/ufind.htm) has some more info about the `-regex` option of `find`. &gt; Returns true if a file name matches the regular expression pattern. This is a match on the whole path, not just the file name. For example, to match a file named './fubar3', you can use the regular expression '.*bar.' or '.*b.*3', but not 'f.*r3' (because the complete path does not begin with an f). The regular expressions understood by find are by default Emacs regular expressions, but this can be changed with the -regextype option (see above). I think you'd want to change the format of your regular expression, maybe use something like this instead: find . -regex '.*[0-9]+' For what it's worth, I've never actually used this feature of find. I've usually piped the output of find into grep. But TIL !
`find . -regextype egrep -regex ".*[0-9]{5}"`
If you do this, you can just use the normal wildcards that you know from the command line. You don't have to use regex. This here matches `.log` and five digits at the end of a file name: find . -name "*.log[0-9][0-9][0-9][0-9][0-9]"
I'd like to say if youre gonna go for the egrep regex type then try either "\.log[[:digit:]]*" should work. I believe in the regex use of the wildcard means the previous, any number of times, not exactly as globbing would work in normal bash shell.
It really needs to be a function. It can't be a variable. The script would not work right if it's not a function. I don't know how to explain this well. Just look at the code and think about what the values are at the different lines when the 'while' and the 'if' are doing their thing.
&gt; 'portable' Hmmmppphh ▓▒░$ uname -a SunOS ares 5.9 Generic_Virtual sun4u sparc SUNW,SPARC-Enterprise ▓▒░$ date +"%s" %s 
Few things here. Firstly; though not breaking your code - your shebang is wrong. It should be #!/bin/bash without a space. Next up - don't call bash with sh. They are not the same thing. Set executable permissions and run it with ./filename or call it with the specific shell - i.e; bash &lt;filename&gt;. Don't use '-n' when calling your shell either. Just use 'shell filename' - i.e; 'bash myscript'. I've pasted what -n does from 'man sh' below: -n noexec If not interactive, read commands but do not execute them. This is useful for checking the syntax of shell scripts. And finally - use pastebin or any other number of text paste sites when giving code for people to look at. Images are the devils work.
Use (( .... )) to compare numbers.
Yeah. Great point. Is that even using bash? Can you do `unixtime() { date +"%s" }` and call `unixtime` instead and have it work?
New snag. Once I get the 2 files over to the new server, I have PUT1234567 and the FILENAME1234567.lst files. I still need to build a log file so i can send a successful completion back over to the old server and finish the original script. Basically it just CAT's the sftp command and creates the .out and .log files. #sftp -v $OPTION -b $TRANSFER/incoming/PUT* $SERVER &gt; $TRANSFER/PUT$request_id.log 2&gt;&amp;1 #cat $TRANSFER/PUT$request_id.log #if [ `cat $TRANSFER/PUT$request_id.log | grep "Uploading *_file.*lst to /$INBOUND/*_file.*lst" | wc -l` -gt 0 ] #then # echo `date`: File transfer executed &gt;&gt; $TRANSFER/out/o$request_id.out # echo File transfer executed # rm -f *_file*.lst # scp $TRANSFER/out/o$request_id.out OLDSERVER:$REMOTE/bin/incoming/o$request_id.out # echo `date` : WF TRANSFER Copied to OLDSERVER &gt;&gt; $TRANSFER/PUT$request_id.log The $request_id variable is my issue. How to i get that variable? It is the number part of both the PUT and .lst file and will change every time this process is run. I've been playing with sed and cut to no avail and Thanks to /u/whetu for the helpful hints so far. 
I know you're posting in r/bash, but awk was born for this. `awk '{ print $0 &gt; "s" NR ".txt" }' filename` Where filename is your file with the multiple lines.
He explained that he likes the function because it documents the code. He doesn't like seeing `date +%s`, so he just gave it a name. We don't know what his background is. He mentioned functional programming. For example in a language named "Haskell" using simple functions is no problem. They are the same as declaring a variable, there's no downside for performance or anything. It even looks the same, for example: foo = 123 max a b = if a &gt; b then a else b Personally, I think I would have written that debounce() as follows, ditching all functions same as you would do, and I remembered there's the special $SECONDS that's built into bash that can be used here: debounce() { local interval limit line interval="$1" (( limit = SECONDS + interval )) while read -r line; do if (( limit &lt; SECONDS )); then (( limit = SECONDS + interval )) echo "$line" fi done } 
I remembered there's a special $SECONDS variable in bash that you might be able to use here instead of "date +%s". The $SECONDS variable changes by itself over time. By default it contains the seconds since your script has started. If I understood things right about that debounce() function, this here should behave the same: debounce() { local interval limit line interval="$1" (( limit = SECONDS + interval )) while read -r line; do if (( limit &lt; SECONDS )); then (( limit = SECONDS + interval )) echo "$line" fi done } You can also write a value into $SECONDS. This will not break its special behavior. Bash will still continue adding seconds to it while time passes. You might be able to do something with that.
Would awk not be considered bash for some reason? That's not supposed to have any condescending inflection on it, by the way. Legitimate question here.
Thanks! 
It seems you are comparing the variable $NUM when you only defined $NUM**2**
Bash is only a shell, shells are "just" interpreters. Many languages are interpreted, Python's REPL is also a shell. And so on. The relationship between things is interesting.
wget: double-u-get curl: kerl *grabs popcorn*
wait are there people that actually pronounce each letter? or... god forbid, **'et cetera'** ?!
 $ seq 4 | split -dl1 $ grep . * x00:1 x01:2 x02:3 x03:4
Yep, that's `bash`. `%s` is not POSIX, so Sun's engineers decided to do what they do best. To get epoch time on Solaris and other non-%s implementations requires weird and wonderful approaches. On Solaris up to version 10 IIRC, you can use `perl`, or pluck it out of `truss date` e.g. ▓▒░$ perl -e 'print time."\n";' &amp;&amp; truss date 2&gt;&amp;1 | awk -F '=' '/time()/{gsub(/ /, "", $2); print $2}' 1537915583 1537915583 Noting that I have `PATH` setup to use the better `xpg4` toolset A similar approach works on earlier versions of FreeBSD too. There is also the majority of a mostly `bash
Ok so it’s not my-ess-que-ell ? I never know what to say 
Officially, it is as you said. But no matter which way you say it, you’re going to upset someone. :)
Yeah, I hope OP realizes there's a huge difference between looking up the correct syntax for a command versus looking up the correct pronunciation for a command's name. Usually the syntax will be uniform, but the pronunciation will vary. (I never imagined that someone might spell out "curl", and yet here we are). OP, the most important thing that you can do is to "fail gracefully" when a pronunciation "mismatch" is encountered. There's nothing worse than when two parties encounter a mismatch and both throw fatal errors, resulting in a kernel panic.
... and it's lin-nux, not lie-nux !! Stress is on the first syllable. The origin for this is - the way Linus T. pronounces it.
... and yet 'see-you-are-ell' has it's own logic. Maybe that's the Right Way after all!!! I always just said 'kerl'.
I've seen curl stylized as **cURL** to emphasize how clever they are for incorporating URL into the name. I'd have to guess that's why some people spell it; a word in caps looks like it should be spelled ( like FBI, CIA, etc). We even spell URL, we don't say "yuuuurl".
&gt; &gt; /dev/null 2&gt;&amp;1 `&gt; /dev/null` means redirect the stdout to /dev/null `2&gt;&amp;1` means redirect stderr to stdout (which is /dev/null in this case).
I didn't mean to imply `date` had any dependency on shell. I meant more the declaration of a function. Didn't think bash was the default in 5.9.
Ga-New, lol.
Thank you ! So this would also work? sudo python standalone.py -p 9998 -w ../example/ &amp;&gt; /dev/null &amp; disown Is 2 always stderr and &amp;1 always stdout? 
It depends on a few things, as far as I know. Some of the acronyms and their pronunciation have roots in military, telecom and other areas. Regional conventions may play a part, as well. The rule of thumb to speak caps puts cURL at odds with the popular pronunciation. Americans tend to emphasize the phonetics of the acronym vs shortening the word, like 'eth' vs 'eeth', 'vee-eye' vs 'vie', that sort of thing. "E.T.C." vs "et c..." There are so many old Usenet posts about this sort of thing.. 
&gt;So this would also work? Yes. &gt;Is 2 always stderr and &amp;1 always stdout? Yes, 2 is always stderr and 1 is always stdout, and 0 is stdin.
https://www.telegraph.co.uk/technology/10072420/Creator-of-the-GIF-says-it-is-pronounced-Jif.html The creator of the GIF file format has sparked a lively debate on Twitter after announcing that it must be pronounced "Jif". 
thanks! ive been trying to get this for like an hour, have a great day!
Cheers, much appreciated. 
we.... we don't?
&gt; "et c..." ...as in "eht-see"? I've never heard somebody pronounce it that way. Though it would be convincing and clever if that was where Etsy.com got their name. I looked it up and disappointingly, it's not. &gt; Kalin said that he named the site Etsy because he "wanted a nonsense word because I wanted to build the brand from scratch. I was watching Fellini's 8 ½ and writing down what I was hearing. In Italian, you say 'etsi' a lot. It means 'oh, yes' (actually it's "eh, si"). And in Latin and French, it means 'what if.'" -https://en.wikipedia.org/wiki/Etsy
*...And so began the long war between "yuuuurl" and "oooorull" ...*
Actually, I'm more curious how people say `zsh` and `.zshrc`. I've taken to saying "zish" and "zish are see", personally. 
One of my favourites is still the variety of ``fsck`` pronounciation. I've heard all manner of things, "fisk" seems to be quite, but my favourite is definitely "eff-suck". 
&gt; Firstly; though not breaking your code - your shebang is wrong. It should be #!/bin/bash without a space. It’s definitely not *wrong*. The form without space is more common, but the space has always been allowed, and according to a persistent misconception it was even required in very very early UNIX versions.
In Latin: et cetera, abbreviated etc. Meaning: everything else. Not the os, system binaries, or libraries, but everything else not otherwise specified, config files mostly.
Ah yes. The GIF. The favored file format of NASA (Nah-ay-sah), the POTUS (Puz-uce), and SCUBA (scuh-bah) aficionados. Also used to display silent videos of LASERs (lass-ear). Wait, that's not how any of those acronyms are pronounced? Must be that the words making up the acronyms have no bearing on the pronunciation of the acronyms ;) Remember, "Choosy developers choose gif" ;) 
No, it's pronounced gif, just as it should be, with a soft g :P 
Been doing this for about 7 years and, locally (west coast) i haven't encounterd many variations on pronunciations other than fsck "fisk" vs "eff seck" Curl = kerl (like the word) Vi = "vee eye" /etc = "slash etsy" Wget = "double u get" My point not being i think it's the "correct" way, just that it seemed to be fairly universal in my interactions with other linux / unix admins and I never really thought about it much 
Lee-nux is what he says
WTF
This is the same thing, in a more compact format: num=$((RANDOM%13+1)) 
I quite like "eff-sick"
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
And or Or perhaps? (&amp;&amp; or ||). Whichever works for you. &amp;#x200B; So something like elif \[ $NUM -ge 4 &amp;&amp; -le 7 \]
You should split your condition (it's easier to read) if [ "$n" -ge 4 ] &amp;&amp; [ "$n" -lt 7 ]
I'm pretty sure `[` doesn't like too many arguments like `&amp;&amp;`. Use -a or -o instead, or my proposed solution to OP.
What course are you doing - out of curiosity?
I'll be honest, I didn't fact-check my response, as I never use the "dash" syntax for my conditions.
That's the most common pronunciation I've ever heard of it. Every professor and coworker I've spoken to about it calls it "eht-see". But as someone who's been learning things via text and pronouncing things wrong as a result all my life: it doesn't matter at all. Most times everyone will know exactly what you mean, worst case scenario is someone being a bit of a dick and making fun of you for it.
&gt;I can do this with tar and using -s &lt;regex&gt; but unzip doesnt seem to have the same options. I always get this wrong, but either tar has certain unzip options embedded within it, or gzip has certain tar options within it.
The one that really gets under my skin is "Klee" for CLI. 
Hmm I can try grip and see where I get, thanks for the tip!
Does your version of `unzip` not allow using the `-d` flag to specify the unzip directory? If not, try creating and moving to the directory you want the files unzipped to, and then calling `unzip` from there using the full file path of the zip file
Eeeeeeteeeeeeceeeee or death
OK, for those of you still playing along at home, i found it fairly simply. I just passed along the request_id from the old server to a new location and did : get_request="/incoming/ID/location/*" request_id=$(basename $get_request) Yay $basename! Boo another folder and file to take care of. 
Here is a pretty neat [article](https://wiki.archlinux.org/index.php/Bash/Prompt_customization) about customizing your bash promt for you.
You want to change the PS1 variable. Look it up in the manual.
Don't use `[` in bash, use `[[ ]]` to test strings and files, and `(( ))` to test integers. #!/bin/bash num=$1 if (( num &lt;= 4 )); then printf '%d is less than or equal to 4\n' "$num" elif (( num &lt;= 7 )); then printf '%d is greater than four and less than or equal to 7\n' "$num" else printf '%d is greater than 7\n' "$num" fi
I like your polite way to say "rtfm" 
Search the web for "bash prompt generator"
LIUITM just doesn't have the same ring to it.
Oh god, that's get me. Never encountered that before, thank God.
If I had to guess, it's a split between people who say 'zshell' and 'z shell config file' and 'zeeshrc'
Give [bashrcgenerator.com](http://bashrcgenerator.com) a go. Once you construct your PS1, place the PS1="...." into your .bashrc file and you're away!
 export PS1="\w -&gt; " 
Thanks for the tips! 
Thanks! I was looking videos about how to handle options when I came across the video that explains it, and I had to try it
tangentially try [bash-it](https://github.com/Bash-it/bash-it). Fare warning on older systems. it adds significant delay, to opening new terminal instances. It does have quite cool things, that I use daily. Some obvious aliases, completions and plugins for npm, ng etc.
All you need to accomplish that is `wmctrl -a Waterfox || waterfox`.
Try this: wmctrl -xa Waterfox || waterfox
Same behaviour
Did you uncapitalise Waterfox and put the x in square brackets? Try the command as I posted it please.
Please post the output of `wmctrl -lx | grep -i waterfox`
&gt; wmctrl -lx | grep -i waterfox 0x0120014e 0 Navigator.Waterfox mndrgr Contradicting results involving booleans : bash - Waterfox 
&gt; wmctrl -xa Navigator.Waterfox || waterfox Same behaviour :/
Same behaviour :/
Maybe [this](https://askubuntu.com/questions/394998/why-wmctrl-doesnt-work-for-certain-windows#395476) is related?
Maybe, so try this: wmctrl -l | sed '/Waterfox/!d' | awk '{print $1}' | xargs -I {} wmctrl -i -r {} -b add,above &amp;&amp; [[ $? -eq 0 ]] &amp;&amp; echo true || waterfox
This just opens a new waterfox instance and does not focus it.
Something worked: `wmctrl -a Waterfox || waterfox` did not work on `zsh` But running the following on `zsh` works: `sh -c 'wmctrl -a Waterfox || waterfox'` What the actual f***
Thanks for going through this rollercoaster with me dude. I just cannot explain this.
I didn't test it but I think so.
Can you test it by replacing Waterfox with some other browser, and some other application. Both the simple version `wmctrl -xa Application || application` and the buggy workaround we tried above.
You weren't trying this in bash?
Does the tool manage to focus the window if it's there? If it does, you could check if the program is running or not with a different tool, and then only use wmctrl to do the focus. You could do the following (I experimented with Firefox as I don't have Waterfox): if pgrep -x firefox &gt; /dev/null; then wmctrl -a firefox; else firefox; fi For the record, using wmctrl alone works fine for me. I experimented like this in a terminal window: wmctrl -a firefox || firefox
I think the problem was that I was running wmctrl boolean tests in zsh instead of bash. It works now, as expected!
`awk '{print $1"="ENVIRON[$1]}'` A &gt;&gt; B
POSIX awk has `ENVIRON`, so I'd do this: awk '{print $0"="ENVIRON[$0]}' In your example, it's bash telling you "bad substitution" because it's trying to expand `${&amp;}` before passing it to sed, and that's invalid. $ :${&amp;} -bash: :${&amp;}: bad substitution
I literally just solved it :) awk '{ print $$1"="ENVIRON[$$1] }' A &gt;&gt; B double $ because of `make` 
This is /r/bash. Why use Sed for the job when Bash can do it itself? while IFS= read -r var; do printf '%s=%q\n' "$var" "${!var}" done Reads newline-separated variable names from standard input, writes out appropriately shell-quoted assignments to standard output.
Shell variable substitution happens before running the command (in this case `sed`). It's possible to do what you want by using `eval` which would look something like this: eval `sed -E 's;[A-Z0-9_]+;echo &amp;=\${&amp;};' A` &gt; B But some would consider use of `eval` in any form unsecure. An alternative would be to use `envsubst` like this: sed -E 's;[A-Z0-9_]+;&amp;=\${&amp;};' A | envsubst &gt;B Using `envsubst` requires that VAR1 and VAR2 are exported. Also, your pattern was wrong because it didn't include the digit in the character set. BTW `envsubst` isn't standard on OSX but eval should work. 
Change `s/[/\\[/g` to `s/\[/\\[/g` and `'s/]/\\]/g'` to `'s/\]/\\]/g'`
This really looks like you're solving something that shouldn't actually be a problem. It should rarely be necessary to _add_ escaping to a pre-existing string in order to use it. How do you intend to use this extra-escaped string?
You con't need to escape these symbols if you properly quote your variable when using it in the "mv" command. mv file "$folders" Above will work with no problem, and you are likely to introduce much more problems the way you do it now. 
I could certainly be doing something wrong or have screwed up an environment variable or something. I am not great at bash. I brought in the path listing from fd (similar to find). Similar to find, you I had to use the equivalent of -print 0 to separate elements by null symbols. From that string I used sed to format it so that I could turn it into an array. I have a for loop that loops through the directory paths and moves PNG files to a directory that was created during the script (changes based on where the script is run from). i=0 \# Move files out of subdirectories for file in ${pngs\[@\]} do mv "${input}/${folder}/$(dirname ${pngdir\[$i\]})/${file}" "${input}/${folder}" i=$((i+1)) done I have tried double quoting the variable names and tried leaving them without quotes. I have tried double quoting each individual variable rather than the full path, too. I keep getting: mv: rename /long/path/file.png to /long/path/file.png No such file or directory" The path names in the error message look correct, which made me think I needed to escape everything. Still having problems even with the escaped paths. The error message says "rename" instead of "move", which I thought was weird.
And always use ${var} notation!
It is more a matter of style, but sometimes avoids errors and confusion and is good for consistency, so yes you are right. 
![gif](https://i.imgur.com/AJ63BuV.gif)
Why this doesn't render the image on browser? ![animated gif](https://i.imgur.com/AJ63BuV.gif "tokens")
[https://i.imgur.com/AJ63BuV.gif](https://i.imgur.com/AJ63BuV.gif)
&gt; Head doesn't work unless you combine it with awk because head *.txt will print the filename Is this a problem? OP's was including the filenames in the output. (OP's source below:) for file in $script_full_path/*.txt; # for every text file in current folder do echo "Copied first line from $file"; head -n 1 $file &gt;&gt; $script_full_path/results.txt # copy first line in text file to results.txt done I did some research into globbing and I found a new way to write this command: head -n 1 [^results]*.txt &gt; results.txt `[^results]*.txt` will match any file that ends in **.txt** except **results.txt**. By skipping over **results.txt**, it's not necessary to delete the file prior to running the command. Then the contents of **results.txt** can be overwritten with `&gt; results.txt`. I tested it out and it worked for me, but that's based on my understanding of the requirements. If I've missed something, please let me know.
Wow, I am new to bash scripting so this was extremely educational, thanks for pointing out all the mistakes, appreciate it.
This here won't work right: hostname+=\("acme21"\) It makes bash treat that stuff like text. It will see the operator `+=` instead of the `+=()` you want, and it will see a text `"(acme21)"`. It then treats "hostname" as a text variable. There's also a line where you do this with a variable "choices". Here's an experiment at the command line: $ foo+=\(hello\) $ echo "$foo" (hello) $ foo+=\(hi\) $ echo "$foo" (hello)(hi) In comparison without the `\` added: $ unset foo $ foo+=(hello) $ foo+=(hi) $ echo $foo hello $ echo "${foo[@]}" hello hi 
Thank you by the feedback! I'll try to understand and apply these suggestions.
To do it like this I think you'd need to export -f replayg after you finish declaring it .
Interesting question. Since only bash knows how to execute bash functions, you would have to tell `find` to run the function in bash. But since that is happening in a subshell you also have to export the function to make if visible to subshells. export -f replayg find . -type f -exec bash -c 'replayg "${0}"' {} \; I suggest using `'replayg "${0}"' {}` instead of ` 'replayg {}'` to avoid problems with quoting, odd characters, spaces, ... 
If you have a list of the paths separated by spaces: $ for v in Dir1/a1.jpg Dir1/a2.png Dir1/Dir2/a3.jpg; do basename "${v%.*}"; done a1 a2 a3 Or, if you have a list with a path in each line... $ cat paths.txt Dir1/a1.jpg Dir1/a2.png Dir1/Dir2/a3.jpg $ while read v; do basename "${v%.*}"; done &lt; paths.txt a1 a2 a3
Thank you, problem is now that whitespaces seem to break the script. Here's the output when i run it in a test folder https://pastebin.com/raw/YXqUQRJn
Okay, not all command line utilities are equally well implemented I guess, but then you can use your approach with [[]], but perhaps the find and the function is the problem of the old script, and the for loop will solve this then. 
do you know if there is a way to do it with cp?
Yes, by using the right tool: cat
This whole thread is art. It belongs in a museum.
It depends on what you really want to do. Based on your description it sounds like you want to combine fileA and fileB into a single fileC containing the contents of both, which is exactly what `cat` is for (though much more commonly used to print file contents directly to STDOUT). If, in your example, fileC were an already existing directory, then fileA and fileB would be copied as distinct files into the directory fileC, but that doesn’t sound like what you really want.
DUUUDE!!! ...
cat filea &gt; filec &amp;&amp; cat fileb &gt;&gt; filec
Why not have a little fun ;) \# cat \~/selectFiles/{fileA,fileE} /allDaFiles/\* \~/withPrefix/dogs-\* &gt; filesAboutDogs
This ought to do what you want: cat ~/directory1/fileA ~/directory1/directory2/fileB &gt; ~/directory1/directory2/fileC 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
I haven't tested it myself. But try to name your options variable in such way: options=("Option 1" "Option 2" "Option 3" "Quit") Source: https://askubuntu.com/questions/1705/how-can-i-create-a-select-menu-in-a-shell-script
It displays the options really weird that way. Ideally I could just hide the output and echo whatever I want instead but I haven't figured out how to do that yet. 
You should use a Case Statement. It's like an If/Else Statement. Here is an example of script with options.. #!/bin/bash # case example case $1 in start) echo starting ;; stop) echo stopping ;; restart) echo restarting ;; *) echo don\'t know ;; esac
Agreed. Use a Case Statement for your options. 
If you extract the code to find a random file into a function – function randmp4 { ls movies/*.mp4 | sort -R | tail -1 } – then you can call it repeatedly until you find a new file: m1=$(randmp4) m2=$m1 until [[ $m1 != "$m2" ]]; do m2=$(randmp4) done
Could that be expanded? I need to add 4-6 files in this script from the movies directory. m1 and m2 are as far as I've written the code for so far.
Side note: your code to find a random file will likely fail on paths containing spaces. I suggest this alternative: function randfile { local files=($1) printf '%s\n' "${files[RANDOM%${#files[@]}]}" } To be called with a *quoted* glob pattern: m1=$(randfile 'movies/*.mp4') Alternatively, you can define a function to return a random *argument* – function randarg { local args=("$@") printf '%s\n' "${args[RANDOM%${#args[@]}]}" } – and then call it with an unquoted glob pattern.
GNU coreutils has `shuf` Pipe your list to that, and pipe that to `head` with the number you want from it. 
Can't you just do it in one go if you always run similar code for each file? You could first create six lines of shuffled names, then read those into a loop with that code you want to apply to each of them. I'm thinking of something like this: count=6 find movies -name '*.mp4' | shuf -n $count | while read -r file; do base=${file##*/} cp "$file" "upload/$base" printf "file '%s'\r\n" "$base" &gt;&gt; upload/buildlist.txt done
I'm actually testing a similar method right now... # Features movies=`ls movies/*.mp4 | shuf | head -4` m1=`echo "$movies" | sed -n '1,1p'` m2=`echo "$movies" | sed -n '2,2p'` m3=`echo "$movies" | sed -n '3,3p'` m4=`echo "$movies" | sed -n '4,4p'` Not as elegant, but it works. After this, I need to add shorts and bumpers. And get the whole package down to under 20 GB.
You can create those m1 to m4 variables with bash alone without 'sed', like this: { read -r m1 read -r m2 read -r m3 read -r m4 } &lt; &lt;( ls movies/*.mp4 | shuf | head -4 )
Yes, that's how it is. You can only use the `{a..b}` feature of bash with literal values, you can't use variables. If you only care about numbers, you can use the "C programming language" style of the 'for' loop: for (( value = $1; value &lt;= $2; value++ )); do echo $value done You can also use the 'seq' tool: for value in $(seq $1 $2); do ...; done If you really need `{a..b}` because your problem is about text characters, you can try to cheat with 'eval', which is ugly and can be dangerous: start=a end=f for value in $(eval echo {$start..$end}); do echo $value done
This is the actual script I am trying to write: for value in {1..4} do echo run $value proj2 &lt; public0$value.in &gt; mypublic0$value.output echo diff $value diff public0$value.output mypublic0$value.output done echo done proj2 is a c program I wrote for class. I want to run the program with x input files and output them to x output files that i can then compare to what they are supposed to look like. but I want to be able to test arbitrary ranges of files, for example 2-5 or whatever
Use that form here: for (( value = $1; value &lt;= $2; value++ )); do ... done 
ooh cool, I didn't think of that. Thanks :)
I'd suggest `seq`
just for giggles, how would you write a program that ran the middle part for all files of the form public*.in and public*.out?
just for giggles, how would you write a program that ran the middle part for all files of the form public*.in and public*.out?
Put the filenames in an array: movies=( movies/*.mp4 ) n=${#movies[@]} # array length Now you can pick a random index and grab the file at that index r=$(( RANDOM % n )) m1=${movies[r]} Then replace the picked value with the array's last element, and then decrement the length (`n`) of the array movies[r]=${movies[--n]} Rinse and repeat until n reaches 0 r=$(( RANDOM % n )) m2=${movies[r]} movies[r]=${movies[--n]}
for value in $(seq $1 $2)
 for file in public[0-9][0-9].in; do outfile=${file%.in}.output proj2 &lt; "$file" &gt; "$outfile" diff "$file" "$outfile" done
You could perhaps change your script so that it does expect the full input file names as arguments, not just the numbers. You would then run it like this from the command line: ./yourscript public*.in Or you can do something like this: ./yourscript public{01..04}.in And the script could then look like this: for file in "$@"; do # that "$@" is a list of the command line arguments out="${file%.in}.output" myout="my$out" echo "run proj2 on '$file'" proj2 &lt; "$file" &gt; "$myout" echo "diff '$out'" diff "$out" "$myout" done 
1. Why sleep and slow down your script? 2. No error handling. What happens if something goes wrong? (see `set -e`) 3. `rm -rf /tmp/*` is bound to break things (systemd directories, mpd fifo, bspwm socket...).
Thank you. 1. Taken from [https://github.com/Utappia/uCareSystem/blob/master/ucaresystem-core](https://github.com/Utappia/uCareSystem/blob/master/ucaresystem-core). Had the impression that it was useful because it paused the script between each command. If not required, I'll remove. Before today, I did not have those sleeps. 2. `/usr/bin/dpkg --configure -a` and `/usr/bin/apt install -y -f` are there to repair anything that may go wrong with regards to the upgrades. ref: [https://forum.pinguyos.com/Thread-Automatic-Updating](https://forum.pinguyos.com/Thread-Automatic-Updating). Are these not enough? 3. I think I better remove that, after all, Ubuntu does clean it on restart or shut down - can't remember right now.
Can you highlight which parts are unnecessary besides `rm /var/lib/apt/lists/* -vf` ? I agree that `rm -rf /tmp/*` is dangerous. I shall remove. &gt;rm /var/lib/apt/lists/\* -vf &gt; &gt;By why? In truth I do not need this, actually, it is wholly unnecessary. I only included it because quoting from source: [https://forum.pinguyos.com/Thread-Automatic-Updating](https://forum.pinguyos.com/Thread-Automatic-Updating), *Because of issues I have seen with the Spotify repo this will force the regeneration of the package lists.* 
You can break your system with this script. Apt isn't perfect, it does stupid shit sometimes, packages can have bugs etc etc. Running these commands blindly is not a good idea. As the other /u/moviuro mentions, you have no error checking, any of these commands could fail, yet the commands that come later assume success. Imagine your script is cruising along, it executes `/usr/bin/apt --yes --assume-yes full-upgrade`, which can remove packages, however it fails partway through and doesn't install the replacement. Next, you kick off this badboy: `/usr/bin/dpkg -l | grep '^rc' | awk '{print $2}' | xargs /usr/bin/apt --yes --assume-yes purge`, and you blow away the old package and all your configuration. Oops. TBH, there are many ways that all of this could go wrong. You would do well to listen to /u/moviuro's advice and look into `set -e` at a minimum. Also, unless you are critically short on disk space, you don't need to be cleaning everything out every time you update your system, it's only freeing a few megs.
I am against running apt-upgrade and its variations unattended. Especially on distributions like Ubuntu where package compatibility is not the first priority. If it was a CentOS machine, I would consider it but only after thinking ten times whether that machine is important to me in terms of stability. The point of this comment is not that Ubuntu is a bad distribution but packages have to be updated carefully. 
That was tremendously helpful. I redid the entire script using the way you explained and it worked perfectly. I appreciate your help, it was very helpful! :)
There's a "Unix time" date format which is "seconds since 1970". This means it's just a number, so it's good for comparing dates in scripts. This here prints the date of a file in Unix time, using my ~/.bashrc as the example file: $ stat -c %Y .bashrc 1533811606 The current date in Unix time you get like this: $ date +%s 1538414714 And now calculating that age of my example file can be done like this: $ echo $(( $(date +%s) - $(stat -c %Y .bashrc) )) 4603166 Doing a question like "is it older than X seconds" looks like this: $ if (( $(date +%s) - $(stat -c %Y .bashrc) &gt; 60 * 60 * 24 )); then echo "file is older than 24 hours"; fi file is older than 24 hours Here's that code with line-breaks added: if (( $(date +%s) - $(stat -c %Y .bashrc) &gt; 60 * 60 * 24 )); then echo "file is older than 24 hours" fi 