`last` accepts a user name, not a uid.
I am also trying to learn, starting from scratch, and doing it on my own. I second cooper12 suggestion of &gt;http://tldp.org/LDP/Bash-Beginners-Guide. It was extremely helpful to plough straight through it even if I didn't understand everything. It gave a great overview, a sense of possibilities and things to think about. I would also recommend http://mywiki.wooledge.org/BashGuide. It covers the same material, but filled in some of the holes remaining in my understanding, with slightly more information about hows and little hints. 
Another user said that last takes usernames, not UID's and I think he's right judging from the man page. So sorry for the wrong info. You can probably just use the usernames. Also before you cut stuff, run the command without cutting and count the fields using space as the separator between fields. I don't think you should follow my approach anymore. Last does seem useful, but using last with usernames doesn't seem to work so well.
Whoops you're right. last -h last: option requires an argument -- h usage: last [-#] [-t tty] [-h hostname] [user ...] Why they decided to use -h for anything other than help is beyond me.
Hi, There is an old O'Reilly book called "Unix in a nutshell" which is available as a free ebook download at - &lt;link removed&gt; The bash reference manual is at - https://www.gnu.org/software/bash/manual/bashref.html One of the best things that helped me as a newbie was to write down the logic step by step on paper, to figure out what needed to be accomplished. Once you figure that out, you just need to look for the tools for every step. HTH. Ninja edit - There is a free "Linux in a nutshell" too -&lt;Link removed&gt; Ninja edit 2 - The site I linked was from a search on Google. The servers are hosted in Russia and the site does not look legit. Removed the links. Sorry.
Thanks, I've updated the post with your suggestions.
Regarding using alias, instead of putting in in /bin - I call the script with 'source &lt;name&gt;', for reasons mentioned in the other reply, so putting it in /bin wouldn't suffice, unless there is a way to have it run as if called with source, by default. About the running --install twice - true, it would create double entries, although I didn't worry about that, for it doesn't cause any real harm and usually people have a slight idea of what they're doing so they will add the alias directly. I guess I could add a check, so that it only puts the alias if it's not there. I went through your script, and tried using it for the same purpose. For some reason it doesn't play the episodes in order, even though the proper episode number is kept in the .config/series/shows file. Importantly, it doesn't have the functionality of not having to enter an episode if you want to play a series you just played, but I believe that's only possible when ran with 'source'. I understand you rewrote it without rigorous testing, so I'm not going to go further than that. I see how your script conforms to standards better, such as a configuration file in .config, but as I have written in another reply, I find not going with that has its benefits. I appreciate the input, I will see how removing functions from the script will work, I think that's the main reason you found it difficult to follow(I do as well). It should be more readable. Please feel free to throw anything else you see fit. 
&gt; I think the main source of confusion stems from me trying to group things in functions, whereas bash is not a language quite suitable for that. There's nothing wrong with functions, in shell scripting. There isn't always a need for them, especially to the degree which you want to implement them. Functions are very good for grouping common commands you utilize more than once, and there isn't much within this script that it makes sense to re-factor as functions.
I source the script, so that it can have control over my shell working directory. That way, it can leave the working directory set to the series I have just watched. This gives visual confirmation of what will be played right away, if you type series again(you don't have to change the command used as well). It allows for a quick ../&lt;somethingElse&gt;, then series to play a different series as well. I considered your suggestion, but a flag such as --next would rely on the user remembering what they watched last, also they would have to change the command used, from 'series' to 'series --next'. I don't see the benefits compared to the current method, apart from forgoing a 'forbidden' practice in scripting.
Not sure if you can do it in Ubuntu, but on Mac OS X, if you type cd and then hit the tab button twice it'll do exactly what you're asking for. EDIT: For those who didn't know, this also works if you type in the first couple letters of a folder/file and then hit tab, it'll either auto-complete the folder/file name or show you all the folders/files that start with those letters if there's more than one.
Tab completion is a default function in bash on most modern *nix distributions these days. Tcsh also has this. I think esc is the default in csh. I'm not a korn guy but I'm pretty sure it also has completion support.
You don't need to source your script to do that, you're already using a bookmark file in the directory. You basically just need to do `if [ -e './saved' ]; then` and run your function / code to read `saved` and play the next episode. I don't really recommend this approach, as you can track that in a central file somewhere, but it's your script and you can do whatever you're comfortable with in it. You could have `~/.config/series/last` bookmark the last watched episode, as well. There's all sorts of ways you can track your stuff, if you want.
I'm pretty sure you can turn cd foo into an alias for cd foo; ls
The fact that 'cd' requires a parameter (unless moving to home directory) makes this tricky; I had to alias a function like so: #!/bin/bash # ~/.bashrc cd_func() { cd $1; ls .; } alias cd=cd_func Then source the rc file: $ . ~/.bashrc
 #In my BashRC #Auto ls after CD cd() { builtin cd "$@" &amp;&amp; ls -a } 
This is broken due to lack of quotes, and you don't want to run `ls` if `cd` fails. /u/InvaderOfTech has the correct function.
Not possible with an alias, but possible with a function, like /u/InvaderOfTech's
Oh yes, thank you. This is exactly what I was looking for. Double tab works perfectly on Ubuntu.
Ok. I was close.
Thanks, that's a good tip! I'll fix that asap:) I don't have any OSX machines to test on. Is there any list of OSX no-nos for BASH scripts? Edit: I'm compiling a list of fixes from people's feedback, so any corrections are welcome.
Try zsh you will get even better TAB completion. See oh-my-zsh and antigen projects for zsh
Cheers! I sort of learned my BASH from "Bash shell scripting bible" I had lying around, plus trial and error, so that is probably why you see "ancient code". (Until very recently, I used backticks and expr too.. :) I'll check out the preferred methods you suggested. I think I use echo a lot as a bad habit from just testing things in the terminal and then using the same thing in the script. Not really smart for longer scripts. Thanks for the link to the POSIX standard! The use of VARIABLE, FunctionThis, loopVARIABLE was something I picked up from a presentation (pdf) I found online on Bash "best practices".. Don't have the URL now, unfortunately. Since I don't know any other coding languages, I just followed its suggestions.
The only one I currently have is the main menu here: [Biamin main menu] (http://sigg3.net/wp-content/uploads/2014/02/biamin-release.png) But if you look at the bottom of this page you'll see the ASCII for the orc header: http://sigg3.net/biamin/about#license
Thanks! This is just a very elaborate sequel to a game I created on my TI83+ calculator in my math classes:)
Thanks, I will! I'm gathering a (growing) list of fixme's and todo's here. Great:) But time for bed.
Just looking for pointers or if anyone can think of any improvements. :-)
Isn't what you provided just SHA-256 though? You are right about SHA (Secure Hashing Algorithm) being a Hashing algorithm rather than an encryption algorithm. Kummo666, maybe you have sha512sum installed. Or try openssl sha512. Also, if you put a space in front of your command, it won't be saved in bash history list. But it will be possible to be snooped on by other processes. You can feed it into a script using the read command with the suppress flag, -s, to prevent snoopers running ps -ef. #!/bin/bash read -p -s "Password: " var_passtoencrypt echo $var_passtoencrypt | sha512sum unset var_passtoencrypt
ahk; yer right. I musta misread that. Let's try again. $ printf 'Put Your Password Here' | sha512sum | cut -f 1 -d ' ' 527f866b6ae9435a14d385cc33f1fafdf5aca84c930f14ffab776b25a981aca036ccc7bad3421c7e9e79b95b5d944f97200b3a17e8ff18f18b8a433b4c2e1f91 You do still want to cut at the first space because the format of the output of the *sum programs includes the "filename" (and mode) even for standard input. And the ps command will still reveal the password when you echo it. that's when it's on the command line.
Great but I need to hash with sha512
yeah, sorry, I misread. there's a sha512sum program as well. $ printf 'Put Your Password Here' | sha512sum | cut -f 1 -d ' ' 527f866b6ae9435a14d385cc33f1fafdf5aca84c930f14ffab776b25a981aca036ccc7bad3421c7e9e79b95b5d944f97200b3a17e8ff18f18b8a433b4c2e1f91 
I haven't worked too hard to break this, but it seems to work for everything except zero arguments. alias cd='{ builtin cd "$(cat)"; ls; } &lt;&lt;&lt;' (Of course, the function is the best way to do this.)
Heh, ok I was wrong. It's apparently possible to hack it up with an alias. cd &gt;/dev/null -
Note to self: I found a list of dashism's on Greg's wiki re: [bashism](http://mywiki.wooledge.org/Bashism) It covers some differences between bash-specifics and POSIX compliant (and/or) dash. Also this list on the differences of [Bash in Posix Mode](http://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html#Bash-POSIX-Mode)
Thanks! I'll try it out in a few minutes and let you know how it goes. 
After the done you need "&lt; /path/to/file1" It's showong on my phone...
well for one the base64 is extremely inefficient here - since you are encoding the textual representation of the hash output instead of the actual hash output itself. running the output of that site yields $ echo 'xxZgumRcEICilvC+DqmNwQ45H65JHAhhno7euifJqNEiMj44jOX9MuRmlEC3zccrdnt7KQgp+RAuWsyfcwYGig=='|base64.exe --decode | hexdump.exe -C 00000000 c7 16 60 ba 64 5c 10 80 a2 96 f0 be 0e a9 8d c1 |..`.d\..........| 00000010 0e 39 1f ae 49 1c 08 61 9e 8e de ba 27 c9 a8 d1 |.9..I..a....'...| 00000020 22 32 3e 38 8c e5 fd 32 e4 66 94 40 b7 cd c7 2b |"2&gt;8...2.f.@...+| 00000030 76 7b 7b 29 08 29 f9 10 2e 5a cc 9f 73 06 06 8a |v{{).)...Z..s...| 00000040 which is what you get when you $ printf '20002' | sha512sum | cut -f 1 -d ' ' c71660ba645c1080a296f0be0ea98dc10e391fae491c08619e8edeba27c9a8d122323e388ce5fd32e4669440b7cdc72b767b7b290829f9102e5acc9f7306068a So if you want comparable results in bash you need to turn the text-hex representation that sha512sum gives you into binary before sending it on for base64 encoding: $ printf '20002' | sha512sum.exe | cut -f 1 -d ' ' | xxd -ps -r | base64.exe | tr -d '\n' xxZgumRcEICilvC+DqmNwQ45H65JHAhhno7euifJqNEiMj44jOX9MuRmlEC3zccrdnt7KQgp+RAuWsyfcwYGig== 
You intend to convert the script to `sh` as well? that might be a bit hard since you're using some arrays in there, and `sh` doesn't have arrays.
I lost to an orc. 
Instead of `grep ... | wc -l`, why not grep -oc "\&lt;$line\&gt;" file2 Note also that the single-quotes will prevent the substitution of $line.
This is correct... I was stuff in traffic and my phone was being difficult. :)
Good point. I aim at bash first and foremost, but a step by step increase in compatibility would only teach me more about the *sh ecology My first aim, thanks for pointing that out, would be dash on OSX.
Orcs are tough as nails and do a lot of damage. Stay on the roads and towns to avoid them, or rest in the towns to regain health before venturing into the orc infested hills and forests.
Thanks! wget was the first that crossed my mind, but I'll check out curl which is something I've wanted to learn to use anyway:) I'm currently re-doing the arithmetic expansions to use (( instead of deprecated [['s.
The simpler way to do it (operates on `sample.txt` and `keywords.txt`): grep -o -f keywords.txt sample.txt | sort | uniq -c Note that this does do sub-word matches ("the" will count towards "there"), but I will leave that as an exercise to you to fix (if the assignment requests that). Notable things here are the `-f` flag to grep. Also, It's worth pointing out that unlike /u/juanschwartz's solution (which looks correct, except for the quoting issue that /u/thesoicattack pointed out), it is operating primarily with streams. The trick to writing efficient shell scripts is that you need to grasp that it is primarily an IPC language; you should let the loops happen *inside* of the other programs you call. Let grep loop over the keywords (instead of the while loop), let sort and uniq loop over the output (instead of calling wc multiple times). juanschwartz's solution will have grep read the text file once for each keyword; that is doing a lot of extra work, especially if the input is large, or the disc is slow.
Wow. Thanks. I tried this in a test situation and it works pretty damn good. I still have quite a bit of work to do in regards to the overall assignment but this was a huge piece of the puzzle that Google couldn't answer. We don't have a book or TA for the course but the professor says the Internet will be our resource. While that is correct to a degree, figuring out the correct sources is a little tough. Thank you. I've been searching for 3 weeks. Quick question for everyone though... I really like these assignments. Are you all sys admins? So do you have some other job that requires this kind of scripting? I'd like to get great at it but don't know if I'll have the chance. 
I'm probably not typical, but: I've got several sys admin certifications, but haven't ever worked as a sys admin. Currently I'm unemployed (full time student). For a while I was a code monkey at a startup, but being a startup, they didn't have any real sys admins, so I ended up helping with the trickier issues. At Comcast I was on a weird team with a foot in dev, and a foot in ops; it was mostly automating work that the actual ops/admins did. The work at Comcast was mostly Perl, but the good understanding of Bash was very helpful. For &gt;2 years, I've lived in either Bash or Emacs, I literally don't have a file browser installed on my laptop. In my free time, I'm a developer for the Parabola GNU/Linux-libre operating system, where I'm the maintainer of several thousand lines of Bash scripts.
Nice find! I've never had occasion to use `grep -f`, but I guess it's one of those things that is extraordinarily useful if you ever need it.
just ask away. man bash is probably all the resources you need.
Here are all the resources you will ever need: 1. Greg's wiki: http://mywiki.wooledge.org/EnglishFrontPage 2. http://wiki.bash-hackers.org/ 3. http://tldp.org/LDP/abs/html/ and occasionally the manpage.
I will second the suggestion of /u/DoelerichHirnfidler for Greg's wiki. Fantastic resource. Really, the best way to learn is to try to write a program to do some task, then come ask us why it isn't working. :-)
Thanks for all the help so far. So right now I have ubuntu running on a virtual machine in windows 8. Right now I am playing around in Terminal to figure out how to start writing commands. Would this be the best way to start out?
You might like INX. http://inx.maincontent.net/
Protip: 1 character mistakes will ruin you.
Though avoid number 3, the advanced bash-scripting guide. It teaches you to write bugs rather then scripts, unless you are advanced enough to filter out the bugs. The [BashGuide](http://mywiki.wooledge.org/BashGuide) followed by the [BashFAQ](http://mywiki.wooledge.org/BashFAQ) should be the best and safest approach.
I agree, it works both as a reference manual for certain things and to get a big picture of the language but it's far from being a "best practise" guide. Another highly useful resource that I forgot to mention is the sh POSIX standard itself.
Indeed, it may. Bash is not a language where you can guess the syntax, because if you make the wrong guess, bash might accept it, but it'll do something different than you intended.
The bash man page is also very informative man bash
I used a lot of number 3 you listed to teach myself about bash. Very good resource.
This is a joke about the mistake in OP's post's title. ;P But seriously, 1 character mistakes will ruin you... Remember that time you had a job? (;
Without wishing to do your *entire* homework assignment for you, I suggest you look into commands grep, wc, tr, chown, rev. Best of luck with the grading.
It isn't the entire homework, I've completed the rest of it, and this is the final question
Google is your friend. Learn to use it: google for: count number times word matched linux change owner of file linux make word lowercase linux reverse order of word linux Though euidzero gave you the various commands that will perform these tasks.
I'm feeling generous, this will get you lower case. The rest, google it. fileowner="Phill" echo ${fileowner,,}
There's a lot of text processing that bash can do itself: http://mywiki.wooledge.org/BashFAQ/073
Glad you like. You also have... echo ${fileowner,} # first char to lower case echo ${fileowner^} # first char to upper case echo ${fileowner^^} # all chars to upper case echo ${fileowner~} # first char case swapped echo ${fileowner~~} # all chars case swapped Bash substitution or parameter expansion is freaking awesome...
No, you use `shift` to remove the option(s) after you've parsed them, so that you end up with `$1` as year, `$2` as month etc. See http://mywiki.wooledge.org/BashFAQ/035 for examples on option parsing.
&gt; No, you use shift to remove the option(s) after you've parsed them, so that you end up with $1 as year, $2 as month etc. See http://mywiki.wooledge.org/BashFAQ/035 for examples on option parsing. THANKS!!!! 
Yea I just threw this together in like 10 minutes based off of [this](http://www.microcreative.pl/blog/?p=369),I know very little bash so I didn't know that the input were put into variables $1 $2 etc so knowing that I can definately make something simpler, thanks for the help.
Yea I just threw this together in like 10 minutes based off of [this](http://www.microcreative.pl/blog/?p=369),I know very little bash so I didn't know that the input were put into variables $1 $2 etc so knowing that I can definately make something simpler, thanks for the help.
I recommend that you learn to use bash's getopts. It's pretty straightforward and it covers tons of details for you.
i am using getopts, hence my question about using getopts. 
Indeed, but it requires bash4 so should be avoided if portability is a concern. Avoiding a fork is always nice though.
Note that `~` and `~~` are undocumented, so don't use it for anything serious.
Hey! Thanks so much for taking the time to reply! &gt;I would prefer using -v for rm and mv instead of adding a custom message -- this would make the code easier to maintain. I hear you, but the custom message came of wanting to know the md5 hash when deleting a duplicate file. The custom message for mv was created for consistency's sake. The script deletes the file **not** having the hash filename, so rm -v wouldn't help there. I suppose I could get around that by deleting the file already having the hash name then renaming the other file accordingly, but that seems a little awkward. Open to suggestions, though. &gt; I think your indentation is a little wonky overall, but I'm not sure what the "proper" bash indentation is Nor I. Probably came from trying to emulate how I've been tought to indent C (only lacking curly braces). Is there something you find makes it less clear, or would you say it's more a matter of preference? Genuinely sorry for neglecting your reply so long. Between the holidays and new year, this project was set aside and forgotten. Thanks again! 
Hey! Thanks so much for taking the time to reply! &gt; You may be interested in utilities that find duplicate files, such as fdupes and fslint. Sound useful — I'll have to check them out! Genuinely sorry for neglecting your reply so long. Between the holidays and new year, this project was set aside and forgotten. Thanks again! 
 [[ -n "${variable}" ]] &amp;&amp; rm ${variable}/*
First and most importantly, [QUOTE IT](http://mywiki.wooledge.org/Arguments)!! `rm $var` is very different from `rm "$var"`! The reason `${var?}` "doesn't work", is because it only triggers if `var` is unset. Include `:` to also check for empty: $ (unset var; : ${var?is unset}; echo "still here") bash: var: is unset $ (var=; : ${var?is unset}; echo "still here") still here $ (unset var; : ${var:?is empty or unset}; echo "still here") bash: var: is empty or unset $ (var=; : ${var:?is empty or unset}; echo "still here") bash: var: is empty or unset See http://mywiki.wooledge.org/BashFAQ/073 for more on that. Now to why you want to "protect it". Why may it be unset or empty? there's mainly two reasons. 1. It's empty or unset because you made a typo earlier in the script, or 2. it's empty because it was provided by the user. If 1, get better at testing your script before going live. One way to make sure the rm command gets the right arguments is to add things like `rm() { echo rm "$@"; }` at the start of the script If 2, does it matter if the user typed in an empty string? You could add a test for empty, sure, but what if the user writes `/` instead? then you'll run `rm -f //*`, which does the same as `rm -f /*`. You can't guard against stupid users.
 #!/bin/bash TempDirectory="$1" # .................... unset TempDirectory # Test if [ -n "$TempDirectory" ] &amp;&amp; [ -d "$TempDirectory" ] then rm "${TempDirectory}"/* fi
Any GNU/Linux would do, but perhaps something more manual than Ubuntu. I started using the shell for programs that had no GUI, like "upping" my USB wireless dongle and stuff like that. See Slackware or the smaller and simpler Zenwalk or SalixOS derivatives. But installing any GNU/Linux and running it without X (you can hit CTRL+alt+2 or 3 to get to a console and 7 typically to get back to the graphical desktop) and set out a goal, eg. Connect to your favorite IRC channel by installing an IRC client in the CLI of the package manager or compile it from source. Bash is simply stringing those commands you learn together, with logic and pipes. UNIX systems can be thought of as robots and the interface is Bash. The point of a robot is that it does things for you. So, the first thing you learn is typically system maintenance (robot house cleaning). You create a script for backing up a directory and ask cron to do that script every n hour. When it fails, find out why and add error handling or pre-flight checks aso. My first non-maintenance script was my tedious procedure for ripping CDs to wav, encoding to FLAC and adding tags. Now I put the CD in, run the script and hit Enter to confirm tags it finds online, saves me all the hassle. I can then further automate it so that the system always does this automatically for audio CDs and so on. Procedures done more than once == a job for a robot.
There has been a lot of changes thanks to your brilliant feedback and suggestions! For a list of changes see http://sigg3.net/biamin/160 or [browse the code directly](https://gitorious.org/back-in-a-minute/code/source/7e0744015ae97f2a84c5669b2eeb26eb47ab6563:biamin.sh) on gitorious! :D
Btw, Rest in towns, the castle or home to regain health.
&gt; It uses bash as /bin/sh. Will I have to have two shebangs? #!/bin/bash || #!/bin/sh or something?
There are two typical ways to do this. Case or stepping. Writing on a phone here so letme ssuggest that for your use case I'd use case.
No. If you want to write a bash script, you use `#!/bin/bash` or `#!/usr/bin/env bash`. If you want to write an sh script, you use `#!/bin/sh`. POSIX says things like there shall be a shell at /bin/sh, it shall have this and that feature. It doesn't matter what shell your OS has set as /bin/sh as long as it *at least* has those features. The shell is free to implement other features, as long as the specified features are implemented. To take an example, `[[ -n $var ]]` will happen to work if the shell is bash or ksh, but it will fail if the shell is dash. `[[` is not specified by POSIX, so dash doesn't need to implement it. the `test`/`[` command *is* specified by POSIX, so you can rely on `[ -n "$var" ]` being implemented and working, regardless of which shell is used as `/bin/sh`. In my opinion, your script will be very cumbersome if you switch to sh, so I recommend keeping it a bash script, and instead work towards only using external commands specified by POSIX, so that your script will work on any UNIX or UNIX-like system as long as it has bash installed.
OK, thanks for clearing that up! ;) I've got rid of most square brackets now, and next up is switching wget with curl. New version 1.2.3 is up at [Gitorious](https://gitorious.org/back-in-a-minute/code/source/7e0744015ae97f2a84c5669b2eeb26eb47ab6563:biamin.sh)!
This is what I came up with, does what I need it to do #!/bin/bash #start/kill trayer and determine width checkifon=$(ps -A | grep trayer) cmd=${1:0:5} width=${1:5:1} if [[ $cmd = "stop" ]] then pkill trayer exit elif [[ $cmd = "start" &amp;&amp; $checkifon = "" ]] then if [[ $width = "" ]] then trayer --align right --edge bottom --width 2 &amp; exit else trayer --align right --edge bottom --width $(expr $width \* 2) &amp; exit fi fi
Right, it might be removed without warning in a later version of bash. For any documented features, bash tries really hard to not break those features. A good example is the `$[ ... ]` syntax for doing math, which was introduced very briefly before being deprecated, and later (around two decades ago now) removed from the documentation. It was documented for a period of time, therefore it still works. The `~` and `~~` parameter expansions has never been documented. Maybe Chet just forgot to include them in the documentation, or they were never meant to be in the checked in code in the first place. Who knows.
No idea what exactly constitutes a valid theme, so in this quick and dirty script i went with existence of dirs in /usr/share/themes/* I collect dir names having gtk-2.0, gtk-3.0, metacity-1 or unity under them. Written in ubuntu 13.10 but not tested as i don't feel like fixing should something go wrong :-) The command potentially setting up things is echoed out so the script only prints out info. What would be the actual command depends on your distro and setup, there are so many management systems atm :/ http://www.fandigital.com/2012/06/change-theme-command-line-gnome.html #!/bin/bash shopt -s nullglob # not sure about the location of user themes in $HOME theme_dirs=( /usr/share/themes $HOME/.local/themes ) declare -A gtk2=() declare -A gtk3=() declare -A metacity=() declare -A unity=() for td in "${theme_dirs[@]}" do for th in "$td"/* do [[ -d "$th" ]] || continue thname=${th##*/} [[ -d "$th/gtk-2.0" ]] &amp;&amp; gtk2["$thname"]="" [[ -d "$th/gtk-3.0" ]] &amp;&amp; gtk3["$thname"]="" [[ -d "$th/metacity-1" ]] &amp;&amp; metacity["$thname"]="" [[ -d "$th/unity" ]] &amp;&amp; unity["$thname"]="" done done printf 'gtk2:\n' printf '+ %s\n' "${!gtk2[@]}" printf 'gtk3:\n' printf '+ %s\n' "${!gtk3[@]}" printf 'metacity:\n' printf '+ %s\n' "${!metacity[@]}" printf 'unity:\n' printf '+ %s\n' "${!unity[@]}" printf '=======================\n' # assuming gtk3... select t in "${!gtk3[@]}" do break done # ... and info from here http://www.fandigital.com/2012/06/change-theme-command-line-gnome.html # it would be something like this, without echo echo gsettings set org.gnome.desktop.interface gtk-theme "$t" 
http://mywiki.wooledge.org/BashFAQ/031 to start you off. I'm on mobile so can't write a detailed response at the moment.
oh man, did you really have to give a broken hack as an example of embedded command? *for i in $( ls )* is literally the very first point of [BashPitfalls](http://mywiki.wooledge.org/BashPitfalls).
Just trying to keep it as simple as possible, and no I wrote this in like 2 minutes with out thinking of any best practices. Plus I am simply listing the items in a directory, not wildcarding any file names. muh bad
yeah, but ls will mangle funky names it can't print either way and it's inherently fragile because of the IFS/word splitting. i'd suggest something like [ $( date +%A ) = 'Sunday' ]
Just pointing out, you gotta have the space before ] or ]] in a test statement. `if [ -f file]` will give an error (sometimes a cryptic one if you're using a different shell), while `if [ -f file ]` is correct.
I'm going to really belabor the point because this is a new bash user's thread. Something as simple as a space in a file name can mess this up. It's important to make sure that your parameters are what you mean them to be. Example: $ ls -1 testdir file1 file2 file number four file three $ for i in $(ls -1 testdir); do echo "$i"; done file1 file2 file number four file three $ for i in testdir/*; do echo "$i"; done testdir/file1 testdir/file2 testdir/file number four testdir/file three $ cd testdir $ for i in *; do echo "$i"; done file1 file2 file number four file three $ If you really wanted to use ls, you could do `ls -1 | while read i`, but that's silly 
... and even then \n is a legit filename char (everything but / and \0) so back to square 1
"X is not a valid construct imho" is a little awkward. It's your humble opinion that the construct is syntactically invalid?
thank you for explaining it so nicely! :) Can you tell me a bit more about using &amp;&amp; and || with normal commands? How does work? Where can i learn more about this?
Yea, I learned that soon enough. I had an assignment to submit and I had errors popping up all over the place. I eventually figured it out but I had no idea what I did right. 
Thank you! What's a subshell?
What, exactly, is a hack? What is the 'un-broken' version of this hack?
i don't recognize it but i don't exclude the possibility that it's some kind of obscure syntactic sugar.
Yup, I guess I don't really use many methods to build lists of strings or arrays with ls. I usually use awk to build my data and then loop through it. Totally 'spaced,' the fact that ls will do that with spaces. 
A subshell is a command you can run with in a command, essentially. I do a lot of client side management in OS X and use bash to get things done. So for example, if I want to loop through all users that have UID of 500 or higher (which is what actual human user accounts are), I would do this. for u in $(dscl . list /Users UniqueID | awk '$2 &gt; 500 { print $1}') ; do echo "${u} is in the above UID 500 user list" done Essentially that sub shell is building my list of users on a Mac that have UID greater than 500, and then echoing them out. Another way to do it would be to have a variable that built that list of users, and if I were to use this code over and over again a variable would be a smarter choice perhaps. I do most of my bash on OS X clients these days, and only a small portion on Linux/UNIX servers. Typically on the server side of things I just leverage a framework or an existing binary that will kick off and start|stop whatever services I need running. These past couple of years I really am getting more into project management and infrastructure architecture rather than writing code, so I could be forgetting to explain something right. I also can't say I am the best at explaining things in general. That link I posted a few comments up though helped me a lot and is a great reference for when you need a refresher. 
the most common pattern is chaining commands using &amp;&amp; the chain breaks if any command in sequence fails so it's a very handy device that saves time cmd1 params &amp;&amp; cmd2 params &amp;&amp; cmd3 params &amp;&amp; ... if they were separated by ; they would always run because the status of the preceding commands doesn't influence the following ones. have you ever seen something like sudo apt-get update &amp;&amp; sudo apt-get upgrade &amp;&amp; sudo apt-get install some_package that's exactly it, "run the whole updating system shebang to install something on top, but don't waste time if something fails on the way, just stop". || on the other hand can be used to create a quick and dirty 'on failure do X' without writing a fullblown *if then fi* $ rm file_that_doesnt_exist || echo "there is no such file, you idiot" rm: cannot remove 'file_that_doesnt_exist': No such file or directory there is no such file, you idiot $ [ $( date +%A ) = 'Monday' ] || echo "Today is not monday" Today is not monday or skipping some elements in loops, based on failing a condition for i in *; do [ -d "$i" ] || continue; echo "$i"; done code above will ignore directories because *continue* shortcircuits the loop and proceeds with the next elem. Using *break* instead of *continue* in the example would kill the loop on the spot and follow with the rest of the program. *cmd1 &amp;&amp; cmd2 || cmd3* is roughly similar to *if cmd1 is success then cmd2 else cmd3* but there are caveats, which should be described somewhere in BashGuide or in BashPitfalls (look for them in sidebar) Theoretical background below: commands return a number (exit code). Success is 0, everything else is error (different numbers can mean different modes of failure so it's convenient to accurately diagnose problems) In the context of command chaining with &amp;&amp; and ||, successes are TRUE and failures are FALSE and running/skipping commands is decided by shortcircuiting of expressions allowed by boolean logic. * *cmd1 &amp;&amp; cmd2* - this will execute cmd2 only if cmd1 is true (success). Why? Because in case of failure FALSE AND ANYTHING = FALSE and the second part can be skipped because it doesn't influence the end value of the whole expression. Success means the end value is undecided and depends on cmd2 **TRUE AND FALSE = FALSE** **TRUE AND TRUE = TRUE** FALSE AND TRUE = FALSE FALSE AND FALSE = FALSE * *cmd1 || cmd2* - this will execute cmd2 only if cmd1 fails, because in case of cmd1 success TRUE OR ANYTHING = TRUE - cmd2 doesn't matter and is skipped. Failure of cmd1 means cmd2 decides the outcome and needs to be executed. TRUE OR TRUE = TRUE TRUE OR FALSE = TRUE **FALSE OR TRUE = TRUE** **FALSE OR FALSE = FALSE** in short: if you know the boolean value of **x &amp;&amp; y** or **x || y** from x alone, y gets ignored.
that's because it's just a command. building blocks of the conditional expression inside the [ ] are separate params and params require spaces between them. In case of [ you can do all the normal stuff you could do with parameters like quoting them in all kinds of funky ways and what not. $ '[' '1' '-'"e"'q' '1' ']' &amp;&amp; echo true true $ set '1' '-eq' '1' ']' $ [ "$@" &amp;&amp; echo true true explanation to the 2nd part: i used set to fill positional parameters $1 $2 $3 $4 with 1, -eq, 1, ] and then ran [ + "gimme all params" ($@) and it still works correctly (shell plugged params in, saw [ 1 -eq 1 ] and said 'success!'). 
a hack is something fugly that sorta kinda works but not always and this example definitely qualifies. kosher way of doing "gimme all things from current dir" is `for i in *; do ...; done`. Native shell globs used to select files/dirs are rock solid. *for i in $(ls); do ...; done* and its variants take output of ls and try to use whitespace to find 'words' which should be names of files/dirs. The problem is that it can't differenciate between "whitespace as a gap between names" and "whitespace as a part of the name" and we know that space is a totally legit char (even newline is in linux) Additionally *ls* can fail to faithfully reproduce the names because there are many characters it can't print that are otherwise legit in filenames (everything but / and NUL is legit). Scripts constructed upon such a flawed base can never be rock solid and guarantee the correctness because in the process original names are at risk of being mangled. If you try to find a file with a funky char that *ls* replaced with a &lt;?&gt; glyph or ignored outright, you are not going to find it because the name you have is not the one the file has. Long story short *ls* is a tool that you use to see contents of a dir at a glance, not something that can be reliably parsed and depended upon. It's only for human eyes. Ls based hack is not unlike printing text and then running OCR software on it. Why bother if you can just use the original? Globs are the original, output of ls is the derivative. read BashPitfalls for details on this issue and on many other minefields people often step on (linked in one of posts above and in the sidebar of this subreddit)
In this case your humble opinion of `[ ( ) ]` coincides with the bash parser's (humble) opinion :) A word beginning with an unescaped `(` is not allowed in a simple command.
[Don't read lines with for](http://mywiki.wooledge.org/DontReadLinesWithFor).
Not being able to use Ctrl-A,E, and arrows is annoying to unlearn.
If you still want to use Ctrl-l to clear your terminal while you're in the insert mode, add this to your bashrc: bind -m vi-insert "\C-l":clear-screen
Copy the script you want into script.sh chmod u+x script.sh ./script.sh
A bash script is a Unix shell program file. The instructions listed have to be saved to a file (by convention with a name ending in .sh, and then given permission to execute) and can then be run from the shell by simply typing their name, or by other means involving more graphical hand-holding. These are short easy scripts, but if you don't know what a shell is or how to work with the basic file-system, you should probably just ask a nice friend to do it in exchange for an incredibly expensive dinner or possibly a week-end in Acapulco.
As you have correctly guessed, I have no idea what a shell is or how to work with the basic file-system. Incredibly expensive dinners are incredibly expensive....and I wish I could go on a week-end in Acapulco myself! The only thing I can offer is maybe a poster or a banner made in Photoshop. I know my way around Photoshop but that's about it.... 
Windows 7 Enterprise! As I said above, I have donwloaded Cygwin but that's as far as I got...
I'm sort of joking, of course - but seriously - get someone to do this in person for you. Most people don't mind doing a little grunt-work when they're teaching something, but there's a lot of small details from knowing your way around a directory structure to piping and chmodding that are best shown in person. Some very small points can cause big confusion.
You could look into Python for doing this, it runs on pretty much every platform.
Hit Win+R, then type bash and hit enter. That should open a bash window (black box of text). There, you can just copy+paste in the commands (right click on the title bar -&gt; edit -&gt; paste; Ctrl+V won’t work!).
There's no way to know whether you're in command or insert mode without typing something. If I could signal the mode in my PS1 in bash, I'd switch right away. 
I'm not a zsh user myself, but you got me googling for an answer with bash. It seems you can do it with zsh and potentially in bash v4.3. http://stackoverflow.com/questions/3622943/zsh-vi-mode-status-line http://stackoverflow.com/questions/1039713/different-bash-prompt-for-different-vi-editing-mode
Have you thought about setting it up as a cron job for root? Might be the easiest way to get around fiddling with sudoers. Couple things I'd change: 1. The line count could be reduced a bit by using `echo -e` and newline characters. So instead of two `echo " "` lines you could have one `echo -e " \n "` line. 2. Consider using `exec`'s stdout redirection capabilities to clean up the redirecting on every line. Here's a rundown on how that works if you're unfamiliar with it: http://www.tldp.org/LDP/abs/html/x17974.html 3. `cat file | app` is a bad habit and causes an unnecessary fork. Use `app &lt; file` instead 4. You don't need to terminate lines with ;
show-mode-in-prompt is a 4.3 rc feature.
Add bindings to your inputrc
I'm actually looking to do this as a cron job under root. I've configured crontab -e appropriately pointing at my script and tested it. The script runs, but commands requiring root access don't run. I'm using ; on every line to insure that the processes run in order and the next one doesn't start until the previous one finishes. I had started this whole project just from the command line and sending commands via Apple Remote Desktop, which led me to say "this is silly!!!! and move on to scripting this process. Thank you for the tip on stdout. I just had a look at your link and my eyes kind of glazed over a little bit. That may be a little bit further in my future. I'm using cat file | app for email, as it's how I read it in these manuals I have (O'Reilly Learning Bash shell and Bash Cookbook) Would the better way be : mail -s "subject" admin@foo.com &lt; /var/log/maint/test.log 
&gt;I've configured crontab -e appropriately pointing at my script and tested it. The script runs, but commands requiring root access don't run. Huh, weird. You're sure it's added to root's crontab (`sudo crontab -e`)? &gt;I'm using ; on every line to insure that the processes run in order and the next one doesn't start until the previous one finishes. You get this for free in a script, semicolon or not. Unless you're messing around with subshells or backgrounding (and you aren't here), the next line won't run until the current one finishes &gt;I'm using cat file | app for email, as it's how I read it in these manuals I have (O'Reilly Learning Bash shell and Bash Cookbook) Would the better way be : &gt; `mail -s "subject" admin@foo.com &lt; /var/log/maint/test.log` Yep, that's what I was getting at. It's not a big deal here 'cause this is a simple script that doesn't run often, but it's a good habit to get into. Piping cat is rarely the best way to do something.
Excellent. After reading your comments I experimented with your advice. Hit the nail on the head with the mail. That's good to know about the script running everything line by line. I guess that means I can conserve my use of ";" haha! With regards to crontab, is (sudo crontab -e) different than being root (root#crontab -e)?
You might want to consider utilizing a heredoc for certain parts of this, where you're doing too much `echo` just to put newlines in. /u/talkloud has a good suggestion for using `echo -e "\n\nsomething\n\n"` But sometimes I find it clean to use a here document when you want to output a big chunk of text with newlines at once, eg - cat &lt;&lt;EOF &gt;&gt; /var/log/maint/test.log --------------------- foo Inc. Http://www.foo.com support@foo.com $(hostname) $(date) --------------------- some other stuff here EOF The text contained between the EOF will be output exactly as it's seen. This is also handy for printing out a clean usage / help page. Note the "closing" EOF has to be on a line by itself. Also it can be any string, most people use "EOF"
 #!/bin/bash # Simple Maintenance Script # By cyberworm_ # For For the company I work for # 2014 # Version 0.1 hostname=$(hostname) date=$(date +%"F %T") company='foo Inc.' website='http://www.foo.com' email='admin@foo.com support@foo.com' slogan='Why are you printing a slogan in to a log?' subject="Maintenance $hostname $date" # Start a sub-shell ( echo "$company $url $email" echo # Why so many empty lines? echo "$slogan" echo #Let's put our hostname in the file for parsing by our ticketing system echo -e "----------------------\n$hostname\n----------------------" echo -e "\n" echo -e "$date\n\n" # How long has this server been running? uptime echo -e "\n" # Does our server require any updates? softwareupdate -l echo -e "\n" # List all drives and partitions on the local machine diskutil list echo -e "\n" # Verify and repair the permissions on all local drives #diskutil repairPermissions / #echo -e "\n" # How much free space do we have? #df -h #echo -e "\n" # What's taking up all of that space? #du -sh /* | sort -h | column -t #echo -e "\n" # Check RAID status raidutil list status echo -e "\n" echo "Maintenance Complete" # Finish the sub-shell, and pipe the output to 'mail' ) | mail -s "$subject" "$email" # Done! echo -e "\n\nProcess Complete" I would call this script `maintenance.sh` or something and plop it in `/usr/local/bin` and then create a cronjob in `/etc/cron.d`.
&gt;I'm actually looking to do this as a cron job under root. I've configured crontab -e appropriately pointing at my script and tested it. The script runs, but commands requiring root access don't run. Are the root commands not running due to permission denied or file not found? Any error output at all?
I know, it's just an annoying habit to unlearn.
Rather than using absolute paths to all executables, just set PATH at the top of the script (or in the crontab). Will save you hours if you ever want to port the script to another system.
That's because you're only capturing the commands' standard output (fd 1), while error messages are typically written to standard error (fd 2). If you go with the heredoc approach, you'd do mail -s "subject" admin@foo.com &lt;&lt; EOF ... $(softwareupdate -l 2&gt;&amp;1) ... EOF to also capture the command's standard error.
In reference to your comment, "why would we put a slogan in a log file"... This is more like a report we generate for our clients. We're an MSP and as part of our monthly service contract we run maintenenace for multiple clients once a week and send them the reports. When I started a year ago with this company, it was done by hand, logging in to each machine and running these checks. As you can imagine, I loathed that process (the guy before me wasn't too smart I htink...) and I've finally gotten to a point where I'm like "f this. There's a better way." Putting our company name and slogan into it, is just a way for them to be reminded of us, though there's never a shortage of calls/tickets. :)
/u/knowsbash Adding this: 2&gt;&amp;1 solved my issue that I was just posting about. Thank you for that. I'm now running a cron process against my script, with that line included as well, to see if it throws errors about why it's not running those portions of the script. I'll know in about 3 minutes what is going on with it. My suspicion has always been that it's just not running with the right permissions, even though I'm logged in as root and setting crontab that way. 
Looks like y'all were right. It does appear to have something to do wit hthe path. After putting in the bit to collect errors, this was returned inside the file after running it via cron: /usr/bin/maintenance.sh: line 36: softwareupdate: command not found Now to fix that! Thanks!!!
&gt; Cygwin forget Cygwin ... it sucks. use a virtual machine with linux! 1 download virtualbox ... a vm app 2 download an easy linux distro: * mint (http://reflection.oss.ou.edu/linuxmint/isos/linuxmint.com//stable/16/linuxmint-16-cinnamon-dvd-32bit.iso) * or ubuntu (http://www.ubuntu.com/download/desktop) 3 open a shell and run the scripts
First, thanks everyone for their input and help with this. I've got it configured and working (from CLI and cron). Now I'd like to start going for a bit more complexity in this and hopefully I'm not biting off more than I can chew. Is it possible to have this script run, and if needed, reboot the system then continue to pickup where it left off? The softwareupdate command has the switches -i -a which will install all updates. Some of those updates will or may require a reboot. In that case, I'd like for the script to first list the updates and output, then install the updates, reboot, and continue with the script, THEN email all of the results. I think part of my brain had this in mind when I started out using command &gt;&gt; /my/path/test.log though not consciously being aware of it. Obviously, if there are no updates to install, then the script should continue as normal and mail the results. Should this be done separately in a different script? If so, my thought would be for a second script to read the specific output from softwareupdate -l and if there are updates available, perform those updates, and rerun the maintenance.sh script after reboot. Ideally, I want one cohesive output to say "these updates were available and have been installed" along with all of the information contained in my current script. Rebooting seems to be the big hurdle towards this goal at the moment (along with my current lack of knowledge). 
Yes... But I would strongly advise against scripting a reboot. You would need status variables with if/then tests to make logical decisions within the script. I would also move your core commands into functions where each command is completed and logged as a function, then use the if/thens against the functions.. Something like: myfunction () { myfunction_code="" echo "myfunction: to_be_logged" &gt;&gt; ${logfile} mycommand &amp;&amp; myfunction_code="1" } # MAIN myfunction if [[ -n "${myfunction_code}" ]] then (do something after success of myfunction) else [[ -z "${myfunction_code}" ]] (do something else after failure of myfunction) fi
I'm guessing you want: #!/usr/bin/env bash if pgrep cgminer &gt;/dev/null; then printf 'Running %s\n' "$(date "+%T_%d-%m-%Y")" &gt;&gt; cgminer.log else printf 'NOT running %s\n' "$(date "+%T_%d-%m-%Y")" &gt;&gt; cgminer.log coldreboot fi I recommend you read the [BashGuide](http://mywiki.wooledge.org/BashGuide) to learn the basics of bash scripting. Also, you might regret picking that date format later on. Better to use a format where the most significant numbers are on the LHS, e.g. `%Y-%m-%d %H:%M:%S` EDIT: Moved the redirections given the revelation of what `coldreboot` does.
Thanks mate if i ever get my hands on some reddit gold ill tip you, i can also send some crypto if you would like that?
Does `coldreboot` reboot the entire system, or is that a poorly named script that 'reboots' cgminer?
`pgrep` only outputs anything on stderr if there's a syntax error or something went seriously wrong. For those cases, you generally want to see the error message. `date +%F` isn't defined by POSIX, but otherwise, just a personal preference.
Cool, thank you.
I probably wouldn't know what to do with either :)
I've tried it a bit. Gameplay-wise it's fun :) but it's a bit annoying having to go to the (m)ap before moving with [ensw]. Would be nice to be able to move even when not looking at the map, and using [wasd] or [hjkl] or arrow keys (the latter is very cumbersome in bash though, so stick with letters). And it would be better if hitting an invalid letter caused nothing to happen instead of exiting the game.
Use `echo` instead of `echo -e "\n"` .
My mistake.
Hey KnowsBash, Im sure the part where it says not running is not working can you check it for me?
What happens instead?
It seems the reboot happens but it does not write that it was not running to the log file.
The `coldreboot` command must be doing something odd then. Is it another script?
I was just guessing. /u/jtheg2 is that what coldreboot does? And yeah I would be curious why a reboot is needed. It seems to me that if the miner process isn't running (and presumably it should be), then the service should just be restarted. 
[Version 1.3 is now out!](https://gitorious.org/back-in-a-minute/code) and [changelog is here](http://sigg3.net/biamin/178). It uses the WASD buttons AND let you move in the sector-view as well, without having to go to MAP first. At the moment ANY undeclared input is taken as a move, e.g. walking in a circle, instead of quitting the game. I'll see how I can implement a "do nothing" functionality without breaking the flow.. Edit: Link
For some reason I need to reboot to continue mining if I restart the services the gpus refuse to mine
&gt; Hope this was somewhat clear. Far from clear. &gt; scriptname -argument 1 -argument 2 filename -o filename Does your command really take the same filename as argument and as argument for the -o? I doubt that. If you're going to make up an example, make it representative. Show how you intend the new script to be invoked, and what commands you want the script to run based on the arguments. [BashGuide](http://mywiki.wooledge.org/BashGuide) is the best to start off with. [The Command Line Crash Course](http://cli.learncodethehardway.org/book/) doesn't teach you anything about bash, just shows you some common commands.
Yes, in fact it does take the same file name as the argument for the output. I apologize that it wasn't clear. I have indeed been using BashGuide, but I am absolutely new to this and as such still unsure how to proceed. Essentially, I can manually enter in the example line I wrote above into the command line and it will get the job done. However, I need to run the command twice with different arguments, and so I'd like to incorporate it all into one script. For instance, I want to run the script on some data for values between 50 and 55, and then for values between 57 and 58 (where these values are one of the arguments). So I need to call a C script with given input parameters twice. I'm sorry. I guess I'm having a hard time explaining myself. 
So it's a little bit confusing the way you presented this, but I think I understand. (You should review your example that you provided though, as the arguments don't seem to make sense, and I based the below from your example) First, you're running a C program, not a script. But you would like to automate the execution of that program by making a shell script, correct? You're manually doing something like this? c_prog --arg1 50 --arg2 2 /tmp/foo.txt -o /tmp/foo.txt c_prog --arg1 51 --arg2 2 /tmp/foo.txt -o /tmp/foo.txt c_prog --arg1 52 --arg2 2 /tmp/foo.txt -o /tmp/foo.txt ... So, you probably want something like this to automate it: #/bin/bash # create a for loop, using bash 'brace expansion' so we get 50, 51, 52 ... 55 # The variable $num will start at 50. When it equals 55, the loop ends. filename="/tmp/foo.txt" for num in {50..55}; do c_prog --arg1 "$num" --arg2 2 "$filename" -o "$filename" done; # note - you weren't clear which of the arguments you were interested in # but the same principle applies, just put the "$num" variable where you # need it. Here's an example with `echo` that you can play with first to understand the concept: #!/bin/bash for foo in {50..55}; do echo "Our variable foo is $foo" done; 
Thank you very much. I really appreciate the reply. Sorry the post was so confusing! Your reply is extremely helpful, though. Quick question for you: One of the arguments for the C program specifies when to end the counting, ie. from 50 to 55, for example. Since that's the case, I wouldn't need a for loop, right? And then all I'd have to do is enter in the line (in the script) almost identical to how I'd enter it in the command line? Thanks again!
It would help at this point if you just posted the exact command(s) you are running manually. I don't want to guess anymore at exactly what you are trying to accomplish and create further confusion
Note that rename is not a standard command, so there are several different rename commands that are completely different from each other. In this case you obviously have the perl rename command installed. You can have multiple substitution commands separated by semicolon, so this might help you on your way: rename -n 's/[^[:alnum:].-]+/_/g; s/_$//' "$@" the `-n` option gives you the same output as the `-v` option, but without actually renaming files.
You saved me from going down many rabbit holes. Thank you.
Did you make this, OP? Why does it say "Run the following command in your Terminal (Mac OS X only)" From a quick glance at the script, I think it should work for any *nix with bash and git in $PATH 
These nested ifs are basically impossible to read. I'm trying to figure out if there's a reason more of them aren't elif. Is there some sneaky logic going on? Also, vim does rock, but you may as well use $EDITOR, since that's what it's there for.
And they should probably look into the `case` statement...
Not that I would ever use something like this, but I can't stand to see bad shell scripts. I'm not claiming my rewrite is perfect (I also did zero testing), but it seems more legible to me. function r() { editor='/usr/bin/vim' case $1 in con) input="app/controllers$2";; mod) input="app/models$2";; view|views) input="app/views$2";; help) input="app/helpers$2";; css) input="app/assets/stylesheets$2";; js) input="app/assets/javascripts$2";; img) input="app/assets/images$2";; public) input="public$2";; gems) input="Gemfile";; readme) input="README*";; env) input="config/environments$2";; initial) input="config/initializers$2";; db) input="db$2";; route|routes) input="$config/routes.rb";; test) input="test$2";; rake) input="Rakefile";; pre) bundle exec rake assets:precompile return ;; -pre) bundle exec rake assets:clean bundle exec rake assets:precompile return ;; mig) bundle exec rake db:migrate return ;; *) input=$1;; esac # Stop calling this so many bloody times. cdup=$(git rev-parse --show-cdup) if [ -z $input ] || [ "$input" = '' ]; then return elif [ "$input" = '/' ] &amp;&amp; [ "$cdup" != '' ]; then cd "$cdup" elif [ -e "./$input" ]; then if [ -d "./$input" ]; then cd "./$input" else file="./$input" fi elif [ -e "$cdup/$input" ]; then if [ -d "$cdup/$input" ]; then cd "$cdup/$input" else file="$cdup/$input" fi fi if [ ! -z "$file" ]; then $editor $file fi } Also, instead of cluttering up your `~/.bashrc` with so many silly functions (I'm seeing this trending) consider writing a shell script and dropping it in `~/bin` and adding `$HOME/bin` to your path. Except in this case, I guess, because the 'main' function is to `cd` which won't work, without some fiddling around, in a script/sub-process. edit: I don't have `$EDITOR` set in my shell, so this fails, added a variable `$editor` to set your editor of choice.
Good point. I don't often use scripts / functions to change directory (outside of the script, obviously), so maybe this is alright as a function.
Yeah, the blank fields are why I elected to go the super-ugly route :(
using the input: 1116559 P1303251287 20130325225906CD 13013822 1 0000 1104220 P1303250282 20130325070119CD 1 0000 1064743 P1303251094ED 20130325191600CD 0 0000 1100819 P1303250369 20130325091722CD 0 0000 1101405 P1303250051CD 20130325010740CD 2 0000 -- #!/bin/bash input=$1 while read line; do f1=${line:0:10} f2=${line:10:14} f3=${line:24:19} f4=${line:43:11} f5=${line:54:4} f6=${line:58:4} echo "$f1|$f2|$f3|$f4|$f5|$f6" | sed -e 's/\ //g' done &lt; $input
Yeah, so random LoL!
Literally put all of that minus the ssh and the tail -f into a file that starts with "#!/bin/bash", add it to your webserver1's crontab for 1am. This won't do any error checking to make sure it actually stopped cleanly, copied the file without running out of disk space, etc, but it's a start.
As /u/sysera has pointed out, `cron` is the 'best' solution for this. However, it doesn't seem that you are correcting the root of the problem in any way. Why does the war file need to be replaced every day? Is there something else running that's removing it?
I know this is r/bash but you may want to consider using something like Ansible for this task if you anticipate additional requirements. For instance deploying on demand, or to multiple servers, or multiple JBoss instances, or if you want an email notification, or executing it centrally, or having advanced logic ( wait for port, do x , do y, etc).
What would I add to the #!/bin/bash script or how can I configure that upon JBoss daemon startup, the server then uses it's postfix MTA to send a personalize email to support@email.tld?
echo "I started JBoss." | mailx -s "I started JBoss." support@email.tld
Is there a reason that whatever deploys the new war cannot, itself, run this script? Usually continuous deployment happens a lot more than weekly, and works precisely because one commit = one deploy-and-restart.
It's to avoid using universal variables. From what i read, it can be lowercase uppercase or mixed. I know $PATH is system variable so I wanted to avoid whatever it's functionality is.
Actually i figured some of it out, I needed to quote "${user[@]}" and put a semicolon before do. Now it appears that the chown command won't take the '$user':$GROUp' part
Makes sense, I am by no means an expert but it seemed off.
I find it strange to look at too.
Given that these variables won't contain spaces, you don't even need to quote them at all.
You can do all of this using `find` in one command. This is totally untested and I might have gotten some of the bits wrong (I can never remember the time predicates), but something like find . -name "*.plist.???????" ( -not -name "*.plist.default" ) -ctime 10d -exec rm {} + Read through `man find` for details.
 test -d $PATh/$user &amp;&amp; continue (note that '[' is an equivalent of 'test' with the only difference that the '[' command needs to be closed with a final argument of ']'. I consider this evil because '[' is actually a command trying to masquerade as syntax, which confuses the hell out of people, so unlike most sh/bash scripters I use 'test' instead.) 
Tested, and it works beautifully. Thanks again!
Whoops, I misunderstood what you were going for. I'd still use find for the first part, then loop over the files in your directory each time.
First of all, my approach here would be different anyway - I'd define a recursive function rather than do recursive script self-executions. Much faster and easier to program. But here are couple of comments on your code first: ---- $("$path_to_script" "$new_start_dir" "$new_indent" "$outfile") I don't think that does what you think it does. It takes the script's output and tries to execute it as commands. You want to lose the $(…) here. ---- files_in_dir=(*) for idx in "${!files_in_dir[@]}"; do is needlessly complex, you can simply do: for file in *; do or if you want the hidden files as well: for file in .* *; do You do have to make sure to skip "." and ".." to prevent infinite loops, as well as ".\*" and "\*" themselves because the wildcards will not be resolved if the pattern doesn't match any file: case "$file" in "." | ".." | ".*" | "*" ) continue ;; esac ---- Anyway, here is my implementation of your spec, adapted from an earlier recursive directory traversal script I did. Feel free to reply if you have any questions. #!/usr/bin/env bash traverse() { local file dir=$1 indent=$2 cd "$dir" || return for file in .* *; do # skip current/previous dirs and unresolved wildcards case "$file" in "." | ".." | ".*" | "*" ) continue ;; esac echo "$indent$file" # recursively traverse/indent directories that are not symlinks if [[ -d "$file" &amp;&amp; ! -L "$file" ]]; then traverse "$file" "$indent " fi done cd .. } traverse ${1:-$HOME} &gt; ${2:-$HOME/hdl.txt} *edit:* minor bugfix so it now works with a relative path as the first argument *edit2:* personally I'd rather leave off the output redirection to $2 or $HOME/hdl.txt so the user is free to view the output directly or redirect/pipe it as they wish, but that's just me.
lowercase variables are typically used for scripting. Capitalised variables are reserved for system vars. $PATH is an important system variable which says where to look when a command is called without the full path. In your case, you refer to your binaries by their full path so it wouldn't affect you - however it is something to be strongly avoided. A small warning that when you are working on multiple distros, they may have their binaries in differing locations. Generally speaking, lowercase variable names are safe to use as needed. They stay local to the execution of the shell/script/subshell unless exported and it's not a good idea to rely on lowercase exports for that reason (amongst others).
Google has a quick shell style guide for scripting. It's no means authoritative, but a good place to start. https://google-styleguide.googlecode.com/svn/trunk/shell.xml Also a good practice is to encapsulate your script in a function[s] in order to help with structure and separation of concerns. I usually use this type of boilerplate. #!/usr/bin/env bash main() { # Code goes here foo bar.... } if [ "$0" = "$BASH_SOURCE" ]; then main fi The if $0 idiom makes sure that the code following only runs if the script is ran as an executable. That way the main function does not run if it were to be sourced from another file.
Also might want to read the man pages for adduser/useradd, since they can take care of creating user home directories when making users, and usermod can move home directories around.
Thanks, I'm glad it was useful and I hope others find it instructive as well. I'm pretty sure the main increase in speed comes from not launching a new shell for every recursive iteration. I have a bugfix though. It didn't work with a relative path. The recursive function call traverse "$dir/$file" "$indent " should be changed to: traverse "$file" "$indent " which is even simpler ;) I edited the original above. 
Looks like your files got converted to DOS format, i.e. line endings are \r\n rather than just \n. run: dos2unix ~/.bashrc dos2unix ~/.bash_aliases And any other file you restored from there.
Thank you so much! I was almost read to just retype out all my .rc files.
Just a few comments: Capitalizing bash variables should be avoided, as you might accidentally clobber a pre-defined/system variable, certainly you shouldn't mix case, like `PATh`. While the array use is fine, it's worth noting you can do it with a white-space separated string, as well, especially in the instance of `for i in foo`. And as others have already pointed out `'$foo'` = `$foo` where `"$foo"` = the contents of the variable `$foo`. path=/home group=purple users="home1 home2 home3 home4 home5" for user in $users; do if [ ! -d "$path/$user" ]; then /bin/mkdir -p "$path/$user" /bin/chown -R $user:$group "$path/$user" fi done
 #/bin/bash directory=/foo/bar tempdir=/tmp/foo/bar find $directory -name '*.plist.???????' ! -name "*.plist.default" -exec mv {} $tempdir/ \; find $tempdir -mtime +10 -delete
I think you can use `+` instead of `\;` in the first find. It's more efficient.
Depends on how many files, it concatenates the output to the `-exec` command. `+` would use 'more' memory, and less processor, but could possibly hit `mv`'s max args.
I don't think I have ever heard of tree or find. It looks like tree is not currently installed by default. Is the tree that I can install from `apt-get` the same tree that is explained [here](http://www.centerkey.com/tree/)? if so that is a pretty neat one-line command. Thank you for your scripts! There are actually several things that I have been getting to research about how you wrote them. I have spent a while now trying to figure out how/why some things are working, I will have to come back Sunday or next week and do some more trial and error on parts of it. (The parts that are throwing me right now is the line `done &lt; &lt;(( cmd1; cmd2))` Why are there two redirect characters and why are there two parenthesis? ) Thank you for the opportunity to see a different way to do it, not to mention the exposure to new concepts! 
Yes, `tree` should be easily installed with `apt-get install tree`. `&lt; &lt;((cmd1; cmd2))` spawns some subshells and redirects the output to the while loop. I shamefully admit I cannot fully explain how it works, but perhaps /u/KnowsBash will pop in sometime. Basically if you do `&lt;(some_command args | some_other_command)` you're creating a 'file'. Then you're using `&lt;` to feed the contents of that file in via stdin to the process you're redirecting to. Many commands that take a file argument can accept &lt;(foo) in place of 'filename'. For example: `diff &lt;(sort somefile1) &lt;(sort somefile2)` will diff two files, like `diff somefile1 somefile2` but sort the contents first. The reason there are 2 sets of parameters is so that I could group the output from both finds into one stream.
 function trayr { action=$1 width=$2 if [ -z "$width" ]; then width=2 fi case $action in start) if pgrep trayer &amp;&gt;/dev/null; then # echo "trayer already running" &gt;&amp;2 return fi trayer --edge bottom --align right --width $width ;; stop) pkill trayer ;; *) echo "Usage: trayr [start|stop] &lt;width&gt;" return 1 ;; esac } Then run `trayr start 4` to get the width of 4. Why would you want a width of 4 to result from an argument 2?
`grep -xocf keywords_file text_file`
The culprit is ftp text mode btw. Next time use scp :-) 
Those are a few nice methods I hadn't thought of myself. A few comments though: ---- About the dirtree function: &gt; while read file; do A simple "read" will interpret the input, removing things like leading and trailing spaces from the filename. To be safer, you should do: while IFS='' read -r file; do &gt; echo "$indent"$(basename "$file") Command substitution $(…) launches a subshell, and doing that for every file makes your code orders of magnitude slower. You can get the base name much faster using a shell substitution which avoids launching subshells: echo "$indent${file##*/}" &gt; done &lt; &lt;((find "$dir" -maxdepth 1 -type d; find "$dir" -maxdepth 1 -type f)) How about symlinks and special files? Instead of '-type f', use '-not -type d'. ---- &gt; for i in $(seq 1 $(($depth - 1))); do &gt; printf "%s" " " &gt; done This can be done more efficiently: printf "%${depth}c" ' ' ' ' ' ' ' ' (meaning: repeat four spaces $depth times) ---- &gt; printf '%*s\n' $((depth - 1)) | sed -e "s/\ / /g;s/$/$file/" To eliminate the slow 'sed' invocation: printf '%*s%s\n' $((depth * 4 - 1)) ' ' "$file" 
Well, /u/cpbills made me think about parsing the output of 'find' with 'sed', so here is a one-liner to end all the others: find * | sed 's,[^/]*/, ,g' 
Is there a gui for scp similar to filezilla?
It will use insignificantly more memory, and no, find is not that dumb. It will not exceed ARG_MAX.
There's winscp: http://winscp.net/eng/download.php . Also I think you can configure filezilla to use binary mode when transferring, if you still prefer that. 
Excellent tips and improvements. One thing that may need fixing is `printf '%*s%s\n' $((depth * 4 - 1)) ' ' "$file"` probably need `printf '%*s%s\n' $(((depth - 1) * 4)) ' ' "$file"` instead, to get the desired result.
/usr/bin/find is super useful. You should definitely learn to use it. There's a bunch of stuff it can do, and you can almost definitely apply it here. # Find all files older than 60 days find /mnt/vol1/ -type f -mtime +60 # Find all files younger than 60 days and remove them find /mnt/vol1/ -type f -mtime -60 -exec rm {} \; # Find directories older than 10 days, and starting with the name "logdir" # Might as well tar them while we're at it. find /mnt/vol1/ -type d -mtime +10 -name "logdir*" -exec tar czvf /root/logdir.tar.gz {} \;
&gt; Top level dot files and directories will be missing, and file names with leading dashes will break this. Good points. Fixed version: find . -mindepth 1 | sed 's,^./,,; s,[^/]*/, ,g' &gt; So will file names with newlines, but that's probably not an issue. They really shouldn't have been allowed. 
if we are worrying about leading and trailing spaces, we should care about newlines too while IFS='' read -rd $'\0' f; do ...; done &lt; &lt;( find ... -print0 ) 
Would using sftp give me the same issues?
No, sftp doesn't mangle data by default. It's just ftp that is insane. See [FTP Must Die](http://mywiki.wooledge.org/FtpMustDie).
Every icon in tasker is 2 wide so I wanted to input the number of icons rather than the actual width.
 function trayr { action=$1 width=$2 if [ -z "$width" ]; then width=1 fi case $action in start) if pgrep trayer &amp;&gt;/dev/null; then # echo "trayer already running" &gt;&amp;2 return fi trayer --edge bottom --align right --width $((width * 2)) ;; stop) pkill trayer ;; *) echo "Usage: trayr [start|stop] &lt;width&gt;" return 1 ;; esac }
In answer to your question, yes bash is perfectly suited for this task. However, I don't like writing scripts that clean up after other misbehaving scripts/programs. Can you figure out what's creating the "lock files?" "lock files" are generally used to tell another instance of the same script that it's already running. The fact that you're having to manually delete them indicates an issue with the script that creates them. Perhaps it's being interrupted? And it doesn't handle interrupts correctly by cleaning up it's lockfile before dying? My point is, try to find the cancer, instead of scripting something that treats the symptoms, I guess. 
Yeah, that's kind of the rub. I would like to have the ability to fix the application that is causing the issue requiring cleanup. Unfortunately it's not my application, it's Apple's. It's been an issue in OSX for a while, seemingly since 10.5. If you've got a mac, you've probably got a bunch of them hanging out somewhere on your system as well. From a stand alone point of view, they are benign, but on an OD environment with roaming profiles, it's not quite so benign as I've recently discovered. I'm going to look deeper into why so many are being created, and it's possible that apps aren't quitting cleanly on logout, which could possibly be cured by having a script launch at logout to clean up any lock files etc. That's something I would have to consider later on down the road. My solution is basically the carmex for the herpes in some of Apple's programs. 
What is the maximum that can be used? 
Thankyou!
Thanks. I appreciate the help.
Thanks. We use a make file system to get all the users from another server. They each are members of between 5 -15 groups. It compares the system files with what it has and combines them.
Thanks. This " if [ ! -d "$path/$user" ]" just means if there is no home already; then continue. If there is a home, it just goes to the next value in the array right?
Your last sentance is beautiful.
If I'm understanding '+' from the man page, it runs the initial find to gather a list of files, and then instead of running, say, rm once for each individual file, it runs rm once for all files? Is that correct?
This is a very, very simple script. You say you have an advanced understanding of linux. You need to work on your bash. I would strongly suggest automating a ton of your day to day tasks. cron and bash scripts go a long way.
You can check with vi to see what's in the 4th field, then write a substitution for it.
Yes, up to the max number of arguments allowed, so it's more efficient (only one process forked). Note that it literally replaces the "{}" with a list of filenames, so you need to be sure that whatever command you're exec'ing takes a list of files in that particular argument.
Awesome. I was worried that it was exactly that. Thank you for confirming. I'll be optimizing some scripts when I'm back at work.
Just another way: I used to use `tr '\r' '\n'` or `tr '\015' '\012'` when I was going back and forth from a mac to *nix a lot 
Don't you end up with duplicate newlines that way? I usually use tr -d for this.
been a while but I seem to remember only cr with no newline coming from the mac end
Return value and output are entirely different things. Your first if is checking the return value of a `[[...]]` command. The `[[...]]` returns 0 if the expression inside evaluates to true, and the expression inside is doing an integer comparison on the output of curl. curl's return value is completely ignored. Also see [FAQ #2](http://mywiki.wooledge.org/BashFAQ/002)
You can use set -x at the point you want to turn debugging on, and set +x at the point you want to turn it off. $? holds the return value of the last command executed, so you may have to string your commands out and assign their output to a variable, and intersperse 'echo $?' to get the return value after each command is executed. The best advice I can give is to play with these two ideas, and you will figure it out. 
As has already been mentioned by the 2 other replies, what the `if` is checking and the "return value" are 2 different things in bash. When a program is run, it outputs text to `STDOUT` and `STDERR` and it returns a status code at the end-- 0 for success and something greater than zero for an error. When using an `if` statement, if the command execute without error and return a 0 status code, then the `if` condition is satisfied and the code inside the block gets run. Whether any data is output to `STDERR` or `STDOUT` is not taken into consideration by the `if` itself, but might be used by the condition inside the double-bracketed section. Some examples: DATA=$(curl google.com) echo $? `$DATA` contains the `STDOUT` output from the curl command, which is captured and not output to the screen. The status code of the `curl` command is stored in `$?` (after each command that is run, the status code is stored in `$?`) and is echo'd to the screen, so assuming that you're connected to the internet and have access to google.com, the above code should just output a `0`. curl www.google.com || echo "oh no" in that example, I use the `||` (or) operator, which says that if the first command returns a non-zero status code, then execute the second. The `||` completely ignores any output from the `curl` command. Running that *should* show html and not output "oh no!" That is functionally equivalent to: if curl www.google.com; then # do nothing else echo "oh no" fi Code inside double-brackets is syntactic sugar for the `test` command. For instance, the following 2 commands are equivalent: if [[ "$DATA" = "spike" ]]; then echo "they are equal" else echo "not equal" end and if test "$DATA" = "spike"; then echo "they are equal" else echo "not equal" end the first version of that, with the double-brackets, is a lot easier to read. I hope this clarifies things and answers your questions.
the `$()` notation is called "command substitution" and its return value is the one of the last command executed inside it (just like a subprocess of any kind). the problem with your setup is that it's wrapped within the `[[` command which has a returncode too (as well as the `if` command). I would suggest that you create a variable first then check things separately, like this: curl_output="$(curl --write-out %{http_code} --silent --output /dev/null 'http://example.com/')" curl_retcode="$?" if [[ "200" -ne "$curl_output" ]]; in the second scenario the `if` reads the returncode directly but it'll only tell you if it was zero/non-zero not the actual value (you can get it within the `then`/`else` clauses with `$?`) so if you need the actual value you can also use a variable: pgrep processName &gt;/dev/null 2&gt;&amp;1 pgrep_retcode="$?" if [[ "$pgrep_retcode" -eq "0" ]]; notice you can use `((` for numerical comparisons.
You can use pxz if you have more CPUs than time.
You can use double quotes to just fix your problem at hand: $ cat teste.sh #!/bin/bash echo $1 $ ./teste.sh one two one $ ./teste.sh "one two" one two But getopts (check bash man page) is the "most accepted" form of parsing command line arguments in bash. I recommend you go for that: it will improve your script-fu. 
Did you already edit it based on /u/nix2170 's suggestion? Also, I see perhaps another problem here: if [[ "${current_arg" =~ ^-{1,2}.* ]] Missing a closing `}` ? But in answer to your question, if you want to shift off the option, then accept the remaining arguments to a variable, you'll want to shift, then use `"$@"` k@ox:$ func() { shift; fullname="$@"; echo "The full name is $fullname"; } k@ox:$ func -f John Smith The full name is John Smith Forgive me if I misunderstand the problem 
Thanks, you caught a translation error. I didn't copy/paste, i had to handjam it from one system through my ipad. Not a great coding device. Hence the snippets rather than the full code. I converted to getopts, and I haven't yet tried the double quotes. I'll retry that tomorrow, but I think during my original testing (a few weeks ago) double quoting the fullname did not work. For the time being I think my solution is: **fullname="$(echo $@ | cut -d'-' -f1)"** if [[ "$1" =~ ^((-{1})([Hh][Ee][Ll][Pp])|)$ ]] then echo -e "Help:\t${usage}" exit 1 else while [[ $# -gt 0 ]] do opt="$1" shift current_arg="$1" if [[ "${current_arg}" =~ ^-{1,2}.* ]] then echo "Error: ${usage}" fi case "${opt}" in "-e"|"--email" ) email="$1"; shift;; "-f"|"--fullname" ) fullname="$(echo $@ | cut -d'-' -f1)"; shift;; "-g"|"--gid" ) gid="$1"; shift;; * ) echo "Error: Invalid option: ${opt}" echo "${usage}"; exit 1 ;; esac done fi edit: Hmm, I see I said 'I converted to getopts' but I think I a word... I did not use getopts, obviously by the code above.
&gt; This script actually started with getopts but was not quite doing what I wanted Because you wanted long args, like `--email`, or for some other reason?
Thank you. I'm such an idiot. I had the knowledge already, I just didn't think of it. As a German saying goes "You can't see the forest because of all the trees". :-) 
Because I wanted long args, and because I have a few args that are either a or b, but not both.
The -f variable is properly set when using double quotes, but it wasn't being properly expanded into the command *useradd* further on in the script. I managed to get that part fixed, so thanks everyone. I understand getopts is the preferred command for pulling arguments, however it did not work quite right with my script.
If you're ok with ditching the GNU-style options, getopts should work very well for this. while getopts e:f:g:h opt; do case $opt in e) email=$OPTARG;; f) fullname=$OPTARG;; g) gid=$OPTARG;; h) usage; exit 0;; '?') usage &gt;&amp;2; exit 1;; esac done shift "$((OPTIND - 1))"
one-liners for what?
the quotes are for the caller not (only) for your script, if you say to bash: `./myscript foo bar` it'll see two arguments and pass them separately however `./script "foo bar"` is just one (including the space). if the caller doesn't use quotes there isn't much you can do to fix it.
&gt; I want the -f to be able to accept spaces when I assign the variable. I'm thinking "$@ | cut -d'-' -f1" should do it? nope. what happens when the user happens to have a name like Catherine Zeta-Jones? And what happens when the user has a name like Joan d'Arc? Single quote opens a string, so you need escaping or... " ". Every problem with oddball names would be solved if you just bothered to add 2 " to be explicit about the bounds of the name. besides, this `echo $@ | cut -d'-' -f1` is ugly. If you really need this hack, something like this would be better $ n=$*; n=${n%% -*} 
Generates a random cron line given an interval. #!/bin/bash I=$1;R=$RANDOM;S=$(( R %= 9 ));C=$S;i=1;while [ $i -lt $( expr 60 / $I ) ];do C="$C, $(( $I * $i + $S ))";let i=$i+1;done;echo $C 
Thanks. I got it figured out. The variable was properly set with double quotes, but I was not properly passing the fullname variable to the useradd command further along in the script. The line of code that was the culprit: ssh -q ${host} useradd -u ${uid} -g ${gid} -c "${fullname}" ${uname} which I ended up changing to: ssh -q ${host} "useradd -u ${uid} -g ${gid} -c \"${fullname}\" ${uname}"
ok, i thought you wanted to apply heuristics so you can be lazy with typing your arguments. always quote variables, especially if there is any chance there will be a whitespace in there. Word splitting of unquoted variables is a bitch and will make things go very wrong. Quotes make the shell ignore the content, without quotes the shell feels like parsing, breaking the contents apart and degrading original amount and mixture of whitespace to 1 plain space. To be honest given the existence of arrays in bash, implicit word splitting is a near useless 'feature' existing for legacy reasons that only causes grief. I wouldn't shed a single tear if it was a thing of past. Pretty much any use case can be programmed around in a much more robust way. TL;DR: rule of thumb: **always double-quote your variables, forget about the whitespace breakage**. 
So there's two shells involved. Your current solution will fail if fullname contains special shell characters, such as a `"`. To be safe, you will want to use `printf` with `%q` to safely quote the string for reinsertion to bash. printf '%q ' useradd -u "$uid" -g "$gid" -c "$fullname" "$uname" | ssh "$host" bash See [FAQ 96](http://mywiki.wooledge.org/BashFAQ/096) for more on that.
numbers &gt; 60 can happen, i don't think that's correct $ is unnecessary inside (()), words are assumed to be variables. You'd need $ only to call positional parameters or to fix the issue with assumed octal numbers (leading 0) by explicitly declaring base -&gt; `(( 10#$var ))` while condition can use (()) as well: `while (( i&lt;(60/I) ))` `let i=$i+1` =&gt; `((i++))` simpler way C=$((RANDOM%I)); for (( i=C+I; i&lt;60; i+=I )); do C="$C, $i"; done; echo "$C" 
Cool, I'm no bash expert, just had to do it for a different project. Improvements appreciated. 
since you're asking you should provide that information.
you're cheating :) concatenating lines with `;` isn't really a one-liner. also, use four spaces of indentation so your post appears as code.
I'd never trust a coder who didn't cheat. 
try the find command - there's exec and execdir switches which will do exactly what you want, and the man page example is pretty good. Just be careful about escaping that last semicolon in the exec arguments! 
As /u/xilanthro mentioned, the canonical answer is to use find. find /media -name "*.MOV" -exec cp -T dest/ {} + As a curious side note, it's showing those spurious "*.MOV" files because there are no MOV files in that given directory. If you set a shell option called `nullglob`, it will do the "right" thing and skip any for loops where the glob doesn't match any files. 
 files=$( ls -r media/*/*/) for file in $file this thing cannot die soon enough. what about whitespace? Do not feed for-loops with `ls` output nor any other plain text with implicit word splitting performed on it. You have globs, arrays and the `while read ...; do ...; done &lt; ...` boilerplate for that.
Thanks a lot, that worked great. So is there an efficiency benefit to using find over globstar?
hard to tell in general. I did some tests on my mp3 collection and `**` appeared to be faster at printing than find. I don't feel like concocting complex tests to discover the whole truth though. In some cases there might be benefits to using `find`, let's say you have to do something fancy with an absolutely huge list of files, where bulk processing is impossible and you have to deal with files individually 1 by 1. The advantage of `**` might be undone by 2 things - most likely `**` waits until it has all elements ready, which means some startup delay and a significant memory footprint - find output can be processed on the fly and discarded so you get results right of the bat and low memory footprint No idea if this assertion is sound though, i don't know lowlevel stuff well enough to guesstimate impact of these factors. also `**` is a rather recent, bash specific feature (v4.0+) so you see `find` based solutions more often.
Also, anyone else who has to read your script might not know about **.
try [Python's lxml](http://lxml.de/), it'll make it easier to process the XML and also to generate the formatted output.
Seems you got a slight error there for var in $(cat filename); do ./test -s $var -p 80 -o out-filename; done
Thank you so much!!!!
You're welcome :)
Seems like the OP clearly says he wants to iterate over the IPs in $var position. So...
It could go one step further, removing the UUOC: for i in $(&lt;filename); do ./test -s "${i}" -p 80 -o out-filename done
Mate, read the post once again slowly, then check your code
To read a file line-by-line, use a 'while' loop with 'read'. The following snipit redirects the file "filename" into the standard input (stdin) of the 'read' command. Every line read by 'read' is saved to the variable $line. while read line; do ./test -s "$line" -p 80 -o out_file.txt done &lt; filename 
FTA: &gt; Signs you should not be using a bash script &gt; your script is longer than a few hundred lines of code I've written a few 2-3 thousand lines shell scripts. A couple were horrible and the rest were so clean and well structured that they've been in use in industry for dozens of years. Script length has no bearing on quality. &gt; you need data structures beyond simple arrays Bash also has associative arrays. With a few functions you can do anything. &gt; you have a hard time working around quoting issues Any language has difficult spots. Quoting isn't that hard once you understand how the shell parses; everything becomes crystal clear. &gt; you do a lot of string manipulation Why? Bash can do concatenation, extraction, regex, etc... &gt; you do not have much need for invoking other programs or pipe-lining them Not using features in a language is no reason not to use it. &gt; you worry about performance &gt; Instead consider scripting languages like Python or Ruby. If I had a penny for every perl and python scripts that I've replaced with shorter, simpler and more efficient bash scripts I'd have a couple of dollars. Bad code is bad code in any language. These aren't "signs" that you shouldn't use bash, they're challenges to write better, more efficient code. 
I work in bash doing TONS of string manipulation... I work on a NOSQL database model that doesn't have an API so I have to use a binary and parameters to generate reports that I parse and then loop through doing more reports, etc. I bookmarked this and I will still go through this guy's article to see if there is anything super helpful, but commented based on the quotes your comment.
is it supposed to extract the mem%? try this bash -c 'ps -p $(pidof myProg) -o pmem='
Completely agree. All the good habits and design patterns of maintaining a program apply to shell languages. I don't see how a bash script is a bad option once the program has reached a couple hundred lines. Things like argument parsing, input validation, and well maintained namespace can easily make a script over 1000LOC
To go one further function get_mem() { while read pid; do mem=$(ps -p "$pid" -o pmem=) echo "$pid $mem"; done &lt; &lt;(pgrep "$1") }
&gt; /bin/bash -c 'top -n 1 -p $(pidof myProgram) | grep myProgram | awk "{print \$10}"' The 10th element is actually the current CPU% which isn't quite so easy as the mem%
ps -p$(pidof myProg) -o **pcpu**= ?
Oh, now I see. It's the combination of the double quotes and the $ of course.
That would give me the CPU% since the process started (in this case days ago). I want the current CPU% just for this moment in time. I ended up using: pidstat -h 2 1 | grep myProgram
[Don't read lines with for](http://mywiki.wooledge.org/DRLWF)
you can add them back; see --&gt; http://git.io/T67vPw
Why do you think `pcpu=` is the cpu utilization since the process started?
&gt; This paste has been removed!
So sorry http://pastebin.com/PA83zzuM
You need a space after every [ and before every ]. if [ $pass == "Ace" ]; then notice the spaces!
Is this what you meant? It still doesn't work http://pastebin.com/cv9WEMgM
 if [[ "$destination" == "Data Storage" ]]; then echo "$destination" open /media/levi/Encrypted/.SecretDataStorages You need to close this if block before starting a new one
/r/commandline would also appreciate it.
I'd use awk rather than two greps there. touchState=$(xinput --list-props 14 | awk '/Device Enabled/{print $NF;exit}' and avoid using the `[` command in bash. Use `[[` to tests strings and files, and `((` to test numbers. if (( touchState == 1 )); then alternatively you could have the awk exit with success or failure instead of capturing its output. if xinput --list-props 14 | awk '/Device Enabled/{status=$NF;exit} END {exit(!status)}'; then
Well, I did that in [this one](http://pastebin.com/7ZazK2ST) but it still doesn't work, doesn't bash have something like elseif?
thanks for the tips.
for item in $(find . -type d); do script.sh basename $item; done
Is a perfect example of how to NOT do it. [Pitfall 1](http://mywiki.wooledge.org/BashPitfalls#pf1)
Wow. I had no idea. Just learning this myself. Thanks for the tip. You know...you try to be clever with one-liners...
&gt; Extensions Carrying scissors without pointing them downward is also *potentially* harmful, but only in combination with ignorance or carelessness. It's a valid argument, but it gets more press than it deserves. 
no harm in being clever but the for loop is not for that. You should use it with things like script globs, positional parameters, arrays. A rule of thumb: if it's file or command output -&gt; while read while read item; do script "${item##*/}"; done &lt; &lt;( find . -type d ) not that different in length ;-) 
That’s not perfect either… filenames with newlines in them are allowed. (The only illegal characters AFAIK are the forward slash `/` and the null byte.)
your afaik is right. for that there is `find -print0` combined with `read -d $'\0'`. I gave a direct replacement to that `for item in $()` to show there is more kosher way to do the same clever one-linery thing. Either way it would be awesome if people even started to use while loops. Amount of for-loops used in dirty hacks on the internet is staggering. 
A couple notes: The shebang is missing bang. If you're using bash, prefer [[ to [. The whole qlmanage thing at the top is more easily written if ! type qlmanage 2&gt;/dev/null; then echo ... exit 1 fi Prefer `$(...)` to backticks. Is nfile supposed to be the absolute path to the file? Why do you need that? Instead of piping wc to awk, just `stat -c %s $thumb`.
`for file in $*; do` 1. $* glues params into 1 string using spaces 2. unquoted = word splitting = bad. use `for file in "$@";` or even `for file`; ("$@" will be used ) Quote variables that can have whitespace, like $file. Any space in sensitive place = boom! 
well if `pr` can't do it it's as simple as that, most commands that follow the old unix principle don't try to do much but instead rely on other commands to do the rest. it seems to me that the intended way to do this is along with `fmt` but I'm not very familiar with each of those so I'm not 100% sure.
 * [autojump](http://packages.debian.org/wheezy/autojump) - shell extension to jump to frequently used directories ([Homepage](http://wiki.github.com/joelthelion/autojump/)) 
From my `~/.bashrc` # http://www.reddit.com/r/linux/comments/1xcdtk/the_generally_helpful_bashrc_alias_thread/cfa6uoj # http://jeroenjanssens.com/2013/08/16/quickly-navigate-your-filesystem-from-the-command-line.html export MARKS=$HOME/files/marks jump() { cd -P "$MARKS/$1" 2&gt;/dev/null if [[ $? -ne 0 ]]; then echo "No mark found: $1" fi } mark() { if [[ ! -d "$MARKS" ]]; then mkdir -p "$MARKS" fi ln -s "$PWD" "$MARKS/$1" } marks() { find "$MARKS" -maxdepth 1 -type l -printf "%f -&gt; %l\n" | column -t } _marks_complete() { local word=${COMP_WORDS[COMP_CWORD]} local list=$(find "$MARKS" -maxdepth 1 -type l -printf "%f\n") COMPREPLY=($(compgen -W '${list[@]}' -- "$word")) return 0 } complete -F _marks_complete jump
I don't actually use the jump/marks thing in my bashrc, but I do keep it around as an easy-to-understand example of adding completions ;) Glad I could help share something I also shamelessly stole. *grin*
I just roll push and popd. For certain boxes, the. bashrc sets it up.
http://ss64.com/bash/pr.html
Not quite a function, but: alias batlife='echo `upower --show-info /org/freedesktop/UPower/devices/battery_BAT0 | grep --color=no "time to \(full\|empty\)"` " (more info with upower --dump)"' Result: time to empty: xyz minutes (more info with upower --dump)
 $ type fortunes fortunes is aliased to `#!/bin/bash -i REPLY=; backupFile=/tmp/fortunes_history_backup; tmpFile=/tmp/fortunes_history; echo -n &gt; $tmpFile; # clear the temporary history file history -w $backupFile; # save history ... history -c; # and clear it. while [ ! \( "$REPLY" = "q" -o "$REPLY" = "quit" -o "$REPLY" = "exit" \) ]; do clear; if [ -n "$REPLY" ]; then echo $REPLY &gt;&gt; $tmpFile; history -r $tmpFile # history -s $REPLY does not work! fi (fortune -c $REPLY) || (clear; fortune -c;) # try to use the command as fortune file; # if that file can't be found, use a random one echo; # write a newline read -e; done history -r $backupFile; # restore history' As you can tell, this is from a while ago. Not sure how aliasing something to `$(cat script)` was ever a good idea… I should probably fix that. Anyways, I made this while I was bored in lectures. I would view fortunes, one at a time, but I got tired of repeatedly typing &lt;up arrow&gt; &lt;enter&gt;, so instead I created this – initially, it just displayed fortunes in a loop (clearing the screen each time because otherwise it could be tricky to tell where one fortune ended and the other started), and then I sorta expanded it to be able to enter fortune files (gimme a `linux` fortune next thanks), then I added history, etc.
By the way, `$(&lt; file)` has the same result as `$(cat file)` (unless you’re actually con*cat*enating several files), but doesn’t spawn a new process.
cat is a shell builtin in bash so it doesn't spawn a new process. so both are essentially same. 
`echo` is a shell builtin, but `cat` isn’t. Try `cat &amp; ps -ef | grep cat` – you’ll see your `cat` in the process list.
sorry. my bad. 
No problem. It would absolutely make sense to make it a builtin, given how often it’s used…
My favorite: `mkcd` $ type mkcd mkcd is a function mkcd () { mkdir -p -- "$@" &amp;&amp; cd "$@" }
Good catch... I vaguely remember that being an issue before, updated my dotfiles now. 1 internet point to you kindsir. EDIT bad spelar
[Fixed it! ](https://github.com/goozbach/dot_files/commit/9040eb4fe7d8f11c81fdbb30f84113974e612e9e) Thanks again 
Here are some of mine, sharing them mostly so people can get ideas as these were written on OSX and might not be portable, and some of them aren't that great anyway. --- First, some that are specifically osx: #cd to current finder window open fcd() { target=$(osascript -e 'tell application "Finder" to if (count of Finder windows) &gt; 0 then get POSIX path of (target of front Finder window as text)') if [[ -n "$target" ]] ; then cd "$target"; pwd else echo "No Finder window found" &gt;&amp;2 fi } #reveal a file or folder in the finder reveal(){ echo 'tell application "Finder" to reveal POSIX file "'"$@"'"' | \ osascript &amp;&gt;/dev/null } #look up the word in the osx dictionary dic() { echo 'open location "dict:///' "$@" '"' | osascript } #open a quicklook preview of item ql() { qlmanage -p "$@" &amp;&gt;/dev/null } Now for others: Similar to /u/goozbach #cd and ls a directory at the same time cdl() { cd -- "$@" &amp;&amp; ls } #get a short summary of a wikipedia page wiki() { mysummary=$(dig +short txt "$@".wp.dg.cx) test -n "$mysummary" &amp;&amp; echo "$mysummary" || \ echo "No result found for query..." } #expands short urls unshort() { url=$(pbpaste) echo "Short URL: ${url}" curl -sI "$url" | grep "Location:" | cut -d " " -f 2 | tr -d "\n" | pbcopy echo "Unshortened: $(pbpaste)" } This one depends on `pandoc` and is only marginally useful. done more cuz it's fun #view .md file in less after markdown is applied markless() { pandoc -s -f markdown -t man "$1" | groff -T utf8 -man | less } This one depends on `imagemagick` / `graphicksmagick`. Kind of primitive, was planning to turn it into a script at some point. #create a 3x3 chart from a folder or set of images. Images are used in order #passed. Can use other dimensioms or '3x' chart() { if (( $# == 1 )) ; then cd "$1" montage -border 0 -geometry 600x -tile 3x3 * ~/Desktop/3x3.jpg else montage -border 0 -geometry 600x -tile 3x3 "$@" ~/Desktop/3x3.jpg fi } Views the lyrics of a song currently playing in VLC. should work on linux also. Depends on `medainfo` lyrics() { if [[ -z "$@" ]] ; then mpath=$(lsof -F n -c VLC | grep 'Music\|\.mp3\|\.m4a\|\.flac\|\.wma' | \ cut -c 2-) else mpath="$@" fi mediainfo "$mpath" | grep Lyrics &gt; /tmp/lyrics.txt if [[ -s /tmp/lyrics.txt ]] ; then cat /tmp/lyrics.txt | cut -d ':' -f 2- | sed -e 's/None: //' |\ tr '/' '\n' | less else echo "No Lyrics Found In Current Song." fi } Create a .m3u playlist of the all the songs in the directory that was passed plmake(){ pushd "$@" &amp;&gt;/dev/null mfiles=$(find -E . -type f -maxdepth 1 -iregex ".+\.(mp3|m4a|flac|wma)" | \ cut -c 3- | sort -g) if [[ -n "$mfiles" ]] ; then echo "#EXTM3U" &gt; playlist.m3u echo "$mfiles" &gt;&gt; playlist.m3u fi popd &amp;&gt;/dev/null } Read .srt and .ass files with the formatting stripped, -r for regular(.srt) and -a for anime (.ass). Very unlikely to be portable cuz of all the sed stuff, but might serve as inspritation to some srtless() { case "$1" in -r) grep -vP "^[0-9]" "$2" | less ;; -a) sed 's/0,,/§/g' "$2" | grep "§" | tr '§' '\n' | \ grep -v "Dialogue:\|Comment:" | sed -e 's/{.*}//g' -e 's/\\N/ /g' | \ uniq | less ;; -h|*) echo "Usage: srtless [-rah] file" ;; esac } Take a screenshot and scan it for QR codes and barcodes qrscan(){ case $@ in -n|'') flag="" ;; -i) flag="-i" ;; -h|*) echo "Usage: qrscan [-inh] (non-/interactive)" &amp;&amp; exit ;; esac cd /tmp/ screencapture -x $flag zbarscreen.png xview -s ./zbarscreen.png echo zbarimg -q ./zbarscreen.png || echo "No barcode found." } 
[here's my functions](https://github.com/git2samus/rcfiles/blob/master/.functionrc), I'll leave it to you to determine if they're useful.
Thank you!
I like this. Thanks for sharing
Even a little shorter: echo &gt; /var/log/foo.log
And &gt;| /var/log/foo.log to override noclobber in case it is set.
Hahaha. And no, I don't sorry. I mostly picked out the ones others would find useful anyway, the rest are just things that have no use at all.
no I'm sourcing it from .bash_profile and I also have an .aliasrc that I source from .bashrc they're not defaults, it's just that my default rc's became too large at some point :P.
Thanks!
Look, I know you already got your answer and have moved along and forgotten about this. But did you even bother using google? Using keywords from your own subject 'clear touch file' you can find an answer within the first 10 results. (#2 for me) If you bothered specifying 'linux touch clear file', it would probably be the #1 result.
Something like this should keep it running: #!/bin/bash echo "Starting worldserver." ret=1 while [[ $ret -gt 0 ]]; do ./worldserver ret=$? done echo "Shutting down worldserver."
It worked perfectly, thanks!
 : &gt; file Done 
Very nice, and a little Buddhist wisdom in there too: "Why redirect something empty, when you could redirect nothing at all?"
this is really neat. I never knew about it before
I didn't know about BASH_VERSINFO. Today I learned. And the claim about 100% pure bash was for the parser, **cmdarg_parse**. ("Its parser is written in 100% pure bash, and is self contained in cmdarg_parse"). That is absolutely true. But I agree there are some parts of cmdarg that still shell out for things they don't have to. Feel free to file a bug on github if it rubs you wrong :-)
 cmddarghashkeythingy31337=$(echo "$arg" | cut -d = -f 1) v=$(echo "$arg" | cut -d = -f 2-) kind of lame ;-) cmddarghashkeythingy31337=${arg%%=*} v=${arg#*=} 
I love how the lame part is shelling out to cut, and not how I had to do the jankiness to get around the bug linked to in the stack overflow comment :-) Opened https://github.com/akesterson/cmdarg/issues/5 to purge instances of things like that, thanks.
i read the bug but couldn't be bothered to invest brainpower understand what it says ;-) trimming stuff on the other hand is parameter expansion 101 :-)
I've been using bash for about 10 years, and I've been making a concerted effort to understand it for about 3 years now, with it being the primary language I work in for about 2 years now (I'm normally a python/c guy otherwise). And it *still* continues to make me go "oh I didn't know it had that feature".
Your script could be way shorter. For debugging purposes it's quicker to use the -x option (echo each command as it's executed) rather than inserting "echo" statements explaining what you're doing. You can either execute the script with "bash -x reportIP.sh" or use the "set -x" command in the script itself. Line 32, 61, 84: the self-reexecution code is repetitive and can be put in a function. Also, it assumes the current working directory is the directory path to the script, which is not guaranteed at all. Finally, it doesn't pass the parameter back to itself, so it won't work in any case. The code to announce a new IP address is nearly redundant with the code to compare the old with the new, so it can be made shorter by just creating an empty file if one doesn't exist and comparing that one instead. The comments are verbose and often redundant. Use comments only when the code is not self-explanatory. Also, try to write self-explanatory code. Using the GPL for a trivial script seems a bit excessive. All in all I would rewrite it as follows: #!/bin/bash # reportIP.sh is a bash script to send the IP address of a computer to a # specific email address # # Copyright (C) 2014 Rafael Beraldo &lt;rberaldo at cabaladada dot org&gt; # # This program is free software: you can redistribute it and/or modify it under # the terms of the GNU General Public License as published by the Free Software # Foundation, either version 3 of the License, or (at your option) any later # version. # # This program is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS # FOR A PARTICULAR PURPOSE. See the GNU General Public License for more # details. # # You should have received a copy of the GNU General Public License along with # this program. If not, see http://www.gnu.org/licenses/. if [[ -z "$1" ]] then echo "Usage: $(basename $0) &lt;email address&gt;" exit 1 fi readonly HOSTNAME=$(hostname) readonly EMAIL_ADDR="$1" readonly IP_FILE="/tmp/ip.txt" readonly REMOTE="http://canihazip.com/s" trylater() { SELF=$(cd $(dirname "$0"); pwd)/$(basename "$0") echo "$SELF $EMAIL_ADDR" | at now + 30 minutes 2&gt;/dev/null exit 1 } IP=$(curl "$REMOTE" 2&gt; /dev/null) || trylater if [[ ! -f $IP_FILE ]] then touch $IP_FILE fi read OLD_IP &lt; $IP_FILE if [[ "$OLD_IP" != "$IP" ]] then echo $IP &gt; $IP_FILE printf "Your IP has been updated @ %s\nYour current IP is %s\n" "$HOSTNAME" "$IP" \ | mail -s "IP report" "$EMAIL_ADDR" || trylater fi 
He used at for trying again in case of an exception, which is not regular by definition. It seems appropriate to me. Without it he wouldn't be notified for another 24 hours if there is a temporary network failure.
some comments (in no particular order) ## you need to modularize more every duplicated block of code is a candidate for refactor and almost always you'll be able to put that into a function and call it elsewhere (this is general advice, bash or wharever). for instance the logic that runs `at` can be put into a function, the code that send you the email too. ## try not to use uppercased var names they can collision with the shell's own vars, the only scenario when people do use this convention is when declaring global constants (e.g. Java's classpath or similar). ## if you're doing bash don't use the .sh extension .sh is generic but there's an actual `sh` shell which is mandated by POSIX to be a POSIX-compliant shell of any kind therefore the .sh extension should only be used for `#!/bin/sh` (i.e. POSIX-compatible) scripts. for bash you can use .bash or no extension at all, the shebang allows the kernel to identify the program that should run it. ## `return` vs `exit` I think functions should not abort your script, they should use `return` to indicate an error and let the caller decide what to do (even if you just exit afterwards). The reason is that the logic becomes clearer, you can see the code of a function and understand what it does disregarding its relationship with the outside world, then you can see from where it was called and based on that context understand the general flow of the program (also general advice). ## use parameter expansion magic in general it's not the best idea to reference the current script's path or filename (there's border cases when that doesn't even make sense) but regardless, this `$(pwd)/$(basename $0)` could be done much more efficiently like this: `$PWD/${0##*/}` of course in this context the efficiency is not a concern, in fact if it were you wouldn't be using bash at all but it's nice to know anyway. in the same spirit, try to use variables instead of commands where available (for instance `$HOSTNAME` and `$PWD` exist as part of the shell's env). ## careful when checking pipe statuses the return code of a pipe is the return code of the last element, even if some command before it crashes and no output was processed. in your case it doesn't matter because the command you care about in the check is also the last in the pipe but in other situations you would have to check the `$PIPESTATUS` array. ## try not to use and/or and blocks unless it's something really short, it's best to use a plain old `if` since it's way more readable and has fewer potential issues (for instance combining `&amp;&amp;` with `||` is *not* equivalent to an `if/else` clause). ## use local variables if you create a var inside a function it'll be global which pollutes the shell's env and can create unexpected errors when there's collisions; in fact you even use it to export values from the functions you call, that is bad m'kay? read [this](http://blog2samus.tumblr.com/post/75759532302/global-variables-and-shared-state-in-bash-and-beyond) and then use the `local` keyword inside functions to create variables that will die when the function exits. also the `readonly` keyword isn't commonly used, I don't have anything against it but people don't care about declaring variables like that (you may want to do so to prevent bugs when assigning by mistake but that's it). ## there's a trick for running `cat` in a command substitution this is a bashism but you can write $(&lt; "$IP_FILE") and it'll be the same as using `cat` ## there's a trick for sending strings to stdin when sending single lines to the stdin of other programs you can do: `command &lt;&lt;&lt;"string on stdin"` instead of piping `echo` and it's more efficient. if you're replacing `echo -e` you can use this notation to interpret escaped characters: `$'this is\na newline'` ## quote EVERY variable expansion this is important. you've got good quoting in general but there's still a few unquoted vars here and there, it's particuarly important when the vars contain paths or filenames. ## keep you comments up-to-date "bad comments" is worse than "no comments" and you have one that says "Get the IP address from ifconfig.me/ip/" but it actually goes to canihazip.com --- here's how I would apply all the previous ideas: #!/bin/bash ip_file="/tmp/ip.txt" remote="canihazip.com/s" log() { echo "$@" &gt; &amp;2 } get_ip() { local ip ip="$(curl "$remote" 2&gt; /dev/null)" || return 1 echo "$ip" } reschedule() { local delay="$1" log "Running again in $delay" at now + "$delay" &lt;&lt;&lt; "$cmd_fullpath" 2&gt; /dev/null } send_email() { local recipient="$1" subject="IP report" message="$2" log "Sending new IP address" if ! mail -s "$subject" "$email_addr" &lt;&lt;&lt; "$message"; then log "Failed to send email" return 1 fi } ## main ## if [[ -z "$1" ]]; then echo "Usage: ${0##*/} &lt;email address&gt;" exit 1 fi cmd_fullpath="$PWD/${0##*/}" email_addr="$1" log "Getting current IP address" if ! ip="$(get_ip)"; then log "Failed to get IP. Maybe "$remote" is offline?" reschedule "30 minutes" exit 1 fi log "Your current IP address is $ip" if old_ip="$(&lt; "$ip_file")"; then log "Comparing IPs" if [[ "$old_ip" == "$ip" ]]; then log "Your IP address ($ip) hasn't changed" else log "Your IP has changed" log "Sending new IP address" message"Your IP has been updated @ $HOSTNAME" message+=$'\n' message+="Your current IP is $ip" if ! send_email "$email_addr" "$message"; then reschedule "30 minutes" exit 1 fi fi else log "IP file doesn't exist, creating one" echo "$ip" &gt; "$ip_file" message="A new $ip_file file was created @ $HOSTNAME" message+=$'\n' message+="Your current IP is $ip" if ! send_email "$email_addr" "$message"; then reschedule "30 minutes" exit 1 fi fi notice your original script didn't reschedule itself when the IP didn't change since the last time and thus, mine either. 
This is really cool. It seems to be *way* better thought out than any of the other tools that purport to do this. Scanning through the source, issues I see (unsolisited code review, hopefully constructive): Code things: - Should use `BASH_VERSINFO` (already mentioned by KnowsBash) - Should use `%`/`#` instead of `cut` (already mentioned by akesterson) - Use `printf` for escaping strings. (`%q` will print a string correctly shell quoted, great for `eval`) - Setting `-x` (export) on the declarations at the end of the file is a mistake; `-x` is inhibited by `-A` or `-a`, and would have been the wrong thing anyway (you don't want to pass them to all child programs of the script). Probably you meant `-g` (global). - There are a couple variables that you never declare as local (example: `arrname`); you probably don't want to leak them as globals. - You are inconsistent with tab use. - You are *way* heavy on `eval` use. Design/interface things: - (IMO) When calling `cmdarg_usage` after an error, it should be called as `cmdarg_usage &gt;&amp;2`. - It would be lovely if cmdarg_describe could be configured to use a custom formatter (like [librelib's `flag`](https://projects.parabolagnulinux.org/packages/libretools.git/tree/src/lib/libremessages#n85) ... and as I linked it, I noticed some improvements that could be made to `flag`). ---- I believe that `cmdarg_set_opt`'s `$CMDARG_TYPE_HASH` section would better be written as: $CMDARG_TYPE_HASH) local arrname=${key} declare -gA -- "$arrname" local k=${arg%%=*} local v=${arg#*=} eval "$arrname[\$k]=\$v" ;; The `declare -gA` should get around the "bug" you linked to on StackOverflow. I'm not sure whether I'd consider it a bug or not; it will only trigger when the hash hasn't been declared as a hash; which isn't a reasonable thing for the programer to expect to work anyway. ---- I believe that `cmdarg_check_empty`'s array and hash code would be better as: $CMDARG_TYPE_ARRAY) local varname="${longopt}[@]" echo ${!varname} ;; Which is shorter, and gets rid of the call to `eval`. (Also, I would consider replacing the calls to `echo` with `printf '%s\n'`)
Well, you're going to want to utilize `find`, with it's awesome options, including `-type f` and `-type d`, `-name`, and `-exec` most likely. Hope that helps. Sounds like a fun project. I wonder. Will a subdirectory only contain a certain file type? Or could there be a subdir, say `/input/xyz/` that contains a mixed batch of .mp3, .avi, and .doc ?
Thanks for the reply! Nope, all the subdirs will contain all the same file types. Essentially the project is for auto-organizing. So like when I go to rip one of my CD's, it'll go into the /input directory (including the album art, etc). The script will check input for the subdir (/input/The Clash - London Calling) for .mp3 - if its found, move everything in /input/The Clash - London Calling/ to /music. Same thing if i throw a folder into /input of school docs/reciepts,etc in .doc, .pdf, etc format - check that /input/xyz/ for those extension and move it to say /backup. Got a good headstart on it, but I'll keep going and check out the find command. Thanks again! so far: #!/bin/bash LOCATION="/input/" for folder in "$LOCATION"/*; do
I made something that might help you #!/bin/bash filetypes=( mp3 doc ) location="/input" for subdir in ${location}/*; do for filetype in ${filetypes[@]}; do if find ${subdir}/*.${filetype} -quit 2&gt; /dev/null; then case "$filetype" in mp3) echo "mv ${subdir} /tmp/music/";; doc) echo "mv ${subdir} /tmp/docs/" ;; esac fi done done The commands to move the directories in the `case` statement are preceded with `echo` for testing. It will simply show what it *would* do. You would remove the `echo` to make it live. The find command has the `-quit` option because if you have 10,000 mp3s in one of the subdirectories, it only needs to know there is at least one (according to you), to know that this is an mp3 directory. So it stops when it finds one, and returns a successful exit status. Let me know if you have any questions. Hope that helps EDIT: I feel like there should be a disclaimer here. You said, if you rip a CD (CD, really? what year is it? ;) that it will go into the `The Clash - London Calling` directory, along with possibly the album art. That means a subdir *could* have a mixed batch of filetypes, yes? The scriptlet I posted above is not intelligent enough to know that this is a music dir, *IF* the first file it finds is a `.png` or something related to album art. Do you kinda get it? By the way, great album, one of my favorites all time.
 #!/bin/bash shopt -s nullglob nocasematch shopt -u failglob dotglob for subdir in /input/*/; do unset dest for f in "$subdir"/*; do case "$f" in *.doc|*.docx) dest=/someplace/docs;; *.mp3|*.flac|*.wav) dest=/someplace/music;; esac if [[ "$dest" ]]; then mv "$subdir" "$dest" break fi done done globs are amazing.
&gt; This is really cool. It seems to be way better thought out than any of the other tools that purport to do this. I'm glad you like it! I was really fond of python's argparse at the time, and it probably shows in cmdarg's design. I'm always thrilled when something in bash proves to be reusable and popular; bash hackers are a notoriously difficult crowd to please and provide libs for :-) &gt; Scanning through the source, issues I see (unsolisited code review, hopefully constructive): That's super constructive, thanks for the great feedback! I have a whole host of new issues to go open &amp; work on.
I'll be damned, you're right. A quick testcase: function shunittest_test_equals_nullstring { cmdarg_purge cmdarg 'x:' 'example ''just an example' cmdarg_parse -x '' [[ "${cmdarg_cfg['example']}" == "" ]] || return 1 } # shunit.sh --format tunit -t tests/null.sh [tests/test_null.sh] shunittest_test_equals_nullstring .... [FAILED] Exit 1 : Missing arguments : -x Missing arguments : -x shunit.sh : Required Arguments: -x,--example just an example v : String. ==== 1 TESTS in 0 SECONDS : 1 ERRORS, 1 FAILURES ==== To the bugtracker!
Actually, this may not be the case, the testcase above was bogus (-x is required, and empty string is considered absent). I'll need to do some more testing, because it seems like that bug should be legitimate. Changing the testcase around so it is optional (not required), and has a default value, but expects an empty value, seems to work: function shunittest_test_equals_nullstring { cmdarg_purge cmdarg 'x?' 'example ''just an example' 'default' cmdarg_parse -x '' [[ "${cmdarg_cfg['example']}" == "" ]] || return 1 } # shunit.sh --format tunit -t tests/null.sh [tests/test_null.sh] shunittest_test_equals_nullstring .... [OK] ==== 1 TESTS in 0 SECONDS : 0 ERRORS, 0 FAILURES ==== 
Thanks all for the help!!! It's working great now!
I wasn't aware of bats, and [wrote something very very similar](https://github.com/akesterson/shunit) What I don't understand is why they decided to essentially make a DSL on top of bash to describe unit tests. That seems entirely unnecessary, and seems like it would hamstring you as much as it would help. **edit**: its grammar and style reminds me of ruby's rspec, which may be another reason I am less than inclined to love it.
Your shunit seems very similar to [Roundup](http://bmizerany.github.io/roundup/).
It does indeed; thanks for the link &lt;gripe&gt; I do wish that bash tools in general were less capricious about namespace pollution; in the example of Roundup, if all of my testcases functions are named "it_", then it's quite possible that there might be other libraries I'm working with that might be in the "it_" namespace. This is why shunit puts your tests in the "shunittest_" namespace; it's awefully unlikely that your tests will collide with anything there.&lt;/gripe&gt;
This is a good solution. If you want more details: [Use exec to direct all BASH script output to a file, syslog, or other command](http://www.xensoft.com/content/use-exec-direct-all-bash-script-output-file-syslog-or-other-command)
That's a short install script, in my experience. You could do something as simple as `(./install-script.sh) &amp;&gt;somelogfile` edit: A very lame and lazy install script at that: sudo chmod -R 777 /etc/vspi sudo chmod 777 /etc/vspi/version
not going to pretend i understand what's going on in the script and no idea if that condition works for you, just saying that param counter $# is more reliable that testing for emptiness :-)
if you're talking about the "argument list too long" error it doesn't happen with globs and shell keywords afaik.
This is a very useful tip (although it's called [process substitution](http://www.gnu.org/software/bash/manual/bash.html#Process-Substitution), not redirection).
That looks good, yes. However, your install script looks horrible. `chmod 777` is always a bad idea. Doubly so when you include `-R`. You're also using some bad practices which I'm guessing you've learnt from the Advanced Bash-Scripting Guide, but it pales in comparison to your complete disregard for security.
Avoid using echo with options. Use printf instead.
I didn't code this script, it was passed down from another developer. I'm aware that 777 is typically poor practice, but there are a few other issues that are more pressing at the moment. What bad practices are you referring to? I'm trying to compile a list of improvements, and your insight would be appreciated.
You already have some answers, but something you can try if you're worried about it is to use the 'script' command and just record your session. I had a programming class in college where turning in a script session of compiling was part of our programming assignments.
well like I said the shell doesn't mind, the statement `for f in "$subdir"/*` can handle large lists and the loop will `break` after the first match.
You don't say why, which makes your opinion just an opinion. I'm assuming you're saying this to avoid portability and shell compatibility issues. Or maybe you just really hate the word 'echo'.
i agree python would be more like it but here cmd=( 'snmpbulkwalk' '-On' '-v2c' '-c' 'public' '192.168.1.1' '.1.3.6.1.2.1.47.1.1.1.1' ) awk -F'[ \t=:".]+' '/STRING/ { id[$4]=""; (NR==FNR)?(a[$4]=$6):(b[$4]=$6); } END { for(i in id) if(a[i] &amp;&amp; b[i]) printf "%s:%s,", a[i], b[i]; }' &lt;( "${cmd[@]}.13" ) &lt;( "${cmd[@]}.11" ) I get `J4859C:0L051,J4859C:0L050,J4859C:0L052,J8726A:34AS1SK,J8698A:SG38DXV43C,` when i plug in echo "$str1" / echo "$str2" with given contents into &lt;(...)'s. there is also `join` command that can be used to tie records together according to some key field.
Yes. Both [POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/echo.html#tag_20_37_16) and [Chet Ramey](http://lists.gnu.org/archive/html/bug-bash/2013-04/msg00008.html) (the maintainer of bash) recommend printf. 
If you use *snmptable* instead it will reduce the problem to one of simply filtering/reformatting output: snmptable -v2c -c public -CH -Cf , 192.168.1.1 ENTITY-MIB::entPhysicalTable | awk -F, -v OFS=: -v ORS=, '($10 &amp;&amp; $12){print $12,$10}' | sed 's/\s\+,/,/g;s/,$/\n/' Note that this requires the *ENTITY-MIB* file to be installed on your system. Assuming you're using Linux, most distros have a package that will download and install the common set of MIBs for you. (e.g., *snmp-mibs-downloader* in Debian and its derivatives)
Check out Google's BASH style guide I am trying to implement these styles myself. https://google-styleguide.googlecode.com/svn/trunk/shell.xml
Or just simply : which cat
no, `which` is an external program and can’t know about shell builtins. $ which echo /bin/echo $ type echo echo is a shell builtin
 if ! grep -q "${myip}" "${storefile}" You'll probably want to either add \^$ to the beginning/end of the grep pattern respectively, or change it to a string equality test (below) - otherwise 4.3.2.1 will match 4.3.2.10, 44.3.2.100, etc. if [[ "$(&lt;$storefile)" != "$myip" ]]
touché
&gt; cat "filename" | while read line; do echo $line; done # Good &gt; while read line; do echo $line; done &lt; "filename" # Also good shouldn't the labels be `# meh` and `# kosher`, especially given the examples? :-) 1. Call to external command (bonus UUoCA for free ;-) ) vs native 2. Inability to communicate with the "parallel universe" of the outer scope 
I've tried -b, treating everything as hex, and many permutations and combinations of other flags too!
Awesome thank you! 
Man I do love Cloud-to-Butt: http://i.imgur.com/aj5afBm.png On a more serious note, this is actually pretty neat. I'll probably use this.
`read` will still split by lines because that's what the `-d` is for, not `$IFS` but you're right on the trimming however that only happens if you use a var for the result, the default `$REPLY` doesn't trim regardless.
There was a thread about that printf behavior at the mailing list last month: http://lists.gnu.org/archive/html/bug-bash/2014-04/msg00017.html TL;DR: POSIX requires printf to count bytes rather than characters.
I don't get it. Can you give some background, or explain why I would want to use a lambda in any language, or what a lambda is?
lambdas are anonymous functions, they're typically used in callbacks (i.e. functions that take another function as a parameter to call back later) but I don't see any practical use in bash, in any case it's probably just an experiment :)
A lambda is a closed over function, lambdas are useful in metaprogramming when you want a function to execute an arbitrary function as part of it's processing. Say you have a function that takes a file name as an argument and then echos the lines of that file in order. Well you can generalize that function to take a file name and a lambda and then the item will execute the lambda on each line. Other common uses include callbacks after long chains of processing.
you're pretty confused about the usage of braces and quoting and it seems you've missed the definition of the `otl()` function (the original not the one created with `eval`). 
&gt; you won't ever get a proper implementation, this doesn't even work. What part of it and on what bash version? 
I'm not helping you keep your delusion, you're too proud of it and you'll keep arguing regardless of what I say so I'll leave you a few examples on the other comment and send you on your way.
few things: * the braces are unnecessary on all plain parameter expansion, you're missing quotes in many of them as well that means they don't "work perfectly" it means you haven't tested enough. * if you use `eval` in anything else that your own toys you're doing it wrong and I know there's no other way thus the reason bash is meant for scripting and not programming, there's no support for advanced constructs and trying to build one is a fool's errand * the whole code makes very little sense, you call a function that isn't declared, there's four other functions that aren't used * i mentioned the quotes regarding variables but you're also forgetting that you need to preserve them during the transformations you make, and you do not. and by the way, the WHOLE point of the lambda is to get a reference to the function to be called later, you execute it right away so in the end it's a broken and very contrived normal function.
You're not understanding this very well. it's 10 minute shitscript I came up with with a quick proof of concept. It's not fully tested and production ready. Boo fucking hoo. Let me sum up your arguments: &gt;the braces are unnecessary on all plain parameter expansion, you're missing quotes in many of them as well that means they don't "work perfectly" it means you haven't tested enough. You're saying: Your coding style is not mine. You're a faggot. My response: Sorry I don't write bash like you. &gt;if you use eval in anything else that your own toys you're doing it wrong and I know there's no other way thus the reason bash is meant for scripting and not programming, there's no support for advanced constructs and trying to build one is a fool's errand You're saying: scripting != programming, because I say so. I don't like eval, even though eval was made for scripting language metaprogramming. My response: People have opinions, apparently there's a difference between scripting and programming, beyond the fact that scripting is programming on an interpreter. So javascript can't have lambdas either cause it's a scripting language. &gt;the whole code makes very little sense, you call a function that isn't declared, there's four other functions that aren't used My response: Yes. This is the whole program. The entire thing. I publish everything on pastebin, and share it with only 2 subreddits and personal friends. Done. &gt;i mentioned the quotes regarding variables but you're also forgetting that you need to preserve them during the transformations you make, and you do not. My response: hooray for code i wrote in 10 minutes at lunch. The most perfectest bug free code ever that I share with you because I want to piss you off. &gt;and by the way, the WHOLE point of the lambda is to get a reference to the function to be called later, you execute it right away so in the end it's a broken and very contrived normal function. Let's go back to the fact that bash functions aren't first class objects. However you can emulate this by passing anonymous functions around as arrays and using lambda as a keyword for execution of said arrays. tl;dr your main argument is "I don't like it, so it's wrong" neck-bearding intensifies. 
&gt; tl;dr your main argument is "I don't like it, so it's wrong" neck-bearding intensifies. no, my main argument is that bash isn't intended for this purpose and does not have the proper tools to accomplish this task, I actually like the idea but I *know* there's no sane way to do it. I'll need a coffee to reply to the rest, gimme a sec.
&gt;bash isn't intended for this purpose So is Brian Fox a pretty cool dude, you must know obviously having his number and everything. You guys chat often? Bash might not be the easiest language to express lambdas in but it is possible. However since you don't know my actual task and I just put up a code fragment I thought was cute, you can't make the "improper tool" argument with good faith.
ok you're accusing me of the high crime of neckbearding!! but are you making an effort to understand my points? I'll try to be more specific: &gt; &gt; the braces are unnecessary on all plain parameter expansion, you're missing quotes in many of them as well that means they don't "work perfectly" it means you haven't tested enough. &gt; &gt; You're saying: Your coding style is not mine. You're a faggot. this isn't coding style, there's an actual difference between `$var` and `"$var"` and `${var}` and `"${var}"` there's two factors here, the braces and the quotes; the braces have two purposes: to separate the variable from the surrounding text (e.g. `${var}iable`) and to perform expansions such as `${!1}` that you used before, all this is independent of the quotes. the quotes are necessary (braces or not) to prevent word-splitting from the shell, that is the case when your variable contents include whitespace and the shell takes it as several elements instead of just one. here's an example: $ v='a b c' $ for e in $v; do echo "$e"; done a b c $ for e in "$v"; do echo "$e"; done a b c that happens with or without braces. so there's places in your script where you're using braces instead of quotes, that is not style that is an error and will fail when the variables have certain values, the only places when unquoted expansions are allowed is when the shell *knows* there's only one argument and thus skips the word-splitting (for instance when assigning other vars) but even in those cases you could still quote and it'll be ok, you can use unnecessary braces if you like too but if you omit the quotes it's wrong. advanced usage can take advantage of this but it's not the case here. &gt; &gt; if you use eval in anything else that your own toys you're doing it wrong and I know there's no other way thus the reason bash is meant for scripting and not programming, there's no support for advanced constructs and trying to build one is a fool's errand &gt; &gt; You're saying: scripting != programming, because I say so. I don't like eval, even though eval was made for scripting language metaprogramming. the difference is not because I say so, it's what every sane person will tell you when you try to do complex things here and there's a reason you're ignoring which is that you DON'T HAVE proper language mechanisms to do so, even the fact that you're defending eval means you're very inexperienced; eval is discouraged universally because the lack of structure is a tremendous security risk and also incredibly hard to manage properly, it is *not* something you'll use ever in anything that pretends to be serious or reusable. and notice that [I do use it in my own projects](https://github.com/git2samus/rcfiles/blob/master/.functionrc) but these are for myself, I know what is there and how to use them. &gt; &gt; the whole code makes very little sense, you call a function that isn't declared, there's four other functions that aren't used &gt; My response: Yes. This is the whole program. The entire thing. I publish everything on pastebin, and share it with only 2 subreddits and personal friends. Done. well if that's the case then don't expect me to understand it. you're posting on a public forum meant for discussion, this is not your email. &gt; &gt; i mentioned the quotes regarding variables but you're also forgetting that you need to preserve them during the transformations you make, and you do not. &gt; &gt; My response: hooray for code i wrote in 10 minutes at lunch. The most perfectest bug free code ever that I share with you because I want to piss you off. so if you agree that it's a bug meant to be solved then fine, I don't expect it to be perfect but why would you take it badly that I mentioned so? it is in your best interest to improve it. &gt; &gt; and by the way, the WHOLE point of the lambda is to get a reference to the function to be called later, you execute it right away so in the end it's a broken and very contrived normal function. &gt; &gt; Let's go back to the fact that bash functions aren't first class objects. However you can emulate this by passing anonymous functions around as arrays and using lambda as a keyword for execution of said arrays. no you can't, what you're doing is a horrible hack and I'm ok with it being a "10 minute shitscript with a quick proof of concept" but when you start mentioning things like "production ready" and things like "I have a test for that. It captures it perfectly well" I start to worry and wonder, is this guy fucking seriously trying to do this besides being evidently obvious that is not right? you're free to pursue your dreams but don't be so stubburn, the shell is not the place to do advanced programming. 
&gt; this isn't coding style, there's an actual difference between $var and "$var" and ${var} and "${var}" &gt; there's two factors here, the braces and the quotes; the braces have two purposes: to separate the variable from the surrounding text (e.g. ${var}iable) and to perform expansions such as ${!1} that you used before, all this is independent of the quotes. &gt; the quotes are necessary (braces or not) to prevent word-splitting from the shell, that is the case when your variable contents include whitespace and the shell takes it as several elements instead of just one. &gt; here's an example: &gt; $ v='a b c' &gt; $ for e in $v; do echo "$e"; done &gt; a &gt; b &gt; c &gt; $ for e in "$v"; do echo "$e"; done &gt; a b c &gt; that happens with or without braces. &gt; so there's places in your script where you're using braces instead of quotes, that is not style that is an error and will fail when the variables have certain values, the only places when unquoted expansions are allowed is when the shell knows there's only one argument and thus skips the word-splitting (for instance when assigning other vars) but even in those cases you could still quote and it'll be ok, you can use unnecessary braces if you like too but if you omit the quotes it's wrong. &gt; advanced usage can take advantage of this but it's not the case here. &gt; 10 minute scripts, minor error checking. Not production level, this isn't finished. I'm just sharing it. How many times do I have to say this. Sorry I didn't pine over every single improper quotation. Yes as it stands doing lambda 3 test "space ing" fails 'echo $1 $2 $3;' Fails. But it's a fucking shitscript one off as proof of concept. &gt;the difference is not because I say so, it's what every sane person will tell you when you try to do complex things here and there's a reason you're ignoring which is that you DON'T HAVE proper language mechanisms to do so, even the fact that you're defending eval means you're very inexperienced; eval is discouraged universally because the lack of structure is a tremendous security risk and also incredibly hard to manage properly, it is not something you'll use ever in anything that pretends to be serious or reusable. and notice that I do use it in my own projects[1] but these are for myself, I know what is there and how to use them. You do realize that there are a shit ton of things that are discouraged universally because they are "hacky" from a SE perspective but are used in code. For example fast inverse square root from Quake. So please write a letter to John Carmack too because he's not using pure numerical representations and relying on machine translation errors to do his dirty work. It exists, sometimes it's useful to use it, yes it's not as maintainable, but this paradigm won't work without it. Not only that but if you're talking about call safety and bash you're surely joking. &gt;well if that's the case then don't expect me to understand it. you're posting on a public forum meant for discussion, this is not your email. How hard is it to undersatnd can't pass functions as first class objects, use array variable names instead, then unroll. &gt;so if you agree that it's a bug meant to be solved then fine, I don't expect it to be perfect but why would you take it badly that I mentioned so? it is in your best interest to improve it. Because you're being pedantic and pretending it's something I'm representing as production ready. I was perfectly fine with your comments about missing quotes until you got all high and mighty. &gt;no you can't, what you're doing is a horrible hack and I'm ok with it being a "10 minute shitscript with a quick proof of concept" but when you start mentioning things like "production ready" and things like "I have a test for that. It captures it perfectly well" I start to worry and wonder, is this guy fucking seriously trying to do this besides being evidently obvious that is not right? you're free to pursue your dreams but don't be so stubburn, the shell is not the place to do advanced programming. Again. I'm sorry you don't agree with the methodology. You're only calling it a hack because it lacks official syntactic sugar. Running any kind of anonymous function passing code in reality has the same risks as a subshell eval. You're just foaming at the mouth because someone else besides you is using eval.
&gt; if you're talking about call safety and bash you're surely joking I am not, in bash you can write solid code by avoiding pitfalls and realizing the kind of tasks that can and cannot be done. &gt; How hard is it to undersatnd can't pass functions as first class objects, use array variable names instead, then unroll. you can't have lambdas, you can't have advanced programming, your "idea" is bullshit, stop patronizing things you don't fully understand. &gt; Running any kind of anonymous function passing code in reality has the same risks as a subshell eval you've no idea what you're talking about.
Are you planning to maintain some sort of monolithic bash library to 'extend' functionality?
Essentially, you'll need to get wget to save the cookie from the login page, then send that cookie again with any HTTP request that requires logging in. That way, their server will know that it's the same session. There's an example here: http://stackoverflow.com/questions/1324421/how-to-get-past-the-login-page-with-wget I don't have time to get in to much depth, but I can come back later if that's not clear. It will help to use a dev console (like Firebug in Firefox or Chrome's Developer Tools) to check the headers your browser sends on the Ravelry login page, then write a wget line to send those same headers.
I like to use this function to compile my java projects function java-compile() { if [ ! -d ./bin ]; then mkdir ./bin javac -d ./bin $@ else rm -Rf ./bin/* javac -d ./bin $@ fi } To keep things clear intead to mix .class and .java files. 
Thanks!
Why `[[` instead of `[`? Why `$(…)` instead of backtick? `nfile` must a realpath to avoid bug with qlmanage.
Shortest way to replace all spaces in filenames with underscores in current directory. I like "for i in *; do mv $i ${i// /_};done"
the yes/no dialogue pops up because you don't have the public key of the server yet. ssh gives you the opportunity to check the host key's fingerprint to make sure the target host is really the server you're expecting. Gather the host keys of all target servers, put them in the known_hosts file (you can find it in your ${HOME}/.ssh/known_hosts) and your script will run just fine. Of course you can write an expect script. Expect is kind of a shell that runs commands and evaluates the output of those commands and sends strings, or runs other commands. Googled [this example](http://www.journaldev.com/1405/expect-script-example-for-ssh-and-su-login-and-running-commands): #!/usr/bin/expect #Usage sshsudologin.expect &lt;host&gt; &lt;ssh user&gt; &lt;ssh password&gt; &lt;su user&gt; &lt;su password&gt; set timeout 60 spawn ssh [lindex $argv 1]@[lindex $argv 0] expect "yes/no" { send "yes\r" expect "*?assword" { send "[lindex $argv 2]\r" } } "*?assword" { send "[lindex $argv 2]\r" } expect "# " { send "su - [lindex $argv 3]\r" } expect ": " { send "[lindex $argv 4]\r" } expect "# " { send "ls -ltr\r" } interact You should be able you adapt it to your needs.
Yeah, no reason to reinvent the wheel when you can just disable key checks
I created a function I use in my scripts to add the fingerprint to the known_hosts file for this purpose: # check_ssh_fingerprint # automatically checks and adds ssh fingerprint to known_hosts # if missing or old. check_ssh_fingerprint() { remote_host=$1 known_hosts="~/.ssh/known_hosts" known_hosts_tmp="$TMP/known_hosts" local_fingerprint="$( grep $remote_host ~/.ssh/known_hosts | awk '{print $3}' )" scanned_fingerprint="$( ssh-keyscan $remote_host 2&gt;/dev/null | awk '{print $3}' )" if [ ! "$local_fingerprint" = "$scanned_fingerprint" ]; then if [ -f "~/.ssh/known_hosts" ]; then mv ~/.ssh/known_hosts $TMP/known_hosts sed "/$remote_host/d" $TMP/known_hosts &gt; ~/.ssh/known_hosts fi ssh-keyscan $remote_host 2&gt;/dev/null &gt;&gt; ~/.ssh/known_hosts fi } You would call the function before running an SSH command, like so: check_ssh_fingerprint $host ssh $host "some command here" &amp;&gt;/dev/null 
Please, please, don't do it. Validating the host key is essential; nearly all security and privacy goes out of the window if you skip this check. What you can do is have a known good key before you run the ssh command. For instance, copy your .ssh/known_hosts file to the client first, or even better, put it in the global /etc/ssh/ssh_known_hosts file so every user can profit from them.
True to your name. Thanks!
I'd never do this if security or privacy was an issue, but I'm not concerned about security or privacy since these are all internal to our network.
Can also be done in .ssh/config StrictHostKeyChecking no By the way, this will still throw a huge warning up if the key doesn't match what you have in your known_hosts file.
If you don't care about the security or integrity of your internal network, where do you care about it? That's like saying you don't lock your doors because you trust the people you live with. 
I wrote a small script that copies public keys into ssh servers that authorize password logins. It's worked pretty well and doesn't hang for interactive promps. https://gist.github.com/jjangsangy/11303683
AFAICT, it's the default behavior of readline: readline.c: /* Each line starts in insert mode (the default). */ _rl_set_insert_mode (RL_IM_DEFAULT, 1); 
I know it's not exactly what you meant, but here's how I go about it: ctrl+z
Might as well use telnet.
With -o vi, get into the habit of not starting your edits with the arrow keys. Start editing by typing ESC, then h,j,k,l to move around. You won't get into insert mode that way. Using hjkl takes a bit of getting used to but is worth it. It's faster in the long run because you won't have to re-position your right hand from the home position.
&gt; repeat a given command while it fails, which means it runs the command and if the return code is not zero it waits a given interval and runs it again `watch --interval &lt;interval in seconds&gt; --errexit &lt;command&gt;` You can also exit when the output changes (e. g. from `unknown host` to `ping ...`) and highlight changes in the output. &gt; for ((n=delay; n&gt;0; n--)); do sleep 1 || return done `sleep $n`?
Ah didn't knew watch could do that nice thanks. &gt; sleep $n? yeah I think I had a countdown or something but later removed it... partially :p
Ah, so that’s why. (Might be fun if you reset the cursor with `tput` so that the countdown always stays in the same line.)
In your $HOME/.ssh/config file: StrictHostKeyChecking no
I'm trying this, it kinda works but it waits until the command finishes to print the output. I think `watch` is meant to execute every n seconds but isn't designed for commands that might take longer than that to finish, it's a valid alternative anyway, thanks.
well for wget yes because it gives you that option but there's plenty of others that don't
What else would you use this with? Typically if something fails on the first run, I wouldn't want to keep running it.
Yes, for `wget`, which as we just established has a builtin function for this. What are some other cases where you might use it?
well, like I said mostly stuff that performs downloads such as `curl`, `wget`, `apt` or even a script called `get_flash_videos` I've found on the packages. I even used it to run `ifup`/`ifdown` to reconnect when my router was having problems, I also use it for commandline VPNs and/or ssh tunnels that drop regularly.
script has huge amount of problems and its not running. im posting it here to see if anyone can fix it so then i can study it and see how to do it right.
let's see, first of all when posting here on reddit add four spaces before each line so it looks like this: #!/bin/bash echo Hello! whats your name? read n sleep 1 echo "well $n today we will be doing some math" sleep 1 echo "would you like to do math? y/n" read a if [ $a = n ] (anyway to make it also accept "no" as an awnser? also the issue here is that even if you type anything at all it will quit the program. anyway to make it so it can only accept "n" and "no" and if you type something else it replies "invalid response" ) then echo "ok fine, stay stupid" exit else if [ $a = y ] then echo "ok lets start" sleep 1 echo "what is 2 x 2" read a if [ $a = 4 ] (anyway to make this section say "WRONG" if anyting else is answered other than "4" ) then echo "CORRECT" fi now, please post *what* is not working, what is the error and/or symptoms, I can see several things that are wrong but let's work on the problems you express first. 
well thats the thing. im really not sure whats wrong. the stuff in () is just what i would like to do but i dont know how.
you realize you haven't posted the actual code but a badly commented version of it AND you still haven't said what the problem was. you initially said "script has huge amount of problems and its not running" so what's the error? and what's the goal that is not being accomplished? I'm sorry for being picky about this but you'll have a hard time getting answers on the internet if you don't ask properly. as for the parenthized comments: 1. you can use `||` or check the `case` command 2. `if` can have an `else` clause for when the test is false
redivert cuprous theromorphous delirament porosimeter greensickness depression unangelical summoningly decalvant sexagesimals blotchy runny unaxled potence Hydrocleis restoratively renovate sprackish loxoclase supersuspicious procreator heortologion ektenes affrontingness uninterpreted absorbition catalecticant seafolk intransmissible groomling sporangioid cuttable pinacocytal erubescite lovable preliminary nonorthodox cathexion brachioradialis undergown tonsorial destructive testable Protohymenoptera smithery intercale turmeric Idoism goschen Triphora nonanaphthene unsafely unseemliness rationably unamendment Anglification unrigged musicless jingler gharry cardiform misdescribe agathism springhalt protrudable hydrocyanic orthodomatic baboodom glycolytically wenchless agitatrix seismology resparkle palatoalveolar Sycon popely Arbacia entropionize cuticularize
cd edit: that is, to change directory to /Users/apple/Library/Movies you would type "cd /Users/apple/Library/Movies"
yup that's it. thank you
Hi! #!/bin/bash echo Hello! whats your name? read name echo echo "well $name today we will be doing some math" echo "would you like to do math? y/n" read answer if [[ $answer = n || $answer = [nN][oO] ]] # either 'n' 'no' without case sensitivity then echo "ok fine, stay stupid" exit elif [[ $answer = y || $answer = [yY][eE][sS] ]] # either 'y' or 'yes' without case sensitivity then echo "ok lets start" sleep 1 echo "what is 2 x 2" read answer2 if [ $answer2 = 4 ] then echo "CORRECT" else echo "WRONG" # now says "WRONG" if the incorrect answer is given fi else # neither a negative or positive response was accepted echo "I don't understand your answer" exit fi 
Type `man intro` for a nice introduction to basic user commands. That covers your question and more.
 #!/bin/bash read -p "Hello! What's your name? " name sleep 1 echo "Well ${name}, today we will be doing some math." sleep 1 ask_about_math () { read -p "Would you like to do math? " a case "$a" in n|N|no|NO) echo "OK fine. Stay stupid."; exit ;; y|Y|yes|YES) echo "OK, let's start."; sleep 1;; *) ask_about_math;; esac } do_math () { read -p "What is 2x2 : " ans if [[ $ans == 4 ]]; then echo "CORRECT" else echo "WRONG" sleep 1 do_math fi } main () { ask_about_math do_math } main ---- This is how I'd do it. I like to use "case" when accepting an answer that can be in many different forms (y, yes, YES, etc...). When using "case", the '*)' case is a catch all that gets run if the case doesn't match a defined answer (like 'no' or 'yes'). Also, notice that you can combine echo "Asking a question..." read answer into... read -p "Asking a question..." answer 
thanks for the tip on using "case" 
One thing with this is that if any answer ends with the letter "y" or letters "yes", it'll match. So, "lksdflksjdflsdfy" is a valid "yes" response. I like case because it looks for the entire string, not just a letter. Although, if you made your regexp string to be, **\^(y|yes)$** that would solve that issue too.
Do you mean the read -s part? I'm not sure what you mean by "the middle part" and what exactly isn't happening.
Here's the one I use, usually for sshing to an instance I just spun up in EC2 so it will retry until sshd is up on the new instance. Usage: DELAY=10 MAXRETRIES=5 retry ssh blah@IPADDRESS #!/bin/bash # # Run a command over and over until it exits 0. Wait $DELAY seconds between # attempts. if [ -z $DELAY ]; then DELAY=5 fi if [ "$1" == "--help" ] ; then echo "$0 command" echo "or" echo "DELAY=30 $0 command" echo echo "Keep trying command until it succeeds. Wait DELAY seconds between attempts" echo exit 1 fi attempts=0 while true do (( attempts = $attempts +1 )) $* if [[ $? == 0 ]]; then exit 0 else echo "'$*' attempt $attempts failed; waiting to retry..." if [ ! -z $MAXRETRIES ]; then if (( $attempts &gt;= $MAXRETRIES )); then echo "max retries was $MAXRETRIES, $attempts failed attempts, exiting..." exit 1 fi fi echo sleep ${DELAY} fi done 
if the stdin is fed from file, you need to force relevant read commands to read from tty, otherwise they will default to slurping stuff from the file too. $ { read a; read b; read -p "prompt: " c &lt; /dev/tty; echo $a/$b/$c; } &lt; &lt;( echo 1111; echo 2222; ) prompt: typed manually 1111/2222/typed manually 
for ssh you should try [mosh](http://mosh.mit.edu/), it's amazing :D
&gt;It just prints this on the same line: &gt;Enter your Active Directory Account Name: Enter Your Active Directory Username... #Ask for Active Directory Account Name echo -n -p "Enter your Active Directory Account Name: " read ADAccount #Ask For Active Directory Account Password echo "Enter Your Active Directory Username..." read -s ADAccountPassword Here's a couple of blocks from an old script of mine that you can use for reference. Here's one where I prompt for the username, but default to whoami: WHOAMI=$(whoami) #Much earlier in the script # Start by prompting for the username to process, if none is given, default to the script runner echo -n "Please enter the username to update and press [Enter] (default: ${WHOAMI}): " read userin # Set the USER variable based on the interaction if [ "${userin}" == "" ]; then USER="${WHOAMI}" else USER="${userin}" fi soon after I prompt twice for the password and compare, that way it's not fat-fingered. It should also solve your same line problem (use the \n on the second echo): # Prompt for the new password twice while [ "${MATCH}" -gt "0" ]; do echo "================================" echo -ne "Please enter the new password for ${USER} and press [Enter]: " read -s pwdin1 echo -ne "\nPlease confirm the new password and press [Enter]: " read -s pwdin2 # Compare the two, if they don't match, try again if [ "${pwdin1}" != "${pwdin2}" ]; then echo "" read -p "The passwords entered do not match. Press [Enter] to start again." MATCH=$(( MATCH + 1 )) else # If the passwords match, write it to .newpwd echo "${pwdin1}" &gt; "${USERPW}" # And give the condition to exit the while loop MATCH=0 fi done As I say, it's an old script of mine. These days I'd be trying to use printf instead of echo, and I wouldn't be using uppercase variable names. But essentially the example might give you some direction. 
Are those brackets really there? They're definitely wrong. I tried to reproduce your problem like this. I did `mkfifo pipe`, then `echo "hello" &gt;pipe`. Obviously, the echo process will hang, waiting for its input to be read off the pipe. Now I have the following script (piperead): #!/bin/bash read -t 1 x &lt;"$1" echo "$x" When I run `./piperead pipe` it prints hello, exactly as expected. So am I not reproducing your problem correctly? We probably need more information please. EDIT: also, it appears that the -t option has no effect unless you're reading from stdin or a pipeline. That is, if I run my piperead program with nothing coming off the pipe, the timeout doesn't appear to happen. So that could be another problem for you.
Well i dont really know how to explain it but ill try: read [ -t 1 ] pipetest &lt; $2 $2 is a pipe that was made with the command mkfifo i am trying to put what ever is in the pipe($2), even if its empty, into the variable pipetest so that i can use if [ -z pipetest ]; then echo "pipe empty" else pipereader fi pipe reader is a function that takes what is in the pipe, reads it and does a few different thing depending on whats in the pipe does this make sense english is not my first language ~~Edit: Wait i seem to have found a possible mistake... fixing now~~ Edit: false alarm didnt fix anything
yes [ -t 1 ] is literally in the script, it doesn't seem to work if put any other way, and yes $2 is the secondary argument of the script i run the script like this ./scriptname.sh instructions.conf pipeplinemade_by_mkfifo
How are you not getting the error below? -bash: read: `[': not a valid identifier `while read [ -t 1 ] pipetest &lt; $2` is not proper, syntactically. I'm not sure what you're trying to achieve, either. Can you explain what your end goal is, a little? There may very well be a better way to do this. 
I'm not seeing (in my env) the need for the brackets and by simply removing them it seems to work fine. $ testvar=mkfifipipe; while read -t 1 imavar &lt; $testvar; do echo $imavar; done test1 test2 -bash: zones3.txt: Interrupted system call I echoed the 'test1' and 'test2' in to the mkfifo file from another session and it shows up as expected. The interrupted system call was just me issuing Ctrl+C to end. As mentioned by a few people, it would be easier for us to help if we were provided some additional lines of the code. If you need to mask file names or directories please do so.
Maybe a better solution is to set a trap and read a particular file when a signal is caught?
yeah that could work. I'll try it tomorrow haven't slept for 22 hours cant reaally think straight thx
i think that the problem is that this part of the program should only execute if there is something in the pipe and continue if the pipe is empty i don't really know how to explain it if (there is something in the pipe); then put what is in the pipe into a variable else; ignore the whole thing fi edit: Sorry if im frustrating you, i really need to sleep so if you have any ideas just throw them all here and ill try all of them tommorow
Dummy check: are you sure you don't want to use "$1" ? I mean, don't know if you're giving enough code for us to look at. Also not sure how you're invoking your script on the command line.
yeah im sure, as i said this is what im trying to do *************** if (check if there is something in the pipe... if there is); then put what is in the pipe into a variable else; ignore the whole thing (dont wait for an input if the pipe is empty) fi
I played around with the program and the [] really were unnecessary but the problem still remains... i found a "almost" solution with a modified version of this: catecho () { if read -t 0; then cat $1 else echo "no input" fi } while [ 1 ] ; do catecho done but it still wants me to press enter in the main program which i have to avoid
you redefine the function over and over again inside the loop? why? btw `while :` is shorter for infinite loop
that is just a copy/paste mistake sry ill fix it
While I don't have a solution to your problem off the top of my head, I do want to bring your attention to a possible problem. Your script attempts to install curl via the package manager, and doesn't give the user the choice to abort before that point. Where I work, the machines are closely controlled for configuration, and I wouldn't expect a script installer to affect yum or apt databases, especially without a rollback option.
no, i haven't formed a strong opinion on your design and like i said, you seem to have it thought through and are already aware of pros and cons. Just saying that dismissing legit criticism and inquiry out of hand by default seems a bit excessive. back to the problem, that PATH upgrade in ubuntu happens in .profile not in .bashrc and .profile is supposed to be not shell specific. the code for it is equivalent to `[ -d $HOME/bin ] &amp;&amp; PATH=$HOME/bin:$PATH` That said in that configuration the update happens only on login, so creating bin and spawning a terminal is not enough for that thing to be working right off the bat. You'd either have to prompt user to relog. For more immediate results you'd need to modify .${shell}rc. Also existing .${shell}_profile most likely overrides the .profile All in all it's a mess. 
Pretty sure afplay can't read from stdin, from file only. Is saving to /tmp a good compromise? curl -s http://x.com/x.mp3 &gt; /tmp/x.mp3 &amp;&amp; afplay /tmp/x.mp3 Is this for a login script?
Maybe afplay &lt;(curl -s "$mp3file")
this yields `Error: AudioFileOpen failed ('dta?')`
I don't know about osx...but in GNU/Linux this works: curl -s http://static.sfdict.com/dictstatic/dictionary/audio/luna/B01/B0132100.mp3 | mpg123 -
Just tested this on OSX, works a charm. I did have to install mpg123 though but through brew its just a simple case of: brew install mpg123 If you don't have brew, [get it here] (http://brew.sh/)
I don't know what that means. It looks like Python.
thanks, I had to use `afplay` as it's shipped with every `osx`, saving to `/tmp` seems to be the only way.
Wait.... what's the point of this? Why not just: [[ $year == 1999 ]] || echo "Test failed"
Like it. But the "acoc mount" example, colorized and all, isn't as good as "mount | column -t". $ mount /dev/xvde1 on / type ext4 (rw) none on /proc type proc (rw) none on /sys type sysfs (rw) none on /dev/pts type devpts (rw,gid=5,mode=620) none on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0") /dev/xvdf1 on /data type ext4 (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw) $ $ mount | column -t /dev/xvde1 on / type ext4 (rw) none on /proc type proc (rw) none on /sys type sysfs (rw) none on /dev/pts type devpts (rw,gid=5,mode=620) none on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0") /dev/xvdf1 on /data type ext4 (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw) $
I put them back in, i had them original but it did not work.. it still comes back with these syntax errors. http://imgur.com/piLLWK9
Replace echo"..." with echo "..." (Note the space)
It seemed to work either way. But i will do that. BTW the script worked. now to tweak. 
Thanks :) will be using this script for my final homework as well. I need to add options to allow the "mail" option to allow the user to select the option to type the receiver, subject and to send. Any pointers on how to do that? As well as for the user to be able to type in domain name to get the IP address of it. Would I use \c for that?
Ohhh right. Makes sense! thanks! ill post what ive done afterwards if it works or doesnt work. I am pretty sure i got it figured out but ya never know. 
http://imgur.com/8aIZ6v0 is it gonna be something like this? im not exactly sure how to write syntax's for these commands. The day we were going to have the lesson on this, my teachers son got a bad cold and he canceled class. can you give me a example syntax and maybe a quick lesson? 
`man tput` `man tcap`
I had to post this to a dropbox-style site so the escape character wouldn't get stripped out like they do with pastebins. This is how I've been using colours in my bashrc (mostly for the prompt): https://app.box.com/s/vdtxhbg9bzstzgtcsvdg ${_red} is regular and ${_RED} is bold/bright. Note if you dump this in hex, there's a literal 0x1b (0o33) byte inside the variables rather than writing it out as some kind of escape sequence - I found this easier, KDE's "kate" editor makes copying the escape character around quite easy, giving it a literal [ESC] token on-screen. 00000000 72 65 61 64 6f 6e 6c 79 20 5f 72 65 64 3d 27 1b |readonly _red='.| 00000010 5b 30 30 3b 33 31 6d 27 0a 72 65 61 64 6f 6e 6c |[00;31m'.readonl| 00000020 79 20 5f 52 45 44 3d 27 1b 5b 30 31 3b 33 31 6d |y _RED='.[01;31m| 00000030 27 0a 72 65 61 64 6f 6e 6c 79 20 5f 67 72 65 65 |'.readonly _gree| 00000040 6e 3d 27 1b 5b 30 30 3b 33 32 6d 27 0a 72 65 61 |n='.[00;32m'.rea| I put the \[ \] where required in the PS1= rather than the variables, but I don't see why you couldn't move those. As an added bonus, in this form it doesn't need 'echo -e', I guess all the interpretation of the escape sequence is done at the point of the variable assignment rather than at printing. echo "${_red}foo${_GREEN}bar" # Works just fine without -e
https://github.com/goozbach/dot_files/blob/master/_profile.d/01-color-list.bash That's how I do it for my prompts. I don't remember where I got that from sadly.
 
 red=$(tput setaf 1) reset=$(tput sgr0) printf '%sSomething%s something\n' "$red" "$reset" See http://wiki.bash-hackers.org/scripting/terminalcodes
This has to do with how bash handles the PS1 prompt. You have to enclose the non-printable bytes in `\[` and `\]` so bash (or rather readline) won't think they contribute to the length of the prompt. See http://mywiki.wooledge.org/BashFAQ/053
 
Looks like it might be vim, in which case it just needs to be turned on. :syntax on If it's nano or some other editor-wanna-be, can't help.
Ya VIM, it's required because I have to email the teacher the whole assignment. 
Thank you!!!
 # regular colors local K="\[\033[0;30m\]" # black local R="\[\033[0;31m\]" # red local G="\[\033[0;32m\]" # green local Y="\[\033[0;33m\]" # yellow local B="\[\033[0;34m\]" # blue local M="\[\033[0;35m\]" # magenta local C="\[\033[0;36m\]" # cyan local W="\[\033[0;37m\]" # white # emphasized (bolded) colors local BK="\[\033[1;30m\]" local BR="\[\033[1;31m\]" local BG="\[\033[1;32m\]" local BY="\[\033[1;33m\]" local BB="\[\033[1;34m\]" local BM="\[\033[1;35m\]" local BC="\[\033[1;36m\]" local BW="\[\033[1;37m\]" # reset local RESET="\[\033[0;37m\]" PS1="\t $BY\$(__name_and_server)$Y\W$G\$(__rvm_prompt)$G\$(__git_prompt)$RESET$ " I use [this](https://github.com/henrik/dotfiles/blob/master/bash/prompt.sh) to colorize my prompt with git branch info. 
Sounds like nested for loops to me. The outer for loop will process each line; the inner loop will process each object in the array. That should be able to handle arrays with varying numbers of items in them even. 
maybe show some code showcasing the intent? I have trouble visualizing the problem from the provided description, because running function for each array elem is as simple as fun() { something something "$1"; } array=( word1 word2 word3 ) for elem in "${array[@]}"; do fun "$elem"; done and i think there has to be more to it.
I always use something like this to iterate through an array: array=( one two three ) for i in "${array[@]}" do echo $i done 
Yup. That's going on my buddy's laptop. 
The other script was large enough to where I just put it in a separate file entirely and put a call to it in the main for loop. I also managed to get it where it would read each line of the input file, and disregard spaces in the strings on each line (like in compound words, for example.)
That's pretty much exactly what I ended up doing, although the array was a text file with multiple lines. 
 curl -s http://static.sfdict.com/dictstatic/dictionary/audio/luna/B01/B0132100.mp3 | afplay /dev/stdin Does that work?
&gt; I wrote a short bash script that can't be killed &gt; I have no idea how to kill that thing success?
&gt; curl -s http://static.sfdict.com/dictstatic/dictionary/audio/luna/B01/B0132100.mp3 | afplay /dev/stdin I get the following: Error: AudioFileOpen failed ('dta?')
I actually tried to look for the PID of the process using top, but it just showed me the corresponding afplayer, not the actual script that called it. Is there a way to identify all running scripts, not the processes called by them?
well, you mentioned arrays yet there are none ;-) IFS=' ' #Start the main loop. for line in $(cat words.txt); do nope. While read loops are for that and they work on per-line basis by default so no global IFS='...' nonsense. You should never run for loops on command outputs (ok, almost never). As a rule of thumb the for loops are generally for stuff that can be enumerated in advance, while loops for stuff in unknown quantity, "run until you hit the end" while read -r line; do .... done &lt; file sh getdef.sh "$line" &gt;&gt;defsout.colored your script is declared to be bash compatible yet you run sh explicitly and it's not guaranteed to be bash. Allow the hashbang to do its job ./getdef.sh "$line" Btw you can redirect the output of the whole loop, you don't have to micromanage every single command while read ...; do ...; done &lt; words.txt &gt;&gt; defsout.colored 
$[] is deprecated, $(()) should be used instead.
Shut... shut down your computer?
I never knew this man page existed... I will need to remember this when people ask me simple questions. Thanks!
This is what I use in my scripts too;)
Wow, you really did mean basic .. ;)
ayo, thanks! These are mostly relatively beginner tips - I've never been bitten by `set -e` hurting (but now I'll probably avoid it in the future), and I totally didn't know about bash's printf being able to do strftime. Thanks a bunch for the tips!
Dang. Glad I keep reading this far. 
As the error indicates, you're not doing quite what you think with the square bracket tests, and are inadvertently passing too many arguments. Each argument is a space (well, more things than just space, but let's start there) separated token, so if we look at the line if [ $option = List Users ] the tokens you'll get are * if * [ * $option * = * List * User * ] when you want List User to be a single token. You can do that by putting quotes around them. So that token should be "List Users" instead of List Users Also note that there are different tests for numerical comparisons and string comparisons. You can read about them by doing man test
So, i created an introduction page with a tab on about creating and connecting a components, i believe showing a graph allows the users to understand the application faster, since expert programmers read code with speed :P
Do you mind adding `echo` to the available commands?
I dont mind, since that is an easy command to add, i also plan to add the ssh and parallel, but they would be better as a subgraph which expands and compresses
added the echo command, I also removed some of the unnecessary named pipes created for better readability of the commands Edit: typo
That... doesn't make any sense.
Imagine two programs prog1 and prog2. "prog1" is an ongoing process which takes input and produces output. Let's say you wanted to examine this output with prog2 and input back into prog1. process1 | process2 | process1 I have never seen anything which suggests that this is possible, but it seems like it should be.
Hi, you might be best to look at using a shell script for this. Depending on your use case, a while or for loop should do the trick. 
Are you trying to make a fork bomb? :(){ :|:&amp; };: *careful!*
Maybe something like: list) sed -n -e '/case \$1/,/esac/s/[[:space:]]*\([a-z-]*\)).*;;/\1/gp' $0 ;; Or you can define all your options in an array and print its content. 
it was deleted - what was it?
 $ help case case: case WORD in [PATTERN [| PATTERN]...) COMMANDS ;;]... esac Selectively execute COMMANDS based upon WORD matching PATTERN. The `|' is used to separate multiple patterns. It doesn't seem like there is such an option built into `case` itself. I think you'll just have to hardcode the possible cases into your echo command. Edit: Actually, one possible method is for you to make the script search itself (The path of the script itself would be the variable `$0`) for the cases. So you'd search for the text between `case` and `esac` (Using `sed` or `awk`), and then just ignore the last line and get the cases with a command like `cut -d ')' -f 1`.
always funny, but he asked for a solution without forking.
I read the man page for sed and awk, but I'm still not sure how to use them. Right now I just have a function with all of the echos, but I'd rather do it with your method so I don't have to update that function.
 bu_list() { sed -n "$1,$2 { /^[[:blank:]]*[^[:blank:]]*)/ { s/).*//; p; }; }" $3 } opt_firstline=$((LINENO+2)) case $1 in all) bu_all ;; dropbox) bu_dropbox ;; local) bu_local ;; school) bu_school ;; l-school) bu_school_linux ;; w-school) bu_school_win ;; bin) bu_bin_dropbox bu_bin_local ;; bitcoin) bu_bitcoin ;; software) bu_software ;; aliases) bu_aliases ;; firefox) bu_firefox ;; crontab) bu_crontab ;; list) bu_list $opt_firstline $LINENO $0 ;; *) echo 'Argument not recognized.' ;; esac This uses the $LINENO built-in variable (which always contains the current script line being executed) to determine what script lines to search. So the list) option needs to be the last one listed in the case statement in order for this to work correctly. 'sed' and 'awk' are programming languages unto themselves, so it's not surprising just reading the man pages will get you nowhere. For 'sed' you need to learn about regular expressions too, if you haven't already. My preferred tool (for no particular reason) is 'sed' so I've programmed it with that. The bu_list function takes three parameters: $1 = first line to search, $2 = last line to search and $3 = the path to the script to search. By default, 'sed' prints all of its output, modified by any editing commands. The -n option to sed causes it to print nothing by default, outputting anything only when instructed by the 'p' command. The sed program contains two nested blocks limited by addresses. It can more legibly be written as: $1,$2 { /^[[:blank:]]*[^[:blank:]]*)/ { s/).*// p } } The outer block has the address $1,$2, meaning "search only the lines $1 through $2". *edit: $1 and $2 is shell syntax, not sed syntax -- it is replaced by the actual line numbers by the shell because the sed program is in double quotes.* The inner block has a regular expression for an address, which matches only the lines starting with a series of blanks, followed by a series of non-blanks, followed by ')'. This is the working definition of an option in a 'case' statement. The two commands 's/).*//' and 'p' are only executed for the lines matching both addressed blocks. The first command eliminates the ')' plus everything following it, leaving only the option name plus any initial blanks. The second prints that edited output, resulting in a list of options. 
Thanks for the clear explanation, that was very educational :-)
That would indeed work.
be careful with loops. while(true) print("hi");
In this particular case you could use an associative array instead. Apart from the `bin)`, all of them run a single function, so consider just changing that one to run a single function too. declare -A commands='( [all]=bu_all [dropbox]=bu_dropbox [local]=bu_local #etc... )' if [[ $1 = list ]]; then printf 'Possible commands:\n' printf '%s\n' "${!commands[@]}" elif [[ -n $1 &amp;&amp; -n ${commands[$1]} ]]; then printf 'Running %s\n' "${commands[$1]}" "${commands[$1]}" else printf 'No such command...\n' fi
 mkfifo fifo &lt; fifo prog1 foo | prog2 bar | prog3 baz &gt; fifo I suspect this is what the deleted post had?
Good point. I thought of CLI but command interface doesn't make much sense. I bet it has something to do with or analogous to builds, given his examples.
For now, i'm i'm creating a temp file on the desktop which write out new code to run iperf3. I just need to have the script auto open a new tab and load the temp.sh file to execute the command.
This is kind of a confusing description. I can't quite understand which variables are set on which computer, and which processes are running where. Can you post the code, or at least a snippet?
The name needs to be looked up in the directory /etc/passwd and matched. If it does then good. Continue with the command, if not then It needs to echo the error message. As well as the file attachment needs to be a ASCII filetype and locatable in the directory. I tried and am still trying to fix the script but it is due today before 11:59 and I need to turn it in before then.. but sadly I cannot figure out why the hell it doesn't work. If someone can fix/help me without I will appreciate it a lot!.. I am already fried out.. 
Use a named pipe to accomplish the same thing but way cleaner.
1. split up your code into functions. Seriously! Every bit of working code that belongs together, that is more or less standalone, put it in a function so that you don't have to think about it anymore. 2. use '==' for comparison and '=' for assignment. Often you can get away with it in bash, but not in other languages. It will bite you in surprisingly subtle ways if you don't. 3. your 'case' statement must end with an 'esac' 4. 'if' statements must end with 'fi'. 4. mixing 'directory' for a lookup table like /etc/passwd and something where you can put files in, confuses everybody. Be consistent. 5. (not sure if it's googledocs or you) learn to indent correctly. This is unreadable. If you applied above tips to your script and it still doesn't work, come back and I'll point you in the right direction
In my opinion for this level of string manipulation it would be easier to use Python
You are probably better off using filebot (GUI and command line) as it is built for this very purpose. It scans the torrent name, looks up the proper name on the movie database and renames the file according to your specifications. It can also move the file to a different location for you depending on criteria you can set. IMO this is the best way to go but if you just want a simple script then I agree with u/nivwusquorum, python is your best bet. I'm mobile at the moment so don't have the link handy, search google for filebot and you will find it Edit: typo 
filebot with the amc.groovy script is a god send.
I feel like this would be too difficult to do with a single script, as not all torrents have the same naming scheme. For This season of Family Guy you could probably write a single line of sed and apply it to all the episodes. However, if you were to apply the same script to another torrent with a different naming scheme, you'd likely mess up all your filenames.
[This may be what you are looking for.](http://www.linuxforums.org/forum/programming-scripting/124151-bulk-rename-using-terminal.html) 
FWIW, you can do `chromium-browser --kiosk http://google.com/ --start-fullscreen` What you likely want to do is: while true; do chromium-browser --kiosk http://www.google.com/ --start-fullscreen done
Is there a pound sign before the ! on the previous line that just didn't get copy/pasted correctly? There should be, though I don't know if that would cause the error on the next line. Can you give any more detail on the error message?
you can skip that line entirely: fswebcam -r 1280x720 --no-banner $(date +"%Y-%m-%d_%H%M").jpg
Really, I'll try that. Although I want this to be a time lapse so, do you think the beginning stuff is pointless
I ended up getting a syntax error on the 1280
Try with ${DATE}.jpg That should fix your problem.
Ah, then I would first check if your bash is really bash and not something that's "like bash" but, say, doesn't support the $() subshell syntax. You can try putting backticks around it and see if that works, it's an older style syntax that might work.
Quotes are at the wrong place. fswebcam -r 1280x720 --no-banner "$(date +%Y-%m-%d_%H%M).jpg"
makes no difference in this case.
Sure, since `IFS` remains unchanged at that point, but why allow bash to attempt word splitting and pathname expansion at all when it's so easy to avoid.
What are these? I can't really parse them correctly without knowing the format. But, here's my solution. It correctly handles all of your test cases. # More complex, but possibly more maintainable version &lt;&lt;&lt;"$word" \ sed -r 's/[svd]{1,2}\*?/\n&amp;/g' | sed 's/^[svd*0]*//' | tr '\n' ':' | sed -e 's/^://' -e 's/:$//' The first line is basically `echo "$word|"`, but without creating another subshell (a handy trick). The second line splits the input into number/blank segments, separated by newlines. Note that it creates a newline before the first segment. `sed` will also add a trailing newline. The third line strips garbage from the beginning of each segment. The fourth line joins the lines with `:`, then removes the leading and the trailing `:`s, which correspond the the extra newlines discussed above. This is probably the more maintainable solution; the second line is (roughly) a lexer/tokenizer, and the third line is (roughly) a parser. That is how I was thinking when I wrote it. However, once completed and correct, I saw that the parsing step was simply removing symbols from the beginning of each token. We can just throw that into the lexer, and get a much simpler version: # Simple, dense, version sed -r -e 's/[svd]{1,2}\*?0*/:/g' -e 's/^://' &lt;&lt;&lt;"$word"
[**@Packt_Paush**](https://twitter.com/Packt_Paush): &gt;[2014-06-16 06:47:43 UTC](https://twitter.com/Packt_Paush/status/478428693455323136) &gt;New book "Penetration Testing with the Bash shell" written by [@k3170Makan](https://twitter.com/k3170Makan) ! Check it out here: [*packtpub.com*](https://www.packtpub.com/penetration-testing-with-the-bash-shell/book) [*pic.twitter.com*](http://pbs.twimg.com/media/BqO4XAtCUAA_cJG.jpg) [^[Imgur]](http://i.imgur.com/5QEIywZ.jpg) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/28d4w7%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
Thanks, I'll work from the top version and meld it to what I need. Will update when finished.
I think you're going to struggle to do the same thing using the same technique. You cannot arbitrarily run a new command whilst mid-command on a shell. And even if you forked it into the background, ran the update command and brought it back - I suspect the forked script would retain the old settings.
I think what you'll want to do is look at loadkeys. Something like sudo loadkeys dvorak This will (in theory) temporarily set your terminal keymap to dvorak, and I believe it will revert once the session is done. If you're in the same session and want to revert to qwerty, you'd just issue loadkeys again, e.g. sudo loadkeys us This means your only challenge is triggering those based off scroll lock. Note: I don't use dvorak, so I've never used loadkeys for it. It was the first thing to come to mind though.
If you're gonna use bash, use bash: use [[ instead of [; use $(...) instead of backticks. Also use bash's string manipulations to save you a bunch of processes: randstr=`md5sum $1 | cut -d' ' -f1` can be randstr="$(md5sum "$1")" randstr="${randstr%% *}" And there's no reason to subshell the stuff that's gonna be sent to sendmail; that is, instead of ( ... ) | sendmail ... you can just do { ... } | sendmail ... But these two things are nitpicks; otherwise looks good! But do fix [ and backticks.
I agree with all of this. In addition, parameter expansions and command substitutions should be quoted to avoid wordsplitting and pathname expansion, and the script's variables should not be uppercase.
I didn't go through and test or scrutinize the code, but I have a crazy idea for you. Add a deduplicator option. Do md5sums of the tracks, and if a duplicate is detected (different filename, but same md5 hash, so same song), remove the duplicate and create a symlink that points to the other one. That could of course be a dangerous option if not done correctly
Try "curl url | xargs afplay" Edit: this will pipe the output of curl to afplay after completion and afplay actually takes the input as a parameter - similar would be this: "afplay &lt; $(curl url)" Xargs, however, doesn't call a subshell, the other one does. Edit 2: the reason, that mpg123 works and afplay not is most likely that afplay doesn't support streaming but needs the complete file preloaded, while mpg123 is a program dedicated to streaming. Edit 3: sorry for the bad formatting, I'm on my phone and had to switch between two keyboards to write this :D
Good idea! I was gonna do something of the sort but I also wanna compare bitrates of songs so that if there is a duplicate in the naming scheme it keeps the song with the highest bitrate
i don't like the flc() function because the word splitting on variables rubs me the wrong way. var=${var,,}; echo "${var~}" or read -a arr &lt;&lt;&lt; "${var,,}" echo "${arr[@]^}"
I think hard links would be a better option. With symlinks you have to update the links if the source moves.
This is what I came up with: cal | fold -w 20 | tail -n +2 | sed -e 's/^/ /' -e '1 c├────────────────────┤' -e '$ c└────────────────────┘' -e '3~2 c├──┼──┼──┼──┼──┼──┼──┤' -e '2~2 s/.\(..\)/│\1/g' -e '2~2 s/$/│/' And a quick expalanation on what it does: + `tail -n +2`: output starting from line 2 + `s/^/ /'`: adds an empty space at the begining of each line + `N c&lt;text&gt;`: changes line at `N` to &lt;text&gt; (`$` means last line) + `N~S s/[..]`: applies the substitution every `S` steps starting at line `N` I wasn't in the mood to search for the correct chars for the upper corners, but you should be able to pick it up from here.
You my sir... deserve a cold beer and my greateful thanks : ) Thank you not only for the answer but more important for the share of info! I am trying to master the POSIX commands and i like to practice with "challenges" like this one, but today i kinda felt stuck. Really thank you from the heart : ) 
Could you explain what each of these does? Whats the ${var~} do? Also I can't find any information on what &lt;&lt;&lt; does as google won't search symbols. The second option looks like a short version of the original flc function 
i know it is not in gnu docs but why are ~ and ~~ undocumented exactly?
`set -x` like the other people have suggested, but also get in the practice of using debug variables and echos. Place statements like `echo "In while loop #2"` in your code, when trying to debug.
I have echos all over the file with regards to the loops - that's how I found it was just ceasing instead of finishing them.
I'll try the -x flag. Thanks.
Do some of the filenames contain spaces? Might I recommend something like: find $PATH -type f -name "$FILE" -print0 | while read -rd $'\0' tempfile; do ... done
 for tempfile in $TEMPFILES don't do it. Just don't. Ever. so you want to find stuff recursively but get only filenames with directory stripped? Do this instead while read -rd $'\0' tmpfile do something with quoted "$tmpfile" done &lt; &lt;( find . -iname "$FILE" -type f -printf '%f\0' ) but aren't you looking in the current dir (otherwise mv part wouldn't work)? Then why use find in the first place?
Or, better yet, redirect all file handle output [to a syslog logger](http://www.cyberciti.biz/tips/howto-linux-unix-write-to-syslog.html): /bin/sh /opt/some_app/do_stuff.sh &amp;&gt; logger 
I was thinking the same thing. 
I had pretty much given up but I'll check it out thanks.
Thanks for the input. I'll implement this change. Unfortunately though, the script crashes before even hitting the find and for loop.
Thanks. My scripts do have the bash shebang at the beginning. The scripts are being called from a 'caller' script - it uses /bin/bash ${SCRIPTNAME} instead of exec, as I understand it, those are the same? I've added the -x to see if there is any different but so far, no hints. I suspect it has to do with a problem with the FTP calls. Unfortunately, there's little in the way of error handling when running FTP commands from bash...
The crash seems to be with the FTP call. It's not making it to find statement. That being said, they should not have any spaces in them.
Thanks. I've implemented echos on literally every other line and redirect both stderr and stdout to separate files to further debug. Even with that, I still don't get any error messages. I have however replaced the standard FTP calls with ncftp and so far that seems to have fixed the problem. No idea why the builtin ftp command has issues compared to this ncftp application, but I've literally read through the ftp man pages, forums, etc. in trying to figure out how to debug - again, without success. Thanks for the help!
I'm curious. If it's undocumented, how'd you learn of it's existence? Just browsing the bash source code one day? Are you a contributor to the code?
I'd always wondered if there was a way to have awk use multiple characters for record separators. Sure, enough there are. Square brackets FTW!
I learnt about it from someone in #bash on freenode. How that someone came about it I don't know.
Fantastic work with that last netstat|awk statement!
And just a heads up to possibly save you some headaches when you go to use square brackets as separators :) awk -F'[\[\]]' '{ stuff }'
Or you can do: awk -F '[][]' '{ stuff }'
 ... | awk '$6=="LISTEN" { sub(/.*:/, "", $4); if (!seen[$4]++) print $4; }' | sort -n
You'll want to look at the `mount` command, to get output to see if something is mounted at `/backups`, if nothing is mounted, then you should establish a 'default' disk to mount, otherwise, maybe call a function taking disk1 disk2 as args, to `umount` disk1 and `mount` disk2. Also be aware `umount` and `mount` can fail, so you should check to see that the commands were successful, and the disk has been umounted.
Try: bash -x script.sh Will give you the debug output if the script, might Give you a hint
Try removing the back slash after Kali
I think the single quotes is makes $DIR expand literally so either double quote or remove the escape char
Try tossing an echo in front of the mkdirs. Easiest way to see what it's seeing, and it doesn't let the script actually do anything. This is like running it through sh -x, but far, far safer. 
&gt;mkdir "$DIR"/upgrade &gt;mkdir "$DIR"/dist-upgrade Try this. 
Always try your code on the command prompt first. Try this on your prompt and check whether it works DIR='/media/Kali\ Live' echo $DIR mkdir "$DIR/upgrade" 
That did the trick, thanks.
That did it. Could have swore I tried that.
Avoid using uppercase variable names; you risk overriding special shell variables or environment variables. Also, `mkdir` can create more than one dir at a time. #!/bin/bash dir=/media/Kali\ Live mkdir -p "$dir/upgrade" "$dir/dist-upgrade" # may be shortened with brace expansion: mkdir -p "$dir"/{dist-,}upgrade
 netstat -tnap | awk '/LISTEN/ {n=split($4,a,":"); print a[n]}' grab lines that contain the word "LISTEN" take the fourth field and split it using ":" as delimiter and save the number of split fields in "n". Also, save the split fields as an array in "a" print the "nth" field (last field) in $4 by printing the last element in the array "a" 
 if mount | grep "/dev/sdc1 on /backups" &gt; /dev/null; then umount -l /backups; mount /dev/sdd1 /backups; elif mount | grep "/dev/sdd1 on /backups" &gt; /dev/null; then umount -l /backups; mount /dev/sdc1 /backups; else mount /dev/sdc1 /backups; fi The "mount" command lists all mounted file systems. If grep finds the exact string match, then it will return 0, which is true, and "then" will be executed. Otherwise, it will check the "elif". You should also have a default mount so that in case both mounts are not mounted, the "else" clause can mount the default one. Also, the above script must be run by root. Lastly, "-l" option forces the file system to be unmounted even if it being used.
 no idea what you are trying to do exactly. i just looked at the output of Ram-Z's command and tried to replicate it with awk: cal | fold -w 20 | awk 'NR &gt; 2 {gsub(/ /,".",$0); if ($0 == "..") print "...................."; else print $0}'
well first, `$*` does not separate individual words - it dequotes everything before word splitting. `"$@"` (quotes included) protect whitespaces inside the "words". Second, `${x+y}`is similar to `${x:+y}` - the latter means 'if x has a value, substitute y', while the former means 'if x is set (even if it's empty), substitute y'. Now when "x" is `$1` and "y" is `"@"`, this means 'if the first script argument is set (i.e.: if there are any script arguments), substitute the whitespace-preserved value of all script arguments". beltorak @ kryos [~/tmp] $ cat wsdiffs #!/bin/bash echo '"$*"' for word in "$*"; do echo "|$word|"; done echo '$*' for word in $*; do echo "|$word|"; done echo '"$@"' for word in "$@"; do echo "|$word|"; done echo '$@' for word in $@; do echo "|$word|"; done echo '${1+"$@"}' echo \|${1+"$@"}\| beltorak @ kryos [~/tmp] $ ./wsdiffs 1 2 "a b" 3 4 "$*" |1 2 a b 3 4| $* |1| |2| |a| |b| |3| |4| "$@" |1| |2| |a b| |3| |4| $@ |1| |2| |a| |b| |3| |4| ${1+"$@"} |1 2 a b 3 4| 
`${x+y}` is not a shorthand for `${x:+y}`, they differ when x is set but empty. $ x=; printf '&lt;%s&gt; vs &lt;%s&gt;\n' "${x+y}" "${x:+y}" &lt;y&gt; vs &lt;&gt;
thanks!
Use `mail` (or `mailx`) instead? It has a flag for attachments: `cat &lt;file&gt; | mail -s foo -a &lt;file&gt; bar@baz.com`
Both your original theory, and /u/ib1984's reason are correct. There's also another one. * Some old unixoids will try to treat the whole thing as the path, and don't split on whitespace. (Note: no standard has specified what the correct behaviour here is) * (this is the new one) If you need a specific shell (say `bash`, which is at `/bin/bash` on most GNU/Linux systems, but `/usr/local/bin/bash` on FreeBSD), you can't always count on it to be in the same place, so you reach for `/usr/bin/env` to locate it. Because most systems just split the *first* whitespace, using another argument is impossible. * Even if you do know the path, someone will try to run it as `bash /path/to/script.sh` without remembering the `-e`. That's why most of my shell scripts that get distributed (that I want to use `-e` in) begin #!/usr/bin/env bash set -e
Exactly. The result of unquoted parameter expansions, like `$*` and `$@`, as well as `$var` and `$(cmd)` (command substitution), undergo word splitting (the result is split into words based on the characters in the `IFS` variable), and pathname expansion (any `*`, `?` and `[...]` in a word will make bash try to replace the word with matching filenames). So `$@` and `$*` are always wrong.
Another reason would be that `set -e` on its own line is a lot more obvious, and stands out, where `-e` on a line that is often colored by syntax highlighting as a comment, is not.
using bash substring manipulation: meme () { x="$*" ; curl -s "${x%.jpg}" | grep -oE "&lt;meta name=\"keywords.*/&gt;" ; }
Right, I'm saying, how can we post-process it to get the three strings? :) Also, given your workflow, the string manipulations make sense.
 $ curl -s "www.livememe.com/tpcs0wx" -o meme.txt $ grep -i gears meme.txt {n:"What Really Grinds My Gears", d:"p:v79cu4h, female:v85cxa9, brian the dog:kkxbtlt"}, &lt;title&gt;livememe.com - What Really Grinds My Gears&lt;/title&gt;&lt;script type='text/javascript'&gt;var album_name = "What Really Grinds My Gears";&lt;/script&gt; &lt;script type='text/javascript'&gt;var hex_album_name = "57686174205265616C6C79204772696E6473204D79204765617273";album_name = '';for (var i = 0; i&lt;hex_album_name.length; i+=2) { album_name += String.fromCharCode(parseInt(hex_album_name.substr(i, 2), 16));}&lt;/script&gt; &lt;meta name="title" content="livememe.com - What Really Grinds My Gears" /&gt; &lt;meta property="og:title" content="livememe.com - What Really Grinds My Gears" /&gt; &lt;meta property="og:url" content="http://www.livememe.com/tpcs0wx" /&gt; &lt;meta property="og:image" content="http://t1.livememe.com/xf0vj1_4.jpg" /&gt; &lt;link rel="image_src" href="http://t1.livememe.com/xf0vj1_4.jpg" /&gt; &lt;meta name="keywords" content="What Really Grinds My Gears YOU KNOW WHAT REALLY GRINDS MY GEARS? LIVEMEME.COM CHANGING THEIR WEBPAGE CODE SO I NEED TO ENABLE JAVASCRIPT TO READ A FEW STINKING LINES OF TEXT." /&gt; &lt;script type='text/javascript'&gt;var img_w = 500, img_h = 379, orig_album_id = '9d4ujr', capdms = ['18.0,13.0,464.0,96.0', '18.0,246.0,464.0,120.0'];&lt;/script&gt; &lt;div id="captionz" style="display: none;"&gt;What Really Grinds My Gears: YOU KNOW WHAT REALLY GRINDS MY GEARS? LIVEMEME.COM CHANGING THEIR WEBPAGE CODE SO I NEED TO ENABLE JAVASCRIPT TO READ A FEW STINKING LINES OF TEXT.&lt;/div&gt; Stripping the title off is easy, just some more code. Splitting the other two parts is harder because they only divide the two lines with a space (hex 0x20). Know any javascript? &lt;script type='text/javascript'&gt;var img_w = 500, img_h = 379, orig_album_id = '9d4ujr', **capdms = ['18.0,13.0,464.0,96.0', '18.0,246.0,464.0,120.0']**;&lt;/script&gt; I'm wondering if the bolded part could be used in the decode?
 put this in a bash script file named meme.sh: MEME_URLS="$*"; curl $MEME_URLS 2&gt;&amp;1 | awk ' function sprint_line(name,line) { gsub(/.*content="/,"",line); gsub(/".*/,"",line); print name,line; } /&lt;meta/ { n=split($0,a,"&lt;meta"); for (i=0;i&lt;=n;i++) { if (a[i] ~ /og:title/) { sprint_line("Title:", a[i]); } else if (a[i] ~ /og:url/) { sprint_line("URL:", a[i]); } else if (a[i] ~ /og:image/) { sprint_line("Image:", a[i]); } else if (a[i] ~ /keywords/) { sprint_line("Content:", a[i]); } } }' usage: pass it as many meme urls and it will print out the title, meme url, image url, and content of each meme example: ./meme http://www.livememe.com/tpcs0wx output: Title: livememe.com - What Really Grinds My Gears URL: http://www.livememe.com/tpcs0wx Image: http://t1.livememe.com/xf0vj1_4.jpg Content: What Really Grinds My Gears YOU KNOW WHAT REALLY GRINDS MY GEARS? LIVEMEME.COM CHANGING THEIR WEBPAGE CODE SO I NEED TO ENABLE JAVASCRIPT TO READ A FEW STINKING LINES OF TEXT. How it works: - parse html for all lines that contain the string "&lt;meta" and save it - take saved line and split it with "&lt;meta" as a delimiter. save split parts into array "a" - search every element of array "a" for og:title or og:url or og:image or keywords - if a match is found, take that array element and extract the useful part, give it an appropriate name and print it EDIT: made it better
cURL doesn't support scripts because it's a very dumb (not bad) agent for http connections. I'd highly recommend phantomjs as your tool for this. You can scrape the entire site for the desired div and only return those values to stdout or wherever. It runs in node.js and is one of the most useful web scraping tools I've ever used as it's basically a headless webkit browser in node.js. Example or what you might want in this case: &lt;div id="captionz" style="display: none;"&gt;What Really Grinds My Gears: YOU KNOW WHAT REALLY GRINDS MY GEARS? LIVEMEME.COM CHANGING THEIR WEBPAGE CODE SO I NEED TO ENABLE JAVASCRIPT TO READ A FEW STINKING LINES OF TEXT.&lt;/div
Nice. Though this is really not bash magic. Its Perl magic. Thank god for that positive look-behind.
well i c what you are referring to. &lt;3 perl ... considered coding something to extract the links in a loops but i thought meh ... ill just write something in bash and use the available tools ... using this for an irc bot to grab the top links from each page did you know perl was originally written as part of an nsa project by someone called larry wall ... b/c the data was classified but the code and app wasnt he released it for free ... larry wall has some good bits on youtube ;) http://hakology.wordpress.com/2014/07/01/larry-wall/ ... if you're refering to the regex ... larry wall describes regex as a separate language from perl completely yeah grep is written in c and regular expressions aren't perl ... this will explain things ... https://www.youtube.com/watch?v=JzIWdJVP-wo 
of course then I'd have to program in Javascript to get around the simple fact that I don't trust a page's Javascript. &gt;cURL doesn't support scripts because it's a very dumb (not bad) agent for http connections. I must be misunderstanding, because I used curl all the time for simple parsing of web pages in scripts. It seems to work great. And why write a whole program when the data you really want to read is extractable via a simple one-liner?
thanks for that ill bear it in mind ... next part of the script will return the header to display the title with the appropriate link ;)
If you add *.json* to the end of any Reddit link, you'll get the contents of the page in easy-to-consume json. Like so: [http://www.reddit.com/r/videos.json](http://www.reddit.com/r/videos.json) This can then be fed into your json parser of choice, for link extraction.
Do you escape the spaces with backslash when passing? 
How would I go about doing that? expand the variable, add backslashes and rebuild it into a new variable?
I just remembered something - use "$1" instead of $1, that prevents bash from expanding the variable and only uses it as a string. This is also good practice - the only time that you want this is, when the string contains multiple parameters for a command (e.g. A list of packages to install for a package manager). Edit: it you want to try the escaping - are the backlashes to the parameter you give the batch. I don't know how windows handles them, but it it's handling the input only as a string that shouldn't be a problem. 
thanks i was going to look at reddits api :) i will check this out ;) 
You cannot run bash scripts natively on Windows. However if it's a system you have control on and will be using it frequently, you could look into installing [cygwin](http://cygwin.com/), which brings a large amount of linux functionality to windows. But it's not something I'd install for "just once" usage, as it can be pretty involved.
&gt; This is also good practice - the only time that you want this is, when the string contains multiple parameters for a command (e.g. A list of packages to install for a package manager). Quoting vars is a good practice, but using strings for multiple params is not. I'd argue that in bash you should use arrays not a legacy, bug prone cruft. 
If "works most of the time" is good enough, sure, but it's not robust. It may allow running arbitrary code based on specially crafted filenames due to shell injection.
Unless they're running a multiplexer you can get it from you won't be able to do this.
multiplexer? 
Oh… I didn’t realize that `who` output actually already includes the display. In that case, it’s much easier with `sed` alone. Let me play around with it for a second… EDIT: [here](http://www.reddit.com/r/bash/comments/2adw6n/getting_another_users_display_variable/ciukvjs).
You shouldn't need to escape the quotes. $ silly() { echo $1; }; silly "testing crap with spaces" testing crap with spaces\ edit: nevermind, that's putty's `plink`, and Windows stuff, which I know very little about. Escaping quotes may actually be necessary.
how come you're posting batch file questions in a bash subreddit? aren't there more windows specific subreddits that can assist you better?
Ah, I missed that. sorry.
Random notes: I think it's better to use `mktemp` than assuming that `tmp_scr_file.png` is a safe name. Plus if you create in the tmp directory then it's not as necessary to worry about cleanup. That mv is not really necessary since you're scp'ing the file anyway. Using an array to split the md5sum is clever. It's not the way I would have thought of it, but I suppose it works. The pattern echo "foo" | command can (should) always be rewritten as command &lt;&lt;&lt;"foo" Here's an untested rewrite: #!/bin/bash screenshot="$(mktemp)" scrot -s "$screenshot" checksum="$(md5sum "$screenshot")" checksum="${checksum%% *}" scp -P &lt;port&gt; "$screenshot" user@example.com:/var/www/"$checksum".png xclip &lt;&lt;&lt;http://example.com/"$checksum".png
I approve of this. The only two things to note is that you'll probably want to handle failures from scp somehow, maybe also scrot. And mktemp is not a standard command, though it is unlikely that the OP cares if the script works on a UNIX system where mktemp does not exist or mktemp does not create temporary files by default.
`sudo watch &lt;command&gt;` is likely a better approach than `watch sudo &lt;command&gt;`.
I wrote this in the last few hours. I know there are many code generators and they are goddam good - but I wanted one, that can apply to all languages and is still small. If you have suggestions (especially on my bash-fu), please post them here or on github.
Add a `case` for '*' that prints 'unknown argument' or something, in addition to help / usage. Quote your strings. (almost)Always. #Initiate default values for variables filename=$(echo "$1" | cut -d . -f 1) fileextension=$(echo "$1" | cut -d . -f 2) Can be rewritten: filename=${1%.*} fileextension=${1##*.} sed -e "s/FILENAME/$(echo $filename)/g" -i "$1" Not that `FILENAME` won't work as a template string, but something lik `##FILENAME##` is more obvious, and less likely to cause issues. Also, you do not need `$(echo $filename)`, just use `$filename`. edit: By quote strings, I mean `if [[ -z "$profilename" ]]; then` in case `profilename="something &amp;&amp; rm -rf /"` or anything else you're not intending.
Thank you for this, I've google around without much luck
Thanks, I implemented the changes you suggested - however I wanted to keep the profile-files easy to read, that's why I chose simple upper case variables. However I think it should be easy to search for something like "##$var##" and use that. Could you elaborate the part about the parameter-substitution? I also used it for in the getops-part to extract the information about the functions - However I didn't read about this way to handle variables. Thanks again!
Everything you could want to know about bash string manipulation can be found [here](http://mywiki.wooledge.org/BashFAQ/100), [here](http://tldp.org/LDP/abs/html/string-manipulation.html) and [here](http://www.tldp.org/LDP/abs/html/parameter-substitution.html) Also, searching the internet for 'bash string manipulation' or 'bash parameter substitution'.
DIR='/media/Kali Live' DIR="/media/Kali\ Live"
Holy over-engineered-fuck. `local LEVEL_CODE=$(( $1 ))` ... why not: local level_code # assignment with local isn't recommended level_code=$1 # no need for $(( )) or even local level_code=$1 # or local LEVEL_CODE=$1 # if your local variables, for some reason, need to be caps Also `bb-tmp-dir` is a bad function name for a function that actually makes a directory; My instinct would be that it would simply return the name of the temp dir, if that functionality is even needed, as it should be stored in one of your numerous variables. If you have multiple local variables, you can declare them all as such: local foo bar baz qux Additionally, you are doing so much with functions, it might be a good idea to create a main application `bb` or `bashb` or `bash-booster` that accepts arguments to do the various tasks. You can build a bash completion list, if you want tab completion. Oh, and use `$()` instead of backticks, it's clearer, nests better, quotes better, etc. Addendum: If you have a function to check for a package, using yum or apt, and it's basically a 1 line command, it's really not needed. Instead, you could make a function called `bb-check-package` that accepts a package name as an argument, and checks either a flag set somewhere else, or check for the presence of yum or apt (or future package managers you wish to support). Also, `bb-sync-dir` appears to be a crude rewrite of `rsync`. `for file in $(find)` is better written as `find [extra args (such as a path?)] -print0 | while read -rd $'\0' file; do` to handle files with spaces or other troublesome characters properly. (Specifying a path for `find` is generally a great idea.) `cat "$SRC_FILE" &gt; "$DST_FILE"` instead of `cp -f "$SRC_FILE" "$DST_FILE"` needlessly obscures what you're doing (Also, if you would just use `rsync`, instead of reinventing the wheel, that probably isn't relevant.)
You can create a linux VM using free virtualization software like [Virtual Box](https://www.virtualbox.org/wiki/Downloads) or [VMware Player](https://my.vmware.com/web/vmware/free#desktop_end_user_computing/vmware_player/6_0).
&gt;Could you please explain it with examples? echo `echo \`echo \\\`echo test\\\`\`` echo $(echo $(echo $(echo test))) is the typical example that I see. 
I didn't ask about nested expressions, I asked about why this: VAR=$(command) Better than this one: VAR=`command` As for your example, I agree, second line looks better.
 echo -n "$output" | awk '{ORS=" ";print "@chucknorris";for (i=1; i&lt;=NF; i++) {print "("$i",,)"} print "(man,,)\n"}' awk automatically splits the line into fields based on white space. The number of fields is saved in special variable "NF". You can access any field by using $1, $2, $3 etc...and the last field is $NF. The for loop goes through all fields (starting at 1 and stopping at the last field) and prints each field surrounded with (,,), no matter how many fields are submitted.
Parsing $() is done very differently from \`\`, as I understand it, which is why you don't have to have the crazy \\\\\\\` escapes. According to the man page, the behavior of \ is different when followed by $, \, and \` for \`\`. I'm unsure what's different about \$. I assume it causes the expression to evaluate inside the subshell, but I couldn't come up with a command that would show this to be true. Aside from that, there's consitency. If you have any nested expressions, using $() is better. Switching between \`\` and $() in the same code is poor style, and, as things go, if you are going to have a habit of using one or the other, $() is as good as or better than \`\` in every case I've ever come across.
Thank you, that's clear enogh
Also see [FAQ 82](http://mywiki.wooledge.org/BashFAQ/082).
Yeah, I've googled around already and fixed that in my sources. Thanks again
Here is a sed solution, but honestly /u/KnowsBash's using printf is better. ssh server1 cat /etc/chuck.txt | sed -r "s/(\S+)/(\1,,)/g;s/(.*)/@chucknorris \1 (man,,)/" edit: use `\S+` instead of `\w+`
Damn... you beat me to it by a few minutes.
I have a habit of not trusting \S for some reason. I can't remember why :\ Great minds and all, though.
 ssh server sed 's/$/ man/; s/[^ ]*/(&amp;,,)/g; s/^/@chucknorris /' /etc/chuck.txt
or grep -i '[^.]*lithium[^.]*[.]\{0,1\}' article.txt # or grep -Ei '[^.]*lithium[^.]*[.]?' article.txt to handle `lithium` appearing at the start and/or end of the line as well.
What filter? What command? knowing bash does not imply psychic powers, I'm afraid.
I'm launching this using a crontab every half hour to surprise myself with something interesting.
Whitespace is important. You use it to separate commands and arguments. $ ls-l bash: ls-l: command not found $ ls -l total 0 Notice the difference? the first one treats the four letters `ls-l` as a command (which doesn't exist on my system), while `ls -l` is treated as the command `ls` with one argument, `-l`. Same with the `[` command, but avoid using `[` in bash. Use `[[` instead to test strings and files, and `(( ... ))` to test numbers. #!/usr/bin/env bash read -rp "You want som post by default (100 maximum): " post if [[ $post = *[!0-9]* ]]; then # if post contains any non-digits printf &gt;&amp;2 'Invalid input blah blah\n' elif (( post &lt; 0 || post &gt; 100 )); then printf &gt;&amp;2 '%d is out of range (0-100)\n' "$post" else printf 'Creating post %d\n' "$post" fi I also recommend against using `set -e` (aka `bash -e`), see http://mywiki.wooledge.org/BashFAQ/105 for more on that.
Houaaaaa, thanks a lot :) I need some works on that ! If i want know some option dans man read I don't have any information ? Do you know a nice place to learn a lit better some cmd et options, like this: read -rp 
Won't grep -v master succeed if there are any lines that do not match master (which will the case here)? Still, thanks for your pointers - With your tips, I've been able to get the mess down to git branch --contains $0 | grep -q master || echo $0 
Also, are those curly braces required by POSIX, or just stylistic? It seems to work for me without them (GNU bash, 3.2.51).
Cool tool. Had problem logging in after I created account - said wrong credentials. It would be awesome if you could create a login straight from CL.
the command doesn't work well ! ➜ bash-wordpress git:(master) ✗ help read zsh: command not found: help ➜ bash-wordpress git:(master) ✗ type read read is a shell builtin 
&gt; zsh: command not found: help :D And suddenly, we are out of this subreddit. man zshbuiltins
Do you want to mean the problem was here: #! /bin/bash -e Can I use ? #! /bin/zsh -e 
I like how this post fits both the old (bash.org) and the current (Bourne Again Shell) designation of /r/bash :)
Look at the command basename
One way is with bash parameter expansion: http://mywiki.wooledge.org/BashFAQ/073 a=/path/to/folder/filename.avi filename=${a##*/} no_ext=${filename%.*} edit: Fixed per /u/Vaphell's correction.
The problem was trying to run bash code in zsh, which is a completely different shell from bash. You can change your script to be zsh, yes, but then you must change it to use zsh syntax instead of bash syntax, but that's a question for another subreddit.
it should be a single %. People use .'s as a substitute for spaces in filenames and %% would strip everything except the first 'word'
Don't parse ls! http://mywiki.wooledge.org/ParsingLs Since you're already using shuf, an easy way to get 1 random file from a directory is shuf -n 1 -e /home/"$WHO"/Pictures/* That way you're sure to get an actual legit file out, whereas if you read ls, it'll break on weird file names with whitespace of newlines. Similarly, your for loop at the end is a little clearer as for func in "$(shuf -n "$N" -e "${ARRAY2[@]}")"; do "$func" done or without the weirdly-named array for func in "$(shuf -n "$N" -e imgur comic bashquote fact)"; do "$func" done Though to be super-pedantic, you shouldn't use a for loop at all to read line-by-line output like shuf's. shuf -n "$N" -e imgur comic bashquote fact | while read func; do "$func" done Ooh, also, there's no need to pipe grep into sed. grep strong | sed 's;foo;bar; can be sed -n '/strong/ {;s/foo/bar/;p;}' And also, you generally don't need semicolons at the end of statements unless you're writing more than one on a line.
Using awk to do simple manipulation on a single string is way overkill.
Definitely would want to use BASH's Parameter Expansion abilities for this. http://wiki.bash-hackers.org/syntax/pe#substring_removal 
 | xargs basename
You can't be serious...
Studying [this function](http://unix.stackexchange.com/a/139427/27437) and its help section shows a way to do this that works pretty well and doesn't require additional tools like awk or sed. The file doesn't even need to exist yet.
Ok, some of this has already been mentioned, but I'll try to go through everything anyway. &gt; #!/bin/bash WHO=$(whoami) You are relying on a non-standard command to find the username, which will already be available in the USER environment variable. Looking at what this variable is used for further down, it is exclusively used to guess the user's homedir, so there's really no point with that line, at all. &gt; popuppic1=$(ls /home/$WHO/Pictures | shuf -n 1); popuppic2=$(ls /home/$WHO/Pictures | shuf -n 1); Never parse `ls` output, especially not to enumerate files, which bash can do quite fine by itself. files=( "$HOME/Pictures"/* ) n=${#files[@]} popuppic1=${files[RANDOM % n]} popuppic2=${files[RANDOM % n]} See [FAQ 26](http://mywiki.wooledge.org/BashFAQ/026) for more on picking random elements. &gt; function fact { fact=$(wget randomfunfacts.com -O - 2&gt;/dev/null | grep \&lt;strong\&gt; | sed "s;^.*&lt;i&gt;\(.*\)&lt;/i&gt;.*$;\1;"); notify-send "$fact" " " -i "/home/$WHO/Pictures/$popuppic2" -t 15000 } Now this is getting bad. Parsing xml with grep and sed is not really sanely possible ([watch out for the pony](http://stackoverflow.com/a/1732454/1524545)). You should really get into the habit of parsing xml with an xml parser. Unfortunately there are no standard commands up to the task, so you'll have to find and rely on one provided by the OS, install one, or use a language which have xml-parsing capabilities. Python and Perl are examples of common scripting languages with such capabilities. Even worse than xml is html, which in the wild often isn't even valid html. You should make very sure that the site does not already provide an API before attempting html-parsing. Let's try with `xmllint` fact() { local fact; fact=$( set -o pipefail wget -qO- http://randomfunfacts.com | xmllint --html --xpath '//font/strong/i/text()' - 2&gt;/dev/null ) || return notify-send "$fact" " " -i "$popuppic2" -t 15000 } &gt; function bashquote { quote=$(curl -s http://bash.org/?random1|grep -oE "&lt;p class=\"quote\"&gt;.*&lt;/p&gt;.*&lt;/p&gt;"|grep -oE "&lt;p class=\"qt.*?&lt;/p&gt;"|sed -e 's/&lt;\/p&gt;/\n/g' -e 's/&lt;p class=\"qt\"&gt;//g' -e 's/&lt;p class=\"qt\"&gt;//g'|perl -ne 'use HTML::Entities;print decode_entities($_),"\n"'|head -1); notify-send "$quote" " " -i "/home/$WHO/Pictures/$popuppic1" -t 15000 } Here you're actually using perl at the end, but you should've done the whole parsing in perl. I can't help any more with this since http://bash.org appears to be down at the moment. &gt; function imgur { feh -B black --scale-down "$(wget -q http://imgur.com/random -O - | grep -Po '(?&lt;=")http://i.imgur.com/[^"]+(png|jpg)')" &amp; } More broken html parsing ... imgur has an [api](https://api.imgur.com/) &gt; function comic { feh -B black --scale-down "$(wget -q http://dynamic.xkcd.com/comic/random/ -O - | grep -Po '(?&lt;=")http://imgs.xkcd.com/comics/[^"]+(png|jpg)')" &amp; } ... &gt; N=1 ARRAY2=( imgur comic bashquote fact ) Avoid using uppercase variable names. You risk overriding special shell variables and environment variables. num_choices=1 commands=( imgur comic bashquote fact ) n=${#commands[@]} &gt; for index in `shuf --input-range=0-$(( ${#ARRAY2[*]} - 1 )) | head -${N}` do ${ARRAY2[$index]} done Woah, indentation. Why did you start that first now? Anyway, never iterate the expansion of an unquoted variable or command substitution. In this case, you instead iterate the desired number of times and extract a random element like we showed before. for (( i=0; i&lt;num_choices &amp;&amp; n&gt;0; ++i )); do choice=$(( RANDOM % n )) "${commands[choice]}" unset 'commands[choice]' commands=( "${commands[@]}" ) n=${#commands[@]} done
Thanks for that hint. curl --silent http://www.reddit.com/r/videos.json | ruby -r json -e 'JSON.parse(ARGF.read, :symbolize_names =&gt; true)[:data][:children].map{|c| ([:data, :secure_media, :oembed, :url].reduce(c) {|m,k| m &amp;&amp; m[k]})}.compact.uniq.each{|url| puts url}' This was fun.
`cut` is probably what you want. (Assuming the columns are delimited by a particular character.) And yes! `ls` is a terrible example! Never, ever do that. If you're splitting on whitespace, `read -a` is a pretty good bet: read -a files &lt;&lt;&lt;"$(process that produces files one per column)" EDIT SIDE NOTES: use `$(...)` instead of backticks. And, even though awk is so overkill, if you're going to do echo foo | cmd use cmd &lt;&lt;&lt;"foo" instead.
You have to know the number of columns beforehand. Here's an example where the file contains three columns, and each column gets stored in a separate array col1=() col2=() col3=() while read -r f1 f2 f3 _; do col1+=("$f1") col2+=("$f2") col3+=("$f3") done &lt; &lt;(some command) printf 'field 2 of line 5 is: &lt;%s&gt;\n' "${col2[4]}" See also [FAQ 1](http://mywiki.wooledge.org/BashFAQ/001). And like /u/thestoicattack said, the `some command` should never ever be `ls`. 
I just needed something to represent output, the actual command is s3cmd. cmd &lt;&lt;&lt;"foo" This is awesome ^ Thank you.
Yeah I know, just needed an example. Thanks!
I'd look to Ansible for this sort of thing.
You may want to look into [pssh](https://code.google.com/p/parallel-ssh/) (parallel-ssh). Just drop your script on the remote machines and have them run it in parallel. Handling exit codes (`&amp;&amp;`, `||`) in your script is a way to ensure it runs in the right order (or do you mean something else by "queue"?). There are also configuration managemnt systems that may help you if you have a lot of machines (ansible, chef, puppet, rabbitmq...)
Along with ansible, check out chef, puppet, and salt. These are configuration management tools. Pick the one you think will work and make your life easier :) 
It is possible to a degree with bash, but process management in bash is limited, so using something that already does what you need is better. If you're curious at how it could be achieved in bash, have a look at [ProcessManagement](http://mywiki.wooledge.org/ProcessManagement).
So some of this stuff I knew, but there is a lot of really good stuff in that link. Thanks a ton. Probably the best resource I've been pointed to so far.
I was about to say the same. It'd be the second time I've said it today.
Checking it out now, looks pretty good! Thank you.
I'll look into it. Another poster mentioned OSSEC which seems good.
Config server &amp; security firewall will change your life. Just google CSF firewall.
Thank you, learned something!
So far, I'm keeping this method. My mail server is the only service really open to any IP (So I can check my e-mail from anywhere) -- everything else is limited to certain IP addresses or use pub/priv keys. Another service running on the server may be a waste of the resources. How can I modify this to keep the IP address list saved? What I mean is, when my log file rotates weekly -- I lose all the IP addresses that were blocked in the previous week. Is it as simple as changing: &lt;/var/log/mail.log | tee /etc/block/blocked.ips | while read ip; to &lt;/var/log/mail.log &gt;&gt; /etc/block/blocked.ips | while read ip; EDIT: On second thought, I don't think that would work..
That's what the `tee` command does -- it writes to a file while still passing things on to stdout. 
If mail.log is empty will it wipe out all the IP addresses that were in the blocked.IPS file? How can I prevent that when the log file rotates?
Yeah, that format string for printf doesn't make any sense, as you'll see if you try it by itself. Take a look at `help printf`. ~~Since %.0s isn't a legal conversion specifier, the format string turns out to just be "\*". Since there are no conversions, the argument $var isn't consumed and you just print a "\*" followed by a newline.~~ EDIT: it is valid; it just truncates the argument to 0 bytes. Thanks, /u/KnowsBash. And, of course, you just do that 5 times because of your while loop. Other random stuff: * No shebang. Are you using bash or sh? * If bash, use [[ instead of [. * This whole "while loop with a post-increment" is pretty awkward; I think it makes more sense to use a for loop instead.
 printf '%*s\n' "$var" "" | tr ' ' '*' also see http://wiki.bash-hackers.org/snipplets/print_horizontal_line for more ways to do it. EDIT: fix. Don't need an extra printf just for the newline.
Printf takes a format string and a bunch of arguments. It replaces parts of the string with arguments. The parts that are replaced are called conversion specifiers, and they start with a "%" and consume one of the arguments. Things that aren't conversions are just passed through. This includes special escaped characters like \n for newline. So for example (in a pretend shell session) $ printf "hello\n" hello $ printf "printf me a decimal number: %d\n" 101 printf me a decimal number: 101 $ printf "hexadecimal: %x\n" 255 hexadecimal: ff $ printf "%s\n" "string conversion" string conversion
Huh! Valid, but not so useful in this case. Thanks for the fix!
Nice find on "\*" for field width specifier.
You're confusing people by suddenly changing your entire question. Now all the comments are about a different question than the one you are asking. Better to create another post for the second question, and to post it in a more relevant subreddit. Some subreddit for your OS and/or a subreddit regarding networking perhaps.
If you don't want parameter substitution to happen in your HEREDOCs, just quote your initial HEREDOC delimiter with single quotes. [Example 19-7 demonstrates this.](http://tldp.org/LDP/abs/html/here-docs.html)
Good information there, but I use substitution for colors and such across them.
Well, to me git is like "magic" and make even worse hehe.
Heredocs is a great tool, but there are much [better ones out there](https://github.com/thoughtbot/rcm). There is a great collection of [Dotfiles](http://dotfiles.github.io/) if you're looking for some inspiration. A couple issues I see with this method is 1. An `EOF` gets skipped by chance, or misspelled, or anything. *Much easier to do if you use `&lt;&lt;-` semantics.* If your `.inputrc` gets written to `.bash_profile` it's no big deal. But you might end up with your `.bash_profile` writing to your `.bashrc` which ends up importing itself infinitely at startup. 2. This becomes just one huge gigantic file and hard to manage. If you modify one environment, do you have to modify it again in this file? You want to have separation of and a single authoritative reference. 3. **Worst case**, someone else can write to this file, they can inject code strait into your sourced environment.
Looks interesting! and thanks for the "&lt;&lt;-" trick. I only change things on the script and then execute it to have all the files updated.
The good folks at Black Hills Information Security produce something called ADHD (Active Defense Harbinger Distribution). You only need a couple of the tools in there but there's a nice walk through on the desktop of how to use all the tools and scripts. http://sourceforge.net/projects/adhd/ PM me if you need help.
I am not exactly sure what you're after here, but I would think about process IDs and the wait command inside of a function launch_program() { $1 &gt;&gt; $2 2&amp;&gt;1 PID=$! wait $PID RVAL=$? } for cmd in `cat cmdlist.txt`; do launch_program $cmd some_log.file; done
What's your use-case?
Well, at first I needed a CLI gui installer to install a web server (one I'm building upon Nginx and which uses less RAM and CPU) on embedded Unix devices lacking dependencies and in general make it easier for the ones owning those devices. It's still immature though as there are open issues, the worst one being the constant (and annoying) screen redrawing when you highlight other buttons. As of this I decided to stick to the text-mode installer. Had fun coding this up by the way :)
Very nice work! Looking forward to seeing how you tackle the issues.
Thanks! As soon as I'm done with my project I'll tackle the other issues and update this post.
I do :) But there are some cases where it's not an option (as in many embedded Unix devices, and to some extent Android too).
The [BashGuide](http://mywiki.wooledge.org/BashGuide) covers such basics. The solution would typically involve a `for`-loop, `cat`, `grep` and `printf`. Which part(s) are you struggling with?
i under stand what i need to do is have the file loop through the contents of the folder and look for *.foo, then echo the contents and print ok nok for everything else i am just unfamiliar with doing multiple tasks in a script and looping is brand new :( i am trying to understand it but i have a bit of a deadline :( 
That doesn't match your initial description. You initially said to print `OK` if the **file** contained `foo`, not when the **filename** ended with `.foo`.
ok i need to take a breath and calm down i am obviously not under standing this. batch script loops through files in dir out puts contents of each file outputs ok if file contains foo nok if not would a grep with an exception work if i pipe it to echo? 
&gt; ok i need to take a breath and calm down i am obviously not under standing this. &gt; batch script bash script &gt; loops through files in dir &gt; out puts contents of each file &gt; outputs ok if file contains foo &gt; nok if not is that the filename containing foo, or the file's content containing foo? It makes a very big difference. &gt; would a grep with an exception work if i pipe it to echo? There are no exceptions in bash, and no it would not involve piping, especially not to echo. You'd also only need grep if you need to search through the **contents** of the files. If you need to look for a match in the filename, you use bash's pattern matching features instead (a `[[ ... ]]` command in this case).
right i have taken a breath and tried to calm down. Sorry about this. bash script, I apologize. The file contains the word foo. #!/bin/bash for FILE in /var/tmp/animals if [[ 'grep 'Foo' $File' ]];then echo "OK" else echo "NOK" fi This is where i am at so far. Thank you for taking the time to help some poor imbecile from the interwebs.
&gt; #!/bin/bash Proper shebang, that's good. &gt; for FILE in /var/tmp/animals 1. do not use uppercase variable names. call it `file` instead 2. that loop will only iterate once. You want a glob (wildcard) there. E.g. `/var/tmp/animals/*` 3. you forgot part of the for-loop syntax. `if` requires a `then`. `for` requires a ___ (run `help for` to see an explanation of the syntax) &gt; if [[ 'grep 'Foo' $File' ]];then This is a common beginner mistake mixing `grep` and `[[`. You want **either** `[[` or `grep`, not both. Again, do you want to check the **CONTENTS** of the file or the **FILENAME**? there's a big difference, and you really really really have to decide. Also, variable names are case sensitive, so `File` and `FILE` are different variable names. The [TestsAndConditionals](http://mywiki.wooledge.org/BashGuide/TestsAndConditionals) chapter of the BashGuide explains how to write tests. &gt; echo "OK" else echo "NOK" fi Odd indentation here, but otherwise fine. I'd also recommend using `printf` rather than `echo`, but it doesn't matter here. Lastly, you haven't closed the for-loop. `if` is closed by a `fi`, `for` is closed by a ____
Not quite, still missing part of the for-loop syntax $ help for for: for NAME [in WORDS ... ] ; do COMMANDS; done Execute commands for each member in a list. The `for' loop executes a sequence of commands for each member in a list of items. If `in WORDS ...;' is not present, then `in "$@"' is assumed. For each element in WORDS, NAME is set to that element, and the COMMANDS are executed. Exit Status: Returns the status of the last command executed. and the quoting around the grep is wrong, you want: if grep -q foo "$File"; then You're also missing the part where you output the content of the files.
 #!/bin/bash for File 'in /var/tmp/animals/*;' cat $file if grep -q foo "$File"; then echo "ok" else echo "nok" fi done Thank you for helping with this, I am getting frustrated with my lack of knowledge and its confusing me. 
Let me just highlight something from the synopsis of the for loop: for NAME [in WORDS ... ] ; **do** COMMANDS; done And again, variables are case sensitive, `$file` and `$File` are different. Also, quoting parameter expansions to avoid word splitting and pathname expansion is **very** important. cat "$File"
sorry i am missing something. i appreciate you trying to teach rather than just give me the answer. :D #!/bin/bash for file in /var/tmp/animals/*; do cat "$File" if grep -q foo "$File"; then echo "ok" else echo "nok" fi done or am i even getting close, should i try a different approach? 
You changed `file` to `File` one place, and `File` to `file` another place. Decide which name to use and make sure all occurrences have the same casing. Otherwise it looks good. I would've indented it better though.
thank you mate. I will gold you the first chance I get. May all your dreams, bar one. come true.
script worked like a charm. I love you!
you should try avoiding calling seq all over the place, it's going to be order of magnitude slower due to overhead than hacking it out in pure bash char_n_times() { local str char=$1 width=$2 printf -v str '%0*d' $width 0 printf '%s' "${str//0/$char}" } also when you embed $var in the formatting string, eg "$char%.0s" you get into trouble in case you try to use % because you get malformed format string. Separation of format and data is a good thing. and why echo -e instead of superior printf? 
Thank you very much! I had dug around on google for a bit but wasnt seeing exactly what I needed so I figured I would see if someone here would be able to answer without beating me up too much for a n00b mistake. Bash and regex etc is something I still haven't mastered even though I have been using *nix since 1993. 
The [[ command has it's own pattern matching operator '=~' via extended regular expressions STRING=Hello if [[ $STRING =~ $REGEX ]]; then echo "Match." else echo "No match." fi
Woah, thanks for the gold :) Unfortunately, google isn't a good resource for bash since most examples, tutorials and guides are mostly crap. I recommend searching through either of the two wikis at http://mywiki.wooledge.org/ and http://wiki.bash-hackers.org/ when you need to look up something. The [BashFAQ](http://mywiki.wooledge.org/BashFAQ) contains over 100 frequently asked questions now, and more will likely be added as new questions emerge in the #bash channel at freenode.
Don't forget the `=` (or `==`) operator which also does pattern matching, but using globs rather than extended regular expressions. if [[ $file = *.jpg ]]; then printf '"%s" ends with .jpg\n' "$file" fi
Sounds fairly basic, the trickiest part potentially being the parsing of the strings, which might require some regex dark arts with sed and/or awk. Or it could be as simple as using cut. Can you provide some example text from the main file? The next question, really to confirm, is do you want to selectively copy the found files, or just copy all of them and then rm the ones you don't want? This will affect whether or not you'll need to build user-interaction into the script.
So the file looks like this in general looks like this: Header (documentclass, usepackage, ect.) \begin{document} \section{chapter} \subsection{problems} \input{problems1} \input{problem2} ... \subsection{solutions} \input{problem1_solution} \input{problem2_solution} ... \section{chapter}... \end{document} The problem and solutions have more complex names but that's the gist of it. I figure the easier way to do it is just search for all the includes and then include or exclude anything with a "_solution" when parsing. As for user interaction it might be nice but at the end of the day we could always just use two scripts that are almost identical, one that excludes all of the "_solutions" and one that excludes everything that's not a "_solution". The main file we're working with is the solutions manual and the thought is that it'd be nice to not have to include all of the problem files individually in the book file tree. Additionally if we added or moved problems around we only have to keep one directory and then we can generate all of the chapter files from that. Eventually it would also be nice to create separate documents for the different chapters but we'll leave that for later. It's much simpler to just copy/paste a bunch of stuff from one file rather than opening up 100+.
This is why the `which` command is pretty useless. Since `which` is an external command, it does not know about shell builtins or functions etc. Use the `type` builtin instead. $ type [ [ is a shell builtin $ type -a [ [ is a shell builtin [ is /usr/bin/[ $ type [[ [[ is a shell keyword Run help type for more. Don't use `which` at all. Not in scripts, and not in interactive shell sessions. `type` is far superior. 
TIL
I'm not sure if I understand you correctly but it sounds like you want to replace the *\input{filename}* lines with the **contents** of *filename*. If this is the case, this example may be helpful to you. user@host:~$ cat header \begin{document} \section{chapter} \subsection{problems} \input{problems1} \input{problems2} \end{document} user@host:~$ cat problems1 This is problem1 speaking. This is still problem1 speaking. user@host:~$ cat problems2 Hello, this is problem2. user@host:~$ bash latex_input \begin{document} \section{chapter} \subsection{problems} This is problem1 speaking. This is still problem1 speaking. Hello, this is problem2. \end{document} user@host:~$ cat latex_input while read -r line do # does line begin with '\input{' if [[ $line = \\input{* ]] then # remove '}' from end filename=${line%\}} # remove '\input{' from start filename=${filename#\\input\{} cat "$filename" else echo "$line" fi done &lt; header To save the output to a new file, you could just use the shell's redirection: bash latex_input &gt; output_file Reading a file line-by-line is explained [here](http://mywiki.wooledge.org/BashFAQ/1). The stuff that removes parts of the line to leave just the filename are explained [here](http://mywiki.wooledge.org/BashFAQ/100). This doesn't perform any error checking that 'filename' exists and the header filename is hardcoded in. A version with the error checking can be found [here](http://pastie.org/private/sgnnqjgsozwfed13rtahdw). It takes the name of the header files as commandline arguments. bash latex_input header &gt; output_file Just incase you are not aware: shell redirections are set up before command execution so if for example you did: bash latex_input header &gt; header The header file would be empty before latex_input was executed. *&gt;* truncates the file before writing, so you must choose a different filename. For the example I created, the same output could be achieved by using: sed 's/^\\input{\([^}]*\)}$/cat \1/e' header But this approach is potentially dangerous as it executes a shell. So if it's possible for *filename* to contain shell metacharacters it would need to be further sanitized. It also assumes GNU sed which added the *e* modifier to s///. You've mentioned Python which you may want to consider learning. The equivalent of the bash script in Python would look something like: user@host:~$ cat latex.py import fileinput import re for line in fileinput.input(): m = re.match(r'\\input\{(.+?)\}$', line) if m: filename = m.group(1) with open (filename) as content_file: for content in content_file.readlines(): print content.strip() else: print line.strip() You would run it in a similar fashion e.g. python latex.py header &gt; output_file Or if you wished to, you could do the writing of the output file within Python itself and add in error checking and the such. EDIT: Replaced inner while loop with a useful use of cat thanks to the eyes of /u/thestoicattack :-)
Hello. If you find any errors or something that could be added to the topic, please tell me!
you can combine braces and $vars $ echo {a,b,c}$SHELL a/bin/bash b/bin/bash c/bin/bash you can't use variables inside {} without using *eval*, but at least in case of integers you can go with a straightforward C-like for loop instead $ var1=4; var2=15; step=3 $ for (( i=var1; i&lt;=var2; i+=step )); do echo $i; done 4 7 10 13 
I think the line-moving commands mostly follow emacs, but it's worth noting somewhere that `set -o vi` will give you vi-style moving and editing around the lines. Thanks also for `fc`. That's one I didn't know.
Interesting options. Any circumstances where you feel that any of the alternatives work better than \aliasescommand ?
To accomplish this first I would use Firefox plugin LiveHttpd Headers to get the post info from logging in and then use curl's cookie jar to store the info. I had a test example saved: #credentials USER="testing@testdomain.com" PASS="testing" #curl script using cookie jar to store the login creds generated by LiveHttpd Headers plugin in Firefox curl -sA Mozilla/4.0 --cookie ./login_cookies.txt --cookie-jar login_cookies.txt --data "t=10&amp;s=0&amp;user_name=testing%40testdomain.com&amp;password=testing" http://testdomain.com/protectedfile.php 
Yes, yes it would be :-) d'oh
Ah, didn't think you could use a backreference as the argument to the r command like that...
Thanks for the *eval* trick! I corrected it and gave you a credit, if you don't mind :)
Copying the content of files is exactly what I aim to do. Sorry I was a little long winded in the explanation. Thank you very much for you example. It looks like I was at least thinking the right way about the problem even if I was a little ways from the implementation you rolled out. In particular that sed command you dropped. It's still boggleing my mind. The only thing I'm having a little problem with is how you closed your loops. So for your inner loop do echo "$content" done &lt; "$filename" If I'm reading this correctly (which is probably a stretch) you're printing the file with the name that you just parsed. Then you end that while statement and copy what was printed to the main file? If that is the case is there any way to prevent the destruction of the main file. That is after running this script would I only have a file without the input statements as opposed to one with and one without?
The "Improving your command line workflow" link is bad. EDIT: all links pointing to other blog posts you wrote are bad.
*eval* is kind of dirty and should be avoided in principle (not to mention it's a security risk if *eval*'ing user input). There are very few problems that don't have kosher solutions if you structure your script properly. 
https://github.com/revans/bash-it may be of interest too
Standard line editing in Bash is via Gnu Readline. (Yes, lots of Emacs compatible stuff there, but a reasonable amount of differences too.)
Do you have a #! line at the start? Have you echo-edd the $?, what error actually occurs?
bash -xv &lt;your-script&gt;
I'm going to guess at http://mywiki.wooledge.org/BashFAQ/089, but it's impossible to tell for sure since you've omitted vital parts of the puzzle.
You might want to do something more like: diff -q /home/me/files/$file &lt;(ssh me@otherserver.xxx "cat /migr/$file")
I just leave 22 open with fail2ban. If I need any other ports, I just forward them over ssh.
 #!/bin/bash program=iceweasel time_limit=10 # in minutes $program &amp; pid=$! sleep $((time_limit * 60)) kill $pid
Thanks! Can you also please make it so that it takes the time to run and the program to run as arguments? I don't know bash scripting. Nevermind, I just looked it up on google and modified it myself. I'll add the code after I test it
Now is a great time to learn. Look at the `getopt` command. You can search for some usage examples, and check out the man page by running `man getopt` on the command line.
Thanks, I think I'll actually be making a project out of this, and have a chance to learn bash. The script doesn't work, though, $pid gets the PID of the script itself, not the program started by it. I've tested it in crunchbang 11, but I doubt it behaves differently in a different distro. Edit : Here's the script, I've added command-line arguments, and made some modifications for testing, but even without them, it works the same #! /bin/bash program=$2 time_limit=$1 # in minutes $program &amp; pid=$! echo $pid ###TEST sleep $((time_limit * 1)) ###TEST it's in seconds kill $pid 
Interesting, in testing, `$!` is returning the proper PID for me. `$$` is the PID of the script / shell. Remove the space in your first line, it should read `#!/bin/bash`. Perhaps that's an issue.
It works, but for me, if there is another instance of either iceweasel or chromium (also tested for geany, and for that it worked fine), I get a no such process error. Weird thing is, the process started in the script actually has the correct PID for a while. I have no idea what's going on, but it's weird. Missed it at first because I had another instance of iceweasel open. Here's what I tested. Also, weirdly, even on something that works, the program still seems to be running in the echo after the kill #!/bin/bash program=$2 time_limit=$1 # in minutes $program &amp; pid=$! echo 'PID determined in script: ' ###TEST echo $pid ###TEST echo 'Script PID: ' ###TEST scriptid=$$ ###TEST echo $scriptid ###TEST echo 'Instances of program PID (before kill): ' ###TEST ps -e | grep $2 ###TEST sleep $((time_limit * 1)) ###TEST it's in seconds kill $pid echo 'Instances of program PID (after kill): ' ###TEST ps -e | grep $2 ###TEST 
I make use of timeout frequently, does exactly what you're talking about. http://linux.die.net/man/1/timeout
You're missing a done for your for/do loop.
[FAQ 68](http://mywiki.wooledge.org/BashFAQ/068) explains several ways of achieving this. I agree with /u/pinkottah, install one of the timeout commands and use that.
Well there are a couple of issues here. As already noted you're missing the done on your for loop. *$()* is called command substitution so `$(dhcp.txt)` will try to execute dhcp.txt. Reading files line-by-line is described at http://mywiki.wooledge.org/BashFAQ/001 but it's not entirely clear to me what you want to do. If dhcp.txt contains 192.168.1.1 192.168.1.10 192.168.1.18 and staticips.txt contains 192.168.1.1 What should the output be? Should it echo 3 times i.e. do you want to perform the test for **each** entry in dhcp.txt or should it echo just once? If the answer is 3 times, you would use something along the lines of: while read -r ip do if grep -x -q "$ip" staticips2.txt then echo "matched $ip" else echo "no match" fi done &lt; dhcp.txt If the answer is once, you would probably use a variable and set it to a true value if a match is found, then after the loop test the variable. &gt;if [$ip != $ip2] *[* and *]* need whitespace on each side so it should be written as `[ $ip != $ip2 ]` but bash has *[[* so you could write it as `[[ $ip != $ip2 ]]`. *[[* deals with things like wordsplitting for you, for example: $ ip='foo bar'; [ $ip = foo ] &amp;&amp; echo match || echo no match bash: [: too many arguments no match $ ip='foo bar'; [[ $ip = foo ]] &amp;&amp; echo match || echo no match no match With *[* you would have to quote your variables `[ "$ip" = ... ]` to prevent such behaviour. You can read more about the differences at http://mywiki.wooledge.org/BashFAQ/031
Have you tried asking some experienced git guys if there's a non-interactive way to do the equivalent? If the solution to a problem is expect, you're probably solving the wrong problem.
Have you tested it with a simpler program? It's possible that ice weasel is checking for an already running process
psst, over here http://www.shellcheck.net/ **EDIT:** After putting your script through shellcheck and correcting the errors. I ended up with this: #! /bin/bash sudo arp-scan --interface=wlan0 --localnet |grep "192" |cut -f1 &gt; dhcp.txt for ip in $(dhcp.txt) and ip2 in $(staticips2.txt);do if [ "$ip" != "$ip2" ] then echo "Unknown Device!!!!" else echo "All Clear" fi done
Don't have the time to go over it all I'm afraid, but here's a working version. #!/bin/bash sudo arp-scan --interface=wlan0 --localnet |grep "192" |cut -f1 &gt; dhcp.txt while read -r ip ip2;do if [[ ${ip} != ${ip2} ]]; then echo "Unknown device"; else echo "Valid device" fi done &lt; &lt;(paste dhcp.txt staticips.txt) Note that you should not use for loops for anything other than arguments, use a while read loop for reading from files. $(textfile) is also invalid. If I had more time, there's no need to write it out to the dhcp.txt either, can just read it in and compare. something like while read -r ip ; do if ! grep "${ip}^" staticips.txt ; then echo "Warning, unknown device @ ${ip}" ; fi ; done &lt; &lt;(sudo arp-scan --interface=wlan0 --localnet |grep "192" |cut -f1)
The ^M is actually a \r, try stripping that instead
pexpect is a Python module and is pretty good and I'd imagine rather easier that TCL Expect. Can easily accomplish what you're looking for.
Awesome.. I figured there is a better way of doing it. Which is better, tr or sed? (or does it not really matter unless you are doing gigantic iterations and datasets?)
Yes that's where I've ended up. Just struggling to work out how to "press enter" after sending the appropriate character.
Had a good chat with the people on #git on freenode. Ranging from learning some awk to just using the `diff -i` option (Note; I don't mean `git diff -i`). I've got plenty of options now. Will post them here later.
It doesn't really matter, but I'm cultivating a habit of using as few processes as possible. Especially when it's some combination of grep, awk, sed, tr, cut, etc., you can often[0] do it all in one shot. [0] awk and sed are actually Turing-complete.
Trying to do a one time job every time you log in? There are several problems with your current script. 1. Don't use uppercase variable names. You risk overriding special shell variables and environment variables. 2. Don't assume the user's homedir is `/home/$USER` when you already have the homedir in the `HOME` environment variable. 3. Surely, `%gconf.xml` should be a regular file, not a directory, so test -d will always fail. 4. Why are you escaping `%` characters and why did you escape one of the `"` characters? 5. Why aren't you doing this with gconf instead? The commands in `/etc/profile` are run by the user that logged in, yes, but this really isn't a task for `/etc/profile`. Consider adding the `%gconf.xml` file in `/etc/skel` or under `/usr/share/gconf/` where defaults are meant to come from.
Thankyou, for most of that I would have worked out the bugs, but I appreciate the best practices. Can you elaborate on "Consider adding the %gconf.xml file in /etc/skel or under /usr/share/gconf/ where defaults are meant to come from." please?
Have a look under `/usr/share/gconf`, in particular the `defaults` directory ...
Also `man useradd`. Basically when you add a user, everything in the skel directory gets copied into their home directory (assuming `-m` is used). So if you put the default `%gconf.xml` file there, the new user will get a copy of it. EDIT: dammit, you know this. I meant to reply to the OP. :)
ouch! this caused me to research this: http://redsymbol.net/articles/unofficial-bash-strict-mode/ writing a ts3 client in bash?
Server activity stuff.. Not so much a client, but a near realtime view of activity on the server. Service to send JSON to a web server.
 ttytter -status="$(/path/to/thebest.py)" Read up on "Command Substitution" in bash's man page.
That's what I was looking for, wasn't aware it was called Command Substitution. Many thanks. Just playing with strips in bash to remove the S' at the start and ' p0 at the end now. Cheers. 
sys.stdout seems to put those tags on either end - is there a different method to stop it doing that? 
There should be, I'm not familiar with Python though. Check the [documentation](https://docs.python.org/2/tutorial/inputoutput.html).
Many thanks, I've got it going.
You can also use backticks, but the $() method is recommended. I don't think there is a technical reason, it is just easier to read and people won't mistake the \` for a '. ttytter -status=\`python thebest.py\`
They have some [differences](http://mywiki.wooledge.org/BashFAQ/082).
You could store the filenames in one file and the sums in another with one find command, such as find "$dir" [... file conditions ...] -fprint0 "$namesfile" -exec md5sum {} + | cut -f1 &gt;"$sumsfile" Then the `$namesfile` file would hold all filenames, separated by '\0' (so you're safe from special characters) and the `$sumsfile` would hold md5 sums, one per line. Now suppose you want to read in your "database"; matching up files to sums. You can do it with an associative array in bash (&gt;=4), but you have do some FD trickery to read from the two files and match them up: declare -A sum_to_file # associative array exec 3&lt;"$namesfile" # open names file for reading on FD 3 while read sum; do read -r -d $'\0' -u 3 filename sum_to_file+=(["$sum] = "$filename") done &lt;"$sumsfile" exec 3&lt;&amp;- # close FD 3 So then `$sum_to_file` is an array mapping, well, sums to files. Now suppose you run another find to find the new files and dedup them: find [... however you get new files ...] -exec md5sum {} + | while read -r newsum newfile; do orig="${sum_to_file["$newsum"]}" [[ "$orig" ]] &amp;&amp; ln -svf -- "$orig" "$newfile" done I have no idea if this would work. I haven't tested it. I never use bash associative arrays cause I always end up in perl. I never do fancy FD stuff. You still have a bunch of work to put these snippets together into a real program. But hopefully this will get you started. EDIT: for example, I realized I included no facility for adding new files and sums to the "database." You'd have to do that part. EDIT2: hell, I don't even know if md5 is a good choice for hashing. Are there ever hash collisions? You don't want to delete stuff accidentally.
Dirvish project may or may not interest you.
If this is all within the same directory, you might find this perl script helpful. It works by looking for identically sized files, then compares md5 sums, only when necessary, because hashing files is time / cpu intensive. It can scan ~5000 1-2mb files for duplicity in less than a second: $ ls -1 | wc -l 5810 $ time smart-dupe 2014-08-06-04464.jpeg duplicate of 2014-08-06-05647.jpeg 2014-08-06-05485.jpeg duplicate of 2014-08-06-03321.jpeg 2014-08-06-05483.jpeg duplicate of 2014-08-06-03316.jpeg 2014-08-06-04465.jpeg duplicate of 2014-08-06-05645.jpeg 2014-08-06-03317.jpeg duplicate of 2014-08-06-05482.jpeg 2014-08-06-05486.jpeg duplicate of 2014-08-06-03327.jpeg 2014-08-06-05308.jpeg duplicate of 2014-08-06-05315.jpeg 7 duplicate files found real 0m0.333s user 0m0.228s sys 0m0.036s https://github.com/cpbills/tools/blob/master/smart-dupe If they're not all within the same directory, I could perhaps work to make it a recursive script that tracks files in multiple child directories.
Since you mentioned not having perl, here is a probably very inefficient way to do it it in bash that may or may not work, but should be something you could work from: #!/bin/bash dir="$1" || '.' _create_sums() { find "$dir" -type f -print0 | while read -rd $'\0' file; do # Skip file if md5sum already saved, or file is md5sum file if [[ -e "$file.md5sum" || "$file" =~ .md5sum$ ]]; then continue fi md5sum -b "$file" | awk '{print $1}' &gt; "$file.md5sum" done } _clean_dupe() { file1=$1 file2=$2 rm "$file2" "$file2.md5sum" # cp to original directories? } _compare_sums() { find "$dir" -type f -name "*.md5sum" -print0 | while read -rd $'\0' sum1; do # Skip if the file has gone missing in the mean-time if [[ ! -e "$sum1" ]]; then continue fi hash1=$(cat "$sum1") file1=${sum1%.*} find "$dir" -type f -name "*.md5sum" -print0 | while read -rd $'\0' sum2; do # Skip if the file has gone missing in the mean-time if [[ ! -e "$sum2" ]]; then continue fi hash2=$(cat "$sum2") file2=${sum2%.*} if [[ "$hash1" = "$hash2" ]]; then _clean_dupe "$file1" "$file2" fi done done } main() { _create_sums _compare_sums } main If you 'don't have perl' because you don't have experience with it, since I would be surprised it isn't installed or cannot be installed, I would still recommend looking at the script I already linked to: https://github.com/cpbills/tools/blob/master/smart-dupe
just came by for a quick look (and reply) ... its like 5am here and I may be a bit drunk so I wont try to understand the others right now, however towards your question/comment. I indeed can not use pearl (or python or ...) since it is not installed. And yes it could(!) be installed by modifying the live system. which will be a bitch. Imagine a live system or rather "firmware". Every modification (like installing software) will be reset after a reboot. Yes, it IS possible to change the live system and/or put files somewhere where it wont be forgotten... but all in all and for all intents and purposes i can not "install" software. I can however create a bash file, put that in a hidden directory on the data pool and run it without being afraid of the script getting lost during a reboot... 
Neat script but use pastebin (or a similar service) next time :) http://pastebin.com/raw.php?i=0kR3LiR7
Ok, will do. Thanks.
unfortunately the files are not all in one directory, there is multiple/many directories... and like I said, I can't use perl or python. but I think this script might help as a guide when creating the bash script 
Just FYI `$HOME` is the user's home directory. You can use the `env` command to view all the available environment variables you can work with.
Rough rewrite to make it a little easier to work with. The URL string you use could probably be broken out a bit to include the use of a `$common_opts` string for stuff like `common_opts="&amp;res_opt=eqeq&amp;res=0x0&amp;thpp=60&amp;purity=100&amp;board=21&amp;aspect=0.00"` to avoid having to rewrite that over and over. Also you can probably avoid using the temp files, and instead pass a string to your download function. http://pastebin.com/Yyj6LSdn edit: Improved re-write; most of the options you were passing to the search URL were 'defaults', and not necessary. http://pastebin.com/RnAD1uc8 - now with 100% more testing!
Sure, it was a quick example, written at 1:30am, that should get the job done at least once. To improve it, you could introduce an array holding inode number as index, and then storing these two arrays in a file with `declare -p`. To get the inode number mostly requires non-standard tools. If you have GNU find, you can get it with its `-printf` action. If not, maybe your system has a `stat` command instead. Worst case scenario, you have to parse `ls -i`.
Not to rain on your parade, but you don't seem to know too much bash either. I'd recommend you dispense with saying things like "I don't know X" unless you're going to start adding the suffix "yet" Learning how to do this with Perl, Python or Ruby will be just as easy (I'd recommend you try Python or Ruby. Perl has been largely superseded by them and their libraries will be more up to date.) Storing these checksums and filenames would be better done in SQLite (all scripting languages have simple wrappers for working with it.) You can also drop out to calling shell commands from within these languages. So you don't lose anything. You only have things to gain. 
I never said I am a bash pro and I freely admitted of being overwhelmed by the task. And I am quite aware that there are countless other languages equally or better suited and believe me, if I could do it in python I would... I also am quite aware that using a database like sql would benefit me greatly, but I'll say it again and maybe for the last time... There is no perl, no python, no ruby, no sql. it is an embedded system built on freebsd booting from read only memory like a linux live cd. sure you can install all the stuff you want. and use it... until you reboot and its all gone... and if I could do it in python with sql I would not have fcome to /r/bash, now would I... 
I did a bad thing and I'm sorry. I mistook your situation with one of skills/intellectual self limiting, as opposed to system limitations.
This may seem dumb, but why redirect help to stderr?
about the collisions, md5 is not foolproof, and you are right, there is a risk of two different files having the same hash and boom, problem. which made me think, to further reduce the possibility of collisions causing data loss, I should only compare md5s of files with the same filesize. which should have the added benefit of saving time. I might even add a further check by comparing a few bytes of the files and only proceed if they are actually equal... 
first, your suggestions have been interesting, but I will be honest, there are a lot of things employed that I dont fully understand... like how exactly the arrays are used or how/why print0 is helping or not... anyway, I've tried a few things and thought about a few things... - only create md5/compare certain files (equal filesize) - put data in one file, add "new" stuff at the end, change stuff above - (have a function to remove entries for files that have been deleted) - try to do the comparison etc in ram (arrays) data in file like this?: fsize \t md5 \t fname1 \t fname2 [\t fname3...] \n --&gt; what if md5 is not yet created? leave empty? possible problems stuff to remember: - getting filesize (bytes): stat -f %z $fn - get md5: md5 $fn | awk '{ print $4 }' get md5 problematic if fn has spaces, need better awk (1st from right?) (to test: awk '{ field = $NF }; END{ print field }') dedupe function: _dedupe() { if [[ -z "$1" ]]; then echo "Dedupe QUELLE fehlt" elif [[ -z "$2" ]]; then echo "Dedupe ZIEL fehlt" elif [[ -f $1 ]] &amp;&amp; [[ -f $2 ]]; then # add check for actual byte comparison echo "$1 and $2 found, deduping..." rm $2 cp -l $1 $2 fi } I will add more info later/next days when I had time to try more stuff... EDIT: next thing I am working on: read the file(size,md5,name(s)) and put the info in an array which is problematic too... I have lines (\n) but I also have multiple attributes(\t) in each line: size(n) md5(n) fn(n,1) fn(n,2), ... 
Thank you!! I will give this a try.
Thank you! :)
Lots of good info thanks! I will play around with this tonight.
btw, the last line was wrong and should be: grep -F "${ip}$" and not 'grep "${ip}^" - the $ is end of line, ^ is beginning of line. -F is also required to ensure the .'s are not taken as wildcards.
Careful there. That function is broken. If the filenames contain special characters like blanks, newlines, *, ? or [...], the commands will not do the expected things. The reason for this is because you failed to quote the parameter expansions for the `rm` and `cp` commands. rm $2 # potentially removes more than one file, not necessarily the file you wanted to remove rm "$2" # removes that one file no matter what weird characters it contains. Quotes are extremely important in shell scripting. If you do not understand how the shell parses the lines, you risk removing the wrong files, or corrupting data. See http://mywiki.wooledge.org/Arguments for more on this.
Ah, alright. 
thanks for the warning... I will thorougly study your page, I am aware of quoting and/or escaping but I will admit that I dont understand it fully... now, your example, wouldnt it break for filesnames with " in them?
No, any quote characters the filename may contain will be treated as literal characters, not syntactical. Or put in other words, it only does the quote removal step once, and that happens before the variable gets expanded.
Thanks, that rewrite is very interesting.... I have a lot to learn : )
tmux has plugins?! There goes my morning!
Presumably because of the use of `eval` in [this function](https://github.com/tmux-plugins/tmux-copycat/blob/18a2c98c3b0cff93bb61d8a00f0be96125f3a11e/scripts/copycat_generate_results.sh#L18-L24): &gt; reverse_and_create_copycat_file() { local file=$1 local copycat_file=$2 local grep_pattern="'$3'" # making sure grep regex is quoted # The below line had to be eval-ed, otherwise it doesn't work eval "(tac 2&gt; /dev/null || tail -r) &lt; "$file" | grep -oni "$grep_pattern" &gt; "$copycat_file"" } If you change it to the following, it'll probably remove the "double backslash" issue: # if there's no tac command, pray there's a tail with -r if ! type tac &gt;/dev/null 2&gt;&amp;1; then tac() { tail -r "$@"; } fi reverse_and_create_copycat_file() { tac "$1" | grep -Eoni "$3" &gt; "$2" } Note that I also added `-E` for extended regular expressions to the `grep`, so instead of `[[:digit:]]\\+` you'd now do `[[:digit:]]+` instead. I have not tested this code at all. I don't use tmux, and I don't feel like trying it just to test this change.
That's cool! How portable is grep with extended regex flag? Is it "safe" to use in a script that's intended to be distributed around?
Not particularly portable, speaking from an illumos standpoint. I didn't realize that plugins were just some bash though, that makes it pretty simple to fix and extend tmux. I never really looked into it beyond replacing screen and having panes. Neat stuff.
Find anything good?
`-E` is POSIX, so you'll have a hard time finding a grep implementation without it. `-o` though, that's a GNUism, but one that BSD has copied, so it will at least work with GNU grep and BSD grep. You could do the "grepping" in bash instead (using the `=~` operator of `[[`), but bash isn't that efficient at parsing files. If we're only talking a "screenful", it shouldn't be a noticeable difference.
 md2html() { { echo '{ "text" : "' sed -e 's_$_\\n_' -e 's_"_\\"_g' echo '" }' } | curl [ ... options ... ] }
You'll have to limit how many times you use this per hour. There's a limit the API places over its usage and I'm sure if you attempt to abuse it they'll just take precautions.
thanks for the improved variation, looks cleaner though I'll still not wanna miss the initial param count check, param to name assignment and some pre-task logic... irrespective of language, the code just doesn't feel safe to be given to all without them
Understand. Thanks for stating that for all. I'm not planning to load test them anyway. Just working on a little fun project of complete bash simple MD-to-HTML blog builder. Which will convert only new added Markdowns. Then it depends per user style of it. Will post that here once usable.
people could use pandoc or other utilities with similar; I've been using pandoc in past... I just didn't wanted the fun-project to depend on anything at user side (asking them to install pre-requisites) other than bash.
Sounds like a race condition. The file is there when the `-f` test is run, the file is gone by the time `cp` tries to copy it. Without seeing the code though, all we can do is guess.
No, surprisingly. It looks like there's a lot of room for growth here. It's not a plugin, but the [tmuxinator gem](https://github.com/tmuxinator/tmuxinator) is priceless. 
I'd imagine it's a bug in the script, and not something with the file itself. Post the script :)
I would go with: [ `md5sum /home/me/files/$file` == `ssh me@otherserver.xxx md5sum $file` ] &amp;&amp; echo "print something if they match" || echo "WARNING: something about how they do not match"
Yeah, need to see the code. Otherwise, couldn't you just use find -maxdepth 1 -type f -exec cp? I've just been through this with a colleague's script, a hundred lines of overplumbing loops and grep -v's replaced with two lines of find. 
[tput](http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x405.html)
And what's the error message you get when it fails?
It appears to be trying to do the equivalent of rsync, except not as well. Any reason why you don't just use rsync instead?
You can write a script to do simple animations of sorts if the terminal emulator allows it. But getting it in the bash prompt. No way. I'll go as far as to claim it can't be done without patching bash and/or readline heavily. See http://wiki.bash-hackers.org/scripting/terminalcodes
Ok, so then we know that the file *is* copied, otherwise you'd see an error message from `cp` in `$MYLOG`. But you don't know if it is the `-f` test or the `cmp` test failing. The source file or destination file might have changed between the copy and those two tests.
It still depends on `curl` and an available Internet connection; but fair enough. Good job
If you can render a Mandelbrot set (albeit 10x slower than ksh-93, according to the article), you should be able to render 10 repeating frames. How nice it would be as a prompt, I can't say. I do believe it's possible, however, without hacking readline. while true; do tput save cursor position, render frame, tput restore cursor, tput save pos, render frame, tput restore pos... ; done Kirby: http://www.reddit.com/r/linux/comments/w8nrk/bash_prompts/c5bai0o
Have you tried `export PS1='&lt;BLINK&gt; :-) LOLZ YOLO &lt;/BLINK&gt; '`?
http://mywiki.wooledge.org/TemplateFiles
Was aiming for the second solution...
I use the followinf bash script to add html content stuff just before a line containing only &lt;!-- insertion point --&gt; in the index.html on the machine ${WEB_SERVER} at the absolute path ${REMOTE_DIR} TMP=$(mktemp -d /tmp/modify_index_html.XXXXXX) scp ${WEB_SERVER}:${REMOTE_DIR}/index.html ${TMP} if [[ ! $(grep '^&lt;!-- insertion point --&gt;$' ${TMP}/index.html) ]]; then echo "Can not find an insertion point!" &gt;&amp;2 exit 1 fi cat ${TMP}/index.html | sed -e '/^&lt;!-- insertion point --&gt;$/,$d' &gt; ${TMP}/new_index.html ###################################################################### echo "&lt;p&gt;This is the new stuff&lt;/p&gt;" &gt;&gt; ${TMP}/new_index.html ###################################################################### cat ${TMP}/index.html | sed -n -e '/^&lt;!-- insertion point --&gt;$/,$p' &gt;&gt; ${TMP}/new_index.html scp ${TMP}/index.html ${WEB_SERVER}:${REMOTE_DIR}/index.back.html scp ${TMP}/new_index.html ${WEB_SERVER}:${REMOTE_DIR}/index.html \rm -rf ${TMP} 
So instead of using sed I should replace my placeholder with a variable and let bash evaluate it. That's pretty smart!
The functionality that the native environment provides was not good enough so I created my own replace function is node.js and was done with it. Still lookinf forward to any other ideas you might have!
Sorry I don't really get where the magic happens...
`sed` has an r command: http://www.grymoire.com/unix/Sed.html#uh-37
Thats nice, I have saved it for later reviewing, thanks!
Thanks! This did it, I knew about the r command but I was invoking into wrong! 
 # This checks that the file index.html contains a line with only # ""&lt;!-- insertion point --&gt;" if [[ ! $(grep '^&lt;!-- insertion point --&gt;$' index.html) ]]; then echo "Can not find an insertion point!" &gt;&amp;2 exit 1 fi # This puts in new_index.html the part of index.html above the # line "&lt;!-- insertion point --&gt;" sed &lt; index.html -e '/^&lt;!-- insertion point --&gt;$/,$d' &gt; new_index.html # This adds some content to new_index.html echo "&lt;p&gt;This is the new stuff&lt;/p&gt;" &gt;&gt; new_index.html # This adds at the end of new_index.html the part of index.html # below the line "&lt;!-- insertion point --&gt;", the line itself # included sed &lt; index.html -n -e '/^&lt;!-- insertion point --&gt;$/,$p' &gt;&gt; new_index.html 
Search "how do I write a bash script" and follow the instructions. When you are asking for are the fundamentals of scripting. Or take a look at the man/info page. 
I am thinking this isn't a basic scripting problem. Maybe you didn't understand my question.
Trust me - it is. What you want to do is to define functions within one file and let the script decide which function to run upon the parameter you give the function. That **is** a fundamental of shell scripting (or better said: of programming itself). Do yourself a favor and read an introduction to shell scripting, you will do yourself a great favor by saving time to write functions which the shell already provides.
Oh, wow, those do look better. I only had time to skim, but it looks like wooledge goes more into Bash as a component in the *nix system instead of a programming language in a vacuum. Is it like that for most of the guide? I may need to start recommending these.
And don't forget #bash on Freenode.
People of reddit bash: you should give him at least an example before sending him off with RTFM bullshit. If you won't/can't, then just do us a favour and keep your comments to your self! Example of what you are trying to achieve (main two concepts are: bash case and bash functions): #!/bin/bash #script name: todo.sh #functions ls () { #function code } add () { #function code } do () { #function code } #### Script starts here ### case "$1" in ls) ls ;; add) add ;; do) do ;; *) echo "Available commands are: ls, add, do" ;; esac pretty straight forward.
Thanks, that helps narrow the search. I am pretty familiar with the rtfm answer. I just assume they lead disappointing lives and have to take it out others to feel better.
To add to that… if you want to pass arguments to the functions, do something like this: cmd="$1" shift 1 # now $1 is the former $2, etc. case "$cmd" in ls) ls "$@" # $@: all arguments ;; # etc. (Edited to quote the $@ as per /u/geirha's comment.)
Also, `do` is a keyword: &gt; bash: syntax error near unexpected token `do' So a function called `do` won’t actually work. (However, a case `do` does work, fortunately.)
You can do it in one pass instead of three. sed -e "/--content--/{ r $filePath" -e D -e } "$templatePath" &gt; tmp &amp;&amp; mv tmp "$filePath"
"$@" quotes the individual arguments? (On mobile, can't test myself right now)
I don't understand your `sed` expressions; are you getting output from `ss` that has those characters in it, because I am not. Ah, formatting issues; this looks 'better': ss -t -a | grep -e "ESTAB" -e "9051" | tail -n 8 \ | sed -e 's/^/├─ /' -e 's/.$/─┤/' -e '1 s/.$/┐/' -e '$s/^../└─/' -e '$s/.$/┘/' Follow up question; Do you care about anything but the 4th field? I think you are trying to add some sort of border 'artwork' prematurely. I would recommend writing a script to take input in and then wrap it in your border, that way you can avoid that horrible looking sed. Something along the lines of: #!/usr/bin/perl use strict; use warnings; my $width = 0; my @text = (); foreach my $line (&lt;STDIN&gt;) { chomp $line; $line =~ s/\t/ /g; if (length($line)+2 &gt; $width) { $width = length($line)+2; } push(@text,$line); } print '┌' . '─' x $width . '┐' . "\n"; foreach my $line (@text) { my $padding = ' ' x ($width - length($line) - 2); print "│ $line$padding │\n"; } print '└' . '─' x $width . '┘' . "\n"; $ fortune | pipe-box ┌───────────────────────────────────────────────────────┐ │ QOTD: │ │ "If I'm what I eat, I'm a chocolate chip cookie." │ └───────────────────────────────────────────────────────┘
Okay, thanks! TIL...
Good point, I did previously find that answer on SO. But since I just found out about the r option I opted for the solution that was as dumb as possible, so that I could easily recognize what I doing.
 sed -e 's/\(\([^ ]* *\)\{3\}\)\([^ ]*\)/\1${color1}\3${color}/' -e \ 's/^/├─ /' -e 's/.$/─┤/' -e '1 s/.$/┐/' -e '$s/../└─/' -e '$s/.$/┘/'
 awk '{ $4 = "${color1}" $4 "${color}"; print; }'
Hmm yes. Just tried it here locally and it fails with ${color1}. It works if you use the ${color blue} or ${color #10BC2A} syntax.
I was able to write the script that I was asking help for. Your comment saved me lots of time. Function would have been easy to figure out but case would have taken much longer to find by rtfm. Thank you again. sometimes you don't even know how to ask the question.
I would look into LVM2. I don't know off hand if Fedora uses LVM by default if you just used the basic auto-partition option during install (I think it might...) but assuming it did you could delete and re-create the partition in fdisk or parted. Expand the Physical Volume layer, expand the logical volume layer and then grow the filesystem to assume the new space. If you are not using LVM, you will need to decide where you need the additional space (/home, /var, /opt, etc) and then I would make a new partition (this time using LVM), migrate what already lives in the place you want to expand and then edit /etc/fstab to mount this location using the new partition. This process could easily be scripted. If you are staging new systems you could create a kickstart file to automatically answer the questions presented to you during install. Hopefully this will get you started but if you can provide some additional information about your situation I might be able to be more exact.
You can pipe stdin into `fdisk`, I think. This requires that you know how to use `fdisk`.
You are correct, example below: echo -e "o\nn\np\n1\n\n\nw" | fdisk /dev/sda
or fdisk /dev/sda &lt;&lt;&lt;"o n p 1 w"
Check out sfdisk -- that is built more for scripting, and has the option to allocate remaining disk space. 
Thanks for answer, will try :)
Thanks, I will check it out :)
Hmm, thanks for making change, but still getting the same error on everything I've tried including .txt, .JPG, and .MOV files. Not sure I really needed this but was curious what exactly it does, so wanted to try it, and saw the error and thought I'd let you know. I've edited it to add echo statements to confirm that it's really running the most recent version, and it is. 
Hmm, it's working fine for me on my Mavericks machine :(. At first I was getting the error that you got but it was fixed. And what it does is just shows the file you give as an argument in Finder, just as if you scrolled to it. You're not missing too much though. As a last resort, make sure you sourced your bashrc again after making the changes. I forget to do that a lot.
It also has the option to screw up the partition table, so feel free to test it from within an environment you don't care about destroying. 
I would sincerely hope that anyone who is looking at scripting anything that modifies the partition table (or anything on that level of the config) would not be *testing* on anything other than a sandbox. 
I'll just leave this here for no apparent reason. http://xkcd.com/327/
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=327#Explanation) **Stats:** This comic has been referenced 315 times, representing 1.0152% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjy6u2y)
You know someone's thinking ... _sure, its prod but we have backups ..._ as he/she screws up the partition table. Plot twist: backups didn't work because they've never worked and were never tested. Result? All data has been lost.
...dumbass
In my opinion, treating data as code is bad regardless. If the user takes advantage of this on purpose, well as you said, he'll be messing with his own database. If the user accidentally puts some wrong characters in there, he may get confusing error messages in return. E.g. Instead of getting an error message that simply tells you "There's no table named 'foo", the user will get some obscure error message about bad sql syntax from the database instead, making it harder for the user to figure out what he did wrong.
 cd "$(dirname "$0")" Don't do this: http://mywiki.wooledge.org/BashFAQ/028 Also, you're using bash, stop using [ everywhere and use the trillion-times-better [[. cd $module_location; . "bootstrap.bash" Don't do this. Or at least, combine them with &amp;&amp; so that you're sure you're sourcing the correct file. If the cd fails you could be running arbitrary commands here.
How big is the site? If you know a specific page, you could just pull down the HTML, diff it with what you have, and if anything is different, then pull down the entire thing. Without a bit more info I can't comment much more. 
Try a cron-job with Git or rsync.
Awesome, thanks for the feedback. I should get some time this week to address those issues. If you find any more things worth noting, feel free to open an issue on GitHub... Feel free to even open an issue on GitHub for this too if you'd like to! I'll probably open an issue for these within the next day or two if not.
Feel free to open an issue up on GitHub, and we'll bring a conversation about it in context of the library over there. I'll need to review the code again before I make a call about it. I do appreciate the fact that you digged into the code, regardless of the down votes your initial post got. Thanks!
I'm pretty sure wget doesn't re-download items that are already saved... I'm not sure if it does that by file name, or check the actual files for changes, but that shouldn't be too difficult to check. What was the reason behind wanting to download incrementally? As far as checking to see if the site was downloaded already, check out my revised version below. I added a line to standardize the value you're using for $URL to account for either entering "domain.com" vs "http://domain.com" as stdin, and I'd also (personally) would prefer to have the option to pass the URL in straight from the command line like: ./script.sh http://domain.com: So I added that as an option, too. (If you don't pass the URL this way, it will ask you for the URL.) #!/bin/bash [ -z $1 ] &amp;&amp; ( echo -n "Please enter the URL: " ; read URL ) URL=`echo $URL | cut -d\/ -f3` if [ -d $URL ] then echo "The website $URL has already been downloaded, would you like to download again? (y/n)" read redownload if [ $redownload == 'y' ] then wget --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains $URL --no-parent $URL else echo "Ok, I will not redownload. Exiting." fi else wget --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains $URL --no-parent $URL fi exit 0 
&gt; URL=`echo $URL | cut -d\/ -f3` What exactly happens in this line after the pipe?
I have these: # wget-download entire page and all necessary assets wget_fullpage() { wget --page-requisites --span-hosts --convert-links "$@" }; export -f wget_fullpage wget_fullpage_no_robots() { wget_fullpage -e robots=off "$@" }; export -f wget_fullpage_no_robots # wget-download and backup all navigable links from and below target wget_mirror() { wget --timestamping --recursive --level=inf --no-parent --page-requisites --convert-links --backup-converted "$@" }; export -f wget_mirror wget_mirror_no_robots() { wget_mirror -e robots=off "$@" }; export -f wget_mirror_no_robots accumulated over time by trial and error. 
There are many bugs in this script. &gt; [ -z $1 ] &amp;&amp; ( echo -n "Please enter the URL: " ; read URL ) At least three problems with this line. 1. quote the expansion of the first positional parameter to avoid word splitting and pathname expansion changing the result. Or better, test `$#` instead. `(( $# == 0 ))` 2. Don't use uppercase variable names 3. `(...)` runs the commands inside in a subshell. Any variables you set inside the subshell, dies with the subshell, so URL will never be set. &gt; URL=`echo $URL | cut -d\/ -f3` quotes again... And, here you are extracting the hostname from the url, turning `http://example.com/foobar/` into `example.com`. I doubt that is desirable. Also, bash can do [string manipulation](http://mywiki.wooledge.org/BashFAQ/100) itself, no need to use an external command. &gt; if [ -d $URL ] then quotes again. Or better, use the `[[` keyword instead (as a bonus it doesn't do word splitting and pathname expansion on unquoted expansions, so: `if [[ -d $URL ]]` &gt; echo "The website $URL has already been downloaded, would you like to download again? (y/n)" read redownload `read` can print a prompt already, and best also add -r to read by default, since the input may get mangled without it. read -r -p "Already downloaded, download again [yN]? " redownload &gt; if [ $redownload == 'y' ] then quotes again: `[ "$redownload" = y ]`, but prefer the `[[` keyword for testing strings: if [[ $redownload = [Yy] ]]; then See [faq 31](http://mywiki.wooledge.org/BashFAQ/031) for more on the difference between `[` and `[[`.
If you are new to bash Scripting, are you new to Linux? You should check the man pages. man cut 
why export?
Alright, and I didn't mean to insult by that, I just like checking that first. You know, "teach a man to fish..." 
No problem, wasn't offended. But maybe you can help me out in a more general problem: Is there a guide which explains tools like "cut" or "sed" in a more easy understandable way than the manpage. Often they are quite good, but sometimes i read them completly and dont get a word.
Why do you say: &gt; Don't use uppercase variable names I use uppercase variable and function names as a way to more easily see where variables and functions are being used within my code. Is this just a stylistic faux pas or is there a technical justification for not using uppercase variables names in bash?
I just read the cut manpage and I can see the confusion. I use cut for field delineation. E.g., CSV parsing. In the example above, the "-d" option marks the delimiter, in this case, "/" (it needs to be escaped, hence "\/"). The" - f" option marks the field that you want. If you imagine the text as an array split along the delimiter, you give it a 1-based index for the field that you want. Sed, on the other hand is used for text sculpting instead of text selection (like cut). There is an entire (pocket) book devoted to Sed/Awk and I recommend it. I would good for examples of things to do with Sed. It is very versatile and I can't cover everything here.
Special shell variables (like RANDOM, SECONDS, GROUPS, ...) and environment variables (like PATH, HOME, EDITOR, ...) are all uppercase (with one exception). Using lowercase variable names ensure that you don't accidentally override special shell variables or environment variables. For functions there's no such problem, apart from uppercase commands being very uncommon (and ugly in my opinion).
most of my functions are utils that i want to be available in subshells, scripts and other places not necessarily the interactive session so I'm used to export everything, this particular ones might not need it but it was copypasta. here's the full list: https://github.com/git2samus/rcfiles/blob/master/.functionrc
Here is one way: find . -name "*.i" -exec echo call runscript {} ./255FGE_WaterBoundary/ \; You can also remove the echo and call to have find run the script for each file.
Hm, it doesn't like the "exec" command, is it possible because I'm running on Cygwin instead of a true linux? Error: find: missing argument to `-exec' 
I'm not sure if I understand 100% what you are trying to do, but is this it? for folder in \*; do for file in ${folder}/\*.i; do echo call runscript ${file#\*/} ./${folder}/; done; done
I'm pretty much just trying to create a .txt file that organizes all this stuff; not really trying to run it all. 
You should be able to redirect the output to a file if you want to: for folder in \*; do for file in ${folder}/\*.i; do echo call runscript ${file#\*/} ./${folder}/; done; done &gt; /some_path/some_file.txt
Cygwin is really hating something about these lines, and I can't quite figure out why... I think using a Windows editor to write a bash script is screwing everything up.
What kind of output/errors are you getting? edit: here's what I get for a toy example (on Ubuntu) ~$ mkdir tmp ~$ cd tmp/ ~/tmp$ mkdir 1 2 3 ~/tmp$ touch 1/a.i 2/b.i 3/c.i 1/a.x ~/tmp$ for folder in \*; do \&gt; for file in ${folder}/\*.i; do \&gt; echo call runscript ${file#\*/} ./${folder}/; \&gt; done; done call runscript a.i ./1/ call runscript b.i ./2/ call runscript c.i ./3/ 
Alright, figured out that I had to use dos2unix converted. Now what I'm getting from your script is this: call runscript ./255FGE_GeoSingle/ ./255FGE_NCTWaterInf/ ./255FGE_Single/ ./255FGE_WaterBoundary/ ./ASX28/ So now I just need to go into the subdirectories to find the *.i files, I think
 for file in */*.i; do printf 'call runscript %s %s\n' "${file##*/}" "${file%/*}" done &gt; result.txt
qgtjvz helped me out, but thanks!
What's the error? 
I also use Cygwin but I figured you were running this on linux. For Cygwin just change "\;" to ";" and it should work. Try "man find" to see all the switches and options for the find command.
Using + is almost always preferable to using \;
If done right, why it should broke? Im doing this in empty virtual machine. I have to partition it manually because I didn't find any other way. Anyway this is temporarily till I find better way. Thanks for you answer, now it's clear how to accomplish this :)
Hey qgtjv, No need for the nested loops. find with a for loop is simpler: for i in $(find -iname "*.i" -type f); do echo "call runscript ${i##*/} ${i%/*}" | unix2dos &gt; some_file.txt; done This will print out: call runscript filename.i absolute_path_to_dir_which_filename.i_is_in
No. Please see [pitfall 1](http://mywiki.wooledge.org/BashPitfalls#pf1).
Very cool. Thanks for sharing. 
this shit is amazing, I've been using it for a while now and it truly delivers. my only pet-peeve is that when the connection gets lost and you stop the client the server process remains alive even after creating a new session, I understand that while there's no connectivity the server cannot know what it should do but when a new session gets established it should automatically clean the leftovers from the previous one. not a big problem tho.
Most likely a previous run of the command has the lock, but has hung for whatever reason. To check if this is the case, use the `fuser` command on the lock file to see what processes are using the file: fuser -v /var/run/sftp_mirror.run
&gt; fuser -v /var/run/sftp_mirror.run [root@prodftpmirror ~]# fuser -v /var/run/sftp_mirror.run USER PID ACCESS COMMAND /var/run/sftp_mirror.run: root 1423 f.... sshfs ps -ef shows that PID 1423 is running a bash script that starts sshfs user@sftp.externalsftpserver.com should I just kill 1423 and see if the next execution runs?
So it looks like the `sftp_mirror.sh` script has ended, but left an `sshfs` command running. `sshfs` has the file descriptor inherited from `flock`, so the lock remains set until that `sshfs` command dies (or otherwise closes said fd). This indicates a bug in the `sftp_mirror.sh` script, causing a child process to be unintentionally orphaned. Or it could be someone used `kill -9` on it.
Do the locks persist through reboots?
No. EDIT 1: Without knowing what the script actually does, it's hard to say something for sure, but my best guess here is that you hit a race condition. The cron job is scheduled to run every five minutes, and it at least requires network to be up, maybe other services too. If you happen to boot it at a certain time of the hour, and crond is started some time before networking is up, it may start the job, and run the sshfs command before networking is completely up, making the script fail in this manner. EDIT 2: It was not EDIT 1.
bash isn't that great at JSON. I'd go with a language/client library Google provides already, below are python examples. https://developers.google.com/blogger/docs/1.0/developers_guide_python#CreatingDraftEntries Here's how to install the client library - https://developers.google.com/blogger/docs/3.0/api-lib/python
Ah understood. I've never done JSON in bash simply because everyone has told me it's not too great (google searches). So I use python, php or js. If you're up for it make a follow up post with your solution! I'd be interested to see.
Has this ever worked? It never unmounts the sshfs mount, and the sshfs process stays around until it gets unmounted, so the lock will be kept for ages. if the sshfs mount is meant to survive, the fix will be to make sure sshfs does not inherit the fd with the lock. To do that, it's best to specify the fd number explicitly. Change the cronjob to */5 * * * * flock -n 9 /root/sftp_mirror.sh 9&gt;/var/run/sftp_mirror.run Now flock is explicitly using fd 9 instead of grabbing the first available fd number. Inside the script, close fd 9 for the sshfs command by adding `9&gt;&amp;-` to it. echo passjesusfuckingchristdontdothisword | sshfs 9&gt;&amp;- user@host:/the/dir ... for a further improvement, see [BashFAQ 69](http://mywiki.wooledge.org/BashFAQ/069)
That worked wonderfully. Thank you! Honestly, I don't know the line below never returns true. I believe the admin had attached the sftp server mount himself and then based on the schedule of the cronjob it kept the connection alive. I believe around august 15th the entity that runs the sftp server shortened the timeout and some how this line was returning true. if grep -qs $SFTP_SERVER /proc/mounts; then 
Will do.
My concern is how secure it is compared to ssh.
Gnu Screen does the disconnect and reconnect thing with plain sshd. 
I use this almost every day at work. http://www.gnu.org/software/parallel/
 #!/bin/bash exec &amp;&gt; logfile cmd1 &amp; cmd2 &amp; `exec &amp;&gt; logfile` redirects stdin and stdout to `logfile`, you know the rest. Edit: The problem will be that you can't identify which line was the output of which command (unless you make let the commands run in a subshell that also echoes the name of the command); I second the suggestion of /u/KnowsBash to use python for this, especially since you seem to want to schedule python implementations. 
Don't know why this is downvoted, it seems useful enough
Yeah. I posted it in the /r/programming/ subreddit and people really hated on it. Don't really know why. Thanks for checking it out.
I got your back man. Seems pretty useful :)
So the main feature is automatic templating? Why shouldn't I just set some autocmds in my .vimrc instead? What on earth is the point of calling this "s" for copying, moving, or deleting files?
Type `s foo`. In doing that, `s` does the following: cp $S_TEMPLATE_PATH/default $S_BIN_PATH/foo vim $S_BIN_PATH/foo if [[ "$(&lt;$S_TEMPLATE_PATH/default)" == "$(&lt;$S_BIN_PATH/foo)" ]]; then rm $S_BIN_PATH/foo fi If you want to do all that by hand, be my guest. Also, you don't have to use vim with s. And you don't have to screw around with vimscript either.
I don't know about Python integration, but I've been using a program called 'jq' to parse json info returned from YouTube. I'm pretty sure the Blogger info for json callbacks is on the spaghetti mess that is https://developers.google.com/blogger/ ... Info about jq can be found at http://stedolan.github.io/jq/ or more than likely installable in your favorite distro by installing the jq package. I hope this helps a little bit.
I have been using something similar for a long time but without using inotify. The script(s) is(are) run periodically by a service in my system called CommonCron. Let me know if you are interested in 'RunIfChanges_Main' #!/bin/bash #Written by Manuel Iglesias. glesialo@gmail.com # #RunIfChanges_Name will source RunIfChanges_Main that will run the user defined function #'CodeToRun' below. CommandName=${0##*/} echoE() { # echo to standard error. Remove leading/trailing blanks and double spaces. echo $* 1&gt;&amp;2 return 0 } #Environment variables check. BEGIN if [ -z "$COMMON_STORE_DIR" ] || [ ! -d "$COMMON_STORE_DIR" ] then echoE "$CommandName: \$COMMON_STORE_DIR not set or not a directory. Aborting." exit 78 fi #Environment variables check. END #ChangesDetectItems variable. BEGIN #List of files or directories whose change of date detects that running function CodeToRun() is needed. Metacharacters allowed. #A directory date is updated if a file/subdirectory is added/removed to/from it. #A directory date is NOT updated if a file/subdirectory is modified. #Note that Directory/* does not include .* items. ChangesDetectItems="$COMMON_STORE_DIR/UsersStore $COMMON_STORE_DIR/UsersStore/* $COMMON_STORE_DIR/UsersStore/*/* $COMMON_STORE_DIR/UsersStore/*/*/*" #ChangesDetectItems variable. END #IgnoreItems variable. BEGIN #Sublist of above list. Items' changes will be ignored. Full paths. Metacharacters allowed. #Note that Directory/* does not include .* items. IgnoreItems="$COMMON_STORE_DIR/UsersStore/manolo2 $COMMON_STORE_DIR/UsersStore/manolo2/* $COMMON_STORE_DIR/UsersStore/manolo2/*/* $COMMON_STORE_DIR/UsersStore/*/.ScratchPad $COMMON_STORE_DIR/UsersStore/*/.ScratchPad/*" #IgnoreItems variable. END #CodeToRun user defined function. BEGIN #Notes: Use 'return' instead of 'exit'; Provide progress/error messages CodeToRun() { ApplicationName="${CommandName#*_}" # Remove 'RunIfChanges_' prefix # AnotherInstanceRunningExitCode=113 # Same Variable in UDir_Main &amp; Bkp_Main if ! type $ApplicationName &amp;&gt;/dev/null then echoE "$CommandName: '$ApplicationName' not found. Aborting." return 127 fi #Add options here Command="$ApplicationName" $Command ExitCode=$? if [ $ExitCode -ne 0 ] then if [ $ExitCode -eq $AnotherInstanceRunningExitCode ] then echo "$CommandName: Another instance of '$ApplicationName' running. Aborting." return 0 fi return $ExitCode fi } #CodeToRun user defined function. END ############################################################# MainShell=${CommandName%%_*}_Main if ! type "$MainShell" &amp;&gt;/dev/null then echoE "$CommandName: Main shell script '$MainShell' not found. Aborting." exit 127 fi . $MainShell 
user is the time the CPU is spending executing your code, sys is the time the CPU is spending executing code in system calls caused by your program. So, yep, you can consider user+sys = total CPU usage. Real is the time your program is running, as measured by a wall clock, including the time the CPU isn't executing your program, like while sleep()ing or waiting for I/O. If you want more info, have a look at the topic of profiling.
Thankyou 
 table=$(( ${interface#eth} + 3 )) See [FAQ 100](http://mywiki.wooledge.org/BashFAQ/100) for more on how to do string manipulations in bash. And avoid using uppercase variable names. You risk overriding special shell variables and environment variables.
Oh man thanks! I knew it could be done. I forgot about the curly braces. 
Keep in mind that in the Linux of the future (read: Arch), interfaces won’t have predictable names like wlan0.
There are multiple ways to execute multiple commands with bash, parallel ( http://www.gnu.org/software/parallel/ ) is a great tool to execute multiple commands in parallel if the objective is to execute multiple python files and save their outputs you can simply run with this command parallel 'python {}' &lt; pythonFileList.txt &gt; output.txt where pythonFileList contains a file to execute, one file per line. If instead, you want to run all python files in a directory, you can use ls *.py | parallel 'python {}' &gt; output.txt In most cases, the parallel executes a command for each line of a file now, if you want to run each command and save them in and individual file you can use " parallel 'python {} &gt; {.}.output.txt' " instead.
it's reading $string as 2 arguments since there is a space; you need to put single quotes around $string: like grep -q '$string' if the keyword you care about is delete and want to log all lines containing it, you could grep for just 'DELETE' and append the output to the log via redirection (grep 'DELETE' &gt;&gt;logfile.log)
Cool, that is helpful. But, I still can't make it work in a loop [manbart]# eval echo {1..$x} 1 2 3 4 5 6 7 8 9 10 [manbart]# eval for y in {1..$x};do echo $y; done -bash: syntax error near unexpected token `do' [manbart]# 
ffor a loop use: y=1 while [[ $y -le $x ]] do echo -n "$y " ~~y=$(expr $y + 1 )~~ let y=$y+1 #this is simpler done
thanks again!
you aren't dumb; you just got tripped up by the order of expansions. From the man page (which I though was in the ABS guide, but I guess I can't find it): The order of expansions is: brace expansion, tilde expansion, parame‐ ter, variable and arithmetic expansion and command substitution (done in a left-to-right fashion), word splitting, and pathname expansion. `{start..stop}` is brace expansion, and it occurs before parameter expansion, so it's literally seeing `{1..$x}` and not `{1..10}`. Which sucks. Your best option if you don't want to `eval` is probably a `for (( ... ; ... ; ... )); do ... done`,
I find it silly to use a non-standard external command to do counting, which bash can do just fine by itself. for (( i=1; i&lt;=x; i++)); do printf '%d\n' "$i" done
Customize the path to your output log in the `tee` line and customize the `echo` line to include your `mail` command, etc. tail -n0 -F /private/var/log/caldavd/access.log | grep --line-buffered 'DELETE /calendars/' | tee /your/output/file.log | while IFS=' []' read -r ip _ user date_time _; do IFS=: read date time &lt;&lt;&lt;"$date_time" echo "$HOSTNAME $ip $user $date $time" done 
Good reference, thanks.
I think maybe this is what you want. I'm not sure if my syntax is correct. Man bash if you want to double check. #!/bin/bash # I am script2 ./script1 &amp;&amp; echo success ./script1 || echo failure if /path/to/script1; then echo success else echo failure fi But now I think about it, maybe you want a trap. A lot of people put a function in their script that gets called if there is ever a signal. Not the same thing I guess. Maybe it's what you want. I never really used this since my scripts were fairly simplistic and I never seemed to need it. http://stackoverflow.com/questions/64786/error-handling-in-bash
Just a quick note, but this: if [[ $(/path/to/script) -eq 0 ]]; then ... won't work the way you want. You can just do this: if /path/to/script; then ...
 oops )
Should be (oops)), obviously. 
Now do it with just bash and no external commands.
[Woop woop woop.](http://pastebin.com/7NFwp9nt)
If you are developer and want to learn some bash tricks, stay tuned! I will be publishing new ones twice a week :)
Gold star for you, sir or madam! :)
 [ -f file ] &amp;&amp; echo yay || echo nay is **not** equivalent to if [ -f file ]; then echo yay else echo nay fi With the former, `echo nay` runs if either of the first two commands fail. With the latter, it only runs if `file` is not a regular file. Further, don't use `test` or `[` in bash. Use the superior `[[ ... ]]` for testing strings and files, and `((...))` for testing numbers. See also [FAQ 31](http://mywiki.wooledge.org/BashFAQ/031)
Are you trying to run the tail as a persistent background process to search for the content? If so, I would use a pipe and a backgrounded tail process, of course you'll need a cleanup routine to maintain that backgrounded tail and the pipe, added for you in this sample: #!/bin/bash HOST=$( hostname ) LOGFILE="/private/var/log/caldavd/access.log" # 1 2 3 4 5 SEARCHSTRING="(.*) - (.*) \[(.*):(.*:.*:.*) .*\] \"DELETE (/calendars/.*) HTTP/1.1\"" export PIPE="$TMP/searchlogpipe" [[ ! -p $PIPE ]] &amp;&amp; mkfifo $PIPE exec tail -n0 -F $LOGFILE &gt;&gt; $PIPE &amp; export TAILPID=$! while read line; do if [[ $line =~ $SEARCHSTRING ]]; then ip="${BASH_REMATCH[1]}" user="${BASH_REMATCH[2]}" date="${BASH_REMATCH[3]}" time="${BASH_REMATCH[4]}" uri="${BASH_REMATCH[5]}" echo -e "$HOST, $user from $ip on $date at $time deleted $uri" | mail -s "Calendar event deleted on $HOST" foo@bar fi done&lt; $PIPE cleanup() { [ -n "$TAILPID" ] &amp;&amp; kill -TERM $TAILPID &amp;&gt;/dev/null [ -p "$PIPE" ] &amp;&amp; rm -f $PIPE [ -f "$PIPE" ] &amp;&amp; rm -f $PIPE } trap "cleanup" SIGHUP SIGINT SIGTERM trap "cleanup" EXIT ** EDIT: added regex routine to split line into components. Probably could tighten that regex up a bit, but don't feel like being too fancy. 
This is because (annoyingly) tr seems to be buffering the output before handing it to awk. I believe if you wait long enough, the output should eventually display as the buffer fills up and is flushed. Unfortunately I don't think there's a way to get tr to flush the buffer after each line. But there *is* a way to do it with sed with the -u (unbuffer) option. Give this a go: ping -t a.b.c.d | sed -u -e 's/.*time=\([0-9]\+\).*/\1/g' | awk '{ if ($1 &gt; 75) print $1 }' The regex extracts just the digits from the time, the -u forces sed to flush the buffer after each line and hand it to awk, and then awk prints out if it's more than 75.
This would filter the ping output to show any ping over 75ms: ping -t a.b.c.d | egrep 'time=([89][0-9]|7[5-9]|[0-9]{3})' Then you could pipe it to tr or sed to do any formatting options you want.
This works nicely, too! Thanks.
Just as another possible solution... ping -t a.b.c.d | awk '!($8 == "loss,") { sub(/time=/,"",$8) if ($8 &gt; 75) print $8" "$9}' awk is a pretty powerful tool if you want to learn how to use it properly - not saying I'm using it properly however. Using windows you may need to modify a few varibales.
in pure bash while IFS=' =' read -ra p do (( ${#p[@]}==9 )) || continue (( ${p[6]%ms}&gt;74 )) &amp;&amp; echo ${p[6]} done &lt; &lt;( ping ... ) use space and = as delimiters to parse lines and put stuff into p array. Proper ping reply line has 9 'words', the time value is 7th, so index=6 
There's been plenty of good suggestions on the awk front so I won't rehash it ; but instead, I'll suggest installing smokeping. It's a tool designed to monitor and alert based on latency.
*Really* close but there's a missing ';' in there between end of sub() &amp; if: ping -t a.b.c.d | awk '!($8 == "loss,") {sub(/time=/,"",$8); if ($8 &gt; 75) print $8" "$9}' Count me among those who feel there is tremendous power in awk, as it can do much, much more than just print out particular words piped out from other commands.