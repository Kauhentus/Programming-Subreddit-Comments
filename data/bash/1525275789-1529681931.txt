It’s hanging on the third line, the one with the timeout. I have no idea why though because that command runs fine by itself in the terminal. 
Try changing that line to: ```timeout -s 9 10 airodump-ng wlan0mon &amp;&gt; output.txt ``` This will hard-kill the process after 10 seconds instead of waiting for it to finish. I'm just guessing that it's hanging on the soft-kill that timeout sends by default. 
Alternatively, use `timeout -s5` to send SIGKILL only if the process didn’t finish for five seconds after the SIGTERM.
That syntax does not appear to be valid for GNU coreutils; where -s takes a signal as a variable. I have vague memories of portability issues with timeout before and am wondering if that might be the case here?
Oops, I meant `-k`, not `-s` :)
 today= date +"%-e-%b" Needs to be: today=`date +"%-e-%b"` or today=$(date +"%-e-%b") 
Well today it would work properly. Yesterday it didn’t lol. But I have gotten it all figured out now
Oh, ok. Well, here's my take on it anyway: today=$(date +"%-e-%b") mapfile -t holidayArray &lt; holidays isHoliday=false for day in "${holidayArray[@]}"; do WHEN=$(echo ${day} | cut -f 1 -d ' ') WHAT=$(echo ${day} | cut -f 2- -d ' ') if [[ "${WHEN}" == "${today}" ]]; then isHoliday=true break; fi done STR="Today is " if ${isHoliday}; then { STR+="${WHAT}" } else { STR+="not a holiday" } fi echo ${STR}
When I do $ echo "hello &gt; world" hello world Things get saved as follows for me in the history: $ history 2 116000 echo "hello world" 116001 history 2 I also extend the size of the file with HISTSIZE and HISTFILESIZE, but I don't use that HISTCONTROL variable. I'm guessing that HISTCONTROL is maybe breaking things for you?
After some more trial and error, I've narrowed it down to this part: `PROMPT_COMMAND="history -a; history -c; history -r; $PROMPT_COMMAND"` 
The `history -a` should be safe. I use that one as well. I don't use the `history -c; history -r`. Those `history -c` and `history -r` make it so all terminal windows will share their history? That part of the config will be break if you decide to remove it. History from other terminal windows will only show up in newly opened terminals.
Yeah that was the intention--to share history.
Yeah, I intentionally don't do that so that when I'm in a certain window, going backwards in the command list I'll only get the command lines that I typed into that particular window, not other ones. But the `history -a`, I still use that one. Without it, bash will only write to the history file when a terminal window closes (I think). I still want a newly opened window to be able to see everything that was typed previously in any other window.
If I keep only the `history -a`, do what I did in the OP--then open a new window--arrowing up will still yield it as two different lines.
I just tried that, and it still works for me. In the first window, I did: $ echo 'hello &gt; world' hello world And then in the second, pressing the "up" arrow key, I see this: $ echo 'hello world' And looking into the history, I see this: $ history 10 ... 116032 echo 'hello world' ... I guess I better share my whole .bashrc setting involving the history. Things look like this in there: HISTFILESIZE= HISTSIZE= HISTTIMEFORMAT="[%F %T] " # Change the file location because certain bash sessions truncate .bash_history file upon close. # http://superuser.com/questions/575479/bash-history-truncated-to-500-lines-on-each-login HISTFILE=~/.bash_eternal_history PROMPT_COMMAND="history -a${PROMPT_COMMAND:+; $PROMPT_COMMAND}" shopt -s histappend That HISTFILE=, HISTFILESIZE= is supposed to make things "infinite". The HISTTIMEFORMAT adds a timestamp to the history (I cheated earlier and removed the timestamp manually from the history output). That weird PROMPT_COMMAND line is so that I can move that section around in my .bashrc without breaking other code that also wants to add to PROMPT_COMMAND.
Another idea... maybe what you are seeing is simply a bug? I noticed the bash package used on my distro is rather new, is from this year: $ bash --version GNU bash, version 4.4.19(1)-release (x86_64-unknown-linux-gnu) Copyright (C) 2016 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software; you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. 
I tried this: HISTSIZE=50000 # big history HISTFILESIZE=50000 # big history shopt -s histappend # append to history, don't overwrite it #save and reload the history after each command finishes PROMPT_COMMAND="history -a; $PROMPT_COMMAND" and it still doesn't work when opening a new tab.
I looked into the actual history file to see how things are saved. I see the following for the earlier experiments: #1525304110 help history #1525304346 echo 'hello world' #1525304360 history 10 That `#1525304346` is probably the date+time. An idea I had was, maybe what you are seeing is simply a bug? The version of bash I have here is this one: $ bash --version GNU bash, version 4.4.19(1)-release (x86_64-unknown-linux-gnu) ...
It must be a bug. I tried yours exactly and it still doesn't work. Oh well. Thanks for all the help! You've been great.
`shuf` is only being run the one time here, you need to re-declare it, or re-declare structure your script slightly
The other day I posted [a function](https://www.reddit.com/r/bash/comments/8fmjgk/need_help_with_bash_script_that_uses_arp_to_find/dy4xjxa/) that you may find useful, and yesterday I posted [this](https://www.reddit.com/r/bash/comments/8g9ut1/trying_to_write_a_simple_script_for_holidays/dyak95k/), explaining the silencing of grep and testing on a command's return codes. Have a read of both of those posts and if they give you any ideas, feel free to try and merge them into your script and test again. Use shellcheck.net to test your scripts, and let us know how you get on!
A [change](http://tiswww.case.edu/php/chet/bash/CHANGES) for readline in bash-4.4-rc1 reads: &gt; b. If readline reads a history file that begins with `#' (or the value of the history comment character) and has enabled history timestamps, the history entries are assumed to be delimited by timestamps. This allows multi-line history entries. So try setting `HISTTIMEFORMAT`
&gt; However, if I have the JPG on the server but not my computer, it doesn't work and I get a different return: It does work. rsync is not bidirectional. It only copies from the source to the destination. If the file isn't in the source folder then there's nothing to copy.
On mobile so can't break it down for you atm, but looks like you're completely abusing the system call. Everything you're calling externally can be done in awk. Even performing actions on files other than the one you called. I'll try and rewrite some of it for you in a few when I get back to a desktop and can read what you're try to do properly. 
Setting it to what? I tried that in another comment above and it didn't work.
I don't get it. If you've got a working solution what's the problem?
Hmm well my only feedback is that I wouldn't use a script for an edit this simple, I'd just use vi. Or if I wanted to do it on multiple files I'd just wrap a sed cmd in a for loop on the command line. But it doesn't get much simpler than "s/something\\n//g"
That won't work, though. Sed by default uses \n as a line separator and will never match on a \n request in the 'simple' way like you wrote. The hacked together sed I wrote will pull out the new line character but it's a mess and difficult to maintain for someone not intimately familiar with how sed works
I am fairly positive I use that all the time and a quick google says it's valid..and I'm 100% it's valid in vi which is basically see syntax. I notice you're not escaping your Reddit comments correctly, so it's hard to follow your code and meaning precisely.. if you mean \\n then use a double escape to display correctly
I'll try to explain more here, I mean maybe it's just my version of sed but the following fails on my machine: I have a text file, test.txt: ``` Hello Goodbye Hello ``` When I run: sed 's/Goodbye\n//g' test.txt The file is unchanged. And yes, the line endings in the file are just \n and nothing more. 
It's showing up correctly on the unofficial mobile app, chrome, and the official mobile app... Yes, that's a \\n
Yes that looks right. Ok it could be my app not showing correctly. Anyway I'll try to remember to look on my desktop tomorrow
Okay; so now I've had a chance to re-read this I'd recommend a couple of changes. Firstly; I hadn't noticed you were using the system call to change an external file. I would suggest awk is not the correct tool for modifying existing files and as such your use of sed is correct. That being said; I feel the mistake here is trying to shoehorn sed into the middle of the awk statement instead of printing the desired output and passing that output to sed. There are a couple of ways you could approach this. Firstly; I'd move away from your grep usage and instead incorporate that output into a while loop: while read -r line ; do echo "${line}" | awk '{ stuff }' ; done &lt; &lt;(find . -type f -name \*.java -exec grep -i '^import' {} \;) It's even better if you move the grep into awk with: awk '/^import/ { stuff }' as you don't have to split the text to get the filename (var FILENAME in awk) and don't have to fork extra greps (this way you're just passing the filename to awk and letting awk parse the file instead of the text). We'll leave it with your method for simplicity though. I'm having a hard time understanding your output without seeing the desired output. Small tip; it's easy to understand your own code - you wrote it. By not giving an example output you force someone to have to step your pseudo-input you provided through your code which I'm too lazy to do ;) So in the above; we've got find producing a file list and passing it to grep. That text is then given as "${line}" and piped into awk. Instead of calling sed within awk we will now use another while loop: while read -r line ; do while read -r infile changes; do echo "Removing ${changes} from ${infile}" sed -i "s/${changes}//g" "${infile}" done &lt;(echo "${line}" | awk '{ stuff }') done &lt; &lt;(find . -type f -name \*.java -exec grep -i '^import' {} \;) So instead of calling sed within awk; we've had awk print our filename and the change we want to pass to sed. By using read with two variables; we've told it to store the first argument as ${infile} and the rest of the line as ${changes}. This can be seen with: :~$ while read -r infile changes ; do echo "${infile}" ; echo "${changes}" ; done &lt; &lt;(echo "myfilename the text I want to have changed in sed") myfilename the text I want to have changed in sed I see you've also got a counter in there with grep. As I read it; you're checking to see if there's more than one instance of it. grep -c makes the wc -l redundant ; but we can do it in awk completely. Especially as we're only printing now instead of performing the actions and we're working with one file at a time. I've actually run out of time for the moment and in trying to give you a short explanation I realised it's a bit more of a complex one to understand; but essentially - associative arrays. You parse the input until you have your import name in a variable and then do array[importname]++ to increment it by one. Then at the end you loop over the array and if the value is only 1 then you print the file+changes so sed can do its job. Hope this helps; fire away on any Qs and I'll look back later. 
http://sed.sourceforge.net/sedfaq5.html#s5.10
Thanks for the link - that's exactly the link I used for my original script which you can see in the OP
So it is ; that's what I get for being lazy :)
Thanks for the comments! I wasn't aware I should shoehorn the grep into awk like that, as you can tell I'm not particularly experienced here. I'm at work right now but I'll see if I can try some of those out on my lunch break and get back to you
That actually seems perfect - what does the /d mean? I just ran it and it looks like it does exactly what I want
It deletes any line that matches the /regex/. I always recommend a read through http://www.grymoire.com/Unix/Sed.html (and there's a version for awk too).
Not sure how big OP's files are, but while read loops are extremely slow on large inputs. It's always possible to avoid it completely with a little workflow structuring. For smaller files, it's an easy way to go about it though.
We are not going to do your homework for you. You will need to provide a gist of what you've done so far, and then we can help guide you. Also some assignments are really dumb, this one is a great real world example.
Here's a [tutorial](https://dustymabe.com/2013/05/17/easy-getopt-for-a-bash-script/) for command line arguments. If you're not aware, bash scripting is really straight foward because you can just type into a script what you would type on the command line. If I want to make a directory named dir1 in a script, I can literally just type `mkdir dir1` in a file, lets call it script1.sh, then run that with `bash script1.sh`. The only other thing you need is to add some of this functionality inside `if` statements that depend on the output of a previous command. The two ways to do this are exit codes and parsing command output. Exit codes the value the previous command returned to indicate success (exit code 0) or failure (everthing != 0) with the variable ?, so access it with `$?`. Depending on how you do number 3 you can store the last commands exit code to conditionally create that group. The point here though is just google how to do these things on the command line, which should be easier. Once you know that you can just type them into a bash script. 
I'll do your homework for $100USD.
Well, uh, rsync does not unpack tar archives the last time I checked. You can rsync the tar archive files as files. But they will stay intact and unchanged. If you want to combine tar archives, you will have to unpack them yourself into a directory. Then re-tar the contents of that directory and delete the directory.
That GZIP environment file will only make rsync more efficient. 
$100 for 5-10 min of work? I don't want to know what you charge by the hour.
That worked! Thank you!! I can't believe that's all it was... :-|
Derp. I knew that! Promise... Thanks. :-)
Ok someone has posted an alternative method, but I'm still pretty sure you've just got an escaping issue in the original. If you double- or triple escape the \\n it may just work, or you can think through exactly how it's being passed down to figure out the correct escaping. Donald Libe's book on Expect is what helped me fully understand this as it's all about automation and interactivity with users and the shell.
Would something along these lines work? for file in *; do if [[ -f "${file}" ]]; then mkdir -p "${file:0:2}" mv "${file}" "${file:0:2}" fi done
No. There are batches of four.
 $ for i in `seq -w 0 4 64` do echo mv ${i}* $i/. done
Wow Is seq like a for loop stack or something?
Its a separate program that generates sequences of numbers. Other programming languages have these built in. Bash may too, but I dont know the syntax if it does. Anyway, its convenient. And it can go backwards -- decrement.
`{0..64..4}` does the same thing in bash.
Thanks!
It's not really a case of shoehorning grep into awk; but that the grep is redundant. It's like doing 'cat file | grep term' - you can just do 'grep term file'. In the same sense; grep is just doing pattern matching. You're doing all the work in awk so why bother with the grep. Just use '/\^input/' in your awk to only bring in those lines. Also as I mentioned before ; if you pipe the output of find straight into awk then you are parsing a file rather than the echo'd content from the grep. This means you can use the internal FILENAME awk variable instead of doing all that splitting: # echo oneline &gt; testfile ; awk '{ print FILENAME }' testfile testfile Let me know how you got on with the rest of it. 
so my tar line is incorrect? what do i need to remove?
Possibly because you are entering "new file" as the filename and 'new' and 'file' are not files that exist in your example directory? How about we back up a few steps, and pose the question 'What are you trying to do?' and 'Why are you making dated archives of a specific file?'
just curious if it could be done, and so i wrote up that to see and im just having a little problem with it
I don't think there's going to be any straightforward way to achieve what you're asking. mpv doesn't look to accept any external requests to play/pause, etc via signal or the like. As I understand your script; you're loading a random video from your folder, starting at between 5 and 35 seconds in - playing it for 5 seconds then killing it and starting another. Another way to achieve similar results might be to parse a playlist and then load that. You can use '&lt;', '&gt;' or 'enter' to skip to the next track and left/right arrow to seek back/forwards by 5 seconds. Alternatively you can use button4/button5 on your mouse. If your goal here is to look over a large number of files then using a playlist, 3-4 right-arrows to hop you into the 15-20s mark followed by enter to skip to the next one makes a usable workflow.
Please explain what your script is supposed to do, instead of asking they determine that from your code, having to read up on mpv's options. A less useless post title would be nice too. I suggest you see if you can avoid ending mpv and control it via its IPC capabilities.
Replying to my own comment to keep it tidy.. So I did find one possibility: xdotool key XF86AudioPlay The above command simulates pressing a media play/pause button on your keyboard. You could indeed launch 2 processes and have the other behind it paused. When you kill one; the next one starts pre-paused behind the first - check out wmctrl to manage window focus.
You shouldn't do it like this. You can use your private/public key. Store your public key on the server and you can login without any password to the server with the private key. 
Didn't you say it's SSH? Can you login via SSH in the command line? 
Yeah I can SSH my username and the address, then it prompts me for a password. But it’s a large company and I assumed only admins could actually save stuff to the server, I’m not sure how I can move a key from my private computer to the server, if it’s possible? 
Look at `ssh-copy-id`. Even though it doesn't sound like SSH on the command line what you are describing there. 
I’ll check that out thank you. What I input is “ ssh user@abcompany” and then I’m prompted for my server password. I’m sorry I’m not much help, I’m not very knowledgeable about SSH or really even coding. 
Oh also I didn’t create the key/account, the admins created my account and all I create is a server password for it. I hope that might clear things up a bit.
You create the public key and private key on your laptop. You use ssh-copy-id to upload your public key to the server while your private key remains on the laptop. Then, when you ssh, the keys will magically do their thing and let you in without a password. You don't have to worry about "storing stuff" on the server because this is not like storing data on the server - its a built in function of ssh and the server will automatically do the right thing and put the key in the right place (as long as its configured too, which it probably 100 percent is) Your IT dept could probably help you if you asked them "can you help me set up ssh public key with on Server X", but you can also do it yourself. There are many tutorials out there..." Ssh passwordless with" "ssh public key" etc... https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2
Hey thank you so much, I wasn’t quite sure how it all went together. I’ll upload the key and see how it goes! So just to be sure what exactly would I type to ssh-copy-id the public key into the terminal? 
You would just type what is in step 3, but change the things in red to your username and server. You don't do step 4...that would be up to the sysadmins of the server so don't worry about it. Also, in step 1, when it prompts you for a passphrase - choose a passphrase! (That is NOT your ssh password BTW). The reason is, if I got a hold of your private key, I could ssh around your company *as you* because you didn't protect the key with a passphrase. If it had a passphrase, It would t matter if I stole it from you because I couldn't use it without the password. So, you will still have a password to type with public keys, but you will only have to do it once per session. (Think once day instead of each time you connect). This is more secure because you won't have to type in you password on the command line or in scripts if you want to automate things like rsync or scp. 
Awesome thank you so much! I’ll give it a go tomorrow at work.
You shouldn't be allowed on a server which requires SSH access in my opinion.
Hey thanks for the opinion. It’s to a GUI we use for work I can’t change anything on it. Also, we all start somewhere, my job doesn’t require any type of coding, it’s provided for us. I just try learning different jobs around me during downtime on the job. Simply trying to get my password to work without typing it each time, nothing huge. Thanks. 
There's Google, "how to not put in my password ssh". Tadaaa lot's of free information! 
&gt; But it’s a large company and I assumed only admins could actually save stuff to the server, So that you're aware, by default every user in a unix-like system has full read and write permissions for their home directory (like My Documents in Windows), they just can't touch other people's stuff. Your company would have to put in extra effort to make this not the case.
That’s good to know, I’m very new with Linux and bash so I thought this would be a good place to ask the question. I tried googling but most of my results were for more experienced people. Thank you very much!
Scp your key to it. 
Give the elitist mentality up you absolute douche. "You shouldn't be allowed to drive if you don't know the number of teeth on the 5th gear in a car" We are all learning and trying to get by here in life. Maybe you should rethink the way you approach people.
Oh hey looks it's the smug asshole that has nothing to contribute other than say "google it". Why he bothers posting at all, the world may never know. 
Yep even if you don't want to do the thing with the keys. There is an option Take a look at sshpass With it you can automatically input the password 
I agree that's a far better way to go. However, I was in a similar position, and didn't have the permissions to set up keys.
&gt;but i dont want the log to be attached just in plain text You forgot to tell what you actually want.
thanks for the reply I would like the log to be send to the email in plain text like this https://pastebin.com/4pinf1bT this is the outcome of the restic.log the issue is that when i put the mail -r "&lt;myemail@mydomain.com&gt;" myemail@mydomain.com -s "Backup Successful" &lt; /media/restic.log i get an email just saying this May 05 2018 15:36:34 -05: restic.sh finished im going on a limb and say its only sending outside of the echo
Thanks for the reply, the issue with tee -a is that it appends the log and when it runs again it appends that old log also. Maybe would i need to add to rm to delete that log after it gets emailed?
Simply making the *first* tee run without -a would suffice.
thank that did the trick
It would literally take a one-liner from your sysadmin to create a home directory for you and drop your public key. Are they worried that .authorized_keys is going to fill up the drive?
[removed]
You're right that OP can give a bit more informations but you don't need to be condescend.
Yeah I don’t have a shell, I was wondering about that but I thought I would be able to find it after all of the info. So with only a TUI I can’t copy the public key over?
Use a password-less ssh key
Some distributions give default 022 umask, which gives users read privileges to other user's directories. In my opinion, this is an insecure default.
I go in to work tonight and I’ll try getting it on the server, fingers crossed! Thank you very much for the info! 
Look here for basic Linux guide http://linux-training.be
I’m not sure, they deal with hundreds of accounts and I’ve never spoken to them, I honestly don’t even know where they are in the building lol. 
So could I write a script to login and then add sshpass “password” into the script? Then when executed it logs me right in?
Your workstation is on Linux? 
Yeah it’s Linux. 
So let's say your password is 12345 and the host you need to connect to is 10.10.10.10 you should go to your .bashrc and add an alias . alias sssh="sshpass -p 12345 ssh root@10.10.10.10" then to apply the change do a "source .bashrc" then to login , just type sssh in your console 
That looks like a really easy fix, I’ll try this if I can’t get the ssh on the server since it’s kinda weird and I’m unsure how the admins have it running. Thank you for explaining it and giving me the info! 
Yes, you're using the wrong brackets. Well, wrong\-ish. Either: if [ "$wireless_mac" == "xx:xx:xx:xx:xx:xx" ] Or: if [[ $wireless_mac == "xx:xx:xx:xx:xx:xx" ]] If you use single brackets and you want to compare strings, you have to put the variable in quotes. If you use double brackets, you don't.
Bash splits the lines of text it reads into "words". Spaces are very important to control how that "word splitting" is done. There are only very few special characters in the syntax that work without spaces, for example `;`, `|`, `&amp;`. What's happening here for you is that for that test you are trying to do, bash sees this single word here: $wireless_mac=="xx:xx:xx:xx:xx:xx" After it interprets the `$wireless_mac` and the `"` characters, it will turn it into a word like this: xx:xx:xx:xx:xx:xx==xx:xx:xx:xx:xx:xx But what you want is for it see three words, like this: 1. `xx:xx:xx:xx:xx:xx` 2. `==` 3. `xx:xx:xx:xx:xx:xx` To make it do that, you have to add space characters between the different words, meaning turn your code into this here: if [ "$wireless_mac" == "xx:xx:xx:xx:xx:xx" ] Check out a tool named "shellcheck". Your distro probably has a package for it. It tries to find mistakes that are very often made in bash scripts. You can also try it online without installing it at www.shellcheck.net. For your problem here, it would have found the missing space characters around the `==`.
Your answer indirectly helped me. I had already attempted the first method \(with the double\-quotes\) and it didn't work. So I tried your double\-bracket method \(which I didn't know about, so thank you for bringing that to my attention!\) and that still didn't work. But adding a space between the values and the "==" was the answer \(for some reason?\). I'm not sure why, but I thought in bash there couldn't be a space there. Turns out going from `"$wireless_mac"=="xx:xx:xx:xx:xx:xx"` to `"$wireless_mac" == "xx:xx:xx:xx:xx:xx"` was the answer. Proof CS is mostly witchcraft. Thanks for the help!
I don't know why but for some reason I was positive that bash actually required it to have no spaces \(something which was bothering me because it makes for ugly code\). That definitely ended up being the answer. Thanks for the in depth explanation \(this is why this community rocks!\) 
Yeah, the strange part about bash is assigning a value to a variable. When you do that, you are forbidden from adding spaces. I mean, the following would be wrong: x = 3 With those spaces surrounding the `=`, bash thinks you are calling a command named `x` with two parameters `=` and `3`.
Missing colon: getopts :i:a:
Oh my !#@$^@#$^#$ god, that was the answer. Thank you. Several tutorials do not do that. Any idea why, or how those tutorials possibly work without the colon separator? 
Have you got an example of such a tutorial?
https://stackoverflow.com/questions/11279423/bash-getopts-with-multiple-and-mandatory-options The answer with 73 upvotes.
If you look closely then you will notice that options that take an argument are preceded with a colon, while flags (options without arguments) don't: &gt; options=':ij:rRvh' &gt; while getopts $options option &gt; do &gt; case $option in &gt; i ) i_func;; &gt; j ) j_arg=$OPTARG;; &gt; r ) rflag=true; small_r=true;; &gt; R ) rflag=true; big_r=true;; &gt; v ) v_func; other_func;; &gt; h ) usage; exit;; &gt; \? ) echo "Unknown option: -$OPTARG" &gt;&amp;2; exit 1;; &gt; : ) echo "Missing option argument for -$OPTARG" &gt;&amp;2; exit 1;; &gt; * ) echo "Unimplemented option: -$OPTARG" &gt;&amp;2; exit 1;; &gt; esac &gt; done See &gt; options=':ij:rRvh' and then compare &gt; j ) j_arg=$OPTARG;; and &gt; R ) rflag=true; big_r=true;;
Very awesome explanation! I was thinking "colon comes after important thing" which I didn't see. I never considered "colon comes before," which seems to be the case. Thank you so much for explaining that!
Sorry my english sucks: &gt; options that take an argument are ~~preceded with a colon~~ followed by a colon
/r/linuxquestions or /r/linux4noobs 
I’ve removed it now.
Thanks!
I feel too lazy to look at this diligently right now, but usually weird differences with crontab are because of $PATH being set differently in its environment. I think it's possible to add a line like this to /etc/crontab to the top of /etc/crontab to fix that: PATH=/usr/local/bin:/usr/bin That would work for the setup on my distro here.
If you want a pure bash solution to argument parsing, you can check out the code that I've written here : https://pastebin.com/pwvPbcR2 
This tool is a godsend and has saved me countless millennia troubleshooting / writing getopts statements from scratch http://getoptgenerator.dafuer.es/project/new
I would use fold to break the string into 1 character lines, use shuf to get a random order of those lines, recombine all of the randomized lines into one line, and redirect to a file. loop as needed.
Of course this is possible... the question is: why? What are you *actually* wanting to achieve here?
Not robust at all, but it does what you want: &gt; #!/usr/bin/env perl &gt; use strict; &gt; &gt; use List::Util qw(shuffle); &gt; &gt; if (@ARGV != 2) { &gt; die "2 params needed... first is a string of characters, second is the length of the target string\n"; &gt; } &gt; &gt; (my $characters, my $length) = @ARGV; &gt; &gt; my @in = split '', $characters; &gt; my @out; &gt; &gt; while (@out &lt; $length) { &gt; @out = (@out, @in); &gt; } &gt; &gt; @out = shuffle(@out[0..--$length]); &gt; &gt; print join('', @out) . "\n"; Should need nothing but a basic perl interpreter on your system...
gen_text(){ tr -dc "$1" &lt; /dev/urandom | head -c "$2"; printf "\n"; }
Thanks. It works as intended. I save this as a text file, then feed it to "morse", a morse code trainer. $ gen_text(){ tr -dc "$1" &lt; /dev/urandom | head -c "$2"; printf "\n"; } $ gen_text 'abcdefghijklmnopqrstuvwxyz.=1234' 500 &gt; Downloads/cw.txt $ morse -r -s -d -w 16 -R 5 -B -f 670 -T &lt; Downloads/cw.txt 'morse' has a built-in option to select characters to send from a string; eg: $ morse -r -s -d -w 16 -R 5 -B -f 670 -C 'abcdefghijklmnopqrstuvwxyz.=1234' but for some reason it doesn't seem to be random enough. 
Thanks. This is another valid function.
No problem. Feel free to remove the trailing `printf` if you don't need it.
Instead of `fold | head -n | paste`, why not just `head -c` ?
That's a good idea. Thanks!
`head -c` is not portable and I work on systems that specifically don't have it. `fold | head -n | paste` should work on everything with maybe the exception of HP-UX.
&gt;I need help creating a script that takes a users **AD username** as a parameter and sets them up as Linux users with sFTP an Samba access. The best way to do this is to authenticate your Linux hosts against AD. Is there a reason you're not doing this?
See this post: https://stackoverflow.com/questions/169511/how-do-i-iterate-over-a-range-of-numbers-defined-by-variables-in-bash &gt; the {a..b} Bash notation works only with literals The following will get around the issue. for i in $(eval echo "{$start..$end"}); do echo $i; done 
thanks!
Rather handy. But note that's using "getopt", not "getopts" (which is an inbuilt). Nicer thing with getopt - support for longopts - but its not portable. 
Turn this one-liner into a script: S="abcdefghijklmnopqrstuvwxyz.=1234"; l=${#S}; for((i=0;i&lt;=500;i++)){ r=$[RANDOM%l]; printf ${S:$r:1}; }
Without googling it, I wonder if you could set a function for `$PROMPT_COMMAND`... Roughly something like this in your `.bashrc`: PROMPT_COMMAND="cls-ls" cls-ls() { if [[ -z "$@" ]]; then clear ls fi } 
Thanks! I'll wait.
Yeah, on second thoughts that approach isn't going to work... because every time you call `cls-ls()` it's not going to have any args, so whether or not you run a command, `cls-ls()` will be invoked. That's obnoxious... it would make for a great prank though.
When you press Enter, there's no line added to the history. I'd try looking at the "DEBUG" trap and see what you can access from there. I have a feeling this kind of idea is more something for zsh instead of bash. I remember I've seen someone show how to hook into the literal command line that was typed by the user with zsh.
I used `bind '"\C-m": "\C-l\C-j"'` in .bashrc to make enter clear screen.
I'm sorry for my ignorance, but how can I adapt this binding to another command?
It's strange that you got the `/f` error considering that there was no `\f` in the command. Maybe you mistyped something?
No, I just copied it.
Maybe you don't have the default key bindings, but that setting simply executes ctrl-l (clear-screen) and ctrl-j (newline-and-indent) when you press Enter (ctrl-m).
Those are emacs style bindings, he said he's using vi-mode.
Yes, that's the reason why it didn't work. Thanks!
You should configure syslogging instead be it `rsyslog` or something else. Then you can simply `grep` or `tail` or whatever on your central syslog server(s). If you absolutely need a pretty interface, invest some time into setting up Graylog. Don't forget that `logrotate` is important too...
Thank for the suggestions. I don't really need logrotate. We simply discard the logs after every sprint. But rsyslog sounds like what I need. Could you (or someone else) ELI5 that for me? Where do I configure rsyslog? On remote machine? And then it will stream it to my local machine via TCP?
personally, I would first split the name = value pairs onto separate lines with something like tr "," "\n" then use sed to match and replace on JUST the "param1 = " line You can then re-wrap the lines back onto one with tr "\n" ","
This is also good. Thanks.
I'd do it like this: $ cat testfile param1 = 1, param2 = 2, param3 = 3 $ sed -r 's/(param1)\s*=([^,]*|$)/\1 = hello/' testfile param1 = hello, param2 = 2, param3 = 3 $ sed -r 's/(param3)\s*=([^,]*|$)/\1 = hello/' testfile param1 = 1, param2 = 2, param3 = hello This needs to be changed if a parameter can have a comma in its value. I mean the rule won't work on something like this: param1 = "hello, world!", param2 = ...
If you have spacing like that, probably with `awk` : name=param1 # or whatever val=3 awk '$1 == '"$name"' { $3 = '"$val"' }' &lt;testfile note the somewhat confusing quoting, where `name` and `val` are interpreted but the field numbers aren't.
Strange times we live in.
Pretty much, yes. You configure rsyslog on the systems generating the log messages to send to a remote rsyslog server (usually over UDP/514). The remote syslog server needs to be configured to listen for incoming traffic on the same protocol/port the log generators are using and could be your dev machine, but I would strongly recommend using a static IP. Once those two pieces have been connected you can view the log messages like any normal log file on your local machine. The cool thing about rsyslog (and SyslogNG) is it can break the log streams from different hosts into separate files, or from different apps (service/severity) into separate log files, or both! For instance, app1 and app2 on server1 sends logs to remote1 as services “local0” and “local1”. Then app3 and app4 on server2 send logs to remote1 as “local0” and “local1” (because app1/3 and app2/4 are the same processes, eg Postgres and CustomFoo). In syslog, a “facility” is basically a service/app and a severity is a class of message (error, info, critical, debug, etc). You can slice and dice on the log server receiving the logs based on those classifications in the log messages. Now on the remote1 receiver, you could send everything from both servers to /var/log/remote/server1.log and /var/log/remote/server2.log which would combine the logs from both apps. Or, maybe you want to combine the apps from the servers into /var/log/remote/Postgres.log and /var/log/remote/CustomFoo.log. Lastly, if you wanted you could even do something like this; /var/log/remote/server1/Postgres.log, /var/log/remote/server1/CustomFoo.log, rice and repeat for server2. A few caveats: UDP has a maximum packet size, and therefore log messages being sent over UDP will be truncated if they exceed the packet size. You can work around this by using TCP and rsyslog supports this AFAIK. Unless you’re sending java stack traces to your logs or similarly huge messages, I wouldn’t bother. UDP works. UDP has no guaranteed delivery. If a message is lost in transit, STBY. If your network is well designed this shouldn’t be a problem. If your logs are transiting the Internet or UDP VPN trunks (OpenVPN etc) or the Internet then you may want to consider TCP. Some apps generate a truckload of logging data so maybe look at your existing log volumes and make sure your network engineers are not going to get their cranky-pants on ;) 
Try this: for filename in put-directory-here/*.wav; do oggname="${filename%.wav}.ogg" if [[ ! -f $oggname ]]; then # at this point, the .ogg version does not exist fi done Put the commands you want to run somewhere in the middle there where you see the `# ...` comment line. Experiment by using "echo" to print $filename and $oggname to see if the script really does what you want. If you want to do a recursive search through all sub-directories in a certain location, you can do this: shopt -s globstar # this enables the '**' wildcard for filename in directory-here/**/*.wav; do # ... done
Agreed, this is the way to go. Reference for how this works (parenthesis capture): http://www.grymoire.com/Unix/Sed.html#uh-4 
Thank you so much for the explanation. We are fairly small organization, so we don't actually have separate log server for our dev environments. We only do that for production apps. Buy I will see if I can convince dev-ops to create one for our internal servers. If not, I will have to stream those logs to my local machine.
All good :) Glad I could shed some light on the dark art of logging. One of the things I really like about rsyslog is the extremely high quality documentation they maintain: https://www.rsyslog.com/guides/ That's probably a good place to start. As for centralised logging, that would be a good first step. Be wary though, once you have all your logs being managed centrally, it will only be a matter of time before someone wants to do some deeper analysis/search and send you down the elasticsearch (and similar) rabbit hole! It's a fun project but non-trivial to get up and running sensibly (ie, so it's not a massive human and compute/storage sinkhole!). Centralised logging and log management/search/etc is almost a discipline in itself these days. However, like basically everything, if you have some spare cash, you can pay someone else to solve your problem too (IaaS et al).
xargs is your friend try ls |xargs -I % formatCommand % Something to that extent sorry for formatting I’m on mobile! 
[cut](https://www.gnu.org/software/coreutils/manual/html_node/cut-invocation.html#cut-invocation) and [rev](http://manpages.ubuntu.com/manpages/bionic/en/man1/rev.1.html) can be used to split a filename. For example, `echo "apacheds-protocol-dhcp-1.5.4.jar" | rev | cut -d- -f2- | rev` would return `apacheds-protocol-dhcp`. Explanation: Because the length of the tokens is unknown, you have to first reverse the string giving `raj.4.5.1-pchd-locotorp-sdehcapa`. Now, you want to use `cut` to split the string by `-` (read the string, whenever you encounter a `-` copy everything before into a new string, return this array in fields) using `-d-` and get everything from the second field onwards using `-f2-`, giving `pchd-locotorp-sdehcapa`. Finally, reverse the string again using `rev` to get the human readable output, giving our `apacheds-protocol-dhcp`.
Thanks for the response. I assume I would do it something like this? How would I check if it exists in the other directory afterwards and then compare its version number? for file in directory_1/*.jar; do md5=$(md5sum &lt; "$file") base=$(basename "$file") for dir in directory_2 directory_3; do file2="$dir/$base" file2 | rev | cut -d- -f2- | rev [[ -f "$file2" &amp;&amp; $md5 != $(md5sum &lt; "$file2") ]] &amp;&amp; echo "changed: $file2" done done 
There's a fair amount of echo/subshell indirection going on here. It's probably better to use an array, like command=(zenity --title="Package Choices" --width=400 [...]) selection=$("${command[@]}")
looks like '|' delimiter in zenity checklist. `IFS='|' command=`
Test the output in a shell or smaller script but it looks right to me. (test, test, test) 
To start with, `commands=$(echo foo bar ...)` can be simplified to `commands='foo bar ...'`, but this doesn't solve your problem. You clearly understand the concept that you can put quotes around something to make Bash interpret it as a single argument rather than split it up into multiple arguments based on whitespace. However, I think you are misunderstanding exactly *when* that happens. Quotes only have that effect when they are typed directly as part of the command. They **do not** have that effect when they show up as the result of a variable being expanded. So, even though `$commands` contains exactly what you would type into the shell, when it is *expanded from a variable* the quotes don't mean anything special to Bash. Bash will split up the result of the variable expansion based on whitespace, treating the quotes no differently from any regular character. The best solution to this is to use an array to store each argument as a separate element. Arrays have a special syntax to expand each element as a separate argument: `"$foo[@]"`. An array is defined by putting the elements inside of parentheses (e.g., `foo=(a b c)`); /u/thestoicattack already gave a good example of how to do this in your case.
thanks a lot
If you look at the code I posted in my other comment, I wrote an argument parser in pure bash that supports long options, short options, and compressed option, no need for getop, getopts, or anything else.
Look at the man page for find. find $certaindirectory -mmin 10
Perfect, thank you.
be sure to copy a couple of the files to an empty directory first so you can test this without risking your original files for file in *.mp4; do ffmpeg -i "/home/i1/i1/Videos/${file}" -codec copy -map 0 -f segment -segment_time 15 -segment_format_options movflags=+faststart "${file/.mp4/}"%03d.mp4; done
tnx - it works!
Ah thank you! I will try that right now!!
Thank you for the deeper explanation, this make a lot more sense. Now that I know what my problem is, I just need to figure out how to use array... 
per `man timeout` If the command times out, and --preserve-status is not set, then exit with status 124. Otherwise, exit with the status of COMMAND. So something like timeout 15s bash -c waitformonitor if [[ $? = 124 ]]; then exit $?; fi do_the_rest
I tested it, your solution works for me.
The `[...]` at the end is just a placeholder for the rest of your command. command=(zenity --title="Package Choices" --width=400 --height=400 --text="Choose item:" --list --column="Selected" --column="Package" --checklist False Test1 False Test2) 
Using a simple AWK script as a "commit-msg"-hook could be a solution: #!/bin/sh awk 'NR == 1 &amp;&amp; length($0) &gt; 50 || NR == 2 &amp;&amp; !/^$/ { exit 1}' \ "$1"
Test: this is code this is more code
 line 7: ${echo "$dir/$base" | rev | cut \-d\- \-f2\- | rev}: bad substitution This is the error I'm getting
Test the output in a shell or smaller script but it looks right to me. (test, test, test) 
I tested it and it gave me that error, I believe that means I'm using a non bash feature? 
`$(echo "$dir/$base" | rev | cut -d- -f2- | rev)`
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
It's a standard Linux command, what distro are you on?
Could you please show an example of how to expand the array "as the argument array for each function"? I'm not sure I know what that means. What would the array look like? Thanks. 
Here's an experiment at the command line, trying to show how it works: $ x=(a b c) $ foo() { echo first parameter: "$1"; echo second: "$2"; echo third: "$3"; } $ foo "${x[@]}" first parameter: a second: b third: c When you write your function, to make things easier to read in the future, you could first save those $1, $2, etc. in variables with good names. Like this: foo() { local name="$1" local url="$2" local filename="$3" ... } I guess you could also just completely ditch your arrays. After you have functions like this, you can then do: run_installation \ ACROBAT \ "http://localweb/cc-2018-mac/mac-acrobatdc-spr18.zip" \ acrobat.zip \ mac-acrobatdc-spr18 \ mac-acrobatdc-spr18_Install.pkg If you have functions that you only call from one single spot in your script, perhaps don't create that function. Just put the code directly into the spot where you call the function. I mean your get_installer, unzip_..., etc., you only call them once from run_installation. You could just put all of that code directly into run_installation. That will help if you decide to use parameters to pass things around.
Something else I just remembered: There's a neat tool named "shellcheck". You should try it out. It tries to find mistakes that are really easy to make with bash. It for example helps find missing `"` quotes that would break a script if there's a space in a filename. I think you can install it with that "homebrew" thing for OSX, or you can also try it online at www.shellcheck.net.
You@YourComp:~$ ssh-keygen You@YourComp:~$ scp ~/.ssh/id_rsa.pub srv_user@ip.addr.of.svr: You@YourComp:~$ ssh srv_user@ip.addr.of.svr svr_user@ip.addr.of.svr:~$ cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; rm id_rsa.pub
I tested this idea and it's the solution I was looking for. Why do the index values \+1 from what they are in my snippet? I'm going to try to see how all of this will integrate with the menu I have for this program and let you know how it goes. Thanks!
https://www.commandlinefu.com/commands/browse someone in r/linux (i think) mentioned this the other day. It's pretty neat.
Wow. Thanks. This is a great start. Cheers.
That script is `df` .
&gt;When you write your function, to make things easier to read in the future, you could first save those $1, $2, etc. in variables with good names. "When you write your function, to make things easier to read in the future, you could first save those $1, $2, etc. in variables with good names. " Great suggestion. Thanks!
I compiled a good list of bash functions that has a great tools for exactly what you're looking for plus a bunch of other cool stuff. Just download the [bash_functions.cfg](https://raw.githubusercontent.com/dr-strangelove9/.bashrc/master/bash_functions.cfg) and source it in your .bashrc and restart bash. 
Quite handy. :) I'll definitely snag some of these.
Hi, I see why you replied this. I'll edit my OP for clarity. Thanks.
 sl for the muthfukin win!
Long ago I started working on a package manager for bash (a la pip, npm, gem, etc.) so that not only could you install libraries for things, but could install all the appropriate deps with one command and keep things up to date easily. But I've gradually moved away from doing much programming outside of work, so it (and many other things) fell by the wayside. I still think it would be incredibly useful though.
Some useful examples here too.. http://www.tldp.org/LDP/abs/html/
I put mine on github.
 df -l | grep ' 9[0-9]% ' Call that from `crontab`. If it emits anything you get an email. If nothing matches, it stays quiet.
Where's the source code?
Monit, check it out.
You're not asking the right question, and you need to provide more details about the exact inputs and outputs you're working with, and how you're trying to put them together.
I think generally you might be looking for *file[1-100].ext*
 for i in file*.ext; do sh $i; done
Your best bet is likely to pipe the filenames from step 1 to step 2 via [xargs](http://manpages.ubuntu.com/manpages/artful/man1/xargs.1.html) which will call a command with the stdin as the arguments. If you're trying to e.g. run it as `step1; step2 file_*.ext` it won't work because bash expansion happens before commands are run. Alternatively you can have step 1 write the names to a file and step 2 read that file.
have you tried .*.ext$ instead? if this is a regex match then file*. would only match zero or more of the character e. 
Very nice. I gotta ask lol. What if it hits 100%?
Actually, all my files are in the same directory except the bash script. I call it from my bash scripts directory. I have to run that script for multiple directories, hence I'd like to avoid having a copy of the bash script in every directory.
The problem is that I have other files that ends with the extension .img, which is why I use volume_*.img in my bash script.
okay buy try: &gt;volume_.*.img$ instead.
It would stay quiet at 100%. It's the system's way of accepting its fate after being ignored on the way from 90 to 99%. :D
Your comment is unreadable. Please escape your * characters.
OK, but I still think the question you need to be asking is "why isn't globbing working here" not "what are the alternatives to globbing" here. You will have to troubleshoot some type of bash-y issues using find, xargs, or regular expressions as recommended by the other commenters.
if step one is in the script, why not simply assign that output to a variable and use the variable? Or perhaps I'm just not understanding the problem.
[removed]
^The linked tweet was tweeted by [@helpermethod](https://twitter.com/helpermethod) on May 14, 2018 16:52:15 UTC (0 Retweets | 0 Favorites) ------------------------------------------------- Tipp of the day: Add [[ TRACE ]] &amp;amp;&amp;amp; set -x to the beginning of your Bash script. Allows you to call you script with debugging enabled like so: $ TRACE=1 ./my-script.sh ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
You're missing a dollar, this statement is always true
Why the braces?
Both are not needed in that case.
The braces are used to make sure bash knows that the variable name is only TRACE. In this case it's not essential, but it's a much better habbit to get into when referencing variables. It is actually nessecary when doing something like var='hello' helloworld='testing' echo "${hello}world" # outputs helloworld echo "$helloworld" # outputs testing
Try here: github.com/terminalforlife It's my GitHub on which there are lots of programs and lil scripts I've written. Check out https://github.com/terminalforlife/i3config in particular. You may wish to use insit to make life easier.
Try `dfc`. it is a wrapper around df to give colorized output.
You’re missing a `$` in front of `fileName` in the `head` command, but apart from that your code works for me…
I fixed the missing symbol, and now I think the head command is throwing an error - "invalid number of lines". This isn't an echo that I've placed in the script myself so I'm not sure what's wrong with my input - I tried with every number up to 9. The file I'm opening has 10 lines.
have a look at [liquidprompt](https://github.com/nojhan/liquidprompt). It does what you need and a lot more. Either you gt to like liquid-promt and just use it, or you dig into it, get your answer and learn a lot :) 
Alright, you’re welcome :)
Wow, that looks fun! I'll take a closer look at this, thanks!
Oh yes, it is :) it's also quite hackable for more and more stuff :)
Honestly.. I think it's too much. It's definitely neat but I had to disable most of the features and still can't get it quite right.. I think I was very close with the much more simplified script, I'd rather come at it from that direction. I really do appreciate the recommendation though.
For future troubleshooting, you can use `bash -x` to make it easier to see exactly what's going on.
I use something like this: parse_git_branch() { git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/ (\1)/' } export PS1="\u@\h \[\033[32m\]\w\[\033[33m\]\$(parse_git_branch)\[\033[00m\] $ "
That sounds like zsh. Check out Oh-my-zsh https://github.com/robbyrussell/oh-my-zsh
That's the same one from the link in the original post :)
Yeah, I'm going through it and gutting out all the extra functionality I don't think I need. Hopefully it works out ok!
Err.. I'm guessing you've tried what actually comes with Git? https://github.com/git/git/blob/master/contrib/completion/git-prompt.sh
Well I figured it out after using echo to check what the output was of $ARG_Label and it was "Movies" when I downloaded a movie. so the above if else statement worked after doing the filebot command. Figured it out myself!
Nope, I had no idea this existed! I'll take a look, thanks!
I know you've put the "solved" tag on, but I just want to make sure about something: When you do this here: [ $ARG_LABEL="Movies" ] This is a mistake because of missing spaces around the `=`. What's really evil about the mistake is, it's a kind of mistake where you will not get an error message. When you test your script, it will behave like you want so you'll think everything is okay. Here's a demonstration at the command line, trying to show what's going wrong: $ x=abc $ if [ $x="Movies" ]; then echo true; else echo false; fi true $ if [ $x = "Movies" ]; then echo true; else echo false; fi false Check out a neat tool named `shellcheck`. Your distro probably has a package for it. It tries to find mistakes in bash scripts. It's super helpful because it really easy to make mistakes in bash.
Ohhh okay thats really good to know. I didnt know if the spaces mattered or not but thats goo to know. Im going to check out shellcheck now! Thanks for the info!
Nevermind, I managed to fix it!
Look at `getopts`
unless you're referencing a different getopts, I don't think that's the one I'm looking for. I don't want an internal arg parser, I want to use something that parses any command's args for me. e.g. I want to run a long aws command so I start typing it out: `aws ec...` What I'm looking for should take my `aws ec` and autocomplete with a scrollable menu of possible options (ec2, ecs, ...) that I can select to further build the command
Also to reproduce the effect of the `&amp;&amp;` in the original you should do one of the following: filebot -script fn:amc --output "$CONFIG_OUTPUT" --action duplicate --conflict skip -non-strict --log-file amc.log --def unsorted=y music=y artwork=y excludeList=".excludes" ut_dir="$ARG_PATH" ut_kind="multi" ut_title="$ARG_NAME" ut_label="$ARG_LABEL" || exit $? or if filebot -script fn:amc --output "$CONFIG_OUTPUT" --action duplicate --conflict skip -non-strict --log-file amc.log --def unsorted=y music=y artwork=y excludeList=".excludes" ut_dir="$ARG_PATH" ut_kind="multi" ut_title="$ARG_NAME" ut_label="$ARG_LABEL" ; then if [ $ARG_LABEL="Movies" ]; then curl localhost:32400/library/sections/1/refresh?X-Plex-Token=L4P7pg5okx1xxxxxxxx else curl localhost:32400/library/sections/2/refresh?X-Plex-Token=L4P7pg5okx1xxxxxxxx fi
doh. I guess I should have read more closely rather than just copying from my bashrc
Ahh okay cool thats some useful information! Thank you!
Ah, that's not a parser. What your looking for is a custom autocomplete. That's going to be command specific. If you are thinking aws specific, are you talking about [aws-shell](https://github.com/awslabs/aws-shell).
ahh I see, nice thanks for the link! While that's not the one I remember seeing, I do use aws-cli pretty regularly and this will be a big help
Nevermind I figured it out. It's just comparing x to x and shifting the flags down each time. I haven't seen this before and I think it's pretty clever. 
Not sure what OS and/or Git package you're using (from you're OP, it's seems not Windows) - but those scripts should be available to you as part of your packaging - you can just source them into a `.profile` / `.bashrc` then make use of them via PS1: https://github.com/magnetikonline/dotfiles/blob/fdb3a98f40ce1372a4e10c4f4e90b5052bd43ca3/.bashrcmagnetik#L13-L15 The `(__git_ps1 " \[\033[00;95m\](%s)\[\033[00m\]")` is the function that emits branch details/etc.
or: echo -n 'text you want to prepend' | cat - path/to/file &gt; path/to/file
Yeah, pretty easy to replace with printf though.
http://lmgtfy.com/?q=bash+project+ideas Snarkiness aside, check this reddit discussion for some ideas. https://www.reddit.com/r/linux/comments/4gqogr/any_ideas_for_a_beginner_bash_shell_project/ Have fun and good luck.
So I looked up what the &amp;&amp; and how I understand it is its to do two separate commands in the shell. Is that right or is there more to it? 
You could just lurk around here and in other subreddits like /r/linuxquestions and your distro's subreddit. Sometimes people have problems that involve bash, and you could read about their problems for inspiration and could help if you know what to do. An example I can remember right now, someone used an NVIDIA card and complained it got really loud while gaming. The person explained in Windows he'd use a third party program to configure a custom fan-curve with slower than normal speed increase for higher temperatures. In Linux you can only set a fixed speed manually in NVIDIA's settings tool, but you can't have a different fan-curve than what the manufacturer has set up in the card's BIOS. The solution is to write a script to do it. About what to do in the script, to get the current temperature, there's this shell command: nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader --id=0 It prints results like this if you run it from the command line, which is in degrees Celsius: $ nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader --id=0 31 Then about how to set a fan speed, a script will first have to enable manual control like this: nvidia-settings -a GPUFanControlState=1 And afterwards it can set a fan speed like this: nvidia-settings -a GPUTargetFanSpeed=55 This would set a 55% speed for the card's fan. If you'd decide to help with this, to make the work more useful you'd put the commands into functions, perhaps like so: read_temperature() { nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader --id=0 } set_speed() { nvidia-settings -a GPUFanControlState=1 nvidia-settings -a GPUTargetFanSpeed="$1" } Adapting a script to something else would just need changes in the two functions, so reusing the work for both AMD and NVIDIA or maybe something completely different would be easy.
Write a script to back up important files, use curl and web APIs to add backups for some important web data, create a .tar.xz.gpg file, and upload it via WebDAV (or some API like Dropbox). Also have it rotate out old backups (e.g. keep the last 3). Write a script to update packages that aren't in your distro's repositories. Like: git clone, checkout the stable branch, configure, make, make install. Or fetch a binary download, extract it to a temp dir and copy files around. Bonus points for adding uninstall options. One good (and easier) starting point would be downloading AppImage files and making them executable, since many apps now go that route instead of distributing deb/rpm files or hosting a repo. Those are my two biggest bash projects... Still relatively small to start, but I've added features and additional stuff to back up / update over the years.
Won't this clobber the file with the redirection before cat?
The first one is interesting. Thank you
According to [ShellCheck](https://www.shellcheck.net/), it does ``` Line 1: echo -n 'text you want to prepend' | cat - path/to/file &gt; path/to/file ^-- SC2148: Tips depend on target shell and yours is unknown. Add a shebang. ^-- SC2094: Make sure not to read and write the same file in the same pipeline. ^-- SC2094: Make sure not to read and write the same file in the same pipeline. ``` 
Here's a list of ideas : \- custom prompt \(PS1\) \- library to emulate object\-oriented \- dotfile manager \- static site generator \(bonus: use fswatch to autogenerate change on file save\)
When you do this: first &amp;&amp; second The first command will run. If it exits without reporting an error, then the second command runs afterwards. If it exits with an error, the second will not run. It's a bit like the following: if first; then second; fi The `&amp;` is something totally different. It's more related to the `;` or `|` in bash. When you do the following: first &amp; second The two commands will be started at the same time. The first one runs in the background. The script continues with the next line of your script after the second command is done. The first command might still be running at that point. You can also write it like this: first &amp; second This is all a bit similar to what `|` does, as it also starts two commands at the same time (but wires their input/output together), and you can also use it like this over two lines if you want to: first | second 
Wow! Thanks for the detailed explanation. That really clears things up for me! I like how the &amp;&amp; works, thats good to know. But I do any one more question about the code corrections you have in a previous comment. Currently the only change I made from my original solution was to add a space in between the equal sign and "Movies". Everything seems to work. But now that you explained what &amp;&amp; does I want that function so if the first command doesnt work then it wont do the if statement. In your post with an example of how that would work you have if and then the filebot command. Is this so it the command works then it will do the next if statement for the curl commands? Also why is it important to have a semi colon for if [ $ARG_LABEL= "Movies" ]**;** and there is also one after the if filebot command **;** but that one has a space after the quotes. 
I like `killall` simple way of killing processes by name
The semicolon there is because bash wants things for 'if' like this: if command then command fi I like to put the 'then' on the same line as the 'if'. That's where the ';' comes from. It's what bash wants to be able to put two lines together. I don't quite understand what you want with &amp;&amp; and 'if', but I guess I just explain something you might not have understood quite right. The 'if' is supposed to look like this: if command Now about that command, when you do that `[ ... ]` thing, that is really a command named `test`. You can read about it with: help test help '[' help '[[' # this one is the bash improvement for [ This means instead of: if [ bla = bla ]; then command; fi You can also do: [ bla = bla ] &amp;&amp; command And it's the same. Maybe that helps?
One of my exams at the university was about writing little Bash projects. I have a zip with ~100 different projects. Cool right? I'm sorry but they are all in Italian.
thx bro
Doesn't the -f "force" option override "interactive" which provides a prompt. These two don't seem to make sense to me. Nice effort though, I certainly don't fault you for that.
Yeah I think from how I understand it the &amp;&amp; makes it so if command1 fails then command2 wont be executed. So I believe with the code that ralfwolf provided it does an if statement for the first command, filebot, and I think if for some reason that fails then it wont do the curl commands in the following if statement. I just updated my script now and Im about to test it and check that it works.
You've already gotten an answer what it does but I'll give you a bit of background on why. The `&amp;&amp;` is a logical AND operator based on the return code of the commands that are run before and after. In the case of bash a return code of 0 is considered true. The reason it behaves as a virtual *if* statement is a standard logic shortcut employed by all modern languages which is logical short circuit. What this means is if I've got a logical stament of: condition1 AND condition2 AND condition3 All conditions have to be true for the total expression to be true. Conversely if any of the the conditions are false then the expression is false regardless of what the remaining conditions are. So, that means if condition1 is false, then there is no reason to evaluate condition2 or condition3. This is an optimization because it is entirely possible to evaluate all three conditions regardless and then evaluate the the whole expression and the end result would be the same. This means that using `&amp;&amp;` as a success conditional if is an assumption that this optimization exists because if it didn't all the commands would have been executed regardless. I don't mean to say that this is bad form, it's very common to use the `&amp;&amp;` in this way in bash scripts. Now note that my suggested command actually uses the `||` which has the inverse effect: condition1 || condition2 || condition3 If condition1 is false then condition2 would be evaluated/executed and if condition2 is true, then condition3 never gets evaluated/executed. This makes the `||` operation effectively an *else* statement. Now for fun (actually quite useful) let's look at a full *if-then-else* construct using just operators: condition1 &amp;&amp; condition2 || condition3 Would translate **sort of** to: if condition1; then condition2 else condition3 fi I say sort of because there is one notable difference in that in the shortcut with just operators, if condition1 is true but condition2 is false, then the else would be executed. That is not the case with the normal *if-then-else* construct. This is why it's important to understand why it's actually behaving this way so you know what the pitfalls are. I use the shorthand with both `&amp;&amp;` and `||` when I know that the second command will never return non 0 or is just an `exit`. For example: [ $# -lt 1 ] &amp;&amp; exit 1 || argument1=$1 
pkill -f is not for force though : -f Match against full argument lists. The default is to match against process names. 
Ahh, ok. Well Today I Learned. :)
Please edit so that your examples are correctly formatted. Add 4 spaces before each line will treat it as code.
Actually, that is pretty cool. Could you do us a solid favour and write out a list of what they are? In Italian, even?
That helps readability quite a bit, thanks.
You are right in that I didn't want the 4th and 3rd line - I want the 3rd line and then the 2nd. I'll give it a run and see what happens. Thanks!!
No, but that looks insanely useful for bash scripts
There's a few things that can be improved here IMHO, but otherwise - great work. Constructive feedback: The `function` word is unnecessary and not portable. Horizontal space can be at a premium when you least expect it, so try to keep within 80 columns of width. Two-space indenting can help. Avoid UPPERCASE variables. UPPERCASE is for global/environment/shell variables. In other languages that might be referred to as "tainting the global scope" or "colliding with the global namespace" or a similar mash of those words. In other languages, it is best practice to avoid those things, so this is a best practice that we can inherit into our shell scripting practices. Backticks were superseded in the early 80's, use `$()` instead. This has advantages like nesting and readability. Your use of `seq` invokes an external command when `bash` can deal with this natively. One way is through `eval`. In this example, I'll declare a function that takes one option. In the absence of that option, it will default to 10 (i.e. `${1:-10}`). ▓▒░$ intseq() { eval echo {1..${1:-10}}; } ▓▒░$ intseq 1 2 3 4 5 6 7 8 9 10 ▓▒░$ intseq 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 But you should try to avoid `eval` - repeat after me: `eval` is evil. The safer way is to use a C-style loop e.g. `for (( i=0; i&lt;$1; i++ )); do` So far, I'd rewrite this: function c_up() { for i in `seq 1 $1`; do printf '\033[1A' done; } More like this: c_up() { for (( i=0; i&lt;$1; i++ )); do printf '\033[1A' done } Moving on, use `printf` over `echo`. `while [ $PROGRESS_BAR_VALUE -lt `expr $PROGRESS_BAR_LENGTH + 1` ]; do` Could be expressed something like: `while (( progBarVal &lt; ( progBarLen + 1) )); do` Here we're replacing `eval` with a builtin approach, because we're only dealing with integers. We also get rid of those backticks and UPPERCASE variables. Finally, `sleep 0.2` is not portable. Try `sleep 0.2 2&gt;/dev/null || sleep 1`
 grep -A1 NEGOTIATED aa |tr "\n" " " GZZ9634I WANGREA1 CONN 0231CCBC LU SELY23F NEGOTIATED SSLV1 IP..PORT: 172.25.47.11..57863 cat aa cat aa GZZ9634I WANGREA1 CONN 0231CCBC LU SELY23F NEGOTIATED SSLV1 IP..PORT: 172.25.47.11..57863
 nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader --id=0 This is exactly why I sub here. For little nuggets like these. Here's a couple that I have. (If you know a better way, please let me know.) ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/') NUM_CORES=$(cat /proc/cpuinfo | grep -c "core id") CPU_MODEL=$(cat /proc/cpuinfo | grep -m1 "model name" | cut -f 2 -d ':') CPU_GOVERNOR=$(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor) #NOTE: Assumes all cores set to same governor. NUM_MEMORY_STICKS=$(sudo dmidecode --type 17 | grep "Memory Device" --after-context=16 | grep " *Size:" | grep -vc "No Module Installed") MEM_STICK_SIZE=$(sudo dmidecode --type 17 | grep "Memory Device" --after-context=16 | grep " *Size:" | grep -vm1 "No Module Installed"| awk '{print $(NF-1),$NF}') MEM_TOTAL="${NUM_MEMORY_STICKS} x ${MEM_STICK_SIZE}" 
That's pretty slick and skinny. Thanks!
TY!
I'd second checking out tput man page, however documentation is quite poor, read terminfo man page at the same time. Really great advice on the rest. Talking about portability I'll just add that {1..10}, c-style loops, and arithmetic without a dollar are ksh/bash (and probably zsh) extensions, so not working on dash and other older or POSIX strict shells.
My absolute first program would RIP a CD to WAV, fetch tags from the web, convert WAV to flac and attach tags with metaflac. This was what I needed to do with my 200+ CDs.. My tip: identify an itch and scratch it. Bonus if you don't use online resources but only what's available (man, apropos).
This is brilliant, looking forward to your updates.
&gt; The function word is unnecessary and not portable. Do you have more information (source) on that please? :)
You should prefer `#!/usr/bin/env bash` over `#!/bin/bash`. Some OSes (OpenBSD, FreeBSD) don't install bash in the bin folder but in `/usr/local/bin`. Same goes for OSX users who installed a newer verion of bash with homebrew or other package manager like it [1]. [1] https://stackoverflow.com/questions/21612980/why-is-usr-bin-env-bash-superior-to-bin-bash 
What's to say? If you try this in a more POSIX strict shell, or a shell that's a little bit 'speshul': function myfunc() { echo "blah" } It won't work. myfunc () { echo "blah" } Will work. [The `bash` manual](https://www.gnu.org/software/bash/manual/html_node/Shell-Functions.html) describes the reserved word as optional. Ditto for [the Google Style Guide.](https://google.github.io/styleguide/shell.xml#Function_Names) The POSIX standard [does not define its use.](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_09_05) Greg's wiki lists it as [a bashism.](https://mywiki.wooledge.org/Bashism) As does the Ubuntu wiki's [dashasbinsh page.](https://wiki.ubuntu.com/DashAsBinSh#function) The bash-hackers wiki also has opinions: http://wiki.bash-hackers.org/syntax/basicgrammar#shell_function_definitions http://wiki.bash-hackers.org/scripting/obsolete?s[]=function http://wiki.bash-hackers.org/scripting/style?s[]=function#function_definitions And more reading is just a web search away.
You should cross post this to /r/ffmpeg too.
Thanks for the links.
You can do logic "and", "or" with find's rules, and you group using parentheses. If you do `man find`, you can jump to the section that discusses it by typing: /^ *OPERATORS
Maybe use find's "printf" rule to print something that you can then use to count with awk or perl.
&gt; How would I structure a 'find' command such that I can count the number of files, the number of directories, the number of symbolic links, the number of graphics files (jpg/bmp/png), and some other stuff, all in one go? That would only really be feasible by having `find` exec a shell that does all this work for you. Here's a start: find your_dir -exec sh -c ' ndir=0 nlnk=0 for f do test -d "$f" &amp;&amp; ndir=$((ndir+1)) test -L "$f" &amp;&amp; nlnk=$((nlnk+1)) done echo "dirs: $ndir" echo "links: $nlnk" ' dummy {} + Note: * Everything between the single quotes is one single argument to `sh -c`, i.e. the script to execute. * I used`+` at the end instead of `\;`. This causes `find` to pass multiple arguments to `sh` at the same time, which is way more efficient but does necessitate a `for` loop in the script executed by `sh` to loop through all the arguments. * The `dummy` argument becomes `$0` in the `sh` script; this is needed so that the first file name would not be stored in `$0`, causing it to be skipped by the `for` loop (which starts at `$1`). 
Yes, it's a homework assignment. Sorry, I thought I was being obvious about it. I'm aware of using -o to combine multiple searches, like (*.xml -o *.txt), but I can't figure out how to get multiple outputs from the same command. I want find to, say, append a count of directories find ~/ -type d | wc -l | cat &gt;&gt; outputs.txt and then *within the same find command*, additionally append the total number of files in all directories find ~/ | wc -l | cat &gt;&gt; output.txt I just don't know how to combine those, and I feel like I'm missing something so simple that I should just bail. 
(This is for a homework assignment, by the way.) I was able to come up with this solution (in theory), but unfortunately, spawning child shells that perform their own finds is specifically disallowed because it's still multiple passes through the filesystem. :( Thanks for contributing!
Your second find command would actually find all nodes (files, directories, symlinks). Also you don't need the final cat. Ok, now think of the find as a giant *if* statement and all of the sections are conditions. Default operator is `-a` (AND) but if you specify `-o` then it will be an or.. so to break down the following find command: find . -type d -exec command1 {} \; -o type f -exec command2 {} \; Each node in the search will be matched with each condition in sequence just like in an *if* statement. If the node is a directory, find will execute `command1` and since the OR logic is already true, the second part doesn't get evaluated because the final answer will be true regardless. Now let's say the next node is a file, well it will fail the `-type d` check so it won't bother running the `-exec command1` because logical AND short circuits on false. But since the next part is an OR operation, it has to evaluate the second half of the find on that same node. The file node would pass the `-type f` check and then the `-exec command2` would be run. Now it's up to you what the command is or even if you want to use `-exec` or if you want to use something else to produce an output that can be used to provide counts of what you need.
I would create a meta-program. A program that gives you an example of every programming construct you know and intend to use. So you never have to lookup anything, you type your program and ask it to show you for loop syntax, or while loop, if then else, or case. And it displays it. The various string manipulation formats, The here-document syntaxes, with and without quotes. Write a program that will save you from having to lookup language features...and in the process you will learn those features. 
&gt; But you should try to avoid eval - repeat after me: eval is evil. Is it always avoidable? There have been times that I didn't know a better way to handle something. A simple example, is I don't know a more reliable way to get the directory of the user as root. # eval echo ~$(logname) There are many other times that `eval` seemed to be the only way to solve an issue I had. I try to avoid it, but if I don't always know a better way.
&gt; Is it always avoidable? There have been times that I didn't know a better way to handle something. I'm pretty much the same. My view is that you should try to avoid it, and if you have to use it, just make sure you're in control of anything that goes near it. The whole "`eval` is evil" thing is more of a rule of thumb for beginners, and an occasional mantra for more experienced scripters to remind themselves that they should try to rethink their approach. I've skimmed it, but this seems like a thorough answer as well: https://stackoverflow.com/questions/17529220/why-should-eval-be-avoided-in-bash-and-what-should-i-use-instead To your example, however, do you mean to get the home directory of the user? If so, maybe something like this: getent passwd $(logname) | awk -F ':' '{print $6}' I did consider a single `awk` invocation (i.e. parsing `/etc/passwd`), but this one should work in the scenario that you're dealing with a network authenticated user. 
Good idea, thank you for this!
Not sure if this is a weird way to do it or not, but this here would work: { read number read -r link } &lt; &lt;(python3 draft.py) This will only work in a `#!/bin/bash` script. It will not work (on for example Debian) if it's a `#!/bin/sh` script. Look through `help read` for documentation about the `-r` parameter. Here's an experiment at the command line to demonstrate that it does work: $ echo hello; echo hi hello hi $ { read one; read two; } &lt; &lt;(echo hello; echo hi;) $ echo "$one" hello $ echo "$two" hi You can also turn things around and use a pipe, like so: python3 draft.py | { read one read two echo "$one" echo "$two" } But the problem with that is that the variables will be gone after the `}` line. That's because of `|` spawning a sub-shell for the right side of the `|`, and the variables will only exist inside the environment of that sub-shell.
This will work, assuming your \(OP, not parent\) script outputs the number and the link on two lines and the IFS variable hasn't been modified to something weird.
[removed]
Try stdbuf var=$(stdbuf -i 0 -o 0 -e python3 draft.py)
This is a really Linux/Unix question, since the answer depends on your OS, not your shell. You will need to use the `mount` command. [Here](https://askubuntu.com/questions/37767/how-to-access-a-usb-flash-drive-from-the-terminal) is an explanation of what to do. It’s from an Ubuntu specific site, but the answer is the same for any Linux distro (probably for BSD distros too).
Because field 2 is the nothingness between "/" and "/" in "//".
What is your delimiter? -d is showing empty on your example.
Field numbering starts with 1. If `/` is the field delimiter, then the fields are: 1. `href="` 2. `` (an empty string) 3. `www.cisco.com` 4. etc.
And this is a very basic question that any decent search engine will find the answer to. "Mount USB drive &lt;insert distro name&gt;"
I guess I try to avoid relying on projects I must build myself favoring pre-packaged versions of them (for work and personal). Obviously if there is no prepackaged version or you need the latest version then you are looking at doing what you have. I tend to automate a much as possible. I've written many scripts like the one you've shared and my experience has been--most often I spend more time on the script than I save from reruning this kind of script--and even minor changes in the project can cause my script to fail at some point. I hate putting a lot of time into automated something only for it to fail for someone else because of some seemingly innocuous difference in their environment. All that being said, and especially in a work environment, I automate anything that will have to be done more than two or three times. Others can share in maintaining it when it needs to get updated. But usually they don't. If this saves you time then it's good. If it saves others time, even better. I can tell you it won't save me time as I don't use a Debian based OS.
Sounds kinda like you reinvented BSD ports? From a sysadmin point of view: The problem with source-built installations is that they're hard to track. I might inherit a fleet of servers while onboarding a client, and I have no guarantee that certain services on certain hosts were installed using the package manager. It might have been cute for a previous sysadmin to custom install some random .tar.gz version of whatever-software for whatever-reason, but if it's not documented, then that's a potential security hole. So when I run some as-built scripts to audit the state of the machine, my package database dump won't capture the source-built stuff. Great. The best approach is to just package everything where possible, and tools like [fpm](https://github.com/jordansissel/fpm) and/or [OBS](https://build.opensuse.org/) help significantly.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/jordansissel/fpm) - Previous text "fpm" [Here is link number 2](https://build.opensuse.org/) - Previous text "OBS" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Try: `sed -i 's|^vm.swappiness=5|vm.swappiness=0|'`
&gt;sed \-i 's|\^vm.swappiness=5|vm.swappiness=0|g' /etc/sysctl.d/99\-sysctl.conf that works for =5, but doesn't change other values. example : if it's 2/3/4/etc. any idea how I can fix that thanks
Try: `sed -i 's|^vm.swappiness=.*|vm.swappiness=0|g' /etc/sysctl.d/99-sysctl.conf` 
If there is something before `vm`, use `^.*vm`
&gt;sed \-i 's|\^vm.swappiness=.\*|vm.swappiness=0|' /etc/sysctl.d/99\-sysctl.conf thanks!
I'd do it like this: sed -i '/vm\.swappiness/ s/.*/vm.swappiness = 0/' filename That first `/.../` part is an "address" that targets a line. For the lines that match, the `s/.*/.../` command will run.
 :g/^vm.swappiness/normal Cvm.swappiness=0
Ahh, sorry my delimiter is in fact "/". That makes sense. So there is an imaginary space between the two //. Gotcha
My apologies, it's "/"
To be honest, I'm 100&amp;#37; sure I understand your goals; however, I do recommend [pureline](https://github.com/chris-marsh/pureline) as it's written in bash itself. My last computer was extremely sluggish, so I hacked together [sparse\-line](https://github.com/paxperscientiam/sparse-line). It doesn't do as much as has limitations related to folder depth, but has some utility. I recommend trying pureline out.
Google stderr and stdout then try again.
Instead of putting ` | tee -a /media/backupusb2.log` at the end of every command, consider piping the entire shell's stdout to tee by putting this at the start: ``` exec &amp;&gt; &gt;(tee -a "/media/backupusb2.log") ``` 
gparted has a built-in screen shot tool?
In the future, use something like Ansible ;) 
It looks like your `if` statement isn't formatted properly. The statement needs to be enclosed in square brackets and executed within a `$()`. I'd suggest using something like this:
Or you could use a text utility instead (gdisk -l /dev/sda), depending on the usage...
When you create a bootable gparted removable drive, there is an icon that says *screen capture*.
what happens when you click it?
do `chmod +x ./run-script.sh` most likely it doesn't have run permissions
Hmm maybe its saving it to some other location. Sorry I don't have much more.
I checked every folder on the removable drive, nothing existed. 
chmod +x or maybe missing/wrong #! ?
Could be that you are not in the same directory as the run-script.sh script. The ```./run-script.sh``` is saying run the run-script.sh from my current directory. The ```bash run-script.sh``` is saying run the run-script.sh looking first in my current directory and if not found look in in each directory of my PATH. And running just ```run-script.sh``` would only look in your PATH.
The `\s` means "space character" in the regex pattern rules that sed supports. This `\s` code will trigger for spaces, tabs, and one or two other strange space-like characters.
The sed command accepts what are called regular expressions (or regex). Regular expressions are a way to do complicated string matching operations. In your example, `\s\s*` is called the *search pattern.* `\s` is an example of something called a *character class* in regular expressions. Character classes can be thought of as shortcuts or aliases for groups of characters. `\s` represents the whitespace characters Space and Tab. So when sed is matching and replacing `\s`, it's matching and replacing any occurrences of spaces or tabs. There are other character classes like `\d` which matches all digits 0-9, and `\w` which matches any alphanumeric character. The second part of your search pattern is the `*` character. In regex, the `*` character is a *modifier.* Modifiers change the matching behavior of the immediately preceding characters in the search pattern. `*` means '0 or more' of the preceding match. So the correct way to read `\s\s*` is: `[one whitespace character][0 or more whitespace characters]` This is a kind of roundabout way of matching any occurrence of 1 or more whitespace characters. There's another modifier, `+`, that accomplishes the same thing. So your search pattern can be rewritten as: `\s+` Now, you were correct in assuming that this sed command is replacing characters with *nothing*. In this case, the command is replacing whitespace with nothing, i.e., it's stripping out all the whitespace. Regular expressions are a broad topic and it takes lots of people a lot of head scratching before they fully get the hang of it. Thankfully, there's a mountain of resources online specifically to help you learn and build regex search patterns.
Do echo "PATH" and check if you have . on the path string. Normally, when . is not on PATH you have that error. Good luck
Thanks for helping out. I see, so there's more going on then I realized. It does give me a bit of a headache ;\-\) So the point of this command, is to weed out multiple whitespaces \(if present\) and replace them with a single whitespace, because the cut program \(in this example\) uses a single empty space \( \-d ' '\) to distinguish separate columns? .
Did the filesystem get mounted `noexec`?
This is false. - Using ./ is equivalent of giving a full path, so the $PATH environment variable is not used. - Having “.” in $PATH is a *very* bad idea.
Thanks for the recommendation. Don't mind my comment. Always learning :-)
Thanks again \^\^ I'm almost done with a, pretty basic, Linux tutorial. Should probably read some more on regular expressions and scripting after that. I'm starting to see how the command line can be a very powerful tool, once you understand the basics.
Additional comments: * What /u/Seref15 said about the contents of character classes is not *entirely* accurate. It's true that `\s` includes space and tab, but it also includes other whitespace characters like line feed and carriage return, and in non-POSIX locales it can include all kinds of other stuff as well (like non-breaking spaces). Similar thing for `\d` -- in non-POSIX locales it may include other digits besides `[0-9]` (such as `٣` and `٤`). It depends on the implementation, though. * Many tools don't support Perl-style shorthands like `\s` and `\d`. For example, GNU `sed` doesn't support `\d`, and the Mac version of `sed` doesn't support `\s` *or* `\d` (so your command will actually fail on that system). The more portable equivalents are `[[:space:]]` and `[[:digit:]]`. * To use the `+` quantifier in `sed` you need to either pass the `-E` (extended syntax) option or escape it like `\+`. * As mentioned, you can do this with `tr` instead, and it was kind of made for exactly this. `tr -s '[:space:]' ' '` will collapse all runs of consecutive whitespace into a single space. Neither one is really better than the other, though (they're about the same in terms of performance and readability IMO). * Parsing the output of `ls` is generally discouraged, though in this case it's probably fine. You could also do this with `stat` and GNU `find`.
Thank you very much! :\)
Shit after 20+ years using `sed` I just learned something new. You **can** use alternate delimiters on address patterns. All you have to do is escape the first use of the delimiter like so: sed -i "\|$sub|c $txt" myfile.txt 
Thanks for the feedback, the point about logging installations is indeed good. I will implement it.
Thanks, that gives me an idea of what to do.
&gt;if \[ $\(mount | egrep \-c '\^/media/usb'\) \-gt 0 \]; then echo yeselseecho nofi Thank for the reply, currently using systemd to automount the usb when connected. I was trying to change the concept this is what i have but im not getting email alert #!/bin/sh # check if usb is mounted mount | grep systemd\-1 | grep '/media/usb ' \&gt; Backups USB=\`cat Backups | sed '/\^\\s\*$/d' | wc \-l\` if \[ $USB \-eq 0 \] then echo \-e "Subject: USB not mounted $USB" | mail \-r "\&lt;email@mydomain.com\&gt;" email@mydomain.com \-s "USB NOT CONNECTED" exit 1; fi
Implemented: https://github.com/UtkarshVerma/installer-scripts/commit/38b1b9e3ef675d44f767294b3f4fdd50e9c30d2a :)
I'd try taking a step back and would first take a good look at where those files come from. The script that creates them and the two directories, there might be something that you can do there to make your current problem easier to solve, or maybe completely avoid it.
Does that `sftp` line work when executed separately?
....and that was it. Thank you very much. I knew it was something stupid.
Unfortunately I can't change that. I need the script to function as I mentioned in the post because the files names were named by other people and also have other dependencies. 
I get a syntax error when I try to send an email the way you're written it. Do you have any entries in your /var/log/maillog? I would also suggest trying this to see if it works / delivers: echo "testing" | mail -s "Test Email" email@yourdomain.dom This would give you a log entry similar to this:
No problem. Glad it worked. We've all been there where we stare at the same code for hours and can't see the missing space or semi colon or something.
You may find ShellCheck useful. It automatically detects this and other stupid issues.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
The first line should be `#!/bin/bash`. I'm guessing you are running ubuntu because you even with the missing `!` it would not have produced this error on most other distros.
Stop dissin Ubuntu. Works for me on Ubuntu 18 with the missing !. I would guess Bash on Windows.
You know what you're right if I run the above script with `dash script.sh` I get the same error. Also, I'd never heard of dash so there's that. 
I don't have access to an ubuntu box right now but what does it show if you type: readlink -f /bin/sh Just curious.
Your script is being executed by `sh`, instead of `bash`. The thing is that your script's filename ends in .sh, but is missing the correct shebang that makes it be executed by `bash`. You can either fix the first line of your script, or change the filename extension from .sh to .bash.
 (d(){ ls -- "$1" | sed -rn 's/^(.*-.*)-([0-9]*\.[0-9]*)$/\1\t\2/p'; }; join -t$'\t' -j1 &lt;(d dir_1) &lt;(d dir_2) | awk -F'\t' '$2""!=$3') It assumes flat directories.
I would suggest modernizing things and at least use rsync... 
To clarify, `&lt;&lt;-EOF` (with the dash) here-documents allow for indentation, including on the closing line, but the indents *must be tabs, not spaces*, or they are not ignored.
Thanks for the reply, yeah your right sorry for the bad codding. im using ssmtp rather then postfix much easier for just an smtp 
Perhaps something like while read -r; do set -- "${REPLY}" printf '%s\n' "${REPLY} $(nslookup "${3}" | grep name)" done &lt; your.textfile 
`cat file.txt | awk '{ print "echo \""$1,$2,$3,"\$(nslookup",$3")\"" }' | bash `
Sorry, I should've mentioned that. Thanks for clarifying.
Please format your post correctly, reddit supports a flavor of markdown for formatting.
It seems like the `Test` in your variable is getting processed by some other part of the script. Have you tried it with single quotes, or with the first variable as something other than a word with `Test` in it?
If you're just trying to concatenate two strings, you shouldn't have to do anything special. a="some" b="thing" c=$a$b In this example $c would be "something"
Sounds like your `$first` ends in a carriage return, which when printed moves the cursor back to the first column. If you're reading "Testername" from a file, that file is probably in Windows/DOS format (where lines end in carriage return + line feed) and you'll need to strip the CR somehow. But if it's being set in your script the way that you wrote: first="Testername" that means your script itself is in DOS format, and you should definitely convert it. If you have vim, you can open the script (you should see `[dos]` after the filename when you first open it), do `:set ff=unix`, and save it again. Otherwise you can use `dos2unix scriptname` or `fromdos scriptname` or even `sed -i.bak 's/\r$//' scriptname`
Thats a work in progress. We have something like 60 scripts running to pull files from other servers. So it is a \*lot\* of work, and there is never enough time/money to upgrade that communication for something other than smoke signals \(which is the equivalent of having a automated process sftp from a server to another to pick up data files\)
Use full paths (/home/user) its a habbit that eliminates many problems. Also if you want to wget to a specific directory i think you need the -P flag. You also shouldn't need to add the eval line. I just tried this in bash 4.2 and it worked fine. (variables not shown) wget $url -P $your_path
~ is only usable in interactive shells. You can't use it in shell scripts. Replace it with $HOME in shell scripts. 
Indent your code lines by four spaces: a="some" b="thing" c="$a$b" Click on the source link under this post to see how I did it.
I tried that and your code isn't showing formatted either
If you simply place four spaces before each line of code, your post will display properly.
Just add ` | tee /path/to/file` At the end 
There's special, confusing rules about `~`. It works when used like this: #!/bin/bash url="https://wordpress.org/latest.zip" path_to_downloads=~/"Downloads" wget "$url" "$path_to_downloads" You should better use `$HOME` instead of `~`. Try to never use the `eval` command. It will make bash interpret code that is in your variables, which can cause really confusing errors. It's also kind of dangerous for security, though I guess that doesn't matter here where you are setting the variables yourself. Check out a tool named "shellcheck". It tries to hunt down mistakes in bash scripts. Your distro will likely have a package for it, and you can also try it online at www.shellcheck.net. Here is what it says about your script: Line 4: PATH_TO_DOWNLOADS="~/Downloads" ^-- SC2088: Tilde does not expand in quotes. Use $HOME.
Quite right. What I should have said is that it's bad practice, because when you are using it in scripts as part of a path name, as you said, it can't be quoted without losing its special behavior. But you generally *do* want to quote path names in scripts. So it's use should be avoided in scripts in favor of quoting and the use of the $HOME variable.
+1 for using full paths. I had a lot of problems with my scripts when I was teaching myself bash scripting.
I can't use full path, because it will be executed on different machines and by different users 
I'm sure someone can do a slick awk one-liner or something great in sed, but off the top of my head, something like this would do what you described: cat output.txt | grep "public void" | awk '{print $3}' | tr -d '();' where: cat: get file contents grep: only match lines with: public void awk: print the 3rd column tr: delete (, ), and ;
&gt; cat output.txt | grep "public void" | awk '{print $3}' | tr -d '();' can i just stick a " &gt; desired.txt " on the end of that to get the results?
https://www.unix.com/man-page/redhat/1/wget/
Yes! if your expected result is a text file containing the lines in question.
Hey, just a quick follow up, how would I loop over two files and use the variables of each in another command? I.e : while IFS= read -r var1 do while IFS= read -r var2 do echo $var1::$var2 done&lt;$file2 done&lt;$file1 
&gt; cat output.txt | grep "public void" | awk '{print $3}' | tr -d '();' &gt; cat file | grep thing can be shortened to grep thing file. then we move the pattern matching to awk with: awk '/regex/ { print $3 }' file Then we move the tr statement to a gsub within awk: awk '/public void/ { gsub(/\(\);/,"",$3) ; print $3 }' output.txt And an example of its use: $ echo 'public void test();' | awk '/public void/ { gsub(/\(\);/,"",$3) ; print $3 }' test 
 while read -r; do set -- "${REPLY}" printf '%s\n' "${REPLY} $(nslookup "${3}" | grep name)" done &lt; your.textfile &gt; your.outputfile This is basic redirection. See (among others): http://wiki.bash-hackers.org/howto/redirection_tutorial http://wiki.bash-hackers.org/syntax/redirection https://www.tldp.org/LDP/abs/html/io-redirection.html
Install the Reddit Enhancement Suite browser plugin. There will now be formatting options for the post editor, including a button that looks like `&lt;&gt;`. Select the text you want to appear as code, hit that button, go on with your life.
OMG!! This is BIG! ^(btw, is it so big as it seems?)
So i was still trouble shooting had no luck, maybe because im trying to incorporate it inside of another script this is the script [https://pastebin.com/iPUzMwPt](https://pastebin.com/iPUzMwPt) the idea is that after restic if finishing to backup to the nas then i call rsync to copy that info from the nas to a usb, but i would need to first check if the USB mounted if not dont run rsync Maybe something like this? !/bin/sh check if usb is mounted mount | grep systemd\-1 | grep '/media/usb ' \&gt; Backups USB=\`cat Backups | sed '/\^\\s\*$/d' | wc \-l\` if \[ $USB \-eq 0 \] then rsync \-rah \-\-stats /media/backupnas/ /media/usb/rsyncbackup exit 1; fi Then try to pipe that down the outcome of that log to an email? Thank you
Nope
\&gt; e. BASH\_ARGV0: a new variable that expands to $0 and sets $0 on assignment. What is the use case for this? 
This is great!
Is there a BASH\_ARGV1: BASH\_ARGV2: BASH\_ARGV3 ..... could be to transform input with less typing, avoiding the set $0 $1 $2 pattern thats actually kind of cool 
Exciting!
You download the page using something like `wget` or `curl` Then you look at the page source and figure out what needs to be done to extract the links that you want. Parsing web code with regex or in a shell pipeline can get a bit tricky. Then once you've got your list of links, you figure out how to feed that to your browser of choice. Here's a start though: `wget -qO - "https://github.com/search?o=desc&amp;q=github.io&amp;s=updated&amp;type=Repositories" | sed 's/&amp;quot;/\n/g' | grep "^https.*github.io"` 
How many do you want? Just the first page? Once newer than "x" days? How often do you want to do this? Once a day? On demand?
This might be of interest https://gist.github.com/livibetter/7140262 
Hi, install lynx and use it like this lynx -dump -force_html -listonly -nonumbers -accept_all_cookies $URL
for starters first page would be good enough. First page has 10 link quite enough. Newer than 10 hours. i want to do this 3 times per day or more.
 $ lynx -dump -listonly "https://github.com/search?o=desc&amp;q=github.io&amp;s=updated&amp;type=Repositories" | grep github.io$ | grep -v "search/advanced" | cut -f 2- -d. | cut -d / -f 1-4 | sed 's/.*/&amp; : &amp;/g' | sed 's/^ */&lt;A HREF=\"/g' | sed 's/ :/"&gt;/g' | sed "s~$~&lt;/A&gt;~g" &lt;A HREF="https://github.com/zjdx1998"&gt; https://github.com/zjdx1998&lt;/A&gt; &lt;A HREF="https://github.com/jsokpo"&gt; https://github.com/jsokpo&lt;/A&gt; &lt;A HREF="https://github.com/louistimothee"&gt; https://github.com/louistimothee&lt;/A&gt; &lt;A HREF="https://github.com/Jonathon007"&gt; https://github.com/Jonathon007&lt;/A&gt; &lt;A HREF="https://github.com/manbearpig2010"&gt; https://github.com/manbearpig2010&lt;/A&gt; &lt;A HREF="https://github.com/emoweb"&gt; https://github.com/emoweb&lt;/A&gt; &lt;A HREF="https://github.com/ajtu"&gt; https://github.com/ajtu&lt;/A&gt; &lt;A HREF="https://github.com/walterewilliams"&gt; https://github.com/walterewilliams&lt;/A&gt; &lt;A HREF="https://github.com/meetmarvin"&gt; https://github.com/meetmarvin&lt;/A&gt; &lt;A HREF="https://github.com/remusao"&gt; https://github.com/remusao&lt;/A&gt; 
 $ lynx -dump -listonly "https://github.com/search?o=desc&amp;q=github.io&amp;s=updated&amp;type=Repositories" | grep github.io$ | grep -v "search/advanced" | cut -f 2- -d. | cut -d / -f 1-4 | sed 's/.*/&amp; : &amp;/g' | sed 's/^ */&lt;A HREF=\"/g' | sed 's/ :/"&gt;/g' | sed "s~$~&lt;/A&gt;~g" &lt;A HREF="https://github.com/zjdx1998"&gt; https://github.com/zjdx1998&lt;/A&gt; &lt;A HREF="https://github.com/jsokpo"&gt; https://github.com/jsokpo&lt;/A&gt; &lt;A HREF="https://github.com/louistimothee"&gt; https://github.com/louistimothee&lt;/A&gt; &lt;A HREF="https://github.com/Jonathon007"&gt; https://github.com/Jonathon007&lt;/A&gt; &lt;A HREF="https://github.com/manbearpig2010"&gt; https://github.com/manbearpig2010&lt;/A&gt; &lt;A HREF="https://github.com/emoweb"&gt; https://github.com/emoweb&lt;/A&gt; &lt;A HREF="https://github.com/ajtu"&gt; https://github.com/ajtu&lt;/A&gt; &lt;A HREF="https://github.com/walterewilliams"&gt; https://github.com/walterewilliams&lt;/A&gt; &lt;A HREF="https://github.com/meetmarvin"&gt; https://github.com/meetmarvin&lt;/A&gt; &lt;A HREF="https://github.com/remusao"&gt; https://github.com/remusao&lt;/A&gt; 
Put that in a script, run it at the desired interval (from cron) redirect the output to a file such as *githubnew.html* open it in your browser, and you've got ten links you can middle click directly 
I believe it's that the `set` command normally starts with `$1`, so this provides a way to redefine `$0` that was not previously available.
And I'm sure various efficiency experts and purists are going to find my solution ugly, improper, archaic, etc. 1. It works. 2. If its only running a few times a day, who cares if it takes 2 seconds instead of 0.2 seconds?
This is the thing that makes me most happy: o. A new shopt option: localvar_inherit; if set, a local variable inherits the value of a variable with the same name at the nearest preceding scope. I recently ran into an issue that I had to *really* work around where I had some locally defined variables but I wanted them to default to the value of some globally defined variables with the same name. It was part of a plugin system, so it was an odd use-case. Runners up that are also pretty nice: b. There is an EPOCHSECONDS variable, which expands to the time in seconds since the Unix epoch. c. There is an EPOCHREALTIME variable, which expands to the time in seconds since the Unix epoch with microsecond granularity. So with those, no more `date +%s.%N` (or `gdate` in macOS) hackiness. I was hoping to see some sort of new `type` functionality or new command where you can pass the name of a variable and one can test whether it's local or global or exported or if it's an array or just a normal string or what. `grep`ing through `declare` output feels so wrong.
I don't know of a script or utility that does that, but you can simplify your example to two commands: mkdir -p folder1/folder2 touch folder1/folder2/file{1,2} folder1/file3
I’ve got something similar to this but I’d like it to be a first class citizen. Something like the `type` function. The downside of this implementation is that it’s potentially got up to 3 calls to `grep` which comes with some overhead and the code, although easily understandable, if you have a decent understanding of the output of those commands with those flags, is dense. It could be lightened up with some `grep -q` goodness but still. Would be nice if there was a built-in. 
Try: find . -type d -exec mkdir -p destdir/{} \; -o -type f -exec touch destdir/{} \; This assumes no symlinks or special nodes.
`cp` has a `--attributes-only` option that can help with this: &gt; cp --archive --attributes-only source target
That is what I'm looking for exactly! Thank you so much, you've saved me a lot of effort trying to create this myself.
Seems to me your best bet is doing up-arrow first and then adding cat at the beginning. I like vi mode, so something like `^]kIcat ` is pretty quick. One thing that might help you is that the variable `$_` always hold the last token of the previous line, so if you need to do several things to the same file, you can often shorten it like $ ls -l the/path/to/my/reallylongfilename 1234 $ cat $_ [ contents of the/path/to/my/reallylongfilename] $ grep foo $_ | do_other_stuff | etc
Maybe `Event Designators` can provide what you need : &gt;!! Refer to the previous command. $ pwd /tmp $ echo !! echo pwd pwd
I don't know how to do that with the arrow keys, but you can use `Alt+Shift+_` for this purpose.
To expand, this inserts the last argument of the previous command (and then cycles through the last arguments of older commands). It's usually also bound to `M-.` (meta key is typically the Alt key), which I find a bit easier to type. See [the manual](https://www.gnu.org/software/bash/manual/bash.html#index-yank_002dlast_002darg-_0028M_002d_002e-or-M_002d_005f_0029).
As i usually type in a prefix and navigate to the correct command (not necessarily the last command). This doesn't work
put something like: "\e[A": history-search-backward "\e[B": history-search-forward "\e[C": forward-char "\e[D": backward-char into ~/.inputrc with that you can start typing and up down to the commands you used from histroy. love it :D 
&gt;I’ve got something similar to this but I’d like it to be a first class citizen. Yeah, I agree. Having something like this would help scripts to be a bit more, dare I say it, 'functional'. &gt;Something like the type function. Heh. It's because you mentioned `type` in your previous post that I chose the outputs that I did. Personally I wouldn't do that, I'd much rather have something like this: isset() { case "${1}" in (-a|-array) declare -p "${2}" 2&gt;/dev/null | grep -- "-a ${2}=" &gt;/dev/null 2&gt;&amp;1 return "${?}" ;; (-g|-global) export -p | grep "declare -x ${2}=" &gt;/dev/null 2&gt;&amp;1 return "${2}" ;; (-l|-local) declare -p "${2}" 2&gt;/dev/null | grep -- "-- ${2}=" &gt;/dev/null 2&gt;&amp;1 return "${?}" ;; (*) declare -p "${1}" 2&gt;/dev/null | grep -- "-- ${1}=" &gt;/dev/null 2&gt;&amp;1 return "${?}" ;; esac } That enables an idiom like `if isset -g TERM; then` &gt;It could be lightened up with some grep -q goodness but still. `grep -q` isn't portable. &gt;Would be nice if there was a built-in. Yep. Have you considered shooting [bringing it up with Chet and co?](https://tiswww.case.edu/php/chet/bash/bashtop.html)
This is exactly what I mentioned in the post that I am NOT looking for
If you wanted to do something with the last-executed command, the following might work for you: $ ls . readme.txt $ cat $(!!) Would output the contents of 'readme.txt'
No arrows needed. som_command cat !! 
Ctrl+r then type part of the command you want to recall. Hit ctrl+r again to search back thru your history.
`shopt` sets shell options for the current shell and its subshells only. It won't be inherited by the Bash process executed by `find`. I think you can achieve your goal without calling out to `find`: shopt -s nocasematch extglob nullglob for dir in /data/process/**/?(*.)disc*([0-9])?(.*)/; do prefix=${dir%/*/} old=${dir#"$prefix"} new=${old/.disc*([0-9])./.} new=${new/.disc*([0-9])\//} new=${new/\/disc*([0-9])./\/} mv --no-target-directory "$prefix$old" "$prefix$new" done This is careful to match a `disc[0-9]*`-style keyword even if it is at the beginning or end of your directory names. I haven't tested this carefully though. You will probably want to replace `mv` with `echo mv` and check what it produces first. 
Thank you. I will give this a try.
Hey whetu, Ok \- I had tried that but the results forced me to conclude I had attempted it wrongly. The output file looks exactly similar to the input file. I've copied your code above directly into a script and run just that. Any idea what's wrong, here?
Hi jaredw, This returns "bash: ./mscript.name 1: FSUM7351 not found Any ideas?
Probably a weird character in the source file 
My bad. Change `set -- "${REPLY}"` to `set -- ${REPLY}`... I quoted my var out of habit, this is a case where we don't want to do that! I just ran a single line test using 8.8.8.8 and modified the script to output all of the positional parameters as well as `$@`. Broken output as follows: ▓▒░$ bash dnslist 0: dnslist 1: 2018141 google-public-dns-a 8.8.8.8 2: 3: @: 2018141 google-public-dns-a 8.8.8.8 nslookup: '' is not in legal name syntax (unexpected end of input) 2018141 google-public-dns 8.8.8.8 So we can see here that `set -- "${REPLY}"` is assigning everything to `$1` rather than splitting each word to its own positional parameter. We expect the IP address to be assigned to `$3`, and without that, the call to `nslookup $3` won't work. Fix that line, re-run it and now we get the desired output... or closer to it: ▓▒░$ bash dnslist 0: dnslist 1: 2018141 2: google-public-dns-a 3: 8.8.8.8 @: 2018141 google-public-dns-a 8.8.8.8 2018141 google-public-dns-a 8.8.8.8 8.8.8.8.in-addr.arpa name = google-public-dns-a.google.com.
If you're typing your plaintext password into a shell the command will get appended to bash_history after you logout, so you would want to disable that. There are probably other ways this is insecure but I'm not sure of them.
Is your decrypted value potentially going to be visible in processlist?
Use pass(1). It's meant for this (password-store on Google). It uses gpg but will require human interaction to unlock the protected file: bash default files would probably be a bad idea.
You were exactly right, the single quotes fixed it. Thank you!
&gt; `grep -q` isn't portable. By what definition of 'portable'? It's [a standard POSIX option](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/grep.html#tag_20_55) which in the 21st century is about as portable as it gets.
I'll second for `pass`, it's an amazing tool : $ pass show some/password will do exactly what you're looking for, it will prompt you with `pinentry` to enter the password for your gpg key. You can setup a gpg agent to avoid re\-entering your password every time. &gt;\(password\-store on Google\) I don't get this though, `pass` is strictly local.
On macOS, I've created an item in Keychain and then added an alias to connect to Cisco Anyconnect VPN without prompting for a password. I guess there are keystore alternatives for Linux. security add-generic-password -a AD_USERNAME -p AD_PASSWORD -s "Cisco Anyconnect VPN" And then added in ./bash_profile: alias vpnconn='PW=$(security find-generic-password -s "Cisco Anyconnect VPN" -w); printf "AD_USERNAME\n$PW" | /opt/cisco/anyconnect/bin/vpn -s connect YOUR.SERVER.TLD 2&gt;/dev/null | grep "Connected"' So it's best not to store any of your passwords in plaintext or base64-encoded somewhere in a text file. Just use a password/key store to keep and retrieve your creds.
Ah gotcha, I thought you meant something like "cloud based password\-store"; makes sense now.
Thanks. I figured there was probably a function that did what I wanted, but I wasnt sure what it was \(in hindsight, \`pass\` would have been an obvious choice...\). I still might try out my method just to see if I can make it work, but ill probably end up using \`pass\` in the long run. My method would still require human interaction unless the system was left with \`keychain\` running \- as long as \`keychain\` isnt actively holding the decryption key password, that password would still need to be entered to decrypt the password you actually want to use. It also hinges on the decryption password remaining secure, since \(potentially multiple\) passwords will rely on that one to decrypt, but it seems like using a similiar setup to auto\-unlock \`pass\` using \`keychain\` would have the same issues. Looking through the comments, I think the bigger potential issue in making sure this is evaluated such that any recorded of the command that show up in history files or in active process listings shows the encrypted password \(or a link to a file containing it\) and NOT the decrypted password.
&gt;but you're still storing the password in the same place as the key used to decrypt it. I dont think this is the case. There are 2 passwords in play: 1\) The PGP/GPG password for the decryption key 2\) The password you are trying to use for some purpose Password \(2\) would be saved on disk but only in an encrypted state that requires password \(1\) to decrypt. Password \(1\) is never saved on disk...It is potentially saved in protected memory somewhere \(after you have loaded it into \`keychain\`, which requires physically typing the password\), and if you left the system with \`keychain\` actively holding password \(1\) then password \(2\) would be compromised, but leaving a system unattended with \`keychain\` actively holding passwords is a bad idea in general. Looking through the comments, I think the bigger potential issue is making sure this is evaluated such that any recorded of the command that show up in history files or in active process listings shows the encrypted password \(or a link to a file containing it\) and NOT the decrypted password. 
Out of the comments Ive gotten so far, this is the one that \*might\* be a dealbreaker. I admit im not sure if the active process would show the decrypted password or the encrypted password \+ decrypt command. The 2nd would be OK, since Someone would still need the decryption key to make the decryption process work \(and even if they could see the process, they wouldnt have your keychain loaded. If they did then you have a major security breach that is not related to the method I proposed\). Obviously though, if the active process showed the decrypted password that would be problematic. I might just use `pass` like another commenter suggested, but if I do try and get my method to work this is something I would have to play around with to see if it is possible to ensure the process list always never shows the decrypted password.
Yes, but in my example I just retrieve the password from my login keychain, store it in the 'PW' variable and pass it as an argument to the Cisco AnyConnect binary without typing anything into the prompt. So the password is never saved to disk. I've read your edited post and now it seems to me that you're looking for some kind of a software analogue of a RAM drive to store your Password \(2\) \(but I may be wrong\). When you lock the system — doesn't your keychain get locked as well? Yes, you're correct. Another way to add a keychain item is via GUI, which would allow to avoid the need of clearing the history.
That is actually a fair question. For most people, `-q` is just fine, and I guess I forget that too easily. For me, though, I have the displeasure of keeping ancient commercial unices running. Or I might have some inane restriction like "Solaris with `xpg4` removed" It depends on the client, but ultimately I just default to the `&gt;/dev/null 2&gt;&amp;1` approach. I'll often be doing that for other tools anyway. No, I'm not joking about the removed `xpg4` thing. I know of a client (looked after by another team at my work) whose SOE removes the fucking `man` pages under the guise of "security"
add -v to the options Also, you might consider typing the following: man tar
 $ ls -l total 4 drwxrwxr-x 2 - - 4096 May 28 07:10 dir1 -rw-rw-r-- 1 - - 0 May 28 07:08 file1.txt -rw-rw-r-- 1 - - 0 May 28 07:08 file2.zip -rw-rw-r-- 1 - - 0 May 28 07:08 file3.jpg $ ls -l dir1 total 0 -rw-rw-r-- 1 - - 0 May 28 07:10 file4.gif -rw-rw-r-- 1 - - 0 May 28 07:10 file5.csv -rw-rw-r-- 1 - - 0 May 28 07:10 file6.png $ tar -zcvf /tmp/files.tar.gz * dir1/ dir1/file4.gif dir1/file5.csv dir1/file6.png file1.txt file2.zip file3.jpg 
i now have this: tar czf $dest/$archive_file -v $backup_files i have an array with excluded files and dirs. what i want to do next is when it is printing the filenames, that for the onces excluded it says "excluded" after the filename 
 man tar
also, you should add the *v* to the existing czf like ... tar czvf ... 
The rm command doesn't work because you haven't added : after r in the optlist. It should be while getopts l:a:r: . Since you are passing parameters to -r flag too.
thanks. It worked:) 
#! /bin/bash dest= excl=() land= backup_files=() while getopts d:x:l: opt; do case $opt in d) dest=$OPTARG ;; x) excl+=($OPTARG) ;; l) land=$OPTARG ;; esac done shift $(($OPTIND - 1)) while [ $# -ne 0 ] do t=$1 shift backup_files+=($t) done # Create archive filename. day=$(date +%A) hostname=$(hostname -s) archive_file="$hostname-$day.tgz" # Print start status message. echo "Backing up to $dest/$archive_file" echo # Backup the files using tar. exclude_options=() backups=() for x in "${excl[@]}"; do exclude_options+=(--exclude="$x") done for x in "${backup_files[@]}"; do backups+=("$x") done echo "Backed up files" tar zcvfP $dest/$archive_file "${exclude_options[@]}" "${backups[@]}" echo "Excluded files" for val in "${excl[@]}"; do echo "$val" done # Print end status message. echo echo "Backup finished" # Long listing of files in $dest to check file sizes. ls -lh $dest This is my file When i run this command: dq -d /home/marcel/backup /home/marcel/desktop/nope i get this error : tar: /home/marcel/desktop/nope: Cannot stat: no such file or directory what could this be?
uh... nmap -sn 192.168.2.0/24 &gt; net.inv
This is giving me lots of extra info, I just need hostnames. host1 host2 host3 is what I was going for. I could substitute "nmap -sn 192.168.1.0/24" &gt; net.inv for my "fing --silent -r 1 -o table,text,fing-raw.txt" but just generating a detailed list of discovered hostnames + ip's etc. isn't my goal.
PSA, because OP didn’t specify: yes, that subreddit is as NSFW as it sounds.
Well done sir. Thanks for your comments and help. This is simple and clean!
I don't use the CLI tool, I have Fingbox instead. They put online a support page for all their tools, hope it helps: https://help.fing.io/article-categories/fing-cli/ 
I would recommend to you to use official Reddit API instead of doing web scraping manually. You are even using regex to search their titles, which is horrifying to look at it. Also, I'd go with Python or Node with this job, pretty sure Bash isn't the right language to do this. There are even Python and Node libraries for Reddit, using one would make things easier for you.
You can also grep within awk awk ‘/needle/ {print $5}’
For the 18+ text, check where reddit stores that information once you accept this dialog. I suspect they store it in a cookie, therefore you could try to manually set the cookie using a wget Set-Cookie header. If they store it in a session I recommend using a different language for the job and using the reddit apis as another user suggested. 
No, don't spend time on it. Seems like OP doesn't know but web scraping is just wasting time. There is an API, and there is absolutely no need for figuring out where those titles are lying around.
But why?
Web scraping is not wasting time IMHO. Sometimes you need data that does not have an API. Any exercise in putting together a web scraping solution is a good learning experience.
Learning the right tools for each job is also a good learning experience.
That's the solution of their problem. Need to use the API. 
Yeah sure but why not spend 5min to see if reddit stores this information in a cookie and just add a single parameter to wget to make this script work. If they don‘t store it in a cookie then use the API.
shred
Couple critiques, first off you should use [getopts](http://mywiki.wooledge.org/BashFAQ/035#getopts) instead of creating a menu, this will allow the script to be automated if necessary. "~/Documents/SFS-GPG/" should be a variable instead of a hardcoded path. As for overwriting the file, [shred](https://superuser.com/questions/879188/how-to-automatically-delete-the-original-files-after-encrypting-with-gpg) might be a more secure way of deleting the file afterward. 
Sorry that's a good point. Perform simple text processing or file manipulation tasks without dropping into python, sometimes between servers. Write scripts in a safe and idiomatic manner which have some control flow, but mainly just kicking off jobs and moving things around. I'd like to be comfortable with standard tools and pipelines, cut, sed, tr, awk, head, etc. Thanks - hopefully that is clearer?
Here is your homework then http://shop.oreilly.com/product/9781565922259.do.
Ah, ok. You kind of answered your own question at the bottom. For file and text manipulation, all of the commands you listed would be at the top of my list. I'd add to that find, sort, uniq, xargs, and stat. Keep in mind that awk is a scripting language in itself. I'd also suggest you look into tmux if you are doing lots of command line work over ssh. For tutorials, look [here](http://mywiki.wooledge.org/BashGuide) to start. It's good if you want to learn modern bash with new features but keep in mind that not all features are supported on all versions of bash. Not a big deal if you don't work on older systems but I have worked on heterogeneous networks where we had Linux, Solaris, HP-UX, and even AIX. Not only did these systems have varying versions of bash but some of them only had bourne shell so I got used to writing most of my scripts in bourne shell compliant language so I could use them on any of these systems.
Lol @ "right" tools. Easier? Maybe. Right? Naaaaaah.
You might consider looking at the `select` statement both for your menu and for choosing from a list of files. In `createFile` you really don't need a tempfile at all, since gpg can read from stdin and so on, so (untested) something like gpg -c &gt;"$file" &lt;&lt;&lt;"$data" and then you don't need the plaintext on disk at all. Other random notes: since you're using bash, not sh, prefer `[[` to `[`. And the replacement of spaces with _ in the filename is a complete hack: what if I want spaces in my files? What if my filename has a tab character? Or a newline? The correct thing to do there is to learn how to quote correctly. The control flow of your program is totally confused: sometimes (like line 35) you call back from a function that does something to your intro function. What's wrong with `return` in that case? Why a `sleep` all over the place? I wouldn't want to use a program that has random 2-second pauses in it.
[shellcheck](https://github.com/koalaman/shellcheck) chances are good that your distro packages it. It's a static analyzer for your shellscripts, at best get a plugin for your editor, or if there is no one available just integrate shellcheck in your iteration process like instead of ./myscript.sh run shellcheck myscript.sh &amp;&amp; ./myscript while developing. It points out obvious and not so obvious errors, and the best part, every type of error got it's own id, and a detailed page in the wiki. I've learned a shit ton of stuff from shellcheck! 
Add [The Bash Hackers Wiki](http://wiki.bash-hackers.org/), a good general tutorial on regular expressions, and also grep to what /u/ralfwolf suggested.
Well, I’m still pretty new to all of this. I just started using Linux and bash a couple of months ago. I appreciate your feedback though, I have lots to learn still. I hacked the underscore thing because I think I was having issues locating it after. So I just made the underscore mandatory for ease. The sleep is just a UX feature so the user gets feedback from the script. And I probably should fix the flow, it’s just that I’m inexperienced. This was a fun project to get my feet wet using bash. Some of the problems like [[ vs [ is just not knowing the difference and using google to find something that worked. 
Reading bash scripts created by other people can be very informative. If you have open source projects that you use or that interest you, you can search their source code for bash scripts and look at them to figure out what they're doing and how they use the language. If you're on linux it is very useful to know about the various features that have been developed over the last 27 years. I strongly recommend you go through this book (which has a free PDF version): http://linuxcommand.org/tlcl.php , even if you're familiar with most of it, there might be some chapters with new information, for example, everything in chapters 16, 17, and 18 is very useful.
\`tr\` is probably the most efficient option: tr ' ' '\\t' \&lt; file \&gt; file.replaced \`sed\` can do in\-place editing: sed \-i 's/ /\\t/g' file \`unexpand\` could also do it, though I can’t find think of any reason why you’d want to do this one: unexpand \-t1 file \&gt; file.replaced I’ll spare you the \`ed\` version :\)
Brilliant will do!
Thanks!
Where is it being copied from? There are a couple ways to go about it
From the internet. To be specific, it is the output of a jupyter notebook cell. Don't ask me why I don't use a python script to directly write into a file instead of first printing and then copying.
Well since writing straight to a file is the better answer, that's what I suggest. If you must paste, I found this article on the subject: [https://stackoverflow.com/questions/18258561/pasting\-a\-huge\-amount\-of\-text\-into\-vim\-is\-slow](https://stackoverflow.com/questions/18258561/pasting-a-huge-amount-of-text-into-vim-is-slow)
I am trying that. Should I type `"*p` in the command form of vim. Sorry am a noob for vim
yes. Already tried this. Takes a lot of time 
There's a command line tool "xclip" that can print the clipboard like this: xclip -o I don't know much about vim. Google says you can run a command and insert its output like this: :r !xclip -o 
Build yourself a console with dialog or whiptail. Pick some operations you need on a regular basis. Stub out a new shell script with your own boilerplate. 
` s/ /\t/g`
Cheers
Sorry I’m dense, I don’t know exactly what you’re suggesting. Could you elaborate please?
Write a shell script that can generate a script with dialog: https://www.linuxjournal.com/article/2807 Bonus: Learn and love functions 
 stty_orig=`stty -g` stty -echo cat - &gt; destination.file stty $stty_orig If you turn off echo before the cat it will go much faster. You'll need to end it with `enter` then `ctrl-d`
 stty_orig=`stty -g` stty -echo cat - &gt; destination.file stty $stty_orig If you turn off echo before the cat it will go much faster. You'll need to end it with `enter` then `ctrl-d` [Source](https://stackoverflow.com/questions/5633472/how-do-i-turn-off-echo-in-a-terminal)
I was in the same boat recently and found the best way for me was to just rewrite one of my old nodejs scripts in bash. It helped me learn a ton of stuff.
u/galaktos's comment helped me with this, but if someone wanted to do this in vim the command would look like this :%s/\s/\t/g
/u/galaktos pretty much nailed it down, but for posterity, you might want to add some extra info next time. How big of a file? What format? What encoding? Maybe even which line endings. Which linux flavour are you using or at least which bash version? Also, why do you need a tab instead of an empty space? Perhaps there's a better option.
Neither of your examples involves calling a function. But I do notice your second function uses `${board[@]}` without quoting it, which is a problem (that causes word splitting, if any of the array elements contain spaces). The proper way to define and call a function that takes an array (assuming it needs no other arguments) is: board=("X" "Y" "Z") print_grid() { for elt in "$@"; do printf "%s\n" "$elt" done } print_grid "${board[@]}" Also, please indent your code blocks with four spaces, so that they are rendered readably.
`unexpand` will only change leading whitespace, by the way. And no ed! :( ed -s file.txt &lt;&lt;EOE %s/ / /g wq EOE
./test.sh $(cat test.sh) ./test.sh will be $0 $(cat test.sh) will be $1, meaning that the output of the command will be your first argument. if there are spaces or special characters, use some quotes ./test.sh "$(cat test.sh)"
If you're using a terminal use ctrl + alt + v If you wanna copy from a terminal, highlight the text and use ctrl + alt + c
Let's say this script is not local. For example. A script I want to pull from curl or whether to stdout, pipe it to bash. How can I add the data that would be represented in $1 to that.
I think this is what you're asking for? % bash -s -- foo &lt;&lt;&lt; 'echo $1' foo From [Invoking Bash](https://www.gnu.org/software/bash/manual/html_node/Invoking-Bash.html) in the manual: &gt;-s &gt; &gt;If this option is present, or if no arguments remain after option processing, then commands are read from the standard input. This option allows the positional parameters to be set when invoking an interactive shell.
So you want: some-command | bash where `some-command` also somehow sets up the $1 seen by the script piped into Bash? And you don't want this done with anything special on the right-hand side of that pipe? I think your best option is just to have the output from `some-command` start off with: set -- args You could even have that as a prefix to some other complete script, e.g.: { echo 'set -- foo bar baz'; curl http://example.com/some-script; } | bash 
&gt; `unexpand` will only change leading whitespace, by the way. Hm, I don’t think that’s right – I tested it with: $ echo 'a b' | unexpand -t1 | hexdump -C 00000000 61 09 09 62 0a |a..b.| 00000005 which replaces the two spaces with two tabs. But then, just now, I tested it with $ echo 'a b c' | unexpand -a -t1 | hexdump -C 00000000 61 20 62 09 09 63 0a |a b..c.| 00000007 where the second two spaces are expanded but the first single space isnt, so now I’m extremely confused. Adding the `-a` option (“expand all blanks, not just initial ones” – so the first command shouldn’t have replaced anything?) doesn’t change anything either.
I was thinking about doing just this (replacing nodejs for python/ruby)
YES that is it! Thank you! the -s -- what I was looking for.
 "+p would do that without an external command in Vim.
Pipes are definitely what you want. You might consider a recursive function that calls sed for the first replacement and pipes that to the recursive call. That way you end up with an arbitrarily-long sequence of pipes, but don't have to write an arbitrarily-long pipeline expression.
Why isn't uppercase a good idea?
&gt; trim() { &gt; awk '{$1=$1};1' &gt; } This has the possibly undesirable affect of reducing all internal blank space sequences into single spaces. For instance: This&lt;space&gt;&lt;tab&gt;is&lt;space&gt;&lt;space&gt;&lt;space&gt;a&lt;space&gt;&lt;space&gt;test would be reduced to: This&lt;space&gt;is&lt;space&gt;a&lt;space&gt;test 
Most (if not all) of the environment variables used by the OS are in uppercase and by setting a variable, you may have some unintended consequences. 
I assume you meant to use backticks there (or, preferably, `$( this construct )`). This is a bad approach because it also collapses internal whitespace. Any sequence of whitespace is turned into a single space: $ var='three spaces' $ var=$(echo $var) $ echo "&lt;$var&gt;" &lt;three spaces&gt; You could try to work around that with creative use of the `IFS` variable, but there's no general purpose solution if you take that approach. A better method would be to do this: trim() { local s=$1 LC_CTYPE=C s=${s#"${s%%[![:space:]]*}"} s=${s%"${s##*[![:space:]]}"} printf '%s' "$s" } The assignments to `s` may look a tad complicated, but they just remove leading and trailing whitespace respectively. We use the `[:space:]` character class to determine what whitespace is, and lock down its definition by setting `LC_CTYPE` explicitly; if you have a different definition for whitespace you could change this (e.g. only trimming space characters, not tabs). Finally the `printf` is used in lieu of `echo` to guard against the possibility of the string being `-n`. A demonstration: $ var=' three spaces ' $ var=$(trim "$var") $ echo "&lt;$var&gt;" &lt;three spaces&gt; 
Nope not backticks. Try out what I did. It works just fine.
If those are single-quotes, I don't see how it can do anything except assign a literal string to `VAR`.
Another problem with unquoted `echo` is that glob characters expand; if your variable looks like ` something * something `, the unquoted `echo` will expand `*` to every single file in the directory. Wouldn't the following be simpler to trim spaces with parameter expansions? It requires `shopt -s extglob`, though. s=${s##+([[:blank:]])} s=${s%%+([[:blank:]])}
Variables, Array names etc are case sensitive. UPPERCASE is generally used for environment/global and shell-special variables. Run `printenv` for an example list of some of them. In other languages that have concepts of namespaces and/or scopes (some use the terms separately, some use them interchangeably), it is widely accepted best practice to use your variables in a way that avoids collisions with the global namespace. In some languages, it is strictly forbidden, and violating this will result in you being dragged to the nearest carpark and beaten with [a cat5 'o 9 tails.](https://www.reddit.com/r/cableporn/comments/2h6mcw/the_cat5o9tails_when_network_admins_get_bored/). `bash` doesn't strictly have namespace/scope concepts. But we can tell that, by convention, UPPERCASE is clearly already in use for "global" or "environment" purposes, and so we can - and should - adopt broader programming best practices and conceptualise UPPERCASE as off-limits. Adopting broader best-practices is also a useful habit that may help you down the track if/when you pick up another language, especially one that may be more restrictive. And I know, there are people reading this who think that nothing will go wrong. My recommendation is that if you absolutely must use UPPERCASE, then you need to prepend it with something. Some people use `MY_VAR` style syntax, others use `_VAR.... I've also told this story before and I'll tell it again: I was once asked to look at a script that was broken and none of my colleagues could figure out why. It took me a few minutes of blankly staring at it before I clicked. The person who coded it had wanted to store the current directory path into a variable, so that person had absent-mindedly used: PATH=`pwd` This obviously broke all subsequent commands. Had this person been using lowercase variables (or snake_case, or camelCase or PascalCase), then it wouldn't have mattered. Usage of backticks (bad) and ironically ignoring `$PWD` aside.
Yarp, thanks for that. To be clear, I didn't mean to imply that that function should be used, I was merely noting that a lot of people seem to settle on it. A better adjusted `trim()` function, by way of demonstration: ▓▒░$ testvar=' this is a test with lots of spaces ' ▓▒░$ declare -p testvar declare -- testvar=" this is a test with lots of spaces " ▓▒░$ testvar=$(trim "$testvar") ▓▒░$ declare -p testvar declare -- testvar="this is a test with lots of spaces" (There's actually a tab in there too) 
&gt; code=$(curl -sL -w "%{http_code}" -I "http://$target$line" -o /dev/null) Cheers!!!!
 Cheers!!!!
Yes, that would be simpler. I tend to avoid turning on `extglob` until I really need it. The very slight advantage of the way I did it is (if you avoid `local`) it is valid POSIX shell, not Bash-specific.
&gt; Try it, yo. $ VAR=' three spaces ' $ VAR='echo $VAR' $ echo "$VAR" echo $VAR Are you sure you're not using `eval` somewhere on that?
First you should look at [shellcheck.net](https://shellcheck.net) \(and [github.com/koalaman/shellcheck/wiki](https://github.com/koalaman/shellcheck/wiki)\) this will prevent you from making stupid mistake, and learn bash as you write it. Each time you see an error code, you should check it and read the reasonning about it in the wiki. Your script should take either a configuration file, or environment variables or runtime arguments.
To be honest, this sounds like what you really need is just three different scripts. If you want to keep it in one script, I wouldn’t bother with the separate `do_task` function, and instead go for something like this: function subtask { local path=${1:?path missing} # ... } if [[ -e path_a ]]; then subtask path_a elif [[ -e path_b ]]; then subtask path_b elif [[ -e path_c ]]; then subtask path_c fi If all the conditions are just “does this path exist”, that could be further simplified to: for path in path_a path_b path_c; do if [[ -e $path ]]; then subtask "$path" break fi done
Isn't that structure kind of needlessly convoluted? There may be some levels of tailoring that are necessary due to oversimplifications in your example, but for the functionality you describe, you could just as well have written (and this is using proper Bash syntax, but I think the analogy to your Python-style layout should be legible): subtask_if_exists () { [[ -e "$1" ]] &amp;&amp; subtask "$1" } subtask_if_exists "$path_on_a" || subtask_if_exists "$path_on_b" || subtask_if_exists "$path_on_c"
It has some problems and shits
Nice feedback bro.
* `ifconfig` is deprecated since 2009, use `ip` instead. In some distros, `ifconfig` isn't even available. * Your method for showing the ip is inadequate. I don't even have a ens3 device, so it can't show my ip because it's grepping for ens3. * Not sure about the `drop_caches`thing. I can't even sudo that (permission denied. xubuntu 18.04) * I don't understand what is the purpose of transfer.sh
To get a list/array of string from a file, I have used this syntax `mapfile -t variable &lt; &lt;(cat /path/to/file)`, where the file contains the strings split by `\n` and a set IFS in the script.
Use printf and redirections. printf '%s - error - %s\n' "$(date)" "$1" &gt;&gt; err.log Though you should probably use stderr and redirect stderr to a file. echo ohoh &gt;&amp;2 ./script 2&gt; err.log
Thanks for the advice. I will apply the changes, the file transfer.sh is to use the web service with the same name, which allows you to host and share files.
Nice job
are all the bash scripts universal ? they are most like take the duct and paste in my opinion.
some personal suggestions: sshaudit.sh: you don't need to cat a log to grep search it. not a big deal but can save some processing and time if your log gets rather large. transfer.sh: always put a shebang. i'd recommend putting "${1}" instead of just $1 to account for any special characters curl --upload-file "${1}" "https://transfer.sh/${1}" you can put your echo lines in a function so you dont have to keep repeating it: function printline() { echo "\e[33m---------------------------------------------------------\e[0m" ; } and then everytime you need to print the line you can just call it by writing printline
Why not just ssh to the other server?
sshaudit.sh * consider `tput` instead of these hard-coded color codes * consider not doing color at all, or at least detecting pipe vs tty so you don't fuck up downsteam programs * UUOC on every line * awk can do everything grep can (and tail too), so instead of 8 (!) processes, you could just use one (untested): awk ' /Failed/ { fail[i++ % 10] = $1 " " $2 " " ... } /Accepted/ { acpt[j++ % 10] = $1 " " $2 " " ... } END { for (f in fail) { print fail[f]; } for (a in acpt) { print acpt[a]; } }' &lt;/var/log/auth.log * and sim. for netstat output system.sh * again with colors * as mentioned, `ifconfig` is deprecated * more piping into awk that can be replaced: free -m | awk ' NR == 1 { print "..." } /Mem/ { print "..." }' which costs you two process instead of 6 (!) transfer.sh * totally broken quoting
I still tend to use `sed` instead of bash built-in for stuff like this. string=$(echo "$string" | sed 's/^[[:blank:]]*//;s/[[:blank:]]*$//') simple and portable.
[See this](http://porkmail.org/era/unix/award.html) for shits and giggles and an important beginner lesson. --- If you grep case-sensitive and pipe to awk, you can remove the grep. Ex.: $ grep Something some.log | grep -v elSe | awk '{print $2}' is the same as: awk '/Something/ &amp;&amp; !/elSe/ {print $2}' some.log --- Then, all those echo lines, man... that's just ugly. What's the point of scripting/automating if you're just gonna serve some text to a human at the end? Eveything you've written here can be achieved by just modifying some config files for services like fail2ban, logwatch, logcheck, logstash, etc., you name it. Why do you need colors? Are you writing these scripts to have them run manually? Besides, in order for those color codes to resolve and actually print out you'll need ```echo -e```. Maybe check out ```tput```. ---- Try to make your scripts as platform independent as possible, while retaining simplicity. Don't target specific distros unless you have no other choice. This is not acceptable: ip=$(ifconfig | grep -A 1 'ens3' | tail -1 | cut -d ' ' -f 10) Don't use ifconfig, go for ip. What happens if that iface naming scheme doesn't exist on my distro? Do I ever want to look at your repos again knowing they're probably be destined for Debian? --- Most of the stuff you write here should be shorter one-liner aliases, if the purpose is to show you the status of some service. If not, write scripts that 'know' when something is wrong, run regularly, and they inform you of the status somehow only if shit goes sideways.
Also: `alias $alias_name="$(echo $f)"` will fuck up whitespace in `$f` yet again. Shouldn't it be `alias "$alias_name"="$f"` ?
Good catch!
I've added the missing feature and switched to \`awk '{print $1}\`, which splits by one or more whitespace characters. I have trouble with bash's whitespace handling, but I finally bit the bullet and started reading more about bash's subtleties at [https://uvesway.wordpress.com/2013/03/11/some\-whitespace\-pitfalls\-in\-bash\-programming/](https://uvesway.wordpress.com/2013/03/11/some-whitespace-pitfalls-in-bash-programming/), any other recommendations? Thank you for your feedback!
https://mywiki.wooledge.org/Quotes
Thank you.
Functions might suit you better, here.
You can also use *loops* in a pipeline, which I do a lot more often than `if`s – usually some version of `while read -r ...; do printf '...\n' ...; done`. (Sometimes that’s easier than devising a `sed` or `awk` equivalent.)
I am indeed familiar with that particular idiom, but again, never really made the connection that you could do it with `if`, probably because I never thought of an instance where it would be useful until I ran into one. I am still not sure if I want to implement it that way though. It's definitely the more LISP-y way to do it, although I'm not sure where using `cat -` to just pass through stdin falls on the UUOC spectrum, though.
Set `LC_COLLATE` or `LC_CTYPE` to `C` (or some other locale that does what you want). For example, in a typical UTF-8 English locale: $ printf '%s\n' 'abracdabra' 'Ångström' 'éclair' | grep '^[a-z]*$' abracdabra éclair $ printf '%s\n' 'abracdabra' 'Ångström' 'éclair' | grep '^[[:alpha:]]*$' abracdabra Ångström éclair $ printf '%s\n' 'abracdabra' 'Ångström' 'éclair' | LC_COLLATE=C grep '^[a-z]*$' abracdabra $ printf '%s\n' 'abracdabra' 'Ångström' 'éclair' | LC_CTYPE=C grep '^[[:alpha:]]*$' abracdabra You can also use the bigger hammers `LC_ALL` and `LANG`.
The following is technically not a mistake in your script, but you might perhaps still want to know about it: You do a text comparison here: [[ $# &gt; 0 ]] To do a number comparison, you should do this: [[ $# -gt 0 ]] or this: (( $# &gt; 0 )) I think this will not matter here because of how $# works and because you compare with zero, but it can matter for other variables. What can go wrong using a text comparison is something like this here: $ if [[ 12 &gt; 7 ]]; then echo true; else echo false; fi false $ if (( 12 &gt; 7 )); then echo true; else echo false; fi true What happens in this example is that `&gt;` is a text operator inside `[[`, and the text `"1"` sorts in front of the text `"7"` in the alphabet that bash uses to compare text. 
Ah, you're right. I'm lazy and normally just use the single bracket form, partly because that's what the skeleton for `sh-if` in emacs puts in by default and partly because that's what I learned first and am more used to. I just banged that out real quick and that's where I screwed up.
Another useful compound command to use in pipelines is `{ ..; }` groups. e.g. `{ echo suffix; somecommand; echo prefix; } | mycommand`
Something inside the loop is consuming the rest of standard input from the pipeline, so the next `read` call gets an EOF. It's hard to see what that could be, but my guess is it might be the `expect` process. Try redirecting its standard input from `/dev/null` to see if the problem goes away, i.e.: sh -c "expect -f bin/devconfig.exp $ip &lt;/dev/null | tee dev-$ip.txt" 
Thank you, /dev/null let the loop complete, but the expect script doesnt seem to be running. Return code is 0. The sh -c is something I thought of during the shower, didnt help hehe
That sounds like a problem with your `expect` script.
Wait, how would that work? Does the loop just finish running and then pass its output along the pipe, or can you somehow break out of a loop with a pipe?
Yeah probably. I ended up looping within the expect script instead of from an external bash script. My initial problem was piping in the spawn part, so I did this spawn -noecho sh -c "ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -l $user $ip | tee dev-$ip"
Question ... how do you hold this knowledge 
what do you expect from adding `echo`, why not leave it out?
`echo` doesn't read from stdin so piping output to it will basically kill the output.
* `echo` reads arguments and writes to stdout. * `cat` reads stdin and writes to stdout * `xargs` reads stdin and passes as arguments. * `somecommand "$@"` reads arguments and passes as arguments. So if you have data written to stdout and want to to go to stdout, You can do `| xargs echo` or `| cat` or just do nothing since that's where it's already going. 
Echo is for displaying strings. It only takes input from command-line parameters. Sed by default sends output to standard output, so piping sed output to a program that displays on standard output is redundant. It's similar to the useless use of cat problem that new users often fall for. What does your desired result look like?
Can you tell what the script is supposed to do? I tried to reproduce but the goal is unclear. " printf "Configuring $ip\n" sh -c "expec ...." running printf and sh -c without break will only run printf, no? instead printf "Configuring $ip\n" &amp;&amp; sh -c "expec ... should do in that case what a puzzle xD
automate all the things. A lot is just like learning vocabulary "all teh operators &amp;&amp; regex", the other part is hands on experience/system knowledge. for bash stream operations maybe get a big wordlist (there are some torrents and sites to dl), use bash tools to clean, sort, split and modify the files and folders - do it a bit and you get very creative. write install scripts and try to automate your system setups. after a while and you can do it with closed eyes ... sort your written scripts straight from the start in a neat structure, youll use them over and over again, nice to have a little library. :) 
I found this project very hard to understand. The only "example" I could find was the animation in your readme file. It would have been easier, I think, to read a transcript of an `la` session. So I must be missing something, because a "local alias" looks a whole lot like a function. Can you explain the concept a little more?
It doesn't break out; it just sends all its stdout output into the pipe. Here's a dumb example: $ for x in foo bar baz; do &gt; printf "%s\n" "$x" &gt; done | sort bar baz foo
Sure thing. So you're right. Local "aliases" are really more like functions. I justify this by the fact that shell functions are more or less a superset of shell aliases (as far as functionality is concerned at least), but you're right. It's confusing. What's special about the functions created by `la`, however, is that they are local to a particular directory. If you leave the directory they were created in, you can no longer use them. So, for example, this allows me to have `r` execute `python localalias` when I'm working on this project, but execute some other command if I'm working on a different project.
its supposed to read a list of IPs and execute a configuration expect script on each IP, with the twist that each run of the expect script needs to create a log file. Initially I wasnt able to pipe to tee from within the spawn line, then I enclosed the SSh command within sh -c and there, the pipe works fine
Just found there is log_file in expect that seems to do what I need very nicely
&gt; just do nothing since it's already going where you want it That's probably the correct answer, right there.
This is a bit like just doing: alias b="make" alias t="make test" Which would automagically work with most `make` based projects. You could also add simple checks to invoke npm, docker or whatever instead. Alternatively, since you're already using files stored in each directory, you can also `alias r="./run"` and add a `run` script in each directory. This way, everyone using the project can easily run it, even if they don't have your localalias script installed.
Yeah that’s a good point. But make cannot easily handle arguments. This is a major problem for ‘r’ and ‘t’ since I often want to pass an extra argument to my program or to pytest for example. Anyway, I don’t think I would ever feel comfortable assigning a global alias to a single letter; just as you wouldn’t assign a global variable to ‘i’ in a program. Drastically reduce the scope (to say a single directory) and I’m okay with it; just as I’d be okay with you using ‘i’ as the dynamic variable in a small for loop. There are also other use cases. For example, I have a directory labeled ‘notes’ that holds various different text files. With localalias, I’ve got each file assigned to a single letter (the first letter of the filename) such that the file is opened in vim when that letter is used.
What the hell is wrong with a single-letter alias/function? It's great for the ones you really use a lot. Remember, aliases are really only for interactive use. As long as you know what it's referring to, where's the problem? Plus, it's not like it's difficult to look up if you forget with `type` or `alias`. Ain't nobody got time for tab completion when I can just type a single letter and hit Enter.
You have a few bugs that partially cancel each other out (give [shellcheck](https://shellcheck.net) a go). `+=` is string concatenation: a=0 a+=1 a+=2 echo $a results in "012". In other words, your leading zero is because you created a variable "0" and appended a string of number to it. `let`'s `+=` is integer addition, so it'll calculate the sum instead of appending the number. The reason why you get roughly the right sum instead of a mess of digits is that your "loop" only ever runs once (it does **not** run once per file), and calculates a single result by `cat`'ing all files that one time. If it had run once per file the way you intended it, it would be quite different. Here's more in line of what you intended: ``` #!/bin/bash sum=0 for f in ./* do sum=$((sum + $(cat "$f" | wc -l))) done echo $sum ``` Now that the loop runs once per file, you can better see the difference between integer addition and string concatenation. 
Why not "$(wc - l $f)"?
Perfect! Thanks!
I use single letter aliases all the time. But I use local aliases for that. I don’t really see why we should hold different standards for interactive use than we would for general programming. Global scope is sacred ground.
That also includes the name of the file in the output, which `$(( ))` does not like. But `"$(wc -l &lt; "$f")"` would work.
Thanks for the great answer! That makes total sense. 
FILE=$1 cat $FILE | while read output do printf "Configuring $ip\\n" sh \-c "expect \-f bin/devconfig.exp $ip| tee dev\-$ip.txt" echo $? done beside the missing &amp;&amp; , while read ouput \- then $ip would have to be $output or vise versa. The formatting added to the confusion... `#!/bin/bash` `IPLIST=/home/$USER/iplist.txt` `sudo netstat -antpl -W -4 | grep ':443' | awk '{ print $5}' | sed -e 's/:443//g' &gt; $IPLIST` `cat $IPLIST | while read IPS` `do printf "Configuring $IPS\n"` `echo $IPS` `# sh -c "expect -f bin/devconfig.exp $IPS | tee dev-$IPS.txt" echo $?` `done` 
You're telling me you don't even have something like `alias l="ls -F"`, something which is clearly useful wherever you are in your filesystem? I just don't get it, but you do you. It's not a bad project, but seems a bit heavy duty for what *I'd* consider to be a very minor problem. If I wanted to tackle this problem I'd probably write a `cd` function that wraps around the `builtin cd` and after successfully changing directories, checks for a file with a specific name (like `.localalias`) which is then sourced, defining whatever aliases/functions/environment variables needed, and also defines a standard function to `unset` everything it defined. This system could also take advantage of one or two special variables, something like `localalias_root`, which the function checks to see if it is still in the project tree, and thus whether it should unset everything. Then a few aliases could be set to edit the file of the current project root or whatever. That's just how I roll, though. The thought would never occur to me to use `python` for something like this. Of course, what I'm describing has basically already been done with `{auto,dir}env`. `desk` achieves a not unsimilar effect. I suppose there's nothing wrong with having choices, though. * https://github.com/kennethreitz/autoenv * https://github.com/direnv/direnv * https://github.com/jamesob/desk
Reminds me of desk https://github.com/jamesob/desk
whats the usecase? Correct me if im wrong, but its the same effect as if i make a bash script, i just dont need the ./ prefix to run it, and i can symlink a bash script.
Yeah, it’s essentially the same thing. It’s more work doing it that way though and you wind up with lots of random bash scripts all over the place. The appeal of local aliases for me is that they are “cheap” in every way. This opens up the use of aliases in cases where I normally wouldn’t deem it worthwhile to create one.
Go to the top level directory that has the subdirectories in it. find ./ -exec echo mv -f {} /target/directory \; If you're satisfied with the list that you see, remove the word 'echo' so it actually runs the command instead of displaying it: find ./ -exec mv {} \; 
What this is doing is finding all files (find) from the current directory (./) that are files not directories (type f) and running a command (exec) that is: move (mv) contents {} to directory /target/directory. I forget what the shit at the end is for: \;
If you use `+` instead of `;` at the end it should be a lot faster The `+` tells `find` to pass in as many arguments as it can when it executes the command, similar to `xargs`. With `;`, you'll be invoking a new instance of `cp` for every single individual file in the tree, and if there are indeed thousands of them it can really add up (especially on a system like macOS with high `fork()` overhead) This is a pretty contrived test, but just for illustration here's the difference between `;` and `+` passing 18954 short file names to `/bin/echo`: % mtime find . -type f -exec /bin/echo '{}' ';' &gt; /dev/null [38620 ms] % mtime find . -type f -exec /bin/echo '{}' '+' &gt; /dev/null [53 ms] In the first case, there are 18954 separate calls to `echo`; in the second case, there are only two
Ahh, cool. Thank you! I love learning tips that save time like this.
previous posted `find ./ -type f -exec echo mv -f {} /target/directory ;` moves all files, even if they are in a directory \- which may result in overwriting \(correct me if im wrong\). below command moves only files in the inputdir, should be more secure. `#!/bin/bash` `for FILESTOMOVE in /opt/inputdir/*` `do` `if [ -f "$FILESTOMOVE" ]; then` `mv "$FILESTOMOVE" /opt/outputdir` `fi` `done` replace \-f with \-d to move folders `#!/bin/bash` `for FILESTOMOVE in /opt/inputdir/*` `do` `if [ -d "$FILESTOMOVE" ]; then` `mv "$FILESTOMOVE" /opt/outputdir` `fi` `done` you can eventually put both command strings in one bash file for controlled moving of files and directories. did i miss something important?
this should do? find /opt/inputdir \-type f \-exec cat {} \+ | wc \-l
The other solutions posted here are all executing `mv` directly from `find`, which can be a tad slow since it's only moving one file per process. I would do this: find name*/ -mindepth 1 -maxdepth 1 -type f -print0 | xargs -0r mv --target-directory=/home/Documents/files Adjust `find` predicates as required. (And if you've got so many `name*` directories that not even that is possible... that makes this problem a bit harder.) The key here is the `--target-directory` option to `mv`. It lets `xargs` pack as many _trailing_ arguments as it can to each `mv` invocation.
&gt; The assignment word operator var+=value is string concatenation Incidentally, Bash variables has an "integer" attribute that can be used to make this work: $ declare -i a $ a=0 $ a+=1 $ a+=2 $ echo $a 3 I probably wouldn't solve this task this way, though. I reckon it'd be simplest like this: cat * | wc -l For once, actually using `cat` to _catenate files_! :-) 
that was my point **widely compatible**
The `find` suggestion is the best general solution, but the simplest way to make your command work is, in my opinion: for file in name*/*; do mv "$file" /home/Documents/files done The loop doesn’t care how long the arguments are, so you can use the same globbing pattern without learning any `find` syntax. (This has the same problem as `find -exec ... \;` has, which was pointed out in the other comments already: it’s inefficient to spawn one `mv` process per file.)
&gt; cat * | wc -l This will fail if there are too many files for a single command line, though. I think /u/caniputtydat’s comment of `find -exec cat {} + | wc -l` is the best solution: concatenate as many files as possible with a single `cat` process, but use multiple processes if the argument list is too long.
you can use multi cpu on commands using `parallel` \-\-help :\)
The error message is trying to tell you what's wrong.
What are the error messages? Kinda important.... 
These are the error messages: \[01bd83e0\] pulse audio output error: PulseAudio server connection failure: Connection refused \[01bf63d8\] core interface error: no suitable interface module \[01b4d918\] core libvlc error: interface "globalhotkeys,none" initialization failed \[01beea20\] dummy interface: using the dummy interface module... \[01b5d140\] core playlist: stopping playback lc error: interface "globalhotkeys,none" initialization failed \[72b04400\] http access: Raw\-audio server found, mp3 demuxer selected \[01beea20\] dummy interface: using the dummy interface module... \[01b5d140\] core playlist: stopping playback \[72b04400\] http access: Raw\-audio server found, mp3 demuxer selected
Ok, I added them!
I added them to the posting...
I found the solution myself! The network connection just wasn't established so quickly. I added "sleep 5" and everything works fine now. Thanks though.
I found the solution myself! The network connection just wasn't established so quickly. I added "sleep 5" and everything works fine now. Thanks though.
You can use `screen` to keep the session open. Run that on the remote server. However, you can get around this by just opening two terminals on the local machine. One to run ssh and another to download the sftp files. When you say "extract the tar file", do you mean to download that to your local machine? 
If the server has SSH available, it likely has SCP available so you could transfer the file lucky and do the work from there. Also, for both SSH and SFTP and SCP you could probably set up keys so that you don't need to enter in a password at all.
Other options to avoid having to put in the password multiple times: 1. Set up `ssh-agent` and then use `ssh-add` to store the password there, then you won’t have to unlock the key again. 2. ssh supports multiple sessions per connection, and you can configure the first process to stay around in the background so that the other sessions can reuse it. Put this in your `.ssh/config`: Host * ControlMaster auto ControlPath /run/user/%i/%r@%h:%p ControlPersist 10m With these settings, you only need to enter your password again if the connection is unused for over ten minutes. (This is useful even when using `ssh-agent`, by the way, since it saves some time when the connection doesn’t need to be established first.)
It's amazing how after 10 years of using linux and Bash I can see commands that I never knew existed. 
I guess you're asking for tmux. More terms in the same session. Try it. I prefer terminal emulators \(iTerm2\) to terminal multiplexers, but for someone it works.
Tell me about it! It's one of the things I love about these sort of environments. :D
This is really something you should use your init system for.
You can also seed shuf too so you can always reproduce the same shuffled list. Very useful in a very small number of places 
FYI, the best place to do something like this is not really in bashrc, but rather would be with a Unit file, making this run as a service. This has the benefits of restarting on failure, and the ability to make it start _after_ the Networking.service begins. For example: https://www.raspberrypi.org/documentation/linux/usage/systemd.md :)
The following will only work in bash script, won't work on Debian in a /bin/sh script: I do it like this, using $UID: #!/bin/bash if (( $UID != 0 )); then echo "${0##*/}: This script needs to run as root!' exit 1 fi That `${0##*/}` cuts off the path from the script name. I can't find documentation about $LOGNAME in `man bash`, but I can find a description of $UID. I think bash itself sets $UID. I don't know where $LOGNAME comes from.
Extract the file remotely or locally? If locally, why not just use rsync? You could do some fancy ssh piping too. 
$LOGNAME is the env variable set for the current user when you login, which I thought changed to root when you used sudo, and then your normal user was put in $SUDO_USER. Basically, this was an attempt to make my script platform agnostic, as I have multiple Linux boxes and wanted to be able to run my scripts on all of them without storing local copies on each of them. I was unaware of ${0##*/} that is extremely helpful! I was going to look into it, but it didnt bother me enough to get to it just yet. Thanks for the feedback! 
There are plenty of ways that you'll see out there to perform this test. Some people compare to `$UID` or `$EUID`, some people compare to the output of `id -u`, which is a more portable option despite using an external command. More portable than that is simply testing whether the user has write access to /, which is arguable - they might not be root, for example, but they theoretically have the same write permissions as root. if (( UID == 0 )); then if (( EUID == 0 )); then if (( $(id -u) == 0 )); then if [ -w / ]; then Can't say I've ever seen `$LOGNAME` used for this task though...
That definitely seems like the better option! I am getting more into scripting and have a few basic scripts that I use and always just ran `sudo /usr/local/scripts/script` because it was easier, however I am trying to build up my skills and thought I would add the conditionals to test for root and I went about it the wrong way it seems. I am definitely happy to learn better more efficient ways of doing such tasks! Thanks for the input!
And some more practical examples of what `shuf` can do: Want a random number between 100 and 1000? ▓▒░$ shuf -i 100-1000 -n 1 110 Why not 10 random numbers between 100 and 1000? ▓▒░$ shuf -i 100-1000 -n 10 539 817 189 737 534 582 528 530 213 928 Here's a fun project: Use `shuf` to generate lottery tickets. ▓▒░$ lotto |Bonus|Power |Ball |Ball ------------------------------ 09 15 16 27 34 39 | 23 | 02 03 14 20 21 24 33 | 01 | 07 07 08 09 12 26 40 | 03 | 06 11 14 18 25 26 36 | 08 | 07 04 12 17 22 23 31 | 34 | 05 06 10 17 25 33 36 | 20 | 07 02 06 09 12 23 36 | 28 | 10 12 25 31 35 36 40 | 26 | 05 03 10 19 25 31 33 | 24 | 02 05 11 16 25 32 33 | 20 | 02 "That's dumb! Big deal! I have `$RANDOM`!" Ok, smartass... `$RANDOM` tops out at 32767 (2^15 -1), have at you: ▓▒░$ shuf -i 32768-1000000 -n 1 514743 Want to make a primitive passphrase generator? ▓▒░$ make-phrase() { shuf -n "${1:-5}" /usr/share/dict/words | paste -sd '\0' -; } ▓▒░$ make-phrase gossipslicensesindictmentsjoystick'sunpleasant You can build on that too and make a fuller-featured passphrase generator e.g. ▓▒░$ genphrase -n 40 -s /r/bash | column [/r/bash]HosesLinda Butts[/r/bash]Poorly [/r/bash]LumpinessComma RetouchWashing[/r/bash] [/r/bash]ChickLeggings [/r/bash]MessGrieve BoostFools[/r/bash] Ridden[/r/bash]Freud Flight[/r/bash]Hacksaw Decker[/r/bash]Braid [/r/bash]OverdueMast Ceased[/r/bash]Divisive [/r/bash]AcuteSpinout MorphingVagabond[/r/bash] [/r/bash]SulfiteSavor CarvedStaten[/r/bash] Demise[/r/bash]Litter BonedUncooked[/r/bash] RicoEarmuff[/r/bash] StuartExiled[/r/bash] RetrainMust[/r/bash] MockingMargin[/r/bash] [/r/bash]TowingDouglas [/r/bash]SaluteTake [/r/bash]SpikedHurrah DiddlePrimer[/r/bash] Healing[/r/bash]Warp [/r/bash]DarkishKnowledge [/r/bash]EnchantedCalms PlayableNerds[/r/bash] [/r/bash]BalmSatoshi Devour[/r/bash]Officer Dayroom[/r/bash]Preflight Mesa[/r/bash]Suggest [/r/bash]DeliverBasil SafeMonotype[/r/bash] Collected[/r/bash]Spotlight Scrap[/r/bash]Sky [/r/bash]HighNests Ergonomic[/r/bash]Psych Want to select a random array element? You might use some kind of `${myArray[$(( RANDOM % ${#myArray[@]} ))]}` looking fucking thing. Problem is, that's a modulo, and modulos can have bias (sometimes terribly), especially when fed from an already biased Linear Congruential Generator (like `$RANDOM` is). `shuf` doesn't have those problems ^(as much) ▓▒░$ alphabet=( {a..z} ) ▓▒░$ shuf -e "${alphabet[@]}" -n 1 i Want to fully randomise your array? Same thing, just drop the `-n 1` `shuf` is a brilliant tool, and it's even more impressive once you wrap your head around the concept of reservoir sampling, which is something that `shuf` does. For the greatest of shames, it is sadly not as portable and ubiquitous as it should be. It is available as `gshuf` via homebrew on OSX, and it turns up on Solaris 11. Predictably, Oracle managed to fuck it up by using an ancient version that lacks features.
Note that the below script is NOT tested due to drunkness and lack of a terminal within reach. Extra lines in-between commands have been added for readability. #!/bin/bash # Gets a random OpenVPN UDP config file (US servers only) # and adds the auth.txt string to it for automatic login. vpnExist="" clear # Checks to see if a OpenVPN process already is started vpnExist=$(ps auxf | grep openvpn | fgrep -v grep) if [ "$vpnExist" ]; then echo echo "Openvpn process already exists. Exiting." exit 1 fi cd /etc/openvpn/ovpn_udp rServer=$(shuf -ezn 1 us*udp* | xargs -0 -n1 echo) echo "Random US server is: $rServer" sed -i 's/auth-user-pass/&amp; \/etc\/openvpn\/auth.txt/' $rServer /bin/bash sudo openvpn $rServer &amp; # Wait a few seconds to have the openvpn process above fully initialize. # Use a a number (seconds) that works for your system. sleep 5 vpnIP=$(dig TXT +short o-o.myaddr.l.google.com @ns1.google.com | awk -F'"' '{ print $2}') echo echo "My new IP is: $vpnIP" echo exit 0 
Note that the below script is NOT tested due to drunkness and lack of a terminal within reach. Extra lines in-between commands have been added for readability. #!/bin/bash # Gets a random OpenVPN UDP config file (US servers only) # and adds the auth.txt string to it for automatic login. rServer="" vpnExist="" clear # Checks to see if a OpenVPN process already is started vpnExist=$(ps auxf | grep openvpn | fgrep -v grep) if [ "$vpnExist" ]; then echo echo "Openvpn process already exists. Exiting." exit 1 fi cd /etc/openvpn/ovpn_udp # Picks a random file we need rServer=$(shuf -ezn 1 us*udp* | xargs -0 -n1 echo) echo "Random US server is: $rServer" # Finds the string 'auth-user-pass' in the choosen file and # adds '/etc/openvpn/auth.txt' at the end of it. sed -i 's/auth-user-pass/&amp; \/etc\/openvpn\/auth.txt/' $rServer # Forking the start of OpenVPN so the script can continue. /bin/bash sudo openvpn $rServer &amp; # Wait a few seconds to have the openvpn process above fully initialize. # Use a a number (seconds) that works for your system. sleep 5 # Ask Google what my new IP is after the VPN has been initialized. vpnIP=$(dig TXT +short o-o.myaddr.l.google.com @ns1.google.com | awk -F'"' '{ print $2}') echo echo "My new IP is: $vpnIP" echo exit 0 
Note that the below script is NOT tested due to drunkness and lack of a terminal within reach. Extra lines in-between commands have been added for readability. #!/bin/bash # Gets a random OpenVPN UDP config file (US servers only) # and adds the auth.txt string to it for automatic login. rServer="" vpnExist="" clear # Checks to see if a OpenVPN process already is started vpnExist=$(ps auxf | grep openvpn | fgrep -v grep) if [ "$vpnExist" ]; then echo echo "Openvpn process already exists. Exiting." exit 1 fi cd /etc/openvpn/ovpn_udp # Picks a random file we need rServer=$(shuf -ezn 1 us*udp* | xargs -0 -n1 echo) echo "Random US server is: $rServer" # Finds the string 'auth-user-pass' in the choosen file and # adds '/etc/openvpn/auth.txt' at the end of it. sed -i 's/auth-user-pass/&amp; \/etc\/openvpn\/auth.txt/' $rServer # Forking the start of OpenVPN so the script can continue. /bin/bash sudo openvpn $rServer &amp; # Wait a few seconds to have the openvpn process above fully initialize. # Use a a number (seconds) that works for your system. sleep 5 # Ask Google what my new IP is after the VPN has been initialized. vpnIP=$(dig TXT +short o-o.myaddr.l.google.com @ns1.google.com | awk -F'"' '{ print $2}') rDig=$(dig +noauthority +noadditional +noqr +nostats +noidentify +nocmd +noquestion +nocomments -x $vpnIP | awk -F'"' '{ print $5}') echo echo "My new IP is: $vpnIP ($rDig)" echo exit 0 
You might consider installing mpd (music player daemon) on your pi. Its easy to control it from your android device with an app like mpdroid.
First, the general syntax for `if` is: if TEST-COMMANDS; then ... elif MORE-TEST-COMMANDS; then ... ... else ... fi Note that `elif` is used to test multiple conditions in turn, and `else` does _not_ have a condition. Putting that aside, the `LOGNAME` environment probably shouldn't be used for anything important. The user can set it to anything, or even unset it altogether. If you need to check that your script is running as UID 0 (which _may or may not_ be named `root`), use: if (( UID == 0 )); then # ... fi It may so happen that you only care whether the _effective_ user ID is 0, in which case you could check `EUID` instead.
slash dillimeter and paths doesent work as im aware try # can try to reproduce that later if problem is still up then..
:o Well said and thanks for sharing!
Nice! I didn't know that.
I put this at the start of scripts requiring root if \[ "$\(id \-u\)" != "0" \]; then echo "This script must be run as root" 1\&gt;&amp;2 exit 1 fi
 I put this at the start of scripts requiring root if [ "$(id -u)" != "0" ]; then echo "This script must be run as root" 1&gt;&amp;2 exit 1 fi
Change `s/` (your current substitute delimiter character) to `s@` ("@" being an arbitrary character that doesn't conflict with your desired substitution). `sed -i '22s@.*@auth-user-pass /etc/openvpn/auth.txt@'`
Ahhh this one is so close! I'm not sure about why s@ works and s/ doesn't, to be honest, but it does make sense to me that I need to specify which file to change. done that \(minus curly brackets\). Now the problem is that it creates the line *just before* the one that was already there, so now the ovp file has: `auth-user-pass /etc/openvpn/auth.txt` `auth-user-pass` which means I get prompted for user/pass again. if I delete that extra line, it works as expected. Sooo... why is the line there? Also, if you don't mind explaining the deal with the s@ vs s/ \(or I guess, s# as someone suggested\) and why there's a conflict? Thank you. 
This is... fancy. Could work, but I wouldn't even know how to start troubleshooting it if it doesn't... the whole thing is meant to be a learning experience and your skills are above my head kind sir :\) I'm actually learning bash from Lynda and making my way slowly. Thanks for the effort, and man, you were drunks... wow. \*bows\*
I added as much comments I thought were necessary. While not tested, I'm quite sure it will work. Use it as a reference. It does everything you wanted to do with your script, but fully automated. Best of luck!
Dear OP, just use the below and all your troubles will be gone... sed -i 's/auth-user-pass/&amp; \/etc\/openvpn\/auth.txt/' us${server}.privatevpn.com.udp.ovpn
Try searching for this: grep -E '"(first|last)Name"'
Yeah, I had tried that or something similar already. I just tried it just in case I screwed it up on my own. That returns nothing.
It works for me if I try it on that example you shared: $ grep -E '"(first|last)Name"' &lt; testfile "lastName":&lt;last name&gt; "firstName":&lt;first name&gt; "firstName":&lt;alternative first name, e.g. Jim instead of James above&gt; 
Ahh, you were passing it in using input redirection whereas I was just passing it the filename. I think that this will get me where I'm trying to get. Thanks!
That shouldn't matter, both filename and redirection via standard input should work.
That's what I thought, but I got different results. 
Ok, I'll check it out, thx. 
 Files: $ ls -l total 12 -rw-rw-r-- 1 x x 2376 Jun 3 20:00 input.txt -rwxrwxr-x 1 x x 326 Jun 3 20:10 process -rwxrwxr-x 1 x x 392 Jun 3 20:10 process2 $ cat input.txt Nullam ultricies mi vel eleifend dictum. "lastName":Smith Nulla non sem molestie, efficitur nunc ac, sodales massa. Aliquam fringilla odio luctus neque lacinia, et pharetra urna maximus. "firstName":James Maecenas vitae dui ut dolor porta congue. Aliquam ornare ligula hendrerit vestibulum dictum. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. "firstName":Jim Cras iaculis est id ex facilisis, vel aliquam lacus euismod. Donec commodo nibh in ligula hendrerit, ac tempor est iaculis. "lastName":Jones Quisque at metus molestie, ullamcorper mi vel, varius massa. Morbi ac sem pellentesque, viverra ex sed, ultrices eros. "firstName":Thomas Aliquam ornare ligula hendrerit vestibulum dictum. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. Ut bibendum sapien id congue dictum. "firstName":Tom "lastName":Johnson Aliquam ornare ligula hendrerit vestibulum dictum. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. Ut bibendum sapien id congue dictum. "firstName":Harold Quisque at metus molestie, ullamcorper mi vel, varius massa. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. Ut bibendum sapien id congue dictum. "firstName":Harry Aliquam ornare ligula hendrerit vestibulum dictum. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. Ut bibendum sapien id congue dictum. "lastName":Miller "firstName":Frederick Quisque at metus molestie, ullamcorper mi vel, varius massa. Morbi ac sem pellentesque, viverra ex sed, ultrices eros. "firstName":Fred Nam finibus velit et nisi sollicitudin sodales. Quisque at metus molestie, ullamcorper mi vel, varius massa. Morbi ac sem pellentesque, viverra ex sed, ultrices eros. "lastName":Samson Aliquam ornare ligula hendrerit vestibulum dictum. Nunc facilisis orci quis lorem cursus, vitae placerat est faucibus. Ut bibendum sapien id congue dictum. "firstName":Joseph Nam finibus velit et nisi sollicitudin sodales. Quisque at metus molestie, ullamcorper mi vel, varius massa. Morbi ac sem pellentesque, viverra ex sed, ultrices eros. "firstName":Joe Aenean euismod enim non odio rhoncus ornare. Cras iaculis est id ex facilisis, vel aliquam lacus euismod. Donec commodo nibh in ligula hendrerit, ac tempor est iaculis. Maecenas ultricies nisl sit amet massa feugiat sollicitudin. Morbi eu orci tincidunt, venenatis dui at, aliquam libero. $ cat process #!/bin/sh file=input.txt grep -n . $file &gt; /tmp/numbered-input.txt for lnn in `grep -i ":\"lastname\"" /tmp/numbered-input.txt | cut -f 1 -d:`; do grep "^${lnn}:" /tmp/numbered-input.txt | head -1 grep -A99 "^${lnn}:" /tmp/numbered-input.txt | grep -i ":\"firstname\":" | head -1 done rm -f /tmp/numbered-input.txt $ ./process 2:"lastName":Smith 5:"firstName":James 12:"lastName":Jones 15:"firstName":Thomas 20:"lastName":Johnson 24:"firstName":Harold 32:"lastName":Miller 33:"firstName":Frederick 40:"lastName":Samson 44:"firstName":Joseph $ cat process2 #!/bin/sh file=input.txt grep -n . $file &gt; /tmp/numbered-input.txt for lnn in `grep -i ":\"lastname\"" /tmp/numbered-input.txt | cut -f 1 -d:`; do last=`grep "^${lnn}:" /tmp/numbered-input.txt | head -1 | cut -f 3- -d:` first=`grep -A99 "^${lnn}:" /tmp/numbered-input.txt | grep -i ":\"firstname\":" | head -1 | cut -f 3- -d:` echo $last, $first done rm -f /tmp/numbered-input.txt $ ./process2 Smith, James Jones, Thomas Johnson, Harold Miller, Frederick Samson, Joseph 
Maybe that `$f` variable isn't set correctly when that line runs in your script?
better take the whole ```date``` command in ```$()```
Just tried that. Got: bash -x mvD.sh ++ date -r '' +%Y.%m.%d-%H%M date: '': No such file or directory +mv MyFile /home/User/Videos/MyFolder/MyFile..log Not sure what it means.
Your `$f` variable wasn't set at that point in the script. It was empty, and bash then translated `"$f"` into `""` (nothing).
Should i put the "$f" at another point in the script? like before the -r? 
I tried this and got: mv MyFile /home/User/Videos/MyFolder1/MyFile1.$(`date -r "$f" +"%Y.%m.%d.-%H%M")`.log &gt; It did nothing to the file and is expecting me to enter something after the &gt;.
Update: hmmm. now the same exact script that was working on it's own in the terminal is producing the same result as when i tried in the .sh script: MyFile..log Which i don't understand because I changed nothing.
That particular line should stay like it is, but think about where you are writing a value into the `$f` variable. There should for example be a line like `f=...` somewhere, or maybe something like `for f in ...`, where a value for `$f` is set. That's apparently not happening in your script.
what are your reference file and why do you need it? just do: ```mv MyFile /home/User/Videos/MyFolder1/MyFile1.$(date +"%Y.%m.%d.-%H%M").log```
Enlighten us with your script OP. 
I guess you've renamed your original file. So now, it is missing. it is very useful to use bash debug to understand what is wrong. run it with ```bash -x &lt;yourscript&gt;```
Care to share the pros and cons compared to the other options you're aware of? I use [bats](https://github.com/bats-core/bats-core) and it works decently well, but it certainly could be better.
Duhh. Woke up this morning, with a bit of a hang over, nevertheless, and the first thing I see in the short script is that I have sed showing up *twice*, line after line. I have no idea how I missed it. Deleted the first sed, now only have `sed -i '22s@.*@auth-user-pass /etc/openvpn/auth.txt@' us${server}.privatevpn.com.udp.ovp` and it works!! Yes!! Now I can start getting a bit fancier and more secure. Considering an option to encrypt auth.txt or hide it somehow so it only shows after password or something of the sort. Another thing I tried \(and failed\) is to get the script "type in" \(keyboard input?\) my user/password for me, and then compile the script \(don't know how\) so password can't be read. That would be interesting. At any case, thanks guys! Still trying to figure out what's the s/ vs s@ thing. 
You could connect using the [zssh](http://zssh.sourceforge.net/) SSH wrapper and transfer files via ZMODEM, like people used to do on BBS in the eighties.
My apologies for not getting back to you earlier. I managed to get it figured out to my liking. Thanks for all your help. 
Thanks for pointing that out. Yeah, it definitely looks similar (and way more mature) than what I'm trying to do. It does look a little more heavyweight than what I was going for though. Desk looks like it would definitely be better if you have well-defined needs for a particular project, but I still think that localalias still serves a purpose for spur of the moment type situations. For example, I often find myself using localalias to create aliases that I use four or five times and then abandon, never to use again. This is worth it for me because it takes me barely any more time to create an alias and run the command then to just run it. But I wouldn't go through the trouble creating a desk workspace for such silly things. 
I've always had great luck using [Asciinema](https://asciinema.org/) \(and it seems to be the de facto standard\). The resulting file when you save is just a text file that you can easily manipulate to edit parts out/speed things up/etc. 
Despite it being `python` and this being /r/bash, that seems pretty cool. I recently tried to do this using `asciinema` and it flipped out. I've been trying out Green Recorder, but I might give your script a shot too.
I think green recorder just records your screen right? The script I wrote doesn’t do that. It just automates the actual keystrokes. Have you tried using Peek for screen recording? I’ve had a good experience with that one.
I think python would work for this, but not using the yaml library since I don't see a way to comment out a block based upon a parent. Seems like a simple readline iterator, determine if the header is found, determined the length of indent to know when to end commenting, and write out the line to a new file with a # followed by the rest of the line. Once the indent length of line has decreased, you stop commenting. 
 #!/bin/bash search_string="header2" files="foo" #files=$(find /tmp -type f) for file in $files; do start_line=$( awk "/^$search_string/ { print NR; exit }" $file ) match_length=$( tail -n +$(($start_line + 1 )) $file | awk '/^[a-zA-Z0-9]/ {print NR; exit}' ) end_line=$(( $start_line + $match_length -1 )) sed $start_line,$end_line"s/^/#/" $file done 1. Find the `$start_line` number that matches our `$search_string`. This is the line we want to start commenting from. 2. Measure the number of lines that follow after our match and until the next non-indented line, this gives us out `$match_length`. 3. Calculate the last line we want to comment by doing some math 4. Use sed to comment the beginning of the line `^` with a `#`. The output of my test file looked like this: header1 sub_header1 # I want to comment from here - a - b - c # to here, and stop at the next equal indented header #header2 # sub_header2 # - a # - b # - c header3 sub_header3 - a - b - c Hope this helps :)
Thank you so much for helping out. No dice so far... It's commenting out the search\_string, and nothing else. To clear things up, all files are correctly formatted in yaml using the 2 space indentation, and I definitely copied the directory of the files to test in... We've all made that mistake once, thank you for checking. Here's an example file, with the true config terms omitted due to privacy \(corp\): name ############### config_parent: config: subhdr_1: - value: "2880" - value: "True" - value: "5" - value: "/var/data" - value: "/var/foo" - value: "180" - value: "True" - value: "True" - value: "40000" - value: "18" nginx: ipfilters: allow: - 192.168.1.1 - 192.168.1.1 
Also, here's the script with the applicable changes \(in case there was something messed up\). I am using OSX, and tried OSX native sed and awk, and also gnu\-sed and gnu\-awk, neither worked \(tried it in debian vm, just to be sure\): #!/bin/bash search_string=' tmp_config:' files=$(find ./tmp_dir -type f) #files=$(find /tmp -type f) for file in $files; do first_line=$( awk "/^$search_string/ { print NR; exit }" $file ) match_length=$( tail -n +$(($start_line + 1 )) $file | awk '/^[a-zA-Z0-9]/ {print NR; exit}' ) last_line=$(( $first_line + $match_length -1 )) sed $first_line,$last_line"s/^/#/" $file done
I'm on OSX too - the reason it was only commenting one line was potentially because it wasn't finding another parent level block. I incorrectly made the assumption that we were commenting at the first level. This one's a little less automagical, but should work:
bash has that functionality built in. Look into the `select` loop construct.
select Hey, thank you. TIL!
 [[: not found Looks like you’re running it with `sh` instead of Bash.
Here's kind of an unattractive Perl script for it: perl -pi.bak -e 'if ( $in_hdr ) { if ( m/^(\s+)/ &amp;&amp; length($1) &lt; length($indent) ) { $in_hdr = 0; } else { s/^/#/; } } elsif ( m/^(\s+)subhdr_1:\s*$/ ) { $in_hdr = $indent = $1; s/^/#/; }' *.yml Change `subhdr_1` to the actual header you're looking for, and you can change the glob if you need to. You can delete the `.bak` files afterwards if you're happy with the result. This assumes that the indentation in your files is correct obv
There's always a Perl one-liner: $ perl -ple 's/^/#/ if ((($i) = /^(\s+)sub_header1/)..!/^$i/) =~ /.(?&lt;!E0)$/' input.yml header1 # sub_header1 # I want to comment from here # - a # - b # - c # to here, and stop at the next equal indented header header2 sub_header2 a b c header3 Use Perl's `-i` option to do an inplace edit, if you're game enough. You can pass multiple filenames (e.g. with `xargs`) to have it edit multiple files.
&gt; ./VoT.sh: 1: ./VoT.sh: netstat: not found &gt; ./VoT.sh: 1: ./VoT.sh: ifconfig: not found And also like `net-tools` is not installed (run `sudo apt install net-tools`).
You can use `printf` to convert decimal numbers to hexidecimal. So something like function would do the rename: rename_file() { local filename=$1 if [[ $filename =~ ^(\d+)_(.+)\.dat$ ]]; then mv "$filename" "$(printf '%04X%s' "${BASH_REMATCH[@]}")" fi } Chuck that into a loop to process each of the files one by one.
 "${BASH_REMATCH[2]}") should be "_${BASH_REMATCH[2]}" to preserve the _ in the file name (or i guess include the underscore as part of the capture. 
Thanks for pointing me towards this project. I have never heard of it before. Do you know if there is an easier way to either play from a text file or edit the *.cast file that is created by making a recording? This way I can get rid of typos and edit the recording without having to redo it? I recorded a pretty simple example (3 lines) and the cast file was 262 lines long and very difficult to interpret. Otherwise, it really can't be used as an alternative to the script I wrote. I use Peek with this script to create GIFs. It looks like if anything, asciinema could serve as an alternative to Peek.
Here's kind of an unattractive awk script that seems to work: #!/usr/bin/awk -f /^ *sub_header1$/ { print "#", $0; margin = index($0, $1); while (getline sec) { if (substr(sec, 1, margin - 1) ~ /^ *$/ &amp;&amp; substr(sec, margin, 1) != " ") { print sec; break; } print "#", sec; } next; } { print; }
Try basename: https://stackoverflow.com/questions/3362920/get-just-the-filename-from-a-path-in-a-bash-script
the command is 'basename' FILENAME="$(basename ${OBJECT})" by the way you forgot to set the DESTDIR variable - and why are you trying to execute $OBJECT ? (final word in last line).
Like I said, it's a text file with timestamps and it shows you all the output from both stdin and stdout. You can delete lines to remove them or edit them in place for typos. It's incredibly simple.
The text file I am getting is not simple at all though. Checkout this scenario: I run `asciinema rec test.cast`. I then run `echo "Hello World"` followed by `exit` (to stop the recording). Here are the resulting contents of `test.cast`: http://ix.io/1coR/ I have no idea what to do with that.
I dunno how yours is looking like that. The default output looks like this: [https://imgur.com/a/xdttdHd](https://imgur.com/a/xdttdHd) definitely not all shoved together. Pretty easy to follow this one along.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/eyQNkFk.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e05ni53) 
You might have looked at my old link. The new one is not shoved together (I shouldn't have added a slash to the URL). Still though, judging from the image you posted, I would still need to change at least 6 lines just to change `echo "Hello World"` to `echo "Hello Buddy"`? It would be easier to just re-record. That kindof defeats the purpose of what I was going for.
I did see the one shoved together. For a simple 3 command thing, maybe it would be. I've used it for hour long demos and it worked just fine to edit a couple lines here and there. You can change the timestamps as well to whatever you want to remove lag, add delays, etc. I get that you wrote a competing tool so you're getting a bit defensive about it, I'm sorry, I'm not attacking your tool at all, just answering your question looking for other options. I figured asciinema would be good to mention since almost every terminal cast you see is going to be done using asciinema as of now.
No dude. Your awesome. I totally appreciate you posting and letting me know about asciinema. I'm just attacking flaws in the comparison to see if you know better how to fix them. Besides, that script I wrote is like 2 hours of work and barely thought out. Its nowhere near being considered competition for asciinema. Sorry if I'm coming off hostile. But not every argument is a fight. :)
Alright cool, I must have just misinterpreted your tone then, my apologies.
With the `perl-rename` tool, you would do it like this: $ perl-rename --dry-run 's/^(\d+)_/ sprintf "%04X", $1 /e' * 1_qwer.dat -&gt; 0001qwer.dat 256_asdf.dat -&gt; 0100asdf.dat 2_zxcv.dat -&gt; 0002zxcv.dat 65535_qwer.dat -&gt; FFFFqwer.dat This is just printing what it would do without renaming. You have to remove that `--dry-run` parameter to actually rename the files. I've seen this command exist under different names. I've seen it as `perl-rename` and as `prename` and as just `rename` (but for this last one, there's also another Linux tool with the same name). What's going on depends on your distro.
Hey! I guess my goal was to write a pure assert library with no DSL, and to have something that is super fast to test with.
Thanks guys, got it working! could you please check my script and tell what could be improved :) #!/bin/bash NAME="docs-est" DESTDIR="/path/to/download/data/" BUCKET="BucketName" OBJECT="$(aws s3 ls $BUCKET --recursive | sort | tail -n 1 | awk '{print $4}')" aws s3 cp s3://$BUCKET/$OBJECT $DESTDIR echo ${OBJECT##*/} tar -xvzf ${OBJECT##*/} &amp;&amp; cd home &amp;&amp; sudo chown -R user:group docs-est/ &amp;&amp; cd .. &amp;&amp; ln -sf home/docs-est/ public_html &amp;&amp; rm -rf ${OBJECT##*/}
So I guess since I'm at 350 lines and realistically not even 50% done with my script I should have learned some python instead of procrastinating about learning some python. 
&gt;Edit: Well for some reason it doesnt treat the other two Scripts as code. Indent by four spaces to get a code block, or use backticks for `inline code` Assuming you are using bash 4+, you might like to look at something like this: readarray -t smartFiles &lt; &lt;(find ...) Demonstration: ▓▒░$ readarray -t binArray &lt; &lt;(find ~/bin -name "*") ▓▒░$ printf -- '%s\n' "${#binArray[@]}" 2969 ▓▒░$ readarray -t binArray &lt; &lt;(find ~/bin -name "pants") ▓▒░$ printf -- '%s\n' "${#binArray[@]}" 0 Is that kind-of what you're after? for file in $(ls $DOWNLOADDIR/debug$DATE/$variable/result/smart.result) do grep -i "Model Family|Device Model" $file &gt;&gt; $sg done &gt;How can I just use the array for this? Is that possible? Don't parse the output of `ls`. It's a golden rule - remember it. There's plenty of reading about that available via google, [here's a start.](https://mywiki.wooledge.org/ParsingLs) UPPERCASE variables... [Avoid unless you need them](https://www.reddit.com/r/bash/comments/8nau9m/remove_leading_and_trailing_spaces_from_a_variable/dzurbak/). And yeah, you can loop through your array for this e.g. for element in "${myArray[@]}"; do do all the things done Finally, check out http://shellcheck.net, you probably have other basic issues going on... 
I think that is what I am looking for, yes, thanks! I will take care of the uppcase variables and i dont need to take care of the ls at the moment, because to files will never have empty spaces or any other special characters in them. I do have a Problem with the loop though. How do I reference to the current $element inside the loop? I tried to do it with $element inside the loop, but this just seems to give me number of elements found (5).
You can disable that annoying `*` behavior like this: shopt -s nullglob That "nullglob" setting is documented like this in `man bash`: &gt; **nullglob** &gt;If set, bash allows patterns which match no files (see Pathname Expansion above) to expand to a null string, rather than themselves. 
Do you systematically double quote `$variables` ? Did you mutate the array at some point before giving it to the `for` loop ?
Yes I do double quote. It looks now like this: readarray -t SmartFilesOld &lt; &lt;(find "${DOWNLOAD_DIR}/debug_${DATE}/${variable}/result/smart"*.result ) if [ "${#SmartFilesOld[@]}" -eq 0 ]; then echo "No Smart-files found in old directory." fi for i in "${#SmartFilesOld[@]}" do echo $i done It still only outputs the Number of found files, not the elements themselves.
Remove \`#\`
That worked, thanks
As I understand it, `done` terminates the `while` loop, and `&lt; ~/.badges` reads the `.badges` file into the loop for iteration. The loop then goes through each line of `.badges` and gives you `$badge` to work with during your iterations. Hope that helps. More details: A simple example of IO redirection into a code block: https://www.tldp.org/LDP/abs/html/special-chars.html#EX8 More on redirection, in general, can be found here: https://www.tldp.org/LDP/abs/html/io-redirection.html
&lt; ~/.badges reads the contents of the file, and sends it to sdtin. while read (which is missing a -r BTW) directory badge will parse each line read, using space as separator to set the $directory and $badge variables.
So the `&lt; ~/.badges` is just a normal redirection, like you've seen before. Like, `cmd &lt; ~/.badges` sends the content of badges to `cmd` on its stdin. Here, you're sending the content of badges to the loop -- in particular, to the `read` condition of the while loop. The `while` line has two conditions and the `||` short-circuits them. On each iteration of the loop, first it will run `read directory badge` to get the two variables directory and badge from stdin (the badges file as we saw above). If the read succeeds, the condition is true, so the loop will execute. If it doesn't succeed, it also tries the other command, `[[ -n "$directory" ]]`. If this is true (directory is not the empty string) then the loop will also execute. If neither are true, the loop ends.
`$(...)` will remove trailing newlines. So you could do $ script=$(printf '%s' '#!/bin/bash') $ script+=$(printf '\n%s' 'function()') $ echo "$script" #!/bin/bash function() or $ script=$(printf '%s' '#!/bin/bash')$'\n' $ script+=$(printf '%s' 'function()')$'\n' $ echo "$script" #!/bin/bash function() 
A heredoc may be a better solution to your problem.
&gt; script+=$(printf '%s' 'function()')$'\n' Thanks! 
Could you elaborate? I'm not very familiar with bash yet.
Thank you this is helpful.
https://mywiki.wooledge.org/HereDocument Basically, if you're writing a bunch of lines somewhere (a process, a file) it's easier to write them all at once than trying to printf or echo a zillion things, which you see has pitfalls: cat &gt;some_script.sh &lt;&lt;END_SCRIPT #!/bin/bash function() { echo "$message"; } END_SCRIPT
Ah, thanks. Though i chose to add every line separately since some lines have variables that i want to put in and some lines have variables that i want to escape (variables that will be in the build script). So in some cases i use "$message" and in some cases i use '$message'. It seems like it would be hard to do with the heredocs style? 
Not really: $ x=5 $ cat &lt;&lt;FF &gt; #!/bin/bash &gt; foo() { echo "$x"; } &gt; bar() { echo "\$x"; } &gt; FF #!/bin/bash foo() { echo "5"; } bar() { echo "$x"; }
Seems pretty neat, will try this out. But just one little question: do i escape the escape `\` symbol by doing `\\`?
Nope, that text is as I typed it verbatim.
Well i was actual curious if escaping the escape symbol in heredocs works as i would expect. So if i want `x` in `"first\$x\third"` to still be a variable, i would think of doing `first\\$x\third`. But it's a bit silly of me to ask something that i can check myself, so i did and it seems to work as i expected `\\` will escape the escape character. Anyway, thanks for the suggestions!
Alternative suggestion: use grouping and output redirection. { printf '%s\n' '#!/bin/bash' printf '%s\n' 'function()' printf '%s\n' '{' printf '%s\n' "echo $message" # ... } &gt; someScript.sh (By the way, if `$message` might contain odd characters, this might be safer: `printf 'printf "%%s\\n" %q\n' "$message"`.)
Thanks!
Spot on! Thank you!
Perfect! Thank you!
I wrote this yesterday so it's a WIP but it's fairly complete. It's a tiny pixel art editor for your terminal. There's a screenshot in the link above so check it out. - Vim hjkl movement. - Supports 256 terminal palette. - Supports true color terminals (all hex colors). - Draw with any character or string. - Save and load "screens". - Open regular text files for editing. - Responsive on window resize. It's all pure bash minus a call to `stty` if the hack to get terminal lines/columns in pure bash fails. I also need to figure out a way to export to an image file. Right now when you save/load a "screen" it just logs all key-presses to a file. Mess around with it and let me know when you break it. :P
&gt; Is it possible to set shopt -s nullglob only for a part of script? You can do this: shopt -s nullglob ... shopt -u nullglob This has the problem that you have to be careful with copy and paste in the future to other scripts. Your other script might need nullglob by default because of other places where it uses globs. Or you can do the following: ( shopt -s nullglob ... ) This has the problem that it creates a "sub-shell" where all kinds of other changes also won't change the environment on the outside. For example, besides that nullglob change only being done inside the `()`, changes to variables will also disappear after the `)`. Doing a directory change with `cd` will also be taken back after the `)`.
Just glancing at it, without knowing much about any of it, the reload error probably means that the script has broken your `ferm.conf` `journalctl --no-pager -uferm -n10` will probably give you more useful journal output `/etc/ferm/ferm.conf` is where the `ferm.conf` file lives. If there's an error it'll likely be something obvious like missing braces or semicolons
The question it asked you was likely a warning about the server key changing. That's an indication that the server you're connecting to is different than it was the last time you connected to. Two likely causes: 1. You changed something on the Mac that caused it to regenerate the server key. 2. The address you're trying to connect to is actually going to a different machine. Some possible debugging steps: - double check the address you're connecting to. - check logs on the Mac to see if you can see the attempted logins - use Wireshark to see if the ssh traffic is present. Bash does have logging, but you have to enable it (command is `script`). Or you can just copy and paste the contents of your graphical terminal.
hunter2
How would "the internet" know the password to YOUR personal login on your personal device? How on earth do *you* know know your *own* password, to *your own* account, on your own device? 
back up. List exactly what you've done on the Mac and what you're trying to do on the Linux box. Paste the commands exactly as you type them (hide the password, of course).
i knew to know where my password is stored
what username are you using? Use the password that does with that user name. 
on my linux: ssh username@remoteaddress it then prompt's for a password. Icloud password wont work. The current user password wont work. Ssh server is already on the mac. It *comes* with it. You just toggle it on or off in remote managment in the system preferences. There was no download or account creation. It's just *there.* I typed ssh-keygen -t rsa -C "email@email.com" It created the key and stored it in a .ssh/id_rsa file. Do i need to connect that to another server or website to use it?
bruh im an idiot. i got it.
on your linux box, ssh yourmacusername@themacsipaddress when you get a password prompt type your login password for your user on the mac. Not the icloud password.... Make sure your user is allowed to ssh in by selecting the 'All Users' radio button. That should work. If not, on the linux box, do it again this time with ssh -vvv username@themacsipaddress again. Paste the log. 
By the way, just to confuse you further :) you can take the /home/you/.ssh/id_rsa.pub file from your linux box and paste that into /Users/you/.ssh/authorized_keys and avoid the password prompt. Let's leave that alone for now and get password authentication working first.
I'm not sure I understand the question. `scrot` is already a CLI utility.
OP wants to select the coordinates and pass them on to another function, instead of taking a screenshot.
Agree, I see it now. How about this? https://bbs.archlinux.org/viewtopic.php?id=85378
No.
Thanks. I tried another workaround, using `set` to dump variables and functions to a tmp file, Then I source it in the child process. But `set` dumps builtin variables, and what's worse, including readonly variables, and some-how syntax-errored functions. `set` for dumping seems to be unusable.
Just launch a new child shell. The overhead of loading .bashrc again is negligible. You're worrying about nothing.
I can notice a time lag when entering a workspace. It's not a serious issue. I'm just wondering whether it could perform better.
You could try the `--rcfile` option to load an alternative init script instead of .bashrc. See the man page for details.
I like it. 
Thanks, I'll be glad to hear more feedback.
 I have read many comments lately about bash for short scripts only. Which had me kind of demotivated. I'm a hobbyist at best and I foolishly told some friends that own a deer processing shop that I could write them a script for checking deer in. I'm at 823 lines right now with plenty more to go. Good to see someone much smarter than me is OK with longer bash scripts. Thanks for the unintentional motivation.
I got it working. Thought that I was supposed to use the local Mac username. Not that account username. Thanks
Jesus christ, why java for this? Its really cool, i really like it's purpose, but still. If you are learning Java then it's totally legitimate.
I dunno if I’m smarter than you. I’ve just been doing this for a long time. Maybe I’ve lost my mind. :D
Why not java?
Maybe key 86899 isn't in your database?
awk is never attractive. But it doesn't have to be sexy to be awesome.
For this kind of tool i would expect something a little bit more scriptable and modifiable like python or even bash. This is just overkill and java is for me little bit messy. I see you like python and use it often, why not python? :)
Very clever concept!
I'm not OP I just chimed in out of curiosity. What parts of this tool would you script? To me it seems to have a pretty straightforward purpose and I can't think what part of it could be improved by introducing a scripting element to it. 
i do not know the details. but stdin is getting slurped up the bash instance. regardless inside that while loop fd 0 is pointing to a pipe and nothing you can do to get a interactive shell in that case anyway (that i know about). you could use a different file descriptor for find if you wanted... not very different from just using that array solution. sidenote: byebye could just be `exit` i think. exec 3&lt; &lt;(find . -mindepth 1 -maxdepth 1 -type d -print0) while IFS= read -r -d $'\0' -u 3 directory; do pushd "$directory" &gt;&amp; /dev/null || continue bash --init-file &lt;(cat &lt;&lt; EOF source ~/.bashrc byebye() { kill -SIGTERM \$\$; } trap "kill -SIGTERM $CPID;exit" SIGTERM SIGQUIT SIGINT echo -n "Enter subshell $i of $count - "&gt;&amp;2;pwd EOF ) popd &gt;&amp; /dev/null done exec 3&lt;&amp;-
if you're doing multiple it's cleaner to use `--exclude-from=exclude_list.txt` and keep an exclude_list.txt file with all the things you want to exclude. Also you are running rsync twice, each with different excludes which make them cancel each other and sync everything
Maybe post the code in a gist or pastebin, it's pretty unreadable as a reddit post.
i think i fixed it. 
No - move it to a different site. Can’t read it via Reddit.
My bad, it looked good when i visited the post. Got a link up now. 
Here's something to copy+paste for creating testfiles for other people who want to experiment with this problem: touch 'a.png, Warsaw, 2016-02-13 13:33:50' touch 'Eiffel.jpg, Paris, 2015-07-23 08:03:02' touch 'myFriends.png, Warsaw, 2013-09-05 14:07:13' touch 'b.jpg, Warsaw, 2016-01-02 15:12:22' touch 'e.png, Warsaw, 2016-01-02 09:49:09' touch 'notredame.png, Paris, 2015-09-01 12:00:00' touch 'BOB.jpg, London, 2015-08-05 00:02:03' touch 'f.png, Warsaw, 2016-01-02 10:55:32' touch 'photo.jpg, Warsaw, 2013-09-05 14:08:15' touch 'c.jpg, Warsaw, 2016-01-02 14:34:30' touch 'john.png, London, 2015-06-20 15:13:22' touch 'pisatower.jpg, Paris, 2015-07-22 23:59:59' touch 'd.jpg, Warsaw, 2016-01-02 15:15:01' touch 'me.jpg, Warsaw, 2013-09-06 15:40:22' 
Thanks. i just figured it out. I was going to do the list thing as well.
Try collecting the cities list first: ``` CITIES=( $(ls * | awk -F "," '{print $2}' | uniq | sort | xargs) ) echo ${CITIES[@]} ``` Took some googling, but `xargs` strips whitespace. Then you can use each city individually ``` for city in "${CITIES[@]}"; do IMAGES=( $( ls *$city* | awk -F "," '{print $2 $3}' \ | while read filename; do ISODATE=$( cut -d' ' -f 2- &lt;&lt;&lt;"$filename" ) echo $( date --date "$ISODATE" +%s ) done | sort -n ) ) counter=0 for image in "${IMAGES[@]}"; do counter=$((counter+1)) extension="lost" printf "%s%02d.%s\n" "$city" "$counter" "$extension" done done ``` What a heck of a problem. Hopefully i've shown a different approach? I haven't managed to track the original filename and extension through to the end (which would be important for doing anything useful here!) But this ought to be a decent skeleton for a different approach. Good luck
Umm I am guessing what you want is something like this? Not sure, pls next time explain it better ls | sort -t, -k3 |awk -F ',' '{val=substr($2,2); seen[val]++; print val "0" seen[val] substr($1,index($1,"."),100);}' Warsaw01.png Warsaw02.jpg Warsaw03.jpg London01.png Paris01.jpg Paris02.jpg London02.jpg Paris03.png Warsaw04.png Warsaw05.png Warsaw06.jpg Warsaw07.jpg Warsaw08.jpg Warsaw09.png If so you just need to use `mv` to change the filename
Maybe bust it up into functions? Separate files?
Use a debugger. [https://www.shellcheck.net/](https://www.shellcheck.net/)
Dang, separate files would have made sense. Then a main one to call them in order. That would at have at least made future changes easier. Definitely going to do that at least.
What kind of help you need? To make bash your default shell again. Just do it the same way as making zsh your default shell. chsh \-s $(which bash) [https://www.sanfoundry.com/4\-practical\-chsh\-command\-usage\-examples\-in\-linux/](https://www.sanfoundry.com/4-practical-chsh-command-usage-examples-in-linux/) [https://bash.cyberciti.biz/guide/Chsh\_command](https://bash.cyberciti.biz/guide/Chsh_command) Just experiment with those clips you have there. Don't do them all at once. Just a few at a time. You can put the whole thing in if you like. Just commit out some of it. So you can test things properly. Before removing the commits to enable things.
Hey you really did it thanks!! But could you explain me a little bit what you did? The part of the: seen[val]++; print val "0" seen[val] substr($1,index($1,"."),100); Looks really new for me. Thanks again!!
That taught me not to use variables in the printf format string. Thanks for the tip. Guess i got lucky that hadn't caused me a problem yet. 
Glad to help. I just dab a little on all coding methods. Not a pro at all. Except on HTML and CSS(markup languages). Everything else like bash, C, python, golan, rust, lua, etc. I'm just a novice on it all.
Thanks. This works with another fd. Still, I did not expect that stdin is read by bash.
Calling myself a hobbyist is a bit of stretch. I sat down to start hashing this ticket for out. Next thing I knew I was several hundred lines deep and was getting lost in own sauce. At that moment I realized, surely there is a better approach than my current organizational skills.
Sure. `seen[val]++; ` means in the array 'seen' in the position 'val' increment the value by one. That is our counter, to know how many of that type you have seen so far. And in awk not initialized variables are 0 so the first you get with this (0++ is 1) is a 1. The print has several parts mixed in (you can see it is all part of the print because there is no ';' separating them): `val` is the string of the place ($2 starting from position 2 instead of 1, to get rid of the initial space) `"0"` is just a 0 string, that I understood you needed based on your opening comment. `seen[val]` holds the counter `substr($1,index($1,"."),100)` gets the format of the image. `substr` returns $1 from the position indicated from `index($1,".")` (which means get me the position of the first dot in $1) and the 100 is the limit of characters to get (this argument is optional, I added it because I forgot). So this `substr` command gives us $1 from the dot to the end.
Could always go with node if you're more familiar with JavaScript. Tends to be cleaner than doing everything in bash...sometimes. 
I always put the -- parameters before any - parameters. It used to be necessary.
I'm not familiar with anything really. I'm a construction worker by trade. I went the entire decade of my 20's not owning a computer because "I didn't want anything I couldn't fix with a hammer". Then I realized computers weren't going anywhere and I was crippling myself with my stubbornness. So about 18 months ago I got one and installed linux. Which blew my mind. Wrote a few small scripts less than 20 lines to help do some tasks. Then foolishly told a friend who was looking at some software that cost a couple thousand dollars I think he could do everything he needed with bash. I clearly underestimated how much work would go into a simple check in script and figured since I lurk around on reddit I may as well ask some people smarter than me how to better organize or maybe some shortcuts I'm not aware of.
&gt;Still, I did not expect that stdin is read by bash. yea this is how is works. also in other shells. bash's man page isn't as blunt as dash's is. &gt;-s stdin Read commands from standard input (set automatically if no file arguments are present). so, afaik just avoid this situation - using a for loop or redirecting to a different fd. still, am curious if there is some trick to make interactive with stdin loaded up - even an ugly one. &gt;Using exit would not help since it would continue the loop but I want to exit the whole loop with byebye which is why I trap the signals. well, in that case i think you want to kill `$PPID`?
Yes
Oh wow, it looked alright on desktop but mobile it's a jumbled mess of nonsense. 
You are having a bunch of mistakes in your script that makes it not work at all. I can see missing spaces around a `==` word, and I can see missing `done` words. Try to fix that stuff first before continuing. Check out a tool named `shellcheck`. It's really neat. Your distro will likely have a package for it, and you can also test it online at www.shellcheck.net without having to install it. It will help you a lot because it's really easy to make mistakes in bash. There is an evil type of mistakes you can do in bash where for example those missing spaces around a `==` won't cause bash to print any error message, instead the line will silently not work right. It will always be seen as "true", no matter what the variable contents are. With regards to mistakes about `done` and `fi` and `esac` words, see if the editor you are using has a plugin to do "auto-indentation" of your script. It's something where the editor will automatically add or removes spaces at the beginning of lines while you type stuff or while you copy things around. You'll see immediately that there's for example a `fi` missing somewhere because the indentation will look off.
Thank you! Yeah I very been having trouble with indentation and closing conditionals, I will definitely check those out. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Look up pushd/popd.
Run your script with `bash -x scriptname` to see what bash sees after it interprets your lines. Maybe that will help to find out what's going wrong. About what you should change, perhaps make the 'find' program only look for directories so that your search pattern will not trigger for normal files as well. You can do that like this, by adding `-type d`: output=$(find / -type d -iname '.test*' -print -quit 2&gt;/dev/null) Add quotes to the line with `cd` so that the command won't break if there's a space in the directory name: cd "$output"
If you are trying to change the directory of your bash prompt, that's not possible through a script. The `cd` command in your script will only change the directory inside your script. After the script ends, you will be back to where you were originally at the bash prompt. If you want to change the bash prompt's current directory, you need to turn your script into a "function" and add that function to your ~/.bashrc file. The function would look like this: findtest() { local output=$(find / -type d -iname '.test*' -print -quit 2&gt;/dev/null) if (( $? )); then echo "Folder is not located" else echo "Folder is located ${output}" cd "${output}" fi } I changed your code a little to fix mistakes. I added a `-type d` to the find command to make it only trigger on directory names and not also on normal files. I added quotes on the cd command line to make it work if there is a space character in the directory name. I added a check for the find command not finding anything. Check out a neat tool named "shellcheck". It tries to find mistakes in bash scripts and is really helpful because making mistakes is really easy with bash. It will tell you things like where to add `"` quotes. Your distro will likely have a package for it.
Hey, nice elegant function there. It always suprises me when people come up with neat solutions such as this. Linux and the whole thousand ways to skin a cat and stuff.
Are those functions in the PATH? Haven't seen those functions ever.
Pretty sure it's a joke
Hey this is great info thanks! I was wondering when adding anything to .bashrc does it automatically run it after saving it? If so all I have to do is run a script to execute the function? If I add aliases here as well why dont they populate the alias when I type them?
Thanks will do 
This seems to be a joke, but [Wikimedia Toolforge](https://wikitech.wikimedia.org/wiki/Help:Toolforge), for example, does have a `become` command ([source](https://github.com/wikimedia/labs-toollabs/blob/master/misctools/become)), which is used to assume the identity of a tool account: you log in as your personal account and then `become yourtool`.
You have to end your shell session or source your .bashrc to use what was just added. By default, .bashrc is only loaded when you open the shell. source .bashrc will reload it in your current session.
If you are referring to the screen shot tool included with [GParted Live](https://gparted.org/livecd.php) then see the [GParted Live Manual on Capturing a Screenshot](https://gparted.org/display-doc.php?name=gparted-live-manual#gparted-live-capturing-screenshot).
solution verified - I wanted the output to be `~/.test` rather than `/.test`when I added that command in the .bashrc file it looks as though it created the directory /.test, where can I change this?
This helped, thank you. 
I don't understand what's happening. The command to create a directory is "mkdir", and there is nothing like that here or in what you showed earlier.
Here is a snip of what Im saying, Im probably not using the correct verbiage to explain myself, I apologize lol newbie here. I know the mkdir is the command for a new dir, it might also be a permission thing. I created a VM environment to learn bash scripting. patronage@ubuntu:~$ findbhash Folder is located /.bhash patronage@ubuntu:/.bhash$ pwd /.bhash patronage@ubuntu:/.bhash$ cd patronage@ubuntu:~$ cd .bhash patronage@ubuntu:~/.bhash$ pwd /home/patronage/.bhash patronage@ubuntu:~/.bhash$ 
+1 for this. When I started getting into more OOP languages I changed my bash code style to use functions and have a main run function that would execute my workflow. It definitely makes the code easier to break out and look at piece by piece. Although a 1,000 line shell script is probably something I would not attempt, but that is just me.
OP, this is amazing. I've been looking for such a program that actually works. And, holy shit, this works.
Cheers! If you come across any issues or have any features you'd like added, let me know. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Found a simplistic answer finally: `eval` ``` #!/bin/bash my_args="--some-arg --another-arg --foo-bar" eval "/bin/some_executable command src dest $my_args" ```
Use an array.
You don't need `eval`; your original method works fine, at least as written. Adding `eval` to the mix just creates an unnecessary step and will probably cause confusion if you ever expand on the script. The issue with the "one string" method is that you will quickly run into problems if the argument string ever contains special characters (especially whitespace). That's why it was suggested that you use an array instead; properly quoted, they are immune to those issues. If your string is never going to contain special characters, though, just do it the way you had it.
Please help
The best way to do this is an array: args=('--some-arg' '--another-arg' '--foo-bar') if some_condition; then args+=('--verbose') fi /bin/some_executable command src dest "${args[@]}" `"${args[@]}"` expands to one word for each element of the `args` array, even if some elements contain spaces or other special characters.
Save the script in `~/.local/bin/`, make it executable (`chmod +x ~/.local/bin/imgls`), and ensure `~/.local/bin/` in in your `$PATH`. --- Off-topic, but if you’re using zsh, why are you asking in /r/bash?
The script itself is a bash script, I didn't think the organization of the files would really depend on the shell. Much appreciated!
Okay, fair enough :)
export says : Take this environment variable and make it available to any binaries or scripts that you run "from this shell". Only children (child processes) can inherit the environment variables that are exported. In your last script, the value was available to the test_script child, but the child cannot pass it back to the parent. Only children get the benefits of exported environment variables. In C, you have: argc, argv, and envp passed into the main() function. main has access to all those.. but they are passed in by value. main has no way to send those values back to its parent. main can only return an int 
remove the `&amp;` from `test_function &amp;` you don't want to background the test_function
That answers why testvar wasn't being modified in the above example, but what about [this](https://pastebin.com/A5Nyfpf8)? It seems to me that without the export, the subshell shouldn't have access to testvar, and would echo nothing. Instead it is able to access testvar. [Here's] another example. I would expect that line 11 would change the value of testvar in both the parent shell, and its subshell (inside of test_function). As a result, test_function would echo 2 (since testvar is set to 2 well before it gets to the echo line). Instead it echos 1. Sorry if this is blindingly obvious. I'm just not seeing it.
That's the first example. I'm asking why the examples produce the outputs that they do, not how to get them to produce a different output.
The &amp; is background the task so it’s sending it to a sub shell. It can’t return anything back to the parent so it is updating testvar is a child process then exiting 
As the parent comment said: &gt;Take this environment variable and make it available to any binaries or scripts that you run "from this shell". So testvar will still be accessible. If you call another script from this one, though, it won't have access to testvar. If you export it then it will. 
Thats more a question of scoping than exports. The function had access because it was "born" with the parent's variables. If that didnt work, then you would have to pass in everything...and shell functions dont require that.
Ah. That makes sense, but it doesn't answer my second question (the one that made no sense 'cause I forgot to add the link to my example). If I do something [like this](https://pastebin.com/NTV4Phkv), which calls a [different script](https://pastebin.com/YpY8FHwc) (./exportTest2.sh in the example), but I modify the value of testvar before the second script reads from it, why doesn't the second script see the modified value? In this example, it prints 1 to the console from the second script after a delay of two seconds (as opposed to printing 2). It seems like its making a copy of testvar when it moves the the other shell, rather than making a reference. I assume that talking about references isn't technically correct, but is that how I should be thinking about it or is something else going on here?
 your exportTest2.sh should be able to see a testvar of 1. But you are running in the background, so I/O may be a little wonky. test2 cannot change testvar for the calling process 
I haven't tried this, but you should be able to use a process substitution to do something along the lines of qemu-img convert -f raw -O qcow2 &lt;(dd if=/dev/nbd63 bs=4M) file.qcow2 So long as the `dd` is writing to stdout it will be presented as a file to `qemu-img`
I'll give this a whirl in the lab and see if it works. Thank you
 # qemu-img convert -f raw -O qcow2 &lt;(dd if=/dev/nbd62 bs=4M) file.qcow2 qemu-img: Could not open '/dev/fd/63': Could not refresh total sector count: Operation not permitted qemu-img: Could not open '/dev/fd/63'
Ah, I guess `qemu-img` must try to randomly seek in the file, so process substitution isn't suitable. 
`dd` is a superstitious charm that people believe protects data on voyages to and from block devices. In reality it's basically just `cat` with EBCDIC support. Just skip it entirely: qemu-img convert -f raw -O qcow2 /dev/nbd63 file.qcow2
You literally beat me to the punch. Went through the man page and found the host_format fmt, so I used this and it worked great: qemu-img convert -f host_device -O qcow2 /dev/nbd62 file.qcow2 Your command would work as well, either way I'm very happy. Not sure why I kept thinking dd was my go to. 
umm why the column command? You can just use `./ciphers.sh 172.217.11.46:443 | sort -k 3,3`
The `column` command will actually create the desired array. It's only after the array is created that I can then sort by the second column. Just using the `sort` command does not produce the array and does not organize the output in the desired fashion.
Apparently, the `column` command was specifying the "second" column as the "third" column, so my final command was actually as follows: `./ciphers.sh` `172.217.11.46:443` `| column -t -s "..." | sort -s -n -k 3`. 
If you want to sort by column, it's better to have one record per line. Like : IP address, Cipher name, result, failure message I added an IP address column if you want to run the script across multiple hosts sharing one output file. To get everything on one line, try using 'printf' instead of 'echo'. Finally, printf \n at the end of each line to add a carriage return.
You can try this: append `sort -k2 -t" "` In the area after the first double quote, press CTRL-V then press the TAB key, then put the double quote. You'll get output like this: Testing DHE-DSS-AES256-GCM-SHA384 NO (sslv3 alert handshake failure) Testing DHE-RSA-AES256-GCM-SHA384 NO (sslv3 alert handshake failure) Testing ECDHE-ECDSA-AES256-GCM-SHA384 NO (sslv3 alert handshake failure) Testing ECDHE-ECDSA-CHACHA20-POLY1305 NO (sslv3 alert handshake failure) Testing ECDHE-RSA-AES256-GCM-SHA384 YES Testing ECDHE-RSA-CHACHA20-POLY1305 YE
Why is your `.bashrc` slow? Maybe it's reinitializing things it shouldn't be? 
Not real bash but wc -l outputs the number of lines in a text file
But here I don't want the number of lines, I want to find the average of days based on a person. Its like group by average in SQL. Please refer Pastebin code for clear data.
You almost have the answer - just combine your two last solutions and that should be it awk '{sum[$1]+=$4;count[$1]++;}END{for(i in sum)print i", "sum[i]/count[i];}' abc.txt
You're awesome.. thanks.. yeah I almost got but I should have used a different variable name, previously I have used the same variable and got the different result. Appreciate it.
If you know that the second monitor is on the right, and know the width of the left screen in pixels, you can use the `-i` option to indent by that width. The message will then be centred on the second monitor. If you want to distribute your script to others, it's more trouble, since you have to know the layout and dimensions of the screens. You could get those by parsing the output of `xrandr` perhaps.
Here's what I do. I create a directory in my home directory called scripts, bin or whatever and add it to my $PATH in my .bashrc or .bash_profile Since the script is in my path I can call it from anywhere by just typing the command. You also don't need to put the .sh on the end of the script so just rename the script and drop it. I'm sure someone will come along and tell me how I'm wrong.
You could put the following function into your ~/.bashrc: showf() { cat "$@" | sed 's/a/b/g' | head -5 } Another version of this: showf() { local file for file; do echo -e "=== Filename: $file" cat "$file" | sed 's/a/b/g' | head -5 done } About your current "showf.sh", you should choose a folder inside your home that you add to your $PATH. Choose for example `~/bin` or `~/.local/bin`. After you've added the location you chose to your $PATH, you can then put your showf.sh script into that location and run it like this: showf.sh myFile
How would you know the contents of the file without downloading it? A better system would require support on the website's end, e.g. a webhook when the file gets generated. You should talk to them about such a thing since they won't particularly want you to keep needlessly downloading files in a loop.
I only run this script once a day, so that's not too much of an issue, but asking for a webhook or something like that is out of the question.
You could try just fetching the HTTP headers and parsing the "Last-Modified" timestamp: &gt; curl -I http://ipv4.download.thinkbroadband.com/10MB.zip HTTP/1.1 200 OK Server: nginx Date: Wed, 13 Jun 2018 02:17:12 GMT Content-Type: application/zip Content-Length: 10485760 Last-Modified: Mon, 02 Jun 2008 15:30:33 GMT &lt;--- THIS ONE :) ETag: "48441219-a00000" Access-Control-Allow-Origin: * Accept-Ranges: bytes Connection: keep-alive Provided your local copy's timestamp is the same format, you can do a similar comparison you're already using. So something like this might work in your case: #!/usr/bin/env bash # Define the local/remote files... remoteURL="http://ipv4.download.thinkbroadband.com/10MB.zip" localFile=~/10MB.zip # Get local and remote timestamps in Unix Epoch format... remoteStamp=$(date --date="$(curl -s -I ${remoteURL} | awk '/Last-Modified/ {$1=""; print $0}')" +%s) localStamp=$(stat -c %W 10MB.zip) # Now compare the timestamps. # local &lt; remote: remote file is newer if [ ${localStamp} -lt ${remoteStamp} ]; then echo "Local file is older...get the new one" curl -s -o "${localFile}" "${remoteURL}" else echo "Both files have the same timestamp...moving on" fi Hope that helps.
Maybe ask them to generate an md5 and store it next to the file and you can compare that 
use md5sum if you want to compare files. mtime and ctime can be set with touch easily. if you really want to know if two files are different check their md5sums.
`curl` actually has an option to handle that for you: -z, --time-cond &lt;time&gt; (HTTP FTP) Request a file that has been modified later than the given time and date, or one that has been modified before that time. The &lt;date expression&gt; can be all sorts of date strings or if it doesn't match any internal ones, it is taken as a filename and tries to get the modification date (mtime) from &lt;file&gt; instead. See the curl_getdate(3) man pages for date expression details. So, assuming that the server sends the correct modification time *and* the local copy's own modification time is reliable, you can replace all of those conditions by this: curl -sz "$localFile" -Ro "$localFile" "$remoteURL" Notes: * The `-R` option is important -- it tells `curl` to keep the server's modification time when it downloads the file. * `curl -z` works by sending an `If-Modified-Since` header to the server. If the server supports this, it will simply send a `304 Not Modified` response. If the server *doesn't* support it, `curl` will try to simulate that exchange by checking the headers in the response itself. I *believe* that it's smart enough to terminate the connection early if that check fails, but you should test it. --- Alternatively, if you *can't* rely on the date, you might be able to use the `ETag` value, assuming that the server implements it in the standard way: &gt;If the resource at a given URL changes, a new Etag value must be generated. Etags are therefore similar to fingerprints and might also be used for tracking purposes by some servers. A comparison of them allows to quickly determine whether two representations of a resource are the same, but they might also be set to persist indefinitely by a tracking server. You'll just have to store a copy of the `ETag` every time you download the file, and then do the `curl -I` check again to compare it. You can also check the `Content-Length`, if you want to be extra sure.
Thanks - beyond the obvious curl use cases I haven’t really dug too deeply into the man page. Appreciate it!
Thanks u/mellow12 and u/ropid Both the solutions seem to be working (tested on Mac). One trouble I now have is, I have two functions/files showf and showg. Using one of those (showf), the tab auto completion to provide the file name doesn't work in terminal or iterm2 (in both the cases of funtions, files)! I tried a solution from stackoverflow to create a ~/.inputrc file and do 'set show-all-if-ambiguous on', but that doesn't seem to be working either. It's a small annoyance, but good to get rid of!
By no means a linux expert but isn‘t there a DISPLAY= env variable? Where :1 would be your main and :2 your secondary monitor? Or is this variable for something completely unrelated?
Sure: grep --perl-regexp --null-data --quiet '\x0d\x0d\x0a' Reads from standard input, exit successfully if and only if the input contains that byte sequence. There are a few gotchas in using Grep for this task. You might think something like: grep --null-data --quiet $'\r\r\n' would work. After all, this construct is a way to get Bash to generate arbitrary strings using ANSI-C-style escapes. The first problem here is that `\n` is a "newline", not a line feed, and those aren't necessarily the same thing. But a bigger problem is that Grep has an almost-undocumented facility whereby multiple patterns can be specified by separating them with newlines. (Curiously, this facility is well-documented only under `-F`/`--fixed-strings`. It actually also applies to basic regular expressions and extended regular expressions... but not Perl regular expressions.) So that's why we fall back to using Perl regular expressions instead, and having the PCRE library parse escape sequences into something useful.
Have a look at the [awesome-ci](https://github.com/cytopia/awesome-ci) collection. It offers scripts for that and will even output the exact regex command used to find it, as well as being able to fix it.
Thank you for great explanation! I came with this quick and dirty thing meanwhile: \`\`\` $ find . | xargs file | grep 'ASCII text' | grep 'CR\[\^LF\]' ./aerodrome.xml: ASCII text, with CRLF, CR line terminators ./arrivalDeparture.xml: ASCII text, with CRLF, CR line terminators ./briefing/charts\_ICAO.xml: exported SGML document, ASCII text, with CRLF, CR line terminators ./CalculateAnalysis/country.txt: ASCII text, with CRLF, CR line terminators ./CfgMultimedialDB.xml: ASCII text, with CRLF, CR line terminators ./cfgRatingCurve.xml: ASCII text, with CRLF, CR line terminators ./ClimatCfg.xml: exported SGML document, ASCII text, with CRLF, CR line terminators ./ClimatTempCfg.xml: exported SGML document, ASCII text, with CRLF, CR line terminators ./ClimatTextCfg.xml: exported SGML document, ASCII text, with CRLF, CR line terminators \`\`\` It's probably way slower but it also gets the job done :)
&gt;curl \-I This is really awesome! Thanks a bunch!
If you are running this from launchd then you are likely not getting the environment variables in the process. Specifically `$HOME` is likely null so the path is invalid. Try outputting to a fixed path like `/var/log/` or `/tmp/` and see if you get anything out as expected.
Turns out the the `&amp;&amp;` operators are *properly* stopping the `echo` command because the database commands `pg_dump`, `reindexdb`, and `vacuumdb` are not returning an exit code of 0. Something's wrong with the `.pgpass` file. Any PostgreSQL gurus here know why the `.pgpass` isn't just letting those commands run without a password? It's weird because the .backup files are indeed created, so `pg_dump` is executing it OK, but throwing an error and preventing the `echo` to into the log file.
Use `file` to recognize it based on file contents: ``` $ file file.doc # Free form description file.doc: PNG image data, 595 x 842, 8-bit gray+alpha, non-interlaced $ file -i file.doc # MIME type file.doc: image/png; charset=binary ```
I suggest you learn to use Google first
Sorry, new to bash. I used the file command before and got different output. I’ll have to revisit. Thanks!
Thanks!
You could do something like: find -type f -print0 | xargs -0 file -ib
Not entirely sure what you mean? Do you mean like an array called January?
Hi, Sorry for being so unclear, I'm trying to create a script that checks if the month is January 2018 then if it is, then change it to go back to December 2017.
a month? any month? hardcoded? based on the output of a variable? homework?
Preferably I'd like to get the month before the current month.
With GNU you can use: lastmonth="$(date -d '1 month ago' +%B)" echo "$lastmonth" Without GNU, you can not. 
Ok just remember to disable automatic date and time in your settings or your scripts changes wil always be reset So if you run the "date" command you're output wil look like this for example Fri Jan 13 00:32:13 SAST 2017 Now you can limit your output to only display the month by using the "cut" command like this date | cut -d ' ' -f 2 The "-d" is your delimiter and the "-f" is the field (If you need me to explain this part better just let me know) And to set the month to a variable you simply type it like this varname=$(date_that_has_been_cut) So it would look like this month=$(date | cut -d ' ' -f 2) I hope this helps
Ok just remember to disable automatic date and time in your settings or your scripts changes wil always be reset So if you run the "date" command you're output wil look like this for example Fri Jan 13 00:32:13 SAST 2017 Now you can limit your output to only display the month by using the "cut" command like this date | cut -d ' ' -f 2 The "-d" is your delimiter and the "-f" is the field (If you need me to explain this part better just let me know) And to set the month to a variable you simply type it like this varname=$(date_that_has_been_cut) So it would look like this month=$(date | cut -d ' ' -f 2) I hope this helps
Based on the comments, this should do what you want. This uses only bash built-ins. #!/usr/bin/env bash # # Check if previous month is January 2018. months=( January February March April May June July August September October November December ) printf -v month '%(%m)T' '-1' printf -v year '%(%y)T' '-1' prev_month="${months[$((month-2))]}" [[ "$prev_month" == "January" ]] &amp;&amp; ((year==18)) &amp;&amp; { # Do stuff here. : } # Test string to show it works. printf '%s\n' "$prev_month" 
You don't need `cut` at all. The `date` command can output just the month. month=$(date +%B) `printf` can also be used and is more portable. month=$(printf '%(%B)T'
Save a subshell :) printf -v month '%(%B)T'
This is something I've been working on for the past few days. It's not finished yet and it's a WIP but I'm posting it here to get some feedback and critique. Hopefully some PRs get opened with some tips I haven't heard of. :) 
Didn't know XD Thanks that's a lot easier
Nice idea, I’ll take a closer look tomorrow :) but a quick suggestion before I go to bed: include `/dev/tcp`?
That’s a good idea, will do! 
Thanks! I’ll have to take a look and add to my collection! Hopefully I’ll have something to contribute! 
Cheers! 
Nice idea. Unfortunately, your very first example ("Trim white-space from string") fails its own example. It collapses internal whitespace. You might want to look at [this recent thread](https://redd.it/8nau9m). :-) 
something like `HOST=KENN; export VER_"$(echo $HOST)"=/tmp` ?
Cheers. :D &gt; Unfortunately your very first task ("Trim white-space from string") fails its own example. It collapses internal whitespace. You might want to look at this recent thread. I did add a note explaining this but I agree, a proper method of doing this is needed. I'll take a look at that thread. &gt; One thing I will suggest here, though, is you ought to be very clear what "side effects" each of your functions have. For instance, many of them use shopt or set and don't reset the shell environment back to its original state afterwards. To do this properly you actually need to save the output of shopt -p and the value of $- and revert things accordingly on function return. I'll work on documenting this and updating the examples, thanks for letting me know.
 $ VER_KENN=test $ HOST=kenn $ eval echo "\$VER_${HOST^^}" test 
when I tried the doulbe '\^' it gave me a bad substitution error when I removed it it worked thanks though
&gt; when I tried the doulbe '^' it gave me a bad substitution error when I removed it it worked thanks though Huh. Maybe you're using an old version of Bash. The syntax I used (it uppercases the expansion of the variable) is only available since Bash 4.0.
Here's a solution without `eval`. #!/usr/bin/env bash # # Dynamically set a variable. # Set the envars for this example. VER_KENN="test" HOST="KENN" # Create the variable name. : "VER_${HOST}" # Store the value of VER_$HOST.. VER="${!_}" # Print it for this example. printf '%s\n' "$VER" 
`IFS=$'\n' read -d "" -ra result` loses empty lines and in turn array elements.
Nice. Bookmarked, saved, screenshotted, called my mom and told her about it, requested this be the class text instead of the garbage we use, took exam, passed exam, posted on Reddit about it. 
ssh hostname tar -czf - /directory/to/backup &gt; mylocalfile.tgz
Here's how you can generate all SSNs in Bash4+: ``` for a in {000..999}-{00..99} do for b in {0000..9999} do echo "$a-$b" done done ``` It's split over two loops to avoid using excessive memory. Bash3 does not support zero padding in brace expansions. 
That is very impressive. Ended up asking a friend of mine who is a wiz with python to do it, but I knew bash could too. I was going really depressingly...horrendously off in to the weeds with indexes and lists and all sorts of shit. Frankly, given the simplicity with which you and he solved this, I think I need a drink and to consider resigning. Cheers. 
This literally echoes the range string for me. Like, squiggly brackets and everything. Doesn’t look like it’s gonna interpreting as a range. Thoughts?
It still has an arbitrary code execution issue though, so maybe `${HOST//[^a-zA-Z]/}` 
It looks like you are sourcing a zsh rc file from somewhere. Check any *.profile* files in your home directory. Check that *.bashrc* or *.bash_profile* don't (for some odd reason) source or contain any zsh commands. Maybe grep all the text files in your home directory for the string *zstyle* to find the offending config file.
I've updated the repo to make contribution easier. - Shellcheck is run on code blocks. - Tests have been written for example functions. - Added a CONTRIBUTING.md file. - Added more snippets and fixed bugs in existing snippets - `travis.ci` runs `shellcheck` and the tests on every commit. - An MIT license was added.
If /u/bikes-n-math idea doesn't work I would check to see if there is anything zsh in `/etc/profile.d/`, I doubt zsh install would have modified /etc/profile but you could check that too. 
I have code for this, but there's a couple of caveats... 1. There's no guarantee that these pseudo-devices will be available. Generally speaking, though, on Linux and OSX you can reasonably expect them to be there. 2. To make using them really reliable, you need to use the `timeout` command. Even on Linux that's not a given... I know this because I have a `check_mk` local check built around this, and have been through the trials of getting it to work portably and reliably across various versions of RHEL and Ubuntu *just* so that I wouldn't need to reach for `nc`. So if I strip it right down and make it generic, it looks something like this: test_port() { timeout 1 bash -c "&lt;/dev/${3:-tcp}/${1:?No host given}/${2:?No port given}" 2&gt;/dev/null } Now, I have code for `timeout` too. This is what I call a "step-in" function. You test for an external, and if it's not there, "step-in" and provide something that delivers the basic/fundamental purpose of that thing. # Check if 'timeout' is available, if not, enable a stop-gap function if ! command -v timeout &gt;/dev/null 2&gt;&amp;1; then timeout() { local duration="${1//[!0-9]/}" shift # I tested a few, this one works nicely and is fairly simple # http://stackoverflow.com/a/24413646 # Run in a subshell to avoid job control messages ( "$@" &amp; child=$! # Grab the PID of the COMMAND # Avoid default notification in non-interactive shell for SIGTERM trap -- "" SIGTERM ( sleep "${duration}" kill "${child}" ) 2&gt; /dev/null &amp; wait "${child}" ) } fi This is a simplified version of a more complete 67 line version, so I don't know if it will work post-simplification. YMMV. I also have a couple of "overlay" functions that you may find interesting - PM me if you're keen. 
This is awesome man! I’ve been writing bash code snippets in a markdown file, I’ll see if there’s anything I can contribute
Cheers!
Cheers for this! I'll see if I can come up with a built-in `timeout` function. iirc I wrote one in a script somewhere. :+1:
[http://sourceforge.net/projects/linuxcommand/files/TLCL/17.10/TLCL-17.10.pdf/download](http://sourceforge.net/projects/linuxcommand/files/TLCL/17.10/TLCL-17.10.pdf/download) this one i a brilliant entry level book - helped me a lot 
http://tldp.org/LDP/Bash-Beginners-Guide/html/ - one of the best guides, if not the best.
This and the advanced scripting guide are excellent starting points. I also recommend supplementing this with the [bash hackers wiki](http://wiki.bash-hackers.org/)
I recommend against that. The Bash Beginners Guide teaches bad practices and is also very outdated. I recommend the wooledge [BashGuide](http://mywiki.wooledge.org/BashGuide) instead.
Search Amazon for bash scripting books. There's a whole bunch. Just read one that has good reviews (and isn't too old). 
Please don’t use this guide. It’s very outdated and has many bad examples and practices. It is far from being the best. Use the wool edge guide as mentioned in another comment.
Wow... If I had it the day I've started my adventure I wouldn't get so many bad habits.
Hi all, thanks for all the great advise, i am looking forward to learn a Scripting language 😀
Small guide: "*" Means any string of any length including nothing, so *.txt refers to any txt file "?" Refers to any single character, so ????.txt refers to any txt file with fur characters, and *????.txt is at least four "$" is used for variables, the terminal simply replaces it with the variable value. So if $v was "the", then *???$v.txt refers to any txt file that has at least any three or more characters before "the" in its name
“Classic Bash Scripting” by O’Reilly is one of the best. I have the paperback (gasp), but it’s available as an e-book (in pdf,ePub,etc) as well. Just google it.
I’ve seen word splitting be an issue in zsh, so if you’re trying to write add-ons that might be sourced by non-bash shells, it’s a solid idea to quote in assignments. For standalone scripts with a proper shebang, it’s less of an issue. But yeah. It probably doesn’t hurt. It’s not the worst advice. 
Thanks for this, I will have a look.
If you're writing a shell script to be sourced by different shells, you're just asking for trouble, and is not a good practice.
note that those definitions of \* and ? only refer to globs, and don't mean that everywhere in the language. $ is also for all expansions, not just variables.
Yeah. I'm kinda new too
Use ShellCheck. It'll point out common noob issues in scripts.
This is definitely true, though quoting on assignments can be a good idea in case you want later to prefix it by an assignment builtin, which in the case of Bash doesn't change a thing but has a different behavior in dash and older shells. Here's an example: bash$ foo=$(echo 1 b=2) bash$ a=$foo bash$ echo "$a" 1 b=2 So this is always the same result no matter which shell you are using (POSIX ones at least), no splitting on assignments. Now imagine this was in a script and after reading your code you want to set your variable 'a' as readonly. You'll just need to prefix it by readonly like so: bash$ readonly a=$foo bash$ echo "$a" 1 b=2 Same as before as expected on bash. But in dash: dash$ foo=$(echo 1 b=2) dash$ readonly a=$foo dash$ echo "$a" 1 dash$ echo "$b" 2 And with quotes on dash: dash$ foo=$(echo 1 b=2) dash$ readonly a="$foo" dash$ echo "$a" 1 b=2 So if you have quotes from the beginning on your assignment, adding readonly or any assignment builtin (local, typeset, declare, export) won't change your splitting behavior whatever shell you are using. But well I agree this is really an edge case so I don't think this is something people should be worried about, but this is definitely interesting to know since /bin/sh is dash on debian and ubuntu, which are widely used Linux distributions :)
Try living in the command line a bit too. [http://www.bashoneliners.com/](http://www.bashoneliners.com/) has a lot of useful gems to help you become productive at common tasks.
thanks!
Heyo! Not trying to be an ass, but is there something specific you're looking for help with? It seems like the script provided works but you wanna add some sweet features and the cool thing about scripting if there's a ton of ways to do anything (generally). You could add if statements (random guide: https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php) to check for updates and output whether there are or not or you could add a menu or two to list the choices (some good stuff on that here: https://askubuntu.com/questions/1705/how-can-i-create-a-select-menu-in-a-shell-script) of the update options or what to update. As for the progress bar, never done one personally so I'll let someone more knowledgeable suggest something. 
You're right. The script is fine. I basically just wanna know a good way to add: it to ask me what I want to do, by having a few options, choosing one of those options, and having it run whatever command is under that option. Then potentially a progress bar just because that's something I would like to know how to do for future reference.
(on mobile, formatting, etc). Well done for learning and teaching yourself! Nothing wrong with your script, but some suggestions : You don't need all those sleeps. In your first test, you don't need double quotes. Use '!=' to compare strings, '-ne' to compare integers. I would check the return code ($?) to verify that the apt commands were successful before announcing "done". Good luck! 
read with a case statement or if statement if you are from the 80s
iwevent will fire an event for when things happen on your wireless interfaces. For example while running it, and disconnecting from a WiFi Network you will receive: 03:05:30.172365 wlp3s0 New Access Point/Cell address:Not-Associated
This is quite a cool script which I might steal. I might suggest that if you're automating upgrades you might want to echo the list of what will be updated to a log file. That way if you upgrade and something breaks you have a list of things that might of caused it. 
Well as of right now, it acts as a regular 'sudo apt-get update' and/or 'sudo apt-get upgrade -y' so you'll see if something goes wrong since occassionally you'll get a dialog box that pops up asking for various things. If I can output to a file and still have the popup box appear, I would gladly do that. Thanks for liking the script, I just wanna figure out how to make it ask which update/upgrade I want to perform. Once I get the ability to i'll look more into that.
I looked into what you were saying but im afraid I don't quite understand what you're saying. I looked up what the '$?' does but not sure where exactly to put it. Like do I add it to the end of the 'sudo apt-get X' command(s) or at the end of the entire script or? Currently looking into the '!=' and '-ne' to see what you mean by that.
I updated it with options now. :)
A nice way to create interactive menus is with the "dialog" tool, see below site for examples, I've used it a few times to good effect. http://linuxcommand.org/lc3_adv_dialog.php
This is useful, thank you. I will bookmark this. Although i'm not trying to make dialog boxes, i'm wanting to output my command to a log file, but make sure I still recieve the dialog boxes(like those shown in the link you gave me) so my script doesn't hang, and I can do whatever input it's asking and the script can continue. If that makes sense, xD
Hey, Exmixx, just a quick heads-up: **recieve** is actually spelled **receive**. You can remember it by **e before i**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Just a tip. Either use the code markup or better yet, post on http://pastebin.org and simply paste the URL in a future post. Also, /r/bash Cheers.
There is code markup, just in quotes so it's easier, I guess but you're right, pastebin would have been better. Also...this is r/bash...this is where it was posted...
Shit LOL. I thought I was in /r/linuxquestions. Cheers.
Nice script. Just for completeness, I want to point out that the modern `apt update` &amp;&amp; `apt full-upgrade` which replaces `apt-get update &amp;&amp; apt-get dist-upgrade` has colors and a progress bar built-in.
Ah ok. I didn't know about that. I'll look into that and maybe modify mine a bit. Thanks.
sorrento
A town in Italy? JK what's Sorrento 
Can you reduce this to about three lines or so that show what exactly you want to do?
Essentially, I am taking an array and creating a new array by taking each element in the original array and multiplying it by itself to create the new array element. It works for integer values perfectly, but I am looking for advice on how to use bc to support rational numbers.
I'm not sure if this is all you're looking for, but \`-l\` makes \`bc\` treat numbers as floating point. \`\`\` $ echo '1 / 3' | bc 0 $ echo '1 / 3' | bc -l .33333333333333333333 \`\`\`
Not what you were asking but you really should use a generic function for squaring an array rather than having two identical methods hardcoded to work on global variables.
Thank you !! you are right I should use a generic function to square my array values rather than global variables
I should note that cat "$f" deny_rule.txt" &gt;"$f" will clobber the file before it can be read. You will destroy all your configurations with that. Anyway, maybe something like (untested) for f in */.htaccess; do if grep -q xmlrpc; then continue; fi # or grep for whatever is useful mv "$f" "$f.bak" cat "$f.bak" deny_rule.txt &gt;"$f" done
to append use `&gt;&gt;`, `&gt;` would overwrite your file instead
`x_squared () {` `local original_data_set_x` `local x_squared` `local elements` `local i` `original_data_set_x=($(echo "$@"))` `x_squared=($(echo "$@"))` `elements=$[ $# - 1 ]` `for (( i = 0; i &lt;= $elements; i++ ))` `{` `x_squared[$i]=$(echo "scale=4;${original_data_set_x[$i]} * ${original_data_set_x[$i]}" | bc -l)` `}` `echo ${x_squared[*]}` `}` My syntax was bunk in my function, if I do not create a generic function your original suggestion will work as well 
If you are appending to a file just use `&gt;&gt;`: cat deny_rule.txt &gt;&gt; "$f" You can trim the fat on that if statement: grep -q xmlrpc "$f" &amp;&amp; continue # ...
$? is a built-in variable that contains the \*return code\* (also known as \*exit code\*) of the last command. In Linux, return code 0 means that the last command executed successfully. Any other return code means unsuccessful. Some programs give you more information than others -- for example, look at the man page for curl. So you can use $? to test whether the last command worked. For example: sudo apt-get upgrade -y if \[\[ $? -ne 0 \]\] ; then echo "that failed" exit 1 fi You can also use \`&amp;&amp;\` or \`||\` as shorthand -- \`&amp;&amp;\` means "continue if successful" and \`||\` means "continue if failed". \# Only display the "Red" code if the sleep was successful sleep 0.2 &amp;&amp; echo -e "${Red}" \# Exit if not the root user \[\[ $(id -u) -eq 0 \]\] || exit 1 Hope that helps!
That does help! Thanks. :)
I think that would just loop over all the .htaccess files without doing anything, because the `-q` option for grep stands for quiet, so it returns always 0, so it would always be true so it would continue all the loop to the end only grepping
&gt; but `-l` makes `bc` treat numbers as floating point. That explanation is misleading and not entirely correct. `bc` always calculates in decimal numbers; however, by default it calculates with zero digits after the decimal point. The `-l` option loads the standard math library, and also sets the default scale (the number of digits after the decimal point) to 20 instead of 0. Since you’re not using the standard library here, you can achieve the same effect by explicitly setting the `scale`: $ echo "scale=20; 1/3 * 1/3" | bc .11111111111111111111 Or choose any other scale: $ echo "scale=5; 1/3 * 1/3" | bc .11111
No, the exit status of `grep -q` is zero if there was a match and non-zero otherwise.
yeah you are right, it returns a 0 if there is an error, but 1 if there is not a match
I updated my script here [https://pastebin.com/Nad70RYp](https://pastebin.com/Nad70RYp) Modificed it so all commands are output a .log file, returns message if a command failed, thanks for [cszintiyl](https://www.reddit.com/user/cszintiyl) for their help on that. Thanks for the help and input everyone. If there's any other ways to improve this, im of course open to suggestions.
man test. if [ ! -z file ] ...
How did you "bork" $HOME permissions in the first place? Or what is the most likely cause for that happening? From what I know, it is really hard to mess it up **unexpectedly**. And what is the reason for using the find command? Why not just "chmod -R 700 $HOME" ? 
I haven't borked my permissions up at all. It's a tutorial video. lol You'd be surprised what can happen when people mess around with permissions. Your solution makes everything executable, whether directories or not, hence the `find` command.
Makes sense. 
Hard to say without seeing the command you're using but maybe somthing like this: `sudo command -u "$1" &gt;"$2"`
Side note: you typically don't need to test for `$?` directly, in that way, as you can have it tested directly within an `if` statement. For example: if COMMAND; then COMMANDS fi
I'm not sure I follow exactly. If you point whatever you have to `$2` it should 100% use the 2nd position parameter. I'm afraid I'm with mTesseracted, in that it's hard to say without seeing the actual code. :\ You could try Pastebin. I usually parse arguments with a `while` loop. Here's an example from my `simplify-ubuntu`: https://github.com/terminalforlife/simplify-ubuntu/blob/master/simplify-ubuntu#L57-L84
Then use this instead: chmod -R u=rwX,go= ${HOME} Just be careful that you don't run this on users that has / set as home (this is seen on some, primarily older, unix flavors) - this would *really* mess up your permissions
If you want to post code, indent each line by four spaces like this Or post it to pastebin and give us a link. Without the code, we basically guessing and can't give you any kind of precisely-actionable answer (e.g. "change line N from XYZ to ABC"). 
I had a feeling there'd be a solo `chmod` method somewhere, but the `find` one works well and gives me more control. I'll take note of the `X` usage though, which I assume will only add the executable bit for directories and not files.
Probably because you ran it second and everything is cached.
Yes, because manually retyping a long command from a fucking VIDEO makes perfect sense.
I like the thought process, however, I run the `find` one quite often and am used to the speed. I also ran them both again afterwards, to get a better idea.
Sure as hell beats the alternative of making the changes one file at a time. You know, you *could* have just been polite and simply suggested I add the command somewhere, which I did. Regardless, I'd type that out pretty quickly, so admittedly, I forget other people aren't so familiar with a keyboard.
The manual page also says, further up: &gt; The following description applies to extended regular expressions; differences for basic regular expressions are summarized afterwards. Then later: &gt; In basic regular expressions the meta-characters `?`, `+`, `{`, `|`, `(`, and `)`lose their special meaning; instead use the backslashed versions `\?`, `\+`, `\{`, `\|`, `\(`, and `\)`. So you can either use `grep -E` to turn on extended regexps, or backslash your `+`.
That did it, cheers. 
 My criticism wasn't of the command - it was of presenting it in a *video* rather than in a text form where it could be copy/pasted. I will refrain from comment on the command.
misc but `egrep` is also a thing, just in case you weren't aware. :+1: 
would `grep -P` have a similar result?
[egrep is non-standard and deprecated. Use grep -E instead.](https://github.com/koalaman/shellcheck/wiki/SC2196)
"Non-standard and deprecated" does not make it inviable. It may be removed at some point, but for now, kernel devs still ship it, albeit in the form of a shell script that invokes `grep -E` anyway, but it's still there and still usable, if desired. If you refer to the parent comment of this thread, you will see that `grep -E` was already mentioned. I was merely giving an alternative (that is also listed as an alternative in the `grep` manpage, fwiw) that will not be going away any time soon and can ofttimes be easier to remember for newbies who may be referring to this thread some day.
Yes, but to what extent, I can't really give you a good answer. I've not tinkered with it much. Doing basic pattern matching like in OP, definitely, but I've not seen any real advanced application with `-P`. I would generally use `perl` itself if I wanted to leverage syntactically perl regex, but that's just me. `grep -P` might be totally awesome, I just don't use it much, sorry. 
&gt; I've not seen any real advanced application with `-P` I don't know if a "real advanced application" should be using complex `grep`. To me, it is a tool used well for small applications and one-offs. 
´grep -P´ always works with regex for me
Here is a link to pastebin https://pastebin.com/Mf8eU8DQ
Are you not using a systemd system? Init scripts can be much more difficult to deal with. I would suggest using a tool like logger (http://man7.org/linux/man-pages/man1/logger.1.html) to do your logging. Also, in your pastebin, it looks like you're not dereferencing any of your variables.
Yeah centos 7. Sure thanks.
Oh man, where to start? I'm no expert, so take this with a grain of salt: First look into pgrep. It is specifically for processes and you can use -F to create a PID file to check against. Personally, I would wrap logic into functions. You might look at the way on SuSE 11.3 or RHEL 5 handles in You may also want to run your script through shellcheck.net to help you along as well.
I was too lazy to read your code but I just tried this on a Webserver and it worked ok: myServices=(apache2 ssh) myLog=./myLog.txt command=restart for service in $(echo ${myServices[@]}) do service $service $command &gt;&gt; service.log &amp;&amp; echo "$service restarted at $(date)" &gt;&gt; $myLog done You should probably add some sanity to that 
I haven’t quite convinced myself of it yet, but I *think* this should be equivalent: dirs=(./*) for dir in "${dirs[@]::4}"; do runsim "$dir" &amp;; done for dir in "${dirs[@]:4}"; do wait -n; runsim "$dir" &amp; done wait This starts four `runsim`s in parallel, and then repeatedly waits for one (`-n`: the **n**ext one to complete) to finish and starts another one. I *think* this should generally keep four processes running until the directories are exhausted. --- Alternatively, [GNU parallel](https://www.gnu.org/software/parallel/) might be helpful here as well. I’m not too familiar with it, but I hear it’s very powerful, and I’m sure “distribute *n* jobs across *m* systems” is part of its basic functionality.
You should check out that tool named 'parallel' that was mentioned by galaktos. A very simple use of it is not much different than using 'xargs', and if that is not good enough, it has a lot of documentation and tutorials on its website.
Within your loop you could use: printf “$i has $(wc -l $i)” I am not by a terminal to check and I think the $() might need tweaking. But that would be one approach to a formatted output.
The wc command is taking the piped output from echo, try this: echo $i; echo $i | wc -c Also, people usually frown on doing "for i in ls" as for loops can be expanded without incorporating ls, it also fucks up if file names contain spaces, so something like this is usually better: for i in *; do echo $i; echo $i | wc -c; done If you wanted specifically files ending in .txt; you can do for i in *.txt... Etc. You get the idea?
I have tried both ways and the closest i get is from the command `for i in *; do echo $i; echo $i | wc -c; done`but as you can see here https://pastebin.com/kXRdxexA it for some reason counts another blank character. What do you think that might be, very strange?
 for file in *; do printf '%s has %d\n' "$file" "${#file}" done
Thank you so much, i have to get better on my understanding and use of regular expressions. May i ask what exactly does the # symbol do inside the curly brackets? In this command how did you make the "has" appear _after_ the "$file"?
Starting out is always the hardest part. Good on you for having a go. Test one command at a time, from the command line, and understand how it works. Then test the next command. When those two work, add them to a script. When you've tested the next line, add that to the script. * Why the sleep between restarts? * Use `systemctl restart _servicename_` or `service _servicename_ restart` * You don't need to wait between restarts. Just issue one after the other. * You're checking to see if the service restarted successfully, then starting it if it did. Instead, check the return code of the restart. If it failed to restart, you should stop and investigate, instead of starting anyway. Look into the **status** argument too. * Be aware that not all services run a process with the same name, so it's entirely possible for `systemctl start _servicename_` to succeed, and `pgrep _servicename_` to fail. * Logging is (almost always) provided by default -- look in **/var/log/messages**, sometimes in a **/var/log/*****subdirectory*** named after the service. You could probably replace the entire script with something similar to: for servicename in service1 service2 ; do if [[ ! $( systemctl restart ${servicename} ) ]] ;then echo "service ${servicename} failed to restart" exit 1 fi systemctl status ${servicename} done Research these commands and see if you can understand how they work: * pgrep * pkill * systemctl * service Also look at the man page for bash, under **EXIT STATUS**. Note that your system almost certainly has only one of **systemctl** or **service**. Usually not both. That looks like a lot of comments, but they're not all criticisms. Hope that helps.
You need a -n mate, it wc -c counts line breaks (I updated my original comment with a link and a cloned up bit script for you).
which os are you referring to?
centos 6.
You could check if it was running first. Something like: ps -u $UID | grep vncviewer &gt; /dev/null || vncserver -geometry 1920x1080
If your quotes are correct everything stays safe. Can you explain what's going wrong? Maybe some code? $ while read x; do printf "%s\n" "$x" # note quotes around variable done &lt;&lt;&lt;'here is a variable with "quotes"' here is a variable with "quotes"
I'm not sure what you mean here. You're saying bash is treating quotes in your input as special characters? Or are you passing a variable from bash to something else (`eval` or `ssh` maybe?) that's treating them as special? If the latter, does one of these help? $ input='LIBNAME Qdata BASE "/home/SomeAdmin/sasuser.v94/QualtricsStuff/Qdata";' # Original value $ val="$input" $ echo "$val" LIBNAME Qdata BASE "/home/SomeAdmin/sasuser.v94/QualtricsStuff/Qdata"; # Value with @Q (requires bash 4.something) $ val="${input@Q}" $ echo "$val" 'LIBNAME Qdata BASE "/home/SomeAdmin/sasuser.v94/QualtricsStuff/Qdata";' # Value with printf %q $ printf -v val '%q' "$input" $ echo "$val" LIBNAME\ Qdata\ BASE\ \"/home/SomeAdmin/sasuser.v94/QualtricsStuff/Qdata\"\; # Value with substitute # (not safe for strings that already contain backslashes, but might be good enough) $ val="${input//\"/\\\"}" $ echo "$val" LIBNAME Qdata BASE \"/home/SomeAdmin/sasuser.v94/QualtricsStuff/Qdata\"; 
No it never does, as I can't seem to make it a service! :( So this is my 'heath Robinson' approach :)
you can do something like this: `for i in *; do echo "$i has $(wc -c &lt;&lt;&lt;$i)"; done` 
Why can’t you check if vnc is not running then to start?
Alright, this is amazing. I actually have an application for this and I wasn't clever enough to figure out how to interact with IMDB's pages to scrape the info I wanted. (I think I read that their movie IDs were pretty hard to parse and they have paid API access or something. Anywho, Basically, I'm building a CRUD app for my fiancee and I to track movies that we've watched, and in every record I wanted to keep the url of the imdb page for that movie. This (with some modifications) would get the job done. Bravo!
Yeah systemd is definitely more correct
There were no regular expressions. `${#parameter}` expands to the length of a parameter, see [Shell Parameter Expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html). $ var=onetwothree $ echo "${#var}" 11 Note you must quote parameter expansions (also other expansions like command substitution `$(...)`) to prevent word splitting and pathname expansion, which is why a for loop over a command substitution is typically broken, as is processing output of the ls command. The order of the arguments of the printf command is specified by its format string. $ printf 'First: "%s", second: "%s"\n' 'String 1' 'String 2' First: "String 1", second: "String 2" See [Bash Builtin Commands ](https://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html).
There were no regular expressions. `${#parameter}` expands to the length of a parameter, see [Shell Parameter Expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html). $ var=onetwothree $ echo "${#var}" 11 Note you must quote parameter expansions (also other expansions like command substitution `$(...)`) to prevent word splitting and pathname expansion, which is why a for loop over a command substitution is typically broken, as is processing output of the ls command. The order of the arguments of the printf command is specified by its format string. $ printf 'First: "%s", second: "%s"\n' 'String 1' 'String 2' First: "String 1", second: "String 2" See [Bash Builtin Commands](https://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html).
Indeed, bc does not support floating point numbers at all.
That's what u/setting_orange suggestion does, or have I not understood his code? Sorry I'm new to all this :)
&gt; ill-fated **R.M.S.** Titanic. I see what you did there
From a quick search I really couldn't find any API, so I just tried to find another way around. I found couple of ways and this was the easiest. I am so glad that this simple script will help somebody and his fiancee. Thank you very much for the positive feedback. I really appreciate it. 
Very clever! :-) I use a bash script I wrote long ago, using google + something like your script, but I have been careful not to post it in case the 'imdb' developers get angry and change their html format (they change it now and then anyway). Last time I updated the script (to adapt it to recent changes), while testing, I noticed that, if my script tries to download data for the same movie several times in a row, 'imdb's server gives an error. Here is an example of my script's output: SearchForMovieInInetDatabase "titanic (1997)" Title: { Titanic } Year: { 1997 } Length: { 194 } Rating (0..100): { 78 } Director: { James Cameron } Cast: { Leonardo DiCaprio Kate Winslet Billy Zane } Genres: { Drama Romance } Description: { A seventeen-year-old aristocrat falls in love with a kind but poor artist aboard the luxurious, ill-fated R.M.S. Titanic. } Plot: { 84 years later, a 100 year-old woman named Rose DeWitt Bukater tells the story to her granddaughter Lizzy Calvert, Brock Lovett, Lewis Bodine, Bobby Buell and Anatoly Mikailavich on the Keldysh about her life set in April 10th 1912, on a ship called Titanic when young Rose boards the departing ship with the upper-class passengers and her mother, Ruth DeWitt Bukater, and her fiancé, Caledon Hockley. Meanwhile, a drifter and artist named Jack Dawson and his best friend Fabrizio De Rossi win third-class tickets to the ship in a game. And she explains the whole story from departure until the death of Titanic on its first and last voyage April 15th, 1912 at 2:20 in the morning. } Country: { USA } Language: { English } PosterUrl: { https://m.media-amazon.com/images/M/MV5BMDdmZGU3NDQtY2E5My00ZTliLWIzOTUtMTY4ZGI1YjdiNjk3XkEyXkFqcGdeQXVyNTA4NzY1MzY@._V1_UX182_CR0,0,182,268_AL_.jpg } Entry's Url: { https://www.imdb.com/title/tt0120338 } The above script is invoked, automatically, if there are new video files in my system, to update my [database](https://i.imgur.com/ALiPQi1.png).
That's awesome! Thanks to you and /u/ropid, much appreciated!
?=/etc/init.d/vncserver
Thank for your notes. I thought about the future changes, but since they don't have an API I was forced to use this method. In case, they change the html format either I need to adapt or just abandon the script (being busy or something) hoping for someone to fork it. I wonder how does your script work for the error to happen? seems like you have more details like the Plot and The PosterUrl. Again thanks for the notes. 
I use 'wget --user-agent="" ...' to download html documents. The server refused the request. Tried different values for '--user-agent' but none was better than "". &gt;seems like you have more details like the Plot and The PosterUrl I use the poster image in my video database. 