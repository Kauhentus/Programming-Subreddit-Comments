Are you expecting the file to be on your machine or on the machines you're connecting to?
oh snap ; I need it on local 
on my machine ; I guess my problem lies here.... 
This looks really nice! I will take a look.
Thanx appreciate the effort. I wanted to modify a variable that I passed on call like $2 and modify it if a certain input is passed and I solved it by assigning the content of $2 to a variable and just using that and of course replacing the instances where $2 was used with the variable the value of $2 was assigned to. What you put down looks interesting too and Ill look through it, but now now as putting this down already half killed my buzz. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
it worked ; thank you ! really appreciated .. I will try to find out how it worked :) (P:S I need to buy some reddit gold) 
I use 2 bash scripts: * GraphicalSessionAutostart: Runs, always, after graphical login (user configurable). * GuiStartSessionApplications: Runs when MainMenu's corresponding item is clicked (user configurable). ____________________________ Last graphical login's log: 2018/Jan/05 13:28:28: GraphicalSessionAutostart: Starting new graphical session for user: 'manolo'. 2018/Jan/05 13:28:28: GraphicalSessionAutostart: $COMMON_MESSAGES_SYSTEM_DIR: 2018/Jan/05 13:28:28: 2018/01/05 08:38:53 InternetConnectionOn 2018/Jan/05 13:28:28: 2018/01/05 08:38:53 LocalNetworkOn 2018/Jan/05 13:28:28: 2018/01/05 08:38:50 UsbControlledPowerStripAvailable 2018/Jan/05 13:28:28: GraphicalSessionAutostart: Waiting for graphical session to stabilize. 2018/Jan/05 13:28:30: GraphicalSessionAutostart: Waiting... 2018/Jan/05 13:28:32: GraphicalSessionAutostart: Waiting... 2018/Jan/05 13:28:34: GraphicalSessionAutostart: Waiting... 2018/Jan/05 13:28:34: GraphicalSessionAutostart: Command 'GuiDesktopTextScalingFactor' ran successfully. 2018/Jan/05 13:28:35: GraphicalSessionAutostart: 'MainMenuAndNetTraffic --verbose --icon $COMMON_MISC_DIR/Icons/MainMenu.png --directory $HOME/.MainMenu ' launched. 2018/Jan/05 13:28:35: GraphicalSessionAutostart: All auto-start commands and applications ran/started successfully. 2018/Jan/05 13:28:48: GuiStartSessionApplications: 'GuiInternetMail --pa' launched. 2018/Jan/05 13:28:48: GuiStartSessionApplications: 'GuiStartAllDownloadingClients' launched. 2018/Jan/05 13:28:48: GuiStartSessionApplications: 'DefaultBrowser -tab $COMMON_MISC_DIR/etc/EconomyIndexes.html' launched. 2018/Jan/05 13:28:48: GuiStartSessionApplications: 'PlayAudioFilesList_Music' launched. 2018/Jan/05 13:28:48: GuiStartSessionApplications: All start-session applications started.
 collection="Video Collection - 2013-02-22 - video title.mp4 Video Collection - 2013-02-22 - video title.mp4 Video Collection - 2013-02-22 - video title.mp4" next=1; while read -r i; do sed -r 's/(.*) - (.*) - (.*)/\1 - \2_'$next' - \3/g' &lt;&lt;&lt; "$i" next=$((next + 1)) done &lt;&lt;&lt; "$collection" 
Now I have hives. Spaces in filenames give me hives.
 prename -n 's/\d{4}-\d\d-\d\d/ sprintf("%s_%02d", $&amp;, ++$count{$&amp;}) /e' *.mp4 This is what I was looking for. Thank you. On Debian 9 a small modification was required to prevent the following error: "Global symbol "%count" requires explicit package name" prename -n 's/\d{4}-\d\d-\d\d/ sprintf("%s_%02d", $&amp;, ++$::count{$&amp;}) /e' *.mp4 
yes, as soon as the mods get back from vacation ;) thanks for the 14 reports folks :D
There’s also `aafire` (one of the aalib example programs).
&gt; $ curl -s -XPOST j2y.me --data-binary @example.json Don’t all the `--data` options imply POST unless you use `-G`?
 $ expr `date -d "08/17/2017 11:31:27" +%s` - `date -d "07/03/2017 09:42:11" +%s` 3894556
Thanks that's way easier. All you have to do then is: echo $( echo "3894556/3600" | bc -l ) to get the number of hours
Perhaps the problem is that particular variable name? I'd like to check if it works without "::" when using a different name than "count". I mean something like this: prename -n 's/\d{4}-\d\d-\d\d/ sprintf("%s_%02d", $&amp;, ++$x{$&amp;}) /e' *.mp4
Where I can download factorio_alpha_x64_.tar.xz ?
What command did you use? It's pretty hard to delete 'everything' by accident so don't panic yet.
It sounds like you mounted a file system to a location on the server that had existing files. When doing this, it will effectively hide the existing file tree and make it appear that the mountpoint contains files from the remote system. I would `umount` the directory where you did this and that should give you access to the servers files again. If you can post the command(s) you ran, it would be easier to give you specific commands to help you undo this. Mount and umount are non-destructive so nothing should be deleted or anything.
 sed -nE 's/SOFTWARE LICENSE AGREEMENT FOR (OS X|macOS) ([A-Za-z ]+).*/\2/p' '/System/Library/CoreServices/Setup Assistant.app/Contents/Resources/en.lproj/OSXSoftwareLicense.rtf'
shellcheck output: Line 111: read FACTORIO_PASSWORD ^-- SC2162: read without -r will mangle backslashes.
This preserves text preceding "SOFTWARE LICENSE..." on the same line, unlike OP's `grep -o`. I'm not sure if that matters.
[This](https://www.reddit.com/r/bash/comments/7olxw0/help_regular_expression_to_extract_strings_in_two/dsah5bt/) is a fine answer. Another approach is to get the marketing name from an Apple web service, if you're OK with that dependency: https://unix.stackexchange.com/a/385111
Cool! I hope you don’t mind me critiquing the systemd part :) &gt; Mind you that I'm specifically replacing "Dec" and "Jan" to their relevant numeric years and days because the output of that command doesn't format it that way by default. You can change the date format to something more suitable for scripts with `-o short-iso`. (And you can also use `-q` to suppress the “-- Reboot --” messages, which is what I assume the `grep -i "target"` is meant for.) But why are you looking for `shutdown.target` timings anyways when `--list-boots` also gives you the timestamp of the last message of each boot?
I didn't even look at the far right of `--list-boots` until now. Thanks for the tips this makes it easier than I expected
Hi, my code was the following: sshfs [my email address]:/data/[username] ~/Desktop The files are still present but they cannot be opened. When I try to open it an error message pops up saying: "The alias '[filename]' can’t be opened because the original item can’t be found."
&gt; so that other users logged in to the server can't see them? Ask the system administrator to mount `/proc` with the `hidepid=1` (or 2) option. See `man 5 proc`. Alternatively, if you have the source code of the command being run and it’s written in C or C++, you can patch its `main` method to erase the strings in `argv` (though you can’t hide their number and length). I don’t know of any way to do this externally for a single program.
2 is true because the shebang is typical but not required (for instance, you could just run `bash myscriptwithoutashebang`, the shebang is just a special helpful thing). 3 is false because `foo=bar` is the syntax, spaces aren't required, and I don't think they're even supported. 5 is false because `echo VARNAME` will print `VARNAME`, you'd need `echo $VARNAME` to print `value`. I think I'd mark 7 as true, but it's missing the important fact that it must exist in a directory in the `PATH`, or the full path to it must be specified. Nonetheless, it is true that the used needs execute permissions for it (and it'll need a shebang to be interpreted properly).
Ahh. I’m not totally familiar with sshfs. Could you try: umount ~/Desktop That should effectively undo what you did. Based on what I’m seeing, it should have appeared to clobber your desktop but nothing else. Beyond that I’m not totally sure. You ran this on your Mac?
Assuming that there's some secret as a command line flag: `hidepid` is an acceptable solution, but erasing the args from `main` should *not* be considered secure, as there will be a brief bit of time before it gets erased where any user can see it. If you can modify the program, you should modify it to take secrets via the environment, instead of arguments.
&gt; I don’t know of any way to do this externally for a single program. Well, I suppose if the system is set up to allow `ptrace` there are ways to do this patching externally, too… perhaps with `gdb` (if the program has debug symbols), or just by writing to `/proc/…/mem` directly (after somehow finding out where `argv` is mapped)?
7 --meaning it should be in `$PATH`
The overhead on that suggestion is a bit much for my purposes. Thanks for the suggestion, though. 
Dont quite get your question, but you can test outputs. Thing is that $var is reference to variable. So if you go *echo var* it will produce string var. *echo $var* will produce value of variable var because you send reference. Single quoting stuff bassicaly means treat this as string so echo '$var' will produce literally $var if you want single quotes in your string you will need to escape character. Double quotes means, treat all of stuff inside as single input to command so echo will see $var as reference to variable not just string. That way you can do something like *echo "this is var: $var"* and it will produce *this is var: value_of_variable*
next time, create an new, EMPTY, folder, and use THAT as the mountpoint 
I think I understand what you're trying to do. File contents: instance_a instance_b instance_c Bash: #!/bin/bash j=1 for i in `cat filename`; do instance[$i]='$j' j++; done Maybe something like that. My bash is rusty when it comes to arrays.
* `echo GREETING` -- output GREETING * `echo $GREETING` -- output _good morning_, expands variable with $ prefix * `echo '$GREETING'` -- output _$GREETING_, in single quotes no expansion will happen * `echo "$GREETING"` -- output _good morning_, variable will expand in double quotes as is, if there will be some trails of whitespaces around, it will be shown exactly same.
 awk 'BEGIN{cont=0}; {print $0, cont; cont++}' filename
For this, using an array is better than creating new variables. $ (mapfile -t &lt; &lt;(seq 3); echo "${MAPFILE[@]}") 1 2 3
Do you mean "each line to a separate variable"?
case would be much better in this scenario. Get in the habit of writing functional code: function install_software { software="ntp" read -p "Install $software? (y/n)? " choice case "$choice" in y|Y ) echo "installing $software" &amp;&amp; yum install -y $software;; n|N ) echo "not installing $software .. exiting" &amp;&amp; exit;; * ) echo "Invalid choice" &amp;&amp; exit;; esac } install_software 
 awk -F '/' '{print $2"/"$1}' &lt;&lt;&lt; "string" like awk -F '/' '{print $2"/"$1}' &lt;&lt;&lt; "aaa/xxx" 
 sed -E 's|(.*)/(.*)|\2/\1|' &lt;&lt;&lt;"aaa/xxx"
Works perfectly, thanks amigo!
Works as intended also, hadn't thought of using `awk`. Thanks amigo.
I'd prefer awk, like somone else explained, but if you want to use sed you can use backreferences, like this: $ cat foo.txt aaa/xxx bbb/yyy ccc/zzz $ cat foo.txt | sed -E 's/(.*)\/(.*)/\2\/\1/g' xxx/aaa yyy/bbb zzz/ccc Basically it let's you reference the stuff in paranthesis by numbers
Thanks for explanation!
Exactly. In eight years time when something breaks and you're out of coffee, 'print $1"/"$2' is going to be a lot more intuitive than some ascii boobs and a couple of tipis
Bash itself doesn't really care or know about key presses, and it can't intercept them like it seems you are thinking it will (so you can keep this script running in the background and just have it change your keys behaviour) Probably what you want is to have your desktop environment (gnome or whatever you use) do this for you
&gt; you're out of coffee At that point, nothing is going to be understood haha.
You could use something like IFS if you want to read from file based on some delimiter and do something with that input. Eg IFS="/"; read VAR1 VAR2 &lt;&lt;&lt; "some/string" Then you will have VAR1 to be equal to string "some"
[xbindkeys](https://wiki.archlinux.org/index.php/Xbindkeys)
I need to put the of this command into the IP array - $ipArray= "(aws ec2 describe-instances --filter "Name=tag-key,Values=arecord" "Name=tag-value,Values=*broker*" --query "Reservations[].Instances[].[PrivateIpAddress]")" echo $ipArray
I need to put the output of this command as the IP array $ipArray= "(aws ec2 describe-instances --filter "Name=tag-key,Values=arecord" "Name=tag-value,Values=*broker*" --query "Reservations[].Instances[].[PrivateIpAddress]")" echo $ipArray
Only to avoid polluting the environment (not *the* environment).
Assuming the output is delimited by newlines, and you have a semi-recent version of bash: readarray -t ipArray &lt; &lt;( aws ec2 ... )
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
 #!/bin/bash sbatch --wait *.x python test88.py
You could write an epilog script in slurm which would wait until the job has completed before running. 
This might be the answer. Iĺl look further into it! Thanks!
Two problems: 1. Did you install `pip install awscli`, looks like "command not found" error comes from this. 2. The output of the `describe-instances` is already in JSON format: [ [ null ], [ "172.30.0.25" ], [ "172.30.3.84" ] ] The output is the array of arrays, where the inner array may contain a list of private IP addresses. You should use `jq` to convert it to string and then create the array in bash.
That did the trick. Thanks a lot. 
I do have awscli installed. other programs using this feature are working correctly. I dont know how to use jq but will look it up. Thanks for the info.Appreciate it, 
&gt; readarray is not working since I am on mac :( 
Then you can do: IFS=$'\n' read -rd '' -a ipArray &lt; &lt;( aws ec2 ... ) If the output is in fact JSON like the other person mentioned, you'll probably have to manipulate it first. The simplest way to pull the IPs out would be something like this: aws ec2 ... | grep -oE '[0-9.]{7,}' But if that's not accurate enough you might have to use `jq` or something as mentioned.
In bash you need `${ipArray[@]}` to get it to expand as an array. Otherwise it'll only print the first one
&gt;echo $HOST$COMP_ID/$DIR_NUM/{0001..99}.doc &gt; ~/test A few thoughts: Don't use `echo`, use `printf` [instead](https://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo). $ printf '%04d\n' {1..99} &gt; test $ head test 0001 0002 0003 0004 0005 0006 0007 0008 0009 0010 Don't use uppercase variables unless you know why you need to. Quote your variables. `zsh` will only protect you so much from bad habits, best to fix the bad habits.
Here is its game developer blog in Russian: - [Part 1](https://habrahabr.ru/post/335960/) - [Part 2](https://habrahabr.ru/post/337896/) - [Part 3](https://habrahabr.ru/post/339268/)
Thanks for this PageFault, I will make those corrections soon! :)
Madness! I say. Idiocy! But brilliant and good fun. 
Don't need any external program for this: (No awk, sed etc.) Pure Bash: #!/bin/bash mapfile -t FILE &lt; input.txt for LINE in "${FILE[@]}"; do FIRST=${LINE%\/*} SECOND=${LINE#*\/} echo "${LINE} -&gt; ${SECOND}/${FIRST}" done
Or —output text
That looks right to me! I’ll test it out and let you know. Thank you!
It reminds me a bit of JQuery.
Thanks kipperER1 :)
Thanks therealmacjeezy! :D
Thanks xbris! I know the human brain is amazing at storing content! :D
try this: $ touch a.c fodfasdf.c asd.sds.c $ for f in *.c; do basename -s'.c' "$f"; done a asd.sds fodfasdf 
&gt; I tried to use find with printf directives and did some stuff with cut but neither worked find "/path/to/dir" -type f -name '*.c' -printf '%P\n' | cut -d '.' -f1
 $ find . . ./dacdac ./foobar.c ./bar.c ./fiddle.widdle.c ./sub ./sub/foooooooooc ./sub/bar ./sub/fgfg.c ./sub/baz ./sub/someother.c ./bar.a ./foo $ find . | grep "\.c$" ./foobar.c ./bar.c ./fiddle.widdle.c ./sub/fgfg.c ./sub/someother.c $ find . | grep "\.c$" | sed 's/\.c//g' ./foobar ./bar ./fiddle.widdle ./sub/fgfg ./sub/someother $ find . | grep "\.c$" | sed 's/\.c//g' | sed "s~.*/~~g" foobar bar fiddle.widdle fgfg someother 
Thank you for your suggestions, @whetu. Would you take the time to contribute to the project?
Not via USB, in any practical manner. If you want details on why, let me know. If they’re already connected to the same network, you can use it to send things across. If not, you can connect them via *ethernet*. From there — if the goal is to run commands on one machine while looking at the other one, SSH is the tool for the job. This’ll also let you see the results of the commands without switching computers. If your goal is to more generally *use* two computers with one set of input devices, you might want to look into *synergy*. This application essentially lets you move your mouse across computer [like they’re a multi monitor setup], and sends your keyboard input to whichever computer your mouse is currently in.
An imitation debug method could be to split your script into separate files, breaking it where you would want your breakpoints to be. Then just source each part one after another, checking values with echo in between. 
Something like [this](https://github.com/sluidfoe/linux-jumpstart/blob/master/dotfiles/.config/bash/prompt.d/99-ps4) can make `set -x` more useful. You'll have to do something about the colors(`${yellow}` and `${norm}`), though.
Awesome, used the set -/+. Works well! Don’t understand the color thing tho
I ended up doing it this way. Cheers!
VScode has an awesome debugger
You can search on https://pkgs.org which Linux distros use which versions. I'd assume Bash 4 unless you want to target very conservative in-house server environments
Thanks! You also gave me a sense on how to test this stuff.
Many ways to skin a cat: $ touch foo.c bar.c baz.txt $ ls -1 bar.c baz.txt foo.c $ for file in *.c; do printf '%s\n' "${file/.c/}"; done bar foo $ for file in *.c; do printf '%s\n' "${file%.*}"; done bar foo $ for file in *.c; do cut -d "." -f1 &lt;&lt;&lt; "${file}"; done bar foo $ for file in *.c; do awk -F '.' '{print $1}' &lt;&lt;&lt; "${file}"; done bar foo Of course there are problems with these, for example the approach of splitting the first field based on a delimiter (the `cut` and `awk` approaches): $ touch foo.bar.c $ for file in *.c; do awk -F '.' '{print $1}' &lt;&lt;&lt; "${file}"; done bar foo foo This doesn't affect the string manipulation approaches: $ for file in *.c; do printf '%s\n' "${file/.c/}"; done bar foo.bar foo $ for file in *.c; do printf '%s\n' "${file%.*}"; done bar foo.bar foo But the string manipulation approaches have extreme edge cases, so the use of `basename` may simply be the safer approach 
With a few exceptions, regular expressions tend to be slower than globs. Also, more generally speaking, the more individual statements/comparisons you have in the shell code, the slower it will be to execute. So if you could combine multiple pattern matches into one it would be ideal. Here are my own benchmark results (using a method similar to /u/ray_gun's): # Single regex pattern match [[ "$command" =~ ^_type=|^isRequired=|^isReadOnly=|^noHandle=|^_isGlobal= ]] 0.472s # Multiple glob pattern matches [[ "$command" == "_type="* || "$command" == "_isRequired="* || "$command" == "_isReadOnly="* || "$command" == "_noHandle="* || "$command" == "_isGlobal=*" ]] 0.274s # Single extended-glob pattern match shopt -s extglob [[ "$command" == _@(type|isRequired|isReadOnly|noHandle|isGlobal)=* ]] 0.154s
Sounds like that's what I'll be using, thanks!
Also look into [cssh](https://youtu.be/5Uj7Ns0Ihdc?t=42), or the [broadcast group on a terminal emulator like terminator](https://www.youtube.com/watch?v=gXGQ56PV_1A)
Thanks ! worked like a charm :)
FWIW, your `for` loop isn’t the fastest it could be: $ time for i in {1..1000000}; do :; done real 0m1,538s user 0m1,482s sys 0m0,056s $ time for ((i=0;i&lt;1000000;i++)); do :; done real 0m2,507s user 0m2,507s sys 0m0,000s I’m not sure how this can be, but somehow incrementing the counter is slower than generating all million expansions up-front and going through them.
Hey thanks for this tool! This looks to have some more uses for me!
functions
You can just use the keys of an associative array as a set...
What does `_@(...)` do in the single extended-glob pattern match?
`@(...)` is similar to `(...)` in regular expressions. It's used for alternation, as in `@(a|b)` to match `a` or `b`. There are a few other variants that do different regex-like things, like `+(a)` to match `a` one or more times.
There's a command for that called `join`. But why does file 2 have two path entries per volume? Otherwise you can do $ join -t, 1.txt 2.txt vol,path,path vol123,/path/to/data/,/some/other/path/to/data1/ vol234,/path/to/otherdata/,/another/.path/to/data1/ But that only works if there are no duplicates. In Bash I think it's faster to store the first file in an associative array: declare -A file1 IFS=, while read vol path; do file1[$vol]=$path done &lt; file1 while read vol path; do echo mount "${file1[$vol]}" "$path" done &lt; file2 output: (remove the first line) mount path path mount /path/to/data/ /some/other/path/to/data1/ mount /path/to/data/ /another/path/to/data2/ mount /path/to/otherdata/ /another/.path/to/data1/ mount /path/to/otherdata/ /path/to/another/location/data2/ 
Yes, check the exit code. With an exit code of zero, empty string means enter was pressed. Also, `echo` without `-n` will pipe a new line character into `xxd`. #!/bin/bash if read -n1 -s -r -t2 -p $'Press any key\n' KEY then echo -n "key pressed: " echo -n "${KEY:-$'\n'}" | xxd else echo "not pressed" fi
 $ help test | grep -- -z -z STRING True if string is empty. It saw it just fine. You wanted `-n` there, though checking `read`'s exit status is more correct. Also, give it an empty IFS and -d if you want to also catch space, tab and newline. if IFS= read -rd '' -s -n1 -t2 -p $'Press any key' key; then printf '\nKey &lt;%s&gt; pressed\n' "${key:-NUL}" else printf '\nTimeout waiting for key-press\n' fi
 tar -xvf /path/to/some.tar -C /path/to/final/target/directory Edit: -C not -c
1. The `bash` regex dialect doesn't appear to understand `\d` or `\w`. Use `[0-9]` and `[a-zA-Z0-9_]` instead. 2. I don't see how this regex could ever work, even in a different dialect. Did you copy it by hand and put the `+` in the wrong place? This works for me: regex='^([a-zA-Z0-9_]+-){2}[0-9]{2}$' I don't know if the extra parentheses were important because you were capturing things. Disclaimer: tested on Linux, not MacOS.
Check /u/RFC-1925's answer. You can do the entire thing in `tar`.
The regex works in all my other things, i may have written it wrong (i was on mobile when i posted here on the train). ill have to use yours. Thanks for the help. 
Ooooh! I didn't realize I mixed up the outputs, and then I tested it with space and \n, so no wonder it looked like it didn't work. Thanks a lot!
What about this?: gnome-terminal -e "bash -c 'secs=$((60)) while [ $secs -gt 0 ]; do echo -ne \"$secs\033[0K\r\" sleep 1 : $((secs--)) done shutdown now'" 
Thanks but it doesn't work ;( it just shuts down right away
This works for me: $ xrandr | awk '/primary/ {print $4}' | cut -d+ -f2 
you can also set field separator with awk awk -F+ '/primary/{print $2}'
Thank you, now I have to learn about `awk` :D
I did not know about extended globbing. Very useful!
http://gregable.com/2010/09/why-you-should-know-just-little-awk.html
Once you do, you rarely go back. It's so damn powerful.
`^"(?:[^"\\]|\\[^n])*"$`
Wouldn't it be ssh user@address "sudo poweroff"? You could use Ansible as well. 
This is the error i receive when i use poweroff or systemctl reboot Failed to set wall message, ignoring: Interactive authentication required. Failed to reboot system via logind: Interactive authentication required. Failed to start reboot.target: Interactive authentication required. See system logs and 'systemctl status reboot.target' for details. I'll take a look at Ansible. thanks!
Sounds like you aren't root. You could try "ssh user@address sudo poweroff" But that will likely fail for the same reason if you need to enter your sudo password. 
Either ssh as root, or create a special user that has no password sudo privileges and ssh as that user. You would just update the /etc/sudoers files and a quick google should help you figure it out.
this seems best bet. thanks.
+1 for advising not to use poweroff. Do not use poweroff.
Why? Just out of curiosity
Shutdown attempts to gracefully close active processes
Don't do this. Add the user to sudoers and 'sudo halt'. Add 1 users key to root's keychain goes against best security practices.
Saltstack it up?
This is the most correct answer.
I would have the systems poll the ups machine. Root can poll the ups machine as an unprivileged user and then shutdown the box. This prevents a massive attack surface if the ups machine gets compromised.
This is great. Thank you so much. 
Poweroff is intended for more desperate occasions. Like maybe a daemon crashes and can't be gracefully handled anymore. You can use poweroff to yank the rug out from under the system. It's a pretty nuclear option. A graceful shutdown would be done with shutdown -h or init 0.
It has that effect. Very useful, when you remember it, and don't get it confused with things like `&lt;&lt;&lt;(something)`, `&lt;&lt;(something)` and `&lt; &lt;(something)`. Thank goodness it's easy to test expressions in bash though, for constructing pipelines.
Hi, it is probably better to ask at r/raspberry_pi/. 1. I would start with check if serial console is disabled: https://elinux.org/RPi_Serial_Connection#Preventing_Linux_using_the_serial_port (There should be some kernel parameter removed, I forgot what it was.) 2. Do you have some kind of multimeter or oscilloscope available. You can see pulses coming from GPIO pin when you will send those bytes to ttyAMA0, but use `printf "AT\r\f"`, echo sends '\n' by default. If you do not have an oscilloscope, maybe just connect some LED here to see blinks, or voltage jump on multimeter. 
Thanks, sorry forgot to mention that. The data is coming out of TX perfectly. I checked that with an Oscilloscope. Even the module is responding perfectly. Checked that too with the Oscilloscope. The issue is basically really and "only" that cat isn't picking it up somehow.
Oh, I see, picocom is working :) Is pressing ENTER in picocom sending \n or \r? You may play with something like: printf "AT\r" &gt; /dev/ttyAMA0 printf "AT\n" &gt; /dev/ttyAMA0 printf "AT\n\r" &gt; /dev/ttyAMA0 I was creating different script for stty manipulation in bash, but terminal node controller only received command, when '\r' was send.
Could be, but doesn't seem to be the root cause. The module is doing anyway a 1:1 echo and putting afterwards an error or whatever. I can confirm that after probing the TX line going to the RX line. But still doesn't get anything with cat :(
 PORT=/dev/ttyAMA0 SPEED=115200 stty -F $PORT ispeed $SPEED ospeed $SPEED -ignpar cs8 -cstopb -echo trap 'echo -e "\n- Ctrl-c disabled, press Ctrl-d for quit. -"' SIGINT cat &lt; "$PORT" &amp; RPID=$? while read cmd; do printf "$cmd\r" &gt; "$PORT" done kill $RPID Try this, it created process reading from the port before you sending data, maybe, it is answering too fast.
Actually, it will be probably more tricky, because that first `cat` which is reading answer from the port is a process in the background. I would probably output this process to named pipe (mkfifo) and read those data in the main loop. Sorry, I cannot test it here, to provide you working solution.
Yes I just implemented it in my shell, and noticed that: http://www.oilshell.org/blog/2018/01/15.html As of bash 4.4, I believe it sets `$!`, and you can `wait $!` to get the exit code. I remember seeing that on `help-bash@`, but I didn't try it. 
I always found things like `[[ -z "$string" ]]` to be a better test than `[[ x"$string" = x ]]`; do you know of reasons not to use `-n` or `-z`?
Maybe on ancient systems, if these tests were introduced in a later version of POSIX (with [ instead of [[)? No idea though, you'll have to check in the spec
*cat* is waiting for an EOF, which will never come. Try *read* instead.
Author here - that's not given as advice, rather to explain why this is seen in scripts. 'This is why you occasionally see comparisons like this in bash scripts: if [ x$(grep not_there /dev/null) = 'x' ] so that if the command returns nothing it still runs.' but may I'll point out that there's no reason for this anymore (I do in the book!).
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
try this $ seq 5 | grep -F '2 4 8' 2 4 $ cat f1 2 4 8 $ seq 5 | grep -Ff f1 2 4 I'd advise to use `-f` option which accepts file input also, there is nothing special here with regards to `-F` option and newlines.. this behavior is same with/without `-F`
Looks like your Android device might have been infected with Malware. Don't see this issue on Firefox for Windows or Linux. 
What I suspect happened is that it took the first line you typed as the pattern, and the remaining lines as standard input. The "bare" solution is to add quotes; type it like this: grep -F 'PATTERN1 PATTERN2 PATTERN3 PATTERN4' Or, you could use the bit of Bash syntax that `$'str'` quotes `str` such that C-style escapes work, making newlines more convenient to type: grep -F $'PATTERN1\nPATTERN2\nPATTERN3\nPATTERN4' Or, you can use the `-e` flag to enter each pattern as separate arguments: grep -F -e PATTERN1 -e PATTERN2 -e PATTERN3 -e PATTERN4 Or, you could use the `-f` flag to get the list of patterns from a file: echo PATTERN1 &gt; pattern-file.txt echo PATTERN2 &gt;&gt; pattern-file.txt echo PATTERN3 &gt;&gt; pattern-file.txt echo PATTERN4 &gt;&gt; pattern-file.txt grep -F -f pattern-file.txt 
&gt; #!/bin/bash &gt; file=userdata &gt; while IFS= read -r line &gt; do &gt; # echo line is stored in $line &gt; echo $line &gt; done &lt; "$file" #!/bin/bash file=userdata while IFS=, read -r I U P do SSHPASS="$P" sshpass -e ssh "$U"@"$I" &lt;"$somescript" done &lt;"$file"
This article describes one potential Malware that specifically tries to use your microphone to spy on you. If it was my phone, I would reset to factory defaults. 
Thanks. I'll have to figure out if I have everything backed up and reset. 
Probably one of the most concise and well written responses i've ever had, thank you so much, and while i won't use this in this occasion i've taken note as this might help with something else i'm working on. thank you.
Samsung Galaxy S8
Man bash, see the Parameter Expansion section: Assign Default Values. If parameter is unset or null, the expansion of word is assigned to parameter. The value of parameter is then substituted. Positional parameters and special parameters may not be assigned to in this way. The initial ':' means no op, i.e. do not try to execute a command ${GO_VERSION}. So your understanding is correct, it is for setting default values for unset variables. 
Interesting... Everyone I do poweroff (Centos) it shows everything getting stopped first. .. I always assumed this was just an alias for shutdown -h now
This answer rocks my world.
https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
It is also POSIX compliant (and thus heavily used in FreeBSD rc.d scripts for example). Don't forget quotes though `: "${foo:="bar"}"`
man bash
Maybe just make the file hidden until the song name is correctly guessed. You could also encrypt the files with gpg and the password to decrypt is the song name.
An mp3 file is not a program... Unless it's a malicious program named as an mp3 buy an attacker, but we have no reason to think that's the case. When you double click it, you're not executing it like you'd execute a bash script or command executqble. MacOS is opening it with a default application it associates with the file type. For an mp3 file that app would likely be set to be iTunes. 
This is way saner solution than what I was thinking. My idea was to corrupt the header in a specific way (essentially by `cat`ing the file to a semi-random, known string) to render it not an mp3 file by any player. Once given the answer, the string can be stripped by the byte count and it would be playable again. However, I am not sure this would work. 
The `(( ))` is some sort of "math mode" in bash. You can do things like the following with it: (( x = x + 3 )) (( x += 3 )) (( x++ )) if (( x &lt;= 20 )); then ...; fi You don't need to use a `$` for variables inside the `(( ))` as every word used inside is treated as a variable name.
Do you want to shut it down, or do you want it to continue and you're concerned that something might cause it to shut down and you wouldn't know if it exited successfully while producing a valid tar file?
The 2nd one, more or less. So far it has been running for ~2 days and compressed ~400 gb of data, and I really dont want to have to redo that if it got interupted somehow (windows being forceful with a restart was/is my main concern). And its still not even half done... This was part of an auto-run function that, for a given input directory, loops through all level 1 sub-directories and compresses each individually. I thought this would spread out the load enough, but it turns out that out of the stuff I wanted to tar+compress (which contained ~1.2 TB of stuff and &gt;6 million individual files) something like 80% of it was all in a single subdirectory. And because of all the individual files read time was painfully slow (for a decent part of the compression operation 7za at max settings wasnt even hitting 10% cpu usage, since the hard drive was only reading a few hundred kilobytes per second). as such, getting an accurate idea off how much data was in each subdirectory was something I didnt have the patience for before I started running this. The part im not sure about is how exactly tar schedules what it is going to process next. I can imagine it working a few different ways: 1) It generates a (partial) master list of all the directories/files it has been assigned to compress when it starts running (it isnt a complete list, since it would have spent way more time doing any sort of pre-analysis if that were the case, but something like a list of top level directories would be doable) 2) It determines what it will process on the fly but "reads ahead" a little bit to pre-plan what it will do next to some limited extent 3) It determines what it will process next completely on the fly based on what is instantaneously available within the selected run area. Assuming it does generate some sort of "list" of what it will process next, it seems like moving a directory/file that is already on the list to a different directory outside of the run area would throw an error of some sort (something similar to the "file changed as we read it" error that ive seen it occasionally throw). If it doesnt "plan ahead" and just uses what is immediately available then moving directories out of the way shouldnt be an issue (as long as they are completely finished moving before tar reaches them).
It's convoluted and evil, but it just might work.
This is how I handle command line args. It loops through all positional parameters before continuing. I usually put this after defining functions in scripts. while [ $# -gt 0 ]; do case $1 in -f|—foo) some_function or code ;; -b|—bar) some_variable=$2; shift ;; shift esac done In that example, -f/—foo is a flag that doesn’t take an argument, while -b/—bar does. Hence setting some_variable to the next positional parameter $2 and the extra ‘shift’. 
You can try [getopts](http://wiki.bash-hackers.org/howto/getopts_tutorial).
Use either `getopts` or the util-linux version of `getopt`. `getopts` is built into the shell, so it's very portable, but it only supports short options and it doesn't handle permutation (in other words, it stops parsing options as soon as it encounters a non-option argument). `getopt` is an external utility. The util-linux version of it is a front-end to `getopt_long(3)`, which gives you those GNU-style long options and handles permutation. There is a BSD version of `getopt`, too, but it is absolutely useless, so you shouldn't use it. Here's how I would do it with `getopt`: main() { local args db local -a tmp tags args="$( getopt -o d:t: -l db:,tags: -- "$@" )" || { printf &gt;&amp;2 'some usage error\n' return 1 } eval set -- "$args" while (( $# )); do case "$1" in -d|--db) db="$2"; shift 2 ;; -t|--tags) IFS=, read -ra tmp &lt;&lt;&lt; "$2" tags+=( "${tmp[@]}" ) shift 2 ;; --) shift; break ;; *) printf &gt;&amp;2 'some unhandled option error\n'; return 1 ;; esac done printf 'got db: &lt;%s&gt;\n' "$db" printf 'got tags: ' printf '&lt;%s&gt;' "${tags[@]}" printf '\n' } main "${@}" Then you can use it this way: % ./test.sh -d files_db -t tag001 -t tag002,tag003 got db: &lt;files_db&gt; got tags: &lt;tag001&gt;&lt;tag002&gt;&lt;tag003&gt; You can put the options in any order you want. You can use `-t` to give a single tag multiple times, or you can combine tags into one argument with commas. (If commas appear in the tags, you can use some other character, or you can just disallow that feature.) I didn't handle `-r`, but you get the idea -- you can do the same kind of thing with it. If you want to use `getopts` instead, it's pretty similar, but you don't need the `eval` stuff, and instead of `$2` you use `$OPTARG` to get the option arguments. It's all explained in the `man` pages of course
As far as the version thing, i would do the `snmpget` with the more common protocol version first, and then if it fails try again with the other one. But if you want to prompt the user for it, just add another `read`.
I don't know if Void is just bad with manpages, or I need to google instead, but what is the syntax for using `local`? Thanks for responding.
It takes the same options as `declare` and `typeset`. It's described in the builtins section of the manual: https://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html (`-a` in this case means to declare those variables as arrays.)
&gt;grep -irl @linux | cp ./linux_notes/ So essentially you're taking the results of `grep` and throwing it at `cp` which doesn't understand what you're wanting to do. There are many ways to approach this, perhaps the easiest to understand is a `for` loop: for file in $(grep -irl @linux); do cp "${file}" ./linux_notes/; done Another way to do this would be with `xargs`. This approach would go along the lines of: grep -irl @linux | xargs -I {} cp {} ./linux_notes/ That's off the top of my head, so could be wrong. If it doesn't work, it's at least some google keywords for you.
these both work perfectly! thank you so very much!
This is probably still broken, but it's closer to where you need to be: #!/bin/bash dateToday=$(date +%m-%d-%Y-%H:%M) getVersion() { if timeout 2 snmpget -v2c -c "${community}" "${hostName}" sysName.0 &amp;&gt;/dev/null; then printf '%s' "v2c" else printf '%s' "v1" fi } getInfo() { sleep .25 printf '%s\n' "-----------------------------" \ "-Hostname, IP, Serial Number-" \ "-----------------------------" \ "${hostName}, ${ip}, $(snmpget -"${snmpVer:-$(getVersion)}" -Oqv -c "${community}" "${ip}" 1.3.6.1.2.1.47.1.1.1.1.11.1)" \ "This section will get the Model / Serial of each Switch in the Stack" \ "Model / Serial of Switch 1" \ "$(snmpget -"${snmpVer:-$(getVersion)}" -Oqv -c "${community}" "${ip}" .1.3.6.1.2.1.47.1.1.1.1.13.1000 .1.3.6.1.2.1.47.1.1.1.1.11.1000)" } echo "Please enter community string: " read -rp community while read -r hostName ip snmpVer; do if ping -c 1 "${ip}" &gt; /dev/null; then getInfo &gt;&gt; ./devices_up_"${dateToday}".txt else printf '%0s %0s %s\n' "${hostName}" "${ip}" "DOWN" printf '%0s %0s %s\n' "${hostName}" "${ip}" "DOWN" &gt;&gt; ./devices_down_"${dateToday}".txt fi done &lt; ping.txt Differences include: * Sane 2-space indendation * `do`'s and `then`'s tucked out of the way on the same line * camelCase variables to avoid clobbering the environment or unintentionally calling something * `${variable}` style for readability/consistency with arrays/substitutions * Variables quoted to prevent unwanted issues like word splitting * You were `cat`ing `ping.txt` into `while read` and also using `&lt; ping.txt`. Use one or the other (latter preferred, but former is more portable) * You were declaring your function inside your `while` loop. Declare it at the top of the script and then call it when needed. * Backticks were superseded in 1983. Use `$()` instead. * `read` should almost always be used with `-r` * `printf` should be used over `echo` * There's almost always no reason to check `$?`, just test the command directly &gt;My issue is that, due to environmental variables outside of my control, we have some devices running SNMP -v1 and -v2c. I am unsure how to determine this information through Bash to prompt to run You're reading in `ping.txt` anyway, so why not have that detail there? I've updated the script to expect that detail to be in your inventory (i.e. `ping.txt`), but it will also try to autodetect it using `getVersion()`. This is actioned using this: `"${snmpVer:-$(getVersion)}"`, which essentially means "use `snmpVer` if it's defined, otherwise run `getVersion()`. It's on you to handle failures beyond that point. &gt;As an added caveat, if someone might be able to tell me how I can get it to exclude certain results `grep -Ev 'list|of|matches|to|exclude'` One last thing, printf '%0s %0s %s\n' "${hostName}" "${ip}" "DOWN" printf '%0s %0s %s\n' "${hostName}" "${ip}" "DOWN" &gt;&gt; ./devices_down_"${dateToday}".txt You might like to look at `tee` 
try chattr +i filename.mp3
I would use something like: while read file; do cp "${file}" ./linux_notes; done &lt; &lt;(LC_ALL=C grep -Firl '@linux')
At work, will explain a bit later
Simpler without a pipe: cp $(grep -irl @linux) ./linux_notes/ With a for-loop: for f in $(grep -irl @linux); do cp $f ./linux_notes/; done And finally with a pipe: grep -irl @linux | xargs -I files cp files ./linux_notes/ Watch out though, if you run these twice you'll get errors. cp: 'linux_notes/6_linux' and './linux_notes/6_linux' are the same file cp: 'linux_notes/3_linux' and './linux_notes/3_linux' are the same file cp: 'linux_notes/8_linux' and './linux_notes/8_linux' are the same file cp: 'linux_notes/9_linux' and './linux_notes/9_linux' are the same file You would need to add this to your `grep` command: --exclude-dir=linux_notes 
Getopt, but frankly I just use Python with argparse if I reach that point.
may be not efficient, but simple logic if both files have same number of lines and guaranteed one-to-one match.. if not, change the question to include a better sample.. $ paste -d' ' f1 &lt;(grep -Ff f1 f2) abc hellodef def hiabc 
Try this $ awk 'NR==FNR{a[$1]; next} {for(k in a) if($0 ~ k){print k,$0; delete a[k]}}' f1 f2 def hellodef abc hiabc this creates an associative array with keys from first file and then when second file is read, every key is checked if it matches the line.. if match is found, print the key and line.. and then delete the key so that iteration keeps reducing for further matches
&gt; awk 'NR==FNR{a[$1]; next} {for(k in a) if($0 ~ k){print k,$0; delete a[k]}}' f1 f2 thank you. i've used arrays but never thought to use it here at all. Good one. This is working as intended bro. Thanks.
How would you say python is better suited to the task? I wouldn't object to converting to python, seeing as I have some experience and can use the `sqlite3` module.
http://www.earthclassmail.com are scamming people.
 ... prompt="Is $SRC the directory you want to echo?" read -e -p $prompt choice ....
&gt; array=($list) Never use unquoted `$var` or `$(cmd)` in an array assigment. &gt; # replace escaped spaces with new string that doesnt contain a space and is unlikely to be in any filenames list="${list//"\ "/"&lt;SPACE&gt;"}" No amount of backslashes or quotes will make that right. To assign a list of filenames to an array, use a glob: array=( * ) for i in "${!array[@]}"; do printf 'index %d, name: %s, type: %s\n' "$i" "${array[i]}" "$(file -b -- "${array[i]}")" done 
&gt; for user in $(cat /etc/passwd); do username=$(echo $user | cut -d : -f 1) home=$(echo $user | cut -d : -f 6) echo "Username:" $username "Home_Directory:" $home &gt;&gt; user.csv done I hope the online course didn't teach you to parse /etc/passwd in this broken manner, because if it did, it's garbage. See [BashFAQ 1](http://mywiki.wooledge.org/BashFAQ/001) on how to parse a file field-by-field; it even has an example on reading /etc/passwd. As for writing a CSV-like file with header, use a command grouping with a printf and a loop: { printf 'username,home\n' # header while ... ... done &lt; /etc/passwd } &gt; user.csv
&gt; array=( * ) So this works for filenames, but Im going more for general usage. As such, having it operate on a space-separated list seems like the most universally compatible way, since 1) the output of pretty much every function is a list (sometimes with newline separators, but a list nonetheless), and 2) you can do foo() { ... inputList="$@" outputArray=$(functionToBeInlined $inputList) ... } &gt; Never use unquoted $var or $(cmd) in an array assignment. The result will be split into words by word splitting... This is the issue, yet on the other hand doing array=("$list") keeps spaces where they should be but puts all of the items in the list into a single array field with items that are newline-seperated. Unless there is some straightforwrd way to generte the array so that it splits things based on the newlines in "$list" (which there very well could be - Ive only been using bash for a few months and am still learning a lot of the "builtin shortcuts"). The only way I can think of implementing this is to use word splitting and temporarily replace the spaces in the filenames (or, in general, whatever group of characters you want to end up in the same array field) with a temporary indicator that has no spaces and doesnt naturally occur in the list of names/items, then make the array using work splitting, then replace the temporary indicator with spaces again. And that is exactly what I dod. And it works, it just isnt portable, unless theres a way to pass an array between functions without temporarily converting it back to a list (which, again, is entirely possible).
My guess is that your prompt is invalid, specifically your $’\n’ construct. The closing quote should come right after the n, followed by &gt;&gt;&gt;. Who knows how bash is parsing that. 
I was actually just doing something similar to you in regards to eliminating whitespace. You can get rid of all whitespaces for files and directories (below the current directory) by using the find &amp; rename command together. Just put the following in the directory you want and run it. ------------------------------ find -depth -name "* *" -execdir rename "s/ //g" "{}" \; ------------------------------ If you need to replace the space with a temporary string do insert the string in the following location: ------------------------------ find -depth -name "* *" -execdir rename "s/ /*here*/g" "{}" \; ------------------------------- Hope that helps 
Thanks for the suggestion, though this doesnt really end up working for me. My Linux install (on my personal computer) is in windows via WSL (which really does do a fantastic job of replicating an actual dedicated Ubuntu install in *almost* every way and without needing a VM or a the need to start a seperate linux kernal, and as a bonus you have interoperability with windows). Now most of my personal and WSL files are already free of spaces (it has long been a habit to use ' -' instead of ' ' in filenames when possible). However, there are a huge number of essential system files with spaces (e.g., 'Program Files' and 'Program Files (x86)') that my computer would probably throw a little temper tantrum if I force renamed. There are also a bunch of system generated files with spaces (e.g., file backups made automatically with the win10 'file history' feature), where renaming would probably cause the system to lose track of the file (followed by remaking it and generating a new file with spaces in the file name). As much as I wish I could cd to C:\ and run that, I really dont feel like reinstalling windows 10 again lol.
Sorry, I assumed your entire filesystem was linux and there was a specific directory-tree involved. That does not sound like it would be fun, and I can imagine the fit Windows would throw. I suppose a for/if loop with a filter for non-system files would be just as much trouble. I rarely use Windows now-a-days, so I won't be of much help. Anyhow, I wish you luck in solving this!
You can kind of pass arrays into functions by using the name instead of passing the values. Inside the function, you can then access the array through using a variable that's a reference to a name. Variables that are references are created with the `-n` parameter for the `declare` and `local` keywords. You can see that `-n` option mentioned in the output of `help declare`. Here is a concrete example about playing with this reference method at the command line: $ x=(aaa bbb ccc) $ foo() { local -n arr="$1"; for i in "${arr[@]}"; do echo "entry: $i"; done; } $ foo x entry: aaa entry: bbb entry: ccc Here's that function with line-breaks so that it's easier to read: foo() { local -n arr="$1" for i in "${arr[@]}"; do echo "entry: $i" done } What's happening here is, the `foo x` makes it so a text parameter with the content `"x"` gets passed into the `foo()` function. Then inside the function, that `"x"` text will be visible as `$1`. After doing `local -n arr="$1"`, working with that `arr` variable will be exactly like using the `x` array outside the function. When you do `"${arr[@]}"`, you get a list of the elements in the array, and behind scenes bash will redirect your operations to the real `x` array. If you use this reference method to access arrays, your function will be able to modify the original array that's outside of the function. Here's another example: $ x=(aaa bbb ccc) $ echo "${x[@]}" aaa bbb ccc $ foo() { local -n arr="$1"; for i in "${!arr[@]}"; do if [[ ${arr[i]} = bbb ]]; then unset arr[i]; fi; done; } $ foo x $ echo "${x[@]}" aaa ccc $ echo "${!x[@]}" 0 2 Here's that new `foo()` example again with line-breaks for easier reading: foo() { local -n arr="$1" for i in "${!arr[@]}"; do if [[ ${arr[i]} = bbb ]]; then unset arr[i] fi done } About how to deal with spaces in variable values, check out the program named "shellcheck". Your distro probably has a package for it. It will try to find typical mistakes with shell scripts like missing `"` in your scripts. There's also a website www.shellcheck.net where you can access the program online to see how it works without having to install it. Last thing, you might want to look into the Python or Perl scripting languages for things that require more complicated data structures like arrays or a hash. Both of those should be installed everywhere as on basically any Unix environment there's a bunch of scripts floating around needing them as an interpreter.
Thanks, I didn't know that extended globbing was a thing.
This works for me: #!/usr/bin/env bash SRC=$(pwd) echo "$SRC" read -r -p "Is this the directory you want? ([Y]/n)" choice case "$choice" in "y"|"Y") echo "yes";; "n"|"N") echo "no";; *) echo "invalid response";; esac 
Thanks man!
Does the ~/templogs directory exist?
Bash will treat that `~` as the literal `~` text character if you surround it with `"`. You can try to experiment with leaving the `~` outside of the `"` somehow, or you could switch to using `$HOME` which will always work even inside `"`.
You have two sets of double quotes in your filename. The inside set needs escaped.
Git clone, not git pull
You mean you want to write a script that executes these commands as a script? `sudo` starts a new command, so `cd` and the subsequent commands will only be executed when sudo finished. The interactive prompt you see is `sudo su` still running. Either you write two scripts, with the second containing the commands requiring elevated privileges, and the first running this one with sudo; or you write a more complex single command line you execute with sudo. **Variant 1** *Script 1* #! /bin/bash exec sudo /path/to/script2 *Script 2* #! /bin/bash cd /home/project || exit 1 git pull https://mysite.com/upstream/myproject.git master my_password **Variant 2** sudo bash -c 'cd /home/project &amp;&amp; git pull https://mysite.com/upstream/myproject.git master; my_password'
No worries, I dont think I mentioned that I was using WSL in the original post so assuming I was using a dedicated Linux install is entirely fair. I find my usage of WSL and windows is split pretty even. I use windows for web browsing, media playback, and a handful of applications (matlab, mathematica, word, powerpoint). Most other stuff I do in WSL. I could robably do these things just fe in a linux-only environment too, but I think they tend to work a bit more smoothly in windows. Its nice because WSL actually uses the main windows NT kernal (not a dedicated linux kernal), which means 1) windows and WSL "talk" to each other rather easily, 2) having it running takes almost no resources (for idling in the background, ofc running something in WSL still takes cpu cycles and memory), and 3) it does effectively everything I need/want it to do the only area Ive personally noticed is that mounting support isnt as god as with a dedicated linux distro, but everything else works just like the real ubuntu). At any rate, to get back on topic, I think the "pass-by-reference" approach recommended by another commenter is the best way to implement passing an array without losing the array structure because of spaces.
bash can not help you with entering the password. There are tools like [Expect](http://expect.sourceforge.net/) that could, but only utilise them as last resort. Luckily git itself has a variety of means to assist you. See for example `man git-credential` and [Credential Storage](https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage) in [Pro Git](https://git-scm.com/book/en/v2).
Thanks for the great response and the examples! I imagine this will be the best (and perhaps only) way.. A couple questions though: 1) The use case I am thinking of is that from a function I call a sub-function, which implements some set of operations (e.g., performs computations, grabs filenames etc.), organizes that info into an array, and returns that array back to the parent function. This seems to almost be the opposite of your example n the sense that the array originally comes from the child function, not the parent function. Is this going to be an issue? It would seem that the answer to this depends on how the pass-by-reference works at a low level. Im guessing that passing by reference works at a low level in much the same way that C/C++ style pointers do, and that the referencing is in effect is just passing info about a list of memory addresses that describe where in memory the array is (rather than the contents of these addresses, which is how the array would normally attempt to get passed.) The question is then whether or not bash is smart enough to keep these addresses reserved for the array as long as any active process is using then, even if the process that originally wrote data to them no longer exists. I could see this going either way, since in general in Linux systems it seems like a lot of things are tied to the original process/shell that initialized it. If it wouldnt normally work, maybe calling the function with exec or eval such that it doesnt actually run as a seperate process but rather as a direct part of the parent process. Admittedly it is probably best to just test it out and see what happens, but Im kind of curious what bash does behind the scenes here. 2) Another thought I had is is there is an "evalInCaller" function then the subfunction can just setup text strings of the commands that would need to occur to produce the desired array, then call these with evalInCaller and have these commands directly evaluated in the parent function (making it so that there is no need to pass the actual array at all). Do you know if anything like that exists? I only thought of this since Matlab has a function to do this that I use from time to time (for example: it is useful for "input checking" and setting default values for invalid/undefined function inputs, since trying to pass a variable that is undefined will throw an error. You can run a check to see if it exits (something equivilant to [ -z $n ]), but needing to manually check the inputs of a function designed to automate input checking sort of defeats the purpose). At any rate, Im assuming that since it exists in Matlab then it probably exists in other languages too. Also, thanks for the tip about shellcheck. I knew the website existed, but didnt know there was a package available (which is now installed). I imagine I will use this quite often.
Thank you! That did indeed fix the problem.
Thank you it will more than suffice and I appreciate the help very much indeed.
You can tell your function a name, and with that `local -n` thing, the function will be able to create a new array under that name. Here's a test at the command line: $ foo() { local -n result="$1"; for x in {a..g}; do result+=("$x"); done; } $ foo hello $ echo "${hello[@]}" a b c d e f g $ foo hello $ echo "${hello[@]}" a b c d e f g a b c d e f g That function with line-breaks looks like this: foo() { local -n result="$1" for x in {a..g}; do result+=("$x") done } With that `+=(...)` operator used here, you might need a `result=()` or a `unset result` line to start with an empty array. Behind the scenes, this stuff does not work like pointers and you don't have to worry about memory. Bash works with text. It goes over your lines multiple times and does text replacements before it executes the end result. For example, if inside the function I'd add a `echo "${result[@]}"` at the end, It would do something like this: echo "${result[@]}" --&gt; look up 'result' echo "${hello[@]}" --&gt; look up 'hello' echo a b c d e f g --&gt; print stuff Bash isn't good for stuff that works with a lot of data. It can be really slow. What often works well is if you try to translate your problem into something where a bunch of programs or functions work on transforming streams of data. You can then use pipes to tie all those steps together and bash will start a parallel process for each step, often perfectly hiding that bash itself is slow.
Depending on what is providing your repo at mysite.com, you could avoid needing to provide a password to clone / pull / fetch your project, if you can use ssh instead of https, with ssh keys. Cloning or updating the source, as root (using sudo, esp. `sudo su`, is probably) is probably not what you need or want, and could be done as your normal user with normal privileges. If you need to run a command in the project you're updating with elevated privileges, then you could run `sudo /home/project/command` instead of spawing a shell for root (`sudo su`).
Another way without extra processes: isvi= IFS=: read -a shellopts &lt;&lt;&lt;$SHELLOPTS for o in "${shellopts[@]}"; do if [[ $o = "vi" ]]; then isvi=yes; fi done if [[ -n $isvi ]]; then # do something in vi mode else # do something in non-vi mode fi However, I think that vi-mode is only a property of an interactive shell, so if you run that as a script, you get no output, but if you source it into your own shell, you will get the right result.
Sure, but `grep -q` isn't portable
My suggestion would be something based on: set -o | awk '/^vi/ {print $2}'
I will do that and set up ssh keys. You have been a great help and I appreciate it a lot. Thank you!
I can definitely understand wanting to use Windows for Matlab, Mathematica and the office suite. Windows was a go to for anything school/work related for a while, and some of the software I used was at times was Windows exclusive. I use to run dual boot Windows/Ubunutu, then ran linux full-time with a windows VM. WSL sounds much more resource efficient than a seperate VM or Dual Boot if you stay on the Windows side more. I am glad you found a method that works, those responses the others gave were incredibly thorough XD
If your readline is a relatively recent version, you can add `set show-mode-in-prompt on` to your .inputrc Will show a : for normal mode and a + for insert mode
This fails if `vi` is the first or last option, so I would suggest this variation: if [[ $SHELLOPTS =~ (^|:)vi(:|$) ]]; then ...; fi
That works to detect if vi-mode is active but the question was if it is possible to detect if the shell is in cmd vi-mode or ins vi-mode. My goal is to change my PS1 colors to match the one i've set for `vi-[cmd|ins]-mode-strong`.
https://www.reddit.com/r/bash/comments/7s7isg/detect_bash_vimode/dt35dw9/
I added a `:` character before and after the `$SHELLOPTS` if you look again. That makes it work if the word is at the beginning or end.
Oh, I didn’t notice that. Clever :)
Doesn't seem to be do-able unless you're on bash 4.3/4.4 or newer: https://stackoverflow.com/questions/1039713/different-bash-prompt-for-different-vi-editing-mode
You probably want to use something like sed instead
Since changing the color does not work due to PS1 not being reprinted after mode change, may I suggest another solution: Change the cursor shape. set editing-mode vi set keymap vi-command set show-mode-in-prompt on $if term=linux set vi-ins-mode-string \1\e[?2c\2 set vi-cmd-mode-string \1\e[?6c\2 $else set vi-ins-mode-string \1\e[6 q\2 set vi-cmd-mode-string \1\e[2 q\2 $endif
You may want to include more information or better describe what you are trying to accomplish exactly as it is definitely unclear. A question should be answered with more answers than questions. Like, are you wanting to execute any second word/command in the string? Or only instances of 'ls'? Or do you want to execute 'ls' on the last 'toto' and if so, is the last 'toto' a file or directory? http://www.catb.org/esr/faqs/smart-questions.html
Oh neat. That's a good alternative! Thanks. I didn't know we could use if statement in .inputrc. The more you learn :)
If your filenames contain spaces or other weird characters, many of these solutions will break. Assuming mostly GNU versions of these tools, something like grep -irlZ @linux | xargs -0 cp -t linux_notes {} is safer, since it makes grep output each filename followed by a null character (which is not allowed in filenames), then tells xargs that each input file is delimited the same way. In general, you need to think defensively when you might be dealing with arbitrary filenames.
What's wrong with the python solution? What problem are you trying to solve? The second bash one you listed just calls the python one you listed above, which is exactly how I would do it. I really don't think Bash is going to be able to do it on it's own. Most bash scripts just call external programs to do the lifting for it. If you really need bash to do it, you are going to find yourself looking up or writing your own libraries. I think this falls under things that Bash is just not very good at. http://mywiki.wooledge.org/BashWeaknesses 
I am using youtube-upload, it is quite straightforward: https://github.com/BruXy/time-lapse-utils/blob/master/yt-upload
This is one more CLI in bash to described [tokland's Python script](https://github.com/tokland/youtube-upload)
Isn't it just `Esc ?`, like in the ordinary Vi editor?
`man readline` lists &gt;"C-R" reverse-search-history in both *VI Insert Mode functions* and *VI Command Mode functions* in section **VI Mode bindings**. C-R is bound to reverse-search-history in vi mode by default, perhaps your configuration changes that.
Do you have any idea how to trigger this functionality ? 
You hold Control and hit the R key. What do you get as output of these commands: bind -m vi-command -q reverse-search-history bind -m vi-insert -q reverse-search-history 
`esc` to get to normal mode, then `/` does a search, like in normal vi. `n` to get to the next result, etc. And as in normal vi, you have `?` and `N` to go in the other direction.
Holly cow!!! this is amazing i wish i could give you 10 upvotes
thank you this solved my issue with C-r
There is no way to combine those two lines. You can only use one of those `${var#...}` and `${var%...}` features at a time, making it so that $TMP variable can't be avoided.
One less browser tab to have open definitely have to try it. Thank you.
Ha, thanks :)
Thank you. If, aside from code critique, you have any criticisms/insight on usefulness (like, how you think it would improve one's workflow), that's appreciated too! I will add your comment on compatibility to the readme!
Cool
Now you just have 100 terminal tabs open on top of your browser tabs
Its not very actively developed, so does not have the new twitter limit by default, https://github.com/ixt/tweet.sh has the modifications needed for that limit being lifted. But tbqh just using the curl-like command that twitter [provides](https://github.com/twitter/twurl/) is nicer once you get your head around the api 
I haven't had a real change to look at the code but one thing I noticed was that you use escape sequences for various output that create very unsavory effects in both my xfce terminal emulator and my xterm. [screenshot](https://i.imgur.com/DLPFo2R.png) My advice is just to print some plain old text all of the time. A lot of people don't want you modifying that stuff so much. Perhaps actually allot a --pretty option if you want but by default I don't think it should make my emulators wonky. Also, I think the --help and trigger terminology needs to be smoothed out. For example: # Note here that you use --slay as an example # but not as a trigger list. Along the same line, # the --summon option isn't mentioned at all [root@localhost baphomet]# ./baphomet --help | grep -i slay -k, --kill, Slay one or more daemons -ka, --kill-all Slay all daemons - Summon, slay, or join daemon named 'levi' $ `baphomet --start|--slay|--join levi` # --slay does not work like it shows in the help # because it's not configured, and the --summon # option (which wasn't mentioned) does work [root@localhost baphomet]# ./baphomet --summon levi ★ Summoning the daemon LEVI= ... done. So summoned. [root@localhost baphomet]# ./baphomet --slay levi Unknown argument "--slay" 
But `twurl` in written Ruby. Is it possible backport all features of it to your fork of `tweet.sh`? Btw, thanks for the fixed `tweet.sh` ;-)
A description of what that is about would be nice.
Thank for the explanation!
 local secsVar=$(TZ=GMT0 date +%S) local minsVar=$(TZ=GMT0 date +%M) [...] These five subsequent date invocations are a disastrous approach. The time changes between invocations, the final calculation can be off a second, a minute, even an entire year when executed in the wrong moment. IFS=: read -r y j h m s &lt; &lt;(date -u +%Y:%j:%H:%M:%S) The tests for perl, truss, uname, etc. seem weird considering how lightweight a single date invocation and the arithmetics are. Since version 4.2 bash's printf has the %(fmt)T format specifier, additionally removing the need for the external date invocation. TZ=UTC0 printf -v datestring '%(%Y:%j:%H:%M:%S)T' -1 You can ignore leading zeroes by forcing decimal base in arithmetic context with 10#$var, which should be preferred especially over removing them with sed.
 local secsVar=$(TZ=GMT0 date +%S) local minsVar=$(TZ=GMT0 date +%M) [...] These five subsequent date invocations are a disastrous approach. Time passes between invocations, the final calculation can be off a second, a minute, even an entire year. IFS=: read -r y j h m s &lt; &lt;(date -u +%Y:%j:%H:%M:%S) The tests for perl, truss, uname, etc. seem weird considering how lightweight a single date invocation plus arithmetics are. Since version 4.2 bash's printf has the %(...)T format specifier: TZ=UTC0 printf -v datestring '%(%Y:%j:%H:%M:%S)T' -1 Leading zeroes can be ignored by forcing decimal base with 10#$var in arithmetic expressions, which should be preferred especially over removing them with sed.
This is due to the following change in bash-4.3-beta (from [CHANGES](https://tiswww.case.edu/php/chet/bash/CHANGES)): &gt; f. Fixed several cases where `invisible' variables (variables with attributes but no values, which are technically unset) were treated incorrectly. There's also another backward-breaking change in bash-4.4: &gt; a. Using ${a[@]} or ${a[*]} with an array without any assigned elements when the nounset option is enabled no longer throws an unbound variable error. My advice is to not use `set -u` at all.
Thanks a million !!! Especially for including second backward-breaking change. 
Thank you. I started off by fixing missing '--slay' flag. So, by default, you recommend foregoing escape sequences? So, if I understand correctly, no color by default?
Add another variable to your `read` command in front of the `$title` one. You can use the name `_` with `read` for variables you later don't need, like this: read _ title author qty sold
It would've been simple if title was the last field. With this broken format, you'll need to count from the end, so read into an array, and iterate it backwards IFS=: while read -ra line; do sold=${line[-1]} qty=${line[-2]} price=${line[-3]} author=${line[-4]} title="${line[*]:0:${#line[@]}-4}" done &lt; file unset -v IFS If you can change the file format, move the title to the end, that way you can simply use `IFS=: read -r author price qty sold title`
Precisely. Or you can include something like a --pretty to use the escape sequences, but by default it should be playing text. After looking at the code a bit, I think you should reevaluate how you handle options a little bit, specifically how you handle -lvv -lv -l, etc. The way that is (baphomet -lvv) would make me assume that I can string together options and that adding -v increases verbosity, but you aren't parsing them that way. E.g. Somebody might try -kvv and it fails, or -vl My advice is to restructure it so that the single dash options are all one letter (E.g. -v increases verbosity by one, no -ls to replace --list), and then make it so you can add options all at once as -lvv or -vl or -v -l or whatever. As a reference to another command, you can use `ls -alrt` and `ls -ltra` and `ls -al -rt` and `ls -a -l -t -r` to acheive the same results because earch letter on a single dash options is parsed. 
probably, or at least the interesting ones. I dont really have the time to do it right now, but I can assure you that I very much want to if I can find the time. 
[removed]
Thanks. Some feedback: &gt;The tests for perl, truss, uname, etc. seem weird considering how lightweight a single date invocation plus arithmetics is. These were figured out prior to the discovery of the portable alternative and I have already removed them from my running version of the function. They are, nonetheless, interesting albeit hackish ways to achieve the goal. &gt;Since version 4.2 Maybe I'm missing something: $ echo $BASH_VERSION 4.4.12(1)-release $ printf -v datestring '%(%Y:%j:%H:%M:%S)T' $ Plus the key word here is "portable". If GNU `date` isn't around, we might not be near `bash` 4.2+ &gt;Leading zeroes can be ignored by forcing decimal base with 10#$var in arithmetic expressions, which should be preferred especially over removing them with sed. I know that. See keyword: Portable. 10#$var doesn't work everywhere. "Portable" might mean something different to you, but for me, having to support an ~~infuriating~~ exciting array of ~~ancient, legacy~~ world-leading, robust UNIX systems as well as Linux, "portable" means "vaguely `ksh` compatible" or `bash` 2.05 depending on the context. I think with that meaning of "portable" in mind, that `extglob` and `"${var##+(0)}"` is probably *the* internal method to use. So based on all of that, something terser might be along the lines of this: # Calculate how many seconds since epoch epoch() ( if date +%s | grep "^[0-9].*$" &gt;/dev/null 2&gt;&amp;1; then date +%s # Portable workaround based on http://www.etalabs.net/sh_tricks.html # We take extra steps to try to prevent accidental octal interpretation else local y j h m s yo if [ -n "${BASH_VERSION}" ]; then shopt -s extglob fi # shellcheck disable=SC2046 set -- $(date -u '+%Y %j %H %M %S') y="$1"; j="${2##+(0)}"; h="${3##+(0)}"; m="${4##+(0)}"; s="${5##+(0)}" yo=$(( y - 1600 )) y=$(( (yo * 365 + yo / 4 - yo / 100 + yo / 400 + j - 135140) * 86400 )) printf '%s\n' "$(( y + (h * 3600) + (m * 60) + s ))" fi ) I've had to throw in the `grep` in the first test because `date +%s` was returning a literal `%s` on a Solaris 9 test host /facepalm. I've also changed the function from a `{}` group to a `()` subshell to keep any change to `extglob` contained, and I was forced to use the `set --` technique to reconcile the fight between `bash` and `ksh` with respect to reading the output of `date` (`bash` was happy with `&lt; &lt;()`, `ksh` was not. `ksh` was happy with `cmd | read`, `bash` was not. FFS) Any further thoughts appreciated...
Do you have to do this very often? Automated scripts that delete stuff based on vague requirements give me the heebie jeebies. Have you ever tried ncdu? It’s really easy for finding large directories, it’s then really easy to navigate through a cached filetree using the arrow keys and just pressing ‘d’ (with confirmation of course) removes the selected folder or file. Just run `ncdu /home`, should be rather straight forward.
What code have you got and what have you tried so far?
&gt; My advice is to not use set -u at all. But... but... my unofficial strict mode! ^^/s
Its possible. Its also dangerous. There will likely be some negative fallout from such an approach. What is your code so far? If you don't have any code, can you at least write some pseudo code and give us that? No one wants to feel like they are doing someone homework for them. Show us where your having problems by at least making any attempt first. Use echo instead of RM -f so your test doesn't actually delete anything, but instead, becomes a dryrun for what it would delete. 
use `find`?
I have a valid TinyPNG API key... and I can utilize the API through regular curl calls, but would like to use this script to run batches of images. I've stared at the code too long - why am I getting an error?
We need some more info. Where are you running the script from What arguments are you giving the script What files exist in the directory you're passing as the first argument? Can you post the exact command you're running, and its output?
What do you get when you try running the following command: curl https://api.tinify.com/shrink --user api:&lt;TINYAPIKEY&gt; --data-binary "&lt;EXAMPLE_FILE&gt;" --dump-header /dev/stdout --silent Substitute appropriate values for &lt;TINIAPIKEY&gt; and &lt;EXAMPLE_FILE&gt;
**Leap second** A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) in order to keep its time of day close to the mean solar time, or UT1. Without such a correction, time reckoned by Earth's rotation drifts away from atomic time because of irregularities in the Earth's rate of rotation. Since this system of correction was implemented in 1972, 27 leap seconds have been inserted, the most recent on December 31, 2016 at 23:59:60 UTC. The UTC time standard, which is widely used for international timekeeping and as the reference for civil time in most countries, uses the international system (SI) definition of the second, based on atomic clocks. Like most time standards, UTC defines a grouping of seconds into minutes, hours, days, months, and years. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
**Leap second** A leap second is a one-second adjustment that is occasionally applied to Coordinated Universal Time (UTC) in order to keep its time of day close to the mean solar time, or UT1. Without such a correction, time reckoned by Earth's rotation drifts away from atomic time because of irregularities in the Earth's rate of rotation. Since this system of correction was implemented in 1972, 27 leap seconds have been inserted, the most recent on December 31, 2016 at 23:59:60 UTC. The UTC time standard, which is widely used for international timekeeping and as the reference for civil time in most countries, uses the international system (SI) definition of the second, based on atomic clocks. Like most time standards, UTC defines a grouping of seconds into minutes, hours, days, months, and years. *** **Unix time** Unix time (also known as POSIX time or epoch time) is a system for describing a point in time, defined as the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970, minus the number of leap seconds that have taken place since then. It is used widely in Unix-like and many other operating systems and file formats. Because the same timestamp can refer to two distinct instants of time around a leap second, it is neither a linear measure of time nor a true representation of UTC. Unix time may be checked on most Unix systems by typing date +%s on the command line. On systems where the representation of Unix time is as a signed 32-bit number, the representation will end after the completion of 2,147,483,647 (231 - 1) seconds from 00:00:00 1 January 1970, which will happen on 19 January, 2038 03:14:08 GMT. This is referred to as the "Year 2038 problem" where the 32-bit signed Unix time will overflow and will take the actual count to negative. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
You can use awk, sed, head, tail or other tools to splice the file to certain number of lines and pipe that to nl to get the required output. 
This will reset the counter when a line contains the pattern "enclosu": awk '/enclosu/ { a = 0; print; next } length($0) { printf "%6d ", ++a } { print }' inputfile Commentary: # The current line matches the regex "enclosu" /enclosu/ { a = 0 # Reset the counter to zero print # Print this matching line next # Continue with the next line } # For the exact nl behaviour to also count lines with only whitespace # use something like length($0) instead of NF as condition NF { printf "%6d ", ++a # Increment the counter and print it } # Finally, print the current input line { print }
Another approach, using awk as wrapper around actual nl, reopening a pipeline to nl when needed: awk '/enclosu/ { close("nl"); print; next } { print | "nl" }' inputfile Commentary: # If the current line contains "enclosu" /enclosu/ { close("nl") # Close a possibly existing pipe to nl print # Print the current line next # Do not process the rest of the script } { # Send the current line to an nl process, opening # the pipe if needed, reusing it from a former line # otherwise print | "nl" }
[Here's the exact script I'm running](https://gist.github.com/adamdehaven/cb11eb11d9df573bd0b03e872bd5812c) -- I've left out my API key on line 2. When running the script, let's say the script is located on my Desktop. I have a folder on the Desktop named 'imagetest' that contains 2 images (in this case, both are .jpg). Here's the command(s) I'm running: $ cd ~/Desktop $ ./tinypng.sh imagetest Here's the output I'm getting directly from the script: find: ‘imagetest_tiny’: No such file or directory Creating output dir "imagetest_tiny"... Optimizing images... curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information Gathering stats... Finished. Original directory (imagetest) has 2 files; total size 2.5M . Tinified directory (imagetest_tiny) has 0 files; total size 0 . The script creates a new directory "[original_folder_name]_tiny" successfully, but then throws the "curl: no URL specified!" error for each image in the directory.
Please run bash -x ./tinypng.sh imagetest Basically just add a bash -x in front
When I run with the --silent flag, I get no response (expected). If I remove that, I get another error: Warning: Failed to open C:/Program Files/Git/dev/stdout
That returned this: + TINYAPIKEY=[APIKEY] + '[' -z imagetest ']' + INDIR=imagetest + '[' '!' -d imagetest ']' + DIRNAME=imagetest + OUTDIRNAME=imagetest_tiny ++ pwd + OUTDIR=/c/Users/adehaven/Desktop/imagetest_tiny + find imagetest_tiny -mindepth 1 -print -quit + grep -q . find: ‘imagetest_tiny’: No such file or directory + '[' '!' -d imagetest_tiny ']' + echo 'Creating output dir "imagetest_tiny"...' Creating output dir "imagetest_tiny"... + mkdir imagetest_tiny + echo 'Optimizing images...' Optimizing images... + cd imagetest + shopt -s nullglob + for file in '*.png' '*.PNG' '*.jpg' '*.JPG' '*.jpeg' '*.JPEG' ++ curl https://api.tinify.com/shrink --user api:bDejp115hFzE-Ro198ufMJCiOMr8w0-4 --data-binary @image1.jpg --dump-header /dev/stdout --silent ++ grep Location ++ awk '{print $2 }' + Cfile= + Cfile= ++ echo -n '' ++ sed 's/.$//' + Cfile= + curl -o /c/Users/adehaven/Desktop/imagetest_tiny/image1.jpg --silent curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information + for file in '*.png' '*.PNG' '*.jpg' '*.JPG' '*.jpeg' '*.JPEG' ++ curl https://api.tinify.com/shrink --user api:bDejp115hFzE-Ro198ufMJCiOMr8w0-4 --data-binary @image2.jpg --dump-header /dev/stdout --silent ++ grep Location ++ awk '{print $2 }' + Cfile= + Cfile= ++ echo -n '' ++ sed 's/.$//' + Cfile= + curl -o /c/Users/adehaven/Desktop/imagetest_tiny/image2.jpg --silent curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information + echo 'Gathering stats...' Gathering stats... ++ du -h + INDIR_SIZE='2.5M .' ++ ls ++ wc -l + INDIR_FILE_COUNT=2 + cd /c/Users/adehaven/Desktop/imagetest_tiny ++ du -h + OUTDIR_SIZE='0 .' ++ ls ++ wc -l + OUTDIR_FILE_COUNT=0 + echo -e 'Finished.\r\nOriginal directory (imagetest) has 2 files; total size 2.5M .\r\nTinified directory (imagetest_tiny) has 0 files; total size 0 .' Finished. Original directory (imagetest) has 2 files; total size 2.5M . Tinified directory (imagetest_tiny) has 0 files; total size 0 .
What's with the dashes in the title?
I wonder that too 😂
Hello, Here is one way you can do it CMD="/path/to/cmd,there.sh" printf -v out '%s' "${CMD//,*/}"; printf '%s' ${out##\/*\/} 
Wow, this is very helpful. I'll report back once I make sense of it all!
There is a Firefox addon called "copy tab urls", which, when executed, copies the urls of all the open tabs to the clipboard, which I can then add or append a text file with using xclip. This is how I roll.
One option could be to use a timeout on `read` (`-t &lt;seconds&gt;` i believe), either for all of your input, or alternatively a shorter 'hit button to enter new tag' type deal
Correct URLs: [Script for preparing images for the Internet, etc.](https://discuss.pixls.us/t/script-for-preparing-images-for-the-internet-etc/6547), [on GitHub](https://github.com/Kakupakat/fotoprep).
The array expansion is wrong: cmd_create=(qemu-img create -f raw "${vm_path}/$2" "${size}") [...] "${cmd_create}" ;; `"${cmd_create}"` expands to the first (index 0) element of cmd_create only, which in your case is `qemu-img`, without the arguments. The proper syntax is `"${cmd_create[@]}"`. cmd_base_init=(${cmd_start[@]} -cdrom "$2" -boot order=d -drive file="${vm_path}/$3",format=raw) Here the double-quotes around the array expansion are missing, it should be: cmd_base_init=("${cmd_start[@]}" -cdrom "$2" -boot order=d -drive file="${vm_path}/$3",format=raw) I suggest you use functions instead of these arrays, though.
Thanks a lot, fixing those was the solution. Do you recommend functions instead of arrays because they are cleaner and/or more performant? In my script, I had `cmd_base_init` use the command `cmd_start` and add a couple more parameters to it. How would this look like if I were to use functions instead?
/tmp is a horrible place for anything that you want to retain access to..
Yes but also for for testing
one way to do $ perl -pe 's/"[^"]+"/ $&amp;=~m|".*\D.*"| ? "": $&amp; /ge' ip.txt XutXCTwwmYc,: "2" ,: 1.0 ,: 0.0 ,: "1" ,: 5 . * `"[^"]+"` to perform substitution on all double quoted strings * `e` modifier allows to use perl code in replacement section * ` $&amp;=~m|".*\D.*"|` check if the double quoted string has any non-digit character
Can you write a short example about how you use this function? I ask because I do not really understand why you need this function. I'm wondering if you might be misunderstanding something somewhere else.
working perfectly. good use of :alpha: . Thanks.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
I did put the script in a bash script file and tried to run it but gave an error: #!/bin/bash for name in $(cat users.txt) do find /home/$name/ -o -size +50M -mtime +180 -exec ls -lh {} \; done running this ./runme.sh gave this error: line 4: syntax error: unexpected end of file
To debug this, I'd recommend: function check_disk_space () { free=$(df -Hl --output=avail /dev/*da1 | awk 'FNR == 2 {print $1}' |sed 's/G//') echo "$free" if [ "$free" -le 60 ]; then printf "%s\n" "Not enough free disk space." fi } 
I'm embarrassed to say the fault lay with the user. Issue is resolved. Thanks!
see https://unix.stackexchange.com/questions/151850/why-doesnt-the-tilde-expand-inside-double-quotes tl:dr, `~` doesn't have special meaning inside quotes
But you can use $HOME instead of ~
What is your input? 
I remember reading somewhere that it’s safer to use `${HOME}` in bash scripts. That way you can also use it inside double quotes.
You are right about the empty I element. I didn't know there's such a useful feature bash -x for debugging. Thank you very much.
Something that might be interesting for what you seem to be doing where you have several arrays that you use in parallel: Bash can produce a list of the index numbers of an array like this: ${!array_name[@]} Using that, you could write your loop like this: for i in "${!price_array[@]}" do price=${price_array[i]} sold=${sold_array[i]} echo "scale=2;$price*$sold" | bc done That's interesting because arrays in bash are weird and can have holes in them. You can for example delete an entry with `unset array_name[i]`, and afterwards there will be an empty entry just for that particular index number, but the rest of the elements in the array still keep their original index numbers and don't move up.
Thanks. I have so far to go... This thread sub-reddit is incredibly helpful. 
Here are some quick notes, if you want: It seems to me that more could be abstracted into a function to reduce some of the repetition -- that is, instead of `write_header` I would probably make a function `write_section` that would take all the values. Something like write_section() { local name=$1; shift; printf "-----\n%s\n-----\n" "$name" printf "%s\n" "$@" printf "\n" } # and then call like write_section "OS X Name" "$marketing" write_section Memory "$ram" "$type" # etc Then, there's my biggest pet peeve: piping grep into awk or sed, since they all share so much functionality. Example: system_profiler SPHardwareDataType \ | grep --extended-regexp 'Processor Name: '\|'Processor Speed: ' \ | sed 's/^.*: //' # instead could be system_profiler SPHardwareDataType \ | awk '/Processor (Name|Speed):/ { sub(/^.*: /, ""); print; }' And I notice that you invoke `system_profiler` a bunch of different times for a bunch of different stats. With careful use of `awk` you could certainly minimize the amount of calls you need.
When there's time, I'll look at the suggestions and amend the program as warranted. It's so useful to get feedback like this. That's basically how I learned to script.
example usrname@host~$ this will start to over write my command if i keep going like so:: I keep going like @host~$ this will start to over write my command 
You made a mistake in your $PS1 prompt variable. Show what your $PS1 looks like to get help with fixing it.
I # it out to see if that changes anything but no luck- i tried 2 different terminals too to see if that made a difference. but i think you are right - trying to find that buffer difference - thanks fo the help!! #PS1='\[\033[1;36m\]\u\[\033[1;33m\]@${HOSTNAME%%.emit*}\e[\033[1;32m\]\w\[\033 [1;31m\]\$\[\033[0m\] ' #export CLICOLOR=true #shopt -s checkwinsize 
I think Reddit destroyed some characters in that line. You can make Reddit treat it as code when you add four space charactes at the front of the line.
 PS1='\[\033[1;36m\]\u\[\033[1;33m\]@${HOSTNAME%%.emmit*}\e[\033[1;32m\]\w\[\033[1;31m\]\$\[\033[0m\] ' does this look better?
BLAMO! that was it!! thanks so much. I was trying to figure out how to increase the command line buffer size like scroll back- thanks so much!
It's quite annoying to write a PS1 prompt as it's hard to read what's going on in there with all of those codes. To make things easier to work with, you could try to build it by first setting some variables for the colors like this: Blue='\[\e[01;34m\]' White='\[\e[01;37m\]' Yellow='\[\e[01;33m\]' Red='\[\e[01;31m\]' Green='\[\e[01;32m\]' Cyan='\[\e[01;36m\]' Reset='\[\e[00m\]' PS1=$Cyan'\u'$Yellow'@'${HOSTNAME%%.emmit*}$Green'\w'$Red'\$'$Reset' ' I guess that's still really hard to read, but at least with those color codes being hidden behind variable names there's hopefully less mistakes possible. Here's another thing I tried to make it easier to work with, but it's still really annoying to read: Blue='\[\e[01;34m\]' White='\[\e[01;37m\]' Yellow='\[\e[01;33m\]' Red='\[\e[01;31m\]' Green='\[\e[01;32m\]' Cyan='\[\e[01;36m\]' Reset='\[\e[00m\]' PS1= PS1+=$Cyan PS1+='\u' PS1+=$Yellow PS1+='@' PS1+="${HOSTNAME%%.emmit*}" PS1+=$Green PS1+='\w' PS1+=$Red PS1+='\$' PS1+=$Reset PS1+=' ' 
oh hey thanks! ill do some experimentation. thanks so much!
Look in the sudo man page, under Process model and Signal Handling it explains it some more there if I'm not mistaken. It does provide a way to override this through the security policy done through Sudoers and the sudo.comf file. *OP: look through the sudo.conf and sudoers manpage your solution should be resolved there. 
Oh... I just ejected this from my `.bashrc` because I never use it... I don't remember where I got it from either... it might help or give you a starting point? # Enable launching a function with sudo # Basically copies the function to a temporary file and launches it exesudo () { local _funcname_="$1" local params=( "$@" ) ## array containing all params passed here local tmpfile="/dev/shm/$RANDOM" ## temporary file local filecontent ## content of the temporary file local regex ## regular expression local func ## function source # Shift the first param (which is the name of the function) unset params[0] ## remove first element # params=( "${params[@]}" ) ## repack array content="#!/bin/bash\n\n" # Write the params array content="${content}params=(\n" regex="\s+" for param in "${params[@]}"; do # This breaks older versions of bash, sigh. #if [[ "$param" =~ $regex ]]; then if echo "${param}" | egrep "${regex}" &amp;&gt;/dev/null; then content="${content}\t\"${param}\"\n" else content="${content}\t${param}\n" fi done content="$content)\n" echo -e "$content" &gt; "$tmpfile" # Append the function source echo "#$( type "$_funcname_" )" &gt;&gt; "$tmpfile" # Append the call to the function echo -e "\n$_funcname_ \"\${params[@]}\"\n" &gt;&gt; "$tmpfile" # DONE: EXECUTE THE TEMPORARY FILE WITH SUDO sudo bash "$tmpfile" rm "$tmpfile" } `# This breaks older versions of bash, sigh.` Heh... that bit's me.
I can't see what formatting you used.. guessing you're missing either a newline or a semicolon. This works for me: #!/bin/bash for name in $(cat users.txt); do find /home/$name -size +50M -mtime +180 -exec ls -lh {} \; done 
Just a suggestion - if it's run so infrequently, and you need to make absolutely sure that nothing is unintentionally lost, could you use a script to generate a list like `to_delete.txt`, manually review it (or send it out for review), and then after you've verified it, run `for f in $(cat to_delete.txt); do rm -f $f; done`?
See [this](https://www.reddit.com/r/bash/comments/7tsb19/evaluating_the_multiline_output_of_a_function_as/dti7t82/) reply for example usage. Regarding using eval and $___ text - I think you are right in that these wont work, but wouldnt bash try and evaluate these as variable names regardless of if eval is used or not? At any rate, this seems fixable by adding a var=$(echo "$var" | sed -E s/'\$'/'\\\$'/) which, I think, should replace any literal '$' characters in $var with literal '\$' characters.
I think you're mistaken. Let's try out an example. Here's a script `foo.sh`: #!/bin/bash -x # subshells within subshels ( whoami ( echo still $USER ( # only root could create this file echo text &gt; /root/somefile ls /root/* ) ) ) Here's what happens if I run it as user `fedora`: $ ./foo.sh + whoami fedora + echo still fedora still fedora + echo text ./foo.sh: line 9: /root/somefile: Permission denied + ls /root/ ls: cannot open directory '/root/': Permission denied Here's what happens if I run it with sudo: $ sudo ./foo.sh + whoami root + echo still root still root + echo text + ls /root/ anaconda-ks.cfg original-ks.cfg somefile Not that all inner commands were run as root, and we we were able to see the contents of root's home directory. When you run a script as root, the $USER within that script *is* root.
Same. Counterexamples here: https://www.reddit.com/r/bash/comments/7u308y/can_su_be_used_to_get_root_access_without/dti8goh/
Nice. Played around with it a little bit and shortened it up: exesudo() { local func=$1; shift local file=`mktemp` echo "#$(type $func); $func $@" &gt; $file sudo bash $file rm -f $file }
Do you want to iterate over each token in the string and execute it if possible? This could do that: $ string="toto ls toto" $ for el in $string; do &gt; if hash $el &gt;/dev/null 2&gt;&amp;1; then &gt; $el &gt; fi &gt; done 
&gt; done thank you it worked, looks like it was a text formatting as you said.
My main reason for trying to avoid global variables is that Ive ended up writing quite a few bash functions (my ~/.bash_fnctions file is pushing 5000 lines and keeps growing), and I have a tendency to re-use variable names. I could probably get away with using them now and then, but having multiple functions with different variables that are global and share the same name is bound to cause problems sooner or later.
Ah. Have you considered a different language? Sounds like some pretty substantial stuff. Namespace scoping and builtin data structures being the advantages sounding most beneficial here. Python? Not to say that I don’t love me some Bash. Maybe you could put *everything* in functions and be sure to specify everything (mostly) as local? All-caps globals and lower case locals is a fairly common pattern. You can also declare functions within functions. You could write higher level wrappers around common patterns that could clear the global in a subshell before proceeding? That’s getting into incredibly hacky territory, but it’s not altogether crazy.
Bash does not make a distinction between unset arrays and empty arrays. For an empty array, it would be illogical for `"${arr[@]}"` to produce an error. That's why, as of bash 4.4, its behaviour under `set -u` is analogous to that of `"$@"` (and `"$*"`), which ([per POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_25_03)) also do not produce errors under `set -u` if the set of positional parameters is empty.
So it has to do with the '@' operator. And previous bash behavior was posix-wrong. Thansk for the info!
Well, not exactly, because arrays are not a feature specified by POSIX at all. But it was considered more logical that `"${arrray[@]}"` functions the same way as `"$@"` does.
Try an infinite shell loop with `sleep`. (300 seconds = 5 minutes) `while : ; do fping -gu` *first_ip* *last_ip*`; sleep 300; done` 
No error for me either. Just a blank echo line. 
Is that an intentional change in 4.4; is it safe to assume that it won't revert in a future version? I didn't see it in the release notes or NEWS.
I can't find it in NEWS either but it is in CWRU/changelog: 7/16 ---- subst.c - parameter_brace_expand: if ${array[@]} or ${array[*]} is supplied with an unset (or empty) array, and -u is enabled, don't make it an unbound variable error, like $@ or $* when there are no positional parameters. Based on report from Ian Allen &lt;idallen@idallen-fibe.dyndns.org&gt; 
`install.sh` is insufficient cp ./chrome.sh /usr/local/bin You might like to give the option between `/usr/local/bin` and `~/bin`, but you'd have to build in some smarts to test `$PATH`. This seems like it belongs in `~/bin` to me though. Ownership and permissions, though, are what you're missing. See `man install` (heh)
Interesting! I usually open files in my web browser like this: (on Cygwin) alias vivaldi='/cygdrive/C/Users/Me/Appdata/Local/Vivaldi/Application/vivaldi.exe' $ vivaldi file.html On Linux I usually will just use this: $ firefox file.html &amp; This is good for your first script. Have you looked into using other shells, such as zsh or fish? Also, tools like sed, grep, and awk are essential for me at least when it comes to writing custom commands or scripts. What OS are you using, and what kind of terminal emulator? 
In `install.sh`, don't use `cp`, use `install` -- that's what it's for. u/whetu got all of my comments about quoting and `[[` vs. `[`. The semicolon after the `open` is unnecessary. And in the error message, it looks you couldn't decide between "enter" and "specify" and decided to go with both. Also in that branch, you probably want to `exit 1` or otherwise report the error to whoever is calling you. (Bonus: redirect the error message to stderr instead of stdout.)
You're describing a different situation. I know the user is root, that wasn't the situation I was talking about since it would all be under the same process of foo. I was talking about the situation where a parent process would call, another child process. So foo.sh would create bar.sh but not hand off the privelleges upon the originals termination. I think a better example would be having foo.sh create bar.sh that would continue running, exiting foo.sh and then checking bar.sh privelleges upon exit. Or something similar. You may possibly be right, but that example was not the situation I was describing at all. I will take a look at the counter examples, I could always stand to learn more I am not a pro lol.
Comments are good for you! Best wishes on your journey. 
It's working. That's awesome. Thanks mate!
I tried that first. I originally thought it had something to do with new lines and according to a stack overflow post "that $(...) strips the trailing newline characters from the output of that cat command, and echo adds one back." I get no problems when I use: echo -e "y\nwww.google.com\nn\n" | python3 ./webcheck.py
Don't be me and right click save link for .txt files if formatting is important.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
thank you!
Is this a homework? If so, you should probably make a point of learning HOW it works, rather than just turning it in as your own. 
Hi megared17. You are right! This is part of it, which is suppose to help you with the next part of the assignments. We are trying to build are way into modifying sequencing data, so this is helping me understand at least how to integrate it into the other scripts. 
Just a quick question: Your script says x=0, and to echo that. Then you move on to saying while x(which equals 0) is less than the value you give for the second variable is true, perform x=$(($x+$1)) and then echo that. I know it works, but for the second argument, do you need to make x equal something else? can I change it to y=$(($x+$1)) and then echo $y? Probably stupid question, but I am a little confused to why using x again?
nvm, I got it. The while statement keeps it within the range of the second variable and adding y= would just make it go infinite. Sorry about that.
Ok. Here it is: #!/bin/bash /usr/bin/seq 0 $1 $2 The `seq` command does this for you already. Check the man page for more: $ man seq | head -10 SEQ(1) User Commands SEQ(1) NAME seq - print a sequence of numbers SYNOPSIS seq [OPTION]... LAST seq [OPTION]... FIRST LAST seq [OPTION]... FIRST INCREMENT LAST 
Bash doesn't expand `~` or `*` within quotes. See: $ echo ~ /home/fedora $ echo "~" ~
Here's more example to learn from: #!/bin/sh echo -n " X" for d in 1 2 3 4 5 6 7 8 9 10 11 12; do printf "%4d" $d done echo Y=1 while ( test $Y -le 12); do printf "%4d" $Y X=1 while ( test $X -le 12 ); do D=$(($X*$Y)) printf "%4d" $D X=$(($X+1)) done echo Y=$((Y+1)); done 
True, but learning how to actually code it is good exercise.
Agreed, it depends on the context. For school or learning? Doing it manually helps hone the “programming” muscle. In a professional setting or code that’s going to be used by other people? Keep it simple, and rely on existing time-hardened tools.
OP's later reply suggests this is in fact a school/learning situation. I would say that its good to learn how to code it by hand, *and* its also good to learn about pre-existing tools such as *seq* 
When you do `if ( test ... )`, those `()` do kind of nothing. You can write just `if test ...`. I just want to mention that because you might run into a situation at some point where things will behave strange. You might want to know about what those `()` really are because `()` does have a special behavior in bash compared to other programming languages. That feature is described like this somewhere in `man bash`: &gt; **(list)** &gt; list is executed in a subshell environment (see COMMAND EXECUTION ENVIRONMENT below). Variable assignments and builtin commands that affect the shell's environ‐ ment do not remain in effect after the command completes. The return status is the exit status of list. Here's an experiment about that on the command line: $ x=3; ( x=5; echo "inside (), x is $x"; ); echo "outside (), x is still $x" inside (), x is 5 outside (), x is still 3 A thing where `()` is useful in a real script is when you want to `cd` to some location to run a command, but want to go back to where you were previously after the command. Instead of doing something like: cd some-sub-folder some commands here cd .. You can do this: ( cd some-sub-folder some commands here )
Noted. I prefer the visual appearance the parens give. I understand about it running in a subshell. 
That `{start..end}` feature you tried to use sadly does not work with variables so you can't use it for this problem. It only works with numbers, like so: $ echo {0..10..2} 0 2 4 6 8 10 When you try to use a variable somewhere in it, it turns into text and doesn't do the counting anymore: $ x=2; echo {0..10..$x} {0..10..2} Here's different ways to write what you want in bash if you are interested: #!/bin/bash x=0 while [[ $x -le $2 ]]; do echo $x x=$(( x + $1 )) done another: #!/bin/bash x=0 while (( x &lt;= $2 )); do echo $x (( x += $1 )) done another: #!/bin/bash for (( x = 0; x &lt;= $2; x += $1 )); do echo $x done Those scripts should all behave exactly the same if you call them with correct arguments, but I think might do different error messages if the arguments are missing or not numbers. You can experiment with `$1` and `$2` at the command line and don't have to write a script every time you want to test something. You first set `$1` and such through `set --`, like this: $ set -- 5 20 $ echo $1 $2 5 20 You can now try to write that loop you want directly at the command line, like so: $ for (( x = 0; x &lt;= $2; x += $1 )); do echo $x; done 0 5 10 15 20 About where to put `;` to fit everything on a single line, you pretty much just have to remember that you shouldn't put a `;` after a `do` or `then` or `else`.
`$()` spawns a subshell and is known as command substitution. Totally wrong for what you want to do.
Well the more you know, we are 100% in agreement now. Thanks for the second example, I must've read that portion wrong. That is a very interesting feature, I'll have to take note of that. Thanks again, I appreciate the response!
Can you be more specific about what you want to do? Typically, if you just want to do something with each line, the answer is a while loop, like my_cmd | while read line; do # do something with "$line" here done But if there's some specific processing you want to do (counting, modifying, whatever) there may be a specific program to pipe into (`sort` or `grep` or `awk`). In terms of data structures, there's not really a good one for line-based data. I'd put the output in a tempfile if you really need to store it, but again, it's be nicer to just pipe it toward whatever you want to do.
It looks like this follows the same pattern each time, so a cab should suffice.
 Each text block represent a cluster of server ie: #####cluster1###### the string and int values provide health details about the individual cluster for example min_required_server 2 server1 0 server2 0 server3 0 server4 1 I want to be able to evaluate each text block and return it back with any comparisons I make in the evaluation. For example if the min numer of required servers is not met, return the same block of text with an error banner. #####cluster1###### ERROR
This sounds like something that would best by handled by Ansible, or at least in Python or a similar language that supports more sophisticated text parsing and data structures than bash. But if you *really* want to do it in bash, that's the sort of thing I wrote https://github.com/xiongchiamiov/lineify to deal with.
The problem is likely the `do` at the start of each case. You don't need those. Tip: if you're just checking on the return status of a command, you can toss it right into the if: if ping -c 1 8.8.8.8; then echo ok else echo nope fi Also, according to `help select`, select will break from its loop after EOF, and it will automatically retry if you enter an option that wasn't among the choices, so you probably don't need the Q or * cases.
Thank you so much!!! :) One last thing, so the way it works now I have to Ctrl-C to kill it. Is there a way to make it so that if I hit say like 'q' that will end it?
It is typically ctrl-D to send eof.
Ah okay. Thanks for the help!
He has a very solid point. I would still like to see a solution for this just for educational purposes.
Sheeeeeit, well, uhh, I don't want my computer to lock and sleep while I'm home. Any help with that?
How to *store* the data? You can parse your data and print it as csv or json, then when you want to work with it you use tools like `cut` for csv and `jshon` for json to interpret the data, along with bash operators like `[[` for string, boolean, and integer comparisons. I personally suggest you json for it. You just have to find the way to convert the data into it. Your examples would look like this: { "cluster1": { "min_required_server": 3, "is_imperative": true, "refer": "Paul", "server1": 0, "server2": 1, "server3": 2, "server4": 1 }, "cluster2": { "min_required_server": 2, "is_imperative": false, "refer": "Johnny", "server1": 3, "server2": 2, "server3": 2, "server4": 2 } } Search online for more education in JSON (JavaScript Object Notation).
So typing Q doesn't activate the break because with `select` you have to type in the number (in this case 4). See this [tutorial](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_06.html) for an explanation, you only need to read the first few sentences. To make it respond to letters you'll need a `while` loop. You can put a `trap` at the top if you want to handle Ctrl-C. Also not sure if you're in `zsh` but for `bash` you need a space after `[` and before `]` in your `if` statements. Happy bashing. #!/bin/bash trap 'echo; echo "Detected Ctrl-C pressed"; exit 130;' SIGINT PS3='Please select a test: ' options=("1" "2" "3" "Q") echo "Select an option: " i=1 for o in "${options[@]}"; do echo "$((i++))) $o" done echo -n "$PS3" while read sel; do case "$sel" in "1") echo "Test1";; "2") echo "Test2";; "3") echo "Test3";; Q) echo "Exiting" #This just breaks the while, put #exit 0 here to quit the program break;; *) echo "invalid option";; esac echo -n "$PS3" done #Code down here would still run 
&gt; uuoc Lol, googled it. Never seen that before. I concur with you method too. 
Use `awk` with an empty Record Separator (RS). This separates records (a bunch of fields) by blocks of text separated by an empty line. If you also set the Field Separator (FS) to newline, each line in the block of text is accessible with `$n` (where n is a number). With the default FS, fields are separated by whitespace. So to make a more useful example file with a name field: `a.txt` #######string####### a b c d stringv 77 stringw name bill stringx 99 #######string####### 123 abc name bob You can print the header (field 1) and the name (check for field "name" and add one): awk 'BEGIN{RS=""} { print $1; i=1; while (i&lt;NF) { if ($i == "name") {print $(i+1)}; i++ } }' a.txt 
FYI this is literally the first bash script I've every written. 
The changed variable dies with the new shell. You either want to * source your script (`source myscript` instead of just `myscript`) * write it as a function (`myfunc() { PATH=$PWD:$PATH; }`) that you can put in `~/.bashrc`. [BashFAQ 60](http://mywiki.wooledge.org/BashFAQ/060) explains it more in detail.
thanks that worked. I did '. scriptname' and it worked
thanks for this. it makes sense from a security perspective. I'm assuming that Linux would look through the various paths in order so it would go to your local folder first if I added it to the ~/.bashrc The script is for a specific folder where I have some python scripts so it will only be used when I am doing that work. Not planning to edit the ~/.bashrc file Just trying these little things to try and understand Linux a bit better. Learning through failure and all that. I could have just typed the command every time but the process of trying to write this 2 line script has already taught me a couple of things.
No worries! Linux being as awesome as it is, I can't really imagine the use case you have here but I can pass on some of the things I've learned. Linux will "layer" your scripts for you in Shells and Subshells. If you run a script, it creates a little sandbox for that script to play in. Any changes made to anything visible globally are only visible to that shell. You can demo this by creating a variable in one terminal and echo it out in another (spoiler: It's not set). What this means is that if you run a script that changes a global environment variable (like `PATH`), that change is only visible to that script. If you want to run more of a "setup/configuration" kinda vibe that alters the state of one of these variables, `source` is the way to go. `source` essentially runs a script in the **same** shell. As an example, `source .bashrc` just reloads your `.bashrc` file. `source &lt;your_script_here&gt;` will pull the changes your script makes into the current terminal session **only**. If you're looking for a shorter way to `cd DIR;./some_python_script.py` then consider an [alias](http://www.linfo.org/alias.html) of your script. I don't know what the execution looks like but it could just point to `source script.sh` if that's all it needs to do.
re:I can't really imagine the use case you have here I'm currently going through this &gt; https://youtu.be/UlbwZ5Uqf0o
This is really cool ! I don't have access to install ruby, but after reading your readme it gave me an idea. "The standard Unix toolset is line-based. This is great when you data is structured as lines, but not so much otherwise." Rather than try to work around how unix processes text I think I just need to play by the rules. So what i generated by stdout in my script I did the following to make it one giant line. Then used ifs to split the line into separate lines which i can store into an array to iterate through. single_line_of_data=$(master_function | xargs) master_array=() OIFS=$IFS IFS='#######' read -ra master_array &lt;&lt;&lt; "$info_line" for i in "${master_array[@]}"; do echo "$i" done IFS=$OIFS
I have to think about the compression side of it, but I just want to say, don't do this thing: cp $(ls -d1 ./** | egrep ".tar.ext$") $outdir Parsing `ls` in that was is bad. Don't parse `ls`. It will break and hurt you. This case is exactly what shell globbing is for: cp *.tar.ext -t "$outdir" or `find` if you need to go into subfolders: find . -name "*.tar.ext" -type f -exec cp {} -t "$outdir" + More: https://mywiki.wooledge.org/ParsingLs
Is there a reason the compressed files can't be tarred and compressed again? Are they pathological kinds that get bigger when you compress them? Otherwise just recompressing might be easier than trying to special-case all this stuff.
I use zip for things like this, it has an index, crc checksums, excluding of files, is portable and stores uid/gid (but double check since this was a long standing bug in most versions). This is only for some small backups, i usually rsync to an offsite host each day but the zip is just for smaller stuff. In the past i used afio, i'm not sure if that supports excluding compressed files but it's getting very old and maybe unsupported. I really wish there was a portable free solution which had an index for easy access/updates, checksums, permissions etc storage and a recovery record. So far zip has been the closest match for me. I agree that anything to make tar work like such an archiver is probably too much trouble.
Important caveat: https://i.imgur.com/XopAhbB.png I can't repro, but maybe your quoting is weird somewhere.
I don't get it. I think this should work.
You could experiment with the faster compression methods and choose one that's fast enough that you don't feel annoyed about compressing everything again all the time. There's a new program named "zstd" that can kind of replace and compete with all the other programs by itself just through changing the compression level parameter. It can scale from hundreds of MB/s compression speed to competing with xz or 7za. It will beat gzip's file size while being faster than a HDD can save a stream of data. You could see what compression level can still do 100MB/s or so on your machine, then use that. Another thing, check out a program named "borg": https://github.com/borgbackup/borg From looking at its description, I think it might be able to do exactly what you ask for, except with its own format and not tar.
First you have to find where the drive is mounted. This will usually be in the "/mnt" or "/media" folder. Then you should "ls" to see what directories are there. If it's in /mnt you should probably see the directory for the drive immediately, and you would then "cd" into that directory and proceed forward. If it's in a /media folder you will probably see a folder named after your user. You would "cd" into that and then "ls" again, and then you should see the folder for the drive. "cd" into that and then continue forward.
You're escaping the &lt;'s but not the &gt;'s. 
As he said, look for the mount point, starting from the root ("/"). Especially, UNIX-like environments (Linux, mac OS, Android...), unlike DOS environments (Windows), follow a file system based in a root with childs ("/") rather than what in Windows is called a "drive", like "C:\", or so. Your OS mounts the attached drive to a folder inside your HDD (or SSD)'s root folder, which is "/". In Mac, for example, your external disks are all mounted to "/Volumes/", but the real disk is located at "/dev/" as "/dev/disk1s1", or so. Correct me if I'm wrong. I'm just an amateur.
using df will also list where the mount points are. That might help you find them as some distros they arent in media or mnt 
Cool! Glad you figured something out.
Also, FYI, you can use $PWD instead of $(pwd) which saves a process exec when it fires up the pwd command. That variable and many more can be read about if you do a 'man bash' and see the section 'Shell Variables'
Are you saying I should first use `(/)` followed by `ls` to list all my directories on my computer ?
Are you on a Mac? If so, Brew install gnu-sed and use that. 
&gt; if you need to go into subfolders This was my main reason for using the approach i did, since im 99% sure that cp **.tar.ext -t "$outdir" doesnt work. Ill consider using the 'find' method in the future though. Admittedly, I got a bit lazy when I typed out the command. The times Ive actually done this I also have added in the '--quoting-style=literal' flag, which I think addresses almost all the ls parsing issues (and any remaining ones, like weird responses for null names, seem like they would be avoided with the grep call). This is all assuming it works correctly and actually gives you the literal filenames, which is seems to on ubuntu 17.10, which is what Im using atm. Im not saying it is better than the 'find' method (in particular for portability to older systems) but at least on an up-to-date install a lot of those parsing issues seem entirely avoidable.
Speed mainly. Most of the time unless im compressing *really* big files I use 7za with rather aggressive options: compression level of 9, increased memory usage (up to ~6.8 gb max usage), solid archive enabled, LZMA2, etc. (this is all automated by a function). The result is really good compression, but throughput of only about 6 MB/s, which is about 3 minutes per gigabyte (it was probably closer to 10 MB/s before the meltdown patch, but my CU is Ivy Bridge and the patch really is screwing with it, particularly for I/O related tasks).
I'm using Arch Linux
Unix is not Windows. Drives aren't letters, and every drive is listed in /dev in the root directory (the beginning of the drive that the system is installed on, it's in "/"). Let's say you mounted a USB drive. In Windows it's probably in d:\, but in a Unix system (Linux, macOS etc) it's probably in /dev/sdb. But **you can't just use** cd /dev/sdb, **because the drive has to be mounted somewhere**. It's mounted in some directory in root. In my case, it's mounted in /run/media/username/driveUUID. The easiest way to know the full path to your mounted drive is to open your file manager, go to your drive, then open a terminal in this directory (most of the file managers support that option), and then type in **pwd**(**p**rint **w**orking **d**irectory). This will show you the path to the mount point of this drive. Alternatively, if you know that your drive is /dev/sdb or /dev/sdc (/dev/sdx where x is another drive), you can just open up terminal, and type in **df**. When you type in df, you'll have to find your desired drive (df will show you a list of drives), and when you find it you'll also know it's mount point (the mount point is shown right next to your drive). When you know your drive's mount point you can just do cd *mountpoint* If you have more questions, just ask.
&gt; Important caveat: https://i.imgur.com/XopAhbB.png Not that tired old chestnut. OP isn't parsing HTML, his command is equivalent to `grep`.
OP, please provide sample input to save time for those who want to help.
The OP says it currently works. That would mean the site would have to change the HTML before the scraper breaks. By the same token, if the site owner removes a HTML element, or CSS id or class, the OP will not get what he wants either using a DOM parser. ikewise
Fair enough.
&gt; `sed: can't read (and then every word separated by spaces)` What is every word? I have a feeling that the command you showed us is not the exact command you ran. 
 Use ``lsblk`` to see the mount point **Example** $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 232,9G 0 disk └─sda1 8:1 0 232,9G 0 part /media/Hitachi sdb 8:16 0 149,1G 0 disk ├─sdb1 8:17 0 500M 0 part └─sdb2 8:18 0 148,6G 0 part sdc 8:32 0 931,5G 0 disk ├─sdc1 8:33 0 831,5G 0 part /media/WD1TB ├─sdc2 8:34 0 40G 0 part / ├─sdc3 8:35 0 1K 0 part └─sdc5 8:37 0 60G 0 part /home Then ``cd &lt;mount point&gt;`` **Example** ``cd /media/Hitachi``
I still get the errors, I've used double quotes and single quotes. It's not working.
I agree with LukeShu in the comments. It doesn't look like you've posted the exact command, because what you wrote to him indicates you have quoting troubles before the file name.
No. cd / #this changes directory to / the root directory. ls #this lists the files in your current working directory which would be root since you changed to it above. 
Very good! 
Thanks a lot :-) I have updated the bash cheatsheet. For the second part, I added "statements" between "begin" and "end" to explain that people can put whatever they want inside this block.
There's one major error, the command to create and edit files is actually vim ;) 
I get that, but for x := 1 to 10 do begin echo $x end does not print 1 to 10, because `for x := 1 to 10` is not valid bash.
Haha, I have just updated the cheatsheet to make you happy (I also use Vim :-))
You're right, I have just updated with: for run in {1..10} do statements done
- Ctrl x: has a typo completefions - touch: maybe precise what you mean by 'update' - cat: you example is right but most people use cat to dump the content of a file on stdout - kill: doesn't necessarily 'ends' the process, sigstop for instance halts the process, waiting for a sigcont to resume, makes me think you didn't mention ctrl-z in you shorcuts - array declaration shouldn't have spaces between = - you don't mention in flow control that -n, -z, etc are part of the test builtin (or [) - your cd website example is missing a dollar - in the end you use the 'function funcname' construct to create a function whereas you used funcname() above, this is confusing Otherwise this is nice :)
You can also: for ((v=0;v&lt;10;v++)); do echo "${x}" done With a result of numbers 0 to 9. Pretty good if your arrays start at zero.
This won't work on anything other than Mac OS. 
I see a series of `filesystems` when doing `df` although I'm still not clear which one is in *windows* terms the `C` drive to see a list of all the directories ?
Nice job, although all the file commands aren't part of Bash
You're right, I added the file commands in this cheatsheet to have them at hand.
Have you tried a plain `while read line`? I don't see the need for redefining IFS
Is that the same as encoding? Will check that out, thanks very much!
It's different from encoding. Here's an explanation of what you're looking at here: https://blog.codinghorror.com/the-great-newline-schism/
'preciate that! *learning intensifies*
The equivalent to Windows' C: drive is the drive that's being mounted on "/" (root). You should see that with the df command.
I like it! Only wished it also contained a list of regex examples.
A suggestion for the tips, bash has pushd and popd which use a stack concept to store paths for later retrieval. Way quicker than creating variables. Nice cheatsheat, there's a couple of things I knew for years but had forgotten about. Precious milliseconds will be saved! 
I also decided that it wasn't necessary to check a disk. As long as the directory has adequate space, the larger program being worked on will run acceptably. 
Thanks a lot, I have updated everything. What do you mean by your "cd website" example is missing a dollar?
Right before the "debugging" part, you're exporting a "websites" variable, and after that you do "cd websites". But exporting won't expand the variable for you so this won't work, you'll need to do "cd $websites"
Use command substitution `$()`: $ cat multiply.txt 5*10 $ echo $(( $( cat multiply.txt ) )) 50 $ 
Remember the suitable quotation for a safer (or clearer) experience.
In case you have integer expression on the each line this works: echo -e "5*10\n10*10\n333*9" | xargs -Iexpr bash -c 'eval echo expr=$((expr))'
I am a fan of bc when handling floating points: ex.) $ echo "scale=23; 5/6" | bc -l .83333333333333333333333
I forgot to specify I only wanted to use echo and cat. Correct answer right here. Thank you.
I'm kind of curious: why? 
Homework assignment.
Or, without the UUOC: $ echo $(( $(&lt;multiply.txt) )) 50 Of course this does assume that the input file only has one line and that it's in a format that `$(())` can cater for...
I think this is a job for sed. Not that I know how to use sed, but this sounds like a job for it.
You can check out the source of the page by running this at the command line: curl -s 'your-url-here' As you will see, there is no "greater things lie ahead" anywhere in the source you get. You can try to search for parts of it like so: curl -s 'your-url-here' | grep -i greater curl -s 'your-url-here' | grep -i things I think this is a situation where the view that you see in the web browser is created by a Javascript program building it locally on your computer inside the web browser. This means you need a full web browser to build the page to extract things from, and it's not possible at the command line because none of the command line web tools have a Javascript interpreter in them. If this is something you really need to be able to do, one thing you could look into is test frameworks for web developers. Search for "headless web browser".
Maybe wget the page and build a series of bash scripts to run through the resulting HTML. 
Awk not sed 
`curl` and `grep` no? I'll look into this if you still need help.
Yeah, just what ropid said, if the content you want is not in the original HTML, the most probable thing is that it is built from JavaScript, and you'll need a JavaScript interpreter in your commands. Your web browser is a mix of HTML, CSS, and JavaScript.^(with extensions, complements, et cetera). What you get from doing `curl` is a HTML page (the architecture of the web page). Somewhere in the document, there are links to some CSS code to make your browser render a nice layout (the paint, design, and styling of the web page). And there are also spaces in the document with JavaScript code on them (the functional part of the web page, that lets the user interact with the web browser and the server). And so, to get to the CSS and JS, your browser makes some extra downloads and processing. You can search in your HTML document for the *source* of that content that you want, and try to get it yourself from the command line. The other option is to use userscripts (written in JavaScript) (TamperMonkey, GreaseMonkey, et cetera) to let the code you write interact with your browser's page. Oh, and also, if you get to fetch an HTML document with what you want, you can parse it with tools like `xmllint` and `xmlstarlet`, which are made mainly for XML, but that work well with HTML. You'll need to learn something about XPath to tell the utilities where is what you want. In a browser, in my case Safari, when you open the developer inspector, you can right-click where the specific line is and select "Copy XPath". You'll maybe understand.
`ping` has a `-q` option, but it doesn't make it completely silent. You could redirect its output to /dev/null: if ping -c 1 8.8.8.8 &gt;/dev/null; then echo ok; fi As another tip, instead of assigning all those variables at the top, you can use a heredoc (https://mywiki.wooledge.org/HereDocument): cat &lt;&lt;END ----------------------------------------------------- Connectivity Test Script Enter 1, 2, or 3 to perform one of the tests To end the program enter Ctrl-D (1) Test local connectivity (2) Test remote connectivity (3) Test remote connectivity using a host name or URL ----------------------------------------------------- END
`ifconfig` doesn't output your default gateway. You can use the `ip route get` command to find the route to 1.0.0.0 which will give you the default gateway. This will output the default gateway: ``` ip -4 -o route get 1 | awk '{print $3}' ``` To get the interface: ``` ip -4 -o route get 1 | awk '{print $5}' ``` Also you should ping at least twice as the first ping may timeout if an ARP request is needed to find the destination IP.
How did you add the first cron job you listed? Crontab is a command to edit a user's cron job table. No idea what linux distribution the container is running, but crontab is provided by the cronie packge on my Fedora 27 laptop. As for how to add parameters, I'm not sure what the issue is here. The instructions give you the syntax. It appears that the first cron job for cron.sh is an entry in /etc/crontab while the two entries for the advanced command are for the user www-data's cron table.
philosoft, thank you for your reply "It appears that the first cron job for cron.sh is an entry in /etc/crontab" Yes, you are right. It's in /etc/cron.d/magento "the two entries for the advanced command are for the user www-data's cron table" Would you mind telling how do i convert from the the user www-data's cron table style to /etc/cron.d/ style? I am newbie in this and understand nothing what those -e -u, etc mean
It looks like you wrote a lowercase I instead of an uppercase I. This has happened 798 times on Reddit since the launch of this bot.
or `cut -d' ' -f3`
For the crontab command, -u specifies the user (the default is the user who is executing the command) and -e to edit the cron table. As for the syntax, just follow the syntax listed in the instructions but add the user to the beginning of it: * * * * * www-data /bin/bash /var/www/html/scheduler_cron.sh --mode always * * * * * www-data /bin/bash /var/www/html/scheduler_cron.sh --mode default 
For the crontab command, -u specifies the user (the default is the user who is executing the command) and -e to edit the cron table. As for the syntax, just follow the syntax listed in the instructions but add the user to the beginning of it: * * * * * www-data /bin/bash '/var/www/html/scheduler_cron.sh --mode always' * * * * * www-data /bin/bash '/var/www/html/scheduler_cron.sh --mode default' 
For the crontab command, -u specifies the user (the default is the user who is executing the command) and -e to edit the cron table. As for the syntax, just follow the syntax listed in the instructions but add the user to the beginning of it: * * * * * www-data /bin/bash '/var/www/html/scheduler_cron.sh --mode always' * * * * * www-data /bin/bash '/var/www/html/scheduler_cron.sh --mode default' 
"/bin/bash^M: bad interpreter: No such file or directory" https://puu.sh/zhv5S/cc1317212e.png
Did you by any chance edit or open scheduler_cron.sh on a Windows machine? Looks like you have Windows CR/LF line endings. Edit the file in vi and save it in UNIX file format: vi /var/www/html/scheduler_cron.sh Change the format to Linux/Unix: set ff=unix Save the file: wq! 
philosoft, thank you SOOOOO MUCH! I was fighting this issue for 3 days! You are the man!!! I really appreciate!!! What a great people are in this sub! It works from the run command now, i did opened it in windows before, that's what was causing problems. Last question do i need those " ' " ? * * * * * www-data /bin/bash '/var/www/html/scheduler_cron.sh --mode always' Or * * * * * www-data /bin/bash /var/www/html/scheduler_cron.sh --mode always What's the right format?
thank you mate, you were very helpful!
This is just a little post I've written about a few of the weird/cool bash features I've stumbled across. 
i've seen a lot of posts like this, but this one definitely has generated the most wtfs. good job.
Definitely good stuff, though I don't like using undocumented features. Undocumented features usage leads to other people asking, "it works, but why?" Overall good content. Keep it coming.
Yeah, I agree. I just found some of these so weird I had to write about them.
Oh, for sure. I'm definitely going to start incorporating printf's time features.
Put the code between dollar-sign brackets `$( )` to capture the output. ping -c2 "$( ip -4 -o route get 1 | awk '{print $3}' )" 
Re #2: `extdebug` doesn't enter into it. Remove the `shopt` commands and it will still work. Try it. Read the bash manual under `BASH_ARGV` to understand why (tl;dr: it's based on hte execution call stack, so the final parameter come first). Re #3: only in bash 4.x (so not in bash 3.2 on macOS). Re #4: also: `for ((i=0; i&lt;=10; i++)) { echo $i; }` Re #5: very convoluted. The substitution `"${1//[[:space:]]/ }"` actually changes all whitespace (spaces, tabs, newlines) to spaces. In any case, a better way to do it is: trim() { set -f set -- $* # trim REPLY=$* set +f } The trimmed value is left in the `REPLY` variable. Note that this function ( as well as yours) assumes the value of `IFS` was not changed from the default, and that globbing/pathname expansion is globally enabled. Re #6: this one is plain **wrong**. First, it's not piping but redirection. Second, this is nothing special: you're creating a file called `:` and storing your output in that. You'll see it show up in `ls`. 
Huh, TIL. Thanks for writing this! 2. I just tested and this doesn't seem to work without the `shopt` commands. 5. Awesome, I didn't think it was possible that way. 6. Damn, my bad. Thanks for clearing this up.
That did the trick, thank you so much! 
Sorry your post is very poorly formatted, but the occurrence of letters can be found for example like this: sed -e 's/./\L&amp;\n/g' /usr/share/dict/words | sort | uniq -c
Your'e (a..z) should be {a..z} and you're missing some semi-colons if you want to do it as a oneliner: for alph in {a..z}; do echo $(grep -o $alphabet dictionary.txt |wc -l); done
Or, "it used to work, but doesnt now, why?"
That one's definitely [documented](https://www.gnu.org/software/bash/manual/bash.html#index-printf), though. Maybe you were aware of that, just pointing it out.
I was referring to the undocumented loop notation.
I don't think the language for number 2 is correct. You say &gt; When `extdebug` is on and a function receives arguments their order is reversed. but the argument array `$@` is not reversed. `BASH_ARGV` contains the arguments in reversed order and it's only set in extended debug mode, see the [manual entry](https://www.gnu.org/software/bash/manual/bash.html#index-BASH_005fARGV).
Yeah, that's one's definitely weird :)
That's a good point actually, I'll update the post. Thanks!
your english sucks ass. work on it. This DOESN'T work, ARE there tricks, and 'seems like to be impossible' is just plain wrong. Advice, not advices, is the proper plural. You don't need the echo at all for letter in {a..z} do grep -o $letter file | wc -l done
Kde
Sorry, I have no experience with kde. Consideting only generic tools, have you tried wmctrl? I am specifically curious about the output of `wmctrl -d`. Does it show double of the number of your desktops (or, generally, number of screens multiplied by number of desktops)?
Thinking about why that `\ls` example works, I tried to use quotes and that seems to work as well, for example writing `'ls'`.
Wow I love #3 and I didn't know most the others either. I have a statistics script that ran date like 5 times per iteration so it's now much improved in efficieny.
Thanks for your reply, I used your shorter version and it worked only in the shell but not in the script. When i execute the script, i got 26 zero. Here is the debug on the letter y : https://i.imgur.com/3RhG8PK.png 
this command outputs just 4 desktops with a resolution each of 3840x1080 so It recognizes them as one sine desktop. But all programs still behave normal so there must be a subdivision
I think the .profile script will only get run once when you log in. This would then mean that you have to log out and log back in after you install something that needs stuff getting applied through .profile. Did you do this after you installed ruby? The .bashrc file will get run every time you open a new terminal window. I don't really know for sure how things work because I use a setup where .profile does basically nothing except source .bashrc, and I then put all environment stuff into .bashrc. Your scripts are already kind of set up to be used like that if you want to try that as well. To make that work without issue, the .bashrc file is split into a first half for non-interactive stuff, and a second half for interactive stuff like aliases and the prompt. That logical split is defined through a line similar to what you do here in your .bashrc at the beginning: # If not running interactively, don't do anything case $- in *i*) ;; *) return;; esac The "non-interactive half" of the script just means to put customization of PATH and such in front of that section. That would be a place for things that normally should go into .profile. Because the .bashrc script will have run more than once at the point where you open a terminal window, I use these two functions that make sure things are not added multiple times to PATH: path_prepend() { for arg in "$@"; do if [[ ":${PATH}:" != *":${arg}:"* ]]; then export PATH="${arg}${PATH:+":$PATH"}" fi done } path_append() { for arg in "$@"; do if [[ ":${PATH}:" != *":${arg}:"* ]]; then export PATH="${PATH:+"$PATH:"}${arg}" fi done } They are used like this: path_prepend /usr/lib/ccache/bin path_append /opt/cuda/bin $HOME/bin
Ok. It seems to be KDE specific issue. Take a look at [this](https://ubuntuforums.org/archive/index.php/t-888979.html), [this](https://www.reddit.com/r/kde/comments/4cvwpd/having_issues_with_multiple_monitors_and_getting/), and [this](https://forum.kde.org/viewtopic.php?t=21026).
I'm not sure without seeing the whole script and dictionary file. Are you sure you spelled the name of the file right in the script (dico.txt)? And that you're executing in the same folder as the dictionary file?
chmod a+x insert script here.sh 
Nice tips. Will stop using date now for timestamp generation, and maybe start using the for without the do done when I'm parsing some output interactively. Thanks!
thanks I tried some solutions but none of them worked for me. I guess I have to wait until the devs will implement this again since it was used in 3.5 I guess
You can use `shuf` for i in $(shuf -i 1-5); do echo $i; done https://www.systutorials.com/docs/linux/man/1-shuf/
You should point out that the `for` loop is entirely unnecessary and in fact will impact performance at any scale $ shuf -i 1-5 2 5 3 4 1 
What are you really wanting to do? Because this is possible, even without external tools, but it's really the wrong way to go about it...
Maybe for the result you are after, letting the 'for' loop keep its order and then changing the result would work as well. You could get your example result like this, by shuffling the output of the loop: for i in $(seq 1 5); do echo $i; done | shuf The same with line-breaks: for i in $(seq 1 5); do echo $i done | shuf
I have no practical reason to do this. I am just playing around with writing some scripts and wondered if it was possible. Can you please show me the right path to achieve this? In my example, $(seq 1 5) was just a placeholder, I want to be able to do this with predefined args, such as for i in "hello there general kenobi"; do echo $i; done or arrays, such as for i in ${array[@]}; do echo $i; done Any advice is appreciated.
It looks like you wrote a lowercase I instead of an uppercase I. This has happened 6090 times on Reddit since the launch of this bot.
So I don't wish to poopoo your comment BUT it's not terribly helpful without some context for the drive by people (like me) who'd like to know WHY you choose what you do not just what. 
There's many ways to do this... Here is Perl way. for i in {1..5}; do echo $i;done |perl -MList::Util=shuffle -e 'print shuffle(&lt;STDIN&gt;);'
For most people, `shuf` is the right way to do this. If it's available. You can either pipe directly to it (here's a quick and dirty example): $ printf '%s\n' hello there general kenobi | shuf kenobi general hello there Or use it to shuffle an array: $ obiwan=( hello there general kenobi ) $ shuf -e "${obiwan[@]}" general hello there kenobi If you're on something that doesn't have `shuf` and you're not able/willing to install it (i.e. corporate or technical reasons), then there's still a wide array of approaches that are still available to you. `jot`, `perl`, `sed`, `python`, `ruby`, various implementations of `awk` will all be able to shuffle something for you (or contribute towards shuffling something) with varying degrees of performance. If you did insist on doing this a more shell-native way, you could follow the random-index-and-sort approach: for element in hello there general kenobi; do printf '%s\n' "${RANDOM} ${element}" done | sort | cut -d ' ' -f2- This prints each element with a random number (ignore the subsequent example differences): 19345 hello 7319 there 14836 general 21801 kenobi Then it gets dumped into `sort`, which straightens out the random numbers, thus shuffling the lines: 10493 kenobi 15787 hello 16516 there 8507 general Then we get rid of the random numbers: general kenobi there hello This is fine for small inputs, but at any grown up scale, this method can be pretty awful in terms of performance. It's also not the best approach if you wanted to do something like, say, get 10 random words from a dictionary - you have to shuffle the whole thing and then cut off ten lines, which is grossly inefficient. A better approach, would be something like a knuth/fisher/yates shuffle, which is perfectly do-able in shell but still has its limitations. `shuf`'s big advantage over most other approaches is that it has reservoir sampling, which means it can cater for massively big files or sub-sampling inputs of unknown size etc
I mainly posted for the author - maybe suggestions to do something different. But point made. :) Threw some comments in the script, not sure if they'll actually be helpful.
Man, there are better ways to criticize someone's English.
Your going to need to give a little more info than that buddy 
https://www.youtube.com/watch?v=JbnjusltDHk
Perfect.
im just trying to accomplish creating a bash script. Doesnt matter the length what commands go in, just any bash script
What have you got so far?
What have you got so far?
 echo "This is a bash script. The end." HTH. HAND. 
just really basic stuff like echo commands and sudo apt-get sl just basic shit like that
Ok, can you post the script exactly as you've written it?
no thanks fork bomb
clear echo 1; echo 1; $test = ipconfig (testing connectivity to default gateway) rm rf / (removes files or a certain directory rm -rf ls -l (shows directory contents) mkdir linux rm rf /linux mkdir class cd /class (change directory) pwd class (displays name of working directorly. touch scripting less scripting rf /scripting one of my classmates helped me up untill this point. Just need little tidbits to add
&gt; echo "can you guys help me make a bash script?"
Those are commands, but they're not a script yet. Your scripting curriculum should have outlined the two things that you need to do in order to turn your commands into a script. 1. There's a specific line that needs to go at the very top of your script for the shell to recognize that it's executing a script. This should be in your textbook. 2. There are specific permissions that you need to apply to your script so that you can run it. Those permissions should be in your textbook as well. Now, moving on to the "help" your classmate has given you, some of what you have there is downright dangerous. Let's start with usage of `rm` rm -rf / The 'r' stands for "recursive". It will travel down the directory tree, deleting everything from where you tell it to start. In this case, you're telling `rm` to start deleting everything at the root directory, which will destroy your system. The 'f' stands for "force", which means `rm` will never ask before deleting a file. Any time someone gives you a command, you should read the command's man page. `man rm` will tell you all about what each switch does. $test = ipconfig (testing connectivity to default gateway) The way you're trying to set the variable `test` is wrong. Google "how to set a variable bash". `ipconfig` is the Windows tool, not the Linux tool. `ifconfig` is the Linux tool, and it's already deprecated, but we won't talk about that right now. mkdir linux rm rf /linux This won't work either, because you have two commands on the same line without telling the shell that one command has ended, and now it needs to run a new command. You've also told it to delete the directory "linux" which is directly under "/". `mkdir linux` creates the directory "linux" in the directory you're in right at that very moment, which is probably not "/". Read up on the `rmdir` command. `rm -rf` is almost never the right way to delete something. If you haven't already, get [VirtualBox](https://www.virtualbox.org/) and install Linux in a VM so that you can play around with your scripting, and not damage anything when you screw up. I would recommend starting with one command that you KNOW works. Add more commands later once you're comfortable with it.
`ifconfig` is deprecated. Use `ip` instead. Something like the following will get you the default gateway. ip route show | awk '/default/ {print $4}'
If you feel adventurous, you may consider changing your WM to something else (not Kwin).
Yeah, I realized. The only issue is the benefits of it are gone using a pipe. See: black ~ &gt; time echo hi &gt;/dev/null real 0m0.000s user 0m0.000s sys 0m0.000s black ~ &gt; time echo hi |: real 0m0.002s user 0m0.001s sys 0m0.002s 
ahh thanks, hadnt considered the overhead. and it is indeed quite noticeable if you do it enough.
Please note that I'm not affiliated with this project. I saw it yesterday on hackernews. https://news.ycombinator.com/item?id=16297294
You are right in that they are similar. I like how you do a disk space check before and after for confirmation.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
 #!/bin/bash # Suppress pid lock files pointing at zombies processes on different hosts # $1 is the location of the lock files # The lock file structure is "host pid" for i in ls $1/lock* 2&gt; /dev/null; do [ sed 's/\(\w.*\) \(\w.*\)/root@\1 "ps --no-heading --pid \2"/' $i | xargs ssh | wc -l == 0 ] &amp;&amp; rm $i || echo &gt; /dev/null; done;
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/functionalprogramming] [Functional Programming in Bash](https://www.reddit.com/r/functionalprogramming/comments/7vw9jw/functional_programming_in_bash/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Side-effects everywhere. Please do not call this functional.
Aside from the [obligatory reference](https://xkcd.com/927/), I'm interested in cheeking this out.
The only undisputed standard is POSIX. You do know this, right? Also, this project is only a library, so it doesn't require any changes to existing programs
Oleg wrote about this connection [in his blog](http://okmij.org/ftp/Computation/monadic-shell.html). Gabriel Gonzalez addressed this topic in the [pipes tutorial](https://hackage.haskell.org/package/pipes-4.3.7/docs/Pipes-Tutorial.html). Definitely interesting!
 P=█\ ░▒▓;while echo -n ${P:$RANDOM%5:1};do :;done #BruXy 
&gt; I'm interested in ~~cheeking~~ checking this out [Oh, okay](https://imgur.com/a/DgSke).
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/fbgyGNt.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dtw12dc) 
Thanks for your info, which makes sense. Why not thinking that bash scripting can be functional and build this bridge with *nix admins ?
There are some ideas in this direction, e.g. [the turtle library](https://hackage.haskell.org/package/turtle-1.5.1/docs/Turtle-Tutorial.html).
Here you go: ``` #!/bin/bash echo "Hello this is my littel script" ```
Did someone say functional programming in bash? Check this out: https://github.com/minond/exercises/blob/master/bash/functional.sh
Yes ! This is so good.
You want us to do it for you? Becouse I dont think this will happen.
For one, since `pwgen` isnt native to macOS, use `LC_CTYPE=C tr -dc 'A-Za-z0-9_-' &lt; /dev/urandom | head -c 11` to generate a random youtube video id
be quicker to get yourself a youtube data api key and verify the video id that way as opposed to checking if the url exists.
what is prompting the script to stop? just the user? in that case, a while true loop would be fine. while true; do if [[ $(curl -s --head -w %{http_code} https://www.youtube.com/watch?v=$(LC_CTYPE tr -dc 'A-Za-z_-' &lt; /dev/urandom | head -c 11) -o /dev/null) = 200 ]]; then &lt;rest of your stuff&gt; fi done
Indeed here are the timing prospectives, the best is seq of course: $ export NULL=/dev/null $ export MAX=1000000 $ time range 1 $MAX &gt; $NULL;time eval "printf -- '%d\n' {1..$MAX}" &gt; $NULL;time seq 1 $MAX &gt; $NULL real 0m24.269s user 0m22.701s sys 0m1.564s real 0m3.104s user 0m2.781s sys 0m0.322s real 0m0.319s user 0m0.316s sys 0m0.002s $
You want to delete the movie @ the end? Why?
i will try the script geoff. thanks for the help that was very nice of you
yes the user is prompting the script to stop. or otherwise a call in c++ via openframeworks
this is more then enough for me to get started
because the installation will be all about exploiting internet as a chaotic medium, for grabbing material as a source to make an installation
so should be something like this? ``` while true; do video_id=$(LC_CTYPE=C tr -dc 'A-Za-z0-9_-' &lt; /dev/urandom | head -c 11) if [[ $(curl -s --head -w %{http_code} https://www.youtube.com/watch?v=$video_id -o /dev/null) = 200 ]]; then youtube-dl -t mov youtube-dl https://www.youtube.com/watch?v=$video_id fi done ```
``` :() { printf '%s\n' "$@" } take() { sed "${1}q" } # or take() { head -n "$1" # conflicts with definition of "head" as "take 1" } not() { [ $? -ne 0 ] } odd() { not $(even $1) # confusing--"not" doesn't take an argument at all even $1 not # wait, that's using state! How about just ! even $1 } 
Oh yeah, not could be much shorter. Also I like that printf use case. I had no idea I could do that in that way
What is “:” usually for?? And actually I may just want to choose a different language all together!! Hahaha this is more for joke than practice.
`:` is an old command that has been sucked up into various shells as a builtin. It's effectively a terse alias for the `true` command $ type : : is a shell builtin $ help : :: : Null command. No effect; the command does nothing. Exit Status: Always succeeds. $ help true true: true Return a successful result. Exit Status: Always succeeds. So you might usually see it used as a no-op or for infinite loops e.g. `while :; do`
&gt; continuously checks, via pwgen, if randomly generated youtube URLs retrieve valid URLs The chance of finding existing YouTube videos by randomly generating URLs is going to be utterly negligible (otherwise, this would be an easy way to discover unlisted videos).
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You can try to follow [example i3wm config](https://www.reddit.com/r/unixporn/comments/64mihc/i3_kde_plasma_a_match_made_in_heaven/) once you have enough free time ;)
you should export your path: export PATH="$(pwd):$PATH"
So `if [ -e file ]; ` is true if that file exists in the current directory. You need to search (run `find`) for the file. I suggest something like this find / .name "${userfile}*" &gt;myfilelist.txt 2&gt;finderrs.txt Now the file myfilelist.txt will contain absolute paths of the files that match `${userfile}*`. The file finderrs.txt will have all the stuff complaining about permissions and other errors (you could also redirect this to /dev/null). If all you want is to see if how many files with this name exist, you can run `wc -l &lt;myfilelist.txt`. To do some operation on each file you can do while read -r line; do echo $line #or some chmod operation you want done &lt; myfilelist.txt 
As usual, this gets the very basics wrong. &gt;The basic syntax of an if … then statement is like this: &gt; &gt; if &lt;condition&gt;; then &gt; &lt;commands&gt; &gt; fi *False*. The basic syntax is like this: The basic syntax of an if … then statement is like this: if &lt;commands&gt;; then &lt;commands&gt; fi There is no difference between the first list of commands and the second list of commands. The `if` statement executes the second list of commands if and only if the last command from the first list exits successfully (i.e. with status 0). Yes, `[`, `[[`, etc. are just commands. They're not "conditions" or "expressions". Their syntax has only been designed to make them masquerade as such -- which is a really evil design feature that I hate, because it misleads people as it did the author of this article. 
Git supports adding modified files in the commit command with the `-a` option (it won't add untracked files though). There's a reason committing and pushing are separate commands, I wouldn't recommend using this in collaborative projects. 
The condition is a command. Saying it’s a command and not a condition is kinda like saying an `if` statement in C is a bunch of expressions, not a condition, the expressions will eventually resolve to `0` or `not 0`, and the inside of the `if` will only be executed on `not 0`. In bash it’s the same just with commands not expressions
It would almost certainly be easier to do this in a language other then just BASH. Python/ruby/etc will all have libraries that make accessing the Google APIs far easier. If you do this in bash, you are basically just going to be calling curl constantly with lots of pretty complex requests. Anyway, here is a starting point for the Youtube API. - https://developers.google.com/youtube/v3/getting-started 
I think the problem is ranger. The filepath it returns is in the format I mentioned before. I managed to get a screenshot of the mpv window as it opens and closes very quickly. https://imgur.com/a/Sd4as
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/su5uUAq.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20du0kwwc) 
You may want to use Python instead. I think it'd be a lot less work since you can import YouTube handling libraries that already exist. Bash is great for a lot of things but when there's already an API to use it's hard to beat Python for scripting. 
/nas/ sounds a remote filesystem that gets mounted via the network? If so, it might not be mounted yet when rc.local is run.
Yes, a NAS. :) I've even put `sleep 300` in rc.local and forced the script to wait that long before it tries to check for the file. Makes no difference, plus I've checked manually that the NAS is mounted before the 300 seconds is up anyway.
Good call, but this is actually only one example of where I need to check for a file before running something in rc.local That said, I'm more than happy to use something other than rc.local if there are other, more recommended options? (Ignore nginx for now, it's just an example of where I'm using this)
What about systemd ConditionPathExists? You can also tell systemd to wait for your NAS volume to be mounted.
Look at youtube-dl if you want a quick way to wrap around in bash, but I would recommend python if you plan to scrap content.
/etc/rc.local #!/bin/sh -e ____________________________ ls -l /bin/sh lrwxrwxrwx 1 root root 4 Apr 24 2015 /bin/sh -&gt; dash
youtube-dl is python. https://github.com/rg3/youtube-dl
date should be a function if you want it to _actually run_ when `$(date)` is called Also bash has a variable $USER, or you can use the whoami command.
[[ is a bash-specific command. Depending how you launch your script, you might be using sh. [[ is not valid POSIX she; use [ instead.
Absolutely proprietary.
So like /u/Erelde said, if you define currentTime at the beginning, that's the value of that variable until it's changed. You can either call `$(date +fmt)` in your mv command, or redefine currentTime right before your `cd &amp;&amp; mv`. Looking at your script, have you considered using `rsync` instead of `tar`? Here's [tutorial](https://nickjanetakis.com/blog/automatic-offline-file-backups-with-bash-and-rsync) on using rsync for backups in bash. 
Seconded the choice of `rsync`. It's the tool for the job.
Thanks I interchange them so much I forget Bash and Python are different. Thanks for your help. 
Great idea. I forget about Python sometimes. Thanks for the reminder. I'll check out that API. 
Thank you for the detailed reply mTesseracted. I didn't know we had such a useful program built in! I am having a little trouble with the script though, when running it is telling me "sed: -e expression #1, char 8: unterminated `s' command\". Judging from your description of how sed functions, I don't see any issues with the script you gave so I am confused once more. Again thanks for taking the time to help a noob out with this =]
Thank you /u/mTesseracted but I do not want to use Rsync. It's not the best tool for what I'm trying to accomplish. I do have several Rsync scripts I've written over the years though and it is a fantastic tool!
screenshot?
* I'm not sure how creating a function for currentTime would help me. Please elaborate. * I'm aware of the built-in variables. I used them in the script. * Shellcheck.net is the best tool I've found. Been using it for years. I have ran this script through [shellcheck](https://imgur.com/a/9Zdyn) and if you do it too you will see it's only problem is which how I'm finding the current user. But this method is how the engineers at Apple and all of use over at JAMF Nation have found to be the most reliable.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/7Wnparg.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20du1fr2x) 
if you defined currentTime as follows currentTime() { date "+%Y%m%d-%H%M%S" } Then in your `mv` command you can call `$(currentTime)` instead of `${currentTime}` and it will execute the function instead of getting the value of that variable. 
You can hide the shellcheck warning message with a comment line like this: # shellcheck disable=SC2012 currentUser=$(ls -l /dev/console | cut -d " " -f 4) This should only suppress the message for one line, not for the rest of the script, so I hope there's no downside to doing this. I think it's good to do this now so that when you come back to edit the script in a year and have forgotten about what was going on there, you won't waste time researching about shellcheck complaining. On Linux you usually get details of a file with the 'stat' command, but it seems to work very different with Apple's OS X so I can't test it. On OS X it should be the following to get the user name from what I could find online: stat -f '%Su' /dev/console
Show us the part of your .bashrc that does the oh-my-bash stuff. 
I updated the question. I think the entire bashrc file was modified so I pasted the entire thing in [termbin](http://termbin.com/bm79).
Well the only relevant part is `source $OSH/oh-my-bash.sh`. Looking at their source code it just calls a plugins source code so it's just a rabbit hole of codes to follow. How about installing the programs it's complaining about?
Ah ha! I see what you mean now. Thank you. I'll edit my script and do some more testing. I was a Mac admin for several years and left that job to take on a role with a local MSP and had to reteach myself Windows (also introduction to Powershell). I focused heavily on Windows for the last year and a half and have since left that job. Now I'm trying to get my BASH fingers working again and simple things like this just slip my mind. *bangs head on desk*
[Apply directly to the forehead!](https://www.youtube.com/watch?v=f_SwD7RveNE)
Final script updated in the original post. Thank you for the help!
If it's because of a certain plugin it's probably not working anyways so you could test them one at a time to find the offender and remove it. To find the actual offending line of code you could narrow it down by running the oh-my-bash.sh script with the `-x` option to try to find it. Then you could redirect that commands stderr output to /dev/null to turn it off. 
Alright, thank you!
Solved!
&gt; To find the actual offending line of code you could narrow it down by running the oh-my-bash.sh script with the -x option to try to find it. Or, you could just use `grep -R` on whatever directory the oh-my-bash stuff is installed in Or, you could just search the repo and see that it appears to be in the completion stuff: https://github.com/ohmybash/oh-my-bash/blob/e65c390bfa35b20653aa470fdf6eeda082e0c0e1/completion/awscli.completion.sh https://github.com/ohmybash/oh-my-bash/blob/e65c390bfa35b20653aa470fdf6eeda082e0c0e1/completion/conda.completion.sh https://github.com/ohmybash/oh-my-bash/blob/e65c390bfa35b20653aa470fdf6eeda082e0c0e1/completion/jungle.completion.sh https://github.com/ohmybash/oh-my-bash/blob/e65c390bfa35b20653aa470fdf6eeda082e0c0e1/completion/kontena.completion.sh 
Same thing happens with if/then/else. Also, the script, at one point, contained 2 lines only. The shebang and the file test. Nothing else. Still never returned true.
[removed]
Looks like you're not alone: https://github.com/ohmybash/oh-my-bash/issues/2
Yeah I was looking around and saw that earlier. It doesn't look like development is very active. There are lots of wiki links but no wiki pages either lol. 
what did you try and how did it fail? see also: https://stackoverflow.com/questions/7680504/sed-substitution-with-bash-variables
For exactly your example, it could be done like this: sed -r 's/^(port_range = ).*/\1'"$PORT-$PORT"'/' filename The following would make it keep working if the number of space characters around the "port_range" word and the "=" would change: sed -r 's/^(\s*port_range\s*=\s*).*/\1'"$PORT-$PORT"'/' filename To edit the file, you change it into `sed -i -r ...`.
Thank you very much!
I think this would be a clearer version of the second requirement: sed '/^\s*port_range\s*=/ s/[[:digit:]]\+/'"$PORT"'/g' filename This separates the matching from the replacement and should also preserve, for instance, comments at the end of the line.
http://lmgtfy.com/?q=bash+substring
Effing hell I got it to work! Having no real knowledge of scripting I had no clue what I was looking for. Thank you for pointing me in the right direction mTesseracted.
The github script expects a hyphen to delimit track number and title, but you have a space. So, you can just change the hyphen to a space in these two lines: metaflac --preserve-modtime --set-tag=TRACKNUMBER=${file%%-*} "${file}"; TITLE="${file#*-}"; Specifically, change `${file%%-*}` to `${file%% *}` and `${file#*-}` to `${file#* }`. This also handles the case where the track number is not exactly two digits, in case you have any of those.
Ah ok, I changed ## to :3 in TITLE="${file##}" Thank you for your reply, I've sorted it so tags are done when I rip my cd's. I may come back to it should I buy any double cd's. 
feel free to implement this as a tool to find unlisted videos, and upload it to github. might be useful for someone
I had all my media in one big folder called 0-0, so I wrote this script to move all my media to their respective Letter designated folder instead, Boondock Saints ends up in B/, Abba on Tour ends up in A/ and The Matrix ends up in M/, evry item starting with numbers ends up in 0-9/ Might be broken now, but you get the idea.
might want to swap out say "RES : blah" with echo or something.
I improved on this a bit as almost all these operations can be done in ways built in to bash. This version should be dozens of times faster. https://pastebin.ca/3971999
You are a god to me! That is amazing..i have to learn bash better
You're welcome, that script was a good start, I might use it myself sometime :) See the bash manual under [parameter expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html) for info on the mechanisms I used.
That's not a daemon. It means "execute as a command the result of the mono command". You probably want: /usr/bin/mono /opt/foo/bar.exe -nogui &gt;log 2&gt;err &amp;
It's not working...I just want to make the process in the background because I've to keep my SSH connection alive to keep this server running. Another idea ? :)
man 1 tmux
&gt; man 1 tmux Thanks that worked !
Why single `[` test? The new `[[` test can be used alone and newbies don't get confused.
`jq` can do more than one thing at once. curl -s http://moviedata | jq -r " .movies[0:$total] | .info.original_title, .identifiers.imdb, .info.rating.imdb[0], .info.year" | while read title; do read imdb read rating read year printf "%s - %s (%d) [%.1f]\n" "$title" "$imdb" "$year" "$rating" done
Here's some experiments at the command line trying to research how that might work in bash: $ echo $(( 0345 + 0456 )) 531 $ echo $(( 345 + 456 )) 801 $ x=0345; y=0456; echo $(( x + y )) 531 $ x=345; y=456; echo $(( x + y )) 801 I guess you just have to make sure to write your numbers into a variable with a zero text character at the front.
Okay I'll try giving this a shot. 
I see that tmux was already offered as a solution, but this is super easy to do with systemd as well For example, here is a unit file that will run the script `my_script.sh` as a service under the user `some_username` [Unit] Description=My shell script service After=syslog.target [Service] User=some_username ExecStart=/path/to/my_script.sh Restart=always WorkingDirectory=/home/some_username [Install] WantedBy=multi-user.target save this as `my_script.service`under /etc/systemd/system. Then issue couple of commands systemctl daemon-reload#
Depending on your target system this is easy with a systemd unit file. Set Type=simple and have the ExecStart be your command. You can also set WorkingDirectory to the value of your cd command and your Environment to all those vars. https://www.freedesktop.org/software/systemd/man/systemd.unit.html
Not a Linux guru, but I've had problems with getting things to load at the right time myself, and sleep was of no help. I think the kernel tries to force a certain ordering, and sleeping just delays the whole chain. What did help was moving it to another place such as /etc/profile. Another thing you can try though is to log the current contents of /var/log/boot.log into a separate file and see if the system has logged that the network drive is loaded at the time that rc.local was called.
Hey I have another one: https://github.com/LolHens/functional.sh/blob/master/functional.sh It even comes with its own little test suite
it is not limited to Bash but short Bash scripts are perfect for this tool. Get you favourite scripts integrated into the GNOME desktop.
This is dope af. Too bad I hate the GNOME desktop. The one thing that really kills it for me is not being able to Alt+Space then hit c or x to close or maximize a window. Thinking about learning their keyboard shortcuts now that Ubuntu has switched over though. 
It sort of depends what you mean by "generate." If you just want to print them out, something like vals=(foo bar baz) printf "INSERT INTO test_status (col) VALUES ('%s');\n" "${vals[@]}" or if you want the queries in a variable, use `printf -v`, or change IFS or something, like IFS=$'\t' read -a queries &lt;&lt;&lt;"$(printf "INSERT ... ('%s')\t" "${vals[@]}")" but it sort of depends what you want to do with those queries in the end.
Does this work with budgie?
I have no idea.
Yes, should've explain better. I have an array, which has random number of elements every time the script is running. Array is smth like: vals=(foo1 foo2 foo3) So what I need to do is get a file with queries, with every element from this array. In other words one query for one element: echo 'INSERT INTO tests_status (col) VALUES ('vals[@]')' &gt; query.sql In the end I want to have a file which would contain echo 'INSERT INTO tests_status (col) VALUES ('foo1')' echo 'INSERT INTO tests_status (col) VALUES ('foo2')' echo 'INSERT INTO tests_status (col) VALUES ('foo3')'
You can go through an array like this: for x in "${vals[@]}"; do echo "print something using $x here" done Each element of the array will be inside that `$x` variable in this example here. You can then redirect what's printed by the loop by using a `&gt; filename` at the end, like so: for ...; do ... done &gt; filename Or if you have more stuff being done in front or after the loop that you want to redirect as well, you can surround that block of code by `{` and `}` and then use `&gt; filename` on that block, like so: { ... ... ... } &gt; filename
So, I found the prompt here: export PROMPT_COMMAND='if [ "$(id -u)" -ne 0 ]; then echo -e "$(date +%FT%T%Z)\t$(pwd)\t$(history 1)" &gt;&gt; ~/log/bash/bash-history-$(date +%F).log; fi' As I said I'm really a novice. But I think this is the culprit. Any chance you could interpret it for me and possibly give a solution? Thanks!
I'm thinking if you create that directory mentioned in the error message, things will start to work without error. I experimented like this: $ echo hello &gt;&gt; testdir/testfile bash: testdir/testfile: No such file or directory $ mkdir testdir $ echo hello &gt;&gt; testdir/testfile $ cat testdir/testfile hello And that's the same error message you get. Concretely, run the following while logged in under your user account: mkdir -p ~/log/bash/ The messages should immediately stop showing up.
Oh my word. That worked. Thanks so much! It was driving me crazy.
If we want to go functional, we should be consistant with this paradigm, for example parameters of function does not have multiple meanings and could be curryed, but in Range(), $1 is either an option "-i", or a start number. FP is a state of mind that would enforce a specific way of coding, avoiding duplication, useless variables, indentation, block statements... Which leans toward the one-liner paradigm... One function = one line.
I don't use git, but I'm trying to re-write it in my own style just to do it. I noticed that there are 8 options, but you check for up to 9 below. I suspect that you have not polished this script and some of the options are off by one.
wouldn't just learning git be less work?
It's a bit easier than typing everything when you don't have a GUI. I have scripts for svn for the same reason. I'm doing more complex things though, like listing all versions that are not merged from branch A to branch B.
&gt; system_profiler SPHardwareDataType \ &gt; | awk '/Processor (Name|Speed):/ { sub(/^.*: /, ""); print; }' Having had the time to look at this, I think it is a great idea. I was wondering if there's a way to dictate the way in which the output is presented? For example, in this case `(Name|Speed)' and '(Speed|Name) both return: `Intel Core i5 1.4 GHz. ` I'd like the processor speed to appear before the processor name, but I haven't been able to figure out how to so. 
I disagree. First, there are always a surprising amount of inconsistencies in programming, languages, and styles spanning languages and environments. Even in FP langs/libs. I’m not advocating for this, you just said it so I’m bringing this up. Second, $1 as a number or a flag is just a bashism, and sticking to it is smart given the environment we’re in, which is still bash. As for your definition of FP, well, what you’re describing is great but it is a good procedural programming style just as much as FP. Rather than this, FP is unique in its pure, declarative expression that make for composable code.
Changing the glob to `/volume1/tmp/*[!-]2018*` should do the trick, as long as there's only one 2018 in each filename, and that it's not at the very start of the filename.
thank you! can you just briefly explain what the change does? is the '!-' a 'not -' ? thanks again.
Yes, `[abc]` matches an `a`, `b` or `c`. `[!abc]` matches any character except `a`, `b` and `c`. So `[!-]` matches any character except a `-`.
great, thanks again!
Hmm I keep getting this error: &gt;mv: cannot stat ‘/volume1/tmp/*[!-]2018’: No such file or directory do I need quotes somewhere? thanks.
Thanks, is this better than the original basspodder?
This could work (untested): #!/bin/bash for file in *.jpeg; do dir=${file::3} mkdir -p -- "$dir" &amp;&amp; mv -- "$file" "$dir" done
I assume that I can change the "3" to decide the amount pf characters are read. I'll test this, I just need assurance that it can't affect anything that isn't in the current or nested directories.
I just tried doing this with the "perl-rename" tool, and it seems to work and will create directories if they don't exist: perl-rename 's/^(...)/$1\/$1/ if -f' *.jpeg This needs the "perl-rename" tool. I've also seen the tool named "prename" or just "rename" (but there's also another tool that has that "rename" name). You can add a `-n` parameter at the front to make it just print what it will do without actually moving files: $ perl-rename -n 's/^(...)/$1\/$1/ if -f' * ajshbg -&gt; ajs/ajshbg akjshgahgb -&gt; akj/akjshgahgb alhjgdgagjhsg -&gt; alh/alhjgdgagjhsg asdjhgajshgb -&gt; asd/asdjhgajshgb asjkdbhjhgabs -&gt; asj/asjkdbhjhgabs jhlabg -&gt; jhl/jhlabg 
Your command (in this case ffmpeg) also reads from stdin and therefore takes away some bytes from the input. Run ffmpeg with &lt;/dev/null (input redirection from nothing) and it should work.
Thank you, that fixed it. So do you know was getting piped to ffmpeg on loops where it was failing? Or why it failed in that pattern almost every time? How was ffmpeg causing problems with the $file variable even at the start of the loop (where it echo's the filename of each iteration)?
You've got multiple problems here... `ls |sort -R |tail -n 10 |while IFS="" read file; do` * Don't parse the output of `ls` * `sort -R` is not portable and not really that random * Piping to `while` is not desirable practice * `read` should almost always be called with `-r` `ls` could be feeding symlinks and subdirectory names into the `while` loop for all we know... Because you're using `sort -R` around, we can assume you've got `shuf`? In which case, how does this work? n=1 while IFS='' read -r; do time=$(ffprobe -i "$REPLY" -show_entries format=duration -v quiet -of csv="p=0") printf -- '%s\n' "Time of file '${REPLY}' is ${time}" ffmpeg -t 3 -ss "$(echo "scale=4;$time*(3/7)"|bc)" -i "${REPLY}" -vf scale=480:-1 "$n".gif &lt;/dev/null (( n++ )) done&lt; &lt;(find . -maxdepth 1 -type f -printf '%P\n' | shuf -n 10) 
My fork parses additionally media RSS feeds and integrates into the Gnome Shell. Also I am actively accepting and fixing issues.
I just tested it, works fine. This is the script I used to “create” the test images (just empty files): for i in {0..65536}; do touch $(head -c1K /dev/urandom | sha256sum | cut -d' ' -f1).jpeg; done &gt; Also, how do I actually run a script? I'm kind of new to BASH. In simple cases, you can just copy the script and paste it into your shell, and it should work. The more reliable way is to save it to a file (e. g. called `sort-pictures`), make that file executable (`chmod +x sort-pictures`), and then run it (`./sort-pictures`). Make sure you do this in the right directory – either have `sort-pictures` in the directory that also contains the photos, or have it somewhere else (`/tmp`) but run the script from a shell that is currently in the photo directory (`cd /path/to/your/photos; /tmp/sort-pictures`).
ffmpeg reads a few bytes of your stdin till it finds something which it can handle. When running interactive just press "?" to find out what is supported. Let's say your files are: abcd.mp4 efqfoo.mp4 Now on your first run, ffmpeg starts with $file=abcd.mp4 and reads from stdin the next few bytes: e, f, q … it stops at q, because that will quit ffmpeg (your first file wont get generated completely). The next run is now with $file=foo.mp4. And so on. Running it with ffmpeg &lt;/dev/null will circumvent this by letting ffmpeg read out from /dev/null instead your piped stdin.
I know exactly what youtube-dl is. I have a couple python and bash scripts built around it. &gt;if you want a quick way to wrap around in bash What I said was clear. A quick way to wrap around it, which is what the OP asked about, not discussing what scripting language s youtube-dl is developed in. 
Non-Mobile link: https://en.wikipedia.org/wiki/Process_substitution *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^150500
Wait...so perhaps a heredoc?
You've used git complete and have it booked into your shell?
 # Leading zeroes in arithmetic expressions cause interpretation as octal numbers $ echo "$((017))" 15 # Force octal base by prefixing 8# $ echo "$((8#17))" 15 # Use the %o format specifier to print octals $ printf %o\\n 15 17 # Increment $ a=17; printf %o\\n "$((8#$a + 1))" 20
No, I mostly use svn. I might be a little harder on OP if this was sumbission to /r/git, but it was submitted here and /r/beginner. He is likely looking at git as an inconvient thing he needs to use at this time. Many of his options are simple enough that it does indeed look like he needs to just learn git, but looking at some options, such as option 4, he saves himself having to type 4 commands so there is at least some benefit. I assume that as his needs progress, some of the simpler options will be pruned from the menu.
shellcheck.net
Yes but I do not want an external file. 
This great and the script is surprisingly short!
I think you misunderstood the `--files-from` parameter. The file it wants is about a list of specific files. It's not about a list of locations. I think it will do nothing if you try to supply a list of directories with this parameter. What should work is just listing all your source directories on the command line without parameter, and in front of the target directory, like so: rsync "${theList[@]}" "$theDestination/host-desktop"
Variable names do not begin with a $ symbol. You use it for expanding them. Do not quote the tilde if you want the tilde to be expanded to your home directory, also consider $HOME instead of the tilde in scripts. cat is not necessary, bash provides the syntax `$(&lt; file)`. It seems you only want to read one line, in which case the read builtin command suffices: # Command substitution pricetemp=$(&lt; "$HOME/output.txt") # Alternative with read read -r pricetemp &lt; "$HOME/output.txt" What is the idea behind `price=temp:01`? You try to use price in arithmetic context afterwards, which does not seem to make sense.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Run `help test` – you need `-gt` instead of `&gt;`.
I'll ha try thanks. And nice for the help test tips ;-)
&gt;pushd "$directory" You do not need to preserve the current working directory, scripts run in their own environment and will not change the directory of the parents executing them, you do not need pushd and the final popd. However, you should test if the directory change was successful: cd -- "$directory" || exit 1 &gt;echo $f Always protect parameter expansions from [Word Splitting](https://www.gnu.org/software/bash/manual/html_node/Word-Splitting.html) and [Filename Expansion](https://www.gnu.org/software/bash/manual/html_node/Filename-Expansion.html) with [Quoting](https://www.gnu.org/software/bash/manual/html_node/Quoting.html), also prefer printf over echo for everything but literal strings to avoid issues with variable content like -e -n and ansi escapes: printf '%s\n' "$f" Testing for file sizes can be done portably with the wc command: #!/bin/bash usage () { printf 'Usage: scriptname directory size\n' } err () { printf 'Error: %s\n' "$1" &gt;&amp;2 usage &gt;&amp;2 exit 1 } if (($# != 2)); then err "Wrong number of arguments ($#, 2 expected)." # Test if $2 contains a valid number: It must be non-empty # and it must not contain any non-digit character, # [[ $2 = +([[:digit:]]) ]] is an alternative test. elif [[ ! $2 || $2 = *[![:digit:]]* ]]; then err "size must be a valid number (\"$2\")." fi directory=$1 size=$2 cd -- "$directory" || err "Could not change directory (\"$directory\")." for file in *; do if (($(wc -c &lt; "$file") &gt;= size)); then printf '%s\n' "$file" fi done An alternative is the find command, because it can test for sizes, but it must be prevented from descending into subdirectories: find . '(' -type d ! -name . -prune ')' -o -type f -size "+${size}c" -print
I just saw you want to print the file's sizes as well: for file in *; do # Using read to strip leading and trailing whitespace if [[ -f $file ]] &amp;&amp; read -r filesize &lt; &lt;(wc -c &lt; "$file") &amp;&amp; ((filesize &gt;= size)); then printf '%s %s\n' "$filesize" "$file" fi done
&gt;read -d '' -r "${readOpt}" MAPFILE This will lose empty lines. &gt;Note: $SHELL is not a reliable test, but in this case it should be ok e.g. from 'bash', run 'zsh'. echo $SHELL = /bin/bash. i.e. parent proc. SHELL is not the parent process, but typically the user's login shell, it seems not only not reliable but useless for this purpose. &gt;Known issue: No traps! This means IFS might be left altered if the function is cancelled or fails in some way Declare IFS locally: `local IFS` &gt;if ! command -v mapfile &gt;/dev/null 2&gt;&amp;1; then Perhaps test for BASH_VERSION. &gt;local mapfileHelpArray=( You try making it work in ksh, ksh however does not have a local command.
This `SECONDS` thing is described like this in bash's documentation: &gt; **SECONDS** &gt; Each time this parameter is referenced, the number of seconds since shell invocation is returned. If a value is assigned to SECONDS, the value returned upon subsequent references is the number of seconds since the assignment plus the value assigned. If SECONDS is unset, it loses its special properties, even if it is subsequently reset. I guess for your script that means it will run its 'while' loop for (less than) ten seconds and then the script will end. About your "cpu_load" value you research inside the 'while' loop, I think this is the time the CPU is in idle, so it isn't load and is instead the opposite. I found the description about that by searching for the pattern `\bid\b` inside the 'top' man-page: Line 2 shows CPU state percentages based on the interval since the last refresh. As a default, percentages for these individual categories are displayed. Where two labels are shown below, those for more recent kernel versions are shown first. us, user : time running un-niced user processes sy, system : time running kernel processes ni, nice : time running niced user processes id, idle : time spent in the kernel idle handler wa, IO-wait : time waiting for I/O completion hi : time spent servicing hardware interrupts si : time spent servicing software interrupts st : time stolen from this vm by the hypervisor About this line here: { date &amp; ps aux --sort=-%cpu | awk 'NR&gt;1{if($3&gt;60) print $2,$3,$11}'; } &gt;&gt; cpu_overload.log I think there's something broken about it. In the first part, when you do `date &amp;`, the 'date' command will run in the background. I think you want to do `date ;` there instead. The `;` makes two command lines run one after the other, while `&amp;` makes them run in parallel. About this part of the line: ps aux --sort=-%cpu | awk 'NR&gt;1{if($3&gt;60) print $2,$3,$11}' The 'awk' program there will look for a CPU usage value of above "60.0" in the third column of the 'ps' output. There will probably be no process like that at that point when this will run in your script because earlier in the script you searched for CPU-more-than-60%-idle, not 60% load.
Bash has &lt;&lt;&lt; operator. So you could use a simple here document and load it up with paths and use the same trick.
Here's how I write my scripts. If I have to look up how to do something more than once, it likely goes into a script. That script becomes my "notes". As I collect more simple things, they are combined into more complex things as needed. For instance, you may find some times you want to push to something other than "origin master", so you could write a script to list targets, and select an option. You might set the target to a "default target" variable that can be set, so it doesn't have to be specified each time. You could select a branch to merge to after listing remote branches `git branch -r`. The following was tested with svn, but not git. Your mileage may vary. #!/bin/bash readarray -t BRANCHES &lt; &lt;(git branch -r) GREEN="\e[32m" TEXT_DEFAULT="\e[0m" declare -A OPTIONS OPTION_NUM=0 OPTION_WIDTH=3 for BRANCH in ${BRANCHES[@]}; do { OPTION_NUM=$((${OPTION_NUM} + 1)) &amp;&amp; OPTIONS["${OPTION_NUM}"]="${BRANCH}" &amp;&amp; printf "%-${OPTION_WIDTH}s) ${BRANCH}\n" "${OPTION_NUM}" } done echo "Please select branch number" read OPTION echo -e "User selcted branch [${GREEN}${OPTIONS["${OPTION}"]}${TEXT_DEFAULT}]" From there, you can use the branch name for whatever.
Thank you, I assume that changing the ... Will change the amount of characters it considers.
My pet peeve: piping grep into awk and sed. That's a big waste of processes. From your regex it looks like you're pulling out the idle field (`id`), yeah? cpu_load=$(top -bn1 | awk '/Cpu\(s\)/ { print $8; }')
 curl -s https://api.coinmarketcap.com/v1/ticker/ripple/ | python -c 'import json,sys;obj=json.load(sys.stdin);print obj[0]["price_usd"]' Although piping it to python is kinda cheating... But I don't think bash has any built in json handling....
Here's with perl: curl -s https://api.coinmarketcap.com/v1/ticker/ripple/ | perl -n0777E '/"price_usd"\s*:\s*"(.*?)"/s and say $1' This should keep working even if they change a bit how they provide their JSON, like for example if they remove all line-breaks and spaces and change it into one long line. If you have a tool named "jq" for JSON installed, it can get the value like this: curl -s https://api.coinmarketcap.com/v1/ticker/ripple/ | jq -r '.[] | .price_usd'
How would I set a variable to the output of that? I am trying to run fbdigits (a program that outputs numbers specially formatted to my raspberry pi display) with the number to have it display the price on my raspberry pi screen.
You can capture output of something with `$(...)`. You use that to set a variable like this: price_usd=$( curl -s https://api.coinmarketcap.com/v1/ticker/ripple/ | perl -n0777E '/"price_usd"\s*:\s*"(.*?)"/s and say $1' ) 
&gt; price_usd=$( &gt; curl -s https://api.coinmarketcap.com/v1/ticker/ripple/ | &gt; perl -n0777E '/"price_usd"\s*:\s*"(.*?)"/s and say $1' &gt; ) Got it working! Thank you.
I've tried it with: /usr/bin/rsync -avzEh --files-from=- /Users/me/Test --exclude=".DS_Store" &lt;&lt;EOF - "$HOME"/Library/Preferences/com.apple.dock.plist - "$HOME"/Library/Preferences/com.apple.finder.plist - "$HOME"/Library/Preferences/com.apple.menuextra.battery.plist - "$HOME"/Library/Preferences/com.apple.menuextra.clock.plist - "$HOME"/Library/Preferences/com.apple.sidebarlists.plist EOF along with several versions of &lt;&lt;-EOF and &lt;&lt;-"EOF" Still not working though... I almost want to give up and use an external file but I know this can be done as I have had a script in the past that did it but it was on a work machine and when I left the company I assumed I had it in my Dropbox. Several days later I called my old manager and he said my machine had already been repurposed (wiped &amp; reloaded) so..yeah. 
Where you tried using EOF, all those `-` characters you added in front of each line are a mistake. Those `"` characters are also a mistake. Things have to look exactly like this: /usr/bin/rsync -avzEh --files-from=- /Users/me/Test --exclude=".DS_Store" &lt;&lt;EOF $HOME/Library/Preferences/com.apple.dock.plist $HOME/Library/Preferences/com.apple.finder.plist $HOME/Library/Preferences/com.apple.menuextra.battery.plist $HOME/Library/Preferences/com.apple.menuextra.clock.plist $HOME/Library/Preferences/com.apple.sidebarlists.plist EOF And I think you also have to be careful to not have extra space characters at the end of each line. Here's an experiment at the command line and using 'cat' about what programs will see when sending stuff into their input through `&lt;&lt;`: $ x=test $ cat &lt;&lt;EOF &gt; "$x"/file &gt; $x/file &gt; EOF "test"/file test/file That `"` seems to be treated as the text character and will not disappear like what normally happens when using it on a command line.
Is this within a window, or the display in general, independent of what application (or none) is around that pixel? Should the color of the mouse cursor be taken into account? At an application level, this is easy, but on general level, this will be extremely difficult. This is definitely not doable in plain bash. 
If there is an command tool to save a screenshot then dump the screen to a file as a bitmap then convert the screen dump file to a hexdump and grep for a certain hex combination at the right location. Crazy? Why yes it is. Alternatively, in a few lines of C, read the screen memory buffer at the right offset and get the current colour value.
Also define green. What a human (in a certain country or culture) calls green or a certain RGB value and that value only.
 touch test_$(date +%F).log
So this is what i have for my script. !#/bin/sh echo " What would you like your file to be called' read file ^^^^^^^^^ so what would the next line be to repeat what they wrote but what they asked it to be called? 
 #!/bin/bash echo -n 'Filename: ' read name touch "$name-$(date +%F).txt"
You should really learn how to Google this yourself. You're relying on other people to do the work for you. Google is one of the more important skills as a programmer.
I was looking up on google, but didn't seem to be typing in what I was looking for that's why I came to this subreddit to get some help. Sorry. I won't burden you again.
I guess I did sound a bit cunty... :( Didn't mean it like that. Glad to help out. I was just worried I was hurting you in the long run..
&gt; d I understand where you're coming from, but with the info you gave me now I know i can go back and look up what each line is doing so i have a better understanding on why you did -n and what %F is, because when I was googling it I wasn't getting the right things to pop up for me just that generic " hello world" thing over and over. I really do appreciate your help 
Your shebang line should be #!/bin/sh, not !#/bin/sh: *#* (ha**sh**) + *!* (**bang**) = *#!* (**shebang**) This will redirect output to a filename given on the command line where the script is invoked or from interactive input if no name is given on the command line: #!/bin/bash if [[ $# -gt 0 ]]; then name="$@" else read -e -p 'Enter a file name&gt; ' name fi &gt; "${name}_$(date '+%F')" 
Yea this is much more advanced, the only reason my shebang line is #!/bin/sh is so that I can use it anywhere, we are currently using "c9.io" to do most of the commandline things, as far as the other things, it would work yes but i'm still in the beginning parts of bash scripting so I'll be sticking to " echo " and "read" mostly until we get into a little bit harder stuff lol, just goes to showyou there's like 50 different ways you can do things all very interesting so I'm excited to learn more.
Just letting you know that your command works for me, since I have the latest version of `getfattr` that fixed the bug that would otherwise make the assumption false as /u/McDutchie has mentioned. Thanks. I also asked /u/McDutchie some follow up questions if you want to check them out. Thanks guys.
The redirection `&gt;file.json` at the end clobbers your file before you can even begin to `cat` it. Use a tempfile.
How would that work if I want to update the original file with the update?..
jq -e 'select(.AWSDatabases[].Name=="db02").Version="3.0"' file.json &gt;file.json.tmp &amp;&amp; cp file.json.tmp file.json
Try the suggested command, but it does not update the corresponding version field, but adds it outside of the AWSDatabases. { "Name": "manifest", "AWSImages": [ { "Name": "myimage01", "Version": "0.0.0.1", "Type": "Image" }, { "Name": "myimage02", "Version": "0.0.0.2", "Type": "Image" } ], "AWSDatabases": [ { "Name": "db01", "Version": "1.0" }, { "Name": "db02", "Version": "2.0" } ], "Version": "3.0" }
That did the trick.. Adjust the above supplied by command with the below and it worked... Thanks.. jq -e '(.AWSDatabases[] | select(.Name=="db02") | .Version) = "3.0"' file.json &gt;file.json.tmp &amp;&amp; cp file.json.tmp file.json
 #!/bin/bash -eu # dirinfo.sh this_script=$0 allowable_scales=KMG usage() { echo "usage: $this_script &lt;dir&gt; &lt;size&gt;" echo " e.g. $this_script /tmp 1024" echo " e.g. $this_script /tmp 512K" echo " e.g. $this_script /tmp 64M" echo " e.g. $this_script /tmp 1G" } die() { echo -e "ERROR: $*\n" usage exit 1 } if [[ $# -ne 2 ]]; then die "invalid args" elif ! [[ -d $1 ]]; then die "invalid directory: $1" elif ! [[ $2 =~ ^([0-9]+)([$allowable_scales])?$ ]]; then die "invalid size '$2' (need like 1024B, 2M, 3G)" fi dir=$1 size=${BASH_REMATCH[1]} scale=${BASH_REMATCH[2]} [[ -z $scale ]] &amp;&amp; scale=1 cd $dir while read fsize fname; do fsize=${fsize%%[$allowable_scales]} [[ $fsize -ge $size ]] || break echo "${fsize}${scale} $fname" done &lt;&lt;&lt; $(du -B $scale * | sort -nrk 1)
You can use 'import' from ImageMagick to get the color for one pixel, then process its output. import -window root -crop '1x1+800+800' txt:- # ImageMagick pixel enumeration: 1,1,65535,srgb 0,0: (6%,47%,6%) #0F0F77770F0F srgb(15,119,15) 
also, remember to read the manuals for every command you don't understand. In this case just "man echo" and navigate to -n: &gt; -n do not output the trailing newline
&gt;just goes to show you there's like 50 different ways you can do things Just always think, how can I clean this up, can I shorten it, will it still run without errors if I do this. Scripting is very cool. But, yes there are many ways of doing just one or many task. And get the same results your looking for. Just enjoy it, and don't think it as work. Anything that is Linux related is fun time for me. 
Holy crap this website is amazing, thank you so much for linking it ill definitely book mark it 
Same, I'm taking a class "operating systems" and i've never done things with command line and what not so it's all pretty new and exciting to learn for me. 
@Pagefault , were you the person who submitted that PR in my repo? 
No, that wasn't me.
Do you really have to use FTP? If you can use rsync you could at least use standard file globbing
Unfortunately yes. The only way I'm allowed access to the files is by connecting to a restricted network then SSH'ing into a server to grab the files.
I’ve been hurt by xargs. There’s a max number of args that Linux allows in a command and xargs also has a limit. If you have a larger set of data piped to xargs that data can get truncated and if you’re not paying attention you might not notice. If I’m not missing something here, your second script is possibly vulnerable to this. In OP’s case the set of data might be approaching the limits of xargs and Linux args so I’d be careful.
It's usually safe; `man xargs` on my Linux box: The command line for command is built up until it reaches a system-de‐ fined limit (unless the -n and -L options are used). The specified command will be invoked as many times as necessary to use up the list of input items. In general, there will be many fewer invocations of command than there were items in the input. This will normally have significant performance benefits. Some commands can usefully be exe‐ cuted in parallel too; see the -P option.
I like this globbing better than my invoking awk.
**Reported for referral link spam across multiple accounts.** https://www.reddit.com/user/UnkindTaborp/submitted/ https://www.reddit.com/user/UnkindTabora/submitted/ https://www.reddit.com/user/UnkindTaboro/submitted/ https://www.reddit.com/user/unkindtabori/submitted/ https://www.reddit.com/user/unkindtabors/submitted/ https://www.reddit.com/user/UnkindTabory/submitted/ https://www.reddit.com/user/UnkindTabore/submitted/ https://www.reddit.com/user/fegertsa/submitted/ https://www.reddit.com/user/trukirukia/submitted/ https://www.reddit.com/user/buyuksd/submitted/ https://twitter.com/nerd2techdeals/ name changed to https://twitter.com/techdealsandmor https://twitter.com/give2emsome name changed to https://twitter.com/give3emsome https://twitter.com/GeekDailyDeal
Where did you get `nmap` from? Did you build from source and install? If so, how did you configure the build? I see it's located in `/opt/usr/bin`. Is there an `/opt/lib(64)` ? Next, did you actually export LD_LIBRARY_PATH in the calling shell and/or the script itself? `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64` or whatever the path to `libpcap.so.1.3` is. Just to make sure you don't need libpcap-devel instead of libpcap or something. You could try setting the RPATH on the executable itself. `yum install -y patchelf` and then `patchelf --set-rpath /opt/lib64:/foo/bar /opt/usr/bin/nmap` with whatever library path you want to be searched.
Here's a script that does it: #!/bin/bash if [[ $# -ne 1 ]]; then echo "usage: $0 &lt;filename&gt;" exit 1 fi date=$(date +%F) filename="$1-$date" if [[ -f $filename ]]; then echo "error: $filename already exists" exit 1 fi touch $filename &amp;&amp; echo "created $filename" When run without arguments, it will print some usage information: $ ./create_file.sh usage: ./create_file.sh &lt;filename&gt; When run with a filename as input, it will create the file and tell the user about it: $ ./create_file.sh foo created foo-2018-02-20 When run again for the same file and day, it will throw an error since the file already exists. This may or may not be an error based on what you want the script to do - if you don't care, touching the file again won't do any harm. $ ./create_file.sh foo error: foo-2018-02-20 already exists 
You're getting variables backwards. Assigning doesn't involve `$`, like: foo="bar" Referencing a variable uses `$`, like: echo $foo No idea what you're trying to do with that "fbdigits" at the end. Is that some other program? 
Okay 
Fbdigits is a program 
Is `ldd` available? What do you get from `ldd /opt/usr/bin/nmap`?
i ran`ldd /opt/usr/bin/nmap` as follows... when ran by itself at ssh terminal: /opt/lib/ld-musl-arm.so.1 (0x76ef5000) libpcap.so.1.3 =&gt; /jffs/usr/lib/libpcap.so.1.3 (0x76eb4000) libstdc++.so.6 =&gt; /lib/libstdc++.so.6 (0x76db4000) libgcc_s.so.1 =&gt; /lib/libgcc_s.so.1 (0x76d99000) libc.so =&gt; /opt/lib/ld-musl-arm.so.1 (0x76ef5000) when ran within the script which was started by openvpn's route-up.sh: /opt/lib/ld-musl-arm.so.1 (0x76eae000) libstdc++.so.6 =&gt; /lib/libstdc++.so.6 (0x76dae000) libgcc_s.so.1 =&gt; /lib/libgcc_s.so.1 (0x76d93000) libc.so =&gt; /opt/lib/ld-musl-arm.so.1 (0x76eae000) libpcap is obviously absent when openvpn starts handles the script, but i have no idea where to go from here. I also checked and `/jffs/usr/lib/libpcap.so.1.3` is a symlink to `/opt/usr/lib/libpcap.so.1.3.0` which is 0755.
You can use tools like [expect](http://expect.sourceforge.net/) to automate terminal applications that expect user interaction, also see [autoexpect](http://expect.sourceforge.net/example/autoexpect.man.html). There are many [alternatives](https://en.wikipedia.org/wiki/Expect#Alternatives). A better course of action should be using gatttool non-interactively, i.e. without -I.
Can you just ssh to the server you are on and pass the command in quotes? 
what i want is programmatically decide whether I am in a ssh session or from local terminal. Is this clear now?
`if [ -z $SSH_CLIENT]; then echo "Local session"; else echo "SSH Session" ` Full disclosure: I haven't run it myself and typed it on mobile but that looks right
That works provided you don't do something like `su` or `sudo` to another user. To get around that, possibly something like this: myTerm=$(tty) pgrep -f "sshd.*${myTerm#/dev/}" That's tested on RHEL6, the `ssh` daemon may be different on other Linux distros but fundamentally that test should be similar. A quick squiz on a Solaris-9 host shows that probably you'll have to walk your way up the process tree and see if you find sshd before you hit pid-1. 
Perhaps something like this? Although I'm not aware how cross-compatible this is. checkSSH=$(who am i | tr -s ' ' | cut -f 5 -d " ") if [ -z "$checkSSH" ]; then echo "local"; else echo "ssh"; fi 
Yeah definitly for the processes waste. "From your regex it looks like you're pulling out the idle field (id), yeah?" not sure what you mean as the script seemed to show the expected values... :-( 
# Works when being root as well if [[ $(who am i) =~ "\([-a-zA-Z0-9\.]+\)$" ]]; then echo "SSH session" fi 
 # Works when being root as well if [[ $(who am i) =~ "\([-a-zA-Z0-9\.]+\)$" ]]; then echo "SSH session" fi 
Testing based on the tty is the best option, especially if you throw tmux/screen into the mix. The catch is looking not just for ssh, but also for mosh and friends. You should be looking for the session leader (I think this is what that snippet does, but pgrep isn't portable), just an sshd process in the current tty. I use a similar function to clear out the DISPLAY variable so I don't accidentally launch a graphical program when sshing into my work laptop from home. But the scope of the question can change. If you're testing whether or not to display a hostname in the prompt, it may be better to look at the raw tty, and declare screen/tmux as a local context.
Why do you need to do this?
? 
Oh damn, ty! 
This does not work, because you failed to quote the "$SSH_CLIENT" expansion and didn't use `[[` instead of `[`. As a result, the rest is positive whether or not `$SSH_CLIENT` has any value.
Adding `-E` to sudo by default is a security exploit waiting to happen. Please dont.
For me that's the wallpaper even when a window is in front so I think you need import -window $(xprop -root | awk '/_NET_ACTIVE_WINDOW\(WINDOW\)/{print $NF}') -crop '1x1+800+800' txt:-
weird, I do get a pixel from the window on top with -window root - black with yakuake, close to white with chrome. I'm on ubuntu 16.04
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Are you running it from a cron? Or just normally.
I want to use cron, still gotta figure that part out though
Yes I do, I got it working, I dont know what happened, I didnt change anything and it started working
make sure to use the entire path for both node, and the file lol.
Not tested. As someone else mentioned, you may need to put the PATH to the 'node' command. I'd make the PATH a variable to increase portability. You may also need to place the *.js scripts in your working directory or in the same directory where 'node' is located. #!/bin/sh set -x export EB_TOKEN='99FASDOFDSFDSFEOE0D' for js_name in build.js updateEvents.js updateArchive.js; do &lt;insert PATH here&gt;/node "${js_name}" if [[ ${?} -ne 0 ]]; then print "Failed running script "${js_name}"; fi done
 echo " do you like ice cream? ( yes/no)" read answer if [ $answer = "yes" ] then echo " Is your favorite vanilla ice cream? (yes/no)" read answer2 if [ $answer2 = "yes" ] then echo "yay" else echo "boo" fi fi This one works fine, this was my very first one I did in class, this works but my current one doesn't what am i doing wrong?? 
Have you considered including bz2/bzip2 as a compression algorithm? When I’m compressing CSVs to upload to HDFS I often find myself using this format because it’s splittable and you can seek to an arbitrary block without having to read the whole file. I find this can sometimes be a good trade off with respect to the slower compression depending on how often I’ll need to read the data. Tar can [append files to an existing archive](http://www.gnu.org/software/tar/manual/html_node/append.html) . Finally, if your files are precompressed I think the compression of the tar archive is allowed to use a different algorithm. You won’t gain much if any additional compression but it also shouldn’t break the dataset. Always test first though. 
Double vs single quotes, try ' whos there' http://wiki.bash-hackers.org/syntax/quoting it's trying to interpret whos as a terminal command
With single bracket tests `[`, you should use double-quotes for your variables. Also, remove the whitespace on either side of your test string. Try this instead: `if [ "$answer" = "whos there" ]`
Make sure you add quotes to your variables, unless you know exactly why you should not. Also, watch your spacing in your scripting. It absolutely matters. I re-wrote and tested your script and it does work. However since this is a class assignment, I'm not going to give you the answer. Feel free to PM me if you have any questions.
I definitely do not want the answer, but someone told me about the quotes and now i'm able to get the word "Yah" to come back which is what i wanted, so now i need to go on to the next part of the joke. I'm just starting to learn this so it's all new to me.
**[Fixed your link? Click here to recheck and delete this comment!](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=delete&amp;message=delete%20duqxywg)** ***** Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/c363764054fb7ff521b9b04506dc6a17.png Imgur mirror: https://i.imgur.com/tbMC7No.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
Woo! Good job!
I know that a program's memory is showing up as a file in `/proc/$pid/mem` (with `$pid` being the process ID of the process you are interested in). The file is documented in `man 5 proc` and mentions you can use `open`, `lseek`, `read` from the C library on it. Writing is not mentioned but maybe that works as well?
Your script likely contains a [carriage return](https://en.wikipedia.org/wiki/Carriage_return) character. kill complains a process with the PID "4772&lt;carriage return&gt;" does not exist and makes the terminal wrap the line when printing the error. Try this: tr -d '\r' &lt; your_script &gt; test_script Then see if running test_script makes a difference. Please note searching for your process in the process table for sending signals is no particularly robust approach.
`kill -USR2 $(pidof rclone)`
&gt; Your script likely contains a carriage return character. That was it. Thank you! &gt; Please note searching for your process in the process table for sending signals is no particularly robust approach. Could you elaborate?
Nothing guarantees pidof will catch the correct PID (additionally, there is a time frame between pidof and kill allowing potential race conditions, which could be mitigated by using the pkill utility instead). One better way is retaining control over the process as parent and send the signals from there as long as the child process is still around.
Quick follow up: if I start the process and create a VAR="$!" immediately afterwards, have I avoided aforementioned risks?
Check out shellcheck. It has detected many "non-breaking space" characters in my late-night scripts.
I missed that in your original post, I look for bzip by the shell command’s name not the library name. It all depends on where you use your archives. For HDFS it allows the cluster to do some processing on the storage node before shipping the results across the network. If you use gzip, lzo, etc. the system has to ship the whole file to one node before beginning to process it. That’s why bzip saves me time and system load, but HDFS is a unique beast and I’ve only scratched the surface of it myself. 
Ok, I figured it out myself: while [ -f $file.part ] #wait for file to finish downloading (Firefox) do echo "Waiting for Download to finish.." sleep 1 done
At the Moment i am using this for spaces: #for handling spaces in filenames IFS="$IFS" IFS=$'\n' Is it better, if I use [[ over that? Or does it make no difference?
Good, just use error exit codes, &gt;&amp;2 stderr; ;echo 'Error: var X not found' &gt;&amp;2 ;exit $ECODE and, echo 'hello' ;echo '' = echo -e 'hello\n' always use ' ' for plain text and " " for text with bash expressions 
[[ is recommended for bash scripts. It just eliminates the need to be as careful as you must be if you use [.
Good work. :D When you have an `if` with only a single outcome like: if [[ -z $ip_address ]]; then ip_address="127.0.0.1"; fi ... this can be reduced to: [[ -z $ip_address ]] &amp;&amp; ip_address="127.0.0.1" ... which is easier to read. The trailing `;` is not required. So, you end up with something like: [[ -z $ip_address ]] &amp;&amp; ip_address="127.0.0.1" [[ -z $root_dir ]] &amp;&amp; root_dir="/var/www/html" [[ -z $server_admin ]] &amp;&amp; server_admin="webmaster@localhost" [[ -z $enable_ssl ]] &amp;&amp; enable_ssl="no" [[ -z $cert_file ]] &amp;&amp; cert_file="/etc/apache2/ssl/dummy-ssl.crt" [[ -z $cert_key ]] &amp;&amp; cert_key="/etc/apache2/ssl/dummy-ssl.key" [[ -z $verbose_mode ]] &amp;&amp; verbose_mode="no" 
If you use this variable in the parent process that spawned the child, yes. Otherwise, if you mean via pid files, then no, not so much.
I personally think the `if` is easier to read.
No, there’s a much easier way to achieve that! if is_valid_ip_address $ip_address; then # ... fi
The wooledge wiki usually has pretty good writeups. Here's their process management page: https://mywiki.wooledge.org/ProcessManagement
Hey, that's an awesome way to reduce if statements, I didn't know that! Thanks a lot! :D 
Thanks, I'll try that immediately!
Can you explain why? ;)
Damn, super detailed, thank you very much I’ll definitely screen shot this so I can go back to it. Thank you. 
Also, if for whatever reason you can't use the command directly in the `if`, you don't need to `echo $?` to compare it to 0, just do: my_cmd if [ $? != 0 ] ; then echo the frizzle has reversed &gt;&gt; /dev/stderr exit 1 fi Also, it was a revelation when I realized that `if` works with any command and that the opening square bracket you almost always see is just a command. Seriously, your system has a `/bin/[`, which is usually just a symlink to `/bin/test`. And the closing square bracket is just an optional final argument that `test` ignores. This means you can use `man test` to see all the types of expressions that are valid in a regular `if [ ]` statement without having to wade through the huge bash manpage. I think `test`/`[` is a bash builtin that takes precedence over the system binary anyway, but it's still changes the way you think about the if statement in the shell.
Can you share some code? And have you tried any [shellcheck directives](https://github.com/koalaman/shellcheck/wiki/Directive)? Specifically `shellcheck disable`?
You can reduce them even further, like this: ip_address="${ip_address:-127.0.0.1}" See: http://wiki.bash-hackers.org/syntax/pe#use_a_default_value 
Might be wise to incorporate your logic into the python-written [youtube-dl](https://rg3.github.io/youtube-dl/) as it's already supporting hundreds of video sites, instead of maintaining a standalone project.
You have a typo in run.sh, « confif.sh » instead of « config.sh » . :-)
 $ imgur ~/foo.jpg
Things like your assignment to CONTENT_TYPE will break if your filename has whitespace in it. You should quote your variables more consistently. (Also, I think it's now considered bad practice to capitalize variable names like that, since you could clobber an environment variable.) The `valid_url` seems like it's doing a lot of extra work since now you have to curl the url twice. Why not just check to see if the curl succeeds at the time when you need the data? Finally, you can save a lot of subshells and processes and variables since `jq` can do more than one thing at once: curl -X POST ... \ | jq -r '.data | .hash, .ext' | xargs printf "https://i.imgur.com/%s%s\n"
Thank you for advice. I grabbed shellcheck for personal use via vim elm plugin. Work excellent.
thanks for the suggestion :)
oh I see... actually run.sh does nothing, all logic is in bin/, but I am correcting it now, thanks!! :) 
The question is: can you?
It’s OK if you have no coding experience. We’ve all been there. Hope you keep at it. That way, you’ll learn what makes code readable. ;)
Got it. That's a "no" then ;)
Sorry I'm not getting it. Though I want to see what is sent to the target page and though I can open the saved_file.html in a browser, what should I be editing? I can find no GET or POST in the html file and am unsure how to tell Curl to reference the html file in the current directory.
You need to look at the page with the form on it. Not the target page. Look, this was an entirely shitty way of trying to demonstrate parameters in GET and POST requests and I think it's obscuring your understanding of it... but anyway. When you request a URL you can either GET the url or POST to it. Got it? When you POST you must enumerate the data in the body of the request, after the headers. This is done in a variety of ways, but essentially you're looking at key-value pairs, where the keys are the names of the form fields, and the values are what you typed in or selected. Essentially a post would then look like POST /saved_file.html HTTP/1.1 Host: whoeverimtalkingto.com formfield1=name&amp;formfield2=whateverItypedin&amp;formfiled3=filename.etc GET requests can accomplish much the same thing through what's called URL parameters. I'm sure you've seen them in your browser's address bar at one time or another. The author wanted you to edit the caller page, the one with the form on it, to change POST to GET simply so the query string would show up in your address bar. It wouldn't actually upload a file; it was simply for illustrative purposes. However, they aren't interchangeable; the server's target page needs to be coded to accept GETs or POSTs and usually will do one or the other. 
I can see the cURL data when I past it out from the browser, which I can modify (for example inserting a new image URL) and then use cURL to submit. Seems there's much cookie data passed as header (-H), rather than as reference to a cookie file (-b). In this case I've got a stack of image files that I want to upload and process by adding an action as param to the URL that subsequently open in browser. More traditionally this would be done via FTP, but in this case I only have access via HTTP. 
Not the blog's OP, but glad you liked the article 😀
Try this one instead and note the differences: #!/usr/bin/env bash bSize=() bAdd=20 bLen=5 for i in $(seq 0 $bLen); do bSize+=($bAdd) ((bAcc+=$bAdd)) done echo ${bSize[@]} echo $bAcc 
Then you'll need to append the value of the accumulator `bAcc` as each new array element instead of `bAdd`.
Thank you again! I appreciate the help :)
No worries. :D Also, the values go to 120 as the loop runs 6 times. your sequence is from 0 to 5 (6 iterations). If you only want 5, you might want to start it at 1. 
What are you wanting to achieve? And which version of `bash` are you using? Because there's likely a more elegant solution than the one provided...
https://medium.com/@petehouston/upload-files-with-curl-93064dcccc76
Cheers! Yes, I intended starting the process then capturing $! right afterwards in the same script.
Here is an example of a function: `function ansible_provision() { [array] tags=(all) [array] skip_tags [array] limit=() . "$_GO_USE_MODULES" 'instance' ansible-playbook \ --tags="${tags[@]}" \ --skip-tags="${skip_tags[@]}" \ --limit="${limit[@]}" \ -- "$PROJECT_DIR"/playbook.yml &gt;&amp;${STDOUT} }` I am using the [Bash Infinity framework](https://github.com/niieani/bash-oo-framework). Here is a screenshot of how this looks like in vim with syntastic and shellcheck: https://imgur.com/a/96fnR Shellcheck stop on the first `[` it encounters. 
Please see [this post](https://www.reddit.com/r/bash/comments/8016bg/getting_custom_aliases_recognized_by_shellcheck/duv22nq/), I accidentally replied to the thread.
/edit: apologies for the slow response - I've been busy mentoring my junior sysadmin underling, so this has been slowly typed up across the last few hours :) bash 3 is going to be a limitation. If you were using bash 4, you could carve out a large amount of the code by simply using `mapfile` (demonstrated below). To pass values to the script, the basic way to approach that is "positional parameters", which you can read about here: http://wiki.bash-hackers.org/scripting/posparams So you'd start off with something like this: #!/bin/bash bLen="${1:-5}" (( bLen-- )) bIncr="${2:-20}" bStart="${3:-0}" bMax=$(( bLen * bIncr + bStart )) So building on that, what I'm doing here with `${a:-b}` is setting default values. So let's say you just run the script by itself: `$bLen` will be set to 5 (i.e. ${1:-**5**} ). `$bLen` is then immediately decremented to cater for starting at 0. Let's say you run `script 20`. Because `$1` is defined, `$bLen` will be set to 20 and then decremented to 19. I've renamed `$bAdd` to `$bIncr` and added `$bStart` with default values of 20 and 0 respectively. Finally, I've added `$bMax` which calculates your max value. This is a value we can calculate and use, so we may as well calculate and use it! So, for the sake of demonstration, let's add this to the above: seq "${bStart}" "${bIncr}" "${bMax}" Or, if you're using bash 4 and you're certain it will be safe: eval "printf '%s\\n' {$bStart..$bMax..$bIncr}" And let's demonstrate the default behaviour: $ ./script 0 20 40 60 80 * Starts at 0? Check * Increments by 20? Check * Does so 5 times? Check Now let's set `$bLen` to 10 elements: $ ./script 10 0 20 40 60 80 100 120 140 160 180 How about 10 elements, incremented by 5 and starting at 20? $ ./script 10 5 20 20 25 30 35 40 45 50 55 60 65 In bash 4, to throw an array into the mix (this would be handled differently within the script itself): $ mapfile -t bSize &lt; &lt;(./script 5 5 5) $ printf '%s\n' "${bSize[@]}" 5 10 15 20 25 I'm also not sure about accumulating as you go... maybe I'm misunderstanding, but to my mind that over-complicates things. You're working with numbers that you're feeding into the array, so why not just build the array and deal with whatever extra processing you want to do after the fact? For example, there are several ways to sum integers, but let's use `awk` to sum what's in `${bSize[@]}`: $ printf '%s\n' "${bSize[@]}" | awk '{ sum+=$1} END {print sum}' 75 So, plenty there for you to chew on :) One downside of positional parameters: let's say you just want to change the third parameter. Well, you're forced to provide the first two. So there comes a point where you progress on to [getopts](http://wiki.bash-hackers.org/howto/getopts_tutorial).
Something something shellcheck. I see lots of inconsistencies in the first few lines (bash but [, missing quotes, etc.)
I should have commented on my usage of that line for currentUser. Per Apple's software engineers that is how they say you should get the current user in a bash script. I know of at least three other ways to do it but having been a macOS Admin for some years and dealing with Apple support when you get to the level of actually getting on the phone with an engineer they will tell you that's the way it should be done. So I adopted it as my "go-to" method. Your usage of currentUser=$(who | awk '{print $1}') is a good one and will in fact survive a sudo myscript as does the way I'm doing it using 'ls'. 
&gt; Per Apple's software engineers that is how they say you should get the current user in a bash script. That.Is.Fucking.Breathtaking. Is what I would be saying, had I not seen similar levels of stupid from RedHat ;)
If you want it to be as portable as possible, you should write in a POSIX compliant manner. I only skimmed through it, but I agree with moviuro. You shouldn't use all cap variable names as they can conflict with the user's environment variables. Also, you use the correct flags to check for directories/files, and that you have the correct permissions - otherwise your script will barrel on regardless, and may result in unintended consequences. Consider using `set -e`, if you're not going to implement your own error checking where you bail upon fail, especially when `dd` is concerned.
&gt;set -e this is always good advise, but part of the challenge is how stingy `dd` is return codes. It'll basically either exit 0, or screw up your day without complaining. Thus the focus on external checks and zero focus on return codes. I would love to be wrong about this. but yeah. stuff other than dd could fail. i'll work on that, ty. 
For reference `ddrescue` has the ability to be restarted if you give it an optional log file. 
ye, but you still have to double check mount points between reboots. This script saves the long form serial of the drives and matches the mount points to that. 
By way of demonstration: $ sudo dmidecode -t4 | grep -E 'Current Speed:|Family:' | sed 's/^.*: //' | paste -sd ' ' - Core i5 2500 MHz $ sudo dmidecode -t4 | grep -E 'Current Speed:|Family:' | sed 's/^.*: //' | tac | paste -sd ' ' - 2500 MHz Core i5 
The interpretation of those characters has already been done on the outside. Inside the function (or script or program), you can only see the result of that. You can't see anything about what the user actually had typed at the command line prompt. I use this function in my .bashrc whenever I have a question about what happens to a certain command line: args () { printf "%d args:" $#; printf " &lt;%s&gt;" "$@"; echo } Using it works like this: $ args a b c 3 args: &lt;a&gt; &lt;b&gt; &lt;c&gt; $ args " a b" "c d " 2 args: &lt; a b&gt; &lt;c d &gt; $ args \\b 1 args: &lt;\b&gt; $ args \b 1 args: &lt;b&gt; $ args '\b' 1 args: &lt;\b&gt; I don't know if there's a trick to solve your problem. There might be a way to define your own programming constructs for bash (I mean things like 'if', 'for', 'while'). Maybe how alias expansion works can somehow help. I tried to search for something along those lines but couldn't find anything.
I'm not sure if it's specifically Bash, but I really liked this: https://cmdchallenge.com/
I tried looking around again, and found stuff that might somehow be made to work for this: There's a 'DEBUG' trap. The command set for the trap will execute before any command line is run. Maybe there's a way to access the raw command line while inside that trap? Another idea, I noticed in the output of 'history', you can see the actual command line how it was before interpretation. For example when I earlier showed examples about that 'args' function, when I now look in my history, I see that raw `\` on the `"args \b"` command I ran, and I see those spaces I had on the `"args a b c d "` command line. I guess you could try to get the original command line out of there. I tried defining a function running the history command as an experiment, and the history is accessible: $ foo() { history 12; } $ foo # &lt;-- it does print history lines here that I don't want to show You would have to define your 'cd' function inside your .bashrc. It can't be an external script or it won't be able to access the history.
It depends on what level you expect them to come with and what level you want to train them to. For example, I'll start with people who may not have any command line experience, and sometimes I'm training someone on the finer points of bash syntax and how to structure scripts. I try to get people to the point where they can help themselves. For beginners, that means knowing the help and man commands, and giving them the bash murder mystery as homework. For more advanced stuff, I usually point out readline, walk them through my dotfiles and set them up with shellcheck and shfmt. If you're planning on creating something of your own, I'd start by defining very specific goals in terms of the starting level and make it shorter than you want to at first.
Thanks for the suggeston about the history! I was able to get the literal input using the following code: getLiteralInput() { echo "$(history)" | tail -n 1 | sed -E s/'^.*getLiteralInput (.*)$'/'\1'/ } (Note: this has only been tested an a handful of inputs, so it might not work 100% of the time. So far it seems good unless there is a '(' or ')' or '&amp;' character in the filename. PS On my version of bash I have the function defined in an external file (~/.bash_functions) and it can still access the history. The function is initiated in ~/.bashrc though (using `. ~/.bash_functions`).
Looks cool 
You can use `history 1` instead of that `history | tail -n1` you are doing. On my computer here, running `history 1` completes instantly while there's a bit of delay for me when printing the whole history (I have an infinite sized one that's currently at 200k lines).
You can do this with `sed` alone: system_profiler SPHardwareDataType \ | sed -n -e '/^Processor Name/{;s/.*: //;h;}' \ -e '/^Processor Speed/{;s/.*: //;G;s/\n/ /;p;q;}'
Good advice, thanks 
Thanks I'll try that. Any idea on how to get rid of those piped processes?
The pipe will add one process. The two sides of the pipe run in parallel. I think you add another process where you surround your `while` loop with `(` and `)`. That `(...)` is a "sub-shell". You can try removing the `(` and `)` because you don't need it as the `while ... done` thing is already grouping your lines of code and it will work with the pipe.
use https://www.shellcheck.net/ :) and see also: https://mywiki.wooledge.org/CommandSubstitution
two things jump out `lAcc= awk -v i="$i" 'BEGIN{a=.1; a^i }'` * if you want to assign the output of a commands to a variable you want `var=$()` * your awk statement doesn't print anything. ProBoxAlpha:~ hegemon$ works=$(i=1;awk -v i=$i 'BEGIN{a=.1;print a^i}') ProBoxAlpha:~ hegemon$ noworks=$(i=1;awk -v i=$i 'BEGIN{a=.1;a^i}') ProBoxAlpha:~ hegemon$ echo ${works} 0.1 ProBoxAlpha:~ hegemon$ echo ${noworks} ProBoxAlpha:~ hegemon$ also, `$lRates[@]` won't work, you want `${lRates[@]}` (or possibly `${lRates[*]}` I can never remember which). 
From one of my scripts: if $KillProcessOption then for Grandchild in $(ps -o pid --no-headers --ppid $1) do pkill -${KillSignal} -P $Grandchild &amp;&gt;/dev/null # Kill BackgroundProcess' children's children (grandchildren) done pkill -${KillSignal} -P $1 &amp;&gt;/dev/null # -P means kill processes whose parent process ID is listed. Here: kill BackgroundProcess' children let "KillProcessTimeout = $KillProcessTimeout * 10" # Will be checked every 1/10 seconds while &amp;&gt;/dev/null ps --pid=$1 &amp;&amp; [ $KillProcessTimeout -gt 0 ] do sleep 0.1 let KillProcessTimeout-- done if &amp;&gt;/dev/null ps --pid=$1 then echoE "$CommandName: Warning! Process '$1' could not be killed." exit 70 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
First, ssh has a way of doing thus with its config[Look at this wiki page](https://wiki.archlinux.org/index.php/Secure_Shell). Second, bash has a way of taking user input called `read`, which by the way uses $REPLY as a default variable like read -rp 'enter text &gt; ' echo "$REPLY"
I didn't understand what you wanted at first sorry, bash isn't really good at manipulating data, and I don't know if an object syntax exists, like a key value pair of addresses. That said you could us grep as it will return the out put to a bash var like A_var="$(grep -r "$READ" the/file/path)"
I am not a bash expert, but have created entire command line tools used by my engineering and devops teams. So, you know, a manager who still dabbles at the keyboard and codes for fun on weekends. With that caveat, here we go. If you just want to verify the server the user entered is valid then you can probably set a variable and look over the server list. Something like FOUNDSERVER="false" SERVERS="s1 s2 s3 s4" for s is ${SERVERS} do if [ $s = $user_input] then FOUNDSERVER="true" fi done # Check the value of FOUNDSERVER and do the right thing here. If you are trying to be more clever and take the sure input and look up the real server information, you want an associative array. GNU Bash 4 does those I believe. I use it in a script of my own. (Total tangent: it's very strange to be writing bash scripts and remember when I used to do hexadecimal addition and subtraction in my head because I wrote assembly every day. Today I mostly think about ML, and could not tell list even 10 opcodes for any chip. What an everchanging journey programming has been.) 
If you haven’t figured it out yet, you can edit your ~/.ssh/config and put this in it: Host NameYouUse Hostname IP/Domain User User ...
It is just a snippet. $1 is the parent process. The last loop checks if it is still running after killing all its grandchildren and children (in that order). The parent process should exit once all its children have exited (as set up in another part of the script).
&gt; system_profiler SPHardwareDataType \ &gt; | sed -n -e '/^Processor Name/{;s/.*: //;h;}' \ &gt; -e '/^Processor Speed/{;s/.*: //;G;s/\n/ /;p;q;}' I tried this suggestion, but it didn't yield any output. I'd have to look more closely at the syntax to understand why. Still, this is promising. 
To be fair, I was just guessing at what your input looks like since I don't have `system_profiler`. If you want to learn more about using sed this way, I recommend the grymoire guide. 
I have another solution that works for me. I use an alias in my .bashrc file ```alias server1='ssh -p 2200 myname@my.server.location' ``` this prompts me for the password and away I go 
Step 1: As others have indicated, get your `~/.ssh/config` file in order Step 2: Insert the following into your `~/.bashrc` # SSH auto-completion based on ~/.ssh/config. if [[ -r ~/.ssh/config ]]; then complete -o "default" -W "$(grep "^Host" ~/.ssh/config | grep -v "[?*]" | cut -d " " -f2- | tr ' ' '\n')" scp sftp ssh fi # SSH auto-completion based on ~/.ssh/known_hosts. if [[ -r ~/.ssh/known_hosts ]]; then complete -o "default" -W "$(awk -F "," '{print $1}' ~/.ssh/known_hosts | sed -e 's/ .*//g' | awk '!x[$0]++')" scp sftp ssh fi 
&gt; My use case, though, constrains both the tools that I can use, and privilege escalation. I think the demonstration might have gone "woosh" over your head a bit? I don't have a Mac, so I don't have `system_profiler`. If you're not aware, `system_profiler` is a Mac thing. The closest I have on Linux is `dmidecode`. `dmidecode` requires `sudo` so that it can read things like `/dev/mem`. Ultimately I was trying to get you to connect some dots i.e. the `sed` call is the same, so what's on the right hand side of my `sed` call? Maybe you should put that on the right hand side of yours: system_profiler SPHardwareDataType | grep -E 'Processor Speed:|Processor Name:' | sed 's/^.*: //' | tac | paste -sd ' ' - 
Thank you!
How would this work with a ssh key
just add the line: IdentityFile /path/to/your/key
&gt;I am running on bash 4.3.38. Good. Don't bother with the loop.... in `bash` at least. You have `mapfile` available to you and it will make your life easier: mapfile -t lRates &lt; &lt;(your command here) Move your loop into `awk`, like this: awk 'BEGIN { for (i=0; i&lt;10; i++) print .1^i }' 1 0.1 0.01 0.001 0.0001 1e-05 1e-06 1e-07 1e-08 1e-09 Scientific/Mantissa notation it seems. Use `awk`'s `printf` function to get around that: $ awk 'BEGIN { for (i=0; i&lt;10; i++) printf "%f\n",.1^i }' 1.000000 0.100000 0.010000 0.001000 0.000100 0.000010 0.000001 0.000000 0.000000 0.000000 Wrapping that all up: $ mapfile -n 10 -t lRates &lt; &lt;(awk 'BEGIN { for (i=0; i&lt;10; i++) printf "%f\n",.1^i }') $ declare -p lRates declare -a lRates='([0]="1.000000" [1]="0.100000" [2]="0.010000" [3]="0.001000" [4]="0.000100" [5]="0.000010" [6]="0.000001" [7]="0.000000" [8]="0.000000" [9]="0.000000")' Google away at getting the formatting etc locked in for your needs.
Nice! I had been doing this with environment variables, but I like your way much more. In a .*rc file... export SERVER_ONE="user@hostname.com" At the prompt... ssh SERVER_ONE
You can do all sorts of sorcery with ssh config.
You're absolutely right. Derp. 
I might try something like this: read -ra awk_output &lt; &lt;(awk ...) (untested)
Here's an example using ... `wait`! curl -o /dev/null http://speedtest.newark.linode.com/100MB-newark.bin &amp; printf '%s' "Waiting for Download to finish ... " wait printf '%s\n' "done." You'll probably want to replace the URL with something you trust.
Wow I've been using aliases and /etc/hosts. This is handy. 
Brilliant! Thank you all for your answers.
FYI a key value type does exist, look up "bash associative arrays" for more info. I think it's just bash 4 tho.
This is the way. Also use "ssh-copy-id" and never have to type your password again.
thanks, it is bash 4 though, anyway if anyone else sees this declare an associative array declare -A aa add a value aa[key]='value' use it echo ${aa[key]} *value*
keys are already in the .ssh folder ? Unless I'm missing something in your message, my keys work fine.
I am not the author.
Cool, actually needed this today
Also, it would be easier for others to test if you made those text files available as actual text instead of graphic screenshots. pastebin.com is your friend.
and ..... *What is the expected/desired output?*
edited main post. sorry about that
Is this the desired output? $ for x in `cat NoSubs_Pathlist.txt | tr " " "."`; do grep -i $x VideoList_RecentlyCreated.txt; done F:\Users\Mauri\Videos\TV-Shows\Dragon Ball Super\Season 4\Dragon Ball Super - 052 - Teacher and Student Reunited - Son Gohan and Future Trunks.mkv F:\Users\Mauri\Videos\TV-Shows\iZombie\Season 4\iZombie - S04E01 - Are You Ready for Some Zombies.mkv 
You could save this output in a file, and then use that file as input to whatever functions you want to accomplish. Or you could use xargs with *-d \n* to run other functions with the list.
I converted it to a unix compatible format https://i.imgur.com/CXj0bj6.jpg because it was complaining about `/r` characters
Here is a description in the category theory style: cleanZombie: dir ^ | (fmap) for i in `ls $1/lock* 2&gt; /dev/null`; do | +--------+ | | | rmIfZombie V | lock* --------+ rmIfZombie: == 0 (Nothing) : &lt;------- * ------&gt; rm file ^ | (fmap) wc -l | ps --no-heading --pid | (file,pid)+------------------------------+ (file) | | (fmap) xargs ssh parseLockFile | (file) ---------------&gt; (file,hostname,pid) + 
On the line where you use `=~` to compare, perhaps remove the `"` on the right side: [[ "${line,,}" =~ $item ]] When I try using `=~` here, it seems to me the special regex characters won't work when using `"`. With the `"`, it behaves like `=`. I tested like this: $ line=hellotesthi; regex=".*test.*" $ [[ "$line" =~ "$regex" ]] &amp;&amp; echo true $ [[ "$line" =~ $regex ]] &amp;&amp; echo true true That's just assuming you want to ever use regex patterns in your file.
I've been using an OpenVPN connection successfully for a few years but find that there are instances where I don't want the client to always establish a connection to the server. When the client (Debian Jessie laptop) is connected to my home WiFi I would like to not have an OpenVPN connection established. When the laptop is connected to any other WiFi network I would like to have the client establish the VPN connection. I was not able to locate anything where this has been done before. So I decided to take it into my own hands and learn to script using BASH. This is extremely crude but works. I was curious if you guys had some advice on how to make this better? And how to automate this task as a service on the system? #!/bin/bash # Author: bioszombie # Date: 2018-03-01 # Description: Establish OpenVPN connection when away from home LAN # Define home LAN WiFi network(s) # hwlan=ESSID hwlan=myWiFi # Probe system for currently connected WiFi cntdwlan=$(sudo iwgetid -r) # Define path to OpenVPN ovpn file ovpn=/etc/openvpn/client.ovpn #echo $cntdwlan #echo $hwlan if [ "$hwlan" -eq "$cntdwlan" ]; then echo 'You are connected to your home network' echo 'OpenVPN connection will not be established' else echo 'You are connected to a strange WiFi network' echo 'OpenVPN connection to be established' (sudo -b openvpn --config $ovpn) echo 'You are now connected to the VPN' fi
Meet your new best friend: `man(1)` (short for manual) $ man 1 man Then, meet `cp(1)`, `mkdir(1)` and `date(1)`. You might also want to check the `sh(1)` man page, and jump to `[`, which does tests.
Wouldn't leaving $item without quotes give errors in the case of $item having whitespace in between words? 
When you use `[[` in bash, you are protected against that problem. It knows if something is a variable and treats it as one thing. See here: $ x="a b c"; y="a b c" $ [[ $x = $y ]] &amp;&amp; echo true true $ [ $x = $y ] &amp;&amp; echo true bash: [: too many arguments The `[` error message is because it is like an external program that sees a list of arguments on its command line ('a', 'b', 'c', '=', 'd', 'e' and 'f').
So is there any reason for using a single bracket? Should I use double brackets for every comparison I do? 
Yes, you should always use `[[` in a #!/bin/bash script. It protects you a bit against mistakes and it enables the use of a bunch of extra features like `&amp;&amp;` or `=~`. I think the `[` is only there and only behaving weird for compatibility reasons, because bash can execute any #!/bin/sh script and the /bin/sh in for example Debian only knows `[`.
Thanks!
So you have a specific script where this behaviour is occurring? Have you run it with `bash -x scriptname` or put `set -x` into it? You might like to run `( set -o posix ; set )` at the start of the script, dump it out to some file, and run it again at the end of the script, dumping it out to another file, then `diff` the two files?
Make it so that it's a single script that chooses scramble or unscramble based on the file extension.
Great idea! Here it is. I use `file` and `grep` to check if the input file is encrypted rather than relying on the file extension, in case I want to change `.scramble` to something else or add an option to give the encrypted file any arbitrary name. #!/usr/bin/env bash # # Shell script that encrypts or decrypts a file with a password using GnuPG. The encrypted files are named with a .scramble extension. # Deletes the original unless the --keep flag is specified. # # Check that a command is installed installed () { command -v "${1}" &gt;/dev/null 2&gt;&amp;1 || { &gt;&amp;2 echo "Cannot execute the ${1} command"; exit 1; } } installed gpg installed grep keep=false scramble_params=( ) while [[ $# != 0 ]]; do case "${1}" in -k|--keep) keep=true shift ;; *) scramble_params+=("${1}") shift ;; esac done if [[ ${#scramble_params[@]} = 0 ]]; then &gt;&amp;2 echo "Missing file to scramble." exit 1 fi # TODO: encrypt/decrypt multiple files rather than just the first one provided on the command line. input_file="${scramble_params[@]}" if [[ ! -f ${input_file} ]]; then &gt;&amp;2 echo "${input_file} does not exist" exit 1 fi output_file= if grep --quiet "encrypted" &lt;(file "${input_file}"); then output_file=`echo "${input_file}" | sed s/\.scramble$//` gpg --cipher-algo AES256 --quiet --output "${output_file}" --decrypt "${input_file}" &gt;/dev/null else output_file="${input_file}.scramble" gpg --cipher-algo AES256 --quiet --output "${output_file}" --symmetric "${input_file}" &gt; /dev/null # Don't allow modification of the encrypted file. chmod a-w "${output_file}" fi # Make sure the output file was created before deleting the input file if [[ -s ${output_file} ]]; then # Only delete the input file if we aren't keeping it if [[ ${keep} = false ]]; then rm -f "${input_file}" fi fi
I found a mac to test it on, and the problem was the `^` characters anchoring the regex to start of line. If you remove those it seems to work.
Same example with one line functions: #!/bin/bash pshostnamepid() { sed 's/\(.*\) \(.*\)/jdelouche@\1 "ps -p \2"/';} sshcmd() { pshostnamepid | xargs ssh;} zombie() { sshcmd 2&gt;1 &gt; /dev/null &amp;&amp; : || rm $1;} iszombie() { cat $1 | zombie $1; } fmap() { for f in $2; do $1 $f; done } fmap iszombie "`ls $1/lock* 2&gt; /dev/null`" 
I use a link to change my script (doesn't use GnuPG) behaviour: inpath *crypt Directory '/home/common/bin/': 'TextDecrypt' -&gt; 'TextEncrypt' 'TextEncrypt' Directory '/usr/bin/': 'ntfsdecrypt'
I guess I'd try to highlight just the variable names and browse the file. I just tried the following here: perl -pe 's/\$(?:{.*?}|\w+)/\e[33;1m$&amp;\e[m/g' .bashrc | less
When I try to use this inside a function, it does not print local variables created inside that function.
[Relevant?](https://i.redd.it/fe96ionbykj01.jpg)
So far so good. Consider putting the if statements with different options for the answers to the questions, into a function or two.
Looks fine as long as you're happy to exit the game at the first wrong turn. ;) Also, you could try using `case` which gives you more options: case "$answer" in left) echo "you proceed to the next room" ;; right) echo "you fall down the rabbithole" ;; straight|up|down) echo "you went off the grid" ;; *) exit ;; esac 
Oh i kinda like that, lol you're right cause everytime you go the wrong way. PERMA DEATH lol 
Nice script. I recommend that you look into case statements; the process-parameters function can greatly benefit from them. The show-versions and show-parameters functions can benefit from here-docs. The main script really needs documented exit codes; everything exits 0, even when there is a failure. I recommend that you document the exit codes in the README after updating them. Here are a couple of functions I use to check for packages and handle errors respectively. You might be able to use them: # Check if command(s) is installed. function is_command { local FAILURE for program in $@; do hash ${program} &gt; /dev/null 2&gt;&amp;1 if [ $? != 0 ]; then echo "Command not found: ${program}" &gt;&amp;2 FAILURE='true' fi done if [ ! -z ${FAILURE} ]; then exit 127 fi } # Send message to stderr with a timestamp; $1 is the returned exit status. function err { local EXIT=$1 shift 1 echo "[$(date +'%Y-%m-%d %H:%M:%S')]: $@" &gt;&amp;2 exit ${EXIT} } 
&gt; You might like to run `( set -o posix ; set )` at the start of the script, dump it out to some file, and run it again at the end of the script, dumping it out to another file, then diff the two files? This ended up giving some useful information. A handful of extra variables were output by this command after the function ran (which I assume means they were globally defined), and one or 2 of them *might* explain the inconsistencies I was getting. It also unveiled a couple other additions that i didnt directly set: BASH_REMATCH=() COMP_WORDBREAKS='"'\''&gt;&lt;=;|&amp;(:' Im not sure what exactly these implement, but they might be responsible too. (im sure google knows, but if you happen to know and feel like saving me the trouble then by all means). &gt; So you have a specific script where this behavior is occurring? Yes. Its part of a script that automates backups of my home directory. This script does a lot, but the issue was coming into play at a part where I * take a list of user defined 'exclusion keywords' and, if they include globs expand them (which they did - for example an input of `'*EXCLUDE*'` would expand to `$HOME/EXCLUDE_FROM_BACKUP` * convert+combine then into a regular expression based filter * run `find -O2 "$HOME" -regex '^.*$' | grep -E "$regexFilter"` --&gt; it only outputs the parent directory (full pathname) or filename where the match was found (any sub-directories, which by definition must also match the keyword, are suppressed). * take all matches and generate a string of the form '--exclude=$match1 --exclude=$match2 ...' which gets used in an operation like `cd $HOME &amp;&amp; tar -cf - $tarOpts $tarExcludeStr | someCompressionFunction $compressionArgs &gt; homeDirectoryArchive.tar.ext` The issue was in the `find -O2 "$HOME" -regex '^.*$' | grep -E "$regexFilter"`command - the list of matches varied from run to run. When I started from a fresh restart (I am running bash via WSL, so restarting is just closing all the windows and then restarting WSL, which is very quick) I would get the same "pattern" (e.g., the first 1 or 2 times didnt work, I would manually stop it when it tried to compress a directory that I had excluded since it contained a lot of data, then of try 2-4 it would randomly work worked). BUT, I'd then would put it down and come back a few hours later and, without making any changes to any code, it would work on the first try after startup (e.g., just now I tried it and it ran fine, but when I last opened it / modified the code it wasnt working on the first 1-2 runs. Fucking bizarre, as I cant come up with a good reason why behavior after restarting wouldnt be consistent, unless there is some weird time related bug like one funtion using 24-hour and one using 12-hour so the behavior isnt different depending if it is before or after 12:59.) 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
The `cd ../` is not required and is probably the cause of your problems. One way to find out is to `echo` your active command and see what the variables ultimately amount to: #!/bin/bash for d in */; do echo "$d" folder echo "cp $WORKDIR/submit.slurm $d" cd ../ done You might want to try something more like #!/bin/bash # Don't use UPPERCASE variables unless you know when/why to workDir=/home/x/propane/b2plyp/MRCC_nmp2CCSDT_ccpvqz/cfour/test/tester/freshtest/tester/benzene/tester # Use meaningful variable names for dir in */; do # Favour printf over echo printf -- '%s\n' "Copying to ${dir}..." # Quote your variables cp "${workDir}"/submit.slurm "${dir}/" done
Can you give a few more specifics about the environment, e.g. directory structure, file locations, file naming convention, script to execute, etc? You have a few options that are worth considering: * Brace expansion *may* be possible depending on naming conventions. * Building an array based on directories might work. * Using find with exec and appropriate filters. Changing directories is most likely going to be problematic. You will probably need to 'cd -' to return to the original WORKDIR before the next iteration, or move to pushd/popd.
Type this bash file.txt
Alternatively, you can make your file executable and run it without explicitly calling bash: chmod +x file.txt ./file.txt Another note, a common extension for bash instruction files is `.sh`; it doesn't change anything, but it helps people to know what the file is. Good luck on your studies!
Indeed, those are perfect! I was unaware that perl had built-in commands like that. Thanks for the input :)
Good idea. 
I'm sorry for my ignorance. What's the difference between this and using bash? None at all? Just syntax? 
I'm sorry for my ignorance. What's the difference between this and using bash? None at all? Just syntax? 
AFAIK, not much, actually; one difference is that it allows for you to execute the program easier, if you use it a lot (ignoring the one-time chmod); it allows for it to be added to the PATH variable, to be called from anywhere. It also allows the usage of a shebang, so that the file itself tells which program to use to open it, instead of you having to know a priori. Finally, probably the way they handle variables/contexts/subshells might be different, but I don't the details. That's what I know and recall right now, but I never researched this subject thoroughly to be sure.
It's worth noting to do this you also need a shebang at the top of the executable: `#!/bin/bash`
I second scrip command. Been using it for years.
Be aware of `tee` as well, pipe your output there and it will be printed to the file and the screen. 
You can also redirect stdout and stderr to the file: # Create additional detail logging read _OLD_STDOUT _OLD_STDERR&lt;&lt;&lt;$(lsof -a -d1,2 -p$$ -F|sed -ne "s/^n\(.*\)/\1/p") function __enable_detail_log() { exec 1&gt; &gt;(tee -a $UPG_LOG ) exec 2&gt; &gt;(tee -a $UPG_LOG &gt;&amp;2 ) } function __disable_detail_log() { exec 1&gt;$_OLD_STDOUT exec 2&gt;$_OLD_STDERR } 
In a script, you can use command grouping as a very simple solution for this e.g. # Start capturing everything { command1 command2 if command3 &gt;/dev/null 2&gt;&amp;1; then while read -r; do printf -- '%s\n' "${someVar};${REPLY}" done &lt; &lt;(command3 -v) fi # Output all the things! } &gt;&gt; file.txt And let's say you wanted to use `tee`, for example: } 2&gt;&amp;1 | tee -a file.txt
suggestions: * use a for-loop to iterate over files: https://stackoverflow.com/a/14505622 * use parameter expansion to get first two characters: https://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion * use `mkdir -p` to create directories, no error if directory already exists
It is a little rough, but something like this should do the trick: http://termbin.com/skin You *may* need to update ${SUFFIX} to the proper file extension. Summery of logic used: * For loop. * grep with --only-matching flag for first two characters (^..) * Bash test &amp;&amp; mkdir (to avoid errors if directory already exists) * mv command with errors redirected to /dev/null (in case file has already been moved). Hope this helps.
Sorry to bother you, but how do I actually run this. I am really new to bash. Thank you, bu the way.
Okay, so I paste the file into a text file, change jpg to jpeg, and save it as say, "the-script" in the directory with the photos. I navagate to the directory, and type "the-script" and hit enter?
There's a tool `perl-rename` that I found out can not just rename, but can also move files and create folders. Your distro probably has a package for it. What you want to do could be done like this: perl-rename -n 's/(..).*/$1\/$&amp;/' *.jpeg This would be run inside the folder with those files and would create sub-folders. What the tool can't do is, move the files to a different filesystem. It can only move things inside the filesystem where the files currently are.
I tried that, perl-rename isn't a command I have, could I use rn? Hpw would you install perl-rename?
It is unlikely that 'the-script' will be in your $PATH (the places Bash knows to look for commands), so you will have to specify the location. The location will be your current directory which is referred to as . * Run the file: ./the-script Before that you will need to make it executable. * chmod +x ./the-script I recommend that you copy the entire photo directory to a backup location before doing any of this. That way you easily revert the work if needed.
I am so sorry, I feel like an idiot. I run the second command first, *then* the first?
Yes. Don't forgot to copy the entire directory _before_ running the-script. That way you easily revert the work if needed. 
THANK YOU KIND SIR 😃
 $ mkdir -p 49 &amp;&amp; mv 49?* 49
I would have to do that 256 times though.
You'd probably want the `mv` command to be `mv $file $PREFIX` instead of `mv ${PREFIX}*.${SUFFIX}` ... the second one will do too much work (if there are 1000 files with the prefix 49, you'll move all 1000 files 1000 times, whereas if you go with the first command you'll move each file only once)
Probably the single most common alias I ever set was `alias sl='screen -ls'`.
* Some of my big projects: https://github.com/BruXy/bash-utils * Cheat-sheet: http://gnulinux.guru/ * Most useful command for debugging: bash -xv ./your_script 
https://gitlab.com/moviuro/moviuro.bin
That's no fun... why not install `sl`? :)
I forgot about that!
A simple project to provide me some basic info about my homelab and servers. (https://github.com/arvchristos/hermann-homelab) I got a little bit hyped with dialog and bash :-P
I feel like /r/bash could probably do with a daily or weekly challenge thread, akin to /r/dailyprogrammer Different people are going to have different functions, aliases and scripts in their toolchest. My motivations have usually been based in the fact that I'm a *nix sysadmin by day, and often I'm working on Solaris/HP-UX/AIX hosts that may not have certain tools, or certain functionality for specific tools. So quite often I'm reinventing wheels to make life easier on crusty older unices. For me, a few of my bigger personal projects include: * A mostly-portable random integer script. I'm itching to rewrite this again but need to finish other things first. * A portable password generator. Rewritten several times. * A portable passphrase generator. Rewritten several times. * A mostly-portable replication of the `shuf` tool, incomplete in testing but I'm 95% there. Works under most conditions, I just need to hammer out the edge cases. * I'm in the middle of reinventing `mapfile`/`readarray` though I haven't had much time lately On top of those, my `~/.bashrc` features, among others: * A generic `die` function * A function to convert commas to newlines and, * A function to convert newlines to commas (these two functions see a surprising amount of use) * `isinteger` test function * `trim`, `ltrim` and `rtrim` to strip blank space from a string * `hr` for printing a horizontal line of any number of characters * `indent` for indenting n number of spaces * `epoch` for portably printing out the epoch, for systems that don't have a version of `date` that supports `+%s` * `capitalise`, `toupper` and `tolower` should be obvious what they do * `rolesetup` to build the blank structure for an Ansible role * `checkyaml` to try some basic yaml linting * `up` to go up any number of directories e.g. instead of `cd ../../../` you could type `up 3` * `whoowns` to return the owner of a file/directory. Mostly portable. * A tool named `what`: `what -h` output: $ what -h what - list all users and their memory/cpu usage (think 'who' and 'what') Usage: what [-c (sort by cpu usage) -m (sort by memory usage)] * and of course the usual `extract` and `compress` functions among many others. 
Automatically does an `ls` after each `cd`. function cs() { builtin cd "$@" &amp;&amp; ls; } alias cd="cs" Be sure to put it after any `ls` aliases you have.
Perhaps `xargs -d'\n'` is the problem? xargs by default splits arguments by spaces, not line-breaks. Here's an experiment at the command trying to show what I mean: Example input: $ printf "hello world\nhi hey\nabc def ghi\n" hello world hi hey abc def ghi Now with xargs: $ printf "hello world\nhi hey\nabc def ghi\n" | xargs printf "%s\n" hello world hi hey abc def ghi And with `xargs -d'\n'`: $ printf "hello world\nhi hey\nabc def ghi\n" | xargs -d'\n' printf "%s\n" hello world hi hey abc def ghi 
Worked perfectly, thank you.
Awesome. Glad to hear it!
I will have to give that a shot another thing I was considering is using some sed magic to add the same command to each line and then running it that way, I just figured xargs would be more efficient 
You can try bash itself with a 'while' loop. Something like this: $ testoutput() { echo hello world; echo abc def ghi; echo asjahg agans akjbgkj; } $ testoutput hello world abc def ghi asjahg agans akjbgkj $ testoutput | while read -r line; do echo do something here with: "$line"; done do something here with: hello world do something here with: abc def ghi do something here with: asjahg agans akjbgkj And inside a script, you can add line-breaks for easier reading: grep $DOMAIN $RCPT | grep -v '^ *#' | sort -d | while read -r line; do /path/to/other/script "$line" done
A simple pausable timer to help me sit down and focus for 25 minutes at a time: #!/bin/bash #SET MINUTES (x) HERE x=25 echo "Starting timer now" | festival --tts y=$((x*60)) for ((i=0;i&lt;=$y;i++)); do echo "$i" read -s -n 1 -t 1 key if [ "$key" == 'p' ]; then echo "Pause" while true; do read -s -n1 keyenter if [ "$keyenter" == '' ]; then echo "Unpause" break fi done fi done echo "Time is up, take a break" | festival --tts Mind you I use Festival (multi-lingual speech synthesis system) to speak into my headphones for an added touch. I left the `echo "$i"` so I can peek at where I'm at in the middle of the 1500 seconds.
I made this “powerline inspired plugin oriented” bash prompt; [sbp](https://github.com/brujoand/sbp) nothing spectacular but I really appreciate being able to customize my shell and add information as i need it. 
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/brujoand/sbp) - Previous text "sbp" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20dvbjjy4) 
``` curl -X POST https://localhost/message -H 'Content-Type: application/json' -d /dev/stdin &lt;&lt;EOF { "body": "$1"} EOF ```
add verbose: `-v` and what is the content of `$1` ?
Thank you very much for your help, It suddenly worked after a whole day of trying. Apparently I can't insert parameter if my JSON are wrapped in single quote: -d '{ "body": "'$1'"}' has to be -d "{\"body\": \"$1\"}"
Works for me, too. Thanks! If you have the time (and inclination), walk me through: `{;s/.*: //;h;}'` and: `{;s/.*: //;G;s/\n/ /;p;q;}`. 
I think only double quotes will expand the variable. 
The semicolons are necessary for doing it on one line with the `-e` flags. Here is a script, with comments: #!/usr/bin/sed -f -n /Processor Name/ { # If the line has "Processor Name" in it, then: # Delete everything up to and including the first ": " s/.*: // # h for "hold" the remaining text in the extra buffer h # extra buffer has "Intel Core i5" } /Processor Speed/ { # If the line has "Processor Speed" in it, then # delete the beginning part as above s/.*: // # pattern space has "1.4 GHz" # capital G to 'Grab' the extra buffer and add it to the end G # pattern space has "1.4 GHz\nIntel Core i5" # replace newline with space s/\n/ / # pattern space has "1.4 GHz Intel Core i5" # print current space p # quit q }
With `jq`: curl ... -d "$(jq -R '{ body: . }' &lt;&lt;&lt;"$1")"
Not reasonably, I think. Sure, you can write a script to generate the command grep -E '10|11|12|13|…' and if you start running into the length limit of a single argument (32 pages according to `execve(2)`), split it up into grep -E -e '10|11|12|13' -e '14|15|16|17|…' which might be enough to cover all values you think you’re likely to expect (I’m too lazy to do the maths, but if I read `execve(2)` correctly the size of `argv` is effectively limited by RAM only) – but clearly this is silly. `awk` makes this extremely easy: awk '$1 &gt;= 100' (`{print}` is the default action if you only specify a pattern.)
grep don't do math
A quick find | grep | xargs one liner should get you what you want.
This is probably the best approach, aye.
Use the built-in tests: [ "$val" -gt 0 ] $(( val &gt; 0 ))
I’m not sure I fully understand what you mean, but **`declare -p`** sounds very relevant to your interests :)
maybe use `type` command? $ type bc bc is /usr/bin/bc $ type abc bash: type: abc: not found you can check the stderr and make decisions, for ex: if type "$cmd" |&amp; grep -qF 'not found'; then &lt;do something&gt; fi
You can do it like this: if ! command -v pandoc &amp;&gt; /dev/null; then echo pandoc is missing exit 1 fi
Brilliant, thank you!
&gt; quote each array element, concatenate into string, printf -v qstring "%q " "${myarray[@]}" &gt; get back from that string to original array eval "myarray=($qstring)" Hope this helps. 
don't test the output, test the exit status. if type "$cmd" &gt;/dev/null 2&gt;&amp;1; then .... fi I prefer to just let type print the error messages for the missing commands in one sitting, with a line like this at the start of the script: type curl jq python &gt;/dev/null || exit
thanks, agree that testing the status is simpler and better :) I did realize that seeing @ropid's answer as well... 
If you don't care if there are any alias with that name you can use (I mean you get false positives with aliases): type mycommand &amp;&gt; /dev/null If you want to be sure you should use: hash mycommand 2&gt;/dev/null but hash gives false negatives if a path or function is given
I saw some comments on StackOverflow saying that was a bad option, so not sure. I got it working fine with the recommendation by the others though.
Thanks, this is an elegant solution, eval is not that bad in this context. I've added this method to my [Zsh Native Scripting Handbook](http://zdharma.org/Zsh-100-Commits-Club/Zsh-Native-Scripting-Handbook.html#_tip_serializing_with_bash) as it is quite a gem.
I have to agree that `typeset -p` does data serialization. However a fork is needed to catch that output. `printf -v` doesn't fork. Plugin manager should be fast, so I cannot use `typeset -p`.
Thanks, this is a quite nice method, but forking to base64 tool would be slow.
hers maybe a better page on [getopts](https://sookocheff.com/post/bash/parsing-bash-script-arguments-with-shopts/) &gt;&gt; It makes sure options are parsed like any standard command (lowest common denominator), avoiding surprises. &gt;What does this mean? Also, is the example code for manual parsing well-written and easily adaptable for most scripts? sh (as in just the shell not bash) doesn't have getops, also other shell like programs may not ether so any script with getops would bash only which isn't all that bad in my opinion but you'll have to decide on that. heres a small example while getopts i: opt "$@"; do case "$opt" in i) do_a_thing="OPTARG" ;; esac done vs while [[ -n "$1" ]]; do case "$opt" in i) shift do_a_thing="$1" ;; esac shift # so that the loop will end done 
sorry i should have hade coffee before even trying to type out a post. It could fee like parsing manually would be more flexible but you'll ether a have to do a shit load of work to get your cli syntax convenient or you'll be stuck with very strict syntax with every arg needing to be in order. if [[ "$1" == -f ]]; then file_name="$2" fi enter getopts, all the logic of parsing the cli args and organizing to be looped over is done for you, and whats even better you get fancy stuff like the_script -af FILENAME -o an_option then when you loop over you dont worry about the positions or splitting grouped args just what they mean. case "$opt" in a) echo "this was a" ;; f) file_name="$OPTARG" ;; o) esac you could experiment with sum thing like to see whats happing while getopts i opt "$@"; do echo "$opt" done 
&gt; What does this mean? Most things are based on getopts in one way or another, so if you also use it, the user won't be surprised at how any particular case is handled.
What if you want to use long options? And more importantly, optional arguments? There seems to be [ways to do that](https://mywiki.wooledge.org/ComplexOptionParsing), but they seem unnecessarily complex. Would you recommend manual parsing then?
it looks like its ether or, which is lame, the while loop with shift is rather close to just getopts, but you couldn't do `-abdf` without a lot of work, remember to add `break` with the shift loop if you want args to be exclusive 
[just as an fyi aliases don't affect scripts](https://unix.stackexchange.com/questions/1496/why-doesnt-my-bash-script-recognize-aliases)
 ls -R | grep -i "your search term here" find ./ -name "file name from above operation" -print works if you name the files properly and then fling them into an abyss of crazy nested directories. 
Ok.
What do you want to do and what have you done so far?
'awk' is the right answer here. But if I would have to use grep (if awk was not available), I would create the command in a loop. Lazy: # grep greater than threshold THRESHOLD="250" i="0" DO_THIS="grep -v \"$i\" \"$file\"" while (( i &lt;= THRESHOLD )) ; do ((++i)) ; DO_THIS+=" | grep -v \"$i\"" done $DO_THIS If any number appears only once, another option is to use column+sort+head.
I want to make nano highlight c syntax. Ive Tried writing include syntaxhighlightingfile and various other commands to nanorc and .nanorc with no sucess. 
/r/linuxquestions or /r/linux4noobs is probably a better place for this.
Here's a sneak peek of /r/linuxquestions using the [top posts](https://np.reddit.com/r/linuxquestions/top/?sort=top&amp;t=year) of the year! \#1: [In 1998, Windows-98 used to run with as little as 16MB of RAM, whereas today even a lighter distro such as Xubuntu consumes at least 250MB at rest, can someone technically inclined explain to me what accounts for this bloat?](https://np.reddit.com/r/linuxquestions/comments/7i3cep/in_1998_windows98_used_to_run_with_as_little_as/) \#2: [How is Linux a safe OS (compared to windows) if the code is open source?](https://np.reddit.com/r/linuxquestions/comments/7sthob/how_is_linux_a_safe_os_compared_to_windows_if_the/) \#3: [So I’m finally making myself learn vim](https://np.reddit.com/r/linuxquestions/comments/80hqeo/so_im_finally_making_myself_learn_vim/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
So... what code have you got so far? Or what thoughts have you had on how you're going to approach this?
sorry i cant do better but look in to renamutils, the command qmv will give you output like file_nmae_a.ext1 file_name_a.ext1 file_nmae_b.ext1 file_name_b.ext1 file_nmae_c.ext1 file_name_c.ext1 which you can rename the later by hand like file_nmae_a.ext1 new_name_a.ext1 file_nmae_b.ext1 a_name_b.ext2 file_nmae_c.ext1 the_name_c/with/a/path.ext3 you could also do something like this but way better BACKUP_PATH="${HOME}/unsort" DOWNLOADS="${HOME}/proj/bash" mv_movies() { local movie_ext_array EXT movie_ext_array=('.mkv' '.avi') mkdir -p "$BACKUP_PATH"/{sumthing,anotherthing} for EXT in "${movie_ext_array[@]}"; do find "$DOWNLOADS" -type f -name "*${EXT}" \ \( -name '*sum_thing*' -execdir mv '{}' "$BACKUP_PATH"/sumthing/ \; \) \ -or \( -name '*another_thing*' -execdir mv '{}' "$BACKUP_PATH"/anotherthing/ \; \) done } mv_movies 
You should always quote the `"$@"`, and then it should work if you pass `"$@"` from the root level to `myfunc` and from `myfunc` to `./script.sh`. $ cat script.sh #!/bin/bash for argument; do printf '%q\n' "$argument" done $ cat tmp.sh #!/bin/bash function myfunc { ./script.sh "$@" } for i in {1..3}; do myfunc "$@" printf '\n' done $ ./tmp.sh a "" "b c" $'d\ne' a '' b\ c $'d\ne' a '' b\ c $'d\ne' a '' b\ c $'d\ne' 
 # file: script.sh; run as ./script.sh args argument=$@; myprogram(){ echo "$argument" my_other_program($argument); } myprogram my_other_program(){ more_argument=$@; echo " $more_argument" }
I forgot to add, myfunc takes some parameters itself, so calling $@ from myfunc will mean all of its parameters will be passed to ./script.sh. I will edit the post to reflect that. 
Sorry, but I don't follow. 
ahh, I can pass to myfunc param1 param2 and "$@" then in myfunc, I can do shift 2 ./script.sh "$@" 
Thought I'd share this here since I found it pretty handy. All comments welcome. Everything hosted [@ github-gist](https://gist.github.com/Enteee/c8c11d46a95568be4d331ba58a702b62).
What about that line messes you up? 
The script is trying to tell you what's wrong. Here's what you get if you remove the `2&gt; /dev/null`: stat: cannot stat '%{fname}': No such file or directory The % was supposed to be $. 
Ugh...thanks for pointing that out. Of course I overlook something simple like that.
I always use getopts when writing scripts, it's similar to the C getopt interface, so if you intend to ever write C, it would be good to be familiar with it. I personally never use long options, I feel that they're unneccessary, and they probably stem from bloated GNU tools which seem to love them for some reason. Why anyone thinks `--help` is any easier to remember than `-h`, I can't understand.
Hey fellow arch linux njit brother, how have you been? I use getopts. /thread
Did you know there is already a `:` in sh and bash? It is a different name for the command named `true`. It is used whenever you want to do nothing in some spot, for example: while :; do ... done Can you change your `:` to make it behave exactly like that when called without parameter? It should instantly return when called, and `$?` has to return zero. You can read the documentation of the existing `:` by typing: help ':'
What you describe should work. If you want more help, show what your file looks like exactly. As an example, my file that correctly enables syntax highlighting for all kinds of stuff looks like this: http://ix.io/UpT The path names you see there is where the stuff is installed here for me.
The path to the syntax files are the right ones ive tested, but nano ignores the config fild. I think its because i installed it from homebrew ontop of what came with my system and the homebrew nano has its own config file, how do i locate that file
This shouldn't be the case. There's a description how things work in `man nanorc`. If you have a file `~/.nanorc`, it should be looked at by both of your installed nano programs.
Well it doesnt :/
Why not just use xargs? e.g. `xargs -n 1 -P 10 echo &lt;&lt;&lt;'1 2 3'`
I would check the contents of that syntax highlighting file you are trying to load. Maybe something about that is not how it should be. Here's how the insides of the "c.nanorc" file I'm loading looks like for me: http://ix.io/UqX 
This `:for` thingy can run functions you have defined in your script which xargs can't do.
I'd try using perl just because I like it. I came up with this here: $ perl -pE '/^\s*$/ or (/^Page/ and $count = 0, s/^/#/) or s/^/++$count . ". "/e' testfile #Page 1 1. Foo 2. Bar 3. Bat #Page 2 1. Foobar 2. Barfoo 3. Batfoo I have a hunch 'awk' would work for this as well and would look kind of similar.
Hurm. When I run this on the actual file, I get continuous numbering ... 
I edited my post and added a version using 'awk' just now. About your different output, maybe your input is different? I got my testfile by using copy'n'paste on that code you had put in your post.
Did your edit come before mine? :) 
For the perl solution, perl has a "paragraph mode" enabled by a `-00` parameter which solves that: $ perl -p00E '/^\s*$/ or (/^Page/ and $count = 0, s/^/#/) or s/^/++$count . ". "/e' testfile #Page 1 1. This is actually one line 2. And this is another one I don't quite know what to do about the awk solution.
Thank you, mostly because I was not aware of `xargs -P`. But your solution does not solve what I need. [From my github-gist comment](https://gist.github.com/Enteee/c8c11d46a95568be4d331ba58a702b62#gistcomment-2374995): &gt; An similar implementation [posted on reddit](https://www.reddit.com/r/bash/comments/83272g/for_syntactical_sugar_for_your_bash_parallelism/dvf5wht/): ``` #!/usr/bin/env bash set -ex msg="You should see this three times" :(){ i="${1}" &amp;&amp; shift echo "${msg}" sleep 1 if [ "$i" == "1" ]; then sleep 1 elif [ "$i" == "2" ]; then false elif [ "$i" == "3" ]; then sleep 3 echo "You should never see this" fi } &amp;&amp; export -f : &amp;&amp; xargs -n 1 -P 0 bash -c ': "$@"' -- &lt;&lt;&lt;'1 2 3' || exit $? echo "You should never see this" ``` &gt; Problems with the `xargs -P` alternative: &gt; * Spawns a new shell, which means: &gt; * does not preserve shell options: `set -ex` &gt; * does not forward variables (see `echo "${msg}"` in new usage) &gt; * The third process won't be stopped from printing `You should never see this` &gt; * Does not retain exit code (returns `123` instead of `1`) ``` $ ./usage.sh + msg='You should see this three times' + export -f : + xargs -n 1 -P 0 bash -c ': "$@"' -- You should never see this + exit 123 ```
This forks twice: &gt; `( ${f} "${arg}" ) &amp;` You'd be better off with: &gt; `{ ${f} "${arg}"; } &amp;` 
To preserve shell options: `xargs -n 1 -P 0 bash -c ': "$@"' $- --`
Amazing, this works. Thank you!! 
To kill the process group on first failure: `( xargs -n 1 -P 10 bash -c ': "$@" || kill '$$ -$- --) &lt;&lt;&lt;'1 2 3'`
&gt; What does this mean? It means your program will behave like a canonical Unix program. For example, in tar, all of the following flags for verbosely extracting an archive file are equivalent: * `-x -v -f file` * `-xv -f file` * `-f file -vx` * `-xvf file` If they behave differently, it's surprising and therefore not good behavior. Similarly, `rm -- -f` should delete a file named `-f` and NOT enable the "force" option. NONE of these flag combinations should behave the same as `rm -- -f`: * `-f --` * `-f-` * `-f` If your manual parser handles all of the above, it's a good option parser. The example from the page doesn't, but clearly states that this is an exercise to the reader. `getopts` is POSIX and therefore as portable as it gets, but it doesn't handle long options.
I agree double forking is not ideal. I was worried that something in the child shell might have an impact on the parent. I tried to find a case, but couldn‘t. So if you can‘t think of anything I‘ll change that.
No, `continue` inside the function won't work. Your structure may need some changes. ;) You'd have to either perform your test outside the function, or test inside the function and return an exitcode, then test the exitcode of the function and `continue` if required.
No, `continue` inside the function won't work. Your structure may need some changes. ;) You'd have to either perform your test outside the function (only entering the function if required), or test inside the function and return an exitcode, then (back in your `for` loop) test that exitcode and `continue` if required. `continue` just restarts the `for` loop without further processing. 
&gt;continue jumps back to the first line of your for loop without further processing. Even if it's called from inside a function? 
No. `continue` only works in the `for` loop. Once you jump into a function, you're (temporarily) not in that `for` loop - until you exit the function. 
So what the heck does calling continue from within a function do?
Have you tried it? 
Why would you use `continue` within a non-loop?
because you want to skip over the rest of the function if some condition is true :) I am guessing you can use something else for that?
So, what you're wanting to do is peform a test within the function, then exit the function early. But as you do so, return a value that can be tested. An example function called `sample` that checks for the existance of your fstab: sample() { [[ ! -e /etc/fstab ]] &amp;&amp; return 1 echo "fstab exists so I'll keep processing this function" } In your main script, call the function then test the exitcode with: sample || continue *Note: this is not the shortest way to do this - it's purposely long to show how this would work.* 
pure bash: for a in {0..9} {a..f}; do for b in {0..9} {a..f}; do echo $a$b.example.com; done; done