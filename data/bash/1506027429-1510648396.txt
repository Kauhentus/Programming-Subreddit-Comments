Your mom is ugly. A true statement doesn't mean a shareable one.
Kinda late to your post, but here goes. bash has two ways to make a function, the second one in my example is more portable. Only use one though. function foo() { ... } foo() { ... } Also: `if []; then`, can be made shorter with: [ "$foo" -gt "$bar" ] &amp;&amp; { foobar+=("$j"); } Often when I read an 'if' I assume an 'else'. Perhaps also test what the $1 is, or perhaps check the var: `[[ ! -n $var ]] &amp;&amp; { return 1; }` My 2 cents :)
Another suggestion for connections that go down due to quality is mosh. It can tolerate a connection even through complete drop outs as well as poor latency. It works in conjunction with ssh. Sadly although there's mosh clients on Linux and Android (JuiceSSH) there's no mosh support in putty which is irritating if you use Windows a lot. I guess there's always cygwin... 
There are many different types of system information.. If you are on Linux though, "sudo lshw" lists all the hardware specs and a little more (I think.. Im not on Linux so I have no clue). You might want to narrow your question a bit.
&gt;Also, any tips on improving the script? It's my first one ever. Anything from logic to syntax to coding style to make it less buggy and more clean and readable. It appears to work as intended. More comments, replace those `echo` calls with `printf`, use 2 space indenting (this will allow you to do away with that line-wrap, BTW kudos for sticking as close as possible to an 80 column width). And if you pass it through Shellcheck, it'll tell you to switch `printf "%s\n"` for `printf '%s\n'`
&gt; It's only just occurred to me in teaching one of the juniors just now that I don't actually know how it works. It's scary that you're training 'juniors' when you're so junior you don't know the basics of ssh or bash.
For windows, use XShell - It's SO much nicer - It has has a keepalive timer that you can custom set that will keep your shells working properly :D Edit: [Link to XShell](https://www.netsarang.com/products/xsh_overview.html) - It has a Buy Now option but it is free for "personal" use. 
Everyone starts somewhere - Depending on the size of his shop, he might not have a need for these tools at great length - My first IT job was pure windows with some Linux timeclocks - All we did to support them was image them and run a few scripts so we never had to go beyond the scope of the utter basics. Point is that what is junior in your mind is arbitrary; the position that he holds will have different requirements. Don't be judgmental. 
&gt; Depending on the size of his shop, he might not have a need for these tools at great length I assumed someone with red hat in their username would know the fundamentals of interactions with Linux servers. &gt; . Don't be judgmental. But this is the internet. Judgmental is as prevalent as TCP.
&gt; I assumed someone with red hat in their username would know the fundamentals of interactions with Linux servers. The fact that you claim to be decent with Linux and assume things means you're nowhere near as competent as you think you are. 
i know this but thanks if you interested in this please see [this]( https://www.reddit.com/r/Codings/) this is my subreddit
&gt; The fact that you claim to be decent with Linux I don't recall ever making such a claim. &gt; and assume things means you're nowhere near as competent as you think you are. Knowing the fundamentals of bash and ssh doesn't make someone competent at Linux, but it is a requirement to be competent. Again, I don't recall claiming to be decent with Linux.
&gt; Again, I don't recall claiming to be decent with Linux. Then how can you say who is junior and who is not? Bad troll is bad. You've contributed nothing to this conversation and your comment history indicates that you're a general waste of everyone's time. If you have some free time, which I am sure you have a lot of, I'd suggest some soft skill courses on how to not be a waste of space. 
&gt; Then how can you say who is junior and who is not? Sounds like you're the one making ~~an ass~~ assumptions
https://media.tenor.co/images/409e8c4ecbf804e3282c96e02a0ad523/raw
Lol. Sounds like your being judgmental again, just like you told me not to be. Funny how you seem to do that time and again over our multiple interactions.
&gt; sxhkd -c ~/.config/sxhkd/sxhkdrc &lt;( cat &lt;&lt;'EOF' sxhkd -c &lt;( cat &lt;&lt;'EOF' I suspect you didn't intend both of these to have a heredoc. Especially not with the same delimiter word.
 xz -zkvv -F lzma -C none --lzma1=dict=$( size=$(du -b "$FILE" | cut -f1); (( size &lt; 1610612736)) &amp;&amp; echo $size || echo 1610612736),mf=bt4,mode=normal,nice=273,depth=4294967295 (FILE)
Thanks for pointing that out! I decided to write the config myself instead of copying and pasting chunks and that may have been a poor decision.
Trying to set up on a Debian 9 system in Hyper V. I am trying to create an authority server so the company I'm working for can sign internal automation code. Active Directory's software restriction policy has all code execution disabled on our domain except for our white list of certs and hash tables. There are a whole bunch of internally created scripts that currently won't run without a domain admin and we want to avoid caching credentials even on our secure automation servers. I believe I was in the top level of the openssl directory which was sitting on the top level of my user folder. Is this correct? I scrapped that VM because I switched workstations and the older machine I'm on now doesn't have HyperV support. I'm loading a virtual box VM now.
&gt;[v3_ca] To &gt;[ v3_ca ] Any difference? 
Sadly, no. That was one of the first things I tried. It seems to make no difference on any of the config headers.
Setup an API or drop your scripts in cgi-bin, either way, web server. 
He did clearly say it was his first script. Try giving some constructive criticism instead of just telling him how bad it is. He at least demonstrated some knowledge of scriptable commands, which you have not.
I could swear the text said "a friend of mine created this" before so I thought better warn him about it. But I looked again and it reads "my first one" so I agree with you. 
lol Well did.
In that case I retract my previous comment supplanting it with "It was very nice of you to warn him about that." Carry on.
Do a pwd for me please. You should be in /etc/pki/tls when issuing the openssl command. Even though the guide is a couple of years old, I still reference this one occasionally: https://jamielinux.com/docs/openssl-certificate-authority/index-full.html
Explore the /proc directory. Lots of great information about processes. Great for gdb 
From a ‘man grep’ -f file --file=file Obtain patterns from file, one per line. If this option is used multiple times or is combined with the -e (--regexp) option, search for all patterns given. The empty file contains zero patterns, and therefore matches nothing. (-f is specified by POSIX.)
When you say you want to join the files, are you trying to create a third file which sorts lines from both together? You can probably do that with 'cat file1 file2 | sort &gt; file3' Might not be exactly what you want, but those two lines will appear next to each other and the files will be merged.
#thanks!
That's exactly why I'm confused. It confuses me how it's passed in.
Easiest is to make file1's format "Short ID, Info" and file2 "Short ID, Long ID, Name". Then you can join on the first field with a custom output format of Long ID, Name, Info (`join -o 2.2,2.3,1.2`). Converting Long ID to Short ID can be done with sed. I don't see tabs in your example, but it looks safe to just make the first space a tab with sed as well. And I guess it needs to be sorted and the tabs converted back to space. join -t $'\t' -o2.2,2.3,1.2 &lt;(sed 's/ /\t/' file1.txt | sort) &lt;(sed 's/ /\t/; s/^\([^\t]\{,8\}\)\([^\t]*\)\t/\1\t\1\2\t/' file2.txt | sort) | sed 's/\t/ /g' 
I wouldn't say that, I use &gt;&gt; (append) all the time when writing shell scripts. It helps when experimenting with certain commands, like bullet points etc.
Thank you for the suggestion. I have a feeling it's a 'workaround' from secure servers. Timeouts etc.
Create a temporary file with the same content as File_Two, but with the username truncated to 8 characters (read line from File_Two, split in 'username' and 'rest', do string magic on username, echo "$short_username $rest", repeat until done) Then join File_One and Temp_File_Two on field 1. Remove Temp_File_Two Except for the `join`, everything can be done with `bash`. edit: have a look here: http://www.thegeekstuff.com/2010/07/bash-string-manipulation/
This should do it, looking for jpgs three levels deep and using bash with to `mv` to its respective `..`. You can put `echo` before `mv -vit` to see what it will do. find -mindepth 3 -iname '*.jpg' -execdir bash -c 'mv -vit .. "{}"' ';' edit: That is I think you mean 3 levels down e.g. $ find -mindepth 3 -iname '*.jpg' ./a/c/f.jpg You can also add `-maxdepth 3` to avoid moving other jpgs: find -mindepth 3 -maxdepth 3 -iname '*.jpg' -execdir bash -c 'mv -vit .. "{}"' ';'
Check out kdialog It's in kde-baseapps 
That was... glorious. It's a very nice feeling to see 10 hours of work being performed before your very eyes in ten minutes. Thank you.
I’m personally still confused as to what you wanted. I’m happy to help if you narrow the question a bit.
Something like this: &gt; \#!/bin/bash &gt; echo "What's your name?" &gt; read name &gt; echo "Hello" $name Also this link should help you out a bit: http://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_08_02.html I'm not sure how to help you with the graphical input though, sorry. edit formatting stuff
You can also use a for loop: for f in */*/*.jpg; do cp $f ${f%%/*}; done
Thanks for the tips. I'm on Arch and with latest version of shellcheck (0.4.6) and it doesn't tell me to switch to single quotes for `printf`. Also, I checked this [page](http://wiki.bash-hackers.org/commands/builtin/printf) and I don't see it use single quotes so I'm wondering if this specific script benefits from single quotes. And if you know what the script does and/or use youtube-dl, sxhkd (or the like), and xdotool, then maybe you might know why I sometimes need to restart the script to get it working. Basically the first part of the script runs an instance of `sxhkd`, which is a keybinding application, and feeds it a config file that modifies a hotkey combo that triggers a separate script that uses xdotool to automate mouse/keyboard clicks). The usage would be: run the script, then hover the mouse over any URL link on Firefox, do a `ctrl + space` and it will automatically append that url to a text file. On occasion, after executing the script `ctrl + space` doesn't copy the URL and I have to `ctrl + c` to terminate it and restart until it works--I think it might be an issue with the process not terminating or starting in a timely manner so I would have to do some checking to make sure enough time is given for processes to start/end but I don't know where the culprit is specifically. Doubt you use these tools to have an idea but maybe someone else knows.
Pipe the overall output to | sort | uniq 
You could pipe it into a sort -u command to find only one occurrence of each result. 
thanks! worked!
`sort -u` does the exact same thing, without an additional pipe to `uniq`.
&gt; I'm on Arch and with latest version of shellcheck (0.4.6) and it doesn't tell me to switch to single quotes for printf https://github.com/koalaman/shellcheck/wiki/SC1117 is the specific gotcha
Thank you. 
Thanks, good to know :)
Or if order is important, pipe to: perl -pe '$_ = "" if $seen{$_}++'
&gt; if i type in "cd" it becomes "~". That makes it sound like /home/marksteve4 is not your home directory... What is the output of `echo $HOME` ?
According to the man page for bash under the Pompting section \w the current working directory, with $HOME abbreviated with a tilde (uses the value of the PROMPT_DIRTRIM variable) \W the basename of the current working directory, with $HOME abbreviated with a tilde You can see your current PS1 by just doing `echo $PS1` 
Or the output of `pwd` when the prompt is a tilde `~`
Surprised no one has mentioned regex101.com - it's an excellent tool for testing your regexes.
Part personal utility, part training exercise. Any suggestions for improvements? (You can spare yourself the `shellcheck`, that much I can do by myself :P )
If it's sftp, you should be able to do scp ftp.whatever.com using your ssh username and password. If this works, you can use rsync. ex. rsync -avz -e ssh --remove-source-files yourname@host.com:/source_directory/file destination_file This will remove the file(s) from the source when they're copied to your destination.
I would rewrite if [[ $stat == $'END\r' ]]; then break fi if ! [[ $slabnum =~ ^[0-9]*$ ]]; then continue fi etc. to [[ $stat == $'END\r' ]] &amp;&amp; break [[ $slabnum =~ ^[0-9]*$ ]] || continue style, like in line `4`. I think the `if..then.fi` is too dramatic. Other than that: I like your bash script. It does a job with clean code. You have good script skills Now I have to look into the `/dev/tcp` thing
&gt; I'd like to make sure I'm moving those files from the sftp so they are not still existing, and log somehow the files downloaded. Like /u/darkciti mentioned, rsync is perfect for this. -v gives you the files that are processed. https://linux.die.net/man/1/rsync shows the following option: --remove-source-files &gt; Another useful thing I can think of is somehow datetime the files and put them to an archived folder based on date and time You could make a directory and use that as the destination for rsync: mkdir /your/dir/$(date +'%Y%m%d')/ So something like this will show you which files are processed. By using redirection you can save the output to a logfile. So at the end you can use something like this: rsync -avz -e ssh --remove-source-files yourname@host.com:/source_directory/ /your/dir/$(date +'%Y%m%d')/ &gt; archiver.log
Yup. And to expand on it even further, you can try it with dry-run if you only want to see what it will do without actually doing it. rsync -avzd -e ssh --remove-source-files yourname@host.com:/source_directory/ /your/dir/`date +%Ymd`/ &gt; /tmp/archiver.log.txt 2&gt;/tmp/archiver.err.txt . 
Hm, I’m not sure about that… to be honest, I just tacked on the `|| exit $?` in line 4 after I noticed that the script didn’t behave very well when the connection failed. I think the `if..then..fi` version is more readable, even though it also takes up more space. But you’re right, it’s not very consistent in the current version.
Good point on the 2&gt;. I would like to mention that using $() vs `` is preferred nowadays.^[1](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02)
Yeah, I know. I'm just lazy. :) 
What is $@?
Nice! I'll be checking this out tomorrow at work! I love the fact it's simple. Thank you!
You can use [process substitution](http://mywiki.wooledge.org/ProcessSubstitution) to put the `grep`, but not the `while` loop, into a subshell: while read line; do : ... done &lt; &lt;(grep views index.html) This is not POSIX, so you should make sure to use `bash` rather than `sh` in your `#!` line.
This makes me giggle if I understand this correctly. The scripter in me wants to say quote the damn variable expander..
I didn't fully understand what's going on in that file. For that large, single file, to get the last entry, I would first reverse the file with `tac`. The output of that, I'd send to sed or awk or perl to cut until that one message you mention that comes between the different runs of the tree commands. After that, you can reverse again with another `tac` before sending it to grep. Just awk or perl might work without tac but I would have to experiment a bit to say how. About cutting stuff, in for example sed's scripting language, it has an address you can put in front of any of its commands to select just certain lines of a file. In the following example, sed prints the stuff it reads by default, and it quits if it runs into a certain line: tac &lt; logfile | sed '/^Date indexed:/ q' | tac You then use grep on the output of that. About creating logfiles for each date instead of one large file, make sure you use a file name that can be sorted without much work to be able to find the last file. That `$(date +%Y-%m-%d_%T)` you mentioned should be good. If your file names are like this: ~/logs/index-"$a_drive"-$(date +%Y-%m-%d_%T) Then you might use this to find the file with the most recent date in its name: find ~/logs/ -name "index-$a_drive-*" | sort --reverse | head -n 1
All the arguments passed into the script. It's like individually listing each argument with a space separating them, but for an unknown number of args
If you installed python: Type `python` in bash Or Type `python script.py` in bash. If you did not already install python you have to do that for, but I don't know about your OS.
no, i asked how to run python inside a bash file.
Exactly, you 'tacked it on'. Do a thing, else do another thing. Concise, brief, readable. All through your script you minimalize, write compact code. Try it, before you know it, you'll get annoyed by clunky `if..then..fi`
Well now you did... Edit : lmgtfy https://unix.stackexchange.com/questions/184726/how-to-include-python-script-inside-a-bash-script
yes
&gt; Well now you did... Lol. Why can't people ask what they mean?
awesome! I'll give it a try, is there a way to statically defined the password within a task in this? because I would like to run this as a task. If I read the prior posts correclty, you can make a directory with the "mkdir /your/dir/$(date +'%Y%m%d')/" command replacing with the corresponding directory, and achieve the same thing with rsync via ssh with the "rsync -avz -e ssh --remove-source-files yourname@host.com:/source_directory/ /your/dir/$(date +'%Y%m%d')/ &gt; archiver.log"
I'm not sure about that. You can use ssh keys to connect without a password, but I'm not sure about passing a password on the command line.
I don't understand. You mean like `ls` **-al**? Those are usually referred to as flags, or more broadly, arguments, and are generally different for each tool
If you are looking for the test operators available in the `[[` keyword, and `[` and `test` commands, run help [[ help test
I think arguments are $1, $2, etc passed from the command line. The "dash" ones I think are called simply options or modifiers (?).
In a script, `$` represent the arguments the user passed in
That's why I think OP is referring to what the manual calls "options". $* is the full args, $@ an array of args, and $# the number of arguments passed. I don't think $ by itself does anything.
They are called options. If you want to find out what options can you pass when executing bash, try: man bash
 man dash if you want to be specific to dash for some reason.
TIL dash = sh 
Back in the days the ~~old farts~~ sages would say RTFM to the youngsters that came blundering in on their systems. The M in RTFM comes from the manual pages that are available on your OS. There you can find useful information about almost every command (and its options), system calls, libraries and config files by issuing: man &lt;keyword&gt; For example `man man` shows everything about the `man` command: MAN(1) Manual pager utils MAN(1) NAME man - an interface to the on-line reference manuals SYNOPSIS man [-C file] [-d] [-D] [--warnings[=warnings]] [-R encoding] [-L locale] [-m system[,...]] [-M path] [-S list] [-e extension] [-i|-I] ... Sometimes a bit hard to read (the option `-l` in the [Solaris manual page for kill](https://docs.oracle.com/cd/E26502_01/html/E29030/kill-1.html) is hilarious), but everything you might want to know is there. If the text makes sense, read it, if not, skip it. A manual page is divided in paragraphs, starting with NAME and SYNOPSIS, options are described in paragraph OPTIONS. Man pages often have EXAMPLES to get you started, and related manual pages are listed under SEE ALSO, where you might find manual pages about config files. The pager (the tool that displays the page one screen at the time) has a few tricks. Type `h` and you'll be presented with a summary. More info of course with `man man`, where you search for pager with `/pager`, to find that the pager is called `more` (or `less`, it depends), so you type `man more` etc. It's how we sunk time before stackoverflow and wikipedia. So, RTFM. For example `man`, `dash`, `ls` and `crontab`. That last one has 2 manual pages: `crontab (1)` for the command and `crontab (5)` for the file. Now you can figure out how to display the page for the crontab file yourself. And your dash commands. Newer executables often have their own built in help function. Then you start with `&lt;command&gt; help`, for example: $ docker help Usage: docker [OPTIONS] COMMAND [arg...] docker [ --help | -v | --version ] A self-sufficient runtime for containers. Options: ... Run 'docker COMMAND --help' for more information on a command.
I believe it's technically Debian Almquist shell. Which is a clone of a clone of sh.
I saw this a little while ago, but I haven't got around to trying it out yet. Does anyone have any feedback?
You should definately try it out!
FYI, plain POSIX shells can do this too, a little more awkwardly, using a here-document containing a command substitution. while read line; do : ... done &lt;&lt;EOF $(grep views index.html) EOF 
Can you elaborate a bit? Not quite sure what you are looking for. Stab in the dark? Tint2 is a panel with "executors" that can execute and display the results of a script.
I need a front end interface that will support Bash. Kind of like PyCharm with Python.
&gt; I need a GUI that supports Bash scripts. My terminal is a GUI that supports bash. &gt; What would you suggest?? Try asking a better question. http://catb.org/esr/faqs/smart-questions.html
Oh, well the only thing I've ever heard of was Basheclipse, I think it's a plugin for the Eclipse IDE.
Thank you. I'll look into that.
English may not be the native language, so nuanced thought is just as important as better questions.
It's possible to build a useful tui with dialog.
You sound like you're asking for a GUI _editor_ for bash scripts. Is that what you mean? An IDE, like PyCharm, as /u/PrB1313 said, can help. The Jetbrains IDEs also have Bash support through plugins, so give that a try.
True, but I did answer the question.
Hello, I mean stuff like -eq, -ne, -lt, -gt. I'm trying to find a reference site for commands like these. 
I believe there is a bash plugin for PyCharm. Try opening your bash script from within PyCharm and it *should* prompt you to install the plugin.
The simple way would be to look into tools like `zenity` and `yad` which will easily give you graphical dialogs from bash scripts. To build a more advanced GUI in bash, look into [GTK-server](http://www.gtk-server.org/intro.html).
If you're building a program complex enough to need a GUI, you should no longer be writing it in bash. Port it to python/ruby/perl/tcl/go and use one of their toolkits, and you'll find not only much more support for what you're trying to do, but much better support for data structures, types, and all the other things that make life easier.
Oh, then /u/DoesntSmellRight gave the answer. Check out the manual page for test.
I always look that up by doing this at the bash prompt: help test I like the result of that better as a reference compared to looking up things online. I'm experimenting with that stuff at the command line anyways before putting it in a script. The things you see printed by `help test` in bash generally also apply to dash. You can find dash's own reference by doing: man dash Then typing exactly this here to jump to the right part of the text: /^ *test expression
Pycharm is an IDE? Are you saying you want an IDE with bash support?
I mostly use [Sublime](https://www.sublimetext.com/3) with [Shellcheck](https://github.com/SublimeLinter/SublimeLinter-shellcheck). It's not a full fledged IDE but if you need that, then Bash was probably the wrong solution for your problem.
No, unless they reference each other.
See if running this on one example file shows the output you want: perl -ne 'print; print if s/Serial\.print/Serial1\.print/' filename It should print the edited contents on screen (without changing the file). If you like what you see, you can apply it to a bunch of files like this: perl -i~ -ne 'print; print if s/Serial\.print/Serial1\.print/' filenames This will edit the files. It will save a backup of the original files with a `~` added to the end of the filenames.
Make mine 1000000, never reach it before a new distro is installed so maybe 50000 would be enough. I use Ctrl+R or history|grep to find those very complicated and long forgotten commands done with ffmpeg, cmake or other little used commands.
That worked perfectly and it is definitely time for me to read the man page for Perl since that single line was a lot easier than writing a bash loop to do the same thing. 
I get the feeling there's a more specific real question going on here. What's the problem exactly?
Add tab completion for ssh, telnet, nc, ftp, ping, dig and host: _ssh() { local cur prev opts COMPREPLY=() cur="${COMP_WORDS[COMP_CWORD]}" prev="${COMP_WORDS[COMP_CWORD-1]}" #opts=$(grep '^Host' ~/.ssh/config | grep -v '[?*]' | cut -d ' ' -f 2-) opts=$(awk '{print $1}' ~/.ssh/known_hosts) opts=`cat ~/.ssh/known_hosts | \ cut -f 1 -d ' ' | \ sed -e s/,.*//g | \ grep -v ^# | \ uniq | \ grep -v "\[" ;` COMPREPLY=( $(compgen -W "$opts" -- ${cur}) ) return 0 } complete -F _ssh ssh ping dig host telnet nc To use: # ssh something&lt;tab&gt;&lt;tab&gt; It reads hostnames from your authorized keys file for tab completion.
Probably a better idea to put this into your .bashrc, so it'll only take for interactive sessions. Ubuntu sets the default value to 1000 in the base, which will truncate your history if you set it larger in your .bash_profile and then source your .bashrc
the answer to the 100% quality requirement is going to depend on the containers and codecs in question, but if the goal is automation, the solution is almost certainly going to involve ffmpeg.
I use Fedora, but this is good advice for those who use Ubuntu.
This. ffmpeg allows copying streams (no re-encoding), is scriptable and available on nix.
Having a long .bash_history file is not a very good idea, this will make booting bash ever slower with the growing of the file. Instead you could add this to your .bashrc: export HISTTIMEFORMAT="%h/%d/%y - %H:%M:%S " export HISTCONTROL=ignoreboth export HISTSIZE=5000 shopt -s histappend PROMPT_COMMAND="${PROMPT_COMMAND:+$PROMPT_COMMAND ; }"'echo $$ $USER "$(history 1)" &gt;&gt; ~/.bash_eternal_history' And never loose a command again. 
&gt; Ideally it can be accurate to the frame (or as close as technically possible without re-encoding, which degrades quality) As far as I understand, cutting to the frame without re-encoding won’t get you the desired result unless you cut right before a keyframe. (I tried this a while ago, and the result looked broken for the first few fractions of a second. In the end I had to reencode it.)
Did you tried with webm?
I think someone has worked on support. (Psst; It’s this guy ^) 
is there a way in which i can regularly export my command history to dated text files using crontab?
I think using pure bash is not practical here. Try [ansible](https://github.com/ansible/ansible) to execute the same task on a lot of machines. Either use a playbook or just run a command on your server inventory like ansible -i hosts.txt -m shell -a "ps aux" 
&gt; if [ $# -ne 1 ] You're only accepting exactly one argument (ne = not equal). Change this to `if [ "$#" -lt 1 ]` (lt = less than; note expansions must always be quoted with `[`). Since you're running bash, you can also use the arithmetic command, `if (($# &lt; 1))`. Also, `for name in $*` is wrong. You want `for name in "$@"` (or its shorthand, which is simply `for name`). In general, unless you know what you are doing, you always want to use quoted `"$@"` to get the positional parameters. 
No, ssh does not support cidr ranges, but nothing stops you from using a `for` loop. Using an ssh key is necessary though. If you have your ssh public key installed on all the hosts, you can run them all as background jobs for performance. for ((ip=0, ip&lt;=255; ip++)); do ssh "192.168.1.$ip" "your command here" &gt;"/some/directory/output-$ip.log" 2&gt;&amp;1 &amp; done wait This works best with a dedicated passwordless key. If your private key has a password on it, make sure to authenticate using `ssh-agent` before running this. 
XY problem? If the problem is how to manage a fleet of machines, go with u/0bp 's solution. If it's just a one-shot operation, go with u/McDutchie 's.
Yea, that's what I understand--I prioritize full quality over exact frame but still want as accurate as technically possible. What software did you use to trim and what's the workflow? For example, people have been saying ffmpeg. So it is a CLI tool and what I need to do is use a video player like MPV, write down the start and end time I want to trim and pass that as arguments to ffmpeg to let it trim? How could I determine the keyframes and specify where to cut to maintain 100% quality while minimizing the number of undesirable frames? Seems like a GUI frontend for ffmpeg makes more sense, but I haven't found one that's proven to be suitable for my needs yet.
You don't want to know the horrors of mid 90s dial up support, or proprietary cash register support
And if you're not using a key (which you should be), you can input a password with either [sshpass](https://sourceforge.net/projects/sshpass/) or expect.
This. OP, don't bother doing this in BASH. It's not the right tool for the job.
Thank you. I've never used ansible and am looking at it right now. If I made a host.txt file and placed cdir ranges will it know what that means? And will it ask me for ssh username and password for each machine? Thanks for being noob friendly
[removed]
Something like 'pssh' might work out well for you. pssh -h hosts.txt -l &lt;user&gt; -t 300 -o &lt;directory&gt; rpm -qa This will dump all output into text files in that directory where the name of the text file will be the hostname of the server connected to. If you don't have keys on all the machines, you can do -A which will ask you for a password for each machine it connects to. If you have different usersnames for each machine, you can make a hosts list reflecting that. hosts.txt user1@host1 root@server jack@bastion:69
BASH is always the right tool. Renounce your heresy at once!
Lolk, this is literally the job Ansible was invented to do, but don't mind the advice of the professional sysadmin.
Ansible is great but Saltstack has built-in modules for this. `salt ‘*’ pkg.list_pkgs` Done.
Are keys needed or can I set it up where it can ask the password once, try it all those times till it fails and asks in terminal to try again. 
 ansible [...] -u admin -k Then you will be asked for a password. 
 nmap -n -sn 192.168.1.0/24 -oG - | awk '/Up$/{print $2}' &gt; hosts.txt ansible -i hosts.txt [...]
Indeed, though keys are *always* a good idea.
Thanks you the best I would kiss you on the mouth if you was here.
I am assuming it will ask me for a password for each host? or will it try the same password for all the hosts till it fails? I know keys are the best bet, but I don't believe I can do that at the moment. If I can I will roll that way 10000% percent.
If using ffmpeg I'm guessing you'd want to cut at keyframes using `-noaccurate_seek` (see the description of the `-ss` option in the [manual](https://ffmpeg.org/ffmpeg.html#Main-options)) and join using the [concat demuxer](https://trac.ffmpeg.org/wiki/Concatenate#demuxer). I haven't cut videos up much, but I had good results making Vine compilations with either of the concat protocol and demuxer after repackaging the h264 MP4 files as Transport Streams (TS). The second link talks about this. Edit: Mention that I'm talking about ffmpeg.
Honestly, I don't know. I almost always used SSH keys. And in rare cases where you only have an u/p, it was not the same password on multiple machines. But I think it takes &lt; 5 minutes to try it in your environment. If -k asks for a password for each host, there's also the possibility to use [Ansible's Vault](https://serversforhackers.com/c/ansible-using-vault) to store your login information encrypted in the inventory file.
Thanks, the vault is my best bet. Going to try this out. Sorry for so many questions. I have to build out the enviornment first before getting started and wanted to make sure this was going to be the best bet.. Thank you again
while you are playing around with ansible, you might want to use the [authorized_key_module](http://docs.ansible.com/ansible/latest/authorized_key_module.html) to deploy your ssh key to the hosts. Will definitely save you some time and hassle until you have everything set up.
I didn't even know you could create a tcp session in pure bash. Can you point me toward some reading that would help me understand the first line? 
I'm with /u/pxsloot. When you can use simple statements, do. It's usually more readable than your standard if statement. In longer and more complex scripts, it keeps your line count down as well.
ffmpeg will do what you want.
Looks good, I would remove the infinite loop :)
Thanks for sharing! This is the type of stuff I came to this sub to see
thanks !
That way it continues and refreshes constantly. I have it up next to my editor to keep track of progress.
Ah, nothing like being reminded of what I haven't done. /jk Love it! 
[removed]
org-mode 4ever
Last access time as seconds since epoch: ```stat -c %X file``` Current time as seconds since epoch: ```date +%s``` Find files in ```/dir/``` accessed between 30 and 60 minutes ago: ```find /dir/ -amin +30 -amin -60``` For testing if there's enough space to copy any give file try https://stackoverflow.com/questions/41127585/shell-how-to-check-available-space-and-exit-if-not-enough
Is the NAS mounted on your local machine? The easiest way to approach this would be to break each of the things you need to do down into separate functions that you can create in your bash script. 1. See if the NAS has space available, you could save this value into a variable if needed 2. Find the files/folders with the specified access time and move them to the NAS There are lots of ways you could do it and you should probably add some tests, like checking the total size of the data you want to move and if there is enough space available. To start, my approach would be; - Use 'df -h' to get the percentage and available space using grep and or awk and save the value to variable. - Use the find command with the -atime flag to get the last access time and an -exec statement to move the files across 
It's also possible to have ssh aliases in your `~/.ssh/config` file. Host snafoo HostName 123.45.67.890 user root Host foobar HostName login.example.com user foobar555 Host github Hostname github.com user git IdentityFile ~/.ssh/github_rsa Then you can connect by `ssh snafoo` or `ssh foobar` to those servers.
I haven't tested this much but it's a start, at least. Make sure you understand what it does and have tested it with with expendable source and destination directories before using it for anything important. Also, it assumes your filenames don't include the newline or vertical tab characters. If I wanted to support all legal filenames I wouldn't use bash since the convenience of working with line-oriented data would be gone. Finally, it doesn't take into account that files occupy more space on disk than just their size, but that probably isn't a issue unless you're mostly copying very small files. #!/bin/bash # usage: cp-mra-cap SRC DST set -eu src=${1:?No source given.}; shift dst=${1:?No destination given.}; shift # Determine the available free space using df to print information about the # filesystem dst belongs to. Have sed eat the header line. [It'd be # negligibly more efficient for awk to eat the header line but I think it reads # better this way.] Use awk to convert the free space from megabytes to bytes. avail=$(df -BM --output=source,avail "$dst" | sed '1d' | awk '{print $2 * 1000**2}') # Find files in the src directory and use the GNU extension -printf to print # file size in bytes, access time, and path relative to src. Sort the files by # atime in reverse, and then use awk to print them out while their combined # size is less that the drive's free space. Feed the src-relative filepaths to # rsync to copy to the destination, printing an itemized report (-i) of what is # being transferred and preserving permissions, timestamps, and other # attributes (-a). find "$src" -type f -printf "%s\v%As\v%P\n" \ | sort -rk 2n \ | awk -F '\v' -v avail="$avail" '{ sum += $1 if (sum &gt; avail) {exit} print $3 }' \ | rsync -ia --files-from=- "$src" "$dst"
This is close to what I need. Unfortunately, most of my folders have a lot of nested sub folders and at least in my testing it isn't working (printing out the file name and full path)
Looks nice and definitly a good idea. One Question: Why do you use a temporary File?
Honestly, the code looked ugly when I wrapped all of the loop in a sub-shell. Writing to the file made it cleaner and clearer. I try to clean it up ASAP tho. The temp file does mess with Dropbox though (all my projects and git repos are in DB) so I might change the paths. 
Maybe add a command line argument that turns enables/disabled the loop. 
actually nah. I was just adding environment variables and was curious if it ever matter. Sounds like it does if it references each other which makes sense .
I don't understand. How isn't it working? Files in subdirectories aren't being copied?
I modified it to list the files to make sure it was working right for my needs, nothing was printing. I found another way to drop what I want though.
You could use the perl file-rename script, Regular expression and bash globbing to perform this easily. 
There are lots of ways you can do this, including some purpose-specific tools, but just to address your immediate issue (the need for recursiveness): shopt -s globstar for i in **/*.jpg; do ... done The `globstar` option enables the double-star glob syntax, which, unlike the regular star, recurses through subdirectories.
There's no fancy stuff like iterators in bash. :) The recursive function you show, I think that's "depth-first" and not "breadth-first" as it will go down into sub-directories immediately when it finds them. I just played around with stuff at the command line and got this here: x=("."); while [[ -n $x ]]; do for i in "$x"/*; do if [[ -d $i ]]; then x+=("$i"); printf "%s/\n" "$i"; else printf "%s\n" "$i"; fi; done; x=("${x[@]:1}"); done Here's the same with line-breaks: x=(".") while [[ -n $x ]]; do for i in "$x"/*; do if [[ -d $i ]]; then x+=("$i") printf "%s/\n" "$i" else printf "%s\n" "$i" fi done x=("${x[@]:1}") done This seems to do actual breadth-first like asked for in your title in my experiments here. Whenever you see it print an item with a "/" at the end, at that point it has added that item to end of that "x" array and will process it eventually after first processing whatever other entries there are before it in the array. That `"${x[@]:1}"` is how you make bash print an array's contents without the first element. This is the line where the first element of the array gets removed from it. When you write `$x` as if the array would be just a normal variable, that `$x` is just the first element of that array, not the whole array.
Even harder: Try returning an array from a function by capturing ~~stdin~~stdout without using eval.
Your question is not very clear. Are you trying to print only the deepest-level directories with size &gt;= 1G? Like, if /a/b/c contains &gt;= 1G of files, don't print /a/b and /a as well?
`du` already descends into subdirectories though. So do you want to use bash to loop over all entries in a directory and if it's a file get its size and if it's a subdirectory rerun the function on that subdirectory? Have a look at this: function divide_bytes() { bytechar=( b K M G T ) size=$1 index=0 while (( size &gt; 1024 )); do ((size/=1024)) ((index++)) done echo "$size${bytechar[$index]}" } function gigabyte() { local dirsize=0 subsize=0 for file in "$1"/*; do if [[ -f "$file" ]]; then (( dirsize += $(stat -c %s "$file") )) elif [[ -d "$file" ]]; then (( subsize += $(gigabyte "$file") )) fi done (( dirsize &gt; 1024**3 )) &amp;&amp; echo -n "FILES&gt;GB " &gt;&amp;2 (( subsize &gt; 1024**3 )) &amp;&amp; echo -n "SUBDIRS&gt;GB " &gt;&amp;2 (( dirsize &lt; 1024**3 &amp;&amp; subsize &lt; 1024**3 &amp;&amp; subsize + dirsize &gt; 1024**3 )) &amp;&amp; echo -n "FILES+DIRS&gt;GB " &gt;&amp;2 echo "Files in $1 are $(divide_bytes $dirsize), and its subdirectories are $(divide_bytes $subsize)" &gt;&amp;2 echo "$((dirsize + subsize))" } for file in "$@"; do gigabyte "$file" &gt;/dev/null done 
So I still don't know if that is the question but I thought my interpretation of it was an interesting problem. As /u/ray_gun said, `du` already descends into subdirectories, and it does so depth-first, printing the size (in 512-byte blocks) and the directory paths, separated by a tab -- so we can use that to avoid printing directories that contain directories &gt;= 1G. The trick is testing if the current directory path starts with a path that we've already printed recently. #! /bin/bash alreadyprinted= du "${1:-/}" | while IFS=$'\t' read -r size dir; do if let "(size /= 2048) &gt; 1024" &amp;&amp; ! [[ "$alreadyprinted" == "$dir"* ]]; then printf '%s\t%s\n' "${size}M" "$dir" alreadyprinted=$dir fi done
&gt; For example, people have been saying ffmpeg. So it is a CLI tool and what I need to do is use a video player like MPV, write down the start and end time I want to trim and pass that as arguments to ffmpeg to let it trim? Yeah, that was pretty much what I did. But /u/torbiak’s suggestion of `-noaccurate_seek` sounds very promising.
Yes
There’s a few things going on there: 1. You can specify a file descriptor number in front of any redirection. For example, `foo &lt; input &gt; output 2&gt;errors` will redirect file descriptor 0 (standard input) to the file `input`, file descriptor 1 (standard output) to the file `output`, and file descriptor 2 (standard error) to the file `errors`. 2. The redirection `&lt;&gt;` opens file for both reading and writing (`&lt;` is just for reading, `&gt;` for writing). 3. `/dev/tcp/HOST/PORT` is a special filename for bash when it appears in a redirection: &gt; If HOST is a valid hostname or Internet address, and PORT is an integer port number or service name, bash attempts to open the corresponding TCP socket. (`/dev/ucp/…` also does what you’d expect.) Note that this is a bash feature, not provided by the operating system: `cat &lt; /dev/tcp/alpha.mike-r.com/17` prints a [Quote Of The Day](https://en.wikipedia.org/wiki/QOTD) (bash processes the redirection, opens the TCP connection, and hands the file descriptor to `cat`’s standard input), but `cat /dev/tcp/alpha.mike-r.com/17` fails with “no such file or directory”. 4. The redirection syntax `&lt;&amp;DIGIT` (for reading) or `&gt;&amp;DIGIT` (for writing) duplicates file descriptor DIGIT. So in `cat &lt;&amp;3`, `cat`’s file descriptor 0 (standard input) is a copy of the shell’s file descriptor 3. 5. `exec` normally replaces the shell with another process, but if no command is specified, it instead applies the specified redirections to the shell process itself. So `exec 3&lt;&gt;/dev/tcp/HOST/PORT` opens a TCP connection to host HOST on port PORT and assigns it to the shell’s file descriptor 3, and afterwards commands with `&gt;&amp;3` write to that connection and commands with `&lt;&amp;3` read from it. 6. And finally, the redirection syntax `&lt;&amp;-` or `&gt;&amp;-` (like copying, but with `-` instead of a digit) closes the specified file descriptor. In the last line of the script, it closes the TCP connection.
**QOTD** The Quote of the Day (QOTD) service is a member of the Internet protocol suite, defined in RFC 865. As indicated there, the QOTD concept predated the specification, when QOTD was used by mainframe sysadmins to broadcast a daily quote on request by a user. It was then formally codified both for prior purposes as well as for testing and measurement purposes in RFC 865. A host may connect to a server that supports the RFC 865 QOTD protocol, on either TCP or UDP port 17. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
youre missing a quote after $file in your if clause. the best thing to do when trouble shooting is to run "bash -x script_name" - it will tell you line by line what is happening and what variabled are bring resolved too edit: corrected some areas (im typing quickly on my phone)
&gt; youre missing a quote after $file in your if clause First thx!!! Second quotation mark is missing only here, I had to wrote out whole code to reddit cause I was working in a vm without x11. No I've transferred the output of bash -x move.sh from sftp to my x11 machine! Here's the result: [root@ toty]# cat output.txt + file2='LinuxBCT Postfix Edition' ++ basename move.sh + script=move.sh ++ ls -A + for file in '`ls -A`' + '[' LinuxBCT == 'LinuxBCT Postfix Edition' ']' + '[' LinuxBCT = move.sh ']' + '[' -d LinuxBCT ']' + mv LinuxBCT /root/dir_pruebas/movidos mv: no se puede efectuar `stat' sobre «LinuxBCT»: No existe el fichero o el directorio + for file in '`ls -A`' + '[' BASH == 'LinuxBCT Postfix Edition' ']' + '[' BASH = move.sh ']' + '[' -d BASH ']' + mv BASH /root/dir_pruebas/movidos mv: no se puede efectuar `stat' sobre «BASH»: No existe el fichero o el directorio So forget about the error language, the thing is there seem to be some sort of problem that split every filename into different $file variables. 
I think you mean capturing `stdout`? I don't think that is possible to do correctly in the general sense. That is, if the array contains arbitrary chars like whitespace and control characters. Bash doesn't even attempt to do this with its own completion system. Instead the protocol is to set the global variable `COMPREPLY=(list of completions)`. In bash, you can set vars in the callers scope -- this is known as dynamic scope. But I think you need `eval` to control the variable name? You can set a fixed variable name, but the caller probably can't specify it without eval. It's a little like `setattr()` in Python. Some code here: https://github.com/oilshell/blog-code/blob/master/return-array/demo.sh 
Just an aside, since I'm guessing you're new to `*x` (== Linux or other Unix-based OS): best practice is **NOT** to run as root! Get in the habit of logging into your device (even if only remotely) as a normal user, then using `sudo` to "get root" only as needed. For a slightly longer explanation, see (e.g.) [this SE post](https://unix.stackexchange.com/a/237222/38638). 
Your question is very much **not** bash-related, hence inappropriate for this subreddit. Furthermore you're more likely to get help @ subreddit=[VideoEditing](https://www.reddit.com/r/VideoEditing/), so I suggest you move your post there. 
Thanks for the thorough reply. I found some reading on it, but most of it was a bunch of people using the same examples. 
`getopt` adds `--` to $ARGS, so you need to add a case for that. The loop is also wrong using `while true; do` because the code will shift until (edit: and including the time that) $1 is empty causing the `case` to go to the default statement always, so you must loop using `while [[ $1 ]]; do`
I'm not new a *x at all Tom_Roche just to scripting but thx anyway! Any insight about the code? :)
You can print the array with "echo", but you won't really see if spaces belong to certain elements: echo "${arr[@]}" You can do the following to add some extra characters surrounding each element to see the exact start and end of each element: $ arr=("a b" "c d" "e f") $ printf '&lt;%s&gt;\n' "${arr[@]}" &lt;a b&gt; &lt;c d&gt; &lt;e f&gt; If you want commas or other separators, things get a bit complicated. You could define a function to help with that: concat() { local result printf -v result "$1%s" "${@:2}" printf '%s\n' "${result#$1}" } That function can do this: $ concat ',' a a $ concat ',' a b a,b $ concat ',' "${arr[@]}" a b,c d,e f $ concat ', ' "${arr[@]}" a b, c d, e f 
I'm a bit confused about what you want. `"${arr[@]}"` expands so that each item is a separate word, meaning `magiccommand` will receive them as separate arguments. So `magiccommand` will see one argument that is `a b`, one argument that is `c d`, and one argument that is `e f`. There are no quotation marks involved. I think what you're *really* looking for is a command that prints shell-escaped versions of its arguments. The `%q` format for `printf` can do this. However, it will typically just escape things with backslashes rather than using quotation marks. For example: $ arr=("a b" "c d" "e f") $ printf "%q\n" "${arr[@]}" a\ b c\ d e\ f 
Thanks. I was using `--add-metadata` which doesn't provide URL but provides video description and some other potentially useful video stats and for larger files (~ 3 GB) it takes at least a minute to add the metadata to the video file for me for some reason (not sure if this is normal--is it actually performing tests to produce the video stats...?). If I were to repeat the command after already running with `--add-metadata`, it would detect video is already downloaded and skip it (as expected), but would re-add the metadata and take a similarly long time in doing so, so this makes downloading a batch of videos really slow (I also often copy a bunch of URLs to download the videos without checking whether I already have the video saved to disk, so `--add-metadata` makes this very inefficient for large files in terms of time to retrieve the video. Using extended attributes doesn't have this caveat at all--it doesn't provide as much info as `--add-metadata` but it does cover the 2 most important ones. Only caveat is tmpfs doesn't support it (sometimes I like to download to RAM when disk usage is high or I want to watch the video without any lag whatsoever, then save it to disk). If I only wanted the URL of the file I suppose a script to parse the filenames to extract the video-id and format it into a URL works without any caveats, but I also want the video description so it's between `--add-metadata` and `--xattrs` so I choose the latter.
I'm looking for a command that is essentially a debugging command for why a bash script didn't do what you expected it to. IE let's say you write: $ arr=("a b" "c d" "e f") $ for i in ${arr[@]}; do echo $i; done a b c d e f and you don't know why that happened, so you use $ magiccommand '${arr[@]}' a b c d e f and this points out to you that you didn't use parenthesis, which are necessary to get the behavior you intended. Now, to be clear, I know how it all works. I'm not asking for help on how to write bash scripts. I'm looking to see if there is a command that will explain to me where I went wrong in complicated bash scripts involving various pipes/expansions/subshells/command substitutions etc where I expect output A and I'm getting output B.
&gt; If I only wanted the URL of the file I suppose a script to parse the filenames to extract the video-id and format it into a URL works without any caveats In general, there is one caveat: `youtube-dl` supports a *lot* of other websites as well, but extracting the video ID is YouTube-specific. (Of course, that doesn’t matter if you’re only downloading from YouTube.)
Yea I figure I could write something that does what I'm asking for, but I'm hoping that somebody has already done it. 
&gt; for larger files (~ 3 GB) it takes at least a minute to add the metadata to the video file for me for some reason I just downloaded a video and looked at the raw data with `less`, and it looks like the metadata is added to the beginning of the file, so I suspect that’s why adding the metadata is so slow: perhaps it’s copying the header, adding the metadata, and then copying the entire video and audio stream(s), because you can’t insert bytes into a file at arbitrary positions.
Update: I had to switch it to a sub-shell because my editor did not like the contents of the project directory changing back and forth every 5 seconds.
That type of mistake you mention would have been found by the "shellcheck" tool. Your distro probably has a package for it. There's also a website www.shellcheck.net where you can paste a script into a text box to test shellcheck without installing it. I never really found a good way to deal with the problem you mention. I guess bash is simply a bit bad for complicated scripts. One thing that might help a bit, when you run the script with `bash -x` to see debug info, you normally just get a `+ ` added to the debug lines and that can be really confusing in long script. You can add line numbers to that output by changing the PS4 variable, like so: PS4='+$LINENO: ' bash -x scriptname You will want to keep the `+` as the first character for PS4 because that's the character that gets repeated for nested statements. I mean this here: $ PS4='+ $LINENO: ' bash -x -c 'echo "nested stuff: $(echo hi)"' ++ 0: echo hi + 0: echo 'nested stuff: hi' nested stuff: hi If LINENO is at the beginning, bash will repeat the first digit of LINENO for that nested part. There might be some more interesting variables to add to PS4. I only remember FUNCNAME: PS4='+ ${FUNCNAME[@]} $LINENO: ' This adds the names of the functions that you are currently in to the lines. When you are in the global parts of the script, you'll see the function name "main".
the issue is using the for loop with filenames with spaces, each space will throw your for loop off. ive read that "ls" is never really good to use in for loops, and instead there's ways to use the "find" command instead. there's a lengthy write up about for loops and spaces [here](http://mywiki.wooledge.org/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29) also a quick google of "bash for loop with spaces" will give you examples of other workarounds one thing you can try is to use globbing instead of an "ls" command output: for file in * # your code as usual edit: punctuations, i can't format
Try this: arr=("a b" "c d" "e f") printf '[%s]\n' "${arr[@]}" Output: [a b] [c d] [e f] 
Thx that solved the issue!!! I read the whole number 1 title from the link you posted BUT I can't get how did you come up with the idea of using just "for file in *" which isn't mentioned anywhere in the text!!! I understood the general idea though! This is getting even more interesting every time!!!!
Thanks, but I get how to examine this specific case. I'm talking about a general command that expands shell input without executing it. A good analogy is the `clang -S -emit-llvm main.c` flag. I'm looking for `bash -emit-expanded script.sh`.
it's just globbing since the shell will expand "*" to anything listed in its current working directory. i found it while going through some of the stack exchange posts. there may be issues/bugs with it though if there is a weird file name character that throws the shell interpretations off, but for the most part if should work. you'll eventually find the use case where a plain "*" won't fit your needs, but we can get to that when the time comes haha (:
Ah, finally your question is clear. There is no command that shows expanded shell input without executing it (this is not possible because bash is an interpreter and not a compiler), but there is a shell option that shows it *while* executing it: bash -x script.sh You can also turn this on and off in a script using `set -x` and `set +x`. *edit:* /u/ropid had already [pointed this out](https://www.reddit.com/r/bash/comments/73o5hu/is_there_a_command_that_will_take_a_single_quoted/dnrx5a9/) and gave more useful info as well.
Ooo spellcheck looks nice, I'm definitely adding that to vim. And yup, I use `bash -x` and `bashdb` but I'm looking for a more special purpose tool. Also, I edited my original post to clarify my question if you're curious.
Thanks, but `bash -x asdf.sh` simply shows which line is presently being executed. It does not do any sort of transformations on the actual command. Also, &gt; this is not possible because bash is an interpreter and not a compiler Huh? That would be very easy to do as bash does this exact pipeline of processing. 
For comparison, in NGS ( https://github.com/ilyash/ngs ) it's * `my_cool_prog $*args` to pass the array to external program as arguments (consistent with `$` for substitution and with `*` for splat in other places in the language) * `b=a` to point `b` to the same array as `a` * `b=a.copy()` to copy the array 
It's your first week and you're already asking for others to do your homework? 
This is not very difficult to accomplish and if we just give you the answer you won’t know the fundamentals. Post what you have tried and we’ll be glad to help.
1) Tells you exactly what you need to do. You may like to learn about shell redirection though. 2) Google Clue (Cluegle?): `chmod` 3) Cluegle: shebang
like [this](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_grep.md#alternation)? - color matched terms as well as display entire input contents
see if these would help then * https://stackoverflow.com/questions/17236005/grep-output-with-multiple-colors * https://unix.stackexchange.com/questions/104350/multicolored-grep 
yes that 'something|$' helped, thanks!
Shouldn't it be ls -1 instead of ls?
You want each parameter on a separate line? Do you not want to execute a she'll or something? I think a shell script with for x in "$@" ; do echo "$x" ; done should do it. What is that missing?
&gt; Thanks, but bash -x asdf.sh simply shows which line is presently being executed. It does not do any sort of transformations on the actual command. That is simply not true. $ cat &gt;test.sh v=foo echo "$v" $ bash -x test.sh + v=foo + echo foo foo Note how `echo "$v"` is transformed to `echo foo`. You get the commands with all the arguments expanded, which is useful for debugging purposes. &gt; Huh? That would be very easy to do as bash does this exact pipeline of processing. What does that even mean? `clang -S -emit-llvm` outputs compiled code. How do you think bash could output compiled code? It's not a compiler, it's an interpreter. 
`declare -p arr` might fit your needs. It outputs a string that, if you eval it, will recreate the array, so it should tell you everything.
The problem is, the `=~` operator is a bit weird. It treats the `"` as part of your regex pattern and tries to match literal `"` characters. You need to write it like this: [[ ! $1 =~ [bd-fh-su-z] ]] Another thing, I don't understand what you want exactly with that pattern. [*EDIT: I understand after McDutchie's comments.*] Those extra `-` inside the `[]` list don't seem to do anything, I think. [*EDIT: This is wrong.*] Your pattern should mean the same as this: [-bdfhsuz]
Just use `grep`: `grep -q '[^acgt]' "$1" &amp;&amp; echo "File is not DNA." || echo "File is DNA."`
The `-` in a bracket pattern denotes a range, so `[bd-fh-su-z]` is the same as `[bdefhijklmnopqrsvwxyz]`. That's not what OP wants though, they want `[[ $1 =~ [^acgt] ]]` (regex) or `[[ $1 == *[!acgt]* ]]` (glob pattern). The `^` is the bracket pattern negator for regex, the `!` for glob. *edit:* fix glob as per /u/giigu's reply
Thanks for explaining the regex range stuff.
The POSIX specification for glob patterns only defines the behavior of `!`, but both `!` and `^` work in most implementations (including bash) The equivalent to `[[ $1 =~ [^acgt] ]]` would be `[[ $1 == *[^acgt]* ]]`, tho, since glob patterns are anchored on both sides Also, regular expressions are a fair bit slower than the shell's native pattern matching in both bash and zsh, so the latter should be preferred if performance is a concern (probably isn't in this case, admittedly)
&gt; The POSIX specification for glob patterns only defines the behavior of !, but both ! and ^ work in most implementations (including bash) True, but not in all, so at some point it might bite you in the ass. Might as well not get in the habit. The `!` in glob works everywhere. &gt; The equivalent to `[[ $1 =~ [^acgt] ]]` would be `[[ $1 == *[^acgt]* ]]`, tho, since glob patterns are anchored on both sides That's very true. Brain fart on my part. Fixed. 
Apart from the regex/globbing issue covered in the other comment thread, with `[[ ! $1 =~ … ]]` you’re also testing the literal value of `$1`, which if I understand correctly is the file*name*, not the file *content*. To compare the file’s content, you can use `$(&lt; $1)` instead of `$1`. (`$(&lt; FILE)` is a shortcut for `$(cat FILE)`.)
Instead of `read -r line` you want `IFS= read -r line` to avoid stripping any leading and trailing whitespace. Instead of `eval "$line"` you want `eval " $line"` (note space before the variable expansion) to avoid lines starting with `-` to be parsed as options to `eval`. Finally, to make `^D` (i.e., EOF) work to exit, shuffle the order a bit: while echo -n "$PWD &gt; "; IFS= read -r line; do eval " $line"; done so that the loop is exited when `read` exits with a non-zero status. 
Thank you! With this and the other advices from the comment, I was able to get it working. I assigned the output of `cat $1` to a variable and used that variable in the if statement. Bash scripting has been a hit or miss affair for me so far.
Thank you for you help, guys! /u/ropid, /u/McDutchie, /u/giigu, /u/ray_gun, /u/galaktos!
you can try using the while loops and seconds function for your purpose. something like while [ ! -f ~/some_trigger_file ] do SECONDS=0 sleep 59 if [ $SECONDS -ge 59 ] then #commands to run fi done You could run the script in background sh script_name &amp; Instead of echo into the terminal, put it into a log and run another script in a similar fashion to send you a mail whenever it detects host down errors in the log. for the number of packets failed, since you are using a predetermined value for $COUNT, you can get the ($COUNT - $count) for the lost packets. I used sleep 59 and not the rounded value , becos this would help in identifying this script uniquely when running the background. ps -ef | grep sleep this script will stand out when you are trying to get the pid for this.
Thank you so much for that! I'll have a deeper look into it shortly and report back. 
[removed]
`eval` doesn't have options.
But on bash and some other shells, it still parses them and will give you an "invalid option" error if its first argument starts with a `-`.
I'm not sure I understand the issue. Are you expecting `dialog` to run at the same time as `pacman` in the excerpt bellow? The `until` construct works the same way as a `while`; it'll run `pacman &amp;&amp; exit`, wait for it to finish and execute the body of the loop if it retuns false. until sudo pacman -Syyuu &amp;&amp; exit do dialog \ --no-lines \ --no-shadow \ --infobox "Running process..." 4 30; done ;; If you want the infobox to be shown while `pacman` runs you can do something like this (untested code): pacman -Syyuu --noconfirm &amp;&gt; /dev/null &amp; pacman_pid=$! while ps --pid "$pacman_pid" do dialog --no-lines \ --no-shadow \ --infobox "Running process..." 4 30; done I don't really recommend obfuscating pacman's output that way, though; Arch is fickle and using `--noconfirm` isn't generally a good idea. You should consider adding the `discard` option for the appropriate mount point in your `/etc/fstab` instead of running `fstrim` manually like this. &gt;I have also tried running pacman in the background but im getting an unexpected token when i append the &amp; like so: pacman -Syyuu &amp;; You're getting an unexpected token error because that is not valid bash. You can have a command after the `&amp;` but not a semicolon and something like `while pacman -Syyuu &amp; ...` is also not valid. &gt;cmd1 &amp;&amp; cmd2 or cmd1; cmd2 or by using a new line for every command Note these aren't all equivalent. The latter two work the same, but using `&amp;&amp;` means cmd2 only gets executed if cmd1 returns zero (true). 
Thanks! Yes, I want the infobox to be displayed during the time it takes for fstrim/pacman to finish it's job. Once the job is completed the infobox should disappear (exit to shell or maybe back to another menu in the future) I tried what you suggested which is some good help on the way. I'm gonna have to tinker around with this new info though. The snippet you suggested didn't work straight away, but I do think it's in the right direction! I've heard fstrim is safer to use than the discard mount option which is why I use it. No idea if this is true or not for general use. And also because I plan on adding the ability to run it at a specified time that I can set to whatever time is convenient by using the 2nd option in the menu. I dont know enough about discard yet to easily do that. edit: added last 2 sentences
Discard is no more or less "safe" than a manual fstrim. Though TBH I'm not even sure what you mean by "safer" in this context. Ideally the system would discard frequently enough for it to not take very long or affect performance in any meaningful way.
Just what I heard from reading comments like these https://www.reddit.com/r/archlinux/comments/1jhed8/fstrim_necessary_with_discard/cbez5uh/ and the likes of this one from https://forums.gentoo.org &gt;I prefer fstrim on a weekly basis. In most cases it doesn't really matter if a free block is trimmed instantly, or a week afterwards. fstrim does not suffer from the performance issues discard still imposes on you when deleting a lot of files. with fstrim, you have better chances to recover from an idiotic messup, like an accidental rm -rf photos/. with discard, they're gone instantly, with fstrim (if you disable the cronjob immediately), you can still get the photos back using photorec. But yea probably better and easier to use discard for most people these days.
OK those reasons do make sense - I just wouldn't classify them as "safety". Then again I'm primarily a server guy where an "rm -r photos" isn't going to happen so maybe my bias is showing ;-)
You probably would have figured this out on your own, but take a look at these variables you have in your code: TITLE="fstrim --all --verbose" and later: BACKTITLE="Username @ 2017-10-03 | fstrim --all --verbose" Now, what take a look at these, as a bash scripter, and imagine what these do. Yup. They assign the variable TITLE the string value "fstrim --all --verbose". That's probably not what you wanted. I'm guessing what you wanted looks more like this: TITLE="$(fstrim --all --verbose)" BACKTITLE="$(Username @ 2017-10-03 | fstrim --all --verbose)" Except, that won't accomplish what you want, either, because fstrim does not take input piped from stdin, so $BACKTITLE will be the same as $TITLE, and $TITLE will have a value of null most of the time. At least on my system, but maybe it works differently on your system.
Pretty cool. Can't use tab though. Wow, I tab a lot. Really don't like this anymore. #@#$ MAKE IT STOP.
This is my fav, it lists the top files and location for the entire system: sudo du -h / | grep -P '^[0-9\.]+G' 
You should have the script corrected by shellcheck.
yes it doesn't have any features (completion/history/...) besides being able to execute commands.
It's not supposed to take a pipe, its there purly for esthetical reasons. But now that you mention it I I should not have used a legit command as something esthetical. Somehow it seems to work anyway. I ended up doing #Depending on choice in menu do one of these cases case $CHOICE in 1) sudo fstrim -a -v while pgrep -x "sudo fstrim -a -v" &gt; /dev/null do dialog \ --no-lines \ --no-shadow \ --sleep 2 \ --infobox "Running process..." 4 30; clear done ;; Which checks if fstrim is running, and while it is, draw the dialog box. But in the end it doesn't work anyway, because fstrim will run and finish before the script gets to the part where it can check if it is running. To get it to work I would need to make it two scripts that run at the same time , but even then I can't control which command has priority, fstrim or dialog. The kernel will do that for me. This was a lost cause to begin with, in the way that im trying to make it at least. On a positive note, I now got a scipt that will check if a program is running if I just exchange fstrim for whatever I want. E.g if I switch fstrim for htop and start htop manually, and then run the script the dialog box will render perfectly, and when htop is closed, the script will close too. Could be useful some time.
me_shirley() { echo "surely you can't be serious."; }; echo 'i am serious.' &amp; exit
Colleague wanted to capture the current directory to a variable, so colleague put this in his script: PATH=`pwd`
Why not just...$PWD? 
The bigger question is what can you even do once you wrecked your path?
 :(){ :|: &amp; };:
I think I'll try tha---- 
Fix your path. Log out. Run built-ins.
Before you try it, DON'T. It's a fork bomb. Best case, your system reboots. Worst case, you potentially lost all unsaved files/docs/spreadsheets etc. 
 ix () { [ -z "$1" -o -r "$1" ] &amp;&amp; curl -F "f:1=&lt;${1:--}" ix.io || printf '$%s\n\n%s' "$*" "$("$@")" 2&gt;&amp;1 | ix; }
Yeah, that was one of the things I taught my colleague once I'd stopped laughing :) The other things were: Here's a prime fucking example for why you shouldn't use uppercase vars unless you know why you need to, and backticks belong back in 1983.
Everybody knows never to simply copy/paste/run code found on internet. That's where vagrant is for.
The winner is: :(){:|:&amp;};: Some other inspiration: http://bruxy.regnet.cz/web/linux/EN/useless-bash/
&gt; backticks belong back in 1983 Tell that to my coworker. I think he's stuck in the 80s though.
yeah, I tried to post that. Downvotes commenced
What's wrong with backticks? Much quicker than $( ) Edit: thanks for the down vote, but I'm legitimately asking a question...
A little comment would be nice, warning about. Just pasting it without any explanation feels troll-ish. You would think everybody knows not to run copy pasted code, but you would be surprised. 
it's pretty harmless, well documented, nothing hidden is installed, it just makes your machine go tits up. The learning from running it is priceless: you learn about recursion, bash, fork bombs, copy/paste internet code, BOFH. And all that from 11 characters. Running it is a rite of passage, always initiated by a troll. OP's question is perfectly answered.
The `\b` you are using is fancy stuff from "Perl" = "PCRE" = "Perl-Compatible Regular Expressions" and it doesn't work in grep. The `\d` also does not work. This here would work: egrep ' zip\([[:digit:]]+\)' If you want exactly the number, not the whole line, then grep is not the tool for you. You need "sed" or "perl": sed -nr 's/^.* zip\(([[:digit:]]+)\).*$/\1/p' perl -nE 'say $1 if / zip\((\d+)\)/' 
If you have libpcre installed, you can use the grep -P option and do it like this: grep -P '(?&lt;=\()\d{4}(?=\))' This uses perl lookahead and lookbehind assertions to see if the previous character is a ( and the next character is a ). It will allow you to in this example match exactly four digits within parenthesis, but not the surrounding parenthesis themselves. grep -P '(?&lt;=zip\()\d{4}(?=\))' might be a bit more safe since there are more parenthesis in the line in which to do the matching. To clarify a bit, using the last example it goes something like: if there is a zip( before the four digits, and if there is a ) after, match it.
**Edit: I stupidly only verified the accuracy of my "BSD supports it" comments against macOS, not an actual BSD. Take it with a grain of salt.** &gt; The `\b` you are using is fancy stuff from "Perl" = "PCRE" = "Perl-Compatible Regular Expressions" and it doesn't work in grep. `\b` is undefined in POSIX EREs (meaning that implementations are free to do whatever they like with it, including something helpful!). It is supported by most common egrep implementations: GNU, BSD, and BusyBox. &gt; The `\d` also does not work. `\d` is also undefined in POSIX EREs, but is supported by BSD and BusyBox egrep, but not GNU. &gt; If you want exactly the number, not the whole line, then grep is not the tool for you. Generally good advice, but the non-POSIX `-o`/`--only-matching` flag means that in many cases, grep *is* the tool for you. The short for, of this flag is supported by GNU, BSD, and BusyBox egrep, and the long form is supported by GNU and BSD egrep. ---- Of course, `egrep` itself is not specified by POSIX, though `grep -E` (which `egrep` is shorthand for) is. Anyway, if you have no problem relying on `egrep` being a thing, then you should probably have no problem relying on `\b` or `-o`. ---- Your `sed` solution is probably the best solution, though it should probably use `-E` instead of `-r`. Though it hasn't made it in to an actual release of POSIX, `-E` has been approved for POSIX Issue 8 (the next major version); and `-r` is just a compatibility alias for `-E` for compatibility with scripts written for pre-2012 GNU sed (historically, BSD sed used `-E` while GNU sed used `-r`).
&gt; \b is undefined in POSIX EREs (meaning that implementations are free to do whatever they like with it, including something helpful!). It is supported by most common egrep implementations: GNU, BSD, and BusyBox. It's actually supported system-wide, for both BRE and ERE, on platforms that use the GNU or Apple `regex` library. I think that whatever `regex` library BusyBox uses must have the same feature, since it also works in `sed` there. Not sure about other platforms (FreeBSD's `re_format(7)` doesn't seem to mention it, but I don't have any BSD systems to test with right now).
&gt; It's actually supported system-wide... I didn't quite want to discuss libc regex implementations, since I know GNU grep rolls its own regex implementation as an optimization to avoid having to break text in to lines. :) &gt; Not sure about other platforms (FreeBSD's re_format(7) doesn't seem to mention it Shoot, I looked at `re_format(7)` on a nearby macOS box, and stupidly assumed that since it said "BSD" and didn't say "Apple" anywhere that it hadn't been changed meaningfully from FreeBSD. I guess `REG_ENHANCED` is an Apple extension.
Run `set -x; source ~/.bash_profile; set +x`. Does it look like it ran the commands contained in the file?
&gt; I didn't quite want to discuss libc regex implementations, since I know GNU grep rolls its own regex implementation as an optimization to avoid having to break text in to lines. :) I don't know if that's accurate -- it has some fancy optimizations for determining when it's worthwhile to attempt a match, but once it does so it just uses the GNU `re_*` functions (which have basically the same internals as GNU's POSIX interface).
Yea, it looks correct. I tried messing around with it some more, and if I add aliases to `.bash_profile`, I can source it and the new aliases work fine. But if I comment out or remove the alias, sourcing it won't update with the removed aliases.
Sourcing a file just runs the commands in it. The aliases don't get removed when you comment them out because the commands to set them up have already been run, when you opened your shell.
They’re less readable (since they don’t stand out as much, especially in fonts that don’t distinguish them well from `'`) and trying to nest them gets you into backslash hell.
The `else` block has to go last. You can't have an `elif` block after it.
You are trying to bind the elif to the then, which is not allowed. It must bind to the if. Rearrange the code so the elif follows the if, and adjust your code as necessary
I didn't down vote you, and I shake my fist at whoever did... /u/galaktos has already answered though. They're less readable, they don't nest well - at all, and they were essentially-obsoleted by `$()` back in 1983 with the introduction of the korn shell. To my mind the only valid use for backticks these days is SVR4 package scripts or instances where you can't be certain you're dealing with a POSIX shell - and most anything from about '93/94 onwards has a POSIX shell. &gt;Much quicker than $( ) I've been touch-typing for *mumble* fuck I feel old *mumble* years. Backticks are not quicker, not by any means. Easier (which is arguably an argument against them), but not quicker. There are plenty of other resources available that all point to the preference of `$()` over backticks. http://wiki.bash-hackers.org/syntax/expansion/cmdsubst#a_closer_look_at_the_two_forms https://github.com/koalaman/shellcheck/wiki/SC2006 http://mywiki.wooledge.org/BashFAQ/082 https://google.github.io/styleguide/shell.xml#Command_Substitution https://stackoverflow.com/questions/9449778/what-is-the-benefit-of-using-instead-of-backticks-in-shell-scripts https://unix.stackexchange.com/questions/126927/have-backticks-i-e-cmd-in-sh-shells-been-deprecated https://en.wikipedia.org/wiki/Command_substitution
**Command substitution** In computing, command substitution is a facility that allows a command to be run and its output to be pasted back on the command line as arguments to another command. Command substitution first appeared in the Bourne shell, introduced with Version 7 Unix in 1979, and has remained a characteristic of all later Unix shells. The feature has since been adopted in other programming languages as well, including Perl, PHP, Ruby and Microsoft's Powershell under Windows. It also appears in Microsoft's CMD.EXE in the FOR command and the ( ) command. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
First, to make sure we're on the same page here, when you wrote your example: for i in var1 var2 var3; do ... done In this example, the contents of `$i` will be the text `var1` etc., not the contents of a variable `$var1` etc. When you read through `help declare`, there's this parameter here: -n make NAME a reference to the variable named by its value This is what can be used to do what you describe. It looks like this: $ for i in var{1..3}; do declare -n ref="$i"; ref="test in $i"; done $ echo $ref test in var3 $ echo $var1 test in var1 $ echo $var2 test in var2 $ echo $var3 test in var3 There's another thing you might need to know, the `unset` command has a `-n` parameter for references. Without the `-n`, you will delete the variable that the reference points to. See here: $ echo $ref test in var3 $ unset ref $ echo $var3 $ echo $ref $ var3=hello $ echo $ref hello $ unset -n ref $ echo $ref $ echo $var3 hello Another thing, there were a bunch of times I thought I needed references, and nearly every time I found out that reorganizing my stuff instead of using references was just as good or better. I can only find one example in my scripts here where I ended up actually using it. It's a function where one parameter is used to choose an array where it will write stuff into, something along these lines: collect_stuff() { local -n output=$1 ... output+=( ... ) ... } Note: `local` has the same parameters as `declare`. You would then do `collect_stuff some_name_here` and would get changes in `${some_name_here[@]}`. I think I heard this whole reference stuff won't work with the bash version that's installed on Apple's OS X.
/u/ropid brought up `declare -n`, which is a neat feature. However, as mentioned, it's not available in older versions of bash, and also there are some subtleties to the behavior of `declare` that you might have to consider (mentioned below). I think it's a good option for most cases, but here are the others I can think of, for the sake of completeness: --- The first is `eval`, which you mentioned: $ i=var1; eval "${i}=foo"; echo "${var1}" foo This one isn't great. It's (comparatively) slow, for one thing. It's also error-prone if the value you're assigning isn't static -- i.e., if you were assigning the contents of another variable instead of just `foo`. (There are some ways to avoid errors, though. In bash 4.4+ you can do `eval "${i}=${other_var@Q}"` -- the `@Q` quotes the value of `var` so that it's safe to evaluate. In earlier versions of bash you can use `printf`'s `%q` spec.) In any case, I wouldn't use this option unless it's necessary for portability. --- The second is `declare` with an interpolated variable name: $ i=var1; declare "${i}=foo"; echo "${var1}" foo (You can also use `typeset`, which behaves the same way.) This option is faster and safer than `eval`, and it seems to make the most sense idiomatically. There is one issue though: `declare`'s behavior changes depending on whether it's used inside of a function. When used outside of a function, it behaves the same as a "bare" assignment (`i=foo`); when used inside of a function, it behaves the same as `local`. You can give it the `-g` option to have it create a global variable, but those are the only two options -- local to the current function or global. For simple scripts, this is probably fine, but you definitely need to be aware of the subtleties of shell variable scope/inheritance if you use this method. --- The third is parameter expansion with the `!` and `:=` operators: $ i=var1; : "${!i:=foo}"; echo "${var1}" foo This is about as fast as `declare` and it avoids the scope weirdness I mentioned. However, aside from being maybe a little too clever, it requires that the variable be null or unset (bash doesn't have an unconditional `::=` assignment operator like zsh). To get around this you can simply set `var1=''` before reassigning it. --- The fourth is `printf -v`: $ i=var1; printf -v "${i}" '%s' foo; echo "${var1}" foo The `-v` option tells `printf` to assign its result to the named variable instead of printing it. This is pretty fast (not quite as fast as `declare`, but close enough), and you can do some interesting things with formatting this way. --- The fifth is `read`: $ i=var1; read -r "${i}" &lt;&lt;&lt; foo; echo "${var1}" foo This is pretty straightforward, but it's *by far* the slowest option, because it requires the shell to juggle file descriptors. It can be useful when you need to split the input up and/or when it's coming from a command, though. --- I think it's true that indirect variables are *usually* not the way you want to go; at the very least you could probably use an associative array instead. But, if you *do* need them, I would personally recommend `declare -n` or `printf -v`.
&gt; In this example, the contents of $i will be the text var1 etc., not the contents of a variable $var1 etc. Yeah, I'm an idiot who shouldn't write at night. They are variables, so it's actually: `for i in $var1 $var2 $var3` `do` `content of $var1, $var2, $var3 to changed in here somehow` `done` And thank you for your answer, and I'm sorry for having put the question so poorly.
Yeah, I'm an idiot who shouldn't write at night. They are variables, so it's actually: `for i in $var1 $var2 $var3` `do` `content of $var1, $var2, $var3 to changed in here somehow` `done` And thank you for your answer, and I'm sorry for having put the question so poorly.
Using `cut -f1` works here for me: du --bytes --summarize ~/path/to/directory | cut -f1
Wow, I completely forgot about trying cut... Thank you! edit: punctuation
First of all: you’re setting yourself up for a security minefield here. I wouldn’t trust myself to get something like this up without overlooking anything. In general, I guess your approach will be to have one (or several?) users with almost no permissions, which have some special program set as login shell, and also enforced with `ForceCommand` in `sshd_config` that they can’t run anything else (I assume users will log in with `ssh`). You could write this special program as a shell script (`read -r command; case $command in …`), but that feels risky to me. Also, what you’re describing for your Java command line application (let me guess – it’s a Minecraft server, isn’t it? :P ) is service management, so you should look to your distro’s service manager for that – on Ubuntu 16.04, that’s systemd, so you’ll create some kind of `minecraft.service` unit that you then manage with `systemctl start` and `systemctl stop`. And since Ubuntu 16.04 ships a recent enough version of systemd, you can configure PolicyKit to allow the special user(s) to manage that one service without root rights, [like this](https://serverfault.com/a/841150). (Alternatively, you can allow them to run just `sudo systemctl start minecraft.service` and `sudo systemctl stop minecraft.service` in your `sudoers` policy.) &gt; The administrator user is required so that the the person connected via the internet can log out without stopping the server I don’t think you need an administrator user for that, any user should be able to background and `nohup` a process. (However, you would need that for “stop server” to work even if the application was started by a different user, and anyways you’ll want to do this via your service manager, see above.)
&gt;it’s a MInecraft server, isn’t it? It sure is. &gt;you’re setting yourself up for a security minefield here. I wouldn’t trust myself to get something like this up without overlooking anything. If there is a better way I'm all ears, I will need some guidance though either way. My main goal is that my friends can start and stop the server at will, but not control it, or anything else on that server for that matter. 
 du . --max-depth=0 
It's in a subdirectory, which it shouldn't be. Move the file up one level (into `/usr/local/bin`) Should work then *edit* Actually, re-reading your post, this probably isn't the best way to do it. If there's other files and directories under "bowtie2" I'd actually move the whole thing into `/opt`, and symlink the program into `/usr/local/bin`
Yep, here is the quote from OP &gt; if I go into /usr/local/bin/bowtie2, `/usr/local/bin/bowtie2` is not in `$PATH` so of course it cannot be found by searching `$PATH`.
thanks. bowtie2 has dependencies that need to be in the same folder when I try this: "ERR: expected bowtie2 to be in the same directory with bowtie2-align: /usr/local/bin" There are several other bowtie2 'program dependency programs' (I don't know the term for this). Rather than making /usr/local/bin messy with all the bowtie2 dependencies, would it be better to move into /opt or add the whole /usr/local/bin/bowtie2_blahblah directory to $PATH? To be honest I had never heard of /opt and don't know what it is, what is the advantage of moving it there and creating a symbolic link to it there? 
Only files that are _directly_ inside the paths in PATH and with the executable bit set will be executed when running its name. So I'd reccomend you to use symbolic links to have a file inside /usr/local/bin that points to the executable file. You can create symbolic links with the following command: ln -s /path/to/executable /path/to/new_link
So first, my suggestion is to stop what you're doing, delete all the bowtie2 stuff you've installed, install [homebrew](https://brew.sh) on your mac, and then run `brew install bowtie2`. No point in doing all this stuff manually when homebrew exists and your bowtie program is available on it. That said... for Mac the "standard" is to put the program dependencies in a directory within /usr/local, most people going with /usr/local/opt since that's the equivalent practice on linux and dumping the directories there to be organised. If you're using something like homebrew or macports, those package managers will do the same, creating a /usr/local/Homebrew or /usr/local/macports directory to store all the package dependencies. Then just symlink the bowtie2 executable to /usr/local/bin cd /usr/local sudo mkdir opt sudo chmod 0775 opt sudo chown $(id -u):admin opt mv bin/bowtie2 opt ln -s opt/bowtie2/bowtie2 bin/bowtie Now you can run the program with `bowtie`
&gt; for Mac the standard is usually putting the dependencies in their own dir in /usr/local Ok, ignore me. I'm a Linux guy. Seems Mac is different, so listen to people who know that system
&gt; for Mac the standard is usually putting the dependencies in their own dir in /usr/local apparently Apple had the brilliant idea of making /usr/local un-writable, even using sudo, so I have to place in /usr/local/bin I will create the /usr/local/opt and symlink, thanks - just to check do I symlink to /usr/local/bin/bowtie or to /usr/local/bin? 
I use word standard loosely. Since there's no native command line package manager on macOS there's no real correct way, making a /usr/local/opt directory to contain them is just an adopted practice for manually-installed packages. For the most part, if you do use a package manager on macOS, you're using homebrew, which creates a directory at /usr/local/Homebrew for storing dependencies.
For the command line: for uf in /tmp/archive/*.tgz; do tar -xvf "$uf" -C /tmp/test; done The same with line-breaks: for uf in /tmp/archive/*.tgz; do tar -xvf "$uf" -C /tmp/test done
Why did you put quotes around "uf"? Also that worked! Thank you.
/usr/local is not SIP-protected. That's what they left /usr/local for. just make sure you set the correct permissions on the directory you create. See my previous comment which I've edited. My first piece of advice is to just stop what you're doing, delete the files you've added, run the homebrew installer and get bowtie2 that way.
Without quotes, it will break if there are spaces in the path or the file name.
Makes sense. Thank you!
yes, thanks. I didn't even realize there was such a thing as brew/science, so I did brew tap science and installed samtools and bowtie2 now through brew. I had to install bismark manually, though. Bioinformatics is a bit of a rabbit hole to get started! I appreciate all of the assistance. 
Don't bother scripting this. Look at something like rundeck or ajenti.
I don't think I understand what you're asking; you gave some examples of path fragments but I have no idea what the context is. Giving it my best shot, though, you might be interested in the `--strip-components` option of the FreeBSD and GNU versions of `tar`. It only works at extraction time, but if for example you have `foo/bar/baz.ext` in your archive, and you want it to extract as `bar/baz.ext`, you can give `--strip-components 1` to strip off the first path segment.
Sorry for the lack of context. I just didn't want to dull it out. The path issue I'm dealing with is that my path looks like /s/Bach/d/under/username/direct/alpha/alpha.txt When taring is there a way I can tar the path smaller? So when I untar it, it doesn't have a humongous path? Thank you for your reply though.
Since you mentioned `alpha/files` in your original post I assume that means that your current directory (`.`) is `direct`, right? If that's the case, I'm not sure why you would have a humongous path in the first place. In your `find` command you're passing a relative path (`.`) and you're only getting directories immediately under it, so you should get a result like this: % tree ./ ./ ├── a/ │ └── file ├── b/ │ └── file └── c/ └── file % find . -maxdepth 1 -mindepth 1 -type d -exec echo '{}' ';' ./a ./b ./c By default `tar` stores the paths exactly as you provide them on the command line, so when you tar up the files their paths should be "anchored" to whatever was returned by `find`: % find . -maxdepth 1 -mindepth 1 -type d -exec tar -cvPf '{}.tgz' '{}' ';' a ./a a ./a/file a ./b a ./b/file a ./c a ./c/file % tar -tf a.tgz ./a/ ./a/file If you were giving an *absolute* path to `find`, I could see why you'd have that problem. If that's the case, you can just `cd` to the directory you want to be in before you run `find`.
that is not how you iterate over array, use for number in "${NUMBERS[@]}" there are other issues as well... also you can use http://www.shellcheck.net/ to easily spot issues and get suggestions to improve
Some thoughts: In your `for` loop, when you write `$NUMBERS`, that's just the first element `951`. To get the full list of numbers, you need to write `"${NUMBERS[@]}"`. Your `until` loop is an infinite loop. There is no syntax error and the script will just keep running forever. You can press Ctrl-C to stop the script and return to the prompt. You will want to replace your `until` with an `if`. You can control and stop the surrounding `for` loop with `break`: for ...; do if [[ $number = 237 ]]; then break fi done # &lt;-- the 'break' will jump to this spot The `=` is technically a test to compare text but works fine here. The test for numbers is `-eq`. You can look up the names of all tests by doing `help test` at the prompt. Using `=` is not a problem, but `&lt;` and `&gt;` can cause hard to find mistakes because a text "73" comes after a text "215". Bash also has some sort of math mode with `((` which has similar operators for numbers as what you might know from C or Java: if (( number == 237 )); then ... if (( number % 2 == 0 )); then ... Bash has `[[` which you will always want to use over `[`. It has some more features and protects against strange mistakes that are possible with `[`.
 ls -l /usr/local/bin/netbeans lrwxrwxrwx 1 root root 31 Jun 16 2014 /usr/local/bin/netbeans -&gt; ../Bins/D.Netbeans/bin/netbeans 
I thought the until loop would work as long as the condition inside the brackets is false...I don't understand why it would be considered "infinite" in this case....
After the first `done`, bash will jump back to the `until [ $number = 237 ]` line. It will then repeat that test there. At this point, the `$number` variable still has content `951` so the test will again be false. Bash then goes through the body of your `until` loop again, again hits the first `done` and jumps back to the `until` line. The `$number` variable at that point will still have content `951`. The test will again be false. Bash then goes through the body of your `until` loop again. ... And so on.
you want a loop that gives you every item in the NUMBERS array and you want to break out of that loop as soon as you reach an item with the value '237'. The `for number in $NUMBERS; do` is that loop (you need a bit of syntax to tell bash you want items from an array, not the whole array), you only have to test for '237' to know when to stop. And a test for even numbers: for number in ${NUMBERS[*]}; do (( number == 237 )) &amp;&amp; break (( number % 2 == 0 )) &amp;&amp; echo ${number} done Working with integers has its own syntax. You use `(( ... ))`, and drop the `$` before a variable name (in most cases). With `(( ... ))` you can write tests as you're used in other languages (`==`, `&gt;=`) 
Thanks
This is easier to solve directly in shell than with `find`. cd /s/Bach/d/under/username/direct for f in *; do [[ -d $f ]] || continue tar -zcf "$f.tgz" "$f" done 
du -sh &lt;dir&gt; is easy to read 
Use .basrc for aliases and such 
I appreciate the reply! &amp;nbsp; I had to change to the directory for it to be exactly what I was looking for like you suggested. For some reason on the outside, I would get a longer path. So when I untar'd it, I had to traverse /s/bach/d/under/username/direct/alpha. However, when I cd into the directory it came out fine as direct/alpha/alpha.txt &amp;nbsp; Thank you for your reply! I'll definitely save it for future reference.
for i in $(ls /Users | grep "Google Drive"); do echo rm -rf "$i"; done Remove the word "echo" when you're ready to actually do it.
 tgts=($(ls /Users | grep 'Google Drive')) That's going to create an array for you. At this point you should probably be made aware of one of the golden rules: [Don't parse ls](http://mywiki.wooledge.org/ParsingLs). Use `find` instead. So your problem is the space in "Google Drive" but we can work around that. Let me setup an example structure for you: $ mkdir /tmp/Test\ Folder\ {1..4} $ find /tmp -type d -name "Test Folder*" 2&gt;/dev/null /tmp/Test Folder 1 /tmp/Test Folder 2 /tmp/Test Folder 3 /tmp/Test Folder 4 Ok, now build the array: $ tgts=($(find /tmp -type d -name "Test Folder*" 2&gt;/dev/null)) $ echo "${tgts[@]}" /tmp/Test Folder 1 /tmp/Test Folder 2 /tmp/Test Folder 3 /tmp/Test Folder 4 Seems ok? Let's try to print out the some of the array's elements: $ echo "${tgts[0]}" /tmp/Test $ echo "${tgts[1]}" Folder Uhhh... so we can see that the array elements are splitting on spaces. The key here is [$IFS](https://en.wikipedia.org/wiki/Internal_field_separator) $ IFS=$'\r\n'; tgts=($(find /tmp -type d -name "Test Folder*" 2&gt;/dev/null)) $ echo "${tgts[0]}" /tmp/Test Folder 1 $ echo "${tgts[1]}" /tmp/Test Folder 2 And now it's working. So we can just loop our way through the array elements: $ for elem in "${tgts[@]}"; do printf 'bing bong %s\n' "${elem}"; done bing bong /tmp/Test Folder 1 bing bong /tmp/Test Folder 2 bing bong /tmp/Test Folder 3 bing bong /tmp/Test Folder 4 
**Internal field separator** For many command line interpreters (“shell”) of Unix operating systems, the internal field separator (abbreviated IFS) refers to a variable which defines the character or characters used to separate a pattern into tokens for some operations. IFS typically includes the space, tab, and the newline. From the bash man page: The shell treats each character of $IFS as a delimiter, and splits the results of the other expansions into words on these characters. If IFS is unset, or its value is exactly &lt;space&gt;&lt;tab&gt;&lt;newline&gt;, the default, then sequences of &lt;space&gt;, &lt;tab&gt;, and &lt;newline&gt; at the beginning and end of the results of the previous expansions are ignored, and any sequence of IFS characters not at the beginning or end serves to delimit words. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Then someone goes and does something like this... ln / ‘/Users/myhomedir/Google Drive’ So make sure you’re sanitising those directories (find with ‘-d’ should be safe). Yet another reason not to attempt parsing with ls ;)
&gt; And now it's working. Well, until a file/directory name contains a carriage return or a newline… you’ve just shifted the problem. I’ll concede that such names are less likely than names with spaces, but it’s really not necessary when there are other ways to achieve this that really work for *any* name (like the one you mentioned below). &gt; `find … -delete` `-delete` isn’t equivalent to `rm -rf {} \;`, it’s equivalent to `rm {} \;` or `rmdir {} \;`: it will fail for nonempty directories. Also, you can optimize this by using `rm -rf {} +` instead of `rm -rf {} \;`.
Why does your sources have a perl script named `test.sh`?
Yeah the name a bit confusing , if I had it as test.pl then cpan installer would try to run it as cpan module unit tests but those ones are not meant for this, these are actually black box tests for Sparrow/Outthentic. But you are right , probably I need to find to better name anyway.
I was getting the feel he/she was just going to bitch about every programming language ever made til i reached the NGS part, that’s cool. 
Thanks! I was just bitching about the obvious candidates for ops tasks :)
I don't see the problem with bash exit codes: echo " #!/bin/bash if [ ! -f "my_file" ] then exit 70 fi" &gt;t ./t;echo $? 70 echo " #!/bin/bash if [ ! --f "my_file" ] then exit 70 fi" &gt;t manolo@Linux-1 ~/Temp $ ./t;echo $? ./t: line 3: [: --f: unary operator expected 0
Good read! 
sed -i '/#/d' $2 - keeping in mind that your "-f" would be $1 -i for saving changes -i.bkp to save changes and create backup file
Your sed-fu is just
&gt; - Functional programming is second level citizen. In particular list/dictionary comprehension is the Pythonic way while I prefer map and filter. Yes, that’s probably one of the features that makes Python easier to learn and suggested first language. Not everything that’s optimized for beginners must be good for more experienced users. It’s OK. There are some libraries that significantly improve Python's functional toolkit, e.g. fn.py and Funcy. They're not perfect (e.g. I believe Funcy doesn't always work well with generators), but when Guido is so against adding functional programming in Python I think they're pretty good. &gt; - Mixed feelings about `array[slice:syntax]`. It’s helpful but `slice:syntax` is only applicable inside `[ ]` , in other places you must use `slice(...)` function to create the same slice object I don't really get this; when would you need to create a slice that you couldn't use `[]` brackets?
Note here that even if you add "set -e" in second case, the exit code will still be zero. I would prefer "exception" or at least non-zero exit code. Imagine this script is called from another script which counts on exit code being correctly set. In addition, the calling script might have much output and you will be missing the stderr regarding the syntax error of `[` . That's the kind of situation I would not like to be in: there is an error and I don't even know about it. That's because apart from small blip on stderr, there is no difference as far as bash is concerned between missing file and syntax error in `[`. I don't see how three possibilities (file exists, file missing, syntax error) could be handled in a straightforward manner by `if` without exceptions. In bash, all exit codes are treated as zero or non-zero, while I prefer more fine grained approach. I hope that clarifies why I do see a problem with how bash handles exit codes. 
&gt; fn.py Looks good. I guess it will still be a bit weird for me to use it in Python. &gt; Funcy I'll have to take a look at both libraries when I have a bit more time. It's a bit strange I haven't seen these being used. &gt; I don't really get this; when would you need to create a slice that you couldn't use [] brackets? Imagine you have a function that processes a slice of an array. Say it can read and optionally modify or delete a slice of an array. I would prefer to pass the array and a slice to the function, not an array, start, end and step. 
Double quote the variable. E.g. `a=$(wmctrl -d | awk '/\*/ {print}'); echo "$a"`
Without looking too much into this, I'm guessing that `*` is probably being treated as a glob. Could you swap your `awk` invocation with `grep` e.g. wmctrl -d | grep '*'
Putting it in quotes worked :)
It is a problem of all interpreted languages. A compiler would detect a syntax error.
grep -v "#" filename
$ echo '"DV35-en_US" = "MacBook Pro (15-inch, Mid 2012)";'|cut -d"=" -f2|cut -d'"' -f2 MacBook Pro (15-inch, Mid 2012)
I'm trying to use `grep` and `sed`, but thanks. In part it's stylistic, and in part for me to learn. 
This is the pipe to `sed` I tried to use: `|sed 's/^.*: //'`
This will give you the members of that key (`CPU Names`) only: defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' And then, since you can probably trust that the data in this case won't contain anything weird, you can pull the names themselves like this: defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' | cut -sd '"' -f 4 On my machine there are actually two entries with the same value, so I guess to eliminate that you'd do: defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' | cut -sd '"' -f 4 | uniq Or possibly: defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' | cut -sd '"' -f 4 | sort -u If you want to use `sed` specifically, you can get the values out like this: defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' | sed -E '/=/!d; s/^.*= "//; s/".*//;' GNU `sed` can also remove duplicate lines, but I'm not sure if the BSD `sed` that comes with macOS can, so you might have to use `uniq` or `sort -u` still. By the way, you can get the model *identifier* (slightly different from the model *name*) with `sysctl -n hw.model`. Not what you asked for, but maybe it's relevant to what you're doing, idk
here with sed: | sed 's/^.*=\s"\(.*\)".*/\1/'
sed 's/\^.\*=\s"\(.\*\)".*/\1/'
&gt; defaults read ~/Library/Preferences/com.apple.SystemProfiler.plist 'CPU Names' | &gt; sed -E '/=/!d; s/^.*= "//; s/".*//;' That's incredible. I was working on this for hours with no luck. I'll have to re-consider my stance on the use of `sed` considering how gory that expression looks. THANKS!
&gt; If you don’t spot the syntax error with your eyes, everything behaves as if the file does not exist. There are three important tools when working on shell scripts: shellcheck, shellcheck, and shellcheck ;)
I agree with a lot of this and it seems that other people have been having similar ideas over the years. Powershell seemed to hint at these issues and [elvish](https://elvish.io) tries to solve a lot of these problems too. As someone who writes bash full time, this looks like it could be a great alternative in the future. Keep up the good work!
.plist is XML, so instead of trying to mess around with regex, you could just extract the element you need directly. something like: `xmllint --xpath "string(plist/dict/dict/string)" com.apple.SystemProfiler.plist`
Bash can actually detect this fine with the `[[` construct. It doesn't have to be compiled -- just statically parsed. Bash doesn't complain about this problem: $ bash -c 'if false; then [ -f foo ]; fi' $ bash -c 'if false; then [ --f foo ]; fi' But it does complain here: $ bash -c 'if false; then [[ -f foo ]]; fi' $ bash -c 'if false; then [[ --f foo ]]; fi' bash: -c: line 0: conditional binary operator expected bash: -c: line 0: syntax error near `foo' bash: -c: line 0: `if false; then [[ --f foo ]]; fi' See these posts for details: http://www.oilshell.org/blog/2016/10/12.html http://www.oilshell.org/blog/2016/10/13.html And more generally this series of posts: http://www.oilshell.org/blog/tags.html?tag=parsing-shell#parsing-shell 
For the sed regex, you can probably also use `sed -n 's/^.*" = "\(.*\)".*$/\1/p'`. Is it necessary to use extended regexps in this case?
Yeah, that's a little more compact. There's no need to use ERE in either case, I just always do because BRE is tedious
This seems like an [XY problem](http://mywiki.wooledge.org/XyProblem). They are already named as you specified. File 1 is `Folder B/File 1` and file 2 is `Folder C/File 2`. Quoting `greycat`: &gt; Literal answers to bad questions can be dangerous.
I think that ```find * -type f | while read f; do mv "$f" "${f:h}/${f/\//,}"; done``` would work for that. Before: ``` . ├── Folder A │ └── File 1 └── Folder B └── File 2 ``` After: ``` . ├── Folder A │ └── Folder A,File 1 └── Folder B └── Folder B,File 2 ```
Thanks. I usually use them because they're the default, but you're right, they are more tedious.
Yeah, been trying all day with -exec, with no success so far. I'm getting a "impossible to access such and such folder: 'Folder_A/B/29.jpg/Folder_A/B,29.jpg' is not a folder" error message. I'll try to figure out what I'm doing wrong tomorrow. Thanks for the help at any rate.
I think it's because `${f/pattern/replacement}` replaces only the first match. To replace all matches, which seems to be what you want, the parameter substitution to use is `${f//pattern/replacement}`. I have edited my answer accordingly.
The problem is `${f:h}`, which is a zsh feature -- in bash you'd need `${f%/*}` It's probably more idiomatic to do this with globs but if you change that the `find` thing should work
Using -F "," is sometimes not enough to extract the 3rd field of a csv file. From the GNU awk manual: &gt; Normally, when using 'FS', 'gawk' defines the fields as the parts of &gt; the record that occur in between each field separator. In other words, &gt; 'FS' defines what a field _is not_, instead of what a field _is_. &gt; However, there are times when you really want to define the fields by &gt; what they are, and not by what they are not. &gt; &gt; The most notorious such case is so-called "comma-separated values" &gt; (CSV) data. Many spreadsheet programs, for example, can export their &gt; data into text files, where each record is terminated with a newline, &gt; and fields are separated by commas. If commas only separated the data, &gt; there wouldn't be an issue. The problem comes when one of the fields &gt; contains an _embedded_ comma. In such cases, most programs embed the &gt; field in double quotes.(1) So, we might have data like this: &gt; &gt; Robbins,Arnold,"1234 A Pretty Street, NE",MyTown,MyState,12345-6789,USA &gt; &gt; The 'FPAT' variable offers a solution for cases like this. The value &gt; of 'FPAT' should be a string that provides a regular expression. This &gt; regular expression describes the contents of each field. &gt; &gt; In the case of CSV data as presented here, each field is either &gt; "anything that is not a comma," or "a double quote, anything that is not &gt; a double quote, and a closing double quote." If written as a regular &gt; expression constant (*note Regexp::), we would have &gt; '/([^,]+)|("[^"]+")/'. Writing this as a string requires us to escape &gt; the double quotes, leading to: &gt; &gt; FPAT = "([^,]+)|(\"[^\"]+\")" 
That code will not safely parse CSVs, thought I'll leave the specifics to /u/moustix who has given a good post. With regards how to print a specific line in awk though - use awk 'NR == &lt;line-number&gt; { print }' i.e :~$ for i in {1..10}; do echo $i ; done | awk 'NR == 5' 5 
I don't know how to get pattern from "/dev/stdin","val" is an user defined variable,in bash shell. /u/moustix has already mentioned FPAT (field pattern)variable,it describes what each field looks like. asort is a built-in array sorting function.Sorting $1 .I should not be using "cat " though. cat race.csv | awk 'BEGIN{FPAT="([^,]+)|(\"[^\"]+\")"} {if((index($1,"\"")&gt;0)) {arr[NR]=substr($1,2,length($1)-2)}else arr[NR]=$1} END{asort(arr);for(i in arr) {print arr[i]}}'| awk -v val=$val 'NR==val{print}'
[Exit and Exit Status](http://tldp.org/LDP/abs/html/exit-status.html) Why not return a success or failure status?
aside from the other comments, you can also pass an assignment to awk with `-v var=value`; `echo "1,2,3" | awk -v myf=2 -F, '{print$myf}'` this is probably easier in combination with a shell script instead of escaping and using double quotes. It also comes in handy when you need to use a single quote in the awk part. If you want proper handling of arguments you should look into getopts, most if not all shells have one builtin. An example of usage for bash is [here](http://wiki.bash-hackers.org/howto/getopts_tutorial)
As that document shows, the exit status of the last command is automatically used. Unless exec’s exit status has some semantics that don’t integrate well somehow (?)
I'm trying it now and it indeed seems to be a little bit faster. execvsend.sh #! /bin/bash [[ $1 == exec ]] &amp;&amp; exec ls /tmp &amp;&gt;/dev/null || ls /tmp &amp;&gt;/dev/null in the shell $ time for ((i=0;i&lt;1000;i++)); do ./execvsend.sh exec; done real 0m1.964s user 0m1.615s sys 0m0.374s $ time for ((i=0;i&lt;1000;i++)); do ./execvsend.sh exec; done real 0m1.880s user 0m1.515s sys 0m0.389s $ time for ((i=0;i&lt;1000;i++)); do ./execvsend.sh; done real 0m2.088s user 0m1.806s sys 0m0.324s $ time for ((i=0;i&lt;1000;i++)); do ./execvsend.sh; done real 0m2.082s user 0m1.826s sys 0m0.298s I also tried it with a script that checks if `$1` is "end" and then runs `ls` normally or else `exec`, and the times are pretty much the same.
Thank you so much! This worked perfectly. Would it be possible to explain this part to me? &gt; "${f%/*}/${f//\//,}" 
Nice response, I never knew about FPAT in gawk! Just nitpicking, but you forgot to escape the carets in the regex near the end of the part copied from the manual, it looks like ([^,]+)|(\"[^\"]+\") but it should look like ([\^,]+)|("[\^"]+")
`${f%/*}` -- `${x%y}` expands to the value of variable `x`, with the shortest substring matching pattern `y` stripped off the end. For example, if `x=foo`, then `${x%o}` is equal to `fo`. In this case we're stripping off the last slash-delimited segment of the path (the file name) `${f//\//,}` -- `${x//y/z}` expands to the value of variable `x`, with all occurrences of pattern `y` replaced by `z`. For example, if `x=foo`, then `${x//o/a}` is equal to `faa`. In this case we're replacing all of the slashes in the path by commas https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
If using exec, does that impact traps that were defined in the parent script?
It's not necessarily a performance win as much as it is a process marshaling issue. Looking at this: $ cat tst #!/bin/bash ps -f $ ./tst UID PID PPID C STIME TTY TIME CMD user 5141 5140 0 18:15 pts/0 00:00:00 -ksh user 5185 5141 0 18:18 pts/0 00:00:00 /bin/bash ./tst user 5186 5185 0 18:18 pts/0 00:00:00 ps -f Note that running the script first starts a bash process which waits around taking up a process slot until the ps process completes. When I edit in an exec before the ps, the ps uses the process slot from the bash which disappears from the system: $ cat tst #!/bin/bash exec ps -f $ ./tst UID PID PPID C STIME TTY TIME CMD user 5141 5140 0 18:15 pts/0 00:00:00 -ksh user 5189 5141 0 18:18 pts/0 00:00:00 ps -f So yeah, there's a performance win where you won't have a bunch of dormant bash/shell processes clogging up your process table. (It's nice to keep a tidy process table!) 
Yes, apparently they won't run.
Sendmail?
My provider doesn't allow SMTP
bash actually exposes TCP sockets via `/dev/tcp`, so if you can start a server somewhere you can just dump shit into it like you would a file. For example: # SERVER: Start listening on port 1234, log incoming data to file $ nc -kl 1234 | tee -a server.log # CLIENT: Send data to server $ echo hello &gt; /dev/tcp/127.0.0.1/1234 It's not really the most robust method, but for something simple that doesn't require any fancy dependencies it should work I guess.
Sorry but which command is to be run on the vps and which on the local computer? If I understand correctly the server in the example above is the my local computer. If that's the case I can't do it cuz my local computer is behind a carrier grade NAT. Or am I getting confused?
Sorry but which command is to be run on the vps and which on the local computer? If I understand correctly the server in the example above is the my local computer. If that's the case I can't do it cuz my local computer is behind a carrier grade NAT. Or am I getting confused?
VPS=server Your computer=client Try the following to get your public IP, without hitting a public IP you can't access the internet. curl ifconfig.me Most home firewalls will close all outgoing ports by default; so you might want to open one (in this example 1234). There's a similar explanation on [this page](https://www.digitalocean.com/community/tutorials/how-to-use-netcat-to-establish-and-test-tcp-and-udp-connections-on-a-vps). I think what you have is a server and no access to it and you want to push a key to it?? If that's the case running commands on it is likely to be difficult; but basic webpages is probably best (perhaps github? Or as you've said pastebin).
&gt;Or am I getting confused? No, that's right. If you don't have a machine that can be configured as a (reachable) server then I can't see how you're going to be able to initiate a direct connection to anything local from the other end. You could try an SSH reverse tunnel, I guess, but at that point you might as well just use a regular SSH connection to pull down a file or whatever. You can always just pre-encrypt your data and use `curl` to send it to any paste bin or whatever. Some services (like ptpb) let you specify the URL, so you don't need to transmit it back if you come up with a naming convention. Others like Gist are tied to an account that you can just log in to. openssl enc -e -a -bf-cbc -k mypass -in myfile | curl ...
My ISP double NATs my connection. Meaning I can't forward any ports unless I get a business plan. Anyway I can echo to the file(from within the server TO the server) and view it by going to http://serverip:1234/? Or something
Like you just want to make something on the VPS available via HTTP? `python -m SimpleHTTPServer 1234` will host the contents of the current directory via HTTP port 1234.
I do this very easily by using SaltStack. Not sure how much time you want to spend setting up a master, and installing an agent, but it’s been proven very useful for me in a similar situation as yours. Master-Minion communicate over ports 4505,4506
while : ; do if ! ping -c3 Google.com; then systemctl restart openvpn-client@profile # you should use your daemon manager fi sleep 5 done
i don't like scripts that rely on pinging a thing. What happens if Google changes and stops replying to ICMP? You're while loop should be looking to see if the VPN interface has an IP address assigned to it. That way your script doesn't need to 'leave the box' in order to work and you have some control over the elements it's checking for.
Simply having an IP on an interface doesn't mean connectivity. There has to be some kind of connectivity test. Ping is fine. 
a VPN interface isn't going to have an IP if it's not connected as the address is assigned via DHCP. No connection, no address. Considering this guy is a self-described 'noob' and his VPN has a killswitch, I'm also guessing he's using Private Internet Access as his VPN. So this isn't some VPN you'd manually configure and control both ends of. It's just a consumer VPN service. He'll probably be fine pinging something. I think think it's bad form to depend on a variable that's not within your control and runs the potential, however slim, or not giving a consistent result. I'm doubling down on the fact that checking if the interface has an IP would be less likely to faulty.
DHCP has a lease time. It may be short, but the interface doesn't just drop the address the second the VPN server goes away.
Yes it does.
You'll often see execs in scripts that launch a long-running process as their final action, such as a java process. This replaces sh with java. It saves a 'skosh' of memory to not have both sh and java running concurrently, plus any signals sent by the parent process are correctly propagated to java. I'm seeing this used very frequently in Docker containers as well.
You can create block partitions using drbd but it's not very easy and sometimes it's a downright pain in the ass. But if you get it set up you can also use something called Heartbeat which will sync the processing powers of the two servers together. This involves repartitioning drives and ultimately you lose some hard drive capacity because drbd replicates itself across all machines and you have to repartition a drive to be used for this. It's essentially creating 1 file system across all the servers so each one has to create a partition that will replicate the partition on the main controller. 
As a general rule "`eval` is evil", however &gt;but since I control the inputs 100% it's up to me to be smart, right? Yes. `eval` has its place, and as far as I'm concerned, so long as you're using it safely it's fine.
Am I the only one bothered by how the fonts used in "code sections" always have lowercase Ls that are literally just 1s?
My Guess is that you should be doing; run.sh "thing1 &amp;&amp; run.sh thing2"
 &amp;&amp; means and || means or ; means run regardless... That's typically based on exit codes. If you always want something to run, do ";" so: run.sh "thing1; thing2"
This might also work: systemctl set-property Restart=always openvpn-client@profile
i think it's a well readable font where it's not hard to tells ones and ells apart: [screenshot](https://i.imgur.com/ORoByIU.png) My browser renders this as [Liberation Mono](https://www.fontsquirrel.com/fonts/liberation-mono). Yours as well?
I think you meant: run.sh thing1 &amp;&amp; run.sh thing2 What you wrote would still require `eval` to parse the `&amp;&amp;`. But even this way, I'm guessing OP doesn't want the before and after stuff that `run.sh` does to happen twice. If OP only needs the ability to run multiple commands in a row (and no other advanced shell stuff). Then I'd suggest having `run.sh` run each of its arguments. As long as the commands to run are the only arguments to `run.sh`, it would look something like this: # the before stuff ... for command in "$@" ; do ${command} done # the after stuff ... The nice thing about that is if you need to send arguments to any of the subcommands, I believe it would work fine as long as you called it like: run.sh "cmd1 arg1 arg2" "cmd2 argA argB" 
I'm wondering though, is `eval "${command}"` really any more dangerous than `${command}`? What if the argument I sent in was: `eval something_evil`? Wouldn't the second version (what OP currently has), still run the evil `eval`?
you can add another pipe to `awk '{print $2, $1}'`
`awk '{print $2 $1}'` is the simplest way off the top of my head.
the first one worked well! thanks! 
You have two `if`s, but only one `fi`. Use indentation to help avoid errors like this.
You need to surround the escape characters with `\[` and `\]`. That tells bash that the stuff in between doesn't actually take up space in the terminal. See [BashFAQ/053](http://mywiki.wooledge.org/BashFAQ/053).
Cool, I knew those had to go in there somewhere! Thanks a lot!
You need to escape non-printing characters in a block so that the system can keep track of how big your prompt "really" is. In your case it should be: ``` PS1='\[\e[0;32m\]\u @ \W $ \[\e[m\]' ```
In the man-page, following the list of parameters, there's this paragraph trying to explain how the KEYDEF thing works: &gt; KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a field number and C a character position in the field; both are origin 1, and the stop position defaults to the line's end. If neither -t nor -b is in effect, characters in a field are counted from the beginning of the preceding whitespace. OPTS is one or more single-letter ordering options [bdfgiMhnRrV], which override global ordering options for that key. If no key is given, use the entire line as the key. Use --debug to diagnose incorrect key usage. The way I understand it, the `-k1` does nothing in your example. The number in your input is the first field, and when you do `-k1` instead of `-k1,1`, then it will continue looking at the following fields until the end of the line when it sorts. That's basically a description of what sort's normal behavior is. Anyway, I think you have a bug. You will want to do this here: sort -b -k1,1n -k2,2f If you try your current `sort` or `sort -k1` on input like the following, you'll see the number won't be treated correctly and the `12` ends up in front of `1`: 12 cca 1 cca 1 agg 
[Inconsolata Go](http://levien.com/type/myfonts/inconsolata/), available as a patched nerd-font [here](https://github.com/ryanoasis/nerd-fonts/tree/master/patched-fonts/InconsolataGo).
I'm using [Hack](https://github.com/source-foundry/Hack) everywhere I can. 
Thanks
Thank you
I've tried it. Looks amazing for both English and Armenian languages. Thank you.
[Source Code Pro](https://github.com/adobe-fonts/source-code-pro) everywhere I go.
I'm glad you enjoy it. 
It looks good! Thank you.
I'll try with `sort -k1nr,1 -k2bf` but just using `sort -k1r` seems to give me the right output as you can see [here](https://i.imgur.com/LI0bgY3.png) which is kind of weird considering your explanation on how `-k` works.
+1
 $ sort -k1nr,1 -k2bf &lt;&lt;EOF &gt; 1 cca &gt; 1 gta &gt; 2 tga &gt; 3 agg &gt; EOF 3 agg 2 tga 1 cca 1 gta When you use `-k1r` instead of `-k1nr`, it will break when the numbers start getting longer. It will for example think that "12" is sorted in front of a "5" because it will compare it as text, not as numbers. It will think as text characters it's "1"&lt;"5", so you get "12"&lt;"5".
Oh great! Thank you! I was also digging around and found detailed information regarding sort (`info sort`). As far as I understand, what `sort -k1nr,1 -k2bf` is doing is: Sort the whole first field numerically and in reverse order and resolve ties by sorting alphabetically on the second field. Since the file only has lowercase letters, I was thinking I could get rid of the flag `f` and end up with this: `sort -k1nr,1 -k2b`. &gt; ... it will start complaining when you try to use -k1nr because the next field might not be a number. I guess this is the same reason why I don't need to do `-k2bf,2` since the second field is just letters (at least in this case) and a newline.
Yeah, also a last thing, I added the `-b` because the `--debug` suggested that. It printed this message: sort: leading blanks are significant in key 2; consider also specifying 'b' I don't really know why this is important because when I experiment here by adding a different amount of spaces, it still sorts correctly. I mean it's always "agg" &lt; "cca", no matter where I add some spaces. The `--debug` output shows the space underlined when not using a `-b`: 2 cca _ ______ _______ With the `-b` added, it looks like this: 2 cca _ ___ _______ 
Oh, that's good to know! I'm going to keep the `b` then. Sometimes the manpages aren't the most beginner-friendly so you were really helpful. Thank you very much!
gohufont
`dpkg -L coreutils` for the deb people
Or the [manual](https://www.gnu.org/software/coreutils/manual/) (corresponds to the info pages): * [HTML version](https://www.gnu.org/software/coreutils/manual/coreutils.html) * [PDF version](https://www.gnu.org/software/coreutils/manual/coreutils.pdf)
sans-serif
Best solution: sudo pacman -S zsh zsh-completions ! haha
Terminus is my favorite.
`pacman -Ql coreutils | sed '/bin/!d'` on arch.
Could you explain what `a[$3]++` does exactly?
awk has associative arrays... so instead of number based index, one can use any number or string constant as key... default value in numeric context for uninitialized variables is `0` so `a[$3]++` helps to build an array with 3rd field as key and number of occurrences of that key as value
Pt mono
Oh, clever! Thanks! :)
If you comment out https://github.com/GingertronMk1/ANWR/blob/master/Dotfiles/.bash_profile#L15 and re-run bash, does tab completion start working again?
Check out [fdupes](https://github.com/adrianlopezroche/fdupes).
grep bin/
It doesn't, no
leaving that commented out, how about if you also remove the tab complete related lines from https://github.com/GingertronMk1/ANWR/blob/master/Dotfiles/.inputrc ?
Hi, I appearantly wrote such a script, that searches for ovpn files in folder, extracting the remote server, pinging it and stores the server and ping with öower then x ping (50) in a txt protocol. it uses simple bash commands, and it doesn't need any extra packages. #!/bin/bash maxping="50"; file="pingtest.txt"; count=`ls -1 | grep .ovpn | wc -l`; echo "$count Servers found"; echo "" &gt; $file; echo "Please have patience, it will take a long time, depending on the found .ovpn files"; echo "Puting Server in file $file, wich ping is smaller then $maxping ms"; for i in `dir | grep .ovpn` do output=`cat $i | grep "remote "`; output=`echo $output | cut -c 7- | rev | cut -c 4- | rev`; ping1=`ping $output -c 1 | grep time=`; ping1=`echo $ping1 | sed 's/^.*time=/time=/'`; ping2=$ping1; ping1=`echo $ping1 | cut -c 6- | rev | cut -c 3- | rev`; ping1=`echo $ping1 | cut -f1 -d"."`; if [ $ping1 -lt $maxping ]; then echo "$i --&gt; $output --&gt; $ping1 ms" &gt;&gt; $file; fi done 
What about overwriting your `.bashrc` and `.profile` with the system defaults from `/etc/skeleton` or `etc/skel`?
Which would be...? I would be more help myself but I set these like 2 years ago when I only had a Mac and they worked there
Try lines 1, 9, 19, 20?
No joy there
Nothing doing
Are you reloading bash when trying these potential solutions?
Hmm, I thought I was by re-sourcing the dotfiles, but exiting and re-sshing into the machine has actually done it. I'll have a play around with what exactly the cause it, but thank you!
Sanity check: after updating your ~/.inputrc, did you [reload it](https://superuser.com/questions/241187/how-do-i-reload-inputrc)?
It's probably counting the new-line character.
Yeah, in vim this can be disabled with `:set nofixeol` (only in vim though, not in vi).
Thank you, you're right I just learned about hidden characters (https://imgur.com/Ef4jJqf). I had a 12 character string in the file and I was trying to see if the file is a multiple of 4 but it ends up being 13 characters when I use wc -m. Is there a proper way to check if the contents in the file are a multiple of 4? Or should I just grab the length and then subtract by 1?
I don't know if there's a proper way to do this. I guess you could subtract wc -l instead of 1 just to make sure: $(( $(wc -m filename) - $(wc -l filename) )) I feel like there should be a better way to count.
I *think* a hard link is just another name for the same file on disk, meaning you could just delete the link you don't want. I'm not 100% sure on this, and obviously you don't want to make a mistake, so try with a dummy file or try looking at how hard links work a little more in depth. 
I have more than 300 files, I thought people on reddit can do this.
[removed]
It's not very destructive, but would eat some CPU cycles and bandwidth. It simply downloads somesite to /dev/null in an endless loop, that's not forking itself and easily stopped.
You have to double-quote the variable when you use it: cd "$DROPBOX"
This worked! I had to remove the \\'s in the quotes to get it to fully work.
Thanks, my cli skills are very basic. I thought that was about it, just the discussion where I saw it in look fishy as all get out. 
Just delete the files you sent out my want. All files are hard links. Deleting and moving is entirely unnecessary. https://unix.stackexchange.com/questions/50179/what-happens-when-you-delete-a-hard-link
Well see the point of me doing it this way is so that i can throw it on any VM or server i work on with out having to install something. I can just copy it to what ever directory, run it and it will do the deed. Although it looks like a nice backup solution.
you mean zsh-completions has this built-in feature?
Yeah, a backslash within *single* quotes (`'`) counts as a *literal backslash* - basically, a single-quoted string includes *everything until the matching single-quote* literally. These are the four ways you could write the intended variable assignment: DROPBOX=~/"Dropbox\ -\ Personal/Dropbox" # the backslashes here are redundant, but valid DROPBOX=~/Dropbox\ -\ Personal/Dropbox # the backslashes here escape the spaces, making the whole context one large unquoted string DROPBOX=~/"Dropbox - Personal/Dropbox" # the spaces here are enclosed by the quotes DROPBOX=~/'Dropbox - Personal/Dropbox' # same as above However, `~/'Dropbox\ -\ Personal/Dropbox'` is equivalent to `~/"Dropbox\\ -\\ Personal/Dropbox"` (or `~/Dropbox\\\ -\\\ Personal/Dropbox`) - it's describing the string with *literal backslashes*: `~/Dropbox\ -\ Personal/Dropbox`.
Glad I could help. It's always worth it to try the simple things!
Fantastic reference. Thank you for this. I typically use double quotes out of habit. My post was my last iteration. 
&gt; same command - rar, maybe for my own filename extension, You can always use alias for those tasks
Summing up you don't have a favorite pl cause none satisfy your needs completely Well, I can guess you don't like anyone too! haha
this is why sysadmins drink. that aside: depending on the job, you might want to look at ansible.
Speaking generally: * The `BASH_VERSINFO` array and `BASH_VERSION` scalar will give you information about the version of bash you're running * The `uname` utility will give you the OS type (Darwin, Linux, etc.). Given different options it can also provide the kernel version and other information * Many operating systems have some way of getting additional information about the OS. For example, macOS has `sw_vers` for getting the macOS product version, and most Linux systems have `lsb_release` (`/etc/lsb-release`) for getting distribution information * Most UNIX-like systems provide command-line utilities which conform roughly to POSIX, so they should always have the standard tools like `awk` and `sed`, and those tools should support the basic POSIX options regardless of version. If you want your script to be portable you will have to avoid using non-standard options, or else use the output of `uname` or similar to make decisions about alternative options * You can use the output of `--help` or `--version` to see what features a tool supports. For example, `[[ "$( mycmd --help 2&gt;&amp;1 )" == *'--foo'* ]]` might tell you if your version of `mycmd` supports the option `--foo` * `type -p` and friends can tell you if a given utility is available on your system * `typeset -f` can tell you if/how a function is defined * `[[ -n "${foo}" ]]`, `[[ -v foo ]]`, etc., can tell you if the variable `foo` is set
 shopt -s globstar #to enable ** md5sum ** | sort | uniq -D -w 32
Check out a tool named `shellcheck`. It tries to guess mistakes in shell scripts. It would have printed a message about the quotes problem in `cd $DROPBOX`. Your distro probably has a package for it. You can also try it online at www.shellcheck.net.
Show example of desired end result as well as describing it. Someone's suggested simply deleting unwanted files and been upvoted but I'm not sure that on its own will do what you want but I'm not sure because you only show a not very clear example of current state, no matter sample of desired state
&gt;I'm tired, sorry if it's confusing but let me know if I can clear it up. Please, do provide as much context as you can. It doesn't sound all that dramatic so far.
Hard links have no hierarchy the same way symbolic links do. With symbolic links, one file is the real one and one is just a link. If you remove the real file the link will no longer work. With hard links however, both files are real and point to the exact same underlying data. So you can easily remove either one of them in any order, and until you've removed the last hard link the underlying file will still be there. If you want to remove all hard links and only keep one "copy" of the file, then `find` can do that for you. This will find any file that has *more than* 1 hard link linking to the underlying file and delete them. Any file with just 1 hard link to it is just a normal file. It is safe to run even if both hard links are in the same directory, in which case it will delete one of them and keep the other. find /path/to/directory -links +1 -delete 
I don't know what's not linked, there probably 50 GB of files are laying there. The source folder is huge, and re-linking everything will be manual which is a huge pain. Is it super hard to do? or impossible? Why the hell am I getting downvoted?
There is a small misunderstanding here, files are not hard linked to another file, they are all linked to the inode, the number you get from that script, which lets's say somehow points to an area on your hard disk. Thus, */var/www/files/source/Just.A.File.ext* */home/User/Folder/Just A File/Just A File (Something).ext* are just different tags for the same binary data residing on one sigle area on your hard disk. There is no need for all the actions you said, simply remove */var/www/files/source/Just.A.File.ext* Now, if you want somebody to do a script that does that for you, remove all the files from source folder if they already have a hard link in destination folder, I personally would not use bash since it is not my strength, but it looks like with a small modification, the script you mentioned could simply delete the files. Like maybe *do not print the inode, | grep "/source" | -rm -f* or something. Good luck! 
wait a minute, so if I delete the source then the destination will stay there?
if I link files by "ln -s" is that a hard link? Sorry if my question is painful, people hate me here I don't know why.
I think you could write your own completion function and start it by calling the existing completion function and then append to `COMPREPLY`, and then use that new function for the completion. But I’m not sure if that’ll work.
Or read it on the command line, with either `info coreutils` or `emacs -f info-standalone coreutils`. (Personally, I prefer the latter, and have aliased `info` to `emacs -f info-standalone`.)
No, that would be a symbolic link. However, if you used symbolic links then the script in your OP would not have worked. It would help if you ran a command such as `tree --inodes` on the relevant directories and uploaded the result to a pastebin-like site. Without knowing what the directory structure looks like, it's difficult to help.
wouldn't this tie up a port until stopped? what port is curl using? does anything else use that port which would in effect cause a DNS for that app/script. Just curious. 
I think because you don't understand what hard links are and some of your comments make it seem like you don't want to understand, you just want someone to tell you what to do. Read the link I posted. They explain in more detail and also provide a link to an in depth explanation.
If it is a hard link then yes. And if you have the same inode number from your script (290522350), it is definitely a hard link. Make a test if you are skeptical.
No, `ln -s` creates a *soft link*, which is a reference to a file that can be broken (see the parent to your comment). For more information, try reading `man ls` or looking up an in-depth guide online. As far as the second part of your comment, I don't know if people hate you, but they may be put off by what seems to be a combative response style on your part. For example, in your reply to my comment on this thread you disparagingly said, "I thought Reddit could [fix my problem]." We're trying to! The rest of the comments in this thread (that I've seen) are giving you solutions; if you don't understand try asking clarifying questions instead of putting up your hands in a huff. People will be more willing to help. 
The local end will use a dynamic source port, so doesn't tie up any particular port that something else needs to use. The remote end is certainly able to handle more than one concurrent connection. 
You could try writing your scripts using the lowest common denominator. For shell, for example, don't use [Bashisms](http://mywiki.wooledge.org/Bashism), only elements specified by POSIX. For sed and awk, don't use any GNU extensions - and so on. If something works for an older version, it'll most likely also run on a newer version, but you only have to write it once. Examples: * Shell: * No associative arrays * Technically, no arrays either (but maybe you know that you have arrays everywhere?) * No `[[ ]]` compound tests, only `[ ]` * [This list](https://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html#Bash-POSIX-Mode) * Awk: * No arrays of arrays * `BEGINFILE`/`ENDFILE` * In-place file editing * [This list](https://www.gnu.org/software/gawk/manual/gawk.html#POSIX_002fGNU) * Sed: * No in-place file editing * Commands like `e`, `F`, `R`, `T` * No using `\t` and `\n` everywhere without worries * [This list](https://www.gnu.org/software/sed/manual/sed.html#Extended-Commands) * [Excellent Stack Overflow answer](https://stackoverflow.com/questions/24275070/sed-not-giving-me-correct-substitute-operation-for-newline-with-mac-difference/24276470#24276470) about differences between GNU sed and macOS sed
[Shellcheck](https://www.shellcheck.net/) can help with this.
I'm very very thankful, I thought you guys couldn't do a script but instead I understood that there is no need of one. Thanks for the valuable lesson. And I did delete everything in the source, and no major files were lost. Thanks!
Thank you! I never knew this, i'm very thankful for the solution+new lesson. 
Deleting the source worked, thank you!
This is because you have to logout of your shell session and log back in for .bashrc and bash_profile to reload. You can also get around this by doing: source ~/.bash_profile
Rad, I'm so glad it worked! And yeah, a script would've been my first thought too: I probably would've spent way too long over-engineering something that barely sort of worked once. Just trying to save you the trouble! Glad it all worked out.
me too.. 
always use `#/bin/env shellname` to determine the installation path of the shell you are writing for. preferably use posix shell and avoid bashisms 
&gt;always use `#!/bin/env shellname` Except for when `env` is in `/usr/bin`... or when your `$PATH` is fucky... or a [whole range of reasons](https://unix.stackexchange.com/a/29620) The better approach IMHO/IME is to simply be explicit: If `/bin/bash` isn't there, symlink it so that it is. I do have to concede that this is speaking from the point of view of someone who sysadmins hundreds of servers, so obviously what works for me won't work for someone who doesn't have permissions to create symlinks in `/bin` or isn't managing a fleet of hosts. For me, though, I have a few Ansible roles dedicated to straightening out legacy hosts, primarily Solaris, that have `bash` and `sudo` installed god-knows-where by the dumbass cowboys who set them up. And that comes back to the "desired state" mentality of Ansible and its kin: my desired state is for `/bin/bash` to be there - if it's not, make it happen. `/usr/bin/env bash` has its place, but it's not some super-robust gold-plated standard. It is fallible.
Input Mono, CPMono_v07
superb,awesome thanks learnt something new
Does it not print “$bar $foo”? I’m on the shitter and my home cluster is down because I’m moving so I can’t even get the answer via sshing PLEASE TELL ME WHAT HAPPENS
You are right, that is the answer. My linux prof. makes all of his exam questions like this and although it is annoying, I found it does help me when trying to debug BASH scripts. 
No, it doesn't. :) 
Upvouting so I can check later the explanation to the answer.
my guess would be '''$bar $bar''' ?
This would print `$foo`, I'm guessing.
Being pedantic though, you're calling this quoting in bash but calling sh
The suspense is killing me. Time to spoil it: ➜ foo='$bar'; bar=foo sh &lt;&lt;EOF echo "$foo" '$foo' EOF foo $bar Because? ➜ echo $foo $bar ➜ echo $bar ➜ cat bar.sh echo $bar ➜ ~ ./bar.sh ➜ ~ bar=foo ./bar.sh foo 
Haha yes, should have called it "quoting in bash and sh", or I could have just called `bash` instead of `sh`. 
Thank you. I didn't realize that the variables of the echo are evaluated twice. First by bash, secondly by sh.
Was there context around how they were using the command? I find myself doing this or variants or during deployments as a manual test that a path is live. ```while true; do curl --write-out '%{http_code}' --silent --output /dev/null $URL ; done ```
Use a function instead of an alias. Inside the function, the arguments to the function are in $@ or $1, $2, etc. That $# you remember is also available.
That worked like a charm! Thank you so much, you're a real life-saver!
After fully implementing this, it appears the arguments just disappear when you call the function. :(
I guess I don't understand what you are trying to do and what's going wrong. I just tried the following: I created a file test.sh with these contents: echo "$@" echo "$#" When I call it like this, the $# works like it should: $ . test.sh a b c a b c 3 I also tried creating an alias and it works the same: $ alias testalias='. test.sh' $ testalias a b c a b c 3 
So that works for me as well. However, try running it with no arguments. You should get results with the history of the arguments you used in the past. Almost like it's using a history of sorts. 
I see this: $ . test.sh 0 $ . test.sh a b c a b c 3 $ . test.sh 0 
Edit sudoers
If you're ok with sudo being allowed without a password you can change your sudoers file to have this line (you can make a special group if you don't want to edit the sudo group's permissions. %sudo ALL=(ALL) NOPASSWD:ALL
`ssh -t` is what you're after. You may also like to look into ansible.
I can't actually ssh as root and editing sudoers is not permitted. Sorry, should have mentioned that.
We have an LDAP environment. Also, I'm not permitted to tinker with visudo. 
Can you edit the permissions of these files? Also. You sound like you're doing shit you shouldn't be. Unless your boss gave you this task and then didn't give you any resources to achieve it with. 
Nah, I'm just retrieving a bunch of info for a customer. The issue is that I work in a highly restrictive environment with paranoid security IT personnel. I can edit permissions, but again I would have to do that as sudo. 
Can we see your code (even a sanitised version)? It sounds like you've got a variable that isn't being handled properly...
That got me in a better spot than were I was before, Thanks!
If you have legitimate reasons for doing these tasks then you can use Ansible, log in as a user and then assume root/sudo on the target. If that isn't acceptable then I'd question why you were trying to access files you didn't have permissions for
`sudo gdebi ./fontconfig-infinality_1-2_all.deb ./libfreetype-infinality6_2.4.9-3_amd64.deb`
`foo='$bar';` sets the variable `foo` to the literal string "$bar" without expanding the variable `$bar` (which may or may not have been defined). `bar=foo sh` executes the `sh` program setting a variable `bar` to the string "foo" in `sh`'s environment, then starts an inline document with `&lt;&lt;` to be piped to `sh`'s standard input. The `EOF` denotes the start and end of the inline document. The inline document piped to `sh` is a shell script `echo "$foo" '$foo'` which is executed by `sh`. The `echo` command then outputs: $bar $foo
And what end result do you want? Display each group of duplicates and ask the user which file of that group to keep and which to delete?
Don't you need a root login and password to use `ssh -t`? OP is using sudo, what if elevating permissions is only possible with sudo and SSH cannot be used?
Trust me. I'm a sysadmin ;) Here's an example I just grabbed from a colleague's old pre-ansible documentation: for host in $(&lt;/path/to/serverlist); do echo "+++ $host" &amp;&amp; ssh -t ${host} "echo 'YOUR_OWN_PASSWORD' | sudo -S /usr/sbin/userdel -r [USERNAME]"; done I personally used a slightly different method but it was fundamentally the same. I'd recommend against this practice anyway, as your password can be visible in plaintext. Because OP appears to be NOPASSWD'ing, then it shouldn't be a problem. 
^^this. “Highly restrictive IT policies” are there for a reason. Either review the policies or review your actions. 
Already figured it but thanks anyway, i skipped one file lol
Sure. I wrote a test script while playing around with this: #!/bin/bash echo "$#" echo "$@"
I have reviewed them. They look good.
This is what I get: $ ./test.sh 0 $ . ./test.sh a b c 3 a b c $ . ./test.sh 112 {} a b c ...
 -t Force pseudo-terminal allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services. Multiple -t options force tty allocation, even if ssh has no local tty. Nothing about `-t` requires root privileges. What it allows, however, is for people to use `sudo` when `requiretty` is set.
If ownership is an issue, you could create a group for the log files to be owned by that includes the people who need to read the files, and ensure the files have 660 perms. I realize _you_ can't modify the files or do that, but you could put in the request to whoever can. Be sure to have them update any log rotation tools to set the proper group ownership when logs are rotated, as well.
Yes - this is the right answer. I mean the part about Ansible. :-) (`ssh -t` is also spot-on)
In fact, the code will print foo $bar Here is a breakdown of what happens: - First, `bash` does parameter expansion of *the two instances* of `$foo` inside the `heredoc`, even the one within single quotes (just like it would with `$world` in echo "Hello '$b'"` ), so the code that is passed to `sh` is echo "$bar" '$foo' - Then `sh` runs in a subprocess where only `bar` is defined, because we haven't exported `$foo`. Since `$bar` appears within double quotes, it gets expanded to `foo`, whereas `'$foo'` is interpreted literally because of the single quotes.
Another option would be to ask if the target hosts are logging the required data to a syslog/logging server of some description. If that's the case, then stop fucking about with rickety `ssh`+`sudo` loops and get it from the one place. 
I found out if you put something into $1, $2, etc. at the command line, those values will get seen by the sourced script. If you don't know what I'm talking about, you can write into $1, $2, etc. with `set --`, like so: set -- a b c d e f At this point, $1 is "a", $2 is "b", etc. $# is 6, $@ is a, b, c, ... When I now run the test script, I see this: $ set -- a b c d e f $ . test.sh 6 a b c d e f Perhaps something in your bash configuration is running `set -- ...`? There might be for example a bug in whatever you have in PROMPT_COMMAND for special prompt behavior.
That seems really weird. Going off the other discussions, I assume `test.h` looks like this? #!/bin/bash echo "$#" echo "$@" Can you run this? echo -e "SHELL=${SHELL}\n $(${SHELL} --version)" I'm on bash 4.4.12 and my output is fine. Also try this: ./test.sh . ./test.sh a b c read -d '' -e -t 0.1 -n 10000 . ./test.sh
Nah, I just wanted to show separated groups of each duplicate to the user and where they were. I ended up making another command with awk that grabbed the first unique hash from the list of dupes and had it test against each one or replace what it tested with when it was different afterwards. 
 cat GPYLiGbW.txt | while read; do if [[ "$REPLY" =~ Power\ Supply\ [0-9][^|]*\|\ ([0-9]*) ]]; then (( Watts += ${BASH_REMATCH[1]} )) else if [[ $Watts ]]; then printf '%-17s| %-11s| Watts | ok | na | na | na | na | na | na \n' 'Power Meter' "$Watts" Watts= else echo "$REPLY" fi fi done 
I'd change the script you already got instead of writing a second one that changes its output. At the end of the script's get_impi_data() function, you could add code that looks for the power supplies entries in the list and sums up their values, then creates a new "power meter" entry for the result. I don't think I can write this without mistakes. I would need to experiment. Can you show an example of the ipmitool command's output that's run by your script to get input data? It's the following command in the script you shared: ipmitool -I lanplus -U Username -P MyPassword -H 192.168.1.60 sensor With an example output, experimenting with the script should be possible.
I attached the output to the OP, and I'm just looking to modify the existing script to incorporate adding the 2 power supplies values together 
Looks like I have 4.1.2 so I wonder if this is a problem that was addressed in a minor update somewhere. To the release notes!
Hey appreciate the reply, where would that fit in the current script? I only know enough about bash and scripting to break it every 4th edit. 
I think the chance of it being a version issue is very very slim, but at least that is something, play around with. If I am understanding you correctly, I don't think a bug that big would ever make a realease.
That's a fair point. I'm still messing with it. During my time of development, I found some code to grab arguments and there was a "set --" statement in there so I'm trying to figure out if that's the cause.
Another thing to look at is to see if you have anything funny in `~/.bashrc`, `/etc/bashrc`, `~/.bash_profile` or anything else that can effect your environment.
Alright guys, I figured it out. During development, I had the following line: set -- "${POSITIONAL[@]}" Once I removed that line, everything started working as expected. Quite honestly, I'm not sure why I included that line without fully testing it but I'm still new to bash scripting. Anyway, running a "set --" to remove all positional parameters and then testing the script returned success! Thanks for your help guys!
Use `sed -i`.
Thank you! Now it works like charm :)
When executing commands with redirects in them, one thing to keep in mind is that the thing on the right-side of the redirect (either the `|` or the `&gt;&gt;`) is going to execute as your current user, regardless of what user the command on the left is executing as (via `sudo` or not). You were on the right track with your third attempt, but I believe that the way that `tee` is trying to append the data to your file might not be compatible with those special files. The "files" that live in `/proc` are special files and behave a specific way. The way I generally work around this is by doing something like the following: sudo sh -c 'echo "vm.swappiness=1" &gt;&gt; /proc/sys/vm/swappiness' This spins up a new shell as root and executes the entire statement, including the redirect in that privileged shell, similar to switching to `root` and running your first command.
&gt; sudo sh -c 'echo "vm.swappiness=1" &gt;&gt; /proc/sys/vm/swappiness' Thanks for explanation, but it doesn't work. I get output: sh: echo: I/O error 
 echo 1 | sudo tee /proc/sys/vm/swappiness
 echo 'SIZE: TYPE: MEMORY: CATALINA_OPTS:' | sed 's/\([A-Z_]*\): */\1="{{ \1 }}"\n/g;s/\n$//'
The last one should work, but I suspect the problem is that you're trying to write `vm.swappiness=1` when you need to be writing just `1`.
So a quick question, you appear to be trying to change swappiness on the fly, and not permanently. I would recommend using the built in sysctl to do it short term, it will last until a reboot or manually changed. It does the same thing your echo does, but might as well use the already built tool. sudo sysctl -w 'vm.swappiness=1' If you want it tok persist just add the same line to your sysctl.conf file, amd run sudo sysctl -p
Wow. Hmmm. Can you run the command as root? Like if you: sudo su - echo vm.swappiness=1 &gt;&gt; /proc/sys/vm/swappinwas Does that also get the error?
indeed, you were so close, you just missed the ':' sed 's/\([^:]*\):/\1=\"{{ \1 }}\"/' vars
aaah yes. Thank you!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
I'm hesitant to post an answer at all, because you're almost certainly cheating in a game, most likely to the detriment of others. Don't cheat at games; you're a shitty person if that's what you're doing. Reevaluate your life choices if you want further help from the reddit community. Do something productive with your knowledge instead of using it to cheat other people. SECONDS is a variable that is managed by the shell itself, telling you how long that shell has been open. The value in that shell variable will keep increasing on its own. So every time your for loop loops, the value in that variable has gone up by 15, making your script get exponentially longer for each loop. See here: [Bash Reference Manual - Bash Shell Variables](http://www.gnu.org/software/bash/manual/bashref.html#Bash-Variables)
&gt;I'm trying to make script that will write something into /proc/sys/vm/swappiness file Please explain why you want to change swappiness like this. Seriously you should just decide on a value and leave it alone. You're really overthinking things here.
Don't recommend `sudo su`. Instead use `sudo -i` for an interactive shell.
Try using 'su - otheruser'. That gets you their full login environment, where just 'su otheruser' doesn't.
[removed]
Thank you. That'll do perfectly.
I dont make these script i just get it, so. If i delete SECONDS should work better? il try now. I understand what you say more or less, first time Bot will sleep 15 seconds next time 30 next time 45 next time 60 so eentually the script will take hours and hours for open 1 instance.
I have new installation of linuxMint. I want to setup some things for better performance on SSD drive and for saving SSD life. I found this tutorial: https://sites.google.com/site/easylinuxtipsproject/ssd and it says to do it this way. I want to make script that will do all of the steps I want to perform, so in case of reinstalling system or installing it on other machine I will save my time on searching for tutorials and doing everything by hand. Plus I want to learn more about bash scripting
for compatibility with mac, always use `sed -i.bak` in scripts
this looks like a job for awk :) `awk -F: '{print $1"=\"{{ "$1" }}\""}'`
Well, you could use `sed` to print `\n` at the end of each line, something like: # lines=$'line1\nline2\nline3' # printf '%s\n' "$lines" line1 line2 line3 # printf '%s\n' "$lines" | sed 's/$/\n/' line1 line2 line3 
That's what I have found in tutorial. I have never messed with swappiness, but this setting should prevent ssd from to many writes and maked it live longer. If this additional line will work, that will also be advantage in case I would like to get back to previous setting after some time. I can simply forget that it was 60, it will be easier for me to just delete last line. 
&gt; echo vm.swappiness=1 &gt;&gt; /proc/sys/vm/swappinwas bash: /proc/sys/vm/swappiness: Permission denied
Maybe a little late, but how about piping to `sed G`?
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
That's not what I was saying to do.
Instead of the first for loop, try this: while read n r; do noderesult[$n]=$r done &lt;&lt;&lt; "$( for Node in ${Nodes[@]}; do touch /ifs/scripts/logs/$Node-$LogName echo "Starting Search for $SearchTerm on $Node" &gt;&amp;2 echo -e "$Node\t$(isi_audit_viewer -n $Node -t protocol -s "$StartDate" -e "$EndDate" | grep -E 'true' | grep -E 'rename' | grep "\\\\$SearchTerm\"" )" &amp; done)" The `wait` can then be removed.
I tested it with this: #! /bin/bash nodes=( 1 2 3 4 5 1 2 3 ) result=() while read n r; do result[$n]=$r done &lt;&lt;&lt; "$( for n in "${nodes[@]}"; do { sleep $n; echo -e "$n\tOK"; } &amp; done )" declare -p result #this will print the array after 5 seconds. 
You can use parameter expansion: $ var=$(echo line 1; echo line 2; echo line 3) $ var=${var//$'\n'/$'\n\n'}$'\n' $ printf '%s\n' "$var" line 1 line 2 line 3 Consider to store the output in an array instead, with one array element per line. Then you could simply print its contents with `printf '%s\n\n' "${array[@]}"`
Use parameter expansion: $ var=$(echo line 1; echo line 2; echo line 3) $ var=${var//$'\n'/$'\n\n'}$'\n' $ printf '%s\n' "$var" line 1 line 2 line 3 $ Consider storing the output in an array, one element per line. To print it double-spaced use `printf '%s\n\n' "${array[@]}"`
Ok, I think this is getting closer... It does seem to work on the surface, but in a test I just ran rather than getting two lines of output on the second node, I only got one. This also results in all of the \ characters in the result output being stripped out.
That's so awesome that someone else thought to do this! I've written a few variations of a load bar type doohicky in python. Very cool, and great job :D
For multiline I think it might be enough to just use another delimiter character, but not a null byte, but a "one" byte would do. `read -r` will accept `\`. while read -r -d $'\x01' n r; do noderesult[$n]=$r done &lt;&lt;&lt; "$( for Node in ${Nodes[@]}; do touch /ifs/scripts/logs/$Node-$LogName echo "Starting Search for $SearchTerm on $Node" &gt;&amp;2 echo -e "$Node\t$(isi_audit_viewer -n $Node -t protocol -s "$StartDate" -e "$EndDate" | grep -E 'true' | grep -E 'rename' | grep "\\\\$SearchTerm\"" )\x01" &amp; done)"
Than, what were you saying do to?
sed has no knowledge about variables. You're not passing a variable to sed, you are injecting data into a sed script. If home contains `/home/user`, then `sed "s/placeholder/$home/"` will be expanded by the shell to become `sed "s/placeholder//home/user/"`, which is non-sense to sed. If you want to inject data into sed, you must make sure to either validate or sanitize the data first E.g. make home contain `\/home\/user`. I suggest using awk instead. awk -v home="$home" '{sub(/placeholder/, home); print}' foo.txt
Thanks for the help, escaping the / and . characters made it work. Didn't know variables were treated as expressions.
It is most likely failing when your variable contains slashes. The problem is that sed sees this command: s/placeholder/public_html/image.png/ And thinks the `/image.png/` are options (like the `g` in `s/one/two/g`). You could fix this by backslashing the slashes (and also any already-present backslashes: those are allowed in filenames on many filesystems). Alternatively you could use a different delimiter `s@foo@bar@`, but you'll have a hard time finding a character that can't occur in pathnames, and you still have to worry about existing backslashes. As a third alternative, instead of sed you could use a language with variables like awk or Perl, and pass your shell variable as an argument that the other language can put into its own variable. perl -e 's/placeholder/$ARGV[0]/' "$home" &lt; foo.txt Here `$ARGV[0]` is Perl's name for the first command line argument, which we passed in from the shell variable `home`.
I think you only need to escape the `/` characters, no need to do it to other characters like `.`. Here's an experiment with a `.` in it: $ home="/some/test.txt" $ echo "${home//\//\\\/}" \/some\/test.txt $ echo "some placeholder here" | sed "s/placeholder/${home//\//\\\/}/" some /some/test.txt here 
I thought you could surround your sex argument with double-ticks(") instead of single-ticks (') and that would allow variables?
I tried the change and now I get no results at all. 
Yes that's true, but the content of the variable parsed is treated as an expression, meaning characters like / are interpreted by sed. In this case you would need the escape character \ in front of a special character to not have it interpreted by sed.
That's what *he* sed...
Didn't really know about alternatives to sed, so thanks for the suggestion! This wasn't even the first time I had trouble with sed.
Did you put `\x01` at the end of `echo -e`? I tried it with this code: #! /bin/bash set -x nodes=( 1 2 3 4 5 6 ) result=() while read -r -d $'\x01' n r; do result[$n]=$r done &lt;&lt;&lt; "$( for n in "${nodes[@]}"; do { sleep 1; echo -e "$n\tOK\nLINE2\x01"; } &amp; done )" declare -p result 
Yeah, I updated it just a bit ago as I had missed that past the scroll bar. Once fixed that did it.
Fuck escaping, just don't use / as your sed delimiter. you can use any character as the delimiter "s~placeholder~$HOME~" would world just find for example. Also sed can take a file name as an argument, you don't need to pipe cat output to it.
Instead of `` `...` ``, you can write `$(...)`. The two forms do exactly the same, but you can nest multiple levels of `$(...)` which you can't do with `` `...` ``. I also like `$( )` better in general (not just when you need multiple levels of it), because I can read it more easily.
I created one which does things a bit differently; you give it a percentage and it draws a bar upto that leaving the updating to a seperate program or loop. % echo $(progress_bar mybar 45) mybar ##################---------------------- [ 45%] Although the width is hardcoded and it uses portable syntax these are easily changed. I like printf since you don't need to calculate things yourself but just use the %s format string for that. Let's see others progress bars now :) % sed 's/^/ /' src/progress_bar.sh #! /bin/sh # # is_number(n) # is_number() { test -z "$1" &amp;&amp; return 1 case "$1" in *[!0-9-]*) return 1 ;; esac return 0; } # # progress_bar(title, percent) # progress_bar() { _title="$1" _percent="$2" if ! is_number "$_percent"; then echo "error: percent is not a number" &gt;&amp;2 return; fi # clip percent test "$_percent" -gt 100 &amp;&amp; _percent=100 test "$_percent" -lt 0 &amp;&amp; _percent=0 _maxwidth=40 _gap=20 _ammount="`expr $_maxwidth \* $_percent / 100`" _todo="`expr $_maxwidth - $_ammount`" _bar="########################################" _ebar="----------------------------------------" printf "%-${_gap}s%.${_ammount}s%.${_todo}s [%3d%%]\r" \ "$_title" $_bar $_ebar $_percent unset _title _percent _maxwidth _gap _ammount _todo _bar _ebar } 
What this guy said XD
While I completely agree that `$()` should be used (even in non-nesting situations)… I just want to point out that you *can* nest `` `...` `` – it just gets ugly. $ echo 1+$(echo 2+$(echo 3+$(echo 4+$(echo 5)))) 1+2+3+4+5 $ echo 1+`echo 2+\`echo 3+\\\`echo 4+\\\\\\\`echo 5\\\\\\\`\\\`\`` 1+2+3+4+5 It took me several attempts to find the working form of the second command, so yeah… just don’t do it :)
Inside a function, you can use the "local" keyword to create new variables that replace existing variables with the same name. You don't have to worry about overwriting the same names so you don't have to use that `_` character. The local variables are gone after the function returns so you don't need to use unset. Using local would look like this: progress_bar() { local title="$1" percent="$2" local maxwidth=40 gap=20 local amount todo bar ebar bar="########################################" ebar="----------------------------------------" amount=... todo=... ... } 
Thanks, i knew that already but the script was written for plain old sh which doesn't have local afaik. That's also why it uses expr instead of $(()). It would need a bit of work for bash only but i think the main interesting is using printf instead of a for loop. If you want to speed it up and make it more adjustable you could move the fixed bars outside of it and fill them with a loop so you don't have to make them each time you use it. I thought hardcoding them was sufficient at the time :)
Please... please stop using `...`. Instead use $(..) or ${...}.
 #!/bin/bash m_avg=$(awk '{print $1}' /proc/loadavg) processors=$(/usr/bin/nproc) if (( $(bc &lt;&lt;&lt; "$m_avg &gt; $processors") )); then echo -e "\e[1;33mCurrent system load is "$m_avg". Try terminating I/O-heavy processes.\e[m" exit 1 fi
You can stop the loop with the `break` command. You would add this command to both the Y and the N options in your case statement in front of the `;;` end markers.
Stupid me! That was obvious :) Thank you very much!
`nproc` is the number of LOGICAL cores, which includes fake HT cores.
why does y or n need to be in a loop? seems to me you could just put the input into a variable and run some tests against it.
Xidel is a great tool to get the job done clean and simple: http://www.videlibri.de/xidel.html
becaus if case of wrong input i want to start over again
Try pup. https://github.com/ericchiang/pup
If you want to be able to interrupt a while loop consistently you just need to do like: while [ 1 ]; do COMMAND; test $? -gt 128 &amp;&amp; break; done Now the loop will honor your interrupt 
You can reverse lines with `tac`. If your input is in a file "filename", you could do the following with tac and sed working together: tac &lt; filename | sed '\%"failure to read" /usr/bin/lib/Tools.lb%,+8 d' | tac That `\%pattern%` in the sed script is the same as `/pattern/`.
 ed -s file.txt &lt;&lt; EOF ?"failure to read" /usr/bin/lib/Tools.lb? -8,.d wq EOF
I would use tr to replace the newlines with commas ... so something like | tr '\n' ','
If the formatting has to come from IFS, then I guess they mean using arrays and `array[*]` so the printed result is influenced by IFS. IFS=$'\n' for i in google.com apple.com; do names=( $(dig $i ns +short | sort | sed 's/\.$//') ) IFS=, echo "${i%.*}:${names[*]/#/ }" IFS=$'\n' done
I wouldn't worry about trying to specify a working directory within a script. Just save the path you find in to a variable: var=$(command) Then prepend that that path to the file name and redirect the output to the file: $var/file.text &gt;&gt; outputFile.txt
Thanks for giving me another perspective. I might try and go that route.
Find's execdir option sounds like what your looking for
This only partially works.. for i in google.ca apple.ca ; do echo "$i: " ; dig $i ns +short | tr '\n' ' ,' | sed 's/\com./com, /g'; done google.ca: ns1.google.com, ns2.google.com, ns3.google.com, ns4.google.com, apple.ca: nserver3.apple.com, nserver2.apple.com, nserver.apple.com, nserver.euro.apple.com, nserver4.apple.com, nserver.asia.apple.com, 
Thank, you! This works, but can you explain what the IFS=, is doing here? I also don't understand the ${names[*]/#/ } bit.
Yeah. I'm not too familiar with the syntax for doing that.
Thanks! This worked really well after I installed gsed and gtac (using homebrew on my mac). I still don't quite understand the use of %, but I figured out this works the same: cat file | gtac | gsed '/failure/,+8 d' | gtac
About that `\%pattern%`, you normally write `/pattern/` but you can optionally choose a different character instead of writing it with `/`. This helps when you deal with folder names because you normally would have to escape the `/` inside those patterns, like so: \/usr\/bin\/lib\/
You can get a list of words and numbers in a file with grep: grep -oE '\w+' filename
You could do something along the lines of this (I searched for first file named group) cd $(find . -name group -printf '%h\n' -quit) 
Good idea, keep it up
`${var/pattern/substitution}` uses anchors `#%` in place of `^$`. And calling all array elements wth `*` instead of `@` and double quoting it will put the first character of `$IFS` in between each array element `"${array[*]}"`.
 ORS:output record separator,gsub:global substitute, First solution removes the lone comma. for i in google.ca apple.ca; do echo "$i:"`dig $i ns +short|sort`;done|awk 'BEGIN{RS=" ";ORS=","}{gsub(/\.$/,"");print}'|awk '{if($0!~/^,/){gsub(/(\.)$/,"");print $0}}' for i in google.ca apple.ca google.com; do echo "$i:"`dig $i ns +short|sort`;done|awk 'BEGIN{RS=" ";ORS=","}{gsub(/\.$/,"");print}'|awk '{gsub(/(\.)$/,"");print}' for i in google.ca apple.ca google.com; do echo "$i:"`dig $i ns +short|sort`;done| awk 'BEGIN{RS=" ";ORS=","}{gsub(/\.$/,"");print}'| awk '{gsub(/(\.)$/,"");print}'
Here's some thoughts that offer utterly no direct answers, but give you something to think about and google on. Put yourself in the end user's position... what is going to be absolutely irritating as shit? Please enter input: abc Please enter more input: def Please enter more input: ghi Please enter more input: jkl Please enter more input: NEED INPUT? WHAT ARE YOU? JOHNNY 5? Please enter more input: PLEASE GOD TAKE ME NOW Please enter more input: I'M DRINKING BLEACH, ARE YOU HAPPY?!!! Please enter more input: Please die in a fire. Please enter more input: mno If I was using a command across multiple hosts split into two categories as is the case with your requirements, I'd like a switch and the ability to enter multiple hosts at the point of invocation... So let's say I want to run a script that fixes a widget on Linux servers and Solaris servers. With `getopts`, I could do something like: fixwidgets -L linuxhost1 linuxhost2 linuxhost3 linuxhost7 linuxhost80 And that's my target Linux hosts done. Followed up by the Solaris hosts: fixwidgets -S solsucks1 solsucks2 solsucks67 Alternatively, you could use `$1`, `shift` and `$*`/`$@` Finally to wrap that all up, you'd want to add in some code that detects whether or not a `getopt` arg or a positional parameter is used, and if not, *then* you can kill the user with input prompt hell. 
hahahaha! That code was damn funny. Totally agree with you, it sort of sucks to ask for all this input BUT if I can get this to work it'll be a hell of a lot better than what were doing which is ssh to every one of the 50 some odd machines and creating the file, rename it, do the patches, create another file, rename it, diff them, scp them all, exit, ssh to next machine, create the file, etc etc etc. that getopt command is interesting though, I could probably use that for a handful of hosts at a time. Most of the workstations follow similar name schemes so groups will be workstation1, workstation2, workstation3, server1, server2, server3, etc. One thing I thought of last night was to do a "while read" loop for it. So itd ask the user if its single or multiple hosts, if it's single then do the script normally as shown above and if it's multiple then do something like: if [[ $prompt1 == multiple ]] then file="/root/scripts/listofhosts.txt" while IFS= read line do Everything else So the listofhosts.txt file would just have a line by line list of the hosts, and in my imaginary world where everything works the way I plan it to, it would just plug each line into the script at the point of ssh $line "bash -s" &lt; /pre_remote.sh I tried to do it real quick last night and ran into some conflicts, so I think if I can rearrange the variables a bit and toss in that while loop then I'll be golden. Sort of new to this whole bash thing, but having a practical use to focus on is making this fun as hell. Thanks for the suggestions. 
I'll give this a go. Thanks.
Not sure if this is what you want, I was gonna put a fancy awk script then realized that if they are separated by spaces you can just use tr to replace spaces with newline and then sort unique: cat filename | tr " " "\n" | sort -u
I made some more progress and I think I got most of it eventually. The only thing I still need to remove is the ve re and s that are left when I remove apostrophes. 
I guess you can match apostrophes and remove everything after those to the next space. This would replace apostrophe and anything after it until the end of the word. /\'[a-z]+ / /
can you create 2-3 small sample input files and show the final output you require for them? see also https://stackoverflow.com/questions/43472246/finding-common-value-across-multiple-files-containing-single-column-values
Hi, some feedback: You should rewrite it a bit, remove cli-parameter check and example print from function rgb2true. And make this function just print an escape sequence, you will be able to use it like this: printf "$(rgb2true 255 0 0)Red fancy text\x1b[0m\n" You can also simplify parameter check just to [ $# -eq 3 ]. It is good to have some -h and --help option, I am usually using this at the beginning of the executable code: if [[ "$*" =~ -h ]] ; then print_usage exit 0 fi It is also handy to have error messages redirected to stderr (&gt;&amp;2) and print something like "$0: Error...". Thanks to it, you will have name of script in error message, which can help to debug some more complex task and you will see what exactly failed. I am using a lot some kind of unit tests, so even in a shell script is nice to have some testing function, which just calls the function and compare its output to some expected output automatically. I am also having some Makefile on the side running this test and checking syntax with bash -n (you can have it as git commit plugin for example). You can also improve it with some additional formats of input. for example when input is 0xFF00FF, etc. Maybe some fancy regexp on CLI parameters will be able to detect more input RGB formats :) As you can see, there is always something to improve and make more difficult :) 
 for i in google.com apple.com ; do echo -n "$i: " ; echo $(dig $i ns +short | sort) | sed -e 's/. /, /g' -e 's/.$//'; done
sed -e 's/\&lt;[0-9]\&gt;/0&amp;/g' birthdays.txt 
 # GNU/Busybox `sed` (either works) sed 's/\&lt;[0-9]\&gt;/0&amp;/g;' birthdays.txt sed 's/\b[0-9]\b/0&amp;/g;' birthdays.txt # BSD/macOS `sed` sed 's/[[:&lt;:]][0-9][[:&gt;:]]/0&amp;/g;' birthdays.txt (Unlike `grep`, `sed` on macOS doesn't seem to support the `REG_ENHANCED` syntax extensions described in `re_format(7)`)
add four spaces before the command to avoid losing the back-slashes sed -e 's/\&lt;[0-9]\&gt;/0&amp;/g' birthdays.txt `\&lt;` and `\&gt;` are word-boundaries and not available with all `sed` versions I think.. can also use `\b` with GNU sed 
Perfect. Thanks!
&gt; add four spaces before the command to avoid losing the back-slashes Thank you. Still getting the hang of formatting here. :)
TIL `\&lt;` `\&gt;` after years and years not needing/working around it
Thank you. I had to research word boundaries (\&lt;\&gt; or \b\b) to fully understand the command you and others have posted. I'm a little confused on something. While the command you have posted works for my particular situation, something like this is actually NOT working: Delete /usr/local from boundaries.txt **boundaries.txt** contains the following: /usr/bin /usr/local/bin /usr/local /usr/local/project/bin But sed 's@\&lt;/usr/local\&gt;@@' boundaries.txt is not doing the job: root@root:~/Desktop$ sed 's@\&lt;/usr/local\&gt;@@' boundaries.txt /usr/bin /usr/local/bin /usr/local /usr/local/project/bin Could you please explain why the word boundaries are working with the my original problem but not with the one I am showing you with the paths? 
Cheers, I totally didn't think about doing expansion. This is the easiest way.
Can I get an explanation for what it's doing here? 
Try using this so that you don't have to edit quote characters: cat &gt; testcat &lt;&lt; 'EOF' copy and paste your text into here EOF The only thing to take care of is to make sure that there's no "EOF" line in what you paste. I didn't understand the rest of what you are doing and what you are trying to do with sed.
Thanks for this. I am trying to break a single (very long) line of text into two parts. The division in the text is the character sequence ` =&gt; `. Assuming that I use your method then I have a file containing all sorts of stuff including this in about the middle. I need to cut into two files, one from the first character terminating just before this sequence and the other from just after it to the end of the file. These are what I hope to format with `printf` and then compare with `diff`
The things that might be a bit unusual are, in the replace pattern half of the command, where you see a `&amp;` character, sed will insert the text it found through the search pattern. About those `\&lt;` and `\&gt;` in the search pattern, somewhere in `info sed` you can find it documented like this: '\&lt;' Matches the beginning of a word. $ echo "abc %-= def." | sed 's/\&lt;/X/g' Xabc %-= Xdef. '\&gt;' Matches the end of a word. $ echo "abc %-= def." | sed 's/\&gt;/X/g' abcX %-= defX. You probably know `^` and `$` to search for beginning and end of a line, and the `\&lt;` and `\&gt;` are special in the same sense as those. They are not adding characters to the search result, instead make sed do something special and only look at the surroundings of the current character position that it's working on in the input text. Trying to translate that `s/\&lt;[0-9]\&gt;/0&amp;/g` into something that behaves exactly the same but is not using `\&lt;` and `\&gt;` actually seems pretty annoying. I can't manage to do it without always finding some example input text that breaks it.
`\&lt;` and `/&gt;` specify word boundaries. The first one matches when the text changes from a non letter or number to a letter or number and the second one is the reverse, `[0-9]` - match on a single-digit number character only, so all others (such as 2 character numbers) are ignored, `0&amp;` - add a leading zero to whatever was matched on the left, `g` - global check - keep checking the whole line. So, find a single-digit number, add a leading space to it, keep checking each whole line. ;) 
sshpass works very well sshpass -p "password" ssh -o StrictHostKeyChecking=no root@someserver Somecommandonyourlocalcomputer
Use public key authentication instead?
Use TCL/expect
sed doesn't have address range where you can do a search and simply append -1, like ed, which is a shame. Anyway: $ cat a.txt 1234 12345 1234'6 =&gt; 1234 12345 1234'56' $ diff &lt;(ed -s a.txt &lt;&lt;&lt; '1,/=&gt;/-1p' ) &lt;(ed -s a.txt &lt;&lt;&lt; '/=&gt;/+1,$p') 3c3 &lt; 1234'6 --- &gt; 1234'56' 
I ran into the same problem fairly recently. I ended up using expect and it works perfectly. Public keys is another option.
Cool. I didn't know about those characters for word boundaries. It's that just a sed thing? I only knew about \b. 
No, I've also used it in grep, awk, etc... as well. 
assuming your environment setup is not an issue - just: (ideally with exchanged public key first to avoid the password) **ssh root@host /path/to/somescript.sh** enter password done If you want to exchange keys to make ssh password-less, do this: -ssh-keygen -t rsa (on your workstation, then follow the prompts) -cat ~/.ssh/id_rsa.pub -copy that text to your clipboard -ssh to the remote host -ssh localhost (this will create the ~/.ssh directory with proper permissions, you don't even have to fully connect) -on the remote host edit a new file ~/.ssh/authorized_keys -paste the text in (be sure you catted vs more'd the pub key, or you'll get unwanted linebreaks) -disconnect, try to ssh in This is assuming you are using the openssh client (and not something like putty, though putty can do this also)
A script on my laptop: $ cat /tmp/test.sh #!/bin/bash echo "Does this work?" touch /tmp/does_this_work A remote machine: $ cat /tmp/test.sh | ssh root@172.25.73.48 Does this work? Logging in to the remote machine and verifying that the file is there: $ ssh 172.25.73.48 $ ls /tmp/does_this_work /tmp/does_this_work 
Thank you /u/-BruXy- ! You have great ideas. I particularly like option for parsing hex colors. It's funny that you mention Makefiles because I'm working on adding to the bpkg project: an installation method is recommended in the project's package specifications. 
&gt; `gdebi` No.You are asking him to install additional packages,causing more bloat in his system. Correct way: $ su Password:(password) # dpkg -i --force-all ./MULTIPLE.deb ./Multiple.DEB ./MuLtIpLe.DeB # apt -f install --no-install-recommends
OK, so OP wants to install a font package- so presumably a desktop system and is talking about a generic script to install debs. OP says that they are a beginner and you recommend doing 'force' on everything. Is the 'bloat' really so much for a generic use case ion a desktop system? Especially compared to broken packages from forcing dependencies. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Is this what you are looking for? cd $(find $i/User -print | grep -i './places[.]sqlite$' | xargs dirname) ./accessSQLiteFile 
Try it and see!
posting this literally took longer than testing it.
I've done something similar in the past, but instead of using "bash -s" I used "sudo -s" ex: ssh $HOST "sudo -s $STUFF_TO_DO"
Thanks. This works perfectly. Just need to integrate it into script now!
Sorry :P I did try that in a script but it failed, i think thats was because i was passing the command to sudo.
or you ran the script with sh instead of bash
Thanks! That was the problem
It has to do with the tty the remote is using as a backspace. If you enter `stty -a` on the local machine and then on the remote machine you'll probably find the results differ: `erase = ^?` or `erase = ^H` You'll have to bring them both in sync. [Here is a page](http://hints.macworld.com/article.php?story=20040930002324870) on how to do that.
actually its: erase = ^? on both the local and remote machine, do you know what else could be causing the problem?
That's weird. Hopefully someone else knows more than me. 
I think the weird part is that it's only happening when writing input for a program. Well, anyways, thanks for trying to help! 
I think my mind took a bit of a detour from the title...it should have been "Scanning for and changing specific permissions to other permissions"? Apologies for the misleading title...
Just add `-exec chmod 777 '{}' ';'` to find to run that command on each file. This also needs extra parentheses. I'm not sure if the two pairs of inner ones are needed below, but the outer ones are. find /mnt/raid/client/testsubmit/ '(' '(' -type d -not -perm 644 ')' -o '(' -type f -not -perm 644 ')' ')' -exec chmod 777 '{}' ';'
You can use awk's printf command instead of print: awk '{printf "%d\n", $0/60/60/24;}' /proc/uptime And there's an `int()` function you can use: awk '{print int($0/60/60/24);}' /proc/uptime Note both of those do not round the number, just cut off what's after the decimal point. If you want rounding you need to add 0.5: awk '{printf "%d\n", $0/60/60/24 + 0.5;}' /proc/uptime
That's great. I had seen the int function but wasn't quite sure how to implement it in my use case, int definitely looks like the nearest solution. Thank you.
&gt; RAM=128 &gt; screen -dmS $f java -Xms$RAM -Xmx$RAM Not a Java user here (sorry), but it seems the syntax for this should probably include an `M` or `G`. Without these, the size is in bytes only. `screen -dmS $f java -Xms${RAM}M -Xmx${RAM}M` Or you could add this character to your RAM assignment: `RAM=128M`
Yes, I see what you are saying, int() truncates it doesn't round. awk '{ print $0/60/60/24; }' /proc/uptime 86.9237 awk '{ print int($0/60/60/24); }' /proc/uptime 86 awk '{ printf "%.0f\n", $0/60/60/24; }' /proc/uptime 87 Thanks again.
Nice, I tried it with Cobol, and worked flawlessly.
Quick suggestions, didn't test: L005 title="${answer:-"${PWD##*/}"}" #avoids if/then/else/fi L039 shopt -s nullglob; rm ./.tmp.* # removes all files, and expands to NULL if there are no matching files. No more need for L38 L044 # shows that you didn't put the script through shellcheck.net L140 # Capitalize the default option: (Y/n) or (y/N) L142 case answer in [Nn][oO]?) # would support more answers? dunno if necessary L132 # Does it work? $var in hard quotes in soft quotes?...
 string='DISC[0-9]*.?' find -type d -regex ".*$string.*" -exec bash -c '[[ "$1" =~ ^(.*)/([^/]*)'$string'([^/]*)$ ]] &amp;&amp; { dir=${BASH_REMATCH[1]}/${BASH_REMATCH[2]}${BASH_REMATCH[3]}; mkdir -p "$dir"; mv -vit "$dir" "$1"/*; }' -- '{}' ';' and then remove the empty directories with `rmdir *`
Thank you. It works perfectly. 
One other thing to note, this is only setting the heap size, Java more than this. It is not a huge amount but very noticeable in smaller heaps since it doesn’t grow with the heap. 
This looks like it's going to work...when run manually from the command line there's no issues. When I try and create a cron job or add it to the crontab for root, it doesn't run. I've tried creating a script file that the cron job runs every three minutes as well as putting the line directly into the cron job itself and neither option seems to work. I've tried restarting crond but no luck there either. The script file looks like this: #!/bin/bash find /mnt/raid/client/testsubmit/ '(' '(' -type d -not -perm 644 ')' -o '(' -type f -not -perm 644 ')' ')' -exec chmod 644 '{}' ';' And has the file permissions of: -rwxr-xr-x 1 root root 141 Oct 26 14:30 change_to_644_perms.sh And the cron job under /etc/cron.d/ looks like this: 3 * * * * root /scripts/change_to_644_perms.sh Thoughts?
No idea. Does cron write errors to a log? Maybe it needs the full path of chmod like `/bin/chmod`.
Hmmm...tried your suggestion and it looks like it worked? New code in change_to_644_perms.sh: /bin/find /mnt/raid/ppt/testsubmit/ '(' '(' -type d -not -perm 644 ')' -o '(' -type f -not -perm 644 ')' ')' -exec /bin/chmod 644 '{}' ';' Thanks for the suggestion!
Thanks for the suggestions, I'll look to incorporate these
You probably meant `-mtime +14`, not `-14`? To make it easier to see what's going on when you change the parameters, you can make find print the file dates with `-printf` like so: find . -type f -mtime +14 -printf '%t // %p\n' find . -type f -mtime +14 -printf '%Tx // %p\n' find . -type f -mtime +14 -printf '%Tc // %p\n' find . -type f -mtime +14 -printf '%TY-%Tm-%Td // %p\n'
`-mtime` is the other way around, so `-14` is changed in the last 14 days. You probably want something recursive: #! /bin/bash IFS=$'\n' function rec() { [[ $(find "$1" -mindepth 1 -type f -mtime -14) ]] || { echo "$1"; return; } for dir in "$1"/*; do [[ -d "$dir" ]] &amp;&amp; rec "$dir" done } rec . It will stop descending into subdirectories and print if there are no more changed files.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
As your script is for bash, you can do a C style loop: for (( counter = 1; counter &lt;= 100; counter++ )); do echo "$counter" done You can do this as well: for counter in {1..100}; do ... done This `{1..100}` method has the downside that it won't work if your start and end point is a variable, meaning something like this doesn't work: x=100; echo {1..$x} You can experiment with all of this directly at the bash command line. You don't have to write and edit a script all the time. See here: $ echo {1..10} 1 2 3 4 5 6 7 8 9 10 $ echo {0..30..5} 0 5 10 15 20 25 30 $ echo {10..1} 10 9 8 7 6 5 4 3 2 1 $ for x in {3..7}; do echo $x; done 3 4 5 6 7 $ for x in {0..30..5}; do echo $x; done 0 5 10 15 20 25 30 $ for (( x = 0; x &lt;= 30; x += 5 )); do echo $x; done 0 5 10 15 20 25 30 $ start=3; end=7 $ echo $start $end 3 7 $ for (( x = start; x &lt;= end; x++ )); do echo $x; done 3 4 5 6 7 
Thank you so much for all the info you provided!
I know you don't need it, but here's another approach: Generate test directories: for i in $(seq 1 3); do; mkdir FAMILY.VIDEOS.DISC${i}.OCTOBER; done; for i in $(ls); do touch ${i}/${i}.mp4; done Do the thing: #!/bin/bash NEW=${i/.DISC[0-9]/}; DIRS=($(ls)); mkdir ${NEW}; for i in ${DIRS[@]} ; do mv ${i}/* ${NEW}/; done 
Your question has been addressed, so I have some criticism. Uppercase variables are used by the system. Variables are case sensitive. Use lowercase variables, and never have a conflict. 
if you have the `rename` binary, try: rename 'y/A-Z/a-z/' * 
http://oylenshpeegul.typepad.com/blog/2011/12/a-tale-of-two-renames.html
I know, isn't open-source wonderful? ;)
You can't use a 'for' loop for this because it will split what 'find' prints into words when there are spaces. You need to read the output of 'find' with a 'while read' loop, like this: find . -type f | while IFS= read -r line; do echo mv "$line" "$(echo "$line" | tr '[:upper:]' '[:lower:]')"; done This does not run 'mv' commands, it just prints them. If the output you see looks good and like what you want to happen, then type this here: ^echo This will run the commands that were previously printed. 
Yep :) I liked how you specified "`rename` perl script", but just thought I'd add the context :D
Or, if `bash4isms` are allowed: $ mkdir /tmp/casetest $ touch Test\ FILE\ {01..20} $ find . -type f | while IFS= read -r line; do echo mv "${line}" "${line,,}"; done mv ./Test FILE 01 ./test file 01 mv ./Test FILE 02 ./test file 02 mv ./Test FILE 03 ./test file 03 mv ./Test FILE 04 ./test file 04 mv ./Test FILE 05 ./test file 05 mv ./Test FILE 06 ./test file 06 mv ./Test FILE 07 ./test file 07 mv ./Test FILE 08 ./test file 08 mv ./Test FILE 09 ./test file 09 mv ./Test FILE 10 ./test file 10 mv ./Test FILE 11 ./test file 11 mv ./Test FILE 12 ./test file 12 mv ./Test FILE 13 ./test file 13 mv ./Test FILE 14 ./test file 14 mv ./Test FILE 15 ./test file 15 mv ./Test FILE 16 ./test file 16 mv ./Test FILE 17 ./test file 17 mv ./Test FILE 18 ./test file 18 mv ./Test FILE 19 ./test file 19 mv ./Test FILE 20 ./test file 20 
I use the perl script, I find -e flag useful in aliases. 
`cut` works on either stdin or file input... but what you have here is variable content which `cut` would try to treat it as filename... $ a='foo 123 bar' $ cut -d' ' -f2 &lt;&lt;&lt; "$a" 123 $ echo "$a" | cut -d' ' -f2 123 
Sending $line into cut works like this: cut -f9 &lt;&lt;&lt; "$line" or this: echo "$line" | cut -f9 You might also want to do `read -r line` instead of `read line`. Without the `-r` it tries to interpret `\` characters as something special which you probably don't want.
Thank you!
&gt; find . -type f | while IFS= read -r line; do echo mv "$line" "$(echo "$line" | tr '[:upper:]' '[:lower:]')"; done &gt; Thats exactly what i was looking for. Thanks man it was bugging me whole day. 
Can't you just replace the `read` in the script with manually setting the variable to whatever you want it to be?
 bash-3.2$ read -p "ASD: " asd ASD: ehlo bash-3.2$ echo $asd ehlo bash-3.2$ unset asd bash-3.2$ echo $asd bash-3.2$ asd='ehlo' bash-3.2$ echo $asd ehlo 
The info that it asks for is important for finding the specific files that the script needs to look for. Those file names tend to change on a case by case basis, so unfortunately the read part of the script needs to stay. One thought I had was to add prompts to the script to ask for the parameters set by the read function and save them into the script and then bar it to an SSH, so something like: /some/path/set_script_vars.sh | ssh -tt root@DEV01 /some/path/script.sh Since it seems like it will run so long as the script is on the remote host and doesn't require interactive input. Hell... now that I'm typing that out, that seems to be doable/ make sense if I can't get the interactive terminal. So dumb though, driving me nuts that this thing won't work. Tested it on CentOS VMs at home and it worked great. Good suggestion though, I'm going to ponder that for a while. 
Or just command line arguments: bash-3.2$ ssh server "./test.sh" bash-3.2$ ssh server "./test.sh asd" First argument: asd bash-3.2$ ssh server "./test.sh asd qwe" First argument: asd Second argument: qwe bash-3.2$ ssh server "cat ./test.sh" #!/bin/bash if [ -n "$1" ]; then echo "First argument: $1" fi if [ -n "$2" ]; then echo "Second argument: $2" fi
I don't really get what you want to do, but how about this: #!/bin/bash for x in "$@"; do printf "%s\n" "$x text" done You can experiment on the command line with that `$@` that normally only exists in scripts like this: $ set -- abc def ghi $ echo "$@" abc def ghi $ for x; do echo "$x test"; done abc test def test ghi test 
Yes, that `$verbose` executing true/false makes me worry and I like using `[` better. One alternative is using `$verbose` as a number with the rules about what true/false is like in the C programming language: verbose=1 (( verbose )) &amp;&amp; echo "verbose is true" A zero would get seen as 'false'. If the variable does not exist, that also counts as 'false'. You might want to use `[[` always in bash as there are some weird errors you can run into with `[`. With `[[`, you can also write `$verbose` instead of `"$verbose"` (missing `"` is the kind of error `[[` helps against). Another thing, a `==` is technically not quite correct for `[` but works in bash. In for example Debian's /bin/sh, you need to do `[ "$verbose" = true ]`.
Thank you. Some good thoughts there. :)
You can loop over your command line arguments as /u/ropid showed. It sounds like you want the contents of each file, with a blank line between them. You can do: cat "$1" # first file shift # Remove that file from $@ for f in "$@"; do # blank line before subsequent files echo cat "$f" done If you don't mind having an extra blank at the end, you could go a bit simpler: for f in "$@"; do cat "$f" echo done 
A further followup: just been reading up on the differences between `[` and `[[`. Frankly, I tended to use `[` for just about everything only switching to `[[` as required. I guess I thought this would help me when having to switch to other environments with older versions of BASH or with restricted environments like BusyBox. However, I think I'll try reversing this: so, always `[[` unless I need something to work in a less BASH'd environment. Good call @ropid. :)
Haha - just discovered this too: `[ ! $? -eq 0 ]` Really don't know what I was thinking back then. :D
Thanks a bunch. i might have written the description of my problem a bit confusing, but i want a list of directories that havn't received any changes to any file or directories within in the last 14 days. 
Thanks, definitly makes it alot easier to debug!
Is that not what it does? Are you saying it's not supposed to check directories over 1 level deep? Then add `-maxdepth 1` to `find` above.
Have you tried without echoing or the quotes?
Please provide examples.
 ssh "ec2-user@$line" rm /var/log/folder/app.log
 while read line do ssh ec2-user@"$line" rm /var/log/folder/app.log &lt; /dev/null done &lt; /home/x/ipfile The devnull bit helps because in some cases ssh will try to grab input and steal it from the for loop 
Install **logrotate**. Ensure you have a cron daeomon running. Configure logrotate for your specific log file (if necessry). You can now sleep in every morning.
Do you know about logrotate? Logrotate will rotate (obiously) when a log is equal or greater than X, or time based (hourly, daily, weekly). You can gzip log file too
You shouldn't use ssh to do that. Use logrotate to archive/delete/manage your log files automatically. It's easier and way cleaner than doing via ssh.
The list is long is 100MB of plain text...i wonder if there is a solution that works for any url...just consider any url which is not simply name.{org,com,net ...}
Check out these tools: sed cut grep awk They can probably all do what you want, but how exactly depends on that text you work on. You will probably have to use `sort -u`, not `uniq`.
It's easy to do with https://docs.python.org/3/library/urllib.parse.html . This is one of those situations where it's a really good idea to switch to a general purpose programming language.
 if with https : grep -Eo 'https?://[^/]+' | grep -Eo '[^/]+$' if without https : grep -Eo '^[^/]+' Not tested. See if it works: http://example.com/something?q=feefifo # should return example.com https:// # should return nothing https://sub.sub.sub.sub.domain.weird.tld/yadi/yada/ # should return sub.sub.sub.sub.domain.weird.tld
This seems to work, i' ve got to check out regular expressions it seems impossible to do this sort of stuff without them...thanks man...
As other suggested you should probably be using logrotate if you need this to happen automatically. But if you really want or need to initiate this manually, consider using something like ansible. Ansible lets you perform tasks over ssh against multiple hosts. 
Please add 4 spaces in front of a line of code: if you type this -- it will look like that
I have corrected my formatting. Thank you.
Note that `printf` does its own looping internally so you can do something like: $ set -- abc def ghi $ printf '%s text\n' "$@" abc text def text ghi text 
I'd say a log rotate program, or a Cron job would be the way to go on this one. Automates the task, work smarter not harder etc...
Instead of looking at how you would write a pattern that matches only exactl the URL, look at what's surrounding it in the lines of text in your file. It might be easier to do patterns to find the beginning and end and then cut there.
For finding new files, `find` has a `-newer` predicate that returns true for files that have been modified earlier than some reference file. # Save a timestamp by touching a file. touch prev_find # Find files modified since prev_find was touched. find ~/ -newer prev_find -type f To track deleted files you could take a "snapshot" by redirecting find's output to a file and then comparing it to a newer snapshot using diff or comm. Or you could look at creating snapshots using the filesystem, if yours supports it, or a [Logical Volume Manager](https://en.wikipedia.org/wiki/Logical_volume_management#Snapshots).
To show both deleted and new files you need a snapshot like this: #run this once find ~ -not -name 'find_tree*' &gt; ~/find_tree.a function newfiles() { find ~ -not -name 'find_tree*' &gt; ~/find_tree.b diff ~/find_tree.a ~/find_tree.b | sed 's/^&gt;/Addition:/;s/^&lt;/Deletion:/;/^[0-9]/d' cp ~/find_tree.b ~/find_tree.a' }
With bash 4: find . -type f -exec bash -c 'typeset -u uppername="$1" &amp;&amp; mv "$1" "$uppername"' -- {} \; This is the only way that's completely robust for filenames containing any character (even newlines). 
I'd say have a Boolean flag that gets switched once your script had ran through. Then have the main program check to see if that flag has been turned on.
Honestly that sounds an amazing idea, but I have no clue how to switch flags and have it stay that way. 
Check `/etc/passwd` for the line with your username. It should show the users home folder.
If you wanted to make a flag that is persistent through multiple runs of the script. (You wanted the script to make changes for only the third time it's ran) then you could touch a new file then have the program check to see if that file is present (if it is, then the program runs). To allow multiple checks before running you could touch file1 then file2 etc. Then have your script check for file3. Then you know it only ran after file3 was created and found. 
The shell just inherits the working directory of your X session, and for whatever reason, your X session has its current working directory set to `/etc/X11/xorg.conf.d`.
 #!/bin/bash scriptname="${0}" runs=0 if [[ $runs -eq 0 ]]; then echo "I have never run before." elif [[ $runs -eq 1 ]]; then echo "I have ran $runs time before." elif [[ $runs -gt 1 ]]; then echo "I have ran $runs times before." fi # Set variable for sed current_run=$runs # Increment for each run ((runs++)) # Sed to change the runs variable inside the script sed -i "s/^runs=$current_run/runs=$runs/" $scriptname
There's a "perl-rename" or just "rename" or maybe "prename" tool. It's named differently depending on the distro. Using that tool, you could do this: $ perl-rename -n 's/\.([0-9][0-9.]*)\./|$1|/' *docx SOP.8.2.201.Internal Audits.AV.docx -&gt; SOP|8.2.201|Internal Audits.AV.docx SOP.8.2.5.Audits.AV.docx -&gt; SOP|8.2.5|Audits.AV.docx That `-n` parameter makes it just print what it would do. You need to remove that `-n` to make it actually rename files. The regex rule used here will search for a sequence of digits and periods and will trigger on the first such sequence in the file-name. **EDIT:** The following works if you don't have perl-rename: for x in *.docx; do y=$(sed -r 's/\.([0-9][0-9.]*)\./|\1|/' &lt;&lt;&lt; "$x"); echo mv "$x" "$y"; done This prints 'mv' command lines. If its output looks good to you, you can run those commands with the following: ^echo
Slick - thank you!
I'm just guessing and didn't try it, but perhaps change this line: for PATTERN in ${PATTERNS[@]}; do to this: for PATTERN in "${PATTERNS[@]}"; do 
Beautiful!!! That worked! Thanks!
Bash will not replace that second `~` in your command line. It should work if you change that parameter to the following: ~/".*" Or you can use `$HOME` instead of `~` to not run into that weird problem: "$HOME/.*"
Is there a more efficient way other than find to search for newly made files after census has been run?
How about this? if [ ! -z "$verbose" ]; then... That way if it's either zero or undefined then you do something (or remove the exclamation mark and act on what to do if it is zero or undefined). 
Thank you for this suggestion, but this variable will only ever be `true` or `false` so checking for zero-length wouldn't work. ;)
The find is inefficient because it will still descend into all the hidden directories you want to skip. You need to specify that specifically with `-prune` cd &amp;&amp; find . -path './.*' -prune -o -type f -newer census -print
I don’t have time to look over the full script right now (sorry), but in general, it looks like there are several places where you should avoid running a command several times. Like here: chargingstate=$(acpi --battery | awk '{print $3}' | cut -d "," -f 1) batterypercentage=$(acpi --battery | awk '{print $4}' | cut -d "%" -f 1) or here: mastervolume=$(amixer sget Master | tail -2 | awk '{print $5}') masterstatus=$(amixer sget Master | tail -2 | awk '{print $6}') Instead of running `acpi --battery` or `amixer sget Master` twice, you can run it once, capture the output, and then send that into the rest of the two pipelines separately. Additionally, you could probably get rid of some of the other commands as well, e. g. use `read` (first with default `IFS` and then with `IFS=,`) instead of the `awk` and `cut` in the battery case, or use `${variable//'pattern'/'replacement'}` instead of `sed s/pattern/replacement/g` elsewhere. And I think you can also use `printf '%(%B %d %Y %H:%M:%S)T'` instead of `echo -en $(date '+%B %d %Y %H:%M:%S')`
&gt; Instead of running `acpi --battery` or `amixer sget Master` twice, you can run it once, capture the output, and then send that into the rest of the two pipelines separately. So I did this with batinfo=$(acpi --battery) chargingstate=$(echo "batinfo" | ... I did this for each time a ran a command unnecessarily. I'm guessing echo might not be a great way to do it because CPU usage is now up at 30-50% haha. What is a better way to pipe it in.
What exactly is find_tree? or what is it supposed to be?
&gt; Currently it bumps my CPU usage by 10-15% I'm not surprised, you fork a LOT of subshells/call a lot of programs every 0.3 seconds. Do you really need to update ALL the information that often? Or can you cache the output of some of the functions and just update them every 2-10 seconds? As /u/galaktos already mentioned, if you need to parse the output of a program multiple times, it's more efficient to keep the output and work on it instead of calling an external program multiple times. I'm assuming one of the programs you call is causing the CPU usage. 
&gt; Do you really need to update ALL the information that often? Absolutely not. I don't need to, but I want to. I want it to be as snappy as possible which is why I set it to sleep for a short amount of time. But then I seem to run into problems of efficiency. And because some functions can block progression I get lag in things like the time. Ideally if I could hook into the state of something chaning instead of just checking to see if it had changed that would be best and the most efficient. I'm not even sure if that is possible. Is there a better way to access the information I want? Something within the all encompassing systemd that provides all this information with some hooks?
A new function definition, that you put in .bashrc for instance.
First step: Paste your code into shellcheck.net and fix everything that it points out is wrong.
Not bash exactly but one way would be to write a more efficient script in python say, that only uses one process,of anden execute that and save the results in the bash script. That's what I did for a display bar in awesome wm I believe
Thanks for clarifying!
TL;DR Save results to a file, then update that file once every so often. Also, don't pipe grep into awk into cut, awk can do everything for you. &gt; Ideally if I could hook into the state of something chaning instead of just &gt; checking to see if it had changed that would be best and the most efficient. &gt; I'm not even sure if that is possible. Is there a better way to access the &gt; information I want? Something within the all encompassing systemd that &gt; provides all this information with some hooks? ##Caching I have a weather script that I use in my statusbar that looks like this: function my_weather() { if [[ ! "$(cat "$HOME/.config/weather")" ]]; then $HOME/bin/weather.py &gt; $HOME/.config/weather weather=' W:'$(cat $HOME/.config/weather) fi modified_date=$(stat $HOME/.config/weather | awk '/Modify/ {split($3, arr, ":"); print arr[1]}') if [[ ! "$(date +"%H" )" -eq $modified_date ]] then $HOME/bin/weather.py &gt; $HOME/.config/weather weather=' W:'$(cat $HOME/.config/weather) else weather=' W:'$(cat $HOME/.config/weather) fi echo $weather } Granted this is a zsh function, but it would work in bash too with minor modifications. What it does is rather simple. It checks to see if the file `$HOME/.config/weather` exists and has something in it. If it's empty or not there it calls my python script that actually makes a request to the Open Weather API and builds the output. If the file does exist it checks the modified time with `stat` and `awk`. This command is an example of something I saw in your script that is an anti-pattern in my mind. You have lot's of instances of piping `grep` to `awk`, like `$(xrdb -q | grep -i color1: | awk '{print $2}')`. This is more work than necessary. You can replace the pipe to `grep` with `awk` like this: `$(xrdb -q | awk '/color1/ {print $2}')` I'm guessing you don't need it to be case-insensitive, but if you find it necessary, you can change the `awk` command to look like this `$(xrdb -q | awk 'IGNORECASE = 1;/color1/ {print $2}')` Once I figure out what hour the file was last modified, I compare that with the current hour, if they are different, I update the file, otherwise I cat the file and echo the result. ## Other issues Do you need to use acpi to get battery info? The `/sys/` filesystem has that info, at least on my OpenSUSE system in `/sys/class/power_supply/`. Using `cat` to get that info will be faster than using the `acpi` call. See how I do it [on github.](https://github.com/yramagicman/stow-dotfiles/blob/master/root/bin/statusline#L12-L24) Your nework interface info could be cached. Write that info to a file and use the method above to update it once a minute or so. You shouldn't be changing networks that often. Also, the `/proc/` filesystem might be your friend here too. Again, on OpenSUSE I have a lot of good info in `/proc/net/wireless`. Here too you have the `grep|awk|cut` anti-pattern. I'm using this command to find the strength of my wifi connection `awk '/[0-9]*/ {if ($3/1 != 0) print $3}' /proc/net/wireless`. The `if` there is a crude filter to make sure I only print numbers. In `awk` a string divided by a number is always 0, and a number divided by 1 is always itself, so if I divide by 1 and only print what's not equal to 0 I'll always get numbers. Final though. You have loops galore in this script. I'll bet you could get rid of all the `while read -r $x` loops and replace them with simple echo commands, since your script is updateing so often. Try this for your workspaces, it may not work, but if it does, your CPU usage should plumet: bspwm () { buffer="" workspaces=$(bspc query -D --names) active=$(bspc query -D -d --names) nodecount=$(bspc query -N -d "$workspace" | wc -l) if [[ "$active" == *"$workspace"* ]]; then buffer="$buffer %{F$greendark}$workspace%{F-} " else if [[ $nodecount -gt 0 ]]; then buffer="$buffer %{F$bluedark}$workspace%{F-} " else buffer="$buffer $workspace" fi fi echo -en $buffer } 
It's a script so why would it matter if it's repeating the sudo command every time? I would personally stick with sudo because you can specify what commands are allowed
to make the code feel less repeatitive? I think that's generally what I like to achieve whenever I can
Your second example should be within a script that you call as sudo like this: $ cat updater.sh #!/usr/bin/env bash apt-get update npm update -g gem update $ sudo ./updater.sh This way, you don't need to keep specifying it. 
I keep all my stuff wrapped in functions so I can just call them as commands from anywhere. I forget if functions can be sudo'ed
Well it's not really repetitive is it? You're just prefixing different commands. But sure I guess if you're going by feeling then yeah... 
Yes they can. 
Can you explain what you mean? I can't see how it would work as the way I understood it, sudo is a program and has no access to bash internals like a function and can only run other programs.
 $ sudo -- sh -c 'apt-get update; npm update -g; gem update' This would also work as an alias in your .bashrc
This is a `.bashrc` function: $ genphrase CrossPyropenHibbing $ sudo genphrase sudo: genphrase: command not found This is discussed, with potential solutions, here: https://stackoverflow.com/questions/9448920/how-can-i-execute-a-bash-function-using-sudo 
op: you should update the post to mention this. 
`sudo` is the preferred option. If you have a bunch of commands to run back-to-back, put them in a single script and call the script with `sudo`. When you use `su` to get a root shell, you end up doing more in the shell than just the one command that needs root. That's more opportunities to accidentally type something wrong, leave the shell open, etc. Also, when you use sudo, every command you run gets logged. That's good for accountability / security. When you run a root shell, it's a free-for-all. In general, it's a good habit to just forget that su even exists.
If you make them shell scripts and put them in your path it will work, and in general be more flexible.
Or just use rsync with --link-dest
done
Yes rsync --link-dest is a good option. I have a script that uses that too: [https://github.com/paulbearddotname/rsyncbackup/blob/v1/rsyncbackup](https://github.com/paulbearddotname/rsyncbackup/blob/v1/rsyncbackup) However it doesn't handle files that are moved to a different directory, and it doesn't support copy-on-write if you're making a backup to the same device as your source.
Wat? The `diff` format(s) is/are well known and is meant to be both human readable and usable by the `patch` program. By postprocessing it you are destroying it, which sounds kinda dumb. Anyway, it seems your sed script only looks for a single leading `-` and removes it.
I agree, but I need the snap shot of new and deleted files to full fill my objective of the script. I'll try it out, but even with the one dash, it still produced 3 regardless.
 $ ls file1 file2 $ cat file1 a b $ cat file2 aa b c $ /bin/diff --unchanged-line-format='' --new-line-format='new: %L' --old-line-format='delete: %L' file1 file2 delete: a new: aa new: c Always check the manpages before writing ugly `sed` or `awk` parsers. 
So RTFM?
I think there isn't other than splitting the input file into parts and compressing those individually. I guess you could use standby or hibernate for the next few weeks but it would be pretty annoying if the PC fails to resume at some point.
This may not be the solution you are looking for, but how about breaking up the bigger file into smaller chunks, say of ~60GB each? I understand the compression will be less efficient when broken like this. 
You could try using something like [CRIU](https://criu.org) to save the entire `xz` process' memory state, but I've no idea how well that would work. You could also run it inside a VM (with some significant IO performance loss) and save the state before rebooting the host. Other than that I can only think of patching `xz` or, as others have mentioned, splitting the big file into smaller chunks. 
Try this awk -F/ '{print $3}' filename-with-urls It gives domain names 
Tar it into pieces first. Then compress each piece.
If the input is a single file, is there an advantage to using `tar --multi-volume` instead of simply `split`?
I’d advocate for splitting the file as well (e. g. using `split --bytes=64G`). Even with your “ultimate” compression settings, `xz` will only look at the last 1.5 GiB of the file (`dict` option), so I don’t think the compression efficiency should decrease by too much. That said – are you sure you want to use these settings? From what I read in the manpage, you’ve essentially used every option available to make `xz` as slow as at all possible, especially with that huge `depth`. Perhaps consider trading off some compression efficiency for compression time?
I'm not sure how easy is to glue stuff back, but I prefer to do it within one command. Interesting article about tar and multi volume. http://mynixworld.info/2014/04/13/creating-multi-volume-tar-bz2/
That's because `xz` will only use 1 core, I have an 8-core CPU with 32 GiB of RAM, and I can do my usual things while it is compressing, so speed is not a concern. The file in question is rarely used once compressed, and I want to use as fewer Blu-ray discs as possible.
&gt; I'm not sure how easy is to glue stuff back after split It’s literally just `cat` – con**cat**enate :)
Well, splitting up the file will let you use more CPU cores, since you can compress the fragments in parallel. Also, if you’ll be writing to Blu-ray discs, you’ll have to worry about splitting the compressed fragments across discs anyways, and there’ll likely be some room to spare on the last disc. So I would try to split the file so that the compressed fragments fit well on one disc (perhaps do several fragments per disc, so you can try to shuffle the fragments around to fit the discs optimally).
Thanks.
But would you cut it before tar or after?
&gt; CRIU Thanks.It works on my workstation.
No, I wouldn’t do any `tar`! It’s a single file, so just feed it into `split` directly. And I just looked at the manpage, and `split` supports writing to a pipe, so you could do something like split --bytes=64G --filter='xz -zkvv -… &gt; "$FILE.xz"' to reduce the disk space requirement.
I started experimenting with the different compression levels after I had made my earlier post in this thread. I was skeptical of your idea to use really high settings for compression because I thought that the time needed would explode while the file size would stop getting smaller after a certain level. I was wrong about that and it seems the file size does continue to get smaller with better settings with no limit in sight. I used a 4G sized VM disk image to experiment. The VM uses about 2.7G of its space and I've set up discard (aka. TRIM) with the QEMU scsi driver which should make it so the free space in the image file is full of zeroes. The original file has this size: testfile.qcow2: 4235264000 bytes = 4.3G The ten default compression presets of xz produce this result: level | size | time | rate -|-|-|- 0 | 1.1G | 0:02:22 | 28.4MiB/s 1 | 958M | 0:03:02 | 22.2MiB/s 2 | 933M | 0:03:40 | 18.3MiB/s 3 | 920M | 0:04:16 | 15.8MiB/s 4 | 901M | 0:06:23 | 10.5MiB/s 5 | 879M | 0:08:01 | 8.39MiB/s 6 | 875M | 0:09:20 | 7.20MiB/s 7 | 864M | 0:09:23 | 7.16MiB/s 8 | 855M | 0:09:56 | 6.78MiB/s 9 | 846M | 0:10:43 | 6.28MiB/s I tried to use the settings you mention in your post, but they don't work for me because I run out of RAM. Your settings need 16G of memory and xz crashes after a while for me. I reduced the 'dict=...' size in your settings and experimented with the following which needs 4G memory: xz -zkvv -F lzma -C none --lzma1=dict=402653184,mf=bt4,mode=normal,nice=273,depth=4294967295 testfile.qcow2 I then also tried a setting based on the `xz -3` preset with just an increased dict size added to it: xz -zkvv --lzma2=preset=3,dict=$((1610612736/2)) testfile.qcow2 That setting needs 5G memory. The results for those two command lines were the following: 1. `dict=402653184,mf=bt4,mode=normal,nice=273,depth=4294967295` 2. `preset=3,dict=$((1610612736/2))` settings | size | time -|-|- 1 | 28m38.026s | 813M 2 | 6m42.162s | 833M Increasing just the dict size seems really interesting as the compression improves a lot while the time needed stays reasonable. Here's a log of what I did exactly on the command line to show where all those numbers are from: https://pastebin.com/tgC494pb
There is ton of articles talking about it...
Did you check bash global config files too?
I haven't found any, could you link some?
GNU grep has `-B` (before) and `-A` options: grep -B10 -A5 '^# \*\*\*'
https://askubuntu.com/questions/376199/sudo-su-vs-sudo-i-vs-sudo-bin-bash-when-does-it-matter-which-is-used#376386
Hey, that's awesome - didn't know about `-B` and `-A`. Works great. Thanks! :)
[removed]
And -C will do both before and after iirc (I think C stands for context, but I always thought it was funny it went A, B, C)
Yup - saw that in the man pages. So, looks like it comes out as A-C-B. :)
I use the following at the beginning of some scripts that need to run as root: #!/bin/bash (( $UID == 0 )) || exec sudo "$0" This will make the script restart itself through sudo if it's run as a user. You can add a `"$@"` to the end of the line if you want to pass parameters as well. For scripts where you want a mix of commands run as the user, then root for some other commands, I think it's best to just repeat `sudo` on each line that needs root. This is also what you would do for functions in your .bashrc. What's interesting is that you can choose where in your script you want to ask for the password. You can do this with `sudo -v`. With the default setup for sudo, the password gets saved for five minutes or so. If you run a bunch of commands that need a lot of time to complete, the saved password will expire. To fix that issue, you can refresh the password continually in the background with this function: sudo_loop() { while :; do sleep 30 sudo -v &amp;&gt; /dev/null || break done &amp; } trap 'kill 0' EXIT That `trap 'kill 0' EXIT` line is to make sure the background job gets stopped when the script exits. You would do the password prompt like this: if sudo -v; then sudo_loop else exit 1 fi For scripts that need a mix of commands run as the user and root, I also tried to do the reverse: instead of using sudo for root, I tried using using sudo for the lines that need to run as the user. I had a helper function for that: user_run() { if [[ -n $SUDO_USER ]]; then sudo -u "$SUDO_USER" "$@" else "$@" fi } This looks at the SUDO_USER environment variable that sudo sets when a user calls something through sudo. The main script would run as root, and you would add `user_run` to all lines that you want to exexcute as the user that called the script. This user_run function seemed to work in my experiments, but I didn't use it a lot so it might have problems that I never noticed. Another thing is to just print a message to tell the user that sudo is needed, then quit: if (( $UID != 0 )); then echo 'This script needs to run as root.' exit 1 fi
And this is why I'm subscribed to this sub.
this is amazing, thank you!
Honestly, this doesn’t look at all appealing to me. My script is now no longer a regular Bash script (I have to call it with some `strun` command, whatever that is), and its output is clobbered with random extra stuff around it (“ok scenario succeeded”, great)? No thanks. The shell script itself also manages to pack two errors into as many lines of code (using `echo` and not quoting the variable expansions). It looks like this is meant to be part of some larger, non-Bash ecosystem – wherever `cpanm` is the “usual” installation method, I take it (Perl, I think?). Perhaps these idiosyncrasies make more sense within that context? But that still doesn’t make this look more appealing as a general-purpose scripting framework.
Hi! Thanks for comment. If this is the only reason, that your bash script is consumed by "strun" that it does not seems appealing to you ? The output btw is configurable by strun itself, the default output is not random, it's explained in the Outthentic documentation. So far I can't see the reason why you consider Sparrow not appealing. Please be more specific. As for me it just adds feature I found useful in Bash scripting.
&gt; If this is the only reason, that your bash script should be consumed by "strun" that it does not seems appealing to you ? Not necessarily, but it’s a starting point. Perhaps you should explain why this is necessary, because it’s a pretty big point against the framework in my opinion. &gt; The output btw is configurable by strun itself, the default output is not random, it's explained in the Outthentic documentation. I don’t doubt that :) but that doesn’t change the fact that the default output isn’t good. (I’ll get back to this in a second.) More generally – &gt; So far I can't see the reason why you consider Sparrow not appealing. I’ve given you the reasons already. You can argue against them, but that doesn’t just make them magically disappear :) --- Now, after taking another look at your post and also the Outthentic README, the way I see the problem is – you’ve written a “multipurpose scenarios framework”, which supports several languages. That’s fine, on its own. And that framework happens to include a suite configuration mechanism, where the script of a suite can access configuration variables, which seems reasonable enough for such a framework. But now you’re advertising this aspect as a standalone configuration mechanism for Bash scripts, and *that’s* the problem. Regular Bash scripts shouldn’t need some external runner tool (your **st**ory **run**ner). Regular Bash scripts shouldn’t receive arguments as `--param foo=bar`, but just `--foo=bar`. And regular Bash scripts shouldn’t waste two lines of output on the exit status. And this also circles back to the `strun` output, because this is where that extra output comes from. Saying “scenario succeeded” is fine when you’re using `strun` for what it’s intended: running scenarios. But it’s completely superfluous and confusing for a standalone Bash script.
Thanks for reading the documentation :) Sparrow/Outthentic had been originally designed as testing framework ( this where default output comes from ) and later evolved into general purposes scripting framework. I agree with you that this is not convenient for the standard Bash scripting, yet Outthentic is not for Bash only. Actually as you've already noticed it's scripting framework where user choose which language to use ( for not it's Bash, Perl, Ruby and Python ) when writing scripts. Outthentic just enrich you scripts with very useful ingredients with minimum impact to your original code. &gt; But now you’re advertising this aspect as a standalone configuration mechanism for Bash scripts I'm advertising an aspect of scripts configuration mechanism, where Bash is one of the language users write their scripts on. As for me I find the idea of describing scripts configuration as YAML/JSON structures is quite appealing. There is more appealing fact that you get it almost for free ( as described at the dev.to post ) in Bash scripts gets run under strun environment. BTW this is only one of many others features provided by Outthentic/Sparrow ... &gt;is fine when you’re using strun for what it’s intended: running scenarios. But it’s completely superfluous and confusing for a standalone Bash script. I know what you mean, however the border between Bash script and scenario, imho is quite subtle. As for me I often start my script as regular Bash script and once I need extra functionality ( like this one described at the dev.to post ), I easily convert my script into strun scenario. It implies a minimum overheads, though benefits I gain far beyond. 
&gt; Regular Bash scripts shouldn’t receive arguments as --param foo=bar, but just --foo=bar --foo bar Yeah, that is true. But is it a problem to add key --param to your options? For your keys in **pure** bash you must use getopts(or getopt function for GNU style keys) and slice through them. ``` #!/bin/bash OPTIONS=`getopt -o h: --long help,some_cool_key: -n 'parse-options' -- "$@" while true do case "$1" in --some_cool_key ) echo "key" var=$2 shift 2; esac ``` In Outthentic you can just use function config() for that. `var=$(config some_cool_key)` That is all! You don't need to use this strange and old bash things. Have you ever written configuration files for your bash scripts? It is really pain. Sparrow/Outthentic allow make it much easier. &gt;And regular Bash scripts shouldn’t waste two lines of output on the exit status. --format concise &gt;But it’s completely superfluous and confusing for a standalone Bash script. But Why?!
If my bash scripts require that much complexity, I'm not going to use Bash. But maybe that's just me. What would be the advantage of doing that in bash over python? 
&gt; Regular Bash scripts shouldn’t receive arguments as --param foo=bar, but just --foo=bar `--foo bar` Yeah, that is true. But is it a problem to add key --param to your options? For your keys in **pure** bash you must use getopts(or getopt function for GNU style keys) and slice through them. #!/bin/bash OPTIONS=`getopt -o h: --long help,some_cool_key: -n 'parse-options' -- "$@" while true do case "$1" in --some_cool_key ) echo "key" var=$2 shift 2; esac In Outthentic you can just use function config() for that. `var=$(config some_cool_key)` That is all! You don't need to use this strange and old bash things. Have you ever written configuration files for your bash scripts? It is really pain. Sparrow/Outthentic allow make it much easier. &gt;And regular Bash scripts shouldn’t waste two lines of output on the exit status. --format concise &gt;But it’s completely superfluous and confusing for a standalone Bash script. But Why?!
This is very good point. Many people write Bash scripts. It's OK. I respect Bash as well. Why not add some useful features to it, instead of switching to other languages ... Outthentic does that. 
Hey this looks like a great start! You could probably go a step further, and make a `sparrow` (or maybe `sprw` or something) command that could be used in the shebang, and would process the command line args and then pass it to `bash`, so scripts could simply be run by typing `./hello.sprw` and the first line would be `#!/usr/bin/sprw`. (or whatever you want to call it.)
Thanks. Will consider this idea in the future plans.
 grep -n -x ".\{$length\}" should work... what output do you get, do you get any error?
&gt; grep -n -x ".\{$length\}" Thank you! It worked! Just had to use **" "** instead of **' '** and I had no idea.
but your question did have `Putting $length inside "" doesn't help either.` anyway, glad you got it working :)
Does this do what you want? It recursively looks for all files of type, and puts them one directory deeper into a directory of same name as file: #!/bin/bash SEARCH_DIR=${1} FILES=$(find ${SEARCH_DIR} -type f \( -name "*.gif" -or -name "*.jpg" -or -name "*.png" -or -name "*.txt" \)) for FILE in ${FILES[@]}; do { CURRENT_DIRNAME=$(dirname ${FILE}) FILENAME=$(basename ${FILE}) NEW_DIRNAME=$(echo ${CURRENT_DIRNAME}/${FILENAME/\.*/}) echo mv -v ${FILE} ${NEW_DIRNAME}/${FILENAME} } done Also, this should clean up the mess you had. (Didn't run your script, but went by your example. #!/bin/bash function buildTestCase() { if [ -d 1 ]; then rm -rf 1 fi rm -rf 1 if [ ! -d 1/1/1/1/1/1/ ]; then mkdir -p 1/1/1/1/1/1/ fi touch 1/1/1/1/1/1/1.txt touch 1/1/1/1/1/1/1.png touch 1/1/1/1/1/1/1.jpg touch 1/1/1/1/1/1/1.gif } function moveit() { PATTERN=${1} } #buildTestCase #Uncomment to build test data. TXT_FILES=($(find . -iname *.txt)) PNG_FILES=($(find . -iname *.png)) JPG_FILES=($(find . -iname *.jpg)) GIF_FILES=($(find . -iname *.gif)) ALL_FILES=(${TXT_FILES} ${PNG_FILES} ${JPG_FILES} ${GIF_FILES}) for FILE in ${ALL_FILES[@]}; do { FILENAME=$(basename ${FILE}) DIRNAME=$(echo ${FILENAME/\.*/}) mv ${FILE} ./${DIRNAME}/${FILENAME} } done #Delete all empty directories find . -type d -empty -delete 
ah ok, that explains it.. you should read this http://mywiki.wooledge.org/Quotes and generally that site, very good to get grasp of bash
Thank you so much for the reply. Wow... this looks amazing! In the midst of trying to fix the issue i somehow screwed something up and now my system is doing something funky and freezing, so i will have to test this as soon as i figure out what i did! Not my day today.
Your example does not use variables like mine does, but in case you are playing with them, be very careful when using them. If they are unset, then they will often be treated as an empty string. So something like `rm -rf ${VARIABLE}/`, will try to delete from the root if `VARIABLE` is unset. Always test with echo first. So in my example. replace: mv -v ${FILE} ./${DIRNAME}/${FILENAME} with echo mv -v ${FILE} ./${DIRNAME}/${FILENAME} To see what it will try to do. If you are happy with what the variables resolve to, then you can remove the echo.
perl
It's forking to echo, grep, and cut. Does that dramatically slow things down? And does perl have an advantage over other languages? I usually use python.
Yeah, forking the processes will cost you. Perl has a rep for being very fast for the type of thing you are doing. And I have swapped out python...and lost the swap space.
please post your script.
Ah, ok. Thank you for the information. I'm not a trained programmer, so this is very useful. I'll check out Perl.
I'm betting using grep or awk would do the task much, much quicker.
Post the script and sample data. There may well be a way to accomplish your goal faster in bash, or perl as suggested by /u/falderol.
`grep -R '$WORD YOU WANT' ./"$FILENAME"`
 while read p; do if [[ $condition ]]; then consequent fi done &lt; file.txt &gt;It's forking to echo, grep, and cut. Does that dramatically slow things down? Yeah, it does sound like something that could be significantly improved without the need to reach out for `awk` or `perl` or `python` etc... but without more context I can't say for sure. What's the details on the `$condition` test? Because that is likely your biggest time killer...
&gt; The file is a well-known file type and other programs load it in seconds. Sharing the filetype would probably result in a quicker solution. There may be a CLI tool specifically for working with that filetype, or a python/perl library.
The file type is "gcode". Used to command 3d printers. Simple text-based setup. I can load it into slicer programs like Cura and it loads in seconds. So I know it can be parsed that quickly. But I want to manipulate it after it is loaded with fine details. Which is not offered on the slicers I have tried.
What sort of operations are you attempting to manually do on the gcode file?
Read and manipulate the coordinate values. So a line might say: G1 X79.253 Y60.651 E4.70636 And I want to read the X value to a variable, add some value to it, and rewrite the X value to some other value. Basically, systematically do things like what blender does.
https://pypi.python.org/pypi/gcodeutils/1.3.2 ?
Perfect! And I love Python. Thanks!
grep. I work with files &gt;100K lines. Grep is fast. While read loops are not. In my initial (lazy) script, the read -r loop took &gt; 2hrs. In my current script, it's seconds.
Ack is another great command line tool https://linux.die.net/man/1/ack But ag is super fast http://geoff.greer.fm/ag/ https://github.com/ggreer/the_silver_searcher 
https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases 
Aside from using git's alias system, which is what you should do, you *can* create a shell alias for git-grep, just not a passthrough one: alias gg="git grep $GREP_OPTIONS" It's usually better to do this type of thing anyway, because otherwise you'll get weird behavior from shell scripts that expect `grep` to be `grep`.
This might be a bit easier in python. But i took a shot at this and hers what i came up with. First In order to make jq not complain at me i made your example data look like this. I'm guessing this is more like what you have, only not as long of a list. [{ "vote": -1, "username": "anon", "title": "title", "comments": 0, "content": false, "link": "url", "pid":1234, "posted": "date", "ptype": 1, "score": 5, "thumbnail": "jpg" }, { "vote": -1, "username": "anon", "title": "title", "comments": 0, "content": false, "link": "url", "pid": 1234, "posted": "date", "ptype": 1, "score": 3, "thumbnail": "jpg" }, { "vote": -1, "username": "anon", "title": "title", "comments": 0, "content": false, "link": "url", "pid": 1234, "posted": "date", "ptype": 1, "score": 7, "thumbnail": "jpg" }, { "vote": -1, "username": "anon", "title": "title", "comments": 6, "content": false, "link": "url", "pid": 1234, "posted": "date", "ptype": 1, "score": 6, "thumbnail": "jpg" }] Then I figured out how to strip some fields from it and convert to csv. Then I sort it on the score field. cat json.json |jq .[]| jq '[.score,.title,.pid] |@csv' -r|sort -k1 -n The important part is. jq '[.score,.title,.pid] |@csv' -r Strip out the fields you want here, then you can easily sort by score and use other tools like cut to get the titles and such.
Sorry, I didn't make clear what I'm trying to do. Let me try again. I want to rank highest by number of comments, then revisit and grab stuff based on that, or if I could just figure out how to dump ranked by number of comments I could figure it out from there.
this part jq '[.comments,.title,.pid] |@csv' -r can be modified to get any fields you want. by simply adding .fieldname between the square brackets. again, you'd be better off using python. 
Damn, I'm an idiot. That works great, thank you. 
there's a sort_by function: jq 'sort_by(.comments)'
Awesome, will try. Thank you.
Neat til 
If you're having trouble with quoting, you can single quote, end your single quote, begin your double quote, end your double quote, and repeat until your string ends. It's messy, but it does the job. 
I would use double qoutes for both the json and the variables, but escape the qoutes around the variables
I'd install and use `jq` to generate the json safely. jq -n --arg r "$1" --arg g "$2" --arg b "$3" '{$r, $g, $b}' | curl -H 'Content-Type: application/json' -d @- http://192.168.0.17/manual
Thank you, this seems to be the most straightforward solution
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
`cat "$filename"`
Side question, how would I capture a whole command into a file. Like I want the file to contain echo "*"? So if I run a diff against echo * it'll know the difference.
`echo "*" &gt; output.txt`
Im more so referring to your first example. Would it be possible to do it with command line arguments? So that any command (with or without " ") will be inserted to a file? Example: output file could contain echo "*" or just echo *
Sorry, your choice of words is confusing me. :) Are you saying you want to save a command string into a file? 
Sorry about that! So let's say I type ./script echo "*" or ./script echo * Is there a way to echo the arguments ($1 $2) into an output file that would show it as entered? Meaning output file would contain echo "*" or echo * or whatever I entered as an argument.
&gt; Sorry about that! No, the confusion is my own. Ah, I think you're talking about the parameter list. You can do this from within your script by parsing environment variables. Check out a variable called `$@`. It contains everything that was offered to your script when it was called. 
I didn’t read that but is the script in one of the cron folders with a “.sh” filename suffix? Once upon a time I put script in one of the cron.weekly folders or something and discovered this odd nuance to filenames in cron folders. Drove me batshit crazy. 
My immediate thoughts: 1) does the variable need to be escaped, e.g. “${DATABASE}.tar.gz”? 2) what user is the cronjob running as and do they have correct permissions to delete file? 3) try rm -f? (Though I’d probably want to understand what’s wrong tbh) 
try $ a=' 10020 history 1' $ echo "${a#*[0-9] }" history 1 globs are not like regular expressions... `[0-9]*` means match a digit and then any character zero or more times `#*[0-9] ` means minimal match, zero or more characters followed by a digit and a space 
You should "exit 1" on unknown client and you should be careful with relative and absolute paths. In cron-scripts you should always, always use absolut paths. 
yeah. this works
I think you should ssh into that machine first and try commands you want to perform and put them into that com.txt file. I think all commands will be performed by bash by default. What I am not sure is, if plink run commands in login shell (which usually adds some aliases or env. variables). For example `ll` is alias of `ls -l`, try to put `ll $HOME` to your com.txt and see if you will get output of that commnad. Use also plink -t to enable Pseudo TTY allocation, some programs detect the if the output is going to TTY or file and they may behave different. Maybe it is also good to ask here for the exact operation you want to do on remote hosts. 
Thanks. I ofcourse have been doing it manually via Putty ssh. But id like to automate the process since having 60+ PCs ... takes about 2 hrs to get through when you just need to delete a profile, download a file, add a profile and run it. I know all the commands work on the machine itself. 
That's awesome, I had no idea. So if I wanted to know if $2 contains single quotes, would I just do ` if [[ $2 = *\'*\'* ]];then echo $@; fi ` ? 
Try the read command
The read command has me type it and it has no parameter where it allows me to use command line arguments which is what I'm looking for. 
Can you run the script with bash -x in an interactive shell? Does it fail when you do that? 
No, quotes are not shown in the parameter list, so you'd probably need to escape them. Why do you want to check for single quotes? I've never found the need. ;) Found [this](https://stackoverflow.com/questions/1668649/bash-how-to-keep-quotes-in-arguments) on SO which looks like what you'd like to do. 
Huh that's fair and if you use single quotes it will be usually inside double quotes. So if I wanted to escape double quotes would it be \"*\" ? The link you posted does work for one but not the latter haha... Sorry for bugging you, I'm really just trying to learn and knock this correctly into my head haha. 
&gt; The link you posted does work for one but not the latter Yes, this is a common problem with CLI solutions. They don't work for everyone in every situation. I tend to experiment until it works. Your escape looks correct. You could also use single quotes to quote double quotes. i.e. `'"*""`
You're missing something like `complete -F _get_profiles &lt;command&gt;`
Makes sense. So technically for an if statement I could do: if [[ $2 = '"*"' ]]; then do something; fi 
 while read line; do printf "$line" done &lt; filename.txt
Try it and see. ;)
The arguments are in `$@`. It's an array. They are also available as `$1`, `$2`, ... and the number is in `$#`. The two `"` in your command line get eaten by bash. Extra spaces will also get eaten. The exact text the script will see as parameters is: 1. `echo` 2. `*` When you do `"$@"` inside your script, bash will prepare a list of the parameters. You can test this on the command line by setting `$1`, etc. through `set --`, like so: $ set -- a b c d $ echo "$@" a b c d $ echo $# 4 $ echo "$1" a 
If you invoke your script as described, you are already passing in arguments. You reference them with $1, $2, etc., space delimited. If you're curious to know what each positional variable is saving, just write some echo statements in (`echo $1`) to see what each variable's contents is. I think there's a way to pass in a whole array as well if needed (`$@` maybe?)
Actually I had in mind to do something like `COMPREPLY=( $( compgen -W _get_profiles "$cur" ) )` 
What is this useful for?
Right now, it's probably only useful for finding syntax errors in your shell scripts before running them, which can save you time. `osh -n foo.sh` will do that. It parses more of your script than bash does [1]. However, it is intended to be a replacement for bash and then an improvement on it -- e.g. adding better support for structured data like arrays and hash tables. Some people have expressed skepticism about that, but that is why I explained the comprehensive testing and blogged about how it runs real programs, etc. [2] [1] http://www.oilshell.org/blog/2016/10/13.html [2] http://www.oilshell.org/blog/2017/07/02.html
What if there are multiple such directories (e.g. if you created a second profile)? Line 7 would give you an error that way. You could do: if [[ -d $moz ]] then for d in "$moz"/*.default do mkdir "$d"/chrome done fi If you only want to create the new directory inside one of the `.default` directories, you could add a `break` after the `mkdir`.
 tail -f Personal_cryptcheck_2017_11_10_12_56_59.log | grep -v " 0 differences found" 
Close! That will exclude the 0 differences found lines, but it will print all the rest (not all the lines have differences found, so I should have clarified). But thanks to your answer I got this solution: ``` tail -f Personal_cryptcheck_2017_11_10_12_56_59.log | grep "differences found" |grep -v "0 differences found" ``` Thanks a bunch!
Change the grep command into the following: grep " [1-9][0-9]* differences found" It does work for your two example lines. I hope it also works if the numbers get larger but I didn't try it.
Thans, I will try this one as well. It does print ```1 differences found``` so I can avoid the question mark, but I will keep that in mind when I deal with logs that print similar things differently.
If the lines are predictable like that, I'd pipe to awk instead. tail -f yourfile.log | awk '$7&gt;0{print $0}' That should only print lines where that field is greater than zero, going by the lines in your example.
Are you forgetting a slash? 'cd ~/info; ls -ld m*'
Perhaps I misunderstand this, but do you have a folder called literally "~info"? It's odd to begin with the special character ~, as that usually refers to the home directory. You would try 'cd \~info' in that case, though.
No, sadly I'm not 
As strange as it is, I do have a file that's ~info 
It's actually ~info, but even if I try a directory in my home like Documents or Temp it doesn't work either. 
Try ~/\~info for an absolute path. I highly recommend renaming the directory...
You can tell `cut` to use `\0` instead of `\n` as the line terminator: ``` $ cut -z -c 8-16 &lt;&lt;-EOF Bob Is a man Likes egg Samantha Is a woman Likes chocolate EOF a man Lik ``` Note I adjusted the indices from 7-14 to 8-16 to account for the newline characters.
Insert a space between ~info and ;
I suppose depending on who is running the script (your user vs another program) you could get different working directories. When I’ve had that issue in the past (e.g. using cron to run a script) I can usually fix it by using the full path name. (So, in this case something like ‘cd /home/user/\~info’)
Same issue.
Try eval $str instead of just $str
This is why I'm subscribed to this sub. Before this, I had no idea you could write your own completion scripts (obviously I still have a lot to learn). I do have a script that I'm now going to write one for ([timertab](https://github.com/amlamarra/timertab) if you're interested). I found this [askubuntu question](https://askubuntu.com/questions/68175/how-to-create-script-with-auto-complete) that helped me get started. It might help you too. Working from one of the answers there, this might work for you. _get_profiles() { prof="$(ls /usr/share/xyz/*.profile) $(ls /etc/xyz/*.profile)" local cur COMPREPLY=() cur="${COMP_WORDS[COMP_CWORD]}" COMPREPLY=( $(compgen -W "${prof}" -- ${cur}) ) return 0 } complete -o nospace -F _get_profiles ./script_name.sh 
 cd \~info+ perhaps?
you have to `eval $str`
So, I'm a little unclear as to what the code above has to do with the "~info/" problem, but I was able to create and cd into "~info/" just fine with: cd "~info" *with the quotes. That may need to be evaluated in the script as $(cd "~info") 
Check the owner/group permissions on the script and the ~info directory. Also, what is the absolute path to the script file and ~info directory?
Thanks... I'll try this asap ! :)
I'm not sure if this would work but you may be able to use $HOME instead of ~
Oh wow, thanks a ton. That is exactly what I was looking for!
On mobile, can't find the right key.. However, the dir name as you entered it means "the home directory of the user info" Someone here said double quotes work, which seems likely. I'd also try prepending "./"
I think I know what it is. You see, it is the shell the one that makes such substitutions like `$VARS`, file globbing like `*.txt` and also the special character `~`, I guess what you are trying to get into the `info` user's home. The problem is, when you pass that parameter to your script, you use single quotes so it is not being replaced, and your script is not doing any replacing either, resulting in `~info` never being evaluated and taken as a literal string. So in this case what you want is the shell to evaluate the string with `eval` as raw shell code: str=$@ if ! grep -Fxqs "$str" storeCommand echo "$str" &gt;&gt; storeCommand eval "$str" fi Hope that helps, cheers.
I think in its current form it's literally /path/to/homeinfo, as opposed to /path/to/home/~info
This won't work if the number is greater than one digit and has a 0 for the last digit. Add a space before the 0 in your grep -v command
It's hard or impossible to do this (print or save commands before running them) in bash for arbitrary commands. The closest you'll get is using eval, but there are still many awkward cases and debugging through multiple levels of evaluation is usually difficult. [http://mywiki.wooledge.org/BashFAQ/050](http://mywiki.wooledge.org/BashFAQ/050) has more information about this problem. In this case, it's not clear what's actually happening since your code has syntax errors and wouldn't result in the error message you provided even if it didn't, but most likely the problem is related to the [order of shell operations](https://www.gnu.org/software/bash/manual/html_node/Shell-Operation.html#Shell-Operation). By the time the shell expands `$str` (shell expansions, step 4), it has missed the opportunity to be parsed into multiple commands (step 3), so the semicolon is just part of the `~info` string, and `cd` fails trying to find `~info;`. With your sample input, `cd` should complain about too many arguments before it looks for any of the directories.
Good point. Thanks for pointing that out.
the upload works fine when I run the script but the file isnt deleted after OR if I try escaping the variable $DATABASE it throws errors like I showed above.
I tried escaping those variables at the end and it then throws these errors when run: The user-provided path {sitename}.tar.gz does not exist. rm: cannot remove `{sitename}.tar.gz': No such file or directory 
OK, changed the end of the script to this and it seems to work now: rm "$DATABASE.tar.gz" exit 0 else echo "Error, file was not removed" exit 1 fi fi Im still pretty green with bash, so what exactly wasnt working before? 
Thanks, see my latest comment, so I guess it was due to not having 'exit 1' then?
[removed]
how's this? replace sub1 and sub2 with your two predefined sub directories #!/bin/sh read -p "Enter dir: " dir mkdir $dir mkdir $dir/sub1 mkdir $dir/sub2 while : ; do read -p "Enter sub dir: " sub [[ "$sub" = "end" ]] &amp;&amp; break mkdir $dir/$sub done
seems to work now with proper use of 'exit' commands, thanks
Dollar sign goes outside the curly brackets but sounds like that wasn’t your problem 
Huh, you just had to exit the script? (Like using the “exit” command?)
First, I would double quote the database file name. Second, I would use find to search the directory and remove the file. It's more reliable than running rm without an absolute path. find /data/backups/databases -type f -name "$db_name.gz" -execdir rm {} \+ Your problem is one of three things. Permissions, wrong directory, or a bad filename. The reason I asked if you've run the script with bash -x is because -x offers some debugging capabilities. I'll offer you one thing, unfortunately it doesn't relate to your problem. You don't need to tar the database. You can just gzip it since it's a single file. When you do need to tar something, you can use the 'z' flag to tell tar to compress the tarball when it's done. One more thing, look into mysql's .my.cnf. The way you have the script written now, I believe someone can grab the credentials simply by viewing the list of processes.
seems to be that yes, tested it twice now and no file remains after but the new upload is there in the S3 bucket
ah ok thanks
thanks, see my update comment, seems to work ok now after exiting the script. About this though The way you have the script written now, I believe someone can grab the credentials simply by viewing the list of processes. you mean anyone with access to the server can see it? Which part of the script is vulnerable? I forgot to mention when I first posted that I didnt write this script though and Im still pretty new to bash scripting and linux admin... thanks again 
The password would appear during the database dump in the process list. If someone were to run ps -ef, the password would be visible, if I'm right. Instead, it's preferred to read the password from a file. Mysql calls the file .my.cnf. Limit permissions on the file to 400, owned by the user who will be running the script.
On phone, but something like ``` local array=“/“ while true; do local dir if [[ “${dir}” == “end” ]]; then break fi array+=(“${dir}/”) done mkdir -p “${dir[*]}” ``` can't type anything more robust or test whine on phone, but might help point in right direction to handle a theoretically infinite amount of input 
Missing tons of quotes though
This is similar to /u/52358's script, but with a correct shebang and proper quoting #!/usr/bin/env bash read -ep "Directory: " dir mkdir -v -p "$dir"/{sub1,sub2} while :; do read -ep "Sub directory (end with empty line or "end"): " sub case $sub in (end|"") break ;; *) mkdir -v "$dir/$sub" ;; esac done if type tree &gt;/dev/null 2&gt;&amp;1; then tree "$dir" else find "$dir" fi The `-v` (verbose) option on `mkdir` is not standard, so just omit it if you don't want it, or if your `mkdir` implementation happen to not have it. `tree` is a non-standard command that prints a nice tree view, but it is likely not installed by default, so the code only uses it if it is installed, falling back to the not so pretty output of `find` if not. `tree` is likely available in your system's package manager.
ok thanks for pointing that out, will get that changed 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
 (umask 222; date +%s &gt; .timetrack_info) but why?
Set umask 555 on the folder or chmod 222 any file that you create.
Uhh its a linux course I'm doing. I'm comparing how much time passed by using that command. My teacher told me it would be better to have the file as read-only. Since i delete the file when i stop the program that is kind of a problem. If it is true that i cannot create files as read-only. This is the whole code... #!/bin/bash if [ "$1" = "start" ] then date +%s &gt; .timetrack_info elif [ "$1" = "stop" ] then if [[ -f .timetrack_info ]] then starttime=$(&lt;.timetrack_info) statustime=$(date +%s) timepassed=$((statustime - starttime)) echo "$timepassed" rm .timetrack_info else echo Timetrack ej aktiv fi elif [ "$1" = "status" ] then if [[ -f .timetrack_info ]] then starttime=$(&lt;.timetrack_info) statustime=$(date +%s) timepassed=$((statustime - starttime)) echo "$timepassed" else echo Timetrack ej aktiv fi else echo bad argument fi 
A file deletion is an operation on the directory the file is in, so you need write permission to the directory. The permission on the file itself is irrelevant to `rm`. Those permissions only determine whether you get to read/write the content of the file or not. See http://mywiki.wooledge.org/Permissions
You got that backwards
Woops, you’re right. It’s umask 666 or chmod 111
Thanks for all the help. Almost got me to where I needed to be. I kind of understand. #!/usr/bin/env bash read -ep "Enter a semester: " dir while :; do read -ep "Enter a class: (end with empty line or "end"): " sub case $sub in (end|"") break ;; *) mkdir -v -p "$dir/$sub" ;; esac mkdir -v -p "$dir"/{Assignments,Notes} done if type tree &gt;/dev/null 2&gt;&amp;1; then tree "$dir" else find "$dir" fi ~ The last barrier is that the "Assignments and Notes" Subdirectories need to automatically be created in whatever "Class" folder is created. I'm having a hard time getting them to be in there. They just all go into the main folder. Any ideas?
Thanks for all the help. Almost got me to where I needed to be. I kind of understand.
break it up in pieces: first examine the output of `top -n 5`, then see what `grep 'init'` does to it and finally use `cut` to get exactly what you want. Same goes for `free` Also learn to use the `man` command, and pay attention to the EXAMPLES paragraph. Be aware that 'Total' and 'total' are 2 different things. 'Case insensitive' is an option
heh. JUST remembered to capitalize Total. so that part works perfectly now. 
not so sure about `top -n5`. It doesn't do what you think it does. I think you mean `ps`
right, my bad. I need coffee
try this: top -n5 &gt; /tmp/blah and then vi /tmp/blah It will show you the output of `top`. It's not pretty and not meant to be used with filters like `grep`.
read bit is 4, not 1
those quotes are optional, the script above works
look at the -b option in the top man page also.
You may be better off using awk instead of cut. For instance with your free command: free -m -t | grep Total | awk '{print $3}' If you arent familiar with awk it can be must for parsing tools.
Unless of course you are specifically being told to use cut like for a class or something. In that case file awk away for another day haha
Oh man I messed this up badly :)
Have you considered using pidstat instead? eg pidstat -p 1 1 -t 
I have not. I think our teacher wanted us to stick with the commands we’ve covered. He made us do a similar practice using ifconfig...and then hostname -I. The latter was waaay too easy. 
agree on the awk preference. here's a function for a frequent (e.g. this very instance is a particular example: $ field () { awk "{ print \$$1 }"; } and combined with case-insensitive grep, the command line: $ free -m -t | grep -i total | field 3 i like to keep the "whoopee" and "whiz-bang" (non alpha-numeric)_ off the command line $ 
Here's a function to simplify the challenge: mmd () { local dir; read -p "Directory? {end} to quit: " dir; [[ $dir == "end" ]] &amp;&amp; return; mkdir $dir $dir/sub1 $dir/sub2; ls -l; pushd $dir; mmd; popd } *pushd* and *popd* show the working directory. *dirs* shows the stack.
Would something like this work to get a list of file names you are interested in? find your/directory/here/ -type f | grep -F -f file-with-list-of-IDs.txt If it's triggering on directory names or something, maybe something like this works to make it only match IDs in file-names but not directory names: find ... | grep -E -f &lt;(sed 's@^.*$@.*/[^/]*&amp;[^/]*@' file-with-list-of-IDs.txt) 
full screen as the only window open?
I didn't notice a difference between left and right half of the screen, or about top-left, top-right, bottom-right, bottom-left when split into four quadrants.
Fullscreen rules 
You might as well boot up in non-GUI mode; I forget what you call it, though.
I have 14 open terminals right now there is no consistency about where they are. 3 are full screen. 
I use my terminal full screen, with multiple splits via tmux. Usually split left and right, but sometimes, I have a bottom horizontal pane for an IDE-like experience. I always have multiple windows open in tmux and organize my workspace on a per project basis.
 while read file_id; do mv $file_id /directory/to/put/"extracted"/files/in/ done &lt; list_of_ids.txt Perhaps? Depending on what each line of your list and filenames look like.
Middle. I've wondered about this, not enough to check though.
runlevel 3?
I usually have whatever I'm typing into (terminal, text editor, etc.) on the right, with whatever I'm reading (documentation, stackoverflow, etc.) on the left. I think it is because when hand-wtiting something, as a right-hander, the thing I'm writing on is usually to the right -- for example, when solving equations in school, the notebook would be to the right and the textbook would be to the left. I have tried searching for research on this, and haven't been able to find anything. Please let me know if you do.
I'm also interested on what others can say about this. Maybe in a tiling window manager subreddit you get more responses.
Center, portrait, 27"
Both eyes do send information to both sides of the brain, but in a way that the left half of you field of view (the right half of both of your eyes) go to the right side of your brain and vice versa, so it's pretty reasonable to assume that the left and right halves of your vision are optimized for different tasks (this is true about ears). (OP probably already knows this, just thought I'd elaborate) https://en.wikipedia.org/wiki/Optic_chiasm#/media/File:Gray722-svg.svg
I have it at the top center but with a keyboard hotcut to call it and hide it. It is also transparent so I can see what is behind it.
Not if you're using transparency, and running something like a web browser behind the terminal window. Also, split terminal windows (e.g. terminator) make a huge difference.
No, there isn't. They never get seen by the program that you call.
Side question: if I do `echo *` is there a way to output to a file so that is `echo \*`?
If you want exactly these characters added to a file, including the `\`: echo \* Then you can do it like this: echo 'echo \*' &gt; filename echo "echo \\*" &gt; filename echo echo\ \\\* &gt; filename
Is there a way to do it with arguments? When I do it I just a get a `\` and not a `\*` odd enough.
I don't know what you are doing. It seems to work for me: $ echo echo\ \\\* echo \* $ echo "echo \\*" echo \* $ echo 'echo \*' echo \* I ran these command in a folder with files in it, so the `*` really wasn't seen as a wildcard and instead as just text.
Sorry haha I meant with like $@ or $2.
I don't understand what you want to do.
In general can I do `./script echo * ` And have it go to a file such that it looks like `echo \*`
From the outside, you would call your script like this: ./script 'echo *' Then inside the script, you would do this: echo "$1" &gt; filename Or more correct would be using 'printf' because there can be unexpected mistakes when using 'echo' for a job like this that involves a variable. You would do the following with 'printf': printf "%s\n" "$1" &gt; filename Using 'printf' also helps if you want to write all parameters to the file: printf "%s\n" "$@" &gt; filename This will print each argument on a separate line of text.
When I only have one, bottom half of the display.
Full screen with tmux... But which side does IRC go on?
Why choose sides when you can have a whole tmux window for irc, use panes to organize the different channels. Or, with some irc clients (such as weechat, my favorite), it supports splits within one instance. 
Same here
I guess that would work. Weechat does have a /exec command...
Ah, thank you! That makes more sense. 
Across four displays, my terminals are all in the center or far left. My browser is in the right-most screen.
Left half of screen is terminal. Right half is web browser. This corresponds to my mouse and keyboard, since my mouse is on the right and i scroll more often in the web browser and type more in the terminal.