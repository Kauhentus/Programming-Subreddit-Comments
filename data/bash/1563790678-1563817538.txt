&gt;df -BG / | awk '{ print $5 }' | tail -n 1 does not work, but thank you for your reply
&gt;df -h / | awk 'NR&gt;1{print $3 "/" $2}' I get 2 numbers: 817 and 923 GB. I am sorry: I don't understand what they correspond to. thank you for your reply.
thank you for your reply. Finder says 880 GB used, and your script 817. What would cause the difference ?
&gt;df -h / | tail -n1 | awk '{print $3}' the first script gives 817, and finder says 880 GB. The 2nd script does not work. thank you very much for thinking about my post
It seems that your df is using a different format (I am testing on CentOS 6). Can you give us the output of `df -h /` so we can check?
That’s a super odd way to run 4 curl commands... I don’t see any error checking and I don’t even see a skeleton of the curl request you are making. I don’t think there is enough info here for anyone to help you.
What program do you use to listen ? I tried mplayer, and while playing, it prints ICY Info: StreamTitle='I.Y.F.F.E - Storm (feat. Desiree Dawson)';StreamUrl=''; ICY Info: StreamTitle='FLOZEE - BLUR';StreamUrl=''; Among other crud, of course. It seems to be nice too, flushing each output line, so there are no buffer delays in mplayer ... | grep '^ICY '
ah okay but you have to play it, or? you can't get all songs of a day at once for example ?
Correct, those lines appear while each song starts.
On top of what else has been said, you probably want: for useABetterVariableName in "${search[@]}"; do "$useABetterVariableName" curl_close() done This whole "using an array to do this" approach, though, seems utterly pointless. Do you have more code and you've over-sanitised it, or is that literally all you've got so far?
I generally use something like this, adjust the path to point to a specific location: &amp;#x200B; du -h /home | grep '\[0-9\\.\]\\+G'
so the curl_close() code is same in php_curl. interesting. thanks man
Welcome to the zsh club. While you're learning, check out r/zsh, we try to be friendly and helpful. As you get more into zsh, you will find that most people think omz (oh my zsh) is bloated and slow. They're mostly correct, but as a newcomer, don't worry about it, omz works well, and if you never have problems there's no reason to switch to something else.
Opening the stream in vlc shows the current track fine. If you wanted your result, you'd have to [scrape the page](https://en.wikipedia.org/wiki/Web_scraping).
yes sorry about that. im playing around to automate running 4 curl commands every week
`md5sum FileX` does not return `51889`. It returns `51889 FileX`. `md5sum FileX | cut -d' ' -f1` should return what you want.
Uh... that's not what I said at all... You say that you want to run four curl commands, well simply call them in order: #!/bin/bash curl -s http://some.addre.ss/ curl -s http://another.addre.ss/ curl -s http://simple.isnt.it/ curl -s http://another.example.com That's a start. Then you may want to branch out. Let's take your array example - if you absolutely wanted to use an array, well you'd put all your URI's into the array and do something like for targetUri in "${search[@]}"; do curl -s "${targetUri}" done Then you might want to build in error checking, timeouts, handling for `curl` options etc. Maybe add a couple of functions... it really depends on what you're actually wanting to do. You need to give us WAY more to go on than what you have, though...
Thank you. Good Luck to you.
Hmm... Maybe it's a different version. I definitely got used gb over total gb. What platform are you on, and what is the output of just a plain `df -h` command?
cat tracklist.php\\?lang\\=en | tr -d "\\n" | sed 's/song\_time/\\n/g;s/current\_song/\\n/g' | grep song\_title| sed 's\_\^"&gt;\_\_g' | sed 's/&lt;\\/div.\*song\_title"&gt;/\\t/g' | sed 's\_&lt;/div.\*$\_\_g' | sed "s\_\^Now\_\`date +\\"%I:%M %p\\"\`\_g" 09:04 AM Wayvee x GT Vienna - No doubt 02:44 PM Haterade - Big Ben (Jorgen Odegard Remix) 02:41 PM Ape Drums feat. Gappy Ranks - Baddest 02:37 PM Sober Rob &amp; Oshi - Lost 02:34 PM LOUDPVCK - CHIRP 02:31 PM Evoke &amp; PUSHER - TakeU 02:27 PM Ian Munro - Bout That 02:22 PM J. Chris &amp; srsly - Fanatic &amp;#x200B; &amp;#x200B; I don't know what timezone this is in, but it's not local. You'll have to set that before running that command. &amp;#x200B; The server side only gives a few songs - so you'll have to poll.
You may be interested in [autojump](https://github.com/wting/autojump).
WOW thank you
As promised - the line I added to my .bashrc is: &amp;#x200B; `env | grep DBUS | cut -d '=' -f 2- &gt; /tmp/dba` &amp;#x200B; And then the way I set up my cron job is: &amp;#x200B; `*/5 * * * * username DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=$(cat /tmp/dba) /scriptlocation/hews.sh`
I'd grep -Eo '^[a-f0-9]{32}'
Base2 vs base10? 2^10=1024 vs 10^3=1000? Quick Google returns 880GB (gigabytes) equal to 819.564 GiB (gibibytes) https://en.m.wikipedia.org/wiki/Binary_prefix
Desktop link: https://en.wikipedia.org/wiki/Binary_prefix *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^269861. [^^Found ^^a ^^bug?](https://reddit.com/message/compose/?to=swim1929&amp;subject=Bug&amp;message=https://reddit.com/r/bash/comments/cg1pxn/disk_free_space_script_how_do_i_change_from_free/eugjb6s/)
**Binary prefix** A binary prefix is a unit prefix for multiples of units in data processing, data transmission, and digital information, notably the bit and the byte, to indicate multiplication by a power of 2. The computer industry has historically used the units kilobyte, megabyte, and gigabyte, and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, gigabyte customarily means 1073741824 bytes. As this is a power of 1024, and 1024 is a power of two (210), this usage is referred to as a binary measurement. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
If it's hanging, look into your script with strace, and see what it was/is doing when it hangs. `strace myscript.sh` When I'm testing things, I'll place `print()` statements before and after an action, so I'll know how far something succeeds; "starting action x" and "finished action x", for example. I'm currently on mobile, and can't create my own test, but definitely start with strace.
&gt; The script just hangs at the moment. Just at a first glance, your script removes the source .wav files after one read, so repeated tests are bound to fail. There's no reason to delete the source files until the script works as it should. To test this script, put a bunch of printing lines into it so you can tell what's going on. Also, learn to use indentation so the structure of the program is easier to follow. Edit: There's an error in this line: file=`basename "$i" .wav` If the script is executed in the same directory as the .wav files, It should be: file=${i/.wav}
 mail ... &lt;&lt;EOF We are the robots. EOF Is mail waiting for input ?
It could be - it does pause and just wait - I'll experiment. thanks
&gt; mail -s "Lenny" -a $file.mp3 me@domain.tld You forgot to quote a variable. Any spaces in `$file` kill your script.
Well, the command actually works for me, but to show remaining Gb I need to show 4th position. Might be difference in df version? $ df -BG / | awk '{ print $4 }' | tail -1 15G $ df --version df (GNU coreutils) 8.31
You can also pipe it through `awk`: `$(md5sum FileX | awk '{print $1}')`
Exactly what /u/masteryod said. From the `df` man page: `-h, --human-readable print sizes in powers of 1024 (e.g., 1023M) -H, --si print sizes in powers of 1000 (e.g., 1.1G)`