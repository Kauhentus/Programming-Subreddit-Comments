Seems that you will do a polling to check the age of the file. A callback via inotify may be more resource friendly.
I removed the sleeps and `rm -rf /tmp/*` as well as `rm /var/lib/apt/lists/* -vf`. To check error I was thinking of setting up `set -e` as described in this answer. Am I correct in assuming that if the script is not interrupted prior to `/usr/bin/dpkg -l | grep '^rc' | awk '{print $2}' | xargs /usr/bin/apt --yes --assume-yes purge` then it is safe to stay in the script? I am thinking of removing this elephant as well. What are your thoughts? Thanks you.
Not really, I’m not a bash or Unix scripting expert. I’ll leave that part to experts out here. I just don’t try to automate updates and upgrades and leave those outside of my automation scripts. 
TBH I would just remove it. You don't need to be running that sort of maintenance on every upgrade, and you should really be paying attention to what it does. Maybe you could split these scripts out into two scripts, one for upgrading, and one for periodic maintenance? 
&gt;grep '\^rc' | awk '{print $2}' Most instances of `grep | awk` can be merged into a single `awk`e.g. `awk '/searchterm/{print $2}'`
 ~ $ ./isOlder.bash testFile 60 testFile is older than 60 seconds &amp;#x200B;
Use jq(1) for json manipulation. sed and Co. aren't the right tools for the job.
 sed -z 's/}\n[[:space:]]*\]\n[[:space:]]*\[\n\([[:space:]]*\){/},\n\1{/' json.txt
Presumably you would want to daemonize this script. Instead of catting, comparing, and sleeping on every iteration it would be slightly more efficient (and provide a faster response) to monitor those files for changes: while inotifywait -m -e modify /sys/class/power_supply/BAT0/capacity /sys/class/power_supply/AC/online; do $DO_YOUR_STUFF done 
now its saying sed: illegal option -- z Isnt it supposed to be -s?
I see. I haven't used -z much. Apparently it's not ported. Try this then sed '/}/ { N; N; N; s/}\n[[:space:]]*\]\n[[:space:]]*\[\n\([[:space:]]*\){/},\n\1{/ }' stories0-5k.json
The problem with this though, is that if the file has patterns like } } ] [ { It'll try to match the first `}` and the `} ] [ {` pattern is missed.
Now its just an EOF error..... sed: 1: "/\x7d/ { :more; N; s/\x ...": unexpected EOF (pending }'s) 
Yeah, i doublechecked and copied the full string from the command.
man printf
Thanks, the man page wan't very helpful but in the bottom it was the link to the [full documentation](https://www.gnu.org/software/coreutils/manual/html_node/printf-invocation.html#printf-invocation) which lead me to the right direction. I managed to solve it with the following line, for i in {0..2072}; do cp frame.jpg `printf 'frame%.6d.jpg' $i`; done
Thank you, that's even simpler that using `printf`.
 #!/bin/bash for i in {0..2071}; do i=00000$i cp frame.jpg frame${i: -6}.jpg done or for i in {0..2071}; do i=00000$i; cp frame.jpg frame${i: -6}.jpg; done 
That {..} feature of bash can do leading zeroes for you, for example: $ echo {0..3} 0 1 2 3 $ echo {000..003} 000 001 002 003 Creating your copies at the bash prompt could then be done like this: for x in {000000..002071}; do cp frame.jpg "$x"; done I like to use a tool named "parallel" for these kinds of problems, just because the way its command line looks is neat: parallel cp frame.jpg {} ::: {000000..002071}
I really like the `parallel` approach, thanks for the suggestion.
I like `parallel` too, but have found that `xargs -P0` seems to be faster at least in some cases.
You don't need command substitution here, otherwise it should execute. You can use bash's `-x` (e.g. `bash -x scriptname.sh`) option to see what your script is doing, and if it's getting hung up somewhere.
Thank you. Once I removed the back ticks it worked great. 
Great! Thanks for the follow-up.
Backticks is not really recommended anymore. Use the equivalent $(..) instead. This notation also makes much more clear what actually is done, making a variable by executing a command. 
I messed around a little, using the other code, and figured out how to do it.
But i really appreciate the help.
Cool! Glad you got it working :D Also glad I didn't spent more than 2 mins on this, haha :)
I'm also glad you didn't spend more than 2 minutes on it either. I'm going to edit my post and make sure other people don't spend time on it unless they really want to.
Bash would be the easy part here - the hard part would be figuring out how to control the power subsystem (if you even can). What kind of laptop, and what OS?
I recommend training yourself to always use $() instead of ``. Looks cleaner and can handle nested command substitutions.
Why would you want to stop at 70%? Most battery types work better when fully charged. 
Absolutely this. Reduce the number of pipes wherever you can.
https://batteryuniversity.com/learn/article/how_to_prolong_lithium_based_batteries Its best not to fully charge batteries for longer life. Its also best not to use fast-charging if you are not in a hurry. We are talking about laptops here, but I do same thing for my mobile phone. Only charge till 85%, and if I am at home, use regular charger instead of fast-charger. Low heat = more life.
What code have you written so far? Or at the very least, have you got any ideas about how to approach this?
The `pv` command may help.
No I don't use any app for that. Simply use regular charger at home. and I have an automate flow that audibly alerts me that my phone is charged to 85%. It only stops playing the sound when I disconnect the charger. I make sure never to charger overnight.
You are limited by the firmware. If you have one of these Lenevo models ([1](http://www.thinkwiki.org/wiki/Tp_smapi#Model-specific_status), [2](https://github.com/teleshoes/tpacpi-bat/wiki/Supported-Hardware)) you can control the battery threshold via either [tp_smapi](http://www.thinkwiki.org/wiki/Tp_smapi) or [tpacpi-bat](https://github.com/teleshoes/tpacpi-bat/wiki/Supported-Hardware). Otherwise you will have to look for some flavor of "battery saver" in your BIOS. 
Try zenity. 
Not 100% ideal, but you could try [blessed-contrib](https://github.com/yaronn/blessed-contrib). It extends an existing library called `blessed`. It's Node/JS based. I've used it and it's not too bad to learn. But I already knew an okay amount of JS. If you've never touched it you might have a steeper learning curve.
Ok, it's at least semi automated, not bad. 
For bash you are limited to something like Kdialog or Zenity, both of which are really only useful for dialog boxes. If you are creative you can stretch them a bit further, but not enough for a firewall GUI. Python is easy to learn and has several GUI options. Read the first page or two of [this tutorial](https://likegeeks.com/python-gui-examples-tkinter-tutorial/) and see if it's a good fit for you. 
Yeah, this is more of what I am thinking. Can Python, for example, pass user input from a textbox or dropdown or something as a parameter for an IPTables command? Simpler said, can I create a python package that can utilize the full scope of a firewall package like IPTables or UFW?
Yup. Python is the #2 Linux scripting language* ^^^*I ^^^don't ^^^have ^^^any ^^^actual ^^^stats ^^^on ^^^this. ^^^I ^^^just ^^^know ^^^it's ^^^used ^^^a ^^^lot.
Simple GUI: Use ncurses
&gt; Can Python, for example, pass user input from a textbox or dropdown or something as a parameter for an IPTables command? `os.system` and `subprocess` can do that, yes. 
I'd look through this list and grab what you think is best. https://wiki.python.org/moin/GuiProgramming I'd go with whatever the most stable xserver based framework is. 
i like to bash Atlas Shrugged a lot. But honestly - why do you want a book? There's a tremendous amount of online resources to pick from. If you're looking at the bare basics.. maybe here: https://ryanstutorials.net/bash-scripting-tutorial/ There's also this: https://www.bash.academy/
Thanks!
This assumes that the columns are delimited via a whitespace: #!/bin/bash list="list.txt" frequency=$(grep $1 $list | sed 's/[^ ]* //') while read -r name; do name=$(echo $name | sed 's/\s.*$//') names+=("$name") done &lt; &lt;(grep $frequency $list) printf '%s\n' "${names[@]}" | sort Define the list list="list.txt" Search for the frequency associated to the name, trim off the name as it is not needed yet frequency=$(grep $1 $list | sed 's/[^ ]* //') Read each line of the list while read -r name; do Trim off the frequency as it is no longer needed name=$(echo $name | sed 's/\s.*$//') Add the name to the array for sorting and printing later names+=("$name") This is a little confusing. We are substituting, essentially post hoc, with grep to find all matching frequencies and associated names. done &lt; &lt;(grep $frequency $list) Print and sort each element onto its own line printf '%s\n' "${names[@]}" | sort You could also solve it by reading your list into an associative array but that might be even messier.
&gt; simple
Thanks for you feedback. This is really helpful. I have a few questions. How would you use curl? Where would you store the result? How long would you suggest the sleep be? What does readarray do?
&gt; How would you use curl? Someone else can answer this better than I. Just keep using lynx until you get a chance to read up on curl. Where would you store the result? What does readarray do? readarray is storing the result in an array named PAGE_INFO. echo ${PAGE_INFO[0]} will print the first line echo ${PAGE_INFO[1]} will print the second line echo ${PAGE_INFO[2]} will print the third line and so on.. &gt; How long would you suggest the sleep be? That depends on how up-to-date you want your information. By the minute? Set it to 60. Every 10 minutes? Set it to 600. I would set it as large as you feel sensible to wait, but wouldn't go less than 1 second. &gt; Can you explain what the regular expressions you wrote mean? *"final"* matches any string that contains the word final. Nothing else is a regular expression. I always surround my variables in ${} as I use them, so `echo $team` is equivalent to `echo ${team}`. Where I use `${SPIN[${SPIN_IDX}]}` that is simply indexing the SPIN array with `${SPIN[0]}`, ${SPIN[1]}`, ${SPIN[2]}` or ${SPIN[3]}`. Another thing that may look odd, but can be skipped if it's too confusing for now its printing `\033[2A`. That is what allows me to over-write a previous line instead of creating a new line each time. 
How does \033[2A work?
I actually earmarked where I learned how to use that. https://stackoverflow.com/a/44079727
`awk` can do this. I'm just doing this as an exercise as I'd like to learn awk better, so maybe there's a cleaner way or I'm using an anti-pattern or something, but here's what I came up with: sed -e 's/./:/' playlist | awk -F ':' '{hours += $1; minutes += $2; seconds += $3; cent += $4} END { seconds += int(cent/100); cent %= 100; minutes += int(seconds/60); seconds %= 60; hours += int(minutes/60); minutes %= 60; print "Total runtime: "hours" hours, "minutes" minutes, "seconds"."cent" seconds."}' Total runtime: 6 hours, 2 minutes, 52.14 seconds. &amp;#x200B; Assume the playlist is saved as a file conveniently called "playlist". The sed portion converts the decimal after the second to a colon -- this is just so all the fields are split on the same delimiter. This output is piped to awk and split on the colon. We then aggregate the sum of each column ($1, $2, $3, $4), assigning them to the respective values hours, minutes, seconds, and cent. Once the loop body gathering the sums is complete, we then do some modular arithmetic and reassignment, so that for example 1414 hundredths of a second in column $4 is converted to 14 hundredths of a second in column $4, and 14 seconds gets added to the previous sum in column $3. We continue working backwards to minutes and hours until all the time values are normalized. Then it all gets printed out. &amp;#x200B; Again, there's probably other, better ways to do this -- but I hope this attempt made sense to you.
Looks good and useful. Thank you for the hint :) And luckily I'm using linux
You could do this in pure Bash with something like: sum_durations() { local -i hours=0 mins=0 secs=0 csecs=0 local -r rx='([0-9]+):([0-9]+):([0-9]+).([0-9]{2})' local -r LANG=C while read -r; do [[ $REPLY =~ $rx ]] || continue hours+=10#${BASH_REMATCH[1]} mins+=10#${BASH_REMATCH[2]} secs+=10#${BASH_REMATCH[3]} csecs+=10#${BASH_REMATCH[4]} done secs+=$(( csecs / 100 )) csecs=$(( csecs % 100 )) mins+=$(( secs / 60 )) secs=$(( secs % 60 )) hours+=$(( mins / 60 )) mins=$(( mins % 60 )) printf '%02d:%02d:%02d.%02d\n' "$hours" "$mins" "$secs" "$csecs" } Reads from standard input, writes the total duration to standard output. The output uses the same HH:MM:SS.CC formattng as the input, but it would be straight-forward to change it to something else if required.
I'd probably suggest go web based. Pop a graphviz bit of code in there to display chain info depending on how far you want to go. 
Both solutions work beautifully, thank you! Now to figure out which one works for this application. :) Thanks again!
Did mine without looking at the previous replies and we did it in a very similar way yours beingeasier to understand: &amp;#x200B; sed "s/\\./:/" playlist | awk 'BEGIN {FS=":";sumcs=0}{sumcs+=$1\*60\*60\*100+$2\*60\*100+$3\*100+$4} END{print int(sumcs/360000)" hours "int(sumcs%360000/6000) " minutes " int(sumcs%6000/100) " seconds " sumcs%100 " centi seconds"}' 6 hours 2 minutes 52 seconds 14 centi seconds
Do you have the Express written consent of Major League Baseball :-D
Meh...Bash Academy is a noble idea much of which is not backed up with content: [https://guide.bash.academy/loops/](https://guide.bash.academy/loops/) The Ryans Tutorial thing shows more promise.
Am I missing something? Just go one level higher? cp -r foo/{bar,baz} out or cp -r foo/* out 
foo/bar could have multiple subfolders that Op doesn't want to copy, only /1
Try rsync? `rsync -r foo/bar out/` You might need to use `--exclude` and/or `--include` patterns if there are folders in bar/ you *don't* want to copy.
In rsync, you can include subfolders by fully quoting them rsync -a /foo/'bar/1 baz/2' /out Should work. @OP, check the man page, there's a literal example of this in there.
I don't think you have to quote them, just specify the directory and use shell expansion. This works for me: rsync -a foo/{bar/1,baz/2} out
 $ cd foo $ cp -r --parents ./{bar/1,baz/2} ../out $ tree ../out ../out ├── bar │ └── 1 └── baz └── 2 
use a variable?
I don't think it is possible, you probably won't get around assigning the individual steps to a variable result=${abc##def};result=${result%ghi}
Thanks for this suggestion. I didn't know about \`$BASH\_REMATCH\` However I think just reassigning the parameters will be easier to understand.
I guess this is still the best way to do it. Thank you.
You are going to need some kind of static (persistent) file or database. Try looking into CSV, SQLite, or JSON. Not sure which method will suit you best.
&gt;rsync -a foo/{bar/1,baz/2} out It gives me the same result: ```bash $ tree out out ├── 1 └── 2 ```
&gt; foo/bar could have multiple subfolders that Op doesn't want to copy, only /1 Sure, there are a lot of other files in foo, foo/bar, foo/baz that I don't want to copy.
I think that's what I need (--parents). Thx.
I’m afk &amp; on mobile, so can’t test, but would you be able to change the cut statements from &lt;() to $()? I’m not very good with redirection, but I think that’s where your problem is 
Also, if you still want to exit the script immediately when the diff fails for a reason besides the files being different, you could mask only the exit code 1: diff oldfile newfile || [[ $? == 1 ]] &amp;&amp; true That way if diff returns 1 or 2 (or anything but 0), it checks if the exit code was 1, and if so it runs `true` so that bash will see the exit code for the whole line as 0 (success) and not bail out due to `set -e`.
This did it for me. Thanks for your help!
I tried what u/MurkyCola said and it worked for me, but thanks for the help!
Oh nice, yeah his answer makes sense. Glad it worked!
I'm missing something -- in the example, isn't this also nested parameter expansion? : "${1#"${1%%[![:space:]]*}"}" 
Good point. Another comment on this: https://stackoverflow.com/questions/917260/can-var-parameter-expansion-expressions-be-nested-in-bash#comment19696783_917313
Or put the script somewhere and add that somewhere to your path. 
Or hardcode the filename into an if-else statement
I changed my awk command and it seems to work : #!/bin/bash #Script to print odd lines of every file in current directory awk 'FNR%2==1 {print FILENAME, ":", $0}' *.* I changed the file destination to *.* rather than just * and it skips the script file for some reason unknown to me. Could you explain why? 
if you are on newer bash (I think version 4+), try this: shopt -s extglob awk 'FNR%2==1 {print FILENAME, ":", $0}' !($(basename $0)) by using extended glob, you can specify a particular file or particular glob to be excluded.. basename is needed to remove the `./` from script name
`*.*` means any filename that has at least one `.` in it `*` is a placeholder (wildcard) to match any character 0 or more times
So is it an error that *.* does not return the script file contents? Should that file have a .sh extension? 
oh yeah, I flipped my conclusion.. `*.*` means all other files have `.` in it while your bash script filename didn't.. using `.sh` is conventional, not necessary
Based on your comments, I removed that line. It does not meet the simplistic maintenance spirit I envisioned.
I removed that line all together. The aim of the script is to be a simple maintenance one. That line of code as well explained below was more of a trigger to disaster.
Maybe I'm missing something, but servers that are truly shutdown or offline from the perspective of the system you're checking from won't be connected to the system -- although there may be a TIME\_WAIT value in the State field of the output of \`netstat -n\`, there would be no way to positively affirm (that I can think of offhand) if that system was actually shutdown or simply voluntarily disconnected for whatever reason. There are ways to monitor state of a remote system -- using something like Nagios or snmp polling or using netcat to pipe some numeric status to graphite -- but they aren't trivial. I'm open to corrections if someone thinks of something else here. &amp;#x200B; It's also unclear which orientation the ssh connections you're referring to are. Although in general a client connects to an ssh server on port 22, the client port may be something else entirely. For example: $ netstat -n Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 [8.9.10.11:60357](https://64.94.197.249:60357)[199.200.201.202:22](https://198.58.41.100:22)TIME\_WAIT tcp 0 0 [8.9.10.11:22](https://64.94.197.249:22)[10.11.12.13:49387](https://10.11.2.70:49387)ESTABLISHED &amp;#x200B; This output shows an expired connection from our local system [8.9.10.11](https://8.9.10.11) to the remote system [199.200.201.202](https://199.200.201.202), and a connection from a remote system [10.11.12.13](https://10.11.12.13) into our local system [8.9.10.11](https://8.9.10.11). The :22 at the end indicates which endpoint is acting as the server in the connection. Note that our system can be both a client connected to other servers over ssh, or a server allowing other clients to connect to it. Note also that an entirely different, arbitrary port number other than 22 can be used for ssh connections on the server. &amp;#x200B; With all this in mind, if we assume that we are using the standard port 22, and we only care about clients connected to our server (rather than any servers our client might be connected to), we can at least come up with a way to show actively connected clients using netstat and massaging the output with awk and some other filter tools: sudo netstat -n | awk '($4 \~ /:22$/) &amp;&amp; ($6 \~ /ESTABLISHED/) {print $5}' | awk -F: '{print $1}' | sort | uniq &amp;#x200B; I'll break this down for you. First, we grab the output of netstat and pipe it to awk. The stuff inside the first awk checks for ':22' at the end of the fourth field -- which is our local address -- and that the sixth field is "ESTABLISHED". If both of these things are true, this means that the remote client is currently connected to our server on port 22 -- so we print the fifth field, the foreign address, and then pipe that to another awk that chops off the remote port number, before finally sorting and uniqifying the output to get an ordered list of all clients currently connected to our server. &amp;#x200B; Hope this helps. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Agreed. 
...ping?
About the extension `.sh`, the Google guidelines (that being what it is, not strict rules, just some guidelines) recommend using `.sh` to denotates a library file and no extension for executables. https://google.github.io/styleguide/shell.xml
That is quite useful. I didn't know that about the "ls" command. If I get a few extra numbers it should should help provide space for my PHP-FPM calcs, so I can live with that. We have about 150 employee accts per server, so if I jettison those, I've won half the battle. &amp;#x200B; I'll play with this snippet and see what gold I get. Thanks! That puts me ahead of the game.
So I would like to have the .sh extension on the file. However I need to exclude only the .sh that calls the awk command. Is this even possible within a single awk command? Can I do this with a simple if conditional?
I read footjob. 
OK I hope this answer gets indexed on the web because I could not find a single example of this ( which is pretty wack ). I found out through guessing that you can connect patterns in a awk command with &amp;&amp;. IDK why it took me so long to find this. I looked for just about everything dealing with multiple patterns and exclusions and this never came up. All I needed to do was add this to the pattern' &amp;&amp; FILENAME !=scriptname
LOL. I'm not sure that's a fetish I want to know about.
find /home | grep -E -v "/(foo|somerootproc)" &gt;&gt; logfile
&gt; Linux usually denotes non-system accounts with a UID over a certain number (1000) You might find this snippet of code useful: # Figure out the lowest boundary for the available UID range uidMin=$(awk '/^UID_MIN/{print $2}' /etc/login.defs) # Older releases of various Linux distros tended to use '500' as the minimum # So if we can't find it in login.defs, we'll default to '500' uidMin="${uidMin:-500}" In the RedHat world, 1000 has been a thing for about 7 or 8 years. Up to RHEL6 / Fedora 15 era it was 500. Debian for as long as I can remember has used 1000. 500 is a boundary found outside Linux as well e.g. OSX.
so, I can't seem to bind dot (.) in command-mode at all, so here is what monstrosity I came up with using the colon bind (:). There may be issues with it (be warned!) set keymap vi-command ":":"i\e.\eBhx\eOF" set keymap vi-insert ... to break it down: enter insert mode and then use the `yank-last-arg` keybind (\e.), then Back one WORD in command-mode (\eB), left once (h) and then go to end of line (\eOF).
You created that "dims()" function, but you are not using it anywhere. Your script is executing just this one line here: tempDim="tempdimfile"
Okay. So I added: input="$1" before the function and dims "$input" after the function I am wondering though. I have other functions to process within this script. Do I treat each one like this? Write a function then at the end of the script set the function name equal to input?
Awesome. Thanks! 
useless use of cat
This will give you whatever column you want df -h | grep /root/ | awk '{print $2}'. I'm not 100% sure how to display everything after. With the above you can specify numerous columns. $2,$3,etc
To grep after a match. `grep -A 1 "patten"` To grep before a match. `grep -A 1 "patten"` But if want a certain field or column, then use `awk '{print $x}'` where x is the column number. 
For percentage used by root: df -h | awk '$6 ~ /^\/$/ {print $5}' For amount of used space substitute $3 for $5
By the way you've worded this, I'm guessing just taking both files and shuffling all lines together isn't what you want. You want to keep the order of `build.txt`'s lines? The way I'd do this is to load all of `build.txt`'s lines into an array, _then_ loop through `blib.txt`, adding its lines at random positions within this array. Once you've done all that, write out the array.
You guess correctly! I can add lines in random locations to build.txt, but the lines that are already there must stay in that order for scheduling. Now if you were to guess that I have absolutely no idea how to use arrays, you would get a gold star! Sleep deprivation is a horrible thing, and being a week out from launching a tv channel means I have plenty of sleep deprivation. :)
That works beautifully! Thank you! Now to go grab 2 hours of sleep while the encoder job finishes up... I'm whiny, but so excited!
You are correct in general terms, in this particular assignment they wanted us to do it a certain way - but your suggestion is the "correct" way
I used something like that with some modifications for the exact desired output. Thank you!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
All commands within a pipeline are run within individual subshells. When variables are assigned within a subshell, they do not change anything in the parent shell. You should rewrite the loop to avoid a pipline, e.g.: while ...; do ... done &lt;~/my-hosts.txt
It should be up=$((up + 1)) And #!/usr/bin/env bash Also, IFS= read -r line; do ... ... done &lt; ~/my-hosts.txt as u/aioeu suggested
how else would you do it? I didn't think I could use read because its un-pipe-able.
In the comments it suggests to grep the command from above and the instructor is very specific and wants us the follow the instructions carefully. 
Tried erm shellcheck.net?
And in addition, the ssh command needs the `-n` option, otherwise it will eat the remaining lines of the input file. ssh -n "guest@$line" true 2&gt;/dev/null Alternatively redirect stdin instead ssh "guest@$line" true &lt;/dev/null 2&gt;/dev/null
A minor bug in there; `local` used outside a function is an error.
Gah, you're right. I had originally written this as a shell function, but I converted it to a flat script. Missed that one.
 wc -l $1 head -n 1 $1 |wc -w
My only guess would be that your "match before" and "match after" commands are exactly the same. That and people suck
Perfect, thank you. I was looking for something like this. 
'+' has a special meaning in the regex used by sed which may be causing you problems. What happens if you use '|' (pipe) as your delimiter instead? p.s. for multi-line code formatting, add 4 spaces to the front of the line and it's easier to read :)
&gt;| Thanks for helping! shfmt is doing the formatting for me :) Here is an exact copy of the script I just ran, and the error: sed -i 's|{ "test": "echo "Error: no test specified" &amp;&amp; exit 1" }|{ "global-install": "pnpm install -g mocha jsdom jsdom-global", "test": "mocha --reporter min --require esm --require jsdom-global/register -b", "test-watch": "mocha --watch --reporter min --require esm --require jsdom-global/register -b", "build": "rollup --format=iife --file=dist/bundle.js -- src/main.js &amp;&amp; purgecss --css src/main.css --content index.html src/\*\*/\*.js --out dist" }|' ./package.json sed: -e expression #1, char 3: unterminated \`s' command
I tried it on both Mac and CentOS. CentOS had no issue and Mac wanted me to escape the braces in the first half, but worked once I did that. What OS are you testing on and do you mind sharing your package.json file?
Several problems there. Firstly, sed really really is the wrong tool for editing json. Use jq, node or python instead: jq '.scripts = { "global-install": "pnpm install -g mocha jsdom jsdom-global", "test": "mocha --reporter min --require esm --require jsdom-global/register -b", "test-watch": "mocha --watch --reporter min --require esm --require jsdom-global/register -b", "build": "rollup --format=iife --file=dist/bundle.js -- src/main.js &amp;&amp; purgecss --css src/main.css --content index.html src/**/*.js --out dist" }' package.json jq is the easiest, but surely at least you can expect node to be available, right? As for the sed, newlines end sed commands, so you have to escape them with `\` if you want them to be part of the regex/replacement. Further, since sed runs the script on line by line, there will not be any newlines to match unless you put together multiple lines in pattern space, e.g. by using the N command. In the replacement part of the s command, &amp; is special and must be escaped in your case (`\&amp;`).
`+` is not special to sed's regex, so that's not the problem.
That is weird! My package.json is the default created by npm init -y. my sed is "sed (GNU sed) 4.5" I have taken a screenshot of how it looks in my editor, but reddit does not seem to have image attachments. it's only 31kb, would that be of any use?
You could use `jq` to merge new objects into the existing structure, like so: jq '. * {"scripts":{"global-install":"pnpm install -g mocha jsdom jsdom-global","test": "mocha --reporter min --require esm --require jsdom-global/register -b","test-watch": "mocha --watch --reporter min --require esm --require jsdom-global/register -b","build": "rollup --format=iife --file=dist/bundle.js -- src/main.js &amp;&amp; purgecss --css src/main.css --content index.html src/**/*.js --out dist"}}' Inside the quoted first argument to jq the dot `.` means that we're using the root of the data-structure in package.json (instead of .scripts, for instance). The asterisk `*` means that the following json object is to be merged recursively. The new object is seen from the same root-level scope as the object in package.json (i.e. it starts at `{"scripts":...}`). However there's no way (that I know of) to edit the json file in place, like `sed -i` would do. So you'd have to replace the file either on the fly (dangerous) or by directing the output an intermediate file first.
THANKS so much, the perfect solution :)
Whoops, I must have opened the tab before geirha gave their answer. But I wouldn't use the `.scripts=` solution because it will just replace the entire `"scripts"` object. If you want to keep potential existing properties inside `"scripts"` in place (if they aren't supposed to be overwritten), you'd probably want to go with the recursive merge using the `*` operator.
You can write a Node script. The shebang would be `#!/usr/bin/env node`. Then just write JS (node) as you'd expect, load and modify the file.
On all systems i am familiar with, Any user by default can add a cron to run under there user account.
There's various versions of Cron, so I'm going to assume you're using Vixie Cron. Vixie Cron provides access control through the `/etc/cron.allow` and `/etc/cron.deny` files. With these, you can restrict which users can use [`crontab`](http://man7.org/linux/man-pages/man1/crontab.1.html) to edit their cron tables. See that link for details.
 normally each user has their own crontab. crond runs anything in each users crontab, under that users permission. But the admin can restrict this, if desired. 
"These arrays" you say, but your example contains no arrays. There are two objects, separated by , so I'm guessing the top level is an array, and these two objects are elements of it: jq '.[] | select(.name == "10") | .id'
Oh shoot, forgot to include the array brackets. I'll edit it now. 
Do not use unquoted parameter expansions to split strings. Use read to split: IFS=', ' read -p 'Enter group names separated by commas: ' -ra groups for group in "${groups[@]}"; do groupadd -f "$group" done
Thanks for pointing out that typo. Much appreciate. I agree, some people are just online to troll. 
You can do this with either `su` or `sudo`.
I was trying that before, my SITE\_OWNER variable was wrong so Ive changed it to just su - ${SITE} now which sort of works but it changes to the shell of the user then which I dont want it to do, I want the script to continue but running as that user
From the su man page -c, --command=command Pass command to the shell with the -c option.
Use `sudo -u &lt;user&gt; &lt;command&gt;`
You can't "switch" to another user so that the script continues as that new user. You can only run something AS a user. That something can be a script. So if you make two scripts, that should do it. One that runs as root so that it is able to sudo to other users, and one that is meant to be run as a user. If you *really* need them to be the same script, you could have the script check the username/uid at the top and branch. If it was root, make the script call itself with sudo.
There are other getopt(1) helpers out there. Here's mine: [http://bhepple.com/doku/doku.php?id=argp.sh](http://bhepple.com/doku/doku.php?id=argp.sh) 
 #!/usr/bin/env bash # run as current user echo "Running as ${USER}" # run as another user - NOTE: you have to escape the '$' sudo -u nobody bash&lt;&lt;EOF echo "Now running as \${USER}" EOF 
You can do it with xargs too: #!/bin/sh -e find . -name '\*.flac' -print0 | xargs -0 -n1 sh -c ' INFILE="$1" META=$( metaflac --show-tag=REPLAYGAIN_TRACK_GAIN "$INFILE" ) if [ -z "$META" ] ; then metaflac --add-replay-gain "$INFILE" fi' '' I know this is the bash subreddit, but there's nothing in here that has to be done in bash so the specific shell doesn't matter as long as it's posix compliant.
Take a look at this and search for the word subshell: https://gist.github.com/korya/bd0de47f36dbb5c29a90 I haven't tested it, but I think you can control it by choosing the relevant configuration file.
I actually fixed it... I created a variable in my if statement, and then I exported it as an env variable so a subshell had access to it. Here it is: `if [[ $amisubshellin -eq 0 ]]; then if [ $SHLVL -eq 1 -o $SHLVL -eq 3 ]; then neofetch amisubshellin=1 export amisubshellin fi fi `
An alternate would be to use the bash `caller` function. getCallerName() { echo "$(caller 1 | sed -E s/'-?[0-9]{1,9} ([^ ]*) .*$'/'\1'/) --&gt; $(caller 0 | sed -E s/'-?[0-9]{1,9} ([^ ]*) .*$'/'\1 '/)" } This will return `$callerFunctionName --&gt; $currentFucntionName`, and is either/both of these is the main shell the name will be blank. i.e., calling it from the main shell gives ` --&gt; `, calling it from a function directly called by the main shell gives ` --&gt; funcName1`, and calling it from a sub-function called by a function rather than from the main shell gives `funcName1 --&gt; funcName2`. Running `testCallerName` will exemplify this usage. testCallerName() { echo "function depth = 1" echo "getCallerName: $(getCallerName)" testCallerName1 } testCallerName1() { echo "function depth = 2" echo "getCallerName: $(getCallerName)" } 
thanks this works
&gt; An alternate would be to use the bash caller function. Using the `$FUNCNAME` array would be quite a bit simpler than that.
You need double-quotes around the `$(...)` to ensure that the command expansion does not itself undergo word-splitting. You may think that would mean the quotes around `${pri}` would screw that up... but everything actually nests correctly. Shell syntax is parsed recursively
This here works: echo -e "${CLABEL}Ultimate FS layer: ${CLEAR}$(stripDir "${pri}")" When you have these kinds of problems in bash, you can also do things like this here: "aaa""bbb""ccc" Bash will fuse those three parts together into a single word: aaabbbccc You can also mix `'` and `"`, for example: 'aaa'"bbb"'ccc"
Have a look at shflags, That's what I use to parse opts these days: https://github.com/kward/shflags
&gt;You need double-quotes around the $(...) to ensure that the command expansion does not itself undergo word-splitting Err... that doesn't work either because you would end up with a double closing quote and thus it triggers : *The surrounding quotes actually unquote this. Remove or escape them.* &amp;#x200B; `echo -e "${CLABEL}Ultimate FS layer: ${CLEAR}$(stripDir "${pri}")"` seems to work &amp;#x200B;
 &gt;Err... that doesn't work either because you would end up with a double closing quote Um, I meant: "...""$(....)" Not: "..."$(....)" But you solved it the other way I suggested.
This might be helpful r/https://link.nickshelp.info/FpYAk 
 man chmod man chown 
I know it would go against the nature of this sub, but seeing this kind of thing really makes me appreciate stackoverflow's "No homework questions" rule. 
Uh, which bash is showing a different path to bash than the one I set as my default shell. I have no idea what this means.
Just use jq to parse json jq -r '.[].id' Example: $ printf '%s\n' '[{"id":1,"name":"[Archive to Delete] CopyPath"},{"id":2,"name":"[Archive to Delete] CopyPath"},{"id":3,"name":"[Archive to Delete] CopyPath"},{"id":25,"name":"[Archive to Delete] CopyPath"}]' | jq '.[].id' 1 2 3 25
Make sure you put /usr/local/Cellar/bash/4.4.23/bin first in PATH, or at least before the directory where the "wrong" bash executable resides. 
I recommend using a json parser as well, but if you are not in the position to use one. Here is a way to do it with gawk. (NB FPAT is a feature only available in GNU AWK) gawk -v FPAT='"id":[^,]+,' '{for(i=1;i&lt;=NF;i++){sub("^\"id\":","",$i); print substr($i,1,length($i)-1) }}' file 
&gt; then changed the default shell to the new directory located at What does "changed the default shell" mean exactly?
I did add it to PATH, but it is not before the path to the other directory. I will try this.
In the Terminal Preferences GUI, there's a section for "Shells open with" Command - Complete path. I changed that to the aforementioned path.
Ok, what's the output of this: type -a; echo "$BASH_VERSION"; bash -c 'echo "$BASH_VERSION"'
 bash is /usr/local/bin/bash bash is /bin/bash 4.4.23(1)-release 4.4.23(1)-release &amp;#x200B;
Ok, I assume /usr/local/Cellar/bash/4.4.23/bin/bash is the same as /usr/local/bin/bash. Now earlier you said you used `#!/usr/bin/env` as shebang, I assumed you meant `#!/usr/bin/env bash`. Is that the case?
I guess it's that time of the week on /r/bash. Let's get this all out of the way at once: OP: How do I parse this enormous json blob with sed or awk? Commenter: It's super easy with jq. Commenter 2: HOW DARE YOU discourage him from using the tools he wants to use? jq isn't part of any distro's base installs! His script won't be portable! Commenter 3: Here, I made you this 108-character regex. OP: Oh my god jq is exactly what I needed! You have saved me so much work! I wish I knew about this earlier...
&gt; BEFORE YOU GO! Is there anything else that this change might have done that could cause me issues later in life? Not really. There are some changes between 3.2 and 4.4 that may lead to a script behaving differently, but you're unlikely to encounter that, and the bash scripts that shipped with your Mac will be using /bin/bash in the shebang and thus not notice a thing. I do the same on my macbook, though instead of changing the terminal settings, I changed my login shell by 1. Adding the new bash binary to /etc/shells, which allows any user to do step 2: 1. Change my login shell with the `chsh` command The main difference will be if you install a different terminal emulator (such as iterm2), which in your case will use /bin/bash until you configure it as well, while in my case, it uses the newer bash because that's my current login shell.
Here's another possible solution - since I'm relatively new to bash, any feedback is appreciated printf '%s\n' '[{"id":1,"name":"[Archive to Delete] CopyPath"},{"id":2,"name":"[Archive to Delete] CopyPath"},{"id":3,"name":"[Archive to Delete] CopyPath"},{"id":25,"name":"[Archive to Delete] CopyPath"}]' | grep -oE "\"id\":[0-9]+" | cut -d":" -f2 &amp;#x200B; &gt;Commenter 3: Here, I made you this 108-character regex. It will probably work as long as the thousands of other records are structured exactly like the example snippet. Sorry for not making it to 108 characters :/
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Why do you need to elevate to root in order to pull web application source? Your web server should not run as root, nor should the ownership of any of the application scripts. This is also has the potential to be hella dangerous if exec bits are set on web app code files (intentionally or otherwise).
Oh I need to run sudo because I do not have elevated privileges to run "git pull". This server is very secure and it mainly hosts a demo version of the web app so not much is vulnerable.
Oh I need to run sudo because I do not have elevated privileges to run "git pull"(not a personal project). Everything else runs smoothly up until the "sudo git pull" command. 
You could replace ls with find. 
Thanks. This is what I ended up with that worked. #!/bin/bash dirs=("/movies1" "/movies 2" "/movies3") mpv "$(find "${dirs[@]}" | shuf -n1)" exit 0
That's what groups are for - set the group ownership on the live directories to a group that only contains folks who should be able to `git pull` there, and `chmod -R ug=rwX,o=rX` so people outside the group can't change anything. It's less work to manage and easier to keep secure than managing sudoers. If they can `sudo git pull` then they have write access everywhere. They could `git pull` a new /etc/shadow file or web server config. More likely, they'll wind up accidentally pulling to the wrong directory and leaving you with a huge mess. 
That find is including the directories themselves in the output, so there's a chance shuf will pick one of the three directory instead of a file. Anyway, here's a version using bash to pick a file: files=( /movies1/* /movies2/* /movies3/* ) mpv "${files[RANDOM%${#files[@]}]}"
I actually found out that the issue was my shebang, not the new location of Bash. Mac will not release anything later than 3.2.57, so it's very possible you will have compatibility issues with Mac users. I was using [https://apple.stackexchange.com/questions/193411/update-bash-to-version-4-0-on-osx](https://apple.stackexchange.com/questions/193411/update-bash-to-version-4-0-on-osx) as a reference on updating to 4.4, that's how I got the updated version of bash working. It's important that they set the new path as the path to their default shell in the GUI, or else they won't be using the updated version.
You can use either cut or sed for this. I recommend cut, as the syntax is easier for grabbing fields.
thanks, it works for adding groups but how can I now get groups with commas for: useradd -G "group1, group2, group3" ? 
I don't read know what you're trying to say, or if you just want to kill the discussion before it begins..? It is perfectly fine to use specific tools like jq, and it is also pretty commonplace to have certain restraints on the environment, prompting the need for more general purpose solutions using UNIX tools. The former does not exclude the latter.
&gt; I don't read know what you're trying to say, or if you just want to kill the discussion before it begins..? Not kill it, speed it along. This thread happens several times a month and I was plowing us through the boilerplate parts. 
``` #!/bin/bash set -euo pipefail #============================================================================== # Global Constants #============================================================================== readonly GREEN=$(tput setaf 2) readonly RESET=$(tput sgr0) #============================================================================== # Associative array(s) scaffolding templates for JS projects #============================================================================== declare -A vue_template=( [PROD_PACKAGES]="vue" # for npm install [DEV_PACKAGES]="vue-template-compiler" # for npm install --save-dev [SOURCE_DIRECTORY]=/vue-template-files/ # where the files we are going to copy live [BUILD_INSTRUCTIONS]=$( cat &lt;&lt;EOF index.html:. main.js:./src/ app.js:./src/ app-test.js:./test/ :src/components/ EOF ) ) #============================================================================== # Check all the required keys are present in associative array in the argument #============================================================================== function check_keys() { local associative_array=$1 local required_keys=(PROD_PACKAGES DEV_PACKAGES SOURCE_DIRECTORY BUILD_INSTRUCTIONS) for i in "${required_keys[@]}"; do if [[ ! -v $associative_array[$i] ]]; then echo "${i} key is missing in ${GREEN}${associative_array}${RESET}" &amp;&amp; exit 1 fi done } #============================================================================== # Create the files and folders in current directory #============================================================================== function create_files_folders() { local SOURCE_DIRECTORY=$1 local BUILD_INSTRUCTIONS=$2 local DIR_SCRIPT_CONTAINED="$(dirname "$0")" # dir the script is running local file_folder=$DIR_SCRIPT_CONTAINED${SOURCE_DIRECTORY} OLDIFS="$IFS" IFS=$'\n' # test files exist before writing for ROUTE in ${BUILD_INSTRUCTIONS}; do # split the line on the ':' and define the variables as what is on the left and right TPLNAME="${ROUTE%%:*}" TPLPATH="${ROUTE#*:}" # if there is something on the left and right we know it is a file, not just a folder if [[ "$TPLNAME" &amp;&amp; "$TPLPATH" ]]; then if [[ ! -f "$file_folder""$TPLNAME" ]]; then echo &gt;&amp;2 "File not found!${GREEN}""$file_folder""$TPLNAME""${RESET}" &amp;&amp; exit 1 fi fi done # write files for ROUTE in ${BUILD_INSTRUCTIONS}; do TPLNAME="${ROUTE%%:*}" TPLPATH="${ROUTE#*:}" if [[ "$TPLNAME" &amp;&amp; "$TPLPATH" ]]; then mkdir -p "${TPLPATH}" cp "$file_folder""$TPLNAME" "${TPLPATH}" fi # if there was no file reference then its just a folder, so make it mkdir -p "${TPLPATH}" done echo "files copied and folders created OK!" IFS="$OLDIFS" } #============================================================================== # npm install the development and production packages if they exist in template #============================================================================== function npm_install() { local PROD_PACKAGES=$1 local DEV_PACKAGES=$2 if [ -n "${PROD_PACKAGES}" ]; then echo "npm install $PROD_PACKAGES" fi if [ -n "${DEV_PACKAGES}" ]; then echo "npm install --save-dev $DEV_PACKAGES" fi } #============================================================================== # MAIN #============================================================================== function main() { check_keys vue_template create_files_folders "${vue_template[SOURCE_DIRECTORY]}" "${vue_template[BUILD_INSTRUCTIONS]}" npm_install "${vue_template[PROD_PACKAGES]}" "${vue_template[DEV_PACKAGES]}" } main ```
Thanks! Would you mind sharing how you did that? I tried many many things.
or if using vs code [https://marketplace.visualstudio.com/items?itemName=timonwong.shellcheck](https://marketplace.visualstudio.com/items?itemName=timonwong.shellcheck)
I like to [use markdown](https://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6/) when writing anything on reddit. This gives you the option to format your code either in an inline or block format. To format my original comment (your code), you can add 4 spaces before each line **or** use three backticks (\`) before your code and after, [like shown here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#code-and-syntax-highlighting) \- reddit is able to use this style of markdown as well. I simply copied and pasted your code in between two sets of three backticks to have it formatted above. Hope that helps, cheers!
In the `case` statement, the wildcard section `*)` is like the 'else' section of an if-then-else statement. What that means is that the aliases within that section are only evaluated if nothing else matches - With case, you only get one matching segment. What you'd have to do for it to work is to put your 'blah' alias in each of the other blocks you have configured and/or in the wild card section have your 'blah' alias be something like `alias blah="echo 'it did not work!'"`
The \*) statement is only executed if nothing else matches. You should move any aliases that are universal out of the case statement; no need for duplication.
that makes more sense, I understood it would fire the *) if nothing else matched, but for some reason thought that it was all inclusive, if that makes any sense. For example, if I was on macOS it would set all the macOS aliases **and** the *) section. 
You may want to consider just doing functions instead? A bit more verbose but you could keep them grouped by command instead of by OS.
If you need the blah alias under all cases, then put it outside the case statement after esac.
&gt; reddit is able to use this style of markdown as well If possible, please use the 4-spaces method. Many of us still use the old Reddit interface, and for some inexplicable reason Reddit does not turn render triple-backtick sections properly on that.
 alias ga='/usr/bin/git add' alias da='ga --git-dir=$HOME/dotfile-repo.git --work-tree=$HOME' alias gc='/usr/bin/git commit -m' alias dc='gc --git-dir=$HOME/dotfile-repo.git --work-tree=$HOME' Also you can have the `--git-dir=$HOME/dotfile-repo.git --work-tree=$HOME` in a variable, like whatever="--git-dir=$HOME/dotfile-repo.git --work-tree=$HOME" alias da='ga $whatever' 
I think you are right. I also have the impression that curl returned 0 (success) in some situations it shouldn't. Can't remember the exact situation since it's been a while but it definitely rings some bells.
I came across [this](https://superuser.com/questions/1256963/create-aliases-dynamically) but I'm not sure if it's considered hacky and may be ill-advised. Also not sure if it's compatible with zsh which I also use. 
Also it feels clunky to me to spell out the full path to a binary so frequently. If it wasn't already in $PATH for some reason I'd only provide the full path once. git="/usr/bin/git" alias ga="$git add" alias gc="$git commit -m" d_args="--git-dir=$HOME/dotfile-repo.git --work-tree=$HOME" for cmd in ga gc; do alias d${thing#g}="$cmd $d_args" done
Something that pisses me off to no end. Markdown was basically invented for reddit. RIP Mr. Schwartz.
# ash as /bin/sh In Ubuntu 6.10, the default system shell, /bin/sh, was changed to dash (the Debian Almquist Shell); previously it had been bash (the GNU Bourne-Again Shell). The same change will affect users of Ubuntu 6.06 LTS upgrading directly to Ubuntu 8.04 LTS. This document explains this change and what you should do if you encounter problems. The default login shell remains bash. Opening a terminal from the menu or shortcut \[crtl-alt-t\] provides interactive bash. A script run from the desktop or file manager, through the dialogue 'run in terminal' will execute as POSIX dash.
**THANKS!!** Of course, it should come after I have set base\_template!
That, to me, gets overly complex, since there are a number of codes one would check for. I guess there could be an array with the various codes. I was looking more just to see if the page is reachable without interference. Thanks. 
You are correct, /u/sep00 curl will return a 0 in situations where you can't actually access a site. That's why I said the test isn't "bulletproof." 
Will test and let you know. Thanks. 
Thanks. Will test and let you know. 
Thanks for the clarification, /u/cttttt
You can chain grep to sed, but not with the --in-place flag (-i) because this only works on input from files.
Try =~ instead 
Use an array per column awk '{a[$2] += $6; b[$2] += $7} END { for (i in a) print i, a[i], b[i] }'
&gt; awk '{a[$2] += $6; b[$2] += $7} END {for (i in a) print i, a[i], b[i]}' Worked perfectly, greatly appreciated. 
Actually, your regex is fine. I think the problem is your piping to zenity at the end of your \`find\`
&gt;answer=$(find \`pwd\` -name "\*.epub" -o -name "\*.mobi" -o -name "\*.rtf" -o -name "\*.pdf" -o -name "\*.txt" | shuf | head -n14 | zenity --list --title "Choose a Book" --column "Random Books" --width=800 --height=400) Seems to be working for me, after I took out the \`shuf\` answer=$(find `pwd` -name "*.epub" -o -name "*.mobi" -o -name "*.rtf" -o -name "*.pdf" -o -name "*.txt" | head -n14 | zenity --list --title "Choose a Book" --column "Random Books" --width=800 --height=400) echo $answer /Users/alop/test/fish.pdf alop@macbook-pro:~/test 10:54:09$ if [[ $answer == *.txt ]]; then &gt; echo Text file &gt; elif [[ $answer == *.rtf ]]; then &gt; echo RTF File &gt; elif [[ $answer == *.pdf ]]; then &gt; echo PDF File &gt; else &gt; echo I Dunno &gt; fi PDF File &amp;#x200B;
Ok then what am I doing wrong? I need to mod the file. What do I need to do differently??
Why are you reinventing rsync?
Because as far as I know I can't send the rsync copies into different threads to speed it up.
Try doing the same thing without the -i flag on send and specify an output file.
The following should work. If the output looks good, add a `-i` parameter at the beginning to edit the file: sed '/ addthis$/! { /thefile/ { /myline/ s/$/ addthis/ } }' thefile I like using perl one-liners instead of sed. Here's how that would look: perl -pe 's/$/ addthis/ if /thefile/ and /myline/ and not / addthis$/' thefile Same as with sed, you can add a `-i` parameter at the beginning to make perl edit the file: perl -i -pe '...' thefile 
Thanks actually you stating that made me look harder into how I could potentially thread rsync and found a way to do it.
&gt; As for the actual problem, are you sure you are running the script with bash? OMG. **This was it.** I put in the spaces, nothing--then I realized there was no hashbang...I reverted all of the changes and put in the #!/bin/bash line and it works perfectly now, even without the spaces in the command/arguments. Thank you.
Thanks. According to that, the script was running via /bin/bash. But when I added the missing shebang line at the top of the script, with no other changes, the bug was magically fixed somehow...? So strange.
Here's a solution using `awk`: awk '{if (/this/ &amp;&amp; /that/ &amp;&amp; !/( end$)/) print $0, "end"; else print $0}' test_file &gt; new_file Input file: $ cat test_file this that this that that this this that end Output file: $ cat new_file this that this that end that this end this that end So it will append "end" to lines that contain both "this" and "that", irrespective of the order they appear in, which do not end with "end". Replace "this", "that", and "end" and filenames to suit your needs. 
Yea, it was working for me too, but I didn't try the change directory bit, and I always just out of habit add the `#!/bin/bash` at the top, which I guess I shouldn't just add when helping someone. You still have some decent advice here though. One other thing you can do is shorten your script by a few lines since multiple conditions have same result. if [[ ${answer} =~ \.(txt|rtf|epub|mobi)$ ]]; then fbreader "${answer}" else evince "${answer}" fi or if [[ ${answer##*\.} =~ txt|rtf|epub|mobi ]]; then fbreader "${answer}" else evince "${answer}" fi
You probably won't see much of a benefit from parallelizing rsync. Rsync will generally run as fast as it can until it runs into a bottleneck. I don't know what your project is but it may benefit from lsyncd. It utilizes inotify and rsync to synchronize small changes as they happen instead of large sync blobs. 
If the shebang is missing, it's up to the caller to decide what to do. If bash is the caller, it will run the script with bash. Most other programs will leave it up to execve what to do, which will most likely run it with sh. And with sh, all the [[ ]] would most likely fail.
Thanks, I had started out thinking that I'd use separate software for those extensions, but it turns out FBReader does a lot. I appreciate the code and put it in place. Lots of good advice here for sure.
For the record I think comm would be a better match than diff in this scenario. But as the other user posted rsync it is probably better :) Could you post the rsync command that youd want to use? If you think rsync isn't run with many threads you could try with parallel :) 
Can you tell me what ` alias d${cmd#g}="$cmd $d_args"` does and/or what it's called (something substitution)? I also came across [this](https://superuser.com/questions/1256963/create-aliases-dynamically) but seems more complicated but I assume more complete (maybe supports completion)? Not sure it it does the same kind of substitution. Much appreciated.
https://www.tldp.org/LDP/abs/html/string-manipulation.html &gt; Substring Removal &gt; ${string#substring} &gt; Deletes shortest match of $substring from front of $string. `${cmd#g}` is `$cmd` with the first `g` removed. `d${cmd#g}` adds the `d` and removes the `g`. I used that to transform your `ga` and `gc` into `da` and `dc`. (But it sounds like you probably already figured out what it's doing and wanted to read more about how it works... the link above should help.)
create an alias and put it in your .bash_profile alias "rm=rm -v" Once you save it in your .bash_profile, execute the profile by using the "source" command, shortened to a period . .bash_profile 
You can put this line into your ~/.bashrc file: alias rm='rm -v' This makes it so whenever you type a line that starts with the word "rm", bash will replace it with "rm -v ". This change will only apply to new terminal windows you open. The ones that were already open before you edited .bashrc will not get this new "rm". It will also only apply to what you do at the command line. When 'rm' is used in a script, that will still be the original without the '-v'.
[removed]
I'm sure your problem is somewhat easy to solve. I have to admit I didn't understand what your problem/goal is here. &amp;#x200B; To delete a folder from the terminal you have to pass the "-r" option to rm. `rm -r folder1/` &amp;#x200B; When it comes to set certain commands as a default there is always the option to set aliases. You might want to look into that. [https://davidwalsh.name/alias-bash](https://davidwalsh.name/alias-bash) &amp;#x200B; hope I was able to help you in some way.
Ahh man, sucks. Just realized `/usr/bin/git status --git-dir=$HOME/dotfile-repo.git --work-tree=$HOME" doesn't work whereas `--git-dir=$HOME/dotfile-repo.git --work-tree=$HOME status` does. For `git commit` it's the same and I assume for all other git commands. So it's not as simple as `alias d${cmd#g}="$cmd $d_args"`. Hmm, looks like gotta expand the aliases and do a string substitution somehow.
This will not do what you want: pass=$pass | base64 you probably want to do this instead: pass=$( base64 &lt;&lt;&lt;"${pass}" ) The `$( … )` construct spawns a subshell, and it gets replaced by what it returns on standard out. The `cmd &lt;&lt;&lt;"${pass}"` is a construct that's equivalent to `echo "${pass}" | cmd`, avoiding a useless use of `echo`. 
Ah okay. That makes more sense. Awesome man, thank you for the feedback.
was just about to reply with this. pass="password"; for i in {0..5}; do pass="$(base64 &lt;&lt;&lt;"$pass")"; done but op I hope u understand this isnt security in any way shape or form. there is also no reason to ever base64 a base64 string.
Ah okay. That makes more sense. Awesome man, thank you for the feedback.
First, if you start a line with 4 spaces Reddit will treat the line as code. Regarding your actual question, this is a good use of processes substitution. enc_pass="$(base64 &lt;(echo 'password'))" # Result echo "${enc_pass}" cGFzc3dvcmQK The first line does two things. 1. base64 &lt;(echo "password") The &lt;() part is a special syntax for creating an in-memory file and setting it's contents to the output of the echo command. The base64 command simply sees it as a regular file and proceeds be to b64 it. 2. enc_pass=$(command) This part defines a variable enc_pass and had it's value set to the results of the `command` given inside the $(). Look up bash process substitution if you want to dig into it further. Hope this helps!
It does help! Thanks for the tip
About this suggestion: base64 &lt;(echo "password") Had `base64` not accepted data on *standard in* and insisted on using a file, process substitution would have been a neat trick, avoiding creation of a temporary file on disk. But as `base64` accepts data on *standard in*, process substitution is here an over-complex method and `base64 &lt;&lt;&lt;"password"` is way more efficient and readable.
&gt;Had `base64` not accepted data on *standard in* and insisted on using a file, process substitution would have been a neat trick, avoiding creation of a temporary file on disk. Wouldn't really refer to process substitution as a trick... &gt;But as `base64` accepts data on *standard in*, process substitution is here an over-complex method and `base64 &lt;&lt;&lt;"password"` is way more efficient. I was genuinely curious about this statement after reading it and decided to take a quick look at the source for [coreutils base64](https://github.com/wertarbyte/coreutils/blob/master/src/base64.c) to see how it handled stdin vs a file being passed to it. Relevant section shown below if (STREQ (infile, "-")) { if (O_BINARY) xfreopen (NULL, "rb", stdin); input_fh = stdin; } else { input_fh = fopen (infile, "rb"); if (input_fh == NULL) error (EXIT_FAILURE, errno, "%s", infile); } fadvise (input_fh, FADVISE_SEQUENTIAL); In my example, the process echo "string" wouldn't even require the shell to spawn an actual process since echo is a shell built-in. The only "overhead" would be from creating a new file descriptor instead of using stdin's descriptor. Once the file descriptor is opened, is handled the same regardless of how it was opened. So the claim that it would be **way** more efficient is a bit the hyperbolic side. &gt;and readable. &lt;(echo "Pass") VS &lt;&lt;&lt;"Pass" Readability is pretty subjective, can't say either is more difficult to read than the other.
I HIGHLY recommend you use ShellCheck on your scripts, it catches all these little syntax errors and alerts you: [https://www.shellcheck.net/](https://www.shellcheck.net/) [https://github.com/koalaman/shellcheck](https://github.com/koalaman/shellcheck) [https://github.com/timonwong/vscode-shellcheck](https://github.com/timonwong/vscode-shellcheck) &amp;#x200B;
Just to add, great info on the difference between \[ \] and \[\[ \]\] here [https://unix.stackexchange.com/questions/306111/what-is-the-difference-between-the-bash-operators-vs-vs-vs](https://unix.stackexchange.com/questions/306111/what-is-the-difference-between-the-bash-operators-vs-vs-vs)
Solved by escaping the inner doublequotes `""` inside argument to `bash -c`....
&gt; bash -c "echo -n "External1TB:... You can't expect it to guess that the second quote in the above should be a nested quote. Use single quotes for the outer set, and double quotes for the echo and grep. (`bash -c 'echo "...$(grep "..." &lt;(df ...))iB"'`) Alternatively use awk instead of echo and grep, and in either case you can use a regular pipe: df -H --output=target,avail | awk '$1 == "/mnt/External11TB" { printf("External11TB: %siB\n", $2) }' 
My current favourite is a collection of a few bash scripts used for writing and viewing notes about my working day. `./today` will make a new file named like `2018-10-14-Sunday.md`, with the date already at the top (if the file didn't already exist), and open it in `vim`. `viewday` will open highlight the days notes and display the formatted and colours in `less`. Or `viewday $NUM` will open notes from the previous days, where the number increments each entry in the past. `days` will list all the days and their corresponding numbers. Very much enjoyed using these things!
I do something similar for my notes. Do you use a single multiarg function or do you have them set up as separate aliases?
Thanks!!
Local git or local git via an ide ( i use eclipse photon )
I don't have any good answers, but I wanted to thank you for the link to wttr.in. I've been meaning to set up a cron script to pull the weather and email me under conditions, but I haven't been able to find any sites that play nicely with curl.
I have an ultra wide monitor and screen sharing for work I has been received terribly. My solution was to make a script, that when I call it, changes the resolution from 2k to 1080p and vice versa. 
You asked about version source control.
Np. Most importantly: have fun!
I've used this for a while but never shared it. `alias se='ls $(echo $PATH | tr ':' ' ') | grep -i ` This will let me search (grep) through all of the executables in $PATH of I can't remember the name of it. This is in my .bash_aliases. e.g. `$ se cat`
&gt; echo "${gnome_desktop_settings[@]}" That echo outputs each element of the array separated by space. Stop using echo, and learn to use printf. prinf '%s\n' "${gnome_desktop_settings[@]}" However, you should rather just have your gsettings function assign a global array. It's much more efficient.
I have a few I like: &amp;#x200B; alias tree='find . -print | sed -e '\\''s;\[\^/\]\*/;|\_\_\_\_;g;s;\_\_\_\_|; |;g'\\''' alias fuck='eval $(thefuck $(fc -ln -1))' alias shttp='python -m SimpleHTTPServer'
I have: host -t txt istheinternetonfire.com | cut -f 2 -d '"' | cowsay -f elephant in my .bashrc file. When I launch a terminal, it displays the security panic de jour.
Thanks for the feedback! I will consider using a global, I just don't like the idea of them, seems really messy, If I swap out echo `"${gnome_desktop_settings[@]}"` for prinf `'%s\n' "${gnome_desktop_settings[@]}"` I actually get nothing printing outside the function. Were you saying that I should be able to swap them and it work, or I need to go global? Cheers!
Thank you for wttr!! 
Thanks again, very helpful information!
Looks too heavy for what I want, plus I don’t use emacs. Still, thanks for the suggestion.
That seems expensive... Try this for a performance boost: se() { compgen -c | grep -i "${1:?No search parameter given}" } 
&gt;curl ipinfo.io/ip Show public ip from command line
Thanks! Never used compgen before. 
The `wttr.in` thing comes around here every so often, FWIW here's my variant, in a `.bashrc` function: # Get local weather and present it nicely weather() { # We require 'curl' so check for it if ! exists curl; then printf '%s\n' "[ERROR] weather: This command requires 'curl', please install it." return 1 fi # If no arg is given, default to Wellington NZ curl -m 10 "http://wttr.in/${*:-Wellington}" 2&gt;/dev/null || printf '%s\n' "[ERROR] weather: Could not connect to weather service." } Obviously it relies on this: # Functionalise 'command -v' to allow 'if exists [command]' idiom exists() { command -v "${1}" &amp;&gt;/dev/null; } Which I should alias one of these days to `iscommand` Right now, my favourites (in that I seem to use them a lot) are these three: # Convert comma separated list to long format e.g. id user | tr "," "\n" # See also n2c() for the opposite behaviour c2n() { while read -r; do printf -- '%s\n' "${REPLY}" | tr "," "\\n" done &lt; "${1:-/dev/stdin}" } # Wrap long comma separated lists by element count (default: 8 elements) csvwrap() { export splitCount="${1:-8}" perl -pe 's{,}{++$n % $ENV{splitCount} ? $&amp; : ",\\\n"}ge' unset splitCount } # Convert multiple lines to comma separated format # See also c2n() for the opposite behaviour n2c() { paste -sd ',' "${1:--}"; } `c2n` I use far more than the others. The way that I generally use these three together is for fixing up `sudo`ers aliases. 
or: `curl icanhazip.com`
While you're at it: `sudo systemctl disable firewalld --now` /s
I work a lot with AWS these days, and jq is amazing for parsing, pretty printing, syntax highlighting, and reformatting all the json outputs. Cool stuff!
c-x e (or ESC-v if you have bash in vi mode) is one of the most useful and easily forgotten features in bash.
This is a great source for this sort of thing: [http://www.shell-fu.org/](http://www.shell-fu.org/)
alias vi=‘vim’
Nethack
`sudo !!` Runs previous command as sudo. 
/r/philosoft What does that even do/mean?
I keep all my alias related functions as part of my .bashrc and manage that via a gitlab repo. my notes related function just creates a new file in my notes folder, adds a line at the top marking it with a date and time, and opens it in vim. I want to build it a little smarter but haven't had time. 
I use a function called neat_log in all my scripts for consistent and cleanly formatted logs and stdout. neat_log () { if [ "$1" == "---" ]; then echo $2 else printf "%-3s %-80s %-30s \n" "$1" "$2" " $(date -u)" &gt;&gt; $LOG echo "$2" if [ "$1" == "!!!" ]; then echo $2 | mail -s "$SUBJECT" "$ADDRESSES" fi fi } neat_log "---" "Informative but not important enough to log. Stdout only." neat_log "!!!" "Something exciting. Log and email." neat_log "+++" Everything else is logged and printed to stdout." 
IPTables all the way &amp;#x200B;
Theres a vim-org mode
Cool I’ll take a look 
Hard to believe iptables is no problem for you but selinux is.
If you want to remove all the quotes in lines that contain `property="` you could use `sed`. If you have GNU `sed` you can use the `-i` (with BSD you need to give an argument to `-i`) or you could have it write to a temp file and move it over the original name. With `sed` you can match a pattern, then perform an action if it is matched, which can include doing substitutions. We can embed that in your `find` and just do the replacement when something is found, and nothing will happen if the pattern isn't found. So I'm going to just look for `property="` and if we find that, remove all the double quotes. If that isn't really what you want, we'll need more information find . -type f -name '*.tra' -exec sed -i -e '/property="/ {s/"//g}' {} \; 
I tested it in a single file and it works, thanks a lot. Do you know if there is a way to make this find print the edited files? Also I guess it only edits the current line
This will only edit any line that matches the first regex, here `/property="/` any line that doesn't match that will be left alone. If you want it to print out the filename when it's making a change, that gets a little trickier (at least in my mind) with `sed`. You could switch to using `awk` which could both modify the file (again, assuming GNU `awk`) and also print to `stdout`. Alternately, you could do the `grep` first and have it print, then do the edit after, something (untested below) like find . -type f -name '*.tra' -exec grep -H 'property="' {} \; -exec sed -i -e '/property="/ {s/"//g}' {} \; which will do the `grep` and if the pattern is found will print its regular output, and only on those files that contain a match will execute the `sed` command. The `sed` will be duplicating the work on `grep`, but it'll get you the output you want I think. 
Firstly, `args` must be an array; assign as args=( --git-dir="$HOME/dotfile-repo.git" --work-tree="$HOME" ) Use: ds() { git "${args[@]}" status "$@"; } Secondly, why on earth does this need to be done dynamically? Just do a search and replace in your favourite editor, save, and you're done.
Every time I want to add/change/remove a git function I don't want to do it again for the corresponding git function. Why is my idea so bad?
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
It depends. Duplicating words doesn't necessarily mean duplicate code. For example, a function similar to `git "${args[@]}" status "$@";` seems useless and doesn't de-duplicate anything. How many "git &lt;something&gt; status &lt;something&gt; will you use? Except for something like `git &lt;add .; git&gt; status` which I'd consider as an utter hack. Also, btw, realistically, how many git functions will you ever need..? I use like 8, 10 tops, and I consider myself advanced in GIT (been using it for over 3 years now--of course that's not an insane amount, but I feel confident in it). Simply having 8 lines of very readable and clear code is, in my mind, much better than 4 lines of hacky shell that only you understand because it's cool... PS: Of course, these are your dot files. You do as you please with the level of coolness you decide :)) But, if you tried to push something similar into my codebase, that'd be a one large nope from me.
If you really want to go through with this exercise... &gt;What's a good way to implement this? Generally the good way to implement anything is to start by approaching it from a high level. Ask yourself: "What steps do I need to go through to get from a to b?" Then split the problem/task up into digestible, achievable chunks. e.g. * You need to identify your `git` specific aliases and generate a list of them. * For each (hint) list member, you need to check whether the counterpart function exists or not. * If (hint) the counterpart function doesn't exist, you need to create it. * Beneath that is a sub-list of tasks. How do I go about converting an alias to a function? So you might build some functions for each of those tasks, something like this: # Print a list of git related aliases # The double awk isn't elegant but it's obvious get_git_aliases() { alias | grep "^alias.*git" \ | awk '{print $2}' \ | awk -F '=' '{print $1}' } # Test if a given item is a function and emit a return code isfunction() { [[ $(type -t "${1:-grobblegobble}") = function ]] } # Convert a git alias to a git function gitalias_to_gitfunc() { # If no parameter is given, fail out [[ ! "${1}" ]] &amp;&amp; return 1 # Convert the parameter (i.e. alias name) to our desired function name gitFunc="d${1#g}" # Pluck out the alias definition from BASH_ALIASES, # use set to convert it to a parameter array set -- ${BASH_ALIASES[$1]} # Print out our desired function format printf -- '%s\n' "${gitFunc}() { ${1} \"\${args[@]}\" ${*:2} \"\${@}\"; }" } Then start tying them together like this: for gitAlias in $(get_git_aliases); do gitFunc="d${gitAlias#g}" if ! isfunction "${gitFunc}"; then gitalias_to_gitfunc "${gitAlias}" fi done Proof of concept, I've declared your aliases: ▓▒░$ for gitAlias in $(get_git_aliases); do &gt; gitFunc="d${gitAlias#g}" &gt; if ! isfunction "${gitFunc}"; then &gt; gitalias_to_gitfunc "${gitAlias}" &gt; fi &gt; done dc() { git "${args[@]}" commit -m "${@}"; } dd() { git "${args[@]}" diff "${@}"; } dl() { git "${args[@]}" ls-files "${@}"; } dlog() { git "${args[@]}" log "${@}"; } ds() { git "${args[@]}" status "${@}"; Writing those back into your dotfiles is an exercise that I'll leave to you. If it were me, I'd have a `.git_functions` file, so it would be simply a matter of `gitalias_to_gitfunc "${gitAlias}" &gt;&gt; .git_functions` 
Just shows an elephant telling me what is burning on the Internet that day. It queries the txt DNS record of the istheinternetonfire.com (Could certainly hit the page with cURL or wget) host -t txt istheinternetonfire.com Grabs the second field of the returned text with " being the delimter: cut -f 2 -d '"' Displays the text using the cowsay utility (Check it out, it's fun. A bunch of different animals talk in ASCII. You might need to install the package. It's in the base Fedora repo, probably in others as well): cowsay -f elephant 
There is always an easier method. I always used 'dig +short [myip.opendns.com](https://myip.opendns.com) @resolver1.opendns.com' .
If you think shellcheck is wrong, you can tell shellcheck to ignore it by adding a comment like this in front of the line with the problem: # shellcheck disable=SC???? Replace the "?" with the code number you see printed in the shellcheck output. This will only apply to that one line of your code, not for the rest of your script.
 cd() { builtin cd "$@" if [ $? -eq 0 ]; then printf "\033]2;%s\007" "${PWD}" fi } This one will set terminal window title to current directory after each `cd`. You can place it in .bashrc file.
It makes sense to me what you are doing. You want those texts split into words, and you want that output where things have categories. I don't see why mapfile would be good for this. It would just be annoying to try to use.
I don't know whether to think this is going to be a huge help to me, or drive me bat shit trying to use it. Half the time the only reason I remember I'm on a remote machine is when I go to run a command I have aliased and it doesn't work. Either way kudos. 
I've set custom PS1 in my config just for this purpose, so now all remote shells are easily identified. Check example in README (:
Looks neat. Been using sshrc for a while and the "self replicating" feature seems nice. \&gt; won't create any files on remote hosts (even temporary) this is actually part of the reason I use sshrc. It transports some of my dotfiles and utilities with me when I connect to a remote box.
In that case it's going to improve my life more than anything since learning alias fuck='sudo $(history -p \!\!)' 
:-D
This is pseudo code, as I haven't tested your commands, but it should point you in the right direction. Directories=$(ls -d */) for Directory in ${Directories[@]}; do find $PWD/$Directory -iname "*.mp4" -print -exec ffmpeg -i {} -c copy -bsf:v h264_mp4toannexb -f mpegts {}.ts \ find $PWD/$Directory -iname "*.ts" | sed 's:\ :\\\ :g'| sed 's/^/file /' &gt; $PWD/${Directory}fl.txt ffmpeg -f concat -safe 0 -i $PWD/${Directory}fl.txt -c copy $PWD/${Directory}${Directory::-1}.mp4; rm $PWD/${Directory}fl.txt done 
It worked perfectly, with one small exception. Creating the output mp4 filename needed to be: ${Directory::${#Directory}-1}.mp4 Thank you so much for the assist. Now, being a complete bash newbie, how would I get this to be, say, a shell script I can call from the terminal, and bonus points if I can call it from an arbitrary directory and have it run on that directory? 
I'm glad I could help. Just add the shebang (#!/bin/bash) at the beginning of the file and make it executable (chmod +x). It will already work from whatever directory it's called from ($PWD).
But would I have to copy it to each directory? Can I put it in some sort of path directory to run it from anywhere? 
What shellcheck says is: In reddit.sh line 22: PACKAGES_TO_INSTALL+=(${packages[$package]}) # shell check says use mapfile? ^-- SC2206: Quote to prevent word splitting, or split robustly with mapfile or read -a In this situation, `read -a` is probably the better suggestion than `mapfile`. Because shellcheck isn't quite smart enough to figure out what `${packages[$package]}` contains, it doesn't know which to suggest; `mapfile` will generally be better for newline-separated lists, while `read -a` will generally be better for space-separated lists. When you do this: PACKAGES_TO_INSTALL+=(${packages[$package]}) The high-level description of what you're asking it to do is take the *string* stored in `${packages[$package]}`, and turn it in to an *array*. That's fine. The lower-level description is that it turns it in to a array in a 2-step process: 1. First, it goes through *field separation*, where it is split in to parts by sequences of any of the characters in `$IFS` (space, tab, and newline by default) 2. Then, it goes through *glob expansion* where any of the parts with `*` or `?` (or a few other things too, if extglob is enabled) get expanded in to a list of 1 or more things (or 0 or more things if nullglob is enabled) based on filenames that exist. You probably want #1, but not #2. Many Bash programmers don't even realized that #2 happens in this situation, which is why shellcheck warns about it. If you really do want #2 to happen, then you can ignore this warning from shellcheck. We can use `read -a` to safely split an `$IFS`-separated string in to an array. Unfortunately, it will set the entire array, and doesn't have an option to append to an existing array, so we'll have to do the append as a separate step: read -r -d '' -a tmp &lt;&lt;&lt;"${packages[$package]}" PACKAGES_TO_INSTALL+=("${tmp[@]}") If we didn't add `-r`, it would allow a backslash to inhibit separation; `foo\ bar` would become `([0]='foo bar')` rather than `([0]='foo\' [1]='bar')`. Maybe that's what you want, but it's not what #1 above does. If we didn't add `-d ''`, it would ignore everything after the first newline. Right now there are no newlines, but if you added one, you probably wouldn't want it to ignore everything after it. Plus, #1 above doesn't ignore everything after the newline. ---- Or you can append to an existing array with mapfile, but you'll have to take some extra steps to make sure that `&lt;&lt;&lt;` doesn't insert a trailing newline, and it changes how it handles sequence of spaces: mapfile -d' ' -O "${#PACKAGES_TO_INSTALL[@]}" PACKAGES_TO_INSTALL &lt; &lt;(printf '%s' "${packages[$package]}") Also note that the `read` version would split `foo bar` (2 spaces between `foo` and `bar`) in to `([0]='foo' [1]='bar')`, but the `mapfile` version would split it as `([0]='foo' [1]='' [2]='bar')`; it would require things to be separated by *exactly* 1 space; and it would only allow space, not not tab or newline. ---- But really, why do you have an associative array at all? You don't seem to actually be using the keys for anything. Why not just use a plain array with comments? PACKAGES_TO_INSTALL=( # drivers libva-intel-driver fuse-exfat # multimedia gstreamer1-vaapi gstreamer1-libav ffmpeg mpv mkvtoolnix-gui shotwell # utils gnome-tweak-tool tldr whipper keepassx transmission-gtk lshw mediainfo klavaro youtube-dl freetype-freeworld # gnome_extensions gnome-shell-extension-auto-move-windows.noarch gnome-shell-extension-pomodoro gnome-terminal-nautilus # emulation winehq-stable dolphin-emu mame # audio jack-audio-connection-kit # backup_sync borgbackup syncthing * # languages java-1.8.0-openjdk # webdev chromium chromium-libs-media-freeworld docker docker-compose nodejs php php-json code zeal ShellCheck ) 
Thanks for the detailed info :) I have to conclude that in this particular case if I want simple clean code I am better sticking with `PACKAGES_TO_INSTALL+=(${packages[$package]})` .It says what is being done, adding something to something, `+=`, and contains no cryptic meaningless switches and extra steps. I am never going to have `*` or `?` in my file names as it would be invalid for Fedora package names, and I can test in advance to see if any other strange things happen that should not. I am not sure how it could be considered bad code otherwise if this is the situation? I have thought to myself is it worth having as associative array. The purpose is to show the user in advance what will be installed so they can quit if they don't like what they see. Much easier for non coders. If I don't split it into categories it looks like chaos and is unreadable. This sub-reddit is helping me learn bash fast! I started out having the functions 'return' variables as if I were programming in JS, and soon learned that was a lot more trouble than it was worth! Seems it is simple global variables all the way, I am just going to make them CAPS so I know what is going on. I am going to be referring back to your reply again soon I am sure, awesome.
You can override the echo command with a function at the start of the script: echo() { builtin echo "$*"; sleep .5; } echo "Hello, $USER" echo "Ok, bye" Though you shouldn't be using echo in new scripts. Learn to use printf. You can override it in the same way printf() { builtin printf "$@"; sleep .5; } printf 'Hello, %s\n' "$USER" printf 'Ok, bye\n'
No. You can call it from any directory using it's absolute path and it will work in the directory you called it from. [user@localhost covertdad]# pwd /home/covertdad [user@localhost covertdad]# ls MovieCombine.sh [user@localhost covertdad]# cd ./videos1 [user@localhost videos1]# pwd /home/covertdad/videos1 [user@localhost videos1]# /home/covertdad/MovieCombine.sh (magic script stuff performed in videos1 directory) [user@localhost videos1]# cd ~/videos2 [user@localhost videos2]# pwd /home/covertdad/videos2 [user@localhost videos1]# /home/covertdad/MovieCombine.sh (magic script stuff performed in videos2 directory) 
I think your life would be a whole lot easier if you used rsync for this, you'd get the "compare the copied files against the source using find, md5 and diff" for free with a standard built-in tool. Not trying to talk you out of this if you're intentionally making it more complicated as a bash exercise, just making sure you know it could be a lot easier and cleaner. 
I'm guessing that runs the last command with `sudo` in front?
Yep it's much more cathartic than sudo !!
[removed]
 du -sk ./* | sort -n | awk 'BEGIN{ pref[1]="K"; pref[2]="M"; pref[3]="G";} { total = total + $1; x = $1; y = 1; while( x &gt; 1024 ) { x = (x + 1023)/1024; y++; } printf("%g%s\t%s\n",int(x*10) y = 1; while( total &gt; 1024 ) { total = (total + 1023)/1024; y++; } printf("Total: %g%s\n",int(total*10)/10,pref[y]); }' This piece of code lists the size of every file and subdirectory of the current directory, much like du -sch ./* except the output is sorted by size, with larger files and directories at the end of the list. Useful to find where all that space goes. 
My Synology NAS has an rsync option, but I was not aware it had this ability. I'll have to look into it, thanks.
This feels very much like homework, am I assuming this correctly? I think I would loop over the files and concatenate them to a temporary file somewhere in /tmp. Then strip all the unnecessary crap with `sed -i '/needle//g /tmp/file’`, you only need the date and the number, don’t strip the colon, that’s an ideal separator. Then, with `awk` or `cut`, grab everything in front of the colon and use `| sort | uniq` to get a list with only unique dates. Again, for every date (put them in an array or a new tmpfile), grab everything behind the colon if the date matches `awk -F: ‘/${date}/ {print $2}’`, add the resulting values with `bc` and print the result with `echo “${date}:${sum}”`. There might be a more elegant way to do this but I guess this should work.
Besides, if anyone is going to use backpack it will be triple nice if you, guys, give me a little feedback on the thing ^.^ Thanks!
what this does is: split the fields ($1, $2, ...) by `localhost_access_`, just to remove that part of the string easily in every case remove the `.tar.gz:X` part get the number in the variable `n`, because is after `:`, to the end of the field store an array using the string as an index and the value is the number, so we can easily increment it in each occurrence of the string at the END, for each index of the array str, it prints the index (our string) followed by `:` and the calculated number awk -F 'localhost_access_' '{ n=substr($2,1+index($2,":")); gsub("\.tar\.gz.*","",$2); str[$2]+=n } END{ for (i in str){ print i":"str[i] } }' file1 file2 file3 file4 with the files you posted here the result would be 2018-06-23:0 2018-07-21:12 2018-07-04:2 2018-06-24:0 2018-07-22:4 2018-07-05:3 2018-06-25:0 2018-07-23:2 2018-07-06:6 2018-06-26:1 2018-07-24:2905 2018-07-07:0 2018-06-27:0 2018-07-25:10440 2018-06-19:0 2018-07-26:2644 2018-07-27:1896 2018-07-28:1238 2018-07-19:28 2018-07-29:932 2018-06-20:0 2018-06-21:1 2018-06-22:0 2018-07-20:17 
Your beginning point is fairly clear but your requirements are a little tough to follow. You may want to list them out. &amp;#x200B; From what I gather you want to list all the contents of each file into one and add a count at the end of the line? Is this a one time thing or a recurring script? How I would go about it would be to grab each input file and insert each line of the oncoming file into the output. As it is input count the number of times it already exists in the output then insert it after the last entry. Sounds tough but it'll be fun. The rough pieces I would use are \-xargs to grab lines of the file. \-grep to count the number of times it exist in the file already. \-sed to place the line in the file in its proper place. \-tac to reverse the order of the file(may make the use of sed easier). &amp;#x200B; This is just a spitball of an idea. I'm sure someone will have a better idea. I may come back and code it out if I have time later today. Good luck! &amp;#x200B;
Thanks for your help!
\`\`\` \#!/bin/bash &amp;#x200B; TMPFILE="/tmp/tmpfile" &amp;#x200B; for file in file1.txt file2.txt; do rm -f ${TMPFILE} cat ${file} &gt;&gt; ${TMPFILE} done &amp;#x200B; date\_arr=($(awk -F: '// {gsub("localhost\_access\_",""); gsub(".tar.gz", ""); print $1}' ${TMPFILE} |uniq|sort)) &amp;#x200B; for date in "${date\_arr\[@\]}"; do int\_arr=($(awk -F: '/'${date}'/ {print $2}' ${TMPFILE})) sum=0 for int in "${int\_arr\[@\]}"; do sum=$(($sum + int)) done echo "${date}:${sum}" done \`\`\`
I started working on this a couple of days ago. It's a TUI for a torrent-daemon called transmission-daemon written in pure bash. There are external calls to the torrent client but the rest of the code is all bash. I think I've squashed all the bugs. :D
Please post your solution when you are done!! Nobody likes a story without an ending!
For all the nay-sayers, it truly is incredible what can be done with bash!
See this one as well: https://github.com/dylanaraps/pxltrm :P
The Markdown on Reddit is super crappy and doesn't know ```` ``` ````. You instead have to add four spaces in front of each line of your code, and then it will start treating it as code. Like this here: normal text here code here code here normal text again 
For formatting your code on reddit, put four spaces before the lines of code and reddit will show them like this Your array assignment isn't working. You can assign arrays in bash like so: ARRAY=('apple="' 'banana="' 'peach="') Then, change the double quotes to single quotes around ${ARRAY[i]} like this: fruitNcolor=$(cat fruit.txt | grep -E '${ARRAY[i]}') and grep will properly use them in the search pattern. 
hmmmmm. i tried putting four spces before the lines of code . looks like not working .. i will try formatting ARRAY like you suggested
Ah, you have to also put a blank line before the line beginning with four spaces. Otherwise it completely ignores it.
fruitNcolor=$(grep "${ARRAY[i]}" fruit.txt) in the for loop doesn't work. fruitNcolor1=$(grep 'apple="' fruit.txt) works still . 
Nice start. This is basically a wrapper script for 'transmission-remote'? I'm guessing that's the client you mentioned. If you'd like to make it when ever better. I would add some input validation with help on exit and not use $1 directly. 
&gt; This is basically a wrapper script for 'transmission-remote'? I'm guessing that's the client you mentioned. Yup &gt; If you'd like to make it when ever better. I would add some input validation with help on exit and not use $1 directly. It actually doesn't take any arguments. There's a warning message when `transmission-daemon` isn't running but you just start it and it works. The `$@` is leftover from a copy/paste boilerplate I've made (I'll remove it now). 
Bash rocks dudes ! Aldo zooqle as well .
&amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; \#!/bin/bash ARRAY=('apple="' 'banana="' 'peach="') for i in "${ARRAY\[\*\]}"; do fruitNcolor=$(cat fruit.txt | grep -E "${i}") echo $fruitNcolor; echo '' done fruitNcolor1=$(cat fruit.txt | grep -E 'apple="' ) echo $fruitNcolor1 fruitNcolor2=$(cat fruit.txt | grep -E 'banana="') echo $fruitNcolor2 fruitNcolor3=$(cat fruit.txt | grep -E 'peach="') echo $fruitNcolor3
I understand this is trivial. Please answer, do not downvote me. Thanks
`output_sorted` will hold the sorted content of output.txt. I guess you wanted to `echo "$output_sorted" &gt; somewhere.txt`. Or `awk ... | sort &gt; somewhere.txt` directly.
 source fruit.txt printf 'apple is %s\n' "$apple"
Okay. Let's try this step by step. Remove everything except `awk -F ... nodo1.txt nodo2.txt nodo3.txt nodo4.txt` and run the script. Does it output what you expect? If so, add `| sort` to the end so it becomes `awk -F ... nodo1.txt nodo2.txt nodo3.txt nodo4.txt | sort`. Is output sorted now? If so, add `&gt; somewhere.txt` to the end so it becomes `awk -F ... nodo1.txt nodo2.txt nodo3.txt nodo4.txt | sort &gt; somewhere.txt` and run it.
People who say that probably prefers a different language. My job is largely writing shell scripts.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
 echo 1 &gt; /proc/sys/kernel/sysrq while read Trigger; do echo o &gt; /proc/sysrq-trigger done &lt; &lt;(inotifywait -m $WatchedDeviceDir)
check the values of word_count_{1,2} (wc outputs more then just the line count)
Oh. Thanks, didn't realize I could do that. I'll update the script.
&gt; he is not giving us enough information or "fruit" so that we can do labs by ourselves. Dude, you're sitting in an orchard. It's not even like you're being lazy, or if you are you're doing it wrong - posting here is *more* work than googling `bash script show usage`, `bash script add line to file`, `bash script test if line exists in file`, etc. 
Hey, add `sudo slapt-get -i youtube-dl` if you are going to list package managers for flavors. Slackers exist too!
I'm not asking for anyone to do my homework, just was wondering if someone could help explain it better. About 75% of the class agrees he isn't doing well at teaching it (Even my friends who take other scripting classes). Just saying. 
I will. When I made the script, I just listed 3 distros off the top of my head.
`if test $1` `then` `if test $2` `then` `file="classlist.tst"` `if grep-q $1 $file;` `then` `echo"$1 is already in list"` `else` `'echo $1 &gt;&gt; $file'` `fi` `if grep -q $2 $file;` `then` `echo "$2 is alread in list"` `else` `'echo $2 &gt;&gt; $file'` `fi` `else` `file="classlist.txt"` `if grep -q $1 $file;` `then` `echo "$1 is already in list"` `else` `'echo $1 &gt;&gt; $file'` `fi` `fi` `else` `echo "Usage:` [`addstudent.sh`](https://addstudent.sh) `[email-address]..."` `fi`
Looks like you're on the right track. I'd advise against all that conditional logic, especially when you might get anywhere from 0-2 parameters passed (per the spec). I'd suggest you iterate over the `$@` array (array of arguments passed to script). It results in much cleaner code and you can accept any number of arguments now! for student in "$@" do #make sure at least one param passed (else show usage info) #make sure classlist.txt is writable (hint: extra credit if you made the file writable within your script) #check if student already exists #write to file done 
Can you please explain me what it does? 
Thank you! At least I know my friend and I are on the right track. Currently having a different issue but I'll let you know if it worked out in the end. 
So far it works the way we need it to I believe! :D
Here's what it does. You enter in a song you want to hear from youtube. For example... I'd type in 'type o negative dead again' (without the quotes). It'll take what you searched for (in my case 'type o negative dead agin') and make youtube-dl search youtube for it. youtube-dl will then filter out the results for the video with the best audio it can find, then make mpv play the audio from that video that it found. After the song is finished, it'll exit the script then clear the terminal so that it's like you just opened your terminal.
Can you try gt in place of /&gt;
Yup, understood the case. Didn't get why putting the var search that way. My ignorance. 
As mentioned, &gt; is string comparison and -gt is integer comparison. But it's probably better to use an arithmetic context, like while (($current &gt; $maximum)); do ... done See https://mywiki.wooledge.org/BashGuide/TestsAndConditionals Single `[` is the worst.
Ah,.... okay, interesting. It actually works with my above script, and that fixes it, but I seem to have simplified my example TOO much, as it fails if instead of 413 the number is, say 413.542, which in real life it may be. testing 413.542 vs 1350 freespace.sh: 9: [: Illegal number: 413.542 smaller &amp;#x200B;
 [ 413 -gt 1350 ] &amp;&amp; echo "math gone wrong" || echo "math is fine"
bash doesnt do floats. mb im wrong but bash decides 413.542 is a string and compares them like that - or in this case, fails
Bash doesn't do floating point.
See [BashFAQ 22](http://mywiki.wooledge.org/BashFAQ/022) on how to deal with floats
Sheesh - 150 lines of bash. Amazing!
Me too (before I retired this year). bash everywhere - bash-3, that is, so it would run on RHEL5 as well as 6 and 7. Management persistently nagged about 'time to move to a better language' which sounds nice until you actually try it on a couple of hundred tightly integrated, complex scripts in a production environment undergoing rapid change and moving to a completely new environment. Applying (a modified) Kernighan's razor led us to identify 2 or 3 places where a speed improvement would help - of course we then had the problem of different python versions and managing incompatible python libraries with puppet. That was enough pain - when you're mainly massaging files and processes, working with python really can seem like doing keyhole surgery in boxing gloves compared to bash. Not saying it could not be done, just the cost was very high and the benefit marginal.
ah - ok, thats explains a lot. I'll see if I can figure out stripping the decimals out of that variable then - nearest whole number is close enough &amp;#x200B;
Not an error, but you don't need to terminate lines with ';' &amp;#x200B;
Putting aside the pointless nature of the script, it seems to be working on my machine, but I don't think that that `while` loop is doing what I think you think it is. You are escaping the greater-than symbol ( `&gt;` ) which is causing it to evaluate the entire statement as a string (i.e. "Is there any text between these brackets?"). Bash cannot natively evaluate decimal numbers, so you need to make use of another utility such as `bc` to evaluate the expression. Also, I'm not quite sure why those parentheses were there, but you don't need them. &amp;#x200B; #!/bin/bash CURRENT="413.542" MAXIMUM="1350" echo "testing: $CURRENT vs $MAXIMUM" # Piping the expression into `bc -l` will return 1 for true, 0 for false while [[ $(echo "$CURRENT &gt; $MAXIMUM" | bc -l) == 1 ]]; do echo "larger" sleep 05 done echo "smaller" exit &amp;#x200B;
Can you give us the error?
https://www.tldp.org/LDP/abs/html/string-manipulation.html ... search/scroll down to "Substring Replacement" 
You need to escape the semi-colon at the end of the line otherwise `find` throws a fit.
I tried with your script. #!/bin/bash ARRAY=('apple="' 'banana="' 'peach="') for i in "${ARRAY[*]}"; do fruitNcolor=$(cat fruit.txt | grep -E "${i}") echo $fruitNcolor echo '' done fruitNcolor1=$(cat fruit.txt | grep -E 'apple="' ) echo $fruitNcolor1 fruitNcolor2=$(cat fruit.txt | grep -E 'banana="') echo $fruitNcolor2 fruitNcolor3=$(cat fruit.txt | grep -E 'peach="') echo $fruitNcolor3 the output I'm getting is: apple="red" banana="green" peach="yellow" It does not output anything from the for loop .
ahhh... I wasn't paying attention. you want this: yourdog@badtz:~/blah$ ./ex.sh apple="red" banana="green" peach="yellow" apple="red" banana="green" peach="yellow" That comes from: #!/bin/bash ARRAY=('apple="' 'banana="' 'peach="') for i in ${ARRAY[*]}; do fruitNcolor=$(cat fruit.txt | grep -E "${i}") echo $fruitNcolor; echo '' done fruitNcolor1=$(cat fruit.txt | grep -E 'apple="' ) echo $fruitNcolor1 fruitNcolor2=$(cat fruit.txt | grep -E 'banana="') echo $fruitNcolor2 fruitNcolor3=$(cat fruit.txt | grep -E 'peach="') echo $fruitNcolor3 
Isn't that going to get you `file.mkv.mp4` results? \;
okay, so I tried that and got this &gt;find: ‘/media/carpe/675aa2f0-5968-4007-8b04-60d8930cfa33/Tv Shows/Mr.Robot.Season.2.Complete.720p.WEB-DL.EN-SUB.x264-\[MULVAcoded\]/Mr.Robot.S02E06.eps2.4.m4ster-s1ave.aes.720p.WEB-DL.x264-\[MULVAcoded\].mkv’: Permission denied I still get that even if I'm root. 
bad habits!
what outputs more than that?
syntax error: operand expected (error token is "" 8 log_1.log" + " 40 log_2.log"")
I'm both in embedded systems (busybox) and local services, so I use a lot of deprecated stuff. We don't even have bc for math :/
Managed to work it out. Used `printf` to create the query then `eval` to resolve it: &amp;#x200B; printf -v array\_length "\\${#%s\[@\]}" $random\_array printf -v value "\\${%s\[%d\]}" $random\_array $array\_length eval "echo $array\_length" eval "echo $value"
 array1=( 11 12 13 ) array2=( 21 22 23 ) array3=( 31 32 33 ) arrays=( ${array1[@]} ${array2[@]} ${array3[@]} ) random_array_number=$(( $RANDOM % ${#arrays[@]} )) # pick a random number between 0 and the array length random_array=${arrays[$random_array_number]} # randomly select array with above number random_array_length=${#random_array[@]} # Determine the length of the selected array random_element=${random_array[$(( $RANDOM % $random_array_length ))]} # select random value from that array echo "$random_element" By the 5th step your array only has 1 element. 
Thank you for this. I need to learn much more about "indirect references" and then why it's a bad idea :) 
Updated post to describe how it was solved. My thanks to everyone who helped!
You can use a single array to hold a matrix matrix=( 11 12 13 21 22 23 31 32 33 ) width=3 height=3 Now you can calculate the index for a given row and column with `row * width + col`: printf 'row 0, col 0: %s\n' "${matrix[0 * width + 0]}" # row 0, col 0: 11 printf 'row 1, col 2: %s\n' "${matrix[1 * width + 2]}" # row 1, col 2: 23 while also being able to deal with it as a 1d array, which greatly simplifies your random selection: printf 'random element: %s\n' "${matrix[RANDOM%(width*height)]}" 
Is it possible to run this for transmission-daemon running on a different computer using SSH?
Sounds like you are trying to store a matrix by using N*M variables. If that's the case, just use a single array to hold the matrix.
Oh wow! thank you! . ${i} vs ${ARRAY[i]} made a lot of differences! . 
I use the exact same thing :) I wanted a dream journal I could roll out of bed and launch an already named file and be able to start typing without even opening my eyes so I could jot things down before forgetting them. You said scripts, do you use youralias=vim "date -%dateoptions".yourextension /foo/bar? or something more complicated?
That `c{1..4}{1..4}` already makes bash create that list, see here: $ echo c{1..4}{1..4} c11 c12 c13 c14 c21 c22 c23 c24 c31 c32 c33 c34 c41 c42 c43 c44 
Yeah it’s a collection of 4 small scripts in my bin
&gt;curl -L r/https://raw.githubusercontent.com/ziishaned/wifi-password/master/installer.sh | sudo sh &amp;#x200B; Terrible advice. [https://www.seancassidy.me/dont-pipe-to-your-shell.html](https://www.seancassidy.me/dont-pipe-to-your-shell.html) &amp;#x200B; &gt;npm install -g wifi-pass To "install" a bash script? That does not make any sense.
 #!/bin/bash STRING="c34"; increment () { val=$1 val=$(($val + 1)); if (( $val &gt; 4 )); then val = 1; fi return $val } adjust () { item=$1 field=$2 p1=$( echo $item | cut -c 1) p2=$( echo $item | cut -c 2) p3=$( echo $item | cut -c 3) case $field in 2) increment $p2 p2=$? ;; 3) increment $p3 p3=$? ;; *) echo "Field incorrect"; exit 1; ;; esac newstring="$p1$p2$p3"; echo $newstring } adjust $STRING 2
``` local a [[ a == b ]] &amp;&amp; a=1 || a=0 ``` This is about the closest you can get. 
Arithmetic context already has it (( max = cur &gt; max ? cur : max )) For anything else, use if else
 $ b="value" $ a=$([[ ${b} == "value" ]] &amp;&amp; echo "foo" || echo "bar") $ echo ${a} foo you have to take care that the `&amp;&amp;` command exits true, else the command after the `||` will fire, and STDERR might clutter your intended output: $ a=$([[ ${b} == "value" ]] &amp;&amp; ls /nosuchdir || echo "bar") ls: /nosuchdir: No such file or directory $ echo $a bar `echo` will work just fine, but if you're not sure use a subshell with `exit 0` and some STDERR redirection: a=$([[ ${b} == "value" ]] &amp;&amp; { ls /nosuchdir 2&gt;/dev/null; exit 0; } || echo "bar") 
 stu@nook:~ $ echo $str foo1.bar@baz_foo2*bar2 stu@nook:~ $ echo ${str%_*} foo1.bar@baz stu@nook:~ $ https://www.tldp.org/LDP/abs/html/string-manipulation.html
/thread &amp;#x200B; Thank you!
alias a='alias' :)
If I understand your question it is not. :) What you should do is to use an if-statement for your echo line. 
could you give me an example, please?
I second this
 echo $i |nc localhost port | grep "Correct" 
First of all, do you have permission from the forum admin to do this? Second, have you looked for applications that do this already? It might be worthwhile looking at existing software before rolling your own. Third, what do you mean anonymous sms? I don't think such a thing exists (other than hijacking status messages for a specific recipient on a particular cell tower).
I modified my approach as my use case doesn't need to check all possible outcomes. I know what `HTTP` code to expect for a successful connection and test against that here: url_check(){ # curl to check HTTP status code; error message if it fails status_code=$(curl --output /dev/null --silent --head --write-out '%{http_code}\n' $url) if [ $status_code -ne "200" ] ; then printf "%s\\n" "BAD URL" else printf "%s\\n" "GOOD URL" fi } url_check THANKS!
I don't plan on releasing the script untill I have asked them for such permissions. Last couple i knew about have stopped working, been unsupported and/or cost money now Well its more semi anonymous, the website above lets you send an sms to a recipient in the selected location using a random number owned by them, just fill out the forum (county, recipient, message and sender ID) and it'll be sent automatically
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day!
[strings](https://sourceware.org/binutils/docs-2.31/binutils/strings.html#strings) [tr](https://www.gnu.org/software/coreutils/manual/html_node/tr-invocation.html#tr-invocation) 
you can dump stout to file, e.g. `od -j 0x1000 -N 0x10 -c FILE1.BIN &gt; FILE1.txt` extending this a little: `cp FILE1.BIN $(od -j 0x1000 -N 0x10 -c FILE1.BIN | tr -d \ | cut -c8-).BIN` 
I haven't really installed a lot of stuff from github, but how would I go about installing this?
Download the `fff` file and copy it to a folder in your `$PATH` (or you can just run it with `./fff`). Simple steps (for setting up a user based dir): 1. `mkdir -p ~/.local/bin/` 2. Add `export PATH+=:~/.local/bin:` to your `.bashrc`, `.zshrc` or etc. 3. Copy `fff` to `~/.local/bin`. 4. Re-open your shell.
So I don't have to clone the whole directory? Just the one file and put it in my bin? That's way easier than I was expecting. Thanks :)
Holy shit, this is amazing! It reminds me of scrolling around on a TI-83 (the first thing I ever programmed on) ... not to say that your code is trivial or anything, by any means. I'm going to read it and learn how to make TUI programs in bash! :)
Thanks! I'm working on the `CONTRIBUTING.md` file which will outline how it all works. I'm also working on a boilerplate script you can build on top of (it'll include input handling, resize, scroll, etc). :)
that would be so cool. I mean, *is* there a better way to write a TUI than a bash script? yes. but this would be the most *fun* way!
exactly :)
 cd $HOME
Same error occurs. it thinks ~ is part of the name
This just goes to the directory of the user currently running the script. need it to actually change directories if another user is running it.
in a nutshell my script is supposed to ask for the user's login name, then throw the script to their home directory based on what is entered and then list the files of that directory.
Is its intended for use outside of your system? Could just ls "/users/$entered" - replacing users with whatever the base of the homedir is
Do you have root access? Cause afaik the best way to do it otherwise is to change to that user then echo $HOME. You could also use eval, but if you're accepting anything as the user input then people could just as easily fry your machine. 
no i dont, this is a class exercise. I can see other uer's files but i cant run or change them. only list.
Eval might be your best bet then. Eval echo ~$entered. Read up on the pitfalls of accepting user input to eval though. 
okay that worked, but then the listing went back to the location of the script. 
Well yeah, it'a an echo lol. Since it's a class assignment I'll leave the rest as an exercise to the reader. 
thanks for the help
Maybe cd ~"$foobar"
 #!/bin/bash IFS=': ' let dx=$1 dy=$2 set `xdotool getmouselocation` let x=$2 y=$4 while [ $x -ne $dx -o $y -ne $dy ]; do [ $x -lt $dx ] &amp;&amp; let x+=$(((dx-x)/10+1)) [ $x -gt $dx ] &amp;&amp; let x-=$(((x-dx)/10+1)) [ $y -lt $dy ] &amp;&amp; let y+=$(((dy-y)/10+1)) [ $y -gt $dy ] &amp;&amp; let y-=$(((y-dy)/10+1)) xdotool mousemove $x $y sleep 0.1 done 
Cheers. Will work on those.
Favorites are done. You can define keys `1-9` to a dir or file and on key-press `fff` will go there (if it's a file it'll be opened). https://github.com/dylanaraps/fff/commit/fc87e5d0c6ea091b7fd180aa2b3a9d3fcdd756d4
That was quick! Whenever it gets scp capabilities i'll use it about 7 hours a day, 5 days a week to test it.
How would scp support work? What's the expected behavior? 
I don't expect you to get into the hassle of browsing files thru scp but I've done these two: * A) ```rsync -ap user@server:$remotePath $localPath``` * B) ```scp -p user@server:$remotePath $localpath``` To copy from/to remote servers. Of course you can switch the remote and local to upload files instead. Doing any of these two will ask for a password if the user has not set up an ssh key authorization (like in ssh-copy-id). 
Great work, but I wouldn't call a Bash script \`Fucking Fast\` ;).
Here is a [working demo](https://ideone.com/H6DZB1) of the following `sed` command: sed -r "s/([12]?[0-9]?[0-9])\.([12]?[0-9]?[0-9])\.([12]?[0-9]?[0-9])\.([12]?[0-9]?[0-9]) \4\.\3\.\2\.\1.in-addr.arpa.name =/\1\.\2\.\3\.\4/" 
You might consider using *dig* instead of *nslookup* It provides output in a much more usable format. 
Well, it's possible that some optimization is possible. There might be some syntax shortcuts that didn't occur to me. Furthermore, the thing about regular expressions is that when you **know** the input will always follow a pattern, you can get away with making your regex less strict. So, for example, the regular expression I used would *not* match `312.683.900.1` because such an IP address is impossible. But maybe your input file will never have a substring like this... So the regular expression can become simpler if it only needs to match digits and dots. If you look for corners to cut in this style, you can likely simplify this regex. This should at least get you started and then you can experiment and make improvements ;) Best of luck to you!
Here's "cut": cut -d ' ' -f 1-3,6 It behaves like this in action: $ cat testfile 2018152 LUNAME1 172.16.131.37 37.131.16.172.in-addr.arpa.name = xyz-016600.corp.mybiz $ cut -d ' ' -f 1-3,6 testfile 2018152 LUNAME1 172.16.131.37 xyz-016600.corp.mybiz It will misbehave if there are tab characters instead of spaces in the file or if there are several spaces. If you need something that can deal with that, you could use this perl one-liner here: perl -anE 'say join " ", @F[0..2, -1]' About using a regex rule that does exactly what you describe, you could do a rule that searches for the "=" character and looks for where the spaces are in the surroundings and uses that to delete stuff. It would look like this: $ sed -r 's/\s+\S+\s+=//' testfile 2018152 LUNAME1 172.16.131.37 xyz-016600.corp.mybiz The pattern it searches for is `\s+\S+\s+=`, which translates into "spaces" + "not-spaces" + "spaces" + "=". This pattern here can also deal with tab characters.
This one awk '{$3=$4=""; print $0}' &lt;file&gt; will leave empty columns and the = sign. This: awk '{$3=$4=""; print $0}' &lt;file&gt; | tr -d "=" will remove the "=" sign, but leave the empty columns; while this awk '{$3=$4=""; print $0}' &lt;file&gt; | sed -e "s/ / /g" -e "s/=//g" will do what you want, I think.
 ~ λ echo "2018152 LUNAME1 172.16.131.37 37.131.16.172.in-addr.arpa.name = xyz-016600.corp.mybiz" | awk '{ print $1, $2, $3, $6}' 2018152 LUNAME1 172.16.131.37 xyz-016600.corp.mybiz Awk can just do this. 
Hey everyone! This is a little project I did to gain some experience using Bash and git hooks. Would love to know what you think, especially in terms of the quality of the code itself.
It works for me with `bash 3.2.57(1)-release`. What exact version are you on? 
Since you're asking this in r/bash and not r/sed, I'll start off with the bash version: shopt -s extglob filename=lastnamefirstname_id_id_filename-1.cpp dest=${filename#*_*_*_} dest=${dest/%-+([0-9]).cpp/.cpp} printf '%s -&gt; %s\n' "$filename" "$dest" Using sed (which is the wrong tool to use for renaming files): sed 's/-[0-9][0-9]*\.cpp$/.cpp/; s/.*_//' &lt;&lt;&lt; "$filename"
Here's a sneak peek of /r/sed using the [top posts](https://np.reddit.com/r/sed/top/?sort=top&amp;t=year) of the year! \#1: [Stripping 5 different numeric characters from the end](https://np.reddit.com/r/sed/comments/9ayshu/stripping_5_different_numeric_characters_from_the/) \#2: [combining sed commands](https://np.reddit.com/r/sed/comments/8dejkk/combining_sed_commands/) \#3: [Asking help on regular express in sed.](https://np.reddit.com/r/sed/comments/8c3lx5/asking_help_on_regular_express_in_sed/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
I would :P
&gt; If you have the time, could you explain this line: &gt; dest=${filename#*_*_*_} `${filename#*_*_*_}` expands to the same as `$filename`, but with the shortest string matching the (glob) pattern `*_*_*_` from the start, removed. See [BashFAQ 100](http://mywiki.wooledge.org/BashFAQ/100) for more on string manipulations in bash.
Something like this? #!/bin/bash main() { local f last_geo last_sgeo geo sgeo city for f; do while IFS='|' read -r geo sgeo city; do [[ -n $city ]] || continue [[ $geo != $last_geo ]] &amp;&amp; { last_geo=$geo printf '%s\n' "$geo" } [[ $geo == $last_geo &amp;&amp; $sgeo != $last_sgeo ]] &amp;&amp; { last_sgeo=$sgeo printf ' %s\n' "$sgeo" } printf ' %s\n' "$city" done &lt; "$f" done } main "$@" Use like `./myscript file1 file2 file3`. You might have to add more error-checking or whatever depending on how reliable your input files are Note that using pure bash for this is very slow compared to something like AWK or Perl. If you're dealing with large files you might consider rewriting it for one of those, it's the same concept. For smaller inputs it probably doesn't matter 
This is a great alternative to mps-youtube. Thanks very much.
 awk -F '|' '{geo[$1][$2][$3]=1} END{ for (x in geo){ print x for (y in geo[x]){ print "\t" y for (z in geo[x][y]){ print "\t\t" z } } } }' filename If I understood it correctly
the problem is the teacher forgot the upload homework instructions and he uploaded so late and im not a fast learner i checked that site but its no help
this should take you about an hour even if you're a slow learner. do your homework.
then which part will make me do my homework in this
for FILE in $(ls /etc); do echo $\{FILE\} done Reading a text file is easy: FILE="$(&lt;$\{FILE\})" Or you could do: IFS=""; while read LINE; do NEXTFILE+="$\{LINE\}" \#read into array: \# NEWARRAY=("$ \{NEWARRAY\[@\]\}" "$\{LINE\}") done "if" is easy.. if \[\[ " $\{LINE\} == "this is important" \]\]; then echo "Got this: $\{LINE\}" fi I should note that the "\{\}" chars around the variable names isn't really "necessary", but I personally consider it a best-practice. That way, your variable syntax is consistent (easier to look at in the code when syntax is consistent, and you WILL inevitably end up using the "\{)" chars at some point, so might as well just do it for everything.). ..Nothing irks me more than trying to learn the syntax for a new language. Bash is crazy-easy, once you get used to it. Hope this helps a little.
really thanks but this is so complicated for me unfortunately. the instructor never touch on these commands. he only read the file used for and if with grep to find strings on that data etc. I'm not even sure because he was like saying bash-scripting was not important for us but his first homework is this. unbelievable. and the time given dropped to 1.5 days instead of 7 because of his inactivity.
for loop looks like for k in ${Items_to_Search_on} ; do ${actions} done If ${Items_to_Search_on} is output of a command, you would use the syntax $(command) if startement looks like if [ ${VARIABLE1} == ${VARIABLE2} ] ; then ${action1} else ${action2} fi for grep syntax: man grep 
This is a little simpler `awk` command, no pipes necessary: awk '{ print $1, $2, $3, $6 }' It's a little "brittle" because changes to the columns will break it; however the same can be said for the awk commands above. /u/SirGuileSir , you'll just have to use your own judgement about what fits best with your scenario.
That is basic. You need to sit down and study, way more than you do now.
does this work for you? for i in {1..10}; do sha1sum &lt;&lt;&lt; $i; done &amp;#x200B;
`seq 10 | xargs shasum` Your pipe to xargs echo just makes shasum take the same 10 lines. You need to run shasum for each line. 
You're looking for the 'split' command: seq 10 | split --lines 1 --filter sha1sum You can use split to get "block" checksums of binary files: cat /bin/bash | split --bytes 256k --filter sha1sum You can confirm it with dd, for example the third line should be identical to: dd if=/bin/bash bs=256k skip=2 count=1 2&gt;&gt;/dev/null | sha1sum
Thank you, but I don't think that this is half as good as mps-youtube. This is just a quick script I made that uses 2 external programs. mps-youtube is not only one program (2 depending on the features you use), but it has features like making playlists and actually choosing search results.
With parallel, you can use the `&lt;&lt;&lt;` bash feature if you send a whole command line into parallel by surrounding it with quotes, like this: $ parallel 'sha1sum &lt;&lt;&lt; {}' ::: {1..10} e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e - 7448d8798a4380162d4b56f9b452e2f6f9e24e7a - a3db5c13ff90a36963278c6a39e4ee3c22e2a436 - ... I saw you mention in another post that you are battling with the `\n` line-break character. You want to get the hash result of exactly `1` and not `1\n`? If you work manually at the bash prompt, the `\n` works like this: $ sha1sum &lt;&lt;&lt; 1 e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e - $ echo 1 | sha1sum e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e - $ printf "1\n" | sha1sum e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e - $ printf "1" | sha1sum 356a192b7913b04c54574d18c28d46e6395428ab - So that `&lt;&lt;&lt;` and 'echo' always add an `\n`, and you can use 'printf' instead to get text without it. You can use that 'printf' version with parallel like this: $ parallel 'printf {} | sha1sum' ::: {1..10} 356a192b7913b04c54574d18c28d46e6395428ab - da4b9237bacccdf19c0760cab7aec4a8359010b0 - 77de68daecd823babbb58edb1c8e14d7106e83bb - ... Note, it's best to write `printf {}` and not `printf "{}"` in that command line you give to parallel. Parallel will automatically add quotes if there are spaces and do escaping for weird characters, and that won't work right if you try to add quotes yourself as well.
Here's an approach: seq 10 | xargs -n 1 -- bash -c 'echo $0 | sha1sum' 
&gt; every time a pipe is invoked a subshell and subprocess appear True. &gt; is the practice of opening too many (often useless) pipes considered bad practice? Not if they're doing something useful. Of course if a long pipepline can be shortened, it's almost always worth it for efficiency.
Which level of efficiency are we talking about? Have you got benchmarks?
If a pipe is useless, don't use it.. There are times where pipes allow for a more efficient method to process data. Pipe may also make the flow more readable/debuggable by the next person to look at the code. If it's a one-liner, my initial guess would be that efficiency isn't the primary purpose behind the saving of the command in the first place.
Remove the "for opn X" at the end of each. I'm assuming you're basing this off of https://askubuntu.com/a/56685 The portion for opn X, just means do the commands that option X above should do. "for opn 1/2/3/4" should all be removed. "invalid option" also isn't a command. You'll either want echo "invalid option", or something else. 
The most important aspect of this that each process in a pipeline read()s and write()s data independently, therefore causing a lot of memory to be copied from/to user space to kernel. I'm not aware if any under-the-hood optimisation at this level - such as COW, however the relatively new, Linux-only`splice()` system call allows for a zero-copy process, but very few traditional power tools use it for various reason (code simplicity/robustness/portability, or simply 'cause they predate it by ages and the code was not updated). 
If there's "useless" pipes, those are of course useless. That would be doing `cat filename | ...` instead of `... &lt; filename`. Or doing `grep pattern | awk '...'` instead of `awk '/pattern/ ...'`. Other than that, you need to share more details about what's happening on that machine. Is it under heavy load all the time, or is there nothing really going on? Pipes can be neat because it's a simple way to parallelize stuff. Each step in the pipe will get to run at the same time on a different CPU core. This could be good if the script has the whole machine to itself.
worked like a charm is there a way i can make them select 1,2,3 with the arrow keys instead of input string
see useless use of `cat` award.
You're getting into ncurses or something similar, and it's outside the scope of something simple like this. Sorry, if you're going to google it, I would look into creating a tui interface.
Don't count pipes, count forks. Pipes are basically free, and all of the cost of a subshell goes into the fork. It is obviously bad practice to do "too many (often useless)" forks, whether or not they're subshells or use pipes. It is obviously good practice to do an appropriate amount of forks, whether or not they're subshells or use pipes. My suggested rule of thumb would be to feel free to fork O(1) times for any sane constant factor, but to evaluate each fork carefully if you need O(n) of them. In other words, don't worry about it unless it's in a loop. 
Yep. Having performance concerns is generally a good sign that you've exhausted what Bash was intended to provide. Obviously, in the rare cases where people actually are playing into Bash's strength -- being simple Unix process model glue -- then there may be a legitimate case for optimization. We'd have to look at specific examples to say whether this is the case.
 INTERVAL=$(cat /dev/urandom | tr -dc '10-45' | fold -w 2 | head -n 1) .... sleep $INTERVAL
Thanks for the reply.... I had kept googling and found that: INTERVAL=$(($(cat /dev/urandom | tr -dc '10-30' | fold -w 2 | head -n 1))) seems to work. I don't know if there is a functional difference but it seems to be working. I do know that the person who's phone keeps getting kicked now though is going to lose their shit thinking that it's broken lol. 
The copying of data from one process to another via a pipe is rarely a concern. You'd have to write a hundred megabytes per second for the effects of this to become noticeable. When a bash script is slower than it should be, the problem is almost always due to excessive forking.
You capture the output of something with `$( )`, so that alone should be enough. That added `$(( ))` is the math mode of bash. The person that used this weird `$(( $( ... ) ))` probably wanted to make sure that the result is always a number, even if something goes wrong with the command and it outputs text. The `$(( ))` will turn text into a zero. Your command seems really complicated. Is it just supposed to find a random number? You can do that with bash alone like this: $ echo $(( RANDOM % 50 + 10 )) 11 $ echo $(( RANDOM % 50 + 10 )) 25 $ echo $(( RANDOM % 50 + 10 )) 55 $ echo $(( RANDOM % 50 + 10 )) 19 This finds a random number between 10 and 59. You can use it in your script like this: INTERVAL=$(( RANDOM % 50 + 10 )) Or you could use it directly where you call the 'sleep' command without a variable: sleep $(( RANDOM % 50 + 10 ))
I like to &lt;code&gt; r/https://hellothere.icu &lt;/code&gt; it's pretty much [canihazip.com](https://canihazip.com), but I host it. 
 $ bash -c "$( printf 'sha1sum &lt;&lt;&lt; %s\n' $(seq 10) )" e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e - 7448d8798a4380162d4b56f9b452e2f6f9e24e7a - a3db5c13ff90a36963278c6a39e4ee3c22e2a436 - 9c6b057a2b9d96a4067a749ee3b3b0158d390cf1 - 5d9474c0309b7ca09a182d888f73b37a8fe1362c - ccf271b7830882da1791852baeca1737fcbe4b90 - d3964f9dad9f60363c81b688324d95b4ec7c8038 - 136571b41aa14adc10c5f3c987d43c02c8f5d498 - b6abd567fa79cbe0196d093a067271361dc6ca8b - 4143d3a341877154d6e95211464e1df1015b74bd - 
Then on the same note, COW makes `fork()` incredibly cheap. What you probably mean is the cost of `exec()`ing binaries (or `system()`ing commands) , which is definitely non-negligible.
Your position here is unreasonable. There's *lots* of times where having some crazy cat foo|grep 1|awk this|sed that|awk whatever|grep bar|etc|etc Is perfectly appropriate. Ad hoc diagnostic work, debugging, first drafting ideas for a function, script or tool... or staying with the funky one-liner it if it's functional - not everything has to run at Awesome Scale. If you want or need to, you can make it more elegant later. If I'm on the horn with someone and need to deal with something *now*, I'm going to go for basic functional rather than fuss around with optimized shell voodoo. Not everything needs perfect. Sometimes you just need to get stuff done. Source: decades of this 
0nce they choose an option, it will go through the case statement and then the script will exit.
Yes and no. A fork and an exec are roughly equally expensive, and both (separately or together) are dramatically much slower than the simple operations they're often asked to perform, such as removing a prefix from a string. You'll see a nice 2-10x speedup if you manage to remove the exec while keeping the fork (especially when dynamically linking in a lot), but you won't see the 1000x speedup you get from removing both fork+exec.
It's my first bash script, so i'm just playing around. But i want to make a mysql cli gui, where people can select,edit and remove the databases and tables in mysql from the gui with just a few clicks. 
That'll be a though one, but its Great for learning the language atleast. I have a pointer though, you should avoid passing sensitive data though commandline, mysql supports a variety of ways to log in without having to pass it like that. The security guys in here Will most likely throw a fit if they see password being sendt through cmdline The script say GUI, are you planning on using dialog or somethig similar? 
You could convert it: °F=°C/5*9+32
You could stuff an `awk` into that pipeline to convert the first column (temperature): ``` curl -s https://api.weather.gov/stations/KHND/observations/latest | grep value | sed '4s/value/Temperature/;17s/value/Humidity/' | egrep '(Temperature|Humidity)' | sed 's/"//g;s/://g;s/,//g;s/Temperature//g;s/Humidity//g' | xargs -n2 &gt;&gt; OutTemp.csv```
That was the goal I just don't know how to integrate it into my command. The csv readout is like this: 22.00000000000 45.000000000000 First value is degrees c second is humidity. I just want to change the first value. 
I've been trying to use `awk` whenever possible because it's actually super powerful and useful for things other than "print nth field" which is 90% of what I ever see it used for. This site is great for learning Unix tools: http://www.grymoire.com/Unix/Awk.html
I did it on purpose. I needed to separate the values I needed then I needed the csv file without those characters attached to my csv
Are you saying that you really do want a space instead of a comma? If so, simply replace `@csv` in the expression with `map(tostring) | join(" ")`.
Neat, I didn't realize that jq would handle the math also.
K
Great answer and a elegant solution. Thank you for this detailed description. 
Thank you! I should have mentioned that I want to use parallel in the end and ropid posted this great minimal solution. &gt;parallel 'printf {} | sha1sum' ::: {1..10} Nevertheless good to see this approach and how to split the numbers for sha1sum.
Simply put... Thank you.
You don't need to echo it. Echo does display it but to run the command, don't echo. Eg. If I ran 'echo "sudo apt update"' it would enter it into the terminal. But if I did it like 'sudo apt update' it would actually run the command. Also sorry for lack of editing. On Mobile.
hmm, so you are suggesting doing: ``` ls &gt; /proc/bash_proc_id/fd/0 ``` but that would be passing the stdout of ls in the current bash shell to the other one. I want to run ls on the shell of id: bash_proc_id thats why i thought that, if i send the command as text, to my desired bash process as stdin, then it might run it ``` echo 'ls' &gt; /proc/bash_proc_id/fd/0 ``` but as you mentioned, that didn't work, any suggestions?
I don't believe you can. I used to do this as an April fool's joke on people in the office from the host node. &gt;knock knock neo Then fire off a matrix script, lol. Why do you need to execute in an existing she'll which isn't yours?
I read it, but I do not want to read and check it every time when a new version was released.
Just look at the script, it is all there. That is what Bash is about.
Then don't update it ;)
I think a better deal would be to just do ssh $(grep "^Host" ~/.ssh/config | awk '{ print $2 }' |fzf) But good work on actually making something workable and shareable :D
I am not the author btw.
This is actually super cool.. :D in a way learned a bit more about piping. &amp;#x200B;
What are you actually trying to do? A user's `authorized_keys` is only editable by that user or by root. This is by design.
Don't know why this was downvoted. If a program does what you want and you don't need it to do anything else. Then you don't need to update. I'm not saying to not update your webserver, kernel or anything else. This is a program for selecting a ssh server to connect to, not how you secure your computer. You don't need to update your hello world program either....
Really :) nice :) what did you learn?
Just type the user’s username instead of $USER
I’d love to see an answer for that.. lots to learn! Do you see the output at the prompt or in stdout? Have you tried passing ctrl+V-Enter at the end of the command string?
The 'sudo' tool sets an environment variable named "SUDO_USER" when a user uses it to run something. You could try to use that. Check out the output of this here to see what I mean: sudo bash -c 'echo $USER; echo $SUDO_USER'
The `id -un` command always outputs the actual username of the calling process.
Althought I am unsure how that awk and print $2 worked.. I learned to grep from lines that start with certain way... and it can be somehow piped to fzf like that.
So much wrong. Home is possibly not in /home/$USER (think massive universities which split by year); don't create then chown, use umask instead; backticks are ancient See shellcheck.net . Also, RSA is so yesterday ;-) use ec25519 keys instead. You might also find ssh-copy-id(1) helpful Good luck.
This is what I was looking for! I knew that there had to be a way, probably with awk, but was having issues working through it myself. I had to modify it slightly because of small issues with my examples, but I was able to get to where I needed to with this.
This is what I was looking for! I knew that there had to be a way, probably with awk, but was having issues working through it myself. I had to modify it slightly because of small issues with my examples, but I was able to get to where I needed to with this.
This is what I was looking for! I knew that there had to be a way, probably with awk, but was having issues working through it myself. I had to modify it slightly because of small issues with my examples, but I was able to get to where I needed to with this.
How could I fit that in with my if statements? Unfortunately the if statement are needed in this case. 
Let me save you some time :) awk processes text differently than cut. Cut requires a defined seperator when it is not tabs that seperates values. Awk on the other hand sees first column of text and assigns it to $1 and $2 to collumn 2 and so on no matter what the seperator is by default.
You can get the final part of a pathname with [basename](http://man7.org/linux/man-pages/man1/basename.1.html), and the superior hierarchy part with [dirname](http://man7.org/linux/man-pages/man1/dirname.1.html).
In your "redacted" parts, are you using any quotes, single or double or parentheses? If so you might need to properly escape them. 
They are edge cases. You might want to wrap everything in a custom function to account for that. Say, if the PWD is `/` return `/` else ... . 
Thanks, I have this now: ``` PS1='`get_pwd_head`' PS1+='/' PS1+='`get_pwd_tail`' PS1+=' &gt; ' get_pwd_head() { local head="$(dirname $PWD)" if [ $head = / ]; then head='' fi echo "${head}" } get_pwd_tail() { local tail="$(basename $PWD)" if [ $tail = / ]; then tail='' fi echo "${tail}" } ```
Could you post this in a pastebin?? Reddit's markup messed it all up. Also, try running your code through shellcheck.net - it lists out most common possible runtime bugs in your script. 
Do look more into the bash case menus, they are better suited for what you are doing. It will be easier on users because they will enter a number that corresponds to their choice rather than typing the whole string. 
I'm not entirely sure but this is what i gather. The standard io are just buffers that the kernel allocates so programs know where to look, in this way `/proc/bash_id/fd/0` is just a regular file more or less. When using the normal cli, (readline on bash) the shell will call the [execve function](http://man7.org/linux/man-pages/man2/execve.2.html) that will actually execute the program. When piping to `/proc/bash_id/fd/0` your just forcing the string in to the kernel buffer, which i guess means execve is never called as bash doesn't know theres a command to execute... I guess. 
Thanks for checking: https://gist.github.com/Asheq/accbbb41a366900e77372894c0428c39 (pastebin is blocked for me right now)
Avoid back ticks (\`). Use `$()` instead. Other than, I don't see any logical issues. There are few style issues * Use consistent quotes (all double or all single) * Avoid variable/functions with names of built-ins - `head` is a coreutil program. As of now, I don't see any issues, but it might cause confusion in the future. I ran this through shellcheck, and it pointed out only one thing: in the pwd_tail function, there is *apparently* no need for echo. Not sure if that will work. What happens without the echo in that function? 
Thanks! &amp;#x200B; At first I thought the $2 variable would be something not defined yet. But knowing now that its about the column number. &amp;#x200B; Everyday I can learn something new.
Have you considered using a Linux FIFO pipe? You could write to it and also read from it.
Yeah ive been trying to mess around with it. Cant get it exactly right yet though
Sorry, I just reread your full post and saw you already were trying it. Why don't you post your code here to get some feedback?
mkfifo test_pipe ./test &gt; test_pipe ./p_to_test &lt; test_pipe ??? Not sure how to redirect the output of p_to_test back to the pipe then back to test. For the purpose of this example im trying to do, the c++ programs are literally just this: Test.cpp int main(){ Int x = 5; Std::cout&lt;&lt;x; Std::cin&gt;&gt;y; Std::cout&lt;&lt;“y is:” &lt;&lt; y; } P_to_test.cpp Int main(){ Int x; Std::cin&gt;&gt;x; Std::cout&lt;&lt; x*2; } Sorry for formatting im on mobile.
I asume it will work for now, but using double quotes here would be a good habbit and result in good practice in the future. I try to do this all the time. That way when it suddenly is actually needed, it won't be an issue I can't figure out. elkserverIP="xxx.xxx.xxx.xxx" That is how I would do it. Especially the dots could be an issue later when you adapt the script to do more (as scripts tend to grow) You might later add atest to see if the server is reachable and what not. 
Place "set -x" on the second line when you are testing. It will give you output and indications on where it goes wrong.
It really sounds like `./test` should be invoking `./p_to_test` by itself and capturing it's output instead. These kinds of loops are tricky and easy to deadlock. However, you can do it with: mkfifo portal &lt; portal ./test | ./p_to_test &gt; portal 
&gt;I can't seem to figure out why it won't print properly Here's ShellCheck: ``` $ shellcheck myscript Line 18: padding="$(printf '─%.0s' {1..$((30-${#cwd}))})" ^-- SC2051: Bash doesn't support variables in brace range expansions. ``` Use `seq` or something instead.
How would i go about invoking p_to_test and capturing output? Its not an interface. It just accepts stdin and puts out stdout? 
The `{a..b}` feature of bash does only work if you use literal values. It does not work if you use something like that `$((...))` formula or a variable name. It simply completely stops working in that case and turns into text. Here's an example about what I mean: $ echo {1..10} 1 2 3 4 5 6 7 8 9 10 $ x=10 $ echo {1..$x} {1..10} In this example, when I used "$x" instead of a literal "10", it turned things into normal text and the only thing that happened was that bash inserted the value of "$x" into the text.
Thanks! How does this look?: https://gist.github.com/Asheq/accbbb41a366900e77372894c0428c39#file-prompt-sh-L13
The solution works as advertised, the problem is that it's very easy to write `./test` and `./p_to_test` with assumptions about buffers and streaming without thinking. For example, if `./test` does `echo input; read output` and `./p_to_test` does `sed 's/foo/bar/g'`, then each script will appear to work on their own, but can cause two separate deadlock: one because `sed` will buffer output, another if `input` is too large causing the write to block before the read can be executed. Meanwhile, if `./test` invokes `./p_to_test` internally, you'll avoid both of those deadlocks without thinking, because you'll naturally open and close the stream, and do reads and writes asynchronously.
Test and p_to_test are c++ programs that simply use cin and cout
In C/C++ you would use `popen` 
&gt; grep -v "^#\|^$" That's an Extended regular expression, but the -E option is not given.
Also, your code is not robust for file and directory names containing spaces or other non-shellsafe characters. To make it robust, change &gt; | while read script; do to | while IFS= read -r script; do and quote the `${script}` expansion on the next line with double quotes (like `"${script}"`). 
Thanks a ton man, sorry I haven't been able to get back to you earlier. I went ahead and modified a response I saw before I couldn't respond. Thanks anyways! len="((17-${#cwd}))" if [[ $len -lt 1 ]]; then len=1; fi padding="─────────────────" padding="${padding:0:$len}" 
I changed what you did a bit. len="((17-${#cwd}))" if [[ $len -lt 1 ]]; then len=1; fi padding="─────────────────" padding="${padding:0:$len}" I knew I should use local I just hadn't completely read about it yet. I'm using it now though! 
Are the comments not excluded starting with a whitespace or tab ? If Yes you can remove them with: sed -e 's/\^\[ \\t\]\*//' &amp;#x200B;
Your `find` command is not the problem. It's in the pipeline using the two `grep` commands. The first command, `grep -v "^#\|^$" ${script}`, correctly eliminates comments and empty lines from the file, and outputs the rest of the file to standard output. The second command, `grep -Eil 'PATTERN-01|PATTERN-02'`, reads the output from the first from standard input. But the `-l` option causes it to output only the filenames of the files containing the matches found. This is meaningless because there is no filename; it's reading from standard input. Get rid of that `-l` option and see what happens.
You're right. I'm trying to do something like this now: grep -Ev "^#\|^$" "${script}" | grep -Eiq 'PATTERN-01|PATTERN-02' if [ $? -eq 0 ]; then echo "${script}"; fi
This isn't elegant, but I've had great success sticking `&gt; listoffiles.txt` on the end of my find commands and then running a simple `for $file in $(cat listoffiles.txt); do stuff; done`.
true if the variable `addr` contains the string `:`
Can you explain how it works? Is it a regex?
This is very cool! Although, I do find "trusted self-signed" to be rather oxymoronic. 
Take the value of the `BASH_SOURCE` variable, remove everything from the right starting with the last '/' in the string, and substitute the resulting value for the expansion. See: http://wiki.bash-hackers.org/syntax/pe#substring_removal
I dont trust that you trust yourself.
No, those are the same wildcards that you know from the command line for selecting files. It's the same rules as when you do things like: ls -l *.txt
Ah, I see the reasoning. Especially when a quit option is included. I appreciate the constructive criticism. 
It doesn't work as intended I guess it is just poorly formated the dude wanted to write something like : [[ "${addr}" =~ "*:*" ]] maybe operator is `~=` and yes it uses a type of regex I usually do the following : if echo "${addr}" |grep -Eq ':'; then hth
I’d keep it in. Remote non-interactive shells, for example, still source `.bashrc`, [as I found out the hard way](https://github.com/lucaswerkmeister/home/commit/b8478bdb068224535fc2e66b4366452d1fe31c70). &gt; **Bash** attempts to determine when it is being run with its standard input connected to a network connection, as when executed by the remote shell daemon, usually `rshd`, or the secure shell daemon `sshd`. If **bash** determines it is being run in this fashion, it reads and executes commands from `~/.bashrc`, if that file exists and is readable.
Definitely the main reason to keep it (that I’ve encountered) is for scp. I don’t want bash to source all my bashrc when I’m just transferring a file. Of course YMMV but it’s certainly not hurting anything to keep it there.
nah, it's formatted correctly. e.g. addr='123:456:789'; if [[ $addr = *":"* ]]; then echo TRUE else echo FALSE fi # output: TRUE
Thank you u/lucaswerkmeister and u/_zio_pane! I also realized that I am sourcing .bashrc from .bash_profile, which is read for login shells, both interactive *and* non-interactive. I could put the guard around the source line in .bash_profile, but putting it in .bashrc ensures protection against all possible scenarios (sort of a "tell-don't-ask" approach). 
I have that line in my ~/.bashrc because I source it from my ~/.bash_profile like this: [[ -f ~/.bashrc ]] &amp;&amp; . ~/.bashrc That person you got your .bashrc from is perhaps doing something similar.
Wow. TIL.
Looks like they're looking for ip:port in the addr variable
Actually they’re checking to see if it’s an IP6 or IP4. 
Yeah, technically it's a use of wildcards, whereas globbing specifically refers to pathname expansion, but since globbing is a specific use-case of wildcards, they tend to be used pretty interchangeably. Wildcards are insanely useful (especially for string validation like in your post) . Some relevant pages from my go-to bash resources (ordered from least to most nitty-gritty, IMO) * [The Bash Hackers Wiki | Patterns and Pattern Matching](http://wiki.bash-hackers.org/syntax/pattern) * [Greg's Wiki | Patterns](http://mywiki.wooledge.org/BashGuide/Patterns) * [The Linux Documentation Project | Wildcards](http://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm)
That's awesome! Lol
I'd strongly recommend an XML-centric utility. (This sub usually wastes very little time suggesting `jq` anytime someone tries to parse JSON with regex, so I don't see why this is any different) I use `xmllint` and `xsltproc`so I'll vouch for them. ( `xmlstarlet` is another option that I hear recommended often, but I still haven't tried it. ) You should be able to use the following one-liner to get a list of just the **link** elements: curl -s "http://showrss.info/user/121163.rss?magnets=true&amp;namespaces=true&amp;name=null&amp;quality=anyhd&amp;re=null" | xmllint --xpath "//link/text()" - | sed -r "s/(magnet:)/\n\1/g" 
To make it all a variable, you can do cmd=“mysql -u $UserName —password=$PassWord”. And call it like this ${cmd} Note there are no spaces between “cmd”, the equal sign, and the first quote in the variable assignment. You can’t have spaces there.
I will always upvote a reference to that StackOverflow answer. :)
Somehow OP found the perl getopts documentation. 
I wouldn't trust myself. What if I wanted to trick myself?
With bash you usually do this kind of thing by handing the job over to a different command line tool. Bash is then only there to maybe do some logic, like check if a file exists or not. You can experiment with everything manually at the command line before you write your bash script. Everything that works in a script also works at the normal bash prompt. You then need to think about what command line tool might work to edit your file. The traditional Unix tools like 'sed' or 'awk' treat a file as simple text. They can search for a certain text pattern and then do something at that line in the file, can edit the line or insert new text etc. Nowadays with file formats like XML, there are specialized tools that understand the XML structure. Those tools are harder to learn and they are often not installed by default. How is your XML file formatted? A text tool like 'sed' is usually pretty bad about things that span several lines of text. It can only be used easily if it's about finding and editing single lines of text. Are the things you want to change in the file just a single line of text somewhere or are there line-breaks in that XML element you want to edit? About a specialized XML tool, search for `xmlstartlet` and see if you can find a tutorial you like for it.
It's multiple lines of text, I think I'll maybe just use bash as a existence check like you suggested and then pass it on... I'm more familiar with python so maybe I'll see if I can do something with that since it's pre-installed on most Linux distros.
There are possibly three solutions, the first is simple, but dumb: 1. You will use bash + sed and because you will modify text formatting. You can possibly has some stencil to be able to simple generate output data. You can use regex to find and replace some text, but do not try to parse XML with regex, capisce? The second solution is more advanced and smarter: 2. The is XSL Transformation language which defines transformation of XML file, from CLI you can call it with xsltproc utility. The XSLT language is very verbose and when I was doing some very simple modification of XML file, I have spent a lot of hours just to figure it out. The third one is probably the best option, and if you are already familiar with perl or any other higher scripting language: 3. You will use libraries which that language provides, you will deserialize XML to some advanced data structure, modify that structure and convert it back to XML.
Can you share an example file and show what you would want to search for and change in there? I'd like to play around with a Perl one-liner to learn a bit more about Perl and its regex engine. You edited your post and added that you found out that regex is a bad idea, which is definitely true. You can still always try to cheat your way through these kinds of problems with regex. This might be easy to do without looking super terrible, but this depends on what your files are like exactly.
aw man I was wrong :( thank you for the links
No prob
I did what you told me to and it worked... which i dont know why because it has always worked before on this [site](https://regexr.com/3uf69) that i would test my regex at.. Anyways thanks a lot man!
cool site, but i use this; [https://regexr.com/3uf69](https://regexr.com/3uf69). and the regex that wrote for some reason works at both of the sites. I guess bash is just much more strict when it comes to regex 
Interesting. I'm not sure how that site works on the back-end, but if the creator coded it by hand rather than using an actual RegEx program, they may have forgotten to make hyphens special characters inside square brackets. 
I tried that pattern in Perl, and its regex engine doesn't complain. It doesn't see a mistake. Trying different input, it seems to me Perl treats that `-` in the pattern as the `-` text character, not as a range. I tried input like `hello-world` and `hello_world` and those texts both matched.
`:0,-1r MYFILE` seems to work fine for me.
I don't know
To pick valid email addresses you should consider include the characters `.` (dot) and `+` (plus sign) too.
To pick valid email addresses you should consider include the characters `.` (dot) and `+` (plus sign) too.
Vim only uses the `read` to do inserts linewise, not at a column. You can insert on a specific line using `:0read filename` to top of current buffer, or `:.-1read filename` to the line just before your cursor position. Another option is insert a line break, read in the file, then delete the line break. 
i would try to implement Command Line Options for more flexible use also like said before, try splitting up the script in functions also for DB Backups u might wanna be able to specify a destination folder. Also if you are interested in PITR u might also want to have the Binlog File &amp; Binlog Position corresponding to the backup you just did. If it's a single MariaDB Instance hosting multiple databases u also might want to be able to do PITR for every DB Itself Maybe implement a PITR helper? just some quick thoughts :) nice work for the beginning Greetings &amp; have a nice one &amp;#x200B;
doesnt "." include characters like "\[{)&amp;%..." also? I dont know if you can use that stuff in emails 
Right, you need quote the dot, like `\.` Also, see a more detailed especification at [Wikipedia](https://en.m.wikipedia.org/wiki/Email_address#Syntax)
Non-Mobile link: https://en.wikipedia.org/wiki/Email_address#Syntax *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^223716
[This one](https://www.mailboxvalidator.com/resources/articles/acceptable-email-address-syntax-rfc/) may help too.
` cat list | sort -k4 | head -n1` should do the trick
I'd probably go the lazy path and use date to convert the runtime to seconds and just compare that. If you want pure bash you could just take the hours,minutes,seconds and multiply them by foo to get the same result. Untested, but should do what you want: shortest= minlength= while read -r line; do length=$(date '+%s' --date="1/1/1970 $(cut -d: -f2- &lt;&lt;&lt; $line)") if [[ -z ${minlength} || ${length} -lt ${minlength} ]]; then minlength=${length}; shortest=${line}; fi; done &lt; foo; echo "${shortest}" 
If the "Movie xx" structure is persistent. If not, the field numbe would be different for each line. &lt;list sed -En 's/(.*)([0-9]{2}:[0-9]{2}:[0-9]{2})$/\2\1/p' |sort -k1 | sed -En 's/^([0-9]{2}:[0-9]{2}:[0-9]{2})(.*)/\2\1/p' | head -n1 With this sed magic, the movies will be sorted independet from the prefix.
That really wouldn't be a problem. The list can be modified to just be the timestamps. 
Yeah, your pattern works with "Perl" style regex. Perl does not complain at all about that pattern you used. It will treat the "-" as the normal text character. The Perl documentation still says you are supposed to do this differently. They write this: &gt; Within a list, the "-" character specifies a range of characters, so that "a-z" represents all characters between "a" and "z", inclusive. If you want either "-" or "]" itself to be a member of a class, put it at the start of the list (possibly after a "\^"), or escape it with a backslash. "-" is also taken literally when it is at the end of the list, just before the closing "]". They seem to want you to use one of these three versions here: [a-zA-Z0-9\-_] [-a-zA-Z0-9_] [a-zA-Z0-9_-]
This works perfectly, thank you!
thanks a lot man. perl style regex seems to be less strict. it also accepts this version; [a-zA-Z\-\_] which is what worked for me with awk.
If you control the structure of the list, putting the time first would make this pretty damn easy. Parsing text from a script is a lot easier when all the fixed-length fields are at the beginning of the line and the variable-length fields at the end. 
Sorry to be "that person" but this is a wasteful use of cat... Sort can provide the function of cat already.
More like if [ "$(cat /sys/class/thermal/thermal_zone0/temp)" -gt 7000 ]; then poweroff ; fi
"test" is also the same as the left bracket, and doesn't need the right bracket. Combine that with a short-circuit "and", and you get... test "$(cat /sys/class/thermal/thermal_zone0/temp)" -gt 70000 &amp;&amp; poweroff 
So far I have the length needed, in 3 variables chained together: $nh:$nm:$ns The filler files are located in /mnt/media/video/[subdirectories sorted by type], and a local mySQL database listing the filenames and lengths. And I have no idea how to proceed.
Trying to fill a container with objects to achieve an efficient arrangement is called the Bin Packing Problem: https://en.m.wikipedia.org/wiki/Bin_packing_problem I'd recommend picking an algorithm (from memory, "first fit descending" is pretty efficient while being very simple) and implementing it in Bash. There's probably implementations of the most common algorithms in Python or similar, which should be easy to convert. 
Non-Mobile link: https://en.wikipedia.org/wiki/Bin_packing_problem *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^223796
**Bin packing problem** In the bin packing problem, objects of different volumes must be packed into a finite number of bins or containers each of volume V in a way that minimizes the number of bins used. In computational complexity theory, it is a combinatorial NP-hard problem. The decision problem (deciding if objects will fit into a specified number of bins) is NP-complete.There are many variations of this problem, such as 2D packing, linear packing, packing by weight, packing by cost, and so on. They have many applications, such as filling up containers, loading trucks with weight capacity constraints, creating file backups in media and technology mapping in Field-programmable gate array semiconductor chip design. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Fine, I edited the link, you robot bastard.
I’d suggest using `echo` instead of `sleep` though, so you might see an effect. Or maybe something like `(sleep 3; echo “an effect”)`. As it turns out, `coproc`’s stdout and stdin are not connected to the terminal, while those of a background job are. *Do* try this at home! Also, see `man bash` (specifically the `List` subsection of the `SHELL GRAMMAR` section) and `help coproc`
&gt;if \[ "$(cat /sys/class/thermal/thermal\_zone0/temp)" -gt 7000 \]; then poweroff ; fi Sadly even though this is through crontab, I see the following errors: &amp;#x200B; Oct 30 00:43:01 hostname CRON\[8457\]: (user1) CMD (if \[ "$(cat /sys/class/thermal/thermal\_zone0/temp)" -gt 7000 \]; then shutdown now ; fi) Oct 30 00:43:04 hostname CRON\[8453\]: (user1) MAIL (mailed 32 bytes of output but got status 0x0001 from MTA#012) &amp;#x200B;
I tried this with sudo crontab -e also to launch it as root but doesn't work.
This is what I've used finally as part of a root crontab (sudo crontab -e) &amp;#x200B; [ "$(&lt;/sys/class/thermal/thermal_zone0/temp)" -le 7000 ] || /user1/1 &gt;&gt; /user1/1.txt &amp;&amp; cat /user1/1.txt | mail -s "EMERGENCY_SHUTDOWN_INITIATED" name@domain.com &amp;&amp; sudo service apache2 graceful-stop &amp;&amp; sudo umount /mnt/nxtcld &amp;&amp; sudo poweroff the bash script at /user1/1 will capture current CPU &amp; GPU temperature. &amp;#x200B; 1.txt is configured to capture temperature once and hour. So the idea is to send the entire file to me before shutting down. &amp;#x200B;
`cron` uses the `sh` shell so the `$(..)` won't work. You either need to replace the dollar brackets with backticks \` or set the shell to bash by adding `SHELL=/bin/bash` to the line above the crontab entry. 
Ok so first off stop calling yourself kid code lmao. Next, can you do all of these things independently without tying them together? Loop over a text file, make a user, etc. Break it into chunks and see if you can answer each little chunk. Attempt to make a script to do this and then ask for help again after showing a proper attempt.
Or let cron execute a script with bash as shebang.
On a related note, one of the best things I came across was “shellcheck” for syntax checking/linting/etc my Bash scripts. It’s usually available via a distro’s package manager (or Brew) but I think you can just clone it from GitHub too. Another reference I use all the time is http://wiki.bash-hackers.org/start.
Aliases are by default disabled in non-interactive shells, though, so that shouldn't affect usage in scripts.
* You could get rid of `.bash_profile` and just source `.bashrc` at the end of `.profile`. * Do you want unlimited history instead of just very large history? If you set (in a reasonably recent Bash) `HISTSIZE` and `HISTFILESIZE` to a negative value, the retained history will be unlimited in size. * Instead of [ -f ~/.bash.d/__prompt_command.sh ] &amp;&amp; source ~/.bash.d/__prompt_command.sh [ -f ~/.bash.d/setproxy.sh ] &amp;&amp; source ~/.bash.d/setproxy.sh [ -f ~/.bash.d/printargs.sh ] &amp;&amp; source ~/.bash.d/printargs.sh (and similar in many places), you could do something like for fname in ~/.bash.d/{__prompt_command,setproxy,printargs}.sh; do [ -f "$fname" ] &amp;&amp; source "$fname" done * The startup files are always sourced and not run as scripts; they don't need a shebang line. * I don't understand the purpose of the temporary prompt in `.bashrc` * If you want your multi-line commands not just stored as a single command (as you do with `cmdhist`), but in multiple lines, you could use `lithist`) additionally. Since you use `HISTTIMEFORMAT`, that'd work. Matter of taste, though. * In my `.profile`, I stick to POSIX sh, as this might be sourced by any shell; for `.bashrc`, though, I use everything Bash has to offer, including `[[ ]]` everywhere, arrays (if required) and so on. Your test for an interactive shell uses `[[...]]` already, why not use `[[ ... ]]` everywhere else in `.bashrc` as well? All in all, very clean, I'd say. My suggestion are mostly bikeshedding, but isn't that half the fun in dotfile fiddling? ;)
Then get Ansible
Unfortunately, compgen seems to be a Bash-specific function and I'm using Zsh on my main laptop :(
i missed a comma, `while IFS=, read name group; do` `if ! grep -q "$group" /etc/group; then` `group add "$group"` `fi` `useradd -g "$group" -m "$name"` `echo -e "$name\n$name" | passwd "$name"` `done &gt; Usernames.txt` now Reads `' is not a valid group name` `' does not existvisitor` `passwd: user 'ellipsiscoterie' does not exist.` &amp;#x200B; and that continues for each line
Thanks for the awesome advice! &gt; You could get rid of .bash_profile and just source .bashrc at the end of .profile. Like you mentioned in a later point, `.profile` could be sourced by another shell. So shouldn't I avoid having a line in there that applies to just bash? I supposed I could check if the shell is bash before doing the source though. Do you know how I would do that? &gt; The startup files are always sourced and not run as scripts; they don't need a shebang line. I put those lines in there so that I could run `shellcheck` on them without it complaining: `^-- SC2148: Tips depend on target shell and yours is unknown. Add a shebang.` I assume that they are harmless to leave in there. &gt; I don't understand the purpose of the temporary prompt in .bashrc `~/.bashrc` takes a second to execute (because I source `"$NVM_DIR/nvm.sh"` which is a slow script for nodejs that I'm not in control of). During that second, I noticed that I can technically still press keys and they are "buffered" and interpreted by bash once it is ready. The temporary prompt is there to give those buffered key-presses a nice place to be displayed. See this [gif](https://imgur.com/a/TIhpqSS) &gt; bikeshedding, but isn't that half the fun in dotfile fiddling Yes! :) 
Thank you, I've installed `shellcheck` via homebrew.
Everyone is unique. I like to make all of my common typos into aliases. alias cp-i='cp -i' alias sl='ls' alias ls-l='ls -l' alias sl-l='ls -l' \# This one's just funny alias peinrd='printf' These have likely saved me several hours of lost productivity, just for stupid muscle-memory typos. I'm not any worse on a system without these aliases setup. But I am much better off for having them on my work and home systems.
 autoload bashcompinit bashcompinit Et voila! `compgen` works now. http://zsh.sourceforge.net/Doc/Release/Completion-System.html
Hah, forgot what Sub this was. Thanks again, I'll try that. 
There's a tool named "perl-rename". Besides renaming files, it can also move things into different directories. It will create the target directories if they don't exist. Here's how using it would look: First, my example directories and an example file: $ ls -F -1 stringa date1 stringw string10/ stringb date2 stringx string11/ stringb date3 stringy string12/ stringc date4 stringz string13/ test file here Here's the perl-rename tool run on those: $ perl-rename --dry-run 's{^\S+}{/your/path/here/$&amp;/$&amp;} if -d' * stringa date1 stringw string10 -&gt; /your/path/here/stringa/stringa date1 stringw string10 stringb date2 stringx string11 -&gt; /your/path/here/stringb/stringb date2 stringx string11 stringb date3 stringy string12 -&gt; /your/path/here/stringb/stringb date3 stringy string12 stringc date4 stringz string13 -&gt; /your/path/here/stringc/stringc date4 stringz string13 The rule used is this here: s{^\S+}{/your/path/here/$&amp;/$&amp;} if -d The `s{}{}` is a different way to write `s///`. This is the same `s///` command that you might know from the 'sed' tool. The `$&amp;` is what the search pattern had matched in the first half of the `s///`. The search pattern was `^\S+` which will find your "StringA" etc. part of the directory names. The `\S` in the search pattern means "everything that's not a space character". The `if -d` you see at the end of the rule tests if the file name is a directory.
After going through many options I've concluded in this sort of arrangement ... [.bashrc.d](https://github.com/MichaelTd/dots/tree/master/dot.files/.bashrc.d) by looping through this directory like so ... ```shell brcd="${HOME}/.bashrc.d" if [[ -d "${brcd}" ]]; then # Load files from ~/.bashrc.d for file in ${brcd}/*; do source "${file}" done fi ``` you avoid having to name each file in your .bashrc. Hiding a file is done, well like you'd normally make a hidden file, by prefixing it with a dot.
&gt; while IFS=, read name group; do &gt; done &gt; Usernames.txt `&lt;` 
That's true. Thank you for pointing that out. I'm using it for testing. :) 
Thank you this was very helpful ! I've removed the quotes. 
The simplest thing to do is to simply move your output file over the top of your original. The more elegant option is to use something like `sed` to match and delete. It's kinda late here and I'm tired, so this may be way off: ( cd "$HOME/scripts/y2b/ || exit 1 while read -r; do sed -i "/${REPLY//\//\\/}/d" youtube.txt done &lt; &lt;(grep --fixed-strings -vf dictionary.txt youtube.txt) ) So the idea is to invert `grep` to show us the lines we don't want, `read` those into `$REPLY`, try to escape all the slashes in the variable that we come across (so that `sed` doesn't interpret them), then tell `sed` to find and destroy. ░▒▓# while read -r; do &gt; sed -i "/${REPLY//\//\\/}/d" youtube.txt &gt; done &lt; &lt;(grep --fixed-strings -vf dictionary.txt youtube.txt) ░▒▓# cat youtube.txt https://www.youtube.com/user/PewDiePie https://www.youtube.com/user/tseries Seems to work. I believe `awk` would provide something more readable
 awk ' FNR==NR {yw[$1]=$0} {youtube[$1]=$0} END{ for (x in youtube){ p=1 for (y in yw){ if (x==y){ p=0 } } if (p==1){ print x } } }' yw.txt youtube.txt this stores the content of the first file in the array yw and the content of the second in the array youtube. After that it checks for each item in youtube if there is too in yw and so it sets the variable p to print or not. You can then redirect `&gt;` the output to a file, but do not use youtube.txt!!! because then it would destroy the file it is reading from, leaving an empty file as a result. So you use the command and redirect it to for example temp.txt and then move it to youtube.txt, like `&gt; temp.txt &amp;&amp; mv -f temp.txt youtube.txt`
&gt; The simplest thing to do is to simply move your output file over the top of your original. &gt; I want to "ditto" this idea, OP. There are utilities that can delete lines from a file (such as `sed` with the `d` command). The problem is that your patterns in your dictionary match "positives" -- the ones you want to keep. If your dictionary of criteria contained "negatives" -- lines that should be omitted from the output -- it would be only natural to use a utility that deletes lines. To delete out lines that fail to meet a list of criteria is basically an "obtuse" use of the syntax. Being obtuse in spoken languages can offer some advantages, such as emphasizing someone's strong points when you want to give them constructive criticism. However for programming languages, it's usually just a chore for both the programmer and the underlying software that has to parse the commands and perform the actions. In short, /u/DivineArcade, I think the way your script works now is superior to the way you want to do it. If the issue is too many files, then I think overwriting the file you don't want is the most elegant answer.
Sorry I didn't think to put it in the OP, but I also need to put a sleep command in the while loop which impacts your script. Here's the gist of my code: cycles=0 duration=5 while true; do test "$cycles" -eq "$duration" &amp;&amp; exit read -rsn 1 test $REPLY == 'q' &amp;&amp; break &lt;Other script stuff, echoing to a file etc.&gt; let cycles="$cycles"+1 done
Oh, for the slow `nvm.sh`, there is a `--no-use` flag that makes startup much faster (postpones running the script until you do `nvm use`).
no need already solved it. goal came to my head just needed unique lines between the new yw.txt and the youtube.txt
&gt; I put those lines in there so that I could run shellcheck on them without it complaining: Use `# shellcheck shell=bash` instead
Do you mean right to left text alignment? Or register transfer level coding? Or something else?
I think he is talkin' about the largest private network in Germany. 
Fundamentally, a terminal displays a grid of characters. There's no clear "right" way to implement RTL support within a terminal; the interface simply wasn't designed for complex scripts. It's hard enough to get ligatures to work as you'd expect!
Shit I always use 'function x {}', is it that detrimental?
I think he's talking about rtlsdr, the realtek TV dongles that were hacked for software defined radio.
Your particular example doesn't benefit from using `bash -s`. Where `bash -s` can be useful is when you've got some script on your local machine, and you want to run it on the remote machine, possibly with some command-line arguments. Take this simple example: $ cat example #!/bin/bash echo "Host: $HOSTNAME" printf '&lt;%s&gt; ' "$@" echo This script just prints out the hostname and the list of arguments. (I've surrounded each argument with angle-brackets to verify that it does not perform word-splitting where it shouldn't.) So on my local machine I have: $ ./example abc 'multiple words' xyz Host: local.example.com &lt;abc&gt; &lt;multiple words&gt; &lt;xyz&gt; Now say I want to run this script with the same arguments on a remote machine, and the remote machine doesn't actually have the script installed. The correct incantation for this is rather tricky: $ ssh remote.example.com 'bash -s -- abc '\''multiple words'\'' xyz' &lt;example Host: remote.example.com &lt;abc&gt; &lt;multiple words&gt; &lt;xyz&gt; Let's pick this apart bit by bit. First, why use the `-s` option at all? This option forces Bash to read commands from standard input (here redirected from `example`) as if it were a shell script. Without this option, Bash would treat the first non-option argument (`abc` in this example) as the name of the script to execute, which is certainly not what we want. Next, why do we use `--`? This ensures that any _subsequent_ arguments are simply passed through to the script being executed, rather than being interpreted by Bash itself. After this we have the script's own arguments. You might think that you can simply pass them into SSH as separate command-line arguments, but if you try this it won't work correctly: $ ssh remote.example.com bash -s -- abc 'multiple words' xyz &lt;example Host: remote.example.com &lt;abc&gt; &lt;multiple&gt; &lt;words&gt; &lt;xyz&gt; The problem here is that SSH [concatenates all arguments into a single command string](https://github.com/openssh/openssh-portable/blob/595605d4abede475339d6a1f07a8cc674c11d1c3/ssh.c#L1061-L1067). This is extremely annoying, but [the SSH protocol](https://tools.ietf.org/html/rfc4254#section-6.5) requires it. So we need to do some kind of extra quoting on the client end so that the single-string command works correctly. We can wrap this up nicely as follows: remote() { local host=$1 binary=$2 shift 2 ssh "$host" "bash -s -- $(printf '%q ' "$@")" &lt;"$binary" } And this works pretty well: $ remote remote.example.com example abc 'multiple words' xyz Host: remote.example.com &lt;abc&gt; &lt;multiple words&gt; &lt;xyz&gt; There is one significant limitation to this technique though: it uses up the standard input file descriptor. That is, it's not possibly to do: $ some pipeline | remote ... or: $ remote ... &lt;filename I can't see any clear way to work around this.
So... what appears to be going wrong? You've told us what you expect to happen, you haven't told us what actually is happening... Some comments though: sudo addgroup visitor sudo addgroup staff You're in a script that obviously needs to be run with appropriate privileges, so you can drop the `sudo` calls. You need to add a check like this at the start of your script: if (( EUID != 0 )); then printf '%s\n' "You must be root to run this script" exit 1 fi Next: -d "/home/$name" Assuming Linux, `/home` is where the homedir is going to go, so this is unnecessary. There are smarter ways to automatically figure out where the home base is, but that seems to be outside the scope of your requirements. Removing that leaves us with this: useradd -m -c "$name" Ok. I see a problem. NAME useradd - create a new user or update default new user information SYNOPSIS useradd [options] LOGIN ... -c, --comment COMMENT Any text string. It is generally a short description of the login, and is currently used as the field for the user's full name. You've given the username variable to the comment field of `useradd` but not actually told `useradd` which login to act on. useradd -m -c "${fullName}" "${userName}" See how that's different? Fix that problem and try your script again. 
See: http://wiki.bash-hackers.org/scripting/obsolete 
Cheers! I managed to find a solution last night involving forking the main process to the background and then starting read, but your approach is much more compact! Didn't think to use the read timeout as a timer. 
He is clearly talking about Register Transfer Level, the design language of digital hardware.
Why would you want to? I really want to understand.
Right to left
Any work that involves rtl strings needs it🙄 Why do you use strings of English?
Your data probably has windows-style line endings (CRLF). See [BashFAQ 52](http://mywiki.wooledge.org/BashFAQ/052) on how to deal with them.
I wrote this in vim on my centos 7 machine. &amp;#x200B; $ file SCRIPT3.0.bash SCRIPT3.0.bash: Bourne-Again shell script, ASCII text executable &amp;#x200B; $ dos2unix SCRIPT3.0.bash dos2unix: converting file SCRIPT3.0.bash to Unix format ... &amp;#x200B; $ file SCRIPT3.0.bash SCRIPT3.0.bash: Bourne-Again shell script, ASCII text executable &amp;#x200B; &amp;#x200B; &amp;#x200B; $ sed -n l SCRIPT3.0.bash \#!/usr/bin/bash$ $ $ awk '{print $2}' mobile | uniq | sort -s | uniq | while read line$ do$ \\techo $line$ \\techo "curl -X POST -d api\_id=&lt;REMOVED&gt; -d api\_key=&lt;REMOVED&gt; -d site\\ \_id=&lt;REMOVED&gt; -d name=Redirect$(date +%Y%m5d%H%M%S) -d action="RULE\_A\\ CTION\_REDIRECT" -d response\_code=301 \\\\\\\\"$ \\t\\tgrep -wF $line mobile | while read newline$ \\t\\tdo$ \\t\\tSRCURL=$(echo $newline | awk '{print $1}')$ \\t\\techo "-d from=\\\\"$SRCURL\\\\" \\\\\\\\"$ \\tdone$ \\t$ \\techo "-d to=$line $incap"$ \\techo " "$ \\techo " "$ $ sleep 5$ $ done$ $ &amp;#x200B;
Not the script, the file you are parsing, `mobile`.
I was just checking that. Looks like each line looks like this, &amp;#x200B; SRCURL\\tDSTURL$ &amp;#x200B; Strangely, it's writing the DSTURL as if there's word wrap. SRCURL\\thttps://www\\ .domain/locatio\\ n/more location\\ file.html$ &amp;#x200B; &amp;#x200B;
AH HA! I ran dos2unix on mobile and now my script seems to be working. &amp;#x200B; &amp;#x200B; Thank you very much for your responses!
&gt; Why do you use strings of English? I typically don't in terminal, this would be why I asked why. Thanks I guess.
Dictionaries, utilities made for non-English people, sending messages (a la wall), scripted manipulation of rtl text files, greping, ...
This is an absolutely amazing and in-depth response! Thank you so much for taking the time to write this!
While that is true, I think u/fredzannarbor was referring to a situation like this: if the script that you borrow (or copy into your bash startup files) creates a function that you can invoke from interactive bash, the alias still applies. Example: In your bash startup files, if you create this alias: ``` alias ls='echo "no"' ``` Later on in your bash startup files, you source someone's script that creates this function: ``` function my_ls() { ls } ``` When you type `my_ls` in an interactive git, the alias applies. However, if you do it in the reverse order, the alias does not apply. So my recommendation is, you can create aliases, but do it near the end, or at least *after* you source external scripts. 
Try using the `tee` command between `tail` and `wc` to get a look at what is being sent to `wc`.
wc doesn't miscount. It's just data you were not expecting ; in this case; it's wc itself that's contributing to the process count. i.e; 'ps' shows 3 items. 'ps | wc -l' shows the same three items but ALSO shows 'wc' as its a running process. :~$ ps PID TTY TIME CMD 19886 pts/79 00:00:00 bash 19993 pts/79 00:00:00 ps :~$ ps | cat PID TTY TIME CMD 19886 pts/79 00:00:00 bash 19994 pts/79 00:00:00 ps 19995 pts/79 00:00:00 cat
Yes I just realized that after following the advice from u/bfcrowrench/, after I tee'd it to an output file. Anyways, thank you good people for help, much appreciated, I am closing this thread. 
You need to also show us, how is that alias defined. That debug output looks weird, there is also that Python exception mixed in. Your shell is probably not bash, but sh. And you need to check the environment variable PATH: echo $PATH It contains colon-separated directories with executables. Is it possible that you are running your script/alias without PATH available?
Is the alias inside an app? If that case have you tried running it inside the directory with “cd” command? Can you confirm the alias points to the right address, it exists and have the permissions to be exec? Maybe make new alias? Can you show your code entry?
Also, when running the script do this for debugging: bash -x script-name.sh
You can reverse a file with `tac` (that's `cat` backwards). I would then use `sed` instead of grep to cut away the start of that reversed file until that text you want to match. That would look like this: tac filename | sed -r '1,/texttomatch/ d' You can then also try to add the search for that line with a path to this 'sed' command. The following might work: tac filename | sed -nr '1,/texttomatch/ d; /^\// { p; q }'
You can also do it in `sed` without `tac` by holding each path you see: sed -n -e '/^\\/h' -e '/texttomatch/{;g;p;q;}' filename
Thanks! This is my alias alias houdini_run='pathold=$PATH; PATH=/home/jim/yt-conda/bin:/home/jim/yt-conda/sbin:/opt/hfs17.0.352:/opt/hfs17.0.352/bin:/usr/bin:/usr/sbin/;/opt/hfs17.0.352:/opt/hfs17.0.352/bin/houdini-bin; PATH=$pathold'
Thanks! So if I’ve created an alias called houdini_run I would call bash -x houdini_run ?? 
In the sed command you demonstrated, how would I use a variable name instead of 'texttomatch'? No matter what I try it doesn't seem to work. Thanks
Like this: sed -nr '1,/'"$file_name"'/ d; /^\\/ { p; q }' If you look closely, what's being done here is that the argument to sed is built out of three parts: '1,/' "$file_name" '/ d; /^\\/ { p; q }' The first and the last parts use `'...'` quotes, and the part with your variable name use `"..."` quotes. You will have problems with this if there are characters inside $file_name that have a special meaning in patterns for sed. It will for example break if there is a `/` character in the variable. A `.` character will also be problematic. I think in that case you need to switch to a totally different tool.
Yes, the text to match is a filename.ext
There's a semicolon trying to pass itself off as a colon in there. Also, no point in the pathold variable when you pass PATH as a temporary environment variable alias houdini_run='PATH=~/yt-conda/bin:~/yt-conda/sbin:/opt/hfs17.0.352/bin:$PATH houdini-bin' or as a function houdini_run() { PATH=~/yt-conda/bin:~/yt-conda/sbin:/opt/hfs17.0.352/bin:$PATH houdini-bin } Another option is to simply add those dirs to your PATH in ~/.profile or ~/.bash_profile and run houdini-bin directly
Thanks! When I do that just down to 1 error cat: /sys/devices/virtual/dmi/id/board_{vendor,name,version}: No such file or directory
Hard to say without seeing the corresponding code, but it looks like it's an sh script doing bash-isms.
$ who This will tell you who is logged in and how they are connected. I think someone at the machine, using gnome dm on ubuntu will be connected at tty7 (you will need to check that). I'm not sure of an easy way via process. 
$ ps -ouser -p 11319 
`... grep gnome|session`. But there are a lot of other more reliable ways to get the current user.
You could check if it’s set to `1`, that might help: `if [[ “${cur}” -eq 1 ]]` Or the looping might not go as you expected, which is probably the case. Check this: https://stackoverflow.com/questions/1521462/looping-through-the-content-of-a-file-in-bash#1521498 Bash has *a lot* of gotchas
That helped! Thank you. :)
When I run manually, I am able to use a single `-i` and a wild carded file name (as in my example at the start of the post). If the issue is that I need to identify each input file individually, then I fear the coding is far beyond my meager skills! Still, thanks for your comment. You have given me something else to add to the 'must learn' list. :-)
You can build up an array: inputs=() for file in "$Day8"*.avi; do [[ -e $file ]] || continue inputs+=( -i "$file" ) done if (( ${#inputs[@]} &gt; 0 )); then avimerge "${inputs[@]}" -o "$outFile" fi 
While the previous comment about checking for equality to 1 is the right way to go, I'd be curious to see if changing 'cur=' to 'unset cur' makes any difference.
Didn't make any difference for me. :(
Good to know. Thanks for giving it a try!
Cool! Thanks! :)
Thanks for the detailed response. I learned several new things :)
Perhaps use `\e` instead of `\033` so that things are a bit easier to read: PS1='\[\e[36m\]\u\[\e[m\]@\[\e[32m\]\h:\[\e[33;1m\]\w\[\e[m\]\$ ' Use this line to start your own experiments, not the version with `\033` codes in it. The `\e` code has the same meaning as `\033`. It is called "escape". It is the same code as what the Escape key on the keyboard sends to terminal programs. Here is a short script that prints example text with color codes so that you can see what codes create which color: http://ix.io/1qRx Save it as for example "colortest.sh" and run it. You will see text like `1;33m` on the left side. When you use those codes to print color text, add a `\e[` in front of it, for example: echo -e '\e[1;33mThis here is yellow text. \e[mThis here is back to default colors.'
Yup, I use it as a reference when I'm stumped. I'd never gotten the hang of working with multiple lines though.
&gt; You don't even need the dollar sign or braces inside the square brackets. That is completely wrong.
Thanks for explaining me that 033 is for esc, not color code. I tried another way and get strange output, you have an idea what it going wrong? PS1+="\\\[$(tput setaf 20)\\\]\\h"; #mashine blue colour PS1+="\\\[$(tput setaf 15)\\\]\\@"; #white at sign PS1+="\\\[$(tput setaf 9)\\\]\\u"; #red user PS1+="\\\[$(tput setaf 226)\\\]\\W"; #yellow dir PS1+="\\\[$(tput setaf 15)\\\]\\h"; #everything else username:\~ mashinename$ username10:17 AMmashinename\~username If I remove comment out those line I get: username:\~ mashinename$ //this looks weird, maybe I changed something and forgot. However, I have nothing else in the .bash\_profile &amp;#x200B; Thank you.
but it doesn't print out the contents
An alternative suggestion to u/thestoicattack’s comment: find . -name '*.txt' -exec head -n1000000 {} + With `+` instead of `\;` terminating the `-exec`, `find` will call the program (`head`) with multiple arguments, one per file, instead of spawning a new copy of the program for each file. And when `head` is called with more than one file argument, it will print a header before each file. (This header is somewhat fancier than `find`’s bare `-print` option.) However, if there’s only one file in total, or if `find` has to split the command lines because there are too many files to fit in a single argument list and the last argument list happens to only contain one last file, `head` will omit the header. We can fix this by adding the `-v` option (“verbose”). Also note that this won’t work if a file has more than one million lines – increase the `-n` argument in this case. Also, it might be a good idea to add `-type f` to the `find` invocation somewhere before the `-exec`.
Oh, so that means I have to have a dedicated virtual machine for this? Can you still compile shell file with Ubuntu on windows?
My answer was more a joke than other. However, I'd try 'whereis clear'. If no output given, then you'd need to install a VM probably. If there's an output add the path in your .bash\_profile.
Okay, I think I'm going to need to get a virtual machine working, because now the error is this: :lear No idea what is happening, thanks for your response though :)
Your script has windows line endings, which is why bash thinks the first line is `$'clear\r'`, and `$'name\r'` isn't a valid identifier.
Alright!! Setting ff=unix fixes everything, it's working now. Thank you so much :D I guess I'll have to use vi from now on
 shopt -s globstar; tail -n+1 **/*.txt Format customization will need something like `-printf '#### %P ####\n'` as part of /u/stoicattack's suggestion.
`tail -n +1 *.txt` Change the file extension to suit your needs.
this one is also very nice and simple. it does output the whole files contents right?
Thank you tuxmaniac, veeery cööl.
i think it also works with head -n -1 too. why does this happen? how does this addition and subtraction by one make the whole content show?
you are a pro. thank you. that was very informative. I learned much more than i was planning to, thanks man.
If not, then maybe the [yes command](https://unix.stackexchange.com/questions/102484/what-is-the-point-of-the-yes-command) may be helpful. Admittedly, I've no experience with it but from what I've read, it may be worth looking into. 
&gt; for archive in "$(find $loc -name '*.rar')"; do This only works if there's exactly one rar file found. Never use a for-loop to iterate the output of a command. You use a while loop find /mnt/downloads -name "*.rar" -print0 | while IFS= read -rd '' archive; do ... done 
PS1 belongs in .bashrc, not .bash_profile, but apart from that, it looks somewhat sane. You forgot to empty PS1 before starting the appending, and you probably want to have the \$ at the end, and you accidentally put a backslash in front of the @ sign, getting you the current time instead of an @. With some minor adjustments to your lines, this looks right on my end, in an iterm2 that sets TERM to xterm-256color. PS1="\[$(tput setaf 20)\]\u" PS1+="\[$(tput setaf 15)\]@" PS1+="\[$(tput setaf 9)\]\h" PS1+="\[$(tput setaf 15)\]:" PS1+="\[$(tput setaf 226)\]\W" PS1+="\[$(tput sgr0)\]\\\$ " 
I would use something like this. Evaluate if the unrar was successful, if it was print $archive to finished.list to be excluded next time else rm -r $destination. unrar e -x@finished.list "$archive" "$destination" Result=$? if [$Result -eq 0 ]; Then printf "$archive" -&gt; finished.list; else rm -r $destination fi 
thanks that was helpful ) is there a way to do this with the "xargs" command? just curious.
The 'rename' command will rename files according to a substitution regex like so: stu@nook:~/test $ ls jupiter.txt mars.txt saturn.png stu@nook:~/test $ rename 's/\.txt$/.py/' * stu@nook:~/test $ ls jupiter.py mars.py saturn.png steven@nook:~/test $
The easiest way I know of: rename 's/txt$/py/' *.txt The `rename` command is typically not installed by default AFAIK, so `apt install rename` or your distro's equivalent is probably required to make this work. Using only installed-by-default tools, you'd need something like ls *.txt | sed -r "s/(.*)\.txt\$/mv '\1.txt' '\1.py'/" | sh but that's not nearly as easy to use or remember.
xargs just appends a list of arguments to another command, so you can just do: &lt;however you are you are building the list&gt; | xargs rename txt py
Both of your code examples are definitely [*batch* scripts](https://en.wikipedia.org/wiki/Batch_file), not [*bash* scripts](https://en.wikipedia.org/wiki/Bash_(Unix_shell\)). Completely different thing.
gotcha thanks bud &amp;#x200B;
Thank you it worked &lt;3
Thank you. This is not about searching previous commands, though. This is about using vi-mode commands to edit them. Let me know if this example helps a bit (it's not a very good example for several reasons, but I'm just using it to describe things better). If I'm watching my pi-hole log for blocked things in real-time, I might do this: `tail -f /var/log/pihole.log | grep '10.0.0.11' | grep gravity` Then I want to do it again but looking for `'``10.0.0.12``'` by just changing my last command. In tcsh, whether I use esc-k or up-arrow, my cursor is at the end of the line (after 'gravity') and I will use the following keystrokes followed by return: bbbbhr2 (or I can do: 4bhr2). Even if I left out the spaces around the last pipe when I typed it, they will be there when I bring up the command. In bash, I'm started at the beginning of the command instead of the end (if that's changeable, I'd like to know about that, too) so either I can: EEEEEEhr2 (or 6Ehr2) or $bbbbhr2 (or $4bhr2) (honestly, I only tend to use numbers before an action for repeating in actual edits rather than in movement; I'm just including them here to further the example). Whew, follow so far? ;-) So, if in my haste I slipped and missed a space before/after a pipe, that doesn't affect the command. And in tcsh it will be cleaned up to look like the example (therefore, consistent and predictable). Here it is missing that space. `tail -f /var/log/pihole.log | grep '`[`10.0.0.11`](https://10.0.0.11)`'| grep gravity` That changes things to : EEEEEEhhr2 (6Ehr2) or $bbbhr2 ($3bhr2) in bash, but changes nothing in tcsh. This example isn't a big deal. It's a minor difference. I just haven't thought of a better example that both shows what I'm asking and how/why it matters to me. Here is another example to show the shell difference and what I'm asking. These are two similar commands (I prefer and would use the second one). `find /usr/bin -type l -ls | head` `ls -l /usr/bin | grep ^l | head` Then I want to repeat it for `/usr/sbin` and I've missed the spaces around the first pipe. `ls -l /usr/bin|grep ^l | head` In tcsh, they will be added when I bring up the command and I would use 0EEEbis and it would be good to go. In bash, using EEE will put the cursor at the end of grep and not bin so I would end up using EEEbbbis (and the readability of the command is more difficult since part of it is jammed together). &amp;#x200B; I hope my question makes more sense. I want a typed command like `ls -l /usr/bin|grep ^l|head` cleaned up into `ls -l /usr/bin | grep ^l | head` when I bring it up. The predictable and consistent presentation of previous commands simplifies edits \[in tcsh\]. I'm hoping bash has it. Thanks.
ah so.. \#!/bin/bash #!/bin/bash for archive in "$(find $loc -name '*.rar')"; do find /mnt/downloads -name "*.rar" -print0 | while IFS= read -rd '' archive; do destination="${archive%.rar}" if [ ! -d "$destination" ] ; then mkdir "$destination"; fi unrar e "$archive" "$destination" done sorry not very good with bash thanks
You are correct. Using e/E and b/B and w/W jump based on different criteria (bottom-left of [vim cheat sheet](http://www.viemu.com/vi-vim-cheat-sheet.gif) shows that graphically). I do change around which I use based on where I want to move and the kind of edit needed. I can adapt based on the format of the command (did I get all the spaces in or miss some). However, using tcsh normalizes it every time to the same expected format and I'm hoping bash can do it, too. I have "muscle memory" for jumping around to get where I want and having an unexpected missing space throws that off. It's just a small thing overall, but it shows up in my workflow/style a lot and not having it slows down my willingness to adopt using bash as my default shell. We each have our preferences and sometimes we irrationally rank our priorities in ways that are baffling to others. So I apologize if my reasoning seems weak, but it is a feature that is important to me. So, I'm not really looking for a way to work around the difference but a way to make this aspect of bash match what I'm used to using (even though that might not be reasonable). I'm hoping someone can identify the feature itself (in the command-line editor of tcsh, see [man tcsh](https://linux.die.net/man/1/tcsh)) and if it exists in bash (and just needs enabling). Maybe it's part of readline (I've looked there, too, without success). Thanks for trying to understand my request. I appreciate it.
No worries :) But I'm not that knowledgable about the bash shell to know if it could be done anything about it to match your workflow. I guess it comes down to you adapting the workflow to fit around the tool you want to use or change the tool to support your workflow or making your own tool. I would suggest to try IRC and the freenode #bash channel. I find that the people there are usually very knowledgable about bash. But I would try to sort my thoughts out to a short question as in "can bash autoformat commands that I've used after I press enter?". It is a bit much to read through all this to see what you want.
[removed]
I always prefer this: rename 's/.txt/.py/g' *.txt $:~/testing$ ls -l -rw-rw-r-- 1 b b 0 Nov 4 18:52 bar.nottxt -rw-rw-r-- 1 b b 0 Nov 4 18:52 file3.txt -rw-rw-r-- 1 b b 0 Nov 4 18:52 foo.txt -rw-rw-r-- 1 b b 0 Nov 4 18:52 test1.txt -rw-rw-r-- 1 b b 0 Nov 4 18:52 test2.txt -rw-rw-r-- 1 b b 0 Nov 4 18:52 yolo.nottxt $:~/testing$ rename 's/.txt/.py/g' *.txt $:~/testing$ ls -l -rw-rw-r-- 1 b b 0 Nov 4 18:52 bar.nottxt -rw-rw-r-- 1 b b 0 Nov 4 18:52 file3.py -rw-rw-r-- 1 b b 0 Nov 4 18:52 foo.py -rw-rw-r-- 1 b b 0 Nov 4 18:52 test1.py -rw-rw-r-- 1 b b 0 Nov 4 18:52 test2.py -rw-rw-r-- 1 b b 0 Nov 4 18:52 yolo.nottxt 
there’s also this [rename](http://plasmasturm.org/code/rename/) which is great. 
Aha! Wrong sub. Thank you for clearing that up.
Using `basename,` you can achieve this too. `for f in *.txt; do mv "$f" "$(basename -- "$f" .py).txt"; done`
I don't understand what you want to do. Show an example filename and show what command lines you would run manually on that file.
it would be something like : pandoc -s -o finance.pdf finance.md for a single file 
Run the following while in the folder with your .md files: for x in *.md; do echo pandoc -s -o "${x%.md}.pdf" "$x"; done This will only print stuff. It will not actually run those pandoc commands. If what's printed looks good, like what you want to happen, then type the following to actually run those commands: ^echo
thank you ! this coupled with my other command to move all the pdf files somewhere else makes it way easier ! ( would you mind telling me what this part does : "${x%.md}.pdf" "$x" Thanks a lot kind stranger !
That's two arguments for pandoc's command line. The `"$x"` argument is simple: it's the value of the variable `$x`. The `"` quotes are there so that space characters in the file name will work correctly. About that `"${x%.md}.pdf", first you need to know that writing `$x` and `${x}` are the same, it's both accessing a variable. When you use `${x}` instead of `$x`, the `{}` enables a bunch of extra features. One of the extra features is the `%` you see there. That `%` will cut off text from the end of the variable's value. The text that's getting cut off here is `.md`. The `.pdf` you see is normal text that gets added to the argument for pandoc. The final argument is then the value of the `$x` variable with a `.md` text cut off and a `.pdf` text added.
can you go into more detail about ..."also halts the compiler from actually creating an executable"?. gcc itself definitely doesn't care how the shell treats stderr, but I'm guessing maybe I didn't read it quite the way you meant it.
So, just to clear things up `&amp;&gt;` sends both stdout and stderr to the file. You might be looking for `2&gt;`. `2&gt;` will send only stderr to the file. I don't know the details for the compilation, but can this help you?
if you only want the output in the file: command 2&gt;file or (if you want to append to that file): command 2&gt;&gt;file or (if you want to get fancy and still keep the original error stream, although it might show up a few lines later): command 2&gt; &gt;(tee -a file 1&gt;&amp;2)
That works fine for me and does produce both the executable and the error file. You won't get an executable if there are fatal errors, of course.
I'll give it a shot and come back with the results. Thank you.
the first option turned out to do exactly what I needed, but the other options may be much more desirable when my project goes live
&gt;ah my bad was on mobile sweet got it to work #!/bin/bash find /mnt/downloads -name "*.rar" -print0 | while IFS= read -rd '' archive; do destination="${archive%.rar}" if [ ! -d "$destination" ] ; then mkdir "$destination"; fi unrar e "$archive" "$destination" done &amp;#x200B;
I'm not sure this is clearer than just using a function or alias to provide the command prefix. Aliases are even available in non-interactive shells (i.e. shell scripts) when the `expand_aliases` shell option is enabled.
There's definitely an element of taste here; with an alias, we still have a repeating prefix and clutter up the namespace if it's just a one off thing. Personally, I think the xargs trick is nice because it sort of feels like dropping into a new, special context where you're executing a bunch of subcommands.
I don't know where but I ones saw a function which enabled getting rid of the first command. Looked like a general purpose CLI. Example &lt;unknownFunktionName&gt; git pull log --oneline ... ... exit
Not sure I understand you perfectly, but we could certainly use a function that threads it's first argument through the following ones: ``` mapply() { local fn=$1; shift local cmd for cmd in "${@}"; do "${fn}" "${cmd}" done } ``` which would be called like this: ``` mapply git pull log branch tags... ``` However, `mapply git pull log --oneline` certainly won't work as intended. We need a way of grouping "subcommands" together, hence the xargs trick.
The Markdown on Reddit is crappy and doesn't know ```` ``` ````. You instead have to add four spaces in front of each line of your code.
The Markdown on Reddit is crappy and doesn't know ```` ``` ````. You instead have to add four spaces in front of each line of your code.
Are you viewing via an app or with some browsers plugins *etc.* enabled? It seems to display just fine for me.
Yes. My suggestion would be like: `mapply() {` `IFS=','` `args="$@"` `IFS=‚,‘ commands=($args);` `base=$(echo ${commands[0]});` `for element in "${commands[@]}"` `do` `if [ "$base" != "$element" ]; then` `echo "Executing: $base $element"` `echo "========================="` `eval "$base" "$element"` `fi` `done` `}` &amp;#x200B; So you can comma-separate the method calls. E.g. mapply ls, -l, -la, -latr Which then executes: 1. ls -l 2. ls -la 3. ls -latr
xargs also has a `--delemiter` option, `-d` for short: xargs -n1 -d ls &lt;&lt;&lt;'-l,-la,-latr' In the end, it seems like we'd just be trying to recreate `xargs` altogether. heh
Hahaha damn
Huh. That's very non-intuitive. Anyway, changed.
I'm on the desktop website, the "old" version. Maybe the new version is different?
 find /mnt/downloads \( -name "*.rar" -o -name "*.zip" \) -print0 | while IFS= read -rd '' archive; do destination=${archive%.rar} mkdir -p "$destination" || continue case $archive in *.rar) unrar x "$archive" "$destination" ;; *.zip) (cd "$destination" &amp;&amp; unzip "$archive") ;; esac done
I'm not sure how to use patch for it, but you could certainly use sed more carefully, as in sed -i -e '/^icon-theme-name/s/Adwaita/Numix/' where the s is guarded by a pattern.
`sed 's/Adwaita/Numix/g'` this will output the change to the terminal so you can see that it's doing what it's supposed to be doing. `sed -i 's/Adwaita/Numix/g'` will make the changes to the file itself. sed can also be evoked within the VIM text editor. You go open the file in VIM `vim /etc/lightdm/lightdm-gtk-greeter.conf` Go into command mode by hitting Esc type `:%s/Adwaita/Numix/g'` and hit Enter. `:wq` to save file and quit.
The reason is because I've seeing it used. Out of curiosity that how if a certain line has being detected from a certain file I can input like few lines of info. I think its generated by diff but not total sure tho. I think I should put more time to learn sed/awk/grep thanks for the info. Tho awk has always felt a bit hard to grasp.
Thanks this look like a healthy way but how do I define the file to use the command on?
it doesn't matter how many years you use awk, it's always hard to use. Luckily though, solving problems with it are often a DuckDuckGo/StackOverflow search away.
Which CLI editor are you using? There are ways in which you can find and replace either one of the occurrence or all the occurrences.
I am using vim. but wanted to make a bash script that changes load of different files with certain lines.. so I would ease the procedure after new linux installation.
Use sed then. That's one of the best options.
To make a patch, copy the original file to some temporary dir, then make another copy of it with .orig added to the end ~$ cd "$(mktemp -d)" /tmp/tmp.12345$ cp /etc/lightdm/lightdm-gtk-greeter.conf . /tmp/tmp.12345$ cp lightdm-gtk-greeter.conf{,.orig} /tmp/tmp.12345$ ls lightdm-gtk-greeter.conf lightdm-gtk-greeter.conf.orig Open lightdm-gtk-greeter.conf in your favourite editor and make the wanted changes, then use `diff -u` to create the patch /tmp/tmp.12345$ vim lightdm-gtk-greeter.conf /tmp/tmp.12345$ diff -u lightdm-gtk-greeter.conf{.orig,} &gt; greeter.patch You can now apply this patch to the real file with # patch /etc/lightdm/lightdm-gtk-greeter.conf &lt; greeter.patch patching file /etc/lightdm/lightdm-gtk-greeter.conf Lastly, the heredocument syntax is a way to let you embed a file inside your script, so instead of patch /etc/lightdm/lightdm-gtk-greeter.conf &lt; greeter.patch you can do patch /etc/lightdm/lightdm-gtk-greeter.conf &lt;&lt; 'EOF' &lt;content of greeter.patch here&gt; EOF
https://github.com/uhub/awesome-shell
You're probably looking for `break`: [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect\_09\_05.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_05.html) Another method I can think of off the top of my head (i.e. w/o testing) is if the loop is in a function, sticking a `return` statement, which would not only end the loop, but the function.
On the heals of break: **continue** will end the current iteration of the loop while allowing subsequent iterations to proceed.
 #!/bin/bash function doEncoding() { local FILE="${1}" echo "Encoding $(basename ${FILE})" } readarray -t FILES &lt; &lt;(find .) for FILE in "${FILES[@]}" ; do TARGET_DIR=$(dirname "${FILE}") #echo FILE=${FILE} TARGET_DIR=${TARGET_DIR} if [ ! -w "${FILE}" ]; then echo "Cannot write FILE=[${FILE}]" break; fi if [ ! -w "${TARGET_DIR}" ]; then echo "Cannot write FILE=[${FILE}] to TARGET_DIR=[${TARGET_DIR}]" break; fi doEncoding ${FILE} done
Since we know he's having permissions issues, I think it's better to test if we can write so we don't waste time encoding something when we don't have permissions to save the result. for f in *.ts; do if [ -w "$f" ]; then encode "$f" &amp;&amp; rm "$f" fi done
Nice and logical - I like it. However a few observations from an old neck beard ;) * The `doEncoding` function will always return zero unless the `echo` statement fails. Remember `$?` only stores the return value of the previous command. Pretty sure that’s not what you meant. I normally assign a local variable to the particular return code I want immediately after the command whose status I need to pass back t the caller. Something like `command foo; local retVal=$?`. * I always use lowercase or mixedCase for creating my own variables/identifiers because shell built-ins are always UPPERCASE. It doesn’t “matter” in the strictest sense (unless you reassign the value of a built-in), but in my scripts I know if a variable is all upper case, it’s a built-in and anything in lowercase or mixed is mine. It’s more about style, than functional difference. ¯\_(ツ)_/¯ * Usually, I avoid bash arrays whenever I can, and when I can’t, I jump over to Python (or Perl). In this case a `find . -name “*.ts” | while read file; do ... ; done` construct would also work.
do you mean `date "+%D %H:%M:%S" -d 'a day ago'`?
[https://www.shellcheck.net/](https://www.shellcheck.net/) is a really helpful tool for this. I see nothing immediately wrong with your script apart from you probably should be using read -r, but I'm also not about to type it out from the image into Shell Check for you... :D &amp;#x200B;
Thanks a bunch, I will check out that site! 
There's a space after your shebang, and you should probably get in the habit of using env instead. `#!/usr/bin/env bash`
What does env do? Portability? Or security?
Take a look at `column(1)` for formatting the output; you shouldn't use backticks, they're ancient; you don't need CURL nor GAWK variables: just call curl and gawk; URL=$url is pointless: simply use $url instead; don't forget to quote everything: https://shellcheck.net
Look at httpstat , pip install httpstat , similar to what ure doing but you’ll love it 
Sure.
&gt; There is reason for that IMHO. Not every distro place those binary in uniform location.And inside script , it's a standard practice to use the full path of the binary considered good, that is why the variables are there to find and pinpoint it. `which` will only work when `curl` is in your $PATH. If `curl` by itself won't work, neither will `which curl`. By adding `CURL=$(which curl)` instead of calling `curl` directly, you only made the script a tiny bit worse. Instead of `curl: command not found` you'll get `-o: command not found` on line 7 since $CURL will be empty. 
Hey people! Would you mind giving this [Simple GetOpts](https://github.com/sakishrist/sgo) script a try? I'll be happy to receive your feedback! Thanks!
That's longer than the test he already has, and less efficient since it's replacing a native bash builtin with calling an external binary.
Would be nice if you'd shown what the output of that yum command looked like. Anyway, I tried the command in a docker container, and came up with this jq to parse it: # yum -v repolist all | jq -sR 'split("\n\n")[] | [match("(?:^|\n)(Repo-\\w+)\\s+: ([^\n]*)"; "g")|{(.captures[0].string): .captures[1].string}] | add' { "Repo-id": "C7.0.1406-base/x86_64", "Repo-name": "CentOS-7.0.1406 - Base", "Repo-status": "disabled", "Repo-baseurl": "http://vault.centos.org/7.0.1406/os/x86_64/", "Repo-expire": "21600 second(s) (last: Unknown)" } { "Repo-id": "C7.0.1406-centosplus/x86_64", "Repo-name": "CentOS-7.0.1406 - CentOSPlus", "Repo-status": "disabled", "Repo-baseurl": "http://vault.centos.org/7.0.1406/centosplus/x86_64/", "Repo-expire": "21600 second(s) (last: Unknown)" } ...
I’ve occasionally done the `foo=$(which foo 2&gt;/dev/null)` but only so I can do something like this: if [ -z “${foo}” ]; then echo “Couldn’t find ‘foo’ in your path - please install it” exit 1 fi At least then you’re doing something with it, although you could just as easily do that as an inline too: `if [ -z “$(which foo 2&gt;/dev/null)” ]; then ...` Sometimes assigning a command’s output to a variable is helpful because it might make the script easier to read/debug, or it’s an expensive/long-running command (E.g., `du` can punish you).
Very odd to see 2 forms of command substitution in the same script. Best to remove the backticks.
It is unclear what the FILE_LINE looks like, and why you end up with so many values separated by space in INPUT_NAME. Shouldn't you have just a single value there? Maybe you can give more details?
I would use functions. This will make your code more modular and allow you to add things more easily. You need to iterate through your packages no matter what. But you should probably be creating 3 arrays. code, node, mpv. Then you can iterate through them easily and break out quickly. 
Comparing to `${array[*]}` is not robust in that you could have something like array=(notnode blah) and your if condition will match `node` anyway. I'd write a function like contains() { local term=$1 shift local el for el in "$@"; do [[ $el == "$term" ]] &amp;&amp; return 0 done return 1 } and then check like if contains 'code' "${PACKAGES_TO_INSTALL[@]}"; then setup_vscode setup_shfmt fi or, if you want to be fancy, use namerefs (Bash 4.3 or newer): contains() { declare -n arr arr=$1 local term=$2 local el for el in "${arr[@]}"; do [[ $el == "$term" ]] &amp;&amp; return 0 done return 1 } and use it like if contains PACKAGES_TO_INSTALL 'code'; then 
times=\`last -f /home/OS/predtest\_csh/vstupy/wtmp.02Feb | tr -s " " | cut -d " " count=0 for timemy in ${times\[@\]} do if \[\[ "$timemy" &gt; "22:00" \]\] || \[\[ "$timemy" &lt; "04:59" \]\] then count=$((count+1)) fi done echo $count &amp;#x200B; The questions is solved now using the above script, one more thing which is important is how to find only unique users among them
&gt;if contains PACKAGES\_TO\_INSTALL 'code'; then Thanks, this is great! After thinking about it, [obiwan90](https://www.reddit.com/user/obiwan90) is right about ${array\[\*\]} not being robust, his namerefs solution below seems like the best way to do this?
Brilliant, cheers!! Your namerefs function is working well in my script. I will link the github here when it is ready in case you want to check it out.
Maybe, but it's probably a little overengineered. If you know that your package names can never contain spaces (pretty fair assumption), then changing the `case` statement to: case " ${PACKAGES_TO_INSTALL[*]} " in * code *) # ... ;; * node *) # ... ;;&amp; * mpv *) # ... ;;&amp; * jack-audio *) # ... ;;&amp; esac would be perfectly safe, and would never match part of a word. Note carefully where I've put spaces in the above code.
[Here](https://i.imgur.com/buVuCjm.png) you can see, highlighted, how I make a list ($Users), without duplicates, of currently logged users.
Putting the space in front now makes your 'case' solution the best, fantastic! Cheers!
Depends, how you keep that data structure. You can do: grep -v -F some-name-x86 input.txt &gt; output.txt It is an inverted match, so all lines with some-name-x86 are filtered.
Yes, that would somewhat work. We need to have the %D but can hard code the hours into the script. Thank you for your help.
awk is a programming language it self. It basically can do what ever you need. for your case, you would have to create in memory lists of files and as you process, compare and decide: if current filename is 64, but you have already the 86 version, skip it, otherwise, store it. and then in the end, print the list from memory.
I think this will work awk -F 'x[0-9]*.iso' '!uniq[$1]++' $file Split off the x64/x86 part of the filenames and then count when the first part of the filename appears for the first time. Just sort the file first so that the x64 versions appear first in case of duplicates. Based off of this https://stackoverflow.com/questions/38655484/uniq-but-only-on-part-of-the-string 
Others have pointed out that awk is powerful and capable of doing this. I completely agree. However, I like piping together individual filters. I like the feedback of seeing the output transformed one step at a time as I piece together the filters. So, [here's a one-liner](https://ideone.com/aSYhv2) that uses several commands piped together: tail -n +2 | sed -r "s/([^ ]+).(x[0-9]+\.iso)/\1 \2/g" | sort -k 4,5 | cut -f 1-4 | uniq -f 3 | sort | awk -v OFS='\t' 'BEGIN { print "ID", "new name" }; { print $1, $4 "." $3 }' If you want to reverse-engineer this, the pipes make that pretty easy too. Remove piped commands to see how it affects the output. 
Cool
That’s pretty vague. 
Actually I have a PDF containing detailed information but I am not able to upload it on reddit
https://docdro.id/ncgWoIr
Looks really fun. You should try and do it yourself. Have you tried to start it? Which bit are you stuck on?
What he said. Looks fun. And also more complex than a simple “help” can answer. 
I have no clue from where to start.Can you guys give me any link or resources that have similar premises ie SQL on bash
Are you a third year CS student? You really should be used to finding information for yourself by now. The first instruction is: &gt;First write a script create database.sh that takes 1 parameter, `$database`, and creates the directory for the database `$database.` Do you know how to create the `create_database.sh` file? Do you know how to give a parameter to a script? Do you know how to create a directory? These are all very little steps. Tackle each step little by little, and don't worry about how large the overall project is. Have a go doing this bit. If you have specific questions that you can't solve yourself with some googling and reading then come back. 
Did the whole assignment... Can confirm it was fun.
So, you're not using MySQL at all. You're just making a well structured data collection. It's all just regular directories and files, which UNIX/Linux/Bash is very good at handling. The instructions are laid out in a well thought out process, and each step is relatively simple when taken successively. Don't get hung up on the big picture. Just focus on each step one at a time and you'll get there.
Hard to know, I dipped in and out. Definitely more than an hour I’d say, I’m not too familiar with bash and a couple of bits held me up for a while. 
I solved my own problem. For those of you that may be interested. cheapo_usb # This is the name of my USB flash drive tether, on a lanyard to my wrist in case of emergency. usbvar # This is a variable I create, essentially a count of how many instances in /Volume/ there are of cheapo_usb &amp;#x200B; usbvar=$(ls -1 /Volumes | grep cheapo_usb -c) echo -e "Script is running, pull USB to shutdown or press CTRL-C to break!" while [ $usbvar -eq 1 ];do usbvar=$(ls -1 /Volumes | grep cheapo_usb -c) sleep 2 done /usr/sbin/shutdown -r now &amp;#x200B;
Put the file names in quotes? Not sure I understand the question
It would be helpful if you outlined what you’re actually trying to do, rather than outlining the “options” as you understand them. What are you trying to do with these files? What’s the directory structure that these files are living in?
my current directory has about 200 files in it (just one of a few to be done). I did a grep :: /content/*.txt &gt;index.txt to make an index of the file names and the strings i want to rename the files with. the index list looks like /content/11111.txt::: string :: /content/11112.txt::: string :: I would like to rename file 11111.txt to string.txt and so on for all the txt files. 
I'm converting html pages to wiki pages. The html was auto-generated, not by me... resulting in the numeric naming. The wiki pages would be better having proper names, imo. I converted from the HTML to text and then will convert to wiki. The test run on that part is about as good as it gets. the string is the first line of the text file after conversion. It's formateed "::_noun_name_::". 
If I get your question right .. I think what you doing is right . You could do , for i in $(ls) ; do c=$(grep content ${i}) ; mv i ${c} ; done ... for example 
Something like this could do your job: ``` while read -r file; do oldfilename="${file%:*}" newfilename=$(cut -d ':' -f 4 "$file") newfile="${file%/*}"+"$newfilename" echo "mv $oldfile to $newfile" done &lt; indexfile ``` Run it and if the output is correct, change the `echo` to a 'mv` command and delete the `to` on this line It may be that the substring operations or the cut command are wrong, but i'm on my phone, can't test them..
This here should print that "string" you want for a file, and it also sends an error exit code if it can't find that string so that you can check for that in an "if" in bash: perl -nE 'if (/:: *(.*?) *::/) { say $1; exit; }' filename You can then wrap a loop around it where you go through all your text files in your current folder: for file in *.txt; do newfile=$(perl -nE 'if (/:: *(.*?) *::/) { say $1; exit; }' "$file") if [[ -n $newfile ]]; then mv -v -- "$file" "$newfile" fi done Here's an experiment that you can run directly at the command line: for file in *.txt; do newfile=$(perl -nE 'if (/:: *(.*?) *::/) { say $1; exit; }' "$file"); [[ -n $newfile ]] &amp;&amp; echo mv -v -- "$file" "$newfile"; done It just prints stuff and does not actually rename. If what it prints looks like what you want to happen, then search for the "echo" in the line and remove it to actually rename the files.
pipe the whole line cut command. cut -f 2 -d";"
Awk is probably the best option for this. Assuming the fields (columns) are separated by semicolons: awk -F; '$1 == "5." { print $0 }' file.dat That is, if the first field equals `5.`, then print the whole line. (The awk program can also be abbreviated to `$1 == "5." { print }` or even just `$1 == "5."`, but the long form is easier to understand.)
Does `grep "^5" file.dat` solve it? 
"quick script" eh? It seems solid, but I'd call it thorough, complete, useful or something more descriptive. 
what does \^ do? 
thanks it's working! 
Thanks. Whenever I start scripting something I tend to keep adding on to it a lot. Even if I pass something off as finished, I'll still add stuff to it. Don't be surprised if you see more stuff on here soon.
And now how can I echo the 2nd column of the result? Because I have to make the 2nd column into variable. goodline="$(awk -F';' '$1 == "5." { print $0 }' file.dat)" secondcolumn="$(echo "$goodline" |awk -F' ' '{ print $2 }' $goodline)" echo "this is the second column: secondcolumn" I tried thi above but doesn't work 
Also how can I replace the value with variable? instead of awk -F';' '$1 == "5." { print $0 }' file.dat I need awk -F';' '$1 == "$variable" { print $0 }' file.dat
Ftfy for i in *; do c=$(grep content "$i") mv "$i" "$c" done 
Thanks for that !
I solved it with “‘$variable’” 
The safest option is this: awk -v match="$variable" -F';' '$1 == match { print $0 }' file.dat This sets the `match` awk variable to your `$variable` shell variable and then compares against it. (You could also directly include the `$variable` in the program with some tricks, but then you need to worry about escaping.)
I'd do it with bash alone, like this: variable1="5;35;30" secondcolumn=${variable1#*;} secondcolumn=${secondcolumn%%;*} 
But as I said 5;35;30 are results from previous awk-s I have a .dat file where are the following datas: 5.; 35; 30 8.; 50; 49 7.; 40; 40 &amp;#x200B; if my input is 5 then the output is the line where 5 is in the first column so godline will be 5;35;30 &amp;#x200B; goodline="$(awk -v numbervar="$number" -F';' '$1 == numbervar{ print $0 }' file.dat)" echo "goodline: $goodline" secondcolumn="$( awk -F';' '{print $2}' $goodline)" echo "second column is: $secondcolumn" &amp;#x200B; and I need second column
You can pass shell variables to awk (see answer here: https://stackoverflow.com/questions/19075671/how-do-i-use-shell-variables-in-an-awk-script) If you are not set on awk, pipe your variable to cut: cut -d\; -f2
use awk. &amp;#x200B; \-F - sets a delimiter, then you print in any order you want. `echo London;Berlin;2018.12;6:30;4.;45;6 | awk -F";" '{print $3,$1,$2}'` &amp;#x200B;
I *think* it makes some of the wiki ish thing easier. It might be that the wiki system uses a Title instead of page name... Currently, the auto import script set the file name to the Title so fixing the naming fixes both. 
The right way to write that is type foo &gt;/dev/null || exit And you can test for multiple commands in one go, and it will report all the missing ones, not just the first missing one type curl jq &gt;/dev/null || exit
I think this is nearly it, probably it. It renamed all the .txt files, but left the extension off. I dont *think* that will be a problem. I can make some serious headway from here, Thank you.
&gt;The next step after this would be to get 3rd column. After that I would use if statement and compare 2nd and 3rd column. What are you wanting to do based on the results of this comparison... it seems to me that this detail could change the advice you get.
Thank you all.
I forgot about the ".txt". You could add it after the $newfile variable when executing the 'mv' command: mv -v -- "$file" "$newfile.txt"
It prints which commands that are missing on stderr. And since you mention portability, `type` is standardised, `which` is not. With `which` you can't rely on a useful exit status, nor that it outputs nothing on stdout when a command is not found. 
I’m yet to have `which` fail in an unpredicted or unintended way in over 20 years of Unix/Linux programming, but you are correct about `type` being internal to `bash`.
...which is exactly the same construct I posted originally: 1. Test for presence of executable atomically 2. If not found, throw an error and exit. Whether you use `which` or `type` for #1 you are still relying on zero/non-zero exit statuses to test in #2. The only subtle difference is the exit is after all the tests, which is usually how I do these sort of things too (sum the return values, if != 0, exit).
least of my worries :D Ran into some -0 issues and spaces in file names... got them sorted. Now I'm onto fighting duplicate content. Thanks again. 
Yes, except, as demonstrated, the the one using `which` is unreliable, hence why I pointed out a correct version using `type`.
s/demonstrated/claimed/
So how can I stop it? The viewers don't like seeing 3 cartoons in a row, or 3 trailers in a row.
I don't know anything about how your data is being generated, but did you just want to rearrange it after the fact?
I can rearrange it either after, or during the choosing process. Doesn't matter either way. If need be, I can link to a pastebin of the generator script.
If you want truly random, expect runs. If you want random-ish with additional qualifications, I suggest defining your rules, starting with random and then removing any row which breaks the rules. This will be the easiest way to get the most random result which still fits your desired output rules (also the most extensible in the future).
Sometimes this happens, how big is the sample size? How is the randomness being generated? 
I have no idea where to begin with that.
I have a grasp on the concept, but no clue where to begin with implementation at all.
Depends on the nature of your rules you want to define. Standard programming algorithm-writing here is all that's needed. &amp;#x200B; For example, if your rule relies on not repeating a category twice, I'd setup a variable like \`last\_row\_category\`, and check to ensure the current row isn't equal. Then before finishing the loop, set last\_row\_category to the current row's category. &amp;#x200B; Or if you want to never show two movies from the same category, setup a global array listing the categories you've already shown and check against it. &amp;#x200B; The rules themselves are an implementation detail. &amp;#x200B;
&gt; and then running the result through sort -R. It's worse if I don't run it through sort, though. Yeah, `sort -R` is probably your problem. It is a somewhat shitty and non-portable way to randomise an input. But if you've got `sort -R`, then very likely you have `shuf`. Use `shuf`. Hell, even `sort`'s man page drops a clue: -R, --random-sort shuffle, but group identical keys. See shuf(1) Beyond that, there's no tool that's going to do this for you, so you'll have to come up with your own algorithm. A fairly simple algorithm shouldn't be too hard to conjure up in `bash`
Randomness inherently has patterns at times though, so I don't understand the question entirely. &amp;#x200B; \&gt; I really don't want any sort of pattern \&gt; Is there any way to stop it from choosing the same category twice in a row? &amp;#x200B; If you want to specifically stop repeats, I've documented some examples above. If you just want actual randomness, you \*will\* see what look like patterns at times.
\`shuf\` could be another tool to use.
No joy. Here's the first 5 lines of output using shuf instead of sort: 00:00:60 - Commercial: Lionel Train - 00:00:43 - Short: Bizarre TV Warning Video - 00:02:08 - Kevin Ryan - Funny Craft Beer Review - DOUBLE CHOCOLATE STOUT - 00:07:29 - Cartoon: Ants in the Plants - 00:08:28 - Cartoon: Along Came a Duck - 00:00:55 - Trailer: Catalina Caper - 00:02:57 - Trailer: Cannibal Holocaust - 00:02:05 - Trailer: Count Dracula and His Vampire Bride - 00:06:07 - Cartoon: The Booze Hangs High - 
Working as intended. &amp;#x200B; See the my other comment thread about the nature of randomness and what your actual goal is.
I'm going to sleep on it for the night, and see what I can come up with. Thanks everyone, for the help so far.
I have ~6k files to Find/Replace a couple hundred terms in. 
Why do a grep and all that? Just run something like ``' while read -r file; do while read -r word; do sed -i "s/$word/[[$word|$word]]/g" $file done &lt; control.txt done &lt; filelist.txt ``` If you dont have a filelist, you should be able to do a `find -type d` to get the and replace the last line with `done &lt;&lt;&lt; $(find -type d)`. But i recommend you to create a backup of all files in case sed does something you didn't intend
I would try to kill excessive repetitions like this: First get a shuffled file that's much longer than what you need, then: * go through it line-by-line * extract the category from the line * keep the previous two categories and the current category in a short array * only print your current line if your short array does not have three repeating categories in it * remove the first entry from your short array * continue You can do this in bash. Here's some untested ideas about how to do each individual steps: Reading text line-by-line: while IFS= read -r line; do ... done Extract category: category="${line#*:*:* - }" category="${category%%:*}" Initializing your short array with two garbage entries, which you'd do before the while loop runs: list=(0 0) Adding the current line's category to the end: list=( "${list[@]}" "$category" ) Check if the categories in the list are different and print line: equal=1 for x in "${list[@]:1}"; do if [[ $x != ${list[0]} ]]; then equal=0 break fi done if (( ! equal )); then printf "%s\n" "$line" fi Delete the oldest entry in your list: list=( "${list[@]:1}" )
 for a in $(cat animals.txt) do echo "This is a funny $a" done 
I'd use `sed`: sed -i -e 's/^/this is a funny /' file.txt assuming GNU `sed` (so it has `-i` and it doesn't require an argument). This will use `sed` to replace the regex `^` which matches the start of a line with the second pattern, `this is a funny `. The `-i` will have it store the output in the same file as it was given to process, replacing that file in place.
This way lies dragons (like what if one of the animals was a Sea Lion or Mountain Goat). Consider https://mywiki.wooledge.org/DontReadLinesWithFor
Yeah yeah, I woke up and typed that within five minutes. You get what you pay for.
With awk: awk '{ printf "This is a funny %s\n", $0; }' &lt;animals.txt
/u/CWinthrop I believe you don't want randomness, randomness by definition will have these sort of patterns, the same way that if you flip a coin twice expecting to get head and tails you will be disappointed 50% of the time: HH, HT, TH, TT All these outcomes have the same probability, so 50% of the time you will get your "expected randomness" of different outcomes. The same concept is in place here, but also because we don't know the items on the list, is there the same number of cartoons, and trailers and so on? A way to "force randomness" (which is a bit of an oxymoron) but still retain some random elements in the way you want would be to: \- Use different lists for each type of content (in this case three lists) \- Assign a weight to each list, and randomly select one of these. \- Once you selected a list, randomly select an item from this list. \- Adjust the weight for the list, so that it's less possible to get the same list again. Example: I have three lists, I assign to each list a 1/3 weight, you can do this in any way you like but for simplicity: I randomly generate a float number with two decimals, if the number is 0.01-1 I select list 1, if the number is 1.01-2 I select list 2, if the number is 2.01-3 I select list 3. Now I adjust the weight for whichever list I used before, lowering the chances for this list to come up again, let's say list 1 came up, now I set it to 0.01-0.50, and the remaining I divide it across the remaining lists, basically making this list only 15%\~ have a chance to come up. If a different list is elected on the next run, reset this list's weight and adjust the weight for the other one. Alternatively if you are super lazy you can just make it so that the previous list never pops up twice by removing the previous list from the available choices on the first election, this way you always enforce a rotation. Sorry for the lenghty post, I feel like I'm ranting so I'll shut up.
I hear you. I do the same. I personally am moving all my scripst where user input is requested to use whiptail as user input and output. (You can also use dialog) https://en.wikibooks.org/wiki/Bash_Shell_Scripting/Whiptail for more info. To me it is much easier for input. With Dialog, selecting a directory (using --dselect) is an option. With whiptail, you can do it, if you write around it yourself.
Is this a bad practice? Because that seems the simplest to me... `for i in $(cat ./filename); do echo "this is a funny ${i}; done`
just put most of my terms list together. Silly me forgot how wiki works, it is basically 1 per page... so not a couple hundred... its 6k. :'(
yes, very bad practice as this will split on IFS and special characters. The correct(er, not perfect still) way if you were using a loop would be `while IFS= read -r line;do echo "this is a funny ${i}";done &lt; filename`
Is this going to have an issue with Spaces? Example : control.txt foo bar foo bar 
The `while read xxx done &lt; file` *may* have problems, but I'm not sure.. I can test it later today if you want
Mm, you learn something every day. Thank you, and cheers!
I'll try it, no real loss if it fails... just burn another back up :D
np
Though I don't think there's much risk in r/bash and with the specific example, in the interest of discussing "best practices" a detailed discussion of `printf` vs `echo` can be found at https://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo (and really, anything by M. Chazelas is worth reading closely--that guy knows a TON about shells at the very least)
&gt; I wanted to get current (at the time the script executes) status of apache2 web service. I have tried "service apache2 status" but the output is too long. I would like to get a simple running / stopped in case of the service status. You should use its exit status, not its textual output. It should follow [the LSB guidelines](http://refspecs.linuxfoundation.org/LSB_5.0.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html). (Alternatively, if this is a systemd-based machine, you can use `systemctl is-active apache2.service`. This always has a simple, consistent output. It also uses the same LSB exit statuses described above.) &gt; Finally I would like for this script to start apache service if it is stopped Then you shouldn't test whether Apache is already running. You should just start it. If it is already running, starting it is a no-op. &gt; and mount the devices under /dev/sd* to their respective mount point. Ditto. With systemd that would be `systemctl start /path/to/mountpoint`. But if you aren't using systemd, there isn't any simple "mount if not already mounted" tool available. You could use the `mountpoint` utility to determine whether a particular path is already a mount point, and if not, mount something there. It's [a bit hacky](https://en.wikipedia.org/wiki/TOCTTOU), but it will probably work OK.
Yep, very true. Although also in the interest of best practices I'd suggest avoiding parsing/processing text in bash altogether and use a more specialised program such as sed/awk/perl, which would have massively improved performance for any non trivial problem. (Those answers had already been provided though)
&gt;systemctl is-active apache2.service Above mentioned command gives me the output I need. I did refer to the LSB documentation but service apache2 status has a very long output. I am not sure if it follows the document but it definitely doesn't follow the document in it's entirety. &amp;#x200B; With the mount option, I am unable to make any headway. I am unable to find status and using systemd i get a warning. So I'm still trying to find a solution for that. &amp;#x200B; &amp;#x200B; Thank you for giving me answer to the Apache problem.
&gt; I did refer to the LSB documentation but service apache2 status has a very long output. The LSB documention doesn't define the allowed output, so that's perfectly valid. This is why I said you should look at the _exit status_.
Thank you very much and pardon me for not being able to grasp the exit status and using the systemctl is-active part. Would it be possible to put this in an if statement and restart the service only in case when it finds it stopped? While i see that using system apache2 start does not "restart" the service and has no effect if the service is in start state, I would like to learn if this is possible via if then statement. &amp;#x200B; Thank you. 
You might like the `script` command. From its man page: &gt; script makes a typescript of everything displayed on your terminal. It is useful for students who need a hardcopy record of an interactive session as proof of an assignment, as the typescript file can be printed out later with lpr(1). &gt; If the argument file is given, script saves the dialogue in this file. If no filename is given, the dialogue is saved in the file typescript. I'd say, in principle, running an interactive text editor in a script is a pretty bad design. What are you having them modify in the file that you can't automate? 
&gt; Would it be possible to put this in an if statement and restart the service only in case when it finds it stopped? Like I said, just do: systemctl start apache2.service This will start it if it is stopped (or failed), and do nothing in any other state. 
&gt; Any ideas with the current breakage? You would need to reconnect the editor back to the terminal again. You could do this either by duplicating the original filehandles, holding onto them until your editor command, then duplicating them back; or by more simply re-opening `/dev/tty` when required: editor 0&lt;&gt;/dev/tty 1&lt;&gt;&amp;0 2&lt;&gt;&amp;0 
Oh wow, I feel like I should've known about this. I'll have to look into this more. Thanks for showing this.
If you want to save command history, do everything in a [tmux](https://robots.thoughtbot.com/a-tmux-crash-course) session.
If you care about formatting but don't want to write it in a file in the first place, I'd pipe it through a formatter.
The cases are treated as patterns, at least the ones that contain glob characters such as `[` and `]`. Use quotes to make the comparison literal: case $option in "${options[0]}") ... 
Hey thank you again. You're absolutely right. The test is unnecessary. I appreciate your assistance.
I'd like to keep track of the output. Not so much the nano sessions. I've decided to go with the script command for now. Played around with fd redirection but never did get it 100%. Would command grouping work across functions? I've used it on a smaller scale a few times. Also interested in a better or a more conventional method to use in place of using an editor within a script. I think I'm beginning to understand some of the issues involved. Seems fd redirection is a MUCH deeper subject than I realized. This is my personal Arch install script and is as much a learning exercise as anything. It uses a shared local package cache so it doesn't have to download all the packages from a mirror, and installs Arch base, base-devel and an xfce DE in a little under 10 min at this point. I get into scripting occasionally as I find it has a good balance of interesting / challenging to be enjoyable. Appreciate the input and perhaps I've found a new place to hang out and learn. Posted a link to it if anyone's interested... I'd appreciate any constructive feedback on what jumps out as really "needs fixed", different methods, etc. https://github.com/Cody-Learner/arch-inst/blob/master/inst
`fc` doesn't have a man page, it's documented in `man bash` under `BUILTIN COMMANDS`. 
To save multiline commands with newlines instead of semicolons, use `shopt -s cmdhist lithist`. As far as I know, that only survives the history file round trip if the entire history file uses HISTTIMEFORMAT. If you want to edit the current command in an editor, `C-x C-e`. This doesn't work well if you use Enter to separate lines, so maybe use `C-v C-j` instead. If you want to save the last command to a file, `history -p '!!' &gt; file`.
&gt; Would command grouping work across functions? It does &gt;Also interested in a better or a more conventional method to use in place of using an editor within a script. You might want to look into `sed`, or, in your use case, the simplest approach might be having some curated heredocs that you simply write into your target files. This approach has the advantage of abiding by "desired state" principles. &gt;Seems fd redirection is a MUCH deeper subject than I realized. There are people out there who say dumb things like "durr if I have to write more than 4 lines of bash, I switch to python. Look at me, I'm so smrt!" For me, manual fd handling is one of the thresholds where I start considering another language.
AFAIK the closest thing `bash` has is `select`, but that is not quite the same. Typically this level of interactivity would be handed off to something like `ncurses`, I'd speculate that it would be possible to build this using `tput` though. Best bet is to search GitHub to see if anyone else has done this before
Here https://github.com/p-gen/smenu/blob/master/smenu.gif But generally speaking you will be using an external utility like `tput`.
Dialog, whiptail or zenity
This and only this. Such useful tools when trying to make interactive lists, progress bars, info boxes, whatever you need
Not sure if bash itself has this ability. Menus and wizards are usually handled by a mix of select and case statements. As others have mentioned, external utilities are probably better suited for this.
I just tried it from RHEL (at work), and `$ man fc` does indeed open BASH\_BUILTINS(1). I see it's got *roughly* the same information as the `$ man bash.` But more concentrated (whereas bash is a bit jumbled with reference to fc)
The biggest obvious thing with `mp3_conv` is the use of [ instead of [[. When you don't need sh-compatibility, you should greatly prefer the latter double bracket. `CAPITAL_VARS` are discouraged because they can clobber environment variables. `set -e` and `set -u` are not panaceas, see for example https://mywiki.wooledge.org/BashFAQ/105 . Most of the rest of it seems fine; good quoting, nice use of `find -print0` and a good `while read` loop. For me, stylistically, it's a little too chatty on the tty. I don't need you to print "error" if I send C-c to your script. If there are no files in the `filenames` array, why not just quietly do nothing?
Maybe one of the examples/suggestions [here](https://serverfault.com/questions/144939/multi-select-menu-in-bash-script) can help you ? I quite like the one by MestreLion myself because of its bash-only code, although you can't navigate with arrows but (de)select by using the corresponding number. (if you put a 'clear' command at the beginning of the menu function the menu stays in place each time you (de)select an option)
You could use awk to do something like: &amp;#x200B; child = $(echo "$input" | awk -F "/" '{print $NF}') ; parent = $(echo "$input" | awk -F "/" '{print $(NF-1)}') ; if \[\[ $parent == $child \]\] ; then dosomething ; else dosomethingelse ; fi &amp;#x200B; Awk will break apart the path on / so that /home/you/beans/cheese/Clara/Clara would be separate fields -- then $NF is the very last field and the $(NF - 1) is the second to last field. Then you can compare them to see if they are the same and do something accordingly.
 for i in $(find . -type d -print) do ls -l $i/$(basename $i) &gt; /dev/null done 
With GNU `find`, you can do something like: find -type d -regex '.*/\([^/]+\)/\1$'
put names you want to search in a file eg. names.txt, then run this command. for i in `cat names.txt` ; do find . -type d(irectory) $i ; done
Run them through shellcheck.net. It's an automated tool that does static analysis of the code. Runtime errors may still exist.
? http://linuxcommand.org/lc3_adv_dialog.php
Just because it doesn't look identical to a screenshot doesn't mean it won't do exactly what is called for
Thanks! This was just what I was looking for! But I guess it's my fault for not asking for what I was really trying to do. This narrowed the directories down from a few hundred to a few dozen for me. But now what I want to do is move all of the files up out of the subdirectory and put them into the parent directory. I thought I could figure it out by playing around with -exec and -execdir at the end of that find command, but no luck. I almost borked my system. Any chance I could get a two-fer?
1. Don't use `ls` in your scripts, replace the for loop with something along those lines: for f in /jsonfolder/*.json; do 2. Use `cut` for that: python -m json.tool $f 1&gt;/dev/null | grep Expecting | cut -d\: -f1 [Explanation](https://www.explainshell.com/explain?cmd=python+-m+json.tool+%24f+1%3E%2Fdev%2Fnull+%7C+grep+Expecting+%7C+cut+-d%5C%3A+-f1)
You can use `cut`, `sed` or `awk`. I don't know how to use `awk` so here it goes: LINE="http://somesite.com/!.gitignore/ (Status: 403)" echo "$LINE" | cut -d\ -f1 [Explanation](https://www.explainshell.com/explain?cmd=echo+%22%24LINE%22+%7C+cut+-d%5C++-f1) # LINE="http://somesite.com/!.gitignore/ (Status: 403)" echo "$LINE" | sed -e 's/^\([^ ]*\).*/\1/g' [Explanation](https://www.explainshell.com/explain?cmd=echo+%22%24LINE%22+%7C+sed+-e+%27s%2F%5E%5C%28%5B%5E+%5D*%5C%29.*%2F%5C1%2Fg%27)
One minor thing to note is that trapping INT, TERM and EXIT is redundant, as EXIT includes the former two.
The awk version: `LINE="http://somesite.com/!.gitignore/ (Status: 403)" echo "$LINE" | awk -F' ' '{print $1}'` Note there is a space after the F'
I use something like this to download wallpapers daily: USER_AGENT="Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0" URL="https://old.reddit.com/r/wallpapers/" curl -A "$USER_AGENT" "$URL" 2&gt;/dev/null | ... After that, start by `grep`ing the portion of the webpage you want, then use `sed` to perform search&amp;replace to transform it the way you like. Another handy tool is `recode` (https://github.com/rrthomas/recode) for html entities.
You might want to consider using [rtv](https://github.com/michael-lazar/rtv) - short for reddit terminal viewer. I understand you just want to get exactly one post and this is not the solution for that.
You should be able to combine Line 28 and 29: https://github.com/reynoldscem/convert-mp3/blob/master/mp3_conv.sh#L28 I assume your check on line 28 is to make sure `bitrate` is a number? if [ "$bitrate" -ge "$BITRATE_THRESHOLD" ] 2&gt;/dev/null; then echo "Number above threshold: $bitrate" else echo "Not a number: $bitrate" fi
You can also change .rss to .json!
I would use: w3m -dump [url] inside of my script, for whatever scripting language you want to use. 
 echo 'http://somesite.com/!.gitignore/ (Status: 403)' | grep -ioE "http[^ ]*" This returns a substring that starts with "http" and ends *before* the first space. OP did not define any edge cases and only gave one sample, not a good way to ask a question. Also noteworthy is the single quotes to keep `bash` from interpreting the bang in the URL. 
 cat /proc/meminfo | awk '{if ($0 ~ /MemTotal/) pat1=$2/(1024^2); if ($0 ~ /MemFree/) pat2=$2/(1024^2); pat3=pat1-pat2}{if (pat1 &amp;&amp; pat2) print pat3, pat2;pat=pat2=""}' 
 if $(systemctl restart apache2); then evaluates the exit status of the command, or at least it does in my shell (Bash 4.2.46).
Look at this exemple : [https://serverfault.com/a/506704](https://serverfault.com/a/506704) This is the closest way you can have with bash code only I think.
 ls -1 | sed -e 's/\(.*[^0-9]\)[0-9]*\..*/\1/g' | sort -u | while read line; do mkdir -p "$line" mv ${line}* "${line}/" done It doesn't work with filenames containing spaces though.
command substitution executes the output of the command inside the `$()`, which will then be passed to the `if` statement to be executed. To illustrate this: if $( echo false ); then echo "this would only run if we checked the exit status of 'echo'" else echo "but instead, this runs" fi In the above example, if what you're saying is true, then the if statement would pass because `echo` returns 0. But we're doing command substitution, so the `if` statement is executing the output, which is `false` and the `false` command returns non-zero by definition. This is how [typical] shells work (`sh`, `bash`, `zsh`, etc). To get the above example to work how one would expect: if echo false; then echo "this is always going to pass unless a malevolent user clobbered our 'echo' command" else echo "this will (should) never execute" fi references: * [command substitution](https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html) * [if statements](https://www.gnu.org/software/bash/manual/html_node/Conditional-Constructs.html) 
Use mmv. https://ss64.com/bash/mmv.html It would be something like: `mmv "*\ *\ *" "#1 #2/#1 #2 #3"` This is untested. It will depend on how various your names are. Not sure if it will make the directories or not.
Regardless a nested conditional tests the exit status. Out of curiosity; what happens when *you* run my code?
it succeeds. but it'll succeed regardless because `systemctl` isn't outputting anything to STDOUT. the command substitution then doesn't execute anything so the `if` statement is going to pass: if echo ""; then echo "this will always be displayed" else echo "this won't." fi the above is effectively what you're testing. In the case that `systemctl` fails, it still outputs nothing to STDOUT and nothing is executed, but this is only working by virtue of `systemctl`'s specific behaviour. This wouldn't work in other cases (like my `echo false` example). It's better to have more correct code than to have something that just happens to work due to the very specific circumstances that it's running. if one were to swap out `systemctl` for another tool in the future that does output something to STDOUT, this could could break down and not work as expected.
It will evaluate to true for exit status 0 and false for anything else. You can try for any value you like. It evaluates as expected. #!/bin/bash #exit.sh exit $1 #!/bin/bash # test.sh [exit integer] while true; do if [ "$(systemctl is-active apache2)" != "active" ]; then echo "In loop." if $(./exit.sh $1); then echo "True." else echo "False: $?" fi fi done 
try: #!/bin/bash #exit.sh echo "now exiting" exit $1 #!/bin/bash # test.sh [exit integer] while true; do if [ "$(systemctl is-active apache2)" != "active" ]; then if $(./exit.sh 1); then echo "True." else echo "False: $?" fi fi done 
Squash stderr stdout to protect the exit status: if $(./exit.sh $1 &gt; /dev/null 2&gt;&amp;1); then 
So why not just write this without command substitution? My whole point is that the command substitution isn’t necessary. At this point you’re jumping through hoops to make this work with an extraneous wrapper. 
Hello both.. I've tried both of your code but it does not work. The script stays in a hanging state. Here is the final script (please note I've tried both your codes as is) &amp;#x200B; #!/bin/bash # Script: temp_email.sh # introduced echo blocks &amp; if condition- https://www.reddit.com/r/bash/comments/9wzvhu/if_then_statements_in_bash_script/ # ------------------------------------------------------- cpu=$(&lt;/sys/class/thermal/thermal_zone0/temp) echo " $(date) @ $(hostname) ------------------------------------------- GPU =&gt; $(/opt/vc/bin/vcgencmd measure_temp) CPU =&gt; $((cpu/1000))'C Apache_Service_Staus =&gt; $(systemctl is-active apache2.service) ------------------------------------------- " while true; do if [ "$(systemctl is-active apache2)" != "active" ]; then if $(systemctl restart apache2); then echo "Restarted Apache." else echo "Failed to restart Apache: $?" fi fi done &amp;#x200B;
FYI - it is often easier to do cat &lt;&lt; EOF $(date) @ $(hostname) ------------------------------------------- GPU =&gt; $(/opt/vc/bin/vcgencmd measure_temp) CPU =&gt; $((cpu/1000))'C ------------------------------------------- EOF Both work, but this way you done need to worry about using/escaping quotes
You can just add `Restart=always` under the `[Service]` section of the apache2.service file and it will restart the service if it ever crashes or is stopped.
Thanks for the suggestion, /u/5k3k73k. I think `free` would be a simpler way to get at this info, though. It's possible that `top` cant' be used in the way I'd like. 
&gt; From what i've understood isolating a target is the same as setting a default target and rebooting the system? It's probably more correct to say that when booting the system, systemd implicitly isolates to `default.target`. &gt; Can someone correct me if I'm wrong and tell me the difference of these two commands? Units can have `IgnoreOnIsolate=true`, which means they are not necessarily affected by isolating to a target.
Yes, I am aware of some of the subtle differences between utilities in Unix derivatives. In Linux, I think `free` is probably the quickest way to retrieve this information. That said, it would be nice to know if there was a way to feed the level to the `top` command. Initially my idea was to use a single `top` command across platforms, but as you mentioned, that isn't always possible. 
nah
piss off
Wow................
That's awesome! Thank you. Totally forgot about `cut` command
You could use the versatile vidir program for that task that allows editing of the contents of a directory in a text editor : https://github.com/trapd00r/vidir
thanks didnt even know that sub exists
Thought I'd knock up this quick script of how I would handle this situation. $ cat restart_service.sh #!/bin/bash if [ "$EUID" -ne 0 ] then echo "Please run as root" exit fi #Ensure drives and service names are correct DRIVES="mmcblk0p1 sda1" SERVICE="ntp.service" MOUNT="/mnt/data" _testservice() { #This check if the service is running echo "Checking ${SERVICE}" RESULT=`systemctl is-active ${SERVICE}` if [ "$RESULT" != "active" ]; then _restartservice else echo " ${SERVICE} is already running" fi } _restartservice() { sudo systemctl restart $SERVICE &gt; /dev/null &amp;&amp; echo " ${SERVICE} Started" #_sendmail } _testmount() { #this checks if the drives are mounted for i in `echo $DRIVES`; do echo "Checking $i" df | grep "$i" &gt;/dev/null if [ "$?" != "0" ]; then #Drive not found lets mount it mount /dev/${DRIVE} ${MOUNT} &amp;&amp; echo " $i Mounted on ${MOUNT}" &amp;&amp; \ #lets restart the service _restartservice else echo " $i already Mounted" fi done } #Begin Prog _testservice _testmount &amp;#x200B;
here is the [link](https://github.com/kdabir/has)
Where were you yesterday? I could have used this to save myself an hour of screwing around.
Better late then never :)
That is awesome! I need this, thank you 😀
It’s really nice , which ++
Thanks, glad u liked it :) 
How do I run it without the version number blinking?
grep buffers it's output. you can use the line-buffered flag to flush the output when a newline is encounterd... ``` $ nc -kl 10000 | grep --line-buffered -iv $(hostname) &gt;/tmp/testfile ```
That fixed it! You rule!
are the numbers blinking? could you please create an issue, preferably with a gif/screenshot. If you just want to hide the version info, I have created this issue, to see how to achieve the result: https://github.com/kdabir/has/issues/29
the version numbers were blinking. I will try to create an issue when I get back to the laptop I installed it on. Its at a different house than I am right now. 
&gt; grep buffers it's output. Specifically, it does that by default if standard output is _not_ a terminal.
&gt; grep buffers it's output Specifically, it does that by default if standard output is not a terminal.
Oh that quite weird then :|
It does not show the path of installed binary though. however nice idea (which++) :)
Hder are some sugestions: 1) Scriots do not need to run with `bash organiser.bash` and the standard extention is *.sh, not *.bash. (I use no extention at all). You make it excutable and place it in your $PATH. 2) The config file should be in a fixed location. This is what the hidden files are for. It should be $HOME/.organiser.cfg That way it is always there, regardless of what directory you are in. 3) There is no test to see if the config file is there, nor what to do if it isn't available. The first part can be done with: test -f $HOME/.organiser.cfg &amp;&amp; source $HOME/.organiser.cfg The second part will depend on what you want it to do. Should it throw an error and exit or use default settings? 4) You only need to set the logging if the logging is true. No need to set it otherwise. 5) Use `date "+%F %X"` as it is shorter. I would als use it as a parameter. 6) You only look at move for movies. `file filename` can be of help. Bonus: As you have beginning and end time, you can calculate the run time (although there are other ways to see how long it was since the script started.) Disclaimer: I have not tried the script, just looked at it. All in all, not a bad first script.
For vim I use instead the [Most Recently Used](https://github.com/yegappan/mru) technique. Neat wrapper idea for some other tasks, such as the grep of recent edits. 
Nice! Can you post the code?
Absolutely. Just posted the link
I don't know how to capture that command's output from the shell, but it reads from `~/.viminfo`. The paths+filenames for recently edited files are stored in there, but with a little metadata sprinkled around them that you'd have to strip out. 
Yeah, it can be pretty volatile. Maybe you could consider seeding your script's data with the entries that are there, if any? Something like, if [ ! -f "${RECENTLY_EDITED_FILES_LOG}" ]; then awk '/^&gt; / {print $2}' ~/.viminfo &gt;&gt; ${RECENTLY_EDITED_FILES_LOG} fi Not certain this is a good idea, but something to think about...
This is awesome and I'd love to use this. Can you please provide an open source license for this? 
The link should include license now
Could you elaborate as to what advantages the double brackets have? I realise the portability thing, but I can't particularly tell the difference in semantics between the two. Thanks for the tip about capital vars. Very enlightening about `set -u` and `set -e`, thanks. Good point about it being too chatty. I should probably dial it back. Thanks for everything.
Ugh, you gotta pastebin that, man, reddit's markdown and that content don't mix well. As an aside, I like how you made the arms/legs different colors to lend to the illusion of a full cycle of movement. Nice touch. 
https://mywiki.wooledge.org/BashGuide/TestsAndConditionals section 4
 $ cat file.txt apple car guitar dog fire cat apple fire skye grass fire grass guitar car rose skye car apple dog rose $ egrep "apple|car" file.txt apple car guitar dog fire cat apple fire skye grass fire grass guitar car rose skye car apple dog rose djc@acerbox:~/jjjj$ 
to show multiple lines around a match you can use the `-A` option for after the match context, like `grep -A 3 "Words you want to have in line" filename` to grep without any order you can use a function: grep_pipes_f () { local i local last=${@: -1} local res="$(grep "${@:1:1}" "$last")" for i in "${@:2:-1}" do res="$(grep "$i" &lt;&lt;&lt; $res)" done echo "$res" } 
Record the path locally, then include it in the appropriate part of the scp command.
Thank you very much. I will give this a try. 
Oh yeah my mistake. But as you can see the input is a variable. I grep with $words so I can't type them one by one.
#!/bin/sh words="apple car" t1=$(tempfile) t2=$(tempfile) cat file.txt &gt; $t1 for word in $words; do grep "$word" $t1 &gt; $t2 cat $t2 &gt; $t1 done cat $t1 
If this is a homework assignment, be ready to explain what `$0` does, what `basename` does, what `pgrep` does and why it has `-c` and `-x`. This is probably a lot cleaner than the answer a professor would expect from a student. 
I'm going to try to sneak this in at work somehow. Thanks!
Thank you.
&gt;:browse oldfiles Niceeeee!
 Can you please try this : [https://www.reddit.com/r/commandline/comments/9xld4n/rtv\_is\_terminal\_reddit\_viewer/](https://www.reddit.com/r/commandline/comments/9xld4n/rtv_is_terminal_reddit_viewer/)
When I grow up I want to be like op. awesome. Thank you !
Thank you for the feedback :-) good tips! Will look into it. 
A few more alternatives: $ eval cat file.txt $(printf "| grep --color=none '%s'" $words) $ eval cat file.txt \| awk \'$(printf " &amp;&amp; /%s/" $words | cut -d\ -f3-)\'
Yes it's a homework. It isn't working when there are only 2? 
also can you explain me the steps please? 
Use mutual exclusion. See [BashFAQ 45](http://mywiki.wooledge.org/BashFAQ/045)
In order to learn something you should be thankful to this fellow redditor to give you a very clean answer. In order to make proper use of it, research the components yourself and what the flags are doing. This way you will not only have an above average answer at hand hand when you are in class, you will also have the base knowledge to explain it to others and teach them how it works. Embrace the learning effect and stop searching for shortcuts to knowledge. There aren't any. 
Maybe while IFS=" \'\",.\?\!" read -r word; do echo "$word" done &lt; "$filepath"
 if [ $# -ne 0 ] &amp;&amp; [ $# -ne 2 ]; then # error mesages exit 1 fi Above wil take care of any number of arguments other than 0 or 2. After this point if [ $# -eq 0 ]; then echo $(( RANDOM % 6 + 1 )) else for (( i = 0; i &lt; $2; i++ )); do for (( j = 0; j &lt; $1; ++j )); do echo -n "$(( RANDOM % 6 + 1 )) " done echo done fi
I'd start by setting up flows for those 3 scenarios. you can use $# to see the number of parameters; try writing the code to recognize the scenarios. pseudocode example: if $# == 0: echo "0" else if $# == 2: echo "2" else: echo "error" then you can work on what to do in each of those cases instead of just printing text. 
thanks, this worked! The only flaw is that it doesn't skip the '.' at the end of the sentence. So I get the word 'wall.' in my output. Any chance that there is a fix for that?
Are you familiar with man pages? man basename man pgrep This will tell you mostly everything there is to know about these utilities.
You can send variable contents into the input stream of a program with `&lt;&lt;&lt;` or with `echo` and a pipe: grep -o k &lt;&lt;&lt; "$sentence" echo "$sentence" | grep -o k The echo method is a bit incorrect as there could be parameters for echo in $sentence. To be really correct, you should use printf: printf "%s\n" "$sentence" | grep -o k It's also possible to do this completely differently, with things that are built into bash, without any external tools like grep. Here's some experiments at the command line about how to do this. First, this here is the variable: $ foo=hello Here's how to address single text characters in there: $ echo ${foo:0:1} h $ echo ${foo:1:1} e $ echo ${foo:2:1} l $ echo ${foo:3:1} l $ echo ${foo:4:1} o This here returns the length of the text content in the variable: $ echo ${#foo} 5 You can then use a "C" style loop to cycle through the text characters of the variable: for (( x = 0; x &lt; ${#foo}; x++ )); do echo "one of the characters:" "${foo:x:1}" done In the loop you can then compare with your "k" and count those.
The echo method can break for certain contents of $sentence. For example: $ sentence="-n" $ echo "$sentence" It prints nothing because it thinks the "-n" is a parameter. Printf shows the correct contents: $ printf "%s\n" "$sentence" -n This is a bit obscure, but that's the reason why printf is recommended for scripts.