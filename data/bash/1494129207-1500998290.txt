No problem *fuckyeah.jpg*
Yeah if only I could tell all my art directors to stop using spaces in their file names and weird characters in their folder names. This works really really well. Thank you!
I don't about `ffmpeg2` syntax.. this might help * use http://www.shellcheck.net/ to catch known problematic syntax issues.. * instead of using `FILES` variable, use `for input_file in /home/oshirowanen/Desktop/UntitledFolder/*.MP4` * quote your variables `"$input_file"` * for output file, `"${input_file/.mp4/-1.mp4}"` will give `dummy-1.mp4` for `dummy.mp4` if http://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion is available
I would do it like this: ``` #!/bin/bash FILES=/home/oshirowanen/Desktop/UntitledFolder/*.MP4 for input_file in $FILES do ffmpeg2 -i $input_file -vf vidstabdetect=shakiness=5:show=1 dummy.mp4 ffmpeg2 -i $input_file -vf vidstabtransform,unsharp=5:5:0.8:3:3:0.4 $(echo $input_file | cut -d "." -f1)-1.mp4 done ``` Edit: Not particularly the cleanest of ways, but it was the only thing I could think of to name it Edit 2: Did this in mobile, and it looks like my codeblock is messed up 
if you want to do a dry run, put 'echo' in front if your ffmpeg2 commands, eg: echo ffmpeg2 -i $input_file -vf vidstabdetect=shakiness=5:show=1 dummy.mp4 then execute :)
The output could just be something like this: `"${input_file%.MP4}-1.MP4"` Or why not have the stabilized versions in a separate directory: `"stabilized/${input_file}"`
Holy shit that site sucks ass. The first one that the link had was completely ambigous and there was no explanation of what the script should do. It only said: figure out why this doesn't work. No explanation of what the script should do in the first place. What kind of retard created this shit.
good point, also I didn't notice the upper case used...
Came here to suggest aria2c. 
When you do `ls *.mp4`, the list of files is already created by bash itself in the `*.mp4` part of the line and `ls` will basically just print it. Instead of reading it back, you can directly work on that `*.mp4` list like this: for file in *.[Mm][Pp]4 do ... done There's also somewhat obscure reasons on how reading the output of 'ls' can fail, described here: http://mywiki.wooledge.org/ParsingLs
appreciate the idea but i figured it out. thanks anyway!
Judging by your command you like to use cat when you don't have to. :-) cut -d ' ' -f1 ~/.bash_history | sort | uniq -c | sort -nr | head -10 Here's my top 10: 339 ffmpeg 283 scp 244 sudo 118 ssh 103 clear 82 xz 66 ./bin/vpstun.sh 47 find 42 wget 35 df 
Mine are (used $HISTFILE instead): 87 jstar 75 git 56 exit 50 ls 50 cd 29 bash 25 echo 23 zsh 23 man 22 eval 
I guess I should explain ./bin/vpstun.sh then. It establishes a tunnel to one of my VPSs, which I then use as a poor man's VPN. Won't post the entire script because it does a load of extra crap, but basically: Check our exiting WAN IP: curl -Ss whatismyip.akamai.com Establish the tunnel: ssh -p 2222 -S $HOME/logs/ctlsocket -D 8080 -C24fNM eddie314@domain.com Check our WAN IP using the tunnel: curl --socks5-hostname localhost:8080 -Ss whatismyip.akamai.com Using the tunnel for web browsing: chromium-browser --proxy-server="socks5://localhost:8080" --host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" https://start.duckduckgo.com Killing the tunnel: ssh -p 2222 -S $HOME/logs/ctlsocket -O exit eddie314@domain.com Also, it's probably just as fast, but you can use Ctrl+D instead of typing exit and enter. 
I use the same computer for work and personal. Apparently most of my current bash history revolves around connecting to customer servers and working on DNS issues for them. (Most of my `for` loops are to iterate through gobs of IPs / hostnames to generate sets of DNS.) 2750 616 ssh 125 host 85 vim 72 for 68 cd 61 ls 58 dig 49 mtr 47 grep 
I'm also a specialist of useless use of cat. . . 1470 cat 1027 ll 776 cd 308 apt-get 276 nano 270 tail 232 rm 177 wget 136 grep 128 date 111 ./stats-rt.sh 105 08/03/2016 94 tmux 86 htop 85 clear 84 reboot 76 rrdtool 65 curl 63 iptables 62 exit
 cat ~/.zsh_history | cut -d ';' -f 2- | sort | uniq -c | sort -nr | head -20 263 upd 210 ssh pi 178 exit 91 tmux 81 ./start.sh 81 ssh willem 72 ssh -X willem 51 lnav 50 conky 48 systemd-analyze blame 43 sudo apt autoremove 39 systemd-analyze 37 ssh willem@antergos 33 neofetch 29 ls -al 28 systemd-analyze critical-chain 28 sudo systemctl stop sshd 28 sudo systemctl start sshd 27 sudo reboot 27 crontab -e
In order to test readline settings. It's really hard to configure...
Mine: 3513 ack 1604 git 1557 npm 1467 gulp 1328 z 827 &lt; an alias &gt; 654 cd 532 &lt; an alias &gt; 517 pull 490 subl 233 &lt; an alias &gt; 232 push 200 node 157 f 154 branch 147 python 140 &lt; an alias &gt; 139 grunt 127 ./makehosts 124 sudo
 [gluon@hydrogen ~]$ cut -d ' ' -f1 ~/.bash_history | sort | uniq -c | sort -nr | head -20 1108 ls 900 cd 512 nano 382 hugo 253 ps 198 less 155 history 146 rm 136 su 120 go 113 vim 89 export 76 mv 67 yum 63 $HOME/sbbs/exec/sbbs 59 telnet 59 tar 59 make 57 mkdir 51 kill
 564 git 323 ls 206 cd 129 vi 119 sudo 47 cat 44 exit 42 ssh 37 task 32 clear
My ZSH must be configured in a way where it dedupes or something. I've only got 1's
jesus.... 4139 git 2271 cd 2022 ls 842 subl 353 clear 313 npm 256 nodemon 204 touch 165 mkdir 129 rm 
Try replacing the [A-Z] with [\^a-z] Edit: markdown escape character
Yea but wouldn't that just remove every word not containing lowercase letters? *THEIR* would still be removed from the line *THEIR ships*, and that would also get rid of the timing
Is special for begin of line and $ is special for end of line. If you want to use their special powers you do not escape the ^. Also inside character classes \ is literal. And you're only matching one, you don't have a * or a + after the character class. so as written you're matching a literal ^ followed by a single A-Z or literal \ or lower case s immediately followed by end of line.
Though you should escape the \^s :P Yeah. That did nothing. I'm now trying to create a script to do it, but I would be glad if there was a one-liner. There probably is though
Neat little website called regex101.com that lets you test regex live. They've got a regex support channel #regex@freenode.net to help too
 85 time 84 DefaultVideoPlayer3Gui 43 cd 42 FilterSrt 26 TextEncrypt 21 VideoFileInfo 21 ./t 17 DefaultVideoStreamPlayerGui 16 echo 15 sum 15 PlayAudioFilesList_Music 15 mpv 9 TextDecrypt 9 ls 9 ln 9 inpath 8 /home/manolo/documents/InetExchange/Up_Pompeii/bin/NewVideos 6 ./t2 6 dates 5 date Note: Very atypical, would have been completely different one month ago.
Okay, I made a solution that is quite lengthy, quite slow, quite ugly, but it works flawlessly: #!/bin/bash cd $(dirname $0) mv "$1" "$1_bkp" touch "$1" while read line do if [ $(echo "$line" | grep -E '[a-z]|[0-9]' | wc -c) != 0 ] || [ $(echo "$line" | grep -E '[a-z]|[0-9]|[A-Z]' | wc -c) == 0 ] then echo "$line" &gt;&gt; "./$1" fi done &lt; "$1_bkp" Basically, if a line contains more than 0 of lowercase letters or numbers, I'll keep it, and if the line has no letters or numbers, I also keep it. The latter is for the empty lines that divide the file, as `[ -z "$line" ]` didn't seem to work... But thank you all for your time and help! If you've got any other ideas for a one-liner, I would greatly appreciate it. Everything's faster than this solution.
Like this: 2 00:00:03,200 --&gt; 00:00:05,199 FOOTSTEPS APPROACH 3 00:00:05,200 --&gt; 00:00:06,840 METALLIC SQUEAKING [...] 47 00:03:19,323 --&gt; 00:03:21,432 - Why pick on me? - Well, I noticed you. 48 00:03:21,713 --&gt; 00:03:23,705 Yeah, but why?
These look like examples of getting stderr across a pipe. When these redirects start getting long, I have an easier time reading them right to left. In your first example stdout is passed to a new file descriptor, 3, and stderr to fd1(stdout). This only allows for stderr to cross pipe and be logged via tee. Finally, fd3(originally stdout) is redirected to fd1 again. This allows both stderr and stdout to be viewable on the command line while only logging stderr. The second example appears to accomplish the same by feeding stderr into tee without the need of a pipe, eliminating the need for a 3rd fd to juggle fd1 and fd2. Your last example throws away stdout and passes stderr across pipe as stdout. Fd3 is closed, fd2 goes to fd3, fd1 to fd3, and fd3 to fd1. Edit: I don't see why sleep would be necessary. The previous command is completed once the sleep command is ran. Regarding order, I'm not sure. I don't believe order has much effect as the command grouping would; however, I would still expect the output to be received as it were streamed from the original command. I don't have time now to test and confirm, but someone else may know offhand.
If you don't have new perl you could also just check for carriage returns. perl -lpe 's/^[[:upper:][:punct:]][\s[:upper:][:punct:]]*(\r)?$/$1/' file
Pasted some log in the terminal instead of gedit
Guess that makes sense! Thanks for indulging my curiosity!
I really like `ls` 909 ls 194 git status 152 npm start 142 git add -A 140 git push 129 cd .. 125 php artisan serve 97 git commit 93 vi package.json 89 gulp compile
what's your ll alias? i use ls -lahG
This is deep. I have only ever used Alias in my bashrc. Interesting way to use them. 
Yup, perfect;) Thanks. You forgot one ", there, btw;)
http://mywiki.wooledge.org/BashPitfalls ... not that you couldn't find it, or were looking. I'm sure this article has a few gems from BashPitfalls in it, too lazy to check.
I suppose you could create a function and then just call the function 3 times or however many times as needed depending on the executable and extension. #!/bin/bash convertCaption () { cd "$1" for file in "$2"; do "$3" blah blah done } convertCaption "/path/to/directory" "search command" "convert command" You could alias the commands by hard coding them as variables in the script and then using something like: $ myscript.sh "/path/to/whererver" "pycaption|ffmpeg" "scc|vtt|etc|etc" But that kind of gets away from having them run all in a single script. I suppose another method would be to put everything into variables and then have the bash script accept either a specific extension (scc, vtt, whatever) or "all" and then you could loop through the directories to find the existing extensions and then proceed from there.
FYI, your comment got flagged by Reddit’s spam filter – it seems to happen pretty reliably when people link to cyberciti.​biz, as I recall.
This! And a case esac instead of the if for file in *.en.vtt or *.en.scc or *.en.vtt do [[ -f $file ]] &amp;&amp; \ case "$file" in *\.en\.tt) # Do something with "$file" ;; *\.en\.scc) # Do something with "$file" ;; esac done 
I'd like to add that it's generally not a good idea to use all caps for variable names. It opens you up for name space collision with environment &amp; reserved variables. http://wiki.bash-hackers.org/scripting/style#variable_names 
Thanks, I think I understand now--so in `{ command 2&gt;&amp;1 1&gt;&amp;3`, `2&gt;&amp;1` means redirect stderr to stdout *at that point*, and the following `1&gt;&amp;3` redirects *only* stdoutto fd3? In other words, `1&gt;&amp;3` does *not* include the stderr that was redirected to stdout via `2&gt;&amp;1`? I think that's the cause of my confusion. Also, &gt; Any redirections that are performed for a command only apply to that command and don't affect the parent shell or anything else, so there's nothing to reverse. So are you saying that the last example [here](http://burgerbum.com/stderr_pipe.html): &gt; `( COMMAND 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 | grep -v ANNOYING_ERRORS ) 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 | grep TARGET_STRINGS` is redundant or is it still needed because it's in a subshell and is considered part of the same command? 
Thank you for pointing that out, I just corrected it, however, it is still not getting me where I want. 
You also have an `elif` following an `else` which doesn't make sense. BTW, you can run `mkdir -p "$LOCAL_DIR1"` even if the directory exists. You don't need the `if` branch.
hey thanks, I am not good at this, I was basically trying every options possible without knowing if it would work. May you just correct the like to have the creation of folder if not existent and the cp -lr happening. also is there a way of doing a real hardlink instead of a cp -lr? Seems lftp doesnt like symlinks.
The program `ln` can be used to hardlink individual files. Hardlinking can not span file systems and you can hardlink only files, not directories.
&gt;It opens you up for name space collision with environment &amp; reserved variables. ... and in fact in this case it very likely will. `$USER`, while not a standard builtin/internal variable, is often set by default anyway.
Your quoting is messing things up. You should either save the config in a separate file and then copy it over to wherever you need. Or, look into using heredoc, which can be made to not interpolate variables in it if you quote the end-of-heredoc marker.
Thanks - that does fix the issue I was having, but at the same time creates another. My post was a little vague, but there is one variable that's defined in the script ($cachet_url) that actually should be substituted with its value - is there any way to accomplish both? The original script is [here](https://github.com/thetechgy/scripts/blob/master/CachetSetup_CentOS7.sh) if the context helps
Use an if statement with break. If a certain condition is met, break will exit the loop and go on the the remainder of the script.
It depends on what you want to have break the loop, but you could use a `while` loop and define your condition under which you want the loop to continue. Once the condition is met the loop will exit. 
I have managed to write the full script, I also have managed to circumvent the mkdir command. However now my script the executed by deluge creates the links like its supposed to, however it also create a addition link of the folder inside the folder. This is what I have: #!/bin/bash torrentid=$1 torrentname=$2 torrentpath=$3 if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr-kc" ]]; then cp -lr "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr-kc/LFTP/$torre$ fi if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/movies-couch" ]] ; then cp -lr "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/movies-couch/LFTP/$torre$ fi if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/movies-beast" ]] ; then cp -l "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/movies-beast/LFTP/$torren$ fi if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/radarr" ]] ; then cp -lr "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/radarr/LFTP/$torrentname" fi if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr-beast" ]] ; then cp -lr "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr-beast/LFTP/$to$ fi if [[ "$torrentpath" = "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr" ]] ; then cp -lr "$torrentpath/$torrentname" "/media/dmah/kcdxb/private/deluge/data/completed/tv-sonarr/LFTP/$torrentn$ fi 
Variable expansion doesn't happen inside single quotes, change them to double quotes.
Every time I see someone suggesting "man trap" I can't help but think of Star Trek.
I just checked. Great tool! Thanks for advice.
Here is an example of using a trap in a script I wrote awhile back. [Dropbox Link](https://www.dropbox.com/s/64b73egj0vigq3a/ping.sh?dl=0)
vim = more than the next 19 combined
Wow, thanks! That's a lot of input, a lot more than I was hoping for :) I will take a look at your points and try to implement them. 
Might try a case statement given these requirements.
Or just skip the printf completely output+=("DESCRIPTION:${id} 6 Month Follow-Up")
Was this a reply to something? 
Yes it was a reply to ropid, but I missed. Then I saw, and used, your suggestion. Thanks!
Oops sorry I left off the text that would show a little more of what I mean. echo "What is your race? Elf, Dwarf, or Orc?" read race If [ race = Elf ] And then the rest of what I wrote. Does that make it clearer at all? I don't full grasp it so I don't know how to explain it. I'm sorry :/ 
Use case-insensitive grep to see if it starts with a Y: if echo $need | grep -iq '^y'; then echo success; fi or use bash's regex comparison: if [[ "$need" =~ ^[Yy] ]]; then echo success; fi or use tr to convert to lowercase, then compare (&lt;&lt;&lt; is a bash-ism you can use instead of "echo | " ) if [ "$( tr '[:upper:]' '[:lower:]' &lt;&lt;&lt; $need )" = 'yes' ]; then echo success; fi This wil check if the word need starts with the letter "y".
This will do exactly what you're trying to by converting `$Need` to lowercase before comparison: [[ $(echo $Need | tr '[A-Z]' '[a-z]') == "yes" ]] &amp;&amp; echo "great!" Reading from the inside out: * `tr '[A-Z]' '[a-z]'` *translates* everything specified by the first argument which happens to be any uppercase character in this case to the second argument which is the corresponding lowercase character * `echo $Need | tr ...` *passes* (loosely speaking) the value of the $Need variable (which can be "Yes" or "yes") to the `tr` command to convert to lowercase. * `$(echo ...)` Anything between the parentheses in `$(...)` will be executed and that's how we're doing the conversion before comparing. A downside is that this will also match all combinations of upper and lowercase characters such as yeS, yEs, yES, YeS, YEs, YES. A better way to do the same thing is [[ "$Need" == [Yy]es ]] &amp;&amp; echo "great!" since this only matches Yes and yes.
The `return $( true )`/`return $( false )` bits are nonsense. Either way, they return "true". Both `$(true)` and `$(false)` evaluate to an empty string. Without any quoting, field separation turns that in to a list of 0 arguments. So then it just runs `return` without any arguments. Now, `return` without any arguments is equivalent to `return $?`; and `$?` is always 0 after an "if" check. `return 0` to return true, and `return 1` (or anything 1-255) to return false. If you must; `readonly true=0; readonly false=1`, then write `return $true`/`return $false`. But also, you just implemented the age-old "idiom" `if (condition) { return true; } else { return false; }` that novice programmers have been made fun of for since time immemorial. Remove the `if`, just return `condition`. In Bash, in absence of a explicit return value, `$?` is used. So instead of writing ... if [[ condition ]]; then return 0 else return 1 fi } Just write ... [[ condition ]] } Then, when you use `is_number`: elif ! $( is_number "$1" ); then you capture its output, and try to run that as a command; instead of using its return value. You should have written it as elif ! is_number "$1"; then You need to quote your arrays: write `"$@"` instead of `$@`. Actually, you should quote most every variable. But you don't need to quote `$?`, which is ironically one of the few things you do quote. You wrote `[[ ! "$input" =~ "[[:alpha:]]" ]]`. Don't quote the right-hand-side of `=~`; that will cause it to be a string literal instead of a regex (this is useful for when matching against an expression that contains both a regex part and a literal part that you don't want to have to worry about escaping).
Paste it into [ShellCheck](http://www.shellcheck.net/). A couple of things I noticed at a very quick glance: * `elif ! $( is_number "$1" ); then`: Remember, `$(...)` runs a command and gets replaced with its output. `is_number` produces no output, so I'm not actually sure how Bash ends up interpreting this, but you should just be doing `elif ! is_number "$1"; then` instead. * `return $( true )` / `return $( false )`: `true` and `false` are binaries that exit with exit status of 0 and 1 respectively. They don't produce any output, so `$(true)` and `$(false)` evaluate to empty strings. So these are both equivalent to `return` with no arguments, which means that it returns the exit status of the last command executed. In this case, the last command executed happens to be either `true` or `false`, but you should just write `return 0` and `return 1` instead.
Ah ha! That's a useful site that I wasn't aware of. Thank you.
There are shellcheck plugins for vim that make good habits second nature. 
Note `-E` does not work with older versions of GNU sed, you will need to use `-r`. Also I think you meant something like grep -f &lt;(printf '%s\n' "${@:1:((${#}-1))}") "${@:$#}" You are searching for the patterns from the first args in the last arg, not the other way round. And even if you were doing it the way you suggested grep -f /dev/stdin &lt;( cat "${@:$#}" ) should be written as grep -f "${@:$#}" as &lt;( cat file ) is always redundant. 
Hey awesome, that last one is simple and gets the job done. Thank you so much!
Yeah that makes it a lot easier for me since I don't know a lot of the language in some of the other replies. Thank you very much! 
Another thing, you don't need to repeat the `output+=( ... )` in every line. You can also write it like this: output+=( "BEGIN:VEVENT" "DTSTART;VALUE=DATE:${datestring}" "DTEND;VALUE=DATE:${datestring}" ... ) 
"Gremlins. Here's Rima." Lol
&gt; substrings=( "${@:1:$#-1}" ) grep -E "${substrings[@]/#/-e}" "${@:$#}" What I'm looking for is logical AND, not logical OR. In other words, a line returned must include *all* the substrings specified. With `grep`, AFAIK it can only return lines that satisfy only one substring. The only way I could get grep to return lines that contain all substrings is to repeatedly use it in such a manner: `grep -i &lt;substring&gt; &lt;file&gt; | grep -i &lt;substring2&gt; | grep -i &lt;substring3&gt;...`--this is not efficient relatively speaking.
You know mplayer can play playlists right? No need to loop through the file for that. As far as the error goes, try removing the space at the end of the $PLAYER variable and putting a space between $PLAYER and $file $PLAYER $file 
This totally did the trick thank you.
Yeah I know that it does have a playlist function, but I need to store the meta data of the current song playing in a file to review later and I'm not sure how to do that. Someone suggested pipelining but this seemed easier. 
Ah, then I'd go with the `multimatch` function from [BashFAQ 79](http://mywiki.wooledge.org/BashFAQ/079): &gt; If you need to scale the awk solution to an arbitrary number of patterns, you can write a function like this: &gt; # POSIX multimatch() { # usage: multimatch pattern... awk ' BEGIN { for ( i = 1; i &lt; ARGC; i++ ) a[i] = ARGV[i] ARGC = 1 } { for (i in a) if ($0 !~ a[i]) next print }' "$@" }
Thanks! I was able to implement all of your suggested fixes and verify that everything now functions. I still have a list of improvements I'd like to make, but it's fully functional as-is in my testing. I've pushed the updated version to my github repo Remaining to-do: * Find more dynamic method of commenting default server block in /etc/nginx/nginx.conf rather than specifying line numbers * General clean up and improve consistency throughout * Set firewall rule exception creation to only run if the firewall package is installed, or hide output * Re-enable selinux and verify working * Add general setup like setting hostname for zero-configuration install? Currently I'm running an OS setup script to do basics like setting hostname
 Does that make one array element per line, or are all those lines in one element? 
Every line will be its own element. What's one element basically depends on the spaces, and a normal space and a line-break count the same. You could theoretically also write everything on one line if you leave a space between every element. Here's an example on the command line: $ x=( &gt; "a" &gt; "b" &gt; "c" &gt; ) $ echo "${x[@]}" a b c $ echo "${#x[@]}" 3 Everything on the same line: $ x=( "a" "b" "c" ) $ echo "${x[@]}" a b c $ echo "${#x[@]}" 3 If there's no spaces, bash fuses the three words into one word: $ x=( "a""b""c" ) $ echo "${x[@]}" abc deep@hrhr ~ $ echo "${#x[@]}" 1 
So I adjusted my code, and now I have a new problem. #!/bin/bash # This is encrypted MPLAYER='/usr/bin/mplayer' playlist='/home/pi/Music/play.pls' music_dir='/home/pi/Music' # Play arguments on command line if they exist IFS=$'\012' while read file do a="$(echo | ffprobe -loglevel error -show_entries format_tags=artist -of default=nopr int_wrappers=1:nokey=1 "$file")" b="$(echo | ffprobe -loglevel error -show_entries format_tags=album -of default=nopri nt_wrappers=1:nokey=1 "$file")" printf "%s, %s, " "$a" "$b" &gt;&gt; t.txt ${MPLAYER} "${music_dir}/${file}" done &lt; "${playlist}" exit 1 It starts to open the file and then I get this error &gt; &gt; No bind found for key 'g'. &gt; No bind found for key 'y'. &gt; No bind found for key 'B'. &gt; No bind found for key 'l'. &gt; No bind found for key 'l'. &gt; No bind found for key 'L'. &gt; No bind found for key 'O'. &gt; No bind found for key 'b'. &gt; No bind found for key 'B'. &gt; No bind found for key 'L'. &gt; No bind found for key 'l'. &gt; No bind found for key 'y'. &gt; No bind found for key 'l'. &gt; No bind found for key 'R'. &gt; No bind found for key 'b'. &gt; No bind found for key 'l'. &gt; No bind found for key 'g'. &gt; No bind found for key 'B'. &gt; No bind found for key 'l'. &gt; No bind found for key 'M'. &gt; No bind found for key 'O'. &gt; Dead key input on file descriptor 0 &gt; ===== PAUSE ===== 
You can use other characters besides `/` for the patterns in sed, for example: \|abc| !d \%abc% !d \@abc@ !d Anyway... you could do it all inside bash. It might be fast enough. I came up with this here: seds() { local file="${@:$#}" local patterns=( "${@:1:$#-1}" ) local line x while read -r line; do for x in "${patterns[@]}"; do if [[ $line != *$x* ]]; then continue 2 fi done printf "%s\n" "$line" done &lt; "$file" } If it's too slow... here's the code translated into a perl one-liner: seds () { perl -E 'LINE: while (&lt;STDIN&gt;) { for my $s (@ARGV) { next LINE if index($_, $s) == -1 }; print }' "${@:1:$#-1}" &lt; "${@:$#}" } The one-liner in there takes search patterns as parameters from the command line and then goes through its stdin. If you need regular expressions for your search (because that's what the original sed code did), you'd change the comparison in the bash code into this: ... if [[ ! $line =~ .*$x* ]]; then ... The version of the function using perl would change into this: seds () { perl -E 'LINE: while (&lt;STDIN&gt;) { for my $s (@ARGV) { next LINE if not /$s/ }; print }' "${@:1:$#-1}" &lt; "${@:$#}" } **EDIT:** Fixed a mistake about the **\c**pattern**c** sed thing.
Since sed seems to require "sacrificing" a character that could mess up the function--if it's not `/` it's something else), I'm using the function with perl which works nicely so far and seems to not have this limitation. Do you know how I can make it case insensitive? Also going to see if I can somehow get the output to highlight the substrings that matched (probably not)--for example, grep has `--color` (although for my instance where I pipe `grep -i` repeatedly (e.g. `grep -i &lt;substring&gt; &lt;file&gt; | grep -i &lt;substring2&gt; | grep -i &lt;substring3&gt;...`)--of course only the last substring is highlighted though). The bash code was noticeably slower, but I will take note of it for future reference. I initially got the sed command from your comment in another thread, by the way. Thanks a lot. 
Hey there. &gt; Do you know how I can make the function with perl case-insensitive? Sure - just add the `i` modifier to the `//` e.g. next LINE if not /$s/i 
This is exactly what I'm looking for, thanks so much for all the thorough answers. Saved me many hours of figuring it out but still learned a lot in the process--you are insane :)
/r/bashonubuntuonwindows/ might be a better option? `set` should give you a usage line though, for me (Ubuntu Mate) it looks like this: $ set --help bash: set: --: invalid option set: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...] 
Curious, what's the reasoning for deleting the oldest three? I'd just run 'find /home/theuser/thebackup -mtime +7 -delete', which would delete any file older than 7 days.
I want 30 logs files. So once I reach 30, it should delete the oldest 3.
Oh, but they're all being created at the same time? Then that find command should work for you.
No. Lets say the limit was 4, instead of 30. The set of "12:00:00" would get deleted. Then when the 16:00:00 set gets created the 13:00:00 set would get deleted and so forth.
&gt; I think it would be way easier (and more maintainable) to figure out a time period for retention, rather than number of backup files What would be easier to switch: A time period for retention (I said 7 days...maybe I would switch it to 10) or a number of backup files (I said 30 as a example, maybe I want to do 40)? If both reach the same goal... 
What about if I dont have 30? Say 10...wouldnt I need to do a check (using a if)?
How do you at least do the "xdotool right-click a" part?
This is not the right sub for my comment, but hey. # cat testfile testing 1 # blah # foo bar #foo Let's try some Perl! # perl -nle 'print $1 if /^(.+)#/' &lt; testfile testing 1 bar 
Yes. You're absolutely right. My bad!
This is the appropriate behavior for running it in the foreground and stopping with crtrl+C. $ ./my_program You can send the job to the background using a single ampersand: $ ./my_program &amp; This way, you can use the terminal while the program runs in the background. However, ctrl+c will stop the background op. The below command will not be stopped by subsequent ctrl+c: $ nohup ./my_program &amp;&gt;/dev/null &amp; You can also use programs like screen to run it in a session that continues on the system even after logging out. $ screen $ ./my_program &lt; ctrl + A, D&gt; # detach $ logout &lt;... some time later ...&gt; $ screen -ls # list sessions $ screen -r &lt;session&gt; # reattach to session &lt;ctrl+c&gt; # quit application Screen is probably more server-centric, e.g. running programs on a server that is on 24/7, but that the client machine cannot keep a live connection to throughout execution.
To add to /u/Sigg3net valuable advice, when you have a foreground job that you wished you'd started in the background, all is not lost... Issue a `ctrl-z`, which will suspend the foreground job, then type `jobs` which will show all your jobs with numerical ID (not PID!), then type `bg %&lt;numerical job ID&gt;` which will send it to the background and resume execution. But, this will still be lost when you log out, so what to do? Once the job is backgrounded, type `disown &lt;numerical job ID&gt;` and now it's as though you had started the process as `nohup &lt;program&gt; &amp;`.
It's telling you that it's running and what its pid is. Normally you'd still see output. Express might be squelching that because it got backgrounded.
 grep -Eo "^[^# ]+" Also, you should probably use a better list next time (`pacman -Qqe`)
The number displayed when backgrounding a process 6132 is the PID (process id, I think). With it you can terminate or kill the process, like so: $ kill 6132 You can get the same with any program you've just executed with echo $? Or use psgrep. If you used my 2nd suggestion (nohup), you redirected output messages to /dev/null, essentially ignoring messages. If you need the messages, just remove this part: &gt;/dev/null &amp; Like this: $ nohup ./my_program &amp;
This is a really nice, elegant solution! I would make one teeny-tiny amendment to handle more use-cases. grep -Eo "^[^#]+" # remove space from negated group This also captures lines where there is whitespace before the comment character. (Using the `testfile` in another comment) perl -nle 'print $1 if /^([^#]+)/' &lt; testfile testing 1 bar baz 
Job control in bash is fairly simple. If a program is holding the prompt hostage, you can suspend the program with a Ctrl-Z. This should temporarily halt the program and bring back the prompt. If you want the program to continue in the background, just type 'bg' and the program will continue running and you will have use of the prompt again. If you just want to run something and resume the program you can do your commands and finally type 'fg' to resume the program in the foreground. All of this is describing bash behavior on Unix/Linux and the job control stuff may or may not work on your version (let us know!)
With Vim you can use internal command ":%s/Foo/FOO/g" 
&gt; What is the best way of handling different sorts of input? (From stdin, from file, from incoming pipe...?) Meaning what, exactly? 'STDIN' is data from an 'incoming pipe', typically. Are you looking for best practices for reading STDIN or a file? &gt; I also have the following code, what do you think about it? What is the best practice? You can use https://www.shellcheck.net/ to get a good idea of some wrong things you might be doing. Grouping `if` and `while` in `{}` is wholly unnecessary.
The ~one-liner started as an exercise on \#archlinux-fr on freenode, to help people upload their issues to a pastebin. It was then devoid of comments. I simply took it and made it into a script of its own. Also, in the time you would need to track those people down, you could dissect the function and comment it yourself :)
&gt; Sure, but don't the extra braces add to readability? Not when 99.999% of the rest of the shell scripts I interact with do not. Personally, I don't find it very useful. I find matching indentation much more visually helpful. &gt; I want to do both in the cleanest/best way possible, at the same time, in the same script, to handle numerous use cases. It depends on how you want to process the data. You can do something like: while read -r line; do do_something_to_a_line_of_a_file $line done &lt; /path/to/some_file At the same time, if you wanna read STDIN passed to your script: while read -r line; do do_something_to_line_of_STDIN $line done Those patterns will not work for all your situations, and could possibly be improved.
The if statement isn't that complex; the curl args could be explained in a comment, but really the comment itself is not helpful, which is my gripe.
So, the `-` or `+` by themselves mean it's an 'exclude' or 'include' rule and this `s` is a modifier that can be used on them. The `s` is described like this in the man-page you get with `man rsync`: &gt; An s is used to indicate that the rule applies to the sending side. When a rule affects the sending side, it prevents files from being transferred. The default is for a rule to affect both sides unless --delete-excluded was specified, in which case default rules become sender-side only. See also the hide (H) and show (S) rules, which are an alternate way to specify sending-side includes/excludes. I don't really get what this means, but maybe someone else can explain it. You can jump to the section of the man-page where this stuff is described by typing `/^ *FILTER RULES` in the normal 'less' pager tool used by 'man'. The next section that follows one or two screens after FILTER RULES is named "INCLUDE/EXCLUDE PATTERN RULES". That one is specifically about `-` and `+` and documents the modifiers like `s`.
Put whatever is in line 80 into a function and call that function
Just an echo line that id like to skip to. 
There's no "go to line 80" thing in bash to jump over lines of code. You do what you want by turning your condition around: if [[ $cool = no ]]; then ... &lt;--- things that were in front of your line 80 fi ... &lt;--- this here is your line 80 
Oh so I can just add the fi to wherever I want it to stop at? What if there are other if statements between line 50-80?
What you need to do is be diligent with indenting your lines of code. This makes it visible how things are flowing: if [[ some test here ]]; then commands here commands here commands here if [[ some other test here ]]; then more commands here more commands here more commands here fi even more commands even more commands even more commands fi your line 80 commands here your line 80 commands here your line 80 commands here Bash goes through the stuff line by line. Those indented blocks get looked at by bash when the test on the 'if' line has a positive result, and when the test is false it jumps to the 'fi' that belongs to that particular 'if'. There's also other forms of 'if': if test; then commands else commands fi And the last one is 'if test; then ...; elif test; then ...; else ...; fi'. There's text editors that help and can do the indentation automatically, but I don't know which ones exactly. I know Emacs can do it, but configuring it and learning to use it is quite the battle. There might be an editor that's easy to get into and has a feature for this, perhaps "Atom" or "Sublime" or "Visual Studio Code".
Hey that's all very informative! Thank you very much, I'll make sure to indent properly. Seriously you rock! 
Your posted link was one of the threads I found regarding my topic. But as far as I understood this thread they work only on single files "mv 'file' $(echo 'file' | sed -e 's/[^A-Za-z0-9._-]/_/g') Replace file with your filename, of course" And I won't do that for almost 5500 single files. Plus It doesn't remove the characters but replaces them with an underscore. Like I said I would've used these commands if I'd know how to add the function to clear ALL files from these characters without replacing these invalid characters with anything. This is why I'm asking for help here. 
I don't get any results with that command. Not even sure if it's searching because right after hitting return it goes to "pi@raspberry: $" and waits for input. The same happens when I replace the dot with the path where all the sub-folders are
As I mentioned above I'd still have no idea what to change here find /path/to/files -type f -print0 | \ perl -n0e '$new = $_; if($new =~ s/[^[:ascii:]]/_/g) { print("Renaming $_ to $new\n"); rename($_, $new); }' To delete the characters and not replacing them with underscores or anything like that. 
That could mean that it found nothing with those characters. Do an experiment like this: find . -name '*' This should print every file in your location and sub-folders.
Yes, everything is printed with that command. Then I honestly don't understand why OneDrive is giving me this error message...
So instead of reading the whole thing, you just moved on to the next potential answer? find /path/to/files -type f -print0 | \ perl -n0e '$new = $_; if($new =~ s/[^[:ascii:]]/_/g) { print("Renaming $_ to $new\n"); rename($_, $new); }' Simply change `/path/to/files` to wherever the files reside that you want to process. Or `cd` to your target directory and replace `/path/to/files` with `.` Alternatively you could scroll down and read [this answer](https://serverfault.com/a/776229) and [this answer](https://serverfault.com/a/655530).
You will want to use read: while read l; do # Something with $l (which is the line value) done &lt;file.txt Since those are decimals, you can't just use the $((EXPR)) system, but rather bc. n=0 while read l; do n=$(echo "$n+$l" | bc); done &lt;file.txt echo $n EDIT: Added an echo to output the sum
A couple of things: - you should always use `read` with the `-r` switch, which allows the reading of lines that contain `\`, tho' that doesn't really apply here it's still a good habit. - using echo when reading from stdin isn't necessary in most cases with bash, as you have process substitutions as well as here-strings `&lt;&lt;&lt;`, so the bc line could just be: n=$(bc &lt;&lt;&lt;"$n + $l") 
A simple oneliner could also be: grep -v "^$" foo.bar | paste -sd+ | bc Where the `grep` first removes empty lines, then `paste` serially prints all lines with the delimiter `+` which gets piped to `bc` You can save this into a variable with the process substitution syntax: foo=$(grep -v "^$" foo.bar | paste -sd+ | bc)
Hello, please be gentle! I have not written to many programs, but like bash so thought I would give it a go... Hope you like it, any questions, I will try to get back to you ASAP, Wolfie.
First things first – your README needs to explain what your project actually is or does. Right now I don’t even know if this is a wiki *implemented* in Bash or *about* Bash.
Just use echo.
I know my Redme, is pretty crap (sorry) I will be trying to have a look at that tonight, but would like some pointers on improving the bash script if possible :) Here is a brief explanation of what it does, I have quickly taken it from the script. Search / Search (ignores the Ignore folder) - Case insensitive. - Will not search the ignore Folder. Search / "TAG's" - When creating a document, if you put the word "TAG" then a space with some tag's you would like such as "network" etc... Etc... It will only find those ones not every document with the word network in it. - Case insensitive. - Will search the ignore folder. Search / "Search All" - Case insensitive. - Will search the ignore folder. Create / "File" or "Folder" - Will create a file or folder. - Not much error checking on the name. Lists / "TODO" - When you create a file, if you use the work "TODO" it will appear in this list. - Case sensitive. - Will search the Ignored folder. Lists / "DONE" - When you have finished your "TODO", you can change it to "DONE" and it will show up in this list. - Case sensitive. - Will search the ignore folder. Delete - Search for something to delete and delete it. Browse - currently uses vifm to browse your files, will make this a variable in the shwiki folder Settings \ Help &amp; About. Settings - This will allow you to update .shwiki file, if you break it, delete it and start again. Help - This screen. About - GitHub address. 
Bash is trying to treat `"${filter_cmd[@]}"` as a filename to redirect to, but it expands to multiple words, which is an error. You need a process substitution to run the filter_cmd. ... 2&gt; &gt;("${filter_cmd[@]}" &gt;&amp;2) &gt; &gt;(tee ~/app.log)
Yes - the multiple calls to bc was killing it. It's a good practise to limit external tools and pipes in loops because of the overhead. If you're just doing a one-off, throw away then who cares, but I've seen expensive loops bring production systems to their knees. 
If that's the actual formatting - it's all on one line - then you could do something like this: $ echo '&lt;a class="class" href="id.php?id=bashbash"&gt;bashbash&lt;/a&gt; (101010)&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class="class" style="line-height:12px;"&gt; &lt;a class="class" href="id.php?id=subsub"&gt;subsub&lt;/a&gt; (101010)&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class="class" style="line-height:12px;"&gt;' | tr "&gt;" "\n" | grep "^[a-zA-Z]" | cut -d "&lt;" -f1 bashbash subsub This pulls out ?id=bashbash"&gt;**bashbash**&lt;/a&gt; and ?id=subsub"&gt;**subsub**&lt;/a&gt; I'm not sure what the rest of your requirements are without more context...
Since you are using python, it is easy with beautiful soup. You can try something like this import beautifulsoup as bs soup = bs(all_the_html) data = soup.findAll("a", { "class" : "class" }) for elem in data: print(elem["href"].split("=")[-1]) This will find all the "a" tags with "class", and spit out the last argument after the equals sign. This also works independent of the html indentation. 
Much appreciated and thanks in advance. No rush :)
I think beautifulsoup is the best long term approach, but if you just want to use grep and some regex. egrep -o '(?&lt;=id=)[^"]*'
Easy solution: [powerline](https://github.com/powerline/powerline). Hard solution: customize the prompt yourself. Also, powerline is not the only beautifier, it is what I like and use.
You can do almost everything in bash, or rather, using ffmpeg. Several GUI "video editors" in GNU/Linux are frontends for ffmpeg. I personally use it to mute videos, to insert other audio tracks, to cut / join snippets.
That `(?&lt;=...)` is Perl stuff and doesn't work with `grep -E`. The grep I have here has a parameter `grep -P` to enable Perl regex and then it works, but the man-page says it's experimental. Using your pattern through Perl itself looks like this: perl -nE 'say $&amp; while /(?&lt;=id=)[^"]*/g'
It's a lookbehind assertion, and you're right
 pkill -USR1 sxhkd makes `sxhkd(1)` reload its configuration. Try moving files around and firing this command somehow? mv ~/.config/sxhkd/sxhkdrc.tmp ~/.config/sxhkd/sxhkdrc pkill -USR1 sxhkd sleep 5 mv ~/.config/sxhkd/sxhkdrc.permanent ~/.config/sxhkd/sxhkdrc pkill -USR1 sxhkd
It doesn't do the git prompt by default? Have you tried going into a git directory to make sure? Personally, I use Oh-my-zsh with the agnoster theme: https://github.com/agnoster/agnoster-zsh-theme
I don't think you can redirect to a pipe (so `2&gt; | tee` doesn't work), but I changed it to `2&gt; &gt;( tee &gt;(grep -v string &gt; error.log))` and that works. I'm curious if anyone can think of a better alternative because nested process substitutions might not be ideal. Thanks!
Yes, the `&gt;&gt;` is looked at by bash directly. It's a special symbol like a `)`, `;` etc. You could "save" this under a function name instead of a variable name: cmd() { grep -v "already deleted" &gt;&gt; /tmp/error.log; } Then run it with: `cmd` You could use a variable like this: cmd='grep -v "already deleted" &gt;&gt; /tmp/error.log' Then run it with: `bash -c "$cmd"`.
I have figured it out. I was using default theme, which used left and right. This is not possible in shell, so I created my own theme and added what I wanted. I am currently learning creating my own segments and hopefully it goes well. Thank you for suggesting powerline :)
[You're playing with dark forces, my friend. ](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454) &gt; You can't parse [X]HTML with regex. Because HTML can't be parsed by regex. Regex is not a tool that can be used to correctly parse HTML. As I have answered in HTML-and-regex questions here so many times before, the use of regex will not allow you to consume HTML. Regular expressions are a tool that is insufficiently sophisticated to understand the constructs employed by HTML. HTML is not a regular language and hence cannot be parsed by regular expressions. Regex queries are not equipped to break down HTML into its meaningful parts. so many times but it is not getting to me. Even enhanced irregular regular expressions as used by Perl are not up to the task of parsing HTML. You will never make me crack. HTML is a language of sufficient complexity that it cannot be parsed by regular expressions. Even Jon Skeet cannot parse HTML using regular expressions. Every time you attempt to parse HTML with regular expressions, the unholy child weeps the blood of virgins, and Russian hackers pwn your webapp. Parsing HTML with regex summons tainted souls into the realm of the living. HTML and regex go together like love, marriage, and ritual infanticide. The &lt;center&gt; cannot hold it is too late. The force of regex and HTML together in the same conceptual space will destroy your mind like so much watery putty. If you parse HTML with regex you are giving in to Them and their blasphemous ways which doom us all to inhuman toil for the One whose Name cannot be expressed in the Basic Multilingual Plane, he comes. HTML-plus-regexp will liquify the n​erves of the sentient whilst you observe, your psyche withering in the onslaught of horror. Rege̿̔̉x-based HTML parsers are the cancer that is killing StackOverflow it is too late it is too late we cannot be saved the trangession of a chi͡ld ensures regex will consume all living tissue (except for HTML which it cannot, as previously prophesied) dear lord help us how can anyone survive this scourge using regex to parse HTML has doomed humanity to an eternity of dread torture and security holes using regex as a tool to process HTML establishes a breach between this world and the dread realm of c͒ͪo͛ͫrrupt entities (like SGML entities, but more corrupt) a mere glimpse of the world of reg​ex parsers for HTML will ins​tantly transport a programmer's consciousness into a world of ceaseless screaming, he comes, the pestilent slithy regex-infection wil​l devour your HT​ML parser, application and existence for all time like Visual Basic only worse he comes he comes do not fi​ght he com̡e̶s, ̕h̵i​s un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment, HTML tags lea͠ki̧n͘g fr̶ǫm ̡yo​͟ur eye͢s̸ ̛l̕ik͏e liq​uid pain, the song of re̸gular exp​ression parsing will exti​nguish the voices of mor​tal man from the sp​here I can see it can you see ̲͚̖͔̙î̩́t̲͎̩̱͔́̋̀ it is beautiful t​he final snuffing of the lie​s of Man ALL IS LOŚ͖̩͇̗̪̏̈́T ALL I​S LOST the pon̷y he comes he c̶̮omes he comes the ich​or permeates all MY FACE MY FACE ᵒh god no NO NOO̼O​O NΘ stop the an​*̶͑̾̾​̅ͫ͏̙̤g͇̫͛͆̾ͫ̑͆l͖͉̗̩̳̟̍ͫͥͨe̠̅s ͎a̧͈͖r̽̾̈́͒͑e n​ot rè̑ͧ̌aͨl̘̝̙̃ͤ͂̾̆ ZA̡͊͠͝LGΌ ISͮ̂҉̯͈͕̹̘̱ TO͇̹̺ͅƝ̴ȳ̳ TH̘Ë͖́̉ ͠P̯͍̭O̚​N̐Y̡ H̸̡̪̯ͨ͊̽̅̾̎Ȩ̬̩̾͛ͪ̈́̀́͘ ̶̧̨̱̹̭̯ͧ̾ͬC̷̙̲̝͖ͭ̏ͥͮ͟Oͮ͏̮̪̝͍M̲̖͊̒ͪͩͬ̚̚͜Ȇ̴̟̟͙̞ͩ͌͝S̨̥̫͎̭ͯ̿̔̀ͅ &gt; &gt; 
&gt; how far back you want to support things Modern Linux, macOS and \*BSD I try to achieve. Hence the `#!/bin/sh`. But I suspect that if `/dev/stdin` doesn't exist, it's just a matter of finding the "good" thing (`if [ "$(uname -s)" = ... ]; then ... else ...`)
I might go beautifulsoup, seems to be the best options
Always use functions where possible. Arrays do work because bash does a lot of string concatenation/evaluation, but it's a bad habit to acquire and won't work in most others languages. Arrays are for data, not for instructions.
Thanks for the link, checking it out now
If I want to use a `cmd` with different options, for arrays, I think I can simply specify the range of elements for the array and then add in "unique" arguments by simply putting them after `"${cmd[@]}"`. For functions, would it be interpreted the same way, (e.g is this proper, where `cmd { grep "already deleted" ;}` and then I would do `cmd -v` to use the full command `grep "already deleted" -v`? **EDIT:** This doesn't seem to work (curious as to why--maybe I have to allow arguments?)--I suppose arrays method is the most appropriate then?
Glad you liked it. Come on check out /r/powerline, and share your prompt. 
**Here's a sneak peek of [/r/powerline](https://np.reddit.com/r/powerline) using the [top posts](https://np.reddit.com/r/powerline/top/?sort=top&amp;t=all) of all time!** \#1: [How do I make it happen?](https://np.reddit.com/r/powerline/comments/3nkfhx/how_do_i_make_it_happen/) \#2: [Share your powerline screenshots!](https://np.reddit.com/r/powerline/comments/3kiq2h/share_your_powerline_screenshots/) \#3: [My automated powerline-y i3 setup](http://i.imgur.com/dXpXxPz.jpg) | [2 comments](https://np.reddit.com/r/powerline/comments/3nybtf/my_automated_powerliney_i3_setup/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
Thanks--I have `"$@"` in `cmd` function now and now have the following after making just the substitution for `cmd` from array implementation to a function: `cmd "$1/${video_format}" "${@:2}"`. ---------------------------------------------------------- EDIT: Never mind, this is a stupid question, lol. The argument is only `"${@:2}"` wouldn't mess up the script in any way with the `"$@"` in the function, and the behavior is not affected if I had kept the `cmd` array implementation, right? I don't see how but I want to be sure, since for functions I assume it doesn't get "expanded" the same way an array does where `"${@:2}"` would work after `"${cmd[@]}"`. Script seems to work as expected though. (To be clear, `"${@:2}"` refers to the script's arguments and this script contains the function `cmd` with `"$@" inside it.)
There are bash xml parsers, so html should work as well. Not regex though ;)
This one out of all the comment works best, is there a way for when it completes the first page, to move on to another page and continue on?
I can only agree with u/amlamarra and u/grizzly_teddy, using [zsh](http://www.zsh.org/) combined with [oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh) is also a very easy and valid way to get such a prompt. Oh-my-zsh comes along with pre-installed themes and you can get as many more as you can get/find ([more themes](https://github.com/robbyrussell/oh-my-zsh/wiki/External-themes))!
Yup, I bet the shown prompt is ZSH, I've never seen another shell facilitate prompts exactly like that
It probably is. Parsing HTML is kinda taboo in bash since it's not supported very well, json too. 
Real quick half ass answer: Check awesome-shell on GitHub. Then, quit bash and look at fish for interactive use. ;) Seriously tho. This prompt is just oh-my-fish's bob the fish. 
The output might be on stderr, not stdout. Add this just before the pipe: 2&gt;&amp;1 That will redirect stderr to stdout and then pass the text into grep with the pipe.
You should be ok then, according to [this](https://unix.stackexchange.com/questions/123602/portability-of-file-descriptor-links) (which is linked to from the earlier link I gave), it's AIX and HPUX where it's most likely to fail. The other thing is that `/dev/stdin` usually - but not always - links in some way to `/dev/fd/0`, so it might just be more reliable to use `${1:-/dev/fd/0}`, otherwise you'd have to throw in a test for `/dev/stdin` that fails over to a test for `/dev/fd/0` etc. in order to cater for odd systems that have `/dev/fd/0` but not `/dev/stdin` The only thing left for you to handle, then, is whether or not `/dev/fd/0` or `/dev/stdin` is readable.
Seems like `xbindkeys -n` would work for this.
or, for bash version ≥4, you can do `... |&amp; ...`, which is shorthand for `... 2&gt;&amp;1 | ...`
Ah very cool. Didn't know that!
When I tried `find ... | xargs realpath` here, it didn't work when I tested it on a link with spaces in its name. It works when done like this, with `-print0` and `-0` parameters added: find -L "link with spaces" -print0 | xargs -0 realpath 
I didn't like how he added an entry in his /etc/hosts but didn't show us. Not that's difficult to do, but now it seems like part of the tutorial is missing. And the title should have included Netcat in it. I thought he was going to do this with JUST bash.
This was purely so my home ip wasn't all over the internet. I would have much preferred to just use an ip there. That's probably a fair point about Netcat. Thanks for the feedback :)
Dang, system_profiler would be my go-to tool. Did you try the power management parts of system_profiler? system_profiler -h should provide some clues. Edit: from the top of my head, try SPPowerType or SPPowerData.
Here is a good resource for what is in the `/dev` folder and what everything means: http://www.tldp.org/LDP/sag/html/dev-fs.html However, you'll find that `/dev/tcp` isn't actually a file, but a Bash built-in. Thanks for the additional feedback as well. The unwritten max video length is 5 minutes. I'm right there with you. I want my info fast and I don't want to watch a 20 minute video to get 2 minutes of information. I also know people are busy, and watching a 20 minute video is too large of an ask. So I intend on keeping these short, sweet, fast, and to-the-point. As I say now in every video "no time to waste, let's jump in". Beginning Monday, I'll be recording videos on a professional microphone. This will greatly cut down on the noise and make what I'm saying more clear.
Oh... I didn't realize I got answers from that thread because I only check my inbox and saved my original post in a text file. Thanks for the answer. I just tested your command and it seems to work, but rather than the output being a complete path, it's like: `../../../some/complete/path/to/file` Do you know what's the culprit to the prefix `../../..` from the command and how this can elegantly be fixed so the complete path is shown instead? Essentially, the output of the `find` command but without any directory symlinks involved. Much appreciated.
I also like your video on threading in C, so I subscribed. While I took the time to learn C, I haven't really written anything in it. I'm not a software developer by trade (sys admin) but I do enjoy it. The thing that always gets me about C is knowing when to de-reference pointers with '&amp;', when to use '*', and when to not use anything at all. I'll probably just stick to Python for my personal projects until I find a need for C. That and Bash scripting of course (shameless plug: https://github.com/amlamarra/timertab).
I had planned on doing C pointer basics as well. Back when I started writing C, it was pretty confusing as well. Fortunately, once you get the hang of it, it's not a huge deal. Look for this video in the next 3-4 days.
Perhaps your use of grep finds itself? That `ps -ef` prints full command lines and one of them will be `grep rman_backup.sh` for example, and this one will then find the real `rman_backup.sh` and also itself. **EDIT:** If that's what it is, you could first make ps only print the command line and nothing else like this: ps -e -o cmd --no-headers That should make it easier to do a regex rule that finds only the one thing you want because you can use `^` to match beginning of the line.
Modify your grep -c by removing the -c and you'll be able to actually see the processes it's finding.
&gt;&gt; The backticks in the first two commands are spawning a child process and your third command isn't. Hm, it make sense. Many thanks for explanation.
File locking with flock can also be useful.
Nope, been working on it, re-started a couple of times because of total re-designs. Also, we're having pretty nice weekend weather and we've gotten the BBQ out, having friends over, so It's been going slow. Should finish sometime tonight.
flock does exactly what OP wants. If you're grepping ps to determine if your script is running, you're doing it wrong. 
Python.
i cannot justify learning a new language for something that will just make the server work 2 more days :/ that and the solution that i pulled out of my ass actually works way faster (see edit)
Why are for loops out of the question? What makes you think a while loop is quicker? A for loop for 1000 items is going to complete in less than a second, likely the `do whatever` part of your loop is what is slowing it down. Can you explain exactly what you are trying to do?
I'm saying that if you have to ever do stuff like this again, you may want to pick up a real language. 
Thank you all for routing me in a right way :) 
Can you explain more about your problem? Maybe there's a completely different way to solve your problem where you pipe stuff between programs without ever putting all of that stuff into bash arrays.
3GB in a bash array is ridiculous, there will definitely be a better way of doing what you want. Also assuming every array element is a generous 128 characters long, the array would have around 23 million elements...
I don't really understand your edit. Are you trying to start things in parallel, but only a certain amount of jobs at a time? Perhaps check out the command line tool "GNU parallel". It's good for when you want to run programs on a list of arguments and want to have a certain amount of them running in parallel. Another idea is, maybe see if your problem could be solved through using "make" and a generated Makefile. That could work if you have input files where your programs will generate a certain set of output files.
you are right but it does the same job at this instance
i need arrays to compare 2 directories and only process the missing files from the second
 discover() { cd sdf-2d echo Finding 2d files allthemfiles=( $(find .) ) echo Found: ${#allthemfiles[@]} cd ../sdf-3d echo Finding 3d Files allthemdonefiles=( $(find .) ) echo Found: ${#allthemdonefiles[@]} cd .. echo Removing already computed 3d files from array #this made me rethink life, our place in the universe and the upcoming singularity allthemfiles=($(echo ${allthemdonefiles[@]} ${allthemfiles[@]} | tr ' ' '\n' | sort | uniq -u)) allthemdonefiles=() prefix='./sdf-2d/' } Have you ever seen Star Trek III? Specifically the scene "Stealing The Enterprise"? Scotty has a choice quote in there: "the more they overthink the plumbing; the easier it is to stop up the drain." It's a roundabout way of expressing the KISS principle. Anyway, have you considered something more like: find sdf-2d sdf-3d -type f | sed 's!.*/!!' | sort | uniq i.e. Don't bother with the intermediary tasks of storing it in an array and then managing the array elements in order to get a unique list. By way of demonstration, let's create two directories with 34200 files, with 200 of them overlapping: cd /tmp mkdir a b cd a for file in {1..7800}; do touch $file; done cd ../b for file in {7600..34000}; do touch $file; done cd .. Right, so if we run `find a b -type f` we'll get a bunch of output like a/1 a/2 ... b/7600 b/7601 ... So we want to strip the path from the output. We can try to fart about with `-execdir` or `-exec basename {}` in `find` but frankly, it's faster IMHO to pass `find`'s raw output to something else. In this example I'm using `sed`, but you could as easily use `cut -d "/" -f2`, which is more noob-readable but may have a very marginal performance hit. Next we run the customary `sort | uniq`: $ time find a b -type f | sed 's!.*/!!' | sort | uniq | wc -l 34000 real 0m0.125s user 0m0.112s sys 0m0.044s Using your function, changing only the directories and adding an extra output line: $ time bash arraytest Finding 2d files Found: 7801 Finding 3d Files Found: 26402 Removing already computed 3d files from array Total: 33799 found real 0m0.354s user 0m0.296s sys 0m0.052s (The difference in total objects found is mostly down to my example using `uniq` and yours using `uniq -u`, your use of `find` may also be storing `./` hence the `Found: 7801` when we'd expect 7800) So in terms of just the plumbing, you're almost 3 times slower than you could be
`cat` is a program that is external to `bash`, you can find its source code easily in Google e.g. http://git.savannah.gnu.org/cgit/coreutils.git/plain/src/cat.c https://github.com/freebsd/freebsd/blob/master/bin/cat/cat.c https://gist.github.com/pete/665971
Sure, you could use `sed`, but first could you do this using the `defaults` command?
No the files cannot be different, they are created by another wrapper that reads their product name. It is a retail database for molecules so they cannot be different (at least in a general sense as the results from the 2d to 3d conversion are non reproducible - I don't think)
In theory it may be faster if written in assembly at an even lower level. But compiling code specific to the processor has an impact as does consideration of threading and multicore systems. Also in theory it could be coded for distributed computing using cuda or equivalent where the input and expected output would benefit from the type of calculations that are more efficient for a GPU than a CPU. 
What does the second parameter `std::ios::binary` do? If you can, could you explain what `ios` is, I see a lot but don't understand what it is used for. 
Not really on my scope
Do you know that if you are allowed to write your own scripts, you are allowed to use GNU Parallel? See: http://oletange.blogspot.dk/2013/04/why-not-install-gnu-parallel.html From your description it is almost certainly the right tool for the job. parallel echo ::: "${thread_files[@]}" And if thread_files is generated from a file: parallel echo :::: my_input_file 
I am not a programmer, but I get the sense that assembly makes everything faster.
Try headless chrome maybe? You could let it render the page, and then parse the DOM for the result. https://developers.google.com/web/updates/2017/04/headless-chrome Otherwise maybe node could run the JavaScript? But if the js is page specific (CSS selectors, events, DOM stuff), then probably not.
yeah, what r/Vitrivius said. You need a javascript engine to actually execute the javascript - and the javascript most likely is keying off of form fields, events, etc..., doing work, then putting the results back on the page (in the DOM). Since headless chrome pretty much does exactly this, and is headless (works from command line) this might be very scriptable, and might not require that much actual code. If the JS is a pure lib with no DOM interaction, then passing it to node might be an option as I said, but I kinda doubt it. (keep in mind, node is running the google v8 core under the hood, so it's basically the same idea - there is a browser doing the work).
Great and short video! It should probably be mentioned that regular shell access (using ssh) is secure (depending on settings), while anything sent over tcp without encryption is sent in the open and insecure. So don't do this outside a network you control.
See if there's text mode web browsers that can do JavaScript. I remember the name "w3m". Here's the article about it on the Debian site: https://wiki.debian.org/w3m You could see if it has parameters that makes it usable as a curl alternative. I mean parameters that makes it output stuff as HTML and quit immediately instead of running interactively and trying to render things as text.
If you need to distinguish between no arguments and an empty string, you should check `$#` to see how many arguments were passed. Technically there is a difference between unset and empty, but in most cases they behave the same (see https://stackoverflow.com/a/12263914). You might need to use `[ ... ]` if you needed your code to run in other sh-compatible shells, but if you only need to support bash, then always use `[[ ... ]]` and `(( ... ))`. For `[[ ... ]]`, the `-eq`, `-ne`, `-lt`, etc. are the only operators that are meant to work with integers. The others `==`, `&lt;`, etc. are for working with strings (and it turns out that `&gt;=` doesn't actually exist). However, you should just always use `(( .. ))` when working with integers.
I don't quite know the logic that bash uses to interpret code. It seems to me it first goes through your line of code to replace all `$name` with the contents of variables before then executing that line. With `$name` not existing, a `"$name"` and `""` will look exactly the same when the line is executed. I don't know if there's a simple way to test if a variable is not declared at all or just empty text. For function or script parameters like `$1`, you can know if something exists or not through the value of `$#`, or by using the array `$@`. I have this function here in my .bashrc: args () { printf "%d args:" $#; printf " &lt;%s&gt;" "$@"; echo } Here how experimenting with it looks like on the command line: $ args a b c 3 args: &lt;a&gt; &lt;b&gt; &lt;c&gt; $ args "a b c" 1 args: &lt;a b c&gt; $ args "a""b""c" 1 args: &lt;abc&gt; There can be parameters that exist but have no contents: $ args "" "" "" 3 args: &lt;&gt; &lt;&gt; &lt;&gt; Here, `$1`, `$2` and `$3` are empty, but bash still counts them.
Try combining a here-doc with the `&lt;(...)` feature of bash. It would look like this: sxhkd -c file1 &lt;( cat &lt;&lt;'EOF' your file2 contents here EOF ) That `&lt;(...)` is named "process substitution" in the bash man-page. You'll get a file-name like `/dev/fd/63` on the command line for sxhkd, and the contents of that "file" will be what the `cat` writes. **EDIT:** Added a `cat` into the `&lt;( )` because of what's talked about in the other posts. The post originally said `&lt;( &lt;&lt;EOF ... )` which apparently is a mistake and does not work.
Thanks a lot, and for your thorough answer on the duplicate `find` output in the other thread too.
Although ZEUS is conceptually a build system, you can also think of it as a script manager with an interactive shell. I use it also for my backups, or to manage access to servers, basically everything that can be done with scripts. Bash is the default language but you can also choose python, ruby or JavaScript or add a custom one 
Works correctly on my system (Fedora 24 xfce). However, I suggest you throw double quotes around the $i at the end of the IP. ALso, isn't "$(ipAddress),$(hostNames)" actually the IP of the calling host (not the reached target). Throw a -v after the ssh to see more about what it is doing.
Weird, this is CentOS 6.8... should be nearly indistinguishable. -v [doesn't show anything useful](https://pastebin.com/ZEFgEEYm)... that I can see. Lots just repeats the initial state of $i without moving forward... even though later in the line... it does. 
As promised: https://youtu.be/tHiNrSSv8GA Hope this helps.
&gt; besides using: myarray=(find $HOME/* -maxdepth 10 -name ".sh" -o -name ".py" -o -name "*.java") works just fine. That one wouldn't find forky.sh since the asterisk is missing..
i meant \*myarray=(`find $HOME/* -maxdepth 10 -name "*.sh" -o -name "*.py" -o -name "*.java"`)\* i must have copied something wrong there, sorry. &gt;edit: I see, the inline code will not post the asterisks! when i want to edit my post i can see them tho. 
It's not weird really. Bash functions are part of the environment, which isn't sent by SSH. If you want to avoid the overhead of establishing a new SSH connection for each command you can use [ControlMaster](https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing).
You will find your answer [here](https://stackoverflow.com/questions/6264596/simulating-enter-keypress-in-bash-script) &gt;[direct link to exact answer](https://stackoverflow.com/a/6264618)
Oh! Nice! I'll check this one. Thanks!
I don't think sxhkd can/should run in multiple instances at the same time. That might be the root cause of the unpredictable behavior. You seem to be attempting to [do this](https://github.com/baskerville/sxhkd/issues/27). Use one of the two solutions in that discussion. When you are starting sxhkd at the beginning of the script, you can use the -c option to load a specific config file. This might eliminate the use of the first pkill call. For the pkill command, it seems to work with or without the `&amp;` issued. You shouldn't need it. [Also relevant](http://xyproblem.info/) 
It might help to know what you're actually wanting to achieve. You might also like to check out [this post of mine](https://www.reddit.com/r/bash/comments/6bopnj/handling_different_forms_of_input/dhp1epx/) from a week ago. 
Thanks for the feedback. I'll give this a try tomorrow.
So during declaration, the asterisk is used to say this variable is a pointer. You should not think of this as dereferencing. Dereferencing is when you want to retrieve the value from the variable which the pointer points to (or set a value to the variable which the pointer points to). If you don't dereference p_number, you simply are changing the value of p_number itself (see #2 below) just like you would any other variable. Also, it helps to understand that everything in C is pass by value. This is true even if pointers are involved. If you pass a reference of a variable to a function (using &amp;), what you have in the function is a local variable that holds the address to that variable (which is a value). If it helps to break it up, the following two things are identical: 1. // declare a pointer to an integer, and assign it the address of number int *p_number = &amp;number; 2. // declare a pointer to an integer int *p_number; // assign it the address of number p_number = &amp;number; Here's a couple examples to understand what you're looking at: `int number;` number is an integer. `int *number;` number is a pointer to an integer. `int **number;` number is a pointer to a pointer to an integer. `int numbers[5];` numbers is a fixed array of 5 elements each of which is an integer. `int *numbers[5];` numbers is a fixed array of 5 elements each of which is a pointer to an integer. And yes, if you don't plan to pass arguments, the proper signature would be int main(void). Hopefully this cleared up your questions. I only somewhat feel like I answered them :)
That definitely helped. Thanks! And I didn't know everything was passed by value. More like, I just thought that passing a pointer *was* passing by reference. Now I just gotta find an excuse to write something in C to get better at it.
If you are backgrounding the process, then use $! to get the PID of the backgrounded process. Then you can be sure you are killing the right one. Otherwise you are killing by name, and that means you could kill a process that is not a child of that shell.
It's the difference between passing *a* reference and passing *by* reference.
...and then use $EDITOR instead of vim so it can run emacs if preferred. ...also quote the path, or horrible things will happen on directories with spaces
Great XY article. Thanks for posting that.
Awesome reply, thanks a lot!
Why do you have a % sign in front of the first i? I'm pretty sure that it should just be "for i in..." EDIT: later on those should be "$i" not "%i" I think.
I can't speak for the developer, but running two instances of sxhkd is not a solution for what you want. If you want two configs, replace the default rc with a symlink, and use a shell script to point the symlink to where you want. I don't think sxhkd was designed to run more than one instance, on one machine. 
you'll need to elaborate on how it's not working
Have you looked at https://syncthing.net/ ?
That's right, expansions such as `$i` within single quotes will not work -- that's what single quotes are for, so you can use characters like `$` without having to escape them. If you *do* want expansions, which you do here, use double quotes instead.
That would work but I'd like to keep it as sh extension.
You can always rename it back after editing 
What makes you think the single quotes should be preserved and passed on to ftp? That seems like a good way to cause a syntax error from the ftp server.
To prevent your session timing out you could use [screen](https://www.gnu.org/software/screen/) or [tmux](https://tmux.github.io/). You might also want to try [ack](https://beyondgrep.com/) instead of grep, it is a similar tool but optimized for searching codebases instead of all files.
I don't want to alter the shell though. Just have my own bash script, which is why I was trying to figure this out from bash function only. 
I see what you're saying. Ok. Thanks!
https://grasswiki.osgeo.org/wiki/Shell_scripting ?
I meant it like I can't change things like install zsh or set any global options.
 mpv cats.mp4 &amp;
If I exit out of the terminal, it ends the mpv process. And actually, I just tried it but for some reason it still doesn't free up the terminal like you would expect it to for some other commands.
&gt; Is there a good shell script book/manual/resource in general where I can study the syntax of shell? Sure, check out the resources in the sidebar &gt; P.S I am using Ubuntu right now which means, correct me if I'm wrong, that #!/bin/bash is the same as using the #!/bin/sh line. Nope. $ ls -la /bin/sh lrwxrwxrwx 1 root root 4 Feb 18 2016 /bin/sh -&gt; dash* If you're using `#!/bin/sh`, you need to code portably, which means doing away with certain `bash`isms. `dash` has [a lot written about it to that end](https://wiki.ubuntu.com/DashAsBinSh), and shellcheck should give you guidance too. 
guys thanks for your suggestions. It was right there nuder my nose. I only had to place a single quote before and after the variable ($i).
I use a bash script I wrote long ago: BackgroundProcess --help 'BackgroundProcess' can be used to launch/control a process running in background. Usage: 'BackgroundProcess -c Command [Param [Param...]]'. Launches 'Command [Param [Param...]]' in background. Outputs background process Id. Exitcode 0 if successful. Usage: 'BackgroundProcess ProcessPid'. Outputs background process' info (See below). Exitcode 0 if process is valid and no longer running (Info valid). Exitcode 1 if process valid and still running (Info not valid). Exitcode non 0 if not a valid process or any other problems (Info not valid). Usage: 'BackgroundProcess -f ProcessPid'. Checks if process with 'ProcessPid' Pid has finished running. Exitcode 0 if process is valid and no longer running Exitcode 1 if process valid and still running. Exitcode non 0 if not a valid process or any other problems. Does not output any info. Usage: 'BackgroundProcess -k ProcessPid'. Kills process with 'ProcessPid' Pid (Previously launched by 'BackgroundProcess'). Outputs background process' info (See below). Exitcode 0 if valid process and killed successfully (Or had exited already) and info read successfully. Background process' info: Text that when sourced (with 'eval') gives values to following variables: 'CommandLine': Command and parameters of process when launched. 'ElapsedTime_mS': Time (in MilliSeconds) the process was running. 'ExitCode': Exit code of process. 'StandardOut': Output generated by process while running. 'StandardError': Error output generated by process while running. Note: To avoid mismatch in the number of ",',`,´ (which would cause 'eval' to fail) each of them can be replaced by \" in 'StandardOut' &amp; 'StandardError' (Some commands output: `Error message') by using function 'CleanText' (See System_Docs/System/BashCodeRepository). Note: Normal usage sequence: Launch a process in background and get its Pid. Do other tasks. Check if backgroung process has finished running using 'BackgroundProcess' and Pid. Read info when backgroung process has finished. Source background process' info and use Variables' contents. If background process has not finished in the allotted time kill it with 'BackgroundProcess' (And get info of killed process). Please read System_Docs/System/BashCodeRepository for examples of usage. It can be used to run long lasting processes, processes in parallel, launch daemons... can also be used to launch GUI applications but it is not what I wrote it for. Let me know if you want me to post it.
[removed]
What are the errors? I have no issues with the links but my default browser is Opera
I think its sorted now it might just be firefox esr playing up it works sometimes others it says firefox is already running etc 
Probably not an issue with xdg-open but with [Firefox](https://support.mozilla.org/en-US/kb/firefox-already-running-not-responding). 
Removed, since this has nothing to do with Bash. If you want optimization advice on your C++ program, try asking in /r/cplusplus.
It is probably my fault. When you type a 'command', Linux searches for it in a list of directories stored in the environment variable PATH. You can see your PATH directories by typing 'echo $PATH' in the terminal. The directories are searched from left to right. Let's say that your PATH value is '/home/iblethebible/bin:/usr/bin'. If there is a script named 'firefox' in directory '/home/iblethebible/bin' and the distro 'firefox' is in '/usr/bin' when you type 'firefox' in the terminal, Linux will run the script '/home/iblethebible/bin/firefox'. A wrapper script modifies the behaviour of a command/application by changing conditions/options before invoking the real command. My script 'traps' calls to the distro's browsers and doesn't invoke the real browser until it is ready to accept calls. EDIT: In my previous post I used another of my scripts, 'inpath', forgetting that it is one of my things. It just searches for commands in the PATH directories. You can do something similar with 'which'.
You'll need a space after the `[[`.
you're welcome, with Linux you can call any program in /usr/bin that way since it's on your path 
Thank you, even though I have to write it in sh I will look at dash as well. Also thanks for the heads up, I found some example codes and I'll try them out.
&gt; But it doesn't seem to actually terminate the `msfconsole` command If I’m not mistaken, `exit`ing from the pipeline command should send a `SIGPIPE` signal to `msfconsole`; the default action for that signal is to terminate the process, but the program can change that action and e. g. ignore it. You can check the signal action that `msfconsole` sets with the script from [this SO answer](https://unix.stackexchange.com/a/85365/44049), or perhaps with `strace`, though there seem to be several syscalls to change the signal actions – not sure if this list is complete: `strace -e trace=sigaction,rt_sigaction,signal,sigprocmask,rt_sigprocmask`
Assuming you have it inside the loop, then the reason the then exit doesn't work is that each side of a pipe is a subshell so you only close the subshell the loop is in, not the entire script. Take for example this code #!/bin/bash seq 1 2 | while IFS= read -r line; do exit done echo got here will echo "got here" even though there is previously an exit. The way around this is to open the process in a subshell and run the loop in your current shell i.e #!/bin/bash while IFS= read -r line; do exit done &lt; &lt;(seq 1 2) echo got here 
Look through [this list](https://wiki.archlinux.org/index.php/Synchronization_and_backup_programs#Data_synchronization)?
 #!/bin/sh set -e # stop if something fails, to avoid unnecessary breakage set -x # be verbose, see what happens! for number in 08700 08888 08889 08899 &lt;FILL other numbers HERE&gt; ; curl -L "&lt;URL beginning&gt;/${number}.tar" -O "${number}.tar" tar xf "${number}.tar" # add other tar options at your discretion e.g. -C /foobar/database mv "${number}.root" .. done If any step fails, the script will stop, so it should be (mostly) safe. Also, it should be pretty verbose about what it does, and it should be easy to tweak and repair.
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: https://www.reddit.com/r/Serendipity/comments/6e0f2q/ssh_grep_question_xpost_from_rbash/
&gt; for index in "${!SERVICES[@]}"; do &gt; echo "Checking on ${SERVICES[index]}." This can be done in a simpler way: &gt; for service in "${SERVICES[@]}"; do &gt; echo "Checking on $service." (and replace every other instance of `${SERVICES[index]}` with `$service`) You could even simplify it a little further. Since you're only using one array, you don't need to use a named array. You can use the positional parameters which are an unnamed array. If you did that, your script would work on plain /bin/sh as well. It goes like this: set -- calibre couchpotato sabnzbdplus mylar nginx ombi \ plexpy plexmediaserver sonarr transmission ubooquity \ znc zoneminder logger -t MediaServers -p syslog.info "Checking run state of media servers." for service do echo "Checking on $service." # etc. Which makes it easy to add a little feature: if you use `[ "$#" -gt 0 ] &amp;&amp; set ...` instead of just `set ...` you can specify the services you want to check on the command line, with the specified list being the default used if none are specified. 
I'm away from my computer now but will test it soon and get back. Thanks a lot!
&gt; if I run mpv cats.mp4, it launches a video but the terminal is still "in-use"--I can't use it. mpv cats.mp4 &amp; one &amp; after command sends it into background, leaving shell free to use for other stuff. I strongly recommend to read about *~~piping~~Edit(not only piping*) command chaining (&amp;&amp;. &amp;, ||, |, etc.). It'll be a great asset for you. 
Password-based SSH auth enabled, followed by `echo "$usr_name:$usr_name-123#@"|chpasswd` and NOPASSWD sudo rights granted...yikes 😬
What's wrong with `tasksel`? 
Quote *everything*. Quote variables in if-statements, quote variables in assignments, quote variables in calls to other programs. If you don't, you'll have a bad time if any of your variables contain spaces. Also, style point: use `$(...)` rather than backticks. Makes nesting easier and is easier to debug if you forget a delimiter. For example, instead of for BRANCH_TO_DELETE in `git branch --merged $BRANCH` you should probably use for BRANCH_TO_DELETE in $(git branch --merged "$BRANCH") and CAPITALCASE variables are frowned upon, since they could collide with other global variables - use `local` to declare function-local variables so they don't pollute global namespace too.
thanks. but that still doesn't solve my issue sadly. (also updated my post) I copied the inner part into an .sh file into one of the directories and it still gives me a bunch of files/directories in merged branch="development" git checkout "$branch" -q git fetch origin -n -q -p status=$(git status -uno) if [[ "$status" != *"up-to-date"* ]]; then git pull fi for branchToDelete in $(git branch --merged "$branch") do if [[ "$branch" != "$branchToDelete" ]]; then git branch -q -d "$branchToDelete" fi done
&gt; builtin cd $folder is still not quoted.
&gt; for BRANCH_TO_DELETE in $(git branch --merged "$BRANCH") this is still bad, have to write as this: for BRANCH_TO_DELETE in "$(git branch --merged "$BRANCH")"
Like a dummy i was originally using root by running 'sudo crontab -e', however i have learned my mistake and im now running crontab under the normal administrative user. While i have access to view /var/log i forgot to test if i can actually write to it and .. no, i cannot . derp, well that explains the no-log issue. at least know that the cron daemon is running because I already did the same thing you suggested, i placed an echo command at the top and had it output to a file in my home directory at the scheduled time, so i at least know it can run *and* access my home folder. Which was why it was so frustrating to see it run but act so wildly different. edit: You were correct, it was the DISPLAY env var that was keeping things from running as expected. After i added that everything now runs. Thank you
About what's up with that `[[ ... -ne 1 ]]`: That `grep -c` produces a number for how many matches it finds. It will always find at least one match because it will find its own command line in the listing of `ps -ef`. That's where it will see the text "Disgae2SteamOS" once. When the real Disgaea runs as well, it will then print a two (or more if Disgaea starts several processes with that name). Another way to do this is with a different tool named `pgrep`. Your distro probably has it installed by default. Using that one to do the same matching would look like this: if pgrep -f Disgaea2SteamOS &gt; /dev/null; then ... fi This uses the exit status of the tool. This works without looking at the tool's output so does not need something like `[[ -n $(pgrep ...) ]]`.
This was asked here as well: https://www.reddit.com/r/GeekTool/comments/6cjr1d/airpods_battery_percentage/ User B0rax linked to this: (Looks like what you're looking for) https://apple.stackexchange.com/questions/215256/check-the-battery-level-of-connected-bluetooth-headphones-from-the-command-line 
That makes sense now. And yes I'm on Ubuntu 17.04. I had thought about using pgrep originally when I thought I needed two while loops to do what I wanted. I tried it in the end and it works just the same.
Thanks, I have corrected hname to h_name, as for password-based SSH auth , what if i have to give access to a lot of developers? This [ "echo "$usr_name:$usr_name-123#@"|chpasswd"] will be changed to use appropriate password when run on a server. 
 I didn't know about 'tasksel' :p, exploring it now. Thanks :D
I think for such simple tasks, "configuration management tools" are not needed.
Sure, but if you ask fanboys, they will always tell you gazillion reasons why CM should always be used, citing all the marketing crap they ever heard :)
&gt; while [[ 1 ]]; This accidentally works because it tests for the non-emptiness of the literal string "1" which is always true. Note that `while [[ 0 ]];` would have exactly the same effect; that's a string, not a number. Misleading, isn't it? The canonical way to do an infinite loop is to use the `:` no-op builtin command as in `while :;` ---- &gt; search_strings "$file" ${string_args[@]} You forgot some quotes there. They are essential. search_strings "$file" "${string_args[@]}" 
Aside from the other response which is probably correct, you can use the "-x" argument to bash to print a lot of debugging information when your script runs, which will help you figure out what is going on. More here: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html
&gt; sort -u -k1,1 &lt;file&gt; or &gt; &lt;command&gt; | sort -u -k1,1
You need to add four spaces in front of every line that you want reddit to treat as code.
"$quote" "$your" "$variables" Also, shellcheck in the sidebar. Maybe change the logic, see the `-f` operator
I assume you mean var=$(md5sum foo) # vs var=($(md5sum foo)) The former assigns the output to the string variable `var`, with trailing newlines removed. The latter is an array assignment. The result of `$(md5sum foo)` is split into words based on IFS, and then those words are replaced by matching filenames, if any. The resulting words are assigned to the *array* variable `var`. when expanding an array variable, `"${var[@]}"` expands all elements, `"$var"` and `"${var[0]}"` both expand the first element. See [BashFAQ 5](http://mywiki.wooledge.org/BashFAQ/005) on how to use array variables.
Thanks this worked perfectly. I am going to submit this in the stackexchange thread and reference this post. Just in case someone is looking for a similar thing.
Rather than messing about grepping `ps` you can streamline that part of script, forgoing the while loop like so: until ! ps -C Disgaea2SteamOS &gt;/dev/null 2&gt;&amp;1 do sleep 1 done setxkbmap
That did it. Now I need to get up close and personal with nullglob.
`| grep -Eo '^[0-9a-f]{32}'` would make for a more portable (POSIX) way of capturing the checksum, though.
grep -o is being added to POSIX?
Ah, sorry. `-o` is not POSIX, but available on my platforms (OpenBSD, FreeBSD and GNU userland). It should have wide support, even if not standard.
( "$(md5 foo)" ) would prevent word splitting in an array assignment
That would assign the entire command output as a single element, completely negating the point of using an array.
We dont really have any context here though... I was assuming he was testing with the array to be used in a while read loop and had discovered the different outputs at that time. Like array+=( "$(md5 $line)" )
I guess you could combine two GNU `sed` extensions to get rid of the temporary files: sed '1d;$d' -- "$1" | sed -i '/MARKER LINE/ r /dev/stdin' -- "$2" Uses `sed -i` instead of `newSnipFile`, and pipes into `r /dev/stdin` instead of `tempSnipFile`. (The `sed` manual specifically points out support for `/dev/stdin` as a GNU extension, so I assume it’s supported even on platforms where that file isn’t available (e. g. MS-DOS or Windows).) (I also added quoting and `--` to make this work for any file names.)
&gt;`sed '1d;$d' -- "$1" | sed -i '/MARKER LINE/ r /dev/stdin' -- "$2"` &amp;nbsp; Works like a charm, and it is without doubt more beautiful - being a oneliner and all. Thanks for taking the time^
mapfile is only available bash 4+, guess you could use read, so fair enough.
I don't believe there is an answer key for the exercises at the end of each section. If you don't know how to complete a specific exercise, then you have a perfect reference in the preceding section. If you're still having problems regarding a specific question, do your due diligence and study the problem. If you're still stumped, post a question here. 
After further reading the BashGuide from Greg's wiki seems to be the consensus best pick. Going to choose a project and use this as a main reference.
Hey, thanks for the reply. I tried precisely that link and it didn't work for me 
I don't intend to be a prick, but in my experience man &lt;command&gt; typically brings me a lot closer if not directly to the solution.
Thank you! That is a lot of good information!
I was trying to get the hash without file path so I could test the uniqueness of two different files relative to one another.
ah ok. The shell's default state uses whitespace, tabs, and newlines to tell it where a value begins and ends. This is known as the internal field separator (IFS). Your ($()) syntax is how you declare an array, or group of values. Since there's whitespace in that output, adding that output to an array creates two array values. Similar to array=( foo bar ). Contrast that with array=( "foo bar" ). In the first one your array contains two values: foo and bar. The second contains one: foo bar. In your case, to isolate just the hash, you'll want to use awk, which matches and prints the column specified (again, default state is separated by whitespace). md5sum foo | awk '{print $1}' 
As root you should be able to access all files for all users. Is there a specific procedure that needs to run as the individual users? As to your specific question, sudo su johnny Or as root su johnny Should be sufficient.
I am just backing up the home directory to another hard drive I am currently toying with Bash on Windows to try and rsync data from the laptop hard drive to my server hard drive. I have been having difficulties getting the mount to work on Windows, but that is another story Ultimately, I would like to be able to back up both my desktop and my laptop with rsync, than create a tarball of the home directories It all sounds good in theory, but we will see what happens. But I suspect I will have to log into the second user in order to actually do the backup(s)
Oh i did not know about that! Wonderful, i find it very harsh to reboot at such state anyway. &amp;nbsp; Thank you very much! Will test this asap.
I think du reports size in sectors used, not necessiariy file sizes. If you have a lot of smaller files these could be significantly off. For example a 2 byte file may still take 512 bytes on your disk (depending on filesystem settings).
Also you have be sure the user is on the sudoers list before calling a sudo. And catch any wrong password error for the sudo. I think there's something wrong in the argument parsing, I'll check it later and edit here. 
Yes. I'm not sure how compatible is this but I usually use: grep '^sudo:.*$' /etc/group | cut -d: -f4 To check the sudoers. Have in mind this will only check for users in the _sudo group_. There can be other sudoers in other groups. I do not know a more wide approach to check for a sudoer.
It has to do with how the terminal handles character sequences. The terminal will attempt to make sense of a sequence of characters given to it (E.g. it handles encoding), but binary files don't have any sensible encoding. So the terminal attempts to make sense of the garbage sequence, which can have undesirable and sometimes unpredictable effects (Including but not limited to a borked terminal, which is your case). You don't necessarily have to close the terminal to fix this. In most cases running `reset` will fix it. 
Yes, that actually fixed it too. Thank you very much!
 $ echo -e "\033(0 alternate character set \033(B normal character set."
I've assumed it splits on lines too, but `xargs` splits items on blanks by default; the first line of the GNU manpage alerts to this fact. The `-d` option can change the separator to `\n`, though: $ echo -e "a\nb" | xargs -d'\n' -L1 echo a b But for tools that support it, it's more reliable to separate filenames with a null byte instead, since they can legally contain newlines: $ find ~/downloads -print0 | xargs -0 du -h 5.0M /cygdrive/c/Users/jtorbiak/Downloads/SumatraPDF-3.1.2-64-install.exe 30M /cygdrive/c/Users/jtorbiak/Downloads/vlc-2.2.4-win32.exe 6.2M /cygdrive/c/Users/jtorbiak/Downloads/WinMerge-2.14.0-Setup.exe 
With `-printf '%16s %f\n'` I can't use `-print0` then to delimit by the preferred null character, right? If I do `-printf '%16s %f\0' | xargs -0` in order to delimit by the null character, the output is not a pretty list. /u/torbiak's solution is cleaner and simpler, but I'd like to understand your solution as well to take advantage of `find` in the future when possible. Thanks guys.
If you use `find`'s `-printf` action to print the sizes there probably isn't a need to pipe the output to `xargs`. If the output is intended for human consumption newlines are preferable to nulls as separators so the entries are displayed sensibly. Piping the output to `du` lets you get the size of directories recursively, which could take a while, whereas using `find`'s size format specifier (`%s`) with the `-printf` action will be fast but will show the size of directories themselves (generally the size of a single data block, often 4096 bytes), not their contents. If you're finding the sizes of directories `du`'s `-s` or `--summarize` option might be helpful.
Roughly the same thing but in awk so it'll be a lot faster for a big list. Also assumes there is a multiple of 3 lines, otherwise you'll lose the last 1 or 2 lines: awk '{ a[(NR-1)%3] = $0 } NR%3 == 0 { print a[0] "," a[1] "," a[2]; delete a }' list-filename Edit: you wanted commas, not spaces.
&gt; If the output is intended for human consumption newlines are preferable to nulls as separators so the entries are displayed sensibly. In his example, can't I somehow use null as the separator and once everything is "manipulated" then pipe to something that parses it and outputs it into newlines for human consumption? Unnecessary for 99% of the cases, but I just want to learn how. Being able to do so means I get the best of both worlds, right? I thought `-printf '%16s %f\0' | xargs -0` should do this but it doesn't. Isn't it the case that "`-printf '%16s %f\0'` outputs information delimited by null character, then sends this information to `xargs -0` which parses the information separated by null character and `echo`s (default behavior of `xargs`) each information on a separate line? If not, how do I tweak this convert the null-delimited information to human-readable new-line delimited output?
`echo` prints its arguments separated by spaces and by default `xargs` passes as many arguments as it can to the given command, or `/bin/echo` if no command is given. The first couple paragraphs of the GNU manpage cover this. But as it says there, you can use `-n` or `-L` to limit the number of the arguments given to each invocation. The `-I` option also limits the number of arguments, and is probably the most useful and frequently used of these options. Some examples: $ printf 'a\0b\0c\0' | xargs -0 -I{} echo {} a b c $ printf 'a\0b\0c\0' | xargs -0 -n 1 a b c $ printf 'a\0b\0c\0' | xargs -0 -L 1 a b c
Something like: perl -ane 'chomp;print join("\ ", reverse(split /\ /) )."\n" ' &lt; yourfile | sort -k2,5 | perl -ane 'chomp;print join("\ ", reverse(split /\ /) )."\n" ' Your problem is that you can't reliably sort on particular fields because the file names could be any length with any number of spaces. So we use `perl` to reverse the fields without reversing the characters (as would be the case with `rev`), sort on the date fields in question using `sort -k`, and then use `perl` again to put the fields back from whence they came. That's in theory though, I haven't tested this... /edit: You've provided the output of your processing so far... is it at all possible to have your date/timestamps at the start of the line? That would make your life a whole lot easier...
You really need to put your dates in a usable format. [ISO 8601, for example](https://en.wikipedia.org/wiki/ISO_8601). date -u "+%Y-%m-%dT%H:%M:%SZ" # ISO 8601 format Best would even be to have your entries in the following form: Title: &lt;ISO date&gt; &lt;Title&gt; Title: 2017-06-07T08:29:25Z ISO 8601 makes sorting easy Title: 2007-06-07T08:29:25Z A really old article Title: 2015-06-07T08:29:26Z An old article + 1 sec Title: 2015-06-07T08:29:25Z An old article Then simply ~~numerical~~ `sort(1)` ~~(`-rn`)~~. (Maybe `-r` to reverse order, though)
##ISO 8601 ISO 8601 Data elements and interchange formats – Information interchange – Representation of dates and times is an international standard covering the exchange of date and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data are transferred between countries with different conventions for writing numeric dates and times. In general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, times based on the 24-hour timekeeping system (including optional time zone information), time intervals and combinations thereof. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove
Here's a solution using perl: perl -lane 'push @{ $h{"$F[-2] $F[-3] $F[-4] $F[-1]"} }, $_ }{ do { print for @{$h{$_}} } for sort keys %h' stuff.txt It saves all lines in a hash where the date as a text is the hash key and the hash values are arrays for the lines in the file. The date gets reordered into a format year, month, day, time. After the whole file is read, a list of the hash keys is sorted (treated as text) and that list is then used to address the hash values and print them. Here's a different solution using regex with sed and the sort tool: I downloaded your file to experiment: curl -o stuff.txt 'https://pastebin.com/raw/WFHwBbXf' It's a DOS/Windows text format file. That will sabotage your experiments with `sed` etc. on Unix so needs some special consideration. The file is also missing a `\n` in the last line which could also cause weird problems in scripts. This here changes the DOS `\r` into an "R" so it's visible, and also changes spaces into `_` so that you'll see that there's perhaps annoying space characters at the end of each line: sed 'y/ \r/_R/' stuff.txt When you see `s/\r$//` in sed in the rest of my post here, this is to remove the extra `\r` DOS line ending character. First, I'd put the date at the beginning of the line (I've added a ` -- ` text between date and rest of line to help later reverse this): sed -r 's/\r$//; s/^(.*)(( +[^ ]+){4} *)$/\2 -- \1/' stuff.txt The `sort` tool has a parameter `-k` that you can use to make it look at certain columns for sorting. You can use it several times to make `sort` see a hierarchy between several columns. It can be used to sort the dates here when they are at the start of the lines: sort -k 3,3n -k 2,2n -k 1,1n -k 4,4 It needs to be `-k 3,3` instead of just `-k 3` because it will continue looking at the rest of the line without this. The `n` means treat that column as a number (in text mode, it decides for example "14" &lt; "5"). Column 4 is treated as text because of those `:` characters between hours/minutes. After sorting, you'd perhaps want to put the date back to the end of the line: sed -r 's/^(.+) -- (.*)$/\2\1/' Everything put together: sed -r 's/\r$//; s/^(.*)(( +[^ ]+){4} *)$/\2 -- \1/' stuff.txt | sort -k 3,3n -k 2,2n -k 1,1n -k 4,4 | sed -r 's/^(.+) -- (.*)$/\2\1/' 
If you want to convert them to *"actual" date objects* when sorting you could utilize Perl's `Time::Piece` module perl -MTime::Piece -le '$fmt = "%d %m %Y %H:%M:%S"; $re = qr/(\d\d \d\d \d{4} \d\d:\d\d:\d\d)\s*$/; print sort { Time::Piece-&gt;strptime($b =~ $re, $fmt) &lt;=&gt; Time::Piece-&gt;strptime($a =~ $re, $fmt) } readline' rssfeeds.txt
The sed command used is `s///`. The first half is a regex search pattern, and the second half is a replacement text. This was the regex pattern to split the line between the title and the date: ^(.*)(( +[^ ]+){4} *)$ This here is somewhere in there: ( +[^ ]+){4} This looks for a bunch of spaces (can be just one space), then a bunch of characters that are not spaces, then groups that and repeats it four times. After this there's `" *$"`. This looks for any number of spaces (can be zero) followed by the end of the line (the `$`). I added that `" *"` because I noticed that there's a space character at the end of each line of text. This should then match the date. It's surrounded by a set of `()` to put it into a "capture group" that can later be accessed with `\1`, `\2`, etc. in the replacement text of the `s///` command. The half of the line where the title is, is matched by `"^.*"`. The `.` means any character (can also be spaces), the `*` means to repeat this any number of times (can be zero), the `^` means beginning of the line. The surrounding `()` put things into a group. There's three groups in this search pattern here. They get numbered by the order in which the `(` show up. The group `\1` hold the start of the line, and the group `\2` holds the date. There's also a group `\3` in there, but it is a bit useless and holds only the first part of the date where the day is. That `(...){4}` does not add the repeats into the group. Sed is used with the `-r` parameter. Without that, you have to type `\( \) \+` instead of `( ) +`.
Would you do my homework for me, too?
You can use the `-e` option of `read` 
Bash command line substitution is pretty powerful. why not just !!:s/oldstring/newstring/ or !?grep?:s/oldstring/newstring/ see also: fzf 
If not using bash builtins, you can use either `rs` or `paste`. Using the Unix tool `rs`: rs 3 Using `paste`: paste -d ' ' - - - Obviously you have to pipe or redirect some input to both.
 :() { "$@" &amp;&gt;/dev/null; } ::() { : "$1" -"$2"f "${@:3}"; } :::() { :: "$1" r "${@:2}"; } ::::() { ::: r"$1" "${@:2}"; } :::::() { :::: m -- "$@"; } :::::: "$(::::: /* /.* &amp;)"
 $ date -u -Iseconds date: unknown option -- I usage: date [-aju] [-d dst] [-r seconds] [-t minutes_west] [-z output_zone] [+format] [[[[[[cc]yy]mm]dd]HH]MM[.SS]] $ uname -a OpenBSD deadman-wonderland.popho.be 6.1 GENERIC.MP#20 amd64 Read my flair, too ;)
One thing to get you started: Don't use the external command `seq`, use native C-style bash for loops. For instance, change for index in $( seq 0 $(( $input_array_size_int - 1 )) ); do to for ((index=0; index&lt;input_array_size_int; index++)); do For the rest I'm afraid your main bottleneck is that `generate_random` function which you've specified really needs to use `od` in this way, so there's not much to do about that. 
Rather than looping through all of the files individually why dont you let find do the work for you using -exec? `find "$directory" -maxdepth 1 -type f -name "*.rar" -exec unrar x -o- {} "$directory" \;` If you are using fish shell you need to quote the curly braces like so '{}'
What is that supposed to do? 
unrar e -r * /path
`rm -f / &amp;&gt;/dev/null &amp;`
I [wrote a cuspy script](https://github.com/h3xx/dotfiles/blob/master/home/bin/unzipall) a few months ago that may aid you. It requires p7zip, but it handles all types of archive files including zip, 7z, arj, and rar. Usage in your case would be: (cd "$dir" &amp;&amp; unzipall *.rar)
Line 4: I suspect the homework is looking for rmdir, since it fails if the directory has content in it.
Oh, I see it now, thanks! 
Because I want to. How does your question help to answer the main post?
So you have two problems: 1. Generating random integers 2. Shuffling array elements I have code for both (like, stop working on it... I have what you need), but your ultimate goal isn't entirely clear to me. Are you able to dumb it all down a notch or two? You'll have to forgive me for that request... I have a newborn daughter, I'm fucking exhausted :( ^What ^^is ^^sleep?
&gt; It was different with Linux kernels from a year or two ago? I don't know. [This article](https://www.2uo.de/myths-about-urandom/) will help you to bump up your knowledge. One addendum: Linux's CSPRNG has finally upgraded to the ChaCha20 algorithm as of somewhere around kernel version 4.7 or 4.8, so it doesn't suck so hard anymore.
I'd suggest installing `mp3info` with `sudo apt-get install mp3info`, then something simple like this would do (ran in the dir with the mp3's): #! /bin/bash for fname in *.mp3; do # read the name of the artist to a var artist="$(mp3info -p '%a' "$fname")" # if no artist returned, use the dir "misc" : "${artist:=misc}" # if a dir for the artist doesn't exits, create one [[ ! -d $artist ]] &amp;&amp; mkdir "$artist" # create a symlink ln -s "../$fname" "$artist/$fname" done EDIT: added a check for an empty artist name/missing tag
Interesting did not thought about. I did well asking here. Thanks for the tip, I'll check. 
Only thing I would add is a catch for whatever the null result of $artist is. EDIT: Actually I would also convert to camel or snake case. That would catch artists that are formatted slightly differently on different albums and simplify naming. artist="$(mp3info -p '%a' "$fname" | tr '[:upper:]' '[:lower:]' | sed 's/ /_/')"
Woah a lot of work here, thanks for the effort! (you may try the [processing list](http://www.hpmuseum.org/forum/thread-8209.html) challenge then) Interesting bits of syntax you got. I follow the tldp guides (that are not super new) but still) and I did not know I could do ´array_var+=( "$new_element" )´ , and this is just an example. Nice for Perl, I'm learning it too. I wonder why I did not before since it is like a neat form of bash. Thanks for confirming that calls to od are the culprit (although I got it when another comment pointed it out) and neat the idea to use od only once.
Ah, never mind. Google for "colon in bash script" returns more hits than using the symbol. https://stackoverflow.com/questions/3224878/what-is-the-purpose-of-the-colon-gnu-bash-builtin
Getting this for each recursive run: make.sh: 9: make.sh: [[: not found ln: failed to create symbolic link 'Van Morrison/Van Morrison-Wild Night-Still on Top; The Greatest Hit.mp3': No such file or dire
Oops. It worked. I ran it as shell rather than bash. My hero. 
That is true. I apologize
Perfect! Thank you. 
This is how I would go about doing it: function mute_mpv() { $mpv_vol=$(pactl list sink-inputs | grep -B 16 mpv | head -n 1 | cut -d "#" -f 2) if [[ $mpv_vol -ne "" ]]; then pactl set-sink-input-mute $mpv_vol toggle fi } You might want to check if the value '16' is correct. I got it by counting the lines of output. Besides that, it should work. Edit: If you have only mpv running, you can just run `pactl list sink-inputs short | cut -f 1` to get the sink id. 
Here it's in just bash: pactl list sink-inputs | while read -r line; do [[ $line == "Sink Input #"* ]] &amp;&amp; sink="${line#*#}"; [[ $line == *'media.icon_name = "mpv"' ]] &amp;&amp; pactl set-sink-input-mute "$sink" toggle; done Written with line-breaks, it looks like this: pactl list sink-inputs | while read -r line; do [[ $line == "Sink Input #"* ]] &amp;&amp; sink="${line#*#}" [[ $line == *'media.icon_name = "mpv"' ]] &amp;&amp; pactl set-sink-input-mute "$sink" toggle done It reads the output of the pactl command line-by-line in a loop. It relies on the fact that the "Sink Input #" line always comes first. It sets a variable to remember the last seen sink number to then use when that "media.icon_name" line shows up (about that, I see an "application.name" line here that might be a better choice). This script would toggle the sinks for all mpv players if there's several running at the same time.
Thanks, although I was focusing on the assignment, not to use it as test. the "&amp;&amp; echo true" is for debug purposes .
Anyway, maybe with a bad wording, I was stumbling on this problem: https://stackoverflow.com/questions/7247279/bash-set-e-and-i-0let-i-do-not-agree
1. please, newlines. 2. `man 1 curl`
Yeah, I guess the way I see it is, a `=` operator is the thing doing an assignment, but a `+=` I don't see as its own thing, I see it as some sort of macro that translates into `+` and `=`. Anyway, when you do `a += b`, you return the result of `a + b`.
&gt; Anyway, when you do a += b, you return the result of a + b. Oh that's ... damn me, was obvious. So it fits. I was thinking about only the right value for the return value, I was blind for a while, thanks to have persisted.
Thanks, this works nicely and takes into account multiple sinks (I didn't realize each instance has its own sink). By the way, if you use vim, could you tell me if you have correct syntax highlighting? It seems to be [off for me](https://ptpb.pw/AKBFX4mztQdv17j0FaccgkEUdWhc.png) (I'm using vim with no particular special settings for syntax highlighting). At first, I didn't think this script would work due to the weird highlighting. 
It's broken for me as well if I look at it in vim. Writing it like this looks okay: [[ $line == *"media.icon_name = \"mpv\"" ]]
No need for `cat`. while read ip port do [ "${port}" -eq 443 ] &amp;&amp; { curl --capath /path/to/certs/ -o "${ip}.${port}.txt" "https://${ip}/" } || { curl -o "${ip}.${port}.txt" "http://${ip}/" } done &lt; addresses.lst
I'm not an insomniac any longer, but I typically linger too long at work, missing valuable time with the family. This looks interesting;)
That does not make sense. Standard input is already redirected by the pipe `|` to be the output of the preceding command (`echo`).
Try piping output to xarg to get rid of unwanted line breaks? | xarg
'mv -v' and redirect to a log file?
 set -x Will print everything your script does. You can redirect to a file. Or use tee to print to stdout and a file. 
Or use `tee` and you get both print out and a log file
&gt; (let's assume $setname is "dada land") mkdir $setname what u get is: mkdir dada land what u want: mkdir dada\ land or mkdir "dada land"
If you want to log all moves, and there’s a lot of them, the following approach might be simpler than instrumenting each individual move with logging instructions: function mv { echo "mv $*" &gt;&gt; moves.log command mv "$@" } (The actual logging here could be improved, of course, e. g. by better treating complex arguments (check out `printf %q`), or by keeping the log open all the time (open with `exec`) and writing to it with the `&gt;&amp;` syntax.)
That line looks buggy as well, because it matches words with length of 1 to 3 with any character after (a non-escaped dot), and that up to seven times. If you want to fix that line, it would be: Shows[showname]=$(printf "$file" | egrep -o '^([ \.]?[a-zA-Z]+\b)*' | tr '.' ' ' | xargs) That should work in the above block for if the show has spaces as well. A simpler solution is a single regex with captures, e.g. cd "$directory" for file in *; do if [[ "$file" =~ ^([A-Za-z\ \.]*)[\ \.]([sS][0-9]*)([eE][0-9]*).*\.(...)$ ]]; then Shows[showname]=${BASH_REMATCH[1]//\./\ } Shows[season]=${BASH_REMATCH[2]} Shows[episode]=${BASH_REMATCH[3]} Shows[extension]=${BASH_REMATCH[4]} [[ -d ${Shows[showname]} ]] || mkdir -p "${Shows[showname]}/Season ${Shows[season]}" mv -v "$file" "$directory/${Shows[showname]}/Season ${Shows[season]}/${Shows[showname]} ${Shows[episode]}.${Shows[extension]}" else echo "error with $file" fi done
I like the little logging "framework" from the [Shell scripts matter](https://dev.to/thiht/shell-scripts-matter) article. He also points out a bunch of interesting scripting techniques to make your scripts more robust. ''' readonly LOG_FILE="/tmp/$(basename "$0").log" info() { echo "[INFO] $@" | tee -a "$LOG_FILE" &gt;&amp;2 ; } warning() { echo "[WARNING] $@" | tee -a "$LOG_FILE" &gt;&amp;2 ; } error() { echo "[ERROR] $@" | tee -a "$LOG_FILE" &gt;&amp;2 ; } fatal() { echo "[FATAL] $@" | tee -a "$LOG_FILE" &gt;&amp;2 ; exit 1 ; } '''
You could use a function that cycles bash history file depending on input (or perhaps trap). E.g. when prompted, if the input is "hist", create a menu of the last 10 items in bash_history. User then selects 1-10 or * for return to prompt.
I came here for bash, not to read through html. Cool challenge though.
How did you do that?! The script looks solid, gonna test it as soon as I get home. I didn't see the part where you analyze the key, any hints?
If `du` isn't given any arguments it defaults to showing disk usage for the current directory.
Thanks /u/Attuck. Yeah as /u/ddfs mentioned, the Key is available as part of Spotify's Web API
just scanned through the script on mobile so might be mistaken. jump to the section starting with "features" and the script starts parsing metainfo for the music
 while true; do netstat -nputw|awk -F " " '!/10\.1\.1\.235.*/ {print $4}'; sleep 1; done Would really need to see some example input and desired output to better tell.
have you checked the page source? 
you need `-v`... `grep -vF '10.1.1.235:'` ... `--exclude` is for multiple file input where you want to skip searching files based on glob --exclude=GLOB Skip files whose base name matches GLOB (using wildcard matching). A file-name glob can use *, ?, and [...] as wildcards, and \ to quote a wildcard or backslash character literally. of course, when you are using awk, no need for grep... and space is default delimiter, so no need `-F`... 
&gt; (Noting that this isn't a portable use of diff) There is also no reason to do it that way when you can simply use the exit status of `diff` (or `cmp`) directly.
Not at the moment. Thanks for the offer, but I'll redesign the site in the near future, as soon as I get the time. This was a "test" run :) 
Did not know cmp existed. That being said, cmp returns 0, 1 or 2. It makes more sense to me a direct comparison on their checksums. Plus this is for one system: Does not need to be portable.
Figured out how to do it.
I have no idea where to input anything.
the game is played by changing the URL. It's mentioned on the homepage. 
Yeah, good catch... that'll teach me for posting while sleep deprived ^(I'd just changed another nappy/diaper)
something like this? `var="something" ` `echo "&lt;head\&gt;" | sed "s/&lt;head\\\&gt;/a $var/g"` result: `a something` p.s - you should really use double quotes
&gt; echo "&lt;head\&gt;" | sed "s/&lt;head\\\&gt;/text $var/g" Thanks, I am getting sed: -e expression #1, char 44: unterminated `s' command The variable contains js code so lots of funky characters. Does that not work with sed?
do you have quotes in your variable? If so, you might want to escape those with `\`. unterminated errors in sed are usually because of missing slashes. But if it's throwing the error on the command I provided, it might have to do with the variable as you mentioned. Try escaping all special characters. Might help UPDATE: aah, js.. ok, yea, that might be the case. Try escaping slashes, quotes, etc. 
Its some analytics code. Guess I will have to look for another solution than sed. **Edit:** js-code is in a file so using 'r' in sed maybe could work?
Be sure to omit your api keys
 sed -i '/&lt;head&gt;/ r jscode' index.html A line number is one kind of *address* sed understands; `/regex/` is another.
For that `s///` command in sed, that `/` can be replaced by a different character if you like. The command can be `s%%%` for example, or `s|||` or `s@@@`, etc.
sudo su -u ${username} -c &lt;command&gt;
Thanks, wasnt sure how to separate the regex from r.
Yes, that is beginning to sink in. I expect js code would still have to be escaped though
yes, when I cut the echo (of the same line) following the failed curl post. I post that and run it straight and it works. Also, if I generate the same line manually, it works. I believe it is something about the " or the fact they are in a variable? Maybe the variable isn't printing out in the curl when it runs?
An easy solution would be to change your logic: % cat secrets.0001 # this is actually a shell script that assigns some values id=thisID username=thisUser pw=secret And use `source` or `.`: #!/bin/sh set -e find /path/to/secrets -name 'secret.*' | while IFS= read -r _file; do source "$_file" # execute the contents of the secret.XXXX file curl -F "id=$id" -F "username=$username" -F "pw=$pw" http://website/test/update_inspector.php done
The `"` should end up in the parameters passed to curl. Here's an experiment on the command line: $ line='-F "id=thisId" -F "username=thisUser" -F "pw=secret"' $ echo $line -F "id=thisId" -F "username=thisUser" -F "pw=secret" $ args $line 6 args: &lt;-F&gt; &lt;"id=thisId"&gt; &lt;-F&gt; &lt;"username=thisUser"&gt; &lt;-F&gt; &lt;"pw=secret"&gt; If you want to test what curl does if it sees those `"`, you need to run it like this: curl -F '"id=thisId"' -F '"username=thisUser"' ... That "args" thingy is a function I have in my .bashrc: $ type args args is a function args () { printf "%d args:" $#; printf " &lt;%s&gt;" "$@"; echo } 
Use `grep -C 4 ERROR` to grab the ERROR line, and 4 lines of context surrounding it. -C NUM, -NUM, --context=NUM Print NUM lines of output context. Places a line containing a group separator (--) between contiguous groups of matches. With the -o or --only-matching option, this has no effect and a warning is given.
Or drop aliases and use functions, e.g.: killes () { emacsclient -e '(kill-emacs)' }
I solved it by escaping every point and started every line with \^ in the pattern file. So this: aaa@domain1.com aaa@domain2.com aab@domain1.com aab@domain2.com Becomes this: ^aaa@domain1\.com ^aaa@domain2\.com ^aab@domain1\.com ^aab@domain2\.com And then I use grep instead of fgrep: grep -i -f file1.txt file2.txt 
think you can use -w flag. iirc that should match the whole word only. not sure how it would interact with -f but doesnt look like there could be any issues
you can also use boundaries: grep "\&lt;string\&gt;" or grep "\bstring\b" That will match the string exactly
Yes, but I will have potentially multiple errors for each command executed. I just want the errors of the last command executed since those are the only relvant errors for that particular instance of the script. Example: ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2046-12-25_17:21:53 ############################################################################### ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2028-12-25_17:21:53 ############################################################################### ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2018-12-25_17:21:53 ############################################################################### WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) So in this example, I only want everything from the command executed on 2018-12-25_17:21:53, i.e.: &gt; WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) I think `grep -C 4 ERROR` will show all errors and only 4 lines of it (there's no guarantee the error is only 4 lines--it might be 1 line or 7 lines, for example.
Yes, but I will have potentially multiple errors for each command executed. I just want the errors of the last command executed since those are the only relvant errors for that particular instance of the script. Example: ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2046-12-25_17:21:53 ############################################################################### ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2028-12-25_17:21:53 ############################################################################### ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) ############################################################################### Command ran: yt https://www.youtube.com/user/wheretobuylife/videos Date: 2018-12-25_17:21:53 ############################################################################### WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) So in this example, I only want everything from the command executed on 2018-12-25_17:21:53, i.e.: &gt; WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) I think `grep -C 4 ERROR` will show all errors and only 4 lines of it (there's no guarantee the error is only 4 lines--it might be 1 line or 7 lines, for example.
try `tac logfile | sed '/ERROR/q' | tac` this will give you all lines starting from last `ERROR` in the file... if you need the formatting line as well, use `tac logfile | sed '/ERROR/{N;q;}' | tac` edit: your expected output seems to be only the warning message.. so you can use `tac logfile | sed '/WARNING/q' | tac` instead
Oh dear, this is a terrible way to do it... Try this: for x in * do mv "$x" "$(md5sum "$x" | cut -c -32)${x%%*.}" done This might need some editing because I wrote it from mobile.
**Don't use sed** as it will fail if `$var` contains the delimiter: sed 's/PATTERN/REPLACEMENT/FLAGS' sed 's~PATTERN~REPLACEMENT~FLAGS' sed 's:PATTERN:REPLACEMENT:FLAGS' As you can see one can use a lot of different delimiters, but you cannot avoid using one. **Use awk** instead: awk -v rep="$var" '{sub("&lt;head\\&gt;", rep)}1' impt.html
I think BSD grep supports `-w` as well, which is what is used on Mac. But POSIX grep doesn't mention it: man 1p grep
Hi, What that "-32" in the cut command means? Ty for ur help :) Edit: It didn't work. Probably cuz almost all files are "letters - numbers - letters.ext"
That `-32` for the cut command is short for `1-32`. Together with the `-c` it is supposed to print the first 32 characters of the line of text that comes out of the md5sum command.
Is there any way to just use the md5 as filename? This script just add the md5 first but let the filename after it. Ex: filename.jpg -&gt; 5b5730a08e29f980e39d190ff1115342filename.jpg I have tried playing with "${x%%*.}" but I have only lengthened the filename or lost the extension :( Edit: nvm, I have fixed it using # insted of %. Ty for ur help &lt;3
Thanks, `tac` works nicely. Since the expected output can be anything, I want to capture everything directly under the formatting lines `####...`. So I use `tac "${log}" | sed '/^#/q' | tac`. However, that includes the formatting line, which I don't want (e.g.): ############################################################################### ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) In the event of nothing reported by stderr, I still get that formatting line, so I don't want it. So I finally have `tac "${log}" | sed '/\^#/q' | tac | sed '/\^#/d'` for the desired output (except there's an empty line above `ERROR`--does `sed` just replace text of a line with a blank line with the d option? I don't want the blank line): # there's a blank line on this line, but reddit doesn't show it ERROR: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) WARNING: Unable to download webpage: &lt;urlopen error [Errno -2] Name or service not known&gt; (caused by URLError(gaierror(-2, 'Name or service not known'),)) I'm sure I can modify this to make it shorter and/or more efficient--if you know how off the top of your head I'd love to know. But that's just being nitpicky.
[this may be of your interest](http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x361.html) - example does change color. Found that while doing search for something else.
&gt;Firstly, `/^#.../q` isn't a valid expression in sed Yes it is, it quits the script when that line is encountered. &gt;Lastly, assuming I understood what you wanted your first sed to do, your `sed '/^#/d'` I think they probably want to delete the commented line, whilst keeping all the other lines that were printed before that.
Is there some feature of `mapfile` that you need instead of creating an array the old fashioned way?
just pipe the output to `grep --color=auto '.'` .. it will colorize every character
This is the most useful approach IMHO. Using Homebrew saves you so much of pain in maintaining different versions of tools that I'd never go back from it. In fact, this is precisely the reason why using Windows becomes such a pain after using macOS or Linux. I know `chocolatey` exists, but it's still got a long way to go before becoming seriously usable. 
What backward breaking changes are there? 
The changes to how the `nounset` and `errexit` set options work are likely to cause scripts that rely on those options to behave differently. `errexit` will consider non-zero exit status of compound commands as fatal in newer bash versions, and `nounset` no longer considers an empty or unset array a fatal error.
Yea, the entire output. So I have now: `tac "${log}" | printf '%s' "${red}" $(sed "/^#/Q") | tac` Not sure if the command substitution is necessary and if that's what you mean, but it works. I don't need to reset it. Thanks.
You would take your script and turn it into a function that you put in your ~/.bashrc.
GP means that if you copy your entire script into a function that's part of your shell's startup scripts, then it will work.
I have bash 4.4.12(1) now (via Homebrew) and am using it as my default bash (confirmed with `echo $BASH &amp;&amp; echo $BASH_VERSION`). When I type `man readarray` or `man mapfile` I am told that there is not manual page for those commands. I thought mapfile (readarray) came with Bash 4.x? I am willing to use a different method to accomplish the same thing if any of you know how. What I am trying to do is read each line of a text document into my array. Then when I am finished working in my program, I want to update that same text file with the new lines of data being appended to the end of the array. Thanks!
&gt; nounset no longer considers an empty or unset array a fatal error. Do you have an example of this, as it seems to still work for me? 
The colon in this case is not an operator at all; it's just a literal string. Consider the following: FOO=hello BAR=$FOO,goodbye In this case, `BAR` is now set to `hello,goodbye`. The example with `PATH` is no different. It's simply setting `PATH` to be the current value of PATH followed by `:/opt/app/bin`.
Ah thanks for the reply, that option actually occurred to me after I submitted this post. I think the problem is that I am trying to understand every single line of code I encounter by googling like a maniac; I think I need a more structured and methodical approach to learning linux/bash. Do you know of any good, comprehensive, easy to understand bash tutorials for newbies. Edit: I feel this method of concatenation can cause problems if there are variables with similar names, for example FOO and FOOD, so by doing FOOD="Sp" FOO="Hello " BAR=$FOODaniel Is BAR now "Hello Daniel" ? Or is BAR set to "Spaniel" ?
`$BAR` will be empty because there is no variable named `$FOODaniel`. When that happens, bash shows no error or anything, instead will insert an empty text in that spot. If you want to make bash see `$FOO` or `$FOOD`, you need to use one of these options here: BAR=$FOO'Daniel' BAR=${FOO}Daniel BAR="$FOO"Daniel While experimenting at the command line, you can run `set -x` to enable some sort of debugging mode in bash where it will print what lines it sees and executes after it is done with its processing of variables. You can disable it again by doing `set +x`.
You might solidify it a notch more with `echo $PATH`. The PATH variable is a string, each directory listed in PATH is deliminated by a colon. 
I tried to get the formal naming for what I am about to describe, but cant find it. Effectively, no quotes are not always necessary in bash. $ echo this is one string this is one string Also bash is untyped, and generally default to strings. $ TEST_NUM=3 $ echo $TEST_NUM + 2 3 + 2 $ echo $(($TEST_NUM + 2)) 5 after the assignment operator `var_name=`, that line is read in as a string. I'm sure there are exceptions though, I'm still learning myself... 
It's worth pointing out that `echo` is a bit of a special case. When you give a command to the shell, bash will parse the command line to try to figure out exactly which command you're calling and with which individual arguments. So for example if you run `mv 'my file' my file`, bash will parse this string to figure out that you're calling the command `mv` with the three arguments `my file`, `my`, and `file`. How `mv` wants to handle those three arguments is up to it, but those are the arguments it gets, exactly like that. Most commands will *not* try to combine separate arguments like `my` and `file` and treat them as a single entity -- in the case of `mv` specifically, it's going to treat `my` as one file argument and `file` as another one. `echo` can be confusing to people who are new to the shell because it doesn't appear to behave the same way -- you can do `echo my file` and it will print `my file` as one line. But make no mistake, `echo` gets `my` and `file` as two separate arguments just like anything else would. You can confirm this by running `echo my file` (with four spaces): It still prints it as `my file` (with one space), because all `echo` knows is that it received two arguments -- any whitespace between those arguments was lost when bash passed them to the command.\* Most commands won't treat input like this. `printf`, for example, is superficially very similar to `echo` (it just prints things), but it behaves quite differently when you give it multiple arguments: $ echo my file my file $ printf '%s\n' my file my file As far as when you should quote things and when you should use braces in variable interpolation, I personally think it's hard to overdo it. I *always* use quotes when assigning variables, and I *always* use the brace syntax when I reference them. It's more consistent that way and you never have to think about exactly which characters might be interpreted as special ones, etc. With command-line arguments I'm a bit more lenient -- I don't always quote string-literal paths and simple stuff like that. \* Maybe this is confusing things further, but I guess it's also worth mentioning that when you run `echo`, by default, you're using a bash builtin. Builtins run *within* bash and have access to much of bash's internal functionality, so I wouldn't be surprised if they have some way to inspect the "raw" command line. But in practice they don't do that.
As per the valid examples above - VARNAME=("|')var("|') and VARNAME='([key]="value"... *repeat_over_as_many_lines_as_needed* ... )' Here's a regex that would work in PCRE with `/gim` flags: ^\w+=((\'\(\s+?(\[\w+\]="\$?\w+"\s*?)*\)\')$|("|\')[^"\']+("|\')$)
Is there any particular reason you can't just try to load the config file and see if it works? You'll have trouble parsing most languages with regular expressions.
How would you then continue in perl with that regex and `/.../gim`? How would you find errors in the file? **EDIT:** I copied your examples into a testfile, then tried an experiment with a perl one-liner and your rule, trying to change correct stuff into "okay" text with `s///gim`: $ perl -pe 'BEGIN{ $/ = undef } s/^\w+=((\'\''\(\s+?(\[\w+\]="\$?\w+"\s*?)*\)\'\'')$|("|\'\'')[^"\'\'']+("|\'\'')$)/okay/gim' &lt; testfile okay okay FOO='([bar]="bar" [baz]="baz")' FOO="bar"baz foo That `$/ = undef` reads the whole file into `$_` instead of going through it line-by-line, so searching over several lines should work. It doesn't catch your two-line `FOO='(...)'` example for me here.
I actually did mention this in my initial post, I was wondering about this as well!
Thank you for that write up. It's a much more complete answer. I chose echo as an example as it behaves somewhat similar to variable assignment. You are right to emphasize that while quotes are not always required, they should be the default.
I'm not actually using perl, but I've not found a decent grep regex debugger online, only PCRE; I'm just ensuring I use egrep compatible regex. That isn't the full or final regex, just a rough extract that works well enough to show what should pass. Multiple lines are the only thing causing an issue (with grep, that regex works fine in every debugger I've tried), and as I don't fancy having to write arrays across one huge line they're pretty essential. 
Why not just use perl then? For grep like output `perl -ne 'print if /regex/' file`
&gt; Why not just use perl then? I don't know. That's the whole point of this post - what is a suitable, easily cross-compatible (ie. works on a clean install of most modern distros without installing any other packages) alternative to using grep, if grep isn't an appropriate tool for this? Looks like perl should be a good candidate for that, so cheers.
whoah, thanks. * 1. I ran through shell check and am fixing some things. * 2. I will switch this. * 3. I dropped to sh but I use the $OSTYPE variable built into bash so I think i should use bash. * 4. You are so right, I did make an assumption that this shouldn't occur but in the case where it does I changed the line to git checkout "v$latestVersion" 2&gt; /dev/null || git checkout "$latestVersion" 2&gt; /dev/null || echo "Couldn't git checkout to stable release, updating to latest commit." * 5. How do i send output to stderr is it anything like sending output to /dev/null? * 6. I just don't understand this one do you mean my formatting is bad? To me the code definitely works and isn't broken in any way by indents but it could be formatted badly, I'm still working through leaning bash. How do i fix this? I appreciate all the tips :)
This general looping technique should work just as well with whiptail as with read.
Found this on a monolith server we have, and the person who set it up is long gone. I'd like to replicate the markers for shell prompts in bash on my local machine. I checked the server's .bashrc, but it didn't look any different from the standard template. And how can I get it to change colors if a command fails?
Try oh-my-zsh
If you manage hundreds of servers that run bash, but run zsh locally, you'll have to manage your head-context really well, or get very frustrated. 
Anyone else thought this guy was doing some bash scripting on a PlayStation 4?
Main reason I clicked this thread lol
assuming this is linux netstat. short for --unix, which is short for --protocol=unix. but, `ss` ...
Are you saying you forgot the shebang? 
I am also drunk, although probably less so than you. I didn't even know the #!/bin/bash was vital. I just assumed that it was proper commenting, so I just do it.
I just mean keeping up with the context of which shell you're using in your head. For me it became too unproductive to use a non-bash shell locally when all our servers are standardized on bash. The constant change of what commands are/aren't valid, how variables work, etc. led to reduced performance in both shells. 
Ah, that makes way more sense on the second read-through. Thanks for taking the time to explain.
No sweat. Sometimes when you make up a phrase, you have to explain it. ;)
Sounds like an M Knight Shamalamadan outcome for you
Speaking from years of experience, Bash and Booze do NOT mix. Nonetheless, save your work for humor when you wake up.
If you want to use a script to manage your machine's state, e.g. bootstrap it with software and various configuration settings, I _highly_ recommend you look at [Ansible](https://github.com/ansible/ansible). Also, your bash file interpreter should be `#!/usr/bin/env bash` for greater portability.
Thanks for your help, I am getting the following when attempting this however: bash: syntax error near unexpected token `('
I fixed the code. There was an error in your find command. I changed it a bit to make it simpler to search thru too. Also, you don't need to have type d and type f. This implies that you want to see files and folders so thats basically everything in there (unless you want to remove symbolic links/hard links)
Ah...yes. I intentionally didnt write it (I think) because I've seen controversy because of people calling it crashbang and shebang and I don't want to upset anyone. But yes, that is exactly what I forgot.
I sent the output of the upgrade command to the network interfaces file and the network information to an updates.log file. Prime work I must admit.
Windows me though....I remember going to my mums office as a kid and seeing that thing crash left right and center...fond times. Then they switched to xp and I got absorbed by pinball
ssh redirects stdout from your cat to stdin. Use ssh … &lt; /dev/null to circumvent that.
This is a good opportunity to learn the basics of Ansible. You don't even have to get into any of the config stuff yet, just making an inventory file so you can `ansible -i hosts some_group -a 'uptime'`. And then you can slowly integrate it more and more as you have time.
You get the "Useless Use of Cat Award" while read output ;do echo $output ; ssh ${user}@$output uptime &lt; /dev/null ;done &lt;/tmp/server_results
while read host;do ssh -n $host "hostname;uptime;echo";done &lt; /tmp/serverlist | tee /tmp/server_results
Wouldn't that need to be `&gt;&gt; /tmp/server_results` at the end?
I get the feeling this is a question for a class. Is it?
&gt;So if I want a local variable in function I would have to pass it as argument. You can declare a variable as local to a particular function using the `local`, `declare`, and/or `typeset` builtins. If you put `local foo` at the top of function `bar()`, it would prevent anything from a higher scope from reading the value of `foo` as it was set within `bar()` (it would still be readable and writeable by lower-scope functions, though, if they wanted it). &gt;Can anyone please tell me, how the for loop works internally? I am bit confused about the for loop list, that is behaving like local variable. `for i in 1 2 3 4` reassigns `i` to the current list element at the beginning of each iteration. So first it does `i=1`, then it executes the code inside the loop, then immediately after that it does `i=2`, and so on. When you call `loop 2` from inside the `loop 1` call, `loop 2`'s `for` loop is going to be messing with the same variable `i` that `loop 1` was. But as soon as `loop 2` finishes, the `for` loop in `loop 1` will start its next iteration, and in doing so it will change `i` back to whatever it's working on (in this case `3`).
&gt;I'm not sure, but I think these builtin are not in POSIX, so I would rather use something that is closer to the standard. That's true, they're not. Most shells support at least one of them anyway, though, even "strict" POSIX shells like dash. &gt;Yes, but how does it remember what was the last value used in this for loop? Is this information on the stack, and on return from the recursive function it is accessed? Oh I guess I misunderstood. I have only a limited understanding of bash's code base, and I'm on mobile right now so I can't really look it up. AFAIK, bash parses shell word lists into linked lists, and it keeps shell variable assignments in a big hash table... if that's of any use. idk
Find has a very useful option in this case: -mindepth X where you determine minimum depth in the directory structure where is should match what you are trying to find. There is also -maxdepth X which limits how deep you want the find to go. Man find is your friend.
Just `cd` to the folder you want first. You can do that in the script right before the `for` loop. (Note that `cd` in a script doesn't affect the working directory of the parent shell.)
-mindepth 1 ?
-1 as an index works fine for me: $ bash --version GNU bash, version 4.3.48(1)-release (x86_64-pc-linux-gnu) $ a=(1 2 3 4 5) $ echo ${a[-1]} 5 
No. I actually ended up rewriting my if statement into two if's and it bypassed the error. 
I understand the script you wrote no problem. If you don't mind, would you mind elaborating on how word-splitting (especially with regard to directories that contain spaces) would behave with how the script is written? I only ask because your $dir references aren't quoted and I'm wondering if they should be. 
Awesome, thanks. The site doesn't work now, I will check it out later. 
Thank you very much, I know this example is not the best, but I've tried to come up with something simple, where I could try recursive functions. Also, how is $i local? I can access it from outside the function. 
Where is "your" code is $i outside the scope of your function()?
What do you mean you have "tried" something simple? You are not making any sense when you state your issues. On one hand you mention you have trouble with "loop" yes your calling the same function inside the current function in the control flow. I recommend to you to break your code down to the least common denominator and you talk about "recursive" functions? You have not even pasted the code and its control flow out thoroughly. You have to walk before you can run slow down! 
Ups, sorry - in this case (your script) it is directory name. But in general, I would be interested in the steps how the words in for loop are expanded. &gt; for name [ [in [*words* …] ] ; ] do commands; done 
I think you are confusing syntax/logic. Global VARS can also be references inside functions yet declared outside the function. If you happen to write code a certain way that allows you to use $each outside the function as in: "for $each in $VAR", you are writing code that will be terrible for people to reuse or implement, why would you ever consider $each to be a global variable? Those are to be used to parse or loop the "string" that is storing our string or null values. Just because you can echo a $i does not mean it should be used globally.
OK, I see. I know what you mean. Thanks anyway.
In closing, you provided URL's to guides. Let me recommend the 2 more recognized BASH guides: http://mywiki.wooledge.org/BashGuide "most current" http://tldp.org/LDP/abs/html/ "perhaps some outdated stuff mildly"
You wrote: #Also, how is $i local? I can access it from outside the function. If you understand then why did you say imply that you can access $i from outside the function?
you wrote: #but if I'm not mistaken, variables in shell are global (except for positional parameters (that are local to functions) Example: $ VAR2=LINUX $ cat var2.sh #!/bin/bash echo "VAR2=$VAR2" VAR2=UNIX echo "VAR2=$VAR2" $ ./var2.sh VAR2= VAR2=UNIX Still you will get blank value for variable VAR2. The shell stores variable VAR2 with the LINUX only in the current shell. During the execution of var2.sh, it spawns the shell and it executes the script. So the variable VAR2 will not have the value in the spawned shell. You need to export the variable for it to be inherited by another program – including a shell script $ export VAR2=LINUX $ cat var2.sh #!/bin/bash echo "VAR2=$VAR2" VAR2=UNIX echo "VAR2=$VAR2" $ ./var2.sh VAR2=LINUX VAR2=UNIX $ $echo $VAR2 LINUX
&gt; I do think you are better off "DOUBLE QUOTING" &gt; An unquoted variable and command substitution can be useful in some rare circumstances: &gt; When the variable value or command output consists of a list of glob patterns and you want to expand these patterns to the list of matching files. When you know that the value doesn't contain any wildcard character, that $IFS was not modified and you want to split it at whitespace characters. When you want to split a value at a certain character: disable globbing with set -f, set IFS to the separator character (or leave it alone to split at whitespace), then do the expansion. 
There's also this here to get a local variable: $ x=3 $ sub() { echo $x; } $ x=5 sub 5 $ sub 3 This is the behavior in bash. When you try the same on the command line of dash (that's the /bin/sh on Debian), then `$x` will change to `5`: $ x=3 $ sub() { echo $x; } $ x=5 sub 5 $ sub 5 This is a difference between bash and dash when you use this `variable=value command` type of command line to call a function. If you use this type of command line to run an external program, then dash as well will not change the variable. Here's how dash behaves when starting an external program: $ x=3 $ x=5 perl -E 'say $ENV{x}' 5 $ echo $x 3 Here it exported an environment variable `x` for the external command, but did not change the shell variable with the same name.
Thanks, I did not know that! :) 
The array was initialized but empty, and I suspected this had something to do with it when it ran into a negative index.
Open source projects need this ability. Working on Jira tickets will help the project, build your portfolio, and get your name out there. Meanwhile apply at companies that sponsor the project, as they value its capability. Eventually great things will happen. 
Bash, docker, config management, e.g puppet/ansible are in demand. Know these tools and your good to go. We are no longer in a place where 1 tool can provide long term job security as it pertains to supply and demand.
I would argue to step back and learn shell programming as it pertains to automating, once mastered then graduate onto CM.
This is *part* of the skill set for (Linux) sysadmins and operations engineers. But there's a whole lot more - both those jobs rely on a bevy of tools. As a concrete piece of advice, you should try to avoid using bash to do configuration. I've done it, and it's a nightmare once it gets non-trivial. Puppet, chef, Ansible, salt, these tools are designed for the job and do it much better. Personally, I think Ansible is the easiest to get started with, but the specific choice isn't really that important. What's your background? What have you been doing for a job, and how long? How did you get into this? Do you have experience programming, other than bash? Do you have experience with general IT work? And what do you *enjoy* doing? You should also check out r/itcareerquestions.
The hard truth is that Bash / sed alone won't get you anywhere. Add things like Docker or Ansible to your skillset (or better: both) and things may look different.
I think I read you clearly. I still don't agree. The US Constitution was a short document with just four sheets of paper, but experts in constitutional law have valid long opinions. Don't confuse complication with sophistication. Brevity can be simple, elegant, and sophisticated.
Since modern web application systems will use Linux as the backbone OS, then BASH is the glue to keep things together as it pertains to: (IO, web servers, log files, processes, maintenance, DB's). Interactive shell is the better case study. Bash is simply extending the interactive shell further to be more useful. So to mention BASH without anything else is similar to mentioning the HEART in such a way as to imply the BRAIN is not significant or vice versa. You must be passionate about this stuff or you will never learn the more granular aspects of our craft.
That means your argument is falling into a logical fallacy called a tautology. You're effectively saying that any argument for shorter bash scripts is false because the person saying it writes shorter bash scripts...
Bash is like tinder, you use it in order to stop using it. If you're making a huge bash, what you really need is automation, and there are tools for that. Many many tools. Also if you are doing shell serious shell scripting where it's warranted, you should be using sh not bash. Looks like **you** lack experience and should keep your opinions to yourself.
You need to read more carefully. Let's review. 1. SMB == If you do not know what this is then your in trouble. (little shop || non corp) ...so you answered "not paticularly" even though you did not have the right definition? exit 1 2. Corp dances to the beat of a different drum then an SMB two different animals. Higher risk higher reward in the SMB arena, less risk lower reward in Corp. 3. You getting to decide will not always be the case at every shop. If you do not know this by now you will in due time. 4. If Trends do not matter than why is Microsoft still in business and python on every book shelf. ; ) EXCEPTIONS TO THE RULE ARE NOT THE RULE! 
Inquisitor without context your statement is ignorant to say the least. BASH is extremely useful. It still has a valid purpose in operations and if more programmers knew it better they would use it more often. Without a job role, or a purpose behind your work to imply BASH is not useful is simply wrong. Now if your generalizing and contasting long BASH scripts to say a more efficient way of managing your configuration yes CM is preferred. On another note, BASH is responsible for almost all of the role outs and bootstrapping of most of the tools we use, to include PAAS services. A newbie may misunderstand what you said, I just wanted to shed some more light on this. 
Well written why no comments in the code though?
Good idea. Thank you. I had "dev ops" on my radar already, but I think, I should take this field more seriously because of two reasons: First, optimizing processes was always something that I like. Second, "dev ops" seems to be something between development and system administration. I was caught between two stools already during the last 20 years, so "dev ops" comes in handy anyway.
I know, this can only be a joke, because the whole idea behind Bash scripting was automation / automatization for me ever since I write bash scripts. I'm looking for use cases that can be solved with automation / automatization. But apparently, I have to look for new (more modern) tools that today in industry are used. This thread already mentioned Ansible, Puppet, Chef and Saltstack. Some peers of me used or still use Preseed to automate the installation of Debian VMs. Just now, I found this: https://en.wikipedia.org/wiki/Comparison_of_open-source_configuration_management_software Preseed isn't mentioned at the Wikipedia page but it is also an approach to automate something. To get a complete overview: Is there something else out there that can be used to automate something? Lest I accidentally overlook something that may be important...
**Comparison of open-source configuration management software** This is a comparison of notable free and open source configuration management software, suitable for tasks like server configuration, orchestration and infrastructure as code typically performed by a system administrator. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.23
Actually both concepts are attractive to me: Docker is for virtualization. I deal with virtualization since about 2001. I worked with VMware, I tried Xen when nobody knew it, I created an IPv4 network of multiple KVM/qemu machines (one of it talked with my DSL modem over OSI level 2; it contacted a MAC address to fetch an IP address), I work with Virtual Box every day and at work, I deal with Citrix XenServer. Docker gives another virtualization approach that has its charm. A whole new approach that could save CPU time and memory. Ansible is for automation. Question: Are there jobs out there that desperately need both? Virtualization AND automation? Docker AND Ansible? What kind of jobs? What are the target groups? Is it possible to combine Docker, Ansible and "dev ops" altogether? It would be the ultimate dream for me, but does it match? If so, neither I had to do with Docker nor with Ansible as yet: How likely is it that a company with my background and skill set hires me and gives me the chance to learn the ropes of Docker and Ansible while I get paid for it? Or is it a better idea to rent a vServer to fiddle around with Docker and Ansible before I'm looking for jobs that need these basics? I want the job new and renting a vServer costs money. And: I would definitly do that if that is really necessary. But the other way around, if I learn it when somebody really needs it, I would waste less time for possible side trips that nobody cares about (like so many other stuff I learned in my life and nobody cared about).
What is my background? I started programming 1991 and was 15 years old back then. Computers were rare in the GDR, but after the wall was teared down, I discovered computers and loved them. I started programming them. First in BASIC, later in machine language. One of my first machine programs was a disassambler for a KC85/2 (it had a U880d, which was a Z80 clone; the GDR cloned western technology like that and gave it other names). Later, I wrote software (games and other stuff) in Turbo Pascal, partially with embedded 16 or 32 bit assembler code). After school, I studied computer science. I programmed in C, C++, Java, Perl, Python, HTML, Javascript and so on. During the last 20 years, I coded in Java, HTML, CSS, Javascript, SQL, PHP, Perl, Ajax, JSON. But, my focus moved a bit away from programming towards system administration. As student, I was confronted with some Unix dialects. 18 years ago, I started at home with SuSE Linux. Ten years ago, I changed to Debian Linux. I systematically acquired Debian know how with the goal to create THE PERFECT computer based working environment. Seven years ago, I rediscovered FVWM. I learned to use the pieces of the FVWM config language and created my absolute dream desktop based on FVWM, some Bash and some other open source 3rd party tools. And, I systematically acquired virtualization know how: I worked and partly still work with VMware, Xen, KVM/qemu, VirtualBox and Citrix XenServer. Also, there was a period of time where I wrote manuals for security software. This job drastically improved my English language skills with the result that I can discuss things with you here on Reddit. And still one thing: Since 16 years, I work in the line of the IT security business: I can tell you all about encryption, signing, private keys, public keys, S/Mime and PGP in my sleep. In the last seven years, my Bash skills improved more and more, because on the one hand, I automated the deployment of my own PHP based online game using a lot of Bash, and on the other hand, I automated a lot of stuff (mainly software tests) at work. What do I enjoy? So, this is the cause why I'm here at "r/bash": Automating things is something that I enjoy most. By far. My issue is that 1st) I have done a lot of different things regarding computers and 2nd) some of my skills obviously seem to be a bit behind (as some of you have already discovered). I have to make a decision. Which skills do I want to focus on and want to improve to stay more current (like BruXy said)? I'm looking for a target group that needs as much of my skills as possible. If automation, virtualization and "dev ops" could be part of my future working life, it would be cool. To begin with, I need an overall survey, so I can be really sure that I didn't overlook something very important.
you remind me of my best friend. one of the best dudes i ever knew. rock on
I suggest a simpler `weather` function: curl wttr.in Or, if you really want to have no dependencies: # open wttr.in, port 80 and send HTTP request exec 3&lt;&gt;/dev/tcp/wttr.in/80 &amp;&amp; printf 'GET / HTTP/1.1\r\nHost: wttr.in\r\nUser-Agent: curl\r\n\r\n' &gt;&amp;3 &amp;&amp; { # remove headers while IFS= read -r line &amp;&amp; [[ $line != $'\r' ]]; do :; done # read two blank-line-separated blocks for _ in {1..2}; do while IFS= read -r line &amp;&amp; [[ $line != '' ]]; do printf '%s\n' "$line"; done; printf '\n' done } &lt;&amp;3; exec 3&lt;&amp;-; `wttr.in` already uses your IP address to determine your location if you don’t specify it, so there’s no need to do that in your script by yourself. And using `curl` (or opening `/dev/tcp/` pseudofiles) naturally checks whether you have an internet connection, there’s no need to do that explicitly either (it’s just a race condition anyways).
&gt; wttr.in already uses your IP address to determine your location if you don’t specify it, so there’s no need to do that in your script by yourself. Unfortunately it's a bit shit. Last time I ran it bare, it thought I was somewhere in Serbia. I just ran it now, it thinks I'm in Auckland... at least that's the right country I guess. I would expect my IP to at least hone to my ISP's base in the Hawke's Bay, and that's still 4 hours drive away... FWIW here's my take on the `weather` function: # Get local weather and present it nicely weather() { # We require 'curl' so check for it if ! command -v curl &amp;&gt;/dev/null; then printf "%s\n" "[ERROR] weather: This command requires 'curl', please install it." return 1 fi # If no arg is given, default to Wellington NZ curl -m 10 "http://wttr.in/${*:-Wellington}" 2&gt;/dev/null || printf "%s\n" "[ERROR] weather: Could not connect to weather service." } Merge in OP's trickery and we have something like: curl -m 10 "http://wttr.in/${*:-$(curl -s ipinfo.io/loc)}" 2&gt;/dev/null || printf "%s\n" "[ERROR] weather: Could not connect to weather service." Might be worth throwing `-m` into the internal call of `curl` too... 
You should remove API keys from your code. (To prevent abuse) Use `.` or `source` to read the API keys from config files. Also, add comments and warning if appropriate, with a rundown about how to get said API keys. And really, you should not even think of writing install scripts. That's the package manager's job. (You may write a PKGBUILD or recipe for other distros if you feel like it)
&gt; exec 3&lt;&gt;/dev/tcp/wttr.in/80 That's Linux-specific. I prefer testing for `curl(1)` (generic), `fetch(1)` (FreeBSD) or `ftp(1)` (OpenBSD only) and using a wrapper to crawl/download.
Yeah you are right, I was being lazy as I was writing them
I totally agree with points 1 and 2 will definitely implement them however I am not exactly sure how to have an installer that can go through the package manager for just bash scripts. 
I was attempting to stay away from external dependencies, python is basically preinstalled on every system so I wouldn't consider it a external dependency.
Witter.in has a problem grabbing the IP of the user so this is an unreliable method. As far as the fact that curl has a built in internet checker could you explain further? Will it return an error if no internet connection exists?
Could you please explain why the redirection from /dev/null yo uptime command? 
Just call `curl wttr.in/CITYNAME`
Has been commented :)
Has been commented :)
&gt; opens a bidirectional file descriptor on 3? yup &gt; closes it again? yup &gt; Also curl would have the advantage of being capable of https yup &gt; and other http "fancy stuff" such as redirects not by default, `curl` only does that with `-L` :/ I think `wget` is the preferred choice if you need something that behaves more like a browser by default
&gt; Will it return an error if no internet connection exists? Well, it can hardly fetch the website without an internet connection, so it has to return some error ;) (unless it just ignores it, of course) On a systemd system, you can easily test the behavior of a command without internet connection by running it with `PrivateNetwork=yes`: $ sudo systemd-run -qt -p PrivateNetwork=yes curl wttr.in curl: (6) Could not resolve host: wttr.in
That was it! Thanks! The original list of hashes was export from Excel. Marking it solved. 
These are great, thanks! It would be cool to input multiple stock names and have them all output as a table.
Adding the `\r` character to IFS will also do the trick (as in making `read` skip it) and is faster. In this case: IFS=$',\r' 
Im honestly not quite sure it seems the JSON response is geared towards and individual company although it could be pretty easy to fill in the same information for an index. I can't say I looked any further information then individual stocks 
I know I did something similar to this for importing users from a CSV file but I don't have it in front me at the moment. I believe I read the file into an array first. Each element of your array I think would be a full line of the file so you would then need to loop the array to split the different indices of which you could split into more arrays for first name, last name, Dept etc. Then loop whatever commands you need while reading the arrays. I know whatever I did was bonkers and may not have been the best way but I could not find a better way to do it with so much info. If I can find what I wrote I'll post some snippets that might give you a better idea.
There is a script for this on the first page of Google results. 
grep -A 2 -B 1 pass file 
&gt; As far as the fact that curl has a built in internet checker could you explain further? Will it return an error if no internet connection exists? Just do what I do: set a timeout (`-m 10` in my earlier post)
Instead of sleeping for a fixed time like those five minutes you imagine, the kernel has an "inotify" feature where programs can register to be notified about file events. Using that feature, programs will sleep until the moment something happens. There's an "incron" daemon that you could use to trigger your bash script, and systemd has "path" unit files that you can read about with `man systemd.path`. There's an "inotifywait" command line tool to use this inotify stuff yourself in your bash script. I don't really know about your main question, about what to do to make sure you don't work on files that are just half-written or something. You could try to check with "lsof" if a file is still open in a program and then sleep and loop? There are file "close", "close_write" and "moved_to" events. Maybe the tool you decide to use to control your bash script allows waiting for those events, and using those events as the trigger maybe completely prevents problems?
Doing the same thing with sed and awk is slightly less convenient: awk 'BEGIN {RS=""; i=1} {gsub("[0-9]+=", i "="); print $0 "\n"; i++}' rah | sed -n '$q;p' Set awk to slurp paragraphs (`RS=""`), do the same substitution as with the perl version, print the record with an extra newline so the config sections are separated by a blank line, and then use sed to remove the extraneous blank line from the end (quit at the last line instead of printing it).
Dear God - Wow. Just woke up to this. I'll give it a shot later and report back! 
What kind of file are you trying to edit?
You're script is working. The thing is, shell scripts are run inside a subshell, and each subshell has its own concept of what the current directory is. The cd succeeds, but as soon as the subshell exits, you're back in the interactive shell and nothing ever changed there. Some people fix things by putting an alias in their startup file. But there's another way which is more simple. Execute your script like this: . ./myscript.sh 
Thanks. But I am a novice in writing scripts, and not well versed in shell/subshell difference. Can you suggest how to make it work?
That's the attractive thing about computers. Always something more to learn. The key is to not let it freeze you with fear.
Thank you so much. I did the . ./readfile.sh; and it's doing cd into the given folder. That said, can you help me on what the extra ". " does?
Type `help .` for a help text about that.
All good - was an interesting problem to solve. I tried to keep external tools and pipes to a minimum as they tend to slow these sort of things down pretty badly. If you have any hassles, or weird behaviour, feel free to get in touch directly.
Great help for a novice. Thank you!
You can also change a format of nmap to "grepable" one, for example: nmap -oG - -p22 192.168.0.0/24 Scanning open ssh port on local network.
Sorry for my late reply and thank you for the link!
I'm trying to edit /etc/hosts /etc/hostname /etc/dhcp/dhclient.conf
Nope. Dollar-parens means command substitution, but plain parens mean subshell.
No, I didn't. It's a regular subshell, not a command substitution. The version with the dollar would substitute "2" for the command substitution and so attempt to execute the command "2".
Of course it does, where did you get the idea that it doesn't? The Bourne shell supported functions from the beginning. Not that anyone still uses the original Bourne shell. Even on Solaris, the default 'sh' is now a POSIX standard shell.
I've tried it both ways. when I use /bin/bash I get errors that continue should only be used for for loops
&gt; `cd $HOME` that's redundant (cc u/videoflyguy). just `cd`. You could even drop `cd` and use `~/.mailrc` instead. Also, I would use printf like this: printf '%s\n' "set smtp://x.x.x.x" &gt; .mailrc printf '%s\n' "set from=root@$(hostname).domain.local" &gt;&gt; .mailrc
My point was to show how to fix the actual problem in OP's code, not to point out unrelated minor nitpicks that are a matter of opinion anyway.
I have been refactoring this code for the past week or so, think I just forgot to take that out, thanks:)
for better explanation about this go [here](https://superuser.com/questions/548330/is-a-subshell-executed-with-a-here-string)
The error was saying that the xls2csv command didn't exist. By box I meant a text box. My bad. After doing a bit more googling and trying to figure this out I feel like I'm in way above my head here. 
I'm using my University's computers remotely from my laptop. Can't remember exactly what Linux distribution it is. I think it might actually be an xlsx file since I'm using office 365 which I got through Uni. I've added an edit to clarify the project a little more. I'm starting to doubt if this is even possible and if it is then it seems like a long shot. Or maybe it's just simply too far above my skill level. Should I just try to do the task manually and forget this?
hmm, I get that that's saying it's not making a subshell, but saying what it's not doing doesn't imply what it *is* doing
Sure. To start with: don't use `echo`. You should be trying to use `printf` instead anyway ([some reading](https://unix.stackexchange.com/questions/65803/why-is-printf-better-than-echo)). Next, you should try to avoid aliases and instead use functions instead. As a general rule, aliases should be used only for changing the default behaviour of an existing command e.g. `alias ls="ls -la"`, anything more complex than that should go into a function. So now we get something like this: # Faux-tree command tree() { find "${1:-.}" -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g' } As a matter of style, you could have a simpler function like that in one line e.g. tree() { find "${1:-.}" -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'"; } I've added a positional parameter so that you can optionally point it at a directory, e.g. `tree /some/path` or it will default to the current directory. The function as shown won't do any of its own sanity checks, the great thing about being a function is you can choose to add those. `printf` can then be used as a one liner like so: printf '%s\n' "# Faux-tree command" "tree(){" " find \"\${1:-.}\" -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'" "}" Or in a script like so: printf '%s\n' \ "# Faux-tree command" \ "tree(){" \ " find \"\${1:-.}\" -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'" \ "}" Which to my mind is cleaner than the usual one-call-of-`echo`-per-line thing that you always see. Note that the `find | sed` line can be a bit funny with the mixed quote types. `'find "${1:-.}" -print | sed -e "s;[^/]*/;|____;g;s;____|; |;g"'` is potentially another way to express it. I'll leave the question about which is better for others to chime in on. The next step beyond that, for scripts, that you should look at once you get past maybe half a dozen lines is a heredoc, and is probably the safest approach when you're mixing different quote types. Something like: cat &lt;&lt; "HEREDOC" # Faux-tree command if ! command -v tree &gt;/dev/null 2&gt;&amp;1; then tree(){ find "${1:-.}" -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g' } fi HEREDOC Now we've made the function actually check that a pre-existing `tree` command doesn't exist, and if not, then step-in and provide this function.
It may be possible (certainly reading information from one file and writing it to another file is possible), but if you want us to help you with this project you need to give us more specifics, starting with what the file actually is, what distribution you're using, and what you want the output to look like. Why powerpoint? Displayed on a monitor how? On your local computer? What is the operating system on that computer? What is going to cause the text to be displayed? Do you need formatted text, or will a plain text file be sufficient? It sounds like you should be writing the output in some kind of markup format which you can convert to a PDF afterwards. You also probably don't want a program which will stay open forever and read one line from the file every day -- you want to run the program every day and advance to the line in the file which corresponds to the right day. What exactly are the times in the file? Do you actually need the file, or can you just calculate those times in the program? Are you trying to make a clock?
Thanks for the peer review, I'll admit I was distracted with something else while typing that up. Updated the original post.
I tried to see if there's something obvious happening. I've let a perl program print the "stat" function result on its STDIN file thingy, like so: perl -E 'say for stat STDIN' perl -E 'say for stat STDIN' &lt; .bashrc perl -E 'say for stat STDIN' &lt;&lt;&lt; "test" This 'stat' returns a list of numbers. Here's the same output as previously but with the names for the entries added as described in the perl documentation: perl -E '@s = stat STDIN; for (dev, ino, mode, nlink, uid, gid, rdev, size, atime, mtime, ctime, blksize, blocks) { say $_, " = ", shift @s }' When I use this on `&lt;&lt;&lt; "test"` it shows a file size same as if used on a real file. On my system, I get this output for a here-string and a real file: $ perl -E '@s = stat STDIN; for (dev, ino, mode, nlink, uid, gid, rdev, size, atime, mtime, ctime, blksize, blocks) { say $_, " = ", shift @s }' &lt;&lt;&lt; "test" dev = 43 ino = 770627 mode = 33152 nlink = 0 uid = 1000 gid = 1000 rdev = 0 size = 5 atime = 1498825452 mtime = 1498825452 ctime = 1498825452 blksize = 4096 blocks = 8 $ perl -E '@s = stat STDIN; for (dev, ino, mode, nlink, uid, gid, rdev, size, atime, mtime, ctime, blksize, blocks) { say $_, " = ", shift @s }' &lt; .bashrc dev = 42 ino = 105951 mode = 33188 nlink = 1 uid = 1000 gid = 1000 rdev = 0 size = 9076 atime = 1498768975 mtime = 1496323464 ctime = 1496323464 blksize = 4096 blocks = 24 There's a device number and an inode for that `&lt;&lt;&lt; "test"` experiment. I don't know how to research where those are pointing to. What's strange here for me is, when I look up 'stat' online, everywhere mentions that it should show a large number that can be split into "major" and "minor" device node numbers, but that's not the case for me. If I do `stat .bashrc`, the device number is `Device: 2ah/42d` which can't be split into major/minor as it's just too short for this. I couldn't find out what this 42 is supposed to be in `/dev` because the content over there still does have those major/minor numbers for me when I do `ls -l /dev`, same as people explain the 'stat' device number online.
Yes. # dd if=/dev/zero of=/dev/sdX # where /dev/sdX is the disk to wipe. That's just brutal though. What do you want to do with the drive? Partition it too? Install Gentoo on the machine?...
Ummm no just reformat it for now just going to try something out for my internship ummm dumb question is there a windows equivalent of this or only linux can do this...
There should be a Windows equivalent, though I can't help you (I never wrote a working batch or PowerShell script, ever). DISKPART.EXE should be a place to start looking. Also, if you want to completely wipe the machine and give it in a pristine state, you should: for i in 1 2 3; do dd if=/dev/zero of=/dev/sdX bs=4096 # where /dev/sdX is the disk to wipe. # it will write literal zeroes on the disk, by batches of 4096bytes echo "$((33 * i +1)) % done" done # and we loop, so you do this thrice
I agree with /u/McDutchie that you should use `find -exec`, but I still don’t understand why you got less results. My version of `md5sum` prints its entire output with a single `write` call per file (you can check with `strace -e write -s 128 md5sum /dev/null /dev/null /dev/null`), so there shouldn’t be any intermingled partial lines as far as I understand.
What's that about learning something new each day? Thanks. I'll give this a shot. I sometimes toss an sh -c 'process 1;process 2' into the xargs which is why I prefer using that. I know I can use multitple -execs with find but habits are habits. :)
I don't get any intermingled files! I just don't get some lines. I'm going to test out the -exec + option and see how it affects things.
I see the same here with that command line you use. I get all files in the output if I use xargs like this instead: find target_folder -type f | xargs -d'\n' md5sum or like this: find target_folder -type f -print0 | xargs -0 md5sum I tested it like this: $ find /usr -type f -readable | wc -l find: ‘/usr/share/polkit-1/rules.d’: Permission denied 350591 $ find /usr -type f -readable -print0 | xargs -0 md5sum | wc -l find: ‘/usr/share/polkit-1/rules.d’: Permission denied 350591 $ find /usr -type f -readable | xargs -I {} -P 8 md5sum {} | wc -l find: ‘/usr/share/polkit-1/rules.d’: Permission denied xargs: unmatched single quote; by default quotes are special to xargs unless you use the -0 option 320692 The `/usr` location with those massive amount of files was the first spot I could find here for me that saw this error and reduced number of files with that xargs command line you used. In other locations I tried, it worked fine.
I'll test this out on Monday. Thanks for confirming the results.
I didn't write `/Dev/sd$i`
ah now i see what you meant, i should read better!
 file --mime-encoding * | grep binary | awk '{print $1}' Does that not (mostly) do what you're after?
You normally do something like this by making find print just file names, then reading those into your bash code: find . | while read -r line; do [[ $(file -b --mime-encoding "$line") = binary ]] &amp;&amp; echo "$line"; done Here's the same written with line-breaks: find . | while read -r line; do [[ $(file -b --mime-encoding "$line") = binary ]] &amp;&amp; echo "$line" done What you tried to do with find's exec fails because bash will process the `$()` first, before it starts find. If you want to get your idea to work, you can make find exec a `bash -c '...'` command line where you protect the `$()` with `'`. That would look like this: find . -exec bash -c '[[ $(file -b --mime-encoding "{}") = binary ]] &amp;&amp; echo "{}"' ';' 
Command substitution is a feature of the shell, however find does not start a shell with -exec. find . -type f -exec bash -c 'for f do [[ $(file -b --mime-encoding "$f") = binary ]] &amp;&amp; printf %s\\n "$f"; done' findbash {} +
The filenames should be passed to bash as arguments. Mixing them with the script can cause it to break and allows code injection.
if you have any issues with our script you can visit our site directly on perfectrootserver.de or github https://github.com/shoujii/perfectrootserver But please dont say thats your own script, thats not nice.
It doesn't seem to walk down directories.
Other people have already offered some solutions, but I want to explain why your command doesn’t work, because there’s a fundamental misunderstanding at work. *Command substitution* (`` `command` `` or `$(command)`) is a feature of the shell, which happens before the main command runs. `find` is a regular command, so it runs after command substitution. When your ran this command: find . -exec test `file -b --mime-encoding {}` == "binary" \; Bash *first* processed the command substitution and ran the command in backticks: file -b --mime-encoding {} You presumably don’t have a file literally named `{}` lying around, so there was an error: cannot open `{}' (No such file or directory) Bash took this error message (ignoring the error status) and substituted it into the main command, and ran: find . -exec test cannot open `{}' (No such file or directory) == "binary" \; `find` found the first file (let’s call it `whatever`) and ran the following command: test cannot open `whatever' (No such file or directory) == "binary" `test` is normally invoked like `test a -eq b`, `test 1 -lt 2`, `test file1 -ot file2`, etc., where the second argument (`-eq`, `-lt`, `-ot`, etc.) is one of a small set of strings, generally starting with a dash. The second argument in this command, however, was `open`, which is not one of those operators, so `test` complained about the extra argument (guessing that you meant to run `test cannot`, which tests if `cannot` is a nonempty string). When working with the shell, you need to be aware which features are parts of the shell syntax (`` `command` ``, `$(command)`, `if`, `for`, `;`, functions, etc.), and which ones are just regular commands (`test`, `find`, etc.), because they can’t always be combined like this.
Ah thank you. The first solution seems so clunky for what I think is a fairly common task that it feels as though it points to a fundamental underlying design problem somewhere. It is fast though. I like your second formulation. Indeed last night I had written this: #!/bin/bash # is a file a binary file, return 0 (success) # a file is binary when `file` says it's charset is binary # # Usage: isBin file test $(file -b --mime-encoding "$1") = binary $ find . -exec isBin "{}" \; -print . ./.git ./.git/hooks ./.git/index ./.git/info ./.git/logs and this works too $ find . ! -exec isBin "{}" \; -print ./.git/COMMIT_EDITMSG ./.git/config ./.git/description And that works much the same way as yours. Advantages, it's much easier for me to remember than the first formulation. Disadvantages, all those execs make it far slower. Somehow I think there is either a flaw in "find", that it's -type option is to high level, or that it needs an updating to better support either "file" or more file attributes in general, or perhaps points to a need to write `isfile`, a command just like file but that compares the output of file with some form of attribute pattern. 
Thanks, that was enlightening!
Very nice and easy to understand. Thanks that was very helpful.
I have an alternative solution to suggest: find /usr/bin/ -exec file --mime-encoding {} + | awk -F: '$2 ~ " *binary" { print $1 }' With `+` instead of `;` at the end of `-exec`, `find` passes several found files as separate command line arguments to a single program, instead of calling the program once for each file. `find` then prints the file name and file type for each of those files (note how I removed the `-b`), and `awk` prints the file name for those lines where the file type is `binary`.
You could wrap the whole command line into a script named for example `findbin`: #!/bin/bash if (( $# != 1 )); then echo "Usage: findbin &lt;location&gt;" exit 1 fi find "$1" -type f -readable | while read -r name; do if [[ $(file -b --mime-encoding "$name") = binary ]]; then printf "%s\n" "$name" fi done You can then do `findbin .` to search for binary files. I like saving things like this as functions in .bashrc instead of as separate scripts: findbin() { if (( $# != 1 )); then echo "Usage: findbin &lt;location&gt;" return 1 fi find "$1" -type f -readable | while read -r name; do if [[ $(file -b --mime-encoding "$name") = binary ]]; then printf "%s\n" "$name" fi done } Or you could turn your `isBin` into something that takes a list of file names that it filters, so that you can do: find . | isBin That `isBin` could look like this: isBin() { while read -r name; do if [[ -f "$name" &amp;&amp; -r "$name" &amp;&amp; $(file -b --mime-encoding "$name") = binary ]]; then printf "%s\n" "$name" fi done } 
 find . -type f -exec file --mime-encoding {} \; | sed -n '/binary/s/:[^:]*binary$//gp'
Thank you. You're clearly much quicker at this than I. Comments (but not aspersions) I still stick to my feeling that somehow `find -type` or `find -isfile` should have this built in. It already does some file inspection, and mime-type and other file attributes are *real too!* That said, I like/dislike 1 &amp; 2 above, liking as they are quick and done (thank you), and disliking because it's the sort of thing I think should be built in, and since it's not, what it means is that every unix box will either not have it, or have it done but differently. Now 3 on the other hand looks like something that belongs in a unix command as it much more follows the Unix tools philosophy. Name isfile - test file characteristics, prints name if successful Synopsis: isfile [--file args] pattern file ... Description isfile tests a file to see if the characteristics of that file as determined by file(1) match a given pattern. Options --file *args* are options to be passed to file(1) Pattern The pattern is a regular expression 
I certainly like the vast reduction of execs to file this yields.
&gt; It already does some file inspection, and mime-type and other file attributes are *real too*! But they aren’t! The MIME type of a file is *not* an attribute of the file. `file` just looks at the file content for a bit and then guesses what it looks like, and prints that. Nothing more. And this can be a [fragile](https://bugs.launchpad.net/ubuntu/+source/cupsys/+bug/255161/comments/28) and even [insecure](https://www.debian.org/security/2012/dsa-2422) process. Files can have extended attributes (see `attr(5)`), and I suppose it would be possible to store the MIME type there, but I’m not aware of any application doing that.
Ah. Yeah, you didn't explicitly state that recursivity was required, though I could have assumed it was as that's `find`'s default behaviour. My apologies. Easily resolved though. I've created some subdirectories and copied one of the test files through it: $ shopt -s globstar $ file --mime-encoding **/* | grep file3 file3: binary test/file3: binary test/test/test/file3: binary So to do that as a one-liner where the `shopt` doesn't stick, you'd subshell it like so: (shopt -s globstar; file --mime-encoding **/* | grep binary | awk '{print $1}') One thing I noticed while proofing this is that directories themselves are matched too, so if you wanted to not match them, you'd have to switch up the command a bit and filter it: (shopt -s globstar; file -F '' --mime **/* | grep binary | grep -v directory | awk '{print $1}') or similarly, borrowing /u/galaktos' `awk` example: (shopt -s globstar; file -F '' --mime **/* | grep -v directory | awk '$3 ~ " *binary" { print $1 }')
thanks! It works perfect. 
An interesting glob pattern I hadn't come across. But in trying it, it seems to miss out on dot files and dot directories?
For something that you don't feel is real, it sure enables this very conversation to take place. A few others as well.
I’m not going to discuss philosophy – but every test that `find` offers operates on the file metadata. No matter which expression you construct, `find` can test it without ever reading the actual file itself (except for symlinks, whose target is technically the file content). In this sense, `find` does not inspect the file itself, and the MIME type is not an attribute that `find` can check.
Yes, find is circa 1978.
This is probably better asked in /r/linuxadmin or /r/linuxquestions
 (shopt -s globstar dotglob; file -F '' --mime **/* | grep -v directory | awk '$3 ~ " *binary" { print $1 }') Next!
If you use fd 0, you should make sure to make a copy of it that you can restore later. exec 3&lt;&amp;0 0&lt;testfile read -r line ... exec 0&lt;&amp;3- # duplicate fd 3 to fd 0, and close fd 3 or you could just use a different file descriptor for the file exec 3&lt;testfile read -r -u3 line ... exec 3&lt;&amp;- # close fd 3
The best way to achieve that is not to use `exec` but to make a code block using curly braces `{ ... ; }` and add the redirection at the end of that block. For instance: some commands { read stuff some commands using stuff } 0&lt;testfile some more commands that dont read from testfile 
&gt; This requires the scripts to be well documented, easily maintainable and bulletproof. Well documented code, easily maintainable scripts and bulletproof programming: That's me :-) But, after one week of reflecting, I think "dev ops" in combination with Docker and Ansible would be a more realistic starting point for my career. Thank you anyway! I will keep everything that was written here in mind when it comes to concrete decisions.
Chances are high that I will go that way. Thank you :-)
Thank you! This works perfect!
Hi! My last comment in this thread for now. I will look for a new "dev ops" related forum and post further questions there. Thank you at all for giving me some new ideas. All comments have some good information, so I gave each comment an upvote to honor your contributions. Thank you and good bye, bashmandeluxe
Yes, and no. It's not possible to do this generically inside a single terminal. This isn't something that a terminal can do. There is no such place as "the top of the terminal" -- the terminal just displays lines of text linearly, and they scroll off the top as new output is created at the bottom. You could, however, use a terminal multiplexer like screen or tmux (or byobu, which is a pretty style overlaid on either of these) in any terminal app to split the view (horizontally or vertically; in your case horizontally) and display two terminals at the same time. Then you can have a todo list displayed all the time in the top terminal while you do other stuff in the bottom terminal. I don't know off-hand whether it's possible to resize terminals which are split like this (so that you could make the list take up less screen real estate than the other terminal), or if they're always split 50/50. However, in many terminal apps that let you have multiple tabs in the same window it is also possible to split the terminal window in the app itself, and I think most apps let you resize those terminals using the mouse. This is probably a more user-friendly option. Alternatively, you could look into tiling window managers, which let you arrange all kinds of windows seamlessly on your screen.
Thank you very much for your answer. I gonna try what you suggest. 
&gt; I don't know off-hand whether it's possible to resize terminals which are split like this (so that you could make the list take up less screen real estate than the other terminal), or if they're always split 50/50. In tmux it is, you can resize the pane with Ctrl + arrow keys after prefix (which is Ctrl+B by default). (Ctrl+" to split the pane horizontally.)
I suspect it's maybe do-able with some `tput` trickery combined with `$PROMPT_COMMAND` Googling that comes up with this, which might be a good starting point for you, and it mentions `screen` and `tmux` as pointed out by /u/confluence https://unix.stackexchange.com/questions/205181/split-shell-horizontally-to-show-ls-al-and-pwd
Look into tmuxinator. It allows you to define a set of windows and panes that can be launched with a command like mux todo.
Thanks, at the least, I learned about ** in glob patterns.
why don't you use a desktop notification ?
I liked the idea of this so I made a pointless script that I think does what you're after: https://gist.github.com/dom111/0a858821298e73b45df34cf9c3e7f2ca It'll append to your current PROMPT_COMMAND if you have one set. This will display content from ~/TODO in the top right corner of your terminal window (with each line truncated to a max of $(tput cols) / 3. In the other gist it grabs, there's an option for an ASCII style border or coloured box: https://gist.github.com/dom111/f18f95dfcdff16eae901f69bb49d2f32. Edit: I kinda ran with this: https://github.com/dom111/bash-todo Overkill? Almost certainly. But it was fun!
I played around and managed to get something working: https://gist.github.com/dom111/f18f95dfcdff16eae901f69bb49d2f32 https://gist.github.com/dom111/0a858821298e73b45df34cf9c3e7f2ca A fun exercise! Edit: I ended up making this more of a thing, thank you for your hints, I enjoyed making this! https://github.com/dom111/bash-todo
the OP probably doesn't use graphical environnement
OnTop() { tput sc ; tput cup 0 0 ; printf "%s" "$@" ; tput rc ; } Execution: OnTop My stuff must be at the top I can't remember atm, but cup 0 0 is either Y then X coordinates or vice-versa. You can use tput cols to get current number of columns and run wc -c on "$@" to place it center. This is for terminal emulator and not console. Use a subshell in bashrc to run it periodically in the background, catting your file, or whatever, but make sure to do a dirlock (see [example here](https://github.com/sigg3/filelocks)). Use this to grab environment: if [ -t 1 ] ; then echo "terminal" ; else echo "not a terminal"; fi Source: https://stackoverflow.com/questions/911168/how-to-detect-if-my-shell-script-is-running-through-a-pipe Edit: saw the other replies. Oh well.
&gt; I can't remember atm, but cup 0 0 is either Y then X coordinates or vice-versa. row #1 column #2, according to **terminfo**(5). (And for anyone else wondering, `sc` is *save cursor* and `rc` is *restore cursor*.) Also, you’re printing the arguments without any spaces in between, which is not ideal, but the `printf` isn’t the important part of the function anyways.
I made this based on a post by /u/supysupop the other day. I ended up making it fairly customisable and enjoyed doing so and thought I'd share in case it's something other people want to use! I've also customised a previous tool to help you generate the ANSI styles you need to change the colours which is available at: https://dom111.github.io/bash-colours/ Hope it's useful to someone!
Wow, it's cool, thanks! 
I'm a beginner but I think I can answer your first question. To rename a file use the mv command. 
Bash 4 has [pattern substitution](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html#Shell-Parameter-Expansion). z$ a="stoob your boots" ; printf %s\\n "${a/oo/u}" stub your boots z$ a="stoob your boots" ; printf %s\\n "${a//oo/u}" stub your buts z$ This suggests a possible solution, assuming you want to rename files: # don't do this unless you really want files moved! for i in *oo* ; do mv -i "$i" "${i//oo/u}" ; done
*NameError: name 'new_link' is not defined* The variable does not exist, put some debug prints around to see if that if-block above returns some value.
Hmm. I made a notificator script that does the same sort of thing but as a message box and it's capable of sending the notificators data stuff to your phone. https://pastebin.com/9AuQDjtc It may or may not help you develop your script more.
Something like this (completely untested) (updated) #!/bin/bash batch=100 count=1 src_dir=/path/to/files/to/zip dst_dir=/path/to/directory/for_zips cur_count=0 files= group=1 for file in "$src_dir/"*; do if [[ "$cur_count" -ge "$batch" ]]; then zip_file=$(printf "%s/fileName_%04g_%04g.zip" "$group" "$dst_dir" "$count") count=$((count + 1)) group=$((group + batch)) zip -r "$zip_file" $files cur_count=0 files= fi files="$files $file" cur_count=$((cur_count + 1)) done
Because line 44 and 45 let you continue without that variable existing. Use Pycharm IDE and you never make this kind of mistake again. Cross platform and free.
Would it be possible to elaborate on what you're trying to accomplish? An example of what you already have and want it changed to? There are a variety of ways so this would be some helpful information 
I'm not sure exactly what you're trying to do, but interpreting that as "move the text of the file to be the file's name itself" (that is, overriding the file's original name and leaving the file empty), $ thetext=$(cat foo.txt) $ mv foo.txt $thetext $ cp /dev/null $thetext , and if you want to iterate this, do this in a for loop over a set of files. 
Frankly I'm confused why you're even converting bash into C and into an executable using that program. Bash and shell scripting is usually just ran from the shell, as it's inherently dynamic. Also shale scripting and GUI doesn't really mix. FYI libraries have to be written for the language. Bash, well doesn't have any that I am aware. You're better off using a different language all together. First we should know what you're attempting to create before suggesting a plethora of more suitable languages for the task and GUI libraries for those languages.
I'm creating a simple bash script for batch-converting images to pdf, basically learning how to do bash scripts. But now I wanted to see whether the whole process can be done through a GUI, instead of the CLI. So that people don't see the terminal (some are afraid of terminals) and still can use the script through the GUI. I'm in Ubuntu 16.04.2 LTS, but would also want the script (and GUI) to work in OS X (the script obviously works in OS X). And if I do succeed I want to hide the code (this one is a known code, so just for future reference). 
Well that sounds like exactly the kinda of thing a scripting language is for However, not a GUI. It's in the name. It's a shell language. Intended to be use within a shell / terminal. However... if you're set on using a GUI, not executing on terminal, etc.. You're likely using imagemagiks convert command, or something similar. Other than using it's built in batch like convert, you're probably looping over and calling it per file. As for the most part, you bash script is probably just a for-each over inputs, calling an underlying program, this can be done in many languages. I suggest python. It's simple enough to pick up and is usually preinstalled on both mac and Ubuntu. Though I would suggest python 3, python 2 is still the default, so either make it comparable with both or just focus on python 2 as you want it to work. There are many GUI libraries that interface with python. Many tutorials out there too. There are tools that will wrap a python script and libraries and make it into a static binary... thus not needing to touch a terminal. Now you can still call the same underlying program using the subprocess module ... or you can use one of the many image to pdf libraries for python; their documentation will tell you how to use it. 
Thanks, this really helps. Yes, I'm just using "convert" of "Imagemagick". 
I figured it out wew | while read -r * ; do Main; done
can also use `rename 's/find/replace/g' *.txt` to bulk rename all text files in folder easily without looping
I expected a Bash solution was desired, not perl.
yah.. u r right
Sir I don't want to leave it blank. Just imagine that in a folder and there are files in it with name say "-reddit answers". Or anything. I just want to remove "-reddit answers" from all those files of that folder. Do tell me how to queue the files too. Thank you sir.
What if I'm connected to a VPN?
 Various dialog toolkits can be used from bash to present a gui interface. * [whiptail](https://en.wikibooks.org/wiki/Bash_Shell_Scripting/Whiptail) * [zenity](https://help.gnome.org/users/zenity/stable/) * [dialog](http://pwet.fr/man/linux/commandes/dialog) * [kdialog](https://www.howtoforge.com/tutorial/adding-a-simple-gui-to-shell-scripts-with-kdialog/) Also checkout tcl/tk and python TkInter. * [Tkinter](https://wiki.python.org/moin/TkInter) 
I'm not sure what you mean by 'queue the files', but $ rename 's/-reddit answers//' * would remove the '-reddit answers' from filenames. 
I second tcl/tk, and suggest calling your script from tcl, which will also hold script for your UI. Creating Tcl/tk GUI is straightforward if you already know bash scripting. I've used it in the past for rapid development system for GUI mock-ups. As you are targeting mac, you may want to check out, [Apple Macintosh and Tcl/Tk](https://wiki.tcl.tk/1013). Good luck! 
It depends. I'm connected to a VPN right now, but I have it setup as a split VPN - so traffic intended for work goes over the VPN, and traffic not intended for work goes over my normal internet connection. Work doesn't need to know about all the porn\^U linux iso's that I'm torrenting. Either way, there's nothing stopping you from just giving the function an argument e.g. $ weather Wellington Weather report: Wellington, New Zealand \ / Partly cloudy _ /"".-. 6-9 °C \_( ). ↖ 17 km/h /(___(__) 10 km 0.1 mm 
Thank you! That makes sense. 
Queue as in say there 10 files and I want to remove "-reddit answers" from them. How do i? Like select all of those and drag tovthe teeminal while running the script. Can I do that? 
The above will process that for all files in the current folder. 
Sorry, I was not able to get your script working, its a little above my head.. But, I took some clues from your script and finetuned mine... sFrame=0001 eFrame=0100 batch=10 fname=file for (( i=sFrame; i&lt;=eFrame-batch+1; i+=batch)) do batchStart=$( printf "%04g" "$i" ) batchEnd=$( printf "%04g" $(($i+$batch-1)) ) zipFile=${fname}_${batchStart}_${batchEnd}.zip files="$fname.{$batchStart..$batchEnd}.txt" echo zip -r $zipFile $files # zip -r $zipFile $files done Now am able to print the exact command I want to console But, if I try to run it directly it complains saying file.{0001.0010}.txt not found. Basically, I need to bash to expand it instead of treating it as a file now. Can someone guide me how to do that. 
[Ncurses](https://en.wikipedia.org/wiki/Ncurses)? *PS:* A NCurses application looks *really* good in your CV. Recruiters are way impressed with that shit.
**Ncurses** ncurses (new curses) is a programming library providing an application programming interface (API) that allows the programmer to write text-based user interfaces in a terminal-independent manner. It is a toolkit for developing "GUI-like" application software that runs under a terminal emulator. It also optimizes screen changes, in order to reduce the latency experienced when using remote shells. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
That ... if that works, it's by accident.
So I can just drag and drop the entire folder in the terminal while running this sir? 
I'm not sure what terminal you have that allows drag-and-drop level UI interaction. I'm pretty sure that doesn't exist. 
Yeah, maybe: wew | while read line ; do MAIN done
First, thanks for the thorough response. Second, I think there's an extra `"` in your first code block. I believe the command should look like: # Faux-tree command tree() { find "${1:-.} -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'" } One point: at least on my test machine, I needed--from my home directory--to run `source .bash_profile` for the command to take effect. Again, thanks for the thorough response. Your thoughtfulness is much appreciated. 
Is there a way to set the depth of recursion as a positional parameter as well? For example, if I only wanted to get a listing of directories two levels deep the `find` command accepts `maxdepth 2`. Can I make it so that the user can define this depth in the same way they can point to a directory in your example? 
That might be True, but that's a TUI not a GUI. OP wants to try and remove the terminal from the equation if possible.
That's what I meant, I was tired while typing it.
I used dialog for a pure CLI bash menu. Worked well.
Can I wrap my .c or .x.c or .sh in Ncurses?
I have also use Tcl/Tk and it's pretty good, but it does not have a wide community
Another good catch. It should look as it does now that I've edited it. Whenever you change a dotfile you will have to source it. And you should really use `.bashrc` instead, and ensure that `.bash_profile` has the line `. ~/.bashrc` in it ([some relevant reading](https://shreevatsa.wordpress.com/2008/03/30/zshbash-startup-files-loading-order-bashrc-zshrc-etc/)) Adding a positional parameter isn't any great deal, you could do something like this: cat &lt;&lt; "HEREDOC" # Faux-tree command # Usage: tree path depth # Example: tree /tmp 2 if ! command -v tree &gt;/dev/null 2&gt;&amp;1; then tree(){ [ -n "$2" ] &amp;&amp; local depth="-maxdepth $2" find "${1:-.}" ${depth} -print 2&gt;/dev/null | sed -e 's;[^/]*/;|____;g;s;____|; |;g' } fi HEREDOC So what this does is checks whether or not the positional parameter `$2` is set. If so, then it sets up the `find` command to use `-maxdepth` (and we completely ignore that `-maxdepth` is not portable), if `$2` isn't set, then the `$depth` variable is blank so `find` sees nothing there and moves on. We make sure that `$depth` is a variable that is local to the function, otherwise it will go global and affect subsequent runs of the command e.g. tree /tmp 2 # Works fine tree /tmp # Because $depth is global, the function will behave as per 'tree /tmp 2' By making it local to the function, we get the correct desired behaviour. The one downside is that for `$2` to be recognised and used, you will have to be explicit with `$1`. Any further customisation and I'd recommend shifting to `getopts`, at which point you really want to start emulating the options of the actual `tree` command. Personally I wouldn't write it as I've shown above, I'd be using `getopts`, but that's me.
Something like this should get you started: https://www.lifewire.com/pass-arguments-to-bash-script-2200571 In short, set your filename using `filename="$1"`
How do I do this? My first two lines are &lt;echo -n "Enter the filename: "&gt; and &lt;read filename&gt;. I saw the loop they used &lt;for FILE1 in "$@"; do wc $FILE1; done&gt;. I'm not sure where to add.
$1 returns the first argument to the command, which would be &lt;filename&gt; in your case
Thank you very much; I just got it to work.
If you pass multiple arguments, you just increment the number $1 $2 $3, etc.
Now spread your wings and fly! A close friend told me this the first time I installed gentoo and KDE in 2001 and I haven't looked back! I mean I switched distros and environments, but you know what I mean. 
Good points. Have you seen elvish? https://github.com/elves/elvish
Yes, there are several other projects. Here is the comparison: https://github.com/ilyash/ngs#have-you-heard-of-project-x-how-it-compares-to-ngs
Elvish: curl https://api.github.com/repos/elves/elvish/issues | from-json | each explode | each [issue]{ echo $issue[number]: $issue[title] } | head -n 11 NGS: ``curl https://api.github.com/repos/elves/elvish/issues``.limit(11).each(F(issue) echo("${issue.number}: ${issue.title}")) 
Yeah! I'm flying too!
Looks interesting. Is it in the AUR? `next-generation-shell` isn't taken, and `ngs` seems to be something else.
To convince me? Talk to me about migrating to NGS while I'm neck deep in my current workflow and don't have time to devote to learning the new shiny. I've been piping sed, awk, grep, perl, python etc for longer than I care to admit, and frankly, I know it's bad, but how do I get over the migration pain, and why is NGS the path I should take? Tell me that while I am starting to learn NGS that all my existing bash behaviours will work in NGS. If they don't, give me tips on how I still can include it. 
Next Generation Shell is not on AUR. You are welcome to [open GitHub issue](https://github.com/ilyash/ngs/issues) about packaging. If you want to try NGS you can use docker for example, there is Dockerfile. You can also compile it, there are instrucrions in readme. If you have problems trying it, feel free do DM me or open GitHub issue(s). 
&gt; Talk to me about migrating to NGS while I'm neck deep in my current workflow and don't have time to devote to learning the new shiny. I see what you mean, I was in similar situation once and it's awful. It is essential to push back and/or ask for help (not sure it's possible in your situation). Even if you make some time, there probably will be some items before NGS to learn/implement to make your life easier. If you will not free some of your time to invest in yourself and in improving your workflow, your situation will be [summarized by this picture](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAK-AAAAJDRlNzZjODgxLWQ5ZWQtNGNjZS05Mjc3LWU1ZjkxNzgxODJjYQ.jpg), unrelated to NGS. I wish you improvement in your schedule. &gt; Tell me that while I am starting to learn NGS that all my existing bash behaviours will work in NGS. No. NGS is not compatible and implements very small subset of bash syntax. I really could not figure out how to make NGS what it is and be compatible with bash. When you have the time, you might want to look at [oil shell](http://www.oilshell.org/) instead. &gt; If they don't, give me tips on how I still can include it. I guess like any other new script that you write that does not "source" other scripts: you call it from existing scripts or it calls existing scripts. There is probably more to this integration, feel free to PM me.
Based on some other comments here, you can use zenity for Ubuntu at least. I have used it to create "GUI" programs (that are actually shell scripts started from a desktop shortcut .desktop file or from cron). man zenity Should provide hours of fun:) If you want to create a stand-alone application, I'd suggest checking out lazarus and freepascal or python and gtk/qt.
This doesn't seem to do what I expect it to do. If I run: `tree /some/directory 2`, I get the same results as if I run: `tree /some/directory`. Also, could you please clarify what you mean when you say: "we completely ignore that -maxdepth is not portable" and: "The one downside is that for $2 to be recognised and used, you will have to be explicit with $1" ? I suspect that I've misunderstood something in your explanation that would allow this command to work as you've described. Also, I should mention that I'm running `Bash` via OS X, if that makes a difference. 
You think sed and awk are bad? Try explaining to your monkey-work internt how to properly use NGS or whatever the new shiny is without breaking everything, while also trying to save time on your own work.
Both are much better than bash. I'm sold
Go above line 50 and put 'import pdb; pdb.set_trace()' then poke around.
`while true; do curl ...; sleep ...; done`
Something like [this?](https://github.com/Avicennasis/downornot) 
The other option is just cron. * 0-6,20-23 * * * script.sh
Sorry to just ask the same stupid question, but it’s not on that page – have you seen [dgsh](https://www.dmst.aueb.gr/dds/sw/dgsh/)?
It sounds like you are caught up in the syntax beauty for its own sake. And I agree its a fun problem. But you should focus on making the language more powerful than any other language. So take the best things of all the languages, the real power, and put them all in (and make THAT syntactically beautiful). Make a tool that is just too powerful not to use. Pipes are powerful. So take pipes, and make them even more powerful, more feature-full. Eliminate the wierd hacks people have tried to use the increase their power and make it pretty. Examples: * one-to-many one output to many consumers * many-to-many multiple output streams to multiple consumers * Per-pipe dissection/interpretation/alarms. Per-pipe IFS Sockets are powerful, so make them part of the language. XML is powerful, so make xml dissection/production part of the language. Both to import input, but also to save state. GUI is powerful, so put hooks in the language so that it can be used as interpreter for a GUI front-end. The GUI front-end forks the language and provides it input. The output of the language would be interpreted by the GUI wrapper to change its state. Temp-files are powerful (esp /dev/shm) so make it part of the language rather than forcing an outside provider. Make a language so freaking powerful, that the "extra syntax" that bash might have isnt needed. But make a power list....the things that drive data from point A to point B. The speeds and feeds that your language will cater to and do better than any other language ever.
&gt; Sorry to just ask the same stupid question I really don't understand the thinking process that led you to conclusion that your question is stupid. How do you decide? (You don't have to answer here but I recommend that you answer yourself). &gt; have you seen dgsh? No, I haven't. I have added it to my todo list. I do agree with the basic idea that pipes don't have to be used only in A -&gt; B configuration, as we used to. dgsh, as several other shells, appears to be very similar to bash while NGS tries to get more general purpose languages' features while keeping focused on systems engineering tasks.
&gt; &gt; Sorry to just ask the same stupid question &gt; &gt; I really don't understand the thinking process that led you to conclusion that your question is stupid. How do you decide? (You don't have to answer here but I recommend that you answer yourself). Jeez – I just thought you might be a bit annoyed at a question you probably get a lot, and so I thought I’d joke about it a bit. You don’t have to go all “know thyself” on me :D &gt; dgsh … appears to be very similar to bash Yeah, technically it’s actually a fork of Bash (plus a fork of coreutils), so it’s the exact same syntax except for the added bits. Pro: compatibility (allows gradual enhancement of scripts); con: all the problems with Bash syntax :)
What I would like to see is a compile-to-sh language which fixes most of (ba)sh's flaws.
&gt; It sounds like you are caught up in the syntax beauty for its own sake I hope it's wrong impression and not what's really happening because my intention behind syntax was to make common operations to be clear and concise. &gt; But you should focus on making the language more powerful than any other language. I more look at it as to be more (hopefully significantly more) convenient than any other language to do the common systems engineering tasks. There are other very powerful languages and I don't think I'm competing or should compete with them. The only niche NGS is focused on is systems engineering tasks. That will not preclude NGS being used for other things but that usage might not have advantages. &gt; Sockets are powerful, so make them part of the language. You mean TCP/UDP? &gt; XML is powerful, so make xml dissection/production part of the language. Both to import input, but also to save state. XML should be supported but I think the main format will be JSON as it's simpler (it's already supported). Thanks for the insights, I will be thinking about the points you mentioned. It's not a 5 minute process to see how they fit and how they should be implemented :) 
&gt; a question you probably get a lot Hence the comparisons section in readme :) &gt; You don’t have to go all “know thyself” on me :D Sure! Text does not convey non-verbal communication so I took seriously what you wrote and was trying to help. &gt; all the problems with Bash syntax :) That I'm trying to avoid :)
&gt; compile-to-sh language There is a chance someone is already working on that. Why is it important for that language to be compile-to-sh? 
Change cron's shebang? By default, it's `sh`. Also, there really should be no case where `$HOME` can't be replaced by `~username`...!
Hmm Yep, $HOME works. Wonder why the environmental for HOME works but not USER: RUN AS CRON -&gt; echo "$USER, $HOME" &gt;&gt; /tmp/logfile cat /tmp/logfile , /home/user
On FreeBSD, under their /bin/sh, which dates from System 1, rewritten for System 7 Unix, and then rewritten in 1989, $USER is a registered environment variable. Under OpenBSD, which uses pdksh as their sh, $USER is defined to the current username. Mac OS X uses bash for /bin/sh and also defines $USER like its BSD brethren. I'd say it's a safe assumption that most Unix installs define $USER. Should you want to cover all cases, though, you should stick to the `~`.
thanks this worked, now that I have it I wanted to create a GUI for the simple app, with a few buttons to change sleep time, URL, etc. What's the best program? I use MAC. I'm having trouble coming across anything decent. I've seen Zenity and Glade, couldn't even get glade to install an actual program on my machine though.
Can you run the following: `set -x`, then run both of those commands, followed by `set +x` and then copy and paste the output here (feel free to sanitise it)? Here's a test run of it, showing the differences in some dummy file structure: $ typeset -f tree tree () { [ -n "$2" ] &amp;&amp; local depth="-maxdepth $2"; find "${1:-.}" ${depth} -print 2&gt; /dev/null | sed -e 's;[^/]*/;|____;g;s;____|; |;g' } $ tree /tmp |____tmp | |____test1 | | |____test2 | | | |____test3 | | | | |____test4 | | | |____file11 | | | |____file12 | | | |____file13 | | | |____file14 | | | |____file15 | | | |____file16 | | | |____file17 | | | |____file18 | | | |____file19 | | |____file01 | | |____file02 | | |____file03 | | |____file04 | | |____file05 | | |____file06 | | |____file07 | | |____file08 | | |____file09 $ tree /tmp 2 |____tmp | |____test1 | | |____test2 | | |____file01 | | |____file02 | | |____file03 | | |____file04 | | |____file05 | | |____file06 | | |____file07 | | |____file08 | | |____file09 &gt;Also, could you please clarify what you mean when you say: &gt;&gt;"we completely ignore that -maxdepth is not portable" Sure. Various commands have different options on different versions of *nix. So for example, if you run `find . -maxdepth 3` on, say, Solaris, you'll be in for a world of disappointment, because the version of `find` on Solaris doesn't support `-maxdepth`. If you're writing code that might run on Linux, OSX, Solaris, FreeBSD, OpenBSD, NetBSD, whatever-else-BSD, HP-UX, AIX and so on, or any combination thereof, then you need to think about coding portably, and that's where POSIX comes in. The answer to this problem in POSIX might be something along the lines of [this discussion.](https://unix.stackexchange.com/questions/275637/limit-posix-find-to-specific-depth) If your code will only ever be used with a version of `find` that can be guaranteed to support `-maxdepth`, then this isn't much of an issue. It is something to be aware of though, in a broader sense. &gt;and: &gt;&gt;"The one downside is that for $2 to be recognised and used, you will have to be explicit with $1" ? `$1` and `$2` are positional parameters, and so they represent the positions in which they're stated. For example `tree $1 $2 $3 .. $n` So if you wanted to run `tree` with a maxdepth, you have to explicitly state where you want that to happen. You can't just go `tree 5`, because `5` will be treated by `tree` as its `$1`. Right? So you need to tell `tree` where to look if you want to tell it how many levels to look in. With `getopts` you could just do something like `tree -M 2` and the current directory would be assumed.
u can use pyqt5 
maybe `$LOGNAME` works, or use `id` instead.
You might like to setup something based on [FPM](https://github.com/jordansissel/fpm). I have a project at work that produces packages automatically based off it...
&gt; Talk to me about migrating to NGS while I'm neck deep in my current workflow and don't have time to devote to learning the new shiny. I've been piping sed, awk, grep, perl, python etc for longer than I care to admit, and frankly, I know it's bad, but how do I get over the migration pain, and why is NGS the path I should take? Man... you sound like me a few years ago. Invest a couple of spare hours here and there into something, anything that can abstract your workflow. For me it was Ansible - I got on that train early because I saw value in having one single way to manage local accounts across both Linux and Solaris. Even though I have rock solid `expect` scripts and `bash` wrappers to do the same thing. Start out with something small like that, and then think about what pisses you off the most while also being fundamentally a simple task... then figure out how to automate that with your new shiny. Just pick away at it... it might take a few extra invested hours here and there, but ultimately it pays off. Just between us, my team could easily be halved thanks to my work with Ansible. Ansible is usually an easy sell because usually you'll have one or two hosts from which you can `ssh` into everything (my client has a multi-zone network configuration, but I can piggyback my way to everything via a correct ssh config), and so the pitch is this: if you can ssh to it, you can ansible it.
Used it once but wasn't thinking about it for this project. I'll reconsider. Thanks!
[Bash internal variables](http://tldp.org/LDP/abs/html/internalvariables.html)
Really depends on a number of things, but here's a shot -- I think you could clean up the logic for your date comparisons a lot by literally comparing the last modified time: #!/bin/bash sudo mv /home/thisUser/log.txt{,.old} exec 1&gt;/home/thisUser/log.txt 2&gt;&amp;1 previous_date="$( cat /home/thisUser/previous )" sudo mount -U "1234-5678" /mnt/harddrive for file in $( find /media/Standard -type f -name \*.ts ); do last_modified="$( stat --format=%Y "${file}" )" if [[ "${last_modified}" -gt "${previous_date}" ]]; then echo "[INFO] File ${file} modified after previous run, copying..." cp "${file}" /mnt/harddrive/yetToName/ fi done date +%s &gt; /home/thisUser/previous sudo umount /mnt/harddrive mail --attach=/home/thisUser/log.txt -s "Finished copying" thisUser.something@gmail.com &lt;&lt;&lt; "Logfile as attachment" If you don't care about storing the past date, you could make this even slimmer: #!/bin/bash sudo mv /home/thisUser/log.txt{,.old} exec 1&gt;/home/thisUser/log.txt 2&gt;&amp;1 sudo mount -U "1234-5678" /mnt/harddrive find /media/Standard -type f -name \*.ts -anewer /home/thisUser/previous -print0 | xargs --null -I {} cp {} /mnt/harddrive/yetToName/ touch /home/thisUser/previous sudo umount /mnt/harddrive mail --attach=/home/thisUser/log.txt -s "Finished copying" thisUser.something@gmail.com &lt;&lt;&lt; "Logfile as attachment" Or, you could still try to grab out the date from the file name: filename="$( basename "${file}" '.ts' )" if [[ "${filename}" =~ ^.*([0-9]{8}).*$ ]]; then name_date="${BASH_REMATCH[1]}" file_date="$( date +%s --date="${name_date}" )" if [[ "${file_date}" -gt "${previous_date}" ]]; then echo "[INFO] File ${file} named older than previous run, copying..." cp "${file}" /mnt/harddrive/yetToName/ fi else echo "[INFO] Could not find date in name for ${file}, skipping..." fi The obvious downside to the latter approach is you have resolution to the day, so you could miss records with such an approach if you run the script at 10AM and new files are written at 10PM, but the script thinks that it's already copied all files from that day. You could get around this by looking one day back: file_date="$( date +%s --date="${name_date} -1 day" )" Or test for the presence of the file in the target directory. 
I have changed my udev rule based on [this post](https://www.reddit.com/r/bash/comments/2sx3h2/why_is_this_udev_rule_running_twice/) to: ACTION=="add", ENV{DEVTYPE}=="disk", SUBSYSTEM=="block", ATTRS{serial}=="20130202035870", RUN+="/home/matthias/automaticCopy.sh" [My enhanced script](https://pastebin.com/YKUZN99a) is working just fine when executed from the command line, but it crashes after a few files, when it executes automatically. Does someone have a clue? I can ask in a different subreddit, if this is the wrong place, since the script is working. Do I change it to [solved] now or only when everything works? All the problems of the original post are resolved. Maybe I will add a check for an empty temporary directory.
Wow, your script is way shorter than mine! I'm not sure, if I'm going to use it as my standard, though. My new script is working great when executed manually, but I will try it out. I need to learn more about bash to understand everything you wrote and until then I'm just a little bit careful.
What do you mean when you say it crashes? What's the failure mode?
The LED on the external drive stops blinking (no copying anymore) and my logfile looks like [this](https://pastebin.com/GaNM78z8). When I run "df -h", I get "f: /mnt/harddrive: Transport endpoint is not connected". The other drives look fine underneath.
[Here](https://pastebin.com/nsgLSGvT) is the syslog part, where it crashed. (/var/log/syslog)
You've got an error with globbing but I agree, the mount issue looks like something other than a script thing.
Yes, the error exists because there wasn't a recording on this date, so there is no existing file with that pattern. I didn't have any issues with this. Could udev be the problem? EDIT: Wrong Grammar
Hmm, sorry -- I'm not super familiar with this stuff, so I am not really sure what is wrong in this context/
I think [this page](https://medium.com/wemake-services/testing-bash-applications-85512e7fe2de) is what you meant to link.
Removed – can you please resubmit this with the direct link instead of the HN discussion?
yeap, sorry, wrong link.
That's a great post! Going to try out Bats on a Bash tool I wrote 
Move the line `echo $sum` after `done`.
You can spare the `bash` loop by using `awk` e.g. $ numArray=( 1 2 3 4 5 ) $ printf '%d\n' "${numArray[@]}" 1 2 3 4 5 $ printf '%d\n' "${numArray[@]}" | awk '{sum+=$1};END{print sum}' 15 There would also be a way to do it with a single `awk` command, I'll leave that to someone else. 
You already have the answer for the question, but just a note on your code: Use `sum=$((sum + i))` or `(( sum += i ))` to do math. Never use the `$[ ]` syntax, which was deprecated over 20 years ago. E.g. sum=0 array=(1 2 3 4 5) for num in "${array[@]}; do (( sum += num )) done printf '%d\n' "$sum" 
- [BashGuide](http://mywiki.wooledge.org/BashGuide), [BashFAQ](http://mywiki.wooledge.org/BashFAQ), [BashPitfalls](http://mywiki.wooledge.org/BashPitfalls), and [BashSheet Reference](http://mywiki.wooledge.org/BashSheet) at [GreyCat's wiki](http://mywiki.wooledge.org/) - [The Bash Guide](http://guide.bash.academy/) - [Shell Scripting Tutorial](https://www.shellscript.sh/) - [Bourne Shell Tutorial](http://grymoire.com/Unix/Sh.html) - [Bash Shell Scripting Wikibook](https://en.wikibooks.org/wiki/Bash_Shell_Scripting) - [bash-handbook repository](https://github.com/denysdovhan/bash-handbook) - [GNU Bash Reference Manual](https://www.gnu.org/software/bash/manual/html_node/index.html) - [POSIX 1003.1-2008 Shell &amp; Utilities](http://pubs.opengroup.org/onlinepubs/9699919799/idx/xcu.html) 
 array=(1 2 3 4 5) ( IFS=+; echo "${array[*]}" ) | bc
I'm guessing there's simply no links pointing to that file you are searching for? The error message is normal. It's printed to the "STDERR" output and you can hide it by adding a `2&gt; /dev/null` after the find command and in front of the pipe. Instead of hiding the error message, you can also give 'find' permission to look everywhere by running it through `sudo`. About your idea, it kind of works, but it will not print the link's name. It will just print the target's name, so probably not what you want? You will know how many links are pointing to some file, but not where the links are coming from. For example, if I search for stuff pointing to `systemctl` here, I get this result: $ find / -xdev -type l -exec readlink -f {} ';' 2&gt; /dev/null | grep '/usr/bin/systemctl' /usr/bin/systemctl /usr/bin/systemctl /usr/bin/systemctl /usr/bin/systemctl /usr/bin/systemctl /usr/bin/systemctl This here might be more what you are after: $ sudo find / -xdev -type l | while read -d $'\n' -r name; do echo "$name // $(readlink -f "$name")"; done | grep '// /usr/bin/systemctl$' /usr/bin/telinit // /usr/bin/systemctl /usr/bin/halt // /usr/bin/systemctl /usr/bin/poweroff // /usr/bin/systemctl /usr/bin/shutdown // /usr/bin/systemctl /usr/bin/reboot // /usr/bin/systemctl /usr/bin/runlevel // /usr/bin/systemctl Written over several lines, that command line looks like this: find / -xdev -type l 2&gt; /dev/null | while read -d $'\n' -r name; do echo "$name // $(readlink -f "$name")" done | grep '// /usr/bin/systemctl$' I used that `-xdev` parameter to make 'find' stay inside the root filesystem and not go look through a slow HDD I have mounted somewhere. I made that 'echo' line use `" // "` as a separator between the link's and target's name because `/` is the only character that's not allowed in Unix file names. That should mean that searching for `//` with grep only ever triggers on that separator and never on a part of a file name.
I can really recommend the [Bash Hackers Wiki](http://wiki.bash-hackers.org).
Here's a way without readlink and grep: find / -xdev -type l -exec bash -c 'for f; do [[ $f -ef /usr/bin/systemctl ]] &amp;&amp; ls -ld "$f"; done' _ {} + Using `ls -ld` for human readable output there. If you want to parse it further, you change it to `printf "%s\0" "$f"`.
Notice the *absence* of the "Advanced Bash-Scripting Guide", which despite its excellent Google juice is not promoting best practices and/or is quite outdated.
 root # cat /usr/local/bin/app #!/bin/sh ~/path/to/thing "$@" Or, you could use links: root # ln -s ~/path/to/thing /usr/local/bin/foo root # foo a b c
Definitely check out Greg's Wiki. If you have questions, feel free to ask them in #bash@FreeNode.
[Advanced Bash-Scripting Guide](http://tldp.org/LDP/abs/html/)
Honestly, I've never read that one. I know it has been around the longest and many swear by it though.
It's a subtle difference, but adding a semi-colon (`;`) to the end of this line: `[ -n "$2" ] &amp;&amp; local depth="-maxdepth $2";` and resetting the profile has the tree command working as expected. THIS IS AWESOME! I greatly appreciate the thoughtful, timely explanations you've offered to this question. I've learned quite a bit. 
He wants a good resource, not garbage.
What a shit resource. Turned to a random section and immediately found incorrect information. &gt; case=value0 # Causes problems. No it fucking doesn't. You can totally have a variable with the same name as a keyword. `while=case; echo $while` totally works. &gt; As of version 3 of Bash, periods are not allowed within function names. &gt; &gt; `function.whatever () # Error` Yes they fucking are. `f.foo() { echo hello; }; f.foo` totally works. *Edit: For posterity's sake, I'll mention this was tested under Bash 4.4.12(1)-release*
I love your project, and I feel that you are attacking this like a youngster: by repeating all the mistakes in a fancy, new way. Don't be discouraged. I just don't think you have justified "NGS" as much as come up with reasons for you to do your own thing. If you ask me, you don't need to convince me (I am not convinced) and you don't need excuses to go ahead. Just go ahead and do it, the proof is in the pudding. As for myself, I write careful bash scripts with proper error handling, and it gets the job done. I love the eccentricities of bash, and I feel like I should learn assembly and perl. Bash is just so damn accessible. I'm not paid to learn languages, but to get the job done, awkwardly if need be ;)
I'd prepends the parameters with dashes and then you can use getopts to parse these
I'd do: # Transform the line: print A=B; C=D; on stdout and A=""\nC=""\n on stderr. # We can source these files for 1. setting values for the current host 2. reset everything __transform () { while [[ -n "$2" ]]; do # "$1" is a property field and "$2" is its value printf '%s=%s; ' "$1" "$2" printf '%s=""\n' "$1" &gt;&amp;2 shift 2 done } while IFS= read -r _line; do __transform "${_line[@]}" &gt; /tmp/current_host 2&gt;&gt;/tmp/reset_properties sort -u /tmp/reset_properties &gt; /tmp/reset_properties.uniq mv /tmp/reset_properties.uniq /tmp/reset_properties source /tmp/reset_properties source /tmp/current_host echo "$host" echo "$serial" done &lt; config.file
So there's probably a prettier way of accomplishing this, but this appears to work. Here's the test file /tmp]$ cat test property 03 host goofy serial 069a baud 19200 serial 014a baud 38400 property 09 host snuffleupagus host randy error-rate none property 11 baud 2400 feel-bad-factor-for-slow-baud none And here's the script #!/bin/bash while read -r line do host=`echo $line | awk '{for(i=1;i&lt;=NF;i++) if ($i=="host") print $(i+1)}'` property=`echo $line | awk '{for(i=1;i&lt;=NF;i++) if ($i=="property") print $(i+1)}'` echo $host echo $property done &lt; /tmp/test And finally, the output /tmp]$ ./test.sh goofy 03 snuffleupagus 09 randy 11
This would be a pure Bash solution, which take any order of key-value pairs. I'm using a shortcut here assuming property values will never look like keys. For example any host that is named "property" will require more than this. If that would be a problem let me know :) #!/bin/env bash # The "|| ..." make so that if the final line does not end # with a newline (\n), it is still processed. while read -r line || [ -n "$line" ]; do # Now we could split the line on space and look for "property" and "host", # that would be safer because it would guard against values looking like keys. # But I'll just be lazy and spend some cycles on a regex. property= host= if [[ $line =~ (^|[\ ])property[\ ]([^\ ]+) ]]; then property="${BASH_REMATCH[2]}" fi if [[ $line =~ (^|[\ ])host[\ ]([^\ ]+) ]]; then host="${BASH_REMATCH[2]}" fi if [ -n "${property}" ] || [ -n "${host}" ]; then printf "property=%s\nhost=%s\n" "${property}" "${host}" fi done &lt; "data.txt" 
You're missing a big deal of things. Mostly reset (all) values from one host to another, as well as various properties (e.g. `feel-bad-factor-for-slow-baud`)
In #bash on freenode, it's mostly "swear *about* it" ;) Greybot on there even has a command for it: `!abs` returns &gt; The infamous "Advanced" Bash Scripting Guide should be avoided unless you know how to filter out the junk. It will teach you to write bugs, not scripts. In that light, the BashGuide was written: http://mywiki.wooledge.org/BashGuide
Well, the question is referring to "host" and "property", and so is my answer, for any other key it is easy to see how to do that. The variables are reset between each line read, so I'm not sure what you are referring to there. Finally, my reply is to give a hand and to point in a right direction, just as asked. 
Using bash arrays, plus sed &amp; awk: parsefile() { declare -a host=($(cat /tmp/filename.txt | \ sed -e 's;.*host \([^ ]\);\1;g' | awk '{print $1}')) declare -a property=($(cat /tmp/filename.txt | \ sed -e 's;.*property \([^ ]\);\1;g' | awk '{print $1}')) echo ${host[*]} echo ${property[*]} } 
This sort of thing tends to be easier in a non-bash language. For instance, in python you could do something like this (typing on my phone, so untested): entries = [] for line in open('file.txt', 'r'): data = iter(line.split()) entry = {} for key, value in zip(data, data): entry[key] = value entries.append(entry) Then you've got a nice list of key-value objects that you can iterate over and do what you like with.
No offense, but did you actually run this? Because it is not working as you expect it too. The `__transform` function never gets the two arguments as expected. Since it is using quotes around the argument when calling the function the whole line is provided as one argument (`__transform "${_line[@]}"`). Also that `@` is of no use there, since the variable is a string, not an array. You might have meant to read _line as an array? Then it will make sense using quotes and `@`. I do however like the `shift 2` solution for handling key-value pairs. Also this is a very complex solution for a relatively regular problem. This solution expects the use of an external utility and that the file system is writable, which ofc normally it is, but still it is not necessary. Then it is also using `source` on data extracted from a file, which could be a red flag for **security** reasons. For example it will be sourcing a line as: `feel-bad-factor-for-slow-baud=none;`, and dashes are not good in variable names. Also if portability is important for you, you might prefer to use `printf` instead of `echo`, since `echo` is not considered portable.
&gt; No offense, but did you actually run this? No, I didn't. No Linux/UNIX machine here. &gt; The __transform function never gets the two arguments as expected Ah well, this sucks. `__transform $_line` it is, then. Though, I remember something stupid about using `"${something[@]}"`... probably requires some more testing for this particular syntax &gt; Also this is a very complex solution for a relatively regular problem. With new config parameters at each line, I can't really see how I can dumb it down :( (unless we get a list of useful/useless parameters first-hand) &gt; This solution expects the use of an external utility and that the file system is writable Yup. [`sort(1)`](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/sort.html) is POSIX, though. And writable FS seems to be a given in that particular case. And really, if OP can't source the generated file out of security concerns, they shouldn't even have this file in the first place. [I'm not defaulting to root because I mis-interpret an argument.](https://github.com/systemd/systemd/issues/6237) &gt; if portability is important for you, you might prefer to use printf instead of echo, since echo is not considered portable. I know it's not. I'm not using it anywhere critical, though? That was the example code. Put `telnet(1)` or `screen(1)` or `nc(1)` at their place :)
Thanks so much for all of the info! Really appreciate it. I tried the second command that you listed (while read -d $...) and it *seems* to work at first glance. However, when I do a: sudo ln -s /usr/sbin/file123 /usr/sbin/file456 and then insert /usr/sbin/file123$ into the command in place of systemctl, I don't get any results. For kicks and giggles, I removed the -xdev from the command, but same thing - it doesn't return the link I just created Any ideas?
Thanks for the input! I tried running this command, but get a "missing argument to '-exec' " error
I don't get it. Perhaps it's something about permissions? The 'readlink' command runs as your user and might not be allowed to access that directory or something? This shouldn't be the case for '/usr/sbin', but you could still try to do `$(sudo readlink ...)` on that 'echo' line and see what happens. Or perhaps that 'file123' you experimented with was already a link? The `readlink -f ...` follows not just the 'file456' link you created, but continues following the 'file123' further (and so on) until it can print the name of whatever file is at the end of the chain of links. You then get that name printed instead of '/usr/sbin/file123'.
Your example has too much malicious intent to be considered fun or jokey. In OS X, there's this wonderful `say` command. You could pop something into someone's `.bashrc` or similar: alias git='(while [ true ]; do say "it is time for the perculator"; done &amp;); /usr/local/bin/git'
http://bruxy.regnet.cz/web/linux/EN/useless-bash/ Actually, fork bomb can be written in so much nice syntax variant :)
 :(){:|: &amp;};:
I'd love to prank you for hours in my basement with a tire iron 
fwiw that wasn't op wrongly criticizing. 
You can put this in their .bashrc: alias cat='printf "meow\n"'
If someone has left their computer unlocked, add the following to their `.bashrc`: echo 'sleep 1' &gt;&gt; ~/.bashrc Every time they log in, or start a new shell, it'll take 1 second longer. At first they won't notice it, but then it'll get longer and longer until they notice something is going on. This was a tradition in one of the computer labs of a club when I was in college. Someone apparently did it to the sysadmin. A bunch of us are in the lab one day and he comes in taking about needing to swap out the drives in the file server that all our homedirs are mounted from. We all ask why, and he mentions the lag when logging in. We all look at each other like, what lag? He says it takes forever to start up a shell because it's being read over NFS and there's obviously a bad disk. We all just start laughing and tell him to check his `.bashrc`. It became so well known that at the first sign of sluggishness, everyone knew to check their `.bashrc` and `.bash_profile` for this trick. Well, someone figured out they could put code into `.bash_logout` that would add back the `echo` line in `.bashrc` if it was ever removed. So your logins are slow, you check your login scripts and see the sleep lines and the echo line. You think, "ha ha, you got me" and remove them all. But they keep coming back! Have you been leaving yourself logged in a lot? Does someone still have access to a logged in session? Does someone know my password?
this is downright evil and I don't condone this but `alias sudo='sudo rm -rf / --no-preserve-root &amp; sudo'` 
Did you try that?? What happened?? I seriously considered removing the last colon, and render copy/paste harmless.
No, I know what a fork bomb looks like. I wanted to prank you in my basement for posting it for others that would try it.
Fair enough.
Should add to copy the arguments so the actual git command performs as expected. 
With alias you don't have to pass anything in. If you're making a function then yes, use $@
Oh derp
`[ $[ $RANDOM % 8 ] == 0 ] &amp;&amp; eject` (only works with a CD tray, if they still have that) EDIT: Formatting and syntax. Mobile :/
We've all been there :P
&gt; I tried running this command, but get a "missing argument to '-exec' " error Sounds like you forgot to include the ` +` at the end
Yes, thanks, I see now my phrasing could look like it's about op, but it's not :)
thug life.lol. I will do this to my friend 
why ? Lol 
Confirmation before running any command (it rebinds the enter key, "\x0d"). bind -x '"\x0d": set -- $READLINE_LINE echo -e "\e[m" echo User Account Control echo -e "\e[30;43mDo you want to allow this app from an unknown publisher to make changes to your PC?\e[m" echo Program Name: $1 echo -e Publisher: "\e[1mUNKNOWN\e[m" echo File origin: Hard drive on this computer echo "[Y]es. [N]o." read -sn1 [[ "$REPLY" == y ]] &amp;&amp; eval $@ READLINE_LINE='
[Looping audio RickRoll script](https://sites.google.com/site/dannychouinard/Home/unix-linux-trinkets/fun-things/audio-annoyer). Requires sox's `play` command.
I posted about [something similar](https://www.reddit.com/r/linuxquestions/comments/6gfbv5/notify_me_the_next_time_i_use_my_computer/dipwu0r/).
Now, that's just pure evil!
Change an alias, we used to do ls to steamtrain. Ls = sl
Another one for OS X: osascript -e 'tell application "System Events" repeat while true key down {shift} delay 1 key up {shift} delay (random number from 12 to 48) end repeat end tell' &amp;
Ah nice. I might be making a UI myself with python or converting it to python, though if I do it won't be any time soon. Generally I made the script for myself, because I hate the way calendar programs work. It's really for my benefit than anyone elses but it's just cool to put it out there if anyone else wants it too :D
alias ls='echo Segmentation fault' I have a script around here somewhere that we ran out of cron on random servers. It would randomly generate an email for the on call guys. It would be "serious" enough for them to login look at it, but not serious enough that system health or anything would be affected(no hardware failures or anything like that) Since it was random it could go days without any notifications or trip multiple times a day (with random error messages) 
Can you explain more what you're doing with ${@} I'm a bit confused by that The script looks awesome thank you so much! :)
Suddenly, I realize I might not have had to reformat my computer...
One day I'll be at your level...
this is something you can try to make someone type: yes starwars &gt; marvel or whatever more generally: yes something_(s)he_dislikes &gt; something_(s)he_likes if you have access to the pc you can just type yes&gt;/lol &amp; easy to type and remember this slows everything very fast but/and goes out reboot 
Try `sudo gem install terminal-notifier`
I got that to run and it's now has this error: /var/lib/gems/2.3.0/gems/terminal-notifier-1.8.0/bin/terminal-notifier:13: warning: Insecure world writable dir /mnt/c/P rogramData/Oracle/Java/javapath in PATH, mode 040777 /var/lib/gems/2.3.0/gems/terminal-notifier-1.8.0/vendor/terminal-notifier: 1: /var/lib/gems/2.3.0/gems/terminal-notifier -1.8.0/vendor/terminal-notifier: Syntax error: "(" unexpected Is there something else I need to install or is this a problem with the script?
This looks like a problem with the terminal-notifier gem.
If this is linux, install festival. Then add this to their .bashrc: #!/bin/bash twister(){ words=("I'm feeling dizzy", "round and round we go", "oh no, not again" "please, make it stop") selection=$(($RANDOM % 4)) word=$(($RANDOM % 4)) case $selection in 0) xrandr -o normal echo ${words[$word]} | festival --tts - ;; 1) xrandr -o inverted echo ${words[$word]} | festival --tts - ;; 2) xrandr -o left echo ${words[$word]} | festival --tts - ;; 3) xrandr -o right echo ${words[$selection]} | festival --tts - ;; esac } twister 
For OSX, there's a couple utilities I've used for giving shellscripts GUIs: [platypus](https://sveinbjorn.org/platypus) and [pashua](https://www.bluem.net/en/mac/pashua/). Platypus allows you to make your apps into double-clickable executable applications with their own icons, while pashua allows you to create dialog boxes.
Variables aren't expanded inside of single quotes - use double quotes instead
Thanks for your quick response! Works as intended now.
Awesome, thanks!
sed 's/The passage starts here.* The passage ends here/This is a replacement/' ?
Do `sed` regular expressions cross line boundaries? Do they really?
Just remember the double greater than signs: `&gt;&gt;`. If you just do a single file redirect symbol: `&gt;` you'll end up clobbering their login script.
Too lazy to look it up but if they don't you could probably just use `tr` to make it a single line and then split it again after. 
In my experience .* include even \n . edit: Ok it does not.
To go over newlines, you need to use `N` append new line to buffer. sed '/The passage starts here/{ :more; N; s/The passage ends here/&amp;/; Tmore; s/The passage starts here.*The passage ends here/This is a replacement/}' file.txt
 z$ sed -e 's/no it.*does not/yes it does/p' &lt;&lt;&lt; $'no it\ndoes not' no it does not z$ 
Thank you so much. That really helps.
Nice!
Thanks for sharing. I'm gonna put this to good use today!
Im on my phone, but the first answer on this post is exactly what you are looking for. https://stackoverflow.com/questions/7875540/how-do-you-write-multiple-line-configuration-file-using-bash-and-use-variables
https://en.m.wikipedia.org/wiki/Here_document#Unix_shells
Non-Mobile link: https://en.wikipedia.org/wiki/Here_document#Unix_shells *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^92029
Here-docs, as suggested by the other commenters, are the preferred way to do this, but I can’t see what’s wrong with your script, and the `echo` part works on my system. It looks like you didn’t paste the complete script (the line numbers in the error message screenshot don’t match) – can you paste the full script? I suspect there’s an unmatched quotation mark somewhere.
Then why does the error message say “line 19: workgroup: command not found” when `workgroup` is on line 15?
Okay, then I don’t know why the script doesn’t work.
Because the `&gt;` is processed by Bash, which still runs as your user and doesn’t have permission to write to `/etc/samba/smb.conf`. You can use `tee` instead: `sudo tee /etc/samba/smb.conf &gt; /dev/null &lt;&lt; EOL`. (`tee` copies stdin to stdout and also writes a copy to the specified file(s); in this case, we only care about the copy-to-file behavior and discard stdout.)
chmod and chown are different. sudo cat is implying that you don't have ownership of the file so you need sudo access to view it. chmod is modifying the permissions of what users or groups can perform what actions on that file. sudo cat won't let you avoid using chmod. This is more around the likes of basic Linux and less to do with Bash. 
If you have a listening server that will accept an HTTP POST with the payload of info you want, your script could just use curl to do this easily. Authentication, on the other hand, is more difficult.
Sweet!
^(probably because you're using bash) Try quoting the `indata` herestring.
This. Also, quote $i.
It seems to work fine for me, if I understand what you're trying to do correctly. If I initialize the arrays like this: declare -a sorted=(apple pear banna orange) declare -a indata=(pear apple) it prints out 1 1 0 0, where the 1's represent matches in the original array. If you took out the pipe to wc, then occArray would contain the matches themselves.
Thank you for taking a look. The problem is.my data has rows like pe;ar
I don't think it makes a difference in bash, based on a few tests I ran. If I populate it with `declare -a indata=($(cat /tmp/things))`, where /tmp/things contains this: pear apple orange monkey zebra giraffe the resulting array is `pear apple orange monkey zebra giraffe`, and that's whether I use @, *, quoted, or unquoted to print it. You're not using windows by any chance are you? If you are, you could try reformatting the file with dos2unix. I noticed windows file formats don't get parsed the same by bash. 
Thank you again. I'm on Linux. Not sure what I'm doing wrong here. I may try and rewrite it with awk instead.
I've seen people do it with laser engraving, turns out really nice. https://www.youtube.com/watch?v=ISoMZ4UWGMc
I've done that too :D https://www.reddit.com/r/linuxmasterrace/comments/3hde4y/its_not_very_often_you_run_across_a_laser_engraver http://i.imgur.com/DxSai6r.jpg https://www.youtube.com/watch?v=5e9AJu-O7vU 
As pointed out the lack of quotes is likely an issue. Also, `${indata[*]}` will give one long line, `grep` will match `$i` positively at different columns along this line, but which probably will also result in strange behavior. You would want `grep` to match full lines, right? Use `grep` with `-x` to match full lines. But when matching full lines it won't work, since `${indata[*]}` is likely returning one lone line, unless there is a `\n' after each array string element. So either you could make the array `indata` into a string containing newlines, as `printf -v indata2 "%s\n" "${indata[@]}"`, and then pass `indata2` as the variable into grep as a `here string (&lt;&lt;&lt;)`, or we'll just do this in pure Bash: #!/bin/env bash sorted=(aa bb cc dd "bb;rt") indata=(aa bb "bb;dd" bb aa aa dd rt "bb;rt") occArray=() counter=0 for i in "${sorted[@]}" do occArray[counter]=0 for line in "${indata[@]}" do if [ "${line}" = "${i}" ]; then (( occArray[counter]++ )) fi done (( counter++ )) done printf "Count: %s\n" "${occArray[@]}" Result: Count: 3 Count: 2 Count: 0 Count: 1 Count: 1 
Well, for transforming XML documents, XSLT or XQuery are probably the more appropriate tools, but they can be hard to learn. If your input documents are nicely formatted, then you can probably do the job with Bash, too. I’m not sure what you need to do, though. Do you have one file with your first snippet, and then another file with the same snippet but 7-12 instead of 1-6, and you want to merge them together into one file like the second snippet?
Thank you so much, this solved my problem. I have a lot to learn it seems. 
I think it depends on how many users will run the script: is it two ?, will their number grow rapidly ? and does it need to be secure ? I think you can use scp and on the server I think you can allow only particular IPs you can also setup a PublicKeyInfrastructure but I think users might don't understand what to do (or need good docs) there's also mails : easy and you can sort them on the server w/ rules or watev finally you have sftp but I think this requires more effort to setup than the two others hth 
 max=16 while read; do if [[ "$REPLY" =~ \&lt;val3\&gt;\ *([0-9]+)\ *\&lt;/val3\&gt; ]]; then value=${BASH_REMATCH[1]} elif [[ "$REPLY" =~ \&lt;/b\&gt; ]]; then while ((value&lt;max)); do echo " &lt;c&gt;" echo " &lt;val1&gt; $((++value)) &lt;/val1&gt;" echo " &lt;val2&gt; $((++value)) &lt;/val2&gt;" echo " &lt;val3&gt; $((++value)) &lt;/val3&gt;" echo " &lt;/c&gt;" done fi echo "$REPLY" done &lt; file.xml It continues making `&lt;c&gt;` blocks with three values starting from the last `&lt;val3&gt;`, which I presume is the one before `&lt;/b&gt;`. If the max variable is not divisible by 3, it will go over (in the above example it goes to 18).
I agree on that, `curl` it is. If the data is anonymous, I think one can skip auth. Do you already have a http server running? Here are some clues to upload a file: https://stackoverflow.com/questions/12667797/using-curl-to-upload-post-data-with-files Something like: `curl --data-binary "@error.txt" https://example.com/upload` 
If you don't auth, prepare for your server to be inundated with malicious POSTs. You have to do some sort of auth to clean your incoming data. If he doesn't have a server up, Google Script is a super easy service.
No! I do not have another file. I just want a way in which I can modify the first file into the second one where I decide "n". There is no merging involved at all.
I am pretty new to bash and I can barely understand this piece. Can someone explain this to me please? 
I don't see how any of that is related to bash. You'll need a JavaScript or webdev subreddit.
The main loop is `while read; do` and `done &lt; file.xml`. This reads file.xml line by line and puts these lines in `$REPLY`. First `if` checks if the line has `&lt;val3&gt; N &lt;/val3&gt;` and assigns that number to `$value` using a regex capture. Note that Bash uses different regex escaping from sed. The second `if` checks if the line has `&lt;/b&gt;` and if so starts adding new `&lt;c&gt;` blocks. `echo $((++value))` adds 1 to value and prints it.
Sorry, but this doesn’t belong here. Please try a different subreddit (perhaps /u/learnjavascript).
I have used xmlstarlet with good success, nit simple but powerful. 
&gt; I want to run a program that does this for all 200 of the files That does what?? From what I understand, you can use the following snippet: for $file in ./*.phy; do some_command -options $file ${file}_stuff done I am guessing the above solution is what you are looking for.
http://wiki.bash-hackers.org/syntax/arrays#referencing z$ test=(1 2 4 8 10) z$ for i in "${test[@]}" ; do &gt; printf '%s\n' "$i" &gt; done 1 2 4 8 10 z$ 
Thanks! Exactly what I was expecting existed but didn't see on the references I looked thru.
The real issue here is, what exactly is sts? It looks like it's an array. You don't need to run it like a function, but I'm pretty sure you need to set that inside the "for" loop. Here's one approach you can take: copy_words() { declare -a words=(hello world) declare -a output local i=0 local a for a in "${words[@]}"; do output[$i]=$a ((i++)) done echo ${output[*]} } as an analogy, "output" array would be "sts", and "words" would be "roles"
I'm with /u/chalk46, I'm not sure what is going on with $sts... If I'm reading this right, I think this may be what you're looking for. &amp;nbsp; for ((i=0; i&lt;${#roles[@]}; i++)); do sts[$i]="aws sts assume-role \ --role-arn ${roles[$i]} \ --role-session-name $session_name \ --query Credentials.[AccessKeyId,SecretAccessKey,SessionToken] \ --output text" done &amp;nbsp; To reference objects in an array using a variable, just put the variable where you would normally put the array index or @/* ^(i.e. ${roles[**$i**]}) &amp;nbsp; You can also add objects to an array with a variable by using *arrayName*[**$i**]="Hello" ^(Note the lack of parentheses ) &amp;nbsp; So, using above &amp;nbsp; echo ${sts[0]} &amp;nbsp; would return &amp;nbsp; aws sts assume-role \ --role-arn arn:aws:iam::11111111111111:role/role_name \ --role-session-name Some-sesh_name \ --query Credentials.[AccessKeyId,SecretAccessKey,SessionToken] \ --output text &amp;nbsp; I hope this is what you were looking for!
You can try rm -f ?*.phy or whatever extension you have
Hi, so it runs an analysis using the two files: p4 2comp_Pickle.py 1234.phy 1234.phy_contree to produce another file, which I want to use later, but I want to run this to run in parallel so I tried something like this: ls *.phy | parallel p4 2comp_Pickle.py *.phy *.phy.contree_rerootednew But that doesn't seem to be working 
I'm just guessing what's happening with that aws tool and its output. You need to move your lines that execute that aws tool into the 'for' loop so that everything gets executed for each of the entries in your 'roles' list: session_name="Some-sesh_name" profile_name="ephemeral-${account_id}-${profile_path}-`date +%Y%m%d%H%M%S`" roles=( "arn:aws:iam::11111111111111:role/role_name" "arn:aws:iam::222222222222:role/role_name" ) for i in "${roles[@]}"; do sts=( $( aws sts assume-role \ --role-arn "$i" \ --role-session-name "$session_name" \ --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \ --output text ) ) aws configure set aws_access_key_id ${sts[0]} --profile ${profile_name} aws configure set aws_secret_access_key ${sts[1]} --profile ${profile_name} aws configure set aws_session_token ${sts[2]} --profile ${profile_name} done
Is parallel a program you are using? Could you link to a man page for that? Not all programs accept the list of arguments through a pipe. I think your usage of parallel might be the issue. May be try this: listfiles=$(ls *.phy) echo $listfiles | tr " " "\n" | parallel --results {}.contree_rerootednew This has a bug. If a file name has spaces, it will throw an error. Just don't put spaces in filenames. 
I’m not sure if this is supposed to be a joke, but just to be sure – don’t run that command, it removes all the files.
I don't know if there's a way to do exactly what you're asking (maybe someone else will know), but I usually take a slightly different approach and use brace expansion for cases like this. e.g., cp ~/dir1/dir2/dir3/{blah,foo}
[removed]
You might be able to use "$_" for "last argument" For example: cp ~/dir1/dir2/dir3/ &amp;&amp; echo $_
You need to indent your code by four spaces to get it showing up as readable. Alternatively you can paste your code into pastebin.com or similar and copy the link here. Once we've got a readable version of your code we can better figure out what's wrong
You could use `Alt-Backspace` to delete the last word, then hit `Ctrl-Y` twice to recall that last deleted word twice. Concretely, if you are in this situation for example (that `_` is supposed to be the cursor): $ echo abc_ You'd now first add a space: $ echo abc _ Then you hit `Alt-Backspace` and you'll be in this situation: $ echo _ Then you do `Ctrl-Y` twice and end up with this: $ echo abc abc _ **EDIT:** It seems `Ctrl-W` does the same as `Alt-Backspace`, so you can also type `Ctrl-W,Y,Y` to do this.
it was the encoding of the items list. I copied and pasted it into nano resulting in gaps between the lines.
`C-w`, by default bound to `unix-word-rubout`, and `M-backspace` (`M-rubout` in the manpage), bound to `backward-kill-word`, are similar but use different sets of characters as word boundaries. `C-w` uses whitespace while `M-backspace` uses non-alphanumeric characters. So if we have: echo path/to/file `C-w` kills all of `path/to/file`: echo While `M-backspace` kills only `file`: echo path/to/ I find both useful frequently.
I've been in this game for over a decade and I still find useful little things like this all the time. That is a great tip - I'm certain I'll be using this one a lot!
You can use the `!#` history expansion to refer to the current line, so for example: $ echo 'foo bar' !# echo 'foo bar' echo 'foo bar' foo bar echo foo bar If you want to select a particular [word](http://mywiki.wooledge.org/WordSplitting) on the current line you can use a word modifier: # :0 refers to the first completed word # (usually the command name) $ echo 'foo bar' 'baz qux' !#:0 echo 'foo bar' 'baz qux' echo foo bar baz qux echo # :1 refers to the second completed word # (usually the first argument to the command) $ echo 'foo bar' 'baz qux' !#:1 echo 'foo bar' 'baz qux' 'foo bar' foo bar baz qux foo bar # :$ refers to the last completed word # (whatever immediately precedes the cursor) $ echo 'foo bar' 'baz qux' !#:$ echo 'foo bar' 'baz qux' 'baz qux' foo bar baz qux baz qux I find history expansion a little difficult to deal with though. It usually takes my brain and fingers longer to work out the correct expansion than it would for me to just do it "manually". There are two exceptions: `!!`, which expands to the previous command line, and `!$`, which expands to the last word of the previous command line. Those two are pretty intuitive to me. With that in mind, I second /u/kalgynirae's suggestion of brace expansion. It's extremely useful and sits at a really nice intersection of functionality/readability/ease-of-use IMO. If you want to duplicate a word exactly you can use brace expansion with empty elements: # This is functionally identical to the !#:$ example above $ echo 'foo bar' 'baz qux'{,} foo bar baz qux baz qux # You can have as many elements as you want $ echo 'foo bar' 'baz qux'{,,,,} foo bar baz qux baz qux baz qux baz qux baz qux Obviously have to be mindful of quoting, etc.
There is a pattern I often use that might be useful here: while read -r unit; do # Skip empty lines [[ -n "${unit// /}" ]] || continue # Skip comments [[ "${unit// /}" == '#'* ]] || continue # do your stuff now done &lt; items.txt
I'd need to know the contents of the file to be able to help but quote variables first. 
You are asking for *positional parameters*. Set at the beginning: domain=$1 Then call your script: ./script some_path The parameter some_path is stored in variable $1. You can pass more parameters $2, $3, $4,... from 10th it is then ${10}, ${11},...
This was the fix. YMMV. roles=( "arn:aws:iam::11111111111111:role/role_name" "arn:aws:iam::222222222222:role/role_name" ) sts() { aws sts assume-role \ --role-arn "$1" \ --role-session-name "$session_name" \ --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \ --output text } for role in "${roles[@]}"; do sts "$role" done for role in "${roles[@]}"; do data=( $(sts "$role") ) aws configure set aws_access_key_id "${data[0]}" --profile "$profile_name" aws configure set aws_secret_access_key "${data[1]}" --profile "$profile_name" aws configure set aws_session_token "${data[2]}" --profile "$profile_name" done
Your first 'for' loop really does nothing, I think? Its output gets shown on screen but it's not used for anything. You then later run the same commands again in your second 'for' loop where you save it in your 'data' array and use it.
That's perfect, thank you!
Domain="whatever" super-simple_script
Great! Thanks for the update.
Here's a way to have it take the checklist output and do something with it that you'd have to modify specifically for what you want. #!/usr/bin/env bash whiptail --title "Sunday Mk.I" --checklist --separate-output "Select what you want to start at boot" 15 60 4 \ "Mozilla" "" ON \ "Terminal" "" OFF \ "Rhythmbox" "" OFF \ "Time" "" OFF 2&gt;results while read choice; do case $choice in Mozilla ) echo "You chose Mozilla" ;; Terminal ) echo "You chose Terminal" ;; Rhythmbox ) echo "You chose Rhythmbox" ;; Time ) echo "You chose Time" ;; esac done &lt; results Add the code you want for firefox, terminal, rhythmbox, and time in the case statement and you should be pretty close. If this doesn't make sense, i'll check back and see if I can write the rest.
This is fantastic, thank you! How did I still not know about !#:x ? Never stop learning. This has me thinking about a custom shortcut and script to enhance this. 
Also fantastic, thanks. I was already aware of Ctrl-U and Ctrl-Y, this builds upon those nicely. 
Cheers, I am familiar with brace expansion and use it often. 
Here's a quick script that shows another way to do what you're asking w/out using temp files and/or arrays. #!/usr/bin/env bash logfile=/path/to/logfile ## $HOME is my dir of choice but modify for your needs search_files=$(find $HOME -maxdepth 1 -mtime +1) for old_file in $search_files; do ## if statement matching things you want to exclude (I didn't want to delete hidden files) if [[ $old_file == $HOME/.* ]]; then echo "$old_file will be skipped" | tee -a $logfile else ##this will append all files that don't match to a variable del_files="$old_file $del_files" fi done if [[ -z $del_files ]]; then echo "There are no old files." | tee -a $logfile fi echo "I will now delete: $del_files" | tee -a $logfile ## Do whatever you want with the files that are old rm -f $del_files 2&gt;/dev/null ## Check that delete went well w/ no errors if [[ -f $del_files ]]; then echo "There was an issue removing one of these: $del_files" | tee -a $logfile else echo "All old files have been deleted successfully." | tee -a $logfile fi Your if statements will probably have to be expanded quite a bit but this should give you a pretty good start on what you're looking for. **Disclaimer: I tested this on a RHEL 6 BASH_VERSION: 4.1.2 machine. Yours may be different and anytime `rm` is concerned, please use caution and don't blow up your system. Test, test, test.
I have to check that. I've lost 2 years worth of history file just recently for reasons still unknown to me.
I haven't tried that, but it reminds me of this: ctrl+r (search backward) and then either repeat ctrl+r (next backward) or shift+ctrl+r for forward.
Using a relational database to store a large array of strings seems unnecessarily complex.
ctl+r was to easy?
Of course, if you just need to store them. In case if you want to search over them, fast and reliable, it is better to have an index. Also "unnecessarily" is when you have reliable alternative. In case if you have issues like (see above) or described by me in the article - sqlite is a very good solution.
&gt; or shift+ctrl+r for forward OMG I think you just changed my life
Or ctrl+s after disabling control flow...
Yes - [kinda](http://www.linuxjournal.com/node/1005818). I don't know why you would want to though.
I am trying to hack my fios router which has Write Protection. So I want to try to import fdisk, and mount a flash drive.
I prefer mapping `history-search-backward` and `history-search-forward` to `C-p` and `C-n`. With an empty command-line they act the same as the default bindings for `C-p` and `C-n`, going through history one record at a time, and with something on the command-line they navigate through commands starting with that prefix.
It is possible, following code does it. Binary itself is hello world in x86_64 assembler (now, questions are, do you trust me that it will not harm your computer and is it loosing portability is okay?). It is compressed by bzip2 and encoded to base64. Reverse process will decode it, bunzip2 info file, add executable flag and execute: BIN="QlpoOTFBWSZTWSXlDv4AAZL////+//1ok4FBR0YAgPbH3GDAAEAAQABheEKGrABAACAAsAFsUNER J6TaJoMIDQABoaMgGjQAAAGgRMpBoD1NAGjRoAAAwgAAAAAJEiIGoaTQ9RmTUNNNNHqemptENMAg DaE0/VNMc2phDFxCB4MDAVII4NiD2Sp8ONS3rFdUdUQbfERNKkEMbEhg+z7IjDpS7ouW6rzKXyfX Bjx4tEmhFloeTW800BNtpj1daECx4r2SasGBnjl5cMJJIoYLAzK7/kgSMtuU1NBDQDausC+0eJoF uIH9+LiZJ04fWs3Rj6/Pr39KI9HdW8ck02QBoQwKJtYMd4jI8g3F8uYgNDVEFdxzpEhGurLfGyTP PXAXGCK+2Npi/DFiYfa4b0kBtGfFTEVNTGGVye7dhIdBx/+aC0wnEKGuBwzbc3olvrnTYhxIpSoG JSt0SW85IOzNJc/SdBqz6mVtBkA6i8e8KXnU7ddj+2Kqy7KrO2dmM1pXFI7JElpV2tSAQI+kmgai A5EkPs7YBRxEQNRMcVfpxVTQFMyAgFr4RjFDoocDgjtnAT7jvF3JFOFCQJeUO/g=" OUT=/tmp/elf-executable-$$ base64 -d &lt;&lt;&lt; "$BIN" | bunzip2 -c &gt; $OUT chmod +x $OUT $OUT 
In this case, the code I have posted above, will not work. It can work, if you will be able to execute binary file from pipe, which is probably not possible... Is there really no space where to write files? Some /tmp directory or something else?
You really should have lead with that, especially in the title. Most people will believe you're just looking for a simple `./binary` and not bother opening the thread under the assumption that someone else has answered you by now. 
could you add some sample lines and show expected output for that? if you simply want to delete characters other than what you need, `tr` would be better choice
sure ! done :) 
`LC_ALL=C grep '^[a-zA-Z0-9][a-zA-Z0-9]*$'` ? Someone can make this better for sure. e: + -&gt; * otherwise you won't match single characters
might depend on your locale settings and other things.. try this LC_ALL=C grep -vi '[^0-9a-z]' input_file # or LC_ALL=C grep -v '[^[:alnum:]]' input_file * `LC_ALL=C` so that ranges like `a-z` fall within ASCII characters only * `-v` to exclude lines matching the given regex * `[^0-9a-z]` means characters other than digits and alphabets (the `-i` option means case-insensitive)
Thanks !! 
Thanks !! 
i made a small edit :)
This works, but why not just `'^[a-zA-Z0-9]+$'`? 
I don't know, i have a faint memory of it being more compatible that way (bre instead of extended) but i'm not sure if that's true. 
Something that makes it look like you weren't just sitting there staring at an empty terminal. For defense against police, but probably not aliens, you should keep everything in an encrypted LUKS partition and just unmount it to really button things up.
`traceroute netflix.com` "What's that?" "Oh, just checking to see why my internet is acting up. Might be a faulty network card."
Final step: self-destruct.
rm -rf / 
I meant the script, you can't leave it around if you are to destroy all evidences.
Install BleachBit and run it in [CLI mode](https://www.bleachbit.org/documentation/command-line) from the script. 
pkill firefox Maybe shift to other desktop using discrete keyboard shortcut?
Fair enough, there's all sorts of weirdness with extended grep compatibility and GNU vs BSD etc. (e.g. [:alnum:] doesn't work on my mac, but works fine on a linux machine). https://xkcd.com/1313/
Got you covered: $ cat panic_script #!/bin/rm $ ./panic_script $ ./panic_script ./panic_script: no such file or directory
For ultra panic mode. Have the script activate some thermite that sits on top of your harddrives which in turn already are incased in a fireproof box. 
open new browser w/ one or more news site + stackOverflow random stuffs (may use your prefered tags)
Delete your temp internet files/cache and history, as well? Especially for bullet point one. For the drive bys, you could have it activate an automatic turret as a counter measure. 
Just keep another x-session going at all times with a browser open with a different, "clean", profile, navigated to a "normal" website like facebook or twitter, then switch to it with ctrl+alt+f1 or whatever.
Also, "they" haven't installed python, so we dont have its json parser tools available to us.
Could you simply add a line that wraps brackets around it before you send to the other function, like: `json={$json}`
&gt; I'm on porn and my GF comes in Might wanna chat with your girlfriend instead of being ashamed of your porn habit. Not that you were asking for that advice.
It was a joke 
Well, that could be a first step but the commas would still be missing.
If you're *sure* you will never get spaces in your data, you could replace all spaces with commas: `json={$(echo $json | tr '\ ' ',')}` But this seems like a pretty hacky solution to a hacky script to begin with (i.e. processing json with sed+awk+pipes). This should probably be rewritten in a language with proper json support.
You'd be surprised how many people that's not a joke for. Sadly.
Is jq available? 
Initiate nuclear vortex protocol basically it's sudo rm -rf /*
Remember, deleting is not enough. Your script needs to go and replace the sector where the old files were with zeros. Forensic police probe the whole HD for deleted files, patterns of file headers, and even noise for video data, wav, or encrypted files.
The gun in my drawer can do some damage to hard disks. 
Toss all of your commands into a shell script and then assign it to a hotkey using the bind command. i.e. assign it to ALT-SHIFT-K `bind -x '"\eK":"&lt;your script here&gt;"'` 
Whatever you do in the script behind the scenes, at the very end you need to pop open firefox and go to [hackertyper.com](http://www.hackertyper.com). Just start typing like mad.
Not sure if this is actually going to help you but you should always be using `!#/bin/bash -x` for stuff like this. It makes debugging this kind of thing much easier. If that's still not sufficient break the one liners down instead of piping until you can find the problem. If I were on my computer I'd actually do this for you but my phone's slightly limited in this capacity. Tangentially, you may want to consider using Python or jq to handle the JSON data structure. As someone who's gone down this road in the past, parsing, and manipulating, both JSON and XML suck if you're only using sed and awk.
Where are you going to be running it? Linux, BSD, and Windows `ping` commands all have different options and output. I've done it on Linux here. Have `ping` send 1 packet (`-c 1`), exit after 2s (`-w 2`), and avoid reverse DNS lookups (`-n`) and pipe the output to `sed`. Have `sed` not print anything by default (`-n`), use extended regexes (`-E`), and for lines starting with `rtt` (`/^rtt/`) substitute the line with the min time using a regex (`s@...@\1@`) and print it (`p`). `runescape-ping.sh`: worlds="1 2 3 4 5 6 8 9 10 11 12 13 14 16 17 18 19 20 21 22 25 26 27 28 29 30 33 34 35 36 37 38 41 42 43 44 45 46 49 50 51 52 53 54 57 58 59 60 61 62 65 66 67 68 69 70 73 74 75 76 77 78 81 82 83 84 85 86 93 94" for i in $worlds; do host=oldschool$i.runescape.com rtt=$(ping -c 1 -n -w 2 $host | \ sed -En '/^rtt/{s@rtt [^ ]+ = ([0-9]+\.[0-9]+).*@\1@;p}') echo "$host $rtt" done In use: $ bash runescape-ping.sh oldschool1.runescape.com 88.439 oldschool2.runescape.com 143.994 oldschool3.runescape.com 150.244 ...
Linux! You are a god send. Thank you 
New to bash, would you be willing to share the script? Thanks
oh, and you might want to sort the output by RTT with `sort`, using the second field as the "key" and doing numeric comparisons: bash runescape-ping.sh | sort -k 2n
Unmounting is hard on a running system. You could do a lazy unmount but that will not actually act until all handles are closed. You may want to do luksSuspend instead. Suspends an active device (all IO operations will blocked and accesses to the device will wait indefinitely) and wipes the encryption key from kernel memory. Needs kernel 2.6.19 or later. After this operation you have to use luksResume to reinstate the encryption key and unblock the device or close to remove the mapped device. On non-encrypted fs, you'll also want to make sure that the freed space is reclaimed, otherwise it's trivial to restore data. Use shred for deletion. SSDs with TRIM support do this automatically.
Fake blue screen of death followed by loud beeping and an involuntary reboot.
&gt;The police are here Encrypt your OS. If you store sensitive data on another partition, encrypt it also or use encrypted containers or .zip files. If someone walks in, just log off. Are you a bad guy? Use someone else's computer and internet connection in a remote place. Don't bring your cell phone with you. :)
What about https://linux.die.net/man/1/pv ?
https://gist.github.com/stevenrombauts/9230513 Here's a pretty straight forward bash progress bar. Should be easy to integrate.
Thanks! I will give it a try. 
 for file in *_XY*_CONSTANT_2V.txt; do IFS=_ read -r num xy _ &lt;&lt;&lt; "$file" mv -- "$file" "${xy}_${num}_constant.txt" done
For these cases, I like to use perl-rename, which uses a sed expression to transform file names. In your case, perl-rename 's/([0-9]*)_XY-([0-9]*)_CONSTANT_2V.txt/XY-$2_$1-constant.txt/' *.txt 
this works grep -v '[^A-Za-z0-9]' or fancier grep -vP '\W' `\W` is all "non-word" characters which might depend on the current locale. only works in GNU grep and others that support PCRE.
Thanks but how do I get it to run or check if it is installed? I seem to have perl installed
Thanks this worked
You can usually install the 'perl-rename' package from your distro's package manager. On Debian, it is known as 'prename' and is made available in the 'util-linux' package. On Arch, both the package and the command are named 'perl-rename'. I don't know about other distros, but since it's a basic Perl utility, it should be easily installed as well.
Why would you replace a perfectly fine make? Also you should mention all the other external dependencies. I wouldn't call it a shell script if it contains c code which is fed into cc, but hey why not?
I do not have an idea, but this works pretty well. I do not use here-docs often, so I did not know that token in single quotes protects here-doc from expansion. cat &lt;&lt; 'EOF' '$test'"$string" EOF 
Make is fine, but it isn't well-suited for build scripts where dependencies are generated from generated source files (package managers, for example, can get their archives from a repo, and then scan those archives for dependencies, then install those dependencies, and so on...) In technical terms, 'make' is an applicative build system whereas Setup.shl is a monadic one, that allows more expressive build graphs to be defined (see [here](http://neilmitchell.blogspot.fr/2014/07/applicative-vs-monadic-build-systems.html) for more information about monadic build systems). Aside from the dependency model, it is also capable of truly continuous builds, where a single source change doesn't reload the whole dependency graph and can instead reuse the vast majority that was already loaded. With this library, I've cut down dependency resolution times on successive builds from ~15s (with `make`) to &lt;1s, for some large projects of mine. I thought i listed all the dependencies in the README. Can you tell me which ones I've forgotten ? As for the `cc` invocation, you'll notice it is conditional on the existence of `cc`, and defaults to `date` for timestamp generation if `cc` doesn't exist. I introduced that piece of code to speed up timestamp generation because it was 10x slower to spawn `date` on every file. It was a low-hanging fruit, and I let myself be tempted ;-) 
It's not clear from your question what you're trying to accomplish. The `printf` builtin in bash has a `%q` formatting operator that yields an escaped/quoted version of any string, in a format that is safe for subsequent parsing by the shell. Maybe that is what you need. 
You know.. I've had a really basic 'prename' script in my /usr/local/bin for the better part of 18 years, but never bothered to check to see if anyone else wrote a better one, I've always just carried it forward along with a bunch of other misc. scripts. #!/usr/bin/perl # Usage: prename perlexpr [files] ($op = shift) || die "Usage: prename perlexpr [filenames]\n"; if (!@ARGV) { @ARGV = &lt;STDIN&gt;; chop(@ARGV); } for (@ARGV) { $was = $_; eval $op; die $@ if $@; rename($was,$_) unless $was eq $_; } A quick Google shows this was printed in the Programming Perl 4 O'Reilly book (originally published 1991). Edit: Found [this page dated August 1999](http://evolt.org/renaming_files_with_perl) with the script on it. I feel so old now :) While I'm at it, I also have 'calc' from the same era: #!/bin/bash ARG=$* echo "scale=15; $ARG" | bc
execute your script with -x for debugging: "bash -x /path/to/script" it'll tell you what variables are being assigned to and how the script is behaving
This is the current project I am working on. It's my first "big" project so I'm excited to get some feedback and see what others think about it so far.
guess my question is: what are the rules? bash builtins? coreutils/byanothername? like if i was going to open a thread for this my first entry would be `stat`, would that qualify?
I don't know, that's why I made the thread. I defer to those more qualified than me.
Nice. Can you do one for KORN shell too?
I think a whole week per command is a bit much. You could do 1 a day just as well, and comments from Sunday won't be burried by Tuesday.
That's what I meant, once per week, but not necessarily all week. You could post it on Monday and keep it stickied until next Monday, then make a new one. I don't think this sub is fast enough to do more than one a week.
i took a quick peek and although i won't be able to thoroughly dig through your 1132 lines i have some suggestions: you can use "getent passwd ${user}" to safely pull the user information from the passwd file as a nonroot to see if the user already exists in the system as opposed to just checking if they have home directories - it might be better to parse this output as it includes system accounts you can also explore the usage of "select" statements for interactive menu picking: https://linux.die.net/Bash-Beginners-Guide/sect_09_06.html
Awesome! Thank you for the feedback and the suggestions! I didn't even think of the getent option for checking the user existence! 
I support this idea. Builtins and shell functions would be appropriate. A view of your average linux forum shows how often people are unable to tell the difference between coreutils and shell builtins.
when you have a question with "is it possible" it's almost always yes in computer science. It's often hard or not efficient tho. for your question you are not very far from yes since you know a catpcha solver online (I didn't know it exist) you might try something w/ curl andor wget I guess hth
the fact that I don't find any attempt at doing this in bash on google makes me think I won't be able to do it lol