Cool, then I'm gonna use getopt as well. Thanks for your answer, that was bugging me a bit knowing which one to use. 
I thought you put 4 spaces in front of your code and it knows it's code?
I'm really smart, I found the code block button!
```test```
I think you are using what reddit refers to as the "Fancy Pants Editor". Markdown syntax is escaped in FPE so your code block isn't showing
Yeah, I finally found the code block button. After 10 minutes of trying 30 different code tags XD
¬øCan you post all of it?
Done.
There's no thing, really. getopts is part of the POSIX standard. getopt is a GNU coreutils app that you might find useful. Odds are you won't write any shell scripts that need to be strictly POSIXly correct though so just use whichever you find convenient and then if/when you ever need to make your scripts support another platform you'll actually probably hafta deal with problems a lot harder than argument parsing. :B
 if [[ "$value" -ne 0 ]] || [[ "$value" -ne 1 ]] || [[ "$value" -ne 2 ]] || [[ "$value" -ne 3 ]] This condition is true for every number: Anything nonzero passes the first condition, and zero passes the second condition. "Neither 0 nor 1 nor 2 nor 3" means "not 0 *and* not 1 *and* not 2 *and* not 3", so you want `&amp;&amp;` rather than `||`.
WWooooowwwwww, that was it? I can see now. Before I said if value equals 1 or 2 or 3, i needed AND. Thank you. That was a stupid mistake lol
# Bash shopt -s extglob var=${var##*( )} # trim the left var=${var%%*( )} # trim the right
Three ways: Indirection, Simulated Multidimensional Arrays, and True Arrays of Arrays. Please tell me about the core of the problem you're working on.
But IIRC the long options are *not* part of the POSIX standard, they're a GNU extension...
Thanks a lot mate..basically I have got a input file which has the below three fields request,response code ,response time Example input file request-1,499,8 request-1,502,7 request-2,502,7.5 request-3,500,6 request-3,502,8 I am required to convert that into Desired output :- request,max response time ,avg response time,no of 499 request ,no of 500 request ,no of 502 request request-1,8,7.5,1,0,1 request-2,7.5,7.5,0,0,1 request-3,8,7,0,1,1 Have figured out all except the no of response .. I have written a for loop to tackle the same (injecting the response codes to be stored in an array ,etc)but want to learn how this can be accomplished using awk ..
Got it done in POSIX Awk. I'm not 100% on the `%g` format specifier for `printf`; it might break with long floats. I'll go through and comment this later. ^lmao When you find prosperity, please donate to transparent charities. BEGIN{ OFS=FS="," } { Req[$1,$2]++ $3&gt;Max[$1] &amp;&amp; Max[$1]=$3 Avg[$1]=($3 + Avg[$1] * Count[$1]++)/Count[$1] } END{ print "Req", "Max", "Avg", "499", "500", "502" for(ij in Req){ split(ij, Index, SUBSEP) i=Index[1]; if(!Uniq[i]++) printf("%s,%g,%g,%d,%d,%d\n", \ i, Max[i], Avg[i], Req[i,499], Req[i,500], Req[i,502]) } }
Actually, `getopts` is a bash builtin, while `getopt` is an external program. This can make quite the difference in speed on some machines. I typically use `getopt` for programs which need the long options, and `getopts` for things which might be run a lot (like keyboard shortcuts in my desktop, or my status bar, etc.)
well this is awesome..thanks a lot mate :) .. yeah sure will look to start donating as well.
It's the `+` that's causing the problems, right? Here's a solution (maybe not the best one?) by disabling the special meaning of `+` (POSIX compatibility), and just using `*` instead: sed --posix '2s/+[[:alnum:]]*"/"/'
actually, because I am a noob, its worse. I can use bash and sed and awk fine. And regex. With examples and man open. And trial and error. But I have no idea where to start here. SED and regex combined breaks my head, and reading your examples dont help. All that escaping... And if I could read it, I still would not know how to make SED grab the 2nd line, match it to my regex, and remove a indetermined string while leaving the rest... So I dont know. I dont think the + is the problem. Is it? As far as I understand your post, it would help me to get sed to find the second line OR all lines starting with X-Kolab-To... But it would not help to get that cursed +delimiterstring out...?
&gt; All that escaping... I'm actually not escaping anything. Let me break down my sed filter: sed --posix '/^X-Kolab-To:/s/+[[:alnum:]]*"/"/' --posix # disable GNU extensions ' ' # outer quotes (everything inside is passed literally to sed /^X-Kolab-To:/ # Match lines beginning with X-Kolab-To: s/ / / # replace in line (s/find/replace/) +[[:alnum:]]*" # the character '+' followed by zero or more alphannumeric characters, followed by the character '"'. " # the character '"' The reason I disabled GNU extensions was because of the `+` character. With the extsnsions, `+` means "match the previous character one or more times". But you want to match strings beginning with a literal `+`, so I disable that. Instead, I use the `*` metacharacter, which matches zero or more of the previous character. 
Right, fpm sure looks nice, making packages available for all major platforms. However I want to keep it transparent to be inspectable for weird stuff.
I see. sorry, I did not understand the command. Your breakdown and explanation helped. I ran a test, and it seemed to work well. Output was in console, file has disappeared. So, running sed with -i should edit the file in place, no output to console and changed file is left behind. Which my test confirms. So.... first, let me thank you very very very much. Then, is there anything you want to advise me on when - running trough all files in the directory - checking the 2nd line is indeed the X-Kolab-To: line - matches my regex (shared folder with +delimiter) - then running your sed (probably best with line number instead of text match) ? 
You can specify all files with `*` in bash: echo * # echo all files in the directory echo *.txt # all files ending in .txt sed 's/foo/bar/' * # run sed on all files in the directory &gt; matches my regex You can be more explicit in your replacement: # -E flag allows for (match groups) sed --posix -E 's/^(X-Kolab-To: "shared+shared/[[:alnum:]]*@domain\.tld)(+[[:alnum:]]*)?(@domain\.tld)/\1\3/' The metacharacter `?` matches zero or one of the previous match. `\1` and `\3` are *backreferences*, allowing you to preserve the matched text in the output. So `s/(fo*)(bar)?(baz)/\1\3/` will do: fooooobarbaz -&gt; fooooobaz fbaz -&gt; fbaz # matches, but the second match group is empty foobazbaz -&gt; foobazbaz # does not match
A quick google search turned up the following which looks similar to what you appear to be wanting: [http://www.webupd8.org/2009/05/ubuntu-embed-terminal-into-you-desktop.html](http://www.webupd8.org/2009/05/ubuntu-embed-terminal-into-you-desktop.html) &amp;#x200B; A perhaps simpler way to achieve something similar would be to use the i3 window manager and set all applications except the terminal to "floating". i3 always places floating windows above tiling windows. I cannot vouch for how well that would work though as I have not tried it.
Check out shflags, I switched to using it last year and haven't looked back. https://github.com/kward/shflags
(Except you want `&gt;&gt;` there, not `&gt;`.)
This will work, but you are opening and closing the output file for each line.
That won't quite work. You need `&gt;&gt;` (append) there, not `&gt;`.
this is me trying to do the later. well you can see that my mailcap folder is full of other stuff too that I tried. There might something wrong with the code. Not sure. https://asciinema.org/a/GNLsQegSnapN2i1XkFYkeAy0k
That should say `print &gt;&gt;` there, not `print &gt;`.
I'll check it out, this looks very interesting. Thanks!
üò§üò§üò§
You do know that you can unpack packages and inspect them if you want... right?
Wait, are you angry at me?
&gt; I have noticed that most of the basic commands available like ls or grep and pretty much all the other major commands support long options. This is only the case for the GNU versions of the tools. On BusyBox, macOS, OpenBSD, Solaris, or POSIX in general, these commands will support few or no long options. 
no so. it doesn't need &gt;&gt;, subsequent &gt; to the same file within the same awk script will *append* again, if you read the remarks from the poster wants to collect output matching *any* of a number of expressions to one file AND output matching none of them to another
&gt; no so. it doesn't need &gt;&gt;, subsequent &gt; to the same file within the same awk script will append Oh! Crap. Didn't realize that. Sorry. &gt; again, if you read the remarks from the poster wants to collect output matching any of a number of expressions to one file AND output matching none of them to another Ah. :)
At Bash üíØ üòÅ. Thanks for the keen eye.
You can parse simple long options with getopts if you want. I usually do it for flags. Just add "-:" to your getopts line. When you come across a long option, enter another case statement on $OPTARG. It's perfect for flags, but it can get convoluted if you want to pass a value with the long option which kind of defeats the purpose. Personally, i just prefer getopts, but also like to have access to something like --help. I can post a full example if you're interested.
Pretty annoyed at this issue. Pretty much I switch to python or perl to handle long form switches, which are the only kind I use.
If you're on a WIndows system, why not use robocopy and rename? I'm not sure bash is your best tool here.
It helps me to remember and it makes scripts more readable. I have seen that Python and Perl have good ways to get this.
You're very welcome, it saved me a lot of time writing my own posix compliant stub to handle arguments.
This right here, don't force bash on Windows when this can be done natively relatively easily with powershell and windows tools
Yes please, I could use some code examples. Thanks :)
Awk would probably be better suited for something like this. Here's a quick one-liner that should do the trick # count lines with more or less than 7 fields awk 'NF!=7 {count++;} END {print count;}' file.tsv
**This didn't work, it only showed the number of lines in the file** I tried both NF!=7 and NF!=6 and NF!=5 &amp;#x200B; **What if I used grep?** grep "\^.\*?\\t.\*?\\t.\*?\\t.\*?\\t.\*?\\t.\*?\\t.\*?$" file.tsv | cut -f 1 &amp;#x200B; This would show me the first ID column of the lines \*with\* 7 columns, can I change it to show me columns that \*don't\* have 7 columns (tabs)? &amp;#x200B;
... or use a wrapper around getopt - saves a lot of grief esp. when you come to maintain the code 2 years hence and forgot what everything does. Here's mine: [http://bhepple.com/doku/doku.php?id=argp.sh](http://bhepple.com/doku/doku.php?id=argp.sh)
Don't use `-ne` with `[[ ... ]]`, use `!=` instead. `-ne` is for `[ ... ]` which is kinda sorta deprecated. Also, you don't need to terminate lines with `;` &amp;#x200B;
To clarify, do you want some sort of output that tells you which rows don't have 7 columns?
&gt; Don't use `-ne` with `[[ ... ]]`, use `!=` instead. `-ne` is for `[ ... ]` which is kinda sorta deprecated. Actually, `-ne` is an arithmetic binary operator. I would recommend using double parenthesis and `!=` instead for that in Bash, though.
To verify that all rows (lines) of a file have 7 columns (tab separated) you could print all lines that don't have 7 columns: awk -F $'\t' 'NF != 7' file.tsv If this command does not print anything, you are sure that all rows have 7 columns (PS. `-F $'\t'` provides the delimiter for awk (default is all whitespace). `$'\t'`is the tab character provides with so-called ANSI-C quoting.)
And to add to this answer, adding /^\t/ ; /\t\t/ ; /\t$/ Will check for emtpy values at the start of the line, in the middle and at the end of the line respectively, since the offered solutions only count tabs, and while required for determining the number of fields you have, are ignorant of missing values (which I assume is really what you are looking for)
That may caused by different version of awk. Try this: `awk -F ‚Äò\t‚Äô ‚ÄòNF!=7‚Äô file.tsv`
I don't think it's a version issue; OP's TSVs probably contain spaces. You're on the money.
I understand that edge case, but I don't understand how and where I have to include these patterns. Can you elaborate a bit?
`#!/bin/bash` &amp;#x200B; `readonly FILE_NAME="myfile.tsv"` `readonly COL_NUMBER=7` &amp;#x200B; `line_n=0` `while read one_line; do` `let line_n+=1` `n_tab=$(echo "$one_line" | awk '{print gsub(/\t/,"")}')` `let n_col=n_tab+1` `if [ $n_col -ne $COL_NUMBER ]; then` `echo "error in line $line_n"` `fi` `done &lt; "$FILE_NAME"` &amp;#x200B;
Great answer, OP doesn't deserve you.
 stat -c%w file
https://www.gnu.org/software/gawk/manual/html_node/Regexp.html
A pure bash solution with no redundant disk ops: matchTest()( while read l; do [[ $l =~ $1 ]] &amp;&amp; match+="$l"$'\n' || nomatch+="$l"$'\n'; done for object in {no,}match; do printf %s "${!object}" &gt; $object; done ) Make sure to strong quote the argument: `matchTest '^regex$'`
&gt;awk -F ‚Äò\\t‚Äô ‚ÄòNF!=7‚Äô file.tsv This was the correct answer, thank you!
I know those patterns are regex patterns, and I know that the field seperator can be regex, but at the moment I don't see how to combine them. What I do know is that GNU awk has an extension that allows to define what a field is (by a regex) and not what the field seperator is. So a solution to this problem can be solved by: awk -v FPAT="[^\t]+" 'NF != 7' file.tsv Now a field is everything that does not contain a tab. Automatically any leading and trailing tabs are ignored. IIRC GNU awk is not the default awk on Ubuntu, but you can download it from the repository by the name gawk. 
bruh
Protip: `'NF!=7'` is a short form that means "do default behavior to lines where the Number of Fields is not 7." Default behavior is to print the whole line (or `$0`), but your OP specified printing the third field, which you can do like dis: awk -F\\t 'NF!=7{print $3}' Awk is a great language and I would love to hear more from you about how you use it in your projects.
 if(f!=$1){print ""; print "";} Just a wild guess :)
&gt; IIRC GNU awk is not the default awk on Ubuntu So you expect people to read your mind and know what version of a program you use?
&gt; It would display the Not empty message just fine, however an additional error message would be shown. When describing problems that include error messages, it's always a good idea to actually mention at some point in time _what_ that error is. So let's just consider this: $ myVar=test $ [$myVar == ""] bash: [test: command not found I bet that's the kind of error you're getting. The reason you're getting this is that `[` needs to be a separate word. `[` is actually a command, just as `echo` is, so you need to ensure that there's a space after it and before its first argument: [ $myVar == "" ] Without that space the `[` and `$v` are part of the same word. That being said, there's a better way to approach this problem altogether. First, instead of doing: ! [ $myVar == "" ] you could more simply do: [ $myVar != "" ] However this, and the code you posted, has a big problem: when `$myVar` is empty that actually expands to the four words: [ != "" ] Now this is certainly "unsuccessful", but it's unsuccessful for all the wrong reasons. It's _invalid syntax_ for the `[` test. The `!=` operator needs a word on each side of it to make a proper comparison. So a more correct approach is to properly quote the variable: [ "$myVar" != "" ] This, finally, is correct.
Patch to your approach: grep ... |awk '$1!=f{printf("\n\n")}; {f=$1; print}'
Try this: #!/bin/bash myVar="$1" if [[ -z "$1" ]]; then echo "myVar is empty" else echo "myVar is populated" echo "$myVar" fi It checks to see if $myVar is populated, if not, it will tell you, if you have passed it a variable (./myScript helloWorld), it will echo 'okay' and print 'helloWorld'. Look up "Check if a variable is empty bash" for more information of -z -f -d etc. Hope this helps.
Try the following change to the if statement line. Fist, adding the whitespace to the brackets. Second, enclosing your variable in quotes. &amp;#x200B; `#!/bin/bash` `myVar="$1"` `if ! [ "$myVar" == "" ]` `then` `echo "Not empty"` `else` `echo "Empty"` `fi`
Swap "" for "\x0A\x0A"
As /u/aioeu pointed out, the problem in your script is the missing spaces around the brackets. Remember, bash scripts are just lists of commands and `[` and the like are commands - there's no compiler/parser that tries to figure out what you actually meant before execution - it's just run as-is. I'll offer another way of implementing this: [ -z $myvar ] &amp;&amp; echo "Empty" || echo "Not empty" Not saying that's "better", it's just another way.
&gt; Not saying that's "better", it's just another way. I would say it is strictly worse. If the first `echo` fails, that will execute the second `echo`. (How, I hear you ask, could there be a situation where the first `echo` fails but the second `echo` succeeds? One situation is when the whole lot's standard output is being redirected to a file. It is possible for the filesystem to be full when the first `echo` attempts to flush its output, but for space to become available before the second `echo` flushes its output.)
Not transparent enough, going for at first look transparency.
Sure, I'm not suggesting it's a replacement for a neat `if` statement. 
That happened to be the exact error, you got it right! Thank you very much for the quick reply, I did not realise that '\[' was an actual operator in itself, this explains a lot. I appreciate the detailed explanation. So, is it generally better to use \[\[ instead of just a single \[, given their similarity? 
Let's say you have a file like this $ cat test.tsv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 If you **do not know how many number of columns the file** should have use something like this: $ awk -F'\t' '{print NF,NR}' test.tsv | sort -nsu -k1,1 3 2 4 1 5 5 This prints the number of fields and line numbers and sorts the output based on first column numerical value. It also prints only **unique output** so if all the rows have same number of columns you'll get single line. If you **just want to know if the number of columns are the same** for each row then use: $ awk -F'\t' '{print NF}' test.tsv | sort -nu | wc -l 3 This should give you 1 if all rows have same number of columns or greater than 1 if not. Finally if you want to **remove extra values and add empty values** for rows having more or less columns respectively use $ awk 'BEGIN{FS="\t"; OFS=","} {NF=4; print}' test.tsv 1,2,3,4 5,6,7, 8,9,10,11 12,13,14,15 16,17,18,19 21,22,23,24 &amp;#x200B;
Nice, well explained answer! Upvoted.
[Yes!](http://mywiki.wooledge.org/BashGuide/SpecialCharacters) It's always better to use `[[ expr ]]` instead of `[ expr ]`, unless you're explicitly attempting to be backwards-compatible with the original Bourne shell (/bin/dash in many Linux distros, or /bin/sh in BSD). Recommended resource: see the #bash channel on irc.freenode.net.
See https://mywiki.wooledge.org/BashFAQ/002 Also: - There's no need to put `ls -halt /` in a subshell, `$( ... )` executes it anyway. - `exit_status` in 2 will be interpreted as the string "exit_status" 
``` #!/usr/bin/env bash grep -wf daysfile.csv myothercsv.csv | awk -F: '{ if (f!=$1) print "\n\n"; f=$1; print $0; }' ```
 $ shellcheck myscript Line 5: if ! [$myVar == ""] ^-- SC1035: You need a space after the [ and before the ]. ^-- SC1020: You need a space before the ]. ^-- SC1072: Missing space before ]. Fix any mentioned problems and try again.
that was a typo on my side. I fixed that by adding the '$'. I looked over the link. Some good info.
Here is a very simple example from a function I use to extract archives. -p (preserve) and --no-delete have the same behavior, and elsewhere in the code is a print\_help function. You can see when we come across --, it is like saying option - with $OPTARG of whatever follows. &amp;#x200B; while getopts "p-:" opt; do case "$opt" in p) short\_circuit=" true || ";; \-) case "$OPTARG" in help) print\_help return 0;; no-delete) short\_circuit=" true || ";; \*) &gt;&amp;2 echo "Invalid long option ${OPTARG}. Use --help for a help menu." return 2;; esac;; :) &gt;&amp;2 echo "Invalid Option: $OPTARG requires an arguement. Use --help for a help menu." return 2;; \*) &gt;&amp;2 echo "Invalid Option. Use --help for a help menu." return 2;; esac done shift $((OPTIND-1)) &amp;#x200B;
 cmd_output=$(ls -halt 2&gt;&amp;1) if [ "$?" != 0 ] ; then echo failed else echo worked fi 
Hey, I just hang around [my help multi](/u/oneturnmore/m/linuxhelp) for fun... About 90% of my comments nowadays are support-related. Idk, I like helping people.
Tangentially, when you call it by `./myScript` without providing an argument (as opposed to `./myScript ''`, providing an empty string), you can detect the difference between these: # ${foo+bar} expands to 'bar' if $foo is set, empty string otherwise if [[ -z ${myvar+s} ]]; then echo 'myvar is unset else echo 'myvar is set (may be empty)' fi If you want to detect all three cases (unset/set-empty/set-nonempty), you can do something like this: case $(( ${myvar+1} + ${#myvar} )) in 0) echo 'myvar is unset' ;; 1) echo 'myvar is set and empty' ;; *) echo 'myvar is set and nonempty' ;; esac Here, we use arithmetic expansion to take the length of myvar: `${#myvar}` and add one if myvar is set: `${myvar+1}`. What's great about this method is that it is actual POSIX complient, so it should work on dash and other bourne shells!
Thanks for that. I've noticed your consistently useful input around here and I appreciate it. If I could pester you for a few minutes, what kind of systems do you work with, and for how long? What would you say are the *N* most vital packages in your role?
I'm actually not an admin or anything... I'm a math grad student (undergrad was math+CS), and just use Linux as a daily driver. I discovered i3 years back, started writing scripts to customize it, and it all went "downhill" (by some definition) from there. I've never worked with any systems other than my personal desktop/laptop/pi. I guess if my math (it's a dream of mine to teach college) doesn't pan out for me, I could probably get certified in something pretty easily and land a job in the sysadmin world. This probably wasn't the answer you were expecting, but hey, life has taken me on a weird journey.
&gt; So, is it generally better to use [[ instead of just a single [, given their similarity? I pretty much use `[[` exclusively when I'm writing a Bash script. I'd only use `[` if I were writing a POSIX shell script.
&gt;downhill It really does feel like something spiraling out of control, doesn't it? Good stuff; thanks again. Good luck teaching! 
Thanks! ...also, if I could ask you a question, what's with your flair (gawk_devel)? Just curious.
A dumb joke. `gawk-devel` is GNU Awk 4.2.x (at the moment). Writing to stable `gawk` (currently 4.1.4) is risky enough for portability reasons. The idea of contributing to the zeitgeist using something so obscure makes me giggle. "Oh, my code didn't run on your machine? Didn't you custom build the bleeding edge of a text processor from 1977?"
What are you trying to accomplish? Just off the bat: where are you trying to redirect your output? AFICT it is `stderr` is going to the same place as `stdout`. Normally they'd both be printed to screen, but you are running your command in a subshell. Intentional?
if [[ $( ls -halt &gt; /dev/null 2&gt;&amp;; echo ?!) -eq 0 ]]; then echo ‚Äúcmd worked‚Äú else echo ‚Äúcmd does not work‚Äú fi
Why are you parsing `ls` output? What's your goal? output=$(ls -halt /) || echo Err: $?: "$output"
Try without the square brackets around the `is_connected` command.
Hey that worked, and was an easy fix. Why do the square brackets cause it not to work? I thought they were used for if statements and comparison.
`[` is `test`, `[[` is `new test`. `if` accepts a single command as an argument. This is a common rookie shell mistake, because brackets appear to be part of the syntax, not a separate command. ping -q -c 3 www.archlinux.org
`[` is `test`, `[[` is `new test` `if` accepts a single command as an argument. This is a common rookie shell mistake, because brackets appear to be part of the syntax, not a separate command. The following are both valid. if ping -q -c 3 www.archlinux.org; then echo Success; else echo Failure; fi - ping -qc3 archlinux.org if [[ $? == 1 ]]; then echo Success; else echo Failure; fi
So `[[]]` are literally commands by themselves? Oh wow I just tried `[[ -d ~/ ]]` in a terminal and it took it. I got now output but I also didn't get an error.
Yes. I use them with Short Circuit Operators all the time. Try: #Print only directories in your home dir for file in ~/*; do [[ -d $file ]] &amp;&amp; echo $file; done
It doesn't return `1` or `0`?
&gt;Please note that both of your current if statements are misleading. The first \[\[ is checking to see if the ping result is a string, which it always will be. I thought I was checking if the command completed successfully or not, like using the `$?` but not?
`$?` is the exit status of the last command. 0 is success, but failure doesn't have to be one. $ ping -c1 gobbledegook; echo $? ping: gobbledegook: Name or service not known 2
Nope, try this out: unset foo [[ $foo ]] &amp;&amp; echo \$foo is an empty variable [[ bar ]] &amp;&amp; echo \"bar\" is a string
So $? is the best way to check if the command completed with success? Ah, I see so I should have this: function is_connected { ping -q -c 3 www.archlinux.org if [[ $? -ne 0 ]]; then return 1 else return 0 fi } &amp;#x200B;
Hmm, What is this doing? What's going on with bar?
This is valid but unnecessary. Like I showed you, `ping` will already return an exit status. What's your goal?
So I can do this `if ping -q -c 3 www.archlinux.org; then echo Success; else echo Failure; fi` without all the other stuff, that's good. I'm rewriting my Archlinux install script that I wrote a year ago, and didn't know anything about functions so its almost unreadable. So I'm re-doing the whole thing and make it look much better.
Here's a line that will silently wait for 3 seconds or until one successful ping, whichever comes first, and if no response is received, print an error message and exit your script. ping -c1 -w3 archlinux.org &gt;/dev/null || { echo Err: ping: $? &gt;&amp;2; sleep 3; exit; }
What is `&gt;&amp;2`? &amp;#x200B; And `ping -c1 -w3 archlinux.org &gt;/dev/null` is redirecting the output of ping to the void?
`&gt;&amp;2` is a redirection to file descriptor 2, aka stderr. It's common practice to put information which shouldn't be piped to other programs there (errors, verbose output, debug information, etc). That way you can take your whole script and pipe it somewhere, but still see errors. $ ./yourscript &gt;/dev/null Err: ping: 3 Here, you can see that you still get the error printed to your terminal even though you piped stdout to /dev/null.
You can create this function here: is_connected() { ping -q -c 3 www.archlinux.org &amp;&gt; /dev/null } And then you can use it like this: if is_connected; then echo all good else echo not connected fi
This helped me identify why I couldn't import the TSV from the command line. Awk is a very strange looking syntax, is it basically a multi-line programming language condensed into a single line? 
&gt; Hmm, What is this doing? What's going on with bar? [[ WORD ]] is a shortcut for: [[ -n WORD ]] and this exits successfully if and only if `WORD` is not an empty string. `bar` is not an empty string, which means the left-hand side of the `&amp;&amp;` operator is command that exited successfully. That means Bash executes the right-hand side of the operator.
Ummmm... telnet. Really ?
There's a code checking app (shellcheck) that you can apt: &gt; $ shellcheck myscript Line 1: function is\_connected { \^-- [SC2148](https://github.com/koalaman/shellcheck/wiki/SC2148): Tips depend on target shell and yours is unknown. Add a shebang. Line 4: if \[\[ $(ping -q -c 3 www.archlinux.org) \]\]; \^-- [SC2243](https://github.com/koalaman/shellcheck/wiki/SC2243): Prefer explicit -n to check for output (or run command without \[/\[\[ to check for success). Did you mean: (apply this, apply all SC2243) if \[\[ -n $(ping -q -c 3 www.archlinux.org) \]\]; Line 14: if \[\[ is\_connected \]\]; \^-- [SC2078](https://github.com/koalaman/shellcheck/wiki/SC2078): This expression is constant. Did you forget a $ somewhere? &gt; &gt;$ &amp;#x200B;
For a start is well done, some points to complain. * Sed in MacOs is different sed from linux (GNU sed) can has different results. * For bash Shellcheck help validate against several rules that catch mistakes. * As development recommend break better code blocks for reading (Break lines before/after conditions and loops). I see you use sed+jq to save profiles data is useful in fact. 
* One tip if you run automatically (CRON) is save output from commands in .log file because when things go wrong you can analysis without execute again to see output. Help catch * Curl errors in get file, docs url change. * Mysql timeouts, imports error, database inconsisteny.
This is a really good idea Do you basically just use the "Greater than" symbol at the end of each line? &amp;#x200B; **Such as** command1 &gt; log.txt command2 &gt; log.txt command3 &gt; log.txt 
thanks for the feedback and the tool (shellcheck!)
Thanks! I love your ideas, especially about deleting the TSV &amp;#x200B; What do you mean by period checks? If the cron job is set to only run once a day - how could it possibly double the data?
Yep, the simple way.Other points command1 made &gt; to create .log Others commands need &gt;&gt; to "glue" content. Otherwise every command will clean log.txt and out your content, isnt the goal. All bash script is a transaction, need save together all commands. In some cases i create /logs folder and made: LOG_FILE=$(date +%Y-%m-%d).log command &gt;&gt; LOG_FILE then will exists a logs/2019-03-21.log
Ah, so &gt; is output, and &gt;&gt; is append? Thanks for showing me $(date +%Y-%m-%d). - that's unreal!
It can be very annoying if duplicate data hits your database. If it‚Äôs scheduled with cron it probably won‚Äôt happen but I like to be sure. It also helps with testing to be confident that running the script manually won‚Äôt cause duplication. One way to do it easily: use md5sum to hash the file and make sure it doesn‚Äôt have the same fingerprint as the archived file.
Okay - so you're running a bash script and comparing the md5sum of the new shell file you're about to run compared with the hash of the existing file name? Not sure exactly what you mean
In this case I mean your data files. Check the md5 of the ‚Äúyesterday‚Äù .tsv against the md5 of today‚Äôs. Or use file timestamps, but md5 is more certain.
Got it! thanks! 
Awk was designed to support one-liner programs and so has lots of built-in variables and functions like variables NF, NR etc that automatically records number of fields and records. You can also directly use any variable or array which are initialized automatically. So yes for a newcomer some awk one-liners may look very strange. It helps to remember the basic structure of an awk program `condition {action}` which is applied to every line, default action is to print a line. However you can write very explicit awk programs too (and you should if you are planning to reuse the code) that looks more like a conventional programming language in a separate file and use it as `awk -f script file`.
Thank you for explaining! Damn I wish more docs were written like this. Thanks for explaining NF and NR - i'll check those out. Thanks for explaining condition { action } - weird syntax but I hope I get used to it
I was using the 'ls' command as a test in my script. I will be parsing 'mysqldump', '7za', 'gpg', and 'rsync' commands in the actual script. &amp;#x200B;
My goal is doing a database backup of multiple databases, compress the sql files, encrypt them, move them to a backup server, then cleanup the db server. Since this runs as a service I'm trying to capture the output of the command to a log file and any errors. In some cases an error need to kill the script. In some cases it just need to warn, but continue running the script. I accomplish this by either placing a named ERROR or WARN in the backup folder and have a monitoring system monitor that folder for those two files which will then trigger an alert to me.
Didn't know that, thanks. One of the sed solutions posted (by random\_cynic) is even much shorter, and somewhat intuitive, unlike awk, which I often find very *awk*ward (did you see the pun??????!!!!!!!!!111111).
Linux System Analyst
Gotcha. Dunno what you were using `ls` for (example or otherwise), but it is dangerous to use for any kind of expansion due to how Bash defines a word. You will want to look into the `trap` built-in specifically for your case. On mobile again, so I'll need to come back and edit all my above code and provide some links for `trap`. The `BashGuide` that was linked to by another reply is a super good resource.
tcpdump -nn -v -i eth0 -s 1500 -c 1 'ether[20:2] == 0x2000' Use this to sniff for Cisco Discovery Protocol (cdp) -- it'll tell you what switch your host is connected to.
Ooooo fancy, very clever, thanks for sharing it! Gonna try this one out
`pnmcat -tb &lt;(pnmcat -lr &lt;(djpeg "/Library/Desktop Pictures/Antelope Canyon.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Blue Pond.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Bristle Grass.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10)) &lt;(pnmcat -lr &lt;(djpeg "/Library/Desktop Pictures/Ducks on a Misty Pond.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Earth and Moon.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Floating Leaves.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10)) &lt;(pnmcat -lr &lt;(djpeg "/Library/Desktop Pictures/Grass Blades.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Frog.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10) &lt;(djpeg "/Library/Desktop Pictures/Yosemite.jpg"|pamscale -xyfit 400 400|pnmmargin -black 2|pnmmargin -white 10))|pnmmargin -white 20|cjpeg -optimize -quality 100 &gt;/tmp/thumbnails.jpg &amp;&amp; open /tmp/thumbnails.jpg`
Ungolfed version: #!/bin/bash PICDIR="/Library/Desktop Pictures" PIC1="$PICDIR/Antelope Canyon.jpg" PIC2="$PICDIR/Blue Pond.jpg" PIC3="$PICDIR/Bristle Grass.jpg" PIC4="$PICDIR/Ducks on a Misty Pond.jpg" PIC5="$PICDIR/Earth and Moon.jpg" PIC6="$PICDIR/Floating Leaves.jpg" PIC7="$PICDIR/Grass Blades.jpg" PIC8="$PICDIR/Frog.jpg" PIC9="$PICDIR/Yosemite.jpg" function pic { djpeg "$1" \ | pamscale -xyfit 400 400 \ | pnmmargin -black 2 \ | pnmmargin -white 10 } pnmcat -tb \ &lt;( pnmcat -lr &lt;( pic "$PIC1" ) &lt;( pic "$PIC2" ) &lt;( pic "$PIC3" ) ) \ &lt;( pnmcat -lr &lt;( pic "$PIC4" ) &lt;( pic "$PIC5" ) &lt;( pic "$PIC6" ) ) \ &lt;( pnmcat -lr &lt;( pic "$PIC7" ) &lt;( pic "$PIC8" ) &lt;( pic "$PIC9" ) ) \ | pnmmargin -white 20 \ | cjpeg -optimize -quality 100 \ &gt; /tmp/thumbnails.jpg &amp;&amp; open /tmp/thumbnails.jpg
WTF did I just read.
for file in *.XXX; do mv $file `echo $file |sed 's/XXX/yyy/'`l; done
#!/bin/bash Yeah I don't use bash much
Haha, LOL, sorry about that. I just edited above to add a link to an image showing what the output looks like.
I'm not a fan of complicated one-liners... However I like this relatively simple construct in particular: # 0 if unset, 1 if empty, 2+ if nonempty # this is actually POSIX-compatible too! case $(( ${var+1} + ${#var} )) in
whoa...
 for file in *.XXX; do mv "${file%XXX}"{XXX,yyy}; done
Risky as some file name can contain ‚ÄúXXX‚Äù and I don‚Äôt mean extension part
`declare` those variables you heathen. 
Ah okay wow, very cool! Awesome man :)
That would be all of my video files!
For keeping me sharp on Whatis&lt;commands&gt; Flashcard style in my terminal using cowsay. I put it in .bashrc &amp;#x200B; cowsay -f $(ls /usr/share/cowsay/cows | shuf -n 1 | cut -d. -f1) $(whatis $(ls /bin) 2&gt;/dev/null | shuf -n 1) &amp;#x200B; Looks like this; 3 random &amp;#x200B; [https://imgur.com/a/0HJS5En](https://imgur.com/a/0HJS5En) &amp;#x200B; I use many acsii art that I found, that I added to cowsay. /usr/share/cowsay/cows/ &amp;#x200B;
 /usr/bin/false || exec echo "$_ failed on line $LINENO with return code $? . Program exiting..." Output: /usr/bin/false failed on line 2 with return code 1 . Program exiting... 
This was just a quick example. I'm much more careful when working at the command line.
I know, just the case when some file is named for example somethingXXXsomething.XXX Then you would like to have somethingXXXsomething.YYY but with your command it will be somethingYYYsomething.YYY
Good :) So let it be the hint for others 
Then why are you even here?
Nevermind, there's no need for this to be a one-liner. I'll do it like this: &gt; URLS=(shuf -n 3 ~/Documents/Misc/read-later.txt) &gt; /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome $URLS &gt; unset URLS
That's one you'd better have a fucking comment explaining it if in use... Still question remains of what "empty" means. Empty string? numeric 0? Both? 
 :(){ :|:&amp; };:
An old one liner we use a lot on IRC: #!/bin/sh # The explanation of the following code is left as an exercise to the user __ix () { [ -z "$1" -o -r "$1" ] &amp;&amp; curl -F "f:1=&lt;${1:--}" ix.io || printf '$ %s\n\n%s' "$*" "$("$@")" 2&gt;&amp;1 | __ix } __ix "$@" It's POSIX, too.
declared but not assigned?
Empty string. If `var=0`, then this will expand to 2. | `unset var` | `var=''` | `var=foo` ---|---|----|---- `${var+1}`: 1 if set, empty otherwise | _empty_ | `1` | `1` `${#var}`: length of $var | `0` | `0` | `3` `$((${var+1}+${#var}))` | `$(( + 0 ))`| `$(( 1 + 0 ))` | `$(( 1 + 3 ))` 
Since he didn't explain, just don't do this on your production server...
I'm not near my laptop, tell me what it's used for! It's recursively printing something. What is curl returning? 
`#!/usr/bin/env bash`
test that the functions input exists and is an enabled option name and file and then use curl to submit a form of...mystery...to ix.io. If it doesn't then....I need to brush up on my printf cause that's a confusing one, but whatever it is gets piped into the function again? That's a brain melter for me
Priorities... `sed -i 's/\/\/\s*\(.*\)/\/* \1 *\//g' ./*.c ./*.h`
`shuf -n 3 file.txt | xargs chrome'
Ran it on prod, things started slowing downs an awful lot... /s
I actually did something similar at work. We run our system by the use of a control file which just sets useful variables and actually has its own syntax where * designates a comment. For some reason these files had executable permissions and i keep "." in my PATH. I think you can see where this is going... I went to edit a file and left the vim off the file name and blammo! Whole production server grinded to a halt.
If it's the name of a readable file, it posts it via curl, otherwise it calls itself and posts stdin via the same curl command.
Ain't Mac hatin, but I'd be tempted to write it for imagemagick.
I use ImageMagick's `convert` a lot too. But I'll always have a special place in my heart for the PNM toolchain. :)
One edge case missing, but on point otherwise
I added a "-r" - random cow flag in [my cowsay fork](https://github.com/blitzkraft/cowsay). Cut the length of the oneliner quite a bit. Also makes invoking the random cow easier, when doing it manually. 
POSIX column sniper: $ awp(){ awk "{print $(printf '$%s,' "${@:-1}") null}"; } $ awp 2 4 7 &lt;&lt;&lt; "one two three four five six seven" two four seven An extra line can specify regex to match: awp(){ [[ $1 =~ ^[0-9]+$ ]] || { local m=$1; shift; } awk "/$m/{print $(printf '$%s,' "${@:-1}") null}"; }
Sorry, - sed -i 's,//\s*\(.*\),/* \1 */,g' ./*.[ch] - while sleep 5m; echo no; done &amp; clear
But that's only temporary. 
You can use `sed 's|/path/and/file.suffix|do-stuff|g'` so you don't have to escape / with \. 
It would be more like `somethingYYYsomething.XXX` with his command.
*Hey just noticed..* It's your **2nd Cakeday** bubbobz! ^(hug)
Sorry; I've become one with the golf - sed -i 's,//\s*\(.*\),/* \1 */,g' ./*.[ch] - while sleep 5m; do echo no; done &amp; clear
_WhY aRe YoU eVeN heRe?_ To learn, whenever I get some time away from PHP and Node.
*OwO, what's this?* It's your **7th Cakeday** notthatbigbrother! ^(hug)
hmmm... OP here. I feel slightly... sad about your comment. I knew I was way out of my dept and asked for help. Why dont I deserve it? Because I didn't understand the answer? Or did I now shower him in Gold? Or didn't say thanks enough? I don't know. I am actually very grateful, what more could I do except for figuring it out myself? Now, if you just turned a phrase, yeah, I guess, you are right. :)
How bad can it be? It's just a bunch of smileys
Yeah. I'm just used to doing it in vim. Can i choose any delimiter in vim? So i guess a better version: `sed -i 's|//\s*\(.*\)|/* \1 */|g' ./*.c ./*.h`
I'm a heretic, I only use nano, I can't answer your question :) 
Why do I see only ps1 code which I assume is PowerShell in this repo posted on the r/Bash subreddit?
Just because there is only powershell code doesn't mean that the repo is powershell only. I'm aiming for prompts of al shells - including fish, bash, zsh, powershell, etc. I'm just a noob in bash so I haven't ported any of the prompts to bash yet.
`set -eEuo pipefail` causes bash-like scripts to actually behave like competent programs
Am.. I missing something? What's the difference between the unsatisfactory and satisfactory cases?
The two `xclip` examples look the same.
\+1 for the name awp =D
Sorry! It's fixed now!
Sorry! It's fixed now!
I can see what you want to do with `xclip`, but what is your end goal? If it's to execute a command from the clipboard, then `$(xclip -o)` should do it, no?
Ah, okay. No script on its own will do this, since you want to edit your readline buffer. You'd have to `bind` some shortcut in bash to insert in the output of xclip into your buffer. While I [do have experience in zsh](https://github.com/zsh-vi-more/evil-registers), I don't know how to do this in bash. --- Alternatively, if you want to run xclip on its own, you could do something like [this](https://gitlab.com/xPMo/snippets/blob/master/shell/check-run-clipboard.bash).
xdotool type "$(foo bar)"
 xdotool type "$(xclip -o)"
You probably want to fork, and add a small sleep to make sure the next prompt is ready to accept input: ( sleep 0.2; xdotool type "$(xclip -o)" ) &amp;
I've been using this for 20 years. I love it so much. PS1=\n\e[4m\u@\h:\w\e[24m\n\$ What it does: 1. Turns on underlining 2. Outputs *username*`@`*hostname*`:`*current working directory* 3. Turns off underlining and emits a newline character 4. Displays the normal prompt, e.g., `$` or `#`.
Add "2&gt;&amp;1" after the &gt;&gt;output.txt bit. The &gt;&gt; is just redirecting STOUT so you need to then redirect STERR to STOUT so it catches it all.
Final bit should look like \&gt;&gt; output.txt 2&gt;&amp;1
You also can use `$?` for the exit code of the last thing you ran. 0 means it didn't error, anything else means error. That way your check can be `if [[ $? == 0 ]]; then whatever` if you only want it to happen if it worked.
Also your if statement is really wacky so I just sort of rewrote the whole thing for you: #!/bin/bash while read -r IP &lt; IPList.txt do if snmpwalk -v3 -u USERNAME -l AuthPriv -a SHA -A PASS -x AES -X PASS "$IP" 1.3.6.1.2.1.1.5.0 &gt;&gt; output.txt 2&gt;&amp;1 then echo "$IP failed!" fi done
Forgot to use four spaces for indent to do block code...
If you want to do an if;then;else;fi based on what a command does, don't whatever it is that you thought you were doing, just do the if directly. &amp;#x200B; `if &lt;command&gt;; then echo "Success." ; else echo "Failure." ; fi` &amp;#x200B; `if ! &lt;command&gt; ; then echo "Failure." ; else echo "Success." ; fi`
What are you trying to do with your `if` statement? You appear to be checking whether the ip you‚Äôve read from the file has the value ‚Äòsnmpwalk: Timeout‚Äô, and I don‚Äôt see why you‚Äôd ever have that in the file. I suspect you‚Äôre trying to check the response from the call, but what you have won‚Äôt do that. You are probably better off checking the response code of the call using `$?`
I'm not exactly sure what `snmpwalk` output is in this case, so I will give the solution in terms of a general condition that when satisfied will proceed normally otherwise it will generate an error output. Use while read -r IP ; do { #snmpwalk command if [ "$IP" != "snmpwalk: Timeout" ]; then echo "$IP failed snmpwalk!" fi } done &lt; IPList.txt &amp;&gt;&gt; output.txt Note that this will automatically output whatever snmpwalk is supposed to print to the `output.txt` file.
&gt;do if ! snmpwalk -v3 -u USERNAME -l AuthPriv -a SHA -A PASS -x AES -X PASS "$IP" 1.3.6.1.2.1.1.5.0 &gt;&gt; output.txt 2&gt;&amp;1 then echo "$IP failed!" fi done Thanks so much. I will give it a spin. My If statement wasnt that messed up...I think I moved something in vi by accident. Either way it didnt work. LOL
Works awesome! Thanks so much for the help!!
Your if statement was pretty messed up, my dude. :) You seemed to be under the assumption that the value of `$IP` was going to change based on the output of the command above, which it would not have done. The way to write it where you evaluated the previous command would have been to do if \[\[ $? != 0 \]\]; then fail status ; else success ; fi -- but that's really unnecessary considering you can do it directly as I put in my other comment with if &lt;command&gt; ; then success status ; else fail status ; fi
I didn't know you could read in from the top of the loop!
You rock. 
Cc u/funknut : ix.io is a pastebin site, that works from curl. The form is named `f:1`. The one-liner allows: * To paste the contents of a file (with `-r "$1"` and `f:1=&lt;$1`) * To paste stdin (with `-z "$1"` and `f:1=-`) * If the first argument is not a file, then revert back to executing the arguments `$("$@")`, and add the prompt `printf '$ %s\n\n%s' "input" "output"`, and feed it to the version that handles stdin.
Oh these newlines are just what I've needed. Thanks!
Oh that third was confusing me. Thanks! Cool script.
`tr 'A-Za-z' 'N-ZA-Mn-za-m'` translates A-Z to N-ZA-M (and the same in lowercase). So A would be translated to N, and Z would be translated to M. `tr 'A-Za-z' 'A-MN-Za-mn-z'` translates A-Z to A-Z, so it does nothing...
because `tr` substitutes one for one... so in the first example, taking `A` as the input, it finds `A` at the first character in `A-Za-z` - so returns the first character in `N-ZA-Mn-za-m` - an `N` ... Were you to provide a `C`, it would find that the third position, so would return the third character in the second string - a `P` Now, knowing that, you should be able to explain why your version *does not* work....
Ah, I was misunderstanding how the command works. Thank you
This is super helpful, thank you
Would you mind adding it to this repository? Just fork it and create a PR.
I actually don't have a github account, otherwise I'd be happy to.
I like it. It's the type of stuff I would write myself...
If you're already doing this then might as well ensure that there are no aliases or functions defined and set path manually.
How should I submit a bash prompt? Just create a folder and name the file with '.bash' extension?
[ZSH](https://www.zsh.org/) on [URXVT](http://software.schmorp.de/pkg/rxvt-unicode.html)
Thanks for validating me lmao
Create a folder in the `prompts/` directory with the name of your prompt (no extension) and create a file in that folder as `&lt;prompt&gt;-bash.sh`. If you want, you can also create a README in that directory describing the prompt. When you make a PR with your changes, be sure to include a screenshot.
aliases are only applied for tty‚Äôs, though overriding functions is a possibility. The aforementioned checks are more for logical accuracy. If someone wants to muck everything up with `ln -sf myfunkybash.bin /bin/bash`, there is no amount of effort sufficient to detect all of these potential overrides.
hmmm, I'm not sure but I think that's something like \`gawk\` (GNU version, the \`awk\` you can find in most Linux system) and \`nawk\` (BWK awk, the \`awk\` you can find in BSD and MacOS). You have to take care of escape characters (\`\\t\`) for \`nawk\` as I remember (using the \`$\`), and you don't need it for \`gawk\`.
https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html Scroll down a bit for the flowchart. Yes, it's a huge ridiculous flowchart.
It works though right? Take an upvote.
Is [namei](https://linux.die.net/man/1/namei) what you're looking for?
Functionally exactly. Thank you! So many years of this stuff and still didn't know this. 
Thanks. It works great.
You're welcome! It's one of those utilities that I know exist but take forever to remember the name. It's the accepted answer to ["How to list symbolic link chains?"](https://serverfault.com/a/333545) on StackOverflow.
Wow you actually back your stuff up? Looks nice, and I'm sure it looks nicer with the colors Reddit isn't showing me.
Maybe Borgbackup and Borgmatic could help ... A lot
Upvote for borg which I think is great software, commenting because I recently started using timeshift and I LOVE it. If you're using a DE it's worth a look. 
Looks great, have an upvote :) Over engineering is great for building up momentum and accumulate experience, it does however get real tiresome soon after that. [This](https://github.com/michaeltd/dots/tree/master/dot.files/sbin) is what I do for my use case if you feel like comparing notes :)
My first script I wrote in the last century. Not yet tired of it. The script just does an rsync. That is done on the "backup" server. I have another server where data like movies is backed up again and that will also handle incremential backups for "non data" stuff.
The first does a LOT I do not need and the second is in python that I do not know, so can not adept to what I want. I also would miss all the fun in writing it myself. ;-)
The colors are on,y visible when I enable debugging. And this is just making a copy. A copy is not a backup. A second server will do incremental backups, except for data (e.g. music and movies) where it just takes a copy again. So I will have backups of my backups. And yes, I have needed that already.
That's one way, another is to use single quotes instead of double quotes: '$basearch' Nothing within single quotes is ever expanded.
Did you know that you can declare a function in bash and then call that recursively? You could do that instead of calling the entire script itself recursively. Might be faster and would create a lot fewer processes.
Thanks for that! One small nice thing about zsh is that `.zshenv` is *always* sourced, interactive or not. ---- Happy cake day!
$bpath isn't set.
Was going to suggest right off the bat to use backslash. Backslash is key to troubleshooting any monkey bidness with printing.
&amp;#x200B; $0 shows the name the script was called as. If entry of the path to the script was not needed and not included (eg, perhaps the script was in a directory that is part of the users $PATH) then it would show as entered. This is often useful when you write a script, and use symlinks to call it under multiple names, and have it perform a different function depending on the name it was called by - $0 will enable you to see what name was used. &amp;#x200B;
Yeah, it's not just the name. That `$0` will contain the path that bash followed and used to start the program. If it's a program that is in one of the locations listed in `$PATH` and you start the program with just its name, then `$0` will be the entry in `$PATH` where bash found the program plus `/` and its filename. When you do `./name` then bash won't look in `$PATH`. You will get the `./` added to `$0`. If you want just the name of your script, you can cut off the path like this: scriptname=${0##*/} # good for #!/bin/bash scripts or like this: scriptname=$(basename "$0") # good for #!/bin/sh scripts
yup. I'm not a bash user -- zsh -- and I found that it was faster as a /bin/sh script than as a zsh function.. I was curious about that but haven't dug into it yet. bash was a little faster than zsh, but namei makes it moot from a performance standpoint. thanks for the pointer!
I think you've got your answer, but here's something else you should be aware of: $ ln -s d2.sh other-path $ ./other-path ./other-path If you want to dereference the symlinks, you have to do that yourself: path="$(realpath "$0")" # dereference name="$(basename "$path")" # get filename echo "$0 :: $path :: $name" This will output: ./other-path :: /path/to/d2.sh :: d2.sh
I use subversion to be honest. Generic stuff goes in the trunk system stuff goes in branches. I try to keep everything up to date. I'll admit it's still painful to keep track of though.
Yeah, I was wondering if git might be useful for easily importing new snippets from branches. Seems like a giant hassle, though, and one more thing to manage.
I tried the singular bash script for a while though. It was a nuisance too because I'd make changes from one machine then overwrite them from another
This looks interesting: [https://github.com/Bash-it/bash-it](https://github.com/Bash-it/bash-it) It sounds ind of like a github for bash snippets. I was wondering if something like this might exist. Anyone ever use it?
May not answer your question, but it should be cleaner to replace `for f in $(ls file*)` with `for f in file*`.
I have a repository for dotfiles(for bash: functions, aliases, exports) and a repository for scripts. The bash specific files are in a folder .bash.d which gets installed with a wrapper script using stow to install under ~/.local/bash.d as well as .bashrc in ~. In .bashrc I only source every file in bash.d with while IFS= read -r -d $'\0' file; do source_file "$file" done &lt; &lt;(find -L "$1" -type f -not -iname *.swp -print0 | LC_COLLATE=C sort -dz) All files have a prefix so I can still make sure files are sourced in specific orders if I need to override or source dependencies for something.
When you write `$(ls)`, bash takes the output of `ls` and applies "word splitting" on it. It looks for space, tab and new-line characters to split the text into "words". After this is done, it then works on your 'for' loop and will iterate over the words that were produced by the earlier word-splitting. What's very important to know about this is, the 'for' loop you wrote will not work for filenames with spaces in them. You cannot use `$(ls)` and a 'for' loop to iterate over a list of files if there are spaces in the names. See for example the following: $ touch 'file '{a,b,c} $ ls file a file b file c $ for f in $(ls); do echo "$f"; done file a file b file c You can try to disable word-splitting by adding `"` quotes, but this will break things in a different way. With the `"` quotes, bash will treat the whole output of `ls` as a single word. The `$f` variable will be set to that single word and the loop will run just once: $ for f in "$(ls)"; do echo "contents of \$f: &lt;$f&gt;"; done contents of $f: &lt;file a file b file c&gt; What you should do is use bash's own wildcard file search to create your list of words instead of calling `ls`. It looks like this: shopt -s nullglob # make it so wildcards can produce an # empty list if there are no matches for f in file*; do echo "$f" done
You can test it yourself: for f in $(ls file*); do echo "input: $f" done But you should avoid piping ls in any case: $ touch foo 'bar baz' $ ls 'bar baz' foo $ for f in $(ls *); do echo "input: $f"; done input: bar input: baz input: foo $ for f in "$(ls *)"; do echo "input: $f"; done input: bar baz foo $ for f in *; do echo "input: $f"; done input: bar baz input: foo
It reads the output as an array with n elements where n=1*files, so f is not substituted by "ls file*" but rather each loop with one of each elements linearly. You can test that by replacing echo statement with FLS+=${f} and after the for loop echo ${FLS[@]} to print its contents.
This looks promising. I'm not familiar with "stow" though. I'll have to read up on that. Thanks for sharing!
I don't know how advisable this is, but I typically add . to my PATH so I can call scripts by name when I'm in the directory which eliminates the '. /' in $0. I suspect a symlink placed in an existing PATH directory will do the same. I think the behavior is probably dictated by what gets passed to the shell, $0 is the **first token** passed, not the filename.
Hi üôÇ I think what you want to know is how the `forloop` takes each item for every cycle. This has to do with the `IFS` (Internal Field Separator) which separates the output of `$(ls file*)` into multiple items, then the `forloop` do its thing. Normally the value for the `IFS` are space, tabs, and newline. You can test it by setting the `IFS` variable to the newline character ( `IFS='\n' ` ) before your loop and it should echo lines instead of words. üòâ PD: Sorry I cannot provide code examples, since I'm on my phone. üòÖ
So you know, the reason `.` isn't in your path by default is because anybody with access to the machine (other users, etc) can make a program called `ls` or `cd` and you'll end up running it inadvertently, potentially with elevated permissions. Even if you're careful and always put `.` at the end, that won't stop "typosquatting" the common misspellings like `sl`.
Luckily I run with encrypted boot and am the only one that gets physical access to my machines.
And assuming you never have to use any other machines in the future, that seems worth saving 2 characters.
On mobile so I can't run it. But first $choice is a local variable to "read_options" so it won't be available for comparison in the global scope. Also the "while" probably fails the first time because $choice isn't initialized before use. Bash is a bit more weird to pass back values from. For your purpose you could use return values from the function , $? in global scope in the while loop and make a pre-condition check call to read_options.
Hey. In your third line you are "calling" $var. $var value us 3780.70 and that's what bash tries to execute as a command and obviously fails :)
Makes a lot of sense, the solution would be to echo it? i'm really new to this :c
If you want to print the value of a variable yeah you have to echo it. `echo $var` Are you storing that value in a variable for reuse later ? Because if your script is just what you are showing there is no need to store it and you can just execute the command.
Noo, that was exactly what i was looking for thanks üòÅ. I just realised i was trying to run the output .. and yes it's meant for another use in a small script. Thank you
sort -n assumes the first field is what you are asking to be sorted. You need to tell it which field has the numbers. I think you want `sort -n -f 2`, but I don‚Äôt have the manual on hand. 
&gt;sort -n -t: -k2 man, if i could kiss you
&gt; while read line; do printf '%s\n' "$line" &gt; [[ $line =~ "switchport mode" ]] &amp;&amp; cat &lt;&lt;-EOF &gt; switchport port-security maximum 3 &gt; switchport port-security violation protect &gt; ip arp inspection limit rate 100 &gt; auto qos voip cisco-phone &gt; storm-control broadcast level bps 4m 3m &gt; storm-control multicast level bps 5m 3m &gt; no shutdown &gt; EOF &gt; done &lt;input.txt &gt;output.txt I know it's been 18 days, but I came back across this and wanted to play around with there heredoc statement. Just FYI, it chokes with whitespace at `EOF` for some reason. The example which works with my version of bash (`GNU bash, version 4.3.48(1)-release (x86_64-pc-linux-gnu)`) works with no leading whitespace at the last `EOF`.
I dont think so and there isnt really a purpose, your pi would already be on the same network as the server it is hosting.
How's this? Tested on macOS. for user in $(last -10 | cut -d ' ' -f 1 | uniq) ; do id -F $user; done &amp;#x200B;
Any spaces in that whitespace? `&lt;&lt;-` will ignore leading tabs only.
Tested with spaces only. I'm a fan of multiple spaces for indentation.
Good for you
I'm confused. I'm trying to be friendly, and I'm shit at interpreting tone over the internet.
Hi confused, I'm dad!
Fuck off, bot
Sorting first means you don‚Äôt know if you‚Äôre actually getting the last 10 unique users. You‚Äôre just getting the 10 users from `last` at the end of the alphabet. I kind of like the idea of just piping into `uniq` (no sorting). Sure, you may get repeats in the final list, but I feel that‚Äôs valuable information about the access pattern (without polluting the last 10 with the same repeated user).
What do you want the check function to do, exactly? You say it doesn't seem to work at all - what does that mean? Does it echo nothing? Does it not check the right thing? Right now, it will only check if there is a directory at the location `/home/test/Downloads/$sitename/` and only if `"$sitename"` doesn't have any spaces, tabs or newline characters in it. To better test if that folder exists, when using single backet test cases, make sure to quote your variables to prevent word splitting. (in case of a sitename with spaces, for example) **if** **\[** \-d **/**home**/**test**/**Downloads**/"**$sitename"**/** **\]; then** Alternatively, since you're using bash, use double bracket test cases. \[\[ is a special bash keyword which doesn't have to adhere to normal shell expansions prior to running the test, and doesn't word split your input variables. [More info on the \[\[ keyword here](https://wiki-dev.bash-hackers.org/syntax/ccmd/conditional_expression#word_splitting). Unless you're trying to be compatible with other shells, it's almost always better to use double-bracket test cases rather than single bracket test. Other than that, it seems fine to me, tho I would suggest using rsync (maybe as an added line after the scp) as it's got a lot more options to check if the files copied over succesfully. A more rigorous check at the end of the copy could even check for the amount of files, and/or the size of the folder, and give you feedback if there are differences in source and destination. &amp;#x200B; If you're still learning bash scripting, a good tool for checking errors is [Shellcheck](https://www.shellcheck.net/) \- it'll show you the syntax errors in your scripts. Also, I think it's awesome that you two are learning scripting together, and this is a great little project to start with! 
I'm a web developer and have only recently become enamored with bash scripting and automating workflows--it's so awesome! Gahh why didn't I do this sooner? lol 
Thanks appreciate the help, I'll go through this comment and check my script later today working currently.
&gt;There's a 5 minute tutorial for that [**right here in this repo**](https://github.com/CorradoRossi/roll-your-own-wp-cli/blob/master). That is a 404 error message. I assume still set for private view. Nice wordpress what I seen of it so far. Nicely setup at your github account. Fix the link and I'll see anything else needs to be attended to. Thanks for sharing this. &amp;#x200B;
Hey thanks for the headsup! Just pushed a fix for that. Appreciate the kind words!
You can do this with `awk` $ last | awk '!name[$1]++{first=$1; $1=""; &gt; if (first != "") { &gt; "getent passwd " first " | cut -d: -f5" | getline fname ; &gt; print fname,$0 }}' Brief explanation a) `!name[$1]++` makes sure only unique entries are stored in the array name and skips a line if the name is same. b) The command stores the first field containing username in variable `first` and sets it to `""` as it will not be printed. c) `getent` is a bash command that searches the output of `passwd` and the `cut` command gets the full name. This bash command as a string is passed to `getline` which is the awk way of running external command. The output is put in `fname` d) Finally we print the full name and the other fields.
Try quoting what you want to echo. `complete` is a shell builtin, and your script might be mucking up over that? `echo "Backup complete."` will prevent your shell from thinking that you're trying to do `echo "Backup" &amp;&amp; complete .` (even tho I'm not sure bash can do that. But just to be safe)
Some decent tips here, nothing overtly awful! One change I'd recommend: #6: : &gt; application.log Easier way to empty a file.
`&gt; application.log` I saved you a character. You are welcome :)
That is bash specific, in Zsh it will open `application.log` for writing until you pass a `&lt;ctrl-d&gt;`. If we're about saving bytes, get rid of the spaces too... :P
I'd look in the bash manpage for `$BASH_VERSINFO` - a readonly array variable whose members hold version information for this instance of bash.
Check the environment variables BASH_VERSINFO (an array), and BASH_VERSION (a regular variable). See `man bash` for more information on these variables.
Cool. So I could write my own "require" something like this: function require_bash_version() { if [[ ${BASH_VERSINFO[0]} -lt $1 ]] || [[ [[ ${BASH_VERSINFO[0]} -eq $1 ]] &amp;&amp; \ [[ ${BASH_VERSINFO[1]} -lt $2 ]] ]]; then printf "Bash version %d.%d is less than the required %d.%d\n" \ ${BASH_VERSINFO[0]} ${BASH_VERSINFO[1]} $1 $2 &gt;/dev/stderr exit 1 fi } require_bash_version 4 3 &amp;#x200B;
Cool. Thanks. Any thoughts on the shebang line?
I would probably go about it in the following way. The spaces are important, especially around `(` and `)`. #!/bin/bash require_version() { local bash_major=${BASH_VERSINFO[0]} local bash_minor=${BASH_VERSINFO[1]} if [[ ${bash_major} -lt ${1} || ( ${bash_major} -eq ${1} &amp;&amp; ${bash_minor} -lt ${2} ) ]]; then printf 'Bash version %d.%d is less than the required %d.%d.\n' "${bash_major}" "${bash_minor}" "${1}" "${2}" return 1 fi return 0 } require_version 4 3 Note that I have used `return` instead `exit`. The latter exits the shell process (i.e. your shell script will halt) - use it if not having the required version is a hard error in your case. BASH_VERSINFO will apply to the bash in the shebang line.
&gt;The latter exits the shell process (i.e. your shell script will halt) Ya, either way is useful. I wanted a hard error (exit script) because I'd like to treat it as an assertion. That's is how "require" works in Perl5 and I always liked that behavior.
Solved it myself. I am so proud of me. :-D * 1-23/4 * * * script.sh
Don't forget sudo rm -r /*
Impressive for bash script. I rolled my own with my own Perl module just a couple of weeks ago. Though I integrated it with wp-cli. I was able to figure out how to extend wp-cli with it. It's nice. So I can quickly create a new wp-cli command and do all kinds of fancy things. It's also integrated with WWW::Mecahnize::Chrome. So, for example, if I want to remove a soliloquy slideshow from the page I'm currently on, I can do \`wp sol disable\` and, poof!, it's gone from the page. I don't even have to refresh the browser, it's all automatic.
&gt; /opt/local/bin/bash5 ‚Üí /opt/local/bin/bash I would keep the bash5 name, would not bother with symlink and use straight shebang of #!/opt/local/bin/bash5 - assuming that you won't be distributing said scripts (or make sure that you have that bash5 binary available in that path). Idea is - if I need something special, I'll be explicit about that, so let's use local bash5. 
I'm setting this up now. How does the `$PATH\_BASH\_CONFIG` variable get set?
It will start at 4am
This guy manages all his dotfiles this way using stow: https://github.com/alexpearce/dotfiles His tutorial: https://alexpearce.me/2016/02/managing-dotfiles-with-stow/
Ok, so just filling in the missing pieces here. I have this directory structure in a git repo: ``` .dotfiles/bash ‚îú‚îÄ‚îÄ .bashrc ‚îî‚îÄ‚îÄ .local ‚îî‚îÄ‚îÄ bash.d ‚îú‚îÄ‚îÄ aliases ‚îÇ ‚îî‚îÄ‚îÄ 00_color ‚îú‚îÄ‚îÄ completion ‚îú‚îÄ‚îÄ exports ‚îú‚îÄ‚îÄ options ‚îú‚îÄ‚îÄ prompt ‚îî‚îÄ‚îÄ utils ‚îî‚îÄ‚îÄ 00_mdcd .dotfiles/install_bash.sh ``` You can see two config files: `00_color` and `00_mdcd`. The last file there, the install script, is: ``` #!/bin/sh cp bash/.bashrc . sed -i 's@$PATH_BASH_CONFIG@'"$HOME"'@' .bashrc mv .bashrc $HOME stow --ignore='\.bashrc' bash ``` This script modifies the `.bashrc` file to replace the `$PATH_BASH_CONFIG` variable, then plops it into `$HOME`. Stow then generates a symlink to the `.local` directory but ignores the `.bashrc` file.
I'm thinking I probably want the `--no-folding` option with stow. That way I can remove or add custom config files for a particular machine without messing with the git repo.
Why don't you tell us what you're actually trying to do? There might be a better way than debugging your script.
1. trying to learn bash scripting 2. wanted to write a script to install linux onto USB via debootstrap
On this line: debsums deborphan debsecan\ you have spaces after the backslash so Bash thinks you're quoting the space instead of doing a line continuation.
You need to provide a [Short, Self Contained, Correct, Example](http://sscce.org/), also known as a [Minimal, Complete and Verifiable Example](https://stackoverflow.com/help/mcve). We're not going to debug a 132-line script, especially when it's not something we can safely run ourselves.
ah! okay - wow thanks!
Your `$i` appears inside a double-quoted string, so it is being expanded _before_ the `bash` command is executed.
It's being expanded before it gets passed to the subshell. You could hack this together with *eval*, but I've been shamed so hard for it that I avoid it as much as possible. Why not just use parens as your command group to run the whole function in a subshell? foo() ( for i; do echo "$@ $i"; done ) $ foo 10 20 30 40 10 20 30 40 10 10 20 30 40 20 10 20 30 40 30 10 20 30 40 40
&gt; Why not just use parens as your command group to run the whole function in a subshell? To add to this, if the only reason for a subshell (or a separate `bash` process) was to prevent changes to `i` from affecting other parts of the script, you could just localize `i`: i=42 foo() { local i for i; do echo "$@ $i"; done } foo 10 20 30 40 # output as above echo "$i" # outputs 42 
thanks, guess that's what they mean by *bash is a command processing language* your thinking's got to change considerably #!/bin/bash x="10 20 30 40" function foo { x="$1" bash -c '{ x="$1" echo "Echo" $x; for i in $x; do echo $x $i; done; }' junk "$1" } foo "$x" 
Why must you hurt me so?
umm.. heh.. :p that's where I read it first but I'm guessing they hang out here as well :)
I feel duty-bound to say that invoking `bash` in this way is utterly awful. Maybe it'd be best if you described the problem you're trying to solve, rather than coming to us with a non-working solution and asking us how to fix it? As /u/HenryDavidCursory showed, there's rarely a need to ever tokenize strings in this way at all.
I'm nearly certain they mean you're ignoring their pretty worthwhile response.
That's less a 'trick' and more a "just because you can, doesn't mean you should". Not quite sure what the problem is you are trying to solve, but I'm pretty sure this isn't the solution.
I'm trying to iterate a list of packages and install them: chroot ${CHROOT_DIR} /bin/bash -c '{ for pkg; do apt-get -qy install $pkg; done }' junk_text $pkgs_to_install It's related to my earlier post https://np.reddit.com/r/bash/comments/b5lkvl/binbash_line_x_mtr_command_not_found_when_using/ People were complaining that I needed to reduce the code to its essence hence this reduced post. In my earlier post I was using \ to make the pkg list more readable, then I decided to ditch the \ and go with a global list.
I'm trying to iterate a list of packages and install them: chroot ${CHROOT_DIR} /bin/bash -c '{ for pkg; do apt-get -qy install $pkg; done }' junk_text $pkgs_to_install It's related to my earlier post https://np.reddit.com/r/bash/comments/b5lkvl/binbash_line_x_mtr_command_not_found_when_using/ People were complaining that I needed to reduce the code to its essence hence this reduced post. In my earlier post I was using \ to make the pkg list more readable, then I decided to ditch the \ and go with a global list and for loop.
totally bash unrelated but I'm curious why not just do `chroot ${CHROOT_DIR} apt-get -qy install $pkgs_to_install`
length limit of the cmd line
The double quotes in `foo "$x"` prevents the parameter expansion `$x` from getting word-split. So, the value of `x` is passed into `$1` in its entirety as a single word.
No, it doesn't. I It would run every minute from 01:00 to 01:59 and then from 05:00 till 05:59. Obviously that is not what I want. So what I have is. 11 1-23/4 * * * script.sh I also found [crontab.guru](https://crontab.guru/#11_1-23/4_*_*_*) that also confirms this. And I get emails from the system that say "Started on Tue 26 Mar 09:11:01 CET 2019" as well as all the other times I want it to run. When I select a time, I will try to avoid whole hours, as several things already have that as a time and just running suddenly a lot at the same time is not the greatest idea.
Can't test. Something wrong with the HEREDOC. Also, it looks like the function should be called \`\`\`helixDemo()\`\`\` instead of \`\`\`helix()\`\`\`
That needs to be a tab leading the closing `EOF`.
You need to set it or export it beforehand. I do ¬¥export PATH_BASH_CONFIG="$HOME/local/config/.bash.d/"¬¥ 
Yeah, it was there. Nevermind what I said about ```helixDemo()```. Still can't make it work. Does nothing, returns nothing. echo $? returns a 0.
here is how the bash package for stow looks like for me: . ‚îú‚îÄ‚îÄ .bash_logout ‚îú‚îÄ‚îÄ .bash_profile ‚îú‚îÄ‚îÄ .bashrc ‚îú‚îÄ‚îÄ .inputrc ‚îî‚îÄ‚îÄ .local ‚îî‚îÄ‚îÄ config ‚îî‚îÄ‚îÄ bash.d ‚îú‚îÄ‚îÄ aliases ‚îÇ ‚îú‚îÄ‚îÄ 10-admin ‚îÇ ‚îú‚îÄ‚îÄ 10-aliases ‚îÇ ‚îú‚îÄ‚îÄ 10-apt ‚îÇ ‚îú‚îÄ‚îÄ 10-audio ‚îÇ ‚îú‚îÄ‚îÄ 10-databases ‚îÇ ‚îú‚îÄ‚îÄ 10-dependant ‚îÇ ‚îú‚îÄ‚îÄ 10-development ‚îÇ ‚îú‚îÄ‚îÄ 10-editing ‚îÇ ‚îú‚îÄ‚îÄ 10-encryption ‚îÇ ‚îú‚îÄ‚îÄ 10-filesystem ‚îÇ ‚îú‚îÄ‚îÄ 10-git ‚îÇ ‚îú‚îÄ‚îÄ 10-interface ‚îÇ ‚îú‚îÄ‚îÄ 10-network ‚îÇ ‚îú‚îÄ‚îÄ 10-pacman ‚îÇ ‚îú‚îÄ‚îÄ 10-printer ‚îÇ ‚îú‚îÄ‚îÄ 10-time ‚îÇ ‚îú‚îÄ‚îÄ 10-virtualization ‚îÇ ‚îî‚îÄ‚îÄ 99-overrides ‚îú‚îÄ‚îÄ completion ‚îÇ ‚îî‚îÄ‚îÄ 10-git.bash ‚îú‚îÄ‚îÄ exports ‚îÇ ‚îú‚îÄ‚îÄ 10-bash_shell_variables ‚îÇ ‚îú‚îÄ‚îÄ 10-custom_exports ‚îÇ ‚îú‚îÄ‚îÄ 10-dev ‚îÇ ‚îú‚îÄ‚îÄ 10-exports ‚îÇ ‚îú‚îÄ‚îÄ 10-pass ‚îÇ ‚îú‚îÄ‚îÄ 10-xdg ‚îÇ ‚îî‚îÄ‚îÄ 99-path ‚îú‚îÄ‚îÄ options ‚îú‚îÄ‚îÄ prompt ‚îî‚îÄ‚îÄ utils ‚îú‚îÄ‚îÄ 00-dependencies.sh ‚îú‚îÄ‚îÄ 10-admin.sh ‚îú‚îÄ‚îÄ 10-development.sh ‚îú‚îÄ‚îÄ 10-filesystem_find.sh ‚îú‚îÄ‚îÄ 10-fzf.sh ‚îú‚îÄ‚îÄ 10-git.sh ‚îú‚îÄ‚îÄ 10-graphic.sh ‚îú‚îÄ‚îÄ 10-network.sh ‚îú‚îÄ‚îÄ 10-pass.sh ‚îú‚îÄ‚îÄ 10-saltstack.sh ‚îú‚îÄ‚îÄ 10-ssh.sh ‚îú‚îÄ‚îÄ 10-tmux.sh ‚îú‚îÄ‚îÄ 10-utilities.sh ‚îî‚îÄ‚îÄ 10-virtualization.sh
OK, thanks. I see the prefix works differently than I though. It looks like it is a kind of weight. Do customize which files load for different machines? So, for example, if I'm on a Machine A with git installed, I'll want to load the 10-git.bash file. But if I'm on Machine B, I wouldn't want to load this file if it doesn't have git installed..
I just tried with curl and it worked perfectly for me. Doesn't error 403 mean your are forbidden from accessing it?
I don't understand why tho, it works over Firefox but won't let me do it through bash. 
yes I do for example for 10-pacman this at the top of the file so it will not be loaded on specific distributions. if ! { cat /etc/issue | grep -q "^Arch Linux" \ &amp;&amp; hash pacman 2&gt;&amp;1 | logger -t bashrc -p user.info; }; then return 1 fi 
Ah, OK. That makes a lot of sense to put it right into the bash script. I was thinking some kind of install script would handle that logic. Cool! I think I got it now.
the prefix is I can sort by lexicographic order
Yeah, I was giving each file it's own unique number starting with 01 in each directory. But seeing your prefixes, I see there is no need to do that for files where it doesn't matter which loads first.
well as far as i see you need to do the following. loop over the target file line by line. if line matches the word to replace then fill it eith value 1 and increment a counter. on next occurence check counter and pick value based on counter. so quick : read token file. get all 9 tokens. assign them to an array, set counter to 1 and then read target line by line. if line contains (regex) call function. then call that function that replaces word by the current counter (so 1 would be token 1 and 3 would be token 3). after replace increment counter by 1 and continue foreach loop.
Perhaps there is a limit on the requests.
Though the same but I'm trying it over the same IP but over browser works and over shell doesn't. Only explanation is they process browser request in a different way maybe.
[Works for me](https://i.imgur.com/Fn3dBbH.png). 
ubuntu 18.10, bash 4.4.19, under gnome terminal and putty terminal, both to no avail.
are you sourcing the script and then running `helixDemo`?
The construct you'll want to use will go like this: select dir in '/some/directory/'*/; do [[ -d $dir ]] &amp;&amp; break echo "Invalid option $REPLY" done do_something --with "$dir"
Why doesn't this work if I type it out in the terminal line by line and hit enter (using a mac os terminal) \`\`\` read email http="https://haveibeenpwned.com/api/v2/breachedaccount/$email" curl -s -A "pwnedornot" $http &gt; pwnd.txt cat pwnd.txt \`\`\` &amp;#x200B; Im assuming its something to do with not using backticks when concatenating email to the end of that http string. I want to start making functional stuff so help us out pls &lt;3
This seems like a good challenge, haven't been able to find bash katas 
Yes, I'm doing exactly that. Switched to a different user and I get an infinite loop of errors: bash: printf: xxx.yy: invalid number (xxx.yy being some numbers from around 80 to 254)
I just read this question in another post, where are you learning bash from?
I'm piecing it together from various places. 
So add the browser identifier. You can tell curl to use the one that Firefox usaes. Not at a PC where I can look that up, so you are on your own.
&gt; bash: printf: xxx.yy: invalid number Weird. Googling points to the issue possibly being with the `LC_NUMERIC` environment variable [[1](https://stackoverflow.com/questions/12845997/unexplicable-error-in-bash-printf-command-inside-a-script-returns-invalid-numb),[2](https://stackoverflow.com/questions/44565980/printf-error-invalid-number?noredirect=1&amp;lq=1)], or maybe leading zeroes [[3](https://stackoverflow.com/questions/8078167/bizarre-issue-with-printf-in-bash-script09-and-08-are-invalid-numbers-07/11804275)]. I'm on ubuntu 17.10, bash 4.4.12. 
Even I'm having a hard time with bash, everything is so different and there is no step wise challenge, there are good ones on codewars but all are advanced.
Yeah, I am getting the results I need but I am not sure I am actually learning too much in the way that bash actually works. Although I did find the solution to my problem, I will edit above. 
Yeah, it's that. It doesn't work if I don't set ```LC_NUMERIC=C```first.
Done it already. I tried different user-agents and got the same result.
I tried the whole command but got the same issue as well as using different user-agents and wget. What the heck :S
Have you tried restarting the process from TTY5?
There is no shell running there. I just want it to print there.
I am positive there is some beautiful looking solution using awk. However, I can't tell you what it is, as I am not good enough at awk to use it without having the man page open.
Yes, I second this. Some bash scripting challenges would be hella fun!
I get blocked also. Found a bash script which is listed on the site and also does not work. Does not work if I paste in browser. I suspect aggressive blocking is going on.
Doesn't make any sense at all. I tried a python script and worked, what the hell? Might worth try with a VPN...
This doesn't edit the file in place. It creates a new file (which you can specify in the variables section). It also does a bit of sanity checking to make sure your # of tokens match up the number of instances you want to replace in your config (the assumption here is your tokens file doesn't have extra lines, etc. The script is safe to re-run since it will empty out whatever $new\_config is if you try to re-run it. #!/usr/bin/env bash ############################### # setting variables at the top config_file="config.xml" tokens_file="tokens.txt" word_to_replace="collab" new_config="new_config.xml" ############################### # sanity check on both files to make sure the number of tokens matches # the instances of the word to replace before we even begin any action tokens=$(wc -l $tokens_file | awk '{print $1}') collab_lines=$(grep -c $word_to_replace $config_file) if [[ $tokens -ne $collab_lines ]]; then printf "The number of lines/tokens in %s does not " "$tokens_file" printf "match the instances of the word \%s\" in " "$word_to_replace" printf "%s\n\n" "$config_file" printf "Number of tokens/lines: %s\n" "$tokens" printf "Lines with the instance of \"%s\": %s\n" "$word_to_replace", "$collab_lines" fi # empty new config file (in case you are re-running this script and it has # the old information in it cat /dev/null &gt; $new_config # declare array declare -a tokenarray slot=0 # populating array from token file while read line; do tokenarray[$slot]=$line ((slot++)) done &lt; $tokens_file # start replacement slot=0 while read line; do echo "$line" | grep -q $word_to_replace if [ $? -eq 0 ]; then echo $line | sed "s/$word_to_replace/${tokenarray[$slot]}/g" &gt;&gt; $new_config ((slot++)) else echo "$line" &gt;&gt; $new_config fi done &lt; $config_file &amp;#x200B;
What about [exercism](https://exercism.io) ?
Ah looks like I misunderstood your question. You‚Äôre not always going to have ‚Äúcollab‚Äù in there and you need a way to overwrite whatever pre-existing tokens are there.‚Äù
Looks interesting, I see they have Bash as one of the languages. It says "Sign up for free", but the fact that your code is "Reviewed by a mentor" would imply to me that the courses themselves are not free. Do you know how much they cost?
Last I checked, it was completely free. 
Last I used it, it was completely free. 
https://www.codingame.com has plenty of challenges to keep you busy if your goal is just practice writing (bash) code.
Sweet, I'll sign up, thanks! I also use Codecademy, which is free, but offers paid extras (code review, extra exercises, etc).
Thanks! I'll give this a try.
Man, that's a lot of lines you're adding to the script for the sole purpose of making the user spend extra effort to type "Edit 01" instead of "edit01"
Yeah, but I wouldn‚Äôt under estimate the stupidity of the users here. I actually got it all working and whilst the script may be a bit longer than necessary it‚Äôs now fairly fool proof. It needed to work with little to no instruction so whilst the script is fairly complicated, the experience from a user standpoint is super simple. I‚Äôve even had to script the opening of putty, logging in via ssh and firing the script. All so they don‚Äôt mess something up. 
That's your prompt and yes [you can customise it](https://www.ostechnix.com/hide-modify-usernamelocalhost-part-terminal/)
Not a basic tutorial but an useful resource for bash scripting at every level. [Advanced Bash-Scripting Guide](https://www.tldp.org/LDP/abs/html/)
[https://www.tldp.org/LDP/Bash-Beginners-Guide/html/](https://www.tldp.org/LDP/Bash-Beginners-Guide/html/)
[https://www.tldp.org/LDP/Bash-Beginners-Guide/html/](https://www.tldp.org/LDP/Bash-Beginners-Guide/html/)
I'd recommend `rsync` with passwordless entry via ssh keys, too. (OP can read up on `ssh-keygen` and `~/.ssh/authorized_keys` if unsure how to configure this.)
OP, just take into consideration this was last revised in 2008. It's a really good resource for starting off, but certainly investigate alternative ways some things are produced.
I don't have a recommendation. I just wanted to add this: Focus on text. Bash is pretty powerful, you could use almost any set of programming challenges that does not focus on data structures. The place where bash is most powerful is text , which makes sense because linux/unix is defined by text. I would say... learn awk, grep, sed, regex. Try to look at bash as a text parser and go as far as you can. I would look for text parsing activities and try to do them with bash.
copying resources mentioned in community wiki: **Resources** * [BashGuide](http://mywiki.wooledge.org/BashGuide) ‚Äì A Bash guide for beginners. * [Beginner's Guide to Command Line](http://cli.learncodethehardway.org/book/) ‚Äì A crash course for some common unix and shell commands. * [Google's Shell Style Guide](https://google.github.io/styleguide/shell.xml) ‚Äì Reasonable advice about code style. * [Sobell's Book](http://www.sobell.com/CR3/index.html) &amp;#x200B; **Other Resources** * [ShellCheck](https://www.shellcheck.net/) ‚Äì Automatically detects problems with shell scripts. * [BashFAQ](http://mywiki.wooledge.org/BashFAQ) ‚Äì Answers most of your questions. * [BashPitfalls](http://mywiki.wooledge.org/BashPitfalls) ‚Äì Lists the common pitfalls beginners fall into, and how to avoid them. * [The Bash-Hackers Wiki](http://wiki.bash-hackers.org/) ‚Äì Extensive resource. * #bash ‚Äì IRC channel on freenode. The main contributors of the BashGuide, BashFAQ, BashPitfalls and ShellCheck hang around there.
This needs to be the top comment.
What do you need help with? You didn‚Äôt ask a question. You just gave us your assignment and said your lost. Start with the first exercise and think about what it‚Äôs asking you to do. You need to read user input and store it into a variable before doing anything else. How do you do that? You‚Äôre going to have to break this down into smaller problems and research those issues one step at a time and then build your script up from there. It‚Äôs an iterative process. And if you‚Äôre stuck, ask about specifically what you‚Äôre stuck on. Otherwise, people are just going to link you to general bash scripting tutorials. Here‚Äôs a page that teaches you how to read user input and store it in a variable (a concept you probably need to understand to complete a lot of the steps in your assignment): https://ryanstutorials.net/bash-scripting-tutorial/bash-input.php
Couldn't you do ```#!/usr/bin/env bash function do_all { files=$@ for file in $files; do do_something $file done } do_all $@``` or simply `for file in $(ls -A); do your_script.sh $file; done` ?
Saved. Thanks!
You could do [these](http://rosalind.info/problems/list-view/) in Bash..
Avoid the bash guides from tldp.org. They are outdated and teach misinformation and bad practices. There's a reason they are not in the sidebar.
Have you tried the [bash scripting bible](https://www.amazon.com/Linux-Command-Shell-Scripting-Bible/dp/111898384X/ref=sr_1_fkmr0_1?keywords=bash+scripting+bible&amp;qid=1553671235&amp;s=gateway&amp;sr=8-1-fkmr0)?
The GNU Bash Manual is a really excellent resource. The only problem is that you really have to pay attention to all the details in the documentation.
The last revision of the excellent book by Cooper available on line for free is from 2014 not 2008 !!! One of the best ressources on Bash in my opinion.
One of the best ressources about Bash methinks. Revised in 2014.
No point in spawning a subshell and running ls. You can glob for files directly in the for loop like so: for file in ./*; do your_script.sh $file; done
Didn't know that actually, thanks
I concur with this. I'm learning this the hard way and it's frustrating as hell. I have been learning bash but with a lot of trial and error and frustration. But I'm slowly picking it up. And as far as syntax goes, I cut my teeth on perl but bash is even more arcane, so there's that barrier, too. I still struggle with when to use double/single quote and when to use curly/regular parens, where the $ has to go, etc. Just got to keep grinding it out. Sooner or later a light will go off if you use bash enough.
You can print it in the most horrible shell scripts record!
also how `set -u` would've helped...
http://catb.org/%7Eesr/faqs/smart-questions.html#bespecific
There is a more fundamental issue here, why are they letting a 'dev' who doesn't know BASH well develop directly in prod?
set -euo pipefail Whenever possible. 
Sometimes I wish I could write scripts in Haskell (language Shellcheck is written in) and compile into Bash.
He must have copied and pasted that script from somewhere. The '//' should have been a glaring indicator that the script needed editing and testing. Or you know, write your own script. Of all the things to automate, database backups are probably the easiest.
They're agile :P
thank you so much &amp;#x200B;
LOL, we actually use a variant of agile where I work. Testing and code review are part of the process so this issue wouldn't have happened. Actually, any semblance of a proper development lifecycle would have caught these errors.
I dug through my 872342 browser tabs to find you this: https://github.com/antonlindstrom/passpwn/blob/master/passpwn I've had that sitting in an idle browser tab for probably a month now... the plan being to build a stripped down rewrite. You might like to try it out, or reference it...
[Things can get tricky with `-e` and pipefail](http://mywiki.wooledge.org/BashFAQ/105), leading to inconsistent and non-intuitive behavior. When I started having more trouble writing scripts with `-e` than without, I stopped using it. [More info](/r/bash/comments/8asn1e/trying_to_understand_a_script_to_delete_all_but/dx1y785/).
Paging /u/GoodNello, it's been 10 months, [I'm still waiting](https://www.reddit.com/r/bash/comments/8k2m0j/project_ideas/dz4xj6c/) ;)
[Disruptive](https://zippy.gfycat.com/IdleZestyAtlasmoth.webm)!
ShellCheck is awesome, this article doesn't give justice just how much it can provide. I use it for all my shell scripts now. Occasionally I have to mark a line with `#disable shellcheck=[code]` if I want some other behavior. The disable markings are a good way to indicate those behaviors too. Most of the time [it's me wanting to do word splitting](https://github.com/koalaman/shellcheck/wiki/SC2086) or [I want to pass a literal `$foo`](https://github.com/koalaman/shellcheck/wiki/SC2016). 
`man bash`, `/^PROMPTING`
I use not only ShellCheck, but linting programs everywhere I can. You can set up Vim or whatever you use for doing it automatically, and there are a bunch of linters, from shell scripting, vim scripting, any programming languages, CSS, JSON, yaml, everything! I've discovered linting does a lot of things for me: * Linting tools teach you new tricks. They make you aware of dangerous practices. When you use linting, you literally become a better programmer. * Linting saves you a lot of time. Never again going to a terminal and running your script only to be greeted with a syntax error, then going back to the editor to try and find it. With linting you just prevent all this overhead from happening. * Linting encourages cleaner, and most importantly consistent code. * And of course last but not least, it prevents bugs. So what's not to like? For me linting is like VCS, I won't do anything serious without it. It just doesn't feel right.
I think this is also a case of folks imagining that _knowing Bash well_ is a bullet proof defense against some of its shortcomings. More established languages make it a lot easier to be a mediocre programmer but still prove that ur program will work under various scenarios and corner cases. As it becomes harder for a shell script to scale (to the needs of a growing team, or against more important or varied requirements), it should be promoted to a program in a more complete language. Look...I'm a die hard Bash fan but I know that there's a point where it makes sense to level it up.
Definitely will look into. Would be nice to merge both into one single script.
Doesn't github's API limit the amount of requests you can make, at least that's what I recall from trying to use it? Would probably be better to just parse the site for repos instead of using the API. There are some minor and some less minor issues that shellcheck found. http://ix.io/1EFD 
The substitutions and expansions are what make Bash wonderful. I wouldn't trade them for something else.
Absolutely. I have shellcheck linked into vim with [syntastic](https://github.com/vim-syntastic/syntastic). It essentially turns vim into a bash IDE. This is nice because a true bash IDE doesn't really exist. There is no compilation, so you have to run to test. I won't run any of my code until shellcheck approves. The disable feature is also nice for a false positive or a well vetted alternative. I have been trying to get my co-workers to embrace it with no luck. Whenever i open a co-workers file, i need to turn syntastic off. The thing will light up with like 40 warnings/errors.
I think there are some good suggestions such as set -eu - whenever I would do a rm -rf $VAR/\* I would always use variable expansion and set var to a dummy value, e.g. rm -rf ${VAR:-NULL}/\* - not pretty but a nice safeguard w/out using set -eu - of course as another comment said, the experience level was probably not there to know how to do this :-)
Use `$()` instead of backticks. Use `getopts` to parse your commandline. `mkdir -p` won't create the directory if it already exists so you don't need to test for it's existence first. Don't print your usage unless the user asks for it (with -h) or makes some mistake on the command line. I recommend using `set -o errexit; set -o nounset; set -o pipefail` to stop your script from muddling on when it has some mistake.
 traphandler () { echo "trap received" exit 1234 } trap traphandler SIGHUP SIGINT SIGTERM
&gt; have shellcheck linked into vim with [syntastic](https://github.com/vim-syntastic/syntastic) Yep, same here. I've actually been getting into code golf recently[,](https://codegolf.stackexchange.com/users/86147/gammafunction) and knowing unusual behavior can be an advantage.
If the call to `slurp` is itself within a function (say `barf`), then you can define `barf` before `slurp` as long as `slurp` is defined before `barf` is executed.
&gt; Obviously I have to define slurp beforehand.. which looks ugly.. What's ugly about it? Write clearly. If you're bothered by functions pushing the meat of your code to the bottom, declare a function `main()` as your first function, then call `main` at the bottom of your script. https://google.github.io/styleguide/shell.xml?showone=main#main
If you only intend to call `slurp` once, you could inline it: $ ( echo hi; cat; echo bye ) &lt;&lt;HERE &gt; one &gt; two &gt; three &gt; HERE hi one two three bye
okay, so function barf { slurp var_name &lt;&lt;HERE big text chunk HERE function slurp { ... } } barf; nice, thanks! &amp;#x200B;
I‚Äôm pretty sure that the definition of slurp needs to be outside (and after) the definition of barf.
Bash doesn't have function definitions in the C sense. What it has is function commands that assign a name when executed. That means that this is totally valid: if [ "$dryrun" ] then run() { echo "$*"; } else run() { "$@"; } fi run ls -l Since functions can be defined conditionally and redefined several times just like variables, it makes no sense to say that a function is defined later.
&gt; Obviously I have to define slurp beforehand.. which looks ugly.. However it is somewhat safer. When writing shell scripts I usually put everything inside functions, ending with: main "$@" to actually start things off. This has several advantages. First, it means the _entire file_ is parsed before anything important is executed. If there's any syntax errors nothing will be executed. Second, it means functions can be marked readonly as you go: foo() { ... } readonly -f foo This protects against accidentally copy-pasting a function definition and blowing away its previous definition. Third, it means the functions can be defined in any order. If `main` goes at the top of the script, the program's logic is still clear.
&gt; does it read line by line and only setup the signal catcher when it parses the 'trap' line ? Yes. &gt; so from the moment vash is started until it read the 'trap' line , it has it's default handler ? Yes. &gt; and if a SIGTERM arrived during that period it will be deal with the default handler ? Yes. 
Check they way PS1 is formatted. For instance you forgot \\ after 00;33m.
You can also use `{ ...; }` instead of `( ... )` to group a set of commands: { echo hi; cat; echo bye; } &lt;&lt;HERE one two HERE The difference is that `(` starts a "sub-shell" while `{` is part of the main process of the script. Inside `(` you have a separate environment for variables or current directory, while inside `{` you are working in the environment of the rest of your script. A function is more like `{` than `(` except for `$@` changing.
 `'\[\e[01;127m\]\u\[\e[00;33m]@\[\e[00;34m\]\h\[\e[00;37m\]: \W\$\[\e[00m\] '` I guess you forgot a \\ after ;33m
The definition for "slurp" can be inside "barf", but you have to put it in front of where you call it: function barf { function slurp { ... } slurp var_name &lt;&lt;HERE big text chunk HERE } barf
Thank you! That did the trick! :)
Just for the sake of my curiosity could you please expand a little bit more what's the purpose of the tool?
You could use `dialog`. Here is a quick demo. #!/bin/bash exec 3&gt;&amp;1 tool=$(dialog --menu "What tool do you wan to use?" 0 0 0 1 grep 2 file 2&gt;&amp;1 1&gt;&amp;3) case "$tool" in (1) # grep regex=$(dialog --inputbox "What to search for?" 0 0 2&gt;&amp;1 1&gt;&amp;3) file=$(dialog --inputbox "Which file to search?" 0 0 2&gt;&amp;1 1&gt;&amp;3) grep -f - -- "$file" &lt;&lt;&lt; "$regex" ;; (2) # file file=$(dialog --inputbox "Which file to identify?" 0 0 2&gt;&amp;1 1&gt;&amp;3) file -- "$file" ;; esac 
Just wondering what specific problem you're seeking to solve here. ie. What is a thing that would become easier with this framework? What things would become possible with this new framework?
thank you! Can you please throw a link to official docs of dialog?
day to day tasks like archiving or unzipping a file, git push, etc.
okay. Noob to shell scripting. I assumed using a framework guides me to follow best practices.
It will take me about the same time to find them as it would for you.
So you basically you want to make a tool that has shortcuts to everyday task. I'd suggest using `case` and arguments combined with functions. I use alias/functions for these kind of things such as: # Update bash_profile function rf() { clear printf "\n${Green}Profile updated.\n" source $HOME/.bash_profile rm $HOME/.bash_profile~ &gt; /dev/null 2&gt;&amp;1 } &amp;#x200B; # Create a bash file function msh() { if [ "$1" ]; then if [ ! -f "$1.sh" ]; then printf "${Green}Creating ${NC}$1\n" echo "#!/usr/local/bin/bash" &gt; $1 echo "# " &gt;&gt; $1 chmod +x $1 o $1 else printf "${Green}File already exist.\n" fi else printf "${Green}Give me a filename.\n" fi } &amp;#x200B; # Find a file with a pattern in name function ff() { find . -type f -iname '*'"$*"'*' -ls ; } # Creates an archive (*.tar.gz) from given directory. function maketar() { tar cvzf "${1%%/}.tar.gz" "${1%%/}/"; } # Create a ZIP archive of a file or folder. function makezip() { zip -r "${1%%/}.zip" "$1" ; } # Make your directories and files access rights sane. function sanitize() { chmod -R u=rwX,g=rX,o= "$@" ;} I also created a 'bin' folder inside the home directory containing all the scripts I made which I go thought using: #!/usr/local/bin/bash # Shows this help for f in ~/bin/* ; do printf "$Green$(echo $f | cut -f 5 -d '/')\t\t$NC" if [[ "$(head -n 2 $f | tail -n 1 | cut -c1-1)" == "#" ]]; then printf "$Yellow$(head -n 2 $f | tail -n 1)$NC\n" else printf "$Red$# (No description)$NC\n" fi done Output: [ 0x00 ~ ] h colors # Color test db # Database manager down # Disconnect from Wi-Fi network duck # DuckDuckGo search geo # Show public ip and location getsite # Download website h # Shows this help h4sh # Generate different hashes hscope # Horoscope from freewillastrology.com img # Open an image with preview macgen # Generate a MAC address ms # Macshifter ninfo # Network information ns # Speed Test pt # Return the pattern of file/folder and copy it to the clipboard pwned # ---------UNDER CONSTRUCTION---------- qrcode # Generate a QR Code using the qrserver's API rsa # RSA key generator scanner # Subnet scanner to retrieve IP and MAC of active hosts str2rev # Print a string in different ways up # Connect to a Wi-Fi network wifi # Wifi Tool xtract # Extract a file yt # Youtube search Very flexible.
I work full time on a framework called ArcShell ([https://github.com/arclogicsoftware/arcshell](https://github.com/arclogicsoftware/arcshell)). It may be something you can put to use. Happy to work with you on any of the problems you would like to solve. Frameworks do make things easier but you have to spend the time learning the framework first. I am work full time on the project. Fixes, new features, and docs every week. You can follow the project there if you want to keep updated.
Interesting! Thanks for the follow-up! Needless to say you're not a fan of 'unofficial strict mode?' I'll dig into it now- Are you the creator of shellcheck, btw? It's really helped me a lot, so thanks again if so.
Why use some crappy web service for QR codes when we have qrencode?
I made while ago for fun and didn't dug into a better service. Downloading it right now, thank you for that (:
You should first look for a local tool.
Depends on the purpose tho and I'm pretty sure that was a script that started as an exercise but never used, so...
For almost any purpose tho...
Not really. For instance qrencode has to be compiled or installed trough a packet manager, a simple script works straight up on every shells. As I said it just depends on the purpose \^\^
But using the service you need an internet connection whenever you use it and they get every string you want to encode.
But that is the purpose for which was made in the first place, to use an online service.
 [https://askubuntu.com/questions/344407/how-to-read-complete-line-in-for-loop-with-spaces](https://askubuntu.com/questions/344407/how-to-read-complete-line-in-for-loop-with-spaces) 
So this can test Powershell scripts? And how does it compare to shellcheck for Bash scripts? 
Use 'read'. Here's a function you can use, I use it for reading configuration files for my bash scripts: readCSV() { csvLine=() while cvsLine= read -r line do csvLine+=("$line") IFS=',' read -a curLine &lt;&lt;&lt; "$line" echo "${curLine[0]} is before the comma, ${curLine[0]} is after" done &lt; "$1" exit } Then you can use this function in a bash script like so: #!/bin/bash . csvFunction.sh readCSV /my/path/mydata.csv Or you can read an input stream and parse it: readCSV &lt; $myDataStream Hope this helps a bit. This makes it reusable and scalable for however many fields you have in your csv files. Just use $curLine[x] for each field.
I think your easiest way is to write a script that reads a line of your text file, and then calls ffmpeg, with that line. I think this would be done with the 'read' command, but I'm not entirely sure.
I'm not a fan, but more because people use it without knowing what it does or what can go wrong when using it. Read what they do and answer for yourself whether that makes sense as default behavior. Feel free to test different implementations with or without it. &gt; Are you the creator of shellcheck, btw? I wish, I barely know any Haskell.
Hello! I think I maybe was a bit unclear in my first post (have updated it now). &amp;#x200B; I would like the script to loop thru the csv and send an separate email for each entry. see my example if it makes things clearer. &amp;#x200B; Example script.sh: &amp;#x200B; line1=$(awk -F"," 'NR==2 {print $1 " " $2}' results.csv) line2=$(awk -F"," 'NR==3 {print $1 " " $2}' results.csv) line3=$(awk -F"," 'NR==4 {print $1 " " $2}' results.csv) line4=$(awk -F"," 'NR==5 {print $1 " " $2}' results.csv) line5=$(awk -F"," 'NR==6 {print $1 " " $2}' results.csv) line6=$(awk -F"," 'NR==7 {print $1 " " $2}' results.csv) line7=$(awk -F"," 'NR==8 {print $1 " " $2}' results.csv) line8=$(awk -F"," 'NR==9 {print $1 " " $2}' results.csv) line9=$(awk -F"," 'NR==10 {print $1 " " $2}' results.csv) &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; /usr/local/bin/sendmail [testuser@test.com](mailto:testuser@test.com) "Alert! - $line1 is locked" &amp;#x200B;
Hello! I think I maybe was a bit unclear in my first post (have updated it now). &amp;#x200B; I would like the script to loop thru the csv and send an separate email for each entry. see my example if it makes things clearer. &amp;#x200B; Example script.sh: &amp;#x200B; line1=$(awk -F"," 'NR==2 {print $1 " " $2}' results.csv) line2=$(awk -F"," 'NR==3 {print $1 " " $2}' results.csv) line3=$(awk -F"," 'NR==4 {print $1 " " $2}' results.csv) line4=$(awk -F"," 'NR==5 {print $1 " " $2}' results.csv) line5=$(awk -F"," 'NR==6 {print $1 " " $2}' results.csv) line6=$(awk -F"," 'NR==7 {print $1 " " $2}' results.csv) line7=$(awk -F"," 'NR==8 {print $1 " " $2}' results.csv) line8=$(awk -F"," 'NR==9 {print $1 " " $2}' results.csv) line9=$(awk -F"," 'NR==10 {print $1 " " $2}' results.csv) &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; /usr/local/bin/sendmail [testuser@test.com](mailto:testuser@test.com) "Alert! - $line1 is locked" &amp;#x200B;
Are the email addresses in the csv ? In your examples the address seem to be constant. &amp;#x200B; email_address="$1" file="$2" IFS=$'\n' while line=read -r line; do sendmail "$email_address" "$line" done &lt; "$file" &amp;#x200B;
No, the email is constant so its only the name ($1) and username ($2) that the csv contains.
Ok, I got it &amp;#x200B; `#!/bin/bash` `filename="URL-List"` `n=1` `while read line; do` `ffmpeg -hide_banner -i "$line" -c:v copy -c:a copy "Video $n.mkv"` `n=$((n+1))` `done &lt; $filename` &amp;#x200B; Output from ffmpeg is video file with sequential number: Video 1.mkv, Video 2.mkv etc... &amp;#x200B; &amp;#x200B;
 for line in $(cat data.csv); do /usr/local/bin/sendmail [testuser@test.com](mailto:testuser@test.com) "Alert! - $line1 is locked"; done
You could create a small shell script to serve as a wrapper for your current ffmpeg calls. This script, for example, finds the next available video-XX filename and uses that as the output file: #!/usr/bin/env bash if [ "$1" == "" ]; then echo "Usage: $(basename "$0") &lt;url&gt;" exit 1 fi in_url="$1" num=0 outfile="video-$(printf -- '%02d' "$num").mp4" while [ -f "$outfile" ]; do echo "- $outfile exists..." (( ++num )) outfile="video-$(printf -- '%02d' "$num").mp4" done echo "+ $outfile is available" cmd="ffmpeg -hide_banner -i \"$in_url\" -c:v copy - c:a copy -bsf:a aac_adtstoasc \"$outfile\"" echo "+ Running command : $cmd" $($cmd) if [ ! -e "$outfile" ]; then echo "+ Error: output file \"$outfile\" was NOT created" exit 1 fi Save the above as "copy-video" and make it executable: $ chmod 755 copy-video Now, with a few "existing" video-XX files in the folder for demo purposes: $ ls video*.mp4 video-00.mp4 video-01.mp4 video-02.mp4 ... call the script, passing it a URL: $ ./copy-video https://www.test123.com - video-00.mp4 exists... - video-01.mp4 exists... - video-02.mp4 exists... + video-03.mp4 is available + Running command : ffmpeg -hide_banner -i "https://www.test123.com" -c:v copy - c:a copy -bsf:a aac_adtstoasc "video-03.mp4" ./copy-video: line 20: ffmpeg: command not found + Error: output file "video-03.mp4" was NOT created *It failed because I don't have ffmpeg on this machine, but you can see the command it wanted to run.* Then just invoke using your current semicolon approach, or as desired.. $ ./copy-video https://www.test123.com; ./copy-video https://www.test456.com; ./copy-video https://www.test789.com; (Robust error handling sold separately.)
neat. If you get real fancy with it, you could probably do something like, put intended filenames, with a comma, then the video link, in your text file. Then, split the line of input, at the comma, and pass the first half, to the name variable. then your text files would look like filename,youtube.com/watch?=whatever Food for thought at this point.
Promise it is not homework but for work :) thought that I had pretty good bash skills but this proves me wrong :) &amp;#x200B; I'll try it right away
Testing software involves running all or part of it on certain inputs and confirming that the side effects (usually output) are expected. This situation way OPs program seems to be doing. shellcheck does something slightly different called static analysis, or linting. Instead of running all or part of your program, it'll simply read it and try to figure out if there are any problems in it. These problems are usually not guarantees the program will fail, but inferences that it may fail if written as it is, or maintained from the current state.
 mapfile Lines &lt; &lt;(awk -F, '{print $1,$2}' results.csv) for l in "${Lines[@]}"; do sendmail testuser@test.com "Alert! - $l is locked"; done
Great! both examples works great! &amp;#x200B; also i added &lt; &lt;(tail -n +2 results.csv) to make it start reading from line nr2 (since i do not want the headers etc.) &amp;#x200B; Newbie question, Pure bash is faster than using AWK since it is native or? &amp;#x200B;
\`c += 1\` is not valid bash syntax. Try \`c=$(( c + 1 ))\`, or \`let c++\`.
Simply escape the brackets to match with a backslash; brackets match any character within them in regex, and thus can't be left unmatched unless escaped to match a bracket character. Basically: sed 's/, TL\] \[TIME_LOGS (84265876)\]/]/g' *.log
Wow, this works perfectly. Thank you.
It always takes time to start up external processes. You should try to avoid that whenever possible. Here are some quick mods to avoid tail: Awk: `'NR&gt;1 {print $1,$2}'` Bash: `do ((++record &gt; 1)) &amp;&amp; sendmail ...` I would appreciate your vote.
Thanks a lot mate! always something new to learn :)
You're correct. GitHub's API has a 60 requests per hour rate limit for unauthenticated users, which is actually good enough for me. 60 requests x 100 repos per response = 6K repos. That's enough to handle any user. For example, sindresorhus has \~1K repos, google \~1,5K, Apache \~1,7, Mozilla \~2k, microsoft \~2,3K... &amp;#x200B; &gt;Would probably be better to just parse the site for repos instead of using the API. Nice idea, I'll try it when it becomes necessary. &amp;#x200B; Thanks.
Wow. All those points are really amazing, I'll start using them... Super thanks. &amp;#x200B; I have one question though: Aren't backticks okay if I am not nesting commands? (backticks look cleaner, in my opinion.)
Bash script aside, FFmpeg has [it's own scaling filters](https://ffmpeg.org/ffmpeg-filters.html#scale-1) and a whole bunch of denoise, sharpen and blur filters. You could do this all in one ffmpeg command, without having to extract audio, export each frame separately, and re-encode the frame sequence. I see that waifu2x is meant to upscale things and make 'm pretty, that it's used for specific kinds of footage, but I'm sure you can do this in a ffmpeg filter string as well, messing a bit with scaling, denoising and sharpening. &amp;#x200B; As for the Bash part of things; \- quote your variables to prevent word splitting over spaces and newline characters - you'll run into trouble if there is a filename with a space in there if you're using straight up $1 as an input. It'd be cleaner to do `inputfile="$1"` and then use `ffmpeg -i "$inputfile"` \- that will also allow you do some error handling or cleaning up on your inputs, and will make your end ffmpeg command a little more legible. \- Testing in Bash is better handled with `[[` test cases, as it's a Bash keyword and has a few more options than the `test` or `[` command, while also not running into weird shell expansions. If you do want to use `[` or `test` to be POSIX compliant, again, quote your variables; i.e. `if [ "$1" == "test" ]]; then ... fi` \- Personally, I'd test against no variables being input in the first place. `if [[ -z ${1+x} ]]` or `if [[ ! ${1} ]]` test if someone actually input any arguments at all. After that, you can test if the first one is 'help', too. Right now, the user needs to just know that inputting 'help' as the first argument will spit out this usage line. \- Another way to go about this is reading inputs line by line - something like read inputfile echo "Input file set to $inputfile" read outputdir echo "Output directory set to $outputdir" read outputfile echo "Output file set to $outputfile" read framerate echo "Frame rate set to $framerate" That way the user can just run UpscaleVideo.sh and it'll ask the user to input these options rather than them having to know the correct order etc. &amp;#x200B; \- The even better way to go about this is building in options using [getopts](https://stackoverflow.com/questions/16483119/an-example-of-how-to-use-getopts-in-bash) \- read up on those :) World of difference &amp;#x200B; Hope this helped
The reason I want to use [waifu2x](https://github.com/nagadomi/waifu2x) is that it was specifically designed to upscale anime and art using AI, it doesn't just upscale it recreates it in a higher res but it only work on images. And thanks for the actual bash help, I'll fix it up.
&gt; That's enough to handle any user. What about when you want to sync multiple users?
If you have dialog installed, the official tools can be found using &amp;#x200B; \`\`\` man dialog \`\`\` 
What does `exec 3&gt;&amp;1` do?
Copies the stdout to file descriptor 3. https://wiki-dev.bash-hackers.org/howto/redirection_tutorial
Thanks I will read that! I have a script approaching 700 lines now, and I have quite a few redirects, I need to read this 
${variable/match/replace} is a more general form of what's happening. It's a regex that lets you rewrite the contents of the variable before using it. For more options, see the the Advanced Bash Scripting Guide: https://www.tldp.org/LDP/abs/html/string-manipulation.html
From the info page of tty: &gt; 'tty' prints the file name of the terminal connected to its standard input. It prints 'not a tty' if standard input is not a terminal. So sometimes `$(tty)` is detecting that stdin is not a terminal (which usually happens when stdin is a pipe) and now your redirect looks like: &gt;"not a tty" So you write to a file called "not a tty".
What are some typical situations? Is for example when doing something in a subshell or something?
Also see [this](https://mywiki.wooledge.org/BashFAQ/001#trimming) url on why you should almost always be using -r in your while loops. i.e; while read -r fname; do instead of just 'while read fname'
`${}` is not brace expansion it is a [parameter substitution](https://www.tldp.org/LDP/abs/html/parameter-substitution.html#PARAMSUBREF) operation. If you a see a $ before any of the other operators like braces, parentheses, brackets it generally indicates some form of substitution could parameter, arrays, command, arithmetic etc. The actual operation depends on the context.
I see... You're right again. Well, though I personally won't face that problem, I guess I will rewrite my script just in case somebody else wants to use it.
Ofc you could ask the user if he wants to use a web service instead if qrencode is not available. Or it could offer to install it if on a distro with supported package manager.
The most common example is what I hinted at before, when stdin is a pipe. For example: $ tty /dev/pts/3 $ echo "test" | tty not a tty There are probably other examples, where a program closes stdin and opens another file descriptor there but that's unlikely to happen in self-written bash scripts.
You could ask the user if he wants to use a web service instead if qrencode is not available. Or it could offer to install it if on a distro with supported package manager.
now that is a description I can look for! Thank you Though I sat down trying to rid the script of pipes, and where possible just use builtins. So that may be why I have not seen that for a while as well. But I am sure there is one occurrence left at least.
Beg your pardon but I feel like what you saying doesn't make sense and this conversation it's a waste of time. We are talking about a script, not a piece of software and the whole purpose for the post was to help out a user finding an inspiration for a tool he wanted to make. Now you are talking about implementing a function in my private script that no one will ever use. For real? Are you trolling or what?
It does work but it's hella slow.
Or `((c++))`.
Thanks!!! I found the faulty line but it was not a pipe. I don't know why its not working. https://github.com/sigboe/pie-galaxy/blob/f6584af4ad6ba66efb7553892eba0206c7e091bd/pie-galaxy.sh#L130
Thank you! :)
Cheers buddy, that makes sense now :)
$() executes the grep command immediately not during if. $() is also for capturing output not return status. Move grep to the if. Also, consider Ansible for this type of thing.
Let me [shellcheck](https://www.shellcheck.net) that for you: Line 12: if [[ !"$LINE_EXISTS" ]]; ^-- SC1035: You are missing a required space here.
[ABS](https://www.tldp.org/LDP/abs/html/) already been mentioned, I'll just attribute it as my one stop shop, all in one, goto place. With that out of the way all other (free) resources to my knowledge can be found [here](https://github.com/EbookFoundation/free-programming-books/blob/master/free-programming-books.md#bash).
There needs to be a space between the variable and not operator. Or you can put it outside like `! [[ "$LINE_EXISTS" ]]` or use a specific operator for the purpose like `[[ -z "$LINE_EXISTS" ]]`. IMO the best option is to combine the if with grep like below (if you **do not need to store the search result** returned from `grep`). if ! grep -q "options hid_xpadneo" "$CONF_FILE" then echo "options hid_xpadneo" &gt;&gt; "$CONF_FILE" fi In above if checks the exit status of the grep command and if it does not find the string it executes the code otherwise does nothing. The `-q` option tells grep to not return the match explicitly.
`( timeout 60 bigprogram &amp;&amp; echo "" )| tr _ \\012 &gt; outfile`
Shell check huh? Nice tool, thanks.
Oh I see, and because this isn't a test case you don't need the `[[]]`? And the short version, so I don't even need the if statement, and can accomplish this on one line? Cool! Thank you.
What is Ansible, I looked it up it seems like some sort of automation software. Thanks for the reply!
Yep, you need to add \\033, so the number of characters is the same. See this: [https://stackoverflow.com/questions/342093/ps1-line-wrapping-with-colours-problem](https://stackoverflow.com/questions/342093/ps1-line-wrapping-with-colours-problem) &amp;#x200B;
I am already closing the thing and `\e` and `\033` are the same arent they ? 
i also tried adding \] at the end of everything which was totally useless the result is still the same
In bash the `if` statement are a little different from traditional if statements in the sense that it checks for exit codes of a program not an actual logical value. `[[..]]` is just another version of the `test` command which returns an exit code 0 if an expression is true or 1 if false. Since `grep` returns similar exit code depending on whether it was able to find a pattern or not, this enables one to use `grep` directly on `if` bypassing `test`. The one-liner is common among shell programmers particularly when you have a single statement to execute if the condition is true (or false).
as per darkciti's adive did you try like: PS1='\w \[\e[31m\]&gt;\[\e[32m\]&gt;\[\e[33m\]&gt;\[\e[0m\]'
Oh yes it works now thanks
That's got it. I had tried: timeout 60 ( bigprogram ) | tr ... but it gave me a syntax error. Thanks a bunch! 
Any particular reason for chopping the leading `./` with sed? The `mv` should work just fine on the filepath as `find` outputs it...
Please read links people provide for your benefit... &gt; And for those too lazy to do a diff, terminal escapes should be surrounded by `\[` `\]` in order for bash to be able to correctly count printing characters to the beginning of the line
I was just being extra safe I guess. But you're right, it's unnecessary.
Safe in what way? I think it actually makes it more brittle, as you have more moving parts. 
I would recommend not using ansible for this or anything else for that matter.
from your c++ code I guess you only need `read` which by default will stop reading after receiving a `\n` if you want something else, you may tell us what is your original need 
Ok then bash is already fine. C/C++ don't work like that, you have to specify all (I mean, not for every input I have to insert, but to eventually clear the buffer). Soz, for the question then, I'm a newbie with bash :)
If they're anything like me when I'm learning new stuff, then they have no good reason at all, but they don't realize it until later or until someone tells them.
putting a file name along with url is not something I want. because the video stream that I took has one title in common, it's just divided into hundreds of chapters. it makes no sense if there are hundreds of video urls that must be entered along with urls into text files. instead, i can add a few lines of code in the script to make video file names have sequential numbering. &amp;#x200B; finally after trial and error by myself: #!/bin/bash read -p 'Open text files: ' urltextfiles if [ ! -f "$vartextfile" ]; then echo echo File not found!; echo exit 2; fi read -p 'Save Video Title as: ' saveas LISTURL="$urltextfiles" TITLE="$saveas" NUMSTART=1 NUMOFLINK=$(wc -l &lt; $LISTURL) echo echo There are $NUMOFLINK Urls in the text file while read line; do ## Number formatting with zero leading ## there might be other better ways to make numbering with ## better and more efficient methods if [ $NUMOFLINK -ge 0 ] &amp;&amp; [ $NUMOFLINK -ge 0 ]; then LZ="" elif [ "$NUMOFLINK" -ge 10 ] &amp;&amp; [ "$NUMOFLINK" -le 99 ] &amp;&amp; [ "$NUMSTART" -lt 10 ]; then LZ="0" elif [ "$NUMOFLINK" -ge 10 ] &amp;&amp; [ "$NUMOFLINK" -le 99 ] &amp;&amp; [ "$NUMSTART" -ge 10 ]; then LZ="" elif [ "$NUMOFLINK" -ge 100 ] &amp;&amp; [ "$NUMOFLINK" -le 999 ] &amp;&amp; [ "$NUMSTART" -lt 10 ]; then LZ="00" elif [ "$NUMOFLINK" -ge 100 ] &amp;&amp; [ "$NUMOFLINK" -le 999 ] &amp;&amp; [ "$NUMSTART" -ge 10 ] &amp;&amp; [ "$STARTNUM" -le 99 ]; then LZ="0" elif [ "$NUMOFLINK" -ge 100 ] &amp;&amp; [ "$NUMOFLINK" -le 999 ] &amp;&amp; [ "$NUMSTART" -ge 10 ] &amp;&amp; [ "$STARTNUM" -le 999 ]; then LZ="" else LZ="000" fi echo Save $TITLE - Chapter $NUMSTART ffmpeg -hide_banner -loglevel panic -stats -c:v copy -c:a copy \ -bsf:a aac_adtstoasc "$TITLE Part $LZ$NUMSTART.mp4" NUMSTART=$((NUMSTART+1)) done &lt; $LISTURL echo Completed!! As example if videourls.txt have 130 urls, then file output is: root@ubuntu:~# mv script /usr/local/bin root@ubuntu:~# chmod +x script root@ubuntu:~# ls /usr/local/bin script root@ubuntu:/home# cd /home root@ubuntu:/home# script Open text files: videourls.txt Save Video Title as: Learn Something There are 130 Urls in the text file Save Learn Something - Chapter 1 .... root@ubuntu:/home# ls videourls.txt Learn Something - Part070.mp4 Learn Something - Part130.mp4 Learn Something - Part001.mp4 Learn Something - Part071.mp4 Learn Something - Part002.mp4 Learn Something - Part072.mp4 &amp;#x200B;
This. You learn from your mistakes üòÄ
npnp we all start
Oooh...careful. The "regex" that the Shell uses here is actually glob pattern. It's NOT your typical BRE, ERE, PCRE, or variants like that. It's glob pattern.
PREMATURELY OPTIMIZED, BABYYYY for file in *.log; do echo mv $file ${file%.*}.LOG; done
Can you please expand on the statement "C/C++ don't work like that", did you mean that not all input devices flush, so you might be indefinitely are waiting for \n. 
Not only that, but if you have a filename that starts with a `-`, you might get unexpected results with your `mv`. Couple that with word splitting because the variable isn't quoted and you're asking for trouble. At least if you don't strip out the leading `./`, you remedy the `-` problem.
The same thing is the case for `while` loops too. Here's a contrived example: while echo -n 'Checking next file... ' diff -q "${a[i]}" "${b[i]}" &gt;/dev/null # exits 1 if they differ do # will continue only if the last command in the above block exited 0 echo 'ok' ((i++)) done
I know it's been a while, but I've been trying to use this and I've just got one problem. My submodules aren't tracking the master branch. I've tried Google-searching this and it doesn't seem easy... Have you done this before? I've also tried \`git submodule update --recursive --remote\`
Depends on your git version I think. Have you tried all of these? https://stackoverflow.com/questions/1030169/easy-way-to-pull-latest-of-all-git-submodules
I have seen that before, but I just now found this in the comments of the accepted solution: git submodule foreach "(git checkout master; git pull)&amp;" The only problem is that one of my submodules is supposed to be on something other than the master branch. Later, I'll try another one of the solutions, like git pull --recurse-submodules
You could write a tiny bash script to pull a different branch for that specific package. Are you comfortable with bash?
Indeed. I wrote something much more complicated to solve a problem that's already been solved by git submodules... See my original post here. &amp;#x200B; I'll look into it.
To be clear I‚Äôm just talking about like a 4-line script, just something that loops over the subdirs with special handling for the non-master one. If you configure the branch for each submodule (I imagine it‚Äôs possible) you could just omit the ‚Äúgit checkout master‚Äù too in the foreach.
&gt;https://www.codingame.com Nice. Thank you for this. I know it is legit because there is a blurry guy riding a four-wheeler in the background image on the homepage. Will def invest some time checking it out!
You need to use a process to run your command that sets RAM. So you surround the command with $(). RAM=$(free -m | awk 'NR==2{printf "Memory Usage: %s/%sMB (%.2f%%)\n", $3,$2,$3*100/$2 }')
I don't really get why you're creating `file.csv` with `touch` separately. You can simplify the code simply to $ free -m | awk '/Mem\:/{printf "RAM: Memory Usage: %s/%sMB (%.2f%%)\n", $3,$2,$3*100/$2 }' &gt; newfile.csv If you do need `file.csv` (or if it is not empty) for anything else use $ free -m | awk '/Mem\:/{printf "RAM: Memory Usage: %s/%sMB (%.2f%%)\n", $3,$2,$3*100/$2 }' | sed "r file.csv" &gt; newfile.csv If you need to store the output of `free` in `RAM` then you can use: $ RAM=$(free -m | awk '/Mem\:/{printf "Memory Usage: %s/%sMB (%.2f%%)\n", $3,$2,$3*100/$2 }') $ sed "1i RAM: $RAM" file.csv &gt; newfile.csv &amp;#x200B;
```echo "RAM: $(free -m | awk 'NR==2{printf "Memory Usage: %s/%sMB (%.2f%%)\\n", $3,$2,$3\*100/$2 }')" &gt;&gt; newfile.csv```
Got it working thanks guys! &amp;#x200B; Anyone know how I would get it to add the value on the second line of the CSV, so I can enter a title such as "Date|Ram" across the top of the document in top column?
Try running this with bash -x / set -x, and the problem should become apparent. My first hunch is that it has to do with your curly braced variables and that expansion isn't working like you supposed. 
&gt; it happens only every other iteration. I've been trying to debug with set -x and PS4='+${LINENO}: ' but it has been highly unhelpful. The expansion is working, the read file is the problem. As $file loses it's first character every other iteration.
Is it possible that `parseTVfilename.sh` is reading a character from stdin? That would be a problem, since inside the loop stdin is redirected from the pipe. You could try adding `&lt; /dev/null` to the call to parseTVfilename.sh and see if that helps. If you do need that script to read from your original stdin, that's more tricky (but can be done with something like `exec 3&lt;&amp;0` at the top of the script, then calling `parseTVfilename.sh &lt;&amp;3`)
`$TVpath` should be an absolute file path; this is good practice that may eliminate your filename problem. Using globs instead of `find` will sort the files by default. shopt -s globstar; for file in $TVpath/**/*.mp4; do grep -Fq "$file" $dbNameTV || ./parseTVfilename.sh $file; done It's hard to debug this without understanding `parseTVfilename.sh`. Would you please post all the relevant code?
I'm not sure if this is the problem you're running into, but it looks like you set `isPresent` and then action off of it, but then never unset it. Isn't that going to mess thing up since it's all inside the loop? After the first match, it's never going to be `-eq 0` again, right? You could probably do something like `if ! grep -c ${filename} ${dbNameTV}` and skip the variable setting step all together. 
This seems to do the trick but it's weird as i'm not purposefully waiting for any input... Which confuses me.
Yeah I know, sorry! I was naively thinking I could avoid bringing attention to that particular script, which is a horrible mess... But [here](https://github.com/pastapojken/Myflix/blob/master/scripts/parseTVfilename.sh) it is, if you want to give it a quick look. The "crash" occurs on line 59. Seems like redirecting to `&lt; /dev/null ` when calling parseTVfilename.sh solves the issue but I can't for the life of me figure where the code expects any input. Apologies for atrocious code and/or for it's optimization in advance... 
Wow. This might take a hot minute. Did you try my solution?
Yeah... It's... convoluted x) Yeah just gave it a go and it works with (seemingly) no issues :D Way too sleepy now to check it out thoroughly so I will do that tomorrow. Seems to work this far though! Thanks for the help! 
Without seeing the contents of `parseTVfilename.sh`, it's hard to say what the problem might be.
Have you tried `adduser --help`?
The output is the --help which I pasted above. So yes I read the help. 
Seems to solve the issue, Script was declaring variables at a time they were not needed yet. Guess Ill see in the end. def create_deployer_user(): """ || Create a user for the user group || """ try: # run('adduser -c "{}" -m -g {} {}'.format( # env.full_name_user, env.user_group, env.user_name)) run('adduser {}'.format(env.user_name)) run('passwd {}'.format(env.user_name)) run('usermod -a -G {} {}'.format(env.user_group, env.user_name)) run('mkdir /home/{}/.ssh'.format(env.user_name)) run('chown -R {} /home/{}/.ssh'.format(env.user_name, env.user_name)) run('chgrp -R {} /home/{}/.ssh'.format( env.user_group, env.user_name)) except: pass &amp;#x200B;
`out: adduser: The home dir must be an absolute path.` You're telling it to set `$HOME` to `--group` if you're using the `-d`/`--home` option you have to give the path to the home directory. It will **not** create the directory, that's what `-m` is for. Why the `-m`/`--create-home` option is not accepted, I'm not so sure. Have you tried `useradd`, I think it's better suited for scripts
turned out i didnt need the options behind the adduser, they were all below it in the script. so I reformatted it as shown above and it seems to work so far. &amp;#x200B; Thanks for the response!
 { [ "$WHAT" == "$b" ] &amp;&amp; echo "a is equal to b"; } || { [ "$WHAT" == "$c" ] &amp;&amp; echo "a is not equal to b"; }
The construct you're after is called [Short Circuit Operation](https://stackoverflow.com/questions/30766270/why-use-short-circuiting-boolean-logic-rather-than-if-then-else-in-bash), BUT If you're comparing strings, consider a case statement: case $WHAT in $b) echo equal;; $c) echo inequal;; esac Comparing integers with Arithmetic Expression and Short Circuits ((WHAT == b)) &amp;&amp; { echo equal; :; } || echo inequal force return true ^ In this example, `echo` will always return true, making this work like an if/else statement, but with anything more complex, you can force `true` with `:` and a `{ command group; }` 
That script is written in Python, not Bash. If you're asking here because of `adduser`, that's actually from `coreutils`.
&gt; I want the below code in a different format to save some lines: Unless there is a need to cram something in a one liner, sacrificing readability just so save a few lines isn't worth it.
to expand on this, one of the big cases where it makes sense to look into optimization is when you want to scale it up. [u/HenryDavidCursory](https://www.reddit.com/user/HenryDavidCursory) said consider a case statement, which is typically a very good way to deal with slightly larger amounts of else ifs, beyond that you might consider using a List of strings to select from. If all you want to do is perform the action you have here, I personally don't think you need to optimize and code clarity and simplicity should always come first. Of course you may have other reasons you want it optimized which is completely valid. 
# Function to look for a string in an array # https://stackoverflow.com/a/8574392 containsElement () { local e match="$1" shift for e; do [[ "$e" == "$match" ]] &amp;&amp; return 0; done return 1 } WHAT=($(/usr/sbin/virt-what | sed 's/:.*//')) g_hwID="GUEST_UNKNOWN" search_strings=(KVM VMWARE XEN) for index_array in $(seq 0 $((${#arr[@]}-1))); do if containsElement "${arr[$index_array],,}" "${arr2[@],,}" ; then echo "${arr2[$index_array]}"; fi; done 
 # Function to look for a string in an array # https://stackoverflow.com/a/8574392 containsElement () { local e match="$1" shift for e; do [[ "$e" == "$match" ]] &amp;&amp; return 0; done return 1 } WHAT=($(/usr/sbin/virt-what | sed 's/:.*//')) search_strings=(KVM VMWARE XEN) g_hwID="GUEST_UNKNOWN" for index_array in $(seq 0 $((${#WHAT[@]}-1))); do if containsElement "${WHAT[$index_array],,}" "${search_strings[@],,}"; then # echo ${var,,} lowercases the content of var g_hwID="${search_strings[$index_array]}" fi done 
just use Perl
Looks more to a case - esac candidate than an if tbh.
 case "$(/usr/sbin/virt-what)" in (KVM:*) g_hwID="KVM" ;; (VMWARE:*) g_hwID="VMWARE" ;; (XEN:*) g_hwID="XEN" ;; (*) g_hwID="GUEST_UNKNOWN" ;; esac
your code should work without an else statement, but imo the better way to do what you are trying to do is using cases, like this. &amp;#x200B; So to answer you: yes it should work without an else statement. This is a clever way to do what you are trying to do. &amp;#x200B; `g_hwID="GUEST_UNKNOWN"` `for i in "${WHAT[@]}"` `do` `case $i in` `"KVM")` `g_hwID="KVM"` `break` `;;` `"VMWARE")` `g_hwID="VMWARE"` `break` `;;` `"XEN")` `g_hwID="XEN"` `break` `;;` `*)` `esac` `done` `echo $g_hwID`
This is a bash sub.
Don't do stackoverlow kids. It fries yer brains.
would you advice the OP to use bash even if he had to deal with multidimensional arrays? Bash is just a command line that was hacked and tricked in being a semi-complete programming language. If you need to do something "greater" than pipes, xargs, tee, etc, you need Perl, Python, Ruby, or alike. End.
 w=($(/usr/sbin/virt-what)) case "${w%%:*}" in (KVM|VMWARE|XEN) g_hwID="${w%%:*}"; ;; (*) g_hwID="GUEST_UNKNOWN"; break ;; esac
 for w in $(usr/sbin/virt-what); do case "${w%%:*}" in (KVM|VMWARE|XEN) g_hwID="${w%%:*}"; break 2;; (*) g_hwID="GUEST_UNKNOWN";; esac done printf '%s\n' "$g_hwID"
This being a bash subreddit and the task being relatively simple and despite your claims not requiring any kinds of multidimensional array. Yes, I think bash is a fine choice.
Fuck it then. I always forget how toxic this community is and post. See ya.
kk üëçüëçüëç
Your script won't be able to change your working directory because each script that runs is it's own process and has its own current directory. When you execute a program from the shell it is run in a new process. In order to have your script change your workign directory you will want to add it to your path. &amp;#x200B; The solution to this is to create directory for your script. So let's say you make one in your home called scripts cd mkdir scripts then in scripts add your script and remove the .sh at the end so mv myscript.sh scripts/myscript then you will want to add the PATH in your bashrc so vim .bashrc then add this line to the bottom export PATH=$PATH:~/scripts now with all of that done you will want to execute your script using the dot ( . ) method so run the following command . myscript etc you can also just run myscript etc but without the dot it will only print the items and won't change the working directory. &amp;#x200B; Hope that helps!
cd directory exec bash
Can you show example output of that 'virt-what' command? You are doing strange things by trying to create an array out of the words that it's printing. Instead of doing this, try to directly process its output and see what happens. You might get to a solution that feels better. You could for example try to run grep on its output with the `-o` parameter: grep -oE '\b(VMWARE|XEN|KVM)\b' &lt; &lt;( virt-what ) The `\b` in the regex rule means "word boundary", it makes it so the search result has to be its own word and can't be part of a longer word. I used `grep &lt; &lt;( cmd )` here instead of `cmd | grep` so that the exit code of grep is accessible. You can now check that exit code and print the "GUEST_UNKNOWN" word if grep found no match: grep -oE '\b(VMWARE|XEN|KVM)\b' &lt; &lt;( virt-what ) || echo GUEST_UNKNOWN This is now a piece of code that prints just the words you want for the $g_hw_ID variable. You can then just put that output into the variable and you are done: g_hwID=$( grep -oE '\b(VMWARE|XEN|KVM)\b' &lt; &lt;( virt-what ) || echo GUEST_UNKNOWN ) I don't know if this would work because I don't know the virt-what command's exact output. About what you are currently doing, you could change your 'if' in two ways: if [[ $i =~ KVM|VMWARE|XEN ]]; then g_hwID=$i break fi or case "$i" in KVM|VMWARE|XEN) g_hwID=$i break ;; esac 
Telling a Linux newb to use vim is bofh level evil. Good luck op 
He said he uses vim in his post... but yes I agree that vim for a beginner has a steep learning curve.
Ahh, I missed that. Carry on good Samaritan
quote the path (folder) i.e. Folder='/home/niko' CHECK="$(du -hs "${Folder}")"
&gt;Folder='/home/niko' CHECK="$(du -hs "${Folder}")" Same error when trying '/home/website.com'
`.` shouldn't be a problem, nor should any other symbols. Does the folder you're trying to check have a `G` in it? You should use a second `%` to remove the *longest* matching suffix: CHECK="${CHECK%%G*}"
I got this response: &amp;#x200B; `bash` [`check.sh`](https://check.sh) `Current Foldersize: 761M /home/kvm3/janeator.com GB` `(standard_in) 1: illegal character: M` `(standard_in) 1: syntax error` `Folder is smaller than 10 GB` &amp;#x200B; this is the code now: &amp;#x200B; `SIZE="10"` `CHECK="\`du -hs /home/kvm3/janeator.com\`"` `CHECK="${CHECK%%G*}"` `echo "Current Foldersize: $CHECK GB"` `if (( $(echo "$CHECK &gt; $SIZE" |bc -l) )); then` `echo "Folder is bigger than $SIZE GB"` `else` `echo "Folder is smaller than $SIZE GB"` `fi` &amp;#x200B;
You're trying to cut off the `G*` when the folder size is only 761M.
Thanks buddy
Could you explain for me the `break 2` here? Since `break` doesn't affect a `case` statement in bash (like it does in C, C++, Java, etc.), a single break would be sufficient to terminate the `for` loop early. What does `break 2` do here?
\`75%\` is not an integer. \`75\` is. 
Maybe, I misunderstood your question, but why are you not removing the \*%\* from your result? You can pipe the results of your current command into the \*cut\* command to remove that symbol, and then you'll be able to treat it like an integer. Example below: &amp;#x200B; #!/usr/bin/env bash percent=$(df -h | grep -o '[^ ]*%' | tail -n 1 | cut -d'%' -f1) if [ 500 -gt $percent ]; then echo "500 is greater than $percent" fi &amp;#x200B;
&gt; percent=$(df -h | grep -o '[^ ]*%' | tail -n 1 | cut -d'%' -f1) you mean like this: percent=$(df -h | awk 'END{print $5}') echo ${percent%*%} or like this: echo ${percent/\%/} Bash has built-in ability to scrub the strings of whatever character you don't want. There are multiple ways to do this.
Title should have been "Beginner not at all trying to turn percentage to integer."
Thank you for sharing that!
Two subshells: (( $(df |awk '/^rootfs/ &amp;&amp; sub(/%/, "") &amp;&amp; $0=$5') &gt; 80 )) &amp;&amp; command
Good stuff as usual.
As an alternative to environment changes, consider using a function with a `{ command group; }` body. Command groups run in the current shell. Please correct me if you want to do something other than run `ls` every time you `cd` somewhere, aborting if you try to cd to where you already are: cd() { [[ $PWD =~ ${!#} ]] &amp;&amp; { echo "You're already there!"; return; } builtin cd "$@"; ls } * `${!#}` is the last argument to your function, which will usually be the target directory. This line doesn't address `cd -`, which sends you to your `$OLDPWD`.
Basically it just exits from 2 levels of a loop. First the case and then the for loop. In C and other languages you would have needed two break statements, one for the switch and one for the for loop. break [n] Exit from within a for, while, until, or select loop. If n is specified, break n levels. n must be ‚â• 1. If n is greater than the number of enclosing loops, all enclosing loops are exited. The return value is 0 unless n is not greater than or equal to 1. 
You want `chmod`. Don't use the octal modes if you don't like slashed tires. Use symbolic modes; something like `chmod g+rx`, which means "add read and execute permissions for all users in this group." 
Ok i tried it but i cant make it working, can you post the full script? 
It won't output anything if the file is under 10GB. You could add `|| echo "$dir is under 10GB` to the end
Ok thanks! Sorry for the lame questions.
&gt;$(( 10 \* (1024 \*\* 2) )) uhm, instead of using GB, what would be the calculation for MB?
Not that doing `sh file.sh` will execute the file without the need to have it executable, because what you run is `sh` and not `file.sh`. 
&gt;g\_hwID=$( grep -oE '\\b(VMWARE|XEN|KVM)\\b' &lt; &lt;( virt-what ) || echo GUEST\_UNKNOWN ) It\`s working fine . Thanks a lot &amp;#x200B;
&gt;g\_hwID=$i break ;; esac but if I have multiple outputs for virt-what like .... sudo /usr/sbin/virt-what ovirt rhev kvm 
For that problem, you can add a `... | head -n1` to cut off the extra lines that grep would print. g_hwID=$( { grep -oE '\b(VMWARE|XEN|KVM)\b' &lt; &lt;( virt-what ) || echo GUEST_UNKNOWN; } | head -n1 ) The way to add the `| head -n1` is added here is a bit complicated because of the `|| echo UNKNOWN` part that needs the exit code of grep. Because of that exit code, the `head -n1` has to go after both the grep and the echo. To make this possible, you need to group things with a `{ ...; }` block. The whole block's output then goes into `head -n1`. 
Dunno if it counts as a "classic" but the poem in surfraw manual is pretty nice. Oh Baybe I need some Deep Linking Let us go Surfin' in the raw! 
I just pick up a copy myself. [https://www.tuhs.org/Archive/Distributions/Research/Dennis\_v5/v5man.pdf](https://www.tuhs.org/Archive/Distributions/Research/Dennis_v5/v5man.pdf) &amp;#x200B; Like page V &amp;#x200B; I pick up a old Unix Shell book and learn many skills from it. Going backwards, I learn more so I can go forward. &amp;#x200B; The book I have is call; &amp;#x200B; Unix Shells by Examples Third Edition by Ellie Quigley. I learn more about awk, sed and grep from this book, then from any other place. &amp;#x200B; Favorite passages....ummmmm.... &amp;#x200B; The Unix-Hater Handbook [http://web.mit.edu/\~simsong/www/ugh.pdf](http://web.mit.edu/~simsong/www/ugh.pdf) &amp;#x200B;
Wow! Cheers for the link, book looks great!
So I tried to use sh to run the script and some unknown function error appeared. Also how would using sh instead allow the users to view the files they are usually not allowed to? 
If you have read rights, you will be able to run a script without the exec permissions. This will depend on the script and on the other rights, but just removing the executable part does not mean nobody could run it.
Yeah, I've noticed that back in the old days text processing was a big portion of software engineering. Nothing wrong with that, it's just weird in retrospect. 
&gt;First the case and then ... Ah. But `break` actually doesn't exit a `case` statement in bash (I know, it's weird, right?). A `break` statement inside a `case` statement will actually break out of the higher level construct (only breaking out of the `case` statement as a side-effect of that). Observe: #!/bin/bash for x in a b c d e; do case $x in a) echo "a";; b) echo "b";; c) echo "c"; break;; d) echo "d";; e) echo "e";; esac sleep 1 done The `;;` is effectively the "break" that we're all used to seeing in C, C++, Java, JavaScript, etc. A `select` loop, by the way, is something different: It puts up a menu. Select loops aren't case statements. :)
Oh, thanks. I've been doing more C lately so I guess that got into me.
Understandable! :-)
 virt-what | awk -v FS=: '$1 ~ /^(KVM|VMWARE|XEN)$/ {print $1; exit} END {print "GUEST_UNKNOWN"}' This could work as well. Single command instead of having to use sed, grep and head.
In a similar vein, here's a 1986 paper "Two bit/pixel full color encoding" speculating on the future of digital images: &gt;A typical 640 x 480 color image can be compressed to 620,000 bits thus the image could be transmitted in approximately one minute over a 9600 bit-per-second phone circuit. and even suggests the feasibility of glorious 12fps digital *video*! &gt;Current microcomputer hard disks have a transfer rate of 5,000,000 bits or 625,000 bytes per second. Allowing for "no data" space on the disk tracks and head movement it means that a 30 megabyte hard disk, now standard on the IBM AT, could store and display in real time approximately 60 seconds of animation.
I love knowing I'm not the only one who gets a lot of pleasure reading old computer manuals. They just come off as so friendly and to-the-point, not chatty. Technical literature these days are usually more verbose and have a lot of corporate sophistry. This is a favourite passage of mine, in the first edition of K&amp;R (I don't know if it is in the second): C Is a relatively "low level" language. This characterization is not pejorative; It just makes me giggle; it's so quick to say 'that's not an insult btw'. This section about logging in from the Unix Programming book is fun to me, too. [https://files.catbox.moe/trzivx.png](https://files.catbox.moe/trzivx.png) case distinctions matter! If your terminal produces only upper case (like some video and portable terminals), life will be so difficult you should look for another terminal. Uses short, easy to understand, human language.
There's a weird contrast with the video from Bell Labs that was posted to /r/Linux yesterday where C is referred to as high level.
You should be able to hit tab twice to see a list of possibilities. Autocomplete only works when unambiguous.
actually, if you wouldn't mind - can you link me to that documentation? 
Yup, that's what I'd like but it isn't working. &amp;#x200B; I tried with a different path and now it autocompletes \*.py files only but not any other extensions; like the \*.csv files I'm after. 
python max 10 lines 
What was the value of $PATH before and after that change?
No change to path, same session. The only change was I tried to autocomplete a different path structure. This new path end point has \*.py files in (in addition to other extensions) and it will autocomplete the \*.py files but not others :-/ 
I ended up editing the /etc/sudoers and added a few lines to give users to run cat and chmod as sudo. Then I would use sudo cat in the script and use sudo chmod 020 to allow them to write the file after they are done I would do sudo chmod 000. 
It's a bash reddit, you didn't even try, and AWK shreds Python at text processing.
I don't see a photo. Can we get a sample of desired input (structure of existing .csv files) and desired output?
[https://www.bell-labs.com/usr/dmr/www/1stEdman.html](https://www.bell-labs.com/usr/dmr/www/1stEdman.html)
awk -v name="newname.bin" '$1 == "FILE" &amp;&amp; $3 == "$BINARY" {printf "FILE %s BINARY", name; next} {print}'
**Unix Commands for Sex** \- If you're not doing it right, try this... gawk; grep; unzip; touch; strip; init, uncompress, gasp; finger; find, route, whereis, which, mount; fsck; nice, more; yes; gasp; umount; head, halt, renice, restore, touch, whereis, which, route, mount, more, yes, gasp, umount, expand, ping, make clean; sleep 
 awk ' BEGIN{ split(FILENAME, a, ".") } NR==1{ printf("FILE \"%s.bin\" BINARY\n", a[1]); next } 1' /path/to/*.cue * `FILENAME` is a built in variable that returns the actual file name. This is the only reason you can't use Sed ;) * `NR==1` will run this rule only on the first record * `next` skips all other processing for this record * `1` is always true, so prints every other line.
This could be nicer but it can indeed be done with sed: `text='FILE "oldname.bin" BINARY` `TRACK 01 MODE2/2352` `INDEX 01 00:00:00'` &amp;#x200B; `echo "$text" | sed 's/FILE.*BINARY/FILE "newname.bin" BINARY/g'`
This should do the whole job for you; PLEASE TEST on a few files before running a glob. awk ' FNR==1 { split(FILENAME, Name, ".") print "FILE", "\"" Name[1] ".bin" "\"", "BINARY" &gt; FILENAME } FNR!=1 {print &gt; FILENAME} ' *.cue
Huh. So awk is smart enough to update the file atomically, at the end of processing, rather than start overwriting it while reading it?
More accurately it's smart enough to read everything it needs first.
What if it has to read a gigabyte first? Does it buffer up the output?
I'm trying to find a source before I make further ass out of myself, but I believe it's file-by-file. This is also likely to be handled differently in gawk and original-awk, but the above works in both.
Did you test this?
I think Awk would be better suited for this. I haven't used it that much but this seems like a job for it
run your hex through `od` or something like that.
I love this. I have a question though. Is there a way to run a list of commands as an argument to this script? For example, I have this function that I use for speed tests. speedtestvpn () { clear { echo -e "VPN ON | Date: $(date)\n------" speedtest-cli --secure --no-upload echo "" } &gt;&gt; ~/speedtest.log sed -r -i.bak 's/[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}/\[IP REDACTED\]/g' ~/speedtest.log cat ~/speedtest.log } Is there a way to run this this portion of it with spinner? { echo -e "VPN ON | Date: $(date)\n------" speedtest-cli --secure --no-upload echo "" } &gt;&gt; ~/speedtest.log
Did you try escaping the exclamation mark, like `\!g` ?
The expansion does not happen in the function, it happens in your shell. You'll have to single-quote or escape it when you run it, e.g. `function_name '!g'`.
Nice! And with something like \[this\]([https://superuser.com/a/175802](https://superuser.com/a/175802)) you could have the spinner run for any command as a part of the prompt setup. Hmmm... might try this tonight :D 
Not really, I made the comment at around 4:30 AM.
 $ echo '017 AAA' &gt;myfile $ hexdump myfile -e '"hex= " 8/1 "%02X "' -e '"\t\ttext= " 8/1 "%c""\n"' hex= 30 31 37 20 41 41 41 0A text= 017 AAA $
I can use `$(basename "$f" .cue).bin`.
That's all you want? `${f%.*}.bin`
Not the best code but this may give you a starting point. Put this in a file, say sensors.awk: $1==13 { $4 = $4 + 20; print } $1&gt;=15 &amp;&amp; $1&lt;=20 { $4 = $4 + 20; print } $1==21 { $4 = $4 + 90; print } Then run: $ awk -F '|' -f sensors.awk &lt; ipmi-sensors-data 13 Baseboard Temp Temperature 61 C 'OK' 15 IOH Therm Margin Temperature -20 C 'OK' 16 Mem P1 Thrm Mrgn Temperature -18 C 'OK' 17 Processor 1 Fan Fan 1942 RPM 'OK' 18 System Fan 5 Fan 1220 RPM 'OK' 19 System Fan 3 Fan 3080 RPM 'OK' 20 System Fan 1 Fan 692 RPM 'OK' 21 P1 Therm Margin Temperature 52 C 'OK' Exercise for the reader: re-instate the '|' separators and output 2 decimal places. &amp;#x200B; &amp;#x200B;
[removed]
Hey everyone, thanks for all the great replies, I have been away a few days...guess that's obvious, but I saw all of them and I got it working. Thank You!!!
The `!` is a special character which you need to escape (as others have mentioned). An alternative is to use some other character which _isn't_ special to the shell, and do a replacement. `%` may be a good choice, as from what I've seen DDG (and most search engines) ignore it. ddg() { set -- "${@/#\%/\!}" # replace leading % with ! in all args firefox "https://duckduckgo.com/?q=$*"
I'd probably use `&amp;&amp;` between them, that way if you Ctrl+C out of read, it won't open a firefox window.
You should probably use the scripting language you know best for this problem. Bash is pretty good at dealing with this problem. I don't quite understand what you want to do exactly, so I'll just explain what you can do in bash. You can go through the output of the command and read the columns into variables like this: ipmi-sensors | while IFS='|' read id name type reading units event; do ... done Inside that 'while' loop, you will have the contents of the columns inside the variables $id, $name, etc. The variables will contain the space characters in the columns, for example $type will be `" Temperature "` and not `"Temperature"`. You need to keep this in mind when working on the stuff. You would then check for $id being 13 or 15 to 20 (bash will ignore the extra space characters in the $id variable when treating it as a number inside `((` ): ipmi-sensors | while IFS='|' read id name type reading units event; do if (( id == 13 &amp;&amp; (id &gt;= 15 &amp;&amp; id &lt;= 20) )); then ... fi done To do some calculation on $reading, you need an external tool named 'bc' because bash's math mode can't deal with decimal points. Doing your calculation looks like this: reading=$( bc &lt;&lt;&lt; "$reading + 90" ) Printing a line of the table you could do like this: printf "%s|%s|%s| %-11s|%s|%s|%s\n" "$id" "$name" "$type" $reading "$units" "$event" The "Reading" column is treated special because the space characters will have disappeared after doing calculations on $reading.
Thank you very much! This is a great starting example that I was looking for.
Thank you! I really appreciate the help you folks bring in this sub.
I need a good tutorial on how to best use grep and awk. I always see them being used, but never understood how to put them to use from scratch.
This is perhaps caused by the "bash-completion" package. It loads scripts that know about the possible parameters for certain programs to help with TAB completion. The completion script installed for python is then probably what's filtering things and only offering directory names and python scripts in the list.
Uuuhh, that looks good. When got it working let me know üíØ
I'm glad it helped you (: &amp;#x200B; You could also do something like this üëá instead of grouping the code inside the \`speed\` function &amp;#x200B; speed () { clear echo -e "VPN ON | Date: $(date)\n------" speedtest-cli --secure --no-upload echo "" } # and then . ~/Scripts/spinner speed &gt;&gt; ~/speedtest.log As the \`spinner\` will echo out to \`stdout\` and then you can redirect it to the log filie or if you leave it alone to the terminal. :D
That looks so much better than what I put together! Awesome! Definitely going to try that out.
try \`eval\` &amp;#x200B;
ooo nice, seems that is getting us closer, now it complains there is a syntax error near unexpected token \`(\`
well it's the I/O redirection at the end, no? `&lt;&lt;&lt;` takes a string, not a command. Command redirection is `&lt; &lt;(command)`
ah I see, I did not realize that. Nice catch! I change it to your suggestion, but it still fails with the previous "(" error. I'll keep testing! :)
consider escaping the characters giving you syntax errors. read up on how eval works
Yeah that's what I was thinking as well, thank you for the help! I'll update it when I figure it out, you gave me a good lead! :)
You must put escape sequences in your prompt within `\[\]` so that bash does not use those bytes to compute the length of your prompt. Because you have not enclosed the escape sequences in `\[\]`, bash is using them in calculating the prompt length, and it is placing your cursor in the wrong place. Try the following: `PS1='\h [\u:\[\e[00;36m\]\w\[\e[00m\]]\$ '` Also, it is a bad idea to use direct terminal sequences because the a terminal capability can be invoked with different sequences in different terminals. A better way to go about it is to use `tput`: cyan_fg="$(tput setaf 6)" reset="$(tput sgr0)" PS1='\h [\u:'"\[${cyan_fg}\]"'\w'"\[${reset}\]"']\$ ' unset -v cyan_fg reset
Thank you so much! I will play around with that!
If you send SIGINT, it should stop the entire function, not just `read`. That would happen if you send it EOF, but I don't see why you wouldn't expect Firefox to open if you do so.
Hey, I think I made a mistake while explaining my thing... So I would start with a file (myFile) containing: 30 31 37 20 41 41 41 20 0A If I hexdump myFile, I get : 0000000 3033 3320 2031 3733 3220 2030 3134 3420 0000010 2031 3134 3220 2030 4130 000a 000001b Now, I would need to create a script with a flag "-h" that when called with hexdump, would do the inverse. It would take the second set and turn it into the first (original). &amp;#x200B; I was thinking of doing something like if [ $2 == '-h' ] then ... echo the inverse fi I think what you did was right, but could you explain it? I think it has to do with 16 and 8 bits difference, and by turning the big set into characters, it returns to the original set ? &amp;#x200B; Thanks
What do you have so far?
You can create that testfile like this: $ echo '30 31 37 20 41 41 41 20 0A' &gt;myfile If you want to see what it contains you can do this: $ cat myfile 30 31 37 20 41 41 41 20 0A If you have the hexdump tool, it is capable of showing you the same information, like this: $ hexdump myfile -e '"%07.7_ax " 8/2 "%04x "' -e '"\t\ttext= " 16/1 "%c""\n"' 0000000 3033 3320 2031 3733 3220 2030 3134 3420 text= 30 31 37 20 41 4 0000010 2031 3134 3220 2030 4130 000a text= 1 41 20 0A So, if you have hexdump available as you say you do, then my answer to your original question "How would you guys go about writing a script" is that hexdump already knows how to do the job you want by itself, so I wouldn't waste time writing a script. 
Even better solution: for i in "${WHAT[@]}"; do [[ $i == @(KVM|VMWARE|XEN) ]] &amp;&amp; { g_hwID=$i break } done || g_hwID=GUEST_UNKNOWN
I keep seeing about `tput`, but I couldn‚Äôt find it/make it work on my FreeBSD or Ubuntu systems. Is it in some nonstandard repo somewhere?
 #!/bin/bash
`tput` should be there by default and if not, it should be readily available from standard sources (repos, PORTS etc), usually as part of the `ncurses` package. It is thought of as the one true portable way to do things... unfortunately it's different on the BSD's because... who knows?! "Fuck standards" I guess? Anyway, pop this into your `.bashrc` and/or your scripts and then treat `tput` as you would on Linux. # Overlay 'tput' so that non-GNU behaves as much like GNU as possible # This is a subset of a fuller gist # https://gist.github.com/rawiriblundell/83ed9408a7e3032c780ed56b7c9026f2 # For performance we only implement if 'tput ce' (a harmless test) works if tput ce 2&gt;/dev/null; then tput() { ctput-null() { command tput "${@}" 2&gt;/dev/null; } ctput() { command tput "${@}"; } case "${1}" in (bold) ctput-null bold || ctput md;; (civis) ctput-null civis || ctput vi;; (cnorm) ctput-null cnorm || ctput ve;; (cols) ctput-null cols || ctput co;; (dim) ctput-null dim || ctput mh;; (lines) ctput-null lines || ctput li;; (setaf) case $(uname) in (FreeBSD) ctput AF "${2}";; (OpenBSD) ctput AF "${2}" 0 0;; (*) ctput setaf "${2}";; esac ;; (setab) case $(uname) in (FreeBSD) ctput AB "${2}";; (OpenBSD) ctput AB "${2}" 0 0;; (*) ctput setab "${2}";; esac ;; (sgr0) ctput-null sgr0 || ctput me;; (*) ctput "${@}";; esac } fi
This is pretty much true. I'm completely new to this, so I've just been compiling bits and pieces from around the web: &amp;#x200B; \#!/bin/bash &amp;#x200B; cd $1 &amp;#x200B; for d in \*/; do cp /Users/Tom/Documents/MFiT/SOX\_MFit/MFiT.pl/. "$d"; for d in \*/; do cp -a /Users/Tom/Documents/MFiT/SOX\_MFit/sox-14.4.1/. "$d"; &amp;#x200B; &amp;#x200B; \# Get current working directory src=$(pwd) &amp;#x200B; \# Pattern match as you described regex="\^s\[0-9\]{2}\\.result$" &amp;#x200B; \# Everything in current directory for dir in "$src"/\*; do &amp;#x200B; \# If this is a directory that matches the pattern, cd to it \# Will early terminate on non-directories if test -d $dir &amp;&amp; \[\[ $dir =\~ $regex \]\]; then cd "$dir" \# Do some other things here fi &amp;#x200B; for d in $(find /Volumes/ExFAT\\ II/LPO\\ Files -maxdepth 1 -type d); do echo "$d"; done
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You could create another script like `/usr/local/bin/set_volume.sh` with just #!/bin/bash osascript -e "set Volume 10" Then add this to your sudoers file: username ALL=NOPASSWD: /usr/local/bin/set_volume.sh Then you can run it like this: #!/bin/bash open ~/Downloads/untitled/dog.png open ~/Downloads/untitled/audio1.mp3 sudo /usr/local/bin/set_volume.sh
osascript doesn't typically need elevated rights to set the volume on a mac. Since you're opening things in `~/` I'm assuming you're logged in as the correct user for these files anyway. Can you try just removing the `sudo` altogether? See if it still does what you want - that way you should not need to enter a password anymore. 
 awk | grep Eew.
There's no need for the awk and grep, you can just do `while ps $PID &gt;/dev/null`
:O a lot much cleaner.
Shhhh, quiet you. Go back into the corner ü§êüôÖ‚Äç‚ôÇÔ∏è
:'/ My thought was like &gt; Take the first column of `ps` output and then filter that with `grep` Other solution I thought was to filter it right inside of `awk` ü§∑‚Äç‚ôÇÔ∏è
Yeah, 99% of the times you pipe awk/sed to grep, you can probably do it without grep. The other poster gave you a good solution. But, you can even do it without `ps(1)`, simply checking if `/proc/$PID` exists.
I have a very similar but much more quick and dirty function: spinner() { condition="$1" a="\|/-\|/-" i=1; echo; while $condition &gt;/dev/null; do if [ $1 -eq 8 ]; then i=1; else i=$((i+1)); fi; echo -ne "\r$(cut -b $i &lt;&lt;&lt;"$a")"; sleep 0.1; done } I like the way you've used the variable slicing to get the individual characters of the spinner, it's a lot nicer than my way
I think I tried `/proc/$PID` and there's a difference between linux and mac. (Or maybe I was doing something wrong üòÖ) And I always like to see other solutions to a problem, that way you can learn from different points of view and make something new. ( : In that same manner, why not use awk/sed + grep? Is it because there are a lot pipes ? i.e. `echo "foo" | sed 's/foo/bar/' | grep "ba" | xargs echo" (Just a random example of a lot of pipes)
Spawning sub-processes is pretty slow. In most scripts you probably won't notice the difference in performance but why not use the most optimal way anyway?
I agree
Fortunately, I have never had any problems getting `tput` to work on any of the systems I have tried (GNU/Linux, FreeBSD, macOS). Sometimes (e.g. if your system is using the older termcap, and not terminfo), you may have to use the older termcap code instead of the terminfo capability name (as /u/whetu has suggested below) to get the desired sequence. For example, `man terminfo` has the following about the 'setaf' capability: set_a_foreground setaf AF Set foreground color to #1, using ANSI escape In the above, 'setaf' is the terminfo capability name, and 'AF' is the termcap code.
You could set the sticky bit on that script.
&gt; Remember to make sure sure that only root can edit the /usr/local/bin/set_volume.sh Which, by the way, could be achieved by doing the following: sudo chown root /usr/local/bin/set_volume.sh sudo chmod 755 /usr/local/bin/set_volume.sh
Good bot.
Is that different from making it setuid?
I would use the SUID bit. The sticky bit is for directories.
Let's consider the following: # A function to log messages to the system log # http://hacking.elboulangero.com/2015/12/06/bash-logging.html may be useful logmsg() { local logIdent OPTIND while getopts ":t:" optFlags; do case "${optFlags}" in (t) logIdent="-t ${OPTARG}";; (\?) printf '%s\n' "ERROR: Invalid option: '-${OPTARG}'." &gt;&amp;2 return 1;; (:) printf '%s\n' \ "Option '-${OPTARG}' requires an argument. e.g. '-${OPTARG} 10'" &gt;&amp;2 return 1;; esac done shift "$((OPTIND-1))" if command -v systemd-cat &amp;&gt;/dev/null; then systemd-cat "${logIdent}" &lt;&lt;&lt; "${*}" elif command -v logger &amp;&gt;/dev/null; then logger "${logIdent}" "${*}" else if [[ -w /var/log/messages ]]; then logFile=/var/log/messages elif [[ -w /var/log/syslog ]]; then logFile=/var/log/syslog else logFile=/var/log/logmsg fi printf '%s\n' \ "$(date '+%b %d %T') ${HOSTNAME} ${logIdent/-t /} ${*}" &gt;&gt; "${logFile}" 2&gt;&amp;1 fi } So in this case we have the following use examples: logmsg something went wrong logmsg -t "${myApp}" something went wrong In the first example, `getopts` isn't invoked (or rather, it doesn't see anything it cares about) so the function gives whatever output it does via `system-cat`, `logger` or its own output, which in this case would look like: Apr 05 09:40:34 SomeHost something went wrong Sometimes when you're logging messages like this, you want to tag it, usually to specify the application that's causing the need for the log message, or to provide some other kind of determinant. So let's say you have deployed a custom script, `my-cool-script`, that is kicked off by `cron` and it uses the above function to log things. logmsg -t "${myApp}" something went wrong In this case, `getopts` is invoked (or rather, it sees something it cares about and steps in). It essentially keeps track of how many parameters it parses by iterating `OPTIND`. So you can think of it like this. Before `shift "$((OPTIND-1))"`: -t "${myApp}" something went wrong ^1 ^2 ^3 ^4 ^5 After `shift "$((OPTIND-1))"`: -t "${myApp}" something went wrong ^1 ^2 ^3 So what this means is that in the case of the above function, `getopts` takes care of the options (`-t "${myApp}"`), and whatever else is there (`something went wrong`) is left in `$*/$@` for us to continue to consume as we see fit. So in this scenario, we would see something like this in the logs: Apr 05 09:40:34 SomeHost my-cool-script something went wrong So, `OPTIND` isn't always needed, because there are scripts where you won't parse anything but args. It's still probably a good default practice to have.
I feel like this example code could have been so much simpler.
Thanks :)
Yeah, he only needed the middle part: So you can think of it like this. Before `shift "$((OPTIND-1))"`: my-cool-script -t "${myApp}" something went wrong ^0 ^1 ^2 ^3 ^4 ^5 After `shift "$((OPTIND-1))"`: my-cool-script -t "${myApp}" something went wrong ^0 ^1 ^2 ^3 
`$OPTIND` is the number of objects that `getopts` has processed, *including the command/function name*. `shift`ing those objects lets you use `$1`, `$2`, etc., to refer to your arguments in order, regardless of how many options are being used.
Whoa thanks for taking the time while you‚Äôre sick! The example is a bit complex for me but I‚Äôll take the time to go through it a few times to see if it‚Äôll register. 
 But why is it used? Couldn‚Äôt a getops while loop still work without the shift ‚Äú$(( OPTIND -1 ))‚Äù part? 
Yes. The shift construct allows you to proceed normally with argument processing **after** you finish dealing with flags and options.
&gt;a simple explanation with an example script on why shift $(( OPTIND -1 )) is needed in a getops while loop It's useful outside a getopts loop, so that once the script is done considering the getopts arguments, it can then pass the rest of the arguments to a case statement or something. For example, I have this simple search engine script which first tests whether any getopts arguments were passed to decide which browser to launch the results in (cli based, graphical or a full-featured browser), if no getopts argument was passed `[ $OPTIND -eq 1 ]` then it launches the results in a predetermined browser. After that, whether or not any getopts arguments were passed it removes them from its consideration by a simple `shift $((OPTIND-1))` and once that is done, the first argument after that (which is now `$1`) becomes the search engine to be used for the search, and rest of the arguments are the phrase to search for. Maybe it sounds a little complex but it's pretty straight forward and a very handy script.
AH HA. ok I think I‚Äôm starting to understand this is a little better. In other words shift $((OPTIND-1)) ‚Äòthrows away‚Äô the options (-e -j -h, whatever) from their positions, now -e which was ${1} is no longer used as ${1} but a different argument, say google, takes its place for the script to continue processing? So let‚Äôs say it looks something like this: $ ./script.sh -e -j -h google After shift $((OPTIND-1) is invoked google is now treated as ${1} ? Hope I‚Äôm getting this right. Would there ever be a use case for $((OPTIND-2)) $((OPTIND-3) etc ? Thank you!
`man gawk` is a rite of passage. glhf.
`man gawk` is a rite of passage. glhf. ipmi-sensors |awk ' func abs(x) {return x&lt;0? x*-1: x} $1~/(1[35-9]|2[01]/ &amp;&amp; sub($4, abs($4))'
Awk is a programming language, and `man gawk` is a rite of passage. `man gawk` is a rite of passage. glhf. $ ipmi-sensors |awk -F\| '$1~/^(1[35-9])|2[01]/ &amp;&amp; $4+=($5~/C/? 90: 0)'
Awk is a programming language, and `man gawk` is a rite of passage. ipmi-sensors |awk -F\| '$1~/^(1[35-9])|2[01]/ &amp;&amp; $4+=($5~/C/? 90: 0)' glhf
Why have you added 20 to the fan speed?
&gt;After shift $((OPTIND-1) is invoked google is now treated as ${1} ? Yes. &gt;Would there ever be a use case for $((OPTIND-2)) $((OPTIND-3) etc ? In the case of `$ ./script.sh -e -j -h google` when you leave the getopts loop the OPTIND will be 4 because it has already processed 3 arguments for getopts and the next argument number (OPTIND) is 4. Shift *n* removes *n* arguments from the argument list. So `shift $((OPTIND-1)` will remove (4-1) 3 arguments from the list. If you had done `shift $((OPTIND-3)` then it would have only removed (4-3) 1 argument from the list, so after that command your `$1` would be `-j`.
I don't understand the functionality of lines 5, 7, or 8. I don't use either of these tools, but here's my golf: yt2m(){ ffmpeg -c copy -i $(youtube-dl --get-filename "$1") "$2"; } yt2m https://www.youtube.com/watch?v=jNQXAC9IVRw zoo.mpeg
Did you open an issue in the YouTube-dl GitHub repo?
puhh never though of it but this also seems just to be an issue for certain apps. Whatsapp or Telegram seem not too happy about the broken files‚Ä¶ which is why i created the script
ahh `youtube-dl --get-filename` really just gives you the filename, nothing else‚Ä¶ it will not download the file at all if that switch is set‚Ä¶ also I do prefer readability over one line funcs‚Ä¶ 
Any particular reason for not using `mktemp(1)`?
mktemp creates the file, ffmpeg would need a `-y` or so to compensate for an existing file‚Ä¶ but nope, mktemp is viable :-)
Here's my take: First read [this](https://stackoverflow.com/a/36495940). Consider this: `program [-OPTIONS] [--] [FILE] ... `getopts` *only* parses options/parameters. But you still might want to process trailing file arguments which don't start with a dash or are listed after a `--`, to mark the end of the options list (getopts stops processing when it finds something not starting with a dash or if it encounters a double dash). The easiest way to check if there are still file arguments left, and to use them, is by shifting $((OPTIND-1)). You are done processing options and you don't need them anymore, so you shift them away so that only relevant arguments remain.
It was just an example.
I use ~/scripts only to store scripts (including a .sh suffix). I added ~/.local/bin to my path and then symlink scripts into it with sometimes a shorter name and always without the .sh suffix. Much cleaner.
mktemp has `-u`, which only prints a filename without actually creating it, or rather it gets removed immediately but it's considered unsafe. I mean it's unlikely that you will try to create a temp file which already exists but it's possible and the reason why mktemp exists is exactly that, to make it 100% safe instead. I guess in your case, ffmpeg would simply prompt anyway. So it doesn't really matter I guess. When I saw that pipeline, I just wanted to make sure you know about mktemp ;)
 #!/bin/bash readarray lines &lt; file length=${#lines[@]} for (( i=0; i&lt;length; i+=2)) do start=$(echo "${lines[i]}" | cut -d" " -f4-) end=$(echo "${lines[i+1]}" | cut -d" " -f4-) start_ms=$(date -d "$start" '+%s') end_ms=$(date -d "$end" '+%s') diff=$(( ( end_ms - start_ms ) /60 )) total=$(date -d@$diff -u +%H:%M:%S) echo "Difference between: ${start} and ${end} is ${total}" done Where `file` contains blahblah Starting blah Apr 1 12:00:00 blahblah Ending blah Apr 1 12:19:00 blahblah Starting blah Apr 1 13:00:00 blahblah Ending blah Apr 1 14:07:00 Gives Difference between: Apr 1 12:00:00 and Apr 1 12:19:00 is 00:00:19 Difference between: Apr 1 13:00:00 and Apr 1 14:07:00 is 00:01:07 
Can `blahblah` and `blah` have spaces or other junk like that? If not you could just get the dates like this: while read -r junk junk2 junk3 date; do printf '%s\n' "$date" done
Beautiful! It ran exactly as hoped. I'll awk for the last column and get that into excel. Thank you so much! I never would have figured that out.
No need to awk, just change the `echo` output
Ah, true.
How about builtin kill, much cheaper while kill -0 "${PID}" 2&gt;/dev/null I tried to turn things over, looping at while \[ $PPID -ne 1 \] but couldn't get it to work.
I‚Äôm a little unclear on the (4-1) (4-3) examples. Do you mind breaking it down a little further for me? Much appreciated. 
Thank you, very good explanation!
 TMPFILE=$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 13 ; echo '')".mp4" What's this for?
Made updates, thanks.
I just punched that in and it worked like an absolute champion. Please help me unpack that line in as plain english as possible. 
OHBOYHEREWEGO * `-F\|`: use `|` as Field Separator * `$1 ~ /regex/`: First Column matches regular expression * `^(1[35-9]|2[01])$`: 13, 15, 16, 17, 18, 19, 20, or 21. `^` and `$` are "anchors" meaning Beginning or End of string. They would prevent 2[01] from matching 420, for example. * `&amp;&amp;`: Boolean AND. Return true if both surrounding expressions return true. * `$4+=value`: Fourth Column equals Fourth Column plus value * `$5~/C/ ? 90 : 0`: Ternary Expression. If Fifth Column contains the character "C", return 90. Otherwise, return 0. This is a lot. Hit me up with any followup questions.
&gt;This is a lot. Hit me up with any followup questions. Dude...you seriously did a great job explaining it. I'm afraid there is no way, even if I wrote all this down that I could find a way to use this technique again. I really need more use cases for myself to pound it home like other things. A few things came to mind and one issue. The issue is that line 20 and 21 are not being returned. I changed `|2[01]` to `|2[0]` and `|20]`. Neither worked. If I tried to use an &amp; nothing happens and I am dumped back to the shell. Two questions come to mind. The first is that ID#13 actually returns the correct temperature...so nothing needs to be added to it, is there a way to omit that one ID in the math sequence while still keeping it tightly packed like that and printing it out? The second is related to changing the sequence. I believe ID20 is the CPU temperature, I'd like to put that with the other temperatures.
Fixed the regex issue; needed to account for whitespace in the first column. See edit above. Am I understanding correctly that you only want to adjust the temperature for records labeled "Margin" or "Mrgn"? If so, only the Ternary Statement needs to be adjusted: $1 ~ /^(1[35-9]|2[01])\s*$/ &amp;&amp; $4 += ($2 ~ /Ma?rgi?n/ ? 90 : 0) 
Thank you sir. I'm adding this one to my list of useful commands. 
No problem. You can run this through a system call to sort it, but you need to re-arrange the output since `sort` doesn't do regex separators: ipmi-sensors |awk -F\| ' $1 ~ /^(1[35-9]|2[01])\s*$/ &amp;&amp; $4 += $2 ~ /Ma?rgi?n/ ? 90 : 0 { printf "%s%20s%4d%s\n", $1 $3, $2, $4, $5 \ |"sort -k2,2 -k1,1"}'
 for i in {1..3}; do for j in {a..d}; do eval test_array$i+=\($j\); done; done
 for i in 1 2 3; do for j in {a..d}; do eval test_array$i+=\($j\); done; done
Ok, I'll try. Shift removes n number of arguments starting from the left side of the argument list. When we say `shift $((OPTIND-1))`, OPTIND means however many arguments were passed to getopts at this instance plus 1, so it means `shift remove all the arguments that were sent to getopts plus one minus one`. So, in your example where we have `$ ./script.sh -e -j -h google`, when we do `shift $((OPTIND-1))`, it means `shift ((4-1))`, that is, remove the first three arguments, which are `-e -j -h` so `google` now becomes `$1`. Now consider if we had said `shift $((OPTIND-3))` in your example, it would mean `shift remove however many arguments were passed to getopts plus one minus three`, that is `shift ((4-3))`, which means remove only the first argument, which is `-e`, so `-j` becomes `$1`. If we had said `shift $((OPTIND-7))` in your example, where 7 is a number larger than the number of arguments passed to getopts plus one, then we would get an error saying `shift count out of range` and your `$1` would remain `-e`. I don't see any practical use for `shift $((OPTIND-3))` or any other number except for 1 because when it's ((OPTIND-1)) all the arguments passed to getopts are removed. Hope it's clear now.
I polished it up a bit more. ipmi-sensors |awk -F\| ' $2~/Fan|Ma?rgi?n|Temp/ &amp;&amp; $4+=($2~/Ma?rgi?n/?90:0) { sub(/Ma?rgi?n/, "Temp", $2) printf("%-48s%4d%s\n", $1$3$2, $4, $5) }' |sort -k2,2 -k1,1
...how the hell. I'm working on unpacking that line of code. I really need to learn awk.
I think you did a great job. And knowing the other OPTIND values besides 1 as not being practical to use really sealed the deal. Thank you kindly for all your help, I understand it now and really appreciate it. 
[Effective Awk Programming, 4th ed., *Robbins*](https://ebooks-it.org/1491904615-ebook.htm)
Which line? Sorry, that last link was to a sketchy site. Here's [the official Gawk manual by Arnold Robbins, certifiable genius](https://www.gnu.org/software/gawk/manual/gawk.pdf)
Is `rm -f ${TMPFILE}` needed? The previous line moves it to `${FILE}`, so it shouldn't exist anymore.
it might already exist, I don't check upfront‚Ä¶ having it in with `-f` should be fine here.
 youtube-dl -qo- "$1" |ffmpeg -i pipe:0 -c copy "$(youtube-dl --get-filename "$1")" Does this work for you?
You can't use variables or command substitution in sequence expressions.
First of all thanks so much! I've done some digging and come up with this! echo "TeeTest" | tee Day$(seq -s ".txt Day" 1 1 $(cal | awk 'NF {DAYS = $NF}; END {print DAYS}')).txt It works exactly how I want it to but it seems awkward, anyways thanks for the help! 
 Cal=($(cal)); for ((i=1; i&lt;=${Cal[-1]}; i++)); do cp $file $file$i; done Negative array subscript `${Array[-1]}` is pretty recent Bash; if this breaks for you, do `${Array[${#Array[@]}-1]}`.
Just a tad simpler with seq -f "Day%g.txt" 1 1 ...
http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html Check this guide to learn about output redirection.
Worked with systemctl disable rsyslog 2&gt; /dev/null Thanks very much!
Processes have two output streams, standard output and standard error. `&gt;` only redirect standard output but standard error is still written to your console. You can redirect both with `&gt;&amp;`: foo &gt;&amp; /dev/null You can also redirect them individually; for example if you wanted to throw away standard error but still see standard output: foo 2&gt; /dev/null That said, I'd say be very careful about throwing away standard error. If there is bug in your logic or some unexpected runtime condition, it can be hard to notice without standard error. Instead what I recommend is to make sure your script doesn't cause anything to write to standard error. Then if something goes wrong you'll notice immediately.
This is very clarifying, thanks!
Glad I could be of help.
You want this systemctl disable rsyslog &amp;&gt; /dev/null It does both stdout &amp; stderr in one shot.
I've used python to make system calls with bash before. In my python script, I called a bash command to run tshark and tcpdump and got the results back into my python script to play with.
I think you did a great job explaining this. And knowing the other OPTIND values besides 1 as not being practical to use really sealed the deal. Thank you kindly for all your help, I understand it now and really appreciate it. 
I think the argument of BaSH vs Python is the wrong argument. As a sysadmin, I can count on BaSH being installed on every Linux system from minute one. My scripts will just work, no tweaking or modules needed. This makes automating install tasks super easy and portable. Add the core Utils, grep/awk/sed, and you have a simple to learn workhorse that will do 95% of your day to day stuff. That last 5% is where you should be arguing. Python vs Perl. The sysadmin's only choice used to be Perl. Its very powerful, especially for text processing, but relies heavily on comments for readability. Python beats it in this, and most other ways. 
I use bash for relatively small things, but avoid it if I'm creating a larger program. I'll usually move to python if I start thinking I need to add functions to bash, or it gets larger then say 30 lines. I just recently built a jenkins pipeline to build and deploy some code using bash. 
It's good for process control. Think about what you use bash for when you use it interactively at the command line - you're launching other programs, processing their output, possibly doing stuff like constructing pipes to connect programs together. Bash scripting is useful for automation tasks that involve launching various programs. Although Python can do this too, personally I find it quite cumbersome, especially once multiple processes and pipes are involved. However, I would use Python for anything that needed anything more than the most trivial processing of data, or if I needed to interact with APIs.
&gt;do people use bash scripts in conjunction with other languages like python? Awk or perl make more sense when used in conjunction with a shell imo.
Assigning/reassigning and redirecting STDIN, STDOUT and STDERR (0,1,2) file descriptors. Historically (especially pre systemd) it was the "glue" that kept linux distros together. 
Perhaps try this: echo -ne "${red}Input: ${reset}" read choice The `-n` makes 'echo' not add a line-break after the text it prints, and the `-e` enables it interpreting codes like `\e` or `\033` into Escape. The other idea I have is, you could change how you create that $red variable and make it contain the actual Escape character instead of a text like `\e`. Things will then work like you expect. The following will work: red=$'\e[1;31m' reset=$'\e[m' read -p "${red}Input: ${reset}" That `$'...'` interprets the `\e` text and puts the actual Escape character into the variable.
since bash 4.4 you can use the new special @P parameter expansion to expand a string the same way PS1 is expanded for the interactive prompt: red=$(tput setaf 1) reset=$(tput sgr0) prompt='\[$red\]Input:\[$reset\] ' read -ep "${prompt@P}" choice This properly handles readline redrawing the prompt too (e.g. when cycling through history lines)
I work doing computational chemistry. Different software is useful for different situations. I also do hundreds of iterations and isomers. So I have all sorts of scripts for converting input files into different formats. Automatically extracting data from output files, batch submitting files after each job has completed. Without bash my job would be ridiculously unproductive and tedious.
The rule of thumb (at least what I was taught) is that if your bash script gets to be more than 100 lines you probably need to write python scripts instead and glue them together with bash. 
whoami
Switching to another language assumes you're the one writing the software. I consider bash a language for using other people's software. If I have two non-trivial programs, maybe written in two different languages, lets say go and C. it might not be the right answer to rewrite those in a single language such that one can import the other. Instead you can use bash to automate the usage of those programs
That's so much cleaner thanks! 
Really depends on what it is you're doing. If it involves multidimensional arrays or bit level stuff instead of just text manipulation then sure. Though even then python isn't necessarily the only choice, Perl can do pretty much everything you could want as well. Even Awk would suffice in most cases. The length of a script doesn't really make much of a difference. There are thousands of lines long bash scripts that are just fine that way and wouldn't really be improved in any way for being written in an another language, especially Python. There are languages out there besides Bash and Python. So many people seem to be completely oblivious to this a lot of the time.
RemindMe! 10 days
I will be messaging you on [**2019-04-16 21:52:22 UTC**](http://www.wolframalpha.com/input/?i=2019-04-16 21:52:22 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/bash/comments/ba56i9/noob_discussion_what_is_bash_really_for/ek9vv3y/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/bash/comments/ba56i9/noob_discussion_what_is_bash_really_for/ek9vv3y/]%0A%0ARemindMe! 10 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
While I agree with you completely I think that if you are writing thousand line bash scripts and doing but manipulation you are probably at a point to make your own educated choice and thus the rule of thumb is meant more for beginners.
My view is that bash is good for text manipulation. Since \*nix operating systems are defined by flat text files, that also makes it very useful for managing systems. You can absolutely use python for all of that and more. However, python has versions and dependencies. Bash is pretty universal. It is like the superglue and duct tape of computing. 
Just wanted to say thank you to everyone who's been replying and giving me all this useful information. I plan on getting back to you guys individually to maybe get a better idea of what youre talking about. A lot of this is foreign to me, so please be understanding lol. Some programs ive written: - a very simple "spammer" that sends out texts or website links to multiple phones or emails - a simple file mover that moves videos that im editing from my c to e drive (which is my cold storage harddrive) - a couple of simple and short text based adventure games - im currently TRYING to make my own keylogger. The reason i bring these up is that i couldnt imagine using anything but bash for these. I also have a lot to learn though. I dont see how you could use python to "glue" scripts together which i actually find interesting.
The following line still downloads the file to a file descriptor, so I'm not really sure that's what you want. Anyways, here: lp &lt;(curl -s 'http://www.orimi.com/pdf-test.pdf')
It‚Äôs best suited for automating operating system tasks and saving you from typing the same things over and over. Whenever you find yourself getting irritated tying in the same thing, write a bash script or function so you can do it with a few keystroke. 
Most *nix utilities either support standard input directly or use `-` to refer to it as a file. Please let me know if you have any issues or questions. curl -s "$URL" |lp -
I do this all the time, just be aware it's only available in more recent versions of Bash.
So like, by more recent.... You mean over 10 years. Or put another way, only Mac users need to beware with their ancient version of bash.
I developed some Awk recently for a set of NACs running 2.04. They will probably never be updated. I don't support holding yourself back on account of dinosaurs, but being aware of limitations is good.
The bash scripts that are thousands of lines long aren't probably doing bit manipulation, though.
I know my point is that at some point you know the languages and their limits and thus are capable of making your own professional choice. Until then its good to have rules of thumb.
Yup!!
I always get a laugh at the expense of Posix tards, who try to claim the Bourne shell feature set is posix compliant. Reality is Korn shell is the Posix shell, and that shell has a rich set of features, and bash in general does a good job a covering those. However, Unix is dead, Posix is irrelevant, and bash is the new standard shell.... for better or worse. In defense of Apple not bundling the more recent version of bash, or awk.... Yeah, that is because of gnu-tards pushing GPLv3 license on those things, which from their perspective is cancer to corporation. All the patent forfeiture clauses, and similar.
&gt;Unix is dead, POSIX is irrelevant Dude, *I wish*, but have you seen how much that stuff pays? So much of our infrastructure depends on systems like those controllers I mentioned, which are locked off behind layers of corporate/government fuckery. It shouldn't be this way, but it is. As for GPLv3, I think Apple will be fine without you defending them. Free software brought us this far, and copyleft is what's required to keep it safe from the corruption that drags us all down.
Use `sysctemctl --quiet ...`. This is better than redirecting standard error, since it allows errors to be showed. `Unit ... is masked, ignoring` is not considered an error.
Ahh very nice. Thank you so much for the explanation. This will come in very handy.
Bash is for speaking to your computer in a simple language.
Just recently switched to using Linux full time. my past two classes have been mostly on C. Our school uses a Linux server for the CS department. Most of my scripts a one liners just to open or close a few things. Like sshfs mount my project folder, open atom and launch another shell window and ssh so I have one local and one remote. Pressing 4-5 key strokes to do all that instead of clicking around or writting out separate commands saves so much time.
Squeezing some more: cal | sed -En '2,$ s/([0-9]+)/Day\1.txt/gp' &amp;#x200B;
We used Korn shell (a bash alternative) at work for process control and data processing. We processed large volumes of mobile phone data. It came to us as comma separated text. Shell scripting was great for searching, editing and extracting data.
Both are equally useless.
Kind of like your comment?
No, his comment made me laugh.
Right. We're only in it for the lulz. Asshole culture is alive and well.
Nice reading about shell options and modifiers, but the image is wrong. The book is not one about docker, but another one name "Learning shell the hard way".
Did you actually look at the code and did you see it's beauty? 
If you're going for style points, you could do the comparison and decrement in the same place: ..() { i=${!:-1}; while ((i--)); do cd ..; done
Indeed I could. Thanks for pointing that out.
The point of my comment wasn't really the short function. It was that these short aliases are pretty useless and seem to be mostly favored by inexperienced shell users. Go to any "cool shell functions", "cool shell aliases" etc. thread and you'll see countless variations of what OP posted and `alias la="ls -la"` repeated ad nausea.
Bash refers to both the shell and the command language. As a shell it acts as the interface between the user and the operating system. This means it can help the user carry out system administration tasks like working with files (including file permissions and metadata), process control, searching files and text, creating and managing user accounts, configuring networks etc. The command language allows one to combine together multiple commands in the form of a script using things like loops and conditionals. It is really good when you're working with multiple processes and streams that communicate with each other. The operators like redirection, pipes, putting jobs in background, subshells etc are incredibly useful and efficient running multiple processes. Most Linux like systems also comes with multiple other tools like `awk`, `sed`, `grep` etc which can work seamlessly with bash language to make the scripting much easier. &amp;#x200B; Python is a general purpose language that has some powerful modules like `os`, `subprocess` etc which allow you to do some of the tasks that you can do with bash. It may ultimately depend on personal preferences. I find using bash and tools like awk, sed etc easier for a lot of tasks even though I use python for most of my programming tasks. You can also check out the [xonsh](https://xon.sh/index.html) shell which combines bash and python into a unified language and simplifies a lot of things. I haven't used it but have heard good things about it.
&gt;In computing, a shell is a user interface for access to an operating system's services. [https://en.wikipedia.org/wiki/Shell_%28computing%29]: I see the command line interface (together with configuration text files) as an API that unifies calls to arbitrary applications / scripts. Every application then is a kind of function wich might take parameters and returns a value. Configuration values are static constants / defines. Or per-instance constants if you can pass a configuration file with each call (for example ssh). Background services are maybe comparable to singleton objects which process calls in their own process. 
He posted his code after I made my comments. All he had was the snarky, useless command and no code.
Actually my 16 year old son thinks alias la="la -la" is cool. You don't start playing baseball trying to hit 100mph fast balls.
http://www.bashoneliners.com/oneliners/popular/ Needs some contributions, it is just starting off. 
reminds me of [commandlinefu.com](https://commandlinefu.com) &amp;#x200B;
in this day and age if u got the curiosity 100mph is the first pitch, lol
Change your first line to this: **vlc $bfile &amp;**
very nice, probably an imitation of commandlinefu.com since it's so popular. 
I tried this and it only works about half the time. After doing some more research, I found the following solution: vlc $bfile &amp; sleep 1 wmctrl -r "$bfile" -e 0,$xoffset,$yoffset,$xwindow,$ywindow This one works much better, but it still has some situations where it won't properly size the window. I think this is because the BASH script proceeds to the next line without waiting for VLC to fully initialize, and the ***wmctrl*** code is executed before the VLC window even shows up. Pausing the script for a second gives it time to fully launch VLC. It still fails about 10% of the time to properly size the videos. Is there a better way to wait for VLC to initialize?
I ran into the same problem. I wanted to start up 6 programs and place them into the preferred workspace, but it never worked because of exactly what you described. Here's the script I use; obviously you aren't interested in moving the window, but the concept is the same: `#!/bin/bash` `# This script moves my preferred programs to their proper desktops` &amp;#x200B; `# Define program windows that we want to move` `window=( Chromium Evolution Console VirtualBox HomeBank LibreOffice )` &amp;#x200B; `# Move programs` `for i in "${!window[@]}"` `do` &amp;#x200B; `# Wait till we get at least one matching window` `until [ -n "$(wmctrl -l | grep "${window[$i]}" 2&gt;&amp;1)" ]` **# this is the key line** `do` `sleep 1` `done` &amp;#x200B; `# Send it to the appropriate desktop` `wmctrl -r "${window[$i]}" -t $i` &amp;#x200B; `done` &amp;#x200B;
Enjoy, curious guy.
&amp;#x200B;
Here's something for ya, than: [https://pasteboard.co/I97Xzxp.png](https://pasteboard.co/I97Xzxp.png)
whoa there big guy, could i see that in bash?
Can you tell the window geometry directly to vlc ? I see --video-x ... --width options. My window manager seems to override the x &amp; y. Maybe you have better luck.
Any bash function can be written in one line!
I think the problem is not docker but the use of Curl. To make curl return none zero based on http errors the --fail switch is required. See curl's man page.
It is not harmful, as OPTARG is a variable just like any other that just happens to be modified by the `getopts` builtin. But I don't know that I would call it good practice. It can make the code a bit confusing to read as it is not customary to use that as a normal variable. What I would do instead is pass the file name as an argument to the create_files function when calling it, and then refer to that argument (`$1`) within that function. This also works with parameter expansions like `${1%"$SUFFIX"}`. Also, to avoid potential clashes with environment variables and shell special variables, using all caps for regular variables is not recommended. 
This will work, but I'd consider passing the filename as a parameter to the function: for file in *.MTS; do [ -f "$OPTARG" ] || break create_files "$file" done And then use `$1` in place of `$OPTARG` in your function. This has the benefit of being easier to follow in your main program (it's clear that you're passing a value to a function) and being easier to use your function elsewhere (you don't set a variable, you just pass a value). &gt; PS I can't seem to find the syntax to make [[ $OPTARG =~ \.MTS$ ]] use the variable SUFFIX in place of .MTS as it should, can anyone help? `[[ $OPTARG =~ $SUFFIX ]]` treats `SUFFIX` as holding a regex pattern. Instead, you could use `=` with globbing: if [[ $OPTARG = *$SUFFIX ]]; then ...
&gt; if [[ $OPTARG = *$SUFFIX ]]; then ... Note that `$SUFFIX` there should be double-quoted (even in `[[`) in case it contains glob/wildcard characters (`?`, `*`, `[`, `]`) itself. if [[ $OPTARG = *"$SUFFIX" ]]; then ... 
Ah, thanks, edited... This is always a gotcha for me, since I bounce between sh/bash/zsh. (Zsh requires `${~foo}` to expand globs, and `${=foo}` for word splitting).
The first step in writing optimized pipelines, is to not use pipelines That is one of the most significant ways to slow down a bash script.
&gt;\[\[ $i == @(KVM|VMWARE|XEN) \]\] \[\[ $i == @(KVM|VMWARE|XEN) \]\] &lt;&lt; Could you please tell what '@' will do that is used before (KVM|VMWARE|XEN)
Yes, agreed, and as I point out, the most common misused pipeline is `cat file | grep pattern`, which can be totally replaced by a single command, `grep pattern file`, without using the pipeline in the first place.
I'm pretty good at avoiding `cat` abuse, but I'm still trying to find a better alternative to this common pattern I use for filtering logs: command | sort | uniq -c | sort -nk1 | tail
I solved this, well at least am getting the behaviour I was looking for. I noticed CentOS doesn't have this file as Ubuntu does. &amp;#x200B; I modified /usr/share/bash-completion/completions/python and changed the \_filedir line: &amp;#x200B; \# if -c or -m is already given, complete all kind of files. if \[\[ "${words\[@\]::$cword}" == \*\\ -\[cm\]\\ \* \]\]; then \_filedir elif \[\[ "$cur" != -\* \]\]; then \#\_filedir 'py?(\[cowz\])' \_filedir else COMPREPLY=( $( compgen -W '$( \_parse\_help "$1" -h )' -- "$cur" ) ) fi &amp;#x200B;
So what do you have so far? Some code might be helpfull.
Use tshark cmd tool. Something like that: tshark -i eth0 -f tcp -w \~/eth0.pcap
That's not a *bad* idea but it shouldn't be necessary to install a whole new package when tcpdump can generate an equivalent pcap. 
tshark is a part of the Wireshark package. At least on Fedora.
But he's asking how to run the capture on a machine other than the one where he's running wireshark. 
If it's a server I'd personally prefer installing tcpdump over wireshark.
run the command without the + format. see what you get. run the command with just +y and see what you get etc. that should help you figure it out
Use `tcpdump`'s `-w` switch to save to a pcap. Wireshark will be able to read the resulting file.
Cheers, I took your advice. Please see my edited post and new refactor :)
Thanks, great advice. Please see my edited post and new refactor :)
I already try but without the "+" the output was just an error like " incorrect date" 
Looking at the man page for date you see: &amp;#x200B; SYNOPSIS date \[OPTION\]... \[+FORMAT\] date \[-u|--utc|--universal\] \[MMDDhhmm\[\[CC\]YY\]\[.ss\]\] &amp;#x200B; &amp;#x200B; Since the format is optional, whoever wrote the 'date' command decided to have you put a + at the beginning of a format string so the program could easily determine that the argument is meant as a format specifier. The + character has no other use and is otherwise ignored but if you're wanting to give a format string that's how you tell the program you are.
Ok.. I readed it but I carried on regardless this. My bad Thanks man! Have a nice day
An operand with a leading plus (**+**) sign signals a user-defined format string which specifies the format in which to display the date and time. &amp;#x200B; [https://www.cyberciti.biz/faq/linux-unix-formatting-dates-for-display/](https://www.cyberciti.biz/faq/linux-unix-formatting-dates-for-display/)
[https://explainshell.com/explain?cmd=DATE%3D%60date+%2B%25Y-%25m-%25d%3A%25H%3A%25M%3A%25S%60](https://explainshell.com/explain?cmd=DATE%3D%60date+%2B%25Y-%25m-%25d%3A%25H%3A%25M%3A%25S%60)
Depends on your screen resolution and font size?
Depending your system, `sort -u` includes `uniq`ing.
Yes, that's true. However, he uses the `-c` argument to `uniq` which outputs the matching lines grouped by their count. And you can't use `sort -u` for this purpose, you'd have to pipe once more through `uniq`.
But that doesn't prefix each matching line with the number of times the line occurs, which is kinda the point. I want to see how many times a line occurs in a log file to determine patterns. A common example is finding patterns in web access logs. For example, to see which IP addresses make the most requests to my web server I usually do: $ cut -d ' ' -f 1 /var/log/httpd/access_log | sort | uniq -c | sort -nk1 | tail 2 66.249.69.167 3 157.55.39.34 3 59.17.71.1 4 185.53.91.24 4 193.106.30.98 33 37.59.55.45 422 122.152.214.116 439 186.23.87.169 447 118.24.124.84 452 154.223.78.58 I don't think `sort -u` can do this.
In that pipeline uniq counts identical lines, which can't be done directly by sort.
would be useful to have a functionality to filter on (select) multiple tags
The whole filtering pipeline (so everything after `command |`) can be replaced by a pure bash script/code. The sorting is the only thing that needs a significant amount of code and complexity in pure bash, but you only need sorting to count the occurences and to select the ones that repeat the most. &amp;#x200B; In pure bash programming, you can select unique values by using associative arrays. Each key name will be the log line you want to filter, and each value of that key will be the number of occurences. A `while` loop that reads the output of `command` line by line, populates the associative array and outputs them at the end. &amp;#x200B; A manual sorting of the values selects the top 10 keys (tail outputs 10 lines by default) and outputs them. &amp;#x200B; I would do it in pure bash if the whole context is in bash. Otherwise, you pipeline is as good as it can get, even though it runs 5 commands in parallel.
Don't think there is much you can do. You could do the entire thing in gawk or perl but it's not something you would type out as it's too long for that. You can also replace the initial `sort | uniq -c` with awk. While not that hard to remember the script, it makes the whole thing quite a bit longer. If you have a use for it, here's the gawk version (you can use `-vn=5` on the commandline to limit to the top 5 instead of the default 10): #!/usr/bin/gawk -f BEGIN { if (!length(n)) n = 10 } { seen[$0]++ } END { for (key in seen) { sorted[seen[key] " " key] = seen[key] } PROCINFO["sorted_in"] = "@ind_num_desc" i=0 for (k in sorted) { if (++i &gt; n) exit 0 print k } }
The question then is if doing it in pure bash is even worth it. Given that `sort` is written in C and is _usually_ quite fast I'd be concerned that the overhead of associative arrays in bash would be far worse than the overhead of these multiple pipes. I should probably just add an alias to my bashrc so I don't have to feel bad typing out all those pipes. It won't make it any more efficient, but at least I won't have to look at it.
Yeah definitely.
Man... I should learn more awk. Thanks!
Breaking the problem up, A shell script is just a plain text file, typically named with the `.sh` extension and starting with a "shbang" for the shell / interpreter: ``` #!/bin/sh ``` You can read a single character into a shell variable with ``` read -n 1 user_input echo ${user_input} ``` You can compare the variable against a string: ``` if test "${user_input}" = "D"; then echo "Delete!" elif test "${user_input}" = "R"; then echo "Rename!" elif test "${user_input}" = "Q"; then echo "Quit!" else echo "unsupported input!" fi ``` You can read an input longer than a single character again with `read`: ``` read file_name echo ${file_name} ``` Read `man test` to get a sense for how to check for string equality, file existence, directory existence, etc. Wrap the entire flow of logic in ``` while true; do echo "my main loop" done ```
Hi Guys, thanks for the comments. So this is what I have so far. &amp;#x200B; ssh username@hostip "tcpdump -i eht0 -w -" &gt; File.pcap &amp;#x200B; I have this working from command line so that it captures the traffic on the remote machine and saves the .pcap file on my host machine. I haven't put it in a bash script yet but that's the next step. Will it work like this? &amp;#x200B; \#!/bin/bash ssh username@hostip &amp;#x200B; "Tcpdump -i eth0 -w -" &gt; File.pcap &amp;#x200B; I'm trying to separate as much as possible for notation reasons. Thanks for all your help by the way these comments and links have been very useful.
```help [[```
I'm not seeing any mention of it _evaluating_ the expression in that output (there's actually no mention of the math compare operators in that output at all, actually). It's just saying to look at the `test` builtin. the output for `help test` doesn't mention that it evaluates it, either.
Yes, that feature has been there ever since `[[` was invented by the Korn shell. But it's little used because it's sort of an ugly hybrid and you might as well go fully arithmetic with the `((` arithmetic command: `((1+1 == 2))` etc.
Thanks for being upfront about this being homework. Very few people here want to do you homework for you so you should at least make an attempt at the assignment and ask for help with specific problems. Nobody is going to spend the time to help you with your homework and explain how/why it works if you don't put in any effort.
You're going to want to set up ssh keys if you haven't already, otherwise you're going to be prompted for a password.
Yep! that's actually what I'm working on right now.
also though this is just character comp with `[[`? for instance `[[ "1+11" -eq 012 ]]; echo $?`
The 'grep with multiple arguments' example doesn't actually work. The pipelined greps will at the end output the lines that match *all* patterns, while the single commands will output the lines that match *any* of the patterns. You can run and compare outputs: seq -w 0 1000 | grep 07 | grep 00 | grep 72 seq -w 0 1000 | grep -E "07|00|72" Shell pipelines by themselves are unlikely to be the performance problem. It's much more likely to be either the command that gathers the data to filter, or one of the intermediate steps. You can run and compare outputs: bash -c "$( echo -n 'time seq -w 1 1000000' ; for i in ; do echo -n '|cat' ; done ; echo ' &gt; /dev/null' )" bash -c "$( echo -n 'time seq -w 1 1000000' ; for i in {1..1000} ; do echo -n '|grep ^02222' ; done ; echo ' &gt; /dev/null' )" bash -c "$( echo -n 'time seq -w 1 1000000' ; for i in {1..1000} ; do echo -n '|cat' ; done ; echo ' &gt; /dev/null' )" bash -c "$( echo -n 'time seq -w 1 1000000' ; for i in {1..1000} ; do echo -n '|grep .' ; done ; echo ' &gt; /dev/null' )" On a vm on my laptop the first one takes about 0.7 seconds, with no pipeline at all. The second one takes 1.5 seconds and it's a pipeline 1000-deep but with efficient filtering early on. The third command takes about 7 seconds, even though we're still running a pipeline a thousand levels deep and just passing about 8MB of text across all those processes. The fourth command does the exact same thing but much slower, at about 50 seconds total. A typical example for additional I/O are find and du on directoriy hierarchies with tens of thousands of files. Further down the pipeline a (predictable) memory consumer is sort, which needs to buffer all input to sort it. Anything that uses regular expressions can blow up in CPU usage if the input has really long lines and the regexp can match long stuff.
You can use `sort -rn` to get the highest amount at the top of the list. `sort -u` is basically a less robust version of `uniq`.
Yes, that is indeed a good question. Bash is known to be slow at times, it says so in its own man page üôÇ
No, that's a different issue... a leading 0 denotes an octal number, so 012 == 10.
Yeah I completely understand that, sorry if you get these types of questions a lot. We've tried to figure it out, but we're so new to this that we don't even know which questions to ask to get the information we need in most cases.
If there are a lot of duplicates, this will be much more efficient in terms of memory: command | awk '{c\[$1\]++}END{for(x in c)print c\[x\],x}' | sort -nk1 | tail
i'm not sure that i should be as surprised as i am that i had no idea about that. thanks
yeah that was my thought, too. I use `((` all the time for stuff like this. I just happened to have some old library code that had a string value (`foo/bar`) and I got a divide-by-zero error, which was confusing the hell out of me until I figured out where this was coming from. you have no idea where it's documented, though, huh?
`info bash` under [6.4 Bash Conditional Expressions](https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html): 'ARG1 OP ARG2' 'OP' is one of '-eq', '-ne', '-lt', '-le', '-gt', or '-ge'. These arithmetic binary operators return true if ARG1 is equal to, not equal to, less than, less than or equal to, greater than, or greater than or equal to ARG2, respectively. ARG1 and ARG2 may be positive or negative integers. When used with the '[[' command, ARG1 and ARG2 are evaluated as arithmetic expressions (*note Shell Arithmetic::).
ah, dope! I didn't think to look there. I was poking around the tldp site and other `help` and `man` pages + some light googling. Thanks! if I could give you two upvotes, I would.
That's the thing with GNU stuff, they consider the `info` manuals to be the only complete and canonical manual and the man pages tend to get neglected. It's annoying.
I'd start with this: grep -v -e '^5$' -e '^22$' -e '^60$' serverlist.txt However, rn I can't find a way to generate this command from the `excluded_server` array.
Online [bash tutorial](https://duckduckgo.com/?q=bash+tutorial&amp;t=ffcm&amp;ia=web) \- usually about lesson 2 or 3. Copy past, bit of editing - you will have to read the article first though.
Nevermind, here it is: grep -Ev "^($(IFS='|';echo "${excluded_server[*]}"))$" serverlist.txt
Well, when you say you tried to figure it out, what did you try? Did you figure out how to create a bash script and get it to print out something simple? Did you figure out how to read input from the user? Can you loop until some condition is met? These are all building blocks on how to put together a script or program. You are taking a class that should be giving you all of these building blocks so you shouldn't be totally lost and not even know how to create a basic script. If you came here saying "I created this script but can't get it to run but I get this ... error instead" then I'd be happy to help with something as basic as that.
Even better, it can be used to trick a script into running arbitrary code. For example, this simple script can be exploited purely by entering a suitable value at the prompt: #!/bin/bash read -p "Guess a number: " n if [[ $n -eq 42 ]] then echo "You guessed it!" else echo "Wrong!" fi Example: $ ./guess Guess a number: 42+a[`date&gt;&amp;2`] Mon Apr 8 14:08:26 PDT 2019 You guessed it!
We can do basic scripts like those, yeah. I guess the main issue comes from incorporating parts 3 and 4 and 5 into the rest of it. Is it done with case statements, is it done with if statements, is it done with if statements inside of case statements? We don't know what we need or how to weave them together to make it work.
[[ was a mistake.
Thanks for the impressive effort/reply! I‚Äôll take a look and adjust the blog post if necessary.
&gt; That's the thing with GNU stuff, they consider the info manuals to be the only complete and canonical manual and the man pages tend to get neglected. It's annoying. I have a contrasting view: Info pages are _far better_ at documenting complicated programs like Bash, since the information is hierarchical and cross-referenced. They're just like web pages.
You can use either if statements or switch/case. The rule of thumb for me is if it's fewer than 2-3 conditions, then I default to if statements. It really is up to you which you want to use unless the assignment specifies you must use one or the other. Think about the flow of the user input and start with the outer loop. What is the biggest cycle where the program basically resets to the beginning and starts over. That will be your outer loop. What conditions will cause that loop to repeat vs terminate/exit. Typically if your outer loop terminates, then the program will exit. Then work your way in to each of the conditions where the user decides what operation that particular iteration of the main loop will perform. Now within that operational condition, what are other inputs and loops that may or may not be needed. Start there and build your way inward. If you show what you've tried, i.e. post your code, and describe what you are having trouble with, then you'll get much better advice.
Is it alright if I PM you my code for you to have a look at?
You could use the array to filter the input to the `while` loop, e.g. while read -r hostName; do printf '%s\n' "${hostName}" done &lt; &lt;(grep -vf &lt;(printf '^%s$\n' "${excluded_server[@]}") /tmp/serverlist.txt) So what this does is prints each element of the array on its own line with the regex line anchors `^` and `$`, denoting a start of line and end of line respectively. We do this so that, for example, `5` only matches `5`, and not `5`, `35` and `55`. This is wrapped into `&lt;()` which appears to `grep` as a file. We've told `grep` to exclude (`-v`) items found in a file (`-f`). As an alternative, you could maybe use `grep`'s exact match option (`-x`) e.g. &lt;(grep -vxf &lt;(printf '%s\n' "${excluded_server[@]}") /tmp/serverlist.txt) This is all wrapped in its own `&lt;()`, which is then fed into the `while read` loop. There's probably a cleaner way to do it using `&lt;&lt;&lt;`, but I'm only one coffee into my day...
if you quote the `$n`, does this still evaluate the same way?
Yes. Quoting does not prevent this.
interesting. wow. ok. that's... no good. any idea how to get around that? or just use `test`?
 while read server; do unset blacklist for exclude in ${excluded_server[@]}; do [[ $server == $exclude ]] &amp;&amp; blacklist=1; done ((blacklist)) || echo $server done &lt; serverlist.txt Basically your file `serverlist.txt` is your main loop, while your array `excluded_server` is your recursive loop. This means that you're going to process that array for every line in your file. `((blacklist)) || command` means "Run command if `blacklist` is not a numeric value higher than 0".
Yeah, it's not great. &amp;#x200B; If you are processing external data, you can use e.g. \`${n//\[\^0-9\]/}\` to sanitize it. Or \`test\` like you say.
Awk is so underrated: $ blacklist()( IFS=\|; awk -v re="^(${excluded_server[*]})\$" '$0!~re' "$@" ) ____ $ blacklist serverlist.txt 18 35 55
Doh. I missed the `-c` for the counts!
It's an `extglob` feature, enabled by default within double bracket tests since 4.2 or so; `@()` matches any of the strings within it, separated by `|`. So it returns true if it's equal to either KVM, VMWARE, or XEN.
What's your output for `type date`? Maybe an alias/function is already bound to it.
Don't you need to escape that last `$` in order to anchor the regex?
 $ blacklist(){ local IFS=\|; awk -v re="^(${excluded_server[*]})\$" '$0!~re' "$@"; } $ blacklist serverlist.txt 18 35 55
Anchored and avoiding subshell: $ blacklist(){ local IFS=\|; awk -v re="^(${excluded_server[*]})\$" '$0!~re' "$@"; } $ blacklist serverlist.txt 18 35 55
I have a hard time getting rid of pipelines when trying to format html or json into colorized csv It‚Äôs usually a mash of some tool from GitHub and awk/sed :p
out : date is in /usr/bin/date
I don't think the for loop is even necessary. This is my solution. The seq should be replaced with the actual exclude and server lists. while read -r exclude; do (( excluded[$exclude]++ )) done &lt; &lt;(seq 1 2 10) while read -r server; do (( excluded[$server] )) || printf '%s\n' "$server" done &lt; &lt;(seq 1 10)
Some possibly helpful things.. Wait until an application has started: [xdotool (mentioned at bottom of thread)](https://ubuntuforums.org/archive/index.php/t-1912264.html) More options for launching VLC: [VLC flags (throughout this thread)](https://forum.videolan.org/viewtopic.php?t=124021) Run processes in their own shell: `$(osascript -e ‚Äú...‚Äù &amp;)` [Subshells (a great answer on stackexchange)](https://unix.stackexchange.com/questions/442692/is-a-subshell)
thanks , a very valuable information.
 grep -Fxf &lt;(printf '%s\n' "${excluded_server[@]}") serverlist.txt
Good application of AAs. Have you had similar problems with flow control?
Good application of AAs.
Not right on the mark but related‚Äîthis is a really useful cross-platform tool: [shx](https://www.npmjs.com/package/shx) . It allows you to run a standard set of bash commands on virtually any machine. Install it in 15 seconds with `$ npm install -g shx` and you‚Äôre able to run `$ shx &lt;command&gt;` with your familiar bash commands `cd`, `rm -rf /*` etc.‚Äîeven on Windows.
But isn't the while loop completely unnecessary? It's just printing every line, which grep already does.
I agree with the first paragraph. The default info viewer is probably the worst choice possible. `pinfo` is what most people should use unless they are emacs users in which case they should just use emacs.
&gt; The default info viewer is probably the worst choice possible. I've never found it _particularly_ bad. I mean, sure, having a movable cursor is somewhat odd, but that's not specific to `info`. Both Firefox and Chrome have a feature called Caret Browsing, for instance. Anyway, in `info` I tend to mostly use Tab, Shift-Tab, Page Up and Page Down. Really, the most egregious of `info`'s default settings is that the one can scroll past the front or end or end of node to go the previous or next node. I much prefer setting `scroll-behaviour=Page Only`.
I suspect you've been confused by my example and its intent. Consider this instead: while read -r hostName; do ssh "${hostName}" ls /tmp done &lt; &lt;(grep -vf &lt;(printf '^%s$\n' "${excluded_server[@]}") /tmp/serverlist.txt) Relevant bit of OP's post: &gt;I'm doing a while look [I suspect OP meant 'loop'] through each line in the serverlist.txt file. However, I want to ignore doing anything to server id's that are in the excluded_server array. Any ideas how to do this? So the solution I offered filters the `serverlist.txt` file of the unwanted items and presents the rest to the `while` loop, this way the `while` loop doesn't have to have any logic built into it for making determinations about its input. The example I gave was intended to prove that both the filter and the `while` loop are working as intended.
You're right, I was confused.
You're question does not make sense.
Whoami
You can always use `w` or `who` to see active logins.
It says 'root', i tried it
Sorry if I didn't made it clear, I need to get the username of the current user when I'm logged as root in the terminal. The idea is: $ su \&gt; password \# echo ${USER} prints my username and not 'root'
Use `su -` instead of just `su`
```su -``` The - assumes the users full env by making it a full login. See man su.
Ty! I added this in order to print just the first work, just in case someone other needs it: who -u | head -n1 | awk '{print $1;}'
Well, you're asking your system "who is logged in on this terminal?" - if you go $su root #whoami or $su #echo ${USER} then root is logged in. By $su, you're putting the "root" nametag on your shirt and asking your system "what's on my nametag?".
Meaning your prompt? Your username or some other one log in? &amp;#x200B; whoami &amp;#x200B; should tell you yours. &amp;#x200B; w &amp;#x200B; should see everybody username that is currently login &amp;#x200B; &amp;#x200B; Your Prompt [https://www.cyberciti.biz/tips/howto-linux-unix-bash-shell-setup-prompt.html#comment-173739](https://www.cyberciti.biz/tips/howto-linux-unix-bash-shell-setup-prompt.html#comment-173739)
Take a look at logname
How about grep -E '^[^:]*:x?:'$(cat /proc/self/loginuid) /etc/passwd | cut -d: -f1
Yes. Because you *are* logged in as root
That will yield unpredictable results on a multi-user system.
 I was able to get this running like so. &amp;#x200B; \#!/bin/bash ssh [root@h](mailto:root@10.1.3.3.32)ostip ‚Äútcpdump -s 0 -U -n -w - -i eth0 not arp‚Äù \\ &gt; /home/root/Desktop/BashPcap/01Bash.pcap &amp;#x200B; I'd like to kill the tcpdump within the script as well. I found this code on stack exchange but I cant seem to get it working. Any other ideas would be appreciated. pid=$(ps -e | pgrep tcpdump) echo $pid #interrupt it: sleep 5 kill -2 $pid
Use sudo instead of su and you will have a $SUDO_USER.
That's already a bash feature! You can enable it with this command: shopt -s autocd To make it permanent, add it to your ~/.bashrc file.
Shanghai had [a store called `ba&amp;sh`](https://i.imgur.com/atcza61.jpg). Lot of apparel and very few shells, though.
I cannot for the life of me get the wireshark portion of this to work. I keep getting 'wireshark -r: command not found. It works in command line fine \#!/bin/bash ssh root@hostip "tcpdump -c 1000 -s 0 -U -n -w -i eth0" \\ &gt; /root/Desktop/BashPcap/02Bash.pcap ; **"wireshark -r" /root/Desktop/BashPcap/02Bash.pcap**
rename command. Ex: http://cheat.sh/rename
`-readable` needs GNU findutils 4.3.0 or newer. If you are not using Linux you may not have `find` from GNU findutils but some other version of `find`.
Please don't use the term crypto for cryptocurrency.
I use `up () { cd $(printf "%0.s../" $(seq 1 $1)) };` so that `cd -` takes me back to where I was. Running cd n times leave you where you unable to jump back.
Your question is not really clear. If you're trying to log in as root from an already active shell, can't you just save the username in a variable before loggning in as root and reference that variable after? Im far from a bash expert but thought I'd give my two cents
if \[ "$(which command)" == "" \]; then
How can I have bash run this anytime I the into the console
What's the output of `type ls`, out of curiosity? Does `\ls` behave any differently for you? What is the output of `locale`? And have you checked that the font settings in Konsole are using a font that supports the non-English characters?
It won't go away
...the fash.
I'm not sure if I understand your need completely, but have you tried tee?
Yes.
Put it in a function then make another function that calls the first after a sleep? Idk. Bash ain‚Äôt js.
Use the control operator &amp; to run the command in the background. Or see https://superuser.com/questions/235072/run-parts-of-bashrc-asynchronously for other suggestions.
Adding `&amp;` makes the Pyenv environment not active, as I get the system default when I run `python`.
Does it write to another file or standard out? If it writes to standard out, and the other command takes input from standard in, a pipe should work. If not, the perhaps redirecting the output would work (using &lt; or &gt;). It would be helpful to have more info on how your program writes the data (where) and what commands you intend to send that data to.
&gt; Does it write to another file or standard out? It writes to another file. &gt; It would be helpful to have more info Sure: My application manipulates a CSV and writes it to same directory where the original CSV is located. I now want a second application to catch that write, manipulate it and finally write it to a file. Thanks for you help.
Maybe you could set things up so that this will only run once when you login to your user account, instead of every time you open a terminal window? I don't fully understand how bash startup scripts are run. I think the files to try would be either `~/.bash_profile` or `~/.profile`, those will not run every time you open a terminal window. Details about how bash runs its startup scripts are towards the beginning of `man bash`, in the section "INVOCATION". You can jump to that section by typing `/^INV` in the normal man-page viewer.
But this would be a major break when I don't add this as an option.
I think your best bet would be to modify your program to write to standard out or perhaps give you the option to. You could also just cat the file after it‚Äôs created and pipe it to another command: go run app.go; cat outputfile.csv | wc I‚Äôm not sure of a way to capture the write on the fly without modifying your program. Maybe someone smarter than me knows a way though.
Do you need `virtualenv-init`? [It seems to be recommended not to use it](https://github.com/pyenv/pyenv-virtualenv/issues/132). For `pyenv init -`, you might just put the output of it at the end of your bashrc: pyenv init - &gt;&gt; ~/.bashrc It sets some env variables and defines a function or two.
This would be ideal. Always better to put in profile rather than bashrc.
It‚Äôs called a fifo or a named pipe
As another poster said, using /dev/stdout and then an ordinary pipe would work. Another option is to use a named pipe, aka a fifo. It looks like a file but behaves like a pipe. mkfifo my_fifo; my_program -o my_fifo &amp; second_program -i my_fifo -o output_file; rm my_fifo
I think in this case you'll need to a "block" background like. { Code } &amp; Even still until the commands are finished, you'll still only get the system python.
Maybe try asdf https://github.com/asdf-vm/asdf
Is there some reason you wouldn't add it?
You need to use a temporary file. There is no way to pipe this.
Do the results of those evals help with bash autocomplete by any chance?
In your bash prompt, type *alias*, hit enter, and see if the *ls* command is aliased to anything. It probably has some color-coding option enabled, that your terminal doesn't like. Many distributions have aliases configured for a number of common commands, such as *ls* or *rm*... They do this to add commonly-used flags to the command, so that you don't have to type them every time. Run the *alias* command to see all of the bash aliases applied to your session. You can temporarily ignore aliases by running the command with a slash in front of it: *\ls*. Or you can edit the relevant .bashrc file and remove the alias entirely. Usually the system-wide aliases are under /etc/bashrc, and the user-level ones are in your home folder under a hidden file like .bashrc or .bash_aliases.
Ye, that's far simpler. Thank you!
Since this thing is initializing variables and functions, putting it in the background won‚Äôt allow it to do what it needs.
Thanks for the reply. This is my alias ls output: ls='ls --color=auto' I've spent a lot of time playing around locale - nothing seems to have helped. But this is my first delve into locale, so I may have missed something or done something wrong.
 ray:~/ $ type ls ls is aliased to `ls --color=auto' ray:~/ $ locale locale: Cannot set LC_ALL to default locale: No such file or directory LANG=en_US.utf8 LANGUAGE=en_US LC_CTYPE="en_US.utf8" LC_NUMERIC=en_US.UTF-8 LC_TIME=en_NL.UTF-8 LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES="en_US.utf8" LC_PAPER="en_US.utf8" LC_NAME="en_US.utf8" LC_ADDRESS="en_US.utf8" LC_TELEPHONE="en_US.utf8" LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION="en_US.utf8" LC_ALL= If I paste a non-English character into Konsole, it displays it properly. I only run into a problem with **ls**.
You don't specify the output file as a command-line option already?
I think this is what you are looking for: https://github.com/dylanaraps/nosj
I should've mentioned that no, I don't want external libraries. I want a pure solution.
That's neat.
That nosj is just bash so you can include it in your script. The license seems to be MIT so just remember to include that and attribution and you should be fine.
Guess it really depends on what kind of developers you're working with. If it were me, I'd be asking "why didn't you just use jq, it's so easy to install, and it's the right tool for the job."
Just a noob here, but have you considered using jq to get the value from the json file?
No!
Tbh I would much rather use a 63 line bash script that can be embedded to the script itself than 32k line compiler that needs to be installed separately.
Is json a requirement or could you use something that's easier to parse with bash?
It's quite likely that he is getting the JSON from elsewhere. Personally whenever I've used JSON it has never been by choice.
Exactly! Also, bash scripting is one of my weak points. I come here for guidance.
Thanks. How do I include it and call it? Treat me as a noob who is good with command line unix but has no experience with bash scripting.
You could throw the functions from it inside your script, rename main to nosj and then use it like so: eval "$(nosj data.json)"
It's not a json file. It's js. Big difference.
I don't get why all your comments are downvoted in this thread. You are the only one helping me.
Eh, I tend to get downvoted wherever I go. It's fine.
The problem here is your list variable is finding all files that end in .png and adding them to the variable with the .png extension at the end. &amp;#x200B; I would perform some string manipulation against the variable first, preferably with awk... var = awk -F. '{print $1} var += ".jpg" &amp;#x200B; That will only take the front half of the variable, otherwise the filename, and exclude the extension, then the second variable will append the .jpg, or whatever file format you want.
 convert -- "$list" "${list%png}jpg" Please check how to format reddit posts properly.
This worked, thanks!
 $ awk -F: '/:/{gsub(/(^ |")/, "", $2); print $2}' credentials.js value