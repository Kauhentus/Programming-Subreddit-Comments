Thanks - I remember well the architecture of a traditional threaded forth - I got my first Forth up by copy-typing the entire assembler listing of the Fig-forth for 8080. I have a co-y of Loeliger somewhere. I was curious about your “hand coded” stacks when the Intel (surely?) has push and pop ?
This was submitted here before. What changed when they revised it in July?
Hmm I forgot to check, probably nothing's changed.
My compiler runs under 32bit x86 mode in Linux. I can't even remember what real mode is anymore. I'm working up to redoing the compiler in 64bit as the 4GB address space is starting to hit me.
Woah! What are you doing that makes 4GB feel small?
It got [posted to HN](https://news.ycombinator.com/item?id=16090364).
A common solution to that problem on Intel Forths is to use SP for parameter stack, BP for Return stack and when pushing and popping on the return stack you execute XCHG SP, BP Then you and push and pop to your hearts content and when you are finished, run XCHG BP,SP Apparently the optimizations in the CPU for these instructions make it worthwhile. (I have not confirmed that experimentally) 
I think the programmer of Freeforth benchmarked these pattern against indirect memory accesses (base-pointer based) with the conclusion that both variants show equal performance on recent, out-of-order x86 processors. The class of XCHG instructions are more compact of course which may be of advantage because the issue width is still a limiting factor (Intel cpu's)
I'm analysing maps to estimate trade volumes and the way Dijkstra's algorithm works means I want to cache the results for each settlement. As the map grows and the number of settlements increases, the caches have outstripped the address space, so even mapping the files in from disc doesn't help. I'm using 3½GB but I have 8GB in the machine. Obviously there are alternative approaches but it's spurring me on to finally update a compiler I originally wrote in 2009.
Additionally, I keep top of return in a register to speed up r@ (and r=, which I added to the normal words, equivalent to : r= r@ = ; ). So XCHG on its own isn't sufficient.
Intel CPUs keep a cache of return addresses which is only abandoned if, when executing RET, the return address on the stack doesn't match the return address in the cache. At least for Forth's stack operations, entering an assembly subroutine whereupon you exchange EBP and ESP and then exchange them back when you're done, ensures the integrity of the call/ret cache. So you either XCHG ESP, EBP in your primitives (totals to 4 bytes per primitive), or you force the CPU to waste at least dozens of cycles refreshing its call cache. Save bytes VS save cycles.
That makes a lot of sense. I feel like I have the same problem you do, which is my hesitance to try integrating Forth itself with anything else (like the C standard library for any OS). It doesn't help that Visual Studio is so monolithic, and GCC equally puzzling.
I have no plans to revisit bare metal x86 if I can avoid it. Nowadays my interest in x86 is only as a host platform for umbilical cross-assembly/cross-compilation rather than as a target. Although that may eventually change with Intel starting to integrate x86s with FPGAs.
Threaded Forths don't use CALL/RET so that messes things up I guess.
It is more common to keep the Top of the parameter stack in a register which gives about a 10% overall performance increase in threaded code. Do you do that as well?
Ah. Well then I dunno why anyone would bother exchanging those registers, since "xchg sp, bp push si xchg sp, bp" are the same amount of bytes as "mov [bp-2], si dec bp dec bp".
I'm not user of gforth but you code is like a direct traslation from other lang, forth have other path, the key is reemplace vars with values in stack. 
At the moment, acceleration for some products I'm still tangentially involved with. More generally, as the integration of commodity CPUs and FPGAs gets tighter (lower latency/higher bandwidth) it will open up more and more interesting opportunities for application-specific co-processing. Makes me wish I were 20 again.
You could probably speed things somewhat up by making the program more forthlike but it probably wouldn't be a huge difference. AFAIK, Gforth doesn't do any particular optimizations and is written in C. The site talks of a gforth-fast... 
Thanks, I'll give this a shot.
This is totally a shot in the dark, but Gforth *might* be faster [on the master branch.](https://github.com/forthy42/gforth/tree/master) 0.7.3, the most recent release, is behind the trunk by several thousand commits. So it's quite possible that there are some performance improvements to be had by using a master build.
JIT native code beating threaded code in a problem as brute-force as this, where one or two small loops is the entire program's execution time, is not surprising. But stuff seems to be a bit different on my system. When I do "javac outcomes.java" followed by "time java outcomes" I get 0.706 seconds (using openjdk 1.8.0_151). gcc 7.2.0 with -O3 gets 0.111 seconds. gforth-fast (after making some changes to the forth version that both improve speed and, in my opinion, style, which can be seen at https://pastebin.com/CxZT8gnv) gets 1.137 seconds. Note that I am using a development version of gforth (specifically 0.7.9_20171026). My CPU is an AMD a10-7850k. Once I get julia and go installed and figure out how to run those fancy new whizbangs I'll see how those perform on my system, but if the java time is indicative, gforth-fast will probably beat julia and take about twice as long as go. Are there any special flags you're using with java?
I share your excitement. I wish I had any experience with FPGA programming. One more thing to learn, I guess?
Great suggestion, this brought the timing down to 0.7 seconds, much more in line with my original expectations. real 0m0.713s user 0m0.700s sys 0m0.000s
This makes Forth about 1/2 the speed of Kotlin &amp; Scala and about 16x faster than Python 3 on my workstation. Quite remarkable for an interpreted programming language, actually.
Richard Haskell, a long time Forth enthusiast, wrote *Learning by Example Using Verilog - Advanced Digital Design with a Nexys 2 FPGA Board (2009)*. Although outdated (old version of Verilog, old demo board), it's probably the only primer whose main design example is a Forth stack machine core. Some worthwhile follow-ups are: * Advanced FPGA Design - Steve Kilts * Advanced Chip Design, Practical Examples in Verilog - Kishore Mishra * RTL Modeling with SystemVerilog, using SV for ASIC and FPGA design - Stuart Sutherland 
Keep in mind, Forth doesn't dictate implementation techniques. It can be interepreted, or compiled, or byte compiled, or threaded, or anything.
Wow, that is quite remarkable. I'm surprised the Gforth developer hasn't released such changes yet...
How do you measure the speed? If in the command line then you're also measuring the compilation speed of gforth. Which seems to be unfair if you compare it to an already compiled code. Many Forths perform a linear search on the dictionary during compilation.
I am not up to speed on modern Intel designs. Would you say then that execution speed would be the same for both pieces of code as well? (not sure that matters so much these days with the complexity of cache memory management and such affecting actual speed as much as instructions)
Not only that you are measuring the start up time of the entire compiler which has nothing to do with the program's execution speed.
A minor point of factoring style in Forth. Factoring is typically more atomic in Forth than most other languages. The goal is not only to create a working program but to create a tiny set of words that are useful to write the program. A small custom language if you will. It's mindset shift. The words DEC and INC would typically be factored simply as: : inc ( addr -- ) 1 swap +! ; : dec ( addr -- ) -1 swap +! ; Then they are useful for any variable, memory location or your CARDS array. Example: variable x x inc variable y y inc Then the phrases in the program like i dec Would read as: I cards dec Which is much clearer. And while I am at it some folks use a piece of punctuation in their naming convention to gives more clarity to word functions. If you renamed cards to ]cards for example to indicate cards is an array it would read: I ]cards dec Which helps me remember that cards is an array. Keeping with naming conventions 'dec' and 'inc' would often be called '1+!' and '1-!' to keep with the '+!' operator naming. But you are free to create the language that suits your needs. It's Forth. 
It wasn't meant to be a formal benchmark.
Thanks, this is quite helpful.
Understood. For your future reference the people at Microprocessor engineering in UK have created a true optimizing compiler that compiles Forth source to native code. Their founder indicated that they used to compare their older indirected threaded systems to the new VFX system. Eventually the benchmarks on the native code exceeded 12X the threaded code and they stopped comparing. Typically threaded Forth will run loops 10X slower than C or assembler because each forth "instruction" (word) is still running through the address interpreter. However real world algorithms tend to be 3..5X slower than native code. So that gives you a sense how old Forth and new Forth implementations will perform in general. And of course better algorithms beat language almost every time. 
I prefer to keep brackets and their ilk balanced on the screen as much as possible so I'd go with cards{ i }dec where cards is created by (minimal, no checking) : array ( size &lt;name&gt; -- ) create cells allot ; and }dec would be (assuming you have cells+ as a primitive) : }dec ( array^ index -- ) cells+ -1 swap +! ; leading to 52 array cards{ cards{ I }dec and so on, so that the only unbalanced brace is at declaration. You could also define words like }inc, }@, }! }sum, }max, }min and so on. Those last three would need you to store array size, of course. This illustrates one danger of Forth: it's so much fun that you often find yourself simply adding code because the ideas flow out of you and you can waste a lot of time on things that are nice but not *actually* needed for *this application*. So you have to focus on "You're Not Going To Need It".
LOL. You are so right. There are legions of people who never write an app in Forth but endlessly play with their implementation... because it's fun. 
I happened to have this page bookmarked for referencing just in situations like this: http://www.c-jump.com/CIS77/reference/Instructions_by_Opcode.html This page says that xchg is quite a heavy-weight operation, taking 3 cycles each even on Pentium processors, whereas the memory operations (for xchg, push; for dec x2, mov) take 2 cycles, and DEC takes 1 cycle each. Simple math dictates exchanging is slower. But I'll also add that the dec x2 operation would also be faster simply because it is better for out-of-order execution, because it only ties up 2 registers instead of 3.
Perfect! Thank you!
&gt; This makes Forth about 1/2 the speed of Kotlin &amp; Scala and about 16x faster than Python 3 on my workstation. Minor nit -- it makes *gforth* half the speed of Kotlin and Scala. An optimizing native code compiling Forth will likely be faster.
Instead of static top-of-stack caching your compiler can benefit from dynamic stack elemination (dynamic mapping of the stack to temporary registers). That wouldn't be much effort from the current state of your implementation resulting in a larger performance boost.
That would require significant complexity in compiler particularly managing the transitions from word to word, but in a register rich machine there would be a pretty big reward. Memory tells me that experiments by Forthers in past with keeping top-of-stack AND next-on-stack in registers did show a performance increase over just top of stack in a register so the register allocation scheme would need to be much more sophisticated.
I honestly can't imagine how that would improve speed unless you knew your whole stack was always going to fit in the registers. I found working with the FP stack in Intel a complete pain because of the interface between the "fake" stack and the larger in-memory stack. In the end I just stopped using FP. Is there an implementation of this somewhere I could look at?
variable allots one cell already. variable deck 9 cells allot can be replaced with create deck 10 cells allot and be equivalent, at least in most implementations. I have no idea why create wasn't used, though.
It basically walks you through one man's very idiosyncratic implementation of an indirect threaded Forth for the Z80. It's worth reading, but I wouldn't use it as recipe book. 
I am only familiar with an old commercial system that has intel Floating point. The new standard assumes a separate floating point stack and the operations all take place on that FP stack. It's not ideal IMHO. https://forth-standard.org/standard/float One Issue is how does the REPL handle floats. The system I am familiar with took a non-standard approach and used vectored words (DEFER et al) in the interpreter. The word "integers" set the REPL to work like normal Forth. The word "FLOATS" changed the REPL to read anything with a '.' in it as a FLOAT and put it on the FP stack. This worked pretty well. But to be sure the Forth "simplicity" model means you do more work in other places. (Some say you can't hide complexity, you can just move it around) If you want to see the state of the art I would recommend the free downloads of MPE VFX Forth and SwiftForth from Forth Inc. Perhaps you can glean some insight from their methods.
Doh!
This series of articles is a good start: http://www.bradrodriguez.com/papers/moving1.htm
Thanks
There is also Camel Forth , by Brad Rodriguez, a minimal Forth kernel, that is available for MSP430 and other micros, as complete assembler code and the comments show the Forth equivalent of the assembler where appropriate. I wrote a hobby cross-compiler that generates a Camel Forth BIN file for the TI-99. It is heavily commented and has some extension files that show how to implement some other words and tools in a more or less ANS standard way. It might be of use to show how Forth goes together. https://github.com/bfox9900/CAMEL99 
How is that an if? How would you write `: mod5 5 mod 0= IF ."yes" ELSE ."no" THEN ;`. You would need to make TOS the decider, e.g. `: ? IF swap THEN drop execute ;`. I used the builtin IFfor that but you could define it without it too. For the reason: I *think* it's because it maps to a jump instruction, which is as cheap as it can get. Forth was originally speedy.
* Your `my-if` does not allow for *conditional execution*, so it is not a replacement for `if` and certainly isn't as useful. * I have never heard of "nothing should really be of variable length" being a Forth principle. Loop bodies are not fixed length either. * I don't think Forth is about following principles, but about finding a solution tailored to the given problem. 
Yeah, I agree... not only are IFs not postfix but definitions themselves (as in your code clip) are not postfix. In my toy language (f-flat) I "fix" both of these (fix is subjective). For IF the primitive (built-in) word is the tertiary conditional `cond`. For example `true 1 2 choose` yields `1`. "if" (called `branch`) is defined as `cond eval`. For definitions, I use primitives `sto` (to store a value) and `:` (converts an array to an action/sentence). Then I can define `;` in a way that the definition of `branch` is: ``` branch: [ choose eval ] ; ``` Which is all postfix and, to me, more readable IMO. 
Wow, I really like your toy language. For one it's really pretty, and when I looked it up, it had a lot of the ideas that I wanted to put into a toy concatenative language &gt;:( lol. One thing I want to try out though, is to make `'` be polymorphic and turn numbers into numbers. Actually, the point of the toy language I want to make is a heavily polymorphic forth. I just dont know where to get started :S
Thanks for the kind comment... I've been working to ensure the module loading is postfix: `core: 'core.ff' import ;` (notice the use of the `;` word). Lately working on pattern matching and lambdas (https://runkit.com/hypercubed/f-flat-quadratic-equation).
If is not in any way "special syntax". Thinking that may indicate that you've misunderstood something fundamental, and rather important/useful, about Forth.
I think it's a common misconception, people think Forth is about postfix notation. That's why things like Factor claim to be Forth-inspired when all they have in common is the data stack. To play more with the example, u/likes-beans, the if you suggest becomes impractical, because now you have to define a word for each branch, like you did with `yesbranch` and `nobranch`. That's where people pull out quotations, which are like anonymous inline definitions that you can find in e.g. Retro Rorth, 8th or Factor. Then you have `[ ."yes" ] [ ."no" ] if`. It would be trivial to add similar facilities to "standard" Forth by defining 2 words that would start an anonymous definition and end it pushing the xt on the datastack. Stealing the comment braces you could write `( 2 + )` and it would be roughly equivalent to `: _some_gensym_ 2 + ; ' _some_gensym_`. You could make word definition completely postfix as well this way, doing `( 2 + ) " 2+" bind`. Using parsing words you could make `bind` infix getting to `( 2 + ) bind 2+`. Changing the parser from "word-number-error" to "word-number-string" or "word-number-symbol" you could stay postfix and have `( 2 + ) 2+ bind`. The point of this exercise is *not* to find a better syntax. Fiddling with syntax all the time is just a game many of us like to play. The point is to remember we're here to code and produce something useful. Yes, we need a language with good syntax for that (where good = simple and composable for me). But I think Chuck was happy with the simplicity of `IF` and how it mapped to assembly nicely. (of course his latest if is different :)
Where is this `galope` library?
You would do well to read all Forth code by Marcos Cruz.
wow, still in developing? 
It is not required to hold the entire stack in registers because recent Intel comp. processors map the accessible register file to a much larger physical one. This includes common access patterns to internal cache memory. As stack references follow a static access pattern changes are high that the entire stack can be hold in this physical register file which of course require some code specific arrangements. A simple way ensuring this kind of caching is mapping the topmost stack elements to some registers at basic block level.
&gt; That's why I decided that I'm going to learn Forth in 2018 well enough so that I can actually do something useful with it. I view FORTH as a control language, which means you need something else to control. GNU FORTH is good for this, because it has the system word, which allows interaction with the rest of a UNIX (Linux or BSD) system. One program I wrote a bit back was very simple; it had a group of FORTH IF branches which, depending on which number I added to the stack first, used the system word to call mplayer and play a certain mp3 file. Nothing fancy, but it was still more consequential than most FORTH programs I've seen. I feel that one of FORTH's main advantages is how close it is to assembly, which means that its' loops and control structures are faster than most other languages I've seen, and also means that it is simpler to write. I like the idea of having single use binaries which are typically written in a higher level language, and then using FORTH for iteration control. FreeBSD also uses FORTH for its' initial bootloader, which is a good use for it as well, I think.
It makes sense as JIT specific optimization
The late Neil Bawd, who sometimes went by the pen-name Will Baden wrote a lot of Forth utilities and language extensions. I believe his family has kept them public for us all to review and use as we see fit. You will learn some nice Forth style from these examples and have some handy code for your projects. http://www.wilbaden.com/neil_bawd/tool2002.txt 
Note: Neil makes heavy use of TEXT macros using S" &lt;text&gt; EVALUATE and making them IMMEDIATE words. It took me a while to get why. This causes the lower level code to be compiled into a definition rather than compiling a call to the lower level words. It makes for faster code but with slight sacrifice of size (it's bigger).
i just tried the new r4a-0.8 , which could run on android again :D thanks for you work, and is it possible to use it generate android apk on android itself? 
yes and not, is possible but not have time to develop, the version you download is very old, I continue developing in wine plataform only. my recent plan is finish new x86 compiler and the make a webassembly compiler.
ok, i am also interesting of wasm implementation
Mind expanding on why you consider typed stacks a problem? Maybe I'm missing some fundamental piece of context here, but [Cixls](https://github.com/basic-gongfu/cixl) stacks are typed to enable generic words among other things; and I can't really see any problems with the approach so far. It's also worth mentioning that I took the liberty of renaming basic stack ops to single char operators which reduces the noise level once your eyes get used to it.
I think you'll run into trouble when you try nesting whiles. Also, there are several good uses for having the extra `then`s, as it allows for different behavior depending on how the loop exited. I don't know if that counts as structured, but it sure is useful.
I should expand a bit, yes. Often in Forth, when one wanted to manipulate a certain datatype exclusively for certain operations, the solution was to have a software stack dedicated exclusively for that specific datatype, as an example a stack of strings. Memory management factors in at that point, and it quickly can become burdensome. The other solution is more abstract, and that is to have everything on the data stack to carry with it a datatype indicator, or be a pointer to a structure which has that information. Once again, memory management possibly comes into play depending on the datatypes. But at the end of the day, either solution is immensely complex. The simpler solution is to manage your stack data exactly as it was managed before; everything that is needed appears exactly in a suitable context, and nowhere else. Generic words can encourage bad habits, like getting lazy with type conversions or letting data stay on the stack for too long.
If you really need to go that far, factor the loop into a separate definition, and just call EXIT when you're done with the loop.
I never thought of the possibility of using the THEN's for correcting the stack and that would be too unreadable. It wouldn't be clear enough what corrections that would belongs to what exit. This nesting of WHILE should only be used when the status of the stack is comparable at each WHILE. Which could be managed by using local variables. My purpose was to give an example of "extending the compiler".
This is almost always the most easy method, but if you want to avoid multiple exits this method could be an alternative (in GForth).
Good point. (Note to self: Don't reuse word names)
I do something similar, but non-standard: The loop can start with either `begin` or `?begin`. You can loop back to the beginning with either `repeat`, `?repeat`, or `0repeat`. You can break out of the loop with either `break`, `?break`, or `0break`. You can break out of the loop and return to the caller with either `bail`, `?bail`, or `0bail`. The loop ends with either `again`, `while`, or `until`. 
There is an F# implementation of colorforth (and GA144 simulator if memory serves well): [color](https://github.com/AshleyF/Color) You can run it everywhere there is a .net runtime (Linux/Windows/Mac)
https://groups.google.com/forum/m/#!msg/comp.lang.forth/MoOlO-g7SG4/e8Twn4JJHHwJ
I'm not aware of any. Maybe you are thinking of VentureForth which was development environment for the seaforth processors and was supported on Linux (it was hosted in gforth and swiftforth). ColorForth is kind of like the next gen IDE for the next gen chip but is a totally different software approach. I'm pretty sure I'm remembering this all correctly... I've personally never had a problem with getting ColorForth working on Linux/Wine (I'm using Ubuntu). If you are just looking for ColorForth links to the old site, you can find it mirrored in a few places such as https://github.com/mschuldt/www.colorforth.com 
I am not a professional Forth developer, and I do not know the Mecrisp-Stellaris system, so everything I will say here is a guess from what I quickly gathered reading only part of the source code and documentation. It may help you gain an understanding by yourself, however, or even, by sheer luck, be correct. Otherwise, the people more competent than me (almost everybody here…?) are welcome to correct any mistake I make. If I am not mistaken, words that read the input buffer, like “cornerstone” are called “parsing words”. “n-foldable” is not a parsing word but an immediate word. That is, it is called immediately, even when in compiling mode (like “if”). A brief look at the source code does not help a lot, because these words only set a flag “Flag_foldable_n” on the word currently being defined (the effect of this flag is applied elsewhere, most likely in the assembly source, so a “brief look” would not be much help, one would have to really read the code and understand it). However, this line in the README of Mecrisp-Stellaris give us a clue: &gt; Mecrisp-Stellaris can compile directly into Flash, generates native code with constant folding and inlining of short words. This line introduce the concept of folding right next to inlining. Inlining means taking the code of a word, and putting it in place of a call to this word (saving the cost of a call, usually at the cost of memory). My guess is that folding is getting the value of a constant and putting it as a literal during compilation, instead of calling this constant at run time (again, saving a call, and maybe memory accesses). I see that “n-foldable” is mostly used on words that take n arguments as input. So, I suppose that when the compiler encounters a n-foldable word, it looks back to see if the word is preceded by n literals (or constants), put them on the stack, call the word, and compile the result as a literal. So, `: foo hex 1B 5 io ;` will compile the same way as `: foo hex 1B05 ;`. As constants are also folded, if we have a `bar` constant set to `1B`, `: foo bar 5 io ;` will also work: the compiler see `bar`, fold it and compile a literal (lets say, as `push 1B`), then compile `5` as a literal too. Coming to `io`, it sees that it is a 2-foldable word. So, it looks back at the *compiled* word, see two pushes, guess it is OK to fold the word, and proceed to fetch these two literals, compute the result, go back to the first push, and compile the `push 1B05` in its place (leaving the compiler in a state where further compilation will overwrite the second push). This is of course pure speculation. It is how I would do it if I had to naively implement “folding”. Note that this process works for any code that compile literals: a literal followed by a “foldable” work (`5 io#`), a constant followed by a foldable too (`bar 5 io`), but even a foldable followed by a foldable (`bar 5 io io-base`): as long as a word end up compiled as a literal, it should work. Again, I remind you that I am not a professional, and not that knowledgeable, and that it is all speculation, only here to help you find what really happens. Hoping it’s not too far from the truth, good luck!
&gt; So, I suppose that when the compiler encounters a n-foldable word, it looks back to see if the word is preceded by n literals (or constants), put them on the stack, call the word, and compile the result as a literal. That's exactly it.
[Rainbow Forth](http://rainbowforth.sourceforge.net/)
At a glance, this doesn't look Forth related.
What does have in common with Forth other than having a data stack? It seems more like a modern concatenative language derived from Joy/Factor.
If you say so, I only have experience from Forth; which is what the language is based on. I would say postfix semantics with a twist and an exposed data stack. But I'll happily refrain from posting here any more if that's not dogmatic enough.
Sorry, I've posted several times about it here which is why I skipped the introduction this time: https://github.com/basic-gongfu/cixl
Thanks for the feedback. I realize that Cixl isn't really competing with Forth for it's core business, that was never a goal; I'm not much for competing at all. But it's one of the more Forth-like ways of extending C that I've come across, and sometimes extending C is just the right thing to do. A [sub-reddit](https://www.reddit.com/r/cixl/) already exists, but since I'm the only one there so far it doesn't really help. Cixl aims to be really good for scripting; application scripting as in Lua and shell scripting as in Perl. It leans heavily onto C for most things, so I expect a Forth-hosted version would feel quite different. I am curious about the speed though, what is it that makes Forth so fast? Cixl is currently designed as a dispatching interpreter, so that explains some of it; as I understand it most Forths are threaded. But from what I hear, threading it would buy me at most 20%; which still leaves a big gap. Regardless; for an embedded scripting language, speed usually means solving the problem in the host language end exposing the solution.
The operators are shorter names for usual [stack operations](https://github.com/basic-gongfu/cixl#stack); clear, drop, dup and swap; $ is special, it works together with ',' for cutting/stitching the stack. I would like to turn that question around and ask why it's not considered good practice in Forth. One reason in my mind would be that they're less convenient to work with in Forth. They certainly help with keeping the stack clean, something that's considered important by most Forth programmers. I'll usually introduce variables once using stack primitives starts getting messy.
&gt; I would like to turn that question around and ask why it's not considered good practice in Forth. One reason in my mind would be that they're less convenient to work with in Forth. They certainly help with keeping the stack clean, something that's considered important by most Forth programmers. I'll usually introduce variables once using stack primitives starts getting messy. My understanding is as follows: The problem is that they make it more convenient to work with many values at once. This is often not a good thing, because it tempts the programmer into writing more abstract code than necessary instead of just solving the actual problem. Avoiding them thus makes it harder to design overcomplex solutions. This means that a strong tendency to avoid them can help to improve program design. If something really can't be expressed well without them, they should of course be used.
Just wanted to note that I ended up removing the local variables to get more speed. That's how I've come to use variables, as a short cut when I need to move fast and don't have the motivation to write stack poetry. But if I have reasons to revisit code, it often ends up more Forth-like with each rewrite. It's a more gradual approach, which is one of the design goals for Cixl.
&gt; That's how I've come to use variables, as a short cut when I need to move fast I cannot remember a single time when "moving fast" wasn't an euphemism for "accumulating technical debt" over here, so I have become a bit skeptical about this phrase. :) &gt; and don't have the motivation to write stack poetry. I think one should never ever write stack poetry. The general rule I've heard goes something like: If two shuffling words are next to each other (and they preferably shouldn't be), none of them should be among the more complicated ones (like `tuck`, `rot`, or `nip`). If I have a need for something like that, I ask where my design went wrong. I also think that `rot` shouldn't even exist. 
When you're writing a throwaway script or exploring unknown territory, there is no such thing as technical debt; having options is an advantage. I feel like you're referring to a different kind of poetry, as I agree with the rest of your stack heuristics.
Out of curiosity, what in particular about `rot` makes it unfit for existence? I can see most of its uses being replaced with return stack stuff, but often that requires longer sequences of shuffling. But I suppose perhaps you mean that the need for `rot` itself is what should not exist?
"I am curious about the speed though, what is it that makes Forth so fast? " Good question. There are few things IMHO. 1. A brutal policy of simplicity. No extraneous code. 2. It's not really "interpreted" Even indirect threaded code is a list of addresses. The addresses are "interpreted" yes, but the code to do that is 2 or three instructions depending on the CPU. Here is the "inner-interpreter" of a Forth a wrote for the TMS9900 CPU. \ Forth ITC NEXT routine, "inner-interpreter" *IP+ W MOV, *W+ R5 MOV, *R5 B, You can see how small it is. When I skimmed your code I saw a lot of C code. I would recommend studying a Forth written in C to get a sense of how it works under the hood. Then do a version 2 of CIXL with what you learn. 
Intriguing although I don't have it in my head yet how you actually do this. But I really like the possibilities. 
Perl and Cixl share the same hacker mindset, besides syntactic preferences I would expect them to appeal to the same people. {...} is a lambda, it simply captures the environment and has nothing to do with the stack. Functions scanning for arguments is here to stay, but It's perfectly possible to code in postfix style without ever using ',' or '$'.
for implementing such optimizing assembler the arity of each word should be known somehow. Because rearrangement occur at edit time and such state relevant information change with editing, dynamic reassembling (JIT, "Just In edit Time") is an option I think.
Okay, then I didn't get something: {} really has nothing to do with the stack? I thought it makes a cut, so that infix can be used. 
Shit happens :) Have a look at the readme for [cutting](https://github.com/basic-gongfu/cixl#expressions) and [lambdas](https://github.com/basic-gongfu/cixl#lambdas), and please yell if it's still unclear.
I can imagine using a FIFO for *one* world designed to use it, but how would it work for two preceding words of the kind? How would the parameters be distributed to the second word?
Ah, it hit me when I saw the "2 2 3 3 * * + . 13" this is pretty clever. It still seems to me that this is still no silver bullet, but is quite an immense improvement for what would normally alternate between stack juggling, or simply treating the stack as an array.
Here's an implementation that I used in gForth to feel this out. ( This is an implementation of this style of fifo: https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem#Without_semaphores_or_monitors ) variable produceCount variable produceIndex variable consumeCount variable consumeIndex $800 constant size create fifo size cells allot ( empty and reset the fifo ) : empty 0 consumeIndex ! 0 produceIndex ! 0 consumeCount ! 0 produceCount ! fifo size cells erase ; empty ( consume a value non-destructively ) : peek consumeIndex @ fifo + @ ; ( analogous to pop ) : consume consumeIndex @ fifo + @ consumeIndex @ cell+ size cells mod consumeIndex ! cell consumeCount +! ; ( analogous to push ) : produce produceIndex @ fifo + ! produceIndex @ cell+ size cells mod produceIndex ! cell produceCount +! ; ( drop a value from the fifo ) : drop consumeIndex @ cell+ size cells mod consumeIndex ! cell consumeCount +! ; ( Recover the most recently consumed value. Put it back in the fifo. ) : remem consumeIndex @ fifo + @ consumeIndex @ -1 cells + size cells mod consumeIndex ! -1 cells consumeCount +! ; ( display fifo contents, analogous to .s ) : .f consumeIndex @ fifo + produceCount @ consumeCount @ - bounds ?do i @ . cell +loop ; ( non-destructively display next value ) : ? peek . ; ( destructively display next value ) : . consume . ; ( needed for this gForth wrapper to accept number literals ) : lit produce ; ( wrappers around various forth words... ) : char char lit ; : key key produce ; : emit consume emit ; : * consume consume * produce ; : + consume consume + produce ; : - consume consume - produce ; : / consume consume / produce ; : @ consume @ produce ; : ! consume consume ! ; : 1+ consume 1+ produce ; : 1- consume 1- produce ; : = consume consume = produce ; ( The best practices are different with fifos... ) ( Putting a lot of items in the fifo is fine, but you will want your words to clean up after themselves. ) : accept peek 0 ?do key loop ; : type consume 0 ?do emit loop ; 2 lit 2 lit 3 lit 3 lit * * + . \ 4 lit accept type ( accept 4 characters and print it) 
The `WITHIN` example from the slides doesn't work with this, if I'm reading it correctly. I'd love to see some more examples of this in action. I don't think I quite understand how to use this properly. Suppose I want to write a word which takes three values (a, b, c) and produces (a - b - c). How would I do that? Is this intended to be used alongside the stack? As far as I can tell, there is no way to arrange the parameters in the fifo that makes that word work, unless we add what are basically stack manipulation operations. I should probably watch that video sometime...
I think it's more than that. Its existance seems to suggest that such deep stack access is a perfectly legitimate thing to do, which is a way of thinking that does not seem to be helpful - even if it is occassionally true. But even `pick` might be the better choice, because with `pick` it is much more obvious that we're doing something we should better avoid wherever we can.
Efficient in what way? Space? Execution time? This is really easy to test if you actually wrote a little code. Let's take a look. Space first: \ stupid test with locals here : test { a b -- c } a b * a b * * ; here swap - . cr This takes 128 bytes on my test system. \ stupid test w/o locals here : test2 ( a b -- c ) 2dup * &gt;r * r&gt; * ; here swap - . cr This takes 96 bytes on my test system. How about execution time? Let's do a loop of 20,000,000 iterations, running the word twice with different inputs (discarding the results, so we won't be overly I/O bound). \ test loop for locals version : try ( n -- ) 2000 0 DO 10000 0 DO 1 2 test drop 3 4 test drop LOOP LOOP ; This takes 2.05s to run on my test system. \ test loop for non-locals version : try ( n -- ) 2000 0 DO 10000 0 DO 1 2 test2 drop 3 4 test2 drop LOOP LOOP ; This takes 1.64s to run on my test system. So in my quick test, the non-locals version is both smaller in compiled space and faster when running. Notes: - Tested under Gforth 0.7.2 on a Debian GNU/Linux system ("Linux debian 4.14.12-x86_64-linode92 #1 SMP Fri Jan 5 15:34:44 UTC 2018 x86_64 GNU/Linux") - Performance was measured using the standard `time` tool, averaging the resulting `real` line based on 12 runs. - Test system is a Linode 1024. 
I just noticed your edit. They don't add to the dictionary each time the word is called. This is trivial to test, so I'll leave that to you to figure out if you want to verify this.
This is a nicely written series. Thanks for sharing.
Local variables are there to be used when you need them. Everything has trade offs. Understand them and make a decision, Locals can help simplify some code for the programmer. The Forth solution to complex code without locals can be to factor your code into small pieces so each piece (word) does a simple stack manipulation. YMMV. My 2 cents.
That was clever how you subtracted the two here pointers. Thanks I will think to do that next time I want to measure the growth of memory.
I googled tail call recursion and forth and found a page that references Color Forth's tail call recursion. http://wiki.c2.com/?TailCallOptimization I guess what I would need to do is make my own version of the colon compiler that works like color forth's version.
Pure, functional programming allows to write word definitions in continuation-passing style which ensures that each recursive call is in tail-call position. Within some limitations this is possible in Forth.
While not factually incorrect, tail-call optimization in no way requires purity and isn't limited by or functional programming languages.
Thanks for the interesting link and good luck with your own Forth!
Can you explain more what you mean by "the general case"? In colorForth, the word `;` checks to see if the last thing compiled was a `call` instruction. If so, it replaces the opcode with a `jmp`; if not, it compiles a `ret`. Although colorForth is STC (it compiles code to x86 assembly), this doesn't seem too hard to adapt to an ITC or DTC Forth, so I'm curious if by "the general case" you're talking about something broader than this.
Well I could make a word called tail-recurse
As others have already said: have `;` check if the last instruction emitted was a call, then either replace that call with a jump, or emit a return. Simple, effective. Time tested. Alternatively, you could remove the check and implement `-;` to do explicit tail call optimization. Personally I like to use exit compile an return and use `;` for explicit tail call optimization.
For reference, `r&gt;` and `&gt;r` are the standard names for what you call `return-stack-pop` and `return-stack-push`. `r&gt; dup &gt;r` can also be written as `r@`, so your two snippets of code become: r@ current-word ! r@ &gt;r
If ; checks for the last instruction to be a call then why does : f recurse ; f cause a stack overflow? I think I'm missing a step.
ANS Forth doesn't support tail call optimization. You'll want to look into machine Forth and or colorForth, which came about later.
Well, the general case is checking to see if any `RECURSE` can be turned into a jump. This is semantically acceptable if-and-only-if the next *dynamically* executed forth instruction other than branches is an `EXIT`. For example, if you have a `IF ... RECURSE ELSE ... THEN EXIT` sort of structure, that `RECURSE` just jumps straight to an `EXIT`, after it finishes, so it could be turned into a jump. The way colorForth does it is a very literal interpretation of the "tail" in "tail recursion", and only converts a particular instance (the end of definitions) of where the next *static* forth instruction occurs. Does that make sense? In general, tail-recursion means "anywhere it would immediately return afterward, /including indirectly/". At least, that's how I've seen it used most of the time, and is how many other languages interpret it (scheme, for example). I remember reading about the colorForth implementation, and how it basically just required flipping one bit. Potential problem with ITC or DTC (in the general case where the stuff being changed isn't very close to `HERE` - it might not be known that a `RECURSE` can be replaced with a jump until much later): going from a simple call to a branch requires using an extra cell (in most `BRANCH` implementations I've seen, the offset is expected inline), and at the very least the cell right next to it is already in use (for the `EXIT` that tells you it's legal to do this conversion). And of course shifting code is a huge headache (suppose there's a relative branch that goes back to before where you inserted this, and now it's off by 1!). An argument could be made that having your `RECURSE`s that you want changed to jumps always at the end of a definition is good style because it makes it obvious that it's right before an `EXIT`. But whether you're adding `0=` to the front of your conditions to get that recurse into the other branch or explicitly using `TRECURSE`, you're still trying really hard to point out to the compiler what you want to do. Seems more forthlike (weasel word) to just explicitly say you want a tail call when you want a tail call. Thus concludes my long-winded essay I didn't even realize I was making.
I am very interested in colorforth. I just want to make sure that I can get an up to date version and I have to be able to run it inside of a terminal in ubuntu linux.
I imagine most people would find that a bit disgusting and you would have to find the right place to put `r&gt; drop` so that you keep the return stack balanced, and you end up back where you expect, but there is nothing really stopping you. My question would be how you intend to get the return stack into the starting position and still use `recurse` for recursion? It would probably be easier to do in a multi-headed Forth but then you probably wouldn't need to use `recurse`. Enjoy your puzzle
colorForth doesn't work the way you think. There is no colon compiler in the sense that you're thinking. It's a quite different beast. If you want a hint then I would look into subroutine-threading. Once you've understood that then it should be fairly self-explanatory how colorForth does tail-call optimization.
If you have the understanding and ability to do that then that might be a reasonable approach. But perhaps it's easier said than done in a traditional threaded Forth design, where definitions are just lists of words to be called.
AFAIK there's no way to run it in a terminal window. But if you're actually interested, here's a few starting points: There's an ancient port that ran under X11 on 32-bit x86 Linux at https://www.forthworks.com/mirrors/colorforthray.info/XcolorForth.tar.gz - building &amp; running on modern systems might be difficult at best. Howerd Oakford has a current, updated distribution that runs on actual hardware (as has been the traditional way of running colorForth) or under an emulator (Bochs) at http://www.inventio.co.uk/LegacyIndex.htm You can browse a set of colorForth source blocks at https://www.dnd.utwente.nl/~tim/colorforth/Raystm2/mv050314.html (start at #24) Some notes on using the colorForth editor: http://www.greenarraychips.com/home/documents/greg/cf-editor.htm You could also look at http://www.greenarraychips.com/home/support/download-02b.html (arrayForth, from Green Arrays) and it's user manual at http://www.greenarraychips.com/home/documents/greg/DB004-131030-aFUSER.pdf For another alternative implementation using ideas from colorForth, see http://rainbowforth.sourceforge.net/ Be warned though: colorForth is pretty low level which you've expressed disdain for in the past, so it's quite likely not to be all that appealing to you.
To expand on what u/dlyund said ("you would have to find the right place to put r&gt; drop so that you keep the return stack balanced, and you end up back where you expect"), I recommend studying the solution to two similar problems in [FIRST &amp; THIRD: almost FORTH.](https://www.ioccc.org/1992/buzzard.2.design) It's a very simple, very well commented build of a Forth-ish system from the ground up, and it will enlighten your mind and put joy in your fingers. First off, THIRD's main read-code-and-compile-it loop relies on `]` which does some sleight of hand to the return stack so that it can loop forever without blowing up the stack. Get up to that point and make sure you understand it. Later on, THIRD also defines `tail` which generalizes the trick so it can be compiled into other words. It's not exactly what you're asking for because it has a glitchy behavior that its user has to work around (see the comment that reads "the first time we enter this recursive routine"; when/if you return from the tail-recursive word it hops back not to its caller but straight to its grandcaller, so you have to enter the tail-recursive word *from tail position* or Weird Things Happen). But it's close. If you've understood the primitives it's built from you will understand `tail` itself pretty quickly.
Hey, he appears to be teachable this time. You never know.
There is no way to be ans compliant and also be multi-headed?
Well my opinion is that a system like gforth should be able to do anything that assembler can do on a unix system. And so there is nothing stopping me from building another forth inside of the same process that gforth is running and hand control inbetween my other forth and gforth. In my mind this is still being compliant. The key is if it can be done with a minimal wordset.
Sorry to hear about your health and financial status, I hope you find a way to work from home or something. From the quick looks I took at your language it seems different than, well, anything else. For the masses it's too crazy because it doesn't adhere to the algol-style syntax and typical subroutine calling. For the type-lovers it's not ML/haskell enough. For forthers there's too much syntax and options, and the names are cryptic at first look. None of that matters too much as long as you're having fun. But I don't think you can expect too many donations.
This reminds me a lot like the EX instruction in green arrays F18a instruction set; if I recall correctly this is usually called coroutine [call], after it's common use. I consider it more of an optimization because there are other ways to coroutine, but it's a cutie :).
I call it `later&gt;`. Because the part after later&gt; is done... later. Example: \ Temporarily set base; restore when the calling word exits. : base&gt; ( u -- ) base @ swap base ! later&gt; base ! ; \ Parse an octal number. : o# 8 base&gt; parse-name number ; immediate
That's a good use for it. I removed the `base` user variable from my current Forth, unsure if I'll end up regretting it in the long run or not.
In some versions of my Forth I've had this as `later`. (On and off since ~2005/2006). IIRC, I picked it up from a user of colorforth around that time. I've definitely used it for things like your HTML generation, and in some experiments with infix style math. E.g., :later pop pop swap push push ; :add: later + ; :subtract: later - ; [ #1 add: #3 subtract: #2 ] call putn 
Have you read the standard yet? Given that you have a fixation on the ANS standard, you should. Here's a PDF of it: http://forthworks.com/forth/standards/DPANS/DPANS.pdf This is about 220 pages, though 18-20 are just indexes, tables of content, etc. So there's about 200 pages of actual content. At three minutes per page this should take around ten hours to read through completely, though you could focus on the beginning part (covering CORE and CORE EXT, and the introductory bits in sections 1-5), which would be around 60 pages (or perhaps 2-3 hours of reading at a modest pace). If you just want to know the requirements for a system to be considered compliant, see sections 3 and 5.
:) I always liked `pass`. I believe the name EX comes from its common usage; calling a word dynamically at runtime. This is called EXECUTE in ANS Forth. If I understand correctly, the instruction sequence `... PUSH EX ...` would be equivalent to EXECUTE. Its usage as coroutine [call], where two words hop back and forward between each other is nice but less common in practice. Strictly speaking, either use case can be implemented by combining CALL and ; : EXECUTE PUSH ; Simple enough that you may be wondering why it would be implemented in hardware. My guess is that it's half convenience and half to avoid the overhead of the call and execution of the definition of EXECUTE in memory just to save the instruction pointer to the return stack before `;` /me Shrugs :). 
I had always assumed the name was short for exchange, since it exchanges the instruction pointer and the top of the return stack.
We'll see.
Neat, it's nice to see that I'm not the only one playing that game. [Cixl](https://github.com/basic-gongfu/cixl#expressions) allows using infix/prefix without reversing, to the point where people question if it's really a Forth; but it still defaults to postfix.
Very cool, even if I do hate the forward scanning and the syntax and the bloat. It's awesome to see people working on and trying new things, that's how languages improve.
As to why implement such an instruction hardware, it's so that you can have an extremely fast and lightweight concurrency primitive.
Good publicity, in any case.
Pastebin link is gone.
You can answer to your friends: Yes, you are right, techincally speaking forth is slower than C (because, effectively, most of them are wriited in c), but in fact, it's false. and you can add: - SQL is slower than raw files (because SQL is based on raw files), so continue to use raw files for huge DB managment. - C is faster than Forth, because C is only one letter and Forth is five letters - A ferrari is faster than a truck, it's why UPS, Fedex, etc. use exclusively ferrari. in my life, I have a lot of sample where forth software are smaller, faster, easier to maintain, ..., than C software :-) let your friend thinking whaamt is think and may the foth be with you. 
FORTH should not ideally be compiled in C and then run within a UNIX. This is why (although I admit I haven't persued it) I was originally planning to try and write a FORTH in DOS, or maybe play around with Pygmy; because DOS is the only environment I know of which allows raw binaries and metacompilation. No open source UNIX allows that any more, AFAIK; and with the advent of UEFI, running FORTH directly on bare hardware without an operating system is no longer possible on a PC either. No modern FORTH environment is the real thing, in truth. FORTH was meant to be the only program running on a given machine, and metacompilation gave it an advantage that virtually nothing else has. It's not meant to be hosted within an operating system like Windows or UNIX; cmFORTH was developed before Windows existed. Also, although GNU FORTH runs within an operating system, its' primitives are written directly in assembly, and its' loops are faster than anything else I've seen. The way I would ideally use FORTH in a modern context, would be to use GNU FORTH as an iteration/logic structure language, for controlling binaries written in C or the OOP languages. The binaries only run once by themselves, but my loops for automating them are written in FORTH; think of it like Star Trek's warp drive, to make an analogy.
EXchange is just how I visualized it, if Chuck meant EXecute I have no qualms with that. To my mind what the instruction actually does is yield the processor to a co-routine, the fact that it's a useful factor for implementing an execute macro if your instruction set lacks a native execute instruction is secondary. But that's only my personal take on it, I make no claims of deep insight on the matter. As far as a justification for including it in your stack machine's instruction set, if you're doing hard (deterministic) realtime it gives you a low overhead way to interleave lengthy non-critical foreground work with time-critical polling of devices (look ma no interrupts, no scheduler!). 
If you want to see even more speed improvements on Gforth, use a build from the [master branch.](https://github.com/forthy42/gforth/tree/master) As you can see, Gforth 0.7.3 (the most recent release) is behind the master branch by several thousand commits. And there are, in fact, [major speed improvements](https://www.reddit.com/r/Forth/comments/7parow/gforth_speed/dsg56ps/) in using a master build.
Maybe someone who isn't actually using forth can't give relevant suggestions. There are forths written in assembly (freeforth, jonesforth, eforth, ...) and forths that generate optimized assembly (swiftforth). Again, have you used a search engine or looked at some implementations? Beating C in raw speed isn't easy given the x86 architecture's register oriented design and monstrous complexity that only an advanced optimizing compiler can take advantage of.
Thanks for the tip, although unfortunately I am in Windows 7 at the moment, and I've never really wanted to subject myself to source code compilation in this environment. If I install FreeBSD in a vm again, though, I will definitely try it there.
No problem. It's a shame that the Gforth developer hasn't released his changes yet...
It seems to me that if you implemented forth inside of c then it would be c compliant as long as you switched control back over to c. When the forth was operating though those operations would not be c compliant.
yes i agree with that the problem is on the other side. like python, i use it for my daily work, i earn money on it. but compared to all these other language, i think forth need a huge modernization, like much more modern tech support. i knew in the old time, its very advanced than other lang to write a arcade game, but nowaday, it looks like much after. people in community always talk about coding for microcontroller or how much kilobytes they could run on. i think its amazing, but if they could tell me how less code they could to build an HTTP API gate, or an android application, that wil be much more amazing
I think it is possible for forth and c to live in the same process and pass control to each other. Forth simply has to respect that c controls the memory.
Okay. Well you're wrong... But I'll be damned if I'll be the one to educate you. Give it a few months and maybe you'll that realize what you're saying is absolute nonsense. In the best case it lacks all nuance and understanding, and in the worst case it demonstrates your pathological ignorance.
To create a Forth as fast as modern C compilers, you must use the same technology. That being optimizing native code compilers. To see what that looks like in terms of speed, take a look at VFX Forth by MPE. It comes close to C but GCC can always beat it because GCC is megabytes of code and it can also do a global code review to squeeze the last bit of speed out of the program. However modern is beginning to suffer from "nasal demons", unexpected side effects of intense code optimization. So with native code compilers Forth gives you speed plus a REPL to interactively test things. That means not living in the debugger for days as you might do "occasionally" with C. And you can compile 100,000 lines of code almost instantly on a modern x86 machine. My 2 cents. 
Gforth was built for easy compilation on multiple platforms. They have allowed the C compiler to make excellent optimizations but absolute speed superiority was not the objective. Its an open source product. VFX is a commercial product that had it's origins 30 years ago as a threaded Forth as was common then.
What 'recurse' does depends on the implementation. In 8th, it does "tail-call elimination" (as do ";" and ";;") so that it's perfectly safe to have "recurse ;" as an infinite loop. It may be more clear to the maintainer of the code if you used "repeat ... again", though.
Speed is measured within a specific context. If that context doesn't apply, the speed comparison is not useful. "speed" can also include "speed of development" (or ... debugging, ... maintenance, etc.). Even though 8th is build using C/C++, it can outperform C equivalent programs because of memory caching effects (for example). But that's not the point of 8th: ease of use, security, etc. are.
Thanks! I think the block editor you've made is a great example of a Forth application; it's concise, simple to extend and understand and most importantly is that is reuses already existing mechanisms available in the Forth interpreter. It really captures the essence of Forth and I was amazed you could make something that simple but functional at the same time. It didn't take too long to port either. Let me know if you play around with it any more.
its very cool
Very cool. This is exactly the kind of project I've been looking for to experiment with and work off of.
Sure, it wasn't a linear approach, I've continuously tweaked and modified the system at all levels as I've learned more about what works and what does not. I'd already worked with FPGAs at university, using the same board, so I already had a working build system and new what it should be capable of. I heard of the J1 processor, and leaning towards VHDL I decided to make my own version in my preferred language. I used GHDL/GTKwave and an assembler I wrote in Perl to get a basic simulation working, as I don't have an oscilloscope I adapted modules available from OpenCores.org to do most of the external interfacing (the VGA output, PS/2 keyboard and the UART). Once you have this you can start writing small test programs to exercise that hardware, getting the processor to echo back what it has read in via the UART was the major step in proving that the SoC works, after that implementing the Forth for really is quite simple and proceeded quite rapidly. The most difficult bit is getting that PC&lt;-&gt;UART&lt;-&gt;CPU bridge working. Developing the CPU in the beginning was mostly a case of: 1) Trying something out in simulation 2) Putting it on the FPGA 3) Determining whether the new system was correct by looking at the debugging information on the VGA monitor 4) Going back to the simulation to work out what went wrong. The system has evolved quite a bit, and I've managed to simplify things after getting everything working. Just getting things working is the difficult bit, after that you've got a known working system you can play around with. For example the VGA and PS/2 keyboard used to have their own interface to the CPU, the VGA was memory mapped and the PS/2 keyboard was similar to the UART interface, but not exactly the same. I rewrote the VGA and PS/2 interface to look like UART by putting a VT100 emulation module I made in between it all. After I have got this working, I might go back to the drawing board when it comes to the toolchain, or some other different part of the system. For example, now I have a compiler for the system written in C, and I am thinking of moving to a more traditional metacompiler written in Forth. Apart from this the main difficulties are the same as you would have on any FPGA based project: 1) Poor tooling 2) Simulations not matching what the hardware actually does 3) Fighting against the language (VHDL). I hope this helps!
Feel free to use any part of the project, but be aware that different components have different licenses attached to them, if it was up to me everything would be under the MIT license but I've used some components from elsewhere. You can check each file for its license, and they should all work together with no problems. What were you thinking of doing?
Thanks, that's very interesting.
Thanks, that's very interesting.
I don't understand how Forth -&gt; C without making a lot of, IMO, language-breaking concessions, in one direction or the other. I think whether or not your long ints are unsigned or not is the least of your issues in this project. Also it doesn't matter if they're unsigned or not, which you should already know.
Forth is typically not typed, a 64bit forth usually has 64bit words. There's no information about the sign in those 64 bits, it's up to the user to use it one way or another. If you want to go low level (which you clearly stated you *don't* previously) you should first try low level (assembly) before trying to implement it. If you are trying to build a typed forth then you need to think about the design and what datatypes do you want to have. One can e.g. encode the type in 4 bits (giving space to 16 datatypes) and have the other 60 bits represent the data. One can also have all data indirect, the word being a pointer into memory where you can build any structures you like (and pay with performance for the indirection and memory management).
Did it use a virtual data stack? That's the only way I can see this working, if the C functions pass back the virtual data stack pointer on return. I'd like to see it, although I'm still doubtful it could match the flexibility in execution. Regarding signedness, I understand that. I doubt read_harder does, or he wouldn't need to ask about it.
You should google *explicit type casting in c*
8-bit Commodore systems also had 2x2 blocks.
It was quite popular on the PET as the characters couldn't be customized and it didn't have bitmap graphics.
It's like a Forthy (pared-down, imperative) relative of the method modifiers in CLOS and its offspring. I'm mostly familiar with the Moose/Perl 6 versions of these, so these may not be the CLOS names, but "around" lets a subclass or a consumed role (a.k.a. trait, mixin) add behavior before/after the original method implementation, and the more controversial "inner/augment" lets a superclass leave a hole in its implementation for a subclass to fill. But, of course, its Forthiness is going to make `reverse` a different beast in practice. I get the feeling that letting it hang around my brain long enough is likely to result in a very educational mind warp.
This is fun.
This was written for a Python to Forth compiler which is written in Python, so it needs to provide a python interface. The compiler was written in Python because it utilizes the ast module for parsing the Python source so it is easier to keep the whole program in Python to avoid some kind of weird python/forth layer. Writing a Python parser in Forth is not something I ever want to do. C was chosen for the bits that need to be fast because...C is fast, C Python modules are easy to write, and translating Javascript (from the original forth wizard) to C is trivial
Ah, OK. Yes, writing a Python parser is not something fun to try in Forth.
To what extent is the RTX2000 compatible with the N4000? Do you think it could run cmForth?
It looks like the RTX2000 is the successor to NC4016, I'm not sure how the 4016 compares to the 4000. This page http://users.ece.cmu.edu/~koopman/stack_computers/sec4_5.html contains some comparisons between the RTX2000 and the NC4016 It looks like most of the changes are additional features so I'd guess that cmForth would not be hard to port. I know lots of people around here have actual experience with these chips, hopefully they have something more to say about this.
Sounds like a hash table of objects. How big is the default map? Does it resize automatically? What about collisions?
Nice. Very thoughtful metrics.
It is a complete Colorforth environment.
I've seen that called "CO" on comp.lang.forth a long time ago. : CO 2R&gt; &gt;R &gt;R . Probably not standard since it assumes a thing or two about return addresses.
Apologies for that, I must have done something. I'll post it again if you're interested though it may take me about a week as I'm a little busy at present. :)
Cool!
Unfortunately, I'm busy as well and would probably only glance over it.
Cool, I'll do so once I get a chance and fix the link. :)
I was looking for this, thanks!
I suspect the interviewer would think you just made it up. :-) Sad that they would not even ask you what it was.
What was the code size comparison between C and 8th?
I know at least a few routines have been written but I don't know how popular any of them are. The best person to ask is probably Andre Fachat, who I believe has written one himself but is very knowledgeable about the PETs. Here is something he wrote: https://www.youtube.com/watch?v=AUkQB_b16SI
I did that with `awk`once. I wrote it out as a one-liner, and the interviewer said, "Oh. Well. Wow, I don't think I've ever seen one so short before. Could you... uh... make one that's more efficient?" Interviews like that are fun.
Heh :) I was just feeling more aggressive than usual...
I'm not really clear what the test is asking for. Do I need coffee?
Probably. ;-)
That's awesome! The 6809 is a badass piece of hardware, too. Love them.
https://github.com/gordonjcp/miragetools/ It's part of a wider project of disassembling the Mirage software and firmware.
I know there's some html versions [here](http://lars.nocrew.org/)
Thanks, but unfortunately still with the same html problems. The word details are split across multiple pages, text needs to be pulled out of the html page, and the stack comments need to be split up. Ideally I want a .tsv file with lines like this(but using tabs instead of commas): +,n1 n2,n3 /mod,n1 n2,n3 n4 &gt;resolve,adr,, 
&gt; But I don't want to have to pull the info out of a html page. I don't know why. It's some of the clearest and simplest HTML I've ever seen. For most of the definitions, the most you'd have to strip out are the strong tags, which could be done easily with sed. You could also parse from one HR tag to the next. Use cat -n to learn the line numbers of each HR tag, and then use sed to dump each of those ranges out to individual files.
It does not really matter how easy this could be, it will always be easier to use existing work. And probably also less error prone. 
Very nice. Sometimes a preemptive system is the only thing that does the job. Have you ever worked with a system that uses the Forth cooperative model? It is very light weight in that the context switch times are extremely fast. And it removes the need for locking critical resources since I/O primitives run to completion. The secret I believe is to put the truly time critical tasks on the interrupt like grabbing a byte and putting it in a queue. Everything else "cooperates" I was able to have 3 tasks (Console, remote RS488, robotics task) and an interrupt routine doing time critical data reading on an industrial device that did real-time robotics with live baby birds using this method. It all ran on a 68HC11 @ 8MHz using ITC Forth and some assembler. (there were not many resources left however with that little CPU) ;-) 
Can you use a pdf file? https://www.openfirmware.info/data/docs/dpans94.pdf
http://forthworks.com/temp/z.tsv or gopher://forthworks.com/0/temp/z.tsv has CORE and CORE EXT, with the reference numbers, word name, and stack before/after. This includes compile-time effects for some words (e.g., DO/LOOP/BEGIN/AGAIN, etc); most are execution time effects.
Thanks for sharing this! It's to bad the html does not include the return stack effects
I might actually end up using it, it's probably the least ideal format I can think of but this looks like the most complete resource I've found so far, it even contains the return stack effects. I wonder what they generated the pdf from
This is exactly the type of document that I'm looking for! It's just missing the return stack effects
It actually needs a bit more than that. Ideally it'd have columns for runtime and compile time effects, as well as return stsck effects. I may take a stab at that in a few weeks (too many projects currently underway to take on another)
What about http://lars.nocrew.org/dpans/dpansf.htm
Good initiative. I would try to make clear that, although most people will not find a practical use for the language, learning it is a beautiful journey and a very mind-opening experience.
And ...that it may influence how you code in your normally used languages as well. 
Forth has ruined me. Using anything else now is an exercise in swimming up a waterfall.
Too true!
Exactamundo!
why not make a high performance socket server? or even http server
It's looking more like IRC from here, I've written enough HTTP servers to last a lifetime. But I have a few more pieces of infrastructure to attend to first, TCP servers being one of them.
Probably the SVFIG can help.
The easiest answer I can provide is probably the reason everything else is managed the way it is: caching. The registers are a cache as much as anything else is at this point. I know this is a very unsatisfying answer, but it does solve the issue of using the same data over and over. Unfortunately, compilers are not as smart as people seem to think, and that's why high-traffic sections of code are still coded at the assembler level.
It is because RAM and ROM are not the same. Different costs, sizes, speed and power requirements. The trade-offs might be blurred these days, but still valid for most embedded systems. See the Wikipedia page on [Harvard architecture](https://en.wikipedia.org/wiki/Harvard_architecture) for more information.
**Harvard architecture** The Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit, and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
There exist some CPU architectures with a minimal, non accessible register set. These designs commonly include internal memory as part of the address space. Typical examples are the mc6802, the ti9995 and Parallax Propeller. For such architectures register names may be indeed a somewhat debatable nomenclature.
I somewhat dislike exceptions. I can easily tolerate them if they're actually *exceptional* (= rare), like they tend to be in C++ code. When they're everywhere, like is often the case with python code, they feel very cumbersome. IMO most usages of exceptions in most languages would be better replaced by more expressive use of types. Sum types, for example, are a step in the right direction; a file reading routine does not need to throw `not_found` or `permission_denied` if it's return type can be `Success contents | NotFound | PermissionDenied`. Sum types may not seem very forthy at first, but underneath lies a simple preference for encoding things in values that you can handle directly instead of constructing convoluted control flow that is not always obvious at first glance. This preference, to me, seems quite forth-like.
Excellent point. These types can also be used in conjunction with multiple return values from a function, similar to python, but without the explicit need for a tuple. After all, the stack is our playground. Why fixate on one return value when we can return as much as we please?
[Here](https://github.com/basic-gongfu/cixl/blob/master/devlog/collab_server.md) you go :)
If you need exception handling in the form of extensive debugging (preserves the stack for inspection), take a look at this post I made about exceptions in Forth: https://www.reddit.com/r/Forth/comments/7g80a1/try_throw_catch_release/
thanks, checking. also, are you chinese?
More like swedish. The story behind the logo is that I've been training/teaching Ving Tsun gongfu for 25 years; and the more experience I get, the more it all looks the same; I apply pretty much the same principles to everything these days, including programming.
I'll try to address your concerns as best as my coffee will permit. *When to use them* Using exceptions has to do with how much abstraction you want to preserve in your code. By raising an exception in a deeply nested routine that doesn't have the contextual knowledge to handle a situation, you transfer control to the level in your code that does know what to do. Often to code that is abstracting away differences in order to present a unified interface to other parts of the system. *How to avoid them* Reduce abstraction, but of course sometimes the cure is worse than the disease. *Simplification* You can sprinkle some syntactic sugar to make it more palatable, I use words like *suppress*, *avert*, *abide* that are really just sugar. You could also simplify the semantics if you don't care about standards compliance, e.g. avoiding throw codes altogether. At the VM instruction level a fairly clean exception handling mechanism can be done with two paired VM instructions, one that 'tries' and skips over the next instruction on failure; and one that pops the return stack into a VM register (the handler list head) and pushes 0 onto the data stack.
I agree 100%. My only caveat is that if multiple levels of callers will each in turn have to check for `NotFound` and `PermissionDenied` only to once more percolate those values upwards then throwing an exception is a cleaner solution. The greater the nesting between the identification and the disposition of an *exceptional* case the more using the exception mechanism is justified, and conversely the shallower the nesting the less it is justified.
Thanks for your reply. I agree and I used to enjoy the richness of these type systems. I can also imagine other ways of working with exceptional cases like putting multiple values on the stack or filling `errno` like in C. My issue lies with real world examples - does it work well?
&gt; Reduce abstraction, but of course sometimes the cure is worse than the disease. Have you actually tried this? Or am I the only one who doesn't like exceptions :)
I also dislike exceptions. So in '8th', I made exceptions actually exceptions which are (almost always) fatal -- more suited to the testing/alpha/beta phase of app design. Production code should check for the appropriate error return and do something sane if it is possible (and throw an exception if it is impossible to handle).
So your exceptions aren't typical exceptions, they are just fancy exits. No catch. Cool :)
Not very fancy, just exits which *can* be caught and handled, if there's a way to do it reasonably.
It might depend on why you actually want to catch the exception instead of just having it pass through. You can think of exceptions as continuations (see [this article](http://matt.might.net/articles/implementing-exceptions/)) if that helps. Mentally pair up your throwers and catchers in your code (instead of having them "blindly" do their half of the job). It might give you a better idea of the actual control flow you want. And you can decide if and how you want to remove each pair. For example, by doing "Look Before You Leap" instead of "it's easier to ask for forgiveness than permission". I don't have exceptions implemented in Flpc (yet) and found exit with extra info was good enough. I didn't find I miss them even though I normally use Python a lot. There I mainly have try/catch around REPLs and evals and calls to libraries I don't want to dig into the source of.
Python is the loser in my book, regarding exceptions. They're ubiquitous and are used for behavior that isn't really exceptional. It favors throwing exceptions instead of handling errors. This necessitates a simple, clean syntax for handling them, sure. It might just be the name that bugs me, really: calling a non-exceptional control flow feature exceptions.
I tried writing there, got rejected because I'm not subscribed. Wrote an e-mail to the person who manages the list, no reply. Dead ends everywhere -_-
My rule of thumb is no more than one stack shuffling word at the beginning of any phrasal fragment and none within. 
I fix the deep to "pick4" and work ok for now, R stack can be for aux.
I wonder how hard it would be to implement a [Maybe/Option type](https://en.wikipedia.org/wiki/Option_type) in Forth, which is a form of explicit exception handling that sees a lot of use in functional programming.
**Option type** In programming languages (more so functional programming languages) and type theory, an option type or maybe type is a polymorphic type that represents encapsulation of an optional value; e.g., it is used as the return type of functions which may or may not return a meaningful value when they are applied. It consists of a constructor which either is empty (named None or Nothing), or which encapsulates the original data type A (written Just A or Some A). Outside of functional programming, these are termed nullable types. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I would like to note here that the size of an 'unsigned long int' might widely vary depending on what kind of platform it is compiled on: a long has to be as large as an int and never larger than a long long, but an int has to be as large as a char and never be larger than a long. Ergo: It is possible for char, int, long (and even long long) to all be the same size, which might be 8 bits. If you want to be sure the field is large enough, use types like int32_t or int64_t that are provided by types.h.
This is very interesting, although the article already seems to be written in 1996. I wonder: Wouldn't the easiest way to make a (threaded) Forth be preemptively scheduled to add, at the start of the 'threading' part, the code that yields control back to the scheduler?
I want to make the side-note here that at least systems languages that statically compile to bare-metal assembly like C++ and Rust attempt to rival C in speed and succeed in doing so: Their enhanced type systems allow compilers to optimize more rigoriously than a C compiler itself can. Obviously, interpreted or JIT-compiled languages that have a dynamic runtime or implement their own virtual machine (Java, C#, Python, JavaScript) are slower because they need to do more work (like typechecks) at runtime, and also performs garbage collection. But of course 'raw execution speed' is only a single metric, and these languages optimize for different metrics (like ease of use, interoptability, portability, debuggability, etc.) than C.
There is nothing really stopping interpreted languages from rivaling statically compiled languages in speed. JIT technology is powerful and has come along way and only continues to improve. One thing to keep in mind is that JIT can perform optimizations that static compilation cannot. Garbage collection is not really a performance issue when we all have these monster 8+ core machines. The garbage collector runs on another core. 
You are right that JIT languages get closer and closer to statically compiled languages in speed. I would like to argue that the inverse is also true: static compilation can do optimizations that JIT-compilation cannot. Garbage Collection is unfortunately not a problem that can easily be pushed to another core, because of race-conditions: Either your execution model is set up in such a way that the current core can collect its own garbage with the other cores continuing on (which is allowed only if you are certain that each core has its own exclusive memory.), or we have to do a stop-the-world to make sure there are no race conditions.
Technically speaking, in a way FORTH isn't really a ***language***, per se. What I mean by that, is that at its' most basic or cut down level, FORTH consists purely of those operations which are universal; which you can and will perform in any language on any Turing machine. Increment a pointer, execute a function corresponding with the number at the pointer, repeat. That is not true of GNU FORTH, perhaps; but it was of many others. FORTH's default state consists almost entirely of primitives, with as close as possible to no composite words. Composite words were considered the domain of the individual application. Individual applications consisted not only of the program itself, but literally the higher level vocabulary or building blocks that were used to create said application.
I agree with you that Forth could make a great platform to bootstrap efforts for VMs and interpreters. If we did a survey of how many Forth courses are offered at Comp. Sci or E.E. programs it might provide some insight as to why it is not more common. Some work was done on compiling C to Forth for the RTX line of Forth CPUs and it was found that an extra register to keep a stack frame would have been a big improvement. It's not a perfect fit to conventional languages.
If your target is a Forth machine (real or virtual) you're better off just extending the Forth with whatever concepts from the other language you find useful for your application. Forth assimilates.
Forth works best as a compiler than anything else, I've learned. As an interactive language as well, it performs extremely well. But modern CPUs are built with the idea of a specific style of execution, which can be difficult for Forth to meet the requirements of.
With this 'specific style of execution' do you mean that most modern CPUs are register-based rather than stack-based?
Isn't the idea of many Forths to only implement the bare minimum as primitives (the set of which is expanded only when the extra efficiency of implementing something primitively is really necessary), with nearly all words being written in Forth? And what about the ANSI Forth standard? &gt; Individual applications consisted not only of the program itself, but literally the higher level vocabulary or building blocks that were used to create said application. Isn't the same true in all (non-trivial) applications? Of course, how large a standard library that certain tasks can be offloaded to that a certain language platform has varies, but in its essence I'd say this is universally true in programming. 
Interestingly, I read somewhere that C is actually quite a bad choice as intermediary compulation language because you (a) throw a lot of semantic- and type-related information away to conform to C's peculiarities and (b) it is not a context-free language. 
Maybe one could convert Java byte code to Forth. Then all programming languages that compile to Java byte code could run on a Forth system.
C may be a questionable choice for immediate code generation. It is however the only language I know (inclusive the non ANSI de facto label-address extension) which allows the portable Implementation of threaded interpreters.
I think it's more than that. For example, IIRC many ARM processors will predict every indirect branch as a subroutine return, which is horrible for indirect threaded code.
no matter what forth, i am always care about the benchmarks
Depends on the platform, as well. A stack frame might not be best for a bytecode Forth (leads to bytecode bloat), but if frame access was assembled, it would be quite fast on an Intel chip that has so many commands for indirect memory access.
Maybe I didn't state it as well as I intended. In any application where you require a transformation from one code to another with extreme precision and control, Forth does a stellar job. In the instances where I've built compilers on top of Forth, it always performs beyond expectations. In the instances where I've used Forth to construct a proof-of-concept, it always performs beyond expectations. That's more accurate to my point.
Register or Stack is orthogonal to my point. Modern CPU architectures are geared towards C, where the stack is a cache of a specific set of values and the return address, and very rarely undergoes radical change except between function calls. In Forth, this is practically the opposite: the stack is in a constant state of change, and function calls modify practically nothing.
Sometimes a proof-of-concept is useful as well.
Hello, You will find some benchmarks into the "examples" directory. Particulary, the "binary tree" benchmark that is used to benchmark various languages according to memory management here : http://benchmarksgame.alioth.debian.org/u64q/binarytrees.html A difference is that Oforth is a 32bits application and I think those benchmarks are compiling 64bits applications. If you want to test with the GC doing the job, you can use : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeNew.of If you want to test with manual allocation/desallocation : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeAlloc.of The --W4 option is to use 4 threads, whatever the number of cores on your plateform (I think this is the conditions of the benchmark). You can alos use : --W1 (or no option) to use only one thread. --W0 to use the number of threads equal to the number of cores of your system. Franck 
Here is the last version I found corresponding to the benchmark implemented in Oforth : https://web.archive.org/web/20170103000340/http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=binarytrees 
I don't have time right now to compile or run other languages. On my system (core I7) : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeNew.of takes 5.4 seconds. oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeAlloc.of run takes 4.8 seconds 
thanks, that also helps
It gives you a lot of flexibility. The Forth source code becomes a relocatable object file for the toolchain. The Forth compiler replaces the linker as well as the compiler in a conventional tool chain. Forth source code can be compiled to native code for performance, or compiled to byte-code for small size or threaded code for something in between.
The change was [one year ago](https://alioth.debian.org/forum/forum.php?thread_id=14996&amp;forum_id=2965&amp;group_id=100815).
Just in case you didn't spot it already, there's some material on Youtube. IIRC when Jeff Fox died Samuel Falvo among others took actions in order to backup the site, thinking it would go down sooner or later. See for instance [1x Forth](https://www.youtube.com/watch?v=NK0NwqF8F0k)
Thanks I have updated the benchmark (in fact, it is simplier than the previous one) On my system (Core i7), the results for the new benchmark are : oforth --P"#[ 21 binarytree ] bench ." --W4 examples/newBinarytreeNew.of ==&gt; 5.2s oforth --P"#[ 21 binarytree ] bench ." --W4 examples/newBinarytreeAlloc.of ==&gt; 4.1 s 
fyi There are some gforth programs from 12 years ago in [these subfolders](https://alioth.debian.org/scm/viewvc.php/shootout/bench/?root=shootout).
I'm not sure what you're getting at , but at the end of the day, you still have a Forth interpreter, whether it's implemented in cpu instructions or verilog. Maybe you want to 'hardcode' your forth app in the FPGA. No interpreter, it's wired to only run the one app. If you are going to do that, why use forth at all? Why have a middleman? 
The solution lays in the initial configuration of a specialized soft-core which directly exposes FPGA ressources though it machine-code format. The overhead for this is minimal. I currently work on such design (of which I can not tell more details at current, sorry). Beside this, there exist already some chips which allow fast reconfigurations. You can expect much more development in the near time.
Yes.
That was not my question. I question the future **demanded** requirement.
I'm sorry then, I still don't understand what you're asking. 
I also started a web backup after reading about his death. However, the videos where not part of it. It can be that Samuel Falvo or others had direct access to the server(s?) and given there where still somewhere restored now the possibility make them available.
I beg your pardon that I did not written specifically enough to prevent misunderstandings.
The idea you describe is exactly how the Forth Cooperative tasking system works. Technically it would not be "preemptive" because a Forth definition would never be interrupted in the middle of the code. I think you have realized the potential of the Forth cooperative model in that every I/O operation returns to scheduler. This means the time-slices are very atomic which makes the system more responsive than is intuitively obvious. Combined with a fast response interrupt for hard real-time requirements (asynchronous data acquisition mostly) you get an extremely good real-time O/S
Something exactly like the defstruct macro, generating words like: title@, author@, subject@, book-id@, title! author! subject!, and book-id!. Each of these words will have different behavior according the type, considering a Forth implementation with a floating point stack.
In Gforth I can write: s" mydef" nextname :, but this only works in the interpreted mode, not inside a colon definition.
Thanks, very interesting.
Ah, so that would be a list of words you provide. So you want it to work like "defstruct book title author subject id" and it would generate "book.title! book.title@ book.author! book.author@ book.subject! book.subject@ book.id! book.id@". Possibly a "book.new" as well? It seems like you'd have to do a little bit of work to actually make a type system. Still, it can be done. There's very little in Forth that will be able to just give you structures like this; you'll have to work to get them. However, this too, is not very hard. What you're asking for basically boils down to a few string manipulations (and knowledge of how to construct a new entry in the dictionary with them), and calculating offsets into the structure itself, and finally reserving space in memory for a new structure. I suggest (automatically) creating an intermediate structure that holds all the offsets and structure names, and then generating the definition words from there. Look into "create" and "does&gt;" as well as "postpone".
The presence of parsing words without non-parsing factors is an annoyance in many code-generating cases. There's a really clever way around it that gforth includes called EXECUTE-PARSING, but it's quite a hack and requires some words from the core, block, block-ext, exception, file, memory, search, and search-ext wordsets (but all of which are standard). It works by constructing a string to evaluate, but avoids the usual late-binding issues associated with that method by ensuring that the first word in the string to be executed only does exactly what the XT you pass does. The exact details are in compat/execute-parsing.fs. That being said, I recall some saying that the automatic name-generation of defstruct in Common Lisp was a bad idea. In standard forth, it's certainly more difficult. The usual structure-defining words have a much easier time of it by letting the user supply all the names (for an example, look at how simple forth 200x structures are...). That being said, it's definitely possible. I'd note, though, that while some-field@ may correctly get a floating-point number on the floating-point stack, the user will still have to know that it's a floating-point number and use f+, f*, etc. 
It's usually trivial to define a variant of `:` that can take the name string as an argument off the stack if your Forth doesn't provide one out of the box. That's all you really need to build up the rest of your hypothetical. Why bother and would it lead to a well factored Forth application is a different matter.
At first I thought Forth has the same capabilities for metaprogramming than Lisp with postpone/compile, and etc. But thinking more deep about it, I came to the conclusion that things are way harder in Forth since there isn't a stage for "macro expanding", before compilation, that exists in Lisp. It should not be difficult to create such environment, though.
EXECUTE just runs a function, EXECUTE-PARSING runs a function, but sets a string as the input source. E.g., : s+ { addr1 u1 addr2 u2 -- addr u } u1 u2 + allocate throw { addr } addr1 addr u1 move addr2 addr u1 + u2 move addr u1 u2 + ; : reorder { a b c -- a b c a b c } a a b c a b c ; : getter ( n a u -- a' u' ) s" @" s+ ['] create execute-parsing , does&gt; @ + @ ; : setter ( n a u -- a' u' ) s" !" s+ ['] create execute-parsing , does&gt; @ + ! ; : make-field ( n a u -- n-1 ) reorder getter setter 1- ; 5 s" title" make-field In this, `setter` and `getter` will pass the constructed string to `create`, so that `create` uses this instead of the input stream.
So basically registers serve as an directly accessible L0 cache. 
That would be a good way to think about them. Think about it: what do Forthers usually store in the CPU registers? The element at the top of the stack, the pointers for both the parameter and return stack, the pointer to the next instruction. Those are all high-traffic items, at least as far as the primitives and the virtual machine are concerned. I wouldn't trust a compiler to make the same executive decisions.
I'm curious as to why my posts here get downvoted so heavily. Maybe a function of so few people in the subreddit?
May be because you are posting about every micro feature of a language nobody cares about? P.S. I've never down-voted your posts. 
Thanks. Considering this is a Forth group, and 8th is a Forth implementation, I would assume posts about it could be at least somewhat interesting to the group; and if not, ignored. If I posted about every micro feature, there would be a ton more postings from me.
I often upvote your posts but downvoted this one. The reason is simple as u/giant-torque said - I consider this a micro feature and therefore bordering on spam. If someone wants to read about small changes like this one they will visit the forums.
I can't speak for anyone else but I upvoted this post because there is more than meets the eye with microfeatures when they are compared and contrasted with other re-factorings and approaches. At least for me, forth programming is sometimes about slowing down and thinking deliberately about one plus one equals two. Examining the least examined assumptions, those that we usually take at face value before moving on is often the most fertile (and disorienting) source of new ideas.
Once again, Forth's low floor high ceiling strikes again. Hell yes!
What do you mean by &gt; it will fast forward through the same instructions as otherwise without executing Does that mean it will start to scan through code until it finds a catch? Seems like the scanner would have more general overhead than, say, a linked list of exception handlers embedded in the return stack.
It won't start anything; it will go on as usual, skipping anything that's not a catch; which means that it won't really scan that much code since no function calls are made and pretty much everything is a function call in Cixl. Like I said, it may be slightly slower when throwing; but I consider that a small price to pay.
Amazing! So cool work. I am realy impressed:-) 
Hey, Rudditavafasen, just a quick heads-up: **realy** is actually spelled **really**. You can remember it by **two ls**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
If you're saying "Forth written in HDL with routines that reconfigure and recompile it's own bitstream" then I think your question should be rephrased as "Is memory still a requirement for Forth?"
Return is indeed special, it simply closes the scope and ends the call. There are a couple of similar instructions that likewise need to clean things up on the way out but no user code but `catch:` is run, which means no loops or branches.
Ok. From Sweden, so english is not used everyday. Still so impressed by the work on Forth 1!!! Whish i could se it IRL. Almost went to the Forthday meeting in SF last year but my work got prioritized. (Did i manage to spell right this time?)
Isn't it a little mis-leading to call it "zero-overhead". ITC Forth spends 50% of the time running NEXT. You have added some code to NEXT therefore there is overhead. No?
I think it is at least a clever idea to put an interrupt check somewhere in the VM code. After reading this article I thought that you could do it probably more effectively during the execution of EXIT, since it would at least let certain definitions execute atomically.
This is quite impressive, it's good to see people interested in computing archeology. 
To me it was hard to write a metacompiler in the beginning, because many words exist in three versions: the host word, the metacompiling word, and the target word. I only got it at the third attempt, and then not very well. I made a fourth one which I felt better about. And then a number five which is a generic cross compiler. By then, it felt rather easy and natural. But it wasn't in the beginning.
Part of the power of Forth is that it gets out of your way; I've never struggled to do anything in Forth out of a lack of expressiveness, only "hitting a wall" when the task I was attempting was, itself, poorly defined. Overall, pretty badass work. Once you get a metacompiler running, porting Forth (or other languages if you are motivated enough) becomes trivial.
I had a different experience, but I haven't created a generic metacompiler, it's very much a 16-bit affair only. I found using the metacompiler to be more difficult as there can be a lot of subtle problems that are difficult to trace down, I would not want to use someone else's as I would find it too difficult to debug. This metacompiler has gone through many revisions, but they've all been incremental. It also helps that I can modify the Forth it works with. For example I made 'literal' into a vectored word so the metacompiler can cross compile literals, but it wasn't like that in the beginning. I did the same with strings. I wish I would have known more about the technique earlier on, for my [Forth CPU project](https://github.com/howerj/forth-cpu) I made a cross compiler in C for a pseudo-Forth like language, doing it all in Forth is much better. That might frame things better, metacompilation was easier for me than writing a traditional compiler in C. You're right that juggling the word-lists is the main difficulty, you're best off hiding the built in words as soon as you can and as much of the metacompilation wordset as possible. It's one of the main sources of errors. I certainly think more Forth programmers should learn about it.
Thanks! It's pretty satisfying getting everything working. I don't have problems with the expressive power of Forth too much either, but I think there is definitely a Forth-way of solving a problem and it can be quite difficult getting there, but once there the solution if not being more elegant will certainly be more compact at least.
Thanks! I just hope people find it useful.
 sp@ nth cell * + Basically, use the stack pointer to access the stack array-style. I don't think there is a better/different way to do this. 
thanks, but doesn't this involve pushing the index onto the stack? Is there a trick we could do using that in combination with immediate words or something to get around that? 
Yes, we must use the stack index to get the nth item on the stack. No, there's no tricks, unless you want an extreme loss of generality. Typically, when using a word that fetches the stack pointer, it returns the position of the stack previous to execution of the fetch, so you don't need to adjust your arithmetic at all.
The only way to avoid that is to write a word in assembler; most likely, a different one for each index back: e.g., "4-pick", etc.
Also, "4 pick" is not particularly inefficient; and an optimizing Forth will generate a minimal set of instructions so that '4' isn't actually pushed at all.
It is bad Forth style to treat the stack as if it were an array.
I forgot! Whoops!
It is a necessary evil. In the set of all functions computable, I doubt that simple stack shuffling effectively solves all but a (possibly large) minority. Sometimes concessions have to be made regardless, especially in consideration of certain metrics for speed: getting the item 5th from the top is easy, but that's a lot of moves for a comparatively simple calculation and memory read.
Ranks high on the retro cool list.
IMO *application-level* code shouldn't assume or rely upon array-like access to the stacks. A stack processor (real or virtual) wouldn't necessarily provide array-like access to an on-chip stack. Treating the stack as an array breaks the implicit locality of reference assumption of the stack machine model, at that point you're really coding to a register machine model rather than a stack machine model. A beginner in particular needs to learn to steer clear of register-machine thinking when learning to code in Forth.
I agree; I will amend my above comment.
 : pick 1+ cells sp@ + @ ;
In which Forth? Any approach to this will likely not be portable (apart from using something like `PICK`) and very much implementation dependent. E.g., in my Forth, there's not a good way to do this as the stack is not addressable as an array. It's possible to access any element, but not efficiently. (The MISC VM underlying it only provides a handful of instructions; none allow diving deeply into the stack, so reading arbitrary values involves moving values to/from memory, which is quite costly in time.)
Wow, this is really neat. Any idea how to run this?
I think it needs a couple more files with the initial dictionary: ;Load the file to bootstrap the forth dictionary. (let ((fet (status features))) (cond ((member 'mc fet) (load '|dsk:kle;fordic &gt;|)) ((member 'ai fet) (load '|ai:kle;fordic &gt;|)))) 
Wasn't there a post a bit ago about constructing Lisp in Forth too? Yo dawg, I heard you like programming.
Good catch!
Of course in most implementations there's an efficient way to index the stack. But while you might think that having that sort of primitive would optimize things, in reality it's just a way to half-fix what you broke. It's an optimization over a "decimization". You should understand why you ended up in this mess instead. You might think that you grok stacks a little, but in reality if you come from other languages you have habits or you have been taught to do things in certain ways that will be problematic in Forth. Like passing everything up and down as parameters (for instance, a file handle). The common programmer wisdom is that global variable are evil. Just like "goto considered harmful", this is repeated over and over but not all programmers read Dijkstra's letter and not all programmers can tell you why they are evil. In particular, in which context and which way of using them makes them "evil". Programming in Forth is the chance to question those religious beliefs. Be evil, break those rules and have fun. If a global variable is more convenient, use a global variable and see what happens. I have here a small convenience HTTP server written in my Forth dialect. Here I should mention that it is of the minimal variety, not one of those fancy Forth++ dialects. The listening socket handler is in a global variable. The client socket handler is in a global variable. I/O buffers are static too. Why? Because it's just a small HTTP server that listens on just one port and handles just one request at a time. It's not something that's meant to be used by Google or Facebook. It's good enough for what I want to do - that was #1 requirement. Using global variables made it a lot easier to write. If one day I need a server that handles concurrent connections, that's a whole different problem that requires a different approach - partly because that kind of server has to handle SSL and some form of CGI etc. Sure, passing handles and buffers up and down might make more parts more reusable, but I believe that changing "client-socket @" into "client-socket@" and have that actually fetch stuff from a client-connection structure might do the trick... I don't know. Why would I solve problems I don't have nor expect to have anyway? How would I evaluate the adequacy of a solution to a problem that doesn't exist yet? Code reuse is overrated. Doing *ruthlessly* the simplest thing that could possibly work saves you a lot of time. Enough to cover the cost of a total rewrite if necessary. Actually the experience you gain while writing stuff is way more valuable than the lines of code themselves. 
thank you so much
I use IMMGUI in :r4 [https://github.com/phreda4/reda4](https://github.com/phreda4/reda4)
[Using Glade to create GTK+ Applications with FORTH. [PDF]](http://www.complang.tuwien.ac.at/anton/euroforth/ef10/papers/mahlow.pdf)
No offense, but if you're someone who likes things done for you, then you might not enjoy FORTH very much. This is not said as a negative value judgement; I virtually never cook for myself, but eat out constantly. There is no McDonald's for FORTH, though; so if you want that, you'll be cooking.
&gt; It is a necessary evil. In the set of all functions computable, I doubt that simple stack shuffling effectively solves all but a (possibly large) minority. I tend to implement such functions as simple stack-based operations on appropriate structures in memory. This is clearer than pick and roll and doesn't require you to abuse the stack by treating it as an array. If you need an array, use an array. If you find you need applicative functions/function application or functions with local variables and/or lexical you can implement them too. As you well know, Forth provides a minimal computational model that you can use to implement the language or semantics you need. tl;dr Forth is a stack-based language, and I love the stack, but I've come to believe that as Forth programmers, we tend to overuse the stack.
I write a lot of Forth. Please don't try to extrapolate things about me from a very simple question
Thank you.
If I do end up using something like Pick, I switch to using locals, sparingly, and most often for large mathematical calculations. I implemented locals by putting them in the Return Stack (somewhat similar to C's stack frame), so they're out of the way. 
arent you stoped developed it for android platform?
for now, yes, if anybody like compile the code , I can send the last patches, my interest is in programming with: r4, not the interpreter, the android ide is a mess.
**Fluent interface** In software engineering, a fluent interface (as first coined by Eric Evans and Martin Fowler) is a method for designing object oriented APIs based extensively on method chaining with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
thx botbro
I'm back! Announcing my millionth stab at a game engine in Forth, and hopefully the last. This one is actually usable and designed for portability, yet very simple and straightforward. SwiftForth (Windows + Linux) only at the moment.
I have seen a lot of expert code doing just that, so I guess the practice is canon. Efficient it is not, but efficiency is overrated in programming. 
Depends on the implementation on whether it can be fast or not. Many processors will support a stack frame-like structure and indexing into it, such as x86, and it also doesn't do anything nasty with the cache.
Simplicity is a prerequisite for reliability. (Edsger W. Dijkstra)
&gt; Adding complexity to manage complexity is a losing proposition. (Jeff Fox) Well not according to the gods of Java :cough: :coughard:
The goal was very simple: to minimize the complexity of the hardware software combination. As far as I can see no-one else is doing that. (Charles H. Moore)
"There seems to be a propensity to make things complicated. People love to make them complicated. You can see that in television programming you can see it in products in the marketplace, you can see it in internet web sites. Simply presenting the information is not enough, you have got to make it engaging. I think there is perhaps an optimal level of complexity that the brain is designed to handle. If you make it to simple people are bored if you make it too complicated they are lost" 1xForth is a treasure trove.
There's always state, what differentiates different approaches is where it is kept. (Charles H. Moore)
https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub
Some issues: - First, this doesn't load unless I'm logged into LinkedIn (which I do seldomly). I've noticed this with LinkedIn before; while it may help you get some views, it'd be better to post a direct link as well so those not using LinkedIn can view the article. - Secondly, the LinkedIn is just an excerpt, the full article is at [https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub](https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub) Digging into the article a bit, you mention things that you never actually touch on. For instance: &gt; To explain the interesting properties of the web browser and the Unix terminal, we need to resort to something even older than Unix itself, and still surviving. I didn't see anything explaining interesting properties of web browsers and Unix terminals in the article. Also: &gt; No, it is not LISP. But it is also related to LISP. More later about the relationships between Forth and LISP. I didn't see anything else about this in the article. The rest of the article is really barebones. You present an example of using JavaScript in an RPN-ish fashion, but I don't see an example showing how this: document.body e: .childElementCount e: s: Is any better than: document.body.childElementCount I'd be more interested in seeing examples involving actual code. How do you define functions? Handle conditionals, loops, creating data structures? Have you developed any actual applications using this or written any documentation for it? Given that your language appears to be built as a sort of reverse polish layer over JavaScript (based on the limited article, and some things posted about it on comp.lang.forth), I'd be curious to know how it works alongside existing JavaScript libraries.
Forth is the only processor-independent language lets you think like a microprocessor. This is a skill that is necessary, though not sufficient, to evolve past cut &amp; paste kiddie koding. &gt; “A language that doesn't affect the way you think about programming is not worth knowing.” - Alan J. Perlis
This one's from John McCarthy ([who taught Chuck Moore](https://colorforth.github.io/bio.html) at one point): &gt; Program designers have a tendency to think of the users as idiots who need to be controlled. They should rather think of their program as a servant, whose master, the user, should be able to control it. If designers and programmers think about the apparent mental qualities that their programs will have, they'll create programs that are easier and pleasanter -- more humane -- to deal with.
[Chuck Moore...] Behind everything he does is a radical message: 'Embrace the entire problem, Keep it simple'. (Richard Morris)
Forth-iness is very intertwined with blockchain. I believe a dual-stack virtual machine would potentially be a perfect VM for blockchain applications (smart contracts), given their inherent simplicity.
New hardware is poised to disrupt supply chain management with blockchain, like this [10-cent computer from IBM](https://singularityhub.com/2018/03/26/ibms-new-computer-is-the-size-of-a-grain-of-salt-and-costs-less-than-10-cents/) What this and other IoT ideas will need is a tiny, efficient VM language that's good at machine-machine communication. I'm sure their labs are hard at work trying to contort Java to somehow fit.
There's an article that was posted a few months ago with nearly the same topic: https://www.reddit.com/r/Forth/comments/7kx0on/advanced_bitcoin_scripting/ I have no doubt that the person that invented the blockchain was very familiar with Forth.
So you want to replace the word compiler to asm to one that compiles to FPGA layout. Sure, yeah.You still need to implement that compiler-thing in either a cpu or the fpga, and have an interpreter, whether it's on a cpu or fpga. An FPGA can (and often does) simulate a small cpu like a microcontroller, however I think I agree that compiling to FPGA would be interesting. Now, I'm no FPGA expert, but it seems you'd have to have some sort of intelligent whole-program compilation in order to route words correctly together. Can you have FPGA sections like functions that are dynamically connected? I guess.
I got volksFORTH to work in Apple 1js, by pasting it into the Load box from the file f6502.hex in the latest zip in https://sourceforge.net/projects/volksforth/files/volksForth%20Apple1/ and then use 300R to run it.
A video showing FORTH on Apple 1js An Apple 1 Emulator in JavaScript &amp; towards FORTH on Apple-One Clone in Verilog FPGA https://youtu.be/VJMdmNWM_ZQ https://www.scullinsteel.com/apple1/#Volksforth http://volksforth.sourceforge.net/ https://scratch.mit.edu/projects/137676871/ https://en.wikipedia.org/wiki/Apple_I https://github.com/alangarf/apple-one https://usborne.com/browse-books/features/computer-and-coding-books/ https://drive.google.com/file/d/0Bxv0SsvibDMTcHNXalEtYkVtU00/view https://youtu.be/2yIBnhapzEU
**Apple I** Apple Computer 1, also known later as the Apple I, or Apple-1, is a desktop computer released by the Apple Computer Company (now Apple Inc.) in 1976. It was designed and hand-built by Steve Wozniak. Wozniak's friend Steve Jobs had the idea of selling the computer. The Apple I was Apple's first product, and to finance its creation, Jobs sold his only motorized means of transportation, a VW Microbus, for a few hundred dollars, and Steve Wozniak sold his HP-65 calculator for $500; however, Wozniak said that Jobs planned to use his bicycle if necessary. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
To answer the licensing issue: 8th's license pertains to the tool, not to the programs produced. So at the point when 8th can make tiny executables for MCUs, the license would still be a "purchase the 8th version you want, and make as many executables as you like, forever, without any restrictions or license fees". If you choose to never update or upgrade your 8th version, whatever version you do have will continue as before.
Probably the concept of a Rump kernel is interesting for you: [Rump Kernels](http://rumpkernel.org/) There can be build to my knowledge from NetBSD user land and as these operating system is known for it wide range of supported platforms this can be a good start for supporting embedded systems: [NetBSD Embedded Systems](http://www.netbsd.org/about/embed.html)
Thanks for that link, it does sound interesting in fact. 
You're not wrong, man...
Shit, his comments about IoT are prophetic. And the internet being a gated community turned out to somewhat true, especially with the costs associated with running a website, and end-users now dealing with horrendous website bloat (multi-megabyte pages). 
The former case.
What costs do you mean? You can rent your own small physical server for $5. If you need less than that, it costs even less.
For you and me, maybe sure, a website could be built on a whim. But for the lay man?
I was thinking of someone who had absolutely zero knowledge and didn't even know what terms to look up to get started. It is a huge investment of time, at the very least. That's all I wanted to say.
Yes, the modern tech stacks are all overcomplicated; if we didn't agree on that we probably wouldn't be here. :)
Check out jonesforth, it answers almost every question you've asked (jonesforth.S and jonesforth.f): https://github.com/AlexandreAbreu/jonesforth And DO doesn't *need* LOOP, I think gforth is syntax checking that on your behalf. DO does need a start and limit though.
&gt; And then there's a return stack, where the next word to execute in a definition is stored, or something? "Something". The return stack is the call stack, where the return address is pushed before calling a word. The example you use is not good, because '+' is typically a primitive word. Rather, work from these examples: : square dup * ; : cube dup square * ; In cube when calling square, the return address is pushed. It is popped back to the instruction pointer when executing the semicolon. &gt; CREATE takes the next token from the input stream Yes, there are a few special words that do this kind of things. like ":" "'" or "TO". &gt; How do they (words like CREATE) work under the hood? It depends on the implementation. &gt; Can I make my own words that work like this, without defining them in terms of CREATE? I haven't read the ANSForth standard for a while, but I don't think it's possible. &gt; What is the difference under the hood between ' and ['] Basically ['] is the "immediate" version of '. &gt; And related to this, what about words like ." that scans the following characters instead of just doing something with the next token? How do they work? Implementation-dependent. Doing is seeing. Just try to implement a Forth-like language in your language of choice. Or read the source of some simple implementation (the sidebar recommends Jonesforth, but a C implementation should be easy to find; implementations other languages such as Haskell, Lisp, Python are often too toy-ish to be useful for you or anyone else actually). &gt; Are there any words that a user of Forth could not define himself, in principle? Of course. I would add that there are an infinite number of them. &gt; Is there like a syntax checker, or does it work some other way, by for example DO leaving some state that LOOP later uses? Basically GForth detects that something is left on the stack when the definition ends. It can do that because a "control-flow stack" is used to resolve the implicit forward references made by some control-flow words. See for instance [IF](https://www.taygeta.com/forth/dpans6.htm#6.1.1700) &gt; I don't understand exactly what DOES&gt; does. Should it be used only after CREATE? It defines what the CREATEd word does. In simple implementations, CREATEd words by default pop the return stack, thus leaving on the data stack (at runtime) the dictionary address just after the word. DOES&gt; allows you to make it do extra things. CREATE DOES&gt; are a bit tricky because they mix compile-time and run-time actions. &gt; Also, I don't understand how immediate works. It does set a flag in the header of a dictionary entry, right? But how does it find the place? It just sets the flag for the latest definition. &gt; HERE now points to the end of the dictionary entry, doesn't it? "dictionary" is an ambiguous term. It can refer to the list of words, or it can refer to the space where you can allocate data (with ALLOT or the comma). It's ambiguous because in many historic implementations, the dictionary (of words) is a linked list stored in the dictionary (of cells). It might even have been mandatory in the previous standard (Forth84). A current standard makes more distinctions (see [the standard](https://www.taygeta.com/forth/dpans3.htm#3.3) ). That's what I do in my dialect for instance. The dictionary (list of word) is just an array of structures in the C program, and I compile words and ALLOT in an other array that I called "workspace" in my C source precisely to avoid that kind of ambiguity. &gt; And what is an "execution token" exactly? A Beginner's Guide to Forth says that it is "often an address, but not necessarily". Hm, OK. "execution token" is ANSForth lingo for a thing that you can EXECUTE or COMPILE. Think of it as a function pointer; or think of it as: "'" returns something of the "executable" type, EXECUTE only accepts something of the "executable" type. 
[jonesforth](https://github.com/AlexandreAbreu/jonesforth) is a good start to understand these things if you know assembly. There's words like `PARSE` that can read the next word or anything you wish. Words that use these kinds of words are usually immediate so that they can actually run `PARSE`, well, immediately :) Words like `DO`, `LOOP`, `IF`, `THEN` etc are often implemented as immediate words that leave a value on the data stack (like `IF`) for the next word to pick it up and compile it into an e.g. jump (like `THEN`). How is GForth exactly checking for an unstructured word I don't know. If the standard doesn't require these checks one could in theory omit them and say it's undefined behavior when used like that. The xt (execution token) is the address of the word's definition that you can invoke directly with `EXECUTE`. When `CREATE` is creating a dictionary entry it writes down some information about the word, it's name, name length, xt and maybe some other metadata, and a pointer to the next/previous word. So when the forth interpreter sees a word and looks for its definition you can imagine a word like `FIND` looking for a name match, `XT` (I don't know if this word really exists) to fetch the address from the entry and `EXECUTE` to finally run it. A wordlist is a list of words :) It's similar to a package, module, namespace in other languages. If you want to understand the internals read jonesforth. For a non-ANS take freeforth is fun too :)
&gt; It is popped back to the instruction pointer when executing the semicolon. Maybe that's pedantic in this context, but I don't think this is completely accurate, as the semicolon must be an immediate word (since it has an immediate action, namely switching from compile mode to interpret mode). So the semicolon word itself isn't actually executed when the definition is executed.
That's a good point too, about English and non-native speakers. And yes, "cost" in English doesn't just apply to money. It broadly applies to resources, regardless of what those resources might be.
Fixed: https://github.com/PDP-10/its-vault/tree/master/files/kle
Another good resource is the Jupiter Ace manual, which is for a home computer from the early 1980s. Its version of Forth is a little idiosyncratic but it does explain everything in gradually increasing levels of complexity. 
Thank you! I'm looking forward to studying this in the future :)
Check these guides about forth \(FlashForth\) on certain microcontrollers. [http://flashforth.com/ff5\-elements.pdf](http://flashforth.com/ff5-elements.pdf) [http://flashforth.com/ff5\-tutorial\-guide.pdf](http://flashforth.com/ff5-tutorial-guide.pdf)
You're welcome to try to run it. If you don't have a Maclisp handy, I can help with that. :-)
The cost of some beer for someone suitably skilled, and a few quid a month for hosting.
Very well, let me put it this way. How many on-ramps will connet the world’s ghettos to the Information Superhighway?
If you've got absolutely zero knowledge in any subject there's got to be a massive investment in time, money and effort to do anything. That's like saying "playing the guitar is a gated community" because you can't just pick it up and be Carlos Santana.
That's hardly a reasonable comparison. Guitar can be reasonably mastered with some effort. I don't expect anyone to simply run a website with an "Internet for Dummies" book.
It seems VolksFORTH is 16 KB in size which is loaded into RAM, and then it requires some RAM to run. Apple I for FPGA currently only has 8 KB RAM. Apple 1js probably has 32 KB RAM.
What, specifically, is so hard about running a website? Anyone that's basically functionally literate can do it!
I'm constantly taken aback at how even experienced forthwrights who should know better blithely disregard the most basic precept of the Forth approach: &gt;Forth is highly factored code. (Charles H. Moore)
Starting MMSFORTH v2.0 Forth programming language on a TRS-80 Model 1 emulator running on DuinoMite w/ PIC32, and running some arcade games (Breakout &amp; Tron) and board games (Othello/Reversi &amp; Tic-tac-toe). Source code for each FORTH program is available on the diskette. https://archive.org/details/MMSForth_v2.1_1982_Miller_Microcomputer_Services https://hackaday.io/project/9077-trs-80-model-1-on-a-pic32 https://www.olimex.com/Products/Duino/Duinomite/DUINOMITE-IO/ https://www.electrokit.com/produkt/duinomite-io/ https://www.olimex.com/Products/Duino/Duinomite/DUINOMITE/ https://www.electrokit.com/produkt/duinomite/ Tron (1982): https://www.imdb.com/title/tt0084827/ Another video showing the hardware: https://youtu.be/b3IxyqHwq80
In this case, I'd probably just use a variable: variable key : find-key ( list -- value|false ) key ! foreach key @ str= if exit then drop next false ; " foo" find-key " bar" find-key 
Well actually, in 8th you *can* use "curry" to accomplish this. Regarding postpone etc, yes: it's ugly... but it's also extremely powerful when used judiciously.
Oh, that's odd! "help curry" doesn't work, but "help curry:" gives: ns: curry name: stack: x w1 -- w2 IMMEDIATE desc: Given a word "w1" which takes N parameters, returns a new anonymous word "w2" which takes N-1 parameters, where the first (TOS) parameter is "x". ns: curry name: stack: x w1 &lt;name&gt; -- w2 IMMEDIATE desc: Same as "G:curry", but creates a named word "w2". There is also samples/misc/curry.8th
The 'ns:' is wrong, which is the problem w/ the help. It should be "G:", and in fact "curry" and "curry:" are both in "G:....
The corrected help reads: ok&gt; help curry ns: G name: curry stack: x w1 -- w2 IMMEDIATE desc: Given a word "w1" which takes N parameters, returns a new anonymous word "w2" which takes N-1 parameters, where the first (TOS) parameter is "x". ok&gt; help curry: ns: G name: curry: stack: x w1 &lt;name&gt; -- w2 IMMEDIATE desc: Same as "G:curry", but creates a named word "w2".
The problem is that I'm solving a somewhat abstract problem here, which is not what one usually does in Forth. I'm implementing a JSON library to allow easy communication with web frontends. So I don't know the size of objects in advance. My looping construct does indeed make the return stack unusable. This probably isn't necessary, I could keep the current position on the data stack. However, it's consistent with my JSON-array iterator loop, which keeps the data stack free, which is nice for calculating sums and such. But maybe consistency is hurting more than it helps here... I'm aware of the performance implications of lists - I use them here for convenience. And I have a cute way to construct them with O(1) insertion at the end. I don't think this kind of meta-code is hard to write, since I can easily transform a non-meta version of it into the meta-version. Regarding maintainability... you may well be right, though.
True. In my Forth, I'd hide the variable from the global namespace. I don't think there's an actual standard way to do this though. Or, depending on how `foreach` is written, just use `here`: : find-key ( list -- value|false ) here ! foreach here @ str= if exit then drop next false ; 
Using `here` is clever. I usually don't use `here` for temporary data, but maybe I should become more flexible about that. And I actually do have a mechanism for making words local in my Forth.
This sounds fine, and even perhaps good advice, if there are a finite set of values you know in advance and are prepared \(or have to\) type in yourself. But how does this help Wolfgang, who is implementing some kind of JSON thingy to communicate with Web stuff? He doesn't know the size of his objects in advance, so presumably he can't know the values, either. He hasn't said what exact role his program is playing, but I can't see how this technique could be useful to him. His program may well be consuming data from sources outside his control. Even in the example you give, I am wondering about the practicality. If you only have a small handful of star trek episodes in your media library, then probably the release number is of little value to you, and even if it is you hardly need a program to manage it. If, however, you have all of Star Trek, or even just a couple of series then I'm not going to be typing hundreds of definitions to map this information — I'm going to be trying to find this recorded somewhere else already, and slurp it in somehow. I suppose you could write something in Forth that parses this in some kind of tabular format, produces a forth source file containing all the definitions, and then INCLUDE it... is this what you would recommend? Honest question — it seems like a bit of a kludge to me, but perhaps it is in fact the easiest thing to do. In a language that provides some kind of key\-value mapping structure out of the box, I'd certainly rather use that then start exporting files only to import them and execute their contents. Forth has its dictionary, so it's good advice to use that. But AFAIK doesn't provide any really convenient way to programatically add entries \(at least, not in the ANS standard\). I have used Gforth's `execute-parsing` to do this before, however if there's a better way, I'd sure like to know... 
&gt; I suppose you could write something in Forth that parses this in some kind of tabular format, produces a forth source file containing all the definitions, and then INCLUDE it... is this what you would recommend? Honest question — it seems like a bit of a kludge to me, but perhaps it is in fact the easiest thing to do. My suggestion here will be considered heretical, but GNU FORTH contains the ability to execute pre-written FORTH scripts; which are added to the dictionary of words at runtime. For Star Trek episodes, there are probably any number of textual episode lists around or IDF database entries. Then I need to find out what number range, within the overall total, that Voyager as my example occupies. The start of my loop is the lowest number in that range, and the end is the highest. A text file contains the list of Voyager episode names, and for every number in my loop range, I have ed extract and delete (pop, essentially) the first line in said file. Commands.ed:- 1p 1d wq My main script:- 1 169 #!/bin/sh -x index=169 count=1 main () { fn=$(ed star-trek-filenames.txt &lt; commands.ed | sed -n '2p') filename=$fn echo -e "\: $filename $count \;" &gt;&gt; star-trek-records.forth count=$(expr $count + 1) next } next () { t2=$(expr $index - $count) temp=$t2 [ $temp -eq 0 ] &amp;&amp; ZF=1 [ ! $temp -eq 0 ] &amp;&amp; ZF=0 [ $ZF -eq 1 ] &amp;&amp; exit 0 [ $ZF -eq 0 ] &amp;&amp; main } next I'm stoned and I am probably forgetting something, but hopefully you get the idea. The above is a shell script counter with a loop, that assigns a number from said counter to each one of the Voyager file names, as they are pulled out of the text file by ed. Said name and its' associated number are then formatted into a word to be compiled, within a GNU FORTH script file. From there, I could rename my files simply as numbers, and from within the Voyager subdirectory, run a GNU FORTH script calling mplayer with the system word. This might seem pointless and excessive work, but again, the point is that I can either type in the number or the file name myself, and only the number still ends up on the stack; the conversion is performed automatically.
This is what I do in order to avoid the shuffle, but I guess I can do one better in my forthoid, because I have local CONSTANTS: : find-key ( list -- value|false ) qonst key foreach key str= if exit then drop next false ; The operative word here is 'qonst' which takes from the stack and binds the value to the name following 'qonst'. (In my forth, you have named parameters, too, which are also constants. In the example above, you could access the parameter 'list' with that name, even if that name is already taken outside the scope of the colon definition, because those qonst-constants are local.) Constant data is best data. 
This is very impressive performance. gForth is a relatively fast version of Forth if you use gForth-Fast. Is that the version you used for testing? I know nothing about WebAssembly. I need to do some studying. It appears to run native code to achieve those results. (?)
Aha, I didn't know about gforth-fast. Thanks a lot for pointing that out, that's indeed much fairer! I wasn't sure what to expect from gforth, but if it's relatively fast, then 'very impressive' would have surprised me a lot. Turns out my implementation is about 2.5 times slower.
Looks pretty neat. Took me a sec to realize it's case sensitive. It seems like some errors put the system into a broken state. E.g., since I used lower\-case `cr`, everything breaks thereafter: WAForth 1 2 + . 3 ok : test ." Hello" cr ; error 1 2 + . error 
I've tried using `dc`, but I prefer [Emacs' Calc Mode.](http://nullprogram.com/blog/2009/06/23/) It's also RPN, but it's been turbo-charged into a CAS.
That's interesting. I hadn't messed with `dc` in many, many years. Thanks for the post!
Have you checked the info pages? 5.5.6 "Floating Point" includes mention of `f**` with stack effect `r1 r2 -- r3` and description `r3 is r1 raised to the r2th power`. (I'm assuming you were looking for a floating point version. An integer version would be significantly easier to hand-roll, if you're okay with a naive version. There's probably something about it *somewhere* in the gforth code, though)
Thanks for this, idk why I didn't see that
Except it isn't at all. I've never seen anyone ever speak specifics with block chains. Why? Because they're inefficient, extremely niche, and traditional solutions almost always do it better. They're next to useless at scale due to a slew of massive inefficiencies. 
* inefficient (depends \- Proof of Work is stupid; Proof of Stake is much better) * extremely niche (yes, so what's the problem? Forth is the embodiment of niche.) * don't scale (addressed by Ethereum's Layer 2) Ethereum uses a stack\-based virtual machine (EVM). One of the best languages for coding in EVM is Low Level Lisp (LLL). It feels a lot like Forth. Blockchains have evolved considerably since Bitcoin's invention.
And how does any of that create anything that a business needs that isn't filled better by already existing tech? "Its a decentralized***** public ledger! It'll change the world/disrupt industry/make us free!" There's no logical connection between those two ideas. Hell a decentralized***** public ledger isn't even a good idea in 99% of cases. Ethereum's Layer 2 isn't even a concrete thing, its an abstract idea with a set of concrete implementations. You're building a tower of next-to-useless abstractions that benefit no one and burn electricity like nothing else for no reason whatsoever. None of that is forth-like. "feels like forth" my ass. You don't even know what forth means other than "stack machine"
&gt; You don't even know what forth means other than "stack machine" Do you have the faintest ********** idea of who I am ??
A person too afraid to type a word out and entirely swallowed by the silicon valley bubble. In other words: a complete tool.
I'm in about the same place in my understanding. Here are some of the links I'm keeping around to read. Apologies for the disorganized parts of this linkdump. good, detailed intro to using classic Forth http://galileo.phys.virginia.edu/classes/551.jvn.fall01/primer.htm ground-up explanation of the machine code Forth generates and how it gets there https://github.com/AlexandreAbreu/jonesforth/blob/master/jonesforth.S https://github.com/AlexandreAbreu/jonesforth/blob/master/jonesforth.f FIRST &amp; THIRD (almost FORTH): walkthrough of a tiny language FIRST sufficient to create a small language THIRD which strongly resembles FORTH. Probably more intelligible after reading Jones Forth. http://ftp.funet.fi/pub/doc/IOCCC/1992/buzzard.2.design Punyforth: a Forth for the (nifty) ESP8266 microcontroller. Reading the core is a good follow-up to reading the THIRD code. Has quotations, combinators, case statement, deferred definitions, unit testing, exception handling, TCP/IP (the chip has WiFi), and experimental cooperative multitasking. Really quite impressive. https://github.com/zeroflag/punyforth/blob/master/generic/forth/core.forth to play with the controller, consider https://www.adafruit.com/product/2471 Trying to understand create ... does&gt;, starting with an implementation: https://github.com/philburk/pforth/blob/master/fth/system.fth right into the guts: "under the hood" from Learning Forth https://www.forth.com/starting-forth/9-forth-execution/ https://www.forth.com/starting-forth/11-forth-compiler-defining-words/#How_to_Define_a_Defining_Word http://www.vintagecomputer.net/fjkraan/comp/atom/doc/ForthTheory&amp;Practice_10-index.pdf https://users.ece.cmu.edu/~koopman/forth/hopl.html http://mysite.du.edu/~etuttle/math/forth.htm http://www.public.iastate.edu/~forth/gforth_40.html#SEC47 http://www.ultratechnology.com/meta.html http://yosefk.com/blog/my-history-with-forth-stack-machines.html http://www.forth.org/svfig/Len/definwds.htm https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/CREATE_002e_002eDOES_003e-details.html http://galileo.phys.virginia.edu/classes/551.jvn.fall01/primer.htm#create http://www.mostlymaths.net/2010/03/forths-create-does-maybe-im-amazed.html https://www.forth.com/starting-forth/11-forth-compiler-defining-words/ https://www.taygeta.com/forth/dpans6.htm#6.1.1000 http://www.bradrodriguez.com/papers/moving3.htm http://forum.6502.org/viewtopic.php?f=9&amp;t=3153 https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/User_002ddefined-Defining-Words.html#User_002ddefined-Defining-Words http://softwareengineering.stackexchange.com/questions/339283/forth-how-do-create-and-does-work-exactly Stone Knife Forth. "It is not expected to be useful; instead, its purpose is to show how simple a compiler can be. The compiler is a bit under two pages of code when the comments are removed." https://github.com/kragen/stoneknifeforth "In this file a meta-compiler (or a cross compiler written in [Forth][]) is described and implemented, and after that a working Forth interpreter is both described and implemented. This new interpreter can be used in turn to meta-compile the original program, ad infinitum. The design decisions and the philosophy behind Forth and the system will also be elucidated." https://www.reddit.com/r/Forth/comments/8e4j57/metacompiler_howto_and_small_eforth_interpreter/?st=jh2act4g&amp;sh=224f50ab A forth in MacLisp, dating to 1978. The comments are good reading if you want to know about forth implementations. https://github.com/PDP-10/its-vault/blob/master/files/kle/forth.396 Moving Forth: "Everyone in the Forth community talks about how easy it is to port Forth to a new CPU. But like many 'easy' and 'obvious' tasks, not much is written on how to do it!" http://www.bradrodriguez.com/papers/moving1.htm Also: Build your own Forth -- details of various implementations. The Heart of Forth http://www.figuk.plus.com/build/heart.htm more on implementations http://www.bradrodriguez.com/papers/tcjassem.txt a (very) few notes on learning to metacompile (cross-compile) Forth http://www.lowfatcomputing.org/23/ At bottom of first link, setup instructions for a very cheap small playground, STM32F103 microcontroller with Mecrisp-Stellaris Forth. "The combination of Mecrisp-Stellaris with the JeeLabs libraries is a tremendous Forth ecosystem for a whole bunch of ARM microcontrollers, and it has all been developed within the last two years by a small core of thoughtful hackers. Combining the two provides a very pleasant Forth microcontroller hacking experience, like a weird interactive Arduino." No FPU, no DAC, but whatever. NB: The STM32 is not the ESP32 in any way. Don't get mixed up. http://hackaday.com/2017/01/27/forth-the-hackers-language/ getting started: flashing the chip, hello world, blink an LED, and multitasking (!) http://hackaday.com/2017/04/19/moving-forth-with-mecrisp-stellaris-and-embello/ http://jeelabs.org/article/1608d/ 
I’m not sure where the example ``[ [ "data" [ 2 1 ] \numbers child root`` falls down when evaluated in the opposite order. (Note: I’m using backslash instead of backtick for attributes to avoid the formatting problems in the article.) In “stack order” it would be evaluated like this: MARK ( -- mark ) STR ( str -- val ) NUM ( str -- val ) ANON ( mark ... -- node ) ATTR ( node name -- node ) NODE ( mark ... name -- node ) MARK \ mark MARK \ mark mark "data" STR \ mark mark "data" MARK \ mark mark "data" mark "2" NUM \ mark mark "data" mark 2 "1" NUM \ mark mark "data" mark 2 1 ANON \ mark mark "data" node "numbers" ATTR \ mark mark "data" attr "child" NODE \ mark node "root" NODE \ node In “normal order” ``root child \numbers [ 1 2 ] "data" ] ]`` it could be evaluated like this, where the box denotes the “cursor” pointer: EMPTY "root" NODE "child" NODE "numbers" ATTR ANON 1 NUM 2 NUM UP "data" STR UP UP Broken down: * Start with an empty document: EMPTY □ * `root` = Allocate node `root` and enter it: "root" NODE ( node -- node ) &lt;root&gt;□&lt;/root&gt; * `child` = Allocate node `child` under `root` and enter it: "child" NODE &lt;root&gt; &lt;child&gt;□&lt;/child&gt; &lt;/root&gt; * ``\numbers`` = Allocate attribute `numbers` under `child` and enter it: "numbers" ATTR &lt;root&gt; &lt;child numbers="□"&gt; &lt;/child&gt; &lt;/root&gt; * `[` = Allocate anonymous node under `numbers` and enter it: ANON &lt;root&gt; &lt;child numbers="&lt;&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `1` = Push `1` into anonymous node: "1" NUM &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `2` = Push `2` into anonymous node: "2" NUM &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `]` = Leave anonymous node; **plus** current attribute `numbers` is full, so leave attribute too: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;□&lt;/&gt;"&gt; □ &lt;/child&gt; &lt;/root&gt; * `"data"` = Push `data` into `child`: "data" STR &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; □ &lt;/child&gt; &lt;/root&gt; * `]` = Leave `child`: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; &lt;/child&gt; □ &lt;/root&gt; * `]` = Leave `root`: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; &lt;/child&gt; &lt;/root&gt; □ Either you keep a stack of values and allocate nodes incrementally, or you keep a stack of node pointers and add values to them incrementally. Also, you *must* differentiate “nodes” from “attributes” in either method, which it’s not entirely clear that he does, due to the bad formatting in the article.
Factor has Gtk bindings: http://factorcode.org/
Unlike languages that give you really abstract string api, forth is lower level too start with. In principle, think about allocating a new buffer that is as long as the sum of the two strings, and copy them into it. You have some more interesting options to keep the code svelte, simple, and avoid allocations, but I'll leave it to the forth wizards to comment. There are places like the pad that might work for constructing it, or at the end of the dictionary if it's provably safe, etc. 
(I initially wrote a long post breaking down appending, but then realized `+place` was already defined in gforth and did most of what you want) Simplest solution would, in my mind, be: pad dup 8 test c@ - accept test +place There's some unnecessary copying happening here (you could just tell `accept` to place it directly at the end of the string), but then you'd have to reimplement most of `+place` yourself. Defining a general word for doing this (unnecessary, and mostly for style points), you run into a problem: the buffer size is hardcoded. You could get around this by doing something like: : buffer dup , create allot ; : #buffer cell - @ ; : +accept &gt;r pad dup r@ #buffer r@ c@ - accept r&gt; +place ; 8 buffer test s" ls " test place test +accept 
 s" ls " 2constant command command 3 - swap 3 + 2constant switch-buffer switch-buffer accept 3 + command drop swap system 
Does this help at all? : test s" ls " pad place bl ['] parse execute pad +place pad count system ; 
Give to 'accept' the address of your buffer + 3 chars A bit overdone, but that's what I would do: `8 constant #ls-max` `create 'ls-cmd #ls-max allot` `s" ls "` `dup constant #ls` `'ls-cmd swap move` `'ls-cmd #ls + constant 'ls-options` `variable ls-len` `: get-option 'ls-options #ls-max #ls - accept #ls + ls-len ! ;` `get-option` `cr 'ls-cmd ls-len @ system`
Thank you! Exactly what I was looking for and I have some material to study here.
Thank you. I will look into this and put it in my knowledge base.
Yes, but not for this application. You jogged by my memory and taught me something new.
Thank you. I will look into this further.
I took on a task to try and rehabilitate BASIC programmers. :-) Here is an BASIC type input statement that uses ACCEPT: https://github.com/bfox9900/CAMEL99-V2/blob/master/LIB.TI/INPUT.FTH Also the work of the late Neil Baud (aka Wil Baden) has some excellent tools for this kind of thing. Here is one of his words that appends a character to a counted string in memory. : APPEND-CHAR ( char caddr -- ) \ add a ascii char onto the end of a string DUP &gt;R COUNT DUP 1+ R&gt; C! + C! ; Usage example: S" Missing a period" PAD PLACE CHAR . PAD APPEND-CHAR 
Nothing here? Was this removed?
That confused me too. I assume he didn't want to actually create the nodes until all the terms were on the stack. Would that explain it? 
The link works fine for me on my laptop, though I had to manually download it with the latest youtube-dl on my desktop (strange, as they're both using icecat).
Wow. Emacs really is the text editor which just keeps on giving. 
I agree. Emacs actually ties a lot of the Unix philosophy together by providing many useful utilities such as this in one environment with consistent cooperation between these utilities. It eliminates the *de facto* separation that exists in Unix, where everything focuses on its own task, but there is nothing enforcing them to all work together well. There's a great article which expounds on this topic [here.](https://www.quora.com/Does-Emacs-violate-the-UNIX-philosophy-of-doing-one-thing-very-well/answer/Tikhon-Jelvis)
Unless you speak fluently lex/yacc, yes it is not a good idea. It will just be a distraction. The more straightforward way is to just use something like fscanf(), and even that is probably overkill.
hey, thanks!
in [cforth](https://sourceforge.net/p/cforth/code/HEAD/tree/) lex/yacc used to generate core image file.
great, I'll have a look at it.
Yes, this is a terrible idea. I would go so far as to say that autogenerated code is antithetical to the entire Forth philosophy. yacc is completely unnecessary, since you're not generating a parse tree I'm not even sure what you would do with it. A single production that just executes the word? But then how does `WORD` work to suck up the next token in the parse stream? I guess if you really wanted to, you could bend over backwards to use lex as the tokenizer, but because numbers can be `:` defined in the dictionary, you would basically have to have only one lexical token anyway. Just write `WORD`, it's bound to be easier than wrangling even the makefile/link. If you need help send me a note and maybe we can pair on it. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programminglanguages] [okami 0.1](https://www.reddit.com/r/ProgrammingLanguages/comments/8tzxav/okami_01/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
named after the video game? as someone who hasn't used forth very much, I appreciate how easy your [] system is to understand.
Named after the japanese word for "wolf". (Hint: look at my first name / username). But I'm aware of the beautiful game with the same name. And thank you for your positive feedback.
I _could_ have spent 10 seconds googling, but I decided to ask a dumb question instead. thanks for humoring me. I see you implemented it in assembly. Did you base your implementation off of anything? JonesForth?
The lack of spacing between the words and the brackets bugs me as a Forther. I hope that isn't an intentional feature of the parser.
It's part of the parser...
Similar, though I made ( designate interpreting instead: [https://github.com/darius/tusl/blob/master/eg/babble.ts](https://github.com/darius/tusl/blob/master/eg/babble.ts)
amazing work, i'm quite glad of someone now noticed web domain :D
Think of it as a special kind of whitespace (so special it isn't even completely white, haha). It's convenient to have it this way. But if `" this"` additional space seems right to you in Forth, I guess I have no chance to convince you. :)
When I wasn't sure about how to implement some feature, I looked at other implementations, including JonesForth. I think this happened two times. But I didn't want to inherit the limitations of JonesForth, like not having `does`, so I didn't want to copy it.
It's not what seems right or wrong; if the parser looks for the left and right bracket separately, the how would it distinguish between `[` and `[[` if I were to define double-bracket?
Have you had a look at the Postscript programming language? The way you use square brackets here reminds me of how Postscript uses curly braces. 
It looks syntactically very similar indeed, but I don't think there is a semantic relation.
You cannot define `[[`... just like you cannot define a word with whitespace in it. The brackets are never a part of any word. You are limited to `{}` and `()` if you need parentheses in word names.
http://web.archive.org/web/20171201212822/http://dloh.org/
I should have mentioned that most of the articles are missing on archive.org.
http://web.archive.org/web/20171201212822/http://dloh.org/ - none of the links work, but his e-mail is there. Also you could try contacting here: https://github.com/cthulhuology or just search for "Dave Goehrig" or "Dave J Goehrig".
https://twitter.com/lowfatcomputing/status/1013791717630173184 We'll see what he says.
I know, sorry... I realised after posting that.
WORD
Wow. That seems quite reasonable. You pay some money for a tool one time and have zero annual fees or usage fees or deployment fees. If I did more with Forth I would be tempted to try it out as I don't at all mind paying for good software that doesn't twist my arm.
Thank you. To be perfectly clear about that, for the purchase price you get one year of updates; after that, if you want updates, you pay an 'update renewal' which is a fraction of the cost of a new version. But you don't have to do that if you don't want to.
I don't see a problem with counting up the references for a specific word (and branches thereof), assuming that running the word executes your application. Just count out how many times each primitive and composite is reached during execution (modify NEXT for the primitives, DOCOL for the composites).
I think you just restated the halting problem, which is what u/reepca was referring to.
Okami HD: Hype! \^\_\^
It can also translate as "great god," that said, it can be really hard trying to verify japanese translations, so I may be wrong.
Warning: pessimism incoming. It's target compilation... in the general case, you can't execute it on the host machine (and compiling and executing on the host machine to try to figure out which words are used rules out the possibility of choosing which words are used based on the target). You also have to run every single branch (I assume that's what you meant by "and branches thereof") in order to see everything that could be used. That seems nontrivial - a naive implementation would scale exponentially with the number of branches, and don't get me started on what happens when computed jumps and calls through tables get used - in short, you need to produce every possible input for the program. Even supposing you make that all work, now you have an answer to "does this xt get used" for every xt. You somehow need to translate that to a chunk of memory or similar structure that contains only those xts - or rather, the code represented by those xts, because the addresses will need to be different. Even copying colon definitions is tricky - much as we like to pretend that it's just a nice "list of xts", there's actually inline data strewn all over the place - string and number literals, for example. In the general case, you can't know whether a number literal is an address that needs to be replaced or not. It's hard to even tell where a colon definition ends, since `EXIT` can be manually placed anywhere. Additional bookkeeping would be needed to know where the "semicolon" actually is. And then there's `CREATE`d words... How would you propose this be made to work?
You could try a minimal implementation for the target machine and compile based on dependencies, but that would also hinge on words that use literals to be re-coded towards such a consideration. Whatever you end up doing, you'll most likely need to build a dependency tree. Or with the minimal implementation on the target machine, simply compile code on the target itself. People usually do exactly this because it is the path of least resistance.
Forth coding is all about customization to fit your needs, and cutting corners to a ludicrous degree. Or so I read somewhere.
&gt;There is no way to manipulate or build up strings other than printing them. It may be possible to build some string functions: `$ dc -e '16i 0A21444C524F57204F4C4C4548 [100 ~ dP0&lt;F]dsFx'` `HELLO WORLD!` Not having to think in reversed hex notation of the string would be nicer but chiseling the print string example would have taken much longer...
Ugh, I don't know why the SO overlords are so haughty. They'll let plenty of questions slide, which are *far* worse than this, only because they *like* those questions better. Ask whatever you want about the newest fad, even if it *is* opinion-based, but don't you dare ask about what you're interested in! It's just too darn painful for us to read about your interests which don't align with ours.
\`resolve-all-forward-references\`
&gt;Nice, I use \`converge\` for that purpose.
Seems like the long names in okami are all network related: ``` reset-(/sockaddr_in6) config:remote-access setsockopt(syscall) parse-request-line content-length-hdr ``` 
It's difficult to avoid long names in network programming.
Some statistics from Retro on iOS: Average name length: 8 Average name without namespace: 5 Longest names are 23 characters The longest words are part of the interface configuration: 23 config:restore-defaults 23 config:set-toolbar-size 23 config:set-toolbar-font 22 config:set-output-size 22 config:set-editor-size 22 config:set-output-font 22 config:set-editor-font There are also a few long names in other vocabularies: 22 file:open&lt;for-writing&gt; 22 file:open&lt;for-reading&gt; 21 file:open&lt;for-append&gt; 20 n:strictly-positive? 20 s:tokenize-on-string 20 s:tokenize-on-string 20 set:contains-string? 
 long time no see a small and quite community, aren't we delete-on-same-piece get-word-and-advance last-char-and-length next-dictionary-word print-all-test-words advance-till-new-line global-char-separator log-global-dictionary scnd-dictionary-stack temp-dictionary-stack write-chain-to-handle decode-too-big-for-pad encode-too-big-for-pad load-symbols-from-file find-word-in-dictionary how-much-left-after-fit run-till-current-return find-word-in-dictionary compare-stacks-and-clean end-of-global-dictionary create-word-in-dictionary regulate-dictionary-stack complie-call-to-test-words delete-on-different-pieces store-link-to-previous-word search-address-in-dictionary stash-stack-till-saved-depth find-word-in-dictionary-stack update-dictionary-with-new-word get-color-according-to-highlight complie-reverse-call-to-test-words 
&gt; I make things type void***** to make the compiler more compliant with my demands. This confuses me. Can you explain what you're talking about? It was the first thing that stood out to me in the code.
Wow, full-blown sentences as names.
Well there is a simple reason for this. First of all the stack I use can hold any type and so is untyped, so naturally it needs to be able to hold a type as big as the biggest type I am required to have by the function of the c language. It turns out this seems to be void*. Int* or char* does not work apparently because on some systems a int* or a char* is a smaller number than a char*. And so some people including myself often tout void*. But I actually use void***** and here's why. Lets say I am calling one of my named variables of type void*. Then I try to read from index 0 of it. Then I have to type ((void**) my_data)[0]; Well what if I want the 0th index of the 0th index. Then I need to have ((void**) ((void**) my_data)[0])[0]; Lets just to another to show how much boilerplate it is. ((void**) (((void**) ((void**) my_data)[0])[0]))[0]; Look carefully at the length of the above code. The following is what I would have to write if my my_data was of type void*****. my_data[0][0][0]; Much shorter and easier to type and easier to read. Also a void***** is the exact same size in memory as a void* and the casting costs nothing.
&gt; it needs to be able to hold a type as big as the biggest type I am required to have by the function of the c language. It turns out this seems to be void*. A `double` can be larger than a `void*`. The safest way would probably be to use a `union` &gt; Int* or char* does not work apparently because on some systems a int* or a char* is a smaller number than a char A `char` is always one byte (although a byte can be larger than an octet). So there is no way a `int*` or `char*` could be smaller. &gt; Much shorter and easier to type and easier to read. True, although one could also use macros for that kind of access. And did you consider using a typedef for your `void*****`?
My language does not allow for doubles however. If you want to do a double you have to do it with ints or chars. What I said was any that I would be required to have by c and by the design of my language c doesn't require me to have the double type. In the future if I do require the double type I'll make my stack of size double but it is best as others in this forum have said not to optimize early. "A char is always one byte". True. A char is always one byte. A char* is not always the same number of bytes as a char. If you don't understand that you don't understand what a pointer is. Maybe I am wrong but I doubt it. I could use macros but meh I don't care about c to much. I don't learn a language or program feature until the exact moment it becomes useful to me.
&gt; I don't learn a language or program feature until the exact moment it becomes useful to me. That is a profoundly foolish statement. I cannot imagine in what situation someone would say "Jee, I sure need a wrench!" without knowledge of what a wrench was or how it could be used.
You're wrong. If I never knew what a wrench was I would go to take something apart or put something together I would realize I couldn't get it. So I would find somebody who knew how to do it and they would say, "Oh you need a wrench." And I would say, "A what?" And they would explain what a wrench was and then I would know. Then I could use a wrench whenever I wanted.
Cool. Just did a similar thing on 8th: null words-like ( s:len nip swap s:len nip n:- ) a:sort That leaves a sorted array of the words; then a:shift s:len . space . cr Repeated a few times gives: 22 g:edit-on-double-click 19 g:root-item-visible 19 g:show-line-numbers 18 g:outlinethicknes 18 cr:chacha20box-sig So the GUI words are the longest, followed by some crypto words. I'm happy to see you using a similar namespace idea to 8th's :)
&gt; I want to know if I can "safely" use them despite their age No, FORTH isn't safe. In programming terms, FORTH is about doing things in a fundamentally different way to the modern consensus. If adhering to said consensus, and acceptance from others who do, is your main priority, then I would actually encourage you not to have anything to do with it.
I don't think he meant anything like memory safety, or safe according to current programming culture paradigms.
I should probably try to scare you off. Forth isn't a language that you can just crack open and "get" right away like C or derivatives of it. This is something that will fundamentally change you. But I suppose if you're doing Project Euler, maybe Forth will sate a thirst you didn't even know you had. So here's my suggestion: grab up all the Forth material that you can and read it twice. Despite the age of a lot of the documentation, it is all still 100% valid. Learn all you can, and then go build your own Forth implementation to learn about it more. If languages like C give you enough rope to hang yourself, Forth is the sharpest knife you can imagine. There's a reason we gathered here in this subreddit. It's not just a language, it's a concept, maybe even a philosophy.
&gt; I want to know if I can "safely" use them despite their age Older forth code and articles tend to be just as useful and interesting as newer forth code and articles. Forth is not really oriented around software ecosystems as much as say, Python is. Forthers mostly exchange ideas to implement themselves, rather than actual monolithic source code libraries so there isn't any api with a large surface area that you need to keep up with. A forth implementation can be viable without a large community supporting it. Forth is personal computing for everything through the hardware and up rather than just the superficial GUI level. I'd say a user-interface is characterized by transient, one-off interactions. A program is more for long-term, off-line, one-shot recipes. Forth works best as a flexible, extensible user-interface.
I'm interested in trying this, but it won't even compile. clang reports "29 warnings and 17 errors generated" when building. After adding in a bunch of missing semicolons there's still numerous errors along the lines of: dg.c:117:12: error: invalid operands to binary expression ('void ****' and 'void ****') *r0=*r0&lt;&lt;*s; ~~~^ ~~ I'm giving up for now as I have no interest in messing with broken, undocumented code. If you get this to compile and work I'll be willing to take another look.
Which Forth implementation are you planning learn with? I don't know about other implementations, but I know GForth has a thorough tutorial/manual that you could use in conjunction with those books to check that the specific details they mention directly apply. I would guess whatever other implementation you use would have that sort of documentation too. I'd recommend trying out the code in those books, and if they don't work, check the implementation manual and see how that implementation approaches the particular programming issue differently. 
Right this is just my rought draft. There is another version in that github repo which should compile if I left the working copy alone but I didn't even debug that copy all the way cause that too was just a rough draft. If you just pick the program gnu dc you'll sort of figure out how dc works and then how this works. This is an op code interpreter that has a maximum of 256 ops so that each super useful op can be bound to a character which phonetically makes sense in english. This is very similar to the design of dc.
so true... http://gen.lib.rus.ec/search.php?req=Threaded+Interpretive+Languages ...wu wei 
The [Gforth Manual](https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/) has lots of info.
Yea, I downloaded gforth manual. Thanks the hint!
I don't think there's a standard way to do this, since the ANS Forth standard doesn't specify *how* a Forth must implement word creation. That is, if you look at other Forths you will find other ways of accomplishing the same thing, though they will usually be similar (and similarly named, perhaps; but the factors are not standardized AFAIK).
I agree. We don't even have a standard dictionary format, or even a standard structure apart from agreeing that a "simple" one is a linked list.
True, though there's no reason to standardize the *format* of the dictionary. It might be useful to standardize some of the factors so that what the OP is interested in could be done in a portable way.
I think an effort to "standardize" anything having to do with Forth is a futile one. OP would be best off learning how gforth does it if he wants to stick with it, or make something up and move on.
Can't argue with that (being the author of a non-"standard" Forth...)
Every time I dig into how gforth does something like that, it turns out to be more complicated than I want it to be. You can `see` your way around how everything works, and so then you'll work out how you want your code to do it. And then you'll try it in `gforth-fast` or `gforth-itc`, or upgrade to the current git repo, and it'll all bork. Since it's not specified in the standard, you're on your own to go monkeying around in the guts of the machine.
&gt; Learn all you can, and then go build your own Forth implementation to learn about it more. Note: this should not be your first project - or probably even your sixth. For one thing, nobody new to a language knows it well enough to implement it - it even took Chuck Moore ten years, and he invented Forth. For another, writing some other stuff in Forth will get you to the point where you know it well enough to decide whether or not it's worth putting in the time to make one that's *yours* - because there's really no other reason to write a Forth¹; and by the time you're fluent in it, it'll be a much easier job. Also, Forth is about the easiest language to implement (especially if you lop off awkward corners like DOES&gt;); unfortunately, that implementation tends to be the most complicated job most people who write their own Forth ever undertake with the language. __ 1. This is a bit of a fib. There's one very good reason to implement a Forth - you find yourself with a desperate need for a fast interpreter, say on a microcontroller, and nothing else will fit.
I had decided to not do a vocabulary/search order setup when I started on R12. The namespace idea from 8th looked like a good approach so I adopted it and have been very happy with it.
Yeah I'm actually quite disappointed with how complex gforth is. Implementation simplicity is one of the reasons I like forth but that seems totally lost when I'm using gforth. What you are saying makes sense, but - I don't feel like I'm trying to fiddle with the guts of the machine. My impressions from Starting Forth and Jonesforth is that creating your own words and 'compiling' addresses into them is kind of a normal forth thing to do. The existence of words like `create`, `compile,`, and `postpone` are all geared towards that. Why would a forth have `compile,` and `[comp']` but no stable api for creating a new word header? It doesn't make sense to me why gforth would only have `create` work for variables with no stable way to create a code word or change the header to one. 
That is quite reasonable and I wish more commercial vendors had that mindset. Annual fees make me uncomfortable as I'm only renting a toolset. Mathematica is a great engineers toolbox, but if you have to leave your company, paying thousands out of pocket can be a no-go if your new employer doesn't wish to do so. I'd be more willing to invest in it knowing I had a personal seat for as long as it runs. Again, I wish you and your company the very best of luck.
I don't know about others, but I learned loads by attempting an implementation for myself. Of course, I was already an experienced programmer by then, so writing assembly code for the primitives and various system calls was the fastest part.
Maybe I'm misunderstanding what you're trying to do. If you're just trying to make a colon definition, that's what the colon compiler is for (`:`). Maybe you could explain what your objective is? Are you trying to duplicate `:` for education reasons, or is there some goal that standard compilation doesn't get (even with `[` and `]` breakouts to interpreter mode in a `:` def)?
If he is trying to replicate a colon, he should probably use POSTPONE, but he did mention that he wanted to use strings, which implies that he probably doesn't want to create new definitions from the prompt, but rather in an automated way. That seems to be the major hangup here. @johnboudewijn Best suggestion is to figure out how the dictionary is structured, or just SEE how colon works, and define a new word that does all the work of colon except with the changes that are relevant to your application. I myself don't have access to gforth at the moment, but at least some of the constituent words should be self-explanatory.
It would be really nice to see more people not stop after writing a basic Forth system. But I wonder: Do you advocate using a Forth system that is essentially a black box to the programmer? If not, could you clarify on what you would recommend a Forth beginner to do?
Yes, because obviously it's a binary choice between "one I made myself" and "a black box". \**sigh*\*
I was at that point. Tried to scratch my head around implementing jonesforth, without knowing Forth itself good enough. My dream is a minimal, bootable x86 Forth. But I really should learn programming clean Forth programs first. 
&gt; Yes, because obviously it's a binary choice between "one I made myself" and "a black box". *sigh* I didn't say or imply that - in fact, that's exactly why I asked what you would recommend! I want to learn something new. One thing I can imagine is that one could study an existing implementation. But since I don't think this is an easy way to get started, I wanted to know about your preferred approach. Unfortunately, you didn't give an answer yet. You emphasized the word "use". Well, but what does that even mean in this context? How would a person new to Forth "use" the language without treating the implementation as a black box or writing their own? All I know is that I actually tried using an existing implementation and I hated it. If I hadn't written my own Forth, I'd definitely given up on Forth entirely.
&gt; I didn't say or imply that You did, you know. And for good measure, you did it again: &gt; How would a person new to Forth "use" the language without treating the implementation as a black box or writing their own? See? There's that implicit binary choice again. Where does that leave Bernd Paysan, author of bigforth and MINOS and co-author of gforth, who claims [never to have implemented a Forth from scratch](https://bernd-paysan.de/why-forth.html)? &gt; Unfortunately, you didn't give an answer yet. I did, you know. It just wasn't what you wanted to hear, so you decided to be dismissive instead: &gt; You emphasized the word "use". Well, but what does that even mean in this context? Well, how does one usually use a programming language? It really wasn't the starting point for a philosophical discussion. And I have to repeat this: **I am shocked that anything I said is controversial**. In fact, you kind of prove my point: &gt; All I know is that I actually tried using existing implementations and I hated it. If I hadn't written my own Forth, I'd definitely given up on Forth entirely. There are only two explanations for this. Either you give up at stuff really quickly - which seems unlikely, because then you wouldn't have had the patience to implement your own Forth and stick with it - or *you did as I suggested*: &gt; writing some other stuff in Forth will get you to the point where you know it well enough to decide whether or not it's worth putting in the time to make one that's yours So honestly, what the hell are you trying to argue with me about?!
Thanks; I'm also happy with it...
Thank you. Best regards, and I hope you decide to be an 8th customer!
let me try to clarify, I want to create new words pragmatically, from inside other words. for example: ``` : create-adder ( uau - ) nextname header reveal docol: cfa, postpone literal [comp'] + drop compile, postpone exit ; ``` which when called with `4 s" add4" create-adder` will create a new word `add4` in the dictionary
yes, this is right. I've tried modifying the definition of `:` but was unable to get it working, and stopped trying after I got it working using words from the definition of `create` instead. It just seems like something that a forth implementation would provide a word for, even if it's not part of a larger standard
How does it work? Does the interpreter/ compiler resolve namespace names to pointers to separate dictionaries?
Yeah, it is weird. GForth sometimes feels like a kludge. I'm glad you were able to figure it out, however. What did you end up with?
In my Forth it's just a string prefix at the start of the name. I only have a single dictionary. In the prior generation of my Forth, I had support for multiple dictionaries and search order. In actual use I ended up using a prefix handler to access words in the dictionaries directly most of the time. This looked like: (R11) ^file'open In this, ^ was the prefix to open a vocabulary (file') and access a word (open) in it. Adopting the use of a short namespace string lets me do much the same. It's more verbose if using multiple words as opposed to adding a dictionary to the search order: (R12) '/dev/stdin file:R file:open (R11) with file' "/dev/stdin" R open But it avoids name conflicts and makes it completely clear which words are being used.
Anton Ertl posts daily on comp.lang.forth, you might get his recommendation by reposting the question there. I wish I could help but I'm neither a gforth user nor an adept of the ANS and post-ANS standards. 
Shouldn't the title be Atari Forth Do?
Fantastic! I’ve been wondering what kind of performance one could get from a Forth implemented in web assembly. Not bad. 
Very 1980s.
awesome. bit different from RPL but more or less understandable
I used an array for years but they come with limits and I've come around to loving the linked list, in this small area, because of the wonderful effect that they have on the development experience. It's nice not having to think in advance about the maximum number of words but it's even nicer when you don't run into a hard wall during development. Using arrays can make the implementation significantly simpler, and faster, but unless you double (or more) your estimates then something is going to fill up sooner or later. Tradeoffs :-). An earlier version of our Forth-like language have zero lookup cost, and a mouse driven "text" editor. I don't know how I'd make Forth any simpler than that. At the same time it turns out that if you go drift too far from the norm then you're going to be travelling alone. We ended up ditching the editor to make people happy and that eventually lead to an unravelling of the whole system. When you remove a foundational design decision like that and start pulling you'll quickly (or not so quickly) find that you're building on sand and none of the other tradeoffs you made make much sense any more. So we're slowly edging our way back towards a more normal Forth. I'm not sure how I feel about that, but the language is far less useful than the platform it's built on so it makes sense from a business/commercial standpoint. I'm hopeful about reimplementing those lost ideas at some point, but for now, on with the show ;-).
You see that big G in GForth. It stands for GNU (less commonly expanded: Giant, Nasty, and Unavoidable)
That's why I built my own Forth! With blackjack, and hookers!
You can always contruct auxiliary structures after the fact. The original structure is always a linked list, but there's nothing stopping you from implementing a binary tree or a hash table on top of it.
Bingo :-) but now you have "two things", and if we're still aiming for simplicity we have to choose one. If you have a decent metacompiler there may be another option -- it wont save you much in the way of complexity but it might save you some space -- make it a configuration option and compile a different Forth for development and deployment. If you really know what you need in advance or can afford to allocate large swaths of memory for holding headers then arrays win hands down. If you just don't want to think about it, linked lists take the cake. When it's your won system and your own problem you can do what you want to do. In practice people place great value convenience, and that implies normal. Normally, people don't want to think about it. Who would have thunk it, looking at the state of software today?
This is a very early book from 1970 and it describes an earlier ancestor to what we now know as Forth. Most of Forth is here, but there are a few differences. [The free pdf is here.](http://www.forth.org/POL.pdf)
I think it would be simple to update "quit" to search the dictionary with the new structure and, occasionally, update it. Let's remember that we can "cover up" older words with functionality updates. Although now that I mention it, retroactively updating older definitions with the new words could be interesting in and of itself.
Differences such as?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] ["Programming A Problem Oriented Language: Forth - how the internals work" (by Chuck Moore) is now available in print \[and in pdf\] • r\/Forth](https://www.reddit.com/r/programming/comments/93onxi/programming_a_problem_oriented_language_forth_how/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
My pleasure :)
What is the link to that thread you mentioned?
Here you go: https://github.com/ForthHub/discussion/issues/71
&gt; Developers essentially extend it by writing the "words" which each do more complex things, each of which then become inherent language keywords. Well, yes. Isn't that the point? You don't solve your problem in forth, you use forth to write a language in which solving your problem is trivial.
Seemingly this was some sort of joke article, as stated in the contents. Or written by someone who didn't know Forth. The jury is still out.
Reading this article was a waste of time.
This is what constitutes a "hate" for Forth? A smattering of weak arguments and whining? LOL
Writing comedy is not easy. IMHO this was an attempt to do just that. It highlighted all the features of Forth in the form of complaints. Not completely successful comedy.
I was doing some physics simulations and non-linear optimization and the most straightforward thing to do was use python libraries in my limited time. However, I was soon frustrated by not being able to arbitrarily slice up functions into factors. Now I can do EM simulations and visualization in a Jupyter notebook from forth thanks to H.C. Chen's peforth.
&gt; Sometimes it is conventient when you don't want to work through a stack operation just **to** r1 ! and get it back later with r1 @. I think you meant to write **do** instead of **to** here? But this problem is already solved in a better way by having a second stack.
I dunno I have never seen the appeal of the second stack thing. I want want like three or four second stacks.
I wondered why I had you tagged Dunning–Kruger and then looked at your post history. You make a lot of incorrect statements about low level architecture and then state you're not a low level programmer as an excuse when corrected. You also supposedly left Forth and yet here you are still displaying Dunning-Kruger.
I am not a low level programmer. I am not interested in low level details, only the way that the syntax works out in my code. if someone can point out a more efficient way to do what I am doing with just as easy of a syntax I am all ears.
At the last SV-FIG meeting Sam Falvo showed us the words `D!` `D@` `&gt;D` and `D&gt;`. These are analogous to the words `R!` `R@` `&gt;R` and `R&gt;` but they are for the parameter stack rather than the return stack. The `D!` `D@` `&gt;D` and `D&gt;` data stack words look a bit noisy at times but I think they are much clearer than implicit stack jugglers, and high level words still compose/juxtapose implicitly with the data stack words buried in their lower level words. https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/examples/hello.bs https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L53 
I don't really see how it is better. As far as I know these words you suggest still do a pointer store and a pointer read. Perhaps I am wrong. Like I said if I am going to add stacks I am going to want a syntax where I can keep adding variable numbers of stacks.
Variables aren't quite the same as registers... Ignoring that, using global variables will require some work to ensure that state is preserved/restored. It may be convienent to just throw a value into `r1`, but you will need to make sure that nothing else you call will alter this in ways you aren't expecting. If you just need to move a value out of the way for a bit then using `&gt;R`, `R&gt;`, and `R@` are a better solution. Or define things to help massage the stack into the state you want/need.
I have a small word set for locals that I've stored on the return stack. It kinda sits between Forth and C. It is hard to avoid in some cases, as per the Pigeonhole Principle, no data structure is a silver bullet. 
Would this still work assuming Forth was on "bare metal"? Or should I assume this is for a hosted Forth with Python installed on it?
Yeah you have to finish your transactions with the registers by the end of your word.
"Oh he doesn't use the word the same way I like to use the word. He should only speak the way I speak because I was told by somebody to speak that way. Only my job is important and none of the other ways that people program works except mine."
This isn't Forth elitism. It's obvious you've never set foot in a software engineering class. You state things everyone knows, and misuse standard concepts. Also, what are you doing attempting low-level programming if you don't know anything about the low-level machine? No sane person would attempt assembly programming without thinking about a target CPU.
&gt; "Oh he doesn't use the word the same way I like to use the word. He should only speak the way I speak because I was told by somebody to speak that way. Only my job is important and none of the other ways that people program works except mine." That you don't recognize how batshit crazy that statement is is worrying. How do you expect to communicate effectively if you reject the definition and usage of standardized language for concepts? Especially on topics like technology and programming.
peforth is a python forth that makes it possible to mix python and forth. You could probably use peforth with micropython for bare metal embedded use. I personally prefer forth for small systems. But micropython code could temporarily fill needs that I don't have time to fill myself in the short term or aren't worth my time, such as a USB-C driver. A USB driver is written once and forgotten. There's not much that's thoughtful or designed in USB. I can't think of anything I like about it. It'd be nice if we could tell USB-IF: "How about I give you the finger, and you give me my serial port" but looking at the side of my laptop I don't think they would be impressed. Robert Sexton is doing something similar with C and MPE Forth at Apple: https://github.com/rbsexton/sockpuppet 
I don't see forth as a low level language. Forth is the result of my search for ideal programming langauges suited for high level programming with syntactic macros.
Now you're just making assumptions. I've done high level programming, and I've done all the way down to HDL. I've never encountered anyone who used those terms differently. In academia or the professional world. If there's ambiguity someone will preface stack with 'hardware' or 'software' as stacks can also be software constructs in memory. They are dramatically slower than their hardware brethren though. No one with a computer science background will misunderstand me when I use the terms: 'push it to the stack', or 'write it to a register'. Those without the background that have just learned something like Python will most often possibly heard the terms but never used or thought about either. They won't use them differently, they won't use them at all.
No you are just rigid and ignorant for assuming that when someone says stack they mean a hardware stack automatically. I don't give a damn what terms you like and don't like. Use your brain to parse my words and understand them without getting triggered. You don't need a safe space with special words just cause you have a college degree.
micropython's speed on SPI is really a problem i think. but i am glad to hear that it could be support :D
It's more involved than that. Consider your example `f(x) = x + 1`. Using an `r1` variable for `x`: : f r1 ! r1 @ 1 + ; `f` is finished with its transactions with `r1` and still has an issue. In this case, any word using `r1` that also calls `f` would need to save/restore it. Alternatively, `f` could do this: : f r1 @ &gt;r r1 ! r1 @ + r&gt; r1 ! ; But this gets messy and unreadable very quickly. You could add abstractions to aid in readability, but it's neither efficient or elegant in most cases. This will also have detrimental side effects in terms of how it alters the ability to factor code out. Local variables can avoid some of this, but they have their own tradeoffs.
True. But I doubt read_harder is going to be dealing with actual registers anytime soon.
I'm not the one getting triggered by this exchange. It's not that I like or dislike terms. You have clearly stated that you reject the field's standard definitions. I have simply pointed out that without the use of standardized language when interacting with other programmers how exactly do you expect them not to misunderstand if you're not using terms the same way? You keep latching onto details and attempting to use them strengthen your position in your own eyes. Now you've resorted to insults. Your position is untenable and you are just doggedly refusing to accept reality.
Where could I read more about these words? They don't make much sense to me - `&gt;D`, for example, I would imagine "pushes to the parameter stack". But... from where? `&gt;R` takes an item from the parameter stack and pushes it to the return stack. Does that mean that, if we just subsitute "parameter" wherever we see "return" and "return" wherever we see "parameter", `&gt;D` should be equivalent to `R&gt;`? Also, I'd like to just note that to the open-minded programmer, there are no non-transient resources in a computer.
On the topic of USB, I completely agree. "Designed by committee" to an extreme detriment.
Then not only are you arrogant, you are hopelessly wrong. Forth is one of the most low-level languages I have ever encountered: it has no syntax, no types, no garbage collection, no objects, no events, no prototyping, no macros, no templates. Nothing except bytes and a few stacks. In fact, if you don't know what most of those missing features even are, your knowledge is inadequate, not just of Forth but programming in general; those are all basics. Forth assumes nothing. Any "high level" aspects Forth could have need to be programmed in, or are otherwise completely absent. Porting it to another system involves coding at the assembly level, requiring a serious understanding of the target CPU and whatever host system it might have, if any. After all, Forth can be bare metal. No high-level language can do that in any serious capacity. You are groping around in the dark. I beg you, take some classes, read some books.
I agree that your first code example is my current model sort of abbreviated to show what boilerplate it adds. On the other hand I don't see your second example as a better way of writing it. I would say you need to try to rewrite your second example without the r1 and express the way that you prefer to do it so I can decide if it's better.
I am going to speak how I want. I do not care what you think about it.
I think you are foolish for not seeing the high level potential of forth after working with it for so many years when I found it within the first few days of working with it.
That's easy: `f(x) = x + 1` can be written to just use the stack: `: f ( x -- y ) 1 + ;` (including a stack comment here, which I neglected to do in my prior post). No need for a variable: it's shorter and avoids a lot of overhead associated with variables. Or if it was something I was using frequently, I might drop to assembly to get more performance or better code density (retro example, using nga assembly): :f (x-y) as{ 'liad.... i #1 d }as ; The second example was intended to demonstrate saving/restoring `r1` in the event that you used `f` in a word that also uses the same `r1` variable. This example is really not ideal though: it's too short and incomplete on its own. It'd be better to compare a larger set of words using the variables to a stack based solution.
That looks like pseudocode to me. It also seems to be infix and to be very different from your original example.
None of the Forth code bits I've posted in this thread are pseudocode. The initial examples are traditional Forth code and will work in gforth. The one using assembly is for RetroForth and does work as expected (tested on retro12, 2018.08 release).
Ok, so for a better example than your `f(x) = x + 1`, consider a word which fills a memory region with a specific value. Here's an approach using some variables: 'r1 var 'r2 var 'r3 var :fill (can-) !r1 !r2 !r3 @r1 [ @r3 @r2 store &amp;r2 v:inc ] times ; And an example using it: $e here #100 fill This works. But any word using `r1`, `r2`, and/or `r3` *and* `fill` will need to preserve and restore these. Or add boilerplate to `fill` to do this every time: :fill (can-) &amp;r1 [ &amp;r2 [ &amp;r3 [ !r1 !r2 !r3 @r1 [ @r3 @r2 store &amp;r2 v:inc ] times ] v:preserve ] v:preserve ] v:preserve ; This avoids the issue with tracking all the words using these shared variables, but it's messy and the logic of the word gets buried. A better way is to just use the stack: :fill (can-) [ 'ab 'aab reorder store-next ] times drop-pair ; Or, with the return stack and a simple `dup`: :fill (can-) [ push dup pop store-next ] times drop-pair ; The return stack approach is faster (avoiding all unneeded memory read/writes) and shorter. The form using `reorder` is less efficient, but makes the stack change easily seen. Both are superior to using variables IMO.
I never said Forth can't be high-level, I stated that `Any "high level" aspects Forth could have need to be programmed in` which means it is possible, but needs work to do. But alright then. Prove me wrong. Show me a Forth with Objects that is as comprehensive as, say, Visual Basic.
I can't really read those examples unfortunately. Maybe one day I will be able to read them and I'll come back and see what they do.
Well implicit parameter passing is usually implemented with push and pop asm code at the front and end of each word and literals are pushed with `LIT` which is not typically visible in high level forth. Sam Falvo just exposed parameter stack push, pop, and lit in high level forth so you can embed them in the middle of a word instead of just having them hidden at the front and end of each word. [`&gt;D`](https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L53) looks a store-increment for riscv. [`D&gt;`](https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L54) looks like a load-increment for riscv. [`D@`](https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L55) looks like a load (from parameter stack) instruction for riscv. [`D!`](https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L56) looks like a store (to parameter stack) instruction for riscv. `D#` is like LIT but it's used in high level code. He's got some examples here: https://github.com/KestrelComputer/kestrel/tree/master/software/src/bspl
^ recipe for getting shunned by every community you try to interact with
Intrigued. Tell me more: How do they work? Why do you prefer this way over (just using the return stack|ANS locals|a fixed number of locals per word|turning the dictionary into a scary saguaro stack|whatever)? Published anywhere?
Well, this started off well... I think it comes back to this /u/readharder: your attitude is toxic, you lack maturity and humility, and it prevents meaningful discussion. If that were all these posts of yours would only be an annoyance, but your attitude is preventing you from making progress, which is more a tragedy. You've been stuck on these trivialities for a year or more by my count. Maybe you should just move on already? btw, you aren't compelled to use the same terms as everyone else, although it helps to have an understanding of the accepted definitions are, but if you're not going to use your own terms then you need to define them. If you're not going to use accepted definitions and you wont define your terms I would be more than happy to show you to the door. Permanently. You've had more than enough chances here.
While I know you don't like to read documentation, all of these words are documented and can be found in the Retro [dictionary browser](http://forthworks.com:9999). Here are the current URL's for the code in the above: http://forthworks.com:9999/10 ; http://forthworks.com:9999/62 [ http://forthworks.com:9999/63 ] http://forthworks.com:9999/150 drop-pair http://forthworks.com:9999/152 dup http://forthworks.com:9999/281 reorder http://forthworks.com:9999/267 pop http://forthworks.com:9999/268 prefix:! http://forthworks.com:9999/271 prefix:&amp; http://forthworks.com:9999/273 prefix:( http://forthworks.com:9999/275 prefix:: http://forthworks.com:9999/276 prefix:@ http://forthworks.com:9999/278 push http://forthworks.com:9999/348 store http://forthworks.com:9999/349 store-next http://forthworks.com:9999/354 times http://forthworks.com:9999/382 v:inc http://forthworks.com:9999/387 v:preserve http://forthworks.com:9999/389 var The URL's are valid as of the date of this posting, but may change in the future as words are added, renamed, or removed. (This is served live from the glossary database file in my working copy of the main source so always reflects the system I'm using) There's also an [overview](http://forthworks.com/retro/r12-design.txt) document and a [short file](http://forthworks.com/retro/papers/3.txt) on the prefixes that may be helpful.
I really don't like reading terse guides.
Let's say you have a word that [interpolates polynomials](https://en.wikipedia.org/wiki/Polynomial_interpolation) and the points of the data set are passed on the stack, as they are transient values. You can see how quickly the data begins to pile up, can't you? There would be no reasonable way to shuffle the stack during this process even with only two points in a Cartesian Plane. There's no way a "fixed amount of locals" would be suitable for this, since the number of points are highly variable. And as the values are transient, this would just waste memory on the dictionary without garbage collection, which I'm currently avoiding in this particular implementation. Therefore most data that gets rarely accessed but exists long-term ends up in the return stack. Since this "stack frame" is indexed by EDI it is fast. In fact it is a linked list embedded within the return stack, so child words can access the locals of their parent words if it is necessary to do so (I abuse this functionality to maintain "contexts" of a call chain), and this still retains the functionality of "scope" in the call chain. The return stack looks like this when locals are loaded, from lowest memory address -&gt; higher memory address: #-of-locals (local-0 is implicitly the count), local-1, local-2, local-3, ..., local-n, context pointer, previous value of EDI, return address, ... I should probably throw some of this stuff up on Github, at least the framework of it, but I think you get the idea. 
**Polynomial interpolation** In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Neat. Thanks for the writeup.
Could someone help me expand mine. I don't quite comprehend it.
The under cleavage rocks -- truly ahead of it's time ;-)
I'll try. A two-argument bitwise boolean operations, such as `and`for example, applies the same operation to all bits of two machine words. Even though most processors typically provide `and`, `or`, and `xor`, there are actually 16 (2^2^2) possible operators (one for each 4-bit truth table value). A very simple way to implement all 16 in one circuit is by using multiple 4-1 multiplexers in parallel, one for each bit of the machine's word width. Although on a register machine some of the 16 operations are somewhat pointless, `nip` for example with takes *x* and *t* and always gives back *t*, they are all potentially useful on a stack machine. One special case though is the column in the above table that I've labeled as `drop`. The operation as described in the table takes two arguments, *x* and *t*, and returns *x*. In Forth's stack comment notation this would be ( x1 x2 -- x1 ), but Forth's `drop` is normally described as ( x -- ), that is it requires a minimum of just one item to be present on the stack. So the `drop` from the table wouldn't work as a replacement for when the stack is down to just one item. However, if the Forth in question uses a circular stack, as do Chuck Moore's chips, the stack never really empties, and the stack effects ( x1 x2 -- x1 ) and ( x -- ) are equivalent. 
I hope that helps make it clearer, if not don't hesitate to ask.
I now have the FORDIC file. It seems to work. `1 2 + .` says 3.
Code validation is always important, even if it is just a simple hash of the program.
I found (this article,)[http://kestrelcomputer.github.io/kestrel/2015/09/15/bspl-compiler] for people who prefer to read.
This is a great idea. A question if I may? What's the difference between this and downloading the code and checking the e.g. SHA256 checksum, like we usual do. Perhaps I've missed the purpose of this :-). I can see the convenience of having the SHA256 embedded in the file but is there more to it? 
Yes, there's more to it! Using a PHP signature has two purposes: * First, it ensures that the person who claims to have signed it is actually the person who signed it * Second, it guarantees the integrity of the signed bits, because if any are changed, the checksum will be different, and it is essentially impossible to forge a PHP signature The reasons are: a lot of math. The details are a bit complex, but you can find lots of information on the web about how PGP (and GnuPG) works and how to get things set-up. A regular hash (SHA256, etc) is simply a hash of the contents (file, message, etc) itself, with no connection to the individual generating the hash. The difference is that if one posts a file and its hash (on a web-site, or an ftp site or whatever), and a Bad Person swaps out that file and posts a hash of the new file in their places, the User who downloads the file and dutifully checks against the posted hash, cannot know whether the Good Person actually posted the file or not. If GP posts a file and its PGP signature (or, as in my sample, embeds the signature in the file), then U can check using PGP and see that in fact, GP signed the file and therefore it's trustworthy. This assumes, of course, that BP didn't somehow steal GP's private PGP key and passphrase (which is possible, but an entirely different level of hackery).
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/crypto] [Ensuring a script is not tampered-with](https://www.reddit.com/r/crypto/comments/99jz6q/ensuring_a_script_is_not_tamperedwith/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The SHA256 hash you're referring to will only verify the integrity of the file you just downloaded, assuming the website where the SHA256 hash is displayed, is not compromised. The use case for these hashes is slightly different: it ensures that the file you just downloaded doesn't have weird bits flipped around or something broken during the download. This isn't uncommon in very large downloads and important to check for. It doesn't prove much beyond that.
I should have worded that better. I was more interested in the embedding of the signature inside the forth code :-) but thanks you both for your explaination/description of PGP and cryptrographic signatures.
Apologies, I didn't pick up on that. I must admit I'm more into crypto than into Forth. Not sure how I got here in the first place; must have been that pesky random button ;).
Sorry, it seems my question was very unclear. What I was trying to ask about the decision to embed the the cryptographic signature is in the forth code itself where it is presumably skipped or verified on each execution? Im more familiar with OpenBSDs signify(1) which uses a "SHA256.sig" file to contain the signed SHA256 hashes for each file :-). Hopefully that's clearer but thanks for the nice summary of PGP.
Just updated the original post with this and more information...
Maybe I'm missing something, but what prevents me from stripping the PGP header and footer, then modifying the script?
Nothing at all.
Something a little bit out of leftfield, you can use something like IPFS to ensure that the file you're getting hasn't been tampered with. A lot of banks are using it for this reason alone, since all they need to do is store a hash to the file, and the file will live on within the IPFS network, so it also offers a certain level of reliability as well.
Huh; that's interesting, I'd never heard of it. Thanks for the info.
That seems to be part of the idea at least. Or in simpler terms it allows Forth to directly put data into (some) registers. Personally I am not sure of the the advantage. A good compiler would optimize for which bits of data should be cached in registers anyway, right? I don't think it makes sense now a days to run Forth as an interpreter for production apps (unless that is good enough for what you are using it for).
 [...] if "File was modified!" throw then ; I notation polish reversed missed life professional my in have not definitely.
pretty hard!
Super simple. You could probably do it in an hour or so. 
Yeah. He can spend the rest of the morning writing a c compiler, then spend the afternoon on Haskell.
Even so it's pretty cool work. Wonder what Chuck would say?
Why do you think so?
it's difficult to make a forth compiler
this is so hilariously ignorant. please go try and come back and tell us how it went in an hour.
Which part of it is hard?
On the first one I found it difficult to understand how all the low level details went together to create the finished system. YMMV.
If you have written a dozen Forth system before and already know RISC-V assembly well, then it's certainly possible very quickly. Whether one hour is realistic I don't know, but I can imagine that the basic system could be done that fast.
I said that it may be possible that fast when you have all the necessary requirements. Telling a beginner that they can do it that fast is certainly not very helpful, I agree with you on that; I just wanted to provide context for the answer given by ohboyohboy1234. As for me, I have written a Forth system in assembly and there was absolutely nothing difficult about it. But I had done a little bit assembly programming before and I had read about implementation methods for Forth. It certainly took me far more than an hour, but then again I started with implementing buffered I/O and didn't aim for a minimal system. If Forth was hard to implement, there wouldn't be more toy Forth systems than toy Lisp interpreters out there. ;-)
On youtube: [https://www.youtube.com/watch?v=rlayTh3sjiw](https://www.youtube.com/watch?v=rlayTh3sjiw) From the Bitwise series of videos: "Bitwise is an educational project where we create the software/hardware stack for a computer from scratch." 
I used to have a similar word. I had named it `apply` (I use the name `^` for another purpose, ticking of compile-time xts). It was a dual-behavior implementation, compiler directive at compile time (more or less a macro), and normal execution behavior at interpret time. I never really got into the habit of using it, and there came a time when I felt I was generally overusing compiler directives throughout my codebase, and `apply` was one of the many victims of the great purge.
Really? You must be equating forth to ans forth or something like that. All forth really is is a way to run words. It can be extremely simple and needs almost no words implemented to be a real forth. This is a valid forth: http://pygmy.utoh.org/3ins4th.html You are reading a lot into the answer but not reading a lot into the question. 
nope not talking about ans forth. im already aware of 3ins4th.
Just because you can't do it, doesn't mean others can't. Even if I couldn't, it doesn't mean others couldn't. I couldn't because I don't know RISCV assembly and it is not worth my time to learn it. I'm assuming the poster does know it. I think you are making an assumption that the poster isn't very smart. I was not making that assumption. 
I'm assuming the poster is really smart and a good programmer!
Good, failure is a great way to learn.
The obligatory smart-aleck answer: it depends. For those who've done it a couple times, it's as simple as picking a threading method, assigning registers, and jumping into writing primitives. The amount of time you spend pondering each of those increases as your concern with performance does. If you're capable of not worrying about performance and Just Doing It, and you know well enough how to do basic tasks in the assembly you're building it in (traverse a linked list, increment a global variable, compare strings, push something onto a linked list, parse space-delimited tokens...), it's quite conceivable that a working core could take less than an hour. The part that will be most challenging for beginners, though, is getting the threading right. Mentally splitting your model into an implementation machine and an imaginary machine can be a bit difficult. It's easier to think about something like bytecode (in my opinion), because the implementation of the imaginary machine isn't so closely tied to the real machine. Even though the actual implementation of threaded code is simpler. In short: as long as you don't care about performance, just use indirect threaded code, get your NEXT and DOCOL working as fast as possible, and start building primitives. You don't need the outer interpreter to be fully finished to test your threading, as you can assemble lists of addresses with your assembler as well.
The advantage lays in lower complexity compared to conventional compilers and as such the ease of porting.
It's the other way around, but what do you expect anybody to say other than "describe the problem"?
In the 1970s when computer time was hundreds of dollars an hour and an engineer might make 10 or 15 an hour writing code, you spent a lot of time in code review and optimization.
IMO If you are loosing more than 4x speed from a C version of the same program, then that is too much. Simple rule of thumb.
https://github.com/nornagon/jonesforth
Interesting, and based on a tutorial. Not bad! Does it have some kind of turnkey support?
Okay, thank you! 
Albert Vanderhorst has tried to make something like JonesForth but closer to a real system. I believe they are called LINA and WINA for Linux and Windows versions respectively. http://home.hccnet.nl/a.w.m.van.der.horst/lina.html http://home.hccnet.nl/a.w.m.van.der.horst/wina.html And the author can always be reached on comp.lang.forth if you needed assistance. 
Lina seems to be quite capable and small. Nice one!
Thank you! The thing with drivers, I am unsure how they work with Forth systems. I guess they are implemented like other words, too? For example, detecting a harddisk controller could be a word, and then reading a sector could also be a word? Nice work, I always have respect for people that write their own OS!
I think most forth programmers would say that forth is so easy to learn once you learn how to code one that you can make one, ans or otherwise, in a very short time on a completely new machine. And so as forth programmers advance they eventually just start writing their own forths alot of the time. Though some people like me are lazy and so we just choose forths others have written for us.
Hey, read\_harder, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Writing an Forth interpreter variant on top of java or any other language, without the compiler part should be really easy. I could do that. Okay, I have to think a bit about the linked list, how it presents the dictionary with different types like words, string constants, integer values etc. and how everything is executed from there on. I guess I find help reading jonesforth. But then, when I think about the compiler part... I guess, when I understood the linked list completely, then the compiler is nothing more than a "addWordToLinkedList" function for the linked list. Perhaps I should read some old books, describing the core concepts. What I don't understand yet is how forth in-built assemblers work or how they are implemented in forth. And if code is executed, does the system check everytime, if it now has to execute a word or assembler code? I mean, low level words are in assembler and higher level words are often a combination of other words. There must be a kind of differentiation while/before executing.
Life itself is a good example of Worse Is Better. Most mammals can choke because their oesophagus and trachea are joined. The win from being able to breathe through your nose (and smell stuff, and filter air with the little hairs up your nostrils) and switch to breathing through your mouth (massive air intake, lots of oxygenation for short periods like running) massively outweighs the downside of occasionally choking on food that "goes down the wrong way". "Survival of the Fittest" doesn't mean "Survival of the Best", it means "Survival of What Works Well Enough".
http://pygmy.utoh.org/pygmyforth.html
I am very defiant of these life/software analogies. They are similar to me to misguided ideas like "natural programming languages" or "computers should be like humans". That's forgetting that computers are basically glorified pocket calculators. Type in operations, get the result. You don't want ambiguities in the meaning of operations (natural language) or approximate results (some variants of AI). What you expect first and foremost from a computer is predictability, reliability and a computing process you can understand. To me, making a program is more like building a mechanical watch than like growing bacteria to get a duck. I completely agree that Life shouldn't be a model for software because Life is a huge mess. Appendicitis anyone? And why do we grow old? Only because your genes think that by the twenties you should have had enough children, so it's not a problem if your body heals slower and slower or some parts fail. Gabriel also advocates "self-healing and self-organizing systems" in *Objects have failed*. It always reminds me of an episode of the 4th Dimension series in which the protagonist gets infected by healing nano-machines that transform it into a monster because the nano-machines would mistake his inability to breathe under water or see behind him as "damage". Is it really what you want for software? Well, sometimes. Erlang was built to be resilient to hardware faults. The fuzzy working of neural networks is convenient for certain tasks. But that's not the case for the vast majority of software. The imperative/procedural paradigm is still the king of the hill because in the vast majority of the times, we want the computer to do as we told - nothing more, nothing less. 
Well, perhaps DOS is not that bad for that purpose? In my opinion a better, more traditional Forth experience than using Windows, Linux or MacOS as a host. I give it a try!
Manual: https://github.com/utoh/pygmy-forth/blob/master/pygmy.txt
Wow, much easier than I thought! Taking a CPU programmers manual, I could write down every instruction mnemonic like you did, and bootstrap a new Forth for a different CPU from there on, if we would still be living in the late 70s ;) I like that approach! 
I use the word alot alot.
Don't even think about it.
&gt; It always reminds me of an episode of the 4th Dimension series in which the protagonist gets infected by healing nano-machines that transform it into a monster because the nano-machines would mistake his inability to breathe under water or see behind him as "damage". I think you are actually refering to an [episode of Outer Limits](https://en.wikipedia.org/wiki/The_New_Breed_(The_Outer_Limits)).
Retro Forth is real good.
delete
This was initially done to save time. Retro 7 through 9 were written in x86 assembly and supported a fairly large number of operating systems: Native BeOS L4ka::Pistachio DexOS FreeBSD Linux Windows Others (via a LIBC-backed I/O interface): NetBSD, OpenBSD, FreeBSD, Linux Each port had I/O words (and sometimes other bits) adapted for it. But testing was annoying (booting up 9-12 systems, pulling in changes, building, and debugging wasn't particularly quick to do). I was spending far too many hours jumping between systems and not being productive. After Retro 9 I was getting burned out, and I also wanted to be able to run my Forth on non-x86 systems. Faced with this, I decided to take a small virtual machine I had written previously and see if I could build a Forth on it. This worked out very well for me, so I made it the basis of Retro 10 &amp; 11. I refined my model further with Retro 12, making it easier to adapt the interface(s) to systems without a traditional console &amp; keyboard (phones, tablets). I'm happy with the results. I get to code on a virtual stack processor, have better portability (apart from host-specific I/O, everything is identical no matter the underlying host CPU). I also get some other interesting benefits (e.g., the ability to pause execution and resume it later on a different host).
&gt; To me, making a program is more like building a mechanical watch than like growing bacteria to get a duck. I didn't mean to imply I want "self-healing" systems or AI or unpredictable runtime performance. I think even a watch-maker can feel what I was trying to describe, applying his skills and tools feeling like (s)he is creating something "alive". My post might have been understood as if I agreed with everything Richard Gabriel is saying, which is far from the truth. I was merely contemplating whether a search for tools, patterns, languages etc has any meaning and potential to find something of exceptional quality. Regarding AI and the like - humanity's efforts will sooner or later bring more and more sophisticated tools and robots. Whether that's a good or bad thing and what this "sophistication" will bring is a far more interesting question.
So that is the simplest version of a Forth Assembler. :-) The devil in the details is making your little instruction assemblers cope with the all the variations that are possible for your target machine. That can be simple or difficult depending on the machine. Some machines are evil in this regard and require jumping through hoops to make the end product "pretty". Typically the instructions are made using the CREATE DOES&gt; structure in Forth so one can specify the data value of the raw instruction value and then independently specify the runtime action that kind of instruction is run. (This is organizing the bit fields and compiling them into memory) CREATE DOES&gt; is a Forth's "kind of" OOP, where we can create a data structure at compile time and then specify one method for that data. This predated formal OOP discussions by quiet a few years. (citation needed) This may help you answer your other question: "I mean, low level words are in assembler and higher level words are often a combination of other words. There must be a kind of differentiation while/before executing." Although Forth has untyped "DATA" it actually has "type" information associated with every WORD in the Forth dictionary. I will use the classical "indirect threaded" Forth because it's easiest to understand. The "type" information I allude to in every Forth word is the address of a small piece of code that runs when the word is "Executed". (I call these routines "executors") For example, the runtime action of a CONSTANT is to reach into it's own data structure, get the contents and put that value on the stack. That's it! That's all it has to do. The fact that this address is part of every CONSTANT is like type information in that when we see this address we know that the structure in the dictionary is a constant. The Runtime action of a VARIABLE is to reach into it's own data structure and put the address of the data structure on the stack. The runtime action of a Forth colon definition is to setup the Forth VM so it can read a list of addresses and execute each address as a Forth word. :-) Clever! And the finale... There is NO runtime action for a word written in Forth Assembler. It simply contains a pointer to the address where the machine code begins! TADA! For more detail see: http://www.bradrodriguez.com/papers/moving1.htm 
Hmmm... Are you or a representative talking to any auto makers? In light of some of the news around hacking cars, you might have an opening for this product.
That's an interesting point; no, we haven't done that yet. I'll have to look into it. Thanks!
Indeed.
Hacking cars? When did this happen?
https://www.wired.com/story/car-hack-shut-down-safety-features/
Weird. You'd think manufacturers would learn further towards sub-system isolation, not try to tie everything together since that's harder to engineer.
\+1 *We shape our tools and afterwards our tools shape us.* -- Marshall McLuhan
Here is a trivial one written really quickly https://gist.github.com/tluyben/16ee2645c4c8aed813005d51488d5c6a but yes it is a high level one: I wrote a similar small one for a bunch of asm platforms: I will see if I can post one of them. 
Seems to be available for free (?): https://colorforth.github.io/POL.htm "Make variables as GLOBAL as possible." Now there's a phrase you don't see every day.
I would probably use variables in most cases, because the stack is awkward here and in a real application of the quadratic formula (graphics or something) I'd likely want to use a, b, and c somewhere else. If I were to do this with fifos instead of stacks for implicit parameter passing here's how I might do it. First I grabbed [integer sqrt from here](https://gist.github.com/jdriordan/4a373e39f5a7ef1f900fc4d50b1ed20c) because it's not in gforth and I'm being lazy. : sq dup * ; : is_sqrt? over sq - &gt;= ; : sqrt 1 begin 1+ 2dup swap is_sqrt? until nip ; ( I used [Sam Falvos naming scheme](http://kestrelcomputer.github.io/kestrel/2015/09/15/bspl-compiler) here) : d&gt; depth 1- roll ; ( peek from data fifo) : d@ depth 1- pick ; ( consume from data fifo) : quad- d&gt; d@ negate d@ d&gt; * d&gt; d@ 4 * * - sqrt - d&gt; 2* / ; : quad+ d&gt; d@ negate d@ d&gt; * d&gt; d@ 4 * * - sqrt + d&gt; 2* / ; ( a b c -- x) 1 3 -4 quad+ . ( 1) 1 3 -4 quad- . ( -4) 1 -7 0 quad+ . ( 7) 1 -7 0 quad- . ( 0) 
&gt; use variables in most cases [local variables](https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/Local-Variables-Tutorial.html#Local-Variables-Tutorial) or do you mean [non-local variables](https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/Variables.html#Variables)? I'm pretty new to Forth and I don't know how to use local variables, or floating point variables at all. &gt; If I were to do this with fifos instead of stacks for implicit parameter hmm... this is really interesting. thanks for sharing your solution. I'm guessing the fifo is implemented as a ring buffer of some sort? 
For new users I would recommend GNU FORTH. It has some differences from older FORTH dialects, but it has lots of words which make it easy to interact with a Linux host operating system, which means that you can start writing useful programs more quickly. This is important as a beginner, for maintaining your motivation. Once you become proficient with GNU FORTH, then you can go back and look at the older versions.
Sounds like somebody caught the bug. Having been infected for the past year I understand. :)
Have you seen /u/larsbrinkhoff's [forth-mode](https://github.com/larsbrinkhoff/forth-mode)? It has [a gforth backend](https://github.com/larsbrinkhoff/forth-mode/tree/master/backend).
I prefer non-local variables. In most other languages, non-local variables would result in naming clashes and ever longer prefixes on your variable and function names. Because forth is [hyperstatically scoped](http://wiki.c2.com/?HyperStaticGlobalEnvironment) this isn't an issue. I would recommend using non-local variables with short names that makes sense locally (or for the extent that they need to be exposed). In the above example I used the data (parameter) stack as a fifo by grabbing the deepest item and moving or copying it to the top of stack with `d&gt;` and `d@` words. Here's a second refactoring where I pass the parameters in the order they are needed, and make words non-destructive by default with explicit dropping of items that aren't needed anymore using `drop`. ( Quadratic formula in forth take2) ( consume from data fifo) : d&gt; depth 1- roll ; ( peek from data fifo) : d@ depth 1- pick ; ( throw away an item) : drop d&gt; drop ; ( make words non-destructive) : sq d@ d@ * ; : negate d@ negate ; : * d@ * ; : 2* d@ 2* ; ( explicit drop, args in order of application) : quad- negate sq drop 4 * drop * - sqrt - 2* drop / ; : quad+ negate sq drop 4 * drop * - sqrt + 2* drop / ; ( b c a -- x) 3 -4 1 quad+ . ( 1) 3 -4 1 quad- . ( -4) -7 0 1 quad+ . ( 7) -7 0 1 quad+ . ( 0) 
Oh that's awesome. I'm totally going to (try and) get my GA144 boards working this weekend. That's real cool.
I have, it's actually my default mode for forth! &amp;#x200B; however, what I'm trying to do is get my forth program to function as expected with the simplest of terminals, so I don't have to rely on something like eshell or forth-mode. 
&gt; I prefer non-local variables I really like hyperstatic scope, and the way its used in FORTH. I will say that at this point in the tutorial, VARIABLE hadn't been introduced but local variables had. I wanted to avoid local variables, because their implementation is more or less like magic to me. It seems like variable is a really practical and easy solution. Using them could be slow, because it's a memory lookup/write to a far away memory location. That seems like a recipe for cache misses, but I am very much new to thinking about cache locality. the other problem I see with hyperstatic is accidentally hiding names that you want to keep around later in the code. I haven't looked closely, but maybe vocabularies would solve this problem? &gt; I used the data (parameter) stack as a fifo by grabbing the deepest item Oh, I see how the fifo works now. depth is the depth of the data stack, roll is like an rot but for n elements. It's still tough for me to wrap my head around the queue data structure because now the stack is acting as both a queue and a stack. I am still pretty new to forth, though, so just reasoning about the stack without rotates is a slow process. Your explanation helps a lot though, thank you!!
These are useful. I've implemented some of these in my Forth. Specifically, the non-if+conditional flow control worlds. See http://forthworks.com/retro/s/example/LightWeightFlowControl.forth or gopher://forthworks.com/0/retro/s/example/LightWeightFlowControl.forth The others are doable, but don't fit cleanly within Retro's flow control model. Something like this for the `if` forms: :if &amp;[ call ; immediate :then &amp;] call &amp;if call ; immediate :=if &amp;eq? class:word &amp;if call ; immediate :&lt;&gt;if &amp;-eq? class:word &amp;if call ; immediate :&gt;if &amp;gt? class:word &amp;if call ; immediate :&lt;if &amp;lt? class:word &amp;if call ; immediate 'for/next' loops: :for &amp;[ call ; immediate :next &amp;] call &amp;times class:word ; immediate And simulating the 'a' register: 'A var :&gt;a !A ; :n&gt;a @A store ; :a&gt;n @A fetch ; :a @a ; :+! &amp;a v:inc n&gt;a ; There's no partial word fetches/stores as Retro is cell addressed. 
Docs updated.
I would love to use this, if it were to have support for a FOSS Forth.
So here's the situation. I developed it on SwiftForth because it was what I was familiar with. GForth and SP-Forth aren't being maintained. Not free but also not expensive, I've heard mixed things about iForth. Win32Forth is slow and requires DLL params be passed in reverse. Someone would have to write an extensive adapter. There might be non-standard or C-based Forths out there that it could be made to run on. To get it on at least one free Forth would be ideal. Here's what I dream of ... people try it out on SwiftForth eval version, post feedback and stoke interest. I think it's a pretty good engine for what I was able to do. Very simple and easy to use. Think you could you put up with SwiftForth temporarily just to try it out and get familiar?
gForth is not at all unmaintained. There's discussion on the mailing list and the last repo change was 4 hours ago https://github.com/earl/gforth-mirror/commits/master
Depends on definition of unmaintained. It's certainly not inactive, but it has been quite some time since a release. Personally I'd only ever use the gforth release for either pretty basic demonstration stuff or for compiling gforth from git, there's a \*lot\* of new stuff. For example, just recently work was done on adding locals that can live beyond their stack frame (that is, closures). The performance has also improved quite a lot - going from the release to git it's not uncommon to see benchmarks taking half as long as before. Of course, using the bleeding-edge means encountering more bugs, but it's a lot easier to fix them and get fixes upstream at the same time. I think gforth would make a great target, considering how widely available it is. @mcsleepy, which system-specific knowledge is needed?
My mistake. Did a survey of Forths at one point and misremembered my findings on gforth.
Off the top of my head: - FFL binding syntax, including callback creation words - Turnkey creation - Any optional ANS wordsets that need to be included - Any OS-specific stuff that needs to be bound - Any straggling non-standard words I might be using. Tried not to and those I did I tried to include definitions for. Probably the only thing I didn't negotiate on is the use of CALL ( adr ) CODE&gt; ( adr -- xt ) and &gt;CODE ( xt -- adr ). I call the address returned by R&gt; a lot, and I also drop it a lot to cancel the rest of the calling word. 
Could you elaborate on CALL ( adr )? Is the stack effect the same as ( adr -- )? Gforth has a CALL primitive, but it wants an inline argument. If the objective is just "put current (forth) IP on return stack, BRANCH to address on stack" then it should be doable as simply `: CALL &gt;R ;`. Then, for example, `' cr &gt;body call` will print a newline. I'm also not sure what CODE&gt; and &gt;CODE's adrs are. Intuitively I would guess that &gt;CODE would produce the address of the machine code that handles a given xt. The inverse mapping is a lot less clear, since a lot of xts can use the same machine code (I mean, just look at DOCOL...). Perhaps it's more like "given an xt, get the start of the forth code it's made up of", in which case that's &gt;BODY in gforth. I'm not sure about the inverse. `savesystem` will produce an image file that isn't relocatable, for whatever that's worth. Will read more about it later.
Yeah, I use a colorforth-ish shorthand sometimes when a stack effect doesn't return anything, I omit the "--". You're correct about CODE&gt; and &gt;CODE. On Gforth sounds like you could define &gt;CODE as `: &gt;CODE &gt;BODY ;` . If CODE&gt; isn't possible to define in GForth then as a last ditch maybe it can be weeded out, but it might be hairy not sure. Thanks for continuing to look into it. 
If you're willing to make the assumption that the body is always a fixed distance from the xt (currently true in gforth), then you could just do `: CODE&gt; 0 &gt;BODY NEGATE + ;`. ... was what I was going to write until l I bothered to actually check whether BODY&gt; already existed, and it turns out, it does! It's just not documented in the manual. Definitely a more reliable method there. Also, I took a look at afkit (I assume that's where the FFI differences will matter the most), and I noticed that after each `function:` there was a stack comment. Is that parsed by `function:`? I ask because in gforth the word for defining a word that runs a C function is `c-function`, and it requires the stack effect (including a little type information) of the C function in order to work. Also, passing structs doesn't work as far as I can tell, and neither do variadic C functions. Also, ad-hoc function definition doesn't work, they have to be within a c-library ... end-c-library block. It seems like the best options are to either massage afkit into being able to provide the info gforth needs or, alternatively, having two separate interfaces (generating one for gforth shouldn't be too rough with forth-swig).
Yes, SwiftForth uses the stack comment to figure out # of params and if it should return something. The only pass-by-value struct I've dealt with so far is AL_COLOR; passing in the r,g,b,a as individual parameters works. There is no way to wrap Forth words to pass as C callbacks? That is a shame if so, some cool Allegro functions would be unavailable. But callback support isn't required for the engine core so the features that depend on those functions can be optional. Seems strange to me that gforth can't do that though... It sounds like generating the bindings as gforth needs them shouldn't be difficult. You can load those bindings when kitconfig.f specifies gforth. How I've set up how platform configs work is in the readme at [https://github.com/RogerLevy/afkit](https://github.com/RogerLevy/afkit) , down in the Cross-Platform Support section.
Regarding CODE&gt; though, the main use in Ramen is to transform a code address to an XT so a word that uses EXECUTE instead of CALL can use it. Because EXECUTE on SwiftForth does the same things as &gt;CODE and then jumps to that. It's a fixed offset between the "address spaces".
&gt; It begs the question, is their a place in all this for Forth architectures and the language. Yes, there absolutely is. The end of Moore's Law means that programmers will finally have to start caring about real efficiency again. If processors are no longer getting faster, then there will be a resurgence of the need to squeeze every last bit of performance out of what you have. This could bring back the sort of creativity that produced games for the Commodore 64; and the one thing FORTH is good for more than anything else, is producing very small code.
&gt;There is no way to wrap Forth words to pass as C callbacks? Ah! Sorry, lie of accidental omission there. There is indeed a way, you can define "callback instantiators", which do the wrapping of forth words (xt -&gt; C function pointer). But because a unique function pointer is needed for each different wrapped xt, it's possible to run out of C callbacks if you use the instantiator too much. The number of callbacks generated for a callback instantiator to use can be configured by setting the value CALLBACK# prior to creating the callback instantiator with C-CALLBACK.
There's nothing that inherently prevents a language from being parallel or not. Functional can make it easier in some respects, but it is ultimately just another paradigm which people are either good at using or absolutely awful. Since there's at least some use of the GA144 out there, maybe Forthers tend to be the former for some reason.
The issue is that you need different instantiators for every different function type you use. It's common to name the instantiators based on the signature - for example, I might have an instantiator named "nna--void:". Callbacks can technically be created at runtime, but it requires a C compiler and some other tools I think. Personally my approach is to just explicitly mark the definitions that need to be callable from C, but then again the only time I've done that it was with a somewhat more regular API (interface to Python, where I can have every function take 2 args - self and "those other args", if I remember correctly - and always return a PyObject\*. Specifying the number of arguments to unpack in that case was just a convenience, as the C callbacks didn't need to know anything about the number or types of arguments). The ideal callback-utilization scenario would involve keeping track of how many callbacks are needed for each function type, then at the end of the code generating the appropriate amount for each type. An easier guesswork approach could be making an instantiator for each C function that takes a callback - there's a forth-swig option to generate them. Then you just need to set callback# to a comfortably high number. Some of them will be wasted, but aside from that it should work fine. Er, more accurately, \*creating\* the callbacks, as in compiling C code that can wrap xts, is a compile-time operation. \*Assigning\* the callbacks to certain xts is a runtime operation. How does swiftforth do callbacks?
SwiftForth wraps words with a callable data structure and you access the C parameters within the callback via words like PARAM0 PARAM1 (memory hazy on the actual names but something like that). If you want to return something you just leave it on the stack but you don't have to, you don't even have to clean the data stack. The word is CB: . You tell it how many parameters to expect on the C stack and an XT. My vote is definitely for the one-instantiator-per-word approach. You could wrap it up in another word that makes it transparent
Olofsson (of Adapteva fame) has been doing good work https://www.eetimes.com/document.asp?doc_id=1330568&amp;print=yes and he came to head the two proposed programs recently (check out his Twitter stream https://twitter.com/adolofsson for details). I was thinking about packing a Forth into the 32 k onboard memory https://parallella.org/forums/viewforum.php?f=31 but never came around it.
Would be nice to see cleaner code highlighting and explanation. This is interesting.
Also the first release 1.1.0 is out. Examples repo (you'll probably want to get this): https://github.com/RogerLevy/ramenexamples Link to the release: https://github.com/RogerLevy/afkit/releases/tag/1.1.0 
I think effective parallelization is commonly limited though increased data-access or 'routing' times. Because of this, I expect that future processor designs probably combine memory cells directly with dedicated processing units. Ideally this units then process data by loading independently so the central processing-unit is reduced to a central-synchronization unit. Indeed such an architecture would require complete new (or at least uncommon) approaches to language design.
 VARIABLE NIL ( CREATE a variable called NIL) NIL ( put address of variable NIL on the parameter stack) NIL ! ( store that address in NIL) `NIL` was the first thing compiled, so storing it to `NIL` will set `NIL` to point to the start of the list.
So @ and ' would do the same thing, until someone does another NIL !
Thank you!
This is cool; but it seems like a lot of complication simply to eliminate STATE. In 8th the user doesn't have access to 'state' (which is task-local so different tasks can have differing states), though some words are state-dependent.
I Think that is a nice solution.
Good stuff. I have a dream back-burner project (that I realistically may never get around to) for an umbilical cross-compiler based on combining ideas (well, hopefully synthesizing the ideas into an elegant whole rather than just making a mish-mash) from ColorForth, Fox's Aha, the Canon Cat, and the DOSystems M-Code cross-assembler.
Sounds interesting. I only glossed over the description on Usenet, and didn't think through the implications. (Sorry about that.) Care to explain why you need three bits? A casual reader might think that only two would be needed.
Thanks for looking it over. There are five reasonable combinations of behaviors: P+I+C, P+C, I+C, C, I. 
Coincidently I just finished watching the new Chuck Moore interview from EuroForth 2018 where he mentions at the very end that he's experimenting again with the concept of sourceless Forth, that rekindled my interest in an M-Code like approach even more.
Nothing really, you can avoid the whole state smart word issue by consistent naming and usage discipline, instead of `s"` just have `"` and `["]` for example. Forth already has pairs like `char` and `[char]`, or `'` and `[']`. What I described is an implementation approach for dual behavior words that I find simpler than the others I've seen proposed. But of course that assumes you want them for your Forth in the first place. Avoiding dual behavior words altogether is just as legitamate a design decision for one's Forth as far as I'm concerned. 
I'm not sure what he mean with sourceless programming. Anyhow an environment where I can program either in Assembler or Forth and both representations are converted in the fly is quite useful in my opinion
My understanding of the concept of sourceless programming is development through interactive assembly/disassembly. Your development environment doesn't store *source code* per se, it stores *object code* and some *supplemental information* (labels and comments). When you wish to view or edit your program, the system disassembles the *object code* with the help of a *processor instruction table* and the *supplemental information* to produce a source "view" of the program.
Most of the time you factor out a fragment of code into its own definition because it can naturally stand on its own and you'll reuse it in different contexts. Other times you factor out a fragment simply because it forms a cohesive sub-unit of a definition and factoring it out will make the original definition more readable and self-documenting, even though you don't necessarily expect to reuse the factor in other contexts. It's not surprising that such an *internal* factor would have some coupling with the original definition that gave rise to it. 
Sooo...like a Smalltalk image? Can't you view an object with Source? I'm missing something here.
I'm not sure what you mean by an "view an object with Source". The *object code* here is not in the sense of object oriented, it's in the sense of machine instructions. Chuck tries to minimize the semantic gap between source code and object code, between language and processor. This is what he means in the video when he says "one-to-one correspondence between source and object". In effect his development environment would be closer to an interactive assembler/disassembler than a traditional compiler or interpreter for a parsed language.
I think the author talks about *pure functional programming*, because functional programming on its own isn't any better for parallelism than imperative programming. And in that case he is right, because PFP allows you to *compose* and *decompose* parallelism, which is crucial for any bigger application!
Ah, but how would that work?
I'm not sure what method Chuck uses at the moment, my description above is how I would do it.
Like 8thdev said it depends on the Forth being used. My experience has been that with something like the '1991' web server you would just include the source file in your project.
There is an initiative to establish a Standard Forth package ecosystem: http://theforth.net/ It's mainly based on GForth with some support for VFXForth.
I've been thinking about a Forth code editor for a long time. My idea goes a step further by storing source in a proprietary database instead of plain text, all words identified internally by an id instead of their name. It would check the stack effect as you edit and not allow you to save until it passes.
I just use dictionary's [hyperstatic scoping](http://wiki.c2.com/?HyperStaticGlobalEnvironment) for version control and package management. It's got fine granularity and the dependencies never break. It works like NixOS. I use all the same familiar words for package management and version control: `WORDS`, `SIFTING`, `SEE FOO`, `' FOO .CALLS`, `BIND` ( https://hub.darcs.net/pointfree/forth-bind ), etc. 
Thanks for treat me and your and JesterSks’ advices! OK, I uses GNU Forth, because of handful. I’ll see the documentations about that. 
My pleasure! I can't help too much with gforth, since I don't use it; but it's got plenty of docs. Best of luck!
There was also M-Code, which was a DOS program similar to a machine code monitor that could do sourceless incremental cross-assembly/disassembly for various target processors. I had started writing a similar program for the GA-144 but never got around to finishing it.
You forgot to explain the *why*. I remember you writing something along the lines of "fast enough for my users". What prompted you to work on optimizing? Are you planning to optimize more? Nice writeup.
Thanks for the "Fibonacci hashing" tip, I just add it in my (not so good) pseudo random generator as an utility, it spreads things nicely on a pow2 square.
Well, no one ever said Forth wasn't expressive.
Thanks. Well, I periodically look at improving performance even if nobody specifically asked for it. In particular, I try for reduction of memory usage (which often also results in speedups). Because when running on a lower-end device (for example, and Android watch), slow performance can become visible, which is not a good thing.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programminglanguages] [Some thoughts on, and results of, optimization](https://www.reddit.com/r/ProgrammingLanguages/comments/9me9eo/some_thoughts_on_and_results_of_optimization/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
That is pretty neat, thanks!
Why wouldn't ⇟ (downwards arrow with double stroke) work for ROT?
I played a similar game in the past :-) it's a lot of fun but I ultimately decided it wasn't worth the effort due to the increased barrier to entry and the requirements for custom tools (in my implementation). Of course, if you're doing this for yourself I would highly encourage your experiment. If you have bitmap graphics, the easiest thing to do is to write a simple bitmap font editor and simple character render. From there you can try anything you can imagine.
reminds me of diagrams on HP calculators 
What do you want? Just those glyphs into a ttf? They look kind of like runes.
Those would work better for me upside-down. If I were to make a typeface, I'd just stack tiny letters vertically for the stack operators: `tx` for `swap`, `xty` for rot, `tt` for `dup`, `xtx` for `over`, `tyx` for `-rot`, `xxt` for lift, `xyt` for `swish`, etc...
I do actually have some experience with font design... so i could give you some pointers how to do that on a technical level. But i don't really understand what you have in mind. Do you want to put the stackops symbols somewhere into the private use plane? (and how would you want to type them then?) Repurpose other symbols? If the goal is literal programming, maybe you could just parse normal forth code and spit out a tex file where you substitute non-comment stack op words with tikz drawings. 
In my experience graphics artists will come up with things that techies don't think of, for jobs like this. It would be best to work with an artist and give them programmer's ideas about what the words do and then let them play. It is a bit iterative but you get results that will surprise you. (YMMV) 
&gt; I remember you writing something along the lines of "fast enough for my users". Typical "sour grapes" argument. I'm not dissing 8th in particular here, it's just a general observation. The thing is, when a language is slow your users have to optimize on their side; they have to switch from a readable and simple algorithm to an ugly thing that needs lots of code comments, or to a native code implementations via an FFI for instance (which aside from the inconvenience, has the cost of having you to think about what should hard-coded vs what should be scripted). A preferable situation is when the language is fast, so that your users can use simple "brute force" algorithms. This is the difference between "bubblesort is good enough here because my Forth is fast" and "I need quicksort here because my Forth isn't fast enough".
Immutability is the key to parallel processing efficiency, but you don't have to use a functional language for that; it just happens that most fn languages work that way.
Kudos
So currently the machine EITHER accepts 1 char from stdin OR it accepts 1 char from the call stack. There is are a finite number of op codes which do anything each a character which in english correlate phonetically to what they are supposed to do. So read and write can be r and w. Or they can be c and b for (see and be). It is a token machine that hopefully can be a little more friendly to type by hand. It has no ability to add named functions other than that it is possible to implement macros within in it or even a macro compiler which compiles down to the op codes before executing. &amp;#x200B; That being said each of these op codes in the machine expect where they want their value to be on the stack. Either it is the topmost item, the second, or third item down that they want, and they generally take the results and store them in the downmost empty value in the stack and work their way up. So if an op code takes five arguments and returns two then the fifth and forth slot down in the stack will contain items and the top three items will essentially be empty but really be filled with garbage values. None of the op codes actually moves the pointer to the stack upwards or downwards except &lt; and &gt; which increment it up and down by 1 cell. A cell here being sizeof(void\*\*) or something along those lines. So there is not quite any such thing as pop or drop in this language. You can drop by simply typing &lt; but this does not recycle the value or anything, it just moves the stack head backwards. You can get it back with &gt; if you have not already written over it. Moreover none of the op codes actually do any pushing. There is no equivelant to push. If your op code adds something to the stack you must type a &lt; before hand if there is not space for it. But essentially it should not add any extra code to the machine except one loop, one call for the next op code, a switch to determine the state of the machine, and a switch to call the next op code. I am not sure if this will actually speed things up though mind you. I may just be grasping at a seemingly cool idea. I just thought it was neat though.
The code sounds like a hell of indirections, the most expensive operation is the recovery of a value in memory, if we are lucky or by design, it will be in some cache but it will be several orders of magnitude higher than any other operation . I do not think it's a good idea to make explicit the movement of the stack of the load or calculation of values, it's like it would complicate the language, but all search is good, go ahead. Perhaps the most optimal model for an interpreter is to have the top of the data stack in a separate register and then a pointer to the second. I use this scheme, you can see the definition of + in https://github.com/phreda4/r4MV/blob/master/r4wine2/redam.cpp#L416 Of course this model take it from colorforth !!
I made the top item of the stack a single pointer and it made things alot simpler. First of all it removed alot of extra pointer lookups from my code since many of my ops only take one thing from the stack then return one thing. Also now my &lt; and &gt; words are alot more useful because none of my ops are more than two arguments. This means I can cycle through my stack of things left and right in a whole different way.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day!
Well, 8th is active :) There are a lot of implementations. gforth being a well known non-commercial one.
here :r4 https://github.com/phreda4/reda4 and in developing r3 https://github.com/r3www/r3 
Just reply to it with "bad bot".
can I say register register\* void\*\*\*\* TOS;
This link has a pretty big list. https://github.com/topics/forth
Cool. I have heard of people doing something similar for @ and ! and their cousins, in systems that would be used by non-experts. Simple memory protection.
Thanks. Yeah, in 8th @ and ! don't directly access memory, so no need for such a 'shim'. Although I am giving some thought to a stack-effect checker...
CiForth/Lina Forth is my preferred "normal" Forth. It doesn'doesn't have so many words as to be overwhelming, comes with a lot of documentation, runs well, very customisable. http://home.hccnet.nl/a.w.m.van.der.horst/lina.html I also like Retro Forth, it's completely different and it's great. http://forthworks.com/retro
It looks a lot like the Brainfuck language, well Brainfuck use &lt; and &gt; because it is NOT a stack language. In a stack language you don't need it because the top of the stack is where it is supposed to be I guess.
Yeah it seems to be nice tutorial for understanding Forth deeper!
I like the idea, but DUP is much, much, much simpler
Thanks for a lot of comments! There are many implementation and they are awsome! I feel interest from :r4, Retro Forth and Sporth (from GitHub's topic), especially there are worth to try them.
If I end up keeping a stack then I will just make it a normal stack with dup and swap. The reason I am doing this is to try to experiment with a system that does not use the stack in a standard way.
I really like http://flashforth.com
hi, my forth is derived from colorforth, some several internal diferences with forth, in this context I make static analysis and I can detect dead code, transform variables in constant, make inline words. 
**TL:DR** Yes and no :p. Statically typed *concatenative* languages exist, but Forth with types would no longer be Forth. This is arguing semantics a bit, but I suspect that most people who use Forth consider the fact that it is untyped an essential part of the minimalism that makes it *Forth:* you can only really program in this situation if you never lose sight of the underlying hardware. Your "types" are the raw underlying machine behaviour, and there is no limit to how you may interpret that. This is part of the challenge/charm of Forth programming. So it's a bit of a [No True Schotsman](https://en.m.wikipedia.org/wiki/No_true_Scotsman), but static typing doesn't "exist" for Forth *because it would no longer be a Forth* ;) BUT! Not to worry! You can have your cake and eat it too! :) What you are really looking for is the paradigm of [*concatenate languages*] (http://concatenative.org/wiki/view/Front%20Page) - Forth is basically the Ur-language in this field, the primal force, so to speak^1. And there are statically typed concatenative languages. Kitten by /u/evincarofautumn is your best bet, I would say: http://kittenlang.org/ It also has its own subreddit: /r/kittenlang Sadly, progress is slow because he can't work on it full-time (and on top of that, I think research like this tends to go much slower if you don't have a mental sparring partner - this is a bit of an intellectual niche) Also, I found [Joy](http://concatenative.org/wiki/view/Joyis) to be… well… a *joy* to learn. It's not typed, but still be a fun one for you to read up on, and it is quite small. Joy is what you get when you start from the abstract, mathematical side. It is almost an anti-Forth in that it does not care about the implementation so much. But that is also what makes it a good complement, and very useful when you want to reason about concepts without worrying about the hardware. It really helps to get across what makes a concatenative language *concatenative* in a very elegant way. ^1 Self-indulgent tangent: to me, Forth, assembly and other languages that stick close to the metal and/or have their niche mostly in embedded environments are like the single-celled life forms on our planet: they dominated the world's history for most of its existence, and in terms of biomass and importance to the ecosphere and life in general *are still fundamental*. We undervalue and underestimate them because we are so "anthropocentric": the less human-like something is, the more effort it takes for us to "get them". I actually never did any serious programming in Forth, but I find it endlessly fascinating and love reading about it. 
ColorForth (cf) and r4, they are not interactive, cf has an ide and r4 works with common files, due to this when the program is compiled, either tokens or machine code, you can perform a static journey of the words to be compiled. You can calculate the state of the stack as well, although if you use words that have different stack motions at run time, this is not possible! in https://github.com/phreda4/reda4/blob/master/r4/Compiler/r4-post.txt you can see the code for this static analysis ask anything you want.
Program Analysis for Stack Based Languages: http://www.complang.tuwien.ac.at/anton/euroforth/ef03/poial03.pdf (Jaanus Pöial writes a lot about typed concatenative languages) https://hub.darcs.net/pointfree/strongforth https://www.reddit.com/r/Forth/comments/5yyxaj/type_inference_in_stackbased_programming/ https://www.reddit.com/r/Forth/comments/6jo3qe/type_inference_in_stackbased_languages/ https://www.reddit.com/r/Forth/comments/56fdrv/type_system_for_forthlike_stackbased_language/ https://www.reddit.com/r/Forth/comments/6jt3ai/%CE%BBsuperposition_of_stack_languages/ 
That is what a typechecker is, though: a form of (usually static, sometimes dynamic) program analysis. It may encode any property that can be expressed in the logic of the type system—not just the representations of data types, but also many other program invariants, such as race-freedom, memory safety, security properties, performance properties (e.g. restricting heap allocations), and more. Strictly speaking, a type system doesn’t need to care about data types at all, although they are a very useful thing to include in analysis—especially for guaranteeing memory safety. So I would ask: what specific properties would you want to verify? There may or may not be an existing tool that does what you want, but I can probably at least direct you to relevant information about the subject you’re interested in.
Ah, my bad. Still, as evincarofautumn points out: you can't really do that without some form of types.
Do you know if it supports PIC32? The site says PIC24-30-33, It's not clear to me how similar/compatible they are
Any idea about the status of this? Looks like a similarly inspired conference project
You already mentioned it. Even there is a static analysis tool called Dialyzer for Erlang which has dynamic typing, so I don't think types are needed for a static analysis tool to exist. At this point I'm probably asking too much
Thanks a lot!
There is a big difference between *dynamically* typed languages and *untyped* languages!
Well I already wrong on this one. Thanks for pointing out.
Just clarifying - things will be pretty confusing if you don't see the distinction. Good luck and have fun with learning! :)
Thank you. We are digging deep in history here, so obviously there will be a lot of dead links. I have been able to successfully compile [ips on linux](http://wwwhome.cs.utwente.nl/~ptdeboer/ham/ips/). If someone wants to do that too, be sure to use this Makefile and not the one provided. ``` src = $(wildcard *.c) obj = $(src:.c=.o) CFLAGS = -g -Wall -O9 LDFLAGS = -lm -lncurses ips: $(obj) $(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS) .PHONY: clean clean: rm -f $(obj) ips ```
In case you didn't find it here is the link http://www.camelforth.com/e107_files/downloads/newmicros/
Do we know which of [the active OSCAR ham satellites](https://www.n2yo.com/satellites/?c=18&amp;srt=14&amp;dir=1) are running IPS forth?
* [AO-10](http://www.om3ktr.sk/druzice/ao10.html) was running IPS, but its computer failed due to radiation damage. It is inactive now * [AO-13](https://www.amsat.org/amsat/articles/g3ruh/122.html)'s Integrated Housekeeping Unit worked reliably as well, but the satellite deorbited in 1996 * [AO-40](https://en.wikipedia.org/wiki/OSCAR_40) was running IPS flawlessly for years until 2004 when it ceased operation * [Phase 3A](https://space.skyrocket.de/doc_sdat/amsat-p3a.htm) was running IPS as well, but it never reached orbit due to a launch failure At the moment, there is no operational satellite in orbit utilizing IPS and I doubt there will ever be one again. All satellites used a radiation hardened 8 bit [RCA 1802](https://en.wikipedia.org/wiki/RCA_1802). Nowadays, you'll even find [Android phones](https://www.nasa.gov/phonesat/) up there. As most missions are in low earth orbit where the electronics are relatively unexposed and software is becoming more and more fault tolerant, these times won't come back. Maybe if AMSAT-DL revives its [Go-Mars](https://space.skyrocket.de/doc_sdat/amsat-p5a.htm) plans, but that is unlikely...
Thanks for the link. I may have some files not listed there. So what is going to happen to source code and cad files? It would be a shame to let that go to waste.
[Demo — Ait: A Concatenative Language for Creative Programming [VIDEO]](https://www.youtube.com/watch?v=o5Dc_UQhP3s)
I really love all of Manfred Von Thun's stuff. When I read through his paper on the type theory of concatenative languages, there were so many lightbulbs coming on in my head. I was excited to see that Factor had a release this year, so it's not an orphaned project like I thought it was (sucks when the language creator gets hired by someone who doesn't want to keep his language alive...). I've stopped calling my forth code "stack-based" accordingly -- I'm a concatenative programming fanboi, and hope to see it come into its own someday.
So, do you mean that you have type field instead of a code word field? Classic Forth puts an address in the codeword field, which is JMPed to (*not* CALLed), with the address of the data field in TOS. For composite words, the codeword field points to ENTER, and for native words, the codeword field would basically do a JMP *TOS. Then you can put whatever in the codeword field, and in fact DOES&gt; puts DODOES there. Or maybe I'm misunderstanding what you're saying? Is your Forth compiler a binary generator or a Forth interpreter-compiler?
Yes, one could do exactly that. Although it would make the code a bit more fragile by requiring the 'needs' to be in a specific place in the code. Most Forths I know of are of the "run with scissors" variety. 8th is concerned with runtime security, but not necessarily with dev-time safety. So I'm removing some of those safeguards as i improve performance.. What I've done for the next version of 8th is to simply intercept SEGV and throw an exception; so the performance isn't affected, but a useful bit of information is given. The 'safe' library does give a bit more I think, since the context isn't lost (via a SEGV).
I use circular stacks on my host Forth VM, they can't physically underflow or overflow, but they can logically underflow or overflow if you're not careful.
I'm not sure I understand, some sample code that demonstrates this new capability might help. If I wanted to temporarily override the compilation semantics of a word I would write a new definition, probably in a different vocabulary, to shadow the previous definition. If I wanted to be able to override the compiler itself, I would make `compiler` revectorable through a `'compiler` variable (others might make `compiler` a deferred word).
Native code, so there are no `do*` functions. Each definition needs to be compiled appropriately, and I'm trying to avoid unnecessary `call`s. In testing, I found that the penalty for extra function calls is pretty severe. So, e.g., if I define a `variable`, I'd rather just compile the literal constant that puts the right address on the stack (instead of having a `dovar`-type function that requires a call/ret, etc.). 
I was trying to think of a way to make the compiler extensible by redefining `:`, adding code to handle a new kind of word, and then chaining execution to the previous `:`. This scheme, with the compiler looking up a function pointer for each word-type seemed to make more sense to me at first blush.
Ah; always good to know what the constraints are! If you think you will need that functionality in the future, you should build it in as you're planning. But if you aren't sure, it might be better to delay that and see what develops are your product matures.
I eventually settled on the two extremes of the threaded code spectrum for my Forths, subroutine threading with native code inlining when speed matters most, and token threaded code (e.g. bytecodes) when space or portability matter most. The two extreme threading schemes have more in common architecturally with each other than with either ITC or DTC, the key difference being that SRT/NCI deals with a physical processor's instructions, while TTC deals with a virtual processor's instructions.
Yes, I did something like this, except I didn't bother with index and tables. Each dictionary entry had a "compilation token", the address of a word that would receive the address of the word to be compiled. This was for a subroutine threaded Forth with native code inlining. From my experience, one doesn't have so many different types that this degree of flexibility is worth it. My current Forth is more like a bytecode interpreter that knows three types of words: regular, immediate and constant to inline variable references as literals. My current approach is to make all optimizations manually. For instance my previous interpreter had automatic tail call optimizations, because loops where done by self-call. My current interpreter just has "goto". It also has a bunch of words that let me optimize common operations with literals, such as fetching from a constant address or adding a constant, mainly to optimize access to structured that if needed. For instance (with translation to standard Forth): here is foo 0 , ( VARIABLE FOO ) : bar ... foo @ ... . ( : BAR ... FOO @ ... ; ) Foo is already compiled as a literal in bar, that's the only "optimization" my compiler does (and only because foo was defined with "is"). Now if bar needs to be a bit faster, I can do this: : bar ... [ foo @] ... . @] takes the tos and compile it into "fetch cell at literal address". Manual optimization by inserting two characters. I also have !] (store at literal address) and ,] (which is the equivalent of LITERAL I guess). ANSI Forth would have you to choose between VALUE and VARIABLE upfront, and have you insert/remove things all over the place if you change your mind. 
All unknown to me. I got involved by accident. I haven't worked with the boards etc. since 1998. Brad may know more. 
That compilation token is interesting. It accomplishes the same thing, though at an expense of another pointer in the header. Actually, come to think about it, it's nearly the same -- my table idea is just a space optimization on that where my type field is an index to a table of pointers. I don't have any pointers in my current header design, so I didn't really think of putting a "compile word" in there. Thanks for your comments!
Maybe you can help me understand a more normal way to do it? In an indirect threaded Forth, you have a code word that executes the runtime behavior of the word. But in a subroutine threaded forth, you call directly into the word. I don't want things like variables, values, etc., to require a call. This is distinct from just setting up the dictionary entry, in my understanding. So the defining word lays out the word in the dictionary, but later on when that word gets referenced in another definition, it needs to be properly compiled. How it is compiled depends on what kind it is -- the compiler needs to generate code for whatever the analog for the "code word" is. It seems that either means the compiler needs to have awareness of each kind of word (function, inline, immediate, value, variable, constant, create/does, etc.), which makes the compiler a lot bigger. /u/astrobe's comment about building a forth with a "compilation word" made a lot of sense to me, and is in fact somewhat similar to my field-indexed table of function pointers. I guess I really am kind of putting a "compilation word" in my header, just keeping the space utilization small by using an index instead of a pointer. Does that make sense?
By the way, your `here is foo...` syntax is very interesting. Just reading it as a sentence (Here is foo [it's] 0) makes it seem beautifully idiomatic. 
Let's take the example of a defining word for variables, we'll assume the Forth is either subroutine threaded with inlining, or token threaded with inlining. We'll also assume that headers, code, and data are not separated into different memory spaces. The following describes *just one possible approach* amongst many. `variable xyz` The word `variable` lays down a header, some code, and a reserved cell for the new variable `xyz`: |header|code|cell| We'll drill down into the code. Most likely it contains something like this: `|lit|&lt;address of xyz's cell&gt;|exit|' When we type in `xyz` the interpreter executes `xyz`'s code and we end up with the address of the variable's cell on the stack. On the other hand if we type something like: `: foobar ( -- x ) ... xyz ... ;` Most likely the code that's layed down into `foobar` ends up looking like this: `...|lit|&lt;address of xyz's cell&gt;|...` In other words in the case of a variable instead of compiling a call to `xyz`'s code we would probably inline `xyz`'s code (minus the terminating `exit`) into `foobar`'s code. How does the compiler know to inline `xyz`'s code? Most likely there is a header field that contains the length of the code to be inlined, and if that field is zero, the compiler compiles a call to the code address instead of inlining the code itself. 
My pleasure. I should have mentioned that there might be extra padding bytes between `xyz`'s code and its data cell to align the cell's address.
Although I should mention that with an umbilical (i.e. tethered) cross-development setup it would not be unusual to use both, e.g. TTC on the host Forth and SRT on the target Forth.
Thanks for the links!
Two days ago I released Mecrisp-Quintus 0.9 including support for MIPS M4K architecture (in addition to RISC-V RV32IM) and an experimental port for the PIC32MX570F512L. I am happy to assist in creating another port. Matthias mecrisp.sf.net
Is this at all related to your unchecked stack optimization from the other day?
Not sure this would be helpful in practice; dependence on the exact stack depth is a form of coupling. The only safe way to do this is delineating parameters sets with a magic number or something.
Yes, in that those caused some formerly "working" code to cease working. Turns out that unbalanced stacks were the real problem, and were exposed by the changes to stack handling.
[Here's the link](https://8th-dev.com/forum/index.php/topic,1760.msg9836.html#msg9836) to the SED checker discussion. Yes, it has caveats. First, it's still under development so it's liable to change before it's really released. Second, it requires the SED to have a particular format. Third, it can only handle "simple" SEDs, e.g. ones which don't have multiple paths.
I'm still trying to figure out how I want to handle stack underflow in my compiler. It's position independent and freestanding, and I'm fighting with the Win32 exception filter :-/. 
Right now, "next" gets compiled like this example from my Forth: 00000012 4D3BA7C8000000 cmp r12,[r15+0xc8] 00000019 0F8FD7EDFFFF jg qword 0xffffffffffffedf6 0000001F C3 ret Note that `r12` is the current data stack pointer (PSP), and `[r15+0xc8]` is a global variable denoting the beginning of the stack. So before returning, any compiled word checks to see if the current stack pointer has overshot the start of the stack -- if so, it jumps to a `reset` word that puts everything back to the beginning and re-inits the outer interpreter.
On my experimental blob Forth, I have it so that when the interpreter reaches a word that exists, it executes a word class, and that word class will decide what to do with the word at that time. The standard class at STATE=0 will execute the word, STATE&gt;0 will compile it in. There are other classes which act like sub-dictionaries which switch the vocabulary, and yet others that act like object classes which will do late or early binding. You can probably do something similar if you leave your modifications open-ended enough.
That's interesting. As you describe it, I think you're accomplishing what I'm trying to do -- change compilation semantics for different kinds of words. I don't have a STATE variable in my forth, so the compiler and interpreter are more like coroutines. Thanks!
Thank you kind person!
In the case of stack violations, the entire program quits because it's not in a recoverable state. 8th throws an exception, but exceptions in 8th are fatal errors.
Can you not just return some garbage?
It occurs to me that perhaps what is needed is something like we see in the AIT language by Stian Veum. In his video I see this kind of thing: [ 45 180 ] Lineto It looks like the square brackets are doing parameter checks to lineto. Is it time for 9th? :-) 
If the stack underflows far enough it'll segfault. Right now I make a safety buffer of 256 bytes above the first cell in the stack so a word can underflow a reasonable amount. There's still danger of a spectacularly misbehaving word smashing it. That's the case where I'm fighting with the exception filter. 
Heh. Well, I don't think so :/ The built-in words in 8th do check their parameters and throw exceptions (almost always, anyway). But user-defined words haven't had an easy way to do that until now.
depending on the level you're writing it at (high level language vs. low level code) you could introduce a mechanism where stack position -1 is recursive, i.e. -1 == -2 == -3, 
I've since improved it (e.g. made it work correctly, and also handle 'dont care' data types)
Hmm... would that mean every time anything touches the stack I need an underflow check? So instead of, say, `drop` being two instructions (an `add` to move PSP and a `mov` to set TOS (I keep the top of stack in a register)), it becomes a big check for underflow and looping at -1?
I got RETRO FORTH from the Mac Store. I am running MacOS Mojave. I see 2 panels. I entered words in both left and right and nothing happens. I can't figure out how they work and I don't see them mentioned in your documentation. Is there a user interface guide?
No, but I do have a few notes. (Getting the interfaces documented is a big part of the work I'm doing currently, so the next release will be significantly improved in this regard). Here's a short description of the interface: # User Interface RETRO is built around an dual pane editing environment. On the left is the code editor, where you enter the code for your projects. To the right is the output pane, where results are displayed. Below these is a text field and a button labeled "Eval". This can be used to run or test code without saving it into the editor. Above the output area is the toolbar. Here you'll find buttons to save, open, or create files; as well as buttons to run code in the editor (*Go*) and clear the output pane. # Entering Code Code is entered in the editor pane on the left. In RETRO, I use a literate style, with code to run in fenced blocks intersprsed within commentary. Only code in s fenced block will be evaluated when *Go* is hit. As an example: This is commentary. ~~~ 'This_is_code s:to-upper s:put nl ~~~ And back to commentary. The results will be shown in the output area to the right. # The Listener RETRO allows for some interactivity via the *Listener*. This is provided by the intput field at the bottom of the screen. Enter code directly, and hit the *Eval* button to run it. The results will be shown in the output area. (Code entered in the Listener should not be fenced). 
Hmm. Thanks for the additional info. However, I don't see a prompt in the left pane and if I type 777 dup .s in the left pane and click the Eval button the only thing I see in the right pane is Stack: \_\_\_\_\_\_\_\_\_ I am probably missing something obvious. &amp;#x200B;
But the Elvis operator doesn't evaluate the second operand until it's needed.
I don't understand. When you invoke **?:** it has to check what it's given, then. The word isn't immediate, so it won't fire off until run-time.
Think of Elvis' hairdo, then rotate the "?:" 90 degrees clockwise...
Does 8th use the [call-by-need evaluation strategy](https://en.wikipedia.org/wiki/Lazy_evaluation)?
**Lazy evaluation** In programming language theory, lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing). The sharing can reduce the running time of certain functions by an exponential factor over other non-strict evaluation strategies, such as call-by-name.The benefits of lazy evaluation include: The ability to define control flow (structures) as abstractions instead of primitives. The ability to define potentially infinite data structures. This allows for more straightforward implementation of some algorithms. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Are you saying that in kotlin it doesn't evaluate the second operand? The implementation in the link doesn't take expressions, it takes values. To only conditionally evaluate the default, you'd have to do some sort of quotation and make the `?:` operator execute the quote if necessary.
&gt;Are you saying that in kotlin it doesn't evaluate the second operand? Yes, this is what I was trying to say. &gt;That wouldn't make as much sense in a Forth But this is the half of meaning of the Elvis operator. &amp;#x200B;
arguments are pushed onto the stack before calling the word, right? so the arguments will be "evaluated" before being consumed by ?:
An operator similar to the C-language `a ? b : c` short-circuiting conditional would only make sense in Forth if b and c are XTs. If b and c are just cells, then you want is something similar to a 2-1 mux operator.
It's not really like the C operator. If it were, you'd be right. But think of it as a way of "coalescing nulls". If TOS is null, put a default value on TOS instead.
I didn't see that in the docs I read, but OK... so since 8th doesn't do lazy evaluation, and my ?: word expects two items on the stack, it won't do exactly what the Kotlin version does. If you really wanted to fully emulate that, you'd write a parsing word which only eval'ed the parsed string at runtime, when TOS was null. That would be a bit messy, since you would need a way to discern the end of the string to parse (and it's un-Forthy, but such is life).
Is this what you mean? `: foo ( x|0 default -- x|default ) over 0= mux ;`
Btw, I was looking for a lazy Forth (or any other concatenative language) and didn't find one.
Actually, 8th does have the "l:" qualifier, which is in fact lazy evaluation of the following word. So "l: +" won't evaluate "+" until runtime (and will evaluate it each time that code path is entered), so it could evaluate to s:+ or n:+ for instance, depending on the items on the stack. That's not exactly what you're looking for, though.
Ok, as long as x is a cell then this should work and your `null?` returns a well-formed flag, this should work: `: foo ( x|null default -- x|default ) over null? mux ;` "Mux" is the common abbreviation for "multiplexer". A 2-1 multiplexer takes two inputs and selects bits from them according to a mask. `mux ( x1 x2 mask -- x )`, the resulting x is equivalent to ((~mask&amp;x1)|(mask&amp;x2)). As a primitive it can be very useful.
Yes, all items on the stack are pointers to the actual data of the item (including the information about the type it is). While mux'ing the addresses on the stack would work, for security reasons that's not allowed in 8th. Only valid address to items are allowed (and permitting an operation like 'mux' to work on stack addresses would create a security issue).
I see, yes, if such operations aren't permitted then other means will be required.
Would you my expanding on "security reasons"? In Able you can so this kind of thing perfectly safely; It can not cause a segfault or effect other tasks.
Sure. If you allow access to arbitrary memory, you're opening the door to all kinds of attacks: stack smashing, privilege escalation, etc. So 8th makes (or tries to make) that sort of access difficult, since it was designed to help me write applications which needed to be as secure as one could get with SW only.
Kotlin: What really drives home the lazy eval is the Elvis operator’s use in an early-return idiom: val x = maybeX ?: return
[Snigl](https://gitlab.com/sifoo/snigl) uses generic words that are dispatched on stack contents. It only checks arguments, since that's all it needs for choosing the right implementation; but incorrect stacks are mostly a thing of the past for me.
So what does that mean? If the programmer only put one thing on the stack and the word needs two things, what does snigl do?
Throws an error describing which word and what was on the stack. More interesting situations arise when the wrong values are on the stack, which is where the typed word dispatch really shines.
Never understood why anyone has trouble with I2C. Here's a video of someone doing it via manual bit-banging: https://www.youtube.com/watch?v=8ZYMrcHm91s
Yeah, that's the low end of the complexity spectrum :-). I've wished for a good logic analyzer when trying to drive a display at MHz speeds with ARM Cortex DMA interfaces. Looks like a neat project. Even neater to know it's got some Forth in it.
Is it your project? Or, how do you know it's got myForth in it? Is there some technical documentation or other info on the implementation somewhere?
I don’t see a perfect solution, but the [??](http://www.wilbaden.com/neil_bawd/ultque.txt) and [ORIF](http://www.wilbaden.com/neil_bawd/tool2002.txt) words defined by Wil Baden (aka Neil Bawd) might help: : ?? ( ... f "word" -- ... ) POSTPONE IF PARSE-NAME EVALUATE POSTPONE THEN ; IMMEDIATE : ANDALSO ( aka ANDIF ) ( C: -- orig ) ( 0 -- 0 | n -- ) POSTPONE DUP POSTPONE IF POSTPONE DROP ; IMMEDIATE : ORELSE ( aka ORIF ) ( C: -- orig ) ( 0 -- | n -- n ) POSTPONE DUP POSTPONE 0= POSTPONE IF POSTPONE DROP ; IMMEDIATE ( ... maybe-x ) DUP 0= ?? EXIT ( ... x ) \ keeps zero on stack when exiting ( ... maybe-x ) ORELSE EXIT THEN ( ... x ) \ drops zero on stack when exiting There really is too few of these little general nuggets in modern Forth, I think; at least it feels like there should be more of them (incl. in this case).
Looks rather like Haskell’s fromMaybe, then.
Not much code highlighting to do in Forth, usually, what with the user-defined control structures and parser extensions. At least not much more than in, say, Tcl or even POSIX sh. (The other side of “no syntax”.) As for the explanation... The original posting just describes a way of defining enumerations that makes them look like ML algebraic types. The “product” part of “algebraic” is missing, though,— all that’s proposed is a convention of having the sum tag on top of the fields. Unfortunately, if different terms (i.e. tags) have different numbers of fields, this means you don’t know what the depth of your stack is until you match on the tag, which is extremely inconvenient (\_e.g.\_, if you have two of these values on the stack and want to swap them, you’re in for a lot of pain).
This site might be of interest to forthrights who can read French. It has the manuals for the Hector HRX (1984) which was a French computer with Forth in Rom. The site also has issues of JEDI, a old French Forth newsletter.
PS. You have to register with the site to download the literature.
Ahhh l'Hector.... J'aurais tellement aimé en possédé un. (I really loved to own one when I was younger)
Glad to see Chuck is alive and well. I guess he must be alright to make a trip to Edinburgh for a conference. It's an interesting interview; thanks for sharing.
You don't know how bad you're winning at life 
You don't know how bad you're winning at life 
What do you mean by "swapless"? I don't understand what you're getting at.
The word `CREATE` parses the word it creates from input when it is executed.
Key word: when *executed*, not when compiled.
Oh, yeah that makes a lot more sense. Thanks!
If you want to hear about Chuck's adventure to Edinburgh, there's also [a video from SVFIG Nov 2018 ](https://youtu.be/xYWa2C2_7H0?t=8430)recently available. I've linked to his talk in the video, but it's also worth skipping back to Greg Bailey talk about GreenArrays at the beginning.
This has loads more features than I was expecting. Badass.
This is **very** impressive. You thinking about going into computer engineering at university? Good, now think about getting an advanced degree. 
Wow, you have no idea how much of a kick I get out of this! My first serious attempt at learning programming after TI-Basic was [Z80 Assembly for the TI-83+](https://www.ticalc.org/archives/files/fileinfo/328/32816.html). So this hit me with a wave of nostalgia! :D (Would this work on my old TI-83+ Silver Edition? Probably still in the attic of my parents somewhere) &gt; Add Z80 assembler in Forth (so ASM programs can be made!) What are your plans for approaching this? Some kind of macro that converts to raw opcodes?
Cool, thanks.
I took a look at this and have no idea what you are trying to get across. From the link: &gt; This is the formal specification of a language which I like to call a post-forth language for the time for a no swap forth which is the best name that I have to describe it at the moment. This is not a formal specification, more a stream of thoughts. &gt; A no swap forth is a type of forth which intends to operate using a stack and a global object instead of a swap. I'm not sure what this means. `swap` is just a word for reordering a couple of items on the stack. The use of objects on (or in addition to) the stack wouldn't have any impact I can see on the need (or lack thereof) for a word like `swap`. &gt; In forth data can be accessed by passing it around in a stack. In javascript this works like this. &gt; &gt; g.stack=[]; &gt; stack.push(3); &gt; stack.push(3); &gt; stack.push(stack.pop()+stack.pop()); &gt; &gt; This is similar to how I desire things to work in my own language except that there should be a global object inside of the forth and not a swap. I don't see anything here involving a `swap`. Later you have: &gt; g . foo . bar . bazz 2 @ &gt; &gt; What @ does in this specification is takes an object that is on the stack and stores it inside the object. &gt; &gt; This is the simple specification of the language. &gt; &gt; A way to write an addition in this language might look like this. &gt; &gt; g . foo . bar g . foo . baz + Ignoring the non-standard uses for `.` and `@`, there's nothing here that would appear to have anything to do with a `swap` function. Can you clarify what the actual point is?
I plan to add Forth words that can write the opcodes directly to RAM, so yes, it's kind of like a macro. I have no idea if it will work on the TI-83+. The models are very similar and I consulted various docs for the TI-83+ when I wrote this without any problem. Try it and see!
Won't you have issues with the raw addresses? You don't have the convenience of labels like in assembler
Oooh. I may have to look at this, I could potentially resurrect a Metroid clone I abandoned many a year ago. Could it be made to run on TI-83+?
You can write words that implement labels (by saving the current address of the code being generated, and placing it on the stack, for example)
Oh right, duh.
Comprehension check: You want to build a forth-like language where data values usually live in a hierarchically structured namespace (the global object and its subobjects), and are only brought out to the stack (by chasing pointers) just prior to use. And they're brought out in order, so that there isn't stack juggling. Do I understand you correctly?
Reminds me of [cixl.](https://github.com/TryItOnline/cixl)
I'm not surprised :) Cixl turned out to be more of a prototype, too complex and too many loose ends to go where I'm aiming.
I don't have one at hand, but it should work because I didn't write hardware-dependent code (the two models have different flash to RAM schemes and slightly different ports). Please test it out and report any bugs.
&gt; too complex and too many loose ends You know what they say: "Dude, suckin' at something is the first step to being sorta good at something."
Moving objects around by passing their struct onto the stack then storing them onto a struck further behind on the stack.
It is certainly a forth-like although I think if it is not compatible with ans/jones then it is a good dsl in forth. The thing I like about it is I prefer the c style where things are fetched from a global object however I like the forth style where to invoke a function is just to call the function and the simplest thing to do in the language is to call a function with no boilerplate.
The point is to use global objects in forth as a means of passing input and output between functions instead of swap rot etc. However keeping the paradigm of no boilerplate function calls.
Passing a pointer to their struct, you (probably) mean. OK, I thought you somehow meant to get rid of *swap*
The whole point of the stack is to have your arguments there, ready to be used. Here is a forth-like addition: + Now here is yours: g . foo . bar g . foo . baz + Now here is javascript: g.foo.bar + g.foo.baz Now here is lisp: (+ (bar (foo g)) (baz (foo g))) Can you see where I'm going with this? Furthermore you can still achieve the same code you are proposing in forth with variables/values and objects with OO extensions like gforth provides.
So you want an exposed stack for passing/returning data, but no way to order the data on it? Fetching &amp; storing from values in RAM only?
Ah. So you *can* have structs/objects persisting on the stack, in addition to being accessible via the global? This notion of folding one value into another is reminiscent of currying, and with some care you ought to be able to get closures too. Might be a fun way to program. 
No though you could do that I am more saying that I think that they are compatible but demonstrably separate ways to pass IO. I just don't have a better name for it than swapless forth because I am proposing you can pass with just structs and not swap. Though structs that work like the dictionary hopefully.
If OP does build this system and program in it, I suspect he'll find that keeping values on the stack is more necessary than he thought. At a minimum, something has to say what to retrieve from that global memory, so that something can't itself be hidden in said global memory. But if he wants to do it, hey. Some people juggle geese.
I think you would still want swap for things like addition and subtraction you are right. I think though that what I am aiming to probe at is a way of stack juggling which is not so intimidating for beginners and is not so inefficient if you don't feel like calculating a large stack juggle.
I see now based on another person's analysis swap would still be good for arithmetic. However I think I like this for storing hierarchical data in a named way and also for avoiding some larger stack juggling.
Yeah it reminds me of currying too. The reason I like forth is just because you can flash off a bunch of functions one after another with no boilerplate. I would like to make my programming work like i just name out the programs I want to run in order and have a syntax for soubroutines of that. But stack juggling is my nightmare and I am hoping I can alleviate it some way without adding too much bulk. My research in language design is about taking a system and removing the fat from it and then adding parts back in to try to make it easier to use. Then I play with it a bit and try to remove the fat again. The key to my algorithm for language design though is I keep trying to make it easier. What I have found is one of the best ways to make sure a language is ultimately easy is if it has full syntactic macros. You can get a repl/debugger going and type in a series of functions that will generate the compiled code you want and hand compile the parts you want. But with full syntactic macros that is easier. So for a while my research was just about finding a language with full syntactic macros then it became about filtering that down to a smaller and smaller thing which was forth. I don't actually like forth because of it's speed or size advantages. I personally thing it is the easiest programming language because even if you are a beginner programmer you can learn it all the way through in a short time and bloat it to all hell if you'd like. The thing is some of these die hards really like stack juggling by understanding the right thing to do in the right time I.e. the ideal combination of swap drop rot combined with not leaving extra things on the stack that don't need to be/factoring code/understanding the processor/etc. I want to hear out those people's advice because on my ideal pc the thing that has full macros should also be able to be the kernel of the machine. This is why I love forth so much is that the green arrays chip essentially is a forth or a series of forths at boot so this is in line with what my research aims for. However a huge part of my design in trying to make languages easier is to remove boilerplate. I was looking for boilerplateless languages i.e. lisp/forth. And I really like the forth way of doing things which is just to blare off subroutines one after the other with only a space or newline for separation. I think though that there may always be room for improvement and that for a new user some of these stack operations can be confusing and I want it to be easily approachable by someone who is new to begin with as well as try to be fast and efficient. It is a sort of optimization problem and it depends on where your tastes lie but you find your mix between speed and ease of use. I really lie more in the camp that a forth should be around 8 k to begin with and the rest of it should boot up. I think that it would not be so hard as people make it out to be to put some forth code on a wide range of processors but maybe I am mistaken. I really only care about syntax though in the end. I don't in my own research care much about the speed or efficiency of the thing unless I feel like I am getting away with something. Some ways to do things just make it so that your system is extra hard to use and extra slow. I would cite something like bash as an example of that. Bash is the best shell around probably but it is very difficult to use especially for a new user and it is very slow indeed. Part of why I was driven to forth after so many years of research is my original project was to try to find a way to fix bash or replace it because it was such a terrible shell and I wanted a better one. Forth I think blows bash out of the water. I would rather market forth as a shell and not as a programming language.
Note that unfortunately this will not work on WSL (Windows Subsystem for Linux). WSL does not yet support 32-bit binaries. See [this link](https://wpdev.uservoice.com/forums/266908-command-prompt-console-bash-on-ubuntu-on-windo/suggestions/13377507-please-add-32-bit-elf-support-to-the-kernel). It works fine in normal Ubuntu 16.04 64-bit.
Your post reminds me of http://yosefk.com/blog/c-as-an-intermediate-language.html where the author gets forth source listings working in gdb. &gt; Seriously, yay. We placed a breakpoint. We got a call stack – forth_main called countdown. And we've seen the Forth source code of the function we debug. All that in a debugger that has no idea about Forth.
Have you installed multilib?
I wonder if cooperative multi-tasking is faster than using native threads. 8th uses native threading, which automatically takes advantage of multiple cores; does Snigl also scale WRT cores?
Most definitely in this case, since a task switch is a single pointer update; switching CPU context is more involved. Not the fibers themselves, no; since that would make them preemptive rather than cooperative. But anything delegated to separate threads, like IO actions; will scale wrt cores. Which is part of the idea behind Snigl's approach, to build concurrency into the runtime rather than just throwing threads over the fence for user code to deal with.
Of course the CPU context gets switched anyway by the OS. So I wonder if you've got measurements to substantiate the performance? I understand that you switch tasks with a single pointer, but that's not necessarily faster than a CPU context switch -- depending on a whole pile of factors...
No it doesn't, not as long as I'm not switching OS thread. Snigl's fibers are completely virtual, they have nothing to do with the C stack. But it is according to my experience, and common sense; since that pointer update can't possibly generate a page full of assembly. Look, I get it; you're going to get famous and rich by selling your language, so far so good. But it's getting VERY tiresome to deal with these sneaky car salesman back stabbing tactics of yours. You're not honest, and you're not aiming for truth except where it potentially increases profits. I'd suggest focusing more on creating a better language, finding truth, sharing and learning; and less on awesome profits. Take it or leave it. I have nothing more to say to you.
Umm... I don't know what I did to elicit that response. But, OK. I'm only trying to understand if the model you're using is more performant than the one I'm using. No FUD, and certainly no "back-stabbing". And why are you attacking my honesty, of all things? I only asked for measurements, since you can't really know unless you measure. That's *my* experience. For the record: neither rich, nor famous, nor likely to become either. But again, OK; I thought I was and am focusing on creating a better language.
Who uses This, That, and Them? That's absurd. I don't understand why people lean so heavily on single-word names, when a PascalCase short description would work just as well or better. I dunno about all of you, but if I wrote so tersely in any of my Forth projects, I would forget what it was doing if I left it for a week.
I made several famillies: The `sprout` familly with `dup over pick` . The `seed` familly with `tuck` . The `promote` familly with `swap rot roll` . The `demote` familly with `swap -rot` . The `erase` familly with `drop nip`. 
Well, good for you that you know your strengths and weaknesses, but this kind of comment is not constructive at all.
Modernizing? I don't get it. What instruction mnemonics are "modern" in your estimation? Whether they be for stack or register machine instruction sets.
I always had a soft spot for `larry`, `curly` and `moe`... Pick your fights, the standard names are more descriptive and come with a ton of context and expectations.
Some prefer Chuck style names for the return stack operators: * `push` instead of `&gt;r` * `pop` instead of `r&gt;` * `top` instead of `r@` * `copy` instead of `dup &gt;r` Here are some of the more exotic stack operators: * `xchg ( x1 -- x2 )( r: x2 -- x1 )` * `spin ( x y z -- z y x )` * `swish ( x y z -- y x z )`, some call this `under-swap` * `lift ( x y -- x x y )` * `ddec ( x -- x-1 x )` * `dinc ( x -- x+1 x )` * `+dup ( -n|+n -- -n|{+n +n} )` * `-dup ( +n|-n -- +n|{-n -n} )`
It would be interesting if there was a C compiler written in forth which could be used to build something like tcc or gcc off of something as tiny as jonesforth
Good point. I am writing another article to address this issue. Thanks for your feedback.
I don't have a problem with names like `dup` and `over`. There are a few renamings in mine; e.g., pop for R&gt; push for &gt;R dup-pair for 2DUP drop-pair for 2DROP And there are some I don't have at all: R@ PICK ROLL -ROT 2OVER 2SWAP 2&gt;R 2R&gt; 2R@ 2ROT 
&gt; Who uses This, That, and Them? That's absurd. What's wrong with absurd? Regarding the length of names: The general consensus (if there is such a thing) in the programming community seems to be that commonly used names may well be short since everyone knows what they mean anyway, while rarely used names (and in particular application-specific names) should be longer and more descriptive. Since many of the stack operations are very common, there should not be a problem with them being short. If Forth is special in this regard, then it's by having users which favor short and somewhat cryptic names much more than other language communities. Also, I feel like I don't quite get the point of your comment because I don't see how `them` is worse than `2dup` in that regard.
I like to think that it was a (minor) mistake to focus the name of those operations on how they mutate the stack. For me, Forth code should read much more like natural languages than other programming languages do. So for example, a phrase like `them =` asks whether the two values currently being talked about are equal, while `2dup =` says that we should copy two values on the stack and then compare them for equality. The difference is one of describing what is being done vs. how it is being done. This certainly does not make a significant difference, but I feel it might still be worth thinking about it. 
&gt; because it makes code read less imperative Forth is an imperative/procedural language, why do you want to hide it? Sure, we can do cute things with immediate words and other tricks, but that's best used to present i.e. config files which can be directly interpreted and that don't look backwards (pun intended) to non-technical users, than to show off to other programmers. 
The stack manipulation words are inherently about the low level "how". The data stack operations on a (real or virtual) stack machine are analogous to register file operations on a register machine. You wouldn't rename registers to "this, that, and them" at the level of the processor instruction set. It's the higher-level words that you define for your application that layer on the "what" over the "how".
&gt; Forth is an imperative/procedural language, why do you want to hide it? There is nothing wrong with imperative code, but functional concatenative languages (Joy and friends) have taught us that stack-based code can be seen as either imperative or functional, depending on which is more convenient in a situation. Choosing more neutral names makes it easier to switch the point of view for me. I don't think anything is really being hidden here, do you?
&gt; The stack manipulation words are inherently about the low level "how". I think the example I gave shows that they don't always need to be. Very often, they certainly are, though. &gt; You wouldn't rename registers to "this, that, and them" at the level of the processor instruction set. Why not? Sounds fun to me. :) I think Assembly languages would certainly benefit from improving usability in various ways. &gt; It's the higher-level words that you define for your application that layer on the "what" over the "how". That's the theory, but I've never ever seen Forth code that does not use stack shuffling all over the place, and I don't even believe it can exist generally.
I'm with op: there's probably nothing for us to see here 😆
Those are very interesting. Where do they come from? Can I find them in use somewhere?
I think those names are interesting. I could see them being useful synonyms. Whether this seems interesting or not may depend in how deeply ingrained the old names are. In lisp, sometimes people hate on the car/cdr system of functions. They prefer first, second, etc. But those are far less expressive. The compromise has been to include both in the language for situational preferences. 
I renamed `car` and `cdr` in Lisp to `hd` ("head") and `tl` ("tail"). It's really nice.
But what did you do with `cadr` `cdar` `cadar`? The whole family is disgustingly useful for making data structures out of lists, you have to keep it in. Hence synonyms. 
&gt; jonesforth or similar Forth implementations primarily aim to &gt; implement a program (stack machine) to execute Forth words, &gt; with an architecture that is independent and unrelated to GNU &gt; C Compiler. This makes sense: they're not related at all, so there's no need to cater to GCC. &gt; I am thinking of creating a stack machine, similar to Forth of &gt; course, that is based on GCC, where the main novelty would be &gt; to use EAX (RAX etc.) as top of stack (TOS), so as to reduce &gt; memory access using variables. Use of EAX isn't exactly uncommon. In old versions (4-9) of my Forth, I cached TOS in EAX and used ESI as a stack pointer. It worked pretty well. &gt; To describe it in an abstract manner, Forth has its own way of &gt; implementing a stack machine. Alternatively, a stack machine &gt; can also be implemented using GCC, which would need to incorporate &gt; various C conventions, such as returning function value in EAX. This is where it will get hairy. EAX is clobbered by return values, so your Forth will need to preserve this (and other registers) prior to invoking the C environment, then restore them upon returning to Forth. &gt; Hand tune optimization? &gt; &gt; If we look at se.s (gcc -S se.c) lines 52 to 61, we can easily &gt; spot that lines 53 and 54 are redundant. Removing them may result &gt; in 25% time saving, at least theoretically!! This is what we hope &gt; to achieve by making EAX=TOS. Do you know what the two lines in question are actually doing? They're not completely redundant. I'm also curious about the source of your 25% time savings determination. Does this include calculating overhead involved in saving/restoring registers when transitioning between C and Forth? This is also x86 specific. What will you do on ARM, MIPS32, RISC-V, or other targets with different ABI's and a different set of registers? 
If you really need those, you could use a similar abbreviation scheme with `hd` and `tl`.
Hard to remember specifically, from various Forths and Forth publications over the years. 
Might be this? http://www.forth.org/svfig/Len/bits.htm
Having true as all bits set (and false as all bits reset) is very useful for using the truth value as a mask for subsequent bitwise operations. 
That one's pretty cool, but I remember there being examples of combining multiple conditionals together with bitmasks somehow, which isn't in that one. That's a nice link, though, thanks for posting it!
There's a section in Thinking Forth on using booleans as hybrid values.
In Reva I had all-bits-set be "true", but in 8th there's an actual value of "true", and any non-zero number evaluates as equivalent to "true". I find it's easier on my brain to semantically distinguish between a true value and a set of bits, keeping boolean operations distinct from bitmask operations.
In my own implementation I'm experimenting with ways of doing it. That triggered my memory of this article I can't find that had some ostensibly compelling reason for -1. Wish I could find it!
The only compelling reason I can think of is the bitmask one, e.g. that "true" and/or whatever will work if 'true' is -1.
In languages that don't use -1 as true, you end up having a separate sets of operators for logical and bitwise operations, while in Forth you can use same operators for both bitwise and logical operations, e.g. C has `&amp;&amp;`, `||`, and `!` for logical and, or, and not, as well as `&amp;`, `|`, and `~` as their bitwise analogues.
Yep. Two ways: with a tagged string using "s:tsub", e.g.: {"joe":"mama"} "Hi %joe%" s:tsub That gives "Hi mama". Similarly, using "s:strfmt" which is very similar to printf(): "mama" "Hi %s" s:strfmt
Good questions and comments. There are many related issues which I will address in an article to be published soon. To summarize, GCC optimizer is actually a stack machine. RTL is derived from LISP, which in turn is equivalent to Forth. The bigger goal of this exercise is to open up the black art of compiler optimization to mortals, and to make Forth interesting to younger C programmers.
The closest I've come to summing up the experience of being in a state of flow with Forth is how it feels to play a musical instrument.
Please don't muck around with the core. We're fragmented enough as it is.
Would you have told that to Chuck when he invented ColorForth? (Bonus question: What could have been his answer?)
No, of course not. He develops Forth on his own because he works in a very specific domain - chip design, and the perfection of Forth. And his goal is to be able to do it practically by himself. I don't understand why Fothers still ignore this and hold everyone to the same parameters. Some of us how found Forth useful for other things and also find it useful to be able to share code and collaborate. Some of us are interested in establishing reasonable standards as a way of progressing Forth. That's why I (somewhat cheekily) implored you not to change Forth - obviously you can't and that's the joke. It got lost in translation. Chuck would probably agree with me. Some people want the freedom to do things from the ground up. And others want to be able to do things that are different from what he's doing and are willing to give up some simplicity in order to do that. People interpret his comparisons with himself and other people always as criticism, but I think he's smarter than that. I am a huge proponent of simplicity for the record and I bemoan the complexity of the larger programming world the same way as Chuck does, but my application domain and big-picture goals are totally different. For context, see my work in Forth, a game engine: https://twitter.com/RamenEngine
I remember this company, I used one of there boards for a school robotics project a few years ago. The board was called ServoPod running IsoMax Forth, here's a video of the project I did with it. [https://youtu.be/SuUweLR9zpU](https://youtu.be/SuUweLR9zpU) and the source: [https://gist.github.com/tianic/11190746](https://gist.github.com/tianic/11190746) While I was working at Reveal Imaging, then owned by Leidos/SAIC they used to use those boards on the belt control system of a CT scanner because my dad programmed them all in Forth. It was a great way to control servo motors and extremely simple to program. They got rid of them after a bunch of guys from management went to meet the founder of New Micros to see if they could place a bulk order of boards. I heard when they returned from their trip they said "we're never doing business with them again, use a different board". So they switched to an arm chip running MPE forth. I heard the founder was a bit eccentric and they weren't able to get any production guarantees from him, it's a shame because it was such a great board and IsoMax Forth was great for multitasking servo motors.
I like having the symbols for primitives, that's a good design choice. It's interesting to have only the one "passive" word ';', as one thing that's missing are the traditional comments (which would require another passively run word, '('. Biological systems need comments too (though they're probably more like #if 0 commenting out code, than actual descriptive comments :). 
Right, stack comments like ( n1 -- n2 ) would be nice. The word ( could enter another passive mode which only recognizes ) which exits the new passive mode. Maybe you could also just ignore all words which aren't part of the program, so for example you could always define words in upper case but write all comments in lower case. But it's nice to clear the data stack by typing as;dfjasdfjasdf &amp;#x200B; An advantage to the one stack is that you can produce infinite loops without overflowing the return stack. A simple case you can test is : A B ; : B C ; : C A ; This will constantly cycle through executing A B C forever with no overflow. In a normal Forth with a return stack you could probably get clever with r&gt; DROP but it's more similar to the way ColorForth compiles words followed by ; as jumps which do not store the return address. This is probably the best of both worlds but doesn't feel like something that would happen on a biological system ie. you only have one strand of DNA.
Thanks for your detailed reply! I know about the Ramen engine, great work. :) I'm not sure if a standard can even be significantly better than the Forth standard. Do you think the problems it suffers from could be avoided in a standard? I'm not particularly interested in sharing code (although I see the value that it has, but I've also seen enough of the problems it causes). I'm interested in whether a stack-based systems programming language can be implemented in a way that incorporates what we have learned about programming language design in the last decades and thus won't be generally associated with retrocomputing. If I have to loose "Forthness" in the process, then so be it, although I'm trying hard not to give in to fashions like local variables or even quotations/closures.
I use locals. I say if it you've got it and it helps, use it. Locals for sure have saved me a ton of time. But look at the source, you'll see 90% of my definitions are short one-liners, so it's not something you have to adopt wholesale. Forthness to me is the spirit of innovation and Libertarian-esque individual sovereignty. My take on modern practices is, Forth as it is lets you do a whole lot of it just by understanding how the practices work in principle and then applying them in the language. For instance I just added a form of events and event listeners, but I did it in like 10 lines of code and it satisfies all my current requirements. That's what Forth is all about. Not thinking too much into the future, but dealing with things right now, efficiently and consciously. My main problem with the standard is it tries to cater to too wide a user base. Embedded and desktop applications supposedly need to be covered, but I think that's dumb.
Sometimes I `*` or `AND` something such as an address, an offset, or a character with a flag to make it exist or not exist. This is how I avoid most conditionals. For example, count only numeric characters received from the keyboard by `AND`'ing the flag with increment step. : count# 0 begin key $30 $39 within 1 and + cr .s again ; I use this as a way to get forthier and more [composable parser combinator style pattern matching](http://theorangeduck.com/page/you-could-have-invented-parser-combinators) in forth without conditionals and lots of off-line parsing (it's more algebraic and factorable).
Seeing a matrix and graph theory features is super nice. Any sort of real work with matrices needs a sparse matrix feature, inversion, triangulation...etc. A lot of scientific software makes use of matrix math. Being able to create simple graphs and get the shortest path between nodes and print graphs is also highly useful in my day work. Good to see you have users with similar needs or you have similar tastes.
Thanks. Sparse matrices and arrays are on the todo-list, but I'm not sure how quickly they'll get done. You can emulate them with arrays of arrays, sort of; but since arrays aren't sparse that's only a sorta-kinda solution.
Pretty cool but I don't see the big deal about RISC chips in the first place. Also don't see what this has to do with Forth.
What types of processors do you prefer? 
Complex instruction sets are fun; I've never felt like x86 was hard to program assembly on, although I wish it did a few things slightly differently. I'd also like to see Transport-triggered architectures get more attention. If I could have it my way, I'd want a processor that was some stack-based processor that still had arbitrary positional access into the stack, not unlike a stack frame.
So you would want instructions that allow operands relative to the stack pointer, as opposed to, or in addition to, relative to the frame pointer? 
Both would be preferable. My blob Forth has a frame, but I still end up shuffling things on the data stack a little bit regardless.
Absolute wall of text. What even is the topic?
The FRISC 3 and SF1 stack processors allowed for arbitrary item access of their on chip stacks. They are obsolete however. https://users.ece.cmu.edu/~koopman/stack_computers/chap5.html
Interesting. Thanks! 
The deal with RISC processors is that originally almost all processors were CISC before the 80's, because memory and was constrained on earlier systems, so coding in assembly was required far more at the time. However as processors got faster, and memories got larger, doing most programming in compiled languages was far more practical. Studies were done on instruction selection in compiled programs and it was discovered that only a small number of the instructions were used. The idea with RISC is that by having almost all instructions be simple the same size, the opcode decoder in the chip could be much simpler and faster and each instruction would execute mostly once per cycle, as opposed to the many clock cycles for CISC instructions. Especially CISC instructions that did complicated tasks like move blocks of data around in memory. Sure you need more instructions and space to do the same task in a RISC processor, but the idea was that the simplification of the processor would compensate for that allowing it to go faster. Is that the deal you were wondering about? 
I can read wikipedia too. I guess I was just wondering why there's such a resistance against a variable-length instruction encoding, or slightly more expressive instruction set. Any assembly I write for ARM, as an example, is somewhat tedious to do.
Because of the things I mentioned above. RISC assembly code isn't meant to be written manually. It is meant to be compiled. 
Lisp is not a stack machine.
I find eForth is really an excellent entry point into understanding what is under the hood in Forth and why. ForthonArdunio has links to a Github for the assembly code and a complete explanation. AmForth is overly complex for the same processor. The comparison of the two may be educational as to how some code teaches while other code leads astray.
I know I've seen AmForth, where did you find eforth for the Arduino?
Google "Forth on Arduino" at Arduino Playground and there are links to it, along with others. It listed as 328eForth. C. H. Ting, the developeer of eForth is an elderly academic at Berkeley, California and has written a lot on learning Forth.. The documents are for compiling in Atmel Studio 4, but we are now up to Atmel Studio 7 and a few items need to be adapted. Search for "eForth Overview" by C.H. Ting for a full explanation of a DOS version in a pdf. DRuffer/328eForth at Github may also local the files.
i think arduino's value is its easy to use feature, and the framework it provided, like loop start, etc does any forth could provide features like this? i think they could follow arduino's official bootloader, and append a minimal forth enviroment in it. then provide the start, loop frameworks too. to reduce the difficult of people playing with it
Hey, jyf, just a quick heads-up: **enviroment** is actually spelled **environment**. You can remember it by **n before the m**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
hEy, JyF, jUsT A QuIcK HeAdS-Up: **EnViRoMeNt** iS AcTuAlLy sPeLlEd **eNvIrOnMeNt**. YoU CaN ReMeMbEr iT By **n bEfOrE ThE M**. hAvE A NiCe dAy! ^^^^tHe ^^^^pArEnT ^^^^CoMmEnTeR ^^^^CaN ^^^^RePlY ^^^^WiTh ^^^^'DeLeTe' ^^^^To ^^^^dElEtE ^^^^ThIs ^^^^cOmMeNt.
I like this. Keep it up!
Forth conflicts with Arduino's bootloader. So that feature is removed. The big advantage with Forth on the Atmega328 is that you actually will learn to fully use the hardware. And you learn interactively. Arduino coding simplifies and hides a lot. Users simply cut and paste examples from libraries. Starting a loop is very basic programing. Sadly, it's a big deal in Arduino. I guess you are stuck in Arduino code.
Bad bot should die.
C. H. Ting's "eForth Introduction" covers this is a downloadable pdf. Essentially C. H. Ting created a version of Forth with only 31 primitive commands written in assembler. The rest is Forth written in Forth. While he tends to ramble a bit, his presentations are complete, and free on line. The example is for a DOS based Forth.
Someone once created the universal language Esperanto. It was intended to be the one language everyone could use at any place in the world. That too was considered a missing link. But. All that Esperanto really achieved was a language that nobody knew well and nobody understood the need for it. Where are you going? Forth is extremely fast and close to the hardware, so it's an optimal language for new CPUs, before you have written an OS or for tiny CPUs that don't require an OS. NASA still relieve heavily on Forth. C has proven to be an optimal language to write an OS. Linux and Unix are primarily written in C and C has been easier to identify and remove bugs that C++ or C## with all the OOP additions. Linus Torvalds insists Linux continue to be written in C for the sake of quality control. And Linux has grown from an original 1 million lines of code to over 5 million. Perhaps you should forget about missing links. 
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
As explained here: https://www.reddit.com/r/ProgrammingLanguages/comments/a7llui/what_is_not_a_stack_machine/ The main motivation lies in mobile GPU cryptocurrency mining (and other GPU applications). By rejecting OpenCL, Google has caused a huge unfortunate side effect -- tens of millions if not hundred of millions of Android mobile phone with GPU effectively become underutilized. SMOCL is a project to bridge the gap using stack machine methodologies, to utilize the hundred of millions of GPU in Android phones, especially for cryptocurrency mining. SMOCL will provide the much needed Forth-like interactive development enviroment to develop cryptocurrency mining programs ON THE MOBILE DEVICE ITSELF, as well as potential novel optimization schemes, as described below.
On Esperato: https://www.reddit.com/r/ProgrammingLanguages/comments/a7llui/what_is_not_a_stack_machine/ec4vka8/ Perhaps national languages killing local dialects will be a better analogy -- which has happened throughout mankind's history. You still speak local dialects when needed, but use a national language as interface to communicate with people speaking other dialects. The simplest use case would be a web page, which typically requires PHP, JavaScript, MySQL, HTML, CSS. ....
TI-83, which someone just "Forthified" recently .
It's rare I see one that shouldn't, really. 
It's a tree machine, maybe. 
Forth's power to program interactively is quite appeals, especially when it uses so little code to deploy. But I don't see how borrowing others GPUs to support cryptocurrency mining is a great security feature for my cellphone. 
Forth interactive development is precisely the feature that gives YOU THE OWNER, to monitor and control the level of mining or other computation, which is impossible using other programming languages. You CAN add your own security code as you wish, as Forth code is transparent to everyone. It was C code compilation which created the obfucated code culture that is now the norm, which Forth can overcome. 
sorry i dont agree with you . arduino hides a lot which helps lots of its fans to ignore these details they dont want to know. of course to know that is better, but for most of them, they just want to read a simple temperature value and log it or send over the internet. just it. forth is simple too, dont let the detail trivial cancel it
I just read some of the eforth stuff. That's some neat stuff. Thanks!
I've been struggling with AVRdude and my Pololu programmer to get the binary loaded. But I feel very close to success. If you use AVRdude, get the latest version v6.3 and the latest pdf document that goes with it. YouTube howtos, vendor tutorials, and so on are not helpful. I'll post here my route to success.
In computers, those details which are specific to the chip architecture are really important to mastering the whole subject of programming. Otherwise, you are just reading advertising and buying updates.
Forth is great for verification that hardware its broken or wrongly wired. But, the Russians were exploring GPU exploits to hack encryption.
That is great news. Any link?
Can this convert Forth code to OpenCL? Not sure why you would want to write code on a mobile phone, it was difficult enough typing this comment.
It doesn't look like it has a parser, just some additional functions and wrappers for using a stack within a C+OpenCL application as illustrated by the `main()` function: int main(void) { int *A, *B; // SMOCL(); SMOCL1a(&amp;A, &amp;B); // 2 in 2 out? put both on stack? // f_malloc, f_fill 0 to 1024?, then dup array? or use clVect? // need new function, fill array from 0 to N // then next function need to transfrom clVect to array? // head = f_(head, &amp;A, &amp;B); need parameters for malloc SMOCL1b(A, B); // head = f_(head); no need parameters, already on stack FD(); } Coding on a mobile phone isn't easy, but my kids seem to do ok with it. One of them has been writing a small game in Python, using SceneKit via a module allowing bridging to Objective-C. I have a harder time without a physical keyboard, but over the last few months have learned to use the virtual one on an iPad enough that I can write modest length programs in Forth, C, and Python using it.
How so?
Well, I got eForth installed using AVRdude 6.3 and a Pololu programmer. I had lots of difficulty because I purchased a new Arduino Uno board with a defective 328p. Finally, I tried my Arduino Nano and everything went smoothly. The defective chip may have been my own fault as I used battery power while programming. A low voltage condition while programming can ruin the Flash. I stated over with the Nano powered by a steady 7.5 VDC supply. +++++ The first time is the hardest as you miss details.
Mobile GPU is literally the current last frontier of Computing. We don't know how many untapprd PFlops exist out there. This is only going to continue to grow day by day. How do you know if there is no link to what you said?
Sparse matrices would be great, but I recognize that it would be a lot of work to do the research, capture the edge-cases, and do testing. I'd also be curious how a forth like 8th would represent the sparse matrix data structure. I'm just a forth newbie, but I think you'd want to implement the core structure as close to the metal as possible as you typically don't want this feature unless you're trying to do scientific work on a large network where performance is desired. There are probably some implementations to look at like what they do in Python's Numpy or Julia's Sparse Array libraries. https://docs.julialang.org/en/v1/stdlib/SparseArrays/index.html https://medium.com/@jmaxg3/101-ways-to-store-a-sparse-matrix-c7f2bf15a229
Remarkable as it may seem, I occasionally rely on human memory rather that search engines. The Russians hacking GPUs is a bit dated, maybe a concern in about 2008.
I can't see why anyone would install Linux as a virtual OS inside of Windows. You make this whole project convoluted and distracted. By the way, eForth by C. H. Ting has long been ANSForth compliant in 8, 16, 32, and 64 bit binary schemes which you might compile natively in Windows. Since it has minimal primitives (merely 31), and the rest of Forth is generated in Forth; it's the simplest, most direct deployment of Forth. Also, reliability ports to different architectures. GForth in Linux is adequate and well maintained. And it can be booted from a LiveCD or USB stick, thus leaving your Windows installation intact.
eForth by C. H. Ting is likely to be your best solution. Minimal assembler code makes deployment of a full Forth solution very easy He has written 32 bit versions.
What things are you referring to?
Russian hackers were all excited about using Nvidia GPUs to break encryption. I don't know if they ever got anywhere with the idea.
Why not use the Bluetooth interface to a real wireless keyboard?
Two RISC design choices where simply a result of implementation issues: 1. Fixed operation-ocde lenght: Choosen to ease pipeline and decoder implementation. 2. Load/store Concept: At first introduced to limit or bypass cache requirements (see ARM) Anyhow; Experience shows that both design choices are not such important in terms of performance as microarchitectural optimizations which eventually lead to the development of superscalar out-of-order architectures. As the performance of such processors are more or less independent of the ISA as much as inherently energy ineffective, there is no real advantage of a RISC design against CISC, other than larger code size and cache usage (that's my opinion of course). Implementation wise, a RISC CPU limit the required implementation complexity of course. On the other wise an out-of-order processor is very, very complex in any case...
The proliferation of unnecessary "modern" programming languages with bloated features dumb down younger programmers -- they have to much rubbish to learn that they have no chance of identifying important problems to solve -- like mobile GPU programming -- which is intentionally made more difficult to program by major vendors. We need to spent more time to think how Forth can help overcome this bottleneck.
I do use a physical keyboard (using a lightning to usb adapter) when at work or my home office. But it's not really portable. Due to severe RSI issues I use only split, ergonomic keyboards; I've not found a Bluetooth ergo keyboard that I like yet.
Hi: thanks for the info on Red and Rebol; another person also suggested something like that to me. Do you mind posting on the 8th forum? I mean, our discussion is interesting to the two of us, but it's probably less interested to the /r/Forth community at large. I'm trying to think about exactly how 8th would represent the sparse matrix internally. So far, in truth, nobody's been asking for sparse matrices but I know it's just a matter of time.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programminglanguages] [Meta Stack Machine vs. Meta State Machine](https://www.reddit.com/r/ProgrammingLanguages/comments/a8mg3q/meta_stack_machine_vs_meta_state_machine/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I try to focus on assembler, Forth, and C. I'm on interested in OOP as it seems to be a proprietary ploy. Python seems useful if you have a full formal OS, but my interest lie in programming microcontrollers. These days, nobody really learns CPU architecture deeply as an entry-level.
Please have a look at my latest post. I should call it "stack machine metaprogramming" -- it seems that stack machine IS the solution to metaprogramming all along, but a search on literature on metaprogramming shows that few people realize that! [https://www.reddit.com/r/Forth/comments/a8mb3m/meta\_stack\_machine\_vs\_meta\_state\_machine/](https://www.reddit.com/r/Forth/comments/a8mb3m/meta_stack_machine_vs_meta_state_machine/)
&amp;#x200B; Perhaps I should call it "stack machine metaprogramming" -- it seems that stack machine IS the solution to metaprogramming all along, but a search on literature on metaprogramming shows that few people realize that! 
Frankly. This is r/forth, not r/stack machine. I'm finding most posters herein aren't really interested in Forth. Do I desire to read all this? nah.
"Then the question arise: Which representation of stack in C is optimal?" In terms of performance, an array.
Seems there's lots of way to do this, since it basically comes down to how `create` works and how clever you can be. I'd try the examples if I could get jonesforth to work on my ubuntu VM.
You should post a full write-up if you can. This sounds so cool!
I'm just beginning to use eForth on the e328. I'm not sure how to add much to C. H. Ting's material except to clairfy, completed projects will have to loaded via serial port using the "turnkey project" procedures, and then saved or included in an extension of the original .asm code. Right now, I learning to manipulate I/O for each port, SPI, I2C, and A/D conversions. That's all explained by C. H. Ting. He even has a book on Amazon for sale. But I'm pretty sure the .pdf download covers the same and is free.
Thanks. That's given me something to follow up on. My google-fu is pretty weak lol.
My goal jas always been to convert unused Arduino devices to Forth. My first success was the Arduino Nano with a Pololu avrispv2 programmer via Avrdude in Linux. I can compile .hex files in Atmelb Studio 7, but have had to use Linux to load. Finally, I got my Arduino Uno board workingvin Forth. But I had to pull the Atmega328p, set up direct programming on a breadboard, then re-insert the chip back into the UNO board. The ArduinoUno board is important to get working as I have a pile of shields that work with it. The NANO is harder to adapt. Why all the trouble? I had the AVRdude bitrate parameter too low. Adding -B 16.0 seemed to solve weeks of frustration. Suddenly the chip signature was confirmed, and everything marched nicely through reprogramming. Ask me for help if you can't seem to load eforth.
P.S. Maybe this sort of question pisses people off, but if I am going to start a marathon, I'd like to be sure that I start in the marathon I want to run. 
The question s/b: "what do I want to do with the Forth"?
[removed]
At some point, if you're really serious about getting into Forth, you'll want to make your own implementation. A great opportunity to learn all about a particular hardware, assembly, and the innards of Forth; and then it will be exactly as you need it. This is in line with Chuck Moore's development philosophy (and why he was adamantly opposed to "Forth standards" like ANS. Until then, to get your feet wet in Forth programming, it doesn't really matter. Maybe GNU Forth (gforth) because it's available everywhere and reasonably consistent. Just know that you'll understand the whole system a lot better when you see it from the inside. `CREATE` `DOES&gt;` is like magic until you see under the hood.
AS a long time FORTHer move on this language is DEAD!
I am new. I have had some fun recently following this paper, https://hackaday.com/2017/01/27/forth-the-hackers-language/ I have found that you need a bit more than the article suggests. You also need a 3.3V serial adapter (look up 'DSD TECH 2PCS USB to TTL Serial Adapter with CP2102' on amazon). Ideally, you would also want a soldering iron and and a breadboard. Forth is a different way of looking at systems than the mainstream approach. The forth system is your whole OS. I have had some fun with Mecrisp-Stellaris so far. 
But we cannot tell you why you want to run a marathon. I think that learning Forth is a mind opening experience, more than a practical thing to do. It will teach you a different programming paradigm and will make you a better programmer. And it is fun. It can also be used for useful stuff, but you could probably use Python for most of that. As an exception, Forth and similar languages can be very useful when your hardware resources are very limited (e.g. microcontrollers). If you just want to learn, I would start with Leo Brodie's books and any implementation you can easily get. Once you have a good grasp of the general concepts, finding the right implementation for whatever you want to do or writing your own one will be easy.
As a long time Forthwright, I disagree. It's a great time to use Forth. There are active communities (here, the #forth irc channel, there are a fair number of forth users on mastodon and twitter, and [to a lesser degree] the comp.lang.forth newsgroup) where one can discuss ideas and get help. There are also sites like http://theforth.net and http://soton.mpeforth.com/flag/ which are helping to gather libraries of useful code to help as a starting point. Plenty of implementations exist, both traditional and newer ones that bring interesting ideas and different approaches (colorforth, retro, 8th, and oforth). Also languages influenced by Forth (e.g., Factor, Cat). And new dialects spring up frequently. It's not hard to write a Forth, and many people find it rewarding to build their own implementation tailored to their needs. In the real world, it's still a useful language. I work with multiple applications written either completely or partially in Forth every day. It's still used for space applications, and embedded systems. Forth is far from dead. It may not be popular, but it's still used and evolving.
Hmmmm, I don't know about that. If it were dead, why are we here? If it were dead, why is it still being used in dark corners of ancient servers/embedded applications? Forth intrigues me, but I don't have the time to become a master, but I am willing to become a specialist...
&gt; you'll want to make your own implementation. Yes, I think that sounds like exactly what I want to do. I know a bit about hardware and assembly, so I am thinking that Forth won't be too far above me. I am in need of making some lightweight crossplatform cloud-connected custom devices, and I am wondering if I found my answer. 
Maybe a better question, "Who is the Forth?"
Well, you can use python for nearly everything...which is why I use it. However, for some of the applications I will be working on in the future, python isn't quite the best fit. Hence, Forth...it seems as though, from a glance...it could work for my needs because I would need to have the ability to create my own crossplatforms, working between low-level and high-level coding. Concurrency, networking (inter-networking), integration with embedded systems...the works...and it has to be as lightweight as possible...
Awesome, thanks. I've done some academic work with microcontrollers, so I'm a little surprised I never heard of Forth, but then, I never did run across a group of people that were as serious as I am about this type of stuff. Thanks for the link, I'll definitely check it out.
Seems to be closer to what I am seeing... &amp;#x200B; Forth was written for space telescopes a long time ago...and seems excessively lightweight and portable... &amp;#x200B; Now, I'm no assembly guru, but at a glance, Forth seems like the python of assembly to me...relatively easy to learn and plenty powerful under the hood due to it being an interpreted language. 
You can go to low level using it to comunicate with hardware or high level, using it to find new descriptions of problems. Forth have all the levels in the same time. 
Actually, Python is run by a stack machine interpreter. Forth is oldest surviving stack machine with most versatile implementations. So programming language developers invariably draw expertise from Forth, including Python. Have a look at this article. It propose SMOPINT (stack machine opcode interchange) -- a platform for different programming languages to share and execute common opcode: [https://github.com/udexon/5CSM/tree/master/SMOCL](https://github.com/udexon/5CSM/tree/master/SMOCL) &amp;#x200B;