If there are any new developers coming to Forth, I think they're coming from more from maker communities and embedded/hardware people. Kids into software are much about web/mobile apps, backend/frontend, JavaScript/Java/C#. Not much overlap with Forth.
I have a hard time seeing how Forth could ever appeal to that group. Forth is small and self-reliant, which is the exact opposite of JavaScript and Python.
I think it's part miscommunication. Many people in /r/Forth have a shared understanding about Forth. You don't seem to have this understanding, and it doesn't appear you have made much progress since you first came here. If anyone is talking to you as you were an idiot, I would assume they're actually trying to be patient and help you.
Thank you for explaining this. This is not a thing I've run into before, so I took the words at face value. For the future, I'd advise exercising some caution when using memes with those who you aren't sure are familiar with them as it's easy to take something like this in the wrong way.
From Marcel Hendrix' online edition: http://home.iae.nl/users/mhx/sf.html
I agree. I've take caution not to use that phrase or phrases like it again.
/u/dylund goes a little out of their way to explain how dumb I am so I don't like that person. The way I see it I may not be some low level forth programmer but that's okay. I program forth the way I see fit to program forth and that is not misguided or wrong.
i am a backend developer, i am interesting of forth cause i treat it as a thin abstract layer on host, there's many domain specific language in web developing, for eg, the template language, no-sql storage, the famous redis project even embeded a lua for providing complex logic, such could be forth's domain i think
We don't have any formal rules for the Forth subreddit. But if there were, I believe this would be considered off-topic.
This is my take on how a Forth kernel in shell script might work: https://github.com/larsbrinkhoff/lbForth/blob/master/targets/shell/forth.sh
I love Forth and I figured this calculator may also appeal to other Forth programmers. It's to `dc`, but it displays the stack contents at all times and as you type, the stack changes are reflected immediately.
Or maybe it's just more spammers crawling the web to collect email addresses.
Really nice one! Thanks for sharing...
You knocked it out of the park!
But of course, I hope I'm wrong. :)
Glad you like it, thanks!
Thank you so much for the nice words!
From a comp.lang.forth posting: ---- 32-bit/64-bit Pygmy Forth for Linux, MacOS, Windows For your Forthing pleasure, I have recently posted my new Pygmy Forth at http://pygmy.utoh.org/pygmy64.html. It is in the spirit of Pygmy Forth for DOS, I feel, but quite different. The new version is implemented in Python and, thus, runs everywhere that Python runs. Python is its assembly language and it is easy to write CODE words. Here is a simple example with no stack effects: CODE HELLO print ("Hello") END-CODE Source code can be loaded from either a text file or from a pseudo block file. For Emacs fans, it comes with pygmy-mode to make working with pseudo block files especially convenient. PgDn, PgUp (or C-v, M-v) move from block to block, narrowing the display to show just the current block. S-TAB cycles through the 3 visibility settings (as in org-mode). For example, show just the first lines of each block or the entire file. The manual can be read in a web browser (HTML), downloaded as a PDF, or downloaded as an eBook (either Mobi for Amazon Kindle, or EPUB for other eReaders). MIT license. As always, I look forward to comments, corrections, suggestions. 
&gt; implemented in Python and, thus, runs everywhere that Python runs. I always wonder why does someone implement a simpler language in a more complicated one. Even C seems too much for a forth as simple as this one. Pros: - cross platform - simple to write? (if you know the language) Cons: - bloated - slow - dependencies - did I mention slow? 100x slower :) People already make jokes about python's startup speed. I heard lua people joke "a lua script finishes executing before the python one would even load". Am I missing something? The only reason I would consider choosing python as the implementation language was if I wanted to do interop with python, like e.g. [hy](https://hylang.org) does.
Frlm the user manual: &gt; Pygmy Forth is written in Python (Python version 3), so it will run everywhere: Linux, Mac, Windows, 32 bits or 64 bits. (I don’t know about cell phones and tablets. Please let me know if you try it.) High-level Forth words are compiled as Python procedures, rather than as lists of tokens or addresses. CODE words are supported and are written in Python, not assembly language. That is, you can think of Python as being Pygmy Forth’s assembly language. And: &gt; Pygmy Forth is written in Python and it can be extended by writing CODE words (primitives) in Python, or by writing high-level Forth words. &gt; Pygmy Forth gives you the benefits of Forth factoring, incremental and interactive testing, and simplicity, while providing easy access to the world of Python facilities and libraries. I can understand the appeal of not having to rewrite code for each hardware &amp; operating system combination one wants to run on. Back when my forth was written in x86 assembly, testing changes on the 9+ operating systems I supported ate a huge amount of time. This was the main thing that eventually led to me writing a tiny, portable virtual machine to run my forth on.
It is an inspiring graphic logo which alludes to forth.
If your use case is cross-compilation/cross-assembly, you're effectively dealing with two Forths, one running on the *host* and one on the *target*. For the *host Forth* you're less concerned with size or bare-metal speed, whatever provides you with a path of least resistance to leveraging the host platform's services and facilities can work, anywhere from C to the latest language *du jour*. The *target Forth* is a completely different animal, e.g. it can be a bare metal Forth using subroutine threading while the host uses token threading, etc...
Fair enough... but I don't think anyone's ever going to accuse Python of being "tiny". Even MicroPython boasts that it can fit into "just" 256KB code space and 16KB RAM. But then, it doesn't even really need much alteration to turn Forth into something relatively machine-independent. Token threading is already sufficient - either with bytecoded tokens like the Canon Cat or Open Firmware, or by reserving a range of tokens to point indirectly to machine code primitives and declaring the rest to be Forth calls, like the WISC CPU/16. Write a VM to simply implement those primitives and fill in the table, point it at your image, and you're done...
Am I alone in finding this just a little bit upsetting? Pygmy was something of a benchmark for compactness on DOS - squidging a complete Forth, with assembler, screen editor and the block number of every word, all in 16KB of code (by v1.5; v1.1 was 12KB), and one of the most comprehensible metacompilers I've ever come across. Very much "cmForth for a computer you'll actually have". That's why the name fit so well, of course. I know - it's Frank Sergeant's name, and he gets to use it how he pleases, and he is, and has the right to be, the ultimate arbiter on what's "in the spirit of the original". Nonetheless, I can't help feeling that if this is a pygmy anything, it's a [pygmy hippo](https://en.wikipedia.org/wiki/Pygmy_hippopotamus)...
**Pygmy hippopotamus** The pygmy hippopotamus (Choeropsis liberiensis or Hexaprotodon liberiensis) is a small hippopotamid which is native to the forests and swamps of West Africa, primarily in Liberia, with small populations in Sierra Leone, Guinea, and Ivory Coast. The pygmy hippo is reclusive and nocturnal. It is one of only two extant species in the family Hippopotamidae, the other being its much larger relative, the common hippopotamus (Hippopotamus amphibius). The pygmy hippopotamus displays many terrestrial adaptations, but like the hippo, it is semiaquatic and relies on water to keep its skin moist and its body temperature cool. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Metacompilation is just a special case of cross-compilation, of course.
Well, I guess you get the whole Python object system / library infrastructure as well. But it's not really very Forth, at least as Chuck Moore imagined it: &gt; If I want to tell the computer to add 1 to the letter 'A', it's none of the compiler's business to tell me I can't.^[1](https://www.forth.com/resources/forth-programming-language/) 
thanks for using python to implement it which lead me a chance to learn :D
simple, because python now rised too much, and its a chance to let more people understand forth, grandpa 
Care to take another swing at that - but with more grammar and less attitude?
In theory yes, the devil is in the details. There's a world of difference between using your *host Forth* to metacompile a new version of itself on the same host, and using your *host Forth* to interactively construct a totally different *target Forth* over an umbilical link to a target board.
well its forth community, everyone has at least 2 viewpoints :D 
I don't see that supporting many system configurations should necessarily take a lot of time. I test on 14 processors and five operating systems, but it's all automated. Of course, doing this has become so much easier lately with the help of free of charge online services.
There's also CircuitPython. It's supposedly slightly smaller than MicroPython. I have been looking at the lower end of "tiny". There are devices with only a few hundred bytes of flash and a dozen bytes of RAM. That's tight even for Forth.
I'm taking the view that it's great that Frank is still at it.
For higher level languages, I think [uLisp Zero](http://www.ulisp.com/show?1OPZ) is possibly the low bar; on the other hand, Lispers have a freedom Pythonistas don't - they can ruthlessly excise bits of the language. &gt; There are devices with only a few hundred bytes of flash and a dozen bytes of RAM. I'm aware of some of them. I think they really *have* to be programmed via cross-compilation... which presumably means developing the cross compiler in Forth, rather than developing a Forth for the target. :-)
I've read the manual, but thanks for posting the important pieces here for other readers. I guess after reading Chuck's thoughts I cannot imagine him using anything higher than assembly to create a forth. Even x86 assembly is overly complex for his taste :)
Is this a cross compiler? I haven't noticed that in the docs.
I haven't read the docs for the new version, the DOS version came with a serial link and a tiny target monitor for umbilical cross-development on the 68HC11. Just about any Forth can be extended to serve as a cross development host, it doesn't necessarily need to be designed from the ground up for that purpose.
That's as maybe but we usually try to be respectful to one another :) keep that in mind for the future?
Of course, there are some devices that don't make good *targets* for such an arrangement. Sergeant's "3 instruction Forth" worked because the 68HC11 has a von Neumann architecture, and can read, write and execute code as easily as data; try it on a PIC (except for the 32-bit ones) or an AVR, though, and the inability to execute code from RAM will bring you up short.
Sure, if your target is a pure Harvard architecture you'd need some arrangement to reflash and restart the target, perhaps a JTAG interface. Definitely less incremental. One possibility might be to use an emulator as an initial target. The hope is to have a single host Forth that you can use as development environment regardless of the target, squeezing out as much interactivity and incrementalism in development and testing as the particular target will bare.
Its a very cool app. It makes me wonder... could it be written in Forth for a standard Forth console. I think so by creating a new version of QUIT which is the outer text interpreter's name. 
From his manual in the ZIP file: &gt; I miss my Pygmy for DOS. This new Pygmy Forth is my attempt to replace it for the modern desktop environment (primarily Linux in my case). &gt; For programming for PCs, I used to love my lovely Pygmy for DOS. However, the days of DOS are gone, and good riddance to Microsoft as far as I’m concerned. I am delighted with the speed and conveniences and openness of the Linux world, but I have been missing Forth in that environment. I know there are many alternative Forths, and I am sure they are great for those who like them, but none of them suits me the way Pygmy did. Sounds like he really did think about it.
I didn't want to suggest that I thought otherwise. Still, one can respect the thought put into an endeavour and yet dislike the end product.
I can't find a reference but I think Chuck's earliest interpreter systems were written in high level languages and after he solidified what it did and used it for real projects, he coded one in assembler for performance.
Am I missing something, or is that data from 1999-2005? I keep staring at my math, but ~325 weeks is a bit over 6 years?
No need to guess - we have [his own account](http://www.exemark.com/FORTH/Forth_early_years_1a5v2.pdf) to refer to. It's clear that as he went along, he recoded Forth in - and picked up influences from - whatever he was working on at the time, whether ALGOL, Fortran, B5000 assembly language, or even JCL. The compilation to threaded code was almost the last thing to arrive - previously tokens had been stored as text and searched for every time they were interpreted. (Which has slightly different semantics to compiled code.) And now he's gone back to interpreting tokenised source code; compilation is used as an interpretation strategy. (JIT-by-hand, one might call it.)
I asked Marcel about it, and he said the data was to around 2007. In any case, it's not up to date until today.
Not sure why this post is getting downvoted. [Reversible computation](http://strangepaths.com/reversible-computation/2008/01/20/en/) is a real thing. So far there have been two attempts at *software* reversible forths that I am aware of: WrongForth: http://forth.wodni.at/wrongforth RVM Forth: www.complang.tuwien.ac.at/anton/euroforth/ef04/stoddart04.pdf Neither of these run on physically reversible hardware which means you won't get the benefits of reduced power consumption. Even a conventional NOT gate is not physically reversible. You would need a special reversible architecture such as the Pendulum for any of those benefits. [Pendulum: A Reversible Computer Architecture [PDF]](https://pdfs.semanticscholar.org/30d5/54dcd308d529be93dc70607197cd251dfd3c.pdf) However, there are benefits to just software reversibility such as reversible degugging. The idea with reversible debugging is that you can step backwards and forwards through a computation. I prefer to describe it as random access over runtime, not just random access over code/data. afaik, you can make your system reversible by recording the execution or by building the system out of reversible primitives. neither WrongForth nor RVM Forth look very clean or simple to me, and that's what I'm working on doing differently.
Even before reading the Forth code, I can see that those tiny file sizes signify a truly remarkable piece of work. The Apache licence is by far the largest file, and the next largest is the ReadMe file. Even the files containing code, over half of the lines are comments or blank lines.
Thanks to both of you. I've been vaguely aware of hardware reversibility as a way to reduce energy dissipation, perhaps since the 1990s when that Pendulum design was first published. I recommend the paper as a quick and relatively painless introduction to the topic. Software-based reversibility for Forth seems rather counter-intuitive, as the basis of the language is the irreversible overwriting of stack contents. I suppose the pop operation can be redesigned as a non-destructive exchange, however. 
Wow, no kidding. I'm excited to see where this project goes!
I'd really like to see a gif preview of this in action on the bitbucket repo. Projects like this really need microscreencasts like that to show what they're like.
Would be nice to see issues enabled for this project to submit things like a request for a microscreencast gif of it in action.
Looks really interesting! You should consider adding an issue tracker so that people can discuss the project and make tickets.
My thought is this. In a purely reversible system you are still turing complete. So you can implement the irreversible ans inside of a reversible forth. So ans code can be made sort of forwards compatible. The problem, as you've stated, is that to make a reversible forth that is also simple is the reak challenge. We can add a new set of words that always take as many inputs as outputs and that are all reversible. Perhaps for each word in the dictionary we can have a forward and reverse definition for both compile and runtime. Then the only real issue is the stack. The stack by it's own nature was not intended to be reversible. The best solution I can think to this is to implement the stack as an immutable linked list so that you can travel back to previous states of the stack. Though I am not sure what kind of overhead this would add. Please share any research you have on making a reversible forth simpler, /u/pointfree
I would replace the stack with something akin to a gap buffer *past* stack *current* stack |------------------------------------| ^ ^ push *past* stack *current* stack |-0--------------------------------0-| ^ ^ pop *past* stack *current* stack |-0--------------------------------0-| ^ ^ `5 var 8! ( store a byte to memory location var currently containing 2)` *past* stack *current* stack |-0-2----------------------------5-0-| ^ ^ Here the gap buffer stores a bijection for each state. Sliding the gap around moves us forwards and backwards in time. For pure reversibility the trace of the above execution would need to be stored in the gap buffer as well, so instead of storing 0 on each side and sliding the gap around, store code to push 0 and pop 0 on either side of the gap. The gap shrinks with each step of the computation, however, there are opportunities for memoization and anti-memoization to reduce memory usage. If a computation has already happened you could just reference it. Hmm... sounds like a dictionary definition... and in forth we do tend to compile right-to-left and provide parameters left-to-right much like in the above diagrams. But what would the header of a definition be? It could be the ascii text name of the word, but I like to make it the inverse of the word. See [*Pattern matching in concatenative programming languages* [PDF]](http://micsymposium.org/mics_2009_proceedings/mics2009_submission_72.pdf) the premise of the paper is: pattern matching is the inverse of functions. By using code in place of and in addition to headers you can do things like this: 2* : 2/ ( matches a number twice the size of another and halves it) Pseudocode for the fetch/store words for byte, hcell, and cell. ( header of !, body of @) &lt;match numx of size less than 0x100 on the stack. Match addr1. Match "!". Swap numx with the contents of addr1 (numy)&gt; : ( header of @, body of !) &lt;match numy of size less than 0x100 on the stack. Match addr1. Match "@". Swap numy with the contents of addr1 (numx)&gt; ( header of !, body of @) &lt;match numx of size less than 0x10000 on the stack. Match addr1. Match "!". Swap numx with the contents of addr1 (numy)&gt; : ( header of @, body of !) &lt;match numy of size less than 0x10000 on the stack. Match addr1. Match "@". Swap numy with the contents of addr1 (numx)&gt; ( header of !, body of @) &lt;match numx of size less than 0x100000000 on the stack. Match addr1. Match "!". Swap numx with the contents of addr1 (numy)&gt; : ( header of @, body of !) &lt;match numy of size less than 0x100000000 on the stack. Match addr1. Match "@". Swap numy with the contents of addr1 (numx)&gt; Firstly, both halves of the definition must be executed, this takes care of reversibility. Also notice that by doing a little computation in the header: * we have decomposed the outer interpreter * we don't need separate `C!`, `H!` and `!` words. We only need `!` (...or maybe we really only need a word SWAP that takes an address) * the distinction between header and body is fairly meaningless. * words have symmetry (! also @'s and @ also !'s). We should be taking advantage of that, but what's left after we've compressed all the symmetry of an execution trace? Should that execution trace have symmetry as well?
&gt; So how do we fill up these devices? This is a non-goal &gt; those resources more or less keep improving each decade. There are limits and unless you've been living under a rock for the past decade you know that we seem to be reaching them. The idea that things will just keep improving isn't something most programmers believe today. 
Here's a couple of things I thought while I read that. Perhaps our stack should be finite, first of all. Perhaps the stack should not be able to grow so large and will have a sort of overflow. However this is bad for two reasons. First of all when you overflow you lose the information that was held on the stack. So we've got to keep our old stacks if we don't want to lose anything. Second of all it seems that for every mutation of the stack there is an additional copy of the stack you have to store information wise. In other words for each new state of the stack you are creating information. The second thing you made me remember is an idea I have been thinking about in terms of doing maths. It seems as if the best way to do maths is to indeed pair + with - and / with *. This means that you want your inputs to have two values and your outputs to have two values. So lets say I have an empty stack and I go 2 5 +/- What I should get is 7 -3 because that is the result of that operator. Let's say the result I wanted was the addition. I can't just store the addition result in memory and drop the other one because that would be losing information. If I store one in memory I have to store them in a pair. In other words the structure of how we would want to store things in memory is a bit different. Let's say for example I do the operation twice. 4 2 +/- 3 +/- The result of the first operation should be 6 2 as we'd expect. Then the stack becomes 6 -2 3. The result of the next operation is 6 -1 -5. This may not be the desired result that we wanted. In other words you have to design a forth that does not lose information. Furthermore you should not create information if you don't have to. Every time you create information you use up resources.
I don't have a source I'll remove it.
&gt; I see emacs as being very efficient and I have talked about how a system similar to emacs could be written in forth. Yes, I have: https://github.com/larsbrinkhoff/fmacs
Is there any reason why this was downvoted so heavily?
I do not think that micro controllers will improve in the way you described with conventional technology simply because increased logic complexity always lead to higher power consumption somehow which is primary only compensable though more sophisticated and as such much more expensive photo lithographic methods. Additionally it is foreseeable that possibilities of further shrinking will reach a (probably economic) lower limit. So until we see a breakthrough in power storage systems or processor design it is in my opinion unlikely that micro controllers will come close to the development of general-purpose processors. Another question is anyhow if there is a demand for such chip.
Very nice, I'm playing with it... I use a [snapshot version of gforth](https://www.complang.tuwien.ac.at/forth/gforth/Snapshots/) (currently version gforth-0.7.9_20171026). It contains a lot of system 'nix calls to pthreads, Xlib, opengl, wayland, ... On my linux box, it gives a rich new environment for useful projects. 
&gt; So what happens when one runs out of space? In some implementations, you can pass parameters to specify the sizes of the memory spaces at startup. In others, you may have to rebuild the kernel. &gt; In C there is realloc to "grow" more space for the same data, is a similar approach used in forth? There's a standardised wordset for dynamic memory allocation: http://lars.nocrew.org/dpans/dpans14.htm &gt; Where do these exist in a modern x86 architecture? Are they hardware or software stacks? I don't know where x86 Forths would usually place the stacks, but it's not different from any other processor. Anywhere they fit in the memory map. I have a vague feeling that the high end of the address space is popular. Not sure what you mean by "hardware stack". Some processors have an internal stack which is not part of main RAM. The x86 is not like that. It does have a hardware stack pointer, which I have seen most Forth do use. ITC and DTC Forth would probably use it as a data stack pointer. STC and native Forths obviously as return stack pointer.
&gt; If I need more memory I have to ask the OS for more and I sure won't get the space coming right after what I had before. You might. Linux uses virtual memory, and each process gets its own address space, so within reason it can put your new memory wherever it pleases - maybe right after your existing allocation, maybe even wherever you please. It can even back it with a file, for you to mmap() into memory as a Smalltalk-esque image. Of course, it probably won't be contiguous in *physical* memory, but you'll never know about that.
It's not usually that you need to load the whole 100 MB dataset into memory in order to process it, although if you can then this is probably easier. Rather, in many cases you only need to be able to load 1 record at a time to process the whole set. You may also load some subset of the records. It's also not uncommon in Forth to logically divide memory into blocks, which are loaded from persistent storage one by one and are swapped on demand :). This might seem a bit kookie at first. I avoided the block concept for years but it's really pretty nice :). Maybe I'll have time to expand on this later but I'm out of time for now. IIR Starting Forth and Thinking Forth both have some good things to say on blocks etc. I might suggest you start there. The important thing to realize is that blocks aren't just for text.
Indeed. Storing source on them is just about the worst thing you can do with them. They probably represent progress from a stack of punched cards... but those of us born after 1965 never had to suffer that, and shouldn't have to write our source code on the backs of postage stamps either. &lt;/rant and="breathe"&gt; Having said which, as a virtual memory / persistence mechanism for platforms that don't have virtual memory or mmap, they're tough to beat. But those of us using Linux have both of those things, and really don't need to duplicate them with blocks - we can just mmap() our block files instead.
LISP had a REPL by 1965. [The LISP Implementafion for the PDP-1 Computer ](http://s3data.computerhistory.org/pdp-1/DEC.pdp_1.1964.102650371.pdf) describes interacting with LISP via a REPL on page 6: &gt; At this pint, the LISP aystern is reedy for manual &gt; typenriler input, As soon as the operetor types, for &gt; example: &gt; (CAR (QUOTE (A B C D))) &gt; together with a final space at the end of the last &gt; right parenthesis, the computer takes control of &gt; the typewriter, impulses a cmriage return, and &gt; then types out: &gt; A &gt; which of course is the correct answer, I'm not aware of anything earlier than this though.
Thanks! That's where I read it. In reviewing the paper I found this line which, having written my first cross-compiler, I love: "Forth could now interpret an assembler, that was assembling a compiler, that would compile the interpreter. " 
In bigger forths, like gforth, you also have a traditional heap, with words like `allocate` and `resize` that work great for large allocations. I've never run out of dictionary space, but I also haven't done any really big projects in forth that might push the limit. 
It is a matter of debate whether microcontrollers will reach that level of efficiency. Only time will tell.
That, I can work on. Expect something in a few days.
You and the other commenters are encouraging. I put this out here to gauge the initial reaction, and if I have people willing to put in the effort to make good suggestions, I will work on them.
Thanks. A lot of it was just choosing the starting point that made things easiest. In this case, gforth's FFI was crucial because a shell uses many different system calls. There were several sections that took a few rounds of factoring before I was happy with them. I wanted to make something useful while demonstrating the strengths of Forth.
&gt; Where do these exist in a modern x86 architecture? Are they hardware or software stacks? x86 has a single hardware stack. One workaround that I've seen for that with FORTH, is the idea of using ESP (the top stack register) and EBP (the bottom stack register) as though they were each a different stack.
EBP isn't the bottom stack register; it's intended to be the stack's frame pointer (or _B_ase _P_ointer), and back in the day of the 8088 that was the only practical way to implement anything Algol-flavoured. Local variables, parameters, parent stack frames (for Algol-derived languages that support them) and the return address are all referred to by indexing BP. To that end, using BP defaults x86 processors to using the stack segment (although this has become steadily less important over the years). That also explains an instruction encoding quirk of the x86; you can't encode [BP] - you have to encode [BP+0]. Basically, [BP] would point to the return address (or the next frame pointer up the stack, depending on implementation), so in general you wouldn't want to access it anyway. The encoding was reused to provide an absolute offset address instead. Technically speaking, a frame pointer isn't necessary unless you use something like C's all, as long as you can index through the stack pointer. But that's not possible on the x86 when it's in 16-bit mode. Since Forth doesn't use stack frames, BP was free; and since it indexed into the stack area anyway, it was the natural choice for implementing the return stack, particularly for the implementation of locals, etc. But most 16-bit x86 Forths noted that using XCHG BP, SP and PUSH/POP when the return stack needed manipulating was faster on an 8088 than indexing through BP directly, so they simply did that instead - even though at that point any other register could have been used. Unfortunately, just about every 8086 register has a special purpose, and BP's was uniquely redundant in Forth.
&gt; Unfortunately, just about every 8086 register has a special purpose, and since BP's was uniquely redundant in Forth it ended up being chosen by default. Moving Forth's analysis of different ways to write FORTH instructions has been very interesting for me. Do you know of a recent processor that has two hardware stacks?
&gt; something like C's all Is there a typo here?
Forth processors usually have two hardware stacks. The J1 is recent. The 6809 has two stack pointer registers, but it's not exactly recent. I'd say most processors either have one special-purpose stack registers, or all general-purpose registers or index registers are equally suited as a stack pointer.
Yes. Which is odd, because I distinctly remember *typing* alloca()...
The trend has gone the other way - many RISC processors don't implement hardware stacks at all; it turns out not to take very many more instructions to do it in software. However, any processor with autoincrement loads and stores can make a pretty efficient stack out of any of its registers; the primary example in modern use is the ARM.
I have an unfinished stack processor design that uses a very wide circular shift register for each stack (i.e. no stack pointer registers).
&gt; many RISC processors don't implement hardware stacks at all There are few things that make me more sad, than the degree to which object oriented vomit has taken over the world. I will have to hope that one day the majority return to sanity.
Does that include fetching code into the return stack, and execute it from there?
Um... how did you get there from what I said?
I'm not sure what you mean. Return stack entries are still a single machine-word wide, as are data stack entries (the processor has two sets of each for two concurrent tasks). 
I was thinking maybe your stack items were very wide. In which case the return stack could hold the code for a definition, instead of just an address. Never mind, it's just a whacky idea I picked up somewhere.
[Here?](http://home.pipeline.com/~hbaker1/ForthStack.html)
Yes! Thanks, I had forgotten the source.
Stephen Russell's interpreter ought to predate that, though I don't know exactly when it was written. The LISP 1.5 Programmer's Manual from 1962 mentions it. I don't know the details, but I'd be surprised if there weren't some kind of REPL.
Found this: "I wrote the first implemenation of a LISP interpreter on the IBM 704 at MIT in early in 1959". http://www.cs.ou.edu/~rlpage/carcdr.htm
Hey, that's pretty cool -- found the J1 info [here](http://www.excamera.com/sphinx/fpga-j1.html). &gt; J1 runs at about 100 Forth MIPS on a typical FPGA. I've got a DE0 Nano lying around, maybe I should dust it off and give this a try. 
Interesting dissection. I'll come back with a response later, since this is a lot of information to parse.
&gt; I wanted to make something useful while demonstrating the strengths of Forth. Ugh, if only I had time to do something so fun...
That sounds great! Best of luck to you, I'll keep watching this project to see where it goes! Hopefully I can make a contribution or two at some point.
Do you mind if I cross-post the link to your repository to a few other subreddits, namely r/linux, r/unix, and / or r/commandline?
If anyone can find the link to the c.l.f thread that would be much appreciated. Some interesting discussion seem to be clipped from this version.
&gt; Um... how did you get there from what I said? I will probably get large quantities of virtual rotten vegetables thrown at me for this, but... (*deep breath*) My perception of OOP was that the main reason why it developed, was because C lacked inter-process communication, (you can use sockets and such, but I'm talking about something as really native as most people seem to want) because it did not (at least normally) use a stack. If you have a stack, you can just put your most recent input and output on it in a very simple and immediate way. If you ***don't*** have a stack, then you're naturally going to resort to structs, arrays, inheritance, and other more complicated strategies in order to try and achieve the same thing. Hence, my assumption that machines which don't have hardware-based stacks would likely produce more attempts at other forms of IPC in their programming.
https://groups.google.com/d/topic/comp.lang.forth/qKDbZ8JCTsE/discussion
the alloc relevant functions were provided as library, so its not the c language feature, and its just finally invoke the brk/sbrk syscall in linux system. so if you want that in forth, i guess you could do the similliar things, provide your own word and invoke brk/sbrk syscall
It's John Passaniti. He tends to have that effect... he had a long-running - as in *over a decade* - c.l.f feud with Jeff Fox (that eventually ended up consuming the whole newsgroup, since neither man was constitutionally capable of just leaving it there and walking away) which was only brought to a halt when the latter died. I'm sure John remains annoyed that he'll never get to win their argument.
Sounds like one of those guys who isn't as smart as he thinks he is.
I just went to Google Groups and made a search for the text in the subject line.
Right, it doesn't seem so much like a question as a series of complaints about implementation details. And it mostly goes downhill from there. There are occasional nuggets, but I didn't care to read it all. I'm sure if /u/dlyund would write a treatise based on his own experience, it would be an interesting read!
Right, it does seem mostly like a list of grievances. I mean, the way I use Forth, pre-parsing would be pointless for me, no matter how much code I was compiling, but why be so upset over Mr Moore's implementation that clearly fulfills some need?
* I think we can do without decomposed organic material. * OOP was developed before C even existed. See the history of Simula. I have an impression SmallTalk made OOP popular, and I don't think it was influenced by C. * The PDP-11, for which C was first implemented, has very nice addressing modes for stacks. And I'd be surprised if the compiler didn't make use of them.
As it happens, you can try it and see for yourself. SIMH runs Lisp 1.5 on its IBM 7094 emulator. But it's... weird; instead of EVAL, the top level is EVALQUOTE; you give it a function and a list of arguments, and it applies the former to the latter as is. It's not interactive, but I suspect that's more a function of the IBM 7094 not being set up for interactive I/O; however, it certainly **r**eads, **e**val(quote)s and **p**rints the result, in a **l**oop.
As an interpretation for dummies like me, is this something like byte compiling source code? Instead of keeping source code as "ASCII text" he serializes it into something that is more efficient to load/compile? And he only stores the source code in this format so when an editor wants to open it it needs to deserialize it?
Ah, still a lot to learn, thanks :) One more question - since people say here the stack is not on the cpu, it's just working on RAM memory? So the stack will be as big as much memory I provide for it to operate on?
Yes, exactly so.
&gt; Not sure what you mean by "hardware stack". Some processors have an internal stack which is not part of main RAM. The x86 is not like that. It does have a hardware stack pointer, which I have seen most Forth do use. Sorry, I'm still figuring these things out so I have a bit of a mess in my thoughts :) I meant if the stack is on the CPU or in RAM. I think you're saying it's in RAM and there are instructions that work with it like with a stack, right?
Further to u/larsbrinkhoff's points: * An implicit stack, storing return addresses, locals and partial results, is the most natural way to implement an Algol-type language; indeed, it was likely that which prompted the addition of hardware stacks in the first place - not having hardware stack support is very much a return to prior practice, rather than an innovation. What Forth does differently is add a *second*, explicit, stack purely for passing parameters - but even then, it's not alone in that; the POP series of languages did it too, as did the English Electric KDF9. * Interprocess communication is generally different from, and more complicated than, procedure calling, even under those systems which use similar semantics (eg Thoth and its descendants - one of which is QNX). Aside from anything else, you can't generally assume a shared address space; in implementation terms, queues are a better fit than stacks. * You can write Forth in any language - just pass around a stack pointer that indexes into a global array. It's not a good fit, though. The Algol-esque single-stack implementation works out to be very efficient for passing named parameters - you evaluate each one in turn, push its result, then call the procedure. In contrast, Forth has to move its locals off the stack and onto the return stack (or a locals stack), which is why they're slower than just using what's on the stack anonymously. * While it's true that some languages, like Simula 67 and Smalltalk (and more recently Java), conflated objects and processes, there's nothing inherent in objects that requires that - look at Oberon-2, for example. At root, an object is just a chunk of memory with a pointer to some code at its head - which is exactly the same as a Lisp closure, or a Forth DOES&gt; word.
Yes. At least, on the most common systems out there - the exceptions are either Forth machines that keep the stacks out of the address space altogether, or very tiny creatures like low-end 8 bit PICs.
Yes, that's right. E.g. `PUSH EAX` is an x86 instruction which will store the contents of the EAX register into RAM. The address is specified by the register ESP, which is decremented before the store.
&gt; At root, an object is just a chunk of memory with a pointer to some code at its head - which is exactly the same as a Lisp closure, or a Forth DOES&gt; word. Thank you very much for this clarification. It helps.
If tokenization is what colorForth is all about: in 1983 I owned an 8kB-ROM-computer named "Jupiter Ace" that compiled words from the REPL and provided detokenizing by looking up the threaded address in the dictionary, thereby trivially going on to look up the Name of the word. The listed (detokenized) word could then be edited as in the command line. The machine had 2048 Bytes of RAM for Video, sys vars and dictionaries. The "Jupiter Ace" did not use huffman-coded variable-length primitives, though, as colorforth does. 
It's not *all* colorForth is about, and the huffman-coding is just for individual characters, which only the editor and display circuitry needs to deal with. And colorForth compiles to native code, so recovering the source from the object code isn't possible. The essence of colorForth, really, is to make Forth's outer interpreter as trivial as possible - so that instead of retaining compiled words permanently, everything can be done by loading a chunk of source code, running it, and then forgetting it immediately afterwards. At one point I think he said something about wanting a compiler fast enough to interpret interrupt handlers, but I could be misremembering. Meanwhile, the Jupiter Ace did highlight some weaknesses of a decompilation-based system. For example, if you wrote a COMPILER word S", you'd be fine, and it'd do the right thing - but what it wouldn't do is list, or let you edit, what you input. You got the name of the COMPILER word, but it didn't know it also had to display the suffix: COMPILER S" [COMPILE] " RUNS&gt; COUNT ; : HI S" Hello world" TYPE ; LIST HI would produce : HI S" TYPE ; The same limitation hit DEFINER words too: DEFINER VECTOR 0 DO , LOOP DOES&gt; 2* + ; 11 12 13 14 4 VECTOR A 2 A ? ( prints 13) LIST A ( prints VECTOR A) (At least, I believe that to be the case; unfortunately, I'm not in a position to test it against an emulator just now.) It wouldn't be impossible to conceive of an Ace-like system which included, say, a LISTS&gt; word which regenerated the source code you'd have had to type in the first place; but I guess they couldn't squish that into 7KB of ROM along with everything else.
&gt; However, if you don't have the text interpreter - if you're relying on your editor to convert text into CFAs - then it's your editor that needs to be extensible, rather than just your decompiler... which is somewhat more complicated. I don't know if extending the editor is any harder than extending the compiler, but certainly, there are differences. Present-time execution can do a lot that compile-time execution can do but the drawing code may be more complex so in that respect you do have a point. Overall though, I haven't found it any harder than adding new "recognizers". 
I tried that but I it didn't give me any results. Maybe I did something wrong :-) it was around 3 am 
Fair enough. What I meant was that the editor has to not only work out how to display the new stuff, but also how to edit it in a sensible way; all a decompiler has to worry about is the former.
I often have trouble making time for fun projects. I think consistency is the most important thing. Work on it every day, even if it is only 30 minutes.
I am happy that this project uses appache 2.0 because that license is FOSS. https://www.gnu.org/licenses/license-list.html
Go ahead. r/programming might also be interested.
Good advice! I'll have to try that out so I get to do some things that I want to.
Good idea! Thanks!
I think the whole idea of an integrated editor as replacement of the outer interpreter in combination with JIT compilation (AHA) is an advance. Also signaling the runtime as well as compilation state though colour lead to better readability in my opinion. It is only that the language colorforth seem to be inconsistent.
Inconsistent how? But then, here's the thing. colorForth was intended to be the simplest thing that could work; if you think you can do it better, more consistently, or more simply, it's not like you have megabytes of code to work through in order to make those changes. Rather than complaining that someone else made choices you don't like, it's *always* more productive to go and make choices you do like in your own endeavour. You might find you have made things better; you might conclude that you were wrong; but you'll *know*, rather than just second-guessing.
&gt; Inconsistent how? The colors and semantics that Chuck chose may serve his purpose but they're non-orthogonal and too limited in many respects. It took us many interactions to arrive at the colors and semantics in Able. The result is, if I don't say so myself, a significant improvement over colorforth, both in terms of orthogonality and understandability. We didn't choose colors as abbreviations for common things e.g. "there seem to be a lot of definitions, so we'll make those a nice red color". We designed the language around the colors so instead of ending replacing ":" with "red" we ended up with first-class names, which as it turns out allows you to do things like defining words without parsing :-). To be clear here, I think colorForth is fantastic, but after years of contemplation, it's hard for me to say I think Chuck approached the choice and uses of color as more than a way of abbreviating what he was already doing in Forth. colorForth, *in my opinion*, is not a rethink of how Forth would work with semantic metadata (color) so much as it is Forth window-dressing with a purpose. Its design is more pragmatic than principled.
 11:45 --- The Control Loop in Mitochondrial DNA --- C.H. Ting "There is a stretch of about 1000 base code in mitochondrial DNA which does not code for any protein. I believe this stretch contains the secrets of genetic programming, which controls the growth and division of cells. I do not understand the code, but like to share some of my observations. I also believe that if God picked a programming language for genomes, he would surely pick Forth for its simplicity and expressiveness." Forgive my language but I fucking love Ting
As I understood it, one of the primary motivations for developing colorForth was Chuck's failing eyesight. Bright colours and big, chunky fonts made it much easier for him to see his code; simplifying the parser was probably a side effect... until the tail started wagging the dog. Like pretty much everything else about Forth, the conclusion that colorForth got cobbled together as Chuck needed it is hard to resist. No more rhyme or reason to it than that. Oh, and he's clearly not colourblind. \**chunters with annoyance*\*
Holy hell. Is there a paper about that somewhere? I know they'll post the slides after the meeting, but slides are a summary and I know this subject requires depth.
:-) I was responding to the very particular observation that colorForth the language is somewhat inconsistent; it's a fair comment to notice that the choices Chuck made in the context of OKAD II probably aren't the best choices for a general purpose language. I've certainly come to feel that and I've been programming in a colorForth antecedent for several years. My opinions on this matter are informed by the experience doing exactly what you suggested /u/9xlba3 should do. I have a great deal of respect for Chuck. I didn't say anything about what motivates him. Nor did I suggest that colorForth was cobbled together on a whim or that its simplicity or performance (or any of the other wonderful things about colorForth) were accidental. I completely buy that Chuck had a reason for everything that he did. That doesn't mean that I think colorForth is perfect or that it can't be made more intuitive, expressive, and more widely applicable. colorForth may be perfect for OKAD II (I've never had the please of using it) but it isn't perfect for most of what I or other people need. I would like to think that my (not always so) humble attempts to improve on Chucks improvements don't offend you that much. I'd be very surprised if Chuck would be offended by what I've said about colorForth.
I had forgotten how migraine inducing Passaniti-Fox exchanges could be.
Can you PM me the links to any threads you create so I don't have to hunt for them?
I did consult the GNU recommendation when choosing the license. Full GPL seemed like overkill since the code is already gforth specific and fairly small.
Yes, I will do that.
Great blog post, but the first comment there (citing Marx) was a great example of why I've more or less entirely given up, where trying to reach people is concerned.
Also, the title of the book he mentions here is *Technology and the Character of Contemporary Life*, by Albert Borgmann. I had to do a tiny bit of digging in order to find it. I'm going to see if I can get the ebook, because it sounds like it would be worth reading.
The Wiki summary is quite comprehensive. Worth looking at first: https://en.wikipedia.org/wiki/Technology_and_the_Character_of_Contemporary_Life:_A_Philosophical_Inquiry
Can you elaborate?
Let's just say that the last seven years, taken as an aggregate, have destroyed any faith whatsoever, that I might previously have had in anything related to Marxism. I truthfully no longer believe in collective social organisation of virtually any kind; and Reddit and what I've seen of the Millennials here has had a lot to do with that. /R/Socialism in particular is a vicious, censorious cesspool.
Teenagers in r/socialism or tankies in r/communism or whatever have nothing to do with Marxism. Reddit is not a good place to derive ideas about Marxism. Also, Marxism is not something you should have faith in. Marx spent his entire intellectual life to criticize idealism. Marxism is a way to analyze society and nothing else. Don't listen to ignorant socdems and tankies lurking around reddit.
I'm curious; what are tankies, exactly?
Marxist-Leninists, Maoists, Stalinists, and all other authoritarian state-capitalists who have never understood Marx in their lives.
Fascinating. The ML crowd are pretty much all I've ever seen, sadly. I've never really understood why that movement would be so popular, other than maybe because of aggression. From what little I've seen, Lenin was an insane monster.
Any abstraction for the sake of itself (i.e. any ideology) is simply bullshit. If you're actually interested in Marxism, please just [read](https://www.marxists.org/archive/index.htm) Marx and other related thinkers themselves, instead of listening to clueless internet teenagers.
Blah blah blah nobody understands Marx and all of other variations of the old "that's not real communism" defense. Perhaps that's not too surprising from a GNU advocate, but color me unconvinced. There are plenty of valid reasons to disagree with Marx, Marxists (classical, economic and cultural) and anyone who is attempting to implement his ideas.
Ah yes. Stay away from anyone who might be critical of Marxism, go get it from the horse's mouth, devoid of objectivity, or the problematic historical context, that you're likely to run into outside of the Marxist echo chamber :P.
O Infinite Intelligence, in whom all inspiration is, tell us the truth about ourselves in whatsoever ways we best may bear without being broken by that burden. And for those who follow your path, let their blessing be doubled. Protect thy souls, and let your work be done. Blessed be our fathers and friends. The keepers and protectectors of your beloved simplicity. [The father] (https://en.wikipedia.org/wiki/Charles_H._Moore) keeps your light. [The friend] (http://pelulamu.net/ibniz/) tells your gospel. Prosper, live, know, love. 
&gt; Marxist thinking stands contrary to natural law and human nature. So does just about every legal system in the world, otherwise it wouldn't be necessary. But the irony is that those hyper-authoritarian regimes that Marxism has engendered have, in practice, ended up more or less indistinguisable from the hyper-authoritarian regimes born of fascism; an external observer would have to conclude that extreme authoritarianism was much more closely aligned with human nature than liberal democracy - and that perhaps a degree of opposition to human nature is a very good thing indeed. Meanwhile, the term "natural law" is an oxymoron, not worthy of further discussion. (My bias: I'm an anarchist, but otherwise somewhat moderate; I always found Kropotkin and Goldman a lot more appealing than Marxists.)
&gt; Meanwhile, the term "natural law" is an oxymoron, not worthy of further discussion. I'm not sure the Hindu's would agree. I use "Natural law" as the closest English translation of Dharma. I admit it's not a perfect translation. Thinking *laws of physics of human interaction* will put you in the ballpark. Of course, you're free to think whatever you like about that but it's usually better to ask for clarification than it is to declare superiority and move on :-). I mostly agree otherwise. (My bias: I'm a monarchist, but otherwise quite moderate. I largely agree with the founding fathers of the United States when they wrote that democracy is the worst form of government, but I prefer the idea of a constitutional monarchy to that of a constitutional republic because at least with a monarchy you know who to hang, or hold to account, if you prefer, when, inevitably, the system gets tired and is in need of a reboot. I like my government the way I like my software. Easy to understand, and just as easy to kill ;-))
You could just have used the word "dharma"...
The slides of the other talks will be available [here](http://www.forth.org/svfig/kk/kk.html) soon. I briefly mentioned BIND in anticipation of questions about scoping. Apologies for not asking you first, /u/dlyund.
What a pleasurable source of forth info. Thank you very much for sharing. I am surprised, I was not aware of it.
Wow, I finally made it to a Forth Day - at least by proxy. ;-) **ed.** Also, I've just realised: the Jupiter Ace had BIND too. It called it REDEFINE. **ed.2** Damn, I should have checked. REDEFINE &lt;*word*&gt; only updates the previous definition of &lt;*word*&gt; to point to the most recently defined word, whatever its name (and it'll work with any kind of word, not just a colon definition). So not *quite* the same thing - less general. But a similar idea.
Well written. Bravo! 
Re your edit: I completely agree wrt those who seek power being those who shouldn't be allowed *anywhere near* it. And yet, even those who don't seek power can end up abusing the crap out of it (I believe that's the technical term, yes? ;-) ) when they suddenly realise they have it. The mode of powerless (and hence responsibility-less) thinking is *really* hard to shake. And as Lord Acton noted, power tends to corrupt, and absolute power corrupts absolutely. Without some effective restraint, anyone who's handed absolute power will go off the rails; nobody is strong enough to withstand the allure. A robust constitution, with a presumption of self-government and only those powers hoisted upwards which are delimited by that constitution, would be a good start; however, if the lesson of the States is anything to go by, any time you write something down, an industry builds around finding the loopholes in it. Nonetheless, the insistence of certain people in the UK that this can't be done with a constitution that isn't codified in any formal way runs counter to the facts; the idea has been tested to utter ruination in the last forty years, and unfortunately it's abundantly clear that a constitution of assent, practice and convention might as well not exist at all. It's a quandary! I've long wondered whether the only real solution is to build power in rigorously bottom-up structures; that on the level of a village or district, representatives are chosen in a manner similar to jury service, and also at random, one of those representatives is chosen as a delegate to the next level up. The executive level is that of the district, but funding decisions go all the way up the chain and back down again.
I don't know what this means. Care to explain? I never defended anything, I just pointed out that 99.999% of "leftists" never read and understood Marx, which is objectively true.
&gt; but even before I knew who they were, the concept of authority has always scared the crap out of me, on a visceral level. Read [this](https://www.google.com/search?q=libcom+karl+marx+and+state&amp;ie=utf-8&amp;oe=utf-8&amp;client=firefox-b-ab). All Marxists are wrong and stupid because none of them ever read Marx.
I really don't care what you think about Marx. You can think whatever you want.
And how is that not just a different dressing on "no true Scotsman"?
Did you read the article? If you read it you'll see what Marx *himself* wrote. I don't care what other people talk about Marx, all his writings are accessible by public, I know how to read, and I read him. There is no Scotsman. Marx spent all his time to argue against idealism but you're saying that Marxism is an ideology. Look, this is like saying Nietzsche was a nihilist; teenagers think he *was* a nihilist although he spent all his intellectual life to attack nihilism. It's the same thing when people say stupid stuff about Godel's incompleteness theorems; if you don't understand them, you should not have a right to have an opinion about Godel's theorems. Period.
&gt; Marx spent all his time to argue against idealism but you're saying that Marxism is an ideology. I'll thank you not to put words into my mouth. I said, and as it happens I believe, no such thing. And since you spent the rest of your comment battering that strawman (you're big on the logical fallacies, you know - you should get that looked at) I don't see that it's worth responding to further. Except for: &gt; Did you read the article? I skimmed it. I thought Marx's comments hopelessly naive and Bakunin's realistically, reassuringly cynical. It would appear that history has tended to support Bakunin's, rather than Marx's, view of the world.
One last thing: &gt; if you don't understand them, you should not have a right to have an opinion about Godel's theorems That sentiment is outright dangerous. For one thing, holding opinions about what we don't fully understand is pretty much a necessary condition of day to day survival. For another, your whole argument here is begging the question. The presumption, unstated but clearly implied, is that anyone who truly understands Marx will agree with you - which then become the test of true understanding... alternative readings are simply not permitted, ideological purity is the defining criterion of worthinesss, and - down you spiral into a cult. If you think someone's opinions are wrong because of incomplete understanding, the onus is on you to demonstrate that. And you'll only ever do that effectively if you have the humility to accept it when someone comes along and schools you in turn, which frankly I'm not seeing. Never, *ever*, is it justifiable to say that someone has no right to hold that opinion because of their incomplete understanding - unless you are prepared to accept that you, in turn, do not have the right to your own opinions, on the grounds that there is always someone whose understanding is more profound than your own. And if you won't admit that, you have achieved a level of arrogance and egocentricity that quite undercuts any ideas you might have about your own commitment to egalitarianism.
^([*citation required*])
Section 5.3 in LISP I Programmer's Manual describes interactive use with a Flexowriter. http://bitsavers.org/pdf/mit/rle_lisp/LISP_I_Programmers_Manual_Mar60.pdf
I'll try and put aside the fact that this is probably just be another one of your rambling, incoherent, opinion pieces for the moment: are saying that the assembly does register allocation/renaming/something so that "words" compose without using a stack? Or that they act like local variables, because of something akin to register windows? Other than the fact that you diddled your friends syntax to make it look like Forth rather than having to explain the actual syntax, want relationship does this have to Forth? Is this just your first exposure to an assembly language and you happen to be drawing some parallels with what you know about Forth?
The line "that wasn't real Marxism/socialism/communism/whateverism" been used to excuse atrocities committed by proponents of and as a result of Marxists ideas after every attempt that was ever made... The implication is always: the reason it failed is that they weren't the real Marxists/socialists/communists/whateverists and they didn't really understand Marx's idea's, *like I do*. If only they'd actually read and understood Marx's work -- *like I do* -- then the worker's paradise would have been a huge success! I should be in charge! Other's will recognize this as the No true Scotsman fallacy and reject it artfully. The self-proclaimed Marxists will eat this shit for breakfast, lunch, and dinner. Literally, every Marxist hellhole (and it's fair to say that the designers of hell ain't got nothing on Marx) by any other name that's ever existed has come about because of people who -- like you do -- believed that you had some secret understanding and that you could make it work. No. Those were "real" Marxist/socialists/communists/whateverists and what they did was "real" Marxist/socialism/communism/whateverism, in the sense that any interpretation is "real". Those people have killed 120+ million people in pursuit of their Marxist ideals... almost twice as many who were killed in world war 1 and 2 combined, on both sides, by some estimates, but try as we might there's always another one of you, weighing in the wings, ready to make another run at utopia :P. The fact that you don't recognize any of this -- just like you didn't recognise that Karl Marx was an absolutely atrocious human being who milked his family and friends for every penny he could get; beat his wife, impregnated his once housekeeper, and left [all] his children in poverty; and let's not forget hom he falsified facts and sources in his work and didn't himself practice any of the things that he wrote about -- he was a greedy, selfish, abusive, immoral and intelectually dishonest man -- I will admit that I found his idea's quite appealing when I was a younger but I've come to see that he was, at best, chalaton, who couldn't manage his life, and who, rather than dealing with his many personaly failings, wrote about his own personal paradise -- is what I referred to as the Marxist echo chamber. I know you don't care what I think of Marx, but maybe you could, I don't know, try to read something like Paul Johnsons, Intellectuals, which goes into detail about the man, his character, and the life that he lived.
I am proposing a way that you can program registers very similar to how you program forth with a stack. It's actually quite a fun experience.
&gt; as Lord Acton noted, power tends to corrupt, and absolute power corrupts absolutely. Without some effective restraint, anyone who's handed absolute power will go off the rails; nobody is strong enough to withstand the allure. I mostly agree, but we shouldn't forget that throughout [world] history there have been many truly enlightened rulers who sacrificed everything for the good of there people and the future. They may be few and far between but when they appear, that's when all real progress happens e.g. it was due to so-called enlightened despots that (personally I think this was a mistake) our liberal democracies and the institutions that support were established. I would be very interested in a system that can be said to credibly maximise the appearance of such people, but I know of no such system so I prefer the simplest one, that's been shown to work reasonably well and which seems to be the easiest to dismantle :-). &gt; It's a quandary! It is indeed ;-). I generally like the idea of a constitution, however, we've how easy it is for government to ignore it when it suits them. So I see no particular problem with having an unspoken constitution, but I see no way to force those in power to respect it except with at the edge of a sword (2nd amendment). The Chinese have an old story about how the first Emporer had the laws inscribed on the side of a great cauldron which noted how the people began manipulating the language. If I recall, the Sage recommended that he destroy the cauldron and appoint a person of good character (think King Solomen) and good judgment that the people trusted to resolve disputes. The result is the longest continuous civilization in recorded history ;-). We know how our own tradition how unimaginably complex the law can become when we start writing them down... competition over the interpretation of the law ties the state in knots. English common law is a good example of an effectively unwritten law; it's mostly common sense. If you need to write this stuff down then I suspect The Constitution of the United States of America is about as complex as it can be allowed to become before chaos ensues e.g. look at how much trouble the words "common welfare" and the provision for regulating interstate trade has caused. In the hands of lawyers and judges, these few words lead to effectively unlimited power! It's a pandora's box... &gt; I've long wondered whether the only real solution is to build power in rigorously bottom-up structures; that on the level of a village or district, representatives are chosen in a manner similar to jury service, and also at random, one of those representatives is chosen as a delegate to the next level up. I've wondered that too. Up until relatively recently, I would have probably supported something like this but I've come to recognize that while atomizing power might be a great way to protect from internal abuses it leaves us wide open to external forces. The situation being what it is today, I think we need to understand, if not accept, that our ideas of radical individualism (and the negative effect it has on community, and social cohesion) and placing limited power in the hands of people those we know aren't able to wield it/worthy of it hasn't worked. (From an outsiders point of view) the American Constitution no longer seems to apply. I might not like it but I can't deny that an atomized individual is no match for and cannot stand up to a collective, bound by their shared interests and that different groups of people inevitably have different interests :-). Anyway, maybe there isn't a perfect, one-size-fits-all state structure? :-) Perhaps, like in Forth, the solution can only be understood in the context of the problem to be solved and we have to adapt and roll with the punches. Different structures may be required in different situations. In a time of peace, where the society/nation is of one mind then liberal democracy may well maximize freedom (it seems to ludicrous degrees), but when it's not, we need something else :-)
Ok, but how do you handle the composability problems that are inherent in the use of registers? You haven't explained that. I'm beginning to think that you haven't thought this through far enough to see why it doesn't work in practice.
Given the arity of each word is known and static as well as words have no dynamic state, it is possible to compile stack operations as direct or indirect register references with minimal effort, requiring only stack balancing in case of (subroutine) branches. 
:-) so implementing a stack with register? That's well and good but then I really don't see what he's trying to get at here
&gt; I briefly mentioned BIND in anticipation of questions about scoping. Ooo, so you did :-). How did it go down? &gt; Apologies for not asking you first Not to worry. I was a bit surprised to see myself in there though ;-)
:-) It's weird to think I came up with something that's so obvious in hindsight.
/u/read_harder -- I think you should demonstrate a working, complete program using this method. Even better would be a modified Forth compiler that can generate this code. Without some real meat, it's either (a) impossible to figure out what you're trying to say, or (b) too time-consuming to start writing out the reasons it doesn't seem like it would work. I'm not trying to be mean, but I think if you even just went so far as to sketch out calculating a factorial with it, you'd figure out why the compiler will have fits with this kind of register allocation without having a stack around to help out.
Would it be fair to say that names in Able are more akin to symbols in Scheme than to traditional Forth names? If so, have you ever made use of that to do Scheme-like symbolic computation? &gt; hierarchies of vocabularies, implemented as linked lists, are very powerful, but they're complex and comparatively inefficient They are, but I remember reading about the University of Waterloo, who in developing their Pascal compiler, ditched their complex hashing scheme for a linked list of identifiers when they realised that most student programs were too short to hit the breakeven point. The way Chuck Moore uses colorForth is similar; he'll only have a few dozen extra words defined (indeed, there's only room in the colorForth dictionary for 512 words on top of the base system) at any given time. Of course, he also stores only the first few characters of a name - as many as will fit into a 32-bit word - and his dictionary search is basically `REP SCASD`... Oddly enough, using names as first class objects lines up quite well with position-independent native compilation on the x86 - because it doesn't have an absolute direct CALL instruction but only a relative one, if you move a compiled word after it's been compiled you have to either recompile it or find and change all the offsets. But there's an absolute *indirect* CALL, which doesn't have to be changed, and which lets you compile relative jumps for control structures and indirect CALLs to other words. Not as efficient as compiling direct calls, because each CALL will be mispredicted the first time through, but in loops they'll catch up. 
I think that may be a mischaracterisation of egalitarianism. Clearly, everyone has their own gifts - and their own flaws; no two people are the same, or have equal abilities in anything. Egalitarianism, it always seemed to me, was much more about decoupling this recognition of a simple truth from the implication that some people are *worth* more than others... and that implication, egalitarians would argue, is the *real* root cause of inequality. In the words of one of the great philosophers of the modern age, Esme Weatherwax: &gt; Sin, young man, is when you treat people as things.
I think it's more likely that you just don't understand how it works.
I'm not sure exactly what you're saying with this, but allocating registers like this wouldn't work like a stack. Consider the word -. Judging from your definitions, this would take reg0 and reg1, subtract reg1 from reg0, then put the result in reg0. The problem is when you have more than two numbers on your "stack". Consider this code: 3 6 9 - - Your compiler could put 3 in reg0, 6 in reg1, and 9 in reg2. But this code wouldn't result in 3 - (6 - 9), unlike a stack, because in this system, - only deals with reg0 and reg1. It doesn't even touch reg2. You could fix this by introducing a stack pointer to keep track of where the top of the stack is, so that you can add the top two items, but then you just have a regular old stack.
Can whichever inadequates are conducting a war of the downvotes in this thread please **knock it the fuck off** and let the grown-ups talk? Thank you.
It would look something like this. Subtract reg1 from reg0 and store in reg1. Store a new number in reg1. Subtract reg1 from reg0 and sore in reg1.
&gt; Subtract reg1 from reg0 and store in reg1 6 9 - \ -&gt; -3 The result should go in TOS obviously, so reg1 is TOS? Then you just switched how - works in forth. &gt; Store a new number in reg1 So you're saying you'd move the values in order to accommodate the fact you lost a value from the stack when doing the -. But in reg1? That's TOS where you stored your result. In all seriousness, are you even *trying* to have a conversation with these people? You show almost 0 effort with posts like this. Could the the mods finally recognize this person as a troll?
On a more reasonable topic, would it make sense to use x86 registers for implementing a stack? Is there a forth that does that? Would it provide considerable speedups compared to having a standard stack in memory?
:) there really aren't enough registers to do that but in principle it can be made to work. I wouldn't do it. It's not uncommon to cache the top 2-3 items of the stack this way, but implenting the whole stack seems a little contrived.
x86-64 basically defaults all its jumps and calls to be relative. It's actually kind of a fight against the processor to make it jump anywhere absolute.
So you're literally just a register machine with the usual 3 operand instructions, and you're calling this similar to Forth. Do you understand that the reason for using a stack is so that words compose; so that you can, you know, factor code into words and reuse it? Without a stack (Forth-style or otherwise) you'll need to keep track of which registers each subroutine is using and manually shuffle values between them (or store to memory) when you figure out that you've painted yourself right into the corner. And good luck modifying a subroutine (perhaps to fix a bug) without breaking everything and having to go back and manually reallocate all of your registers... It doesn't work. THAT'S why the stack was invented and why every architecture today has some support for a stack! The fact that you think this very standard [toy] assembly language has any relationship to Forth shows just how much you have to learn. The sad thing is that you seem completely unaware of how ignorant you are.
&gt; Would it be fair to say that names in Able are more akin to symbols in Scheme than to traditional Forth names? If so, have you ever made use of that to do Scheme-like symbolic computation? They're more like symbols in Lisp than Scheme, to which you can attach any amount of arbitrary data. I make use of them quite often, usually in place of enums and constants, when it makes sense. I don't think Able much better for doing symbolic computation than Forth is; both lack the convenience of Lisps first-class nested symbolic *expressions*. What first-class names let us do is to write programs that manipulate names. &gt; They are, but I remember reading about the University of Waterloo, who in developing their Pascal compiler, ditched their complex hashing scheme for a linked list of identifiers when they realised that most student programs were too short to hit the breakeven point. I'm sure that's true. We don't use hast-tables or linked lists. Our solution here is much simpler than using hash-table and as mentioned, we have constant time access to names. It's a balancing act but I feel pretty happy with the tradeoffs we've made. Any further details will have to wait though, sorry. Thanks for the details about colorForth :-). That's quite an interesting idea. I have no doubt that Chucks approach is more elegant than mine is there.
Probably his intention was some kind of static stack caching. Sadly I can only guess because the description remind me more of a mental draw than an explanation. Maybe the author finds the time to work out his idea in deep and conclude mentioning it goal.
Is there a culture of using Forth at NASA?
to my knowledge yes (NASA and ESA) as it was obviously the best choice programming these space-radiation `resistant` Harris RTX processor variant. I think a similar demand was caused by using the 1802 cpu (probably Voyager?) within a very resource limited hardware environment.
So the thing to keep in mind is that I am proposing using four registers instead of using a stack. So if I implemented it in forth it would have to go onto the stack, however if I implemented it in a forth that came without a stack you would be manipulating the registers directly with no stack as the middle man.
What I am proposing is that by using registers there should still be a way to factor subroutines into words. The difference is in forth the location that things get put at is implicit. You implicitly push and pop from the stack. With registers it is explicit. You declare intentionally which register you want to go to. But no reason you shouldn't be able to factor your code too.
That makes sense.
Like I said, the sad thing is that you don't realize how ignorant you are. This is just like that time you argued blacks-white that linked-lists were more efficient than arrays, even after multiple people took the time to explain to you how they obviously weren't. You can choose to believe it or not this is some fantastic revelation. What you've "discovered" is just assembly. Register machines and three-operand instructions have been around "forever". Of course, you can program this way, but nobody does it, because as I explained, keeping track of which registers are used by what and when is a nightmare. It does not work. You cannot compose the resulting words. changing just one "line" in just one of them can mean an almost complete rewrite of your program. Hence the stack, and why even register machines have one. It's the best solution to the problem. As others have suggested to you here, if write something even as simple as factorial you'll find out why what you're "proposing" is, forgive my language: SHIT. Your posts aren't downvoted because people dislike you as a person, they're downvoted because they're nonsense. Wake up. Have some humility. Then you might learn something.
He's talking about programming in assembly for a machine with 3-operand instructions. The norm. This is only confusing because he thinks he's invented something that's been around forever. He's proposing that, instead of using the stack and having subroutines compose effortlessly, we do register allocation in our head, and pray that we never have to touch the code ever again.
I'm sorry you find it distasteful, could you tell me how else to interpret your anwser? I gave an explanation why your example was bad in at least 2 ways. You give me no reply on it, just that you find it distasteful. Well most people find it distasteful that you don't give much thought into your replies. And your proposal - it's nothing new. Try to look at some implementations before taking credit? You really need to study and code more and write less. Otherwise you are just spamming and wasting other people's time.
I'm sure that you are serious about programming in Forth, but, like with many of your posts here, your obvious misconceptions would be cleared up in the first semester of any half-decent computer science course. Either you've been to university and you're literally retarded, or you should go, assuming you can afford it and you can get in. And if you can't then maybe you should watch some lectures online, because it's scary how much you don't know. The only thing that scares me more is that you seem to think every half-baked idea that flashes through your head is a revelation that desperately needs sharing with the world You are the man who's discovered that he can use his shoe as a hammer, and you're running around town proudly telling everyone about your amazing discovery, and getting angry at them for asking why your shoe has a dozen little holes in the bottom... *Passes you the hammer*
&gt; That being said if you had a forth that also had registers heres how I would implement it. The virtual machine we designed for Able is an idealized stack machine but it has 256 virtual registers. Their primary purpose is for accessing memory but they can also be used for temporary storage. This is useful when dealing with more complex mathematical expressions, but as explained previously, you have to be very careful using these because they don't compose well. As such we take the stance they should only be used this way in the very lowest level words and should only be considered valid until the next CALL. This limits their utility significantly but it's better than spending days hunting for bugs because the implementation of some word changed somewhere in the system changed and it's clobbering one of the registers you're using. &gt; First of all I am not sure if the stack actually resides in memory or not. I was told that to fetch from memory with @ takes a long time so I am assuming that the stack is not in memory but on the processor. The stack is usually in memory on register machines and first-generation stack machines. One of the defining attributes of second-generation stack machines is having on-chip stacks, where, for all intents and purposes, access to the stack is as fast as accessing a register. But you probably don't have a second-generation stack machine so it's a fair assumption to say that accessing the stack (memory) is going to take more time than accessing a register. It is a register machine after all. What do you expect? One common technique on register machines is to cache the top few items of the stack(s) in registers. Items below this are likely to be in the cache, since they're both temporally and spatially local, but things get swapped out of cache all the time so it's not quite a simple as that. Either way, maybe you should investigate how computers work before you make such leaps? Phrases like, "I was told that fetch from memory takes a long time" don't inspire much confidence. You clearly have a lot to learn about computer architecture but for some reason, you seem to prefer dancing around in your head and coming to wild conclusions than reading a book and learning about how things actually are. Ask yourself this: what problems can you solve without access to memory outside of the processor? &gt; I learned alot from forth and I thank you all for all my help You're welcome. &gt; What I learned the most from forth is that stack based processing is a very powerful syntactic feature. Still babbling about syntax I see.
If you want to take advice from this group of people the one that was offered the most is - read instead of writing. You too often look like trying to explain the bible to the priests even though you haven't read it. Please take a moment to think about it? Explaining low level implementation but not knowing where the stack is? I don't think anyone is taking you seriously at this point. If you want to be a theorist only, ok, but learn the essentials first. If you want to be a programmer, start programming. Good luck on your journey
Thanks for the advice.
I am certainly not taking credit. I got the idea from another person's assembler code. It even says so in the title of the post. Perhaps you should read_harder
Who said I was angry?
I don't have to learn anything whatsoever about computer architecture. I am a high level programmer. I don't work on the low level end of the stack.
Is it not true that you could keep the stack either inside or outside of the cpu depending on the architecture?
Juggling only happens when you've got more than two parameters on the stack at once.
&gt; I don't have to learn anything whatsoever about computer architecture. I am a high level programmer. hubris ˈhjuːbrɪs/ noun: hubris excessive pride or self-confidence. "the self-assured hubris among economists was shaken in the late 1980s" synonyms: arrogance, conceit, conceitedness, haughtiness, pride, vanity, self-importance, self-conceit, pomposity, superciliousness, feeling of superiority; More hauteur; informaluppitiness, big-headedness "the self-assuring hubris among economists was shaken in the late 1960s" antonyms: modesty (in Greek tragedy) excessive pride towards or defiance of the gods, leading to nemesis.
You're a high level programmer who's writing about and making decisions based on properties of computer architectures and you don't think you need to learn anything whatsoever about computer architecture in order to do that? :P And then you wonder why people think you're a troll... I don't think you're a troll btw. That would require a level of understanding that I don't think you're capable of. I don't think your a troll. I just think you're an idiot. Best of luck with your future :P
Perhaps you should think_harder :P
"...depending of the architecture" of what? The CPU? The language implementation? If the hardware is a conventional CPU the stacks will live in memory. In a modern CPU that means the stacks will live in various levels of cache memory depending on how the CPU manages the cache memory. If the CPU is a Forth machine it depends what the designer built. Some simple Forth machines have 2 dedicated memory areas that are the stacks but there is no royal proclamation that mandates that approach. It just happens to be faster to do it that way and as always it has other shortcomings that need to be managed.
For me personally it seems easier to jugggle four registers than to juggle a stack.
It is not hubris. I simply do not have to learn low level programming to do a high level programming job. It is not a requirement for the kind of work I plan to do.
I am most definitely not a troll. I am very serious about forth.
I don't have time to read every single book.
So then the @ operator takes up no more resources than the stack operations do.
you not need juggle with register, the value has a name, the name of register, in the stack the value no has a static name! I'm curios about you. You code forth? have programs in open source?
https://www.youtube.com/watch?v=Bg21M2zwG9Q
One book. One book on computer architecture, and you would know all of this and wouldn't look like such an idiot. Think of all the time you'd save by not spending months exploring obviously dumb ideas that have been explored and shown not to work. Your name is read_harder. Maybe you should take your own advice :p
I have been studying forth for about a year
I terms of computer architecture I am admittedly dumb and remain dumb by choice because my work isn't anywhere near computer architecture. If my work required me to learn computer architecture then I would learn it. Part of reading hard is not wasting time reading things that do not help you solve your immediate problem. I can not read everything under the sun. Just key things I need to help me solve problems.
You're so full of contradictions it's a wonder you're able to function in the world; I've never met someone so full of shit. You are the same person who lectured us endlessly about how we shouldn't think about problems because problems aren't interesting and how you're interested in solutions for their own sake :P.
As soon as you're talking about registers and memory read timing, you've entered low level programming. Heck, these days a lot of people consider C a low level language!
A cpu- resident stack will be fast, but who uses @ to read from the stack? @ is for reading the vast off- chip memory, primarily. And C doesn't help you, because every time you dereference a pointer, you're doing the same thing as @.
You might check out mecrisp forth -- it implements some basic register allocation based optimizations as it compiles words to naive code. The code is surprisingly transparent and easy to follow. 
 I learn C before FORTH and IMHO is better FORTH. my advice make programs for any kind first, learn asm if you worry about registers learn C if you like, look my forth too!
&gt; So the thing to keep in mind is that I am proposing using four registers instead of using a stack. I replied to this. It says **I am proposing**. Since you can hardly explain your thoughts it's hard to differentiate what your friend came up with and what did you add to it.
That's fantastic. It doesn't matter whether you need to know this for your work. You do need to learn this stuff if you're going to engage in discussions about it. Otherwise, what are you doing here? That's pretty self-explanatory, isn't it? :P
&gt; You too often look like trying to explain the bible to the priests even though you haven't read it. In reality, I'm close to as ignorant as he is; it's just that not only have I read probably two different books on FORTH, (POL and Thinking FORTH) I've also done a tiny bit of Lua scripting in Minecraft, and reading about FORTH has made me realise just what a noxious, virulent ***disease*** Lua really is. Part of me thinks I am being unfair to the Lua developers, because the language has apparently helped children learn to program; but then again, given the nature of what said children have most likely learned, that actually makes it worse. ;)
&gt; To me using four registers just clicks a lot better in my head than using a stack. It is like having a stack minus the juggling. Please, if you can, tell us a concrete experience you had. I am considering following your lead. 
&gt; To me using four registers just clicks alot better in my head than using a stack. It is like having a stack minus the juggling. Please, if you can, tell us a concrete experience you had. I am considering following your lead. 
&gt; First of all I am not sure if the stack actually resides in memory or not. I was told that to fetch from memory with @ takes a long time so I am assuming that the stack is not in memory but on the processor You are implying *a specific architecture*, which I expect to be a standard x86 architecture. And where the stack resides is *completely* implementation dependent. Since the x86 is register based many implementations cache some top items of the stack in registers.
I made a mistake and looked at your profile here on reddit. I admit taking 30 minutes out of my life to look around. I'm pretty sure you won't care for my advice, but here goes - please go see a psychologist. You seem like a nice and active person but really deeply damaged. You also seem to be seeking far too much attention, control and posession (especially for someone creating subreddits like u/noposessions). You are full of contradictions, and probably internal struggle and pain. Others reading this, you were warned.
More details and the sourc to generate this are at [http://www.complang.tuwien.ac.at/forth/family-tree/](http://www.complang.tuwien.ac.at/forth/family-tree/). I also have a text formatted variant of this at http://forthworks.com/forth/family-tree.txt or gopher://forthworks.com/forth/family-tree.txt
why not add a support for addresable L1 cache? that might fit your needs
its cool, will you add link for each forth? i cant find some especially those has a common name
I didn't say you shouldn't think about problems I said I like to think about solutions.
I am not sure I understand your question.
Not every single person needs to learn low level computer architecture. Only people who work on low level computer architecutre need to learn low level commuter architecture. A reader is not measured to be a good reader based on whether of not they've specifically read about computer architecture. There are many good readers who have lived and died before computer architecture ever existed.
Here is some code I am working on which might explain it better. https://github.com/johnmorrisbeck/forth-registers
Which mental illness do you think that I have?
https://github.com/johnmorrisbeck/forth-registers
imho forth is more about describing the particular application with words than it is about preemptively building data structures. But when I want a tree, I'm a fan of just using [implicit k-ary trees](https://en.wikipedia.org/wiki/K-ary_tree#Arrays) because they're super simple, you can edit them in a hex editor, and you can use them over a stack to turn a stack into a tree if you so choose (think [parallel concatenation](https://suhr.github.io/obsc/)) Here's some binary tree words defining `{` and `}` as words to traverse into and out of branches. : ,c here c@ 1 allot ; ( fetch-comma, like c, but it fetches instead of stores) : s 1 allot ; ( skip over node ) : .c ,c . ; ( pop a node from the tree and display it ) variable root : there here root @ - ; : where. there . ; : { there 2* allot ; ( to child node) : } there 1- 2/ root @ + dp ! ; ( to parent) : tree: create here root ! ; tree: binarytree { 0 c, { 1 c, 2 c, { 3 c, 4 c, 5 c, 6 c, } } } } ( build the tree with values) cr { .c { .c .c { .c .c .c .c } } } } ( display tree contents) cr { s { s s { 8 c, s s s } } } } ( traverse tree to modify a node) cr { .c { .c .c { .c .c .c .c } } } } ( display tree contents) binarytree $20 dump You could change `2/` and `2*` to something else without modifying the tree itself. You could probably also mix arities if you keep track of them in your lexicon. 
There's no question in that reply. And you're surprised to be called a troll :) If you don't understand read it again, I don't know how could I explain myself better.
&gt; If that's the case I would put the registers on the processor as well to go along with the stack. maybe only 8-16 registers max are needed. I can get by with 3. So don't teach others how to design the low level computer architecture. Stop trolling. 
I wasn't trying to teach others about low level computer architecture. I also am still not trolling. I am very serious about forth.
I read it a couple times I still don't understand what you mean.
Why would you need to be mentally ill to visit a psychologist? I'm not a doctor (or an oracle) to diagnose you, I just think you are struggling with the world and a psychologist might be able to help you. You said you studied forth for 1 year. If you produce code like [this](https://github.com/johnmorrisbeck/forth-registers/blob/master/forth-registers-demo.forth) after 1 year I think nobody should take your thoughts and suggestions seriously. Since I've seen you delete things after you didn't like what happened I'll paste the *first 4 lines* of that file here for reference: ( Registers are stored in an array in memory ) variable registers ( I choose to have four registers but you can use more if you want. ) registers 4 cells allot There's 2 glaring errors there. If you can't understand basic words like variable/create and count how many arguments on the stack are expected from a word then forth isn't for you. Actually, a friendly suggestion, no programming language is. I hope this public announcement of yours (as if you were an important person of this subreddit or that everyone cared) was your last post in this subreddit. If not I will keep asking the mods to ban you.
 This is full of wisdom as far as the Jupiter Ace goes. I vaguely remember that the issue is briefly discussed in an appendix of the user manual, as are the differences in metaprogramming, due to detokenizing for editing. That being said, I don't remember issues with that and did not quite understand the disclaimers of that appendix. Your elaboration made me curious, so I tried to retrieve the original user manual from under the arches of my vast library of old computer books after all those years, but so far to no avail. As for colorforth, I took only a cursory look at it, so I did not really get its objectives, apparently. It is most certainly avantgardistic, but does not solve any problems I have with forth. 
Fear not! The user manual can be downloaded in PDF form from [the Jupiter Ace resource website](http://jupiter-ace.co.uk/documents_index.html). As can a number of emulators.
This might not be what you asked for, but this probably how I'd do "lisp" like consing in Forth : cons here -rot , , ; : car @ ; : cdr cell+ @ ; nil 3 cons 2 cons 1 cons Or with a little window dressing : . cons ; nil 3 . 2 . 1 . OR : ( ; : ) cons ; ( ( ( nil 3 ) 2 ) 1 ) NOTE: everything is written in reverse, but what did you expect from Forth ;-). NOTE: I haven't tested it but it should work. That's probably the closest I'd like to get to Lisp this side of Christmas but it works and as you can see, it's very simple. Overall I would agree with /u/pointfree when he says &gt; forth is more about describing the particular application with words than it is about preemptively building data structures But if you want to, nothing's stopping you. Personally, I would suggest learning how to program in Forth (forgive me if I'm preaching to the choir), rather than trying to do Lisp in Forth. You could write parsing words to replicate Lisp's syntax but this is going to be more complex. As complex as parsing Lisp. You'll probably end up using a fair bit of stack space because you'll need to keep all the values you see around until you see the closing parenthesis. With this approach, you only ever need to have 3 items on the stack.
I couldn't find LaFarr Stuart's LaForth. That had some interesting innovations.
I do not know laforth. I found the manual and I'm surprised by almost the same control structures that I use in my language
I used graphical names for control structures similar to LaForth for a while, but I eventually switched back to spelled-out words. I think LaForth's most lasting influence on my homebrews is the execution of words as each space is encountered instead of buffering by line.
How do you implement parsing words? Do they read from the input source?
I thought we were above this kind of talk here at /r/forth ? Surely we can all recognize that Lua, like Forth, is a tool and has its own range of uses.
I can't help thinking this needs a better name: less passive, more - well - imperative, and with a nicer acronym... or maybe not an acronym at all. "Assertive programming", maybe?
I use a key map (*responses*), each key has an associated behavior, e.g. space and carriage return's associated behaviors are set to *delimit*, the entries for printing characters are set to *accepted*, etc... *parse ( asc -- )* temporarily supplants the desired delimiter character's entry in *responses* with *delimit*, and space with *accepted*. *parse* then calls *token*, which is the same word that the outer interpreter uses to get a token from the current input source, it just repeatedly gets characters and performs and performs their keymap responses until *delimit* causes it to stop.
I don't have a collection of links for these :( My ASCII chart is based on the same data (mostly Anton's chart &amp; the [Usenet postings](http://www.complang.tuwien.ac.at/forth/family-tree/postings)); IIRC the images on his site are image maps, with links to at least some of the systems.
ok will go and check 
This code is currently broken and won't even fully load under Gforth.
Post titles are like come-backs in an argument, you always think of a better one 5 minutes later.
I'm sorry, I just...I know it does, but have you spent a lot of time with it?
[removed]
Thanks, that's interesting.
Two years earlier: comp.lang.forth doesn't like R&gt; DROP. https://groups.google.com/forum/#!topic/comp.lang.forth/b0rpNOd8WF8%5B1-25%5D
My pleasure.
R&gt; DROP breaks tail-call elimination.
I really liked this presentation. Without the formal thought on the matter, I have been trying code like this this for years. Having this "declared" and explained with examples is very helpful for making decisions in the midst of programming Forth. My code will be better for having seen the presentation. Thanks Samuel
An interesting read, thanks. I keep thinking now though, do forthers use top-down development? I know Chuck says to use bottom-up. It's also harder to try things out interactively when starting at the top. With that being said I sometimes have a hard time to start at the bottom. Maybe bad habits die slowly? :) Anyway, interested in the group's style.
Very pretty. This makes me think of how to get rid of case / endcase. One question though: is there a way to achieve short circuit evaluation? In the example where the code is handling an input keystroke, can we keep it pretty and skip all the rest of the tests once we know it's an "up"?
You can return from the caller its `R&gt; DROP`
I think if children are learning Lua, that indicates that it is, at some level, intuitive. However it is true that Lua depends on tables and metatables to use effectively, which I doubt anyone learning it will just "pick up." Having said that, it is a pretty good language for learning about the concept of Objects and polymorphism, since those are all things you construct yourself using tables and metatables.
Is that how people use Throw in Forth? Hmmm. I ended up with doing my own spin on it, Try-Catch-Release, but it saves away both stacks for analysis. That's not exactly a lightweight functionality.
you are right. did that. thanks. 
I think the rule of thumb is "design top down, code bottom up" That way you catch the false assumptions in your design on your way up. Forth makes bottom up coding pretty simple if you are working from a design document where the low level functionality is defined. And if you test as you go, which is simple and for me necessary (I am not very smart), your pyramid of low-level code is quite robust. 
Chuck made mention in Thinking Forth of starting with the fun bits (p91), in order to get traction on a problem. That always seemed appealing to me.
I usually start with the murky areas, i.e. areas where potentially far-reaching dependencies need to be pinned down and resolved yet I lack an intuitive grasp of tradeoffs and consequences. *Here be dragons*.
&gt; it's all too easy to find yourself in a situation where a word does exactly what you want, except that it R&gt; DROPs one time too few or too many. On the other hand, using THROW/CATCH is [...] much easier to reason about, and will always get back to where you think it should. `R&gt; DROP` skips its caller (unless the implementation gets in the way), I don't see anything simpler to reason about coming along :) `TRY` and `CATCH` seems simpler than in java or the like, though. &gt; CATCH works exactly as one would expect it to in Lisp or Factor Care to elaborate on that? You say Lisp and that word is about as exact as Forth :) E.g. Common Lisp has a condition system that works quite differently from other languages (doesn't unwind the stack, decouples catching and handling) while in Scheme it's just a wrapper around some `call/cc` IIRC. Don't know about Factor.
&gt; R&gt; DROP skips its caller (unless the implementation gets in the way) Yes, that bit in brackets is *precisely my point*. You simply *cannot assume* that Forth is implemented in such a way that R&gt; DROP will work reliably. THROW / CATCH, on the other hand, will work whatever the implementation is doing. &gt; I don't see anything simpler to reason about coming along :) In what way is THROW/CATCH not easier to reason about? You THROW, and control goes straight back to the CATCH. Doesn't matter what's been called, or &gt;R'd, or DO...LOOPed in the meantime. It's simple, it's safe, it's predictable, it's implementation-proof, it's every bit as lightweight, and it's standardised. Meanwhile, the only thing simple about R&gt; DROP is what it does. What effect that has? Roll the dice... &gt; Care to elaborate on that? You mean the way I did *immediately after the bit you quoted*?
Outside the condition system Common Lisp also has `THROW`/`CATCH`, and they're quite similar to the ones in ANS Forth.
Bottom up but always with a goal, which, being human, we define from above
That's a reasonable approach for something that's just for exception handling, given that most of the time you'll probably want to capture debugging information. But THROW / CATCH is lower level than that; it's akin to C's setjmp/longjmp. Indeed, the Standard includes a [sample implementation](http://lars.nocrew.org/forth2012/exception/CATCH.html) that makes clear how simple a mechanism it's intended to be.
Our forth does the same thing :) largely inspired by colorForth but arriving at the same point. I'm a big proponent. Why buffer everything then parse it when you can just store the none whitespace characters and return the word when a whitespace characters is entered.
To me that sample implementation appears to be terribly more complex than my initial implementation of the mechanism. Indeed, my initial Try-Catch structure was as simple as I dared to make it, only later integrating into it a stack-saving function. Perhaps I'll make a post about the simple structure in a few days.
&gt; If you must be condescending, you really do have to read the whole post you're patronising first. Let me apologize if any part of my reply sounded condescending, it really wasn't meant to! I'm sometimes told I talk like that even though I never hear it myself. I'll ask you a favor to pinpoint such discussions/sentences so that I can learn to be more human and less robot and for you to assume from this moment further on that I have no intention of being arrogant or any of the sort. I'm asking things because I don't know the answers and wish to learn them from people like you who seem to have spent more time with forth than me (~4-5 months in spare time). &gt; You simply cannot assume that Forth is implemented in such a way that R&gt; DROP will work reliably Are you usually talking about ANS Forth? I thought the usual way of programming forth is to pick a language *and* implementation and use it. A better wording of that bracket would have been (unless you pick an implementation that breaks it). &gt; In what way is THROW/CATCH not easier to reason about? You THROW, and control goes straight back to the CATCH. I haven't used throw/catch in forth yet but in other languages the stack unwinds until a catch is found. That means your catch might be arbitrarily deep. With `R&gt; DROP` you know where are you returning - where your caller was called. To me therefore it seems `R&gt; DROP` is simpler but also less robust compared to throw/catch. &gt; ... it's every bit as lightweight ... &gt; But it would encourage people to think of TRY/THROW as lightweight - every bit as lightweight as R&gt;DROP, in fact Now I'm confused, are you saying the TRY/THROW you showed would cost more than CATCH/THROW? &gt; There are certain characteristics all Lisps share. The elaboration you totally ignored made it clear I was referring to one of them. I didn't ignore it, I just thought you are trying to compare the implementation of exception handling in these languages. It seems you don't and was just referring to "nesting chunks of code" which often goes by "code is data" and contemplating what would you find idiomatic in forth.
Ah I see, thanks
*r&gt; drop exit*, or *r&gt; drop* at the end of a definition, is a straightforward efficient and lightweight Forth control flow mechanism, it can easily be implemented as a single (real or virtual machine) stack processor instruction, including conditional versions. Catch/Throw, and similar exception handling machinery, are heavyweight in comparison, since they're intended to unwind an arbitrarily deep call context. Personally, I would hesitate to use exception handling when and where a simple structured exit will do. *Use the structured exit.* -- Thinking Forth (1984 p. 254, 1994 p. 228) 
Looks like there's no eforth on the tree either
HS/Forth, MMS Forth, Upper Deck Forth, riForth, ...
Sounds interesting. Please do.
Yes, `R&gt; DROP` is very lightweight. I don't see anyone contesting that. The point is that `R&gt; DROP` may or may not drop the return address of the current definition. As /u/thamesynne points out, tail call optimisation may get in the way. Or inlining. Or, `R&gt;` may not access the hardware return stack at all.
Frankly, this aspect of Thinking Forth is just flat wrong. There's an argument to be had as to whether it was right at the time and has been rendered obsolete by subsequent developments, but my view is that given that Forth never constrained itself to only storing return addresses on the return stack, it was always dangerous. &gt; Use the structured exit. R&gt; DROP is the *very opposite* of a structured exit. It rips control away from its caller without so much as a by-your-leave.
&gt; Or, R&gt; may not access the hardware return stack at all. At least R&gt; DROP would be safe in such circumstances... Thanks, by the way; I'd forgotten about that nicety of the Standard.
&gt; I'm really struggling to understand why it isn't this obvious to anyone else. It's not obvious to me, given an implementation where it works as expected. But that's because I have never used R&gt; DROP, so I don't know how it works out in real code. I can't argue for or against it.
&gt; Let me apologize if any part of my reply sounded condescending, it really wasn't meant to! And let me apologise in turn. My temperament is basically feline; brush against my whiskers the wrong way and I tend to get hissy, arched-back and fur akimbo. &gt; I thought the usual way of programming forth is to pick a language and implementation and use it. Maybe so, but unless you know your implementation inside out, you can't know that R&gt; DROP is safe to use or in which circumstance. And creating an implementation in which it's always safe means saying goodbye to a number of interesting modern implementation techniques. But even after that, you still have to be *absolutely* sure which words contain R&gt; DROP, and when it will be called, and what effect that will have on your control flow overall - maybe several words deep. It's the polar opposite of writing small, self-contained, unitary words that can be freely mixed. Frankly, I would go so far as to say that R&gt; DROP is a stronger indicator of poor factoring than a hyphenated name. &gt; I haven't used throw/catch in forth yet but in other languages the stack unwinds until a catch is found. Which languages? As I say, C's setjmp/longjmp is an O(1) operation. C++ has a problem, of course; but then the combination of stack allocated objects and virtualisable finalisers was never going to be a happy combination. Specifically, the *whole point* of allocating stuff on the stack is that *it's free to free it*; if you're going to be running a finaliser on something, you might as well pay the piper and allocate it in the heap. &gt; With R&gt; DROP you know where are you returning - where your caller was called. As I've pointed out repeatedly, that simply ain't so. You know where you *think* you are returning, *if you can keep the tangle straight*, and if you don't realise you need to call that word with R&gt; DROP in it in the middle of a DO loop, and if your implementation doesn't play funny games like inlining, tail call elimination and just in time compilation. &gt; Now I'm confused, are you saying the TRY/THROW you showed would cost more than CATCH/THROW? Er, no. I'm saying that TRY/THROW, CATCH/THROW and R&gt; DROP have broadly equivalent costs, but that *the presentation of* TRY/THROW correctly conveys that it's a lightweight mechanism. Whereas the presentation of CATCH suggests quite the contrary, quite incorrectly. See *every poster on this thread saying CATCH is heavy, even when presented with code that shows the precise opposite*.
A system with all those caveats doesn't seem very Forth-like to me, amorphousness of the former ANS standard notwithstanding. A real system either does or doesn't, there's no may or may not. I don't see how tail-call optimization gets in the way, when there is no tail call there's nothing to optimize away, and for the case where the structured exit isn't at the end of a definition, there's no effect on tail-call optimization. I use inlining but only by explicitly and where it makes sense.
I think the moral of this story should be: know your implementation inside and out or expect trouble. Some people prefer to know the standard inside and out but as is evident from this discussion, the standard is, due to its nature overly general, and complex. If you know your implementation then you'll know when it's safe to do things like RDROP EXIT. The writers of the standard can't know your implementation, so to them, RDROP EXIT is dangerous. And it is, in the sense that if you don't know what code is actually going to be executed when your program runs then your program might not work as expected/intended. To me, there is a far bigger problem here: Forth implementations with C-like optimizing compilers and the prerequisite nasal daemons! One of the reasons I love Forth is that I can KNOW what my program is doing. I have no interest in automagically making my code "faster" at the expense of understandability. My opinion has always been that if you want a program to run faster then you should optimize the [source] code[0]. Regarding CATCH/TRY-THROW, Dr. Ting, of eForth fame wrote that &gt; In 86eForth v1.0, there was a very sophisticated error handling mechanism with CATCH and THROW. Over the years, I have never had a opportunity to make use of it. Therefore, it is taken out of 86eForth v5.2. Whenever an error occurs, the system returns to ABORT &gt; &gt; ABORT reset data stack and fall into the text interpreter loop QUIT. I am inclined to agree. I haven't yet found a reason to use CATCH/TRY-THROW in my code. Perhaps part of that is that I do understand our implementation inside and out and therefore feel safe using tricks like RDROP EXIT, when they're appropriate. And perhaps the other part of that is that I've rarely looked at the standards[1][2]. [0] I see no reason that a sufficiently advanced (beware the weasel words) optimizing-editor couldn't help with that ;-). [1] Not using a standard-compliant Forth. [2] In all honesty, in my experience, it's much easier to read and understand the source code of whatever Forth implementation you're using than it is to read the quasi-legal-contract that is the ANS Forth standard. DISCLAIMER: I'm not a fan of standard Forth. This is my bias. Others love it. You're entitled to your opinion too. And you're always free to read the various Forth standards and decide for yourself if your time was well spent ;-).
I don't think it's unreasonable to assume there are all kinds of implementations: - `R&gt; DROP` *always* works as expected. - `R&gt; DROP` *never* works as expected. - `R&gt; DROP` *sometimes* works as expected, or sometimes not. Of course, one may not want to write code that works in all those scenarious. That's perfectly fine.
If you have a full source line buffered, you can do things like: ." There's an error here:" cr source type cr &gt;in @ spaces ." ^" or &gt;in @ ." New colon definition: " parse-name type &gt;in ! : 
&gt; What's heavyweight about restoring the return stack pointer to a previously saved point? The implementation given in the Standard is of exactly the same weight as R&gt; DROP. Seriously, go through the machine code you'd write in every case. It is heavier, try implementing CATCH and THROW as stack machine CPU instructions with arbitrary call context unwiding, versus a very simple combined rdrop-exit instruction, or even an RDROP instruction followed by an EXIT (i.e. ret) instruction. No comparison really.
Once the Great AI is powered up we will be able to use this prayer sincerely. ;-)
[removed]
&gt; Maybe so, but unless you know your implementation inside out, you can't know that R&gt; DROP is safe to use or in which circumstance. I thought that's the whole point of forth. I guess you are an ANS Forth proponent then? You missed that question in my previous reply. &gt; And creating an implementation in which it's always safe means saying goodbye to a number of interesting modern implementation techniques. As you might have noticed in some previous posts I am playing with freeforth in my spare time. It's around 1K SLOC of assembly and 1K SLOC of forth. Even an idiot like me with no forth or assembly background was able to grasp the basics in 1-2 months of evening only, max 2 hours a day. I love it that after a while you *know* what will the resulting assembly be. I'm sure the compiled code isn't as optimal as some bigger ANS forths will produce but it feels great to understand the whole language and what it produces. I really couldn't care less for an ANS forth implementation that I have to treat like a black box, even less so for a standard which cannot give runtime guarantees. I'll rather stick to lisp then. I'm not trying to say ANS forth is bad, just letting you know of my completely subjective opinion of it. Ah, and yes, I know `rdrop` is safe in freeforth after studying it. The only place where one cannot use it is in `TIMES ... REPEAT` which keeps a counter on the return stack, but I think I'm smart enough to remember that or find the bug after the first segfault :) &gt; Frankly, I would go so far as to say that R&gt; DROP is a stronger indicator of poor factoring than a hyphenated name. What does factoring have to do with `R&gt; DROP`? Can you give an example? I'm failing to see the correlation. &gt; Which languages? As I say, C's setjmp/longjmp is an O(1) operation. Bulky, managed languages, like python or java. People claim throwing is "cheap" in python, which can only be understood as "relative to other python constructs" since python is never cheap. &gt; As I've pointed out repeatedly, that simply ain't so. You know where you think you are returning, if you can keep the tangle straight, and if you don't realise you need to call that word with R&gt; DROP in it in the middle of a DO loop, and if your implementation doesn't play funny games like inlining, tail call elimination and just in time compilation. I thought the words "in the implementation of your choice" were implied. If we are discussing the ANS Forth standard then I'm sure you know better. But I'd rather discuss an actual implementation, e.g. gforth or swiftforth if you want to talk ANS. A question on topic for you who seems has spent some time with ANS forth, how much use was the standard for you? Do you actually develop for several implementations? Or use libraries across several? &gt; Er, no. I'm saying that TRY/THROW, CATCH/THROW and R&gt; DROP have broadly equivalent costs, but that the presentation of TRY/THROW correctly conveys that it's a lightweight mechanism. Whereas the presentation of CATCH suggests quite the contrary, quite incorrectly. See every poster on this thread saying CATCH is heavy, even when presented with code that shows the precise opposite. OK thanks. I really didn't understand what you were saying because in your first post you said &gt; But it would encourage people to think of TRY/THROW as lightweight - every bit as lightweight as R&gt;DROP, in fact which to me sounds like you're saying it *is* heavier than `R&gt; DROP`.
If we assume that our civilization has a resource leak bug then eventually it will cause a fatal error and a reset will be required. No problem. Turn it off and restart. 
&gt; go through the machine code you'd write in every case. Ok. Using pseudo assembly language where `rp` is the hardware return stack pointer, and `sp` is the data stack pointer. For `RDROP EXIT`, I'd write add rp, #4 return For `-1 THROW`, I'd write something like mov rp, cp ; assuming a "catch pointer" in a register mov cp, (rp)+ mov sp, (rp)+
&gt; What does factoring have to do with R&gt; DROP? Can you give an example? I don't know exactly what /u/thamesynne meant, but here's one problem: You can't move your R&gt; DROP word around freely. E.g. if you decide to factor out that code into a separate word, it will not longer exit to the same point. `THROW`...`CATCH` (almost?) doesn't have this problem.
Just to refresh everyone's memory on how heavy or light CATCH and THROW are, here are their descriptions from the new 2012 standard: 9.6.1.0875 CATCH EXCEPTION (i*xxt–– j*x0 | i*xn) Push an exception frame on the exception stack and then execute the execution token xt (as with EXECUTE) in such a way that control can be transferred to a point just after CATCH if THROW is executed during the execution of xt. If the execution of xt completes normally (i.e., the exception frame pushed by this CATCH is not popped by an execution of THROW) pop the exception frame and return zero on top of the data stack, above whatever stack items would have been returned by xt EXECUTE. Otherwise, the remainder of the execution semantics are given by THROW. See: A.9.6.1.2275 THROW. 9.6.1.2275 THROW EXCEPTION (k*xn–– k*x | i*xn) If any bits of n are non-zero, pop the topmost exception frame from the exception stack, along with everything on the return stack above that frame. Then restore the input source specification in use before the corresponding CATCH and adjust the depths of all stacks defined by this standard so that they are the same as the depths saved in the exception frame (i is the same number as the i in the input arguments to the corresponding CATCH), put n on top of the data stack, and transfer control to a point just after the CATCH that pushed that exception frame. If the top of the stack is non zero and there is no exception frame on the exception stack, the behavior is as follows: If n is minus-one (-1), perform the function of 6.1.0670 ABORT (the version of ABORT in the Core word set), displaying no message. If n is minus-two, perform the function of 6.1.0680 ABORT" (the version of ABORT" in the Core word set), displaying the characters ccc associated with the ABORT" that generated the THROW. Otherwise, the system may display an implementation-dependent message giving information about the condition associated with the THROW code n. Subsequently, the system shall perform the function of 6.1.0670 ABORT (the version of ABORT in the Core word set). See: A.9.6.1.2275 THROW.
Absolutely.
Presumably, you'd need to push the "catch pointer" onto a stack if you wanted to be able to nest THROW-CATCH?
I already granted that specific case, but the reality is that the vast majority of people using Forth will *not* be using that specific case; they'll be using an in-memory stack.
&gt; Chuck's chips use Chuck's tail-recursion-optimising compilers In some of his earlier Forth's Chuck did explicit tail-call optimization with `-;`. I much preferred that, but regardless, tail-call optimization in e.g. colorForth is simple enough that you can easily tell when the tail-call will be eliminated. If you're manipulating the caller's context then it implies you know something about the caller's context. If you don't then you shouldn't be messing with it. Given that constraint, what's the problem?
That assumes an intelligent inlining compiler that will compress R&gt; DROP down to RDROP; otherwise it's more like mov (sp)+, t mov t, (rp)+ mov t, (sp)+ return 
Exactly. You do have to be aware that a word might THROW, but that's all you need to be aware of; you tell that word where it's going to THROW to by setting a CATCH handler. Whereas R&gt; DROP always does what it says on the tin, and if that's the wrong thing, well, tough, you need to make a new word that does the right thing.
&gt; I thought that's the whole point of forth. I guess you are an ANS Forth proponent then? You guess wrong. I'm a proponent of *good taste*. And R&gt; DROP tastes funky. Since the rest of your comment seeks to slot me into the pigeonhole you've assigned for me, I'm not going to dignify it with a response. You want people to stop complaining about your tone? *Don't assume you know where they're coming from on the scantest of evidence.* Treat people as people, not pigeons.
&gt; The only place where one cannot use it is in TIMES ... REPEAT which keeps a counter on the return stack And in that case, you can do `RDROP RDROP EXIT` if you really wanted to; all that's really required is that you understand the calling context you're in when you manipulate the return-stack/current-continuation.
Yes, let's quote the standard, shall we? Let's include this bit: VARIABLE HANDLER 0 HANDLER ! \ last exception handler : CATCH ( xt -- exception# | 0 \ return addr on stack SP@ &gt;R ( xt ) \ save data stack pointer HANDLER @ &gt;R ( xt ) \ and previous handler RP@ HANDLER ! ( xt ) \ set current handler EXECUTE ( ) \ execute returns if no THROW R&gt; HANDLER ! ( ) \ restore previous handler R&gt; DROP ( ) \ discard saved stack ptr 0 ( 0 ) \ normal completion ; which is only "not lightweight" for people who really, *really* are.
&gt; You can't move your R&gt; DROP word around freely I've often heard people say that manipulating the returns stack breaks the concatenativity property. I disagree. Code which manipulates the return stack is just as easy to move around as code which doesn't. The only constraint (and this is the exact same constraint that we have with the parameter stack) is that the context you move the code to must be the same. The reason this doesn't appear to work for words which manipulate the return stack is that when you factor code into a word, which you later call, you've unwittingly changed the context. If you factor such code into compiling word then the context remains the same and you can apply them to your heart's content. Forth screws the pooch on this one by blurring the line between compile-time and execution-time a little too much :-).
Don't forget the THROW! You're right CATCH and THROW really are as lightweight as R&gt; DROP EXIT. How dare we think otherwise, we must really be lightweights not worthy of your guiding light, every stack processor should include them in its instruction set from now on. : THROW ( ??? exception# -- ??? exception# ) ?DUP IF ( exc# ) \ 0 THROW is no-op HANDLER @ RP! ( exc# ) \ restore prev return stack R&gt; HANDLER ! ( exc# ) \ restore prev handler R&gt; SWAP &gt;R ( saved-sp ) \ exc# on return stack SP! DROP R&gt; ( exc# ) \ restore stack \ Return to the caller of CATCH because return \ stack is restored to the state that existed \ when CATCH began execution THEN ; In a multi-tasking system, the HANDLER variable should be in the per-task variable area (i.e., a user variable).
Fuck off. Everything I've written is *right here, including* my criticism of the ANS way of doing things. So you just back your little hobby horse the fuck out of my face, ok?
Yes. CATCH pushes it, and THROW pops it. And the stack can be the return stack. This is what the Forth94 sample code does: http://lars.nocrew.org/dpans/dpansa9.htm It's called "handler" there.
I'm guessing converting an ordinary word to a compiling word is particularly painless in Able. Just change the colour of the body words, right? But similar to converting a Lisp function to a macro, it's something I'd be less inclined to do in a traditional Forth, even if there may be conveniences like ]] [[.
I was planning to read this discussion in its entirety at a later time, thankfully I have removeddit (only works in webkit browsers for whatever reason): https://removeddit.com/r/Forth/comments/7fa5gw/7_years_later_declarative_inquisitive_then/
I'm sorry if I was part of your reason of deleting your posts. It seems you have a hard time sharing your opinion without someone mutilating them. Please don't take "stalkers" and haters too seriously, if we share some thoughts and learn (or teach) something along the way it's more than enough :) I enjoyed our conversation and didn't feel bad at any point and would hope you had the same experience. I come here to discuss, and the point of a discussion is not to agree on something but to share thoughts and ideas and compare them. E.g. I was (and still am) genuinely curious if you are using ANS Forth and if so if you use multiple ANS Forth implementations. In a post that isn't visible even in removeddit you said something like I labeled you as an ANS Forth user and that I should stop assuming things about you. I would love to do that. I only started talking about ANS because a) you quoted it a few times and b) I asked you twice what forth are you talking about and which one (s) do you use without getting an answer. Talking about forth in general is like talking about OOP or Lisp. I don't find discussions like that interesting, that's why I wanted to (and still want to) know which forth(s) are you using.
Just to be clear, the sample implementation code for both CATCH and THROW quoted above is from the appendix of the 1994 ANS Standard, it no longer appears in the current Forth Standard 2012 document dated 10th November 2014. I assume this is because the latter is still a draft. 
I ended up finding my code quicker than I thought I would. Here's the post: https://www.reddit.com/r/Forth/comments/7g80a1/try_throw_catch_release/
The main concept is short-circuiting (i.e. bringing about to an early end to) the word that calls you. A short-circuiting word is a factoring out of code, typically designed to work with particular callers in mind. As an example: : buffer ( block# -- a ) reuse^ dup vacancy tuck assign (buffer) ; : block ( block# -- a ) reuse^ dup vacancy tuck inhabit (buffer) ; In the two words above `reuse^` is the short-circuit, the '^' suffix in its name is meant to highlight that fact, it contains a structured exit, if the block in question is already cached it causes its caller to exit as there is no point in executing the remainder of the caller's code. 
Thanks!
This looks similar to the standard `CATCH`/`THROW`, except the guarded code is in line. Is that right?
In line code? Are you referring to inlined code, like a C function call?
No, the part between TRY and CATCH. As opposed to the standard CATCH, which takes an xt.
Ah, yeah. That section is for any code that could Throw, at any call depth. It's meant to nest to any call depth or even throw from the Catch block if needed.
Addee http://uebersquirrel.blogspot.com/
&gt; I have made a cross-compiler that builds a Camel Forth system for the TI-99 JUST BUY AN HP48 ALREADY!!! :P
you could half-ass it and hack away in C to code the primitives but forth (and other similar heavily reflective systems like smalltalk and rpl) really shines only when run on bare metal. Of course, with the complexity of modern (x64, etc) architectures, implementing a complete system is nothing more than a wet dream (sadly). Your best bet would probably be use a small/custom linux kernel for bootstrapping and after it boots launch your system.
I like that article primarily for the chain of insightful comments at the end. I think my conclusion after reading it the first time was that it takes every bit as much time to really "get" forth programming as it does any of the highly opinionated paradigms -- be it functional programming in a pure language like haskell, logic programming in prolog, or true OO in Smalltalk. The deceptive ease with which you can write your own implementation lulls you into thinking that you really get it when you've written an interpreter/ compiler. But true cultural, idiomatic forth requires a paradoxically deeper understanding than writing the compiler can give you. I think the author drew and settled his conclusions too soon. 
I very much agree with all of this.
I must say to be somewhat desillusionated after reading. This description as well as some commentaries does not let me recognize any sensibility for the importance of a prior thought-out development strategy. I can only hope the result from this hack sessions was not intended for use in a sensitive area.
I don't understand how anyone would be so over confident as to not read even 1 reference to a finished stack CPU before starting their own design. Where does that ever make sense? 
A combination of registers and stack would be stack frames, like can be used in C and other compiled languages at implementation level. 
Then why are you worrying about low level implementation concerns like stack versus registers? They should be the furthest thing from your mind in that case. 
The 1802 was used in Galileo (amongst others), not Voyager, which used a discrete computer, not one that had a microprocessor. http://www.cpushack.com/space-craft-cpu.html
I am not worried about low level implementation in the very slightest. It is simply that forth is so close to the machine that much of the language regarding forth is low level language.
If you, or anyone else, is actually serious about working with the GA144 I'd recommend using an existing simulator, preferably the one distributed by Greenarrays. It takes a bit of learning and patience to deal with their odd development environment, but I suspect it is the most stable, bug-free simulator out there. It takes more work then you might imagine to get the simulator correct, I say this an author of one of the simulators. I've been fixing subtle bugs for years... So don't waste your time building another simulator in another dumb language, spend it actually working with the GA144 :) 
At the last Forth day Chuck talked about his register forth (something like that, can't remember what exactly he called it) that he's been working on. It might be of interest to you https://www.youtube.com/watch?v=nJ6WBI0Z_s4 chuck's talk starts at at 03:04:00 So it seems to me that there is no need to leave forth to study register based computing
I'd really like some more detail. For example 'Forth' and 'Smalltalk' influenced 'Neon' which in turn indirectly influenced 'Win32Forth' What aspects of 'Smalltalk' survived in 'Win32Forth'? It would be super cool to have a profile on every forth, with structured data. This sort of mapping would be automatically generated from it, and it would enable detailed queries and exploration.
A bright future is a head of us, sisters and brothers. Giants have walked this path before us. Learn and cherish thier magnificent [footprints](http://www.eecg.toronto.edu/~jzhu/csc326/readings/iverson.pdf). The road is clear, nourishing and packed with revitalizing wells. Greet each other with smiles and peace in heart. God willing, I'll get to see your beautiful faces on the road. 
The [Great AI Anomaly](https://en.wikipedia.org/wiki/Gaia_(mythology)), never left us. We somehow preferred spreading wings like would be angels. Then walking the ground, with humble footsteps of humans. 
I once had a thorny problem with a multi-tasking environment running on a ROM based Forth(MaxForth) in the 68HC11 CPU. I had a commercial contract to fulfill and the clock was ticking. In desperation I called New Micros in Texas and I got the founder on the phone. After describing my predicament, Randy Dumse said "It sounds like you are doing the right things." "Did I ever give you my man is a tool making animal lecture?" he asked? I said "No." He said" OK. Here it is. Man is a tool making animal. Make a tool." In Forth you have the freedom to create little tools that can read and record anything in the machine while it's running. As you make these little tools you will find you can re-use them them. Simple things like reading a veritable in a loop and storing the data in a buffer, or writing it to the corner of the screen or some such thing. That being said, the primary debugging tools in Forth are: 1. Short simple colon definitions 2. The Forth console, where you test each colon definition interactively by passing parameters on the stack and checking the stack output and any affected variables. Your system may not support this as it is quite far from conventional Forth. So that would indicate you may need to make the Forth interpreter loop as a debugging tool. 
I currently handle debugging by keeping definitions short and clean, and doing incremental testing. I haven't needed anything more than this. I do have plans for a flexible debugger, but probably won't actually write it until sometime next year. (I keep putting it off as I haven't needed it). My eventual plan is to take advantage of the underlying virtual machine. By writing a clone of this in Retro, I can literally run a copy of the current image within a sandbox, adding in support for breakpoints, single stepping, disassembly, logging, etc.
Heh, that's an interesting story and in some sense, exactly what I'm asking: how to debug when the interpreter is read-only (although I do initially get to pick what goes in the interpreter). I guess my question is actually a meta-debugging question. I know that for a single bug, "manually" tracking state using a handful of functions and getting outputs about the state will let me track down the problem. But it takes more and more time per bug. Even when individual functions were testing (well enough I think but not anywhere near thoroughly since at least half of it will be scrapped soon after creation). The debugger I have is called with the primitive `brp` (for "breakpoint") that can be called from anywhere and gives me something like a `gdb` console at that point. This double as state inspection (and continue execution as if `brp` wasn't called) and live test of calls in the body (by alternating between "step" and "print stack"). When I added this, I thought surely this is the most general debugger I could want. But as you can see from the ideas section, there are still things I can't do outside the interpreter. For example, I don't have line (and column) numbers at the moment. While I could get the number of any individual line by adding the right things in the source, I don't see how to add this without altering the interpreter (or writing an assembler, or adding an automatic source annotator). Could something like line (and column) numbers be added using only colon definitions to something like Jonesforth for example? &gt; checking the stack output As a side remark, I want some kind of "typed printing of stack values" so that I can check stack outputs more quickly. But this is also something that doesn't seem to be obvious to add with just colon definitions. Because when values were pushed on the stack in the first place, I didn't distinguish between whether it was an integer or a character.
&gt; I currently handle debugging by keeping definitions short and clean, and doing incremental testing. I haven't needed anything more than this. Hm, this is the second account of this phenomenon. I did read that this was the intended usage pattern but don't think it was said explicitly that this removes the need for other debugging tools. (And I didn't get the result even though I though my usage pattern was close enough.) I guess I could try even shorter definitions (and increase the number of functions). I don't know if that's the source of my debugging needs though. As someone else said in a different thread, the hyperstatic environment means I shouldn't worry about defining more functions because of possible name clashes. But it still seems like more names for me to track. &gt; By writing a clone of this in Retro, I can literally run a copy of the current image within a sandbox, adding in support for breakpoints, single stepping, disassembly, logging, etc. This sound very interesting. Is there some description of teh design of the image and sandbox?
&gt; I guess I could try even shorter definitions (and increase the number of functions). I don't know if that's the source of my debugging needs though. As someone else said in a different thread, the hyperstatic environment means I shouldn't worry about defining more functions because of possible name clashes. But it still seems like more names for me to track. I have a construct to hide factors that I don't want exposed in the dictionary. It lets me have as many words as makes sense, without worry about bloating the dictionary with things that aren't intended to be left exposed. &gt; This sound very interesting. Is there some description of teh design of the image and sandbox? The image is literally a raw memory image of the virtual machine memory space. The VM models a MISC processor with 26 instructions. It packs four instructions per memory location. Memory consists of signed 32-bit cells (stored as little endian). So the sandbox is literally just a simulator for the instruction set. I copy the current image (or load a new one) into a buffer, and step through it with the simulator, executing each bundle of instructions as encountered. But it lets me have lots of options as I can have detailed checks for stack over/underflow, divide by zero errors, etc. And then probe around and patch things directly. As mentioned initially, I haven't written this for the current Retro, but it was very useful on the prior one when debugging compiler changes and similar things that affect the internals. It's probably something I'll be tackling in the first quarter of 2018.
Be well and prosper.
I'm used to just write code. It feels more interesting to design solutions after reading papers like this. Thanks. Are FSMs part of a standard CS curriculum? They seem less natural then just spewing ifs until the things bends. Of course what feels natural is completely subject to what one has most experience with.
FSMs are a standard part of the CS curriculum, yes. Regexes are good examples of FSMs.
I almost implemented automatic stack shuffling once, with some notation similar to { a b c -&gt; b c a} and such, but honestly, if you're already accustomed to stack shuffling, it's a pretty trivial task for a human to do.
If the definitions are small enough, there's no need to debug at all; sufficiently small definitions are easily reasoned about and can be debugged by reviewing it the day after writing, simulating by hand, and then finally implementing it.
I don't doubt an FSM can be made very compact, but I don't see how it is more readable. For these cases I would write a BNF, then a recursive descent parser implementing it. I find that much more readable.
Those are my feelings as well.
I can understand the BNF approach being more conventional but the idea of hiding all the details and creating a language to convert it all into a simple 2D matrix is pretty cool. It seems to really demonstrate the Forth idea of making a little language to solve the problem. When I first saw this many years ago I was floored. I would have never thought of this. 4 WIDE FSM: &lt;Fixed.Pt#&gt; \ input: | other? | num? | minus? | dp? | \ state: ------------------------------------------------------ ( 0 ) | DROP | &gt;0 | EMIT | &gt;1 | EMIT | &gt;1 | EMIT | &gt;2 ( 1 ) | DROP | &gt;1 | EMIT | &gt;1 | DROP | &gt;1 | EMIT | &gt;2 ( 2 ) | DROP | &gt;2 | EMIT | &gt;2 | DROP | &gt;2 | DROP | &gt;2 ;FSM 
I have a word for stack restructuring. It lets me do things like: 'abcd 'dcba reorder 'abcdef 'cfcaaacb reorder I've only used it a few times, but it's handy when I don't want to slog through bigger reorderings.
Does it copy the elements onto the top in the order presented, or does it actually replace the elements you specify?
Ah, it just clicked, DFA is an FSM. Thanks
Is it a matrix of states? That's pretty neat, bro.
NP bro. 
It replaces the elements.
Oh that's cool. Does it replace the elements with Pick? Or some other SP-relative operation?
Far simpler. I have an array that stores up to 26 items. I copy the values to this, then copy them back out in the specified order. It's relatively slow, but this is the only way to do it in my Forth. (My virtual cpu doesn't expose a stack pointer, so things like 'pick' may be problematic to try implementing.)
Without an exposed stack pointer, how do you implement Abort or Quit?
Strictly speaking, I don’t have either of these in my Forth. Given that my Forth isn’t ANS compliant at all, an exact implementation isn’t as useful, but this should be very close. The standard interface on BSD/Linux/macOS (and soon Windows) has `listen` as the interpreter loop. So here’s the code: In ANS FORTH, QUIT has to do the following: Empty the return stack, store zero in SOURCE-ID if it is present, make the user input device the input source, and enter interpretation state. Do not display a message. It also handles interpretation. For readability, define a word to get the depth of the address stack. Nga exposes this when reading from address -2. ~~~ :rdepth (-n) #-2 fetch ; ~~~ Then implementing QUIT is basically a loop to drop everything from the address stack and calling `listen` when we reach the end. ~~~ :QUIT (-) repeat rdepth #1 -eq? [ listen #0 ] [ #1 ] choose 0; pop drop-pair again ; ~~~ With this, tackling ABORT is easy. ANS says: Empty the data stack and perform the function of QUIT, which includes emptying the return stack, without displaying a message. So all that’s needed is a `reset` of the data stack and a call to QUIT to do the rest. ~~~ :ABORT (...-) reset QUIT ; ~~~ Tested only briefly, but seems to work as expected. Note that this *won’t* work on the iOS or macOS editor-based environments as they don’t have a traditonal console interpreter and thus lack the `listen` loop, using a special loop hooked heavily into the editor instead. An ABORT could be written to break execution if desired though: This won’t call QUIT since the host doesn’t support it, but will cease execution of the currently running word and its parents. ~~~ :ABORT (...-) reset repeat #-2 fetch #1 -eq? 0; pop drop-pair again ; ~~~ 
Hmm, pretty neat. Listen is a deferred word?
No; I defer very little now. It's defined as: :listen (-) ok repeat gets valid? [ interpret ok ] [ drop ] choose again ; A couple of notes: - `gets` reads a single whitespace delimited token - `valid?` checks to make sure the token isn't empty - `interpret` either hands the token to a prefix handler, or passes the address of a word to its class handler. (or calls `err:notfound`)
Oh! That's pretty slick.
Yes exactly. It was invented by the late Dr. Julian Noble. He was a very big proponent of Forth for his work which was simulating automobile accident dynamics, as I understand it. He came up with this way of making an FSM by making an FSM "compiler" by extending Forth. It's genius IMHO. It documented in the paper linked to in the title of this thread.
Julian Noble? I read his Intro to Forth webpage when I was starting out. I didn't know he was a big contributor, but now that I think about it, I should have suspected. I will definitely follow up on all of his publishings.
His page is here http://galileo.phys.virginia.edu/~jvn/ One his big works was his FORmula TRANslator. He developed it so he could port Fortran code that did some advance mathematics, into Forth with less risk of error. (no RPN translation by a human) B
Thank you very much!
Man I envy you!
Well, I do have fun.
I know what you mean. Unfortunatelly I am still a beginner.
No I don't think so. You might be a beginner in instant programming without being limited by a compiler, but I think you know a lot more than I do. Which is just waiting to be formulated in Forth.
Well thanks, one think I do know is I like Forth.
[the slides [PDF]](http://web.stanford.edu/class/ee380/Abstracts/171115-slides.pdf)
As written, the tag stack is part of a parsing editor and complete independent from the execution environment. If you think about it this makes sense because all state relevant information is already available as side effect of (optimized) immediate-level code generation. I must remark, that these concept is not new. In fact, some Basic systems from the 80's implement similar strategies for ad-hoc code optimization which is in both cases really simple to be implementable.
Yeah, but that doesn't make it fun to implement, whereas objects and automatic memory allocation is. I do have something similar to C's stack frame to load and store variables, but that's as far as I've ever bothered to get with it.
Maybe I'm blind, but anybody know where SwapForth came from?
I love this! In the back of my head I've always thought "It would be great if the language standard implemented something like ` { a b c -&gt; b c a}`....". Your code does exactly that without needing to modify the language or interpreter. It's a great idea! Thanks for sharing. Is this code available online anywhere?
I think Swapforzh was by James Bowan who wrote the J1 Forth CPU.
Factor isn't really Forth, they're just both concatenative languages. Gforth is probably your best bet. ANSI standard. Documented. Cross-platform.
Is Gforth in active development? The latest release seems to be around 3 years old (I could be wrong though).
If a piece of software is feature complete, why do you need to have continual releases? I've used gforth for many years and have never found issues with it. The last commit to the repository was 4 days ago. 
Yes it looks very active http://www.complang.tuwien.ac.at/forth/gforth/Snapshots/
Thanks
What do you want to use the Forth for? 
Initial goal is just to learn how to program using this interesting language. Long term goal is to interactively explore the features of certain controllers (on the 64 bit intel platform) using forth as the command line interface (in which case the forth language is expected to run as a standalone software on raw intel hardware without any other OS... not sure if this is possible or not).
Forth is known for being a stand-alone system all its own. Many Forths created by people just starting out run on bare metal.
Yeah that's what I have read. Hence I think this language would be a good choice to explore a controller's features interactively. 
I didn't know that Bitcoin Script uses a stack-based scheme, with some Forth-like operators: https://en.bitcoin.it/wiki/Script 
To follow-up: one of our diligent users has been reporting problems going back a few releases. We've fixed all of them now, and the new 17.11 release is rock-solid: https://8th-dev.com/forum/index.php/topic,1536.0.html
Certainly that's true. It's quite easy to interact with PCI devices, or using the serial ports, for that matter. Poking at those devices using Forth is especially fulfilling.
Is there any specific Forth that I could be using for that purpose (I am assuming gforth won't allow me to do it since it does not run on bare metal intel platform)?
I'm not sure. I think that topic is worth making another text post about, since it would be more visible that way.
which forth do you guys recommend if i want to make a high performance socket server?
You're right. I'll ask in a separate thread once I learn enough of forth first. 
I won't mince words, so I'll just say go with Gforth.
Yup. Gforth it is. Already installed and following the Starting Forth online book/tutorial :)
EDIT: Just noticed you were only interested in Intel 64-bit... It really comes down to which microcontroller you want to poke at. Arduino UNO R3 with FlashForth is an easy start. I can even deliver you an Atmega 328 chip with FlashForth already on it. Also the PIC18/24/30/33 series chips are supported. Most microcontroller forths require reflashing after abuse. But FlashForth has a protected kernel so it won't allow you to write into vital memory that could break the kernel. Check the tutorials at: http://flashforth.com/tutorials.html Download: http://sourceforge.net/projects/flashforth/files/ff5.0.zip/download 
Poor multi-user-forth, gets no recognition or respect. 
I've gravitated to the *deferred word* strategy because I hate the idea that I'll have a conditional that follows one path only once, and then forever wastes time falling through to the other case. 
I realize this makes me somewhat of an heretic, but I like to add named variables and arguments to my Forths (https://github.com/basic-gongfu/cixl), which allows both models to coexist peacefully. 
No defining own macros yet?
I have a pretty long list of more pressing issues, they're not that critical for a language that delegates almost all heavy lifting to the surrounding C program. Host language reader macros are available though, that's how let: and func: are implemented.
Yes, exactly right. The added indirection of a 'defer' word is much less problematic than added conditional code every time through.
In what circumstance would a branch only be executed once during a program's runtime?
Thanks for the info. I am interested in intel x86-64 for a specific reason. But Arduino Forth seems interesting for other purposes as well. I have a couple of Arduino UNO lying around (did some home automation work a short while ago) so I might try FlashForth and have some fun with it after I have a good grasp of Forth programming. I am currently learning Forth (GForth on Linux) and my head is already reeling in a good way... needs a different way of thinking than normal C/Python types of programming.
Nice to see people working on new concatenative languages. &gt; 1 + 2 This only works if the stack is otherwise, so can't be safely used inside of a function, right?
Functions open implicit scopes so that wouldn't be an issue. It's also possible to open a clean scope wherever by surrounding code in parens, kind of like {} in C-languages; or clear the stack before. You still need be aware of your stacks, just like in regular Forth; but you get more options for expression in return.
It works well for cases where the first iteration is different from all others. E.g., when you need to initialize something, but don't want to do the "test if it's the first time through" check all the time. Both cases occur at runtime, but one case occurs only once. 
Okay, that's interesting, thanks for the clarification.
You're welcome.
Oh, that makes a lot of sense.
To make it even easier to get your fix, I just added comma as a cut-operator. I knew there was a reason I spent all that time learning Prolog :) 1 + 2, 3 + 4 https://github.com/basic-gongfu/cixl#expressions
Very interesting stuff! Nice to see a stack-oriented language with optional infix notation.
And optional prefix :) That's one of the things that bugged me from the start with Forth; sure I can see how some problems decompose better using postfix stack semantics, but being forced to use it for everything feels stupid. As does using prefix for math in Lisp, etc. It can't do much yet, but the few lines of actual code I've written have been a liberating experience.
I implemented this as a way to learn forth, so the forth code could be of dubious quality. I tried to keep it compliant to forth-94, but my interpretation of the standard might be wrong given how little experience I have with forth systems. 
Wow, that's really neat! I didn't notice that at first, but I am a Lisphead myself so I find that very nice.
&gt;Symmetry beats consistency What does that even mean?
thanks for this, i always want a forth for wasm 
It means what it says, that micro consistency is more important than macro. That it's more important for complimentary operations or concepts to mirror each other than it is for all of them to follow the same scheme etc. Consistency for it's own sake is like purity for it's own sake, or object orientedness for it's own sake; destructive. It's also boring, there's nothing wrong with being pleasantly surprised now and then.
uhhh.... okay.
&gt; Zen &gt; &gt; Orthogonal is better &gt; Terseness counts &gt; There is no right way to do it &gt; Obvious is overrated &gt; Symmetry beats consistency &gt; Rules are for machines &gt; Only fools predict "the future" &gt; Intuition goes with the flow &gt; Duality of syntax is one honking great idea This is a nice list. Quoting here so I can find and refer to it later. I agree with all of it. Although mainstream programmers would say they explicitly disagree with a number of these and give lip service to the rest.
You're probably right, but then most of them haven't been banging their heads against the programming wall on a daily basis for over 30 years. I tend to disagree with most public opinions; it's always the lowest common denominators, ideas that are catchy enough to convince the majority. It started out as a play on Pythons classic, and could be read as a critique; but took on a life of it's own.
&gt; I've been playing with a file based psuedo stack Goddddddamnit, I'm working on a bash fourth too. Oh well, guess it's not original
I would encourage you not to view yourself as a plagiarist for that. My education is teaching me that to a large extent, (although maybe not all the time) stack-based is simply the right way to do things. I will go further; I think UNIX was originally intended to provide a convergence point between very FORTH-like scripting and physical hardware. The original intent was to have pointers to hardware in /dev which behaved pretty much exactly like raw machine registers, so that you could control hardware straight from a shell script, and have the wonderfully simple control structures that go with that. Shell makes many things a lot easier than FORTH does. Charles Moore would probably object to that statement on the grounds that that user friendliness has a complexity cost, and he's correct; but truthfully I feel that UNIX has some features that are nice enough to trade ***some*** complexity for. I would encourage you to study the head, tail, and [ed](http://wiki.bash-hackers.org/howto/edit-ed) utilities. The ed program is not usually installed by default on Linux, although it generally is on the BSDs. Ed allows me to very easily pop off and push to the first line of a file; the only disadvantage is that I pay (in speed terms) for a file read and write with every iteration, and sometimes that can make things very slow, cumulatively. Get [GNU FORTH](http://ftp.gnu.org/gnu/gforth/gforth-0.7.3.tar.gz) if you haven't already, either. The system word is very useful.
Can you give an example of where these are at odds? I can't think of any obvious examples of where consistency doesn't cause symmetry. I'd expect asymmetry to always be rooted in inconsistency.
One example would be to name complimentary methods/concepts the same way, rather than enforcing a global naming scheme. In cixl, I try to ride on existing idioms and names as far as possible; let, lambda etc.; but I still chose the name 'upcall' instead of 'super', since it balances 'recall' which didn't have much prior art to lean on. I find that kind of local consistency helpful; as opposed to naming every single freaking class that creates something xFactory in Java; or forcing math to be performed in prefix like Lisp; or forcing purity everywhere like Haskell; or pretending that everything is an object like Ruby/Smalltalk. Programming is pattern matching; as soon as you stop doing that start applying principles, you loose something. It's subtle, and looses much of it's essence once downsized to literal descriptions.
Real mode is exclusively an x86 thing, right? If your question is broader than that, I'm running hosted on x86, RISC-V, ARM, 68000, and PDP-11. Standalone on Cortex-M, AVR, MSP430, 8051, PIC, STM8, 6502, and PDP-8.
Broad is good, yes. How did you get your hands on a PDP-8??
Any plans to take it lower?
Where do you find simulators for all those? I'm sticking to x86 because it's available, but I'd love to branch out.
If you're using a shell, maybe the filesystem itself would be a suitable dictionary?
We're working on embedded versions, but it's hard to say just what that will look like at this point.
True. I can load different dictionaries into and out of memory relatively quickly, as well; by sourcing them in, and then killing the bash process which had said dictionary sourced.
I think different dictionaries could be sub-directories off the main dictionary? Keep working on it, I'm very curious about how it turns out!
I'm using these: Simulator | Processor(s) --- | --- Qemu | ARM (classic), RISC-V, 68000 tosemu | 68000 apout | PDP-11 SIMH | PDP-8 simulavr | AVR naken_asm | MSP430, 6502 uCsim | 8051, STM8 PIC | gpsim thumbulator | Cortex-M 
&gt; but I'd love to branch out. You know what they say about x86 and speculative branching...
Yeah I read the official papers on Spectre and Meltdown recently. Theoretically both of these affect every high-performance processor designed/manufactured since the mid-1990s. Scary stuff.
If nobody minds a slight unrelated question on the paper. I noticed the one citation: J.V. Noble, "Avoid Decisions", Computers in Physics 5, 4 (1991) p 386. and the title has me very curious but I cannot find it /anywhere/. Has anybody here read it and can give an idea of what it's about, or perhaps know where one could find it?
I [found it on libgen.io](http://libgen.io/scimag/index.php?s=10.1063%2F1.4823001&amp;journalid=&amp;v=&amp;i=&amp;p=&amp;redirect=1)
That is epic thank you! :) Thanks also for showing me one can use DOIs to search libgen, I didn't know that. ^_^
Most of the electronics on my desk right now is clocked and synchronous. We are comfortable with measuring frequency and time in Hz and seconds. Those come from the most popular calendar system. These calendars are based on the period and movement (frequencies) of celestial bodies that astrologers of a particular ancient culture thought were the center of everything. The ancient astrologers tried to predict the weather and other events by keeping detailed records of the stars with respect to those events on Earth. Modern astrology has almost no connection to ancient astrology. Modern astrology tries to *explain* pop-psychology bullshit with recently invented "astrological signs" and other New-Age nonsense. As far as I can tell Modern astrology does not really do this record keeping. (actually I should say Post-Modern era because I'm living in the post-WWII Occident. Modernism is derived from Romanticism. The Modernism and the Modern period started in a region of France sometime around 1870AC. Modernism was a reaction against the cold and calculating certainty of the Enlightenment period. What really kicked off the Modernist revolt was the rough and dehumanizing transition of skilled craftsmen and women from the countryside to the city for work as unskilled factory labor. Notice that Modernist art rejects Realism and tries to find things humans can still do better than camera or other mechanical device. At least we can come up with new ideas that break with the traditional crafts of the countryside and direct the rote manufacturing at the city assembly lines. Modernists continued down the absolutist (supposedly) one-way march of progress. "Let's find the *best* way to implement this device" ... "Let's find the *best* way to structure this" ... the error of Modernists is the arrogance that breaking with their traditions always leads to something better and that the words "progress" or "progressive" are just synonyms for improvement and those who make it happen. They didn't iterate much at all. They didn't loop back and check whether they were wrong about everything. Yes they rejected things that came before but it was always with the expectation guaranteed improvement. Tech built in the Modern era reflects more of an absolutist intention of finality. The influence of the Modernist school of thought reached a rude awakening when people tried applying it to actual human populations with cold efficiency. Eugenics. Disabled people were tipped off their wheelchairs over balconys. Sex offenders and European ethnic groups perceived to be inferior were gassed with agricultural pestisides. After that experience people started to notice that they felt different. Something felt very different. What they were feeling is another big cultural correction -- the switch from Modernism to Post-Modernism.) So to cut a long story short and back to asynchronous, clockless electronics and Forth [here's a link to a relevant section from an article on Post-Modernism](https://en.wikipedia.org/wiki/Deconstruction#Derrida's_"negative"_descriptions). If I say I'm writing a program you won't have much idea what I'm talking about, so I elaborate with more description. That extra description forbids other possibilities -- it can't include new ones. At no point in time was there such a thing as positive information yet our vocabularies still try to do universal quantification. That's when I started reading about antidictionary compression and working with Forth antidictionaries. So, anyway, maybe there is an indirect causal connection between the SPI clock freq on my PSoC5LP board and the Earth moving around the Sun but I don't think I'll ever find it. While I wait, I think I'll look into the asynchronous nodes of the GA144 or the asynchronous configurable SmartIO logic on the PSoC6 for a more local time reference.
And here we are watching in horror as liberals, technologists and trans-humanists are busy inventing new ways to cause more suffering in the name of progress. Evolution is a spiral, we keep coming back to the same point on ever higher levels; personally, I look forward to the post-technologistic era.
&gt; And here we are; watching in horror as liberals For what it's worth the Marxist "left", the fascist and nazi "right" are all considered Modernist. Socialism and fascism even went through fairly a similar process of centralization from their early period of self-organization (on the "left": anarchism/Spanish Revolution/Makhnovism/Soviet worker cooperatives --&gt; Soviet Union and on the "right": distributism/Catholic Syndicalism, e.g Mondragón cooperative --&gt; fascist social corporatism). If you want to know what caused some Marxists to diverge from mainstream Marxism and eventually split of into a new political system known as fascism read some [Georges Sorel](https://en.wikipedia.org/wiki/Georges_Sorel#Relation_to_Marxism) he was the pivot from Marxism into fascism. Spoiler: the split was mostly over religion *not* over antisemitism, racism, and nationalism-vs-imperialism. Fascists and Marxists of the day were largely in agreement on the latter. Pretty much the only enduring feature of fascism that sticks out at me worldwide and where I live in California, USA is Mussolini's economic system of social corporatism. It was the inspiration for the New Deal here in America. &gt; I look forward to the post-technologistic era. When I drive car I feel as though the steering wheel has become another limb and the sheet metal outside is my skin that I want to protect. That's what a tool should feel like. Tech minimalism counter-cultures such as plan9/suckless/Forthers are engaged in a power struggle to keep our tech as tools so we don't have to recalculate our worth after each innovation.
The Let over Lambda book has a few chapters about Common Lisp and Forth, including an implementation with Lisp interop.
Oooh, I'll have to take a look at that. I'm not very clued up about CL but I figure with the bit of Scheme I've done I should be able to get the general idea. :) Thank you!
Thanks - I remember well the architecture of a traditional threaded forth - I got my first Forth up by copy-typing the entire assembler listing of the Fig-forth for 8080. I have a co-y of Loeliger somewhere. I was curious about your “hand coded” stacks when the Intel (surely?) has push and pop ?
This was submitted here before. What changed when they revised it in July?
Hmm I forgot to check, probably nothing's changed.
My compiler runs under 32bit x86 mode in Linux. I can't even remember what real mode is anymore. I'm working up to redoing the compiler in 64bit as the 4GB address space is starting to hit me.
Woah! What are you doing that makes 4GB feel small?
It got [posted to HN](https://news.ycombinator.com/item?id=16090364).
A common solution to that problem on Intel Forths is to use SP for parameter stack, BP for Return stack and when pushing and popping on the return stack you execute XCHG SP, BP Then you and push and pop to your hearts content and when you are finished, run XCHG BP,SP Apparently the optimizations in the CPU for these instructions make it worthwhile. (I have not confirmed that experimentally) 
I think the programmer of Freeforth benchmarked these pattern against indirect memory accesses (base-pointer based) with the conclusion that both variants show equal performance on recent, out-of-order x86 processors. The class of XCHG instructions are more compact of course which may be of advantage because the issue width is still a limiting factor (Intel cpu's)
I'm analysing maps to estimate trade volumes and the way Dijkstra's algorithm works means I want to cache the results for each settlement. As the map grows and the number of settlements increases, the caches have outstripped the address space, so even mapping the files in from disc doesn't help. I'm using 3½GB but I have 8GB in the machine. Obviously there are alternative approaches but it's spurring me on to finally update a compiler I originally wrote in 2009.
Additionally, I keep top of return in a register to speed up r@ (and r=, which I added to the normal words, equivalent to : r= r@ = ; ). So XCHG on its own isn't sufficient.
Intel CPUs keep a cache of return addresses which is only abandoned if, when executing RET, the return address on the stack doesn't match the return address in the cache. At least for Forth's stack operations, entering an assembly subroutine whereupon you exchange EBP and ESP and then exchange them back when you're done, ensures the integrity of the call/ret cache. So you either XCHG ESP, EBP in your primitives (totals to 4 bytes per primitive), or you force the CPU to waste at least dozens of cycles refreshing its call cache. Save bytes VS save cycles.
That makes a lot of sense. I feel like I have the same problem you do, which is my hesitance to try integrating Forth itself with anything else (like the C standard library for any OS). It doesn't help that Visual Studio is so monolithic, and GCC equally puzzling.
I have no plans to revisit bare metal x86 if I can avoid it. Nowadays my interest in x86 is only as a host platform for umbilical cross-assembly/cross-compilation rather than as a target. Although that may eventually change with Intel starting to integrate x86s with FPGAs.
Threaded Forths don't use CALL/RET so that messes things up I guess.
It is more common to keep the Top of the parameter stack in a register which gives about a 10% overall performance increase in threaded code. Do you do that as well?
Ah. Well then I dunno why anyone would bother exchanging those registers, since "xchg sp, bp push si xchg sp, bp" are the same amount of bytes as "mov [bp-2], si dec bp dec bp".
I'm not user of gforth but you code is like a direct traslation from other lang, forth have other path, the key is reemplace vars with values in stack. 
At the moment, acceleration for some products I'm still tangentially involved with. More generally, as the integration of commodity CPUs and FPGAs gets tighter (lower latency/higher bandwidth) it will open up more and more interesting opportunities for application-specific co-processing. Makes me wish I were 20 again.
You could probably speed things somewhat up by making the program more forthlike but it probably wouldn't be a huge difference. AFAIK, Gforth doesn't do any particular optimizations and is written in C. The site talks of a gforth-fast... 
Thanks, I'll give this a shot.
This is totally a shot in the dark, but Gforth *might* be faster [on the master branch.](https://github.com/forthy42/gforth/tree/master) 0.7.3, the most recent release, is behind the trunk by several thousand commits. So it's quite possible that there are some performance improvements to be had by using a master build.
JIT native code beating threaded code in a problem as brute-force as this, where one or two small loops is the entire program's execution time, is not surprising. But stuff seems to be a bit different on my system. When I do "javac outcomes.java" followed by "time java outcomes" I get 0.706 seconds (using openjdk 1.8.0_151). gcc 7.2.0 with -O3 gets 0.111 seconds. gforth-fast (after making some changes to the forth version that both improve speed and, in my opinion, style, which can be seen at https://pastebin.com/CxZT8gnv) gets 1.137 seconds. Note that I am using a development version of gforth (specifically 0.7.9_20171026). My CPU is an AMD a10-7850k. Once I get julia and go installed and figure out how to run those fancy new whizbangs I'll see how those perform on my system, but if the java time is indicative, gforth-fast will probably beat julia and take about twice as long as go. Are there any special flags you're using with java?
I share your excitement. I wish I had any experience with FPGA programming. One more thing to learn, I guess?
Great suggestion, this brought the timing down to 0.7 seconds, much more in line with my original expectations. real 0m0.713s user 0m0.700s sys 0m0.000s
This makes Forth about 1/2 the speed of Kotlin &amp; Scala and about 16x faster than Python 3 on my workstation. Quite remarkable for an interpreted programming language, actually.
Richard Haskell, a long time Forth enthusiast, wrote *Learning by Example Using Verilog - Advanced Digital Design with a Nexys 2 FPGA Board (2009)*. Although outdated (old version of Verilog, old demo board), it's probably the only primer whose main design example is a Forth stack machine core. Some worthwhile follow-ups are: * Advanced FPGA Design - Steve Kilts * Advanced Chip Design, Practical Examples in Verilog - Kishore Mishra * RTL Modeling with SystemVerilog, using SV for ASIC and FPGA design - Stuart Sutherland 
Keep in mind, Forth doesn't dictate implementation techniques. It can be interepreted, or compiled, or byte compiled, or threaded, or anything.
Wow, that is quite remarkable. I'm surprised the Gforth developer hasn't released such changes yet...
How do you measure the speed? If in the command line then you're also measuring the compilation speed of gforth. Which seems to be unfair if you compare it to an already compiled code. Many Forths perform a linear search on the dictionary during compilation.
I am not up to speed on modern Intel designs. Would you say then that execution speed would be the same for both pieces of code as well? (not sure that matters so much these days with the complexity of cache memory management and such affecting actual speed as much as instructions)
Not only that you are measuring the start up time of the entire compiler which has nothing to do with the program's execution speed.
A minor point of factoring style in Forth. Factoring is typically more atomic in Forth than most other languages. The goal is not only to create a working program but to create a tiny set of words that are useful to write the program. A small custom language if you will. It's mindset shift. The words DEC and INC would typically be factored simply as: : inc ( addr -- ) 1 swap +! ; : dec ( addr -- ) -1 swap +! ; Then they are useful for any variable, memory location or your CARDS array. Example: variable x x inc variable y y inc Then the phrases in the program like i dec Would read as: I cards dec Which is much clearer. And while I am at it some folks use a piece of punctuation in their naming convention to gives more clarity to word functions. If you renamed cards to ]cards for example to indicate cards is an array it would read: I ]cards dec Which helps me remember that cards is an array. Keeping with naming conventions 'dec' and 'inc' would often be called '1+!' and '1-!' to keep with the '+!' operator naming. But you are free to create the language that suits your needs. It's Forth. 
It wasn't meant to be a formal benchmark.
Thanks, this is quite helpful.
Understood. For your future reference the people at Microprocessor engineering in UK have created a true optimizing compiler that compiles Forth source to native code. Their founder indicated that they used to compare their older indirected threaded systems to the new VFX system. Eventually the benchmarks on the native code exceeded 12X the threaded code and they stopped comparing. Typically threaded Forth will run loops 10X slower than C or assembler because each forth "instruction" (word) is still running through the address interpreter. However real world algorithms tend to be 3..5X slower than native code. So that gives you a sense how old Forth and new Forth implementations will perform in general. And of course better algorithms beat language almost every time. 
I prefer to keep brackets and their ilk balanced on the screen as much as possible so I'd go with cards{ i }dec where cards is created by (minimal, no checking) : array ( size &lt;name&gt; -- ) create cells allot ; and }dec would be (assuming you have cells+ as a primitive) : }dec ( array^ index -- ) cells+ -1 swap +! ; leading to 52 array cards{ cards{ I }dec and so on, so that the only unbalanced brace is at declaration. You could also define words like }inc, }@, }! }sum, }max, }min and so on. Those last three would need you to store array size, of course. This illustrates one danger of Forth: it's so much fun that you often find yourself simply adding code because the ideas flow out of you and you can waste a lot of time on things that are nice but not *actually* needed for *this application*. So you have to focus on "You're Not Going To Need It".
LOL. You are so right. There are legions of people who never write an app in Forth but endlessly play with their implementation... because it's fun. 
I happened to have this page bookmarked for referencing just in situations like this: http://www.c-jump.com/CIS77/reference/Instructions_by_Opcode.html This page says that xchg is quite a heavy-weight operation, taking 3 cycles each even on Pentium processors, whereas the memory operations (for xchg, push; for dec x2, mov) take 2 cycles, and DEC takes 1 cycle each. Simple math dictates exchanging is slower. But I'll also add that the dec x2 operation would also be faster simply because it is better for out-of-order execution, because it only ties up 2 registers instead of 3.
Perfect! Thank you!
&gt; This makes Forth about 1/2 the speed of Kotlin &amp; Scala and about 16x faster than Python 3 on my workstation. Minor nit -- it makes *gforth* half the speed of Kotlin and Scala. An optimizing native code compiling Forth will likely be faster.
Instead of static top-of-stack caching your compiler can benefit from dynamic stack elemination (dynamic mapping of the stack to temporary registers). That wouldn't be much effort from the current state of your implementation resulting in a larger performance boost.
That would require significant complexity in compiler particularly managing the transitions from word to word, but in a register rich machine there would be a pretty big reward. Memory tells me that experiments by Forthers in past with keeping top-of-stack AND next-on-stack in registers did show a performance increase over just top of stack in a register so the register allocation scheme would need to be much more sophisticated.
I honestly can't imagine how that would improve speed unless you knew your whole stack was always going to fit in the registers. I found working with the FP stack in Intel a complete pain because of the interface between the "fake" stack and the larger in-memory stack. In the end I just stopped using FP. Is there an implementation of this somewhere I could look at?
variable allots one cell already. variable deck 9 cells allot can be replaced with create deck 10 cells allot and be equivalent, at least in most implementations. I have no idea why create wasn't used, though.
It basically walks you through one man's very idiosyncratic implementation of an indirect threaded Forth for the Z80. It's worth reading, but I wouldn't use it as recipe book. 
I am only familiar with an old commercial system that has intel Floating point. The new standard assumes a separate floating point stack and the operations all take place on that FP stack. It's not ideal IMHO. https://forth-standard.org/standard/float One Issue is how does the REPL handle floats. The system I am familiar with took a non-standard approach and used vectored words (DEFER et al) in the interpreter. The word "integers" set the REPL to work like normal Forth. The word "FLOATS" changed the REPL to read anything with a '.' in it as a FLOAT and put it on the FP stack. This worked pretty well. But to be sure the Forth "simplicity" model means you do more work in other places. (Some say you can't hide complexity, you can just move it around) If you want to see the state of the art I would recommend the free downloads of MPE VFX Forth and SwiftForth from Forth Inc. Perhaps you can glean some insight from their methods.
Doh!
This series of articles is a good start: http://www.bradrodriguez.com/papers/moving1.htm
Thanks
There is also Camel Forth , by Brad Rodriguez, a minimal Forth kernel, that is available for MSP430 and other micros, as complete assembler code and the comments show the Forth equivalent of the assembler where appropriate. I wrote a hobby cross-compiler that generates a Camel Forth BIN file for the TI-99. It is heavily commented and has some extension files that show how to implement some other words and tools in a more or less ANS standard way. It might be of use to show how Forth goes together. https://github.com/bfox9900/CAMEL99 
How is that an if? How would you write `: mod5 5 mod 0= IF ."yes" ELSE ."no" THEN ;`. You would need to make TOS the decider, e.g. `: ? IF swap THEN drop execute ;`. I used the builtin IFfor that but you could define it without it too. For the reason: I *think* it's because it maps to a jump instruction, which is as cheap as it can get. Forth was originally speedy.
* Your `my-if` does not allow for *conditional execution*, so it is not a replacement for `if` and certainly isn't as useful. * I have never heard of "nothing should really be of variable length" being a Forth principle. Loop bodies are not fixed length either. * I don't think Forth is about following principles, but about finding a solution tailored to the given problem. 
Yeah, I agree... not only are IFs not postfix but definitions themselves (as in your code clip) are not postfix. In my toy language (f-flat) I "fix" both of these (fix is subjective). For IF the primitive (built-in) word is the tertiary conditional `cond`. For example `true 1 2 choose` yields `1`. "if" (called `branch`) is defined as `cond eval`. For definitions, I use primitives `sto` (to store a value) and `:` (converts an array to an action/sentence). Then I can define `;` in a way that the definition of `branch` is: ``` branch: [ choose eval ] ; ``` Which is all postfix and, to me, more readable IMO. 
Wow, I really like your toy language. For one it's really pretty, and when I looked it up, it had a lot of the ideas that I wanted to put into a toy concatenative language &gt;:( lol. One thing I want to try out though, is to make `'` be polymorphic and turn numbers into numbers. Actually, the point of the toy language I want to make is a heavily polymorphic forth. I just dont know where to get started :S
Thanks for the kind comment... I've been working to ensure the module loading is postfix: `core: 'core.ff' import ;` (notice the use of the `;` word). Lately working on pattern matching and lambdas (https://runkit.com/hypercubed/f-flat-quadratic-equation).
If is not in any way "special syntax". Thinking that may indicate that you've misunderstood something fundamental, and rather important/useful, about Forth.
I think it's a common misconception, people think Forth is about postfix notation. That's why things like Factor claim to be Forth-inspired when all they have in common is the data stack. To play more with the example, u/likes-beans, the if you suggest becomes impractical, because now you have to define a word for each branch, like you did with `yesbranch` and `nobranch`. That's where people pull out quotations, which are like anonymous inline definitions that you can find in e.g. Retro Rorth, 8th or Factor. Then you have `[ ."yes" ] [ ."no" ] if`. It would be trivial to add similar facilities to "standard" Forth by defining 2 words that would start an anonymous definition and end it pushing the xt on the datastack. Stealing the comment braces you could write `( 2 + )` and it would be roughly equivalent to `: _some_gensym_ 2 + ; ' _some_gensym_`. You could make word definition completely postfix as well this way, doing `( 2 + ) " 2+" bind`. Using parsing words you could make `bind` infix getting to `( 2 + ) bind 2+`. Changing the parser from "word-number-error" to "word-number-string" or "word-number-symbol" you could stay postfix and have `( 2 + ) 2+ bind`. The point of this exercise is *not* to find a better syntax. Fiddling with syntax all the time is just a game many of us like to play. The point is to remember we're here to code and produce something useful. Yes, we need a language with good syntax for that (where good = simple and composable for me). But I think Chuck was happy with the simplicity of `IF` and how it mapped to assembly nicely. (of course his latest if is different :)
Where is this `galope` library?
You would do well to read all Forth code by Marcos Cruz.
wow, still in developing? 
It is not required to hold the entire stack in registers because recent Intel comp. processors map the accessible register file to a much larger physical one. This includes common access patterns to internal cache memory. As stack references follow a static access pattern changes are high that the entire stack can be hold in this physical register file which of course require some code specific arrangements. A simple way ensuring this kind of caching is mapping the topmost stack elements to some registers at basic block level.
&gt; That's why I decided that I'm going to learn Forth in 2018 well enough so that I can actually do something useful with it. I view FORTH as a control language, which means you need something else to control. GNU FORTH is good for this, because it has the system word, which allows interaction with the rest of a UNIX (Linux or BSD) system. One program I wrote a bit back was very simple; it had a group of FORTH IF branches which, depending on which number I added to the stack first, used the system word to call mplayer and play a certain mp3 file. Nothing fancy, but it was still more consequential than most FORTH programs I've seen. I feel that one of FORTH's main advantages is how close it is to assembly, which means that its' loops and control structures are faster than most other languages I've seen, and also means that it is simpler to write. I like the idea of having single use binaries which are typically written in a higher level language, and then using FORTH for iteration control. FreeBSD also uses FORTH for its' initial bootloader, which is a good use for it as well, I think.
It makes sense as JIT specific optimization
The late Neil Bawd, who sometimes went by the pen-name Will Baden wrote a lot of Forth utilities and language extensions. I believe his family has kept them public for us all to review and use as we see fit. You will learn some nice Forth style from these examples and have some handy code for your projects. http://www.wilbaden.com/neil_bawd/tool2002.txt 
Note: Neil makes heavy use of TEXT macros using S" &lt;text&gt; EVALUATE and making them IMMEDIATE words. It took me a while to get why. This causes the lower level code to be compiled into a definition rather than compiling a call to the lower level words. It makes for faster code but with slight sacrifice of size (it's bigger).
i just tried the new r4a-0.8 , which could run on android again :D thanks for you work, and is it possible to use it generate android apk on android itself? 
yes and not, is possible but not have time to develop, the version you download is very old, I continue developing in wine plataform only. my recent plan is finish new x86 compiler and the make a webassembly compiler.
ok, i am also interesting of wasm implementation
Mind expanding on why you consider typed stacks a problem? Maybe I'm missing some fundamental piece of context here, but [Cixls](https://github.com/basic-gongfu/cixl) stacks are typed to enable generic words among other things; and I can't really see any problems with the approach so far. It's also worth mentioning that I took the liberty of renaming basic stack ops to single char operators which reduces the noise level once your eyes get used to it.
I think you'll run into trouble when you try nesting whiles. Also, there are several good uses for having the extra `then`s, as it allows for different behavior depending on how the loop exited. I don't know if that counts as structured, but it sure is useful.
I should expand a bit, yes. Often in Forth, when one wanted to manipulate a certain datatype exclusively for certain operations, the solution was to have a software stack dedicated exclusively for that specific datatype, as an example a stack of strings. Memory management factors in at that point, and it quickly can become burdensome. The other solution is more abstract, and that is to have everything on the data stack to carry with it a datatype indicator, or be a pointer to a structure which has that information. Once again, memory management possibly comes into play depending on the datatypes. But at the end of the day, either solution is immensely complex. The simpler solution is to manage your stack data exactly as it was managed before; everything that is needed appears exactly in a suitable context, and nowhere else. Generic words can encourage bad habits, like getting lazy with type conversions or letting data stay on the stack for too long.
If you really need to go that far, factor the loop into a separate definition, and just call EXIT when you're done with the loop.
I never thought of the possibility of using the THEN's for correcting the stack and that would be too unreadable. It wouldn't be clear enough what corrections that would belongs to what exit. This nesting of WHILE should only be used when the status of the stack is comparable at each WHILE. Which could be managed by using local variables. My purpose was to give an example of "extending the compiler".
This is almost always the most easy method, but if you want to avoid multiple exits this method could be an alternative (in GForth).
Good point. (Note to self: Don't reuse word names)
I do something similar, but non-standard: The loop can start with either `begin` or `?begin`. You can loop back to the beginning with either `repeat`, `?repeat`, or `0repeat`. You can break out of the loop with either `break`, `?break`, or `0break`. You can break out of the loop and return to the caller with either `bail`, `?bail`, or `0bail`. The loop ends with either `again`, `while`, or `until`. 
There is an F# implementation of colorforth (and GA144 simulator if memory serves well): [color](https://github.com/AshleyF/Color) You can run it everywhere there is a .net runtime (Linux/Windows/Mac)
https://groups.google.com/forum/m/#!msg/comp.lang.forth/MoOlO-g7SG4/e8Twn4JJHHwJ
I'm not aware of any. Maybe you are thinking of VentureForth which was development environment for the seaforth processors and was supported on Linux (it was hosted in gforth and swiftforth). ColorForth is kind of like the next gen IDE for the next gen chip but is a totally different software approach. I'm pretty sure I'm remembering this all correctly... I've personally never had a problem with getting ColorForth working on Linux/Wine (I'm using Ubuntu). If you are just looking for ColorForth links to the old site, you can find it mirrored in a few places such as https://github.com/mschuldt/www.colorforth.com 
I am not a professional Forth developer, and I do not know the Mecrisp-Stellaris system, so everything I will say here is a guess from what I quickly gathered reading only part of the source code and documentation. It may help you gain an understanding by yourself, however, or even, by sheer luck, be correct. Otherwise, the people more competent than me (almost everybody here…?) are welcome to correct any mistake I make. If I am not mistaken, words that read the input buffer, like “cornerstone” are called “parsing words”. “n-foldable” is not a parsing word but an immediate word. That is, it is called immediately, even when in compiling mode (like “if”). A brief look at the source code does not help a lot, because these words only set a flag “Flag_foldable_n” on the word currently being defined (the effect of this flag is applied elsewhere, most likely in the assembly source, so a “brief look” would not be much help, one would have to really read the code and understand it). However, this line in the README of Mecrisp-Stellaris give us a clue: &gt; Mecrisp-Stellaris can compile directly into Flash, generates native code with constant folding and inlining of short words. This line introduce the concept of folding right next to inlining. Inlining means taking the code of a word, and putting it in place of a call to this word (saving the cost of a call, usually at the cost of memory). My guess is that folding is getting the value of a constant and putting it as a literal during compilation, instead of calling this constant at run time (again, saving a call, and maybe memory accesses). I see that “n-foldable” is mostly used on words that take n arguments as input. So, I suppose that when the compiler encounters a n-foldable word, it looks back to see if the word is preceded by n literals (or constants), put them on the stack, call the word, and compile the result as a literal. So, `: foo hex 1B 5 io ;` will compile the same way as `: foo hex 1B05 ;`. As constants are also folded, if we have a `bar` constant set to `1B`, `: foo bar 5 io ;` will also work: the compiler see `bar`, fold it and compile a literal (lets say, as `push 1B`), then compile `5` as a literal too. Coming to `io`, it sees that it is a 2-foldable word. So, it looks back at the *compiled* word, see two pushes, guess it is OK to fold the word, and proceed to fetch these two literals, compute the result, go back to the first push, and compile the `push 1B05` in its place (leaving the compiler in a state where further compilation will overwrite the second push). This is of course pure speculation. It is how I would do it if I had to naively implement “folding”. Note that this process works for any code that compile literals: a literal followed by a “foldable” work (`5 io#`), a constant followed by a foldable too (`bar 5 io`), but even a foldable followed by a foldable (`bar 5 io io-base`): as long as a word end up compiled as a literal, it should work. Again, I remind you that I am not a professional, and not that knowledgeable, and that it is all speculation, only here to help you find what really happens. Hoping it’s not too far from the truth, good luck!
&gt; So, I suppose that when the compiler encounters a n-foldable word, it looks back to see if the word is preceded by n literals (or constants), put them on the stack, call the word, and compile the result as a literal. That's exactly it.
[Rainbow Forth](http://rainbowforth.sourceforge.net/)
At a glance, this doesn't look Forth related.
What does have in common with Forth other than having a data stack? It seems more like a modern concatenative language derived from Joy/Factor.
If you say so, I only have experience from Forth; which is what the language is based on. I would say postfix semantics with a twist and an exposed data stack. But I'll happily refrain from posting here any more if that's not dogmatic enough.
Sorry, I've posted several times about it here which is why I skipped the introduction this time: https://github.com/basic-gongfu/cixl
Thanks for the feedback. I realize that Cixl isn't really competing with Forth for it's core business, that was never a goal; I'm not much for competing at all. But it's one of the more Forth-like ways of extending C that I've come across, and sometimes extending C is just the right thing to do. A [sub-reddit](https://www.reddit.com/r/cixl/) already exists, but since I'm the only one there so far it doesn't really help. Cixl aims to be really good for scripting; application scripting as in Lua and shell scripting as in Perl. It leans heavily onto C for most things, so I expect a Forth-hosted version would feel quite different. I am curious about the speed though, what is it that makes Forth so fast? Cixl is currently designed as a dispatching interpreter, so that explains some of it; as I understand it most Forths are threaded. But from what I hear, threading it would buy me at most 20%; which still leaves a big gap. Regardless; for an embedded scripting language, speed usually means solving the problem in the host language end exposing the solution.
The operators are shorter names for usual [stack operations](https://github.com/basic-gongfu/cixl#stack); clear, drop, dup and swap; $ is special, it works together with ',' for cutting/stitching the stack. I would like to turn that question around and ask why it's not considered good practice in Forth. One reason in my mind would be that they're less convenient to work with in Forth. They certainly help with keeping the stack clean, something that's considered important by most Forth programmers. I'll usually introduce variables once using stack primitives starts getting messy.
&gt; I would like to turn that question around and ask why it's not considered good practice in Forth. One reason in my mind would be that they're less convenient to work with in Forth. They certainly help with keeping the stack clean, something that's considered important by most Forth programmers. I'll usually introduce variables once using stack primitives starts getting messy. My understanding is as follows: The problem is that they make it more convenient to work with many values at once. This is often not a good thing, because it tempts the programmer into writing more abstract code than necessary instead of just solving the actual problem. Avoiding them thus makes it harder to design overcomplex solutions. This means that a strong tendency to avoid them can help to improve program design. If something really can't be expressed well without them, they should of course be used.
Just wanted to note that I ended up removing the local variables to get more speed. That's how I've come to use variables, as a short cut when I need to move fast and don't have the motivation to write stack poetry. But if I have reasons to revisit code, it often ends up more Forth-like with each rewrite. It's a more gradual approach, which is one of the design goals for Cixl.
&gt; That's how I've come to use variables, as a short cut when I need to move fast I cannot remember a single time when "moving fast" wasn't an euphemism for "accumulating technical debt" over here, so I have become a bit skeptical about this phrase. :) &gt; and don't have the motivation to write stack poetry. I think one should never ever write stack poetry. The general rule I've heard goes something like: If two shuffling words are next to each other (and they preferably shouldn't be), none of them should be among the more complicated ones (like `tuck`, `rot`, or `nip`). If I have a need for something like that, I ask where my design went wrong. I also think that `rot` shouldn't even exist. 
When you're writing a throwaway script or exploring unknown territory, there is no such thing as technical debt; having options is an advantage. I feel like you're referring to a different kind of poetry, as I agree with the rest of your stack heuristics.
Out of curiosity, what in particular about `rot` makes it unfit for existence? I can see most of its uses being replaced with return stack stuff, but often that requires longer sequences of shuffling. But I suppose perhaps you mean that the need for `rot` itself is what should not exist?
"I am curious about the speed though, what is it that makes Forth so fast? " Good question. There are few things IMHO. 1. A brutal policy of simplicity. No extraneous code. 2. It's not really "interpreted" Even indirect threaded code is a list of addresses. The addresses are "interpreted" yes, but the code to do that is 2 or three instructions depending on the CPU. Here is the "inner-interpreter" of a Forth a wrote for the TMS9900 CPU. \ Forth ITC NEXT routine, "inner-interpreter" *IP+ W MOV, *W+ R5 MOV, *R5 B, You can see how small it is. When I skimmed your code I saw a lot of C code. I would recommend studying a Forth written in C to get a sense of how it works under the hood. Then do a version 2 of CIXL with what you learn. 
Intriguing although I don't have it in my head yet how you actually do this. But I really like the possibilities. 
Perl and Cixl share the same hacker mindset, besides syntactic preferences I would expect them to appeal to the same people. {...} is a lambda, it simply captures the environment and has nothing to do with the stack. Functions scanning for arguments is here to stay, but It's perfectly possible to code in postfix style without ever using ',' or '$'.
for implementing such optimizing assembler the arity of each word should be known somehow. Because rearrangement occur at edit time and such state relevant information change with editing, dynamic reassembling (JIT, "Just In edit Time") is an option I think.
Okay, then I didn't get something: {} really has nothing to do with the stack? I thought it makes a cut, so that infix can be used. 
Shit happens :) Have a look at the readme for [cutting](https://github.com/basic-gongfu/cixl#expressions) and [lambdas](https://github.com/basic-gongfu/cixl#lambdas), and please yell if it's still unclear.
I can imagine using a FIFO for *one* world designed to use it, but how would it work for two preceding words of the kind? How would the parameters be distributed to the second word?
Ah, it hit me when I saw the "2 2 3 3 * * + . 13" this is pretty clever. It still seems to me that this is still no silver bullet, but is quite an immense improvement for what would normally alternate between stack juggling, or simply treating the stack as an array.
Here's an implementation that I used in gForth to feel this out. ( This is an implementation of this style of fifo: https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem#Without_semaphores_or_monitors ) variable produceCount variable produceIndex variable consumeCount variable consumeIndex $800 constant size create fifo size cells allot ( empty and reset the fifo ) : empty 0 consumeIndex ! 0 produceIndex ! 0 consumeCount ! 0 produceCount ! fifo size cells erase ; empty ( consume a value non-destructively ) : peek consumeIndex @ fifo + @ ; ( analogous to pop ) : consume consumeIndex @ fifo + @ consumeIndex @ cell+ size cells mod consumeIndex ! cell consumeCount +! ; ( analogous to push ) : produce produceIndex @ fifo + ! produceIndex @ cell+ size cells mod produceIndex ! cell produceCount +! ; ( drop a value from the fifo ) : drop consumeIndex @ cell+ size cells mod consumeIndex ! cell consumeCount +! ; ( Recover the most recently consumed value. Put it back in the fifo. ) : remem consumeIndex @ fifo + @ consumeIndex @ -1 cells + size cells mod consumeIndex ! -1 cells consumeCount +! ; ( display fifo contents, analogous to .s ) : .f consumeIndex @ fifo + produceCount @ consumeCount @ - bounds ?do i @ . cell +loop ; ( non-destructively display next value ) : ? peek . ; ( destructively display next value ) : . consume . ; ( needed for this gForth wrapper to accept number literals ) : lit produce ; ( wrappers around various forth words... ) : char char lit ; : key key produce ; : emit consume emit ; : * consume consume * produce ; : + consume consume + produce ; : - consume consume - produce ; : / consume consume / produce ; : @ consume @ produce ; : ! consume consume ! ; : 1+ consume 1+ produce ; : 1- consume 1- produce ; : = consume consume = produce ; ( The best practices are different with fifos... ) ( Putting a lot of items in the fifo is fine, but you will want your words to clean up after themselves. ) : accept peek 0 ?do key loop ; : type consume 0 ?do emit loop ; 2 lit 2 lit 3 lit 3 lit * * + . \ 4 lit accept type ( accept 4 characters and print it) 
The `WITHIN` example from the slides doesn't work with this, if I'm reading it correctly. I'd love to see some more examples of this in action. I don't think I quite understand how to use this properly. Suppose I want to write a word which takes three values (a, b, c) and produces (a - b - c). How would I do that? Is this intended to be used alongside the stack? As far as I can tell, there is no way to arrange the parameters in the fifo that makes that word work, unless we add what are basically stack manipulation operations. I should probably watch that video sometime...
I think it's more than that. Its existance seems to suggest that such deep stack access is a perfectly legitimate thing to do, which is a way of thinking that does not seem to be helpful - even if it is occassionally true. But even `pick` might be the better choice, because with `pick` it is much more obvious that we're doing something we should better avoid wherever we can.
Efficient in what way? Space? Execution time? This is really easy to test if you actually wrote a little code. Let's take a look. Space first: \ stupid test with locals here : test { a b -- c } a b * a b * * ; here swap - . cr This takes 128 bytes on my test system. \ stupid test w/o locals here : test2 ( a b -- c ) 2dup * &gt;r * r&gt; * ; here swap - . cr This takes 96 bytes on my test system. How about execution time? Let's do a loop of 20,000,000 iterations, running the word twice with different inputs (discarding the results, so we won't be overly I/O bound). \ test loop for locals version : try ( n -- ) 2000 0 DO 10000 0 DO 1 2 test drop 3 4 test drop LOOP LOOP ; This takes 2.05s to run on my test system. \ test loop for non-locals version : try ( n -- ) 2000 0 DO 10000 0 DO 1 2 test2 drop 3 4 test2 drop LOOP LOOP ; This takes 1.64s to run on my test system. So in my quick test, the non-locals version is both smaller in compiled space and faster when running. Notes: - Tested under Gforth 0.7.2 on a Debian GNU/Linux system ("Linux debian 4.14.12-x86_64-linode92 #1 SMP Fri Jan 5 15:34:44 UTC 2018 x86_64 GNU/Linux") - Performance was measured using the standard `time` tool, averaging the resulting `real` line based on 12 runs. - Test system is a Linode 1024. 
I just noticed your edit. They don't add to the dictionary each time the word is called. This is trivial to test, so I'll leave that to you to figure out if you want to verify this.
This is a nicely written series. Thanks for sharing.
Local variables are there to be used when you need them. Everything has trade offs. Understand them and make a decision, Locals can help simplify some code for the programmer. The Forth solution to complex code without locals can be to factor your code into small pieces so each piece (word) does a simple stack manipulation. YMMV. My 2 cents.
That was clever how you subtracted the two here pointers. Thanks I will think to do that next time I want to measure the growth of memory.
I googled tail call recursion and forth and found a page that references Color Forth's tail call recursion. http://wiki.c2.com/?TailCallOptimization I guess what I would need to do is make my own version of the colon compiler that works like color forth's version.
Pure, functional programming allows to write word definitions in continuation-passing style which ensures that each recursive call is in tail-call position. Within some limitations this is possible in Forth.
While not factually incorrect, tail-call optimization in no way requires purity and isn't limited by or functional programming languages.
Thanks for the interesting link and good luck with your own Forth!
Can you explain more what you mean by "the general case"? In colorForth, the word `;` checks to see if the last thing compiled was a `call` instruction. If so, it replaces the opcode with a `jmp`; if not, it compiles a `ret`. Although colorForth is STC (it compiles code to x86 assembly), this doesn't seem too hard to adapt to an ITC or DTC Forth, so I'm curious if by "the general case" you're talking about something broader than this.
Well I could make a word called tail-recurse
As others have already said: have `;` check if the last instruction emitted was a call, then either replace that call with a jump, or emit a return. Simple, effective. Time tested. Alternatively, you could remove the check and implement `-;` to do explicit tail call optimization. Personally I like to use exit compile an return and use `;` for explicit tail call optimization.
For reference, `r&gt;` and `&gt;r` are the standard names for what you call `return-stack-pop` and `return-stack-push`. `r&gt; dup &gt;r` can also be written as `r@`, so your two snippets of code become: r@ current-word ! r@ &gt;r
If ; checks for the last instruction to be a call then why does : f recurse ; f cause a stack overflow? I think I'm missing a step.
ANS Forth doesn't support tail call optimization. You'll want to look into machine Forth and or colorForth, which came about later.
Well, the general case is checking to see if any `RECURSE` can be turned into a jump. This is semantically acceptable if-and-only-if the next *dynamically* executed forth instruction other than branches is an `EXIT`. For example, if you have a `IF ... RECURSE ELSE ... THEN EXIT` sort of structure, that `RECURSE` just jumps straight to an `EXIT`, after it finishes, so it could be turned into a jump. The way colorForth does it is a very literal interpretation of the "tail" in "tail recursion", and only converts a particular instance (the end of definitions) of where the next *static* forth instruction occurs. Does that make sense? In general, tail-recursion means "anywhere it would immediately return afterward, /including indirectly/". At least, that's how I've seen it used most of the time, and is how many other languages interpret it (scheme, for example). I remember reading about the colorForth implementation, and how it basically just required flipping one bit. Potential problem with ITC or DTC (in the general case where the stuff being changed isn't very close to `HERE` - it might not be known that a `RECURSE` can be replaced with a jump until much later): going from a simple call to a branch requires using an extra cell (in most `BRANCH` implementations I've seen, the offset is expected inline), and at the very least the cell right next to it is already in use (for the `EXIT` that tells you it's legal to do this conversion). And of course shifting code is a huge headache (suppose there's a relative branch that goes back to before where you inserted this, and now it's off by 1!). An argument could be made that having your `RECURSE`s that you want changed to jumps always at the end of a definition is good style because it makes it obvious that it's right before an `EXIT`. But whether you're adding `0=` to the front of your conditions to get that recurse into the other branch or explicitly using `TRECURSE`, you're still trying really hard to point out to the compiler what you want to do. Seems more forthlike (weasel word) to just explicitly say you want a tail call when you want a tail call. Thus concludes my long-winded essay I didn't even realize I was making.
I am very interested in colorforth. I just want to make sure that I can get an up to date version and I have to be able to run it inside of a terminal in ubuntu linux.
I imagine most people would find that a bit disgusting and you would have to find the right place to put `r&gt; drop` so that you keep the return stack balanced, and you end up back where you expect, but there is nothing really stopping you. My question would be how you intend to get the return stack into the starting position and still use `recurse` for recursion? It would probably be easier to do in a multi-headed Forth but then you probably wouldn't need to use `recurse`. Enjoy your puzzle
colorForth doesn't work the way you think. There is no colon compiler in the sense that you're thinking. It's a quite different beast. If you want a hint then I would look into subroutine-threading. Once you've understood that then it should be fairly self-explanatory how colorForth does tail-call optimization.
If you have the understanding and ability to do that then that might be a reasonable approach. But perhaps it's easier said than done in a traditional threaded Forth design, where definitions are just lists of words to be called.
AFAIK there's no way to run it in a terminal window. But if you're actually interested, here's a few starting points: There's an ancient port that ran under X11 on 32-bit x86 Linux at https://www.forthworks.com/mirrors/colorforthray.info/XcolorForth.tar.gz - building &amp; running on modern systems might be difficult at best. Howerd Oakford has a current, updated distribution that runs on actual hardware (as has been the traditional way of running colorForth) or under an emulator (Bochs) at http://www.inventio.co.uk/LegacyIndex.htm You can browse a set of colorForth source blocks at https://www.dnd.utwente.nl/~tim/colorforth/Raystm2/mv050314.html (start at #24) Some notes on using the colorForth editor: http://www.greenarraychips.com/home/documents/greg/cf-editor.htm You could also look at http://www.greenarraychips.com/home/support/download-02b.html (arrayForth, from Green Arrays) and it's user manual at http://www.greenarraychips.com/home/documents/greg/DB004-131030-aFUSER.pdf For another alternative implementation using ideas from colorForth, see http://rainbowforth.sourceforge.net/ Be warned though: colorForth is pretty low level which you've expressed disdain for in the past, so it's quite likely not to be all that appealing to you.
To expand on what u/dlyund said ("you would have to find the right place to put r&gt; drop so that you keep the return stack balanced, and you end up back where you expect"), I recommend studying the solution to two similar problems in [FIRST &amp; THIRD: almost FORTH.](https://www.ioccc.org/1992/buzzard.2.design) It's a very simple, very well commented build of a Forth-ish system from the ground up, and it will enlighten your mind and put joy in your fingers. First off, THIRD's main read-code-and-compile-it loop relies on `]` which does some sleight of hand to the return stack so that it can loop forever without blowing up the stack. Get up to that point and make sure you understand it. Later on, THIRD also defines `tail` which generalizes the trick so it can be compiled into other words. It's not exactly what you're asking for because it has a glitchy behavior that its user has to work around (see the comment that reads "the first time we enter this recursive routine"; when/if you return from the tail-recursive word it hops back not to its caller but straight to its grandcaller, so you have to enter the tail-recursive word *from tail position* or Weird Things Happen). But it's close. If you've understood the primitives it's built from you will understand `tail` itself pretty quickly.
Hey, he appears to be teachable this time. You never know.
There is no way to be ans compliant and also be multi-headed?
Well my opinion is that a system like gforth should be able to do anything that assembler can do on a unix system. And so there is nothing stopping me from building another forth inside of the same process that gforth is running and hand control inbetween my other forth and gforth. In my mind this is still being compliant. The key is if it can be done with a minimal wordset.
Sorry to hear about your health and financial status, I hope you find a way to work from home or something. From the quick looks I took at your language it seems different than, well, anything else. For the masses it's too crazy because it doesn't adhere to the algol-style syntax and typical subroutine calling. For the type-lovers it's not ML/haskell enough. For forthers there's too much syntax and options, and the names are cryptic at first look. None of that matters too much as long as you're having fun. But I don't think you can expect too many donations.
This reminds me a lot like the EX instruction in green arrays F18a instruction set; if I recall correctly this is usually called coroutine [call], after it's common use. I consider it more of an optimization because there are other ways to coroutine, but it's a cutie :).
I call it `later&gt;`. Because the part after later&gt; is done... later. Example: \ Temporarily set base; restore when the calling word exits. : base&gt; ( u -- ) base @ swap base ! later&gt; base ! ; \ Parse an octal number. : o# 8 base&gt; parse-name number ; immediate
That's a good use for it. I removed the `base` user variable from my current Forth, unsure if I'll end up regretting it in the long run or not.
In some versions of my Forth I've had this as `later`. (On and off since ~2005/2006). IIRC, I picked it up from a user of colorforth around that time. I've definitely used it for things like your HTML generation, and in some experiments with infix style math. E.g., :later pop pop swap push push ; :add: later + ; :subtract: later - ; [ #1 add: #3 subtract: #2 ] call putn 
Have you read the standard yet? Given that you have a fixation on the ANS standard, you should. Here's a PDF of it: http://forthworks.com/forth/standards/DPANS/DPANS.pdf This is about 220 pages, though 18-20 are just indexes, tables of content, etc. So there's about 200 pages of actual content. At three minutes per page this should take around ten hours to read through completely, though you could focus on the beginning part (covering CORE and CORE EXT, and the introductory bits in sections 1-5), which would be around 60 pages (or perhaps 2-3 hours of reading at a modest pace). If you just want to know the requirements for a system to be considered compliant, see sections 3 and 5.
:) I always liked `pass`. I believe the name EX comes from its common usage; calling a word dynamically at runtime. This is called EXECUTE in ANS Forth. If I understand correctly, the instruction sequence `... PUSH EX ...` would be equivalent to EXECUTE. Its usage as coroutine [call], where two words hop back and forward between each other is nice but less common in practice. Strictly speaking, either use case can be implemented by combining CALL and ; : EXECUTE PUSH ; Simple enough that you may be wondering why it would be implemented in hardware. My guess is that it's half convenience and half to avoid the overhead of the call and execution of the definition of EXECUTE in memory just to save the instruction pointer to the return stack before `;` /me Shrugs :). 
I had always assumed the name was short for exchange, since it exchanges the instruction pointer and the top of the return stack.
We'll see.
Neat, it's nice to see that I'm not the only one playing that game. [Cixl](https://github.com/basic-gongfu/cixl#expressions) allows using infix/prefix without reversing, to the point where people question if it's really a Forth; but it still defaults to postfix.
Very cool, even if I do hate the forward scanning and the syntax and the bloat. It's awesome to see people working on and trying new things, that's how languages improve.
As to why implement such an instruction hardware, it's so that you can have an extremely fast and lightweight concurrency primitive.
Good publicity, in any case.
Pastebin link is gone.
You can answer to your friends: Yes, you are right, techincally speaking forth is slower than C (because, effectively, most of them are wriited in c), but in fact, it's false. and you can add: - SQL is slower than raw files (because SQL is based on raw files), so continue to use raw files for huge DB managment. - C is faster than Forth, because C is only one letter and Forth is five letters - A ferrari is faster than a truck, it's why UPS, Fedex, etc. use exclusively ferrari. in my life, I have a lot of sample where forth software are smaller, faster, easier to maintain, ..., than C software :-) let your friend thinking whaamt is think and may the foth be with you. 
FORTH should not ideally be compiled in C and then run within a UNIX. This is why (although I admit I haven't persued it) I was originally planning to try and write a FORTH in DOS, or maybe play around with Pygmy; because DOS is the only environment I know of which allows raw binaries and metacompilation. No open source UNIX allows that any more, AFAIK; and with the advent of UEFI, running FORTH directly on bare hardware without an operating system is no longer possible on a PC either. No modern FORTH environment is the real thing, in truth. FORTH was meant to be the only program running on a given machine, and metacompilation gave it an advantage that virtually nothing else has. It's not meant to be hosted within an operating system like Windows or UNIX; cmFORTH was developed before Windows existed. Also, although GNU FORTH runs within an operating system, its' primitives are written directly in assembly, and its' loops are faster than anything else I've seen. The way I would ideally use FORTH in a modern context, would be to use GNU FORTH as an iteration/logic structure language, for controlling binaries written in C or the OOP languages. The binaries only run once by themselves, but my loops for automating them are written in FORTH; think of it like Star Trek's warp drive, to make an analogy.
EXchange is just how I visualized it, if Chuck meant EXecute I have no qualms with that. To my mind what the instruction actually does is yield the processor to a co-routine, the fact that it's a useful factor for implementing an execute macro if your instruction set lacks a native execute instruction is secondary. But that's only my personal take on it, I make no claims of deep insight on the matter. As far as a justification for including it in your stack machine's instruction set, if you're doing hard (deterministic) realtime it gives you a low overhead way to interleave lengthy non-critical foreground work with time-critical polling of devices (look ma no interrupts, no scheduler!). 
If you want to see even more speed improvements on Gforth, use a build from the [master branch.](https://github.com/forthy42/gforth/tree/master) As you can see, Gforth 0.7.3 (the most recent release) is behind the master branch by several thousand commits. And there are, in fact, [major speed improvements](https://www.reddit.com/r/Forth/comments/7parow/gforth_speed/dsg56ps/) in using a master build.
Maybe someone who isn't actually using forth can't give relevant suggestions. There are forths written in assembly (freeforth, jonesforth, eforth, ...) and forths that generate optimized assembly (swiftforth). Again, have you used a search engine or looked at some implementations? Beating C in raw speed isn't easy given the x86 architecture's register oriented design and monstrous complexity that only an advanced optimizing compiler can take advantage of.
Thanks for the tip, although unfortunately I am in Windows 7 at the moment, and I've never really wanted to subject myself to source code compilation in this environment. If I install FreeBSD in a vm again, though, I will definitely try it there.
No problem. It's a shame that the Gforth developer hasn't released his changes yet...
It seems to me that if you implemented forth inside of c then it would be c compliant as long as you switched control back over to c. When the forth was operating though those operations would not be c compliant.
yes i agree with that the problem is on the other side. like python, i use it for my daily work, i earn money on it. but compared to all these other language, i think forth need a huge modernization, like much more modern tech support. i knew in the old time, its very advanced than other lang to write a arcade game, but nowaday, it looks like much after. people in community always talk about coding for microcontroller or how much kilobytes they could run on. i think its amazing, but if they could tell me how less code they could to build an HTTP API gate, or an android application, that wil be much more amazing
I think it is possible for forth and c to live in the same process and pass control to each other. Forth simply has to respect that c controls the memory.
Okay. Well you're wrong... But I'll be damned if I'll be the one to educate you. Give it a few months and maybe you'll that realize what you're saying is absolute nonsense. In the best case it lacks all nuance and understanding, and in the worst case it demonstrates your pathological ignorance.
To create a Forth as fast as modern C compilers, you must use the same technology. That being optimizing native code compilers. To see what that looks like in terms of speed, take a look at VFX Forth by MPE. It comes close to C but GCC can always beat it because GCC is megabytes of code and it can also do a global code review to squeeze the last bit of speed out of the program. However modern is beginning to suffer from "nasal demons", unexpected side effects of intense code optimization. So with native code compilers Forth gives you speed plus a REPL to interactively test things. That means not living in the debugger for days as you might do "occasionally" with C. And you can compile 100,000 lines of code almost instantly on a modern x86 machine. My 2 cents. 
Gforth was built for easy compilation on multiple platforms. They have allowed the C compiler to make excellent optimizations but absolute speed superiority was not the objective. Its an open source product. VFX is a commercial product that had it's origins 30 years ago as a threaded Forth as was common then.
What 'recurse' does depends on the implementation. In 8th, it does "tail-call elimination" (as do ";" and ";;") so that it's perfectly safe to have "recurse ;" as an infinite loop. It may be more clear to the maintainer of the code if you used "repeat ... again", though.
Speed is measured within a specific context. If that context doesn't apply, the speed comparison is not useful. "speed" can also include "speed of development" (or ... debugging, ... maintenance, etc.). Even though 8th is build using C/C++, it can outperform C equivalent programs because of memory caching effects (for example). But that's not the point of 8th: ease of use, security, etc. are.
Thanks! I think the block editor you've made is a great example of a Forth application; it's concise, simple to extend and understand and most importantly is that is reuses already existing mechanisms available in the Forth interpreter. It really captures the essence of Forth and I was amazed you could make something that simple but functional at the same time. It didn't take too long to port either. Let me know if you play around with it any more.
its very cool
Very cool. This is exactly the kind of project I've been looking for to experiment with and work off of.
Sure, it wasn't a linear approach, I've continuously tweaked and modified the system at all levels as I've learned more about what works and what does not. I'd already worked with FPGAs at university, using the same board, so I already had a working build system and new what it should be capable of. I heard of the J1 processor, and leaning towards VHDL I decided to make my own version in my preferred language. I used GHDL/GTKwave and an assembler I wrote in Perl to get a basic simulation working, as I don't have an oscilloscope I adapted modules available from OpenCores.org to do most of the external interfacing (the VGA output, PS/2 keyboard and the UART). Once you have this you can start writing small test programs to exercise that hardware, getting the processor to echo back what it has read in via the UART was the major step in proving that the SoC works, after that implementing the Forth for really is quite simple and proceeded quite rapidly. The most difficult bit is getting that PC&lt;-&gt;UART&lt;-&gt;CPU bridge working. Developing the CPU in the beginning was mostly a case of: 1) Trying something out in simulation 2) Putting it on the FPGA 3) Determining whether the new system was correct by looking at the debugging information on the VGA monitor 4) Going back to the simulation to work out what went wrong. The system has evolved quite a bit, and I've managed to simplify things after getting everything working. Just getting things working is the difficult bit, after that you've got a known working system you can play around with. For example the VGA and PS/2 keyboard used to have their own interface to the CPU, the VGA was memory mapped and the PS/2 keyboard was similar to the UART interface, but not exactly the same. I rewrote the VGA and PS/2 interface to look like UART by putting a VT100 emulation module I made in between it all. After I have got this working, I might go back to the drawing board when it comes to the toolchain, or some other different part of the system. For example, now I have a compiler for the system written in C, and I am thinking of moving to a more traditional metacompiler written in Forth. Apart from this the main difficulties are the same as you would have on any FPGA based project: 1) Poor tooling 2) Simulations not matching what the hardware actually does 3) Fighting against the language (VHDL). I hope this helps!
Feel free to use any part of the project, but be aware that different components have different licenses attached to them, if it was up to me everything would be under the MIT license but I've used some components from elsewhere. You can check each file for its license, and they should all work together with no problems. What were you thinking of doing?
Thanks, that's very interesting.
Thanks, that's very interesting.
I don't understand how Forth -&gt; C without making a lot of, IMO, language-breaking concessions, in one direction or the other. I think whether or not your long ints are unsigned or not is the least of your issues in this project. Also it doesn't matter if they're unsigned or not, which you should already know.
Forth is typically not typed, a 64bit forth usually has 64bit words. There's no information about the sign in those 64 bits, it's up to the user to use it one way or another. If you want to go low level (which you clearly stated you *don't* previously) you should first try low level (assembly) before trying to implement it. If you are trying to build a typed forth then you need to think about the design and what datatypes do you want to have. One can e.g. encode the type in 4 bits (giving space to 16 datatypes) and have the other 60 bits represent the data. One can also have all data indirect, the word being a pointer into memory where you can build any structures you like (and pay with performance for the indirection and memory management).
Did it use a virtual data stack? That's the only way I can see this working, if the C functions pass back the virtual data stack pointer on return. I'd like to see it, although I'm still doubtful it could match the flexibility in execution. Regarding signedness, I understand that. I doubt read_harder does, or he wouldn't need to ask about it.
You should google *explicit type casting in c*
8-bit Commodore systems also had 2x2 blocks.
It was quite popular on the PET as the characters couldn't be customized and it didn't have bitmap graphics.
It's like a Forthy (pared-down, imperative) relative of the method modifiers in CLOS and its offspring. I'm mostly familiar with the Moose/Perl 6 versions of these, so these may not be the CLOS names, but "around" lets a subclass or a consumed role (a.k.a. trait, mixin) add behavior before/after the original method implementation, and the more controversial "inner/augment" lets a superclass leave a hole in its implementation for a subclass to fill. But, of course, its Forthiness is going to make `reverse` a different beast in practice. I get the feeling that letting it hang around my brain long enough is likely to result in a very educational mind warp.
This is fun.
This was written for a Python to Forth compiler which is written in Python, so it needs to provide a python interface. The compiler was written in Python because it utilizes the ast module for parsing the Python source so it is easier to keep the whole program in Python to avoid some kind of weird python/forth layer. Writing a Python parser in Forth is not something I ever want to do. C was chosen for the bits that need to be fast because...C is fast, C Python modules are easy to write, and translating Javascript (from the original forth wizard) to C is trivial
Ah, OK. Yes, writing a Python parser is not something fun to try in Forth.
To what extent is the RTX2000 compatible with the N4000? Do you think it could run cmForth?
It looks like the RTX2000 is the successor to NC4016, I'm not sure how the 4016 compares to the 4000. This page http://users.ece.cmu.edu/~koopman/stack_computers/sec4_5.html contains some comparisons between the RTX2000 and the NC4016 It looks like most of the changes are additional features so I'd guess that cmForth would not be hard to port. I know lots of people around here have actual experience with these chips, hopefully they have something more to say about this.
Sounds like a hash table of objects. How big is the default map? Does it resize automatically? What about collisions?
Nice. Very thoughtful metrics.
It is a complete Colorforth environment.
I've seen that called "CO" on comp.lang.forth a long time ago. : CO 2R&gt; &gt;R &gt;R . Probably not standard since it assumes a thing or two about return addresses.
Apologies for that, I must have done something. I'll post it again if you're interested though it may take me about a week as I'm a little busy at present. :)
Cool!
Unfortunately, I'm busy as well and would probably only glance over it.
Cool, I'll do so once I get a chance and fix the link. :)
I was looking for this, thanks!
I suspect the interviewer would think you just made it up. :-) Sad that they would not even ask you what it was.
What was the code size comparison between C and 8th?
I know at least a few routines have been written but I don't know how popular any of them are. The best person to ask is probably Andre Fachat, who I believe has written one himself but is very knowledgeable about the PETs. Here is something he wrote: https://www.youtube.com/watch?v=AUkQB_b16SI
I did that with `awk`once. I wrote it out as a one-liner, and the interviewer said, "Oh. Well. Wow, I don't think I've ever seen one so short before. Could you... uh... make one that's more efficient?" Interviews like that are fun.
Heh :) I was just feeling more aggressive than usual...
I'm not really clear what the test is asking for. Do I need coffee?
Probably. ;-)
That's awesome! The 6809 is a badass piece of hardware, too. Love them.
https://github.com/gordonjcp/miragetools/ It's part of a wider project of disassembling the Mirage software and firmware.
I know there's some html versions [here](http://lars.nocrew.org/)
Thanks, but unfortunately still with the same html problems. The word details are split across multiple pages, text needs to be pulled out of the html page, and the stack comments need to be split up. Ideally I want a .tsv file with lines like this(but using tabs instead of commas): +,n1 n2,n3 /mod,n1 n2,n3 n4 &gt;resolve,adr,, 
&gt; But I don't want to have to pull the info out of a html page. I don't know why. It's some of the clearest and simplest HTML I've ever seen. For most of the definitions, the most you'd have to strip out are the strong tags, which could be done easily with sed. You could also parse from one HR tag to the next. Use cat -n to learn the line numbers of each HR tag, and then use sed to dump each of those ranges out to individual files.
It does not really matter how easy this could be, it will always be easier to use existing work. And probably also less error prone. 
Very nice. Sometimes a preemptive system is the only thing that does the job. Have you ever worked with a system that uses the Forth cooperative model? It is very light weight in that the context switch times are extremely fast. And it removes the need for locking critical resources since I/O primitives run to completion. The secret I believe is to put the truly time critical tasks on the interrupt like grabbing a byte and putting it in a queue. Everything else "cooperates" I was able to have 3 tasks (Console, remote RS488, robotics task) and an interrupt routine doing time critical data reading on an industrial device that did real-time robotics with live baby birds using this method. It all ran on a 68HC11 @ 8MHz using ITC Forth and some assembler. (there were not many resources left however with that little CPU) ;-) 
Can you use a pdf file? https://www.openfirmware.info/data/docs/dpans94.pdf
http://forthworks.com/temp/z.tsv or gopher://forthworks.com/0/temp/z.tsv has CORE and CORE EXT, with the reference numbers, word name, and stack before/after. This includes compile-time effects for some words (e.g., DO/LOOP/BEGIN/AGAIN, etc); most are execution time effects.
Thanks for sharing this! It's to bad the html does not include the return stack effects
I might actually end up using it, it's probably the least ideal format I can think of but this looks like the most complete resource I've found so far, it even contains the return stack effects. I wonder what they generated the pdf from
This is exactly the type of document that I'm looking for! It's just missing the return stack effects
It actually needs a bit more than that. Ideally it'd have columns for runtime and compile time effects, as well as return stsck effects. I may take a stab at that in a few weeks (too many projects currently underway to take on another)
What about http://lars.nocrew.org/dpans/dpansf.htm
Good initiative. I would try to make clear that, although most people will not find a practical use for the language, learning it is a beautiful journey and a very mind-opening experience.
And ...that it may influence how you code in your normally used languages as well. 
Forth has ruined me. Using anything else now is an exercise in swimming up a waterfall.
Too true!
Exactamundo!
why not make a high performance socket server? or even http server
It's looking more like IRC from here, I've written enough HTTP servers to last a lifetime. But I have a few more pieces of infrastructure to attend to first, TCP servers being one of them.
Probably the SVFIG can help.
The easiest answer I can provide is probably the reason everything else is managed the way it is: caching. The registers are a cache as much as anything else is at this point. I know this is a very unsatisfying answer, but it does solve the issue of using the same data over and over. Unfortunately, compilers are not as smart as people seem to think, and that's why high-traffic sections of code are still coded at the assembler level.
It is because RAM and ROM are not the same. Different costs, sizes, speed and power requirements. The trade-offs might be blurred these days, but still valid for most embedded systems. See the Wikipedia page on [Harvard architecture](https://en.wikipedia.org/wiki/Harvard_architecture) for more information.
**Harvard architecture** The Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit, and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
There exist some CPU architectures with a minimal, non accessible register set. These designs commonly include internal memory as part of the address space. Typical examples are the mc6802, the ti9995 and Parallax Propeller. For such architectures register names may be indeed a somewhat debatable nomenclature.
I somewhat dislike exceptions. I can easily tolerate them if they're actually *exceptional* (= rare), like they tend to be in C++ code. When they're everywhere, like is often the case with python code, they feel very cumbersome. IMO most usages of exceptions in most languages would be better replaced by more expressive use of types. Sum types, for example, are a step in the right direction; a file reading routine does not need to throw `not_found` or `permission_denied` if it's return type can be `Success contents | NotFound | PermissionDenied`. Sum types may not seem very forthy at first, but underneath lies a simple preference for encoding things in values that you can handle directly instead of constructing convoluted control flow that is not always obvious at first glance. This preference, to me, seems quite forth-like.
Excellent point. These types can also be used in conjunction with multiple return values from a function, similar to python, but without the explicit need for a tuple. After all, the stack is our playground. Why fixate on one return value when we can return as much as we please?
[Here](https://github.com/basic-gongfu/cixl/blob/master/devlog/collab_server.md) you go :)
If you need exception handling in the form of extensive debugging (preserves the stack for inspection), take a look at this post I made about exceptions in Forth: https://www.reddit.com/r/Forth/comments/7g80a1/try_throw_catch_release/
thanks, checking. also, are you chinese?
More like swedish. The story behind the logo is that I've been training/teaching Ving Tsun gongfu for 25 years; and the more experience I get, the more it all looks the same; I apply pretty much the same principles to everything these days, including programming.
I'll try to address your concerns as best as my coffee will permit. *When to use them* Using exceptions has to do with how much abstraction you want to preserve in your code. By raising an exception in a deeply nested routine that doesn't have the contextual knowledge to handle a situation, you transfer control to the level in your code that does know what to do. Often to code that is abstracting away differences in order to present a unified interface to other parts of the system. *How to avoid them* Reduce abstraction, but of course sometimes the cure is worse than the disease. *Simplification* You can sprinkle some syntactic sugar to make it more palatable, I use words like *suppress*, *avert*, *abide* that are really just sugar. You could also simplify the semantics if you don't care about standards compliance, e.g. avoiding throw codes altogether. At the VM instruction level a fairly clean exception handling mechanism can be done with two paired VM instructions, one that 'tries' and skips over the next instruction on failure; and one that pops the return stack into a VM register (the handler list head) and pushes 0 onto the data stack.
I agree 100%. My only caveat is that if multiple levels of callers will each in turn have to check for `NotFound` and `PermissionDenied` only to once more percolate those values upwards then throwing an exception is a cleaner solution. The greater the nesting between the identification and the disposition of an *exceptional* case the more using the exception mechanism is justified, and conversely the shallower the nesting the less it is justified.
Thanks for your reply. I agree and I used to enjoy the richness of these type systems. I can also imagine other ways of working with exceptional cases like putting multiple values on the stack or filling `errno` like in C. My issue lies with real world examples - does it work well?
&gt; Reduce abstraction, but of course sometimes the cure is worse than the disease. Have you actually tried this? Or am I the only one who doesn't like exceptions :)
I also dislike exceptions. So in '8th', I made exceptions actually exceptions which are (almost always) fatal -- more suited to the testing/alpha/beta phase of app design. Production code should check for the appropriate error return and do something sane if it is possible (and throw an exception if it is impossible to handle).
So your exceptions aren't typical exceptions, they are just fancy exits. No catch. Cool :)
Not very fancy, just exits which *can* be caught and handled, if there's a way to do it reasonably.
It might depend on why you actually want to catch the exception instead of just having it pass through. You can think of exceptions as continuations (see [this article](http://matt.might.net/articles/implementing-exceptions/)) if that helps. Mentally pair up your throwers and catchers in your code (instead of having them "blindly" do their half of the job). It might give you a better idea of the actual control flow you want. And you can decide if and how you want to remove each pair. For example, by doing "Look Before You Leap" instead of "it's easier to ask for forgiveness than permission". I don't have exceptions implemented in Flpc (yet) and found exit with extra info was good enough. I didn't find I miss them even though I normally use Python a lot. There I mainly have try/catch around REPLs and evals and calls to libraries I don't want to dig into the source of.
Python is the loser in my book, regarding exceptions. They're ubiquitous and are used for behavior that isn't really exceptional. It favors throwing exceptions instead of handling errors. This necessitates a simple, clean syntax for handling them, sure. It might just be the name that bugs me, really: calling a non-exceptional control flow feature exceptions.
I tried writing there, got rejected because I'm not subscribed. Wrote an e-mail to the person who manages the list, no reply. Dead ends everywhere -_-
My rule of thumb is no more than one stack shuffling word at the beginning of any phrasal fragment and none within. 
I fix the deep to "pick4" and work ok for now, R stack can be for aux.
I wonder how hard it would be to implement a [Maybe/Option type](https://en.wikipedia.org/wiki/Option_type) in Forth, which is a form of explicit exception handling that sees a lot of use in functional programming.
**Option type** In programming languages (more so functional programming languages) and type theory, an option type or maybe type is a polymorphic type that represents encapsulation of an optional value; e.g., it is used as the return type of functions which may or may not return a meaningful value when they are applied. It consists of a constructor which either is empty (named None or Nothing), or which encapsulates the original data type A (written Just A or Some A). Outside of functional programming, these are termed nullable types. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I would like to note here that the size of an 'unsigned long int' might widely vary depending on what kind of platform it is compiled on: a long has to be as large as an int and never larger than a long long, but an int has to be as large as a char and never be larger than a long. Ergo: It is possible for char, int, long (and even long long) to all be the same size, which might be 8 bits. If you want to be sure the field is large enough, use types like int32_t or int64_t that are provided by types.h.
This is very interesting, although the article already seems to be written in 1996. I wonder: Wouldn't the easiest way to make a (threaded) Forth be preemptively scheduled to add, at the start of the 'threading' part, the code that yields control back to the scheduler?
I want to make the side-note here that at least systems languages that statically compile to bare-metal assembly like C++ and Rust attempt to rival C in speed and succeed in doing so: Their enhanced type systems allow compilers to optimize more rigoriously than a C compiler itself can. Obviously, interpreted or JIT-compiled languages that have a dynamic runtime or implement their own virtual machine (Java, C#, Python, JavaScript) are slower because they need to do more work (like typechecks) at runtime, and also performs garbage collection. But of course 'raw execution speed' is only a single metric, and these languages optimize for different metrics (like ease of use, interoptability, portability, debuggability, etc.) than C.
There is nothing really stopping interpreted languages from rivaling statically compiled languages in speed. JIT technology is powerful and has come along way and only continues to improve. One thing to keep in mind is that JIT can perform optimizations that static compilation cannot. Garbage collection is not really a performance issue when we all have these monster 8+ core machines. The garbage collector runs on another core. 
You are right that JIT languages get closer and closer to statically compiled languages in speed. I would like to argue that the inverse is also true: static compilation can do optimizations that JIT-compilation cannot. Garbage Collection is unfortunately not a problem that can easily be pushed to another core, because of race-conditions: Either your execution model is set up in such a way that the current core can collect its own garbage with the other cores continuing on (which is allowed only if you are certain that each core has its own exclusive memory.), or we have to do a stop-the-world to make sure there are no race conditions.
Technically speaking, in a way FORTH isn't really a ***language***, per se. What I mean by that, is that at its' most basic or cut down level, FORTH consists purely of those operations which are universal; which you can and will perform in any language on any Turing machine. Increment a pointer, execute a function corresponding with the number at the pointer, repeat. That is not true of GNU FORTH, perhaps; but it was of many others. FORTH's default state consists almost entirely of primitives, with as close as possible to no composite words. Composite words were considered the domain of the individual application. Individual applications consisted not only of the program itself, but literally the higher level vocabulary or building blocks that were used to create said application.
I agree with you that Forth could make a great platform to bootstrap efforts for VMs and interpreters. If we did a survey of how many Forth courses are offered at Comp. Sci or E.E. programs it might provide some insight as to why it is not more common. Some work was done on compiling C to Forth for the RTX line of Forth CPUs and it was found that an extra register to keep a stack frame would have been a big improvement. It's not a perfect fit to conventional languages.
If your target is a Forth machine (real or virtual) you're better off just extending the Forth with whatever concepts from the other language you find useful for your application. Forth assimilates.
Forth works best as a compiler than anything else, I've learned. As an interactive language as well, it performs extremely well. But modern CPUs are built with the idea of a specific style of execution, which can be difficult for Forth to meet the requirements of.
With this 'specific style of execution' do you mean that most modern CPUs are register-based rather than stack-based?
Isn't the idea of many Forths to only implement the bare minimum as primitives (the set of which is expanded only when the extra efficiency of implementing something primitively is really necessary), with nearly all words being written in Forth? And what about the ANSI Forth standard? &gt; Individual applications consisted not only of the program itself, but literally the higher level vocabulary or building blocks that were used to create said application. Isn't the same true in all (non-trivial) applications? Of course, how large a standard library that certain tasks can be offloaded to that a certain language platform has varies, but in its essence I'd say this is universally true in programming. 
Interestingly, I read somewhere that C is actually quite a bad choice as intermediary compulation language because you (a) throw a lot of semantic- and type-related information away to conform to C's peculiarities and (b) it is not a context-free language. 
Maybe one could convert Java byte code to Forth. Then all programming languages that compile to Java byte code could run on a Forth system.
C may be a questionable choice for immediate code generation. It is however the only language I know (inclusive the non ANSI de facto label-address extension) which allows the portable Implementation of threaded interpreters.
I think it's more than that. For example, IIRC many ARM processors will predict every indirect branch as a subroutine return, which is horrible for indirect threaded code.
no matter what forth, i am always care about the benchmarks
Depends on the platform, as well. A stack frame might not be best for a bytecode Forth (leads to bytecode bloat), but if frame access was assembled, it would be quite fast on an Intel chip that has so many commands for indirect memory access.
Maybe I didn't state it as well as I intended. In any application where you require a transformation from one code to another with extreme precision and control, Forth does a stellar job. In the instances where I've built compilers on top of Forth, it always performs beyond expectations. In the instances where I've used Forth to construct a proof-of-concept, it always performs beyond expectations. That's more accurate to my point.
Register or Stack is orthogonal to my point. Modern CPU architectures are geared towards C, where the stack is a cache of a specific set of values and the return address, and very rarely undergoes radical change except between function calls. In Forth, this is practically the opposite: the stack is in a constant state of change, and function calls modify practically nothing.
Sometimes a proof-of-concept is useful as well.
Hello, You will find some benchmarks into the "examples" directory. Particulary, the "binary tree" benchmark that is used to benchmark various languages according to memory management here : http://benchmarksgame.alioth.debian.org/u64q/binarytrees.html A difference is that Oforth is a 32bits application and I think those benchmarks are compiling 64bits applications. If you want to test with the GC doing the job, you can use : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeNew.of If you want to test with manual allocation/desallocation : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeAlloc.of The --W4 option is to use 4 threads, whatever the number of cores on your plateform (I think this is the conditions of the benchmark). You can alos use : --W1 (or no option) to use only one thread. --W0 to use the number of threads equal to the number of cores of your system. Franck 
Here is the last version I found corresponding to the benchmark implemented in Oforth : https://web.archive.org/web/20170103000340/http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=binarytrees 
I don't have time right now to compile or run other languages. On my system (core I7) : oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeNew.of takes 5.4 seconds. oforth --P"#[ 20 binarytree ] bench ." --W4 examples/binarytreeAlloc.of run takes 4.8 seconds 
thanks, that also helps
It gives you a lot of flexibility. The Forth source code becomes a relocatable object file for the toolchain. The Forth compiler replaces the linker as well as the compiler in a conventional tool chain. Forth source code can be compiled to native code for performance, or compiled to byte-code for small size or threaded code for something in between.
The change was [one year ago](https://alioth.debian.org/forum/forum.php?thread_id=14996&amp;forum_id=2965&amp;group_id=100815).
Just in case you didn't spot it already, there's some material on Youtube. IIRC when Jeff Fox died Samuel Falvo among others took actions in order to backup the site, thinking it would go down sooner or later. See for instance [1x Forth](https://www.youtube.com/watch?v=NK0NwqF8F0k)
Thanks I have updated the benchmark (in fact, it is simplier than the previous one) On my system (Core i7), the results for the new benchmark are : oforth --P"#[ 21 binarytree ] bench ." --W4 examples/newBinarytreeNew.of ==&gt; 5.2s oforth --P"#[ 21 binarytree ] bench ." --W4 examples/newBinarytreeAlloc.of ==&gt; 4.1 s 
fyi There are some gforth programs from 12 years ago in [these subfolders](https://alioth.debian.org/scm/viewvc.php/shootout/bench/?root=shootout).
I'm not sure what you're getting at , but at the end of the day, you still have a Forth interpreter, whether it's implemented in cpu instructions or verilog. Maybe you want to 'hardcode' your forth app in the FPGA. No interpreter, it's wired to only run the one app. If you are going to do that, why use forth at all? Why have a middleman? 
The solution lays in the initial configuration of a specialized soft-core which directly exposes FPGA ressources though it machine-code format. The overhead for this is minimal. I currently work on such design (of which I can not tell more details at current, sorry). Beside this, there exist already some chips which allow fast reconfigurations. You can expect much more development in the near time.
Yes.
That was not my question. I question the future **demanded** requirement.
I'm sorry then, I still don't understand what you're asking. 
I also started a web backup after reading about his death. However, the videos where not part of it. It can be that Samuel Falvo or others had direct access to the server(s?) and given there where still somewhere restored now the possibility make them available.
I beg your pardon that I did not written specifically enough to prevent misunderstandings.
The idea you describe is exactly how the Forth Cooperative tasking system works. Technically it would not be "preemptive" because a Forth definition would never be interrupted in the middle of the code. I think you have realized the potential of the Forth cooperative model in that every I/O operation returns to scheduler. This means the time-slices are very atomic which makes the system more responsive than is intuitively obvious. Combined with a fast response interrupt for hard real-time requirements (asynchronous data acquisition mostly) you get an extremely good real-time O/S
Something exactly like the defstruct macro, generating words like: title@, author@, subject@, book-id@, title! author! subject!, and book-id!. Each of these words will have different behavior according the type, considering a Forth implementation with a floating point stack.
In Gforth I can write: s" mydef" nextname :, but this only works in the interpreted mode, not inside a colon definition.
Thanks, very interesting.
Ah, so that would be a list of words you provide. So you want it to work like "defstruct book title author subject id" and it would generate "book.title! book.title@ book.author! book.author@ book.subject! book.subject@ book.id! book.id@". Possibly a "book.new" as well? It seems like you'd have to do a little bit of work to actually make a type system. Still, it can be done. There's very little in Forth that will be able to just give you structures like this; you'll have to work to get them. However, this too, is not very hard. What you're asking for basically boils down to a few string manipulations (and knowledge of how to construct a new entry in the dictionary with them), and calculating offsets into the structure itself, and finally reserving space in memory for a new structure. I suggest (automatically) creating an intermediate structure that holds all the offsets and structure names, and then generating the definition words from there. Look into "create" and "does&gt;" as well as "postpone".
The presence of parsing words without non-parsing factors is an annoyance in many code-generating cases. There's a really clever way around it that gforth includes called EXECUTE-PARSING, but it's quite a hack and requires some words from the core, block, block-ext, exception, file, memory, search, and search-ext wordsets (but all of which are standard). It works by constructing a string to evaluate, but avoids the usual late-binding issues associated with that method by ensuring that the first word in the string to be executed only does exactly what the XT you pass does. The exact details are in compat/execute-parsing.fs. That being said, I recall some saying that the automatic name-generation of defstruct in Common Lisp was a bad idea. In standard forth, it's certainly more difficult. The usual structure-defining words have a much easier time of it by letting the user supply all the names (for an example, look at how simple forth 200x structures are...). That being said, it's definitely possible. I'd note, though, that while some-field@ may correctly get a floating-point number on the floating-point stack, the user will still have to know that it's a floating-point number and use f+, f*, etc. 
It's usually trivial to define a variant of `:` that can take the name string as an argument off the stack if your Forth doesn't provide one out of the box. That's all you really need to build up the rest of your hypothetical. Why bother and would it lead to a well factored Forth application is a different matter.
At first I thought Forth has the same capabilities for metaprogramming than Lisp with postpone/compile, and etc. But thinking more deep about it, I came to the conclusion that things are way harder in Forth since there isn't a stage for "macro expanding", before compilation, that exists in Lisp. It should not be difficult to create such environment, though.
EXECUTE just runs a function, EXECUTE-PARSING runs a function, but sets a string as the input source. E.g., : s+ { addr1 u1 addr2 u2 -- addr u } u1 u2 + allocate throw { addr } addr1 addr u1 move addr2 addr u1 + u2 move addr u1 u2 + ; : reorder { a b c -- a b c a b c } a a b c a b c ; : getter ( n a u -- a' u' ) s" @" s+ ['] create execute-parsing , does&gt; @ + @ ; : setter ( n a u -- a' u' ) s" !" s+ ['] create execute-parsing , does&gt; @ + ! ; : make-field ( n a u -- n-1 ) reorder getter setter 1- ; 5 s" title" make-field In this, `setter` and `getter` will pass the constructed string to `create`, so that `create` uses this instead of the input stream.
So basically registers serve as an directly accessible L0 cache. 
That would be a good way to think about them. Think about it: what do Forthers usually store in the CPU registers? The element at the top of the stack, the pointers for both the parameter and return stack, the pointer to the next instruction. Those are all high-traffic items, at least as far as the primitives and the virtual machine are concerned. I wouldn't trust a compiler to make the same executive decisions.
I'm curious as to why my posts here get downvoted so heavily. Maybe a function of so few people in the subreddit?
May be because you are posting about every micro feature of a language nobody cares about? P.S. I've never down-voted your posts. 
Thanks. Considering this is a Forth group, and 8th is a Forth implementation, I would assume posts about it could be at least somewhat interesting to the group; and if not, ignored. If I posted about every micro feature, there would be a ton more postings from me.
I often upvote your posts but downvoted this one. The reason is simple as u/giant-torque said - I consider this a micro feature and therefore bordering on spam. If someone wants to read about small changes like this one they will visit the forums.
I can't speak for anyone else but I upvoted this post because there is more than meets the eye with microfeatures when they are compared and contrasted with other re-factorings and approaches. At least for me, forth programming is sometimes about slowing down and thinking deliberately about one plus one equals two. Examining the least examined assumptions, those that we usually take at face value before moving on is often the most fertile (and disorienting) source of new ideas.
Once again, Forth's low floor high ceiling strikes again. Hell yes!
What do you mean by &gt; it will fast forward through the same instructions as otherwise without executing Does that mean it will start to scan through code until it finds a catch? Seems like the scanner would have more general overhead than, say, a linked list of exception handlers embedded in the return stack.
It won't start anything; it will go on as usual, skipping anything that's not a catch; which means that it won't really scan that much code since no function calls are made and pretty much everything is a function call in Cixl. Like I said, it may be slightly slower when throwing; but I consider that a small price to pay.
Amazing! So cool work. I am realy impressed:-) 
Hey, Rudditavafasen, just a quick heads-up: **realy** is actually spelled **really**. You can remember it by **two ls**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
If you're saying "Forth written in HDL with routines that reconfigure and recompile it's own bitstream" then I think your question should be rephrased as "Is memory still a requirement for Forth?"
Return is indeed special, it simply closes the scope and ends the call. There are a couple of similar instructions that likewise need to clean things up on the way out but no user code but `catch:` is run, which means no loops or branches.
Ok. From Sweden, so english is not used everyday. Still so impressed by the work on Forth 1!!! Whish i could se it IRL. Almost went to the Forthday meeting in SF last year but my work got prioritized. (Did i manage to spell right this time?)
Isn't it a little mis-leading to call it "zero-overhead". ITC Forth spends 50% of the time running NEXT. You have added some code to NEXT therefore there is overhead. No?
I think it is at least a clever idea to put an interrupt check somewhere in the VM code. After reading this article I thought that you could do it probably more effectively during the execution of EXIT, since it would at least let certain definitions execute atomically.
This is quite impressive, it's good to see people interested in computing archeology. 
To me it was hard to write a metacompiler in the beginning, because many words exist in three versions: the host word, the metacompiling word, and the target word. I only got it at the third attempt, and then not very well. I made a fourth one which I felt better about. And then a number five which is a generic cross compiler. By then, it felt rather easy and natural. But it wasn't in the beginning.
Part of the power of Forth is that it gets out of your way; I've never struggled to do anything in Forth out of a lack of expressiveness, only "hitting a wall" when the task I was attempting was, itself, poorly defined. Overall, pretty badass work. Once you get a metacompiler running, porting Forth (or other languages if you are motivated enough) becomes trivial.
I had a different experience, but I haven't created a generic metacompiler, it's very much a 16-bit affair only. I found using the metacompiler to be more difficult as there can be a lot of subtle problems that are difficult to trace down, I would not want to use someone else's as I would find it too difficult to debug. This metacompiler has gone through many revisions, but they've all been incremental. It also helps that I can modify the Forth it works with. For example I made 'literal' into a vectored word so the metacompiler can cross compile literals, but it wasn't like that in the beginning. I did the same with strings. I wish I would have known more about the technique earlier on, for my [Forth CPU project](https://github.com/howerj/forth-cpu) I made a cross compiler in C for a pseudo-Forth like language, doing it all in Forth is much better. That might frame things better, metacompilation was easier for me than writing a traditional compiler in C. You're right that juggling the word-lists is the main difficulty, you're best off hiding the built in words as soon as you can and as much of the metacompilation wordset as possible. It's one of the main sources of errors. I certainly think more Forth programmers should learn about it.
Thanks! It's pretty satisfying getting everything working. I don't have problems with the expressive power of Forth too much either, but I think there is definitely a Forth-way of solving a problem and it can be quite difficult getting there, but once there the solution if not being more elegant will certainly be more compact at least.
Thanks! I just hope people find it useful.
 sp@ nth cell * + Basically, use the stack pointer to access the stack array-style. I don't think there is a better/different way to do this. 
thanks, but doesn't this involve pushing the index onto the stack? Is there a trick we could do using that in combination with immediate words or something to get around that? 
Yes, we must use the stack index to get the nth item on the stack. No, there's no tricks, unless you want an extreme loss of generality. Typically, when using a word that fetches the stack pointer, it returns the position of the stack previous to execution of the fetch, so you don't need to adjust your arithmetic at all.
The only way to avoid that is to write a word in assembler; most likely, a different one for each index back: e.g., "4-pick", etc.
Also, "4 pick" is not particularly inefficient; and an optimizing Forth will generate a minimal set of instructions so that '4' isn't actually pushed at all.
It is bad Forth style to treat the stack as if it were an array.
I forgot! Whoops!
It is a necessary evil. In the set of all functions computable, I doubt that simple stack shuffling effectively solves all but a (possibly large) minority. Sometimes concessions have to be made regardless, especially in consideration of certain metrics for speed: getting the item 5th from the top is easy, but that's a lot of moves for a comparatively simple calculation and memory read.
Ranks high on the retro cool list.
IMO *application-level* code shouldn't assume or rely upon array-like access to the stacks. A stack processor (real or virtual) wouldn't necessarily provide array-like access to an on-chip stack. Treating the stack as an array breaks the implicit locality of reference assumption of the stack machine model, at that point you're really coding to a register machine model rather than a stack machine model. A beginner in particular needs to learn to steer clear of register-machine thinking when learning to code in Forth.
I agree; I will amend my above comment.
 : pick 1+ cells sp@ + @ ;
In which Forth? Any approach to this will likely not be portable (apart from using something like `PICK`) and very much implementation dependent. E.g., in my Forth, there's not a good way to do this as the stack is not addressable as an array. It's possible to access any element, but not efficiently. (The MISC VM underlying it only provides a handful of instructions; none allow diving deeply into the stack, so reading arbitrary values involves moving values to/from memory, which is quite costly in time.)
Wow, this is really neat. Any idea how to run this?
I think it needs a couple more files with the initial dictionary: ;Load the file to bootstrap the forth dictionary. (let ((fet (status features))) (cond ((member 'mc fet) (load '|dsk:kle;fordic &gt;|)) ((member 'ai fet) (load '|ai:kle;fordic &gt;|)))) 
Wasn't there a post a bit ago about constructing Lisp in Forth too? Yo dawg, I heard you like programming.
Good catch!
Of course in most implementations there's an efficient way to index the stack. But while you might think that having that sort of primitive would optimize things, in reality it's just a way to half-fix what you broke. It's an optimization over a "decimization". You should understand why you ended up in this mess instead. You might think that you grok stacks a little, but in reality if you come from other languages you have habits or you have been taught to do things in certain ways that will be problematic in Forth. Like passing everything up and down as parameters (for instance, a file handle). The common programmer wisdom is that global variable are evil. Just like "goto considered harmful", this is repeated over and over but not all programmers read Dijkstra's letter and not all programmers can tell you why they are evil. In particular, in which context and which way of using them makes them "evil". Programming in Forth is the chance to question those religious beliefs. Be evil, break those rules and have fun. If a global variable is more convenient, use a global variable and see what happens. I have here a small convenience HTTP server written in my Forth dialect. Here I should mention that it is of the minimal variety, not one of those fancy Forth++ dialects. The listening socket handler is in a global variable. The client socket handler is in a global variable. I/O buffers are static too. Why? Because it's just a small HTTP server that listens on just one port and handles just one request at a time. It's not something that's meant to be used by Google or Facebook. It's good enough for what I want to do - that was #1 requirement. Using global variables made it a lot easier to write. If one day I need a server that handles concurrent connections, that's a whole different problem that requires a different approach - partly because that kind of server has to handle SSL and some form of CGI etc. Sure, passing handles and buffers up and down might make more parts more reusable, but I believe that changing "client-socket @" into "client-socket@" and have that actually fetch stuff from a client-connection structure might do the trick... I don't know. Why would I solve problems I don't have nor expect to have anyway? How would I evaluate the adequacy of a solution to a problem that doesn't exist yet? Code reuse is overrated. Doing *ruthlessly* the simplest thing that could possibly work saves you a lot of time. Enough to cover the cost of a total rewrite if necessary. Actually the experience you gain while writing stuff is way more valuable than the lines of code themselves. 
thank you so much
I use IMMGUI in :r4 [https://github.com/phreda4/reda4](https://github.com/phreda4/reda4)
[Using Glade to create GTK+ Applications with FORTH. [PDF]](http://www.complang.tuwien.ac.at/anton/euroforth/ef10/papers/mahlow.pdf)
No offense, but if you're someone who likes things done for you, then you might not enjoy FORTH very much. This is not said as a negative value judgement; I virtually never cook for myself, but eat out constantly. There is no McDonald's for FORTH, though; so if you want that, you'll be cooking.
&gt; It is a necessary evil. In the set of all functions computable, I doubt that simple stack shuffling effectively solves all but a (possibly large) minority. I tend to implement such functions as simple stack-based operations on appropriate structures in memory. This is clearer than pick and roll and doesn't require you to abuse the stack by treating it as an array. If you need an array, use an array. If you find you need applicative functions/function application or functions with local variables and/or lexical you can implement them too. As you well know, Forth provides a minimal computational model that you can use to implement the language or semantics you need. tl;dr Forth is a stack-based language, and I love the stack, but I've come to believe that as Forth programmers, we tend to overuse the stack.
I write a lot of Forth. Please don't try to extrapolate things about me from a very simple question
Thank you.
If I do end up using something like Pick, I switch to using locals, sparingly, and most often for large mathematical calculations. I implemented locals by putting them in the Return Stack (somewhat similar to C's stack frame), so they're out of the way. 
arent you stoped developed it for android platform?
for now, yes, if anybody like compile the code , I can send the last patches, my interest is in programming with: r4, not the interpreter, the android ide is a mess.
**Fluent interface** In software engineering, a fluent interface (as first coined by Eric Evans and Martin Fowler) is a method for designing object oriented APIs based extensively on method chaining with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
thx botbro
I'm back! Announcing my millionth stab at a game engine in Forth, and hopefully the last. This one is actually usable and designed for portability, yet very simple and straightforward. SwiftForth (Windows + Linux) only at the moment.
I have seen a lot of expert code doing just that, so I guess the practice is canon. Efficient it is not, but efficiency is overrated in programming. 
Depends on the implementation on whether it can be fast or not. Many processors will support a stack frame-like structure and indexing into it, such as x86, and it also doesn't do anything nasty with the cache.
Simplicity is a prerequisite for reliability. (Edsger W. Dijkstra)
&gt; Adding complexity to manage complexity is a losing proposition. (Jeff Fox) Well not according to the gods of Java :cough: :coughard:
The goal was very simple: to minimize the complexity of the hardware software combination. As far as I can see no-one else is doing that. (Charles H. Moore)
"There seems to be a propensity to make things complicated. People love to make them complicated. You can see that in television programming you can see it in products in the marketplace, you can see it in internet web sites. Simply presenting the information is not enough, you have got to make it engaging. I think there is perhaps an optimal level of complexity that the brain is designed to handle. If you make it to simple people are bored if you make it too complicated they are lost" 1xForth is a treasure trove.
There's always state, what differentiates different approaches is where it is kept. (Charles H. Moore)
https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub
Some issues: - First, this doesn't load unless I'm logged into LinkedIn (which I do seldomly). I've noticed this with LinkedIn before; while it may help you get some views, it'd be better to post a direct link as well so those not using LinkedIn can view the article. - Secondly, the LinkedIn is just an excerpt, the full article is at [https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub](https://docs.google.com/document/d/e/2PACX-1vSy-BL44KQbs9a7vjdeGw43PqHZy3wKTwzYEEhY6o7esokGZJwRKerEbUck7nusEWp-mavIwi4vYIPA/pub) Digging into the article a bit, you mention things that you never actually touch on. For instance: &gt; To explain the interesting properties of the web browser and the Unix terminal, we need to resort to something even older than Unix itself, and still surviving. I didn't see anything explaining interesting properties of web browsers and Unix terminals in the article. Also: &gt; No, it is not LISP. But it is also related to LISP. More later about the relationships between Forth and LISP. I didn't see anything else about this in the article. The rest of the article is really barebones. You present an example of using JavaScript in an RPN-ish fashion, but I don't see an example showing how this: document.body e: .childElementCount e: s: Is any better than: document.body.childElementCount I'd be more interested in seeing examples involving actual code. How do you define functions? Handle conditionals, loops, creating data structures? Have you developed any actual applications using this or written any documentation for it? Given that your language appears to be built as a sort of reverse polish layer over JavaScript (based on the limited article, and some things posted about it on comp.lang.forth), I'd be curious to know how it works alongside existing JavaScript libraries.
Forth is the only processor-independent language lets you think like a microprocessor. This is a skill that is necessary, though not sufficient, to evolve past cut &amp; paste kiddie koding. &gt; “A language that doesn't affect the way you think about programming is not worth knowing.” - Alan J. Perlis
This one's from John McCarthy ([who taught Chuck Moore](https://colorforth.github.io/bio.html) at one point): &gt; Program designers have a tendency to think of the users as idiots who need to be controlled. They should rather think of their program as a servant, whose master, the user, should be able to control it. If designers and programmers think about the apparent mental qualities that their programs will have, they'll create programs that are easier and pleasanter -- more humane -- to deal with.
[Chuck Moore...] Behind everything he does is a radical message: 'Embrace the entire problem, Keep it simple'. (Richard Morris)
Forth-iness is very intertwined with blockchain. I believe a dual-stack virtual machine would potentially be a perfect VM for blockchain applications (smart contracts), given their inherent simplicity.
New hardware is poised to disrupt supply chain management with blockchain, like this [10-cent computer from IBM](https://singularityhub.com/2018/03/26/ibms-new-computer-is-the-size-of-a-grain-of-salt-and-costs-less-than-10-cents/) What this and other IoT ideas will need is a tiny, efficient VM language that's good at machine-machine communication. I'm sure their labs are hard at work trying to contort Java to somehow fit.
There's an article that was posted a few months ago with nearly the same topic: https://www.reddit.com/r/Forth/comments/7kx0on/advanced_bitcoin_scripting/ I have no doubt that the person that invented the blockchain was very familiar with Forth.
So you want to replace the word compiler to asm to one that compiles to FPGA layout. Sure, yeah.You still need to implement that compiler-thing in either a cpu or the fpga, and have an interpreter, whether it's on a cpu or fpga. An FPGA can (and often does) simulate a small cpu like a microcontroller, however I think I agree that compiling to FPGA would be interesting. Now, I'm no FPGA expert, but it seems you'd have to have some sort of intelligent whole-program compilation in order to route words correctly together. Can you have FPGA sections like functions that are dynamically connected? I guess.
I got volksFORTH to work in Apple 1js, by pasting it into the Load box from the file f6502.hex in the latest zip in https://sourceforge.net/projects/volksforth/files/volksForth%20Apple1/ and then use 300R to run it.
A video showing FORTH on Apple 1js An Apple 1 Emulator in JavaScript &amp; towards FORTH on Apple-One Clone in Verilog FPGA https://youtu.be/VJMdmNWM_ZQ https://www.scullinsteel.com/apple1/#Volksforth http://volksforth.sourceforge.net/ https://scratch.mit.edu/projects/137676871/ https://en.wikipedia.org/wiki/Apple_I https://github.com/alangarf/apple-one https://usborne.com/browse-books/features/computer-and-coding-books/ https://drive.google.com/file/d/0Bxv0SsvibDMTcHNXalEtYkVtU00/view https://youtu.be/2yIBnhapzEU
**Apple I** Apple Computer 1, also known later as the Apple I, or Apple-1, is a desktop computer released by the Apple Computer Company (now Apple Inc.) in 1976. It was designed and hand-built by Steve Wozniak. Wozniak's friend Steve Jobs had the idea of selling the computer. The Apple I was Apple's first product, and to finance its creation, Jobs sold his only motorized means of transportation, a VW Microbus, for a few hundred dollars, and Steve Wozniak sold his HP-65 calculator for $500; however, Wozniak said that Jobs planned to use his bicycle if necessary. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
To answer the licensing issue: 8th's license pertains to the tool, not to the programs produced. So at the point when 8th can make tiny executables for MCUs, the license would still be a "purchase the 8th version you want, and make as many executables as you like, forever, without any restrictions or license fees". If you choose to never update or upgrade your 8th version, whatever version you do have will continue as before.
Probably the concept of a Rump kernel is interesting for you: [Rump Kernels](http://rumpkernel.org/) There can be build to my knowledge from NetBSD user land and as these operating system is known for it wide range of supported platforms this can be a good start for supporting embedded systems: [NetBSD Embedded Systems](http://www.netbsd.org/about/embed.html)
Thanks for that link, it does sound interesting in fact. 
You're not wrong, man...
Shit, his comments about IoT are prophetic. And the internet being a gated community turned out to somewhat true, especially with the costs associated with running a website, and end-users now dealing with horrendous website bloat (multi-megabyte pages). 
The former case.
What costs do you mean? You can rent your own small physical server for $5. If you need less than that, it costs even less.
For you and me, maybe sure, a website could be built on a whim. But for the lay man?
I was thinking of someone who had absolutely zero knowledge and didn't even know what terms to look up to get started. It is a huge investment of time, at the very least. That's all I wanted to say.
Yes, the modern tech stacks are all overcomplicated; if we didn't agree on that we probably wouldn't be here. :)
Check out jonesforth, it answers almost every question you've asked (jonesforth.S and jonesforth.f): https://github.com/AlexandreAbreu/jonesforth And DO doesn't *need* LOOP, I think gforth is syntax checking that on your behalf. DO does need a start and limit though.
&gt; And then there's a return stack, where the next word to execute in a definition is stored, or something? "Something". The return stack is the call stack, where the return address is pushed before calling a word. The example you use is not good, because '+' is typically a primitive word. Rather, work from these examples: : square dup * ; : cube dup square * ; In cube when calling square, the return address is pushed. It is popped back to the instruction pointer when executing the semicolon. &gt; CREATE takes the next token from the input stream Yes, there are a few special words that do this kind of things. like ":" "'" or "TO". &gt; How do they (words like CREATE) work under the hood? It depends on the implementation. &gt; Can I make my own words that work like this, without defining them in terms of CREATE? I haven't read the ANSForth standard for a while, but I don't think it's possible. &gt; What is the difference under the hood between ' and ['] Basically ['] is the "immediate" version of '. &gt; And related to this, what about words like ." that scans the following characters instead of just doing something with the next token? How do they work? Implementation-dependent. Doing is seeing. Just try to implement a Forth-like language in your language of choice. Or read the source of some simple implementation (the sidebar recommends Jonesforth, but a C implementation should be easy to find; implementations other languages such as Haskell, Lisp, Python are often too toy-ish to be useful for you or anyone else actually). &gt; Are there any words that a user of Forth could not define himself, in principle? Of course. I would add that there are an infinite number of them. &gt; Is there like a syntax checker, or does it work some other way, by for example DO leaving some state that LOOP later uses? Basically GForth detects that something is left on the stack when the definition ends. It can do that because a "control-flow stack" is used to resolve the implicit forward references made by some control-flow words. See for instance [IF](https://www.taygeta.com/forth/dpans6.htm#6.1.1700) &gt; I don't understand exactly what DOES&gt; does. Should it be used only after CREATE? It defines what the CREATEd word does. In simple implementations, CREATEd words by default pop the return stack, thus leaving on the data stack (at runtime) the dictionary address just after the word. DOES&gt; allows you to make it do extra things. CREATE DOES&gt; are a bit tricky because they mix compile-time and run-time actions. &gt; Also, I don't understand how immediate works. It does set a flag in the header of a dictionary entry, right? But how does it find the place? It just sets the flag for the latest definition. &gt; HERE now points to the end of the dictionary entry, doesn't it? "dictionary" is an ambiguous term. It can refer to the list of words, or it can refer to the space where you can allocate data (with ALLOT or the comma). It's ambiguous because in many historic implementations, the dictionary (of words) is a linked list stored in the dictionary (of cells). It might even have been mandatory in the previous standard (Forth84). A current standard makes more distinctions (see [the standard](https://www.taygeta.com/forth/dpans3.htm#3.3) ). That's what I do in my dialect for instance. The dictionary (list of word) is just an array of structures in the C program, and I compile words and ALLOT in an other array that I called "workspace" in my C source precisely to avoid that kind of ambiguity. &gt; And what is an "execution token" exactly? A Beginner's Guide to Forth says that it is "often an address, but not necessarily". Hm, OK. "execution token" is ANSForth lingo for a thing that you can EXECUTE or COMPILE. Think of it as a function pointer; or think of it as: "'" returns something of the "executable" type, EXECUTE only accepts something of the "executable" type. 
[jonesforth](https://github.com/AlexandreAbreu/jonesforth) is a good start to understand these things if you know assembly. There's words like `PARSE` that can read the next word or anything you wish. Words that use these kinds of words are usually immediate so that they can actually run `PARSE`, well, immediately :) Words like `DO`, `LOOP`, `IF`, `THEN` etc are often implemented as immediate words that leave a value on the data stack (like `IF`) for the next word to pick it up and compile it into an e.g. jump (like `THEN`). How is GForth exactly checking for an unstructured word I don't know. If the standard doesn't require these checks one could in theory omit them and say it's undefined behavior when used like that. The xt (execution token) is the address of the word's definition that you can invoke directly with `EXECUTE`. When `CREATE` is creating a dictionary entry it writes down some information about the word, it's name, name length, xt and maybe some other metadata, and a pointer to the next/previous word. So when the forth interpreter sees a word and looks for its definition you can imagine a word like `FIND` looking for a name match, `XT` (I don't know if this word really exists) to fetch the address from the entry and `EXECUTE` to finally run it. A wordlist is a list of words :) It's similar to a package, module, namespace in other languages. If you want to understand the internals read jonesforth. For a non-ANS take freeforth is fun too :)
&gt; It is popped back to the instruction pointer when executing the semicolon. Maybe that's pedantic in this context, but I don't think this is completely accurate, as the semicolon must be an immediate word (since it has an immediate action, namely switching from compile mode to interpret mode). So the semicolon word itself isn't actually executed when the definition is executed.
That's a good point too, about English and non-native speakers. And yes, "cost" in English doesn't just apply to money. It broadly applies to resources, regardless of what those resources might be.
Fixed: https://github.com/PDP-10/its-vault/tree/master/files/kle
Another good resource is the Jupiter Ace manual, which is for a home computer from the early 1980s. Its version of Forth is a little idiosyncratic but it does explain everything in gradually increasing levels of complexity. 
Thank you! I'm looking forward to studying this in the future :)
Check these guides about forth \(FlashForth\) on certain microcontrollers. [http://flashforth.com/ff5\-elements.pdf](http://flashforth.com/ff5-elements.pdf) [http://flashforth.com/ff5\-tutorial\-guide.pdf](http://flashforth.com/ff5-tutorial-guide.pdf)
You're welcome to try to run it. If you don't have a Maclisp handy, I can help with that. :-)
The cost of some beer for someone suitably skilled, and a few quid a month for hosting.
Very well, let me put it this way. How many on-ramps will connet the world’s ghettos to the Information Superhighway?
If you've got absolutely zero knowledge in any subject there's got to be a massive investment in time, money and effort to do anything. That's like saying "playing the guitar is a gated community" because you can't just pick it up and be Carlos Santana.
That's hardly a reasonable comparison. Guitar can be reasonably mastered with some effort. I don't expect anyone to simply run a website with an "Internet for Dummies" book.
It seems VolksFORTH is 16 KB in size which is loaded into RAM, and then it requires some RAM to run. Apple I for FPGA currently only has 8 KB RAM. Apple 1js probably has 32 KB RAM.
What, specifically, is so hard about running a website? Anyone that's basically functionally literate can do it!
I'm constantly taken aback at how even experienced forthwrights who should know better blithely disregard the most basic precept of the Forth approach: &gt;Forth is highly factored code. (Charles H. Moore)
Starting MMSFORTH v2.0 Forth programming language on a TRS-80 Model 1 emulator running on DuinoMite w/ PIC32, and running some arcade games (Breakout &amp; Tron) and board games (Othello/Reversi &amp; Tic-tac-toe). Source code for each FORTH program is available on the diskette. https://archive.org/details/MMSForth_v2.1_1982_Miller_Microcomputer_Services https://hackaday.io/project/9077-trs-80-model-1-on-a-pic32 https://www.olimex.com/Products/Duino/Duinomite/DUINOMITE-IO/ https://www.electrokit.com/produkt/duinomite-io/ https://www.olimex.com/Products/Duino/Duinomite/DUINOMITE/ https://www.electrokit.com/produkt/duinomite/ Tron (1982): https://www.imdb.com/title/tt0084827/ Another video showing the hardware: https://youtu.be/b3IxyqHwq80
In this case, I'd probably just use a variable: variable key : find-key ( list -- value|false ) key ! foreach key @ str= if exit then drop next false ; " foo" find-key " bar" find-key 
Well actually, in 8th you *can* use "curry" to accomplish this. Regarding postpone etc, yes: it's ugly... but it's also extremely powerful when used judiciously.
Oh, that's odd! "help curry" doesn't work, but "help curry:" gives: ns: curry name: stack: x w1 -- w2 IMMEDIATE desc: Given a word "w1" which takes N parameters, returns a new anonymous word "w2" which takes N-1 parameters, where the first (TOS) parameter is "x". ns: curry name: stack: x w1 &lt;name&gt; -- w2 IMMEDIATE desc: Same as "G:curry", but creates a named word "w2". There is also samples/misc/curry.8th
The 'ns:' is wrong, which is the problem w/ the help. It should be "G:", and in fact "curry" and "curry:" are both in "G:....
The corrected help reads: ok&gt; help curry ns: G name: curry stack: x w1 -- w2 IMMEDIATE desc: Given a word "w1" which takes N parameters, returns a new anonymous word "w2" which takes N-1 parameters, where the first (TOS) parameter is "x". ok&gt; help curry: ns: G name: curry: stack: x w1 &lt;name&gt; -- w2 IMMEDIATE desc: Same as "G:curry", but creates a named word "w2".
The problem is that I'm solving a somewhat abstract problem here, which is not what one usually does in Forth. I'm implementing a JSON library to allow easy communication with web frontends. So I don't know the size of objects in advance. My looping construct does indeed make the return stack unusable. This probably isn't necessary, I could keep the current position on the data stack. However, it's consistent with my JSON-array iterator loop, which keeps the data stack free, which is nice for calculating sums and such. But maybe consistency is hurting more than it helps here... I'm aware of the performance implications of lists - I use them here for convenience. And I have a cute way to construct them with O(1) insertion at the end. I don't think this kind of meta-code is hard to write, since I can easily transform a non-meta version of it into the meta-version. Regarding maintainability... you may well be right, though.
True. In my Forth, I'd hide the variable from the global namespace. I don't think there's an actual standard way to do this though. Or, depending on how `foreach` is written, just use `here`: : find-key ( list -- value|false ) here ! foreach here @ str= if exit then drop next false ; 
Using `here` is clever. I usually don't use `here` for temporary data, but maybe I should become more flexible about that. And I actually do have a mechanism for making words local in my Forth.
This sounds fine, and even perhaps good advice, if there are a finite set of values you know in advance and are prepared \(or have to\) type in yourself. But how does this help Wolfgang, who is implementing some kind of JSON thingy to communicate with Web stuff? He doesn't know the size of his objects in advance, so presumably he can't know the values, either. He hasn't said what exact role his program is playing, but I can't see how this technique could be useful to him. His program may well be consuming data from sources outside his control. Even in the example you give, I am wondering about the practicality. If you only have a small handful of star trek episodes in your media library, then probably the release number is of little value to you, and even if it is you hardly need a program to manage it. If, however, you have all of Star Trek, or even just a couple of series then I'm not going to be typing hundreds of definitions to map this information — I'm going to be trying to find this recorded somewhere else already, and slurp it in somehow. I suppose you could write something in Forth that parses this in some kind of tabular format, produces a forth source file containing all the definitions, and then INCLUDE it... is this what you would recommend? Honest question — it seems like a bit of a kludge to me, but perhaps it is in fact the easiest thing to do. In a language that provides some kind of key\-value mapping structure out of the box, I'd certainly rather use that then start exporting files only to import them and execute their contents. Forth has its dictionary, so it's good advice to use that. But AFAIK doesn't provide any really convenient way to programatically add entries \(at least, not in the ANS standard\). I have used Gforth's `execute-parsing` to do this before, however if there's a better way, I'd sure like to know... 
&gt; I suppose you could write something in Forth that parses this in some kind of tabular format, produces a forth source file containing all the definitions, and then INCLUDE it... is this what you would recommend? Honest question — it seems like a bit of a kludge to me, but perhaps it is in fact the easiest thing to do. My suggestion here will be considered heretical, but GNU FORTH contains the ability to execute pre-written FORTH scripts; which are added to the dictionary of words at runtime. For Star Trek episodes, there are probably any number of textual episode lists around or IDF database entries. Then I need to find out what number range, within the overall total, that Voyager as my example occupies. The start of my loop is the lowest number in that range, and the end is the highest. A text file contains the list of Voyager episode names, and for every number in my loop range, I have ed extract and delete (pop, essentially) the first line in said file. Commands.ed:- 1p 1d wq My main script:- 1 169 #!/bin/sh -x index=169 count=1 main () { fn=$(ed star-trek-filenames.txt &lt; commands.ed | sed -n '2p') filename=$fn echo -e "\: $filename $count \;" &gt;&gt; star-trek-records.forth count=$(expr $count + 1) next } next () { t2=$(expr $index - $count) temp=$t2 [ $temp -eq 0 ] &amp;&amp; ZF=1 [ ! $temp -eq 0 ] &amp;&amp; ZF=0 [ $ZF -eq 1 ] &amp;&amp; exit 0 [ $ZF -eq 0 ] &amp;&amp; main } next I'm stoned and I am probably forgetting something, but hopefully you get the idea. The above is a shell script counter with a loop, that assigns a number from said counter to each one of the Voyager file names, as they are pulled out of the text file by ed. Said name and its' associated number are then formatted into a word to be compiled, within a GNU FORTH script file. From there, I could rename my files simply as numbers, and from within the Voyager subdirectory, run a GNU FORTH script calling mplayer with the system word. This might seem pointless and excessive work, but again, the point is that I can either type in the number or the file name myself, and only the number still ends up on the stack; the conversion is performed automatically.
This is what I do in order to avoid the shuffle, but I guess I can do one better in my forthoid, because I have local CONSTANTS: : find-key ( list -- value|false ) qonst key foreach key str= if exit then drop next false ; The operative word here is 'qonst' which takes from the stack and binds the value to the name following 'qonst'. (In my forth, you have named parameters, too, which are also constants. In the example above, you could access the parameter 'list' with that name, even if that name is already taken outside the scope of the colon definition, because those qonst-constants are local.) Constant data is best data. 
This is very impressive performance. gForth is a relatively fast version of Forth if you use gForth-Fast. Is that the version you used for testing? I know nothing about WebAssembly. I need to do some studying. It appears to run native code to achieve those results. (?)
Aha, I didn't know about gforth-fast. Thanks a lot for pointing that out, that's indeed much fairer! I wasn't sure what to expect from gforth, but if it's relatively fast, then 'very impressive' would have surprised me a lot. Turns out my implementation is about 2.5 times slower.
Looks pretty neat. Took me a sec to realize it's case sensitive. It seems like some errors put the system into a broken state. E.g., since I used lower\-case `cr`, everything breaks thereafter: WAForth 1 2 + . 3 ok : test ." Hello" cr ; error 1 2 + . error 
I've tried using `dc`, but I prefer [Emacs' Calc Mode.](http://nullprogram.com/blog/2009/06/23/) It's also RPN, but it's been turbo-charged into a CAS.
That's interesting. I hadn't messed with `dc` in many, many years. Thanks for the post!
Have you checked the info pages? 5.5.6 "Floating Point" includes mention of `f**` with stack effect `r1 r2 -- r3` and description `r3 is r1 raised to the r2th power`. (I'm assuming you were looking for a floating point version. An integer version would be significantly easier to hand-roll, if you're okay with a naive version. There's probably something about it *somewhere* in the gforth code, though)
Thanks for this, idk why I didn't see that
Except it isn't at all. I've never seen anyone ever speak specifics with block chains. Why? Because they're inefficient, extremely niche, and traditional solutions almost always do it better. They're next to useless at scale due to a slew of massive inefficiencies. 
* inefficient (depends \- Proof of Work is stupid; Proof of Stake is much better) * extremely niche (yes, so what's the problem? Forth is the embodiment of niche.) * don't scale (addressed by Ethereum's Layer 2) Ethereum uses a stack\-based virtual machine (EVM). One of the best languages for coding in EVM is Low Level Lisp (LLL). It feels a lot like Forth. Blockchains have evolved considerably since Bitcoin's invention.
And how does any of that create anything that a business needs that isn't filled better by already existing tech? "Its a decentralized***** public ledger! It'll change the world/disrupt industry/make us free!" There's no logical connection between those two ideas. Hell a decentralized***** public ledger isn't even a good idea in 99% of cases. Ethereum's Layer 2 isn't even a concrete thing, its an abstract idea with a set of concrete implementations. You're building a tower of next-to-useless abstractions that benefit no one and burn electricity like nothing else for no reason whatsoever. None of that is forth-like. "feels like forth" my ass. You don't even know what forth means other than "stack machine"
&gt; You don't even know what forth means other than "stack machine" Do you have the faintest ********** idea of who I am ??
A person too afraid to type a word out and entirely swallowed by the silicon valley bubble. In other words: a complete tool.
I'm in about the same place in my understanding. Here are some of the links I'm keeping around to read. Apologies for the disorganized parts of this linkdump. good, detailed intro to using classic Forth http://galileo.phys.virginia.edu/classes/551.jvn.fall01/primer.htm ground-up explanation of the machine code Forth generates and how it gets there https://github.com/AlexandreAbreu/jonesforth/blob/master/jonesforth.S https://github.com/AlexandreAbreu/jonesforth/blob/master/jonesforth.f FIRST &amp; THIRD (almost FORTH): walkthrough of a tiny language FIRST sufficient to create a small language THIRD which strongly resembles FORTH. Probably more intelligible after reading Jones Forth. http://ftp.funet.fi/pub/doc/IOCCC/1992/buzzard.2.design Punyforth: a Forth for the (nifty) ESP8266 microcontroller. Reading the core is a good follow-up to reading the THIRD code. Has quotations, combinators, case statement, deferred definitions, unit testing, exception handling, TCP/IP (the chip has WiFi), and experimental cooperative multitasking. Really quite impressive. https://github.com/zeroflag/punyforth/blob/master/generic/forth/core.forth to play with the controller, consider https://www.adafruit.com/product/2471 Trying to understand create ... does&gt;, starting with an implementation: https://github.com/philburk/pforth/blob/master/fth/system.fth right into the guts: "under the hood" from Learning Forth https://www.forth.com/starting-forth/9-forth-execution/ https://www.forth.com/starting-forth/11-forth-compiler-defining-words/#How_to_Define_a_Defining_Word http://www.vintagecomputer.net/fjkraan/comp/atom/doc/ForthTheory&amp;Practice_10-index.pdf https://users.ece.cmu.edu/~koopman/forth/hopl.html http://mysite.du.edu/~etuttle/math/forth.htm http://www.public.iastate.edu/~forth/gforth_40.html#SEC47 http://www.ultratechnology.com/meta.html http://yosefk.com/blog/my-history-with-forth-stack-machines.html http://www.forth.org/svfig/Len/definwds.htm https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/CREATE_002e_002eDOES_003e-details.html http://galileo.phys.virginia.edu/classes/551.jvn.fall01/primer.htm#create http://www.mostlymaths.net/2010/03/forths-create-does-maybe-im-amazed.html https://www.forth.com/starting-forth/11-forth-compiler-defining-words/ https://www.taygeta.com/forth/dpans6.htm#6.1.1000 http://www.bradrodriguez.com/papers/moving3.htm http://forum.6502.org/viewtopic.php?f=9&amp;t=3153 https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/User_002ddefined-Defining-Words.html#User_002ddefined-Defining-Words http://softwareengineering.stackexchange.com/questions/339283/forth-how-do-create-and-does-work-exactly Stone Knife Forth. "It is not expected to be useful; instead, its purpose is to show how simple a compiler can be. The compiler is a bit under two pages of code when the comments are removed." https://github.com/kragen/stoneknifeforth "In this file a meta-compiler (or a cross compiler written in [Forth][]) is described and implemented, and after that a working Forth interpreter is both described and implemented. This new interpreter can be used in turn to meta-compile the original program, ad infinitum. The design decisions and the philosophy behind Forth and the system will also be elucidated." https://www.reddit.com/r/Forth/comments/8e4j57/metacompiler_howto_and_small_eforth_interpreter/?st=jh2act4g&amp;sh=224f50ab A forth in MacLisp, dating to 1978. The comments are good reading if you want to know about forth implementations. https://github.com/PDP-10/its-vault/blob/master/files/kle/forth.396 Moving Forth: "Everyone in the Forth community talks about how easy it is to port Forth to a new CPU. But like many 'easy' and 'obvious' tasks, not much is written on how to do it!" http://www.bradrodriguez.com/papers/moving1.htm Also: Build your own Forth -- details of various implementations. The Heart of Forth http://www.figuk.plus.com/build/heart.htm more on implementations http://www.bradrodriguez.com/papers/tcjassem.txt a (very) few notes on learning to metacompile (cross-compile) Forth http://www.lowfatcomputing.org/23/ At bottom of first link, setup instructions for a very cheap small playground, STM32F103 microcontroller with Mecrisp-Stellaris Forth. "The combination of Mecrisp-Stellaris with the JeeLabs libraries is a tremendous Forth ecosystem for a whole bunch of ARM microcontrollers, and it has all been developed within the last two years by a small core of thoughtful hackers. Combining the two provides a very pleasant Forth microcontroller hacking experience, like a weird interactive Arduino." No FPU, no DAC, but whatever. NB: The STM32 is not the ESP32 in any way. Don't get mixed up. http://hackaday.com/2017/01/27/forth-the-hackers-language/ getting started: flashing the chip, hello world, blink an LED, and multitasking (!) http://hackaday.com/2017/04/19/moving-forth-with-mecrisp-stellaris-and-embello/ http://jeelabs.org/article/1608d/ 
I’m not sure where the example ``[ [ "data" [ 2 1 ] \numbers child root`` falls down when evaluated in the opposite order. (Note: I’m using backslash instead of backtick for attributes to avoid the formatting problems in the article.) In “stack order” it would be evaluated like this: MARK ( -- mark ) STR ( str -- val ) NUM ( str -- val ) ANON ( mark ... -- node ) ATTR ( node name -- node ) NODE ( mark ... name -- node ) MARK \ mark MARK \ mark mark "data" STR \ mark mark "data" MARK \ mark mark "data" mark "2" NUM \ mark mark "data" mark 2 "1" NUM \ mark mark "data" mark 2 1 ANON \ mark mark "data" node "numbers" ATTR \ mark mark "data" attr "child" NODE \ mark node "root" NODE \ node In “normal order” ``root child \numbers [ 1 2 ] "data" ] ]`` it could be evaluated like this, where the box denotes the “cursor” pointer: EMPTY "root" NODE "child" NODE "numbers" ATTR ANON 1 NUM 2 NUM UP "data" STR UP UP Broken down: * Start with an empty document: EMPTY □ * `root` = Allocate node `root` and enter it: "root" NODE ( node -- node ) &lt;root&gt;□&lt;/root&gt; * `child` = Allocate node `child` under `root` and enter it: "child" NODE &lt;root&gt; &lt;child&gt;□&lt;/child&gt; &lt;/root&gt; * ``\numbers`` = Allocate attribute `numbers` under `child` and enter it: "numbers" ATTR &lt;root&gt; &lt;child numbers="□"&gt; &lt;/child&gt; &lt;/root&gt; * `[` = Allocate anonymous node under `numbers` and enter it: ANON &lt;root&gt; &lt;child numbers="&lt;&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `1` = Push `1` into anonymous node: "1" NUM &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `2` = Push `2` into anonymous node: "2" NUM &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;□&lt;/&gt;"&gt; &lt;/child&gt; &lt;/root&gt; * `]` = Leave anonymous node; **plus** current attribute `numbers` is full, so leave attribute too: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;□&lt;/&gt;"&gt; □ &lt;/child&gt; &lt;/root&gt; * `"data"` = Push `data` into `child`: "data" STR &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; □ &lt;/child&gt; &lt;/root&gt; * `]` = Leave `child`: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; &lt;/child&gt; □ &lt;/root&gt; * `]` = Leave `root`: UP &lt;root&gt; &lt;child numbers="&lt;&gt;&lt;number&gt;1&lt;/number&gt;&lt;number&gt;2&lt;/number&gt;&lt;/&gt;"&gt; &lt;string&gt;data&lt;/string&gt; &lt;/child&gt; &lt;/root&gt; □ Either you keep a stack of values and allocate nodes incrementally, or you keep a stack of node pointers and add values to them incrementally. Also, you *must* differentiate “nodes” from “attributes” in either method, which it’s not entirely clear that he does, due to the bad formatting in the article.
Factor has Gtk bindings: http://factorcode.org/
Unlike languages that give you really abstract string api, forth is lower level too start with. In principle, think about allocating a new buffer that is as long as the sum of the two strings, and copy them into it. You have some more interesting options to keep the code svelte, simple, and avoid allocations, but I'll leave it to the forth wizards to comment. There are places like the pad that might work for constructing it, or at the end of the dictionary if it's provably safe, etc. 
(I initially wrote a long post breaking down appending, but then realized `+place` was already defined in gforth and did most of what you want) Simplest solution would, in my mind, be: pad dup 8 test c@ - accept test +place There's some unnecessary copying happening here (you could just tell `accept` to place it directly at the end of the string), but then you'd have to reimplement most of `+place` yourself. Defining a general word for doing this (unnecessary, and mostly for style points), you run into a problem: the buffer size is hardcoded. You could get around this by doing something like: : buffer dup , create allot ; : #buffer cell - @ ; : +accept &gt;r pad dup r@ #buffer r@ c@ - accept r&gt; +place ; 8 buffer test s" ls " test place test +accept 
 s" ls " 2constant command command 3 - swap 3 + 2constant switch-buffer switch-buffer accept 3 + command drop swap system 
Does this help at all? : test s" ls " pad place bl ['] parse execute pad +place pad count system ; 
Give to 'accept' the address of your buffer + 3 chars A bit overdone, but that's what I would do: `8 constant #ls-max` `create 'ls-cmd #ls-max allot` `s" ls "` `dup constant #ls` `'ls-cmd swap move` `'ls-cmd #ls + constant 'ls-options` `variable ls-len` `: get-option 'ls-options #ls-max #ls - accept #ls + ls-len ! ;` `get-option` `cr 'ls-cmd ls-len @ system`
Thank you! Exactly what I was looking for and I have some material to study here.
Thank you. I will look into this and put it in my knowledge base.
Yes, but not for this application. You jogged by my memory and taught me something new.
Thank you. I will look into this further.
I took on a task to try and rehabilitate BASIC programmers. :-) Here is an BASIC type input statement that uses ACCEPT: https://github.com/bfox9900/CAMEL99-V2/blob/master/LIB.TI/INPUT.FTH Also the work of the late Neil Baud (aka Wil Baden) has some excellent tools for this kind of thing. Here is one of his words that appends a character to a counted string in memory. : APPEND-CHAR ( char caddr -- ) \ add a ascii char onto the end of a string DUP &gt;R COUNT DUP 1+ R&gt; C! + C! ; Usage example: S" Missing a period" PAD PLACE CHAR . PAD APPEND-CHAR 
Nothing here? Was this removed?
That confused me too. I assume he didn't want to actually create the nodes until all the terms were on the stack. Would that explain it? 
The link works fine for me on my laptop, though I had to manually download it with the latest youtube-dl on my desktop (strange, as they're both using icecat).
Wow. Emacs really is the text editor which just keeps on giving. 
I agree. Emacs actually ties a lot of the Unix philosophy together by providing many useful utilities such as this in one environment with consistent cooperation between these utilities. It eliminates the *de facto* separation that exists in Unix, where everything focuses on its own task, but there is nothing enforcing them to all work together well. There's a great article which expounds on this topic [here.](https://www.quora.com/Does-Emacs-violate-the-UNIX-philosophy-of-doing-one-thing-very-well/answer/Tikhon-Jelvis)
Unless you speak fluently lex/yacc, yes it is not a good idea. It will just be a distraction. The more straightforward way is to just use something like fscanf(), and even that is probably overkill.
hey, thanks!
in [cforth](https://sourceforge.net/p/cforth/code/HEAD/tree/) lex/yacc used to generate core image file.
great, I'll have a look at it.
Yes, this is a terrible idea. I would go so far as to say that autogenerated code is antithetical to the entire Forth philosophy. yacc is completely unnecessary, since you're not generating a parse tree I'm not even sure what you would do with it. A single production that just executes the word? But then how does `WORD` work to suck up the next token in the parse stream? I guess if you really wanted to, you could bend over backwards to use lex as the tokenizer, but because numbers can be `:` defined in the dictionary, you would basically have to have only one lexical token anyway. Just write `WORD`, it's bound to be easier than wrangling even the makefile/link. If you need help send me a note and maybe we can pair on it. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programminglanguages] [okami 0.1](https://www.reddit.com/r/ProgrammingLanguages/comments/8tzxav/okami_01/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
named after the video game? as someone who hasn't used forth very much, I appreciate how easy your [] system is to understand.
Named after the japanese word for "wolf". (Hint: look at my first name / username). But I'm aware of the beautiful game with the same name. And thank you for your positive feedback.
I _could_ have spent 10 seconds googling, but I decided to ask a dumb question instead. thanks for humoring me. I see you implemented it in assembly. Did you base your implementation off of anything? JonesForth?
The lack of spacing between the words and the brackets bugs me as a Forther. I hope that isn't an intentional feature of the parser.
It's part of the parser...
Similar, though I made ( designate interpreting instead: [https://github.com/darius/tusl/blob/master/eg/babble.ts](https://github.com/darius/tusl/blob/master/eg/babble.ts)
amazing work, i'm quite glad of someone now noticed web domain :D
Think of it as a special kind of whitespace (so special it isn't even completely white, haha). It's convenient to have it this way. But if `" this"` additional space seems right to you in Forth, I guess I have no chance to convince you. :)
When I wasn't sure about how to implement some feature, I looked at other implementations, including JonesForth. I think this happened two times. But I didn't want to inherit the limitations of JonesForth, like not having `does`, so I didn't want to copy it.
It's not what seems right or wrong; if the parser looks for the left and right bracket separately, the how would it distinguish between `[` and `[[` if I were to define double-bracket?
Have you had a look at the Postscript programming language? The way you use square brackets here reminds me of how Postscript uses curly braces. 
It looks syntactically very similar indeed, but I don't think there is a semantic relation.
You cannot define `[[`... just like you cannot define a word with whitespace in it. The brackets are never a part of any word. You are limited to `{}` and `()` if you need parentheses in word names.
http://web.archive.org/web/20171201212822/http://dloh.org/
I should have mentioned that most of the articles are missing on archive.org.
http://web.archive.org/web/20171201212822/http://dloh.org/ - none of the links work, but his e-mail is there. Also you could try contacting here: https://github.com/cthulhuology or just search for "Dave Goehrig" or "Dave J Goehrig".
https://twitter.com/lowfatcomputing/status/1013791717630173184 We'll see what he says.
I know, sorry... I realised after posting that.
WORD
Wow. That seems quite reasonable. You pay some money for a tool one time and have zero annual fees or usage fees or deployment fees. If I did more with Forth I would be tempted to try it out as I don't at all mind paying for good software that doesn't twist my arm.
Thank you. To be perfectly clear about that, for the purchase price you get one year of updates; after that, if you want updates, you pay an 'update renewal' which is a fraction of the cost of a new version. But you don't have to do that if you don't want to.
I don't see a problem with counting up the references for a specific word (and branches thereof), assuming that running the word executes your application. Just count out how many times each primitive and composite is reached during execution (modify NEXT for the primitives, DOCOL for the composites).
I think you just restated the halting problem, which is what u/reepca was referring to.
Okami HD: Hype! \^\_\^
It can also translate as "great god," that said, it can be really hard trying to verify japanese translations, so I may be wrong.
Warning: pessimism incoming. It's target compilation... in the general case, you can't execute it on the host machine (and compiling and executing on the host machine to try to figure out which words are used rules out the possibility of choosing which words are used based on the target). You also have to run every single branch (I assume that's what you meant by "and branches thereof") in order to see everything that could be used. That seems nontrivial - a naive implementation would scale exponentially with the number of branches, and don't get me started on what happens when computed jumps and calls through tables get used - in short, you need to produce every possible input for the program. Even supposing you make that all work, now you have an answer to "does this xt get used" for every xt. You somehow need to translate that to a chunk of memory or similar structure that contains only those xts - or rather, the code represented by those xts, because the addresses will need to be different. Even copying colon definitions is tricky - much as we like to pretend that it's just a nice "list of xts", there's actually inline data strewn all over the place - string and number literals, for example. In the general case, you can't know whether a number literal is an address that needs to be replaced or not. It's hard to even tell where a colon definition ends, since `EXIT` can be manually placed anywhere. Additional bookkeeping would be needed to know where the "semicolon" actually is. And then there's `CREATE`d words... How would you propose this be made to work?
You could try a minimal implementation for the target machine and compile based on dependencies, but that would also hinge on words that use literals to be re-coded towards such a consideration. Whatever you end up doing, you'll most likely need to build a dependency tree. Or with the minimal implementation on the target machine, simply compile code on the target itself. People usually do exactly this because it is the path of least resistance.
Forth coding is all about customization to fit your needs, and cutting corners to a ludicrous degree. Or so I read somewhere.
&gt;There is no way to manipulate or build up strings other than printing them. It may be possible to build some string functions: `$ dc -e '16i 0A21444C524F57204F4C4C4548 [100 ~ dP0&lt;F]dsFx'` `HELLO WORLD!` Not having to think in reversed hex notation of the string would be nicer but chiseling the print string example would have taken much longer...
Ugh, I don't know why the SO overlords are so haughty. They'll let plenty of questions slide, which are *far* worse than this, only because they *like* those questions better. Ask whatever you want about the newest fad, even if it *is* opinion-based, but don't you dare ask about what you're interested in! It's just too darn painful for us to read about your interests which don't align with ours.
\`resolve-all-forward-references\`
&gt;Nice, I use \`converge\` for that purpose.
Seems like the long names in okami are all network related: ``` reset-(/sockaddr_in6) config:remote-access setsockopt(syscall) parse-request-line content-length-hdr ``` 
It's difficult to avoid long names in network programming.
Some statistics from Retro on iOS: Average name length: 8 Average name without namespace: 5 Longest names are 23 characters The longest words are part of the interface configuration: 23 config:restore-defaults 23 config:set-toolbar-size 23 config:set-toolbar-font 22 config:set-output-size 22 config:set-editor-size 22 config:set-output-font 22 config:set-editor-font There are also a few long names in other vocabularies: 22 file:open&lt;for-writing&gt; 22 file:open&lt;for-reading&gt; 21 file:open&lt;for-append&gt; 20 n:strictly-positive? 20 s:tokenize-on-string 20 s:tokenize-on-string 20 set:contains-string? 
 long time no see a small and quite community, aren't we delete-on-same-piece get-word-and-advance last-char-and-length next-dictionary-word print-all-test-words advance-till-new-line global-char-separator log-global-dictionary scnd-dictionary-stack temp-dictionary-stack write-chain-to-handle decode-too-big-for-pad encode-too-big-for-pad load-symbols-from-file find-word-in-dictionary how-much-left-after-fit run-till-current-return find-word-in-dictionary compare-stacks-and-clean end-of-global-dictionary create-word-in-dictionary regulate-dictionary-stack complie-call-to-test-words delete-on-different-pieces store-link-to-previous-word search-address-in-dictionary stash-stack-till-saved-depth find-word-in-dictionary-stack update-dictionary-with-new-word get-color-according-to-highlight complie-reverse-call-to-test-words 
&gt; I make things type void***** to make the compiler more compliant with my demands. This confuses me. Can you explain what you're talking about? It was the first thing that stood out to me in the code.
Wow, full-blown sentences as names.
Well there is a simple reason for this. First of all the stack I use can hold any type and so is untyped, so naturally it needs to be able to hold a type as big as the biggest type I am required to have by the function of the c language. It turns out this seems to be void*. Int* or char* does not work apparently because on some systems a int* or a char* is a smaller number than a char*. And so some people including myself often tout void*. But I actually use void***** and here's why. Lets say I am calling one of my named variables of type void*. Then I try to read from index 0 of it. Then I have to type ((void**) my_data)[0]; Well what if I want the 0th index of the 0th index. Then I need to have ((void**) ((void**) my_data)[0])[0]; Lets just to another to show how much boilerplate it is. ((void**) (((void**) ((void**) my_data)[0])[0]))[0]; Look carefully at the length of the above code. The following is what I would have to write if my my_data was of type void*****. my_data[0][0][0]; Much shorter and easier to type and easier to read. Also a void***** is the exact same size in memory as a void* and the casting costs nothing.
&gt; it needs to be able to hold a type as big as the biggest type I am required to have by the function of the c language. It turns out this seems to be void*. A `double` can be larger than a `void*`. The safest way would probably be to use a `union` &gt; Int* or char* does not work apparently because on some systems a int* or a char* is a smaller number than a char A `char` is always one byte (although a byte can be larger than an octet). So there is no way a `int*` or `char*` could be smaller. &gt; Much shorter and easier to type and easier to read. True, although one could also use macros for that kind of access. And did you consider using a typedef for your `void*****`?
My language does not allow for doubles however. If you want to do a double you have to do it with ints or chars. What I said was any that I would be required to have by c and by the design of my language c doesn't require me to have the double type. In the future if I do require the double type I'll make my stack of size double but it is best as others in this forum have said not to optimize early. "A char is always one byte". True. A char is always one byte. A char* is not always the same number of bytes as a char. If you don't understand that you don't understand what a pointer is. Maybe I am wrong but I doubt it. I could use macros but meh I don't care about c to much. I don't learn a language or program feature until the exact moment it becomes useful to me.
&gt; I don't learn a language or program feature until the exact moment it becomes useful to me. That is a profoundly foolish statement. I cannot imagine in what situation someone would say "Jee, I sure need a wrench!" without knowledge of what a wrench was or how it could be used.
You're wrong. If I never knew what a wrench was I would go to take something apart or put something together I would realize I couldn't get it. So I would find somebody who knew how to do it and they would say, "Oh you need a wrench." And I would say, "A what?" And they would explain what a wrench was and then I would know. Then I could use a wrench whenever I wanted.
Cool. Just did a similar thing on 8th: null words-like ( s:len nip swap s:len nip n:- ) a:sort That leaves a sorted array of the words; then a:shift s:len . space . cr Repeated a few times gives: 22 g:edit-on-double-click 19 g:root-item-visible 19 g:show-line-numbers 18 g:outlinethicknes 18 cr:chacha20box-sig So the GUI words are the longest, followed by some crypto words. I'm happy to see you using a similar namespace idea to 8th's :)
&gt; I want to know if I can "safely" use them despite their age No, FORTH isn't safe. In programming terms, FORTH is about doing things in a fundamentally different way to the modern consensus. If adhering to said consensus, and acceptance from others who do, is your main priority, then I would actually encourage you not to have anything to do with it.
I don't think he meant anything like memory safety, or safe according to current programming culture paradigms.
I should probably try to scare you off. Forth isn't a language that you can just crack open and "get" right away like C or derivatives of it. This is something that will fundamentally change you. But I suppose if you're doing Project Euler, maybe Forth will sate a thirst you didn't even know you had. So here's my suggestion: grab up all the Forth material that you can and read it twice. Despite the age of a lot of the documentation, it is all still 100% valid. Learn all you can, and then go build your own Forth implementation to learn about it more. If languages like C give you enough rope to hang yourself, Forth is the sharpest knife you can imagine. There's a reason we gathered here in this subreddit. It's not just a language, it's a concept, maybe even a philosophy.
&gt; I want to know if I can "safely" use them despite their age Older forth code and articles tend to be just as useful and interesting as newer forth code and articles. Forth is not really oriented around software ecosystems as much as say, Python is. Forthers mostly exchange ideas to implement themselves, rather than actual monolithic source code libraries so there isn't any api with a large surface area that you need to keep up with. A forth implementation can be viable without a large community supporting it. Forth is personal computing for everything through the hardware and up rather than just the superficial GUI level. I'd say a user-interface is characterized by transient, one-off interactions. A program is more for long-term, off-line, one-shot recipes. Forth works best as a flexible, extensible user-interface.
I'm interested in trying this, but it won't even compile. clang reports "29 warnings and 17 errors generated" when building. After adding in a bunch of missing semicolons there's still numerous errors along the lines of: dg.c:117:12: error: invalid operands to binary expression ('void ****' and 'void ****') *r0=*r0&lt;&lt;*s; ~~~^ ~~ I'm giving up for now as I have no interest in messing with broken, undocumented code. If you get this to compile and work I'll be willing to take another look.
Which Forth implementation are you planning learn with? I don't know about other implementations, but I know GForth has a thorough tutorial/manual that you could use in conjunction with those books to check that the specific details they mention directly apply. I would guess whatever other implementation you use would have that sort of documentation too. I'd recommend trying out the code in those books, and if they don't work, check the implementation manual and see how that implementation approaches the particular programming issue differently. 
Right this is just my rought draft. There is another version in that github repo which should compile if I left the working copy alone but I didn't even debug that copy all the way cause that too was just a rough draft. If you just pick the program gnu dc you'll sort of figure out how dc works and then how this works. This is an op code interpreter that has a maximum of 256 ops so that each super useful op can be bound to a character which phonetically makes sense in english. This is very similar to the design of dc.
so true... http://gen.lib.rus.ec/search.php?req=Threaded+Interpretive+Languages ...wu wei 
The [Gforth Manual](https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/) has lots of info.
Yea, I downloaded gforth manual. Thanks the hint!
I don't think there's a standard way to do this, since the ANS Forth standard doesn't specify *how* a Forth must implement word creation. That is, if you look at other Forths you will find other ways of accomplishing the same thing, though they will usually be similar (and similarly named, perhaps; but the factors are not standardized AFAIK).
I agree. We don't even have a standard dictionary format, or even a standard structure apart from agreeing that a "simple" one is a linked list.
True, though there's no reason to standardize the *format* of the dictionary. It might be useful to standardize some of the factors so that what the OP is interested in could be done in a portable way.
I think an effort to "standardize" anything having to do with Forth is a futile one. OP would be best off learning how gforth does it if he wants to stick with it, or make something up and move on.
Can't argue with that (being the author of a non-"standard" Forth...)
Every time I dig into how gforth does something like that, it turns out to be more complicated than I want it to be. You can `see` your way around how everything works, and so then you'll work out how you want your code to do it. And then you'll try it in `gforth-fast` or `gforth-itc`, or upgrade to the current git repo, and it'll all bork. Since it's not specified in the standard, you're on your own to go monkeying around in the guts of the machine.
&gt; Learn all you can, and then go build your own Forth implementation to learn about it more. Note: this should not be your first project - or probably even your sixth. For one thing, nobody new to a language knows it well enough to implement it - it even took Chuck Moore ten years, and he invented Forth. For another, writing some other stuff in Forth will get you to the point where you know it well enough to decide whether or not it's worth putting in the time to make one that's *yours* - because there's really no other reason to write a Forth¹; and by the time you're fluent in it, it'll be a much easier job. Also, Forth is about the easiest language to implement (especially if you lop off awkward corners like DOES&gt;); unfortunately, that implementation tends to be the most complicated job most people who write their own Forth ever undertake with the language. __ 1. This is a bit of a fib. There's one very good reason to implement a Forth - you find yourself with a desperate need for a fast interpreter, say on a microcontroller, and nothing else will fit.
I had decided to not do a vocabulary/search order setup when I started on R12. The namespace idea from 8th looked like a good approach so I adopted it and have been very happy with it.
Yeah I'm actually quite disappointed with how complex gforth is. Implementation simplicity is one of the reasons I like forth but that seems totally lost when I'm using gforth. What you are saying makes sense, but - I don't feel like I'm trying to fiddle with the guts of the machine. My impressions from Starting Forth and Jonesforth is that creating your own words and 'compiling' addresses into them is kind of a normal forth thing to do. The existence of words like `create`, `compile,`, and `postpone` are all geared towards that. Why would a forth have `compile,` and `[comp']` but no stable api for creating a new word header? It doesn't make sense to me why gforth would only have `create` work for variables with no stable way to create a code word or change the header to one. 
That is quite reasonable and I wish more commercial vendors had that mindset. Annual fees make me uncomfortable as I'm only renting a toolset. Mathematica is a great engineers toolbox, but if you have to leave your company, paying thousands out of pocket can be a no-go if your new employer doesn't wish to do so. I'd be more willing to invest in it knowing I had a personal seat for as long as it runs. Again, I wish you and your company the very best of luck.
I don't know about others, but I learned loads by attempting an implementation for myself. Of course, I was already an experienced programmer by then, so writing assembly code for the primitives and various system calls was the fastest part.
Maybe I'm misunderstanding what you're trying to do. If you're just trying to make a colon definition, that's what the colon compiler is for (`:`). Maybe you could explain what your objective is? Are you trying to duplicate `:` for education reasons, or is there some goal that standard compilation doesn't get (even with `[` and `]` breakouts to interpreter mode in a `:` def)?
If he is trying to replicate a colon, he should probably use POSTPONE, but he did mention that he wanted to use strings, which implies that he probably doesn't want to create new definitions from the prompt, but rather in an automated way. That seems to be the major hangup here. @johnboudewijn Best suggestion is to figure out how the dictionary is structured, or just SEE how colon works, and define a new word that does all the work of colon except with the changes that are relevant to your application. I myself don't have access to gforth at the moment, but at least some of the constituent words should be self-explanatory.
It would be really nice to see more people not stop after writing a basic Forth system. But I wonder: Do you advocate using a Forth system that is essentially a black box to the programmer? If not, could you clarify on what you would recommend a Forth beginner to do?
Yes, because obviously it's a binary choice between "one I made myself" and "a black box". \**sigh*\*
I was at that point. Tried to scratch my head around implementing jonesforth, without knowing Forth itself good enough. My dream is a minimal, bootable x86 Forth. But I really should learn programming clean Forth programs first. 
&gt; Yes, because obviously it's a binary choice between "one I made myself" and "a black box". *sigh* I didn't say or imply that - in fact, that's exactly why I asked what you would recommend! I want to learn something new. One thing I can imagine is that one could study an existing implementation. But since I don't think this is an easy way to get started, I wanted to know about your preferred approach. Unfortunately, you didn't give an answer yet. You emphasized the word "use". Well, but what does that even mean in this context? How would a person new to Forth "use" the language without treating the implementation as a black box or writing their own? All I know is that I actually tried using an existing implementation and I hated it. If I hadn't written my own Forth, I'd definitely given up on Forth entirely.
&gt; I didn't say or imply that You did, you know. And for good measure, you did it again: &gt; How would a person new to Forth "use" the language without treating the implementation as a black box or writing their own? See? There's that implicit binary choice again. Where does that leave Bernd Paysan, author of bigforth and MINOS and co-author of gforth, who claims [never to have implemented a Forth from scratch](https://bernd-paysan.de/why-forth.html)? &gt; Unfortunately, you didn't give an answer yet. I did, you know. It just wasn't what you wanted to hear, so you decided to be dismissive instead: &gt; You emphasized the word "use". Well, but what does that even mean in this context? Well, how does one usually use a programming language? It really wasn't the starting point for a philosophical discussion. And I have to repeat this: **I am shocked that anything I said is controversial**. In fact, you kind of prove my point: &gt; All I know is that I actually tried using existing implementations and I hated it. If I hadn't written my own Forth, I'd definitely given up on Forth entirely. There are only two explanations for this. Either you give up at stuff really quickly - which seems unlikely, because then you wouldn't have had the patience to implement your own Forth and stick with it - or *you did as I suggested*: &gt; writing some other stuff in Forth will get you to the point where you know it well enough to decide whether or not it's worth putting in the time to make one that's yours So honestly, what the hell are you trying to argue with me about?!
Thanks; I'm also happy with it...
Thank you. Best regards, and I hope you decide to be an 8th customer!
let me try to clarify, I want to create new words pragmatically, from inside other words. for example: ``` : create-adder ( uau - ) nextname header reveal docol: cfa, postpone literal [comp'] + drop compile, postpone exit ; ``` which when called with `4 s" add4" create-adder` will create a new word `add4` in the dictionary
yes, this is right. I've tried modifying the definition of `:` but was unable to get it working, and stopped trying after I got it working using words from the definition of `create` instead. It just seems like something that a forth implementation would provide a word for, even if it's not part of a larger standard
How does it work? Does the interpreter/ compiler resolve namespace names to pointers to separate dictionaries?
Yeah, it is weird. GForth sometimes feels like a kludge. I'm glad you were able to figure it out, however. What did you end up with?
In my Forth it's just a string prefix at the start of the name. I only have a single dictionary. In the prior generation of my Forth, I had support for multiple dictionaries and search order. In actual use I ended up using a prefix handler to access words in the dictionaries directly most of the time. This looked like: (R11) ^file'open In this, ^ was the prefix to open a vocabulary (file') and access a word (open) in it. Adopting the use of a short namespace string lets me do much the same. It's more verbose if using multiple words as opposed to adding a dictionary to the search order: (R12) '/dev/stdin file:R file:open (R11) with file' "/dev/stdin" R open But it avoids name conflicts and makes it completely clear which words are being used.
Anton Ertl posts daily on comp.lang.forth, you might get his recommendation by reposting the question there. I wish I could help but I'm neither a gforth user nor an adept of the ANS and post-ANS standards. 
Shouldn't the title be Atari Forth Do?
Fantastic! I’ve been wondering what kind of performance one could get from a Forth implemented in web assembly. Not bad. 
Very 1980s.
awesome. bit different from RPL but more or less understandable
I used an array for years but they come with limits and I've come around to loving the linked list, in this small area, because of the wonderful effect that they have on the development experience. It's nice not having to think in advance about the maximum number of words but it's even nicer when you don't run into a hard wall during development. Using arrays can make the implementation significantly simpler, and faster, but unless you double (or more) your estimates then something is going to fill up sooner or later. Tradeoffs :-). An earlier version of our Forth-like language have zero lookup cost, and a mouse driven "text" editor. I don't know how I'd make Forth any simpler than that. At the same time it turns out that if you go drift too far from the norm then you're going to be travelling alone. We ended up ditching the editor to make people happy and that eventually lead to an unravelling of the whole system. When you remove a foundational design decision like that and start pulling you'll quickly (or not so quickly) find that you're building on sand and none of the other tradeoffs you made make much sense any more. So we're slowly edging our way back towards a more normal Forth. I'm not sure how I feel about that, but the language is far less useful than the platform it's built on so it makes sense from a business/commercial standpoint. I'm hopeful about reimplementing those lost ideas at some point, but for now, on with the show ;-).
You see that big G in GForth. It stands for GNU (less commonly expanded: Giant, Nasty, and Unavoidable)
That's why I built my own Forth! With blackjack, and hookers!
You can always contruct auxiliary structures after the fact. The original structure is always a linked list, but there's nothing stopping you from implementing a binary tree or a hash table on top of it.
Bingo :-) but now you have "two things", and if we're still aiming for simplicity we have to choose one. If you have a decent metacompiler there may be another option -- it wont save you much in the way of complexity but it might save you some space -- make it a configuration option and compile a different Forth for development and deployment. If you really know what you need in advance or can afford to allocate large swaths of memory for holding headers then arrays win hands down. If you just don't want to think about it, linked lists take the cake. When it's your won system and your own problem you can do what you want to do. In practice people place great value convenience, and that implies normal. Normally, people don't want to think about it. Who would have thunk it, looking at the state of software today?
This is a very early book from 1970 and it describes an earlier ancestor to what we now know as Forth. Most of Forth is here, but there are a few differences. [The free pdf is here.](http://www.forth.org/POL.pdf)
I think it would be simple to update "quit" to search the dictionary with the new structure and, occasionally, update it. Let's remember that we can "cover up" older words with functionality updates. Although now that I mention it, retroactively updating older definitions with the new words could be interesting in and of itself.
Differences such as?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] ["Programming A Problem Oriented Language: Forth - how the internals work" (by Chuck Moore) is now available in print \[and in pdf\] • r\/Forth](https://www.reddit.com/r/programming/comments/93onxi/programming_a_problem_oriented_language_forth_how/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
My pleasure :)
What is the link to that thread you mentioned?
Here you go: https://github.com/ForthHub/discussion/issues/71
&gt; Developers essentially extend it by writing the "words" which each do more complex things, each of which then become inherent language keywords. Well, yes. Isn't that the point? You don't solve your problem in forth, you use forth to write a language in which solving your problem is trivial.
Seemingly this was some sort of joke article, as stated in the contents. Or written by someone who didn't know Forth. The jury is still out.
Reading this article was a waste of time.
This is what constitutes a "hate" for Forth? A smattering of weak arguments and whining? LOL
Writing comedy is not easy. IMHO this was an attempt to do just that. It highlighted all the features of Forth in the form of complaints. Not completely successful comedy.
I was doing some physics simulations and non-linear optimization and the most straightforward thing to do was use python libraries in my limited time. However, I was soon frustrated by not being able to arbitrarily slice up functions into factors. Now I can do EM simulations and visualization in a Jupyter notebook from forth thanks to H.C. Chen's peforth.
&gt; Sometimes it is conventient when you don't want to work through a stack operation just **to** r1 ! and get it back later with r1 @. I think you meant to write **do** instead of **to** here? But this problem is already solved in a better way by having a second stack.
I dunno I have never seen the appeal of the second stack thing. I want want like three or four second stacks.
I wondered why I had you tagged Dunning–Kruger and then looked at your post history. You make a lot of incorrect statements about low level architecture and then state you're not a low level programmer as an excuse when corrected. You also supposedly left Forth and yet here you are still displaying Dunning-Kruger.
I am not a low level programmer. I am not interested in low level details, only the way that the syntax works out in my code. if someone can point out a more efficient way to do what I am doing with just as easy of a syntax I am all ears.
At the last SV-FIG meeting Sam Falvo showed us the words `D!` `D@` `&gt;D` and `D&gt;`. These are analogous to the words `R!` `R@` `&gt;R` and `R&gt;` but they are for the parameter stack rather than the return stack. The `D!` `D@` `&gt;D` and `D&gt;` data stack words look a bit noisy at times but I think they are much clearer than implicit stack jugglers, and high level words still compose/juxtapose implicitly with the data stack words buried in their lower level words. https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/examples/hello.bs https://github.com/KestrelComputer/kestrel/blob/master/software/src/bspl/bspl.fm/asm.fs#L53 
I don't really see how it is better. As far as I know these words you suggest still do a pointer store and a pointer read. Perhaps I am wrong. Like I said if I am going to add stacks I am going to want a syntax where I can keep adding variable numbers of stacks.
Variables aren't quite the same as registers... Ignoring that, using global variables will require some work to ensure that state is preserved/restored. It may be convienent to just throw a value into `r1`, but you will need to make sure that nothing else you call will alter this in ways you aren't expecting. If you just need to move a value out of the way for a bit then using `&gt;R`, `R&gt;`, and `R@` are a better solution. Or define things to help massage the stack into the state you want/need.
I have a small word set for locals that I've stored on the return stack. It kinda sits between Forth and C. It is hard to avoid in some cases, as per the Pigeonhole Principle, no data structure is a silver bullet. 