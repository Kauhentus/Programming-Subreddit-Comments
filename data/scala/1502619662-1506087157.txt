Great, so now we have a Development Environment , that does some of what an IDE can, but it's not integrated and i have to set it up for myself. But at least it's more brittle. What is the point in that? If the argument was that it's better in some way,i might understand, and it might have been the case years ago, but not anymore.
Emacs + ensime does pretty much everything I need from an IDE without getting too much in the way.
&gt; Simple ... 6. not complex or compound; single. [1] &gt; Complex ... 1. composed of many interconnected parts; compound; composite [2] &gt; Precise ... 1. definitely or strictly stated, defined, or fixed [3] The question is, is PEP-20[6] correct in that simple is better than complex? If so, then we should be encoding our libraries and abstractions on simple,general abstractions. If not, we need to choose the level of fundamental complexity that is manageable and expressive. The "just simple enough" level. Clearly, our languages already have enough expressive power to encode these simple objects and use them for a wide range of purposes. What we've done as programmers is fail to identify the most general primitive from which to derive our implementations. Simple, precise objects tend to be very expressive, as they can be composed with others to express a wide range of ideas and algorithms. This makes them very low-level. While simple things are easy to understand, paradoxically they tend to make code opaque because of their expressiveness. An example is Brainfuck: &gt; The language consists of only eight simple commands and an instruction pointer. While it is fully Turing-complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps. [4] The individual instructions of a Brainfuck program are easy to understand, reading one is not very much fun, thus understanding one is not much fun. It's a Turing tarpit [5]. A simple set of Turing complete instructions that are totally understandable on their own, completely expressive, but without any abstractive power. Python is an ironic choice to express the idea of simplicity in its core principles, as it is not expressive, precise or simple and is very complicated and complex. You have to memorize a lot of arbitrary rules in order to write a Python program, and cannot reason about the correctness of individual pieces without knowing the state of the whole program. However, most python programs are readable to novice python programmers. Contrast this to Haskell or FP scala based on cats or scalaz. The programs composed from the most precise as simple parts are not understandable to novice scala or Haskell programmers. Adjunction is an example of this. Simplicity does not begat understanding on its own. Simplicity as the basis for implementing abstractions is great - learning fewer topics to support a given implementation is the goal of simplicity. However, you shouldn't need to understand the implementation of an abstraction to understand an abstraction or explain it. It should stand on its own merits. ```foldMap is equivalent to map(f) _ andThen sumBy(g, zero)``` The above definition is imprecise and imperfect and depends on the reader understanding map, andThen, method conversion, and sumby, and zero. If your audience is used to those definitions, then they'll have no problem understanding that definition. To understand scala, I'd say you need that level of understanding. Thus, the complexity is worth it, as it is familiar and simple enough. We should make our tools out of the simplest, most general parts, but we need not explain our tools in terms of the parts they are actually defined in our laws and documentation, or limit ourselves to low -level simple abstractions for all tools, or rewrite tools that already behave simple laws. Hiding (some of) the details is what makes it possible to make wonderful things. So yes, we need to build libraries from these at the base. Yes, we should look at our languages and compilers to see if we can make them simple at their base level of abstractions. No, we shouldn't make all of our abstractions low - level. No, we shouldn't rewrite every working abstraction in terms of some simpler abstraction - proving something is the same is enough to say it is correct, anyway. No, not everyone *needs* to fully understand the lowest-level abstraction right away to write code with functions that use it. Absolutes are often the enemy of progress in the real world. Simple is not always better. Judgement of complexity is always an imprecise art. [1](http://www.dictionary.com/browse/simple) [2](http://www.dictionary.com/browse/complex) [3](http://www.dictionary.com/browse/precise) [4](https://en.wikipedia.org/wiki/Brainfuck) [5](https://en.wikipedia.org/wiki/Turing_tarpit) [6](https://www.python.org/dev/peps/pep-0020)
I have this same problem. I thought you were supposed to define Module.scala and bind certain things there so that the injection actually happened, but I think the Play code has those define elsewhere so that it's simple to inject anything native to the framework.
I wish I had some BTC bro!!! 
I'm glad to see you and the other sbt core devs plugging away at polishing off the rough edges, /u/eed3si9n. Thanks!
I believe Play uses [Guice](https://github.com/google/guice) by default for runtime dependency injection. Note that in Scala, a common alternative is the Cake Pattern and compile time dependency injection - Play supports doing things this way instead, and it's a little less "magic". [The documentation](https://www.playframework.com/documentation/2.6.x/ScalaCompileTimeDependencyInjection) tends to focus on runtime injection, but it becomes a pretty simple pattern to use once you have the basics down. Edit: Originally I began by explaining this was DI, I then reread your post and realised you began with that - sorry, not attempting to patronise, just me being dumb.
Thank you. I find this new Play totally different from the one I used. Hard to get used to for me so far.
&gt; Note that in Scala, a common alternative is the Cake Pattern and compile time dependency injection It's common (or at least was) but today it is discouraged. There are better alternatives, e.g. Macwire.
Could you provide some reference? I wasn't aware it was discouraged - I'd be interested in seeing advise on best practice.
I am facing the issues when people add JAR files into Git repo or copy/paste a bunch of code w/o any hint of consciousness. Not mentioning that renown "assertEquals(true, true)" in the tests. So I assume that somewhere in the high ivory towers there are programmers who do fancy things. My experience is quite different :)
Sadly I don't think that is unique to Scala, I used to see a lot of that in Java too. "rm -rf" is handy ;)
While it's not Scala related, still - with Java it's easier. First of all, the language is very verbose and fool-proof by design, you can read almost any code, even from offshore. Also there are the number of best practices and guides for Java, so anyone can if not comprehend - but memoize "do not leave empty catch blocks" or "do not inherit from a class - just compose another class in". Not a rocket science, easily traceable and one good dev can lead a horde of keyboard junkies to do something useful in enterprise XML munching. But with Scala - oh dear, the level of shit per LOC soars. It took me couple of months to introduce case classes to the offshore, no less. Still they can't do it right, they can't get to trait mixins, they can't get variance, they can only copy and paste code that happen to compile and work.
Sbt is my IDE, even if it isn't graphical. I've founf that the popular IDEs don't add much if anything over just using Sbt and editing with whatever you please.
The cake pattern was disavowed by its inventor: https://twitter.com/propensive/status/856150425304780801 Check out the examples for Macwire DI or compile time DI: https://playframework.com/download#examples https://github.com/playframework/play-scala-compile-di-example/tree/2.6.x https://github.com/playframework/play-scala-macwire-di-example/tree/2.6.x
https://tersesystems.com/2014/08/19/exposing-akka-actor-state-with-jmx/
Can I piggyback off this and ask what kind of things need improvement in the current libraries? How would one go about improving the existing code? I guess you could go through some of the scala tutorials/books to patch up your knowledge.
I feel like [Functional Programming in Scala](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653) is the obvious answer here.
Write documentation, e.g. Scaladoc 😊 One nice point to put in more Scaladocs is at the package level (i.e. on top of package objects), so that readers can see nice package-level summary documentation of what that package contains.
I dont have any like one stop shopping places for this kind of thing, but Ill dig through my memory for what helped me. How robust you want to make your system depends on how much time you have. The current project Im on is fairly brittle sadly, but we have a tiny team and no time. Idempotency is very powerful if it works for your system. Not bringing any part of your application down when parts of the system are down is very useful. Having a retry system on messages is useful. I tend to not prefer message busses like RabbitMQ and such, but they can be pretty useful. Think of any transaction to a service as something that could be restarted at any time. You can create sticky services like User A -&gt; Service B Machine 2. And that can have state, and cache stuff, etc, but if machine 2 goes down, or you bring up 16 new machines, User A might go to Service B Machine 12 now. Layering the systems helps. So, you might have automatic retry logic at the message layer, but also retry logic at the app layer. You have to pay attention to timeouts here to make sure your low level layer isnt sending out retries when the higher level has already cancelled the action. Addresses of things are logical, and they need to be looked up to be useful. What this lookup looks like depends on the service. Some are just round robin. Some are consistently hashed, and others might be a full cached lookup. Start on a single machine with a single instance of your application. Quickly move to multiple processes though, to make sure you havent introduced implicit same-process dependencies. Chaos monkey everything. Run CPU taxing processes on your DBs and on your service machines and anything else. Drop some messages on purpose. Run networks through bandwidth limiters. Assume things will mostly work, but not completely work. 
Hello. I'm one of the (many) current maintainers of Scalaz, a functional programming library. I definitely want to emphasize just how much I and many others are still learning, and that we often contribute bits and pieces in areas we have some understanding and sit back in awe as others contribute to the areas we are still learning about! You definitely don't need to be an expert to start contributing, you need to be curious, eager to help and willing to play around a lot. We (Scalaz) definitely need help, everything from big redesigns of core concepts to documentation/example code. I also echo /u/i_have_a_gub and recommend Functional Programming in Scala, which is a very challenging and very rewarding book. I'm also a huge fan of [Learning Scalaz](http://eed3si9n.com/learning-scalaz/), and I'm sure the corresponding Cats version is also quite good. You can start checking out the gitter channel for Cats, or come poke your head in the Scalaz channel on IRC (which has less activity overall but bursts of some amazing conversations). 
I can double /u/yawaramin that starting with documentation is a great way to learn library and contribute at the same time. If you have particular questions about FP lib internals - you can ask in their gitter (or me in DM - I'm glad to help where I can). I actually think that starting kind of Learning Groups for FP in Scala can be a viable idea. People could learn together and feel safe to ask "dumb" questions in some dedicated channels.
Why would you use it instead of the jvm? It has barely any libraries...
Learn haskell and then you will know, what cats is all about.
This sounds relevant to my interests - is there a transcript anywhere?
Any performance benchmarks on using Eta vs. Haskell or Scala?
You could start by taking a look at some of the open issues; cats provides two useful labels here: [low-hanging-fruit](https://github.com/typelevel/cats/issues?q=is%3Aissue+is%3Aopen+label%3A%22low-hanging+fruit%22) and [help-wanted](https://github.com/typelevel/cats/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22), these tend to be more approachable problems for new contributors.
Join the FP scala channels on Gitter (cats, shapeless, fs2, doobie, typelevel/general). We are there to help you, e.g. see [this](https://twitter.com/balajisivaraman/status/894976114304499712). And as /u/runT1ME said , always keep in mind that we never stop learning, so never feel intimidated :) I encourage you to ask specific questions about anything that confuses you on Gitter and, as a small piece of practical advice on where to start, you should get familiar with the concept of typeclass (if you aren't already), and its encoding in Scala via traits + implicits. 
I forgot completely about the red book. Definitely I'll take a look. Thanks! 
Thank you for your advice. Part of the resources that I am using is the two guides (learning scalaz and herding cats) but they're full of references to concepts that I don't fully understand yet. The advice about FP in Scala is definitely my new starting guide. I have a question: why do you say that one of the areas in where you need help is on the redesign of the library core? I ask that question because one tends to see that functional libraries are so well designed that you only need little maintenance efforts. This question is more or less related with one of the answers below (by /u/hoodasaur) in where the same question is asked. 
I think that this is the very hard way to do it. I'm not saying that the goal I aim is easy but certainly there's other less difficult ways. Also I know what cats/Scalaz is about (at a "user" level) but definitely I want to understand their inner workings. Haskell is on my to-do list, but I think that I need way more level before diving into it. 
The easiest way to contribute code is to use the libraries in earnest, until you realise something you often need is not in the library, and you find yourself rewriting it all the time, at which point it might be worth asking if it belongs in cats/scalaz. Also, bear in mind that cats is designed to be a foundational, stable library. That being said, there's still a lot to contribute both there and in the purely functional libraries that exploit it, like http4s, fs2 or doobie. These are not the only ways ofc: scalaz 8 should be a fairly extensive rewrite, and one can always improve existing code anyway, but as a beginner contributor is far easier to add little things here and there (which doesn't mean you shouldn't at least lurk in the high level design discussions, there's a lot to be learned there)
I found Scrooge to be more mature and Scala-idiomatic than anything available in the protobuf world; I don't know what Go's support for thrift is like but could that be an option? They're very similar in terms of features and ... most things really.
Ironically its these really large projects where IDE's bring usefulness (compared to smaller projects), particularly if you are using a lot of third party libraries. IDE's ability to navigate, quickly find classes (third party ones too), display documentation and more importantly refactoring capabilities (which become much harder if you do them manually in large code bases) The unfortunate issue here is that Scala's type system is really complicated, and so in order to provide completions in a sane amount of time (and in a lot of cases this amount of time ins't "sane") you often have to aggressively cache a lot of permutations of type computations, which take a lot of memory. I somewhat frequently try out other editors/IDE's, and so far the only IDE which provides comprehensive IDE like capabilities (i.e. not just basic refactor and autocomplete) is IntelliJ. All of the others (ensime/Eclipse) only have really basic refactoring abilities, and actually aren't that much faster than Intellij. Of course, Intellij has the problem where it shows correct code as incorrect, but this is because they had to implement their own typechecker (which they needed to do in order to provide the advanced refactoring/inpsection capabilities which the other tools dont have)
You are telling me that its faster for you to figure out imports for some third party library by manually looking up some code compared to an IDE which will do this for you automatically?
thread title should've been "Start using cats in your project right meow!"
Thank you for your interest in cats. We need and appreciate contributions very much. I agree with other comments here that documentation is a good entry point. It's also our focus for the next release 1.0 RC1. You don't have to write documentation yourself. Providing feedbacks on existing documentation as well as documentation pull requests will be of great help. Reviewing other code PRs is another good way to gradually gain understating of the library through contribution. We also have a resource for learners section on our website http://typelevel.org/cats/resources_for_learners.html. If you have any questions, don't hesitate to ask in our gitter chat room. 
You can, but its very unwise to do so because 1. You lose all interopt, especially with collections (the idea at the time is that you were able to convert scala collections to .net collections and vice versa, just like you can convert between scala and java collections now). This is basically a must, otherwise you lose all benefits of targeting the CLR in the first place (ecosystem interopt) 2. To get any decent performance, you need to implement reified generics because this is the CLR solution to the specialization problem, and its also how they implement value classes efficiently amongst other things So in other words, unless you want to target the CLR just for the sakes of targeting the CLR and completely ignoring the .net ecosystem, its really not viable
I really think it depends on the person ... I use an IDE to code in both C (primary) and scala (non professional). I keep trying to switch to vim environment but have still not set everything up, learnt the tools to integrate other plugins like ENSIME. Its a WIP but I must admit some people are way faster and are able to keep more data in the workspace in their brain ... Looking things up using grep usually distracts me enough to disrupt my flow but for others it's nothing. The main features I like an IDE for like definition lookup and finding calls should be available via text editor and plugins. One great feature I like with Idea based IDE is the call heirarchy which I havent found using vim tools. However that being said coding in vim environment makes your development environment really portable. It's the primary reason why I want to make the switch.
Spark is not good for machine learning. That said, most of the good material (and libraries) are in python or R. Doing machine learning in scala is far from impossible but it's not the easy way for sure. My suggestion is to give up to python, learn everything you need taking a few introductory courses and then, while you proceed with more complicated stuff, you can learn how to do the same stuff in a jvm environment. 
&gt; the blog author threatened to kill me Oh, sorry, I didn't know about this.
What are you on about? Spark plus mllib is amazing for machine learning. Maybe not for doing phd level research but def for production implementations.
OK. So I read the articles you guys provided. Next I added the following lines to my application.conf internal-threadpool-size = 100 play { akka { actor { default-dispatcher { fork-join-executor { parallelism-factor = 100 parallelism-max = 100 parallelism-min = 100 } } } } } Now I run my play application and I run jconsole and attach it to the PID of my play application. what I see there is 17 entires for play-akka.actor.default-dispatcher (1 to 17) 17 entries for New I/O worker (1 to 17) 6 entries for play-internal-execution-context (1 to 6) 17 entries for foo-bar.akka.actor.default-dispatcher (1 to 17) Here foo-bar is the name of the actor system which is initialized in my code (val system = new ActorSystem("foo-bar")). This Actor system is passed to the spray client to make HTTP requests. I don't understand what is "new I/O worker" also I don't understand the 17 number because My config says minimum 100. (Note that I don't intend to run my app with 100 threads. I just put in a high number to see whether my configuration is being applied or not. Looking at jconsole results, it seems my config is not being applied at all.) 
I love Spark, and most of our ETL is done in it, but yeah, we use Python for all our actual ML. That's the same story at the majority of places I know of...people like the idea of using mllib but in practice the Python libraries seem to be significantly ahead.
For production it's ok. But there's a huge debate going on if these kind of distributed learning libs are a good idea. Most of the time the performance and the precision of the model is wasted because of the distributed implementation while a local, parallel implementation could achieve better results in less time. This is not a problem about mllib, flinkml, samoa or mahout. It's about the perceived use case: 95% of the time you don't need it but you use it because it's cool, or because it's faster to implement, but this is a poor choice that can have consequences. Basically, mllib or similar libraries are basically a nuclear bomb that should be used only in extreme cases because they have huge trade-offs but they are advertised as a jack of all trades that can be used in any situation. That's why I say it's not good in general to do machine learning, because you should avoid it if you can. Sometimes you can't (huge recommending systems, for example) and then it's a decent tool.
I wouldn't use spark for any academic research or something like that. But when I want to take hundreds of millions of records from across a bunch of different data sources, do various types of feature transformations on then, run a series of ML algorithms on the resulting features, and then store the results and/or model somewhere to be exposed by an HTTP service Spark is an amazing choice. You definitely pay an overhead using a distributed framework like this (and it makes things harder to reason about, not to mention debug) - but it also makes it possible to build a framework that allows you to drop in more/different data sources and feature transformations and generally handle things (with some tuning, scaling of executors, etc). or e.g., ab test different algorithms that basically plug into the rest of your whole ML pipeline. To OP I'd ask - what's your interest? Research or building data/ML products? 
Do you do this kind of stuff for work? I'm a Machine Learning engineer myself but working with simpler scenarios. I'm very interested in experiences (and eventually material) about more complex scenarios that right now seem to escape a good coverage on books and blog posts. Do you know any good material about good practices and design in ML engineering?
is not a transcript, but it covers general idea very well: http://www.stephendiehl.com/posts/adjunctions.html 
Hah, I found that one but it seemed a bit too abstract for me. Was hoping to see a common pattern pulled out of code with some common use cases. 
I didn't noticed the "Resources for Learners" section. I'll take a look. I'll take a look at the documentation and I'll tell you. I hope to learn a lot. Thank you!
The guy who founded coursera has a new specialisation starting this month. All python in the browser. Google andrew ng machine learning
The "Machine Learning is Fun" series from Adam Geitgey is a pretty good, language neutral, overview of the topic. When you get code it is indeed Python, but the concepts transcend the languages: https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
Have you taken a look at https://www.manning.com/books/reactive-machine-learning-systems, it's still in early release but it may be up your alley
 The team I'm on has basically finished our project building a voice based survey tool using actors + akka. We're working with smallholder farmers in Africa, many of whom struggle with text messaging or even pressing numbers on their phone at prompts. Akka actors have been incredibly great for handling the state of the calls. We're now building in internationalization to our calls. Are Java resource bundles the established way to do this? It seems to be the thing with the best support so far.
More specifically, the new courses are described at [deeplearning.ai](http://deeplearning.ai). 
Been into Scala and Python for an equal measure of about a year, and Python seems to be a much more common language to work in for ML efforts. If you look over the GitHub repos of Facebook Research, OpenAI, DeepMind, et al. No Scala and no Java to be found in my cursory search. There's no reason it can't be done, but there's already a lot of fieldwork in Python, R and C, so you'll have additional battles to fight when implementing an algorithm.
Wonderful information, thank you for taking the time to write it out for me. This gives me a lot to research and think on. Much appreciated! :)
Huh? The gRPC-like part of Thrift is built in already and always has been.
Not that I know of, it would be nice to make a full test including IO+CPU.
The .net ecosystem is actually pretty big, its not Java huge but its in the top tier of languages
And most of them don't/won't be cross-platform. Some scalaist also had a [bad experience](https://groups.google.com/forum/#!topic/scala-debate/aD2hEe-FN3I) with .net. And TBH, I've never heard of any noteworthy library on .net which was born on it. I don't know if you've numbers but I'd guess it's definitely not "top tier".
You can use the spark framework in local mode when that suits your needs. I agree that many people are prematurely distributing, but using a framework that has the capability to go distributed when necessary makes sense.
My concern is about the implementation. I've seen in production cases where an execution of Random Forest on Spark took an hour while locally it took a couple of minutes. Once your architecture has Spark as the only option for training models in production and you discover your models are small, you're fucked. It doesn't make sense to run it in local mode in production.
I'd argue running it in local mode in production is entirely reasonable - if you standardise on small models and then discover you have a large one you're much more fucked. Certainly I'd be interested to see a benchmark that included that option. 
Buy this book and it will a lot easier to start with scala. Believe me.
What's up with Cats? Was there some conflict or does it have some problem or something? Just asking becuase I started to see some 'big' names starting to contribute to scalaZ, and also saw some tweets how would scalaz be better etc... Nothing against scalaz or cats or anyone contributing to anything, just curious. I guess I was kindof looking forward to unified fp model/ depednency
I don't know anything about the politics (I avoid twitter and typelevel), but might it just be that ScalaZ is making a bit more progress and having something more exciting to work on? ScalaZ 8 seems to be gaining momentum and promises to make final tagless style much more practical via a new encoding; meanwhile Cats seems to be taking forever to get to a 1.0 which it's becoming clear will be nowhere near to feature parity with ScalaZ if/when it's actually released.
I think you need types-on-mouseover (or on some kind of keyboard shortcut) to get the most out of Scala, and an editor probably has to be pretty heavily integrated to provide that. OTOH I'd say you can probably be as productive in Scala in a non-IDE as you can in a language that doesn't offer IDEs at all. So it depends what you're comparing to.
Great, I'll look into it
That's a shame, I have never been so excited about a language before Scala. I already know Python, so I guess I'll go that way. Thanks for the input!
Great suggestion, thanks! I'll do exactly that
Ti ho stalkato solo per controllare che fossi vivo :D Bon, scappo, non so neanche che sub sia, ciaociao
I've been looking at tutorials for Play which use the older versions before dependency injection was integrated in. As a noob who has never used dependency injection before, i find it just gets in the way of understanding the core concepts.
It does. I'm no longer learning Play but the javax.inject package :p
Just took a more clear read-through of your question. The point about math not being "an issue" I am hoping you mean in the way that you're familiar with linear algebra, and not that you expect to implement algorithms for classification of text (like spam vs not spam comments) without ever having to understand what goes into it. I very much recommend Andrew Ng's original ML course from Coursera. It walks you through simple implementations of algorithms for ML, then into more powerful ones like neural networks. Avoiding this and diving right into books may be more of an uphill battle than is conducive to your learning. Either way, you're on the right track. Best of luck.
Knowing nothing about proper web development I was able to get up and running with flask (Python) very quickly. Play has proven more difficult than I suspect it needs to be.
yea. Im kind of interested in politics because it bothers me (probably more than it should) that i have to distinguish between them two and in their transitive dependencies etc... and I was pretty happy and naive that community will kind of be united (seeing doobie and http4s adopting cats as primary), ... but then saw some tweets and some fast progress in scalaZ...
Yeah, that's what I meant : linear algebra, matrices, vectors, and so forth. I'll check it out, thanks for the kind words!
Will give it a look. Don't know play or flask lol
Really good book, I'd highly recommend it. 
 [Jarget](https://github.com/caiorss/jarget) A Scala self-contained command line tool to do lots of automation tasks like download jar packages or scala packages without the need to create project. And it can also run scala REPL with classpath set automatically which makes evaluation of Scala packages easier and faster.
Time for a plug! Want to use cats or scalaz in your project that uses the other one? Have no fear! https://github.com/shawjef3/harmony
Time for a plug! Want to use cats or scalaz in your project that uses the other one? Have no fear! https://github.com/shawjef3/harmony 
&gt; so one is usually guessing whether to add parentheses or not. That's rich, since Scala has a similar problem.
Yeah. In general, if you have a `Future` it means the wheels are already in motion somewhere. The statement *is* true for `Task`s though. With a task, you can "wire up" (map/flatMap/filter/etc) all you want, but nothing happens until you say "go" at the end and they all execute *in sequence* (unless told otherwise)
 It's pretty easy to ignore the differences and use both, or write your own typeclass interfaces and delegate to the lib you want to use, and it's good practice to have a layer between you and the lib you use, anyway, and you probably only use a small subset of the libraries abstractions. Anyway, cats is not a drop in replacement for scalaz. They both provide the basic Monad, traverse, Monoid, Kleisli, Free typeclass heirarchies and datstypes. They both have extra libraries to do effects. Scalaz provides a lot more combinators. Scalaz import all is easier to remember, but importing just the things you need from cats is easier to remember. Lots of libraries use scalaz, but they don't list it as a feature. Libraries using cats list it as a feature. Scalaz feels like a self - contained ecosystem while cats feels like it is meant to be extended by users and used for libraries. A lot of those libraries are advertised together under the typeclass site and that makes it easy to choose streaming and db libs that won't conflict in their transient dependencies. I've used scalaz for years, and switched to cats a year back. Other than initially navigating the library, no frustrations. I wouldn't switch based on tweets. Try scalaz in your next project and see how you like it - a Functor by any other name is still a Functor, after all. If you do, great. If not, stick with cats. If you find something missing, contribute to cats! If the main authors don't want it in the main library, shove it in your own extension and publish it. With the async monad addition and freestyle, I don't think you can go wrong with cats. 
I would never allow a signing key to leave my local hardware; as wonderful as Travis is, IMO delegating trust to them is shirking one's responsibility to one's downstream users.
The purpose of signing those packages is for people to know it comes from the official source. When you enable Travis to do the build, then Travis becomes that official source. I find that an acceptable compromise, because as a software author, I do not have the resources to manage releases manually, as I don't get paid for it and I'd rather spend my time somewhere else. Right now I'm in charge of publishing about 4 Scala libraries that people depend on. And I cannot do it all by myself. Somewhere along the line you have to trust somebody else to do that publishing for you. And I could find other people to promote as maintainers, but that's not a responsibility that many people want, plus it's really hard to build trust in people that you meet on the Internet and that you've never seen face to face. Or you could trust Travis and be done with it. Also, let me tell you, a lot of projects are doing it and this is the norm in the Node.js ecosystem for example. If Travis gets compromised, then we are screwed anyway. EDIT: instead of downvotes, I would appreciate feedback, thanks!
I've just read this conversation and I feel like something finally clicked about free monad, great explanation! :) But I still find it difficult to figure out how to incorporate it while designing my applications without forcing it
&gt; I find that an acceptable compromise, because as a software author, I do not have the resources to manage releases manually, as I don't get paid for it and I'd rather spend my time somewhere else. The decision to release is a deliberate one; I don't feel like running the release build locally is a significantly higher cost. I mean it puts my development machine out of action for a little while (but I probably wouldn't want to start new development right after a release anyway; it's a good time to take a break and do some exercise or something) and I guess it costs me some electriticy (but not enough to notice). &gt; And I could find other people to promote as maintainers, but that's not a responsibility that many people want, plus it's really hard to build trust in people that you meet on the Internet and that you've never seen face to face. Or you could trust Travis and be done with it. I'd sooner trust an individual who's at least (presumably) putting their own name and reputation on the line, if they're identified via the web of trust, even if I've never met them. I guess Travis is too but I don't feel confident enough they'll continue to be reputable when I can't see where their revenue stream is coming from - look at what happened to SourceForge. &gt; Also, let me tell you, a lot of projects are doing it and this is the norm in the Node.js ecosystem for example. If Travis gets compromised, then we are screwed anyway. I'd like to think the maven ecosystem is better than that. Node doesn't sign packages at all IIRC? So not the same in terms of end user expectations.
You add it into the Module. See the github project for an example: https://github.com/playframework/play-scala-starter-example/blob/2.6.x/app/Module.scala#L23
&gt; The decision to release is a deliberate one; I don't feel like running the release build locally is a significantly higher cost. The cost of running the deployment script is in my experience quite high, but that's not the only cost either. The cost is in validating that a release should be made and be available to do that release when a user cries out for it, due to pressing issues. Having Travis do the actual release means that the barrier to recruiting maintainers is all of a sudden very low. &gt; I'd sooner trust an individual who's at least (presumably) putting their own name and reputation on the line If you're lucky to live in a tech oriented area, like Silicon Valley, you're probably lucky enough to meet people willing to do this. All the people I'm meeting are from the Internet and most of them I never meet face to face, since the only venues where I can meet such people are Scala conferences and I don't have much time to go to conferences either. I would like to better understand where you're coming from. So what libraries do you maintain?
The page seems to be broken on a phone :(.
I am writing daily in TypeScript (typed language, but nowhere near Scala) and I rarely use mouse in IntelliJ IDEA. GUI doesn't mean you can't use shortcuts. Also IDEA has integrated terminal which is quite handy (unless you use HiDPI monitor on which it sucks badly). 
Because of a memory usage? I am working as a lowly front-end guy in a country where our salaries are quite low compared to e.g. US and yet I have 16GB memory in my PC, running several browsers and two instances of IDEA with no issues. I don't think I ever hit swap at all. Do Scala projects in IDEA take so much more memory compared to TypeScript/JavaScript projects? 
&gt; The cost is in validating that a release should be made and be available to do that release when a user cries out for it, due to pressing issues. Having Travis do the actual release means that the barrier to recruiting maintainers is all of a sudden very low. I just don't see how someone would be in a position to decide whether a release should happen without at least having the ability to run the build, which is pretty much what it takes? I guess I have merged a PR from my phone once, but I can't imagine being able to do any kind of maintainer role without having a machine that can run the build, which is all it takes to do the release. &gt; If you're lucky to live in a tech oriented area, like Silicon Valley, you're probably lucky enough to meet people willing to do this. All the people I'm meeting are from the Internet and most of them I never meet face to face, since the only venues where I can meet such people are Scala conferences and I don't have much time to go to conferences either. It's not a question of meeting people. E.g. I've never met Alois Cochard (who I took over maintainership of one project from), but I'd still sooner trust him to package my releases than trust Travis, because he's a named individual with a reputation on the line. &gt; So what libraries do you maintain? I have three in maven central (tierney, paperdoll and scalaz-transfigure), but I don't have other contributors and may not even have any downstream users (certainly haven't had any asking me to make a new release). So maybe there are things I'm missing. But as far as I can see what I currently do ought to scale to a more popular library.
I would also like Alois Cochard to maintain my packages, however he's busy with more important things. That's the point I'm trying to make, open source development doesn't scale well (unless you're talking about the Linux kernel or other similarly popular projects), capable individuals are rare and I personally cannot cope with being a maintainer anymore without help, therefore I'm automating whatever I can to reduce my burden. Anyway, thanks for the feedback. I've added a warning at the start of the article, such that people are aware that this represents a risk that they need to assess.
Thank you! Well, I kinda prefer final tagless to Free anyway these days ;) My general advice about design questions: rather than asking "how to design things" tackle specific problems, and then ask "how would you design _this_ specific thing" in an FP channel (e.g. Gitter), you'll learn a lot this way. As for designing using Free (or Final Tagless), ideally you would want to identify the different concerns in your program (one algebra per concern), and then try to see if there are any fundamental operations in each concern that could form nice primitives for your algebra, and write composite operations using primitives. That being said, design is still a hard task: FP gives you very nice tools that are composable and easy to reason about (unlike OO imho), but how to use them to reach a coherent design for your specific set of constraints is still hard. Which is good, or it won't be fun :)
From my understanding Akka Streams are pretty different from Akka actors. I think the official doc is pretty good. And no, there is no back pressure in Akka actors. An actor mailbox will always accept new messages, there is no built-in way to limit the size of the mailbox and respond "sorry, I'm over capacity".
To address your question about dynamic logic: flows have operations like collect, filter, etc. You can couple those operations with fan out and fan in stages like broadcast, merge, zip, etc. to address a wide array of use cases and build fairly complex graphs. 
&gt; there is no built-in way to limit the size of the mailbox There is actually, it just isn't the default configuration.
Do you know about [Coursier](https://github.com/coursier/coursier)?
I highly recommend reading the Akka Streams documentation and following along with their examples. To get an idea of how things are implemented, take a look at implementing combinators like map and filter using the GraphStages API. If you have a project in mind that would be even better. For me, I learned by reading the documentation and implemented an application that reads data off an SQS queue, does some aggregation and pushes it to a REST API. This involves handling bad data using concepts like Broadcasts, Merges and utilizing the excellent GraphBuilder DSL. The main point I'm trying to get at is to have a project which involves some ETL and try to implement it using Akka Streams 
If you have extra time, try to see how you would do this with other streaming libraries like fs2, Spark Streaming and Flink to get a better idea of what is out there. Good luck!
I'm not familiar with this configuration enough, but it's not back pressure because a maximum mailbox size requires that when the mailbox is full you either drop messages on the floor or block the sender. You could call blocking the sender back pressure, but different senders in potentially different dispatchers would then be blocked for no fault of their own. I can't imagine that would even work as intended in a cluster, so I suspect dropping the message is the only option in that context.
I have a few Akka Streams Conference videos [here ](https://www.google.com/search?safe=off&amp;q=site%3Ahttps%3A%2F%2Fwww.reddit.com%2Fr%2FScalaConferenceVideos+akka+streams&amp;oq=site%3Ahttps%3A%2F%2Fwww.reddit.com%2Fr%2FScalaConferenceVideos+akka+streams&amp;gs_l=psy-ab.3...6021.7660.0.7944.8.8.0.0.0.0.158.800.7j1.8.0....0...1.1.64.psy-ab..0.0.0.JLk8UhhPj1g) 
Sadly I'm discovering this to be the case. I'm trying to setup an ML pipeline in Spark/Scala, but it requires an algorithm that doesn't exist in MLLib. I thought I'd roll my own (I have years of experience in numerical computation), but I'm finding Spark's linear algebra support to be weak and lacking in maturity. Scala has linear algebra libraries but so far nothing compares to Python's offerings. Pandas/Numpy are just much more mature and full-featured. I might just take the performance hit and write my pipeline in PySpark and rely on horizontal scaleout to boost performance.
There's a more complex example here: https://github.com/playframework/play-scala-websocket-example Also you'd probably like Colin Breck and Beyond The Lines: http://blog.colinbreck.com/integrating-akka-streams-and-akka-actors-part-iii/ http://www.beyondthelines.net/computing/akka-streams-patterns/ 
No. But jarget will have many tools inside just one self-contained application. Such as build fat-jars or uber jars, add files to jar, invoke scala compiler with class path and so on. The idea is to make Scala development and experimentation easier through command line with minimal intervention. It already has lots of functionalities with a single file and few lines of codes and almost no dependencies.
Seems like you're using sbt-git directly. There's also [sbt-dynver](https://github.com/dwijnand/sbt-dynver) which will generate version using annotated git tags (`git tag -a ...`). For snapshot builds I quite like the fact that it put the number of commits ahead since the last release e.g. `1.0.0+15-f873ae` so your `SNAPSHOT` will always load the latest snapshot
Thanks for the tip. In my setup I copied the config from `cats-effect`, which was introduced by Daniel Spiewak in that config, having copied it from somewhere else, I think from Verizon or something. Lots of tribal knowledge going around in these SBT configs :-)
You should really look at [sbt-release](https://github.com/sbt/sbt-release)
&gt; capable individuals are rare and I personally cannot cope with being a maintainer anymore without help, therefore I'm automating whatever I can to reduce my burden. I completely agree with automating as much as possible. I certainly wouldn't want to be without the ability to do a single-command[1] release. I just don't see the scenario where someone (whether that's you or a trusted co-maintainer) is in a position to make the decision to release (which surely implies being able to run JVM tools, unless you're talking about a project that accepts PRs literally without reviewing them - which again I'd view as unfairly exposing end users to risk) is substantially burdened by running that release locally rather than on travis. But maybe I'm missing something. [1] I guess it's single-command if my GPG key and my SSH key were already unlocked, potentially three interactions in the worst case
I wonder how a scala-native compiled version (or even just the node version as a commandline app) would run? Potentially that could keep the speed improvement while also being usable from a non-JS host editor e.g. vim.
That's a very good point, I've been wondering the same and I think I'll give a look soon enough.
This is a pattern the F# community has done with the Ionide suite of VScode tools, which are written in our JS Transpiler, Fable. It really can't be overstated how nice it is to code VSCode extensions in your native language, especially when you're able to use cross compiled tools like this. Nice write-up, and I'm excited to see more like it! 
I don't like it and it doesn't do much.
The most annoying thing about it is that you don't have the released version *and* the developed version accessible via a sbt setting. But apart from that you can accomplish the same steps. Can you explain why you don't like it?
One of the folks working on scala native actually used scalafmt to compare startup times at scala days, so the port is already somewhat in progress. 
These are great, thanks!
A faster compiler that's still FOSS would certainly be welcomed by many. I'm a bit concerned though that they seem to be focussed on indentifying &gt; a subset of Scala that can be compiled with reasonable speed rather than full compatibility. We might end up with a fragmented eco system where some libraries only work with a specific compiler.
Since libraries are shipped as jars, the compiler shouldn't be an issue. If you're trying to compile someone else's code then if you're using the normal compiler it should be fine, just it might not work with the new fast compiler.
&gt; We might end up with a fragmented eco system where some libraries only work with a specific compiler. I don't think that's the case here; from the readme: &gt; However, all Reasonable Scala programs will be compatible with Scala, so codebases that have been migrated will be crosscompilable.
This is nonsense. Twitter is not profitable. Moreover it's 'not' apolitical. BTW, can anyone name the current Lightbend-to-Typelevel scalac usage ratio within the Scala community?
Couldn't figure out how to use Play reverse routing to obtain an absoluteUrl() without encompassing it in an action first. I was planning on scheduling a WS to call the route through the API... couldn't find a way to pass in the implicit RequestHeader trait though. Am I right that there's no real way to reverse-route outside of an action in Play?
How are old style macros going to work with this? Seeing as it's not a fork of Lightbend Scala I can't really see it working too well.
This seems destined for failure. I could be wrong, but based on that announcement, I just can't see them having full scala support soon enough to have adoption. It seems too limiting to use a subset of scala features.
What's "Reasonable" mean?
Do you mean like [this](http://henning.kropponline.de/2016/04/17/scripting-scala-jsr-223/)?
There's JSR-223 support. However a good tutorial for this &amp; latest Scala is missing. I had problems with it before: https://gist.github.com/takawitter/5479445#gistcomment-1958706 You may also use Nashorn which is well documented and does not require extra dependencies. JavaScript will be more familiar to many people. I have a small tutorial about this here: https://www.scalawilliam.com/scala-nashorn-interaction/ There's a trade-off to be made, I personally try to simplify stuff so that it doesn't need scripting in the first place. Is your architecture invertible (Inversion of Control)?
The name comes from our intent to enable Scala programmers to easily **reason** about compilation overhead of various Scala features and idioms.
[Ammonite Scala Scripts](http://ammonite.io/#ScalaScripts)
It is too early to talk about the order in which we're going to implement features in rsc, but I have a comment about whether macros can be supported in principle. With scalameta/paradise, we've successfully shown that macros can be platform independent. At ScalaWorld 2016, we have demonstrated macro annotations that expand in Scalac, Dotty and Intellij. At ScalaDays 2017, we published https://github.com/scalamacros/scalamacros that provides a platform-independent API for both macro annotations and def macros. I am positive that supporting this kind of portable macros in a new compiler is realistic.
Is compilation speed really something developers care a lot about? I mean, sure. Truly all other things equal, fast is better than slow, but I have a hard time imaging wanting that to be a priority.
Our success is not tied to adoption. Ideally, we would like to determine how much different Scala features cost and then use this information to help Lightbend Scala and Dotty developers to optimize their compilers. If we manage to pull this off, I'll be pretty happy. Look at Grzegorz Kossakowski's Kentucky Mule (https://github.com/gkossakowski/kentuckymule). It is a subset of a Scala typechecker that implements just a subset of language features and has zero production users. Still, it is a tremendous success, because before Greg we had no idea that typechecking Scala can be parallel.
Collaboration to improve the Lightened/Dotty builds is something I'm 10000% for. I interpreted your announcement to be independent of that development but this would be far more attractive to me.
An order of magnitude improvement of compilation speed will create opportunities that we can't even imagine right now. For example, bringing even a 10-20 minute build down to 1-2 minutes means a completely different ballpark of productivity. Things that were previously unviable, like running code analysis tools on every commit, suddenly become realistic. (And I know companies whose builds take much longer). For another example, reducing 3-5 second incremental compilations to 0.3-0.5 seconds means that we can get feedback from the compiler almost instantaneously. This makes it possible to run code analysis tools not just every now and then, risking to break the flow, but basically on every keystroke. It is not a coincidence that before founding Reasonable Scala, I've been working on Scalameta and next-generation tooling that it enables. This made me appreciate compilation speed even more that before.
Have you considered creating a Scala parser that produces Java AST? Then you can use **javac**, a subset of Scala compiler focused on compilation speed.
While the Scala REPL/JSR-223 support works, my experience is that the API is clunky and hard to get right, which manifests itself in awkward boilerplate code and confusing classloader issues. The Ammonite REPL and Script-runner can be [trivially embedded](http://ammonite.io/#Embedding) within a larger application, allowing you to use both a REPL and run scripts (using `.runScript` instead of `.run`) within the application' s context. The REPL/scripts can manipulate objects provided by the application, or returning results for the application to use. Ammonite still does the compile-to-bytecode-put-in-classloader-for-isolation thing; it just does it for you so you don't need to spend time fiddling with it. While it's strictly-speaking possible to use Scala for scripting without Ammonite, I'd suggest giving Ammonite a try first since it's easier. Once you get things working you can then decide if it's worth ripping out Ammonite in favor of a dependency-free solution
Wow, thanks for the thoughtful response. Good luck.
I think the tech community would be greatly improved if we just focus on the tech.
This does not cover the full extent of the experiments that we want to perform, but it sounds fun. If someone wants to give it a try by parsing a simple Scala app/library with Scalameta and translating it to Java, let me know, and I can provide guidance.
Why fork the language? JavaScript went down this path with Node vs IO.js, it was a huge mistake and once they rectified the fragmentation the JavaScript community was better for it. Why not put your efforts into making lightbend scala better?
The most important feature missing in javac is type inference. This approach will need http://openjdk.java.net/jeps/286 to compile common Scala source files.
Scalafix rewrites based on semanticdb can insert inferred type arguments and implicit arguments. It's not going to be easy, but it is within reach of current technologies.
Starting from a clean slate provides a new perspective on compilation performance, since this is something that Lightbend Scala cannot afford because of compatibility reasons. We believe that this experience will yield concrete practical insights that will be transferable to Lightbend Scala. More on this in https://www.reddit.com/r/scala/comments/6ubuix/twitter_announces_reasonable_scala_compiler_an/dlrxs6m/.
Cool project. A fully compatible compiler with togglable features (in exchange for speed), with a super fast "reasonable subset" would be terrific. 
&gt; It's pretty easy to ignore the differences and use both, or write your own typeclass interfaces and delegate to the lib you want to use, and it's good practice to have a layer between you and the lib you use, anyway, and you probably only use a small subset of the libraries abstractions. This sounds really annoying to do for foundational libraries like these, like having a layer between your code and the standard library.
I thought semanticdb is not available before typing, considering the goal is fast compilation by replacing scalac's typer.
Compile time and clunky tooling around Scala was the reason one of the teams I consulted for migrated from Scala to Java 8. The team mainly consisted of senior Java devs who excitedly adopted Scala because they worshipped the JVM. It was disheartening to see their excitement consistently drop. First, they expressed their hate for SBT and moved back to gradle for Scala. Then they moaned about IDE and tooling support. Finally, as the codebase grew, the long compile times delivered the final blow. Although I'm not doing Scala anymore, I personally look forward to this. For large codebases, if compile time can be reduced, limiting yourself to a subset of language features is feasible. We already have a coding guideline that governs codebases for consistency.
Sounds interesting, who do you work for?
Ugh. So if your builds are 10 minutes long (I assume just compile, not compile and unit test), you are in c++/c compile time land. This isn't always true, but the only times that has happened to me is when I had macros that were generating code in every test file. I consolidated the macro generated code to a module object and imported the generated values from there. That trimmed 9 minutes off of a 10 minute build for me. Incremental compile times in that project are now really low as well (6 seconds or so). Even with 6 second compiles, people who have coded in java recently complain that is too long.
Depends on the library and what is used from it. Most of the time, people probably only use like 1% any large foundational library. Delegating for that tiny slice will take you one day to set up initially, and gives you freedom to migrate going forward without having to change your own internal library code that is a client of the library. In scalaz/cats' case: For things that use Functor Monad MonadFilter Traverse for option and list and either -- that's a piece of cake. Other parts, like Free, are more difficult to delegate to, because the internals are private datatypes in cats, for example, and not worth dealing with.
Apollo Agriculture. I'm probably a bit biased as I'm one of the founders but I really love it.
It's not entirely obvious to me how Kentucky Mule proved that Scala type checking can be parallel. Is it really a surprise that if you take a language simpler than Java 1.4 (Featherweight Java?) it can be compiled really fast? I'm not sarcastic, I'm genuinely interested in your view. 
I always have this inner desire for Dotty to also follow a similar subset, which is to drop some rarely used features if they happen to significantly improve compile times. Compile times + SBT is one of the biggest pain points of Scala right now, and I have a fear that Dotty is too focused on compatibility for Scala2 that it will end up bringing these issues that are plaguing Scala right now. I would also want to encourage a mindset where performance = correctness. Yes we can ignore performance for certain things, but I believe for tools like compilers its critical. It also doesn't help that some of the community live in a bubble right now where they are ignorant of things like compile time, as /u/xeno has said in other comments there are so many issues that arise when you have insane compile times. We can see how productive people can be when we even have a very simple (and arguably stupid) language like Go with insanely fast couple times. Also with proper macros coming, we can avoid the heavily type code that is a common source of long compile times (this is why projects like doobie and circe are exploring using direct macro's instead of shapeless, its a lot faster to compile)
I care about it a lot. My dev cycle is pretty much: make a change, compile, find first error, fix, recompile and fix next error, etc. If the compilation step takes too long, it takes you out of your flow, you start checking Slack... TLDR: fast feedback loops matter.
Didn't JetBrains already figure out which parts of Scala could be compiled quickly?
Only if you trust them to have got it right. Seeing as they don't seem to understand parameterized type members (at least their IDE never can), I don't have a lot of faith.
I agree 110%, which was the point of my Twitter is 'not' apolitical comment. Goolag has done a lot of damage to itself and other like companies. Expect to see an impact going forward.
Parsing Scala into Java without typing would basically mean throwing out almost all interesting features.
outsourced vendor types, or does your co. actually hire these people?
I completely agree. To me that's OOP-kind-of-best-practice noise. If you use little bit, then rewriting it is piece of cake, where grep gets you 80% of way, and with help of compiler you fill the rest. If you use it a lot then you end up effectively rewriting whole lib you're using. I also wouldn't consider cats/scalaz particularly big libraries. I can relate to what Jackcviers is saying though, I truly can. I guess you can't religiously apply it to 100%. There are places where wrapping lib makes sense, there are places where I don't think so.
That's so cool. 
&gt;I'm not doing Scala anymore What are you doing instead?
I'm a bit confused. I have been told multiple times by Scala users that Scala compile times are a non-issue.
&gt; this is why projects like doobie and circe are exploring using direct macro's instead of shapeless, its a lot faster to compile In my team (in the same building as you, 8th floor) we had some nice applications for shapeless and we are using several libraries using shapeless such as doobie, circe and grafter. The compile times became so bad that we had to * give up annotation-based JSON codec derivation * hand-write type class instances for doobie * rewrite parts of grafter We are also not using shapeless-scalacheck because of the effect on compilation time. As much as I like shapeless I would currently recommend to use it only sparingly. 
For me, it's a real shame that you have to reason about compilation overhead of a feature _at all_. That's not a concern I ever had to worry about in Java, and I like having fewer concerns.
So the way I'd describe it is that a `Flow` is like a more general function. Much like a (tupled) function, it has well-defined input and output types. But it differs from a function in a few ways: 1. It has a lifetime that lets it process many items. 2. It doesn't enforce any relationship between its inputs and outputs, whereas a function is 1:1 (barring exceptions). So outputs can happen without inputs and inputs can disappear without creating outputs. 3. It has a materialized value that can carry a result, or some auxiliary value. If running a graph can be thought of as executing some process in its own sandbox, materialized values provide a way for data to get in and out of that box.
Yes, thread comments there state this support was dropped?
yes it is. 
1. Does JSR 223 work? I read comment around the net that this was dropped. 2. I looked into Ammonite, but feel a bit confused, how can I pass state (scala instances) to the script and read back new state? Passing control is ok and fine, but that's not what I want to do. Scripts must have access to a controlled subset of the applications' state. 
The premise of KM was that you might get away with a simple subset of the language to figure dependencies in typechecking and schedule parallel typechecking as a result. The side effect of looking at Scala typechecking from this angle is that you look at the cost of various language features. The surprising part is that scalac is slow to compile even simple source code where only the subset of the language is exercised. This suggests an implementation flaw that makes people pay the cost of features they didn't ask for.
It's a big company that acquired another one some time ago, that one has large offshore division. So tops and executives got their bonuses along with praises, and now I have to deal with the consequences of the crappy code and underqualified teams. At least they pay well, but I guess at some point the cost of curing my mental damage would overrun the shiny check I receive.
Neil is that you? 
It's not clear to me that it was dropped. Although it [doesn't appear to be working](https://issues.scala-lang.org/browse/SI-10058) in Scala 2.12. That doesn't really help you though.
Not sure about whether it was dropped, I vaguely remember using it before but it was a while ago. Slightly inconvenient to pass args into your script, but you can "pass" them in global (or threadlocal) variables. If you would find it valuable we could make it easier to pass in a set of `(name: String, value: Any)` tuples the way you can pass those into a embedded REPL, but that would be a bit more work
I relate. If your biggest bottleneck is compilation speed, how much time do you spend in questioning whether you need to write the code at all? How much time in business analysis?
Also note that this is not a bijection. All reasonable scala programs are compatible with scala but not necessarily all scala programs are reasonable scala 
But I'm still not clear how the premise was proven or disproven, since KM didn't show how the approach handles the hard cases. The last part isn't clear either. You mention a speed up of 1000x that you cannot explain. My guess is that such a dramatic difference is due to CPU caches, and won't hold for larger examples (or when both compilers do all the other tree transformations). I don't have a proof either, just saying that with the evidence presented there's many ways to explain it. 
This is an awesome idea. Hopefully it works well since they are using scastie, which at times has stopped working for me. 
Current project involves building a bunch of APIs for which node.js was a good fit. Codebase is in TypeScript for keeping things sane. : )
I was really disappointed that I had to cancel my pledge for financial reasons. I'm really glad this succeeded.
Independent of whether they are or are not political, this project seems like it may benefit Twitter as well as others considerably. I assume it will have a good open-source license, given that at least many of Twitter's Github repositories have good open-source licenses (mostly Apache 2.0 from what I can skim), so it seems like it would be safe to use and contribute to in that regard.
I believe they are, and not always in obvious ways. More discussion about this in: https://www.reddit.com/r/scala/comments/6ubuix/twitter_announces_reasonable_scala_compiler_an/dls1w78/.
Here's the comment thread that /u/mdedetrich is referring to: https://www.reddit.com/r/scala/comments/6ubuix/twitter_announces_reasonable_scala_compiler_an/dls1w78/
Thank you for the kind words! 
There has been a lot of focus on faster compilation times by Lightbend on Scala the last couple of years, especially lately, hasn't there? For instance [this discussion on benchmarking the Scala compiler](https://users.scala-lang.org/t/automated-benchmarking-of-scalac-and-performance-improvements-coming-in-2-12-3/954) and [this blog post regarding among other aspects compilation performance over time](http://developer.lightbend.com/blog/2017-06-12-faster-scala-compiler/). And they are also looking at [benchmarking Dotty](https://github.com/scala/compiler-benchmark/issues/29) as well. I also recall that Dotty would have or does have faster compilation times than current Scala, though I could be wrong about that.
Thanks! Your support is appreciated. Your interest is very important, no matter what material contributions you make. 
Looks like I have misunderstood you. What I referred to was basically a preprocessing step that uses already built semanticdbs to rewrite a Scala program into simple Scala. This way, we can experiment with rsc even on codebases that use quite involved Scala features. This, however, only works as a one-time transformation and will not cover the online transformation use case that you seem to have in mind. Generating semanticdbs currently requires running an entire Scalac typechecker, and that is not particularly fast.
I've been long on the lookout for such a library, and it doesn't exist unless you're willing to pay for WolframAlpha's API. I ended up switching to JavaScript libraries and using ClojureScript, but I suppose you could also use Nashorn. I liked Algebrite on the JS side.
I should mention the honorable fallback which is the Apache Common's math library
Bummer that there's no scala one, but I'll check out the apache one, thanks!
Symbolic algebra is pretty hard, so most people that come up with a good solution (like Wolfram) are doing it for a business. 
Yeah, my thoughts too. It wouldn't be terribly hard to make one in Scala, but the way that usually works is you want to plug one in to get some other project moving. 
There is [Symja](https://bitbucket.org/axelclk/symja_android_library/wiki/Home) which (despite the repo being named symja_android_library) is a pure Java implementation of symbolic algebra. We've also written [a PR for the Scala library breeze](https://github.com/scalanlp/breeze/pull/582) which adds compile-time (through a macro that uses Symja) symbolic computations on top of the available numeric computations. Progress on that has stalled a little, but in principle stuff like derivation and simplification already worked.
&gt; I also recall that Dotty would have or does have faster compilation times than current Scala &gt; I also recall that Dotty would have or does have faster compilation times than current Scala Last I heard Dotty is 2X faster than Scala without their yet having done any serious optimization work. Sounds like 3X faster may be in the cards.
My experience is that with incremental compilation you hardly notice compile times, it can be very fast. But, there are times, not too often in my experience, that modifying a file causes a large recompilation if that file's code is us​ed widely. I also find that when writing Scala you tend to spend more time thinking and write less code, so again compilation tends to not be an issue. Unlike Java which involves so much typing to achieve less.
I recommend you don't do this. Postgres and H2 are not the same and will not behave the same way between tests and production. Just use Postgres in test.
No problem. Hopefully there will be another way I can "pledge" in a couple of weeks when I get paid again. I wanted to do the tier where there would be no subscription costs (basically buy a life subscription).
[Scala](https://reactdom.com/blog/scala-books) is doing great actually. It really does make you write great and elegant code. 
We do that all the time. What issue are you having? Basically you just need to create a test database class that overrides the profile to use the H2 ones.
^ This is the best practice, not sure why you're downvoted.
Wouldn't it be better to start from Dotty as already-a-clean-slate?
Dotty is almost 5 years old, and it's quite big already. Some fundamental experiments (like Kentucky Mule-style typechecking) will require much more effort in Dotty than if developed from scratch.
I hope you manage to make progress with getting that in Breeze: that would be a very nice addition
Something like [this](https://github.com/typesafehub/activator-slick-multidb/tree/master?files=1) ?
Not disputing your statement. Can you please give examples of H2 and Postgres mismatches?
Nice article. I liked the HTTP Request/Reponse diagram. Per your final code, does GET /foo return an http 404/400? In other words, does `securedContent` apply to all paths with a GET Method or only the root GET?
Some friends and I have been working on [gdax4s](https://github.com/kramer425/gdax4s), a scala api for the gdax coin exchange. Looking forward to its completion so we can start on a trading bot.
https://github.com/twitter/algebird maybe?
https://github.com/playframework/play-scala-compile-di-example -- basically, you pass things in or use macwire.
I'm sure you can find a few yourself if you start looking around even a little bit. Postgres is a big, complex piece of software and its unreasonable to expect H2 to emulate it perfectly- or even very well. Seriously, just use Postgres in tests if you expect your tests to give you confidence that your code will work in production.
From the top of my head: arrays, enums, JSON(B) support. 
I agree strongly. What are you using for tests? We are using https://github.com/yandex-qatools/postgresql-embedded and it works, but it's slow as hell and often causes problems. 
You can do something like this object MyDb { def shouldUseH2: Boolean = ??? val profile: RelationalProfile = if (shouldUseH2) H2Profile else MySQLProfile val api = profile.api val db = profile match { case x: JdbcBackend =&gt; x.api.Database.forConfig("myjdbcprofile") case _ =&gt; ??? // If you have non-jdbc-based profile put DB construction here } } Then elsewhere (including your schema definitions, etc.): import MyDb.api._ Added: If you only need postgres and h2 - you can replace RelationalProfile with JdbcProfile and do val db = api.Database.forConfig("foo") 
Thanks for the reply. A good point. Do you have an example of an in-memory postures dB for use in tests?
We use a normal Postgres instance per machine, and tests share that database. However, each test creates a new, uniquely named schema in that database for isolation.
Why not just connect to a real instance instead of trying to use an "in memory" instance?
I just don't want my tests relying on external services, I want to be able to run them in complete isolation.
You can use schema naming or database naming to get isolation, but your application does actually depend on your database. It's ok to require development and test environments to have Postgres installed, and with the docker option it's not even that hard to get one installed.
&gt; but your application does actually depend on your database Well, ok, argument time - the whole point of slick is that it _doesn't_ really. Slick is a black box and I am happy to use the highest common divisor of features in both H2 and Postgres for the value it gives of being able to forget about the database entirely. Anyway, yes your point is a good one. There are additional concerns though, one is that I am one guy with a 'server' (an 8 year old laptop) in my living room with 2GB of RAM. I want as little infrastructure/devops as possible for this project. EDIT: I know docker is easy but it comes with 8 hours of configuring environment variables to deal with. That is not where I want my development time to go, I only get to work on this on half days at the weekend as it is.
I'm currently struggling with the same problem. Sadly, there is none so far. Zinc has just reached 1.0.0, so we can assume the API will be more or less stable. When it comes to resources, the best place is the [gitter channel](https://gitter.im/sbt/zinc-contrib). I've asked questions there before, and the people there told to me look at other build tools currently using Zinc. There is only [CBT](https://github.com/cvogt/cbt) and [Pants](https://github.com/twitter/pants) that I know of. CBT however uses reflection to call Zinc, which is less than ideal. That approach doesn't require implementing anything, besides making sure proper versions of compiler-bridge and compiler-interface jars are on the classpath at compilation time. Pants has a full implementation needed to actually use Zinc as a library. You need to handle stuff like caching and performing compile analysis, with which I'm currently struggling. This is the where some documentation would really be useful, because I'd like to actually understand what is required instead of blindly copying Pants's source code. TL;DR Pants source is all we have right now
This is what I was looking for as an introduction to category theory. Thanks! Made a few things click in my head now. I added a private note on the post, but I'll leave a comment here to see if anyone can shed some light on something I noticed: On the snippet completing the `composeM` function, are all the types properly specified? Not too familiar with Scala or the concepts presented in the post, but it seems to be returning some function taking `F[A]` as parameter, but the return type says that the returned function will take `A` as parameter. You also seem to do `f(a)` on the following line, but `a` doesn’t seem to be defined anywhere. There might be something I'm missing here, but that bit felt odd when reading the post.
What you say is true. That said, there are a few open source libraries which are fairly decent (I've used SymPy as an engine for a project and it worked pretty well) https://en.wikipedia.org/wiki/List_of_computer_algebra_systems They're not as polished as Mathematica (which has advanced algorithms like cylindrical algebraic decomposition), but they do definitely implement symbolic equation simplification and solution. Unfortunately I don't know of any open-source JVM CAS libraries that have quite the kind energy put into it as say something like SymPy. If someone were interested in porting something to the JVM, the SymPy codebase would be a good to start from, I think.
You are right, that is a typo, will correct it. Thanks very much for the feedback!
&gt; They are also error-prone, e.g. if you miss a “null” check, it’s not checked by the compiler, it’s also hard to spot during code reviews. That's why a language should have sum types and let `null` be its own type. &gt; For example, in the case of returning null, instead of simply specifying it in the documentation, we model it using the type Option You model *optional parameters* with Option not null because Option has an overhead at runtime and don't interop well with older languages' nulls. &gt; Let’s do the same with the throw-exceptions effect. We can return an Either[E, B] type - it's either the result B or some error E. You can't model or replace exceptions with Either - you'll have serious performance problems(let alone the boilerplate) when you work with indexing, division and similar operations(some errors are hard to catch and wrapping is expensive). Simple exceptions are generally unsafe and won't compose effectively. Option &amp; Either are not safe either but they help to *ignore* errors easily. What we need is *checked exceptions* + Option + Either + Try + sum-typed nulls. &gt; It’s time for category theory to enter the stage. Category theory studies morphisms between objects and the composition of them. Or let's just say simply that it studies the abstract groups of types. &gt; An realistic example of monadic compostion Two typos: "An" -&gt; "A", "compostion" -&gt; "composition". Anyway, that section's example is not good because that "long" code can be enhanced simply *and still handle errors efficiently*: instead of this -&gt; class Program(userService: UserService, fbService: FBService) { def likePost(sessionToken: SessionToken, postUrl: Url): Boolean = { val uid = userService.validateUser(sessionToken) if(uid == null) false else { val token = userService.facebookToken(uid) if(token == null) false else { val post = fbService.findPost(postUrl) if(post == null) false else fbService.sendLike(token, post) } } } } or this -&gt; class Program[F[_] : Monad](userService: UserService[F], fbService: FBService[F]) { def likePost(sessionToken: SessionToken, postUrl: Url): F[Unit] = for { uid &lt;- userService.validateUser(sessionToken) fbToken &lt;- userService.facebookToken(uid) post &lt;- fbService.findPost(postUrl) result &lt;- fbService.sendLike(post) } } you can have this -&gt; class Program(userService: UserService, fbService: FBService) { def likePost(sessionToken: SessionToken, postUrl: Url): Boolean = try { val token = userService.facebookToken( userService.validateUser(sessionToken)) fbService.sendLike(token, fbService.findPost(postUrl)) } catch { case e: NullPointerException =&gt; false } } Of course, neither of these handle invalid connections or anything else(but you can with the latter). And you need to check for nulls everywhere if you're going to use it but the latter won't put unwanted overhead(wrapping and additional abstract concepts) on your app. One way to avoid the overhead is to use something like `type ?[T] = T | null` which is of course not perfect(can't describe nesting) but that's where Option will go. Let's see it in theory: class Program(userService: UserService, fbService: FBService) { def likePost(sessionToken: SessionToken, postUrl: Url): Boolean = { val userService.validateUser(sessionToken) // ?[UserID] .flatMap(userService.facebookToken) // ?[FBToken] fbService.findPost(postUrl) // ?[FBPost] .map(post =&gt; fbService.sendLike(token, post)) // ?[Boolean] .orElse(false) } } &gt; In imperative programming, a great portion of the application code is repetitive effect handling in compositions, which is cumbersom, error prone, and very effect specific. This makes our application unecessarily complex, obscure and rigid to effects. Functional programming, powered by category theory, provides ways to compose components with effects in a concise and effect agnostic way. Such easy compositions make our application significantly more flexible, robust, and easy to read and reason with. You forgot about the RAM and CPU cost of wrappers and that imperative language can have advanced type systems too. Everything has drawbacks and it should be mentioned before ppl start to think it's the silver bullet. Of course, FP is efficient at modeling and transforming data and you don't need monads for that. Being productive is a thing FP excels at but I think being efficient and more safe should be the next goals of FP languages(+ to ignore weirdly-named abstractions). Edit: formatting
So another cornerstone scala project with no documentation. Scala-native, scalameta, Zinc, Dotty, sbt 1.0 server protocol. Gitter, for all the good it does, it also fosters that people don't write documentation anymore. Just come on gitter and ask the same quesiton for the millionth time, because we are not going to write documentation. In the end, it feels like the people that write these projects and use them are the same, they know each other personally and assist each other with integrating their projects.
&gt; Scala-native http://www.scala-native.org/en/latest/user/index.html &gt; scalameta http://scalameta.org/tutorial/ &gt; Dotty http://dotty.epfl.ch/docs/ Though all of these are probably not perfect: writing good documentation is not easy, and when you're deeply involved in a project it's not always obvious what needs to be explained to newcomers. I would encourage you to submit feedback on the issue trackers of projects when you find the existing documentation lacking in some respect, this is useful information for project authors!
Seriously: http://dotty.epfl.ch/docs/reference/changed/type-inference.html Not to mention that the entire doc of dotty appeared from one day to the next when they announced the first preview (at least that's how it felt, because I was checking regularly). Scalameta's one barely scratch the surface, and scala-native is the one I follow the more, thank you very much. Instead of giving me those links, of which I'm aware of, why don't you do the same for the other mentioned projects, of which I know nothing? &gt; I would encourage you to submit feedback on the issue trackers of projects when you find the existing documentation lacking in some respect, this is useful information for project authors! I find it hard to not be cynical in my response, since it's quite obvious that having 0 (the other projects you didn't link) documentation is not good. I appreciate your response though.
Nice article! A lot of other articles talk about category theory's concept of objects and morphisms but do not properly explain how it relates to programming languages. I like that you mentioned that CT objects map to types and CT morphisms map to pure functions that operate on those types and overall the types and functions make up the category. This is really well written. Thanks for taking the time to put this out there 
&gt; Seriously: http://dotty.epfl.ch/docs/reference/changed/type-inference.html Yes, on my personal TODO list :). In the meantime, I have a talk on the subject: https://www.youtube.com/watch?v=YIQjfCKDR5A &gt; Instead of giving me those links, of which I'm aware of I was reacting to your assertion that these projects have "no documentation". &gt; why don't you do the same for the other mentioned projects, of which I know nothing? Indeed, Zinc and the sbt server mode don't have documentation that I'm aware of and that's a pity, the best I can do is encourage you to ask the authors to document these projects. Note that the sbt server mode is not considered ready yet, which is probably why it hasn't been publicized much.
As was mentioned in the original post, the comparison is between a synchronous Go application, and several other asynchronous applications. The comparison is pretty much useless.
At this point I must mention documentation-driven development: https://gist.github.com/zsup/9434452
Do you have any links on your test/CI setup, please, for running tests against postgres locally?
Just had a problem testing insertOrUpdate because the SQL syntax is different...
It's all in house setups for a play framework application. It's not that much code, and its not top secret, but it would take a little bit more work to get into a state that would be useful to other people. I'll see what I can do about publishing a blog post sometime soon to get you started in the right direction.
&gt; Just come on gitter and ask the same question for the millionth time, because we are not going to write documentation. +1. But can I point out that Dotty is not yet really released, and its target audience *is* limited? Agreed on all the other projects.
In this case it applies only on root GET.
"Videos but no docs" confuse me, given how much preparation is needed for a video—I often end up with transcripts anyway. See also tweet: &gt; me: i am excited to read this article website: actually it is a video me: it's unfortunate that no knowledge exists about this topic https://twitter.com/Trillyana/status/897198499594477570 https://twitter.com/Wanderlustin/status/896935862717497344 But I promise I will finish the Dotty internal videos!
I think the general point being made by Kentucky Mule is that there are certain parts of the scalac typechecker which are embarrassingly parallel, however this is not realised because some scalac language features forces some interdependencies in the type checker which means it can't be "embarrassingly" parallel anymore 
I realise this, but this is sort of missing the point I am making here, which is that certain features of the Scala language do significantly increase compile times. So yes, Dotty being 2 or 3x faster than Scalac is great, but this relationship is probably not linear (and even if it is linear, 2x faster compilation on a code base which has terrible compile times isn't really going to cut it, we wan't to get down from something which takes 10 minutes to something which takes 1 minute). The main point is having a clear understanding of the compile cost of certain Scala features, and then having a rational debate about the cost of this feature
Yeah we also use Doobie and Circe, for Circe we ended up using https://github.com/circe/circe-derivation which is a lot faster than shapeless. Not sure about Doobie though, its used in another project and I believe its responsible for around 70% of the compile times of what is a fairly simple CRUD app
&gt; That's why a language should have sum types and let null be its own type. `Option` is a sum type in the sense the term is usually used. Non-disjoint unions seem like a bad idea if that's what you're suggesting - they're always going to violate parametricity. &gt; You model optional parameters with Option not null because Option has an overhead at runtime and don't interop well with older languages' nulls. The point (badly expressed) is that `Option` is a better way of modelling the possibility of returning an "absent" result for something like `Map#get`. &gt; You can't model or replace exceptions with Either - you'll have serious performance problems(let alone the boilerplate) when you work with indexing, division and similar operations(some errors are hard to catch and wrapping is expensive). In most day-to-day code you can replace almost all uses of exceptions with `Either`. The kind of performance-at-all-costs use case where it's worth using exceptions for that reason is a tiny niche. &gt; Option &amp; Either are not safe either but they help to ignore errors easily. They are safe in the usual sense of the term, ignoring partial methods that are easy to rule out via WartRemover or similar. They don't make it at all easy to ignore errors. They make it easy to propagate errors upwards and handle a broad class of possible errors in a similar way at higher level, which is a common pattern. &gt; What we need is checked exceptions + Option + Either + Try + sum-typed nulls. Strongly disagree. That would give an overly complex language with too many ways of doing the same thing. Either covers the overwhelming majority of use cases for checked exceptions or Try. Option covers the overwhelming majority of use cases for sum-typed nulls. And both checked exceptions or sum-typed nulls would be language-level features that readers would have to understand - the bar for that should be much higher than for plain old library types that behave like normal types in the language, as Option and Either do. &gt; Anyway, that section's example is not good because that "long" code can be enhanced simply and still handle errors efficiently: It can, but only with the help of language-level features (nulls and exceptions), and in a way that can't generalize to other types. Whereas with `Option` you can do this in "userspace", you can even reimplement your own option-like type rather than using anything builtin (ScalaZ does this), and you can generalize the code to work with other types that compose in the same way. &gt; One way to avoid the overhead is to use something like type ?[T] = T | null which is of course not perfect(can't describe nesting) but that's where Option will go. The limitation is a major one; I don't believe in compromising the language design for the sake of a small performance improvement in a few cases. Better to keep the semantics of `Option` the same, and if necessary then implement it in an unboxed way where appropriate as a performance optimization (Rust does this). &gt; You forgot about the RAM and CPU cost of wrappers There's no inherent reason these types have to be implemented as costly wrappers - and even when they are, the cases where those costs are relevent are extremely rare. &gt; Everything has drawbacks and it should be mentioned before ppl start to think it's the silver bullet. Misleading. If you dig hard enough you can probably find something that qualifies as a drawback, sure (indeed you have, with this "RAM and CPU cost" business). But some things really are very positive on the whole, and it's much more constructive to make active recommendations rather than just "here's a different way of doing it, some prefer this, some prefer that". &gt; Being productive is a thing FP excels at but I think being efficient and more safe should be the next goals of FP languages FP is already excessively safe compared to what the market seems to want, and more than efficient enough in practice, IME (I've seen plenty of FUD about FP being "slow"; I've never seen a Haskell system have significant trouble meeting its performance goals (nor a Scala system if we're talking about throughput - though I have seen those that had trouble with startup latency or GC pauses)). Development time is still the biggest cost; I think that's still the right focus for the time being. &gt; (+ to ignore weirdly-named abstractions). Many of these abstractions are immensely useful. I'd genuinely love to see better names for these things, but in the absence of a positive proposal with real commitment behind it, consistency with existing documentation is better than some ad-hoc name of my own.
&gt; Scalameta's one barely scratch the surface I totally agree we can do better on the documentation side. Which parts would you like to see expanded?
Afraid I don't have public examples, but I'd tend to use plain old code (that's the whole point of avoiding Guice to me), so I'd usually just have a factory function, or a *lightweight* cake-pattern-style way of doing things where I mix in traits to form my "DI container". So what I'm doing in my current project would look something like (I don't know the Slick API): trait DatabaseDriverModule { def databaseDriver: DatabaseDriver } trait PostgresDriverModule extends DatabaseDriverModule { final override lazy val databaseDriver = ... } trait SlickModule extends DatabaseDriverModule { final lazy val slickThing: SlickThing = new SlickThing(databaseDriver, ...) } trait SomeServiceModule extends SlickModule { final lazy val someService: SomeService = new SomeService(slickThing, ...) } object MyApp { def main(args: Array[String]) = (new SomeServiceModule with PostgresDriverModule).someService.someMethod(...) } //test code trait H2DriverModule extends DatabaseDriverModule { final override lazy val databaseDriver = ... } class SomeServiceTest { val someService = (new SomeServiceModule with H2DriverModule).someService @Test def someMethod() = assertThat(someService.someMethod(...)).isEqualTo(...) } 
I understand that's the goal and claim. My point is that if your simplifying assumptions are so strong that they lose connection to the essence of Scala, said claims are not about Scala anymore. And the language that's used for type-checking is simpler than Java (no generics, for instance), hence my doubts. Ultimately, the focus on compiler performance is beneficial for all Scala users, and Twitter is free to invest their money and time in whatever they like.
Sure, but I don't think that Kentucky Mule removed so much of Scala typechecker that it basically turned into Java, of course the devil is in the details though
It made it!
Yeah, it got two big contributions from IBM and Lightbend. Can't wait for it.
At this point I must also mention (in this vague direction) Feed me READMEs. Just discovered, looks very cool. No, I'm not sure this can help the original issue—but maybe yes. https://github.com/LappleApple/feedmereadmes/ https://twitter.com/feedmereadmes
Yes I think that was the joke. This entire project seems to be a bit of a waste of time - if you're willing to throw out scala features for compile time savings just use java.
This is great - thanks for sharing!
I skipped through it, he mentions tools, talks vaguely about functional programming, a few slides mention Haskell and Clojure. Didn't catch where he specifically talks about Scala anywhere.
Starts at 17:45 
The Scala comments start at 17:45. 
He doesn't criticize Scala in a production environment, he simply claims that Scala is a terrible language because it allows mutable state and side effects, in order to be compatible with Java. He then mentions offhand that it is OK, if you just want something better than Java... It is the same old functional purity argument. That said, it was an interesting talk.
OCaml and F# are exactly as impure as Scala, but he gives them a pass.
&gt; Aaron is founder and CEO of FP Complete, the software technology firm commercializing the Haskell language for Functional Programming. from [LinkedIn](https://www.linkedin.com/in/acontorer) He's entitled to his views of course but one might be forgiven for wondering how objective they are. IMO Scala's problems don't stem from allowing immutability because in practise it's used sparingly. The main problems are weak tool support, compiler speed, and some hangover from early implementation choices which are in hindsight regrettable. All of this is improving, but I'd like it to be faster. From a devops point of view the scala world is far easier to deal with than many others. Try dipping your toes into the node.js ecosystem if you want to feel better about it!
I get the impression for some reason he hasn't actually worked with Scala(almost certainly not within the last half decade) and is working off opinions he heard second-hand.
Haskell consultant says scala isn't very typesafe. No surprise there.
His critique though are not about immutability. He specifically mentions cryptic unreadable syntax of scala. Where he says that coming back to the scala code you wrote yourself you cannot be sure anymore what it means. 
Does he have any industrial experience?
Just because he says it, doesn't make it a critique. The cryptic syntax adage gets repeated ad nauseam because it's thought terminating. It can't be demonstrated concretely, no two persons will agree ever to what constitutes as an example of cryptic code. There's a similar situation with the C++ comparison, it vaguely implies as if there's some merit to reductivist programming languages, which is hypocritical if you're simultaneously advocating for any modern strongly typed programming language. But it gets repeated because the argument is thought terminating because no one agrees what baggage C++ carries from C holds the language back , or if C++ ruined C with extra features. EDIT: The C++ argument is also further meritless because it attempts to relate Scala to Java as if Scala is a superset of Java like C++ is a superset C, instead of it being an entirely different language.
&gt; His critique though are not about immutability. He specifically mentions cryptic unreadable syntax of scala. Where he says that coming back to the scala code you wrote yourself you cannot be sure anymore what it means. His critique is focused on mutability and side affects, he mentions syntax only in passing. He describes Scala as to Java as C++ to C, and says it's a great language to write bugs in in the same way. The syntax criticism is old and not really relevant these days, newer versions of libraries and tools have made strides here. In reality the mutability concern is smaller than he makes it out to be, and is balanced by both the advantages of the JVM and java ecosystem and the type system, neither of which are really mentioned. I don't think it's a very informed view of Scala but the purpose is to promote pure FP in particular Haskell so that's not too surprising.
Show a piece of Haskell code written by a category theory expert to someone only vaguely familiar with Haskell and ask them how cryptic and unreadable they find it...
IMO, Scala is definitely the C++ of modern times. Not that it is a superset of Java, but that it took the dominant language ecosystem of the times, and bolted a trendy paradigm onto it. Scala is a gateway drug for imperative programmers to dabble in FP; like C++ enabled C programmers to do OOP. Like C++, Scala is hated by the old guard, who fear and dislike the new features ("Too slow! Complex! How will we find people who know this new language?"). It is also hated by the FP purists, because it isn't pure enough, just like the Smalltalk people hated C++ and Java.
His critiques make zero sense, I mean sure there its possible to do mutation of state in Scala, but.... - Clojure and Ocaml (which he mentions often as recommended languages) are just as bad as Scala in this regard - Nothing is stopping you from mutating state in Haskell either, and their FFI + common linking with C poses the same problems that Scala has when linking with Java - Clojure also has seamless interopt with Java which poses the same problems that Scala has - Clojure is even weaker than Scala in this regard (although both languages use immutable collections as a default) - Typical Scala code minimizes side effects, and if you do PFP then even these side effects are encapsulated (see cats/scalaz) None of his commentary on Scala appears to be rational compared to the rest of his talk, it sounds like Scala kidnapped one of his kids.
&gt; IMO, Scala is definitely the C++ of modern times. Not that it is a superset of Java, but that it took the dominant language ecosystem of the times, and bolted a trendy paradigm onto it. Scala is a gateway drug for imperative programmers to dabble in FP; like C++ enabled C programmers to do OOP. Except that this isn't true, and has been stated by the language designers many times. Scala isn't a true superset of Java (i.e. constructor overloading is more limited in Scala), and a lot of the design decision in Scala are deliberate (i.e. Scala does deliberately have subtyping because its the only proper way to model modules in a language)
But we are not talking about scala code written by category theory expert. We are talking about scala code written by normal "enterprise" developers. I use haskell in production for more than 5 years already, and no one in my team ever had a problem reading any code in our projects. I cannot say the same thing about scala. 
I think the confusion here is that he critiques scala right after talking about mutability, so the impression is that he blames mutability in scala. But it is not the case. His main scala critique is about cryptic language, that has nothing to do with mutability. That's why he recommends clojure and F# as a much more **readable** and **simpler** languages. Mutability in this case does not play any role.
&gt; Option is a sum type in the sense the term is usually used. Non-disjoint unions seem like a bad idea if that's what you're suggesting - they're always going to violate parametricity. How? It can't be a comonad(what a loss \s) or things like that because you can't nest it without loosing information but that's fine most of the time. &gt; The point (badly expressed) is that Option is a better way of modelling the possibility of returning an "absent" result for something like Map#get. Option is only better (at the case of nesting) if your typesystem don't differentiate nullable types. &gt; In most day-to-day code you can replace almost all uses of exceptions with Either. The kind of performance-at-all-costs use case where it's worth using exceptions for that reason is a tiny niche. Nonsense, constant wrapping and unwrapping is way too expensive. This is why the code of pure FPists are really bad at performance critical and real-time cases(that's why FP has a hard time with gaming and such). However, if the language has zero-cost abstractions then that's another story. &gt; They are safe in the usual sense of the term, ignoring partial methods that are easy to rule out via WartRemover or similar. They don't make it at all easy to ignore errors. They make it easy to propagate errors upwards and handle a broad class of possible errors in a similar way at higher level, which is a common pattern. That's not safety, I'm talking about "overheadless"+safe *resource management* and concurrency. &gt; Strongly disagree. That would give an overly complex language with too many ways of doing the same thing. Welcome to scala. How about all those weirdly-named categories? Aren't they complex? They are. And they also bring a lot of overhead. &gt; Either covers the overwhelming majority of use cases for checked exceptions or Try. It doesn't cover anything because if you'll try to handle indexing, IO or arithmetic exceptions your code will run like shit and you'll loose productivity for no reason. &gt; Option covers the overwhelming majority of use cases for sum-typed nulls. Only when you need nesting. &gt; And both checked exceptions or sum-typed nulls would be language-level features that readers would have to understand - the bar for that should be much higher than for plain old library types that behave like normal types in the language, as Option and Either do. How so? Checked exceptions are pretty easy to understand(see in java) and sum-typed nulls are far easier than how Option is defined in Scala. &gt; It can, but only with the help of language-level features (nulls and exceptions), What's wrong with that? &gt; and in a way that can't generalize to other types. No need to generalize for other types. &gt; Whereas with Option you can do this in "userspace", you can even reimplement your own option-like type rather than using anything builtin (ScalaZ does this), and you can generalize the code to work with other types that compose in the same way. ...and bring even more overhead: it requires the introduction of more weird concepts and runtime wrapping... &gt; The limitation is a major one; I don't believe in compromising the language design for the sake of a small performance improvement in a few cases. Better to keep the semantics of Option the same, and if necessary then implement it in an unboxed way where appropriate as a performance optimization (Rust does this). That's not a "small" performance improvement and you can guess that from the amount of useless function calls and unnecessary pointers Option introduces. Btw, I was thinking about the Rust method too but *there's no way to do that on the JVM.* &gt; There's no inherent reason these types have to be implemented as costly wrappers - and even when they are, the cases where those costs are relevent are extremely rare. 1. Yes, there is: binary compatibility; 2. they're always relevant but you can choose to ignore but then we'd be script kiddies. &gt; Misleading. If you dig hard enough you can probably find something that qualifies as a drawback, sure (indeed you have, with this "RAM and CPU cost" business). There's no need to dig at all - the costs are obvious. &gt; But some things really are very positive on the whole, and it's much more constructive to make active recommendations rather than just "here's a different way of doing it, some prefer this, some prefer that". This is nonsense: those one-size-fits-all solutions are usually crap and that's one of the reasons you like scala, right? &gt; FP is already excessively safe compared to what the market seems to want, No, it isn't: see linear and dependent typing. Most FP languages(especially those in the industry) don't have them. &gt; and more than efficient enough in practice, Scala is fast but I still hear ppl complain about it and I understand why. &gt; IME (I've seen plenty of FUD about FP being "slow"; I've never seen a Haskell system have significant trouble meeting its performance goals (nor a Scala system if we're talking about throughput - though I have seen those that had trouble with startup latency or GC pauses)). I did see slow haskell and scala programs - and the problem is not just "being slow" and "lagging" but they introduce a lot of useless pointers and the GC will need to do more work. &gt; Development time is still the biggest cost; I think that's still the right focus for the time being. Why would we need to give up productivity? Also, I think safety is more important: I wish scala would have uniqueness types and more rules enforced by the compiler(even stolen from linters) but it'll never become true thanks to the jvm... &gt; Many of these abstractions are immensely useful. Yes, and many of them are just plain useless and only serve those who are really bored. It doesn't mean that you should use all of them and you should give them weird names just for fun. &gt; I'd genuinely love to see better names for these things, but in the absence of a positive proposal with real commitment behind it, consistency with existing documentation is better than some ad-hoc name of my own. That's a bad attitude. By introducing and keeping idiotic naming standards and complex symbols the average programmer will never touch category theory.
Hmmm, I'm not sure I understand that. A project, from the moment it's opensource, even if it's preview or experimentation, all the more reason to have it documented, so other people can chime in, test, and provide feedback. I'm not sure I'd agree that Dotty is not released, or that the target audience is limited, surely I agree that it's not production ready, but they even advertised it in the scala blog.
Didn't I say "Not that it is a superset of Java"? The point of the language was to make a hybrid OO/FP language. Regardless of the "goals", Scala is comfortable for Java programmers, because we can drop back into imperative style any time we want. This makes it a good transition language for imperative programmers that want to learn FP.
Great article, thanks!
well actually we use a single connection in tests (vs. pooled in real). This gives the benefit that we only need to connect once to our database. Now you say that might be slower? well it isn't because we load all schema once and fill it with data. Then we patch the JDBC driver to actually run every test inside a custom savepoint and never call commit. So each Test/Query withConnection will create a SAVEPOINT and ROLLBACK after each test or after each Failure. This is blazingly fast, but does not work with deferrable initially deferred and you need to reset your sequences after each test. however I think if one, wants to parallize tests, one can create 4 schema's and then do the same x times. But after we started transactional tests our test suite only took half the time. From 10-15 minutes to a stable 5 minutes (including compilation)
I'd like to use the semantic api as a replacement for scalac's presentation compiler, is this doable? intended usage? if so, how? In the documentation I can read about scalahost, but there's no explanation whether I can use the semantic api myself. Does scalameta integrate with Zinc in order to achieve the semantic api? (this goes back a bit to the fact that Zinc has 0 documentation). What's the status on new macros style with Dotty? Performance characteristics? after my own personal tests I can see that Tokenizing is reasonably fast, but has strange cases that I can't explain, that make adding 1 character to a text, chance the tokenizing time from &lt;1ms to 6-7ms, if I then remove that character, it goes back to &lt;1ms, if I try the input with that character added back, it remains &lt;1ms, so it would seem it's doing caching somewhere. What about parsing, what considerations are there to be had? Not really documentation related, but since I'm asking: I thought I heard once that newstyle macros would remove the limitation of two tiered compilation, from the current docs I see this isn't so? I had high hopes here :( . 
But the said "purist" recommended in his talk Clojure and F#, so your argument does not hold. In fact as a haskell dev, I love working with clojure despite of it being both impure and dynamic, yet I hate scala with passion. It is over-engineered, overly complex, and allows easily producing very obfuscated code. 
I assume you meant disheartening ;)
Consider for example you have a function e.g def doSomething(x: String, y:Int) and you have a t:Tuple2[String, Int], why is it not possible to simply just call doSomething(t), but rather have to do doSomething(t._1, t._2)? It seems like with a type check, this should work. 
Because Tuples are really just case classes. Although a compiler plug in could be written to allow it, maybe with doSomething(t:_*) 
Functions have a set number parameters and you have to match the number of parameters when you call it. In this case you have two parameters but you are only passing one `Tuple2` argument, so it's not even a candidate. In principle you might be able to do automatic tupling and untupling, but I have a suspicion it would end up being somewhat limited and lead to inconsistencies. For instance, if you have a `(String, Int, String)`, and two overloads for a function `def f(s: String, p: (Int, String))`, and `def f(p: (String, Int), s: String)`, how do you choose which to call? Or if you wanted to call `f("a", 1, "b")`, which overload is chosen? Maybe you say, "you can only call a function if the tuple exactly matches the arguments", but that gets you pretty close to what's already available with an explicit call to `tupled`. def doSomething(s: String, i: Int): Unit = { println(s -&gt; i) } val ds = (doSomething _).tupled val p = "test" -&gt; 1 ds(p)
Notably, you can now do this in Dotty: scala&gt; def doSomething(x: String, y: Int) = x * y def doSomething(x: String, y: Int): String scala&gt; val t: Tuple2[String, Int] = ("hello", 3) val t: String, Int = (hello,3) scala&gt; doSomething.tupled(t) val res0: String = "hellohellohello"
Why post Scala hate in the Scala subreddit? I did not have any problems with readability of recent Scala projects. That includes algebra, cats, circe, spire where I made contributions. My only current pain point is SBT, but that's more due to the complexity of the modelling (and of the underlying domain) than the few symbolic operators still used. And in the last 4 years, I was able to go from imperative programming to understanding pure FP style in Scala, to the point I understand how to write equivalent code in Haskell. Whereas I got stuck writing Haskell code before, because the programming model was so foreign.
Ahh good point about overloading. It would have been a nice little plug to have otherwise. 
He praises Haskell as the best FPL for productivity, performance and quality and is misinformed about why F# isn't popular. Also calling Scala just a better Java and comparing it to C++ is disingenuous. https://youtu.be/ybSBCVhVWs8?t=952
Its funny that this article has 70 upvotes in the haskell sub-reddit and not so much in /r/programming /r/fsharp or even here. I don't really understand the point of these functional purists. To me scala is the real sweetspot. Better than Java considering just the OOP part. Has good/enough functional support for one to write functional code. Maybe its not the best language for pure FP, but my question is pure FP worth all the trouble? Aren't some programming problems naturally suited to imperative/OOP style and some to functional style. This is the very idea on how scala was built. 
typical CEO doing his "sales" pitch
What percentage of your code base is functional? I have always believed that certain percentage of problems are well suited for FP and some for OOP. In what situations would you use one over the other.
&gt; It is over-engineered, overly complex, and allows easily producing very obfuscated code. Like every FP language, especially clojure... I don't know why you clojure/haskell fanboys think your language is so special but as I've seen you're very vocal and subjective. There are very - I mean very few reasons to use haskell or clojure thanks to all those warts and design failures(+industrial failures) and when you start to argue against scala your entire arguments against it boils down to "scala is so complex yay!" while haskell is very deep in the monad-kitchen-sink and clojure can't stop introducing lame solutions to avoid having a type system. Both languages have worse tooling than scala and both languages have a much smaller adoption in the industry. So, if I'd need to guess it's just pure insecurity/jealousy from your part because it's the most widely used FP language.
When I was studying at University I had a class where I was taught some principles around this subject. A few years later (I mean... now) I started learning Scala and the concepts I learned came to my mind and helped to understand some things, although I still need to review the lecture's book again. I don't know how useful it will be to someone that never attended to the classes, but in any case the book is here (it's public, that's why I am posting) if anyone wants to have a look: http://www4.di.uminho.pt/~jno/ps/pdbc_part.pdf
Great questions! I'll answer here and follow up to update our FAQ. &gt; I'd like to use the semantic api as a replacement for scalac's presentation compiler, is this doable? intended usage? It's not doable and it's not the intended usage. The prime application we are focused on at the moment is refactoring with scalafix. &gt; In the documentation I can read about scalahost, but there's no explanation whether I can use the semantic api myself. Have you seen? - http://scalameta.org/tutorial/#Installation - https://github.com/scalameta/sbt-semantic-example Alternatively, you can try it out by implementing a scalafix rewrite here - https://scalacenter.github.io/scalafix/#scalacenter/scalafix.g8 &gt; Does scalameta integrate with Zinc in order to achieve the semantic api? The Scalameta Semantic API is enabled with a scalac compiler plugin. Its design has been made to accommodate incremental compilation to play nicely with Zinc. Beyond that, Zinc and scalameta and unrelated/orthogonal. &gt; What's the status on new macros style with Dotty? See https://github.com/scalacenter/advisoryboard/pull/30 It was opened just this morning. &gt; Performance characteristics? Scalameta is at the moment not heavily optimized, but it hasn't been a problem for our intended use-case so far. Scalac compilation is still the bottle-neck. If parsing/tokenization performance becomes a problem then we will look into how to improve that. &gt; it would seem it's doing caching somewhere. Correct, and it's evil and should go away https://github.com/scalameta/scalameta/issues/1068 &gt; What about parsing, what considerations are there to be had? Can you elaborate? &gt; I thought I heard once that newstyle macros would remove the limitation of two tiered compilation, from the current docs I see this isn't so? Separate compilation will continue to be a requirement in scalamacros, sorry about that. The major focus of the new macro system is portability (same implementation running on scalac + dotty + intellij). EDIT: As promised, added your questions to the docs https://github.com/scalameta/tutorial/pull/25
&gt; Didn't I say "Not that it is a superset of Java"? But you said it "bolted a trendy paradigm onto it". I don't think you can say that. It implies that Java would be some kind of "base" where something was put on. But Java is not the base. The same runtime is (or was) but that is a different thing. Just stick with OO/FP which is true, but better not use Java for that argument.
Except that this isn't true, its one of his criticisms but his main ones are actually about mutability and purity. In his talk he emphasis purity (i.e. side effect free functions) ad naseum, yet the other language examples he uses (barring Haskell) are **worse** in this regard compared to Scala
Haskell has code which I consider some of the hardest that I have ever read, only really complex memory management code in C++ holds that title as well. The C++ is the only example which I think should be excused from having hard to read code seeing as doing memory management is **incredibly hard** problem to do. The very frequent overuse of both operators and terminology which only people that are very familiar with category theory can understand plus the fact that most Haskell libraries aren't properly documented (their documentation is just auto generated from code which isn't good enough) contributes to this. Clojure is quite easy to read, but it has all of the same problems that Scala has due to Java interopt (and Scala is actually trying to remedy this, i.e. see scala-native). Not trying to dismiss Scala's issue, it definitely has them, but don't go around throwing rocks in a glass house.
Exactly. Kotlin is an example of a language that has a "bolted a trendy paradigm onto it" (with it referring to Java). Typescript is another example from Javascript land
Where is the link to the Haskell subreddit?
Check other discussions on top just below the scala logo.
Thanks!
An important excerpt from one of the developers of gRPC in that thread. &gt; we have historically spent more time and effort optimizing C++ followed by Java then Go. The grpc go team is now turning its attention on performance optimization and we are starting to see good gains. There is actually a blog post detailing our work so far and plans, we will publish it this week but you can get a preview here - https://github.com/grpc/grpc.github.io/pull/549. 
Considering the ecosystem, they are actually even more impure in Scala. OCaml actually advertises the fact that they don't use monads as persuasively as Haskell, and F# has limitations in this regard because they don't have HKT's (due to having to interopt with the CLR)
All threads here: https://www.reddit.com/search?q=url%3AybSBCVhVWs8
What about linking that video from the stub page? That would help with lower effort than writing the real docs ;-)
PR welcome ;).
Should the homepage direct a first-time Scala user to try out Dotty? I'd say not yet. Should Dotty hackers (not a big team) work on Dotty docs or on making it more production-ready? Right now, I feel the latter. And probably people who can write docs should focus on stuff which *is* supposed to be production-ready now (as you point out). Do you want feedback from first-time users? At some point yes, and that's before release, but if there are too many known and unfixed sharp edges, that'll waste the time of the user who'll find the same sharp edges. Of course, even then, more docs is better all other things being equal, but resources are limited. On the other hand, docs for released products are clearly insufficient—that's a problem. And it seems continuing with the current process will probably produce the same result—we need some action to avert the danger. Also, I'm happy to be shown wrong on all these accounts—lots of what I said on Dotty might be a fallacy. I'd love some compelling rebuttal. (Not that you need to change my mind, I'm just a passerby).
I'm pretty sure it's not getting upvotes here in Scala because of the bait-and-switch clickbait title. Had it been titled to be a video about functional programming, and not specifically a critique of Scala where non exists it would be better received.
&gt; Is pure FP worth the trouble? The problem with any undeclared side-effects or escape hatches is that just one side-effect allowed is enough to undo any referential transparency in a pure program. One throw can make it impossible to compose one function with another. One file read or write can also cause this. In order to accommodate escape hatches the abstraction (functions) has to be abandoned. This raises the unit of abstraction to something else - a module, a class, a type, a package, all the way to possibly an entire program. Functions are simple objects - they have a limited number of argument's, the argument's have static, reified types, all the argument's are non-null, all the argument's are immutable, all types are present in the type signature, possibly all capabilities for all types in the type signature are declared in the type signature. This means you have all the information necessary to understand a function body at the site of declaration. You don't need anything else. Obviously, if you make the unit of abstraction larger by using side effects, you now must understand all the functions in the module in order to understand the fundamental unit of abstraction. This raises the minimum number of parts to understand by at least one (the side effect has to go somewhere). It is more complex because it has more parts. Complexity is what causes programming errors. Errors cause productivity to fail. Eventually, programs with side effects become so complex that you have to be an expert in the program to understand it, rather than an expert programmer. FP concepts keep interfaces small. They manage the complexity of the program by trading off more complex function interfaces for less complex overall programs. They trade off complex composition (you must map to unwrap and transform my value by passing a function to map) and names for smaller groups of abstractions. They trade encapsulation for declarative interfaces. They use mathematical names because it makes it easy to Google them. There's lots of outside help available to understand programs that are lawful and functional. It's all in the types. OOP aims to hide and encapsulate complexity to manage it. However, you can never truly know what code you are calling is doing without jumping into it at runtime. This makes it more difficult to compose OOP programs than functional programs. You have to be an expert on a larger volume of the code. There's a little outside help available outside of some patterns books. There are no certain laws to follow. Imperative programming makes no attempt at managing it - that's the job of the programmer. You have to be an expert in the program - there is very little outside help available. EDIT: Forgot to type my conclusion: Conclusion: If programmers want to manage complexity, then pure FP is the simplest (has fewest parts), paradigm for doing so. If managing complexity is the goal, it is the only paradigm that solves the problem in any sort of verifiable way.
hsaliak: *The async here refers to explicit async APIs that C++ and Java provide. gRPC-Go does not provide an async API. goroutines are used under the hood to achieve concurrency.*
Does the fact that twitter is stuck on an ancient version of Thrift not cause you problems?
Great stuff John, it's good to read such a write-up. It is really fun to see how, after all this years, we still find innovative and improved way to encode our favorite abstractions in the Scala programming language. I'm really enthusiast about this next major release, also glad to see you becoming part of the contributors team :) Scalaz, the never-ending story!
No? It's modern enough to support circular data types; there's nothing I miss that I'm aware of a newer version having, though I guess I wouldn't know what I've never seen. 
Thanks! 👏
Kudos to the Scalaz team for their efforts. Some observations in regards to this article: Related to those benchmarks, I have yet to see any source code, even though [I asked for it](https://github.com/scalaz/scalaz/issues/1399#issuecomment-315412582) a month ago and it's nonprofessional to publish benchmark results without giving the means for third party verification. For example when the FS2 / Http4s folks published benchmarks, they did so in the open, in a clean repository, which was cool as it gave me the possibility of verifying their claims and [release a new version](https://github.com/rossabaker/benchmarks/pull/4). Note that Monix on master has improvements for `attempt`, to be released in 3.0.0, although hash versions are already published. I have no idea what version the benchmarks in the article used, because the article doesn't say. On the modularity aspect, I disagree with John's argument. Of course there is balance to consider, too much modularity hurts, however to give Cats as an example, we now have: - `cats-core` - `cats-effect` - `cats-mtl` These have different release cycles, at least for now, because `cats-core` is mission critical and contains the *bare essentials* one needs for FP in Scala, i.e. Monad, Applicative, Monoid, Eval and related types. Having this library small and with its own release cycle, when compared with a more experimental area like `cats-mtl`, is important for stability. Frankly until now Cats doesn't have a good story for stability, but at least efforts are being made to give some semblance of reliability in that regard past 1.0 and I hope we can deliver. Because really, Scala as a platform is absolutely awful for long lived code and the Scalaz library has been historically part of the problem, breaking compatibility on each of the 7 "epic" versions it had until now and every "major" version in between, thus 7.2, being incompatible with 7.1, being incompatible with 7.0. Also on usability, speaking of `cats-effect`, modularity means providing the needed type classes as to not force people to settle on a single data type, in this instance the `IO` data type. Having `cats-effect` means that libraries can abstract over the `F[_]` data type that's in charge of evaluation, such that more daring alternatives, like Monix's Task, can thrive alongside the very classic and simple, but reliable `cats.effect.IO`. And libraries are doing just that, with work being done for FS2 and http4s to abstract by means of the type classes in `cats-effect`, while Monix will expose a new data type doing that as well. Cats's innovation here is that it encouraged an ecosystem to exist and play nice, having diversity in [Monix](https://monix.io), [Freestyle](http://frees.io/), [FS2](https://github.com/functional-streams-for-scala/fs2), [Http4s](https://github.com/http4s/http4s) and many more, libraries that aren't part of Cats per se, having their own style and personality, competing with each other, while at the same time playing nice. And it also plays nice with Scala's standard library. Yes, we've got [dogs](https://github.com/stew/dogs/), implementing immutable collections for Cats. Interest for it is low though, even though I personally love this small project. So why should the cats-core be a dumping ground for data structures that people don't want? And this is the direction I feel that's worth pursuing, because about 4-5 years ago when I joined Scala's community, the norm was big, monolithic libraries and frameworks that did not compose well with other libraries and frameworks. We laugh at Node's ecosystem, we scorn at LISP communities for not doing static typing, yet those people know the value of small, composable libraries, a virtue that we have yet to learn.
I cannot believe he says Haskell is fast. He also says "Haskell makes fast native code as opposed to JVM or .NET code". What a load of hogwash. "Sorry for OCaml that it hasn't been adopted more". I'm still seeing far more OCaml in industry than Haskell. This guy is a former program manager with a degree in cognitive psychology. What he lacks in technical knowledge he makes up for in marketing. Specifically, speaking about Haskell at 7CTOs and advising them to embrace Haskell is a great way to drive adoption. 
Hi Alex, Some very good points in your message! Thanks! I'll just give a small personal opinion about modularity... I think the approach you are describing will lead to massive usuability issues for non-expert. In the scala teams I did work in the past, the biggest point of frustration with Scalaz 7 is definitely dealing with imports... I feel so embarassed everytime someone ask about it, this is terrible. Even though I never worked with a team using Cats, sadly I don't feel the situation is much better as it can be seen here it's roughly the same: http://eed3si9n.com/herding-cats/import-guide.html There is here what can be considered implementation details of the typeclass encoding that are leaking to the user face. Now let's think about the increased complexity when you have to deal with imports of every specialized sub modules (mtl, effects, fs2, monix, ... you name it). You are an experienced hacker, and you did contribute to this projects so you know by heart what to do... but this kind of thing is a massive issue for newcomers. If I want to write perfectly modular and purely functional librairies, I would definitely use an other language than Scala. It's a tradeoff, and I don't see modularity of my base library (for me mtl is as mission critical as the rest) taking part of it in the context for which I use Scala (entreprise application)... I take usuability instead anytime. I would have a totally different opinion if I did use that language to run my operation system or other desktop application. Different expections, lead to different designs I guess. Cheers
It would be a little odd to publish benchmarks in advance of publishing the scalaz.effect package itself, but please do expect both benchmarks and code to show up as a pull request in time for my [presentation at Scale By The Bay](https://scalebythebay2017.sched.com/event/BLvT?iframe=no). The main point of the benchmarks is just to say that performance is a priority for me and I'm committed to not taking the easy way out. :-) Your feelings on `IO` demonstrate why there's a need for multiple approaches to functional programming in Scala. In the view of myself and many Scalaz contributors, multiple base effect monads are an anti-feature, and one of the single greatest mistakes of Scalaz 7 (`Task` vs `IO` vs `Future`). Just like Haskell doesn't have multiple competing `IO` monads, many of us believe that such a concept is fundamentally incoherent. Scalaz 8 will have rich type classes for describing the features supported by `IO` (which can be implemented by free structures and IO transformers, for example), but we will make no attempt to encourage or support different base effect monads. As for modularity, many (including myself) prefer having a richly-integrated, unified set of data types and abstractions that work together flawlessly (as proven by automated tests) and which are maintained and versioned in a way that's simple for developers to understand. That's what Scalaz 8 is striving for, and what's why you can expect to see, for example, optics and recursion schemes available out-of-the-box for Scalaz data types, and other rich integrations that make developers more productive right away. That said, some developers don't prefer this approach, which is why it's great there are alternatives available to Scala developers. The fact that the Scala ecosystem is big enough to support different approaches to functional programming is quite encouraging and something we should all be quite proud of!
The reason he singled out Scala is that Scala(for all its imperfections) is big enough in FP community to actually compete with Haskell in that particular niche
haha...yeah :)
&gt; I'll just give a small personal opinion about modularity... I think the approach you are describing will lead to massive usuability issues for non-expert. In the scala teams I did work in the past, the biggest point of frustration with Scalaz 7 is definitely dealing with imports... I feel so embarassed everytime someone ask about it, this is terrible. Actually, at least personally and in the places I have worked, Scalaz's insistance on non modularity (i.e. being monolothic) is what has caused huge amounts of issues when using Scalaz in the ecosystem. This is mainly because of binary compatibility issues, which is precisely what /u/alexelcu is describing Furthermore on the issue of imports, the reason why imports are split out is because of IDE/compiler performance. One of the most taxing parts of the scala compiler is implicit search, so if you import the everything (i.e. `cats.instances.syntax.all`) or the scalaz equivalent, then your IDE's grind to a halt and your compile times start shooting through the roof. Also in regards to `IO` types, trying to settle on a single `IO` type is a recipe of failure. There are already enough issues in design when it comes to shoehorning the `IO` types into a relevant algebra, and there are so many concerns (push vs pull based, cancelled vs non cancellable, strict vs lazy, scheduling) that the end result will just be fragmentation or stagnation, and because Scalaz wont be modular the fragmentation will be painful for end users.
&gt;Your feelings on IO demonstrate why there's a need for multiple approaches to functional programming in Scala. In the view of myself and many Scalaz contributors, multiple base effect monads are an anti-feature, and one of the single greatest mistakes of Scalaz 7 (Task vs IO vs Future). They aren't an anti-feature because there is no perfect IO type which can cleanly fit into algebraic laws. There are so many competing concerns when it comes to designing an `IO` type that trying to settle on a single "perfect" IO abstraction is going to cause lots of issues. Haskell doesn't have competing `IO` types because 1. They enforce laziness (which is actually an issue in Haskell). So that means they don't have a `Future` type (future is essentially a strict `Task`) 2. IO fusion and other similar optimizations are built in the compiler, in Scala its up to the library authors to implement these optimizations 3. JVM gives you more control over threading/memory management/green threads than Haskell does**. So Haskell places a lot more limitations on what you can do with their `IO` type (which means if you want to do something that their `IO` type can't do, you have to implement it yourself) 4. And finally, Haskell is actually missing functionality. They don't actually have an `IO` type that implements backpressure properly, which means that you have to manually implement this yourself. ** (technically you can have as much control in Haskell via the usage of C libraries, however I haven't seen this done)
I've always done my wiring manually. I'll have something like this: trait DbDescriptor { def driver: RelationalProfile //or whatever the Slick type is //maybe some other stuff } and then do class Dao(descriptor: DbDescriptor) { import descriptor.driver.api._ //whatever } Then you make a test version, object TestHelpers { val dbDescriptor: DbDescriptor = new DbDescriptor { // Slick H2 setup } } and in your app's non-test wiring, do something similar but initialize a connection to your "real db" via any number of config methods.
I love scala, and I love types. I think they make for awesome documentation and prove a program works. Types do clutter up code: fmap' f (Just x) = Just (f x) fmap' f Nothing = Nothing is easier to read than def fmap [A,B](fa: Option[A])(f: A =&gt; B) = if(fa.isEmpty) { Option.empty[B] } else Option(f(fa.get)) because of type inference. 
the above code doesn't work though
&gt; How? It can't be a comonad(what a loss \s) or things like that because you can't nest it without loosing information but that's fine most of the time. I think you mean monad? Nullable types are [almost fine](http://wiki.c2.com/?AlmostCorrect), which is how bugs tend to make it into production: the nesting is a subtle edge case waiting to become a bug in generic code that uses them. A generic function that uses null to track error cases is likely to break when used with a nullable type, and this wouldn't necessarily be visible anywhere from the outside; there's no way to know which generic functions are safe to use with nullable types other than manual documentation or the like. &gt; Option is only better (at the case of nesting) if your typesystem don't differentiate nullable types. It's always better for anything generic, because of the nesting thing. &gt; constant wrapping and unwrapping is way too expensive. This is why the code of pure FPists are really bad at performance critical and real-time cases(that's why FP has a hard time with gaming and such). Which is a tiny niche, and I'm not even convinced FP does have a hard time with it. &gt; That's not safety, I'm talking about "overheadless"+safe resource management and concurrency. Huh? Either et al are much safer for resource management (you can bracket a function that returns Either the normal way, you only need a `finally`-style construct if you're interleaving opening resources and things that may fail, and that case is no harder than it is with exceptions). &gt; Welcome to scala. How about all those weirdly-named categories? Aren't they complex? They are. And they also bring a lot of overhead. Nonsense. And if the language is already close to the complexity budget (or beyond it, in the view of some potential users) then that's a reason to simplify it, not to add more. But when something's just a normal type that follows the normal rules of the language then there's no extra complexity, because people can just read the code if they get confused. &gt; because if you'll try to handle indexing, IO or arithmetic exceptions your code will run like shit and you'll loose productivity for no reason. Nonsense. Maybe in a few very specialized cases, but most of the time the unreasonableness of exceptions costs far more productivity. &gt; Only when you need nesting. If you don't nest, both `Option` and non-disjoint union work, but when you do nest (which can easily happen by accident when combining two functions that should work on the face of it) only `Option` will work. &gt; Checked exceptions are pretty easy to understand(see in java) Checked exceptions are a major source of cumbersome code in Java; many Java programmers (and I believe also the original designer) regard them as a mistake, and no other major language has adopted them. &gt; sum-typed nulls are far easier than how Option is defined in Scala. To be an actual sum type it would have to behave like Option. Non-disjoint unions are much harder to understand than the current Option implementation which is a plain old datatype you can easily read the source to or reimplement yourself. &gt; What's wrong with that? Ok, the actual thing that's wrong is that exceptions and null invalidate a huge class of obviously correct program transformations that are useful for maintaining code; you can no longer blindly reorder lines, or remove useless field accesses. But the fact that they're implemented as language-level special cases is a big hint that they have this kind of issue. If they behaved like plain old datatypes, they should be implemented that way. &gt; No need to generalize for other types. "Need" is a strong word, but the generalisation is often useful. Having to repeat the same piece of logic twice is bad for maintainability. &gt; Btw, I was thinking about the Rust method too but there's no way to do that on the JVM. The JVM isn't the only Scala compile target, and is constantly being improved. It's not worth compromising the language semantics for the sake of small-scale present-day JVM-specific performance issues; performance can always be improved in the future, whereas if the language itself is wrong then everything we write will ultimately have to be rewritten. &gt; they're always relevant but you can choose to ignore but then we'd be script kiddies. Professionalism means delivering the things that are actually useful, not being perfectionist about performance. &gt; No, it isn't: see linear and dependent typing. Most FP languages(especially those in the industry) don't have them. Right, and how popular are those in the industry? I'm all for those things but even FP languages without them are ahead of the industry's safety curve. &gt; the problem is not just "being slow" and "lagging" but they introduce a lot of useless pointers and the GC will need to do more work. Computers are cheap. How much work the GC is doing is not worth worrying about unless it's affecting the actual functionality. (Which it certainly can, but "useless pointers" are not a problem in themselves). &gt; Why would we need to give up productivity? It's fair to assume that if a language focus is safety then it will end up less productive than if the language focus is productivity. I think there's still plenty to be done on the productivity side. &gt; Yes, and many of them are just plain useless and only serve those who are really bored. It doesn't mean that you should use all of them and you should give them weird names just for fun. Of course you shouldn't use an abstraction that isn't useful, and you shouldn't rename something that already has a name without a very strong reason. I don't know what specifically you're thinking of, but most of the things I've seen in libraries are there because someone found them useful (and if a particular one is not useful, then having an unused class in a library is not a high cost), and I rarely see the category-theory-in-Scala folk coming up with new names for anything, rather they call the thing what it was already called elsewhere. &gt; That's a bad attitude. By introducing and keeping idiotic naming standards and complex symbols the average programmer will never touch category theory. We'll see. At one job I did experiment with trying to give clearer names to some popular FP constructs, but it didn't work out; it made it harder for people to learn from outside tutorials and didn't seem to make understanding appreciably easier.
You seems to be confused about the different trade-offs being made there. &gt; There are so many competing concerns when it comes to designing an IO type that trying to settle on a single "perfect" IO abstraction is going to cause lots of issues. This might be true for librairies that favour strict evaluation, where it seems you see a huge design space (I don't know much about it, as I don't have interest for it) but I think that is wrong when it comes to design efficient non-strict evaluation system in Scala. Instead of competing (for example both Brian, John and me wrote some prototypes) we try to convince each other and agree on a common design, it's how Scalaz works, and actually that's how Haskell design is being done as well... from what I saw since I started contributing there. I won't precisely counter argue on your points, but 3 and 4 are just traditional FUD that haskellers are used to face :-) The first point is also entirely wrong (do you know Async? and the great library from Simon Marlow). I guess it's worth using a technology a bit to understand it's shortcomings.
As I said in my other reply, I think you are mixing many things here. IO/Async are two different concerns, they are mix in Cats/Monix/Scalaz7 but they won't be in Scalaz 8 and they are definitely not in Haskell. We are taking extra care during the design to ensure that the design scale when compiling with SBT, I'm unsure about IDE support to be honest... I guess for Zinc compatible one it should not be an issue.
I wish you would give me information rather than basically just saying he's wrong without explaining why.
&gt; IO/Async are two different concerns, they are mix in Cats/Monix/Scalaz7 but they won't be in Scalaz 8 and they are definitely not in Haskell. Which is precisely the point actually, they are **different concerns** which is why they have different types in cats. You can try and merge everything into a single type, but you are going to have issues down the road due to this
&gt; Instead of competing (for example both Brian, John and me wrote some prototypes) we try to convince each other and agree on a common design, it's how Scalaz works, and actually that's how Haskell design is being done as well... from what I saw since I started contributing there. For starters, its not "competing". The various designs in cats have different tradeoffs. Monix task is cancallable (via their extended `Future` which can be cancelled), fs2 is not. One is push based, the other is pull based. One follows the observable pattern and is reactive, the other does not. This is all because this kind of programming is **hard**. &gt; I won't precisely counter argue on your points, but 3 and 4 are just traditional FUD that haskellers are used to face :-) Actually its not, and its a problem I faced some time ago when I did Haskell work. In Scala, its quite easy (with `Future` but also Monix `Task`) to say "I want to execute/fork this async operation in a separately tuned execution context". Doing this in Scala is really easy because we have an explicit `ExecutionContext` (or `Scheduler` in Monix task). Its actually very hard to do this in Haskell, I at least am unaware of how you would do it At this point you may ask "why you would want to do this". Well its actually a very common problem in non trivial apps. You may have an async logic that is undergoing very heavy load (but is not critical) and so you want to define a specific `ExecutionContext` for this logic. And then you may have some background logic which is very high priority, but doesn't have a lot of load, again you would put this in another `ExecutionContext`. Games is a very common situation where this would happen (i.e. you would put network/keyboard input in the "low load high priority" bucket). So is GUI's, and even in backend scenarious (i.e. putting your Kafka's messages in seperate `ExecutionContext`). If you are logging statistics to a server, you put this in a very low priority `ExecutionContext`, etc etc. Afaik, this is very hard to do in Haskell. Haskell basically treats the entire load globally, and although you can tune this with startup parameters to a compiled Haskell program its not really solving the problem
Sure, you have to either make `doSomething` a `val` in the first place or write `(doSomething _).tupled(t)`, which is not quite as nice, but it's not like the whole thing is new.
Totally agree, they won't be merged into a single type. PS: AFAIK, they are merged in Monix/Scalaz7.
I don't believe there's that kind of distinction. I think most of FP consists of following through on widely-agreed OO principles, even when traditional-OO believes it is impossible to do so.
I feel like some are really competing to each other, but I'm totally happy to disagree. It's interesting to hear about your experience regarding `ExecutionContext`, I wrote real time synthetizer in Haskell which had some pretty tuff real-time constrain... but I never had to give priority to a "pool of threads". Maybe it's because I was offloading some of the work on the GPU, but I never had to give "priorities" to my thread... or maybe it's due to a given style of programming (I never wrote a game). I did experiment tuning EC on entreprise app, but that was not really useful. At the end, the most complex setup I did have was one EC for IO the other for the rest... for traditional server app I don't see how one would need more complex setup. I guess that really comes to use in game, is that something done in C++ for example? or maybe all those tuning is really necessary due to the JVM architecture. I would love to know what was exactly the code you wrote in Haskell where you wanted to do that, I'm sure we could come up with an alternative design. Cheers
I'm happy to give you more information, I'm just evaluating stuff lazily and I was not up to write an essay about each of the points. Could you tell me which point exactly you would like to know more about?
&gt; Maybe it's because I was offloading some of the work on the GPU, but I never had to give "priorities" to my thread... or maybe it's due to a given style of programming (I never wrote a game). Yes, this is the precise reason. But in the example I gave before, you can't offload Kafka events onto a GPU. Haskell lacks this, currently it does have an `ExecutionContext` but its global (its actually embedded in the Haskell runtime) and it can only be configured (globally of course) with runtime parameters to a compiled Haskell app. &gt; I guess that really comes to use in game, is that something done in C++ for example? Yes, this is done all of the time in games and also native apps (where you can't just "spawn more instances" like you can do in server backend apps). You for example don't want to lock up the UI because the native app happens to be downloading some large file in the background. Its also crops up in the webserver space in the example I gave before (i.e. you have a HTTP server that accepts requests and returns responses + a Kafka event passing, and the Kafka event passing is critical priority). You don't want the Kafka event passing to slow down because you happen to have a spike in HTTP requests which will greedily steal all threads/green threads from the global execution context I mean this kind of problem (of having different `Task` or `Future` types) happens whenever you do something thats non generic/trivial. Heck, Twitter is still using their own `Future` instead of `scala.concurrent.Future`, and the only reason is because their own `Future` supports cancellation, which is their business requirement (i.e. they need the ability to cancel a future because they don't want unnecessary requests to run). This is the central point I am making though. Async programming is hard in non generic situations which is why you have these different types with different tradeoffs. Then what happens is you have people that make their own IO/Task types, and because Scalaz is monolithic they are unlikely to target it or they will have problems when doing so, this is what the Cats community has learnt the hard way. Also back on Haskell (and this is my opinion), this is one of the primary reasons why I think Haskell isn't really used in a lot of areas. In their determination to be "correct", they reduce the problem down to a point where you are missing a lot of functionality that is required in non generic situations. For example, without having specific control over `ExecutionContext` (or w/e its called in your language/library), you aren't going to find any game with serious graphic engine coded in Haskell (at best it would be a mainly coded with C/C++/Rust/D with maybe some game logic in Haskell) &gt; I would love to know what was exactly the code you wrote in Haskell where you wanted to do that, I'm sure we could come up with an alternative design. I did this like 7 years ago when I was uni (which is where I learnt Haskell) and I was dabbling around in this stuff. I still look at Haskell every now and then, but I don't code in it seriously anymore. I mean you can work around this by just having 2 separate Haskell apps that communicate via message passing, but that is going to create a lot of overhead and it still wont correctly solve the problem because now the OS is going to be partitioning and fighting over the threads (in a less then accurate manner because the OS doesn't know exactly how your Haskell apps are meant to behave, and it doesn't have a concept of green threads, or sparks as they call it in Haskell)
I was talking about the different ways/peculiarities of handling async, they shouldn't be shoehorned in the same general type
I would be curious to understand which use case you can not cover with haskell IO/Async. I'm happy to share a backpressure example with `machines` if interested.
&gt; This is the central point I am making though. Async programming is hard in non generic situations which is why you have these different types with different tradeoffs. Could you provide some use case example? as I feel I can solve all cases I have with STM and Async, I would love a counter example. &gt; You don't want the Kafka event passing to slow down because you happen to have a spike in HTTP requests which will greedily steal all threads/green threads from the global execution context You can definitely achieve such semantic in haskell using machines/STM, there is nothing preventing one to do that. There is a bunch of games written in haskell: https://wiki.haskell.org/Applications_and_libraries/Games I did play a few and the UI was reacting smoothly, which seems to contradict your general statements. I guess you could not implement a very highly optimized game anyway, but you couldn't really do that on the JVM either... should rather use Rust or low level language anyway, of course YMMV.
&gt; Could you provide some use case example? as I feel I can solve all cases I have with STM and Async, I would love a counter example. I would have to find the code, but its fairly simple to test such a scenario. &gt; You can definitely achieve such semantic in haskell using machines/STM, there is nothing preventing one to do that. Thanks, I am not sure about STM but machines looks like it can theoretically do this (although it does prove my earlier point in that its using a separate type to achieve this) &gt; There is a bunch of games written in haskell: https://wiki.haskell.org/Applications_and_libraries/Games Yes, but I don't believe these games have the real time constraints that I was talking about. Talking about AAA style games that have realistic graphics/physics engines and a lot of other competing concerns &gt; I guess you could not implement a very highly optimized game anyway, but you couldn't really do that on the JVM either... should rather use Rust or low level language anyway, of course YMMV. Well there is scala-native, its not really JVM related (scala is not tied to the JVM). The reason why games aren't done on the JVM is due to GC which is an orthogonal issue. Scala-native for example allows you to have direct control over memory allocation In C++/Rust/D, this ability of having different contexts or priorities is critical 
&gt; Yes, but I don't believe these games have the real time constraints that I was talking about. Talking about AAA style games that have realistic graphics/physics engines and a lot of other competing concerns I see, that makes perfect sense indeed! Makes me think about the issue I faced with that real time synthesizer. Thanks :)
With all regrets, I must say the man fears Scala the most. He mentions Clojure on JVM, F#, OCAML as good choices on existing widely deployed platforms like JVM, .NET, etc. All these languages are impure. So why not Scala. Because with libraries like Scalaz (the upcoming Scalaz 8), Shapeless, Monix, Cats and many others its is possible to do principled and pure functional programming in Scala. You just have to look at the Scala ecosystem. Inspite on being JVM, fabulous range of libraries have been written many of which are inspired by Haskell. BTW, I use more Clojure than Scala. But as a FP language supporting advanced category theory concepts it is one of the best, dare I say the only practical one. Haskell, Clojure, Scala, Purescript, Erlang, F#, etc. are all worthy contenders. I would prefer Scala because: * You can deploy on most widely used industrial platform of today i.e. JVM. Create a Jar and you are good to go in Elastic beanstalk or wherever. * Use the benefits of FP (concise code, safe code, multi-core ready and may other things). After all business need to ship software fast and one that scales (Scala is right up there). * Use huge amount of libs to get started and then build great FP wrappers on them (which already is being done). And as for the developer: 1. It is on one of the best ecosystem - JVM (think all the libs) 2. All Haskell kind of abstractions are there in so many libs in Scala (Haskell guy can get his intellectual fill and make money) 3. It has compiler macros (Lisp guy would not be so depressed or maybe. I am not saying Scala is homoiconic :-) 4. You have Erlang style Actor implementation (and competing ones too. So something for Erlang guys though JVM and BEAM are different creatures). Hey we have got JVM, Haskell, Lisp, Erlang, ML style covered on an industrial platform (Oh by the way I forgot ScalaJS - Browser, nodejs, React Native, etc.). The above video is by a man whose company promotes Haskell. And he is such a fud propagator. CTOs! (BTW, I am a CTO too), please ignore this man. Haskell has many top ambassadors and you do not need such a guy from FPComplete. Lastly, give Scala a try (after all top companies with some of the most serious talent in software world are on it). Let us all coexist and to each what suits him most. Peace.
That's not really a fair comparison though; you can write the Scala very similarly to the Haskell: def fmap[A, B](f: A =&gt; B): Option[A] =&gt; Option[B] = { case Some(x) =&gt; Some(f(x)) case None =&gt; None }
&gt; Maybe its not the best language for pure FP, but my question is pure FP worth all the trouble? Aren't some programming problems naturally suited to imperative/OOP style and some to functional style. This is the very idea on how scala was built. I've found that over ~7 years of professional Scala I've become more FP-purist, solely based on experience of what goes wrong when you violate purity, to the point that I consider many of Scala's less-pure features to be undesirable and ban them from my code. I would probably never have got started in Scala without them though. That's where Scala has succeeded for me: it offers something like Haskell but with training wheels you can use to stay productive as you're learning better ways to do things.
Oh, the pattern match wasn't what I was referring to - it is the necessity to declare the generic A and B and A =&gt; B. Scala cannot infer A =&gt; B. That is, I cannot write: val fmap = fa =&gt; f =&gt; fa match { case Some(x) =&gt; Option(f(x)) case Nothing =&gt; None } or: def fmap(fa)(f) = fa match { case Some(x) =&gt; Option(f(x)) case Nothing =&gt; None } . The problem is that it cannot infer a generic return type correctly. It cannot infer the function type in the argument list. The part of scala that makes people cry rocket science, is the generic parameters.
&gt;I mean you can work around this by just having 2 separate Haskell apps that communicate via message passing, but that is going to create a lot of overhead and it still wont correctly solve the problem because now the OS is going to be partitioning and fighting over the threads (in a less then accurate manner because the OS doesn't know exactly how your Haskell apps are meant to behave, and it doesn't have a concept of green threads, or sparks as they call it in Haskell) There is `forkOS` to make a new OS thread. For simple-ish cases like the input thread in a game game you can happily `yield` or `threadDelay` some small amount where applicable. For other cases you'd probably wind up making your own little scheduler probably using a quantity semaphore. It would really be nice to have actual thread priorities. edit: I found some light reading. https://www.reddit.com/r/haskell/comments/1725g0/there_is_now_a_patch_for_the_ghc_runtime_system/ https://ghc.haskell.org/trac/ghc/ticket/7606
Where else should such post be posted? Criticisms are pretty useful to improve a given technology and know how a given public feels about it, why rejecting entirely negative criticisms?
&gt;So why should the cats-core be a dumping ground for data structures that people don't want? Because if you want to use this seamlessly with other functional concepts you *have* to bake it in. The reason there is little interest in Dogs is because it doesn't play nice with the other libraries in the ecosystem, *because* of modularity! Modularity is great, and for applications it's essential when writing libraries. For a *core* library that will be used everywhere though, it's a curse and it will haunt Cats and Typelevel for a long time. 
Note that `unsafePerformIO` is actually should be called `safePerformIO` or even just `performIO`, because in Scala it can't be used to break type safety (as in Haskell).
Your response contains a lot of inaccuracies, and also misses the point about why multiple IO monads don't make any sense. However, rather than replying here, I think this would be interesting to a broader audience, so I'll write it up in a post and share it.
You'll be a better human :) Seriously. The category theory stuff makes you reconsider your approaches in your daily life even by introducing a lot more structure to them.
Would appreciate if you would also respond to the points I was making in the thread otherwise its very easy to misrepresent what someone is saying. In any case, the reason why competing IO types appear is because some current IO type doesn't fulfill certain properties which are required by the program (of which there are a lot because the problem space of IO is huge) For this reason its better to have good implementations that have various cross sections and recommend a single type to beginners
It doesn't break type safety in Scala or Haskell. What it does break is the ability to reason about the code in an equational way, leading to unpredictable and / or erratic behavior, and this is true in both Scala and Haskell.
&gt; It doesn't break type safety in Scala or Haskell Of course it does! module Main where import System.IO.Unsafe import Data.IORef ref :: IORef (Maybe a) ref = unsafePerformIO (newIORef Nothing) unsafeCoerce :: a -&gt; b unsafeCoerce x = unsafePerformIO $ do writeIORef ref (Just x) Just x &lt;- readIORef ref return x main = let bool :: Bool bool = unsafeCoerce 1 in putStrLn (show bool) -- prints "False" 
I stand corrected! Without an ML-like ValueRestriction, `IORef` allows `unsafePerformIO` to break the type system. I still would not agree to call it `safePerformIO` because it breaks equational reasoning in both Scala and Haskell.
I see your point. However, the reason for unsafe* prefix in Haskell is exactly the fact that it breaks type system - so having "unsafe" in the name is like a red flag for everyone who reads this code. While it's debatable whether it's bad style to "extract" values out of IO-like types just for convenience, it's definitely not unsafe. It is "unsafe" in the same way that `System.Out.println` is. Scala is impure language, and I don't see much point in calling this function "unsafe" as you can easily break your `SafeApp` by using `var`s or any side-effectful function.
I'd like to add one more detail to my original answer: if you want to understand how Zinc works internally, [sbt Reference](www.scala-sbt.org/1.0/docs/sbt-reference.pdf) has a very good explanation of the incremental compilation process for Scala. It's a pretty good read, chapter "Understanding incremental compilation"
FWIW, this has been my experience as well. I have no problem writing monad transformer code in Haskell, but the equivalent code in Scala is just *so* incredibly littered with syntactic noise (such as type ascriptions, generic parameter declarations, etc.) that I usually just give up. (Type inference is a hard problem when you have full subtyping, so I don't think it's unreasonable to require these ascriptions, but I *do* think that the language suffers a lot as a result. I'd rather do without subtyping, but I guess that's why I'm using Haskell rather than Scala whenever I have the choice.)
Are there any functions in Haskell which are not referentially transparent, which are not prefixed with `unsafe*`? Personally, as a developer who writes purely functional Scala code, the `unsafe` prefix alerts me to the fact that the function is not possible to reason about with equational reasoning (but rather, requires imperative reasoning), is likely to integrate poorly (or not at all) into purely functional code, and should not be called unless I really know what I'm doing. Whereas, a `safe` prefix would indicate to me the opposite.
The subtyping makes interacting with java libraries buttery smooth. Which is a feature. There's not a really good tutorial on using scala's generics, outside of the shapeless tutorials. There needs to be a basic generic type tutorial. Something on the lines of: Scala uses generic types. This is a generic F: F[A] Generics mean a type that can hold some other type. Generics can hold more than one type: F[A, B] G[A, B, C] The convention is to name types contained by generics with single character uppercase ASCII characters starting from A within a given scope. Types in a generic are not concrete. They don't represent any real type. An `A` could be an `Int` ... etc. There needs to be a type cheatsheet in the sidebar. Now that I understand the type system -- I love the types now, and they don't look like noise. But it takes a long time to piece together the understanding from experience of using the language. 
Well, sure, but I'm actually not all that interested in Java interop, though I'm not sure Scala would have *any* adoption at all without that interop. It's a tough balance/question for any language designer. Personally, I'm mainly using Scala for the JVM. (But then again, I'm not even sold on that based on technical merit... it's just that you don't have to make the business case from scratch when you can just say "JVM" and all the non-technical people just assume...) Of course, given the abysmal binary compat story for Scala, I'll actually consciously use Java libraries in preference to Scala libraries. (I'm moderately hopeful that TASTY will help a lot here, but it's going to be years and years...)
There's no real way to do files without java interop, or write to sockets, or log to console, or work with streaming buffers. There's no from scratch http stack. You can't work with parallelism without it. If scala had a story for these things, that would be great, but it doesn't. The JVM is fantastic. It really is. Real cross-platform compatibility is necessary. But the jvm isn't the only way of achieving that. I use Scala because it's the best FP programming language to be employed, and has the least amount of development headaches.
&gt; There's no real way to do files without java interop, or write to sockets, or log to console, or work with streaming buffers. There's no from scratch http stack. You can't work with parallelism without it. Uh, what? Of course you can -- there's no need to import the semantics of the underlying platform into your language. Hence why anything other than CPU microcode works. And hence why "IO" in Haskell works. (I mean, it's not particularly elegant and would probably be designed differently given the benefit of hindsight, but there's no fundamental problem here.) &gt; I use Scala because it's the best FP programming language to be employed, and has the least amount of development headaches. I'm happy that you're happy with it, but it's caused endless completely unnecessary headaches for me, esp. given the terrible depencency situation. (But, again, as I say, I'm mildly optimistic that it'll eventually be fixed. Btw, most of this problem could have been fixed by just mandating the shipping of source code like Hackage does. Then again: Would any "enterprise" go for it? Probably not.)
You don't have to have interop without ffi in your language, but you do in Scala unless you rewrite the memory model or manage IO at the language level. 
&gt; Maybe its not the best language for pure FP, but my question is pure FP worth all the trouble? Aren't some programming problems naturally suited to imperative/OOP style and some to functional style. Pure FP languages, surprisingly, don't force you to program everything in a functional style, just a pure one. It's not especially difficult to write imperative code in Haskell.
Not exactly tutorials. but I frequently post Scala conference videos [here](https://www.reddit.com/r/ScalaConferenceVideos/) Lot of stuff on REST api. (play, finatra, akka http). 
Just to add a bit more background: I know there are other scala libraries out there for parsing command-line arguments. It's not exactly a huge gap in the scala ecosystem. But none of them were exactly perfectly what I was personally looking for, and what I felt shouldn't be too difficult to achieve: absolute minimum boilerplate, very type-safe, and simple. So I made a version of this for my own projects, used it for a while, and now turned it into something I hope others can use. Would be happy to hear any feedback!
Btw, if you're looking for scala guys in particular, scala community is huge on gitter. Post [here](https://gitter.im/scala/job-board). You might find some great devs here. &gt; The goal is to find candidates who can get into the mindset quickly and can start contributing with a lower ramp up time. Then quiz them on some of the skills you'd need for your job. Maybe not specifically the language, but the concepts behind it, and you can hold the interview in a language other than scala.
Play comes with a REST API example that may help: https://github.com/playframework/play-scala-rest-api-example
I don't think so. The job is based in San Jose.
Could you give me an example? I've seen newcomers to our team taking quite a while to comprehend options and futures, as well as map/flatmap. This isn't really a "skill" but it does prevent people from even beginning to understand any part of our codebase.
 def flatMap[B](f: A =&gt; Try[B]): Arg[B] This upsets the pedant inside me.
I agree, honestly, but flatMap seemed like the best name for it anyway
My first though was "why we need another library when we have scopt in place?". And then I looked into the readme and found out that Argyle solves basically all the pains I had recently with scopt. Thanks!
&gt; Slick is a black box That does not make the actual database you're interfacing with easy to ignore. SQL is declarative, but still a programming language, implemented differently by each DBMS. Slick might let you structure it through Scala code, but does not hide all the semantic differences that do exist. &gt; I am happy to use the highest common divisor of features in both H2 and Postgres for the value it gives of being able to forget about the database entirely. You'll likely find out that even figuring out what that common denominator is makes forgetting about the database impossible. Debugging issues that differ between development and production will surely follow, and it's always painful. &gt; Anyway, yes your point is a good one. There are additional concerns though, one is that I am one guy with a 'server' (an 8 year old laptop) in my living room with 2GB of RAM. I want as little infrastructure/devops as possible for this project. &gt; EDIT: I know docker is easy but it comes with 8 hours of configuring environment variables to deal with. That is not where I want my development time to go, I only get to work on this on half days at the weekend as it is. Setting up Docker once (which does not take 8 hours at all) is very likely to save time compared to constantly fighting differences between dev and prod environments.
Scale doesn't have subtyping due to Java interopt, it has subtyping because it's the only way to have proper modules in your language, which Haskell is severely lacking Also while you may see types as noise, they are tremendously useful when it comes to documenting and readability of functions
It's exciting to see the results of the work done in Kentucky Mule and the Reasonable Scala compiler, but I really hope these improvements will be integrated in the main Scala compiler, and later Dotty, in the future. I don't think anyone wants to see two or more diverging forks of the Scala compiler.
The team I'm part of is in a somewhat similar position. We started as java folks and moved to Scala and try to do actual FP as much as possible. It really depends on what you're looking for. If you're interviewing for senior positions I would say experience with Scala/Haskell/F#/etc is a must. If you're interviewing for junior/middle positions discussions about principles/concepts would be an idea. For example try to get the person to explain to you why immutability/purity are important. Do they know what referential transparency is? IMO for junior positions actual knowledge of language is not that important (as you said that can be learned relatively quickly), you have to optimize for people that can reason about code.
it's not - `flatMap` is a particularly dangerous thing to misname because it comes with automatic inclusion in `for` comprehensions. It's all based on the name, not the signature. You absolutely should rename it imo!
I agree. (I also prefer Scala's strictness, tooling, and sane dependency management and available libraries, but that's beside the point.) "Better ways to do things" is a rabbit hole. Something to be familiar with, along with data structures and algorithms, upstream library and platform knowledge, and various levels of domain knowledge. This is why I can understand the appeal of languages like Go and Python, that are easy for beginners to get things working in, but don't necessarily make it easy, especially when trying to write correct and extensible systems. It would be nice if functional Scala were as approachable, without giving up the advantages of purity or resorting to using ungoogleable synonyms for existing patterns.
Nobody's preventing one from declaring types for functions at the top-level (and it is in fact considered Good Style in Haskell). I'm talking about declaring types for local/helper functions -- it's incredibly noisy and mostly pointless. EDIT: Re: modules and subtyping (and I can't tell you how weird it feels to say this about !Haskell): IME this is largely a theoretical concern and has absolutely minimal impact in practice. If there were a dedicated "module language" (like in e.g. ML) then it might be a more valuable thing just because it would be less prone to abuse, but as it is, I'm mostly just seeing inheritance being abused :(. (And the cake pattern which was touted as The Solution To Modularity(TM) is rightly seen as 'misguided' these days.)
I disagree. Precisely because it is name-based, it does not need to be renamed, IMO. The *name* `flatMap` does not imply "obeys the Monad rules for `bind`". It implies that it would do the "expected" thing if put in a for comprehension. Pedantic FPers will disagree with me, but I don't care; the Scala standard library already agrees with me (e.g., I can do `(xs: List[Int]).flatMap(x =&gt; if (x &gt; 0) Some(x) else None)`, which definitely does not obey the `bind` laws because the types don't match). In this particular case, it would definitely do what I would expect if I write it in a for comprehension. For example: val intArg = for { stringValue &lt;- stringArg intValue &lt;- Try(stringValue.toInt) } yield { intValue } I would expect this to produce an `Arg[Int]` that contains a successful value if a) `stringArg` contains a successful value and b) that value can be successfully converted to an `Int`. That's precisely what would happen for the implementation of `flatMap` here, I believe.
Sure, my point was just that (as mentioned before), subtyping doesn't play very well with global type inference (in fact it doesn't play well even with local type inference in non trivial languages). And Scala having subtyping is a deliberate design decision. Its just a case of you can't have your cake and eat it too
We'll see, actually. There was a recent paper which demonstrated that you *can* combine inference and subtyping. (Though, I think that was structural subtyping?) Can't remember the name off-hand, but it should be easy enough to find given the "subtyping" and "inference" keywords :)
&gt; We'll see, actually. There was a recent paper which demonstrated that you can combine inference and subtyping. (Though, I think that was structural subtyping?) Yes, but Scala predominantly uses nominal typing, a language that predominantly uses structural typing is Go.
The `Option` example is driven by an implicit conversion from `Option` to one of the `GenTraversableOnce` classes, or something. It is not driven by a definition of `flatMap` with a bad signature. If his class is isomorphic to a `Try` then he should have an implicit conversion to/from `Try`. If you want to go down the standard library route.
`GenTraversableOnce` itself is a "bad signature" for `List.flatMap` if you want to even state the laws. So your argument is moot IMO.
&gt; The standard library does it so it is fine and &gt; What the standard library does is bad practise Yeah...
No, I disagree that it is bad practice. I agree that what the std lib does is bad *if you want to state the laws for every `flatMap` in existence*. But I disagree with that latter premise. It's the difference between a *name* and a *contract*. A contract is more prescriptive, and can be the basis to justify laws. A name only needs to be *suggestive*. If names were isomorphic to contracts, we could never ever build modular systems, because names would have to be different across the entire application if their contracts are different. Fortunately, this is not how the world works: names can be reused, because they only act as suggestive reminders. Names need to be coupled with their "namespace" (e.g., the type of the enclosing class) to map to a *contract*. With that in mind, I reiterate my opinion: `flatMap` as such is a *name* and only has to be *suggestive* of what it will do. Only when you couple it to `IO` or whatever do you get a *contract*, on which you can potentially state the monad laws.
&gt; GenTraversableOnce itself is a "bad signature" for List.flatMap if you want to even state the laws. Which of the laws are violated and how? I'm not sure I understand that from what you said.
No law is *violated* because you cannot even *state* the monad laws to begin with. The monad laws require that the same monad type constructor be used across the statement. But with `List[A].flatMap(A =&gt; GenTraversableOnce[B]` you have the `List` type constructor on the one hand, and the `GenTraversableOnce` type constructor on the other hand.
https://github.com/markhibberd/pirate is a good alternative
I'm curious, what problems did you have with scopt? 
I think we just disagree. The name to me suggests that it is the same as all the other flatMaps of the world, not even that it must satisfy the monad laws. Your for comprehension to me is misleading, reading the code as written I would expect a `Try` at the end. In fact, you can't even have multiple `Arg`s in the comprehension, which is not the case for any standard flatMap at all. 
&gt; I think we just disagree. Yup, looks like we do. ;)
&gt; EDIT: Re: modules and subtyping (and I can't tell you how weird it feels to say this about !Haskell): IME this is largely a theoretical concern and has absolutely minimal impact in practice. If there were a dedicated "module language" (like in e.g. ML) The module system in ML (which they call function subsumption) is actually subtyping, its just very limited and tacked onto the language. Scala chose to do it properly Its actually a big in problem in Haskell, its just one I don't think Haskellers realise. For example, XMonad (one of the few Haskell programs in the wild which has to work with a concept of modules) has to ship with GHC, because your XMonad configs have to be `.hs` files which need to compiled. This is actually unacceptable in a lot of software cases (shipping an entire compiler with your application) &gt; then it might be a more valuable thing just because it would be less prone to abuse, but as it is, I'm mostly just seeing inheritance being abused :(. Actually having the concept of modules is incredibly common. Haskell doesn't even have properly generic containers that can provide efficient operations because it doesn't allow for this kind of behaviour. Granted in "OOP" languages its abused, but in Scala its not. People use subtyping when they want "module" or "plugin" style behaviour, otherwise people use ADT types/parametric polymorphism/typeclasses.
I don't see them as noise. I see them as extremely useful. I've gone through the trough of understanding though. That trough is deep and wide at first, but out made me understand, even in other strongly -typed languages, how I made many assumptions about the types of my code. The more I think about modules, I'm not clear on what that term actually means, and how subtyping is required to do it. Can you clarify? EDIT: Most of the time, people are speaking of traits when they speak of module systems. I don't think you need traits, specifically, for a module system -- https://typelevel.org/blog/2016/09/30/subtype-typeclasses.html You can do it with implicits implicit classes and typeclasses. You don't really need extends, but it does involve an implicit wrapper class to access the this.functor.map as this.map. I may actually try writing a full monad heirarchy to see if it works later. 
There is a lot of these. Personally I prefer https://github.com/scopt/scopt
&gt; The monad laws require that the same monad type constructor be used across the statement. Is that the case? `GenTraversableOnce` is more general than list and it seems that flatMap can still create a `List` at the end. It might not be the exact definition of how monad "arrows" work but it includes/subsumes all this functionality, so I think that one can still state the laws.
Yes, it is the case. Let's say I do try to state the left-identity law for `List(a)` (unit) and `List.flatMap(x =&gt; GenTraversableOnce)`. It would look like the following: forall type A, type B, val a: A and val f: A =&gt; GenTraversableOnce[B], we have: List(a).flatMap(f) === f(a) Now, let us pick `A = Int`, `B = Int`, `a = 1` and `f = (x: Int) =&gt; Set(x)`. Then according to the left identity law that I have stated above, it should be the case that `List(1).flatMap(f) == f(1)`. Let's try out: scala&gt; import scala.collection.GenTraversableOnce import scala.collection.GenTraversableOnce scala&gt; val f: Int =&gt; GenTraversableOnce[Int] = (x: Int) =&gt; Set(x) f: Int =&gt; scala.collection.GenTraversableOnce[Int] = &lt;function1&gt; scala&gt; List(1).flatMap(f) res0: List[Int] = List(1) scala&gt; f(1) res1: scala.collection.GenTraversableOnce[Int] = Set(1) scala&gt; List(1).flatMap(f) == f(1) res2: Boolean = false Uh oh, `List(1).flatMap(f)` is not equal to `f(1)`. Boom! The left identity law is violated. If, however, you restrict `GenTraversableOnce[_]` to always be `List[_]`, we cannot construct such a violation.
Ah yes, I agree! So you *can* state the laws, but they are violated.
Not the person you replied to but I can't stand that you need to have default values in some way for all your case class's members. I end up having to use null and it irks me.
Neat, I love this idea. Do these functions somewhat emulate what matryoshka is doing?
Never used either of these to parse args so this might be a stupid question but why not make those params Option types?
Because if they are required arguments, then the consumer of the config class shouldn't have to deal with an Option type that will always have a value. It just forces a little extra boilerplate for any consumers.
This is somewhat different from Matryoshka, but related. I think it may be possible to formulate what you want to do with Matryoshka, but the focus here are transformations of `Ast[T] =&gt; Ast[T]` where it has some equivalent value, but costs less. This is useful for say compiling some library to spark, hadoop, etc... where you want to apply some rules to simplify the Ast that the user gives you before handling it to a compute platform that may or may not understand the optimizations you could apply with Ast level knowledge.
This was precisely my reason for not loving scopt as well. And having to write a ton of calls to .action and .copy
We had to do this. We asked if they've heard of functional programming and/or Scala, see what types of languages they used before, see if they're familiar with some of the concepts, etc. Even if they never used Scala, if they did their own research and seems enthusiastic about learning, that's a good sign. Also, if they're a great software engineer, they shouldn't have too much of a problem learning Scala if you guys have good code review/training program. 
People reading this thread should do well to re-read [the reddiquette](https://www.reddit.com/wiki/reddiquette), in particular this bit: &gt; **Please don't** &gt; [...] &gt; Downvote an otherwise acceptable post because you don't personally like it. There is a very good, reasoned discussion that has been *hidden* because the top comment received many downvotes, and I strongly suspect that those downvotes were given against the aforementioned rule. There was nothing in that comment (and some others) that deserved downvotes according to the valid uses of downvotes on Reddit. This is really sad.
AFAICT that is the purpose of both projects
I could use a hand with learning Semigroups. I have been using the Cats semigroup, and have been able to use it to merge two Maps. This fails however when one of the value in the map is itself a map. def aggregate(oneFile: TransactionFile): ReportTotal = { oneFile.dataRow.foldLeft(Map[String, Map[Int, BigDecimal]]())((acc, el) =&gt; { val total: Map[String, Map[Int, BigDecimal]] = acc val next: Map[String, Map[Int, BigDecimal]] = Map(el.acNo -&gt; Map(el.transCode -&gt; el.transValue)) val result = Semigroup[Map[String, Map[Int, BigDecimal]]].combine(total, next) result }) This produces the following error: Error:(64, 31) could not find implicit value for parameter ev: cats.kernel.Semigroup[scala.collection.mutable.Map[String,scala.collection.mutable.Map[Int,BigDecimal]]] I am hoping there is a way I can specify the implicit parameter which is missing. Is this possible or am I missing something obvious?
Tbh we don't have any dedicated training, the new engineers usually just work on smaller/easier user stories while taking lots of help from the others. So what do you suggest if the candidate has never done/tried fp before? Conduct a regular interview and hope they can pick it up later?
Well we usually try to gauge during the interview how enthusiastic they get about learning Scala and FP. Some people do the fake, "yeah I would love to learn," and some give genuine enthusiasm. I think one of the criteria is how much do they read/research about other concepts and languages. If they only care about Java and not have read or played with any other languages or concepts other than Java (eg Scala, Kotlin, etc) then we probably don't hire them. In our experience, a good engineer is a good engineer. After a bit, they can navigate around the code fairly well. Some might fumble with monads, but most grok it. 
it's not a video, but I tried to be as detailed as possible here: http://pedrorijo.com/blog/play-slick/ code at https://github.com/pedrorijo91/play-slick3-steps
Yeah, totally agree with that and is one of the reasons for me being curious about the painpoint's Krever has encountered. I'm curious if there is more stuff like that that I haven't encountered yet with the simple command line apps that I have written.
Not much to reason about when there aren't many features ;).
I'm glad to see progress in any way on scala and it's ecosystem of tools. I really have to wonder though about twitter, and with all mixed reports I see and hear about its business. Does it really makes sense for twitters engineering team to spend such resources on a scala compiler rather than on more business oriented dev work?
the course costs $1900 USD, can anyone who have attended these kind of courses from lightbend tell me if its worth? EDIT: I forget to mention that my intention is to get contracts for working remotely.
That is correct. As we mention in rsc's readme, our mission is to complement official compilers and assist with their evolution through our experiments. We are aiming to discover actionable insight into Scala compiler architecture and language design that will help compiler developers at Lightbend and EPFL to optimize their compilers for the benefit of the entire Scala community.
You're using a mutable map, which has no semigroup instance AFAIK. If you meant to use an immutable map, you might want to check your imports.
Now make it an sbt plugin and you're sold
I'd just go with a more imperative approach. val format = raw"(\d\d):(\d\d):(\d\d),(\d{3}-\d{3}-\d{3})"r val fiveMinutes = 5 * 60; case class TimeAndCost(var time: Int, var cost: Int) val timesByNumber = for { m &lt;- format findAllMatchIn input } yield { val number = m.group(4) val secs = m.group(1).toInt * 3600 + m.group(2).toInt * 60 + m.group(3).toInt number -&gt; secs } val totalCostsByNumber = collection.mutable.Map.empty[String, TimeAndCost] for { (number, callTime) &lt;- timesByNumber } { val cents = if (callTime &lt; fiveMinutes) { callTime * 3 } else { (callTime + 60) / 60 * 150 } totalCostsByNumber get number match { case Some(tc) =&gt; tc.time += callTime tc.cost += cents case None =&gt; totalCostsByNumber.update(number, TimeAndCost(callTime, cents)) } } var total = 0 var discount = TimeAndCost(0, 0) for { (_, tc) &lt;- totalCostsByNumber } { total += tc.cost if (discount.time &lt; tc.time) { discount = tc } } val cents = total - discount.cost
At our company we use private RawConfig class and private initial value of it, which can't be created anywhere except parse function, which parses argv *and* transforms into final CliConfig. Feels quite safe.
Just wanted to note that - in my experience - as programmers get more... seasoned... they still love to learn new things, experiment, et al, but it's difficult to elicit "genuine enthusiasm" with the latest FOTM language or trend as it relates to their job. They just want to solve problems well, have a positive impact, and work with good people. The last recruiter that got in touch with me seemed quite shocked that I wasn't leaping at the opportunity to program in Haskell full time. I told him I have no problem programming in Haskell - or any other language - but that I was well past the point of *that* being what excited me about a job (as opposed to problem domain, pay, benefits, work environment, etc.). After all, if I really want to program in X or learn Y, I do it. I don't wait for someone to pay me to do it. Needless to say, the phone conversation didn't last much longer. ;-)
When starting the interviewing process, it's best to understand your needs (clearly) and tailor the process to meet them. Hopefully the candidate does the same for their needs. Generally, positions fall into one of a few categories: junior, senior, and leadership role. In addition to the category, you need to ask yourself if "hitting the ground running" is a key component. Of course, that could mean different things for different roles. For example, "hitting the ground running" in a leadership role may imply being able to wrangle a team quickly and utilize them effectively, but not need to understand (immediately) all the moving code pieces. Or, someone might have recently left who was the sole owner of a system that you need a replacement to be able to take over quickly and learning a new programming language may be a serious cost to the company during that downtime. From your other replies, I gather you don't need someone to "hit the ground running" but would like the "smart and gets things done" candidate who is eager learning new things? If so, your questions really need to be geared towards reducing all the other risks that would impair their ability to be successful in your environment: * Can they communicate effectively? * Do they ask questions? * Can they look at unfamiliar code and make forward progress? * Can they work with others to solve problems at a high-level? * Is there domain knowledge/experience that may really be a benefit (SQL, Kafka, ...)? Assuming you've gotten your "fizzbuzz" question(s) out of the way, one of my favorite things to do with a candidate (usually via phone) is to talk through their work history in chronological order. This is done by asking the same set of questions for each job on their resume: 1. What was your role and responsibilities? 2. Who did you report to, what was it like working for him/her? 3. If I called (#2) what would they tell me about you? 4. What did you learn at this job? 5. How did you apply what you learned (#4 from previous job) at this job? 6. Why did you leave? I've found that as long as you are honestly listening to the answers, and not projecting or judging the answers, that by the 2nd or 3rd job, candidates become very open and some enjoy the trip down memory lane. You - the interviewer - can also begin to take notes: * Did they learn anything (note: I've found the candidates that became great employees talk about the non-programming skills they learned)? * Did they successfully apply what they learned from the previous job in their next? * Can they step out of themselves and introspect (question #3) honestly? * Do they seem to have the same impression of their bosses? * Is there a pattern as to why they left all previous jobs? If they are young, ask about college classes and team projects they may have worked on. At some point, you're going to be taking a leap of faith. And so are they if they accept an offer. Good luck! :-)
ScalaCourses.com offers 365 days of support to Scala and Play Framework students.
Hey, I think you should be able to do .mapAsyncUnordered(8)(docs =&gt; collection.bulkInsert(docs.toStream, ordered = false)) and this allows 8 inserts in parallel and emits backpressure, if these futures can not finish as fast as the incoming data. So you should be fine there without `buffer` `''Backpressures when''' the number of futures reaches the configured parallelism and the downstream backpressures `
That Java code looked nice at first. Then a chill ran down my spine as I realized that all actual application logic is now a `String`. Anyway, here's some quick and dirty scalaz [(@ scastie)](https://scastie.scala-lang.org/oleg-py/HoJ45ha3RCeJpANJHQU3Tw): import scalaz._, Scalaz._ object Solution { val (secondCost, minuteCost, changeThreshold) = (3, 150, 5 * 60) case class CallsInfo(number: String, durations: List[Int]) { def totalPrice = durations.foldMap { dur =&gt; if (dur &lt; changeThreshold) secondCost * dur else minuteCost * (dur / 60d).ceil.toInt } } def parseLog(log: String): List[CallsInfo] = { val parsedRows = for { line &lt;- log.split("\n").toList Array(duration, phone) = line.split(",") Array(h, m, s) = duration.split(":").map(_.toInt) } yield Map(phone -&gt; List(h * 60 * 60 + m * 60 + s)) // Concat duration lists with same phone # // then convert key-value pairs to CallsInfo parsedRows.concatenate.map(CallsInfo.tupled).toList } // Sort by total duration, descending, then phone #, ascending implicit val discountOrder = Order.orderBy(-(_: CallsInfo).durations.sum) |+| Order.orderBy(_.number) // Skip number with longest total duration def solution(log: String) = parseLog(log).sortWith(_ &lt; _).drop(1).foldMap(_.totalPrice) def main(args: Array[String]) = println(solution(Array( "00:01:07,400-234-090", "00:05:01,701-080-080", "00:05:00,400-234-090" ).mkString("\n"))) } 
Yes!! Thank you!
Looks alright; a few minor things: * You can use a `"""` string rather than the `Array`/`mkString` business. * You shouldn't need `sortWith`, I'm sure there's a function to sort directly. * Why the emphasis on everything being a `List`?
Does it work with function call inlining? You mention that the rules have to make the graph smaller but inlining usually makes it larger (but could enable further optimizations that might make the end result smaller).
- `"""` string makes it a bit too easy to slip an empty string in the beginning and/or the end, and the assignment says those aren't expected anyway. Didn't want to change the code to filter empty strings away just for convenient testing - Couldn't find one, actually. The closest I could get is to call standard library `sorted` with explicit evidence `discountOrder.toScalaOrdering`. - For various reasons - `String#split` gives an `Array`, and subsequent map does too, so I need to convert it to something with `Foldable`. - `Map#map` with a function not returning a tuple gives statically `Iterable[A]`. It's `List[A]` at runtime, however. - I also use `List`s in data types because for them `mappend` has the desired behavior (concatenation) for folding rows, tho `NonEmptyList` in resulting data would be a more natural choice (here I just got a little bit lazy :)
He just kept talking about how happy everyone is in Haskell and how they are way more productive. Andddddd I'm gonna try it and report back here if it is BS
This is a library for you to write rules for your own ASTs. So if you want to implement function inlining, you can. We mention making the graph smaller because some algebraic transformations, when applied could create either infinite loops or a divergence in the size of the graph. There is no control on that currently. We would like to implement cost based optimizations and perhaps some known recent heuristics for graph search (in the graph of ASTs where an edge is between any two legit transformations).
 [Play Framework Tutorials](https://www.youtube.com/playlist?list=PLYPFxrXyK0Bx9SBkNhJr1e2-NlIq4E7ED) by Radix Code.
Yeah that definitely works and I even thought about doing that but it requires extra effort that in an ideal world shouldn't be required.
I was just discussing with a colleague today how the best way to optimise a DAG of a batch scheduler would be to use scala pattern matching to transform the graph, and then I see this: a library that makes it easy. Looks very neat. 
I think the point is there is a nice middle ground, i.e. Kotlin is an example of that (and it has a very fast compiler, which is also deliberate).
It also heavily depends on the type/style of code that you write. If you use a lot of cats/scalaz/shapeless/slick then you can expect insanely large compile times. Even incremental compiles can hurt. On the other hand if you use scala as more of a hybrid between FP and OOP, then its very bearable
In the article the link text is https://github.com/scala/scala/releases/tag/v2.13.0-M2 but it actually goes to https://www.scala-lang.org/news/release%20notes which is a 404 for me.
From the article: &gt; Apparently, calculating gradients using Free Monads is not the best idea from the performance point of view, but it could very useful for educational and experimental purposes. Yep. Not only that, free monads also seem to be an overly complicated abstraction for something as simple and elegant as gradient descent. 
Same went with the cake pattern, iteratees, and so many other approaches which once were thought to be a good idea and became hot hype for a few months until everybody wised up and realized these were all terrible ideas. Sounds like `Free` is headed this way too.
Given that the site is Jekyll-rendered Markdown, the source is probably: ```[https://github.com/scala/scala/releases/tag/v2.13.0-M2](release notes)``` instead of ```[release notes](https://github.com/scala/scala/releases/tag/v2.13.0-M2)``` Edit: I made a [pull request](https://github.com/scala/scala-lang/pull/731).
Good on you! Didn't know it was on github
&gt; Sounds like Free is headed this way too. I don't think so. While the cake pattern is generally a bad idea (at least nothing has been able yet to convince me otherwise), `Free` is very powerful and hyped a little bit, but generally a good thing. I would compare `Free` with akka actors: it's quite low-level and especially good for library authors, and usually you can (and should) get around with simpler alternatives and more high level constructs. Final tageless comes to my mind here. But in certain situations it is a very good if not *the* best solution.
https://github.com/scalafx/scalafx may fit the bill
&gt; The module system in ML (which they call function subsumption) is actually subtyping Yes, thank you, I'm aware, that's why I mentioned it. It's a *limited* and syntactically obvious form of subtyping... which was my point. &gt; XMonad Re: XMonad: What the hell are you talking about? I mean the facts are that you have to use GHC to compile your configuration, but that's just because the XMonad developers chose to make XMonad a *library*. You would have exactly the same thing in Scala if XMonad were made in Scala and chose Scala as the configuration language. So, what's your point? (Well, I guess there's bytecode, but Scala bincompat is notoriously difficult.) I *really* don't understand this XMonad thing. EDIT: Though, honestly, they did signal it really badly and perhaps fucked up by the whole "auto-recompile, dynamic reload" thing. This has nothing to do with limitations of Haskell, however. &gt; Granted in "OOP" languages its abused, but in Scala its not. You must have some sort of ideal programmers working for you. I don't. (We *do* try to solve it through code review, but that doesn't catch all of it, unfortunately the "masters" cannot be everywhere.)
Again, yes, I already know this. That's the reason I mentioned structural subtyping. Btw, O'Caml did structural subtyping 15+ years ago. (For objects and modules and sort-of for Variant types. I think they've introduced proper row type polymorphism a few years ago as well... Frankly, I'd be using O'Caml right now if weren't for the unrestricted side effects.)
&gt; Re: XMonad: What the hell are you talking about? I mean the facts are that you have to use GHC to compile your configuration, but that's just because the XMonad developers chose to make XMonad a library. You would have exactly the same thing in Scala if XMonad were made in Scala and chose Scala as the configuration language. So, what's your point? (Well, I guess there's bytecode, but Scala bincompat is notoriously difficult.) Its not hard to define an interface and load a JAR in scala. Also the bincompat issues are going away completely with Dotty (this and Scala maintains bincompat for years, they are taking this seriously now). In any case, the point was to demonstrate that its an issue. I moved away from XMonad precisely of this point (i.e. how annoying it was to manage GHC at the time). BTW techie linux people how tinker with this stuff have a lot more patience compared to typical users, who probably wouldn't bothered and deleted it immediately &gt; I really don't understand this XMonad thing. EDIT: Though, honestly, they did signal it really badly and perhaps fucked up by the whole "auto-recompile, dynamic reload" thing. This has nothing to do with limitations of Haskell, however. It is, Haskell is globally and statically linked and the dynamic module isn't useful in practice because bin compat breaks constantly &gt; You must have some sort of ideal programmers working for you. I don't. (We do try to solve it through code review, but that doesn't catch all of it, unfortunately the "masters" cannot be everywhere.) This isn't a useful counter argument, and its the same kind of attitude that Go has (i.e. if something can be abused, we shouldn't have it at all). Then this causes huge amounts of issues in the language, and then you end up having to implement said features (after the absence of not having them have done enough damage), such as in this case of Go finally implementing a decent package manager. Or in the case of Haskell, just remain in obscurity, because features which other languages have solutions for (which are a requirement for certain kinds of software) basically don't exist or are too difficult to use.
Personally I am actually a fan of structural subtyping, but it does have issues 1. Performance (you often are forced to use reflection in a lot of cases when dealing with structural typing, Go has this problem). You can avoid this with deep code analysis, but like with most things its hard 2. Some of the Scala community (ironically the pure FP kind) hate structural subtyping for some reason. Actually structural subtyping even exists in Scala now, although it got a bad name initially because it was based on reflection (back to point #1) which meant that it had terrible performance, especially in multi-threaded situations. And actually originally structural typing was broken in Scala in multi-threaded situations before they fixed it 3. Its being removed in Dotty because basically none of the community uses it, its being replaced with record types which do have some uses. 4. I still much prefers Scala combination of FP and OOP compared to OCaml. In OCaml the OOP seems tacked on, unnatural and actually a lot of people don't really use it (in OCaml you kind of have multiple camps because the 2 styles are too distinct). In Scala the merge of OOP and FP is more natural and fluent
&gt; Its not hard to define an interface and load a JAR in scala. Also the bincompat issues are going away completely with Dotty (this and Scala maintains bincompat for years, they are taking this seriously now). This is the continual problem for me and my work situation: It's always "Yeah, we'll have that next year!". If e.g. Kotlin has it *now*, what am I supposed to think? Haskell, though it can always be improved, had most of what I need 2+ years ago. So, what am I supposed to do? Sure, Scala and some libraries are taking bincompat seriously, but even *that* does not work, because you have to bump dependencies across *all* of your projects when bincompat changes and your dependencies include projects that *don't* take bincompat seriously! Even that is a ridiculous chore. Just ship source! (Source code distribution helps immensely, but as I mentioned it's not a panacea.) &gt; [Go snipe plus implications of Haskell's obscurity] Oh, *come on*! We're done, I think. I don't think any benefit can be derived from further discussion. I realize that the 'scala' subreddit is obviously biased, but you (personally) take the cake. Good programming! (Even though we disagree vociferously!)
Almost all of this is based on JVM limitations. (4): Yes, it's tacked on, but it demonstrates that it's possible, and IMO even that tacked-on design was better than what we've seen in Scala. (IMO). &gt; In Scala the merge of OOP and FP is more natural and fluent Is OOP in scala immutable? That's one of the promising ideas of FP+OOP for me, but apparently not for everyone. Cheers, I'm going to mute this conversation for now. (See my other comment.)
&gt; Almost all of this is based on JVM limitations. Nope, JVM is a VM that has almost complete type erasure. Its only real limitation is TCO, which is uneralted 
&gt; This is the continual problem for me and my work situation: It's always "Yeah, we'll have that next year!". If e.g. Kotlin has it now, what am I supposed to think? Haskell, though it can always be improved, had most of what I need 2+ years ago. So, what am I supposed to do? Sure, Scala and some libraries are taking bincompat seriously, but even that does not work, because you have to bump dependencies across all of your projects when bincompat changes and your dependencies include projects that don't take bincompat seriously! Even that is a ridiculous chore. Just ship source! I am not sure what your point is here. This is something that isn't really practically possible and yet you are complaining its not perfect in Scala (even though in other languages with dynamic libraries, barring C, you have the same problem) &gt; (Source code distribution helps immensely, but as I mentioned it's not a panacea.) You can do source code dependencies in SBT if this is a problem (although its common practice for everyone to cross publishing) &gt; We're done, I think. I don't any benefit can be derived from further discussion. I realize that the 'scala' subreddit is obviously biased, but you (personally) take the cake. I actually have been more critical of Scala than most on the reddit, but I call a spade a spade. Haskell is a language that has very strong theoretical underpinnings which makes a deliberate choice not to deviate from these principles. This has benefits and disadvantages, I was merely listing one of them. &gt; Good programming! (Even though we disagree vociferously!) You too!
 import cats.instances.bigDecimal._ // monoid instance for BigDecimal import cats.instances.map._ // monoid instance for std immutable map import cats.syntax.semigroup._ // Provides |+| operator aka Semigroup.combine. You can use cats.syntax.monoid._ as well if you want final case class TransactionFile(dataRows: Vector[Map[String, Map[Int, BigDecimal]]]) def aggregate(oneFile: TransactionFile): Map[String, Map[Int, BigDecimal]] = { val result = oneFile.dataRows.foldRight(Map.empty[String, Map[Int, BigDecimal]]) { case (accumMap, thisMap) =&gt; accumMap |+| thisMap } result } val input = TransactionFile(Vector( Map("someKey" -&gt; Map(1 -&gt; 2.0)), Map("someKey" -&gt; Map(1 -&gt; 4.0, 2 -&gt; 3.0)) )) aggregate(input) The above outputs `Map(someKey -&gt; Map(1 -&gt; 6.0, 2 -&gt; 3.0))` By the way you can make the code even shorter by using Monoid.combineAll: def aggregate(oneFile: TransactionFile): Map[String, Map[Int, BigDecimal]] = { // `Monoid` call here summons the Monoid instance of Map[String, Map[Int, BigDecimal]] Monoid.combineAll(oneFile.dataRows) } 
I wouldn't code anything in Scala because it's gay.
&gt; I think you mean monad? No. But probably it isn't a monad either. &gt; Nullable types are almost fine, which is how bugs tend to make it into production: the nesting is a subtle edge case waiting to become a bug in generic code that uses them. When you do nesting you see it. It's not an edge case it's a well-known behaviour. &gt; A generic function that uses null to track error cases is likely to break when used with a nullable type, and this wouldn't necessarily be visible anywhere from the outside; there's no way to know which generic functions are safe to use with nullable types other than manual documentation or the like. This is a pretty weak argument since Option won't save you either from bugs. Also, it can be null too. &gt; It's always better for anything generic, because of the nesting thing. It wastes cpu cycles and memory so, your argument is already invalid. Also, give me an example where you need nesting. &gt; Which is a tiny niche, and I'm not even convinced FP does have a hard time with it. "FP" allocates memory too and it still needs to unwrap wrappers to check the content. Not a "tiny niche" either - see how bad is the performance with autoboxing in java. &gt; Huh? Either et al are much safer for resource management (you can bracket a function that returns Either the normal way, you only need a finally-style construct if you're interleaving opening resources and things that may fail, and that case is no harder than it is with exceptions). 1. Linear typing is the safest and doesn't have any overhead; 2. try-with-resources are also safer with *less* boilerplate and they *actually* close your resources. Either is unrelated to resource management - it's just a way to wrap error types and exceptions. &gt; Nonsense. And if the language is already close to the complexity budget (or beyond it, in the view of some potential users) then that's a reason to simplify it, not to add more. Then scalaz is over the budget... &gt; But when something's just a normal type that follows the normal rules of the language then there's no extra complexity, because people can just read the code if they get confused. "Read scalaz to understand category theory" - yeah, of course... &gt; Nonsense. Maybe in a few very specialized cases, but most of the time the unreasonableness of exceptions costs far more productivity. Please, don't tell me that your indexing function returns with Option[T]... Also, how are exceptions unreasonable? I've already told you why Option and Either are not enough for error handling. &gt; If you don't nest, both Option and non-disjoint union work, but when you do nest (which can easily happen by accident when combining two functions that should work on the face of it) only Option will work. They're not the same therefore ?[T] shouldn't be used as a replacement of Option. And nesting is a rare case which can be avoided easily... &gt; Checked exceptions are a major source of cumbersome code in Java; many Java programmers (and I believe also the original designer) regard them as a mistake, and no other major language has adopted them. 1. Because they're poorly implemented and not used properly; 2. they hate it because they want to ignore most errors to simulate productivity(they give less f*** from my experience); 3. functional error handling have more boilerplate and overhead. Checked exceptions would bring far more safety than you can imagine... &gt; To be an actual sum type it would have to behave like Option. Nope. &gt; Non-disjoint unions are much harder to understand than the current Option implementation which is a plain old datatype you can easily read the source to or reimplement yourself. The way Option is implemented in scala is everything but simple. On the other hand, null is a simple value and type. Also, look at how I presented ?[T] to you - it's much simpler than Option. &gt; Ok, the actual thing that's wrong is that exceptions and null invalidate a huge class of obviously correct program transformations that are useful for maintaining code; you can no longer blindly reorder lines, or remove useless field accesses. But the fact that they're implemented as language-level special cases is a big hint that they have this kind of issue. If they behaved like plain old datatypes, they should be implemented that way. Now, this doesn't make any sense. &gt; "Need" is a strong word, but the generalisation is often useful. Having to repeat the same piece of logic twice is bad for maintainability. I've meant that you don't need the generalization presented in the article and that's not useful either because it can't handle error properly. &gt; The JVM isn't the only Scala compile target, and is constantly being improved. But the only usable target, for sure. &gt; It's not worth compromising the language semantics for the sake of small-scale present-day JVM-specific performance issues; Those are not JVM-specific: those are just simple performance and memory overhead issues what you ignore. &gt; performance can always be improved in the future, Yeah, it can be but ignoring the most obvious optimizations is not a good sign... &gt; whereas if the language itself is wrong then everything we write will ultimately have to be rewritten. You know it has flaws and everything will be rewritten anyway. &gt; Professionalism means delivering the things that are actually useful, not being perfectionist about performance. Professionalism means giving the best you can do in the industry. &gt; Right, and how popular are those in the industry? I'm all for those things but even FP languages without them are ahead of the industry's safety curve. Rust have linear typing and it's a really popular language - probably it has already surpassed scala. Idris is also popular but between CS ppl. &gt; Computers are cheap. No, they are not. &gt; How much work the GC is doing is not worth worrying about unless it's affecting the actual functionality. (Which it certainly can, but "useless pointers" are not a problem in themselves). Memory overhead, startup time and lagging are problems with the JVM. Ignoring them won't make them disappear. &gt; It's fair to assume that if a language focus is safety then it will end up less productive than if the language focus is productivity. If you don't think about safety then your productivity will be and illusion because you **will** loose time in the long run. For example: people say scala is a safe language. They also say that python is productive. The latter is not true - from experience I can tell you that at my company we used to start stuff in python and it was always problematic and sometime I ended up rewriting the modules in scala - in far less time resulting far less and far more concurrent code. Most of the time when ppl say something is "productive" they mean it can let them run and create unstable stuff faster. That's not professional.
A mini project I did on vacation 
&gt; This is a pretty weak argument since Option won't save you either from bugs. Also, it can be null too. With `Option` everything is compositional and can be broken down and tested separately. If `f` passes its tests and `g` passes its tests, `f(g)` isn't going to suddenly break &gt; Also, give me an example where you need nesting. Representing two different kinds of absence, e.g. user hasn't filled in their car details yet versus user doesn't have a car. &gt; Either is unrelated to resource management - it's just a way to wrap error types and exceptions. It's a way to represent the possibility of failure that's unrelated to resource management - which is what we want, since they're separate concerns - unlike exceptions, which require your resource management code to do special things (e.g. `finally` blocks) to ensure they interact correctly. &gt;&gt; To be an actual sum type it would have to behave like Option. &gt; Nope. Look, `?` isn't and could never be a sum because of its nesting behaviour. 1+1+1 is not the same as 1+1. &gt; The way Option is implemented in scala is everything but simple. On the other hand, null is a simple value and type. Also, look at how I presented ?[T] to you - it's much simpler than Option. Nope. It's not simple on any level, it's a magic special case with a bunch of special rules that no other type in the language follows. &gt; I've meant that you don't need the generalization presented in the article and that's not useful either because it can't handle error properly. What are you talking about? &gt; Rust have linear typing and it's a really popular language - probably it has already surpassed scala. Idris is also popular but between CS ppl. Rust only has affine typing and is not "really popular" by any means. Rust and Idris are a tiny tiny fraction of the industry. (And I'm sure neither will ever adopt nulls or checked exceptions).
Can confirm. Accidentally clicked on "New Scala Project" in intellij once and became temporarily bi-curious.
&gt; How do I run the server from inside an App instead of through the IDE? Are you using sbt? If so, [look through the answers here](https://stackoverflow.com/q/24238060). Otherwise, look into creating a fat JAR with IntelliJ, and execute it the same way described in those answers. &gt; How do I send a command like the one above through a method (the one being called at button click) instead of the command line? In a script, I'd do this, import scala.sys.process._ val res = Seq("bash", "-c", """curl -i -H "Content-Type: application/json" -X POST -d '{"url":"http://www.reddit.com/r/scala"}' http://127.0.0.1:8765/url""") !! 
Agree, seems to be a pretty bad fit imho.
Right now I'm trying to port a registration survey to Json shema. We're experimenting with moving all of our "formy" stuff to Json schema because we can then render it using mozillas react Json schema form. Basically, it'll let us dynamically have validatable new forms for our Android app that we can ship out and then get offline responses for. 
&gt; With Option everything is compositional and can be broken down and tested separately. If f passes its tests and g passes its tests, f(g) isn't going to suddenly break I don't see how ?[T] would break with f(g). If you know it can't nest then you can catch it in the tests of f or g. &gt; Representing two different kinds of absence, e.g. user hasn't filled in their car details yet versus user doesn't have a car. Please, be more specific. I can think about your problem like this for example: case class User(id: Int, cars: Vector[Car]) // or case class User(id: Int, car: ?[Car]) case class Car(brand: String, ....) and we still know what is missing. &gt; It's a way to represent the possibility of failure that's unrelated to resource management - which is what we want, since they're separate concerns - unlike exceptions, which require your resource management code to do special things (e.g. finally blocks) to ensure they interact correctly. Resource management SHOULD be automatic because there's no safe way to do it otherwise. Also, we don't need `finally` - try-with-resource close the resources without it and it also catches errors. It's like a flexible scope but without the overhead. &gt; Look, ? isn't and could never be a sum because of its nesting behaviour. 1+1+1 is not the same as 1+1. It's theoretically a sum type, in reality it'd be just a nullable type. &gt; Nope. It's not simple on any level, it's a magic special case with a bunch of special rules that no other type in the language follows. How is it magical? Nullability is there in every language and it'd actually give you null-safety unlike Option which can be null. &gt; What are you talking about? I've shown you samples previously that we can easily avoid monadic abstractions. &gt; Rust only has affine typing Correct me if I'm wrong but in Rust every variable must be used and once at most - which is linear typing. Affine typing is that the variables are used at most once. But not using a variable will fail the compilation and that's why it isn't affine. &gt; and is not "really popular" by any means. I'm pretty sure it's far more popular than any functional language out there - including scala. Adoption is another question. &gt; Rust and Idris are a tiny tiny fraction of the industry. Idris is a research language and rust is young - the latter will be more widespread in the future. &gt; (And I'm sure neither will ever adopt nulls or checked exceptions). Rust don't need nulls because it has zero-cost abstractions which means it can get away with Option. Rust also doesn't have any kind of exceptions which make it really hard to handle errors but it doesn't matter in terms of performance because of the previous reason. Idris is a research language and I don't think its developers care about performance but they do care about dependent types. You're right but those languages have different runtimes and goals.
This was certainly a factor, thanks for the help.
Great info, thanks for the help. With switching to an immutable map and adding the imports for the instances I am able to compile and run now, and I also like your improvement suggestions. Monoids are the next thing I will be spending some time on.
&gt; Final tageless comes to my mind here. But in certain situations it is a very good if not the best solution. The reason Free is useful with Gradient Descents is because it lets you rewrite and simplify programs/expressions. You can't do that with tagless final.
I agree. What I wanted to say is: in many situations (I would say: most of the time), tagless final is sufficient and more lightweight to use. Partly because it seems to me that Scala makes it rather hard to work with Free when it comes to combining algebras in comparison.
&gt; I don't see how ?[T] would break with f(g). If you know it can't nest then you can catch it in the tests of f or g. You have to test every possible combination - when you have any generic function you have no way of knowing whether it's going to work correctly when its argument is a nullable type or not, so you can no longer unit-test combinators `f`, `g`, `h` and `i` in one place and functions `r`, `s`, `t` and `u` in another, and be confident that each pair will work, instead you have to integration-test every pair. Or you have to test every generic function twice, once with nullable and once with not, which is doable when it's just null but quickly becomes exponential when you have other effects as language builtins (test each function with a sync non-null callback that doesn't throw, a sync non-null callback that throws a checked exception, a sync nullable callback that doesn't throw, ...) &gt; Please, be more specific. I can think about your problem like this for example: Even with that representation, it's useful to be able to write a query for e.g. user's car by user ID, and distinguish between "user does not exist" and "user exists but has no car" cases (`None` and `Some(None)` respectively). You can always come up with an ad-hoc solution for any given case, just as you never absolutely need generics. But it's inconvenient to not be able to compose two things without worrying about their details. &gt; It's theoretically a sum type, in reality it'd be just a nullable type. The "nullable type" you're talking about isn't and can't be a sum type, at least in generic form, because of its inconsistent behaviour when nesting. Unless I'm missing something. &gt; How is it magical? It has the bizarre different behaviour when nested, which is something you couldn't implement yourself in a custom type without doing something very nasty like reflection. &gt; Nullability is there in every language Plenty of language don't have it. E.g. you've already mentioned Idris. &gt; and it'd actually give you null-safety unlike Option which can be null. In day-to-day Scala the only place you ever see `null` is Java interop. Under a theoretical Scala-with-nullable-types you'd still see null pointer exceptions from Java interop, because Java doesn't have nullable types. So it doesn't give you any more safety in practice. Removing `null` from core Scala entirely might be possible, and might allow e.g. Scala native to adopt a Rust-style implementation of `Option`. I think that's more achievable than migrating Scala away from `Option`; even if you added `?` to the language, the standard library couldn't move to it without breaking huge amounts of existing code (because of the nesting behaviour). &gt; Correct me if I'm wrong but in Rust every variable must be used and once at most - which is linear typing. Affine typing is that the variables are used at most once. But not using a variable will fail the compilation and that's why it isn't affine. No, it has affine types and some limited checking for unused variables, but not proper linear types. https://gankro.github.io/blah/linear-rust/
Just wanted to note that via a suggestion, this repo has been converted into a g8 template and has now moved: https://github.com/massung/scala-js-skeleton.g8
So of the hundreds and thousands of projects, [three](https://index.scala-lang.org/search?q=docs&amp;contributingSearch=true) qualify as friendly to contributors - Akka and Play (Lightbend), and Cats. Sorry, but that doesn't feel right.
Can you point to some of these ports of react to scala.js? (Anyway the question with them always is interop with other things, like React components or jQuery plugins)
&gt; Free [...] lets you rewrite and simplify programs/expressions. You can't do that with tagless final. Yes you can! It's not nearly as intuitive, but it has some nice advantages wrt to composabilty and modularity. http://okmij.org/ftp/tagless-final/course/optimizations.html
I was gay before I clicked it, and now I'm chums with [Hard Gay](https://www.youtube.com/watch?v=GDEQcqmX7HI).
You're using `docs` as the query string, which is why you're seeing only three results. [I'm seeing 27 results here](https://index.scala-lang.org/search?q=&amp;contributingSearch=true). Edit: From the article (emphasis mine): &gt; For example, if you enter a search term related to documentation like “docs”, **the search results will contain issues related to documentation** for each project.
Ok, thanks for clarifying!
Hi, we pre populated the contributor data with a handful of projects, but of course, we are missing a bunch. This is what you need to do to add your project to the contributor search: * Login with GitHub in Scaladex (https://index.scala-lang.org/login) * Select one of your project (https://index.scala-lang.org/search?q=&amp;you=%E2%9C%93) * Click on the Edit button (for example: https://index.scala-lang.org/edit/masseguillaume/scalakata2) * Fill in the `Contributing Info` section * Profit
How high is demand in Scala aside from Big Data projects? I have the feeling Scala has an enormous potential for other projects, whether small or big, but where I live there's only demand for Scala developers doing Big Data. How's it at your location?
Thanks, I will try to add some of my projects.
https://www.twilio.com/docs/libraries/java make this Scala friendly. Either wrap it and hide java ugliness or create pure Scala SDK.
Great stuff, will try it in the days to come! :)
I'm hoping someone(s) here can help me with a couple issues I've had with this template: First, in the default.properties, I have a `use_yarn=true`, which is then put into the resulting build.sbt as `useYarn := $use_yarn$`. However, I'd very much like it to be (what Giter8 calls) a "truthy", but I haven't found any examples of this in use. Is there a way I can get Giter8 to output `true` or `false` given the value set to `use_yarn` instead of just a simple string substitution? Second, when I run `fastOptJS::webpack`, the source-map-loader fails to find all the scala source files for scalajs: [info] Cannot find source file 'https://raw.githubusercontent.com/scala-js/scala-js/v0.6.19/library/src/main/scala/scala/scalajs/js/Dynamic.scala' Obviously the source file exists if I put it into a browser or use curl, so I'm wondering if there's a setting or such that I've missed that prevents fetching it from a website? Thanks for any help/suggestions!
No offense but if Twillio wants a scala API they can pay me to write it. 
The canonical first project is to make ~~an entire Linux distro~~A small To-Do list system. It's up to you what it does, but if you can build a reasonably elegant back-end and get it to work with files, databases, and a basic front-end, you're ready to tackle the tough stuff.
So you can get started and learn Scala?
I live in Vancouver, BC. My company is using it for game server backends and tools. Off the top of my head there is also Hootesuite, Bench Accounting, Unbounce in town which are web and financial tech companies.
https://github.com/karan/Projects contains a list of small project ideas
My last 3 jobs over 5 years in Seattle have involved Scala backends, none of them big data.
I use vim with ensime-vim and it works fantastically. I wouldn't go back to intelliJ now.
Do not write an application with a million lines of code. Write lots of smaller applications that interact. This way you can benefit from the new code earlier and swap out pieces or technologies quicker in the future. For more info, read about microservices.
There are plenty of non-proprietary open source projects to work on. Why would I volunteer to improve the commercial product of a $2B company?
&gt; You have to test every possible combination - when you have any generic function you have no way of knowing whether it's going to work correctly when its argument is a nullable type or not, so you can no longer unit-test combinators f, g, h and i in one place and functions r, s, t and u in another, and be confident that each pair will work, instead you have to integration-test every pair. That can be said about any non-trivial function. &gt; Even with that representation, it's useful to be able to write a query for e.g. user's car by user ID, and distinguish between "user does not exist" and "user exists but has no car" cases This is another bad example: your query should return with a Vector[User] or a ?[User]. If the user is null then you know you don't have the user. If you've the user but the info about its car is missing then there's no car. You're overcomplicating it. &gt; (None and Some(None) respectively). You can always come up with an ad-hoc solution for any given case, No, you're coming up with ideas which doesn't make any sense. Give me some data model and let's see if it's viable. &gt; just as you never absolutely need generics. Generics is unrelated. &gt; But it's inconvenient to not be able to compose two things without worrying about their details. You CAN compose them. Your only problem is nesting a nullable type with another nullable type which is a **really** tiny niche, like no practical use case... &gt; The "nullable type" you're talking about isn't and can't be a sum type, at least in generic form, because of its inconsistent behaviour when nesting. Unless I'm missing something. Nullable types exist and sum types - believe it or not - can be simple pointers and you can infer the pointer at runtime. Like pattern matching, because that's how we have "sum types" in scala. &gt; It has the bizarre different behaviour when nested, That's not bizarre at all - when you nest values you've nested values. When you nest nulls you've a null. You know what's bizarre? Expecting an Option[Int] and receiving null... &gt; which is something you couldn't implement yourself in a custom type without doing something very nasty like reflection. What kind of reflection are you talking about? It'd only require a null check... &gt; Plenty of language don't have it. E.g. you've already mentioned Idris. There's null in idris. It needs that for FFI. And as I've said, idris don't really care about performance. &gt; In day-to-day Scala the only place you ever see null is Java interop. It's a pretty superficial argument. We do have nulls and you can meet them at any time. Nothing can save you from nulls in scala. &gt; Under a theoretical Scala-with-nullable-types you'd still see null pointer exceptions from Java interop, because Java doesn't have nullable types. So it doesn't give you any more safety in practice. Yes, it'd give us a safer and less bloated scala. &gt; Removing null from core Scala entirely might be possible, and might allow e.g. Scala native to adopt a Rust-style implementation of Option. Rust's eliminates the cost of Option. I don't think scala-native can do that. &gt; I think that's more achievable than migrating Scala away from Option; even if you added ? to the language, the standard library couldn't move to it without breaking huge amounts of existing code (because of the nesting behaviour). Show me what it'd break. null is used *a lot* in the standard library and adding ?[T] would just make it easier to work with. &gt; No, it has affine types and some limited checking for unused variables, but not proper linear types. Affine typing is a case of linear typing where you can have unused variables. Also, I'm not so sure that it's possible to implement the detection of unused variables *by the type-checker*.
[The docs](http://www.foundweekends.org/giter8/template.html#Conditionals) seem to be broken but I **think** it should be possible using the following syntax: $if(use_yarn.truthy)$ useYarn := true $else$ useYarn := false $endif$ As outlined in [this PR](https://github.com/foundweekends/giter8/pull/332/files)
Either a small webservice like an api wrapper or a small site(http4s or akka-http), or a command line utility are both good starting projects.
Scalastyle can do away with cryptic names: &lt;check enabled="true" class="org.scalastyle.scalariform.ClassNamesChecker" level="error"&gt; &lt;parameters&gt; &lt;parameter name="regex"&gt;^[A-Z][A-Za-z]*$&lt;/parameter&gt; &lt;/parameters&gt; &lt;/check&gt; &lt;check enabled="true" class="org.scalastyle.scalariform.FieldNamesChecker" level="error"&gt; &lt;parameters&gt; &lt;parameter name="regex"&gt;^[A-Za-z][A-Za-z0-9_]*$&lt;/parameter&gt; &lt;/parameters&gt; &lt;/check&gt; &lt;check enabled="true" class="org.scalastyle.scalariform.MethodNamesChecker" level="error"&gt; &lt;parameters&gt; &lt;parameter name="regex"&gt;^[a-z][A-Za-z0-9]*$&lt;/parameter&gt; &lt;parameter name="ignoreOverride"&gt;true&lt;/parameter&gt; &lt;/parameters&gt; &lt;/check&gt; &lt;check enabled="true" class="org.scalastyle.scalariform.ObjectNamesChecker" level="error"&gt; &lt;parameters&gt; &lt;parameter name="regex"&gt;^[A-Z][A-Za-z]*$&lt;/parameter&gt; &lt;/parameters&gt; &lt;/check&gt; 
Yeesh. :(
It's a single report, with no indication of how he came to that number. You are invited to do your own load tests using Gatling: https://developer.lightbend.com/guides/play-rest-api/appendix.html#load-testing If you want to see how the Play team measures performance, the framework is here: https://github.com/playframework/prune and the latest results are here: https://playframework.github.io/prune/ I believe that 2.6.x is still shown as master on Prune. The load tests range from 30K to 75K requests per second depending on GET / POST etc based on the engines and overhead of processing, and you can also see 2.5.x available on the same page. 
http://www.scala-sbt.org/1.0/docs/Travis-CI-with-sbt.html
I noticed it was slower almost immediately. The first view of the site would take 5s or more to warm up. I switched back to netty first thing.
You and /u/m50d can continue your conversation, but here's a question for you. &gt; This is another bad example: your query should return with a Vector[User] or a ?[User]. If the user is null then you know you don't have the user. If you've the user but the info about its car is missing then there's no car. You're overcomplicating it. How would you represent this with `?[User]` (which, if I'm understanding correctly, breaks the functor laws like `java.util.Optional`)? val plateNumbers = userIds // Seq[UserId] .map(getUserById) // Seq[Option[User]] .map(_.map(getCarByUser)) // Seq[Option[Option[Car]] .map(_.fold("User ID not found".left)(_.fold("No car for user".left)(_.licensePlate.right))) // Seq[Either[String, LicensePlateNumber]] With your proposal, you either lose the compositional style or need a workaround for the nesting logic, right? &gt; You CAN compose them. Your only problem is nesting a nullable type with another nullable type which is a really tiny niche, like no practical use case... In my experience, it's a pretty common case (if "nullable" means "potentially absent"). Even if it weren't, we have an alternative that permits users to have this functionality (instead of being "almost correct").
You could work on one of the issues for the Scala hackerrank answers https://github.com/mslinn/hacker-rank
I'm not familiar with Cassandra so I'm not sure if these work, but a few ideas I had off the top of my head: * You could use many CassandraSinks all feeding into a akka.stream.scaladsl.Merge * If you want to get lower level you could build your own CassandraSource that either has many outlets or retrieves data in a more parallel fashion with something like: [CassandraSourceStage](https://github.com/akka/alpakka/blob/358bb4ff99146ac567225841d7a0122e62b01eca/cassandra/src/main/scala/akka/stream/alpakka/cassandra/CassandraSourceStage.scala) http://doc.akka.io/docs/akka/2.5.4/scala/stream/stream-customize.html
uh ok, geez, that was a suggestion, first idea that I thought would be cool Scala learning exercise and there would be people that would use it. I don't disagree with you, but I leave that choice up to OP to decide if it's something of his interest or goes against his goals. I had to check because it so seems that you are OP when you're plugging your own beliefs and interest to this.
&gt; That can be said about any non-trivial function. With generic functions in a well-behaved language you get a whole bunch of ["theorems for free"](https://people.mpi-sws.org/~dreyer/tor/papers/wadler.pdf) via parametricity, without having to test them. More generally, you can be confident that a generic function will behave the same way for any type, rather than having to test each type separately. &gt; This is another bad example: your query should return with a Vector[User] or a ?[User]. If the user is null then you know you don't have the user. If you've the user but the info about its car is missing then there's no car. You're overcomplicating it. At that point you're using `User` (which might be costly to load) to emulate `Option` so that you can stick it inside `?`. &gt; No, you're coming up with ideas which doesn't make any sense. Give me some data model and let's see if it's viable. In any specific data model there will be a better replacement for `Option` or `?`, the point is to be able to reuse all the generic functionality that `Option` or `?` already has. Take any two of your favourite use cases for `?` and put them together, and that's your use case. &gt; You CAN compose them. Your only problem is nesting a nullable type with another nullable type which is a really tiny niche, like no practical use case... There's a huge difference between something that works every time and something that works most of the time. If some types of composition break, that massively slows down working with composition, because you have to check you're not hitting those cases every time. &gt; Nullable types exist and sum types - believe it or not - can be simple pointers and you can infer the pointer at runtime. Like pattern matching, because that's how we have "sum types" in scala. To be a sum type it has to follow the rules of sums, and I'm pretty sure your `?` doesn't. It's just terminology, but figured it was important to be precise since we're getting into the details. &gt; That's not bizarre at all - when you nest values you've nested values. When you nest nulls you've a null. So you admit `null` is a magical non-value? &gt; You know what's bizarre? Expecting an Option[Int] and receiving null... Which isn't something that actually happens in day-to-day Scala, and introducing `?` wouldn't help fix it. &gt; What kind of reflection are you talking about? It'd only require a null check... I mean if you try to write a plain-old-Scala datatype (i.e. not using the language builtin `null`, but with your own special value) that behaves like `?` you'd have to do some nasty reflection or similar to get it to have the same nesting behaviour as `?`. Whereas it's really straightforward to write your own `Option` (again, using your own special value rather than the built-in `None`) in plain old code. &gt; There's null in idris. It needs that for FFI. There's a value called `null` but it's restricted to an interop type (`Ptr`), it doesn't infect the whole language. You can't pass it to a function that takes e.g. `String. &gt; It's a pretty superficial argument. We do have nulls and you can meet them at any time. Nothing can save you from nulls in scala. If you really can't stop yourself writing `null`, use Wartremover. Use only reputable libraries, avoid Java interop, and you'll never see `null`. Yes it's a flaw in the language, but it's not actually a problem in practice. &gt; Yes, it'd give us a safer and less bloated scala. How? What examples that cause actual errors today would it actually help with? &gt; Show me what it'd break. null is used a lot in the standard library and adding ?[T] would just make it easier to work with. Any code that uses `Option` might break if it were replaced with `?`. E.g. if `List#find` or `Map#get` was changed to return `?`, then generic code that uses those with collections of `Option` (that also got changed to use `?`) would behave differently in the case of `None`/`null`.
Why is this in /r/Scala, and not in /r/Playframework?
Since this is r/scala, I'm going to say that yes, it makes a lot of sense for Twitter to spend money on the Scala compiler. ;)
Your question misses the whole point of streams. The entire point is backpressure. Your source cannot/should not read things faster than downstream can process them. Otherwise you just fill up your memory. 
&gt; thanks for your input. It's true, upgrading from 2.5 to 2.6 was the easiest update ever. &gt; I'll probably start a new thread for this, as you mentioned. But i just used the 2.6 starter template and downgraded it to 2.5 and i saw the app runing 416% slower in 2.6 than in 2.5. If i stay with 2.6 and use Netty it's still 147% slower than 2.5. &gt; I think that speed is very important, because it decides how much servers you need. This last comment was interesting (not the OP on Google groups though), sadly he didn't create the new thread :(
Check out the white paper: https://info.lightbend.com/white-paper-play-framework-the-jvm-architects-path-to-super-fast-web-app-register.html?utm_source=website&amp;utm_medium=learn-resource-center&amp;utm_campaign=COLL-2017-Play-Technical-Why-Is-Play-Framework-So-Fast-WP&amp;utm_term=none&amp;utm_content=none It talks about throughput and latency, which are more precise ways of talking about speed.
You shocked me there with 25 replies! 
also useful: https://github.com/dwijnand/sbt-travisci
I don't think backpressure should kick in when writing to console. BTW: The code above is significantly slower than my simple jdbc forloop. At this speed, it won't overwhelm any system with messages. (definitely not my console). 
You are sure about it? I had many times situation when printing to console killed my whole app. Try do some counter and display result of it. I'm not saying that's the case, just an idea.
I bet he did a forEach over a string instead of a list of strings
This is a first introductory tutorial to SoundProcesses - it doesn't introduce yet any of the objects defined by SoundProcesses, but gives a walk through until the point where the transactional variant of ScalaCollider is used to play sound. I'm planning to add more tutorials over the next weeks.
You may want to try with Sink.ignore rather than println to check if writing to the console creates back-pressure.
I just wish reddit didn't run on garbage software
oh god. Every so often the iPhone web UI freezes and I don't know whether the connection died or the JS got it or something... 
I had no idea this existed! Thanks for the share
Not familiar with Scala Native so maybe I miss a few things, but wouldn’t it be more efficient to launch a Scala or Java based web server directly?
So cool ! Thanks a lot for sharing ! 
i think the reason is functions overloading, Imagine you declared two functions as follows : def doSomething(x: String, y:Int) def doSomething(xy: Tuple2[String, Int]) if the compiler supported the syntax you discussed, the compiler would not know which of the two functions you intended to call.
Have you tried increasing the `setFetchSize(1000)` value?
i am sure there are lots of Scala JS tutorials out there but ScalaJS is of no use if we dont know how to connect it with latest Akka http reactive server.
Nah, seems to work as expected. You didn't tell it what output type you want, so it just takes what's there. You'll have to be explicit about types with `shouldBe`, because that one is like `==` in terms of typing, it's entirely untyped. Try with ```ScalaCompilerBug.brokenFunction[String]```
Just to be clear, resolving type T through implicit resolution is expected?
It really depends on what you mean by "efficient" -- Scala Native compiles down to binary executables via LLVM bytecode, so it's chief performance advantages over the JVM are: 1. Smaller binaries, 2. Quick startup time, and 3. Lower memory overhead This is a natural fit for command-line tools, but I think it's also really helpful for small "sidecar" web services, like health checks or metrics scrapers, where you want to keep container size and memory overhead as low as possible.
`shouldBe`, or anything else in that context, doesn't provide any specific type bounds. So it'll pick any implicit that works. That's even more unrestricted than specifying ` ScalaCompilerBug.brokenFunction[Any]`, because in that case neither will match. And yes, `T` can also be inferred from the implicit... I think. Would explain what's going on here.
I understand shouldBe doesn't provide a specific type. Maybe writing this as a test made it confusing. I was just trying to show a usage of the function. &gt; And yes, T can also be inferred from the implicit... I think. Would explain what's going on here. This is the part that I'm trying to emphasize. It does explain what's going on here, but is what's going on part of the language spec or a bug?
Pretty sure it's part of the spec, it allows you to do path-dependent typing. See shapeless, where you can let the compiler infer the type from the implicit.
This seems unrelated to path-dependent typing. I'm some-what familiar with shapeless... it's a large project and I'm not sure how it relates exactly... I appreciate you trying to help, but if you don't know if this is part of the spec or not, then you're making this more confusing... 
Yes, this is normal. When you write `brokenFunction(testInput)`, you haven't constrained `T`, so implicit search is free to pick any `Converter[Foo]` it wants. In practice this enable patterns like: https://milessabin.com/blog/2011/07/16/fundeps-in-scala/
Let me differentiate. Inferring generics with implicits allow you to do the things shapeless does. It just happens to infer the path-dependent types.
My understanding is that Parboiled2 is built for speed, and uses complicated side effects to achieve this. This makes complex rules a bit more difficult (and sometimes impossible) to write. This is just what I have read up on the topic. In our project, we use Parboiled, not Parboiled2.
How about [worksheets](https://github.com/scala-ide/scala-worksheet/wiki/Getting-Started)? I'm not sure if there are programs other than IDEs that can run them though.
It's only consistent isn't it? An implicit parameter is just like any other parameter except that it can be filled in by the compiler.
We're looking at moving our training to the [Scala Jupiter kernel](https://github.com/jupyter-scala/jupyter-scala). You basically write slides with markdown and you can sort of embed code in there that can compile/recompile in the slide essentially acting as a REPL inside a slide. So depending on how your content is structured you can have a separate set of slides/notebooks per topic to keep it modular. When we get to more complicated subjects we're planning to have project(s) in GitHub with tests and unimplemented code to get them to pass the tests. If you're interested I'll pull out some links my colleague sent me to get started with it.
I'm working on two things in particular — 1. My personal extensions library for Scala to add some useful data structures and TypeClasses to get things done for which I would otherwise have to write boilerplate code (e.g., retry functionality for Future and Try). 2. I haven't started this yet, but I'm thinking of writing minimal compiler for Haskell/Idris as a fun experiment. I do not currently have a an execution engined plan for this, but I'll probably translate it to Scala code, or else if time permits C code.
It's working perfectly normally - you only have an Int converter in scope, and your T is inferred from your return type. Implicit search first looks in the current scope for any implicit that can be resolved for some type T. It finds your Int one, and then decides that Int is the return type. It's perfectly valid, according to the type signature, and the only possible correct return type according to the functions in scope. It isn't dangerous. If you are concerned with the inferred type, you have to give it one somewhere. If you want the converter to be picked by compiler based on type, with lots of possible converters in scope, you have to choose a type. For example, by assigning the method call result to a typed value other than Seq[Int], you'd get a compiler error showing you were missing the necessary implicit. Stated another way, if you imported the other instance into scope in either test, you would see that your implicit is ambiguous via a compiler error. Scala allows multiple instances of a type class for a type in scope, which is flexible butsometimes annoying. The most obvious way to fix that problem is to give a desired type at the call site. Watch [Implicits without the import tax](https://vimeo.com/20308847) for better ways to organize your implicit instances. If you'd have placed them on the companion of your typeclass, as described there, you'd also have produced an ambiguous implicit error in your tests. It's typesafe, because you only gave it one type to choose (String in the first test, and Int in the second). 
Does SoundProcess work with SCALA.JS?
Fastparse is significantly easier to use (especially when [things go wrong](http://www.lihaoyi.com/fastparse/#DebuggingParsers)), Parboiled2 is significantly faster ([~4x on my Scala parser](http://www.lihaoyi.com/fastparse/#Performance)).
Most common complaint I've heard of this language is that there are so many features it creates too many possible ways of implementing any given solution. Given this context, is there any way/plans to enforce a subset of the language to fix this problem? 
Any idea how Paraboiled2 compares with Haskell's Parsec?
Have you tried the RAML yet? Is it something that you think is relevant? We maintain most of our schemas in RAML, which are converted to Scala case classes.
There is also [atto](https://github.com/tpolecat/atto). I haven't used it but it apparently isn't faster than Parboiled2 - just similar to Haskell's Attoparsec. FastParse so far worked perfectly for my use case (good performance while still easy to use).
Atto is definitely interesting, but I think I'm gonna go with Fastparse for now. It's beginner friendly, and documentation is also fairly good. I don't care about speed that much there won't be a lot of code to parse in a hobby programming language anyway.
I have not compiled it with that target; the main obstacle will be to publish the dozens of libraries it depends on for Scala.js as well. I don't think it is impossible, but the question is what you would gain - for the real-time sound synthesis, the SuperCollider server is used (a native C program), and I don't see how it would be possible to run that for client-side JS scenarios; for server-side, there is probably no incentive to run on JS rather than JVM - but maybe I'm missing something, as there is a [JS client for SuperCollider](https://github.com/crucialfelix/supercolliderjs), although it seems audio streaming via web-sockets is not yet implemented. Do you have a particular application in mind?
I'm working on a web based sequencer, and I haven't decided on sound generation yet. All I know is that I want to keep it in Scala, so it remains portable. 
Then you would have to decide whether to stick to client side audio generation via WebAudioAPI - although it is cool, it's extremely limited, too - or stream sound from the server; I have no experience with this, but I think web-sockets would be the way to go here. For that you could run SuperCollider and a SuperCollider client (such as ScalaCollider on the JVM) on the web server, I think. All projects that mention SuperCollider and streaming via web sockets have _not_ implemented this, however, they only mention it as future plans. So probably the task is far from simple. __Edit:__ For WebAudioAPI, you might have a look at this project: https://github.com/Sciss/Cord - I abandoned it, however, because the client for which the project was initially planned, thought it was too ambitious and they were scared of not reading JavaScript code. I remember, though, that it was functional at this point.
RemindMe! 2 days
Scala3 aka dotty is on the way. They are removing some functionality and trying to make language stricter. 
Working in mobile game dev company and one of the game projects I'm working on (back end part of course) is written in a purely functional style with Akka as a frontend and scalaz tools as fp classes. I wanted to check whether it's possible to apply fp approach to game development domain and don't see any problems by this time. Do hope we will eventually launch in prod if all business questions will be settled 
I've found that FastParse has cases where I think something should work a certain way, but causes problems. [Example](https://github.com/lihaoyi/fastparse/issues/150) I wish I could recommend FastParse over Parboiled2, because its interface is friendlier.
Do you have a more detailed doc on this you can link? Thanks!
This link gives a good overview of the scala future http://www.cakesolutions.net/teamblogs/dotty
I got it to compile with some changes to the `def *`. Check [this PR](https://github.com/abhsrivastava/SlickTest/pull/1). 
&gt; avoid floating-point numbers Is `strictfp` 100% deterministic?
this is a known issue
Thank you so much. yes it works. 
Was there a way (some compiler option) which would have made compiler tell what is the mistake rather than hang? it was a silly mistake on my part... but the compiler shouldn't have hung.
I'd rather avoid web audio. It's very stateful, and hard to keep track of all the connections and avoid leaking audio components. If nothing else, i'm just going to generate the audio samples with Scala (i have built a synth with C on an MCU so doing it from scratch is not a problem for me), and feed it through a scrip processor node.
Instead of hlists which are slow to compile, you can just use nested tuples. 
interesting. is there an example where nested tuples have been used with Slick for more than 22 columns? 
I am learning Scala from scratch, in a couple of weeks I will start working in a project that involves Scala+Spark being used for generation of analytics in a financial firm.
How about something like: data.map((enrich1 _) andThen enrich2 andThen enrich3)
Sadly, my class is not a member of monad.
Where do you see a monadic operation?
You can add any method to a type with a so called extension method, here's how you make them in Scala: implicit class Enriched1Extensions(val data: Enriched1Type) extends AnyVal { def enriched2 = doSomethingWithEnriched1(data) }
`andThen` is available on all functions
Another way of writing this is to use `Function.chain`. It takes a sequence of functions and combines them using `andThen`. [See here](http://www.scala-lang.org/api/current/scala/Function$.html). Editing to add an example with a kind of string editing pipeline. Say I wanted to do a bunch of little fiddly things with a string. I could write something like... scala&gt; val pipeline = Function.chain[String](Seq(_.toLowerCase, _.trim, _.replace("a", "b"))) pipeline: String =&gt; String = scala.Function$$$Lambda$1051/736714033@338b49d3 scala&gt; pipeline(" ABC") res0: String = bbc 
Yes, but it will only work with functions that take and return the same type. andThen gives you flexibility to compose functions with various types.
Yes, that's true. I wasn't sure if an `andThen` pipeline had the same constraints, given that `Function.chain` more or less does the same thing.
What's the deal with optic libraries? Started reading about [Monocle](https://github.com/julien-truffaut/Monocle) and it doesn't seem to wow me with the problem its solving. Even by its own [documentation](http://julien-truffaut.github.io/Monocle/) it seems to be a function built around accessing specific fields in (most cases) case classes. Can't i just make a function called getInnerField() setInnerField() that gets/sets the field by traversing the "dot path" for me? ```scala getInnerField(c) { c.outer.inner.evenmoreinner } ```
I'm no expert and have never used any optic library, but the way I see it, it helps to greatly reduce boilerplate when manipulating immutable objects. If we just wanted to change a field of a mutable object, it wouldn't be problematic to just access and change it. But in purely functional programming, the change we want to express won't result in the original object changed, instead we'd return a new object that reflects our wanted changes. Scala's copy method leads to a lot of duplication in nested case classes.
You could codify your lessons as projects. Somewhere in between the [Ammonite](http://ammonite.io/#Ammonite-REPL) repl, which should help to provide a more flexible execution path (seriously it kicks ass) and [a project named Import](https://github.com/ThoughtWorksInc/Import.scala) you might be able to have a bit more of an interactive experience. IMO part of understanding the first few steps necessary to learn 'puters is a firm grasp on the file system. Instead of having to exit the repl to grab a snippet of code everything could be imported to the session. Of course that might be the complete opposite of what you wanted but I tried to think some something that as a student I would say, "Oh that was cool...how does that work." Cheers. Hope you find something suitable. 
That explanation along with trying to write an example copy method for the example i posted, just solidified the point of having an optics library. That copy method is great, but its very low level and if you're changing it constantly, its gonna suuuck
Oh and this just hit me as I clicked post but https://scastie.scala-lang.org/ and https://scalafiddle.io/ are both superb choices for sharing code snippets (and more). gist.github.com is always out there too.
This tech can lead humans into the next step of P2P tech evolution after Bittorent. A Boon for those fighting centralized tyrannies of Mega WebSites and the evil Cloud techs.
What do people see as the "killer app" for Scala Native? I'd love to see it gain as much traction as Go or Rust so that I could use it in lieu of those languages. But it's early days and hard to see exactly where it's road will lead. Some ideas I've had: something in the devops space for example a terraform-like DSL. Or a replacement for Puppet. Any other ideas? 
Working on improving my [sbt-hepek](https://github.com/sake92/sbt-hepek) plugin for rendering `object` to file (usually HTML). I'm sad because there's no feedback yet... :D If someone is interested, see my blog post [here](https://dev.to/sake_92/render-static-site-from-scala-code).
&gt;I'm sad [Here's a picture/gif of a cat,](http://random.cat/i/sGmv1Az.jpg) hopefully it'll cheer you up :). ___ I am a bot. use !unsubscribetosadcat for me to ignore you.
If you use Emacs there is babel: http://orgmode.org/worg/org-contrib/babel/intro.html
It's a simple syntax error, right? That should be a compiler error yes. I'd try filing it as a bug report in the compiler itself.
There are cases where the hyped thing turned out to be good too. Hype can't tell you one way or another; the only way I've found to tell is to use my own judgement.
I don't think there is a killer app; HKT is really useful but only really comes into its own in large systems, but JVM Scala is fine for those. I mean I can imagine some people getting excited about a natively-compiled version of Spark or Kafka, but I don't think there's any real reason to. I'm looking forward to being able to write the little utilities that work with my Scala system in Scala too, and be able to run them from e.g. `find -exec` without worrying about startup time and memory consumption so much. But I don't see the direct use case for Scala-native (though then again, I don't really see the case for Go/Rust/Crystal/... either)
I use Travis for several of my Scala projects; I just use Maven and follow their instructions for working with a Java project, it works the same way. (I don't use play or akka though)
Why do I often see people writing `sealed trait Foo extends Any` Doen't it always extend any? Also often ` sealed trait Foo extends Any with X1 with X2`
Look at [Value Classes and Universal Traits](https://docs.scala-lang.org/overviews/core/value-classes.html) Traits don't implicitly extend Any, but rather AnyRef, a subclass of Any. Look at the following trait and value class: trait Printable extends Any { def print(): Unit = println(this) } class Wrapper(val underlying: Int) extends AnyVal with Printable If Printable were to extend AnyRef ( which is the case if we just define a trait Printable without any explicit extensions ), we'd get the following compile error: error: illegal inheritance; superclass AnyVal is not a subclass of the superclass Object of the mixin trait Printable where Object basically equals AnyRef. That's why it can be necessary for traits to explicitly extend Any.
Thanks 
I'd probably write `data.map { x =&gt; enrich3(enrich2(enrich1(x)))}`, possibly with more spacing. I don't think any more unusual style is worthwhile.
It also helps to locate items without having to type out long chains of maps/flatMaps getMyThing(x) is better than x.nest1.flatMap(_.nest2.nest3.nest4.map(_.nest5)) Learning them can be painful. Teaching them can be painful, and other than having lawful interactions (which not every dev on your team is going to care about), I don't know if they are that advantageous over simply extracting def getMyThing(x: MyThing): Option[OtherMyThing] = x.nest1.flatMap(_.nest2.nest3.nest4(_.nest5)) def updateMyThing(x: MyThing, set: OtherMyThing): MyThing = x.copy(nest1 = x.nest1.flatMap(_.nest2.nest3.nest4.map(y =&gt; y.copy(nest5 = set)))) to a pair of methods. Of course, your methods won't compose if you have to update a few different nested fields, which is what makes optics handy, but quite often you are simply updating one part of a nested structure repeatedly in a program. 
I haven't met Catch objects before. I guess they might be useful sometimes. Most of the time I would prefer not to have exception-driven error handling, but since some libraries use them (especially Java ones), sometimes catching is inevitable. However, on async computations, that I usually work with, Future, scalaz Task or monix Task handle them the same way Try works - if I need to recover, I recover instantly, otherwise I just let it propagate. Usually I don't need to distunguish exception types, so it covers most of my use cases. edit: typos and grammar
Very interesting, and an entertaining presentation as well! This is just the thing I've been looking for! It's a good thing I hadn't started learning Rest or something yet! This seems perfect for native desktop apps to be super responsive as well. What a godsent! :D
Check out https://github.com/zmanio/atmos for retries, it seems to be abandonware now but has a decent feature set, should be good for inspiration at least.
Yeah this is my technique as well. If I'm going to be doing something asynchronous, I usually will wrap my first function in a Future as a means to handle exceptions (if it doesn't already return a Future). Then I can map/flatMap through my process, and stick a recover at the end with any error handling needed. Agreed that Catch is sometimes inevitable, especially in a synchronous system that uses java libraries. Hell, sometimes even throw is inevitable.
Is there a better way to stop a Play app from spinning up due to improper config reads other than just throwing an exception? E.g. if I introduce some mandatory config and want to stop the spin up if it isn't configured.
Yup. I think Play used some exceptions to indicate different error status codes. They could be passed in other ways, but company I worked for had C# origins and they went with throwing. I saw things like case Left =&gt; throw. But it would be difficult to change the whole codebase to something I'd rather see. Well it kind of worked for them, so I wasn't bitching about it.
If you compile with `-Ywarn-value-discard` the compiler can usually find places where you're smashing a value into `Unit` and will issue a warning. scala-2.12.2$ bin/scala -Ywarn-value-discard Welcome to Scala 2.12.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144). Type in expressions for evaluation. Or try :help. scala&gt; import concurrent._, ExecutionContext.Implicits.global import concurrent._ import ExecutionContext.Implicits.global scala&gt; Future(Future(1)) : Future[Unit] &lt;console&gt;:16: warning: discarded non-Unit value Future(Future(1)) : Future[Unit] ^ res0: scala.concurrent.Future[Unit] = Future(Success(())) scala&gt; If you combine this with `-Xfatal-warnings` it will become a compile error. I [recommend](http://tpolecat.github.io/2017/04/25/scalac-flags.html) both of these flags. 
Use the App trait, sbt-assembly, sbt new, fs2 and fs2-io, and scopt. The readme for fs2 and scopt have everything you need to accomplish this with very few vals. sbt-assembly will let you build a fat jar to call with java -jar myfatjar.jar --add="buy milk".
Don't use inner futures. Flatmap it -- a future of a future is a future.
What's the benefit in comparison to `scalaz.Memo`?
i think using pre-packaged solutions is a bad idea for a person trying to learn the language...
1. You don't need to build in a repl. You're on the right track. 2. Console application is fine if your just learning. A useful app with HTML can come later. You can pass flags but if the list is always empty, then it makes no sense to add something to an empty list and exit. Then start over again. You want to be learn how to operate on lists with this exercise. Which is a powerful lesson you'll use and improve upon in the future. 3. Yes it's sensible. 4. Nope not to restrictive 5. Oh no. Never use a framework when you're trying to learn. 6. It's called a to-do list exercise. Google will give you the solution. But you don't want that. Solve it, then look at the solution. 7. Hello world is a good first app. From there try to query standard input like. What's your name? "Hello, $insertedName" Progress from there to a list if inputs. 8. Sqlite is perfectly fine for something like this. You could also use a plain old text file. Maybe the file name could be passed in as an argument named after the Todo list. Then your app can manage multiple lists. One list per file. 9. Standard way of learning to write your first app hasn't and won't change for awhile. Hope that helps. I can whip up a solution tomorrow. (UTC-7)
For the most basic setup you need: - You need either a val with mutable list or a var with an immutable list. - A variable to hold the last input. - A while loop that breaks when the variable is "quit". - Some processing logic that adds to list when the input is right. I guess that is what you would have done in Java. Are you trying to be especially Scalarific? If so, how? Scala does have some nice features, but I am a strong contender for not making any solution more complicated than merited by the problem. And your problem here is a very simple one. The variations you proposed with making it command line, or making it web app, or adding a database make it more complicated.
nice detail. never saw any code using that feature, probably because it's unusual to see the try/catch on scala codebases
This is a tutorial of scalameta, so I dont intend to claim rolling custom cache decorator is better than any existing solution. I think there is not much difference, does `scalaz.Memo` handle multiple argument automatically?
&gt; The would idea was to learn in baby steps and in the same way i learned java, In that case I'd start with writing the same thing you would in Java, just get comfortable with the Scala syntax. Don't worry about making everything immutable etc. yet. I feel like a lot of your questions are more "what should I be writing?" than "how do I do this in Scala?" - more design than programming really. I think it's important to write something that you, personally, want to use. So how would you like your list to work? Do you think it needs a web frontend or would you rather call it from the command line? 
You've got a good point there.
Cool, but why would you use private var map = mutable.Map[K, V] When you only need private val map = mutable.Map[K, V] OR private var map = immutable.Map[K, V] ? Using a mutable `Map` as if it was immutable is a big waste (will require a lot of needless copying).
Unrelated to your question OP, but you might also want to check out [Wart Remover](http://www.wartremover.org/doc/warts.html) along with using these compiler flags to avoid issues like this.
thanks, that's a silly mistake from me
He's trying to learn how to put together an application. He needs to see how it should look and work first, before he attempts to write an iteratee library or a parser for his arguments, or a Todo List dsl and interpreter. If his goal was to learn how to parse argument's statelessly, then I'd still tell him to use a library first before telling him to go write his own parser. By using the pre-packaged libraries, he can learn how to compose his business logic, learn about IO, learn about argument parsing, learn about stream processing monads, all kinds of useful things that will make him productive and get him progress, subs show him what it is like too actually use the language to get work done. If his goal changes from putting an application together to something more modest, like managing purely functional state (a stated problem above) or recursion, we can start teaching there. But he wants a cli todo app at the moment, and the above is the compromise between fast and from scratch. 
That's what I eventually did to fix it, but it was a mistake that didn't get caught by the compiler.
As a good practice, I always specify the return type of my methods. So in your case, it would have been caught by compilation. Also @tpolecat mentioned that the compiler flag is very helpful
I think it's a good idea for a project. Some of the code in this project I wrote may help as it's a very simple command line controlled app. Just strip out the rest of the app. As for storing your list of todo items the simplest way would be to have "val todos = mutable.List[Todo]" . Remember a val does not change, it will be the same list at any time, but the list itself can change. If you want to use the immutable list instead you would use a var which can change. Then when you add or remove items to the list you're actually creating a new list and assigning that to the var. Hope this helps and good luck 
Also a mutable map should have .getOrElseUpdate, which basically does all the putting and getting you need for the caching.
Thanks for replying :) 1 &amp; 2 what i meant for the application is that you would start it, you would be prompted for an input, you can add elements to the list as you please while app is running e.g. scala todolist Print : Current List: - Input : add "test" Print : Current List: Print : test - Input : add "test2" Print : Current List: Print : test Print : test2 - Input : quit //exit to command line 6. I have for the life of me been Googling for a solution in the nature i have discribed but have drawn a blank (a simple solution you suggest that only lives in the command line and writes to files) 7. I think ill do that this evening, go back to basics again, may have been trying too much to early 8. good idea! A solution would be fantastic :) Thanks again _CR
Thanks for replying :) That sounds like a good solution, I'm embarrassed to admit i didnt realise you could have a mutable list in a val, That is preitty much what i would have done in java, but i would have done it with alot of state changes and variables and i think i cant see how you would perform the actions without setting a local variable. What im eventually aiming for is to have a problem thats solution uses alot of Scalas features, I hope i have made some sense! Thanks again _CR 
**Martin**: If we had Prolog in our type checker then, we could do it, we would be fine. And in fact it turns out we *have* Prolog in our type checker! **Audience**: Intentionally, or...? **Martin**: Well, it turns out it was a fortuitous coincidence. 
Thanks for replying :) Seeing your project sounds like it would be a great help :) Currently investigating the Mutable list solution Thanks again :) _CR
Thanks for replying :) I learned Java in uni which was 3 years and i have been working with it daily for another 2 so its hard to think back to how i started, I feel i am focusing on design as that is where i have been focusing on at work, I am fairly comfortable with the scala syntax and i think i will become more so once doing a few small projects. I aggree with you about writing something i would personally use, but for this first project its only ment to be something simple to introduce me to the general application writing structure in Scala Personally i dont think what i am doing right now needs a web frontend as its more for learning, it could then be build on and added later as i improve, I hope that make some sence, Thanks again. _CR
Thanks for replying :) I would like to do something more modest, i had thought that my project idea would cover that, you have made it sound like i have bitten off more than i can chew, is there ar more simple excersize you would reccomend? Thaks again :) _CR
Wrote my first macros to scrap some boilerplate from a pattern we apply in our major Scala project at work. Check it out, let me know what you think! I spent more time in SBT getting cross-building going and this thing to publish to Sonatype than I did figuring out Scala Meta. Just sayin'.
You haven't bitten off more than you can chew! Your goal is to build a functional todo app without scattering vals or vars in your code. The way I suggested lets you do that in about 50 - 200 lines of code, in a way that you would write an industrial-strength application that could handle lists of pretty much arbitrary (disk space limited) size. It lets you focus on the business logic of your application rather than on making functional building blocks and also a business application. If you don't mind limiting the size of your lists, and want to focus on doing the functional building blocks part, a less ambitious project is to make the same todo application using the standard library and not persisting any of the state to files. It takes an initial list of strings, an action to perform (--done="Buy milk"), modifies the initial list by removing done items or adding add items, and printing the modified list back to stdout and exiting with code 0. Error states should be indicated by printing the error to stderr, and exiting with code 1. You will still want to do sbt new, the App trait, and use sbt-assembly to produce an executable jar. The limitation is that you will use List or Stream like you have in your project Euler code, so the lists have to fit in memory, and you won't be persisting in the program, so you won't have to manage the side effects inherent there. All your io actions will happen at the edges of your program, so we can hand-wave the IO effect a bit. You will want to encapsulate error handling with Either. Every map/flatMap step will basically be wrapped in try/catch, returning an Either [List [String], List [String]], and you will fold to print to stderr/stout as the final output. This will teach you functional error handling, List processing, and some minimal argument processing, along with building and assembling an app from scratch. I encourage you to break the App into four parts: 1. Argument parsing. Separate the list, and the actions into two separate lists. 2. Action validation. Mark only things that are in the list done. Anything else is an error. Report all the errors and put it into a left (not just the first one). 3. Running the validated actions. Modify the list. 4. Fold over your either, and print the result to the proper stream. This app will let you persist and read from files using std Unix pipes. Have fun!
Happened to rust too https://github.com/nikomatsakis/chalk
This is not entirely unheard of. As a sibling poster points out, Rust has it, and Haskell's type-level language is also TC (I think)[1]. As are C++ templates. ("Prolog" just means Turing Complete, essentially.) What's interesting about it is just how easy it is to accidentally introduce TC into a type system. [1] Not sure if it was accidental or intentional. I'm leaning towards "happy accident".
It's not just about Turing completeness though. Generics with constraints (read type classes) force the compiler to do searches through a constraint space: exactly what a prolog engine does. C++ templates aren't quiet like this because they aren't really types, only their specializations are. 
Is there a technical difference or is it just shorter than writing AnyVal classes?
Doesn't scalaz have tagged types already? "@@"
Interesting topic, I read something similar here http://rnduja.github.io/2016/01/19/a_shapeless_primer/
see also: - my original inspiration for my talk https://speakerdeck.com/folone/theres-a-prolog-in-your-scala - and my own: - video: https://vimeo.com/171939792 - slides: https://speakerdeck.com/evacchi/be-like-water-scala-italy-2016 - the original blog post: http://rnduja.github.io/2016/01/19/a_shapeless_primer/ - and also, this fun project https://github.com/densh/typelog/blob/master/src/test/scala/RecSuite.scala#L5
See the rest of the thread. tl;dr speed isn't measured in first-pageloads-since-startup.
Nice work Alan! Have you compared it with https://github.com/alexknvl/newtypes?
Yeah cross building combined with sonatype publish is a pain. Sbt-catalysts helps. See github.com/kailuowang/mainecoon is setup that way. By the way, if you haven't , checkout final tagless, its so powerful and yet so straightforward! 
Maintainer here. If you are using or plan to use kittens, I'd love to hear your feedbacks. And AMA.
There are hundreds of such useless videos on Scala web development out there. None of them shows how to connect WebPage with akka http and the client side code of it, not even one.
The most useful addition in this release is probably the 'Show' derivation. I.e. a better "toString" for case classes and ADTs - they have field names! Checkout the readme for example.
Both scalaz and shapeless have `@@` type. In shapeless, `A @@ Tag` is a subclass of `A` (so e.g. you have all the string methods, returning untagged strings however), while scalaz version requires you to explicitly unwrap the value. OP's macro uses `shapeless.tag.@@`
AnyVal can box, can have members, can have hashCode and toString. This looks like newtype in Haskell, it's an opaque type alias.
Thanks Kai! Nope, I hadn't seen that before. I probably would have just used that had I known :D Looking at it: - I bet their syntax plays better with IntelliJ at the declaration site. I should have thought of that :). - Another good idea in newtypes is that they rolled their own tagged type solutions inline, rather than relying on Shapeless's. This seems much better than pulling in the whole library for one tiny piece of code, which seems unlike to change. - Their `opaque` and `translucent` implementations are slightly different than Shapeless's `tag`. `translucent` is closest, but doesn't explictly use `with` to make the tagged type a subtype of the original type. I'm not sure what the implications of this are. - They did a cool job cataloging the nuances of their approaches. All these things considered, an ideal solution might be for me to contribute to `newtypes` code that offers the `with`-based approach, without the Shapeless dependency. I also think it would be cool for either project to offer a helper to synthesize the `AnyVal`-based approach too. That said, I've probably already devoted more time to this than it really merits, so I'll probably leave it alone for now, haha.
There is -- see https://failex.blogspot.com/2017/04/the-high-cost-of-anyval-subclasses.html
I only read the first half or so, but he didn't seem super irrational. Just ticked off at Scala fanatics criticizing his lang. I don't think that's totally unfair, lol. And of course, I know it's pretty risky to say something like that in this sub
That article is quite good. Up to point where it goes: "as a matter of principle". There it fell into: OK, I will gladly diregard anything you just said, have a nice day.
why is it built against 2.11? does 2.12 have partial unification of https://github.com/scala/bug/issues/2712
LOL, yeah, I try not to throw the baby out with the bathwater.
Thanks, this works nicely! It also found a bunch of other similar mistakes!
This looks seriously awesome! Can't wait to see how it evolves. Curious to know how much you still need to know the intricacies of front end to use it effectively though
It is cross built to 2.11, 2.12 and scalajs. Both 2.11 and 2.12 has partial unification.
Any intensional set with an `Eq` instance for your type will work, right? It will probably always fit in memory as you are essentially storing predicates instead of data elements with infinitely many members, meaning you don't have to have a queue size limit or continuously growing hashset. When you add an item to the set you create a new Iset from your last one using `union`: val newIset = lastISet.union( Iset( _ eq valueToRemember ) ) You check for membership via apply newISet(myDeltaObj) The downside is that this type of set cannot be serialized or enumerated internally - you have a bunch of Boolean ands, essentially. [Dogs](https://github.com/stew/dogs/blob/master/core/src/main/scala/dogs/ISet.scala) has one. You can probably roll your own, too. Anyway, if your item is in the set, it means you have already delivered it, so don't send it again.
[Also see](http://json2caseclass.cleverapps.io/).
https://github.com/suzaku-io/suzaku-starter
why do the docs talk about using an sbt plugin to get partial unification then? or is that just old info?
Did you have a look at [PureConfig](https://github.com/pureconfig/pureconfig)? I'm not a pro at it, but it seems to me that will help you with your problem. The goal of PureConfig is to fail at compile time if a configuration is wrong.
Hey, are there some libraries out there, that provide custom `Gen` generators (for scalacheck) for common problems like generating emails, urls, texts in a language, or something in that direction?
It was the creator of that website who suggested me [on twitter](https://twitter.com/waxzce/status/898472603270172673) to add scala transformation on the website . I created a separate module https://github.com/transform-it/transform-json-types also.
My most immediate want would be fast-booting command-line tools without needing heavy installation dependencies. Scala programs, even not very big ones, end up taking 0.5-1s just to do classloading *even before much code is even run*. Pre-compiled Ammonite scripts, for example, take 0.7s in 2.12 (0.5s in 2.11) to boot. That's only running a pretty thin-shell of Scala code around each script; for pre-compiled scripts, we don't even classload the scalac compiler at all, so I can't blame "slow-compiler" for the sluggishness. And I know it's all classloading, because when I profiled it, most than half the time was spent in `java.lang.Class#loadClass`/`java.lang.invoke.MethodHandleNatives#linkCallSite`. With Scala-Native, non-trivial programs should *start* within single-digit milliseconds, and run on any machine without needing a JVM installed. Even if steady-state performance isn't much better than Scala-JVM, this should make Scala a feasible language to write all sorts of command-line tools and utilities, a space in which it currently doesn't really fit at all.
This is actually really nifty. The only problem is that I actually need to serialize the data. The idea is to keep track of the last X transactions that an event sourced actor has processed.
The plugin is just to add some scala option based on your scala version or use another plugin if you are on older scala.. You can certainly do without that plugin
Not going to lie, that looks exactly like the r/scala front page right now...
I have not! Thanks for sharing.
&gt; an ideal solution might be for me to contribute to newtypes code that offers the with-based approach What makes you think that would offer any advantages? AFAIK they are pretty much equivalent, except that the `&lt;:`-bound version is cleaner: doesn't pollute the type with a de-aliased form that contains `with`, and doesn't introduce potential cast errors by exposing a spurious intersection type: scala&gt; tag[ProfileIdTag][String]("profileId") res1: String @@ ProfileIdTag = profileId scala&gt; res1: tag.Tagged[_] java.lang.ClassCastException: java.lang.String cannot be cast to shapeless.tag$Tagged ... 39 elided 
I haven't analyzed it enough detail to know whether there are any important differences. Perhaps one would be that you can [abstract over tagged types](https://github.com/acjay/taggy#additional-notes). Not sure if you can do this with the Newtypes approaches.
I imagine this can be done if you provide a tag function that does the tagging similarly to shapeless (basically a glorified unsafe cast: `def tag[A,Tag&lt;:A](a: A): Tag = a.asInstanceOf[Tag]`; could be made safer using a type class, which would also remove the need for two type parameters). BTW, why don't you syntax-color your code fragments? Use: ```scala blah ```
Done!
I love PureConfig. I just discovered it a few months ago and realized I've been doing config wrong all this time. I threw away so much boiler plate, it's truly useful.
Out of curiosity, what's holding back batteries-included multithreading? How close is Scala Native to being a competitor to something like Go for utility development?
I will remember this, whenever someone asserts that you should or should not make your fields optional, that you should or should not intern etc. Thank you.
I would actually love for us to move the portion of the JDK we plan to port to other platforms into a scala. namespace. Like, considering the classes scalajs and scala native have already ported (scalajs is ahead I think) would be a good start. Then you could know if you only include scala.* you know things can be cross built.
*"Beware of the leopard"*
Omg the quotes + links are hilarious: * "Functional Programming is Terrible." — [Rúnar Bjarnason](https://www.youtube.com/watch?v=hzf3hTUKk8U) * "Very elegant, very FP, very monadic, very slow." — [Li Haoyi](https://vimeo.com/142341803#t=1026s) * "Please. Stop. Saying. This. Please." — [Kelley Robinson](https://www.youtube.com/watch?v=Yc6nJZK39mU&amp;t=2230) * "Please, don't do this in Scala!" — [Martin Odersky](https://twitter.com/odersky/status/387323191997116416) * "The language seems too complex to make tooling for!" — [Mariano Gappa](https://movio.co/en/blog/migrate-Scala-to-Go/) (not a [sponsor of ENSIME](https://salt.bountysource.com/teams/ensime))
&gt; Can I write Scala code and export it to all three platforms? It depends. &gt; What about Java code? You have to rewrite it in Scala or use what the target has to offer. For example, the javascript VM has a JSON parser. The js ecosystem has libraries, etc &gt; Is it the end goal of these projects to be fully compatible or are they just aiming to provide a scale-like languages/libraries for their respective platforms without keeping compatibility between the three? Scala-Native and Scala.js are 100% faithful to the Scala language semantics. A library needs to be cross-compiled. You can use [sbt-crossproject](https://github.com/scala-native/sbt-crossproject) to do this. You can use maven-central to publish both the jvm, native and js artifact. For example, [scallop](https://index.scala-lang.org/artifacts/scallop/scallop) a CLI parsing library has artifacts for all targets (http://repo1.maven.org/maven2/org/rogach/). Scala-Native and Scala.js re-implements part of the JDK because this is what developers expect. They do this because they cannot re-target the java bytecode to their target (llvm or js). It's the greatest limitation for those projects. It turned out, that compiling Scala code directly lead to better results.
&gt; Scala-Native and Scala.js are 100% faithful to the Scala language semantics. That's not really true. I would say Scala.js is &gt;99.9% faithful to the Scala language semantics but not 100% (i.e., less than 1 line in a thousand should be incompatible). Numeric types don't behave the same wrt to dynamic `isInstanceOf` or pattern matching. And that's assuming your program does not have any Undefined Behavior as defined by Scala.js. However, it is true that for many practical purposes, complete compatibility is what one observes. It is not uncommon to just take an existing Scala library, change its build and have the tests pass in Scala.js out of the box. And citing the OP: &gt; Is it the end goal of these projects to be fully compatible For Scala.js that's not an end goal anymore. It's a living fact ;) It's been like that for 2.5 years.
Using Martin's word, looks like this book is more about categorical programming not functional programming. Good luck.
Can't understand what's the point he put those in.
What is categorical programming?
That's a pretty broad question, what info exactly do you wish to collect?
My guess is the above quotes make FP sound bad, hard, or non-performant. Here's a book full of performant, easy, and well-specified FP examples in scala. Other books teach FP by teaching you how to write map. This one will just teach you what it is and how to use it effectively, from what I've gathered. I'm very much looking forward to it being completed, as an alternative to the approach in FP in Scala. I love FPIS. But there needs to be a Little MLer scala FP book. 
Regarding the author, what is a "chartered mathematician"? 
It's the first time I've come across this term too but it's not hard to guess. It's an application of category theory to programming. The concept of a monad came out of category theory and heavily influenced Haskell.
How many of you are working in Scala now. Could you please let us know real time scenarios?
As far as i see the only purpose SBT serves is to screw Eclipse in favor of intelliJ and make sure JetBrains make money, there is no other need of SBT. And no one who needs a Open Source stack will ever support intelliJ.
Postgis, uuid, inet types, I think some types of indexes as well
We use it to interact with Amazon SWF. Sadly, we use it as a better Java instead of a functional language. 
http://www.scala-lang.org/old/node/1658 That article has some big names and how they use it.
6 months of deeply immersive use. Or about a year. Depends on how you are learning. 
I am planning to learn Scala. I have total of 10 years experience. I have excellent knowledge on C++, Java and COBOL. Could you please let me know how those knowledge will be useful for me to learn this technology. I have never learn functional programming knowledge. Do I need to learn some prerequisite about any concepts?
Oh man, SWF. Hope you're not using the flow framework... 
https://47deg.github.io/scalacheck-toolbox/
We are. :-/
Having your background and learning FP might feel quite challenging. It will probably feel like having to unlearn things first. I recommend the red book: https://www.manning.com/books/functional-programming-in-scala You don't have to work the book all through, but doing the first chapters (especially the exercises!) will give you a good and well founded start to functional programming. After that, just start writing some programs like you would with Java and you will see that it will be quite easy. :)
Oh man, I am so sorry. :'( 
What is the stand alone go to library for oauth2 operations in scala? I'm learning scala and I would like to implement a reddit bot. Thank you.
I know about the compositional 'issue' and I was the one who mentioned it to @m50d. However, composition is rarely useful and we can easily avoid it and provide better solutions. &gt; which, if I'm understanding correctly, breaks the functor laws like java.util.Optional Why would we care about the functor laws and how did I break them? Also, this is wrong from the beginning: userIds // Seq[UserId] .map(getUserById) // Seq[Option[User]] You'd want something like: userIds.filter(getUserById) or userIds.find(_.id == userId) And: .map(_.map(getCarByUser)) // Seq[Option[Option[Car]] only one car for the user? this also looks nasty: .map(_.fold("User ID not found".left)(_.fold("No car for user".left)(_.licensePlate.right))) // Seq[Either[String, LicensePlateNumber]] Why return with a Seq and call it "plateNumbers" when you only expect one car? Why use .left on the string(scalaz?)? &gt; With your proposal, you either lose the compositional style or need a workaround for the nesting logic, right? Not really, because if the userId is null then you know it's null just like when the user has no car it'll be an empty sequence. When you ask for his/her cars you'll get a Seq[Car] what you can transform to a Seq[LicensePlateNumber]. You're only using nesting here to use .map and .fold for error handling - and because you think a user can only has one car. A more realistic approach: users.find(_.userID == userID) match { case null =&gt; Left("User not found!") case user =&gt; { val cars = cars.filter(_.ownerID == user.userID) if(cars.isEmpty) Left("The user doesn't have any car!") else Right(cars.map(_.plateNumbers)) } (I only use filter+map together because I think this problem requires a database and we'd use a functional API which composes those functions) or users.find(_.userID == userID).toLeft("User not found!") .left.map{user =&gt; val cars = cars.filter(_.ownerID == user.userID) if(cars.isEmpty) Left("The user doesn't have any car!") else Right(cars.map(_.licensePlate)) } or if the users can only have one car: users.find(_.userID == userID).toLeft("User not found!") .left.map(user =&gt; cars.find(_.ownerID == user.userID).toLeft("The user doesn't have any car!") .left.map(_.licensePlate)) &gt; In my experience, it's a pretty common case (if "nullable" means "potentially absent"). Even if it weren't, we have an alternative that permits users to have this functionality (instead of being "almost correct"). The only useful case when you've an associative collection X[A, B] and you want it to have methods returning ?[B] and the problem arises when B is ?[B]. But your methods can easily avoid it by unions for example you want a method `get(key: A): ?[B]` but your collection contains `?[B]` values then it'd be nice to change `get`'s signature to `get(key: A): B | KeyNotFound`.
&gt; With generic functions in a well-behaved language you get a whole bunch of "theorems for free" via parametricity, without having to test them. More generally, you can be confident that a generic function will behave the same way for any type, rather than having to test each type separately. We don't need those "bunch of theorems" and we can still have a "well-behaved" language. &gt; At that point you're using User (which might be costly to load) to emulate Option so that you can stick it inside ?. What does this mean? &gt; In any specific data model there will be a better replacement for Option or ?, the point is to be able to reuse all the generic functionality that Option or ? already has. If the "better replacement" doesn't have the functionality I need then it's not a "better" replacement. &gt; Take any two of your favourite use cases for ? and put them together, and that's your use case. You're still not showing any use case I need to be afraid of. &gt; There's a huge difference between something that works every time and something that works most of the time. ?[T] works every time. ?[T] works as ?[T] and Option[T] works as Option[T] while ?[T] != Option[T]. &gt; If some types of composition break, that massively slows down working with composition, because you have to check you're not hitting those cases every time. Show me those compositions. Most of the time they can be thrown out. &gt; To be a sum type it has to follow the rules of sums, and I'm pretty sure your ? doesn't. Then show me those rules if you're pretty sure. &gt; It's just terminology, but figured it was important to be precise since we're getting into the details. This terminology is absolutely useless in our argument. Be it a sum type or not we know what it is. &gt; So you admit null is a magical non-value? No, it's a natural value in programming. But `None` and `Nothing` are magical. &gt; Which isn't something that actually happens in day-to-day Scala, Spare me from optimism in programming, we're scala(or whatever) users after all... &gt; and introducing ? wouldn't help fix it. I said "You know what's bizarre? Expecting an Option[Int] and receiving null..." and ?[Int] would fix it since we'd know if it's null. &gt; I mean if you try to write a plain-old-Scala datatype (i.e. not using the language builtin null, but with your own special value) that behaves like ? you'd have to do some nasty reflection or similar to get it to have the same nesting behaviour as ?. Whereas it's really straightforward to write your own Option (again, using your own special value rather than the built-in None) in plain old code. Then our communication is weak because `type ?[T] = T | null` supposed to use a valid language feature. &gt; There's a value called null but it's restricted to an interop type (Ptr), it doesn't infect the whole language. But if it can "infect" the interop pointer then you can bring it to the "real" pointers, right? &gt; If you really can't stop yourself writing null, use Wartremover. Using linters for a language is already a bad sign and I was talking about better approaches. &gt; ...avoid Java interop, and you'll never see null. Well, if you want to avoid java interop and want scalaz and monads so badly then why use scala? &gt; Yes it's a flaw in the language, but it's not actually a problem in practice ...if your team won't make it a problem... &gt; How? What examples that cause actual errors today would it actually help with? NPE for example. Plus decrease memory consumption thus putting less work on the GC and making the app "fitter" for real-time computing. &gt; Any code that uses Option might break if it were replaced with ?. E.g. if List#find or Map#get was changed to return ?, then generic code that uses those with collections of Option (that also got changed to use ?) would behave differently in the case of None/null. First, those wouldn't compile because you're using null instead of None - you'd need to refactor it obviously. Second, we can forbid ?[?[T]] and evade all your problems. Third, the only use case when ?[T] would be problematic is Map.get and the Map having nullable values - but that case can be easily avoided or refactored.
We're experimenting with multithreading, but due to the fact that it's such a cross-cutting concern (i.e. it affects most of the code we have one way or the other) it might take quite some time before we can merge it to master. It's definitely on our roadmap, you can follow `topic/multithreading` branch for progress on this (Note: here be dragons.)
Don't recommend going straight to the Red Book. Try the Scala for the impatient first. And then go look at cats or scalaz. There is a more modern fp scala book in the pipeline ( https://leanpub.com/fpmortals). Also there's a free cats book available. 
I have been following some of their newsletters and they just take everything out of reddit scala front page.
I am putting together the library [sane-gen](https://github.com/yannick-cw/sane-gen). My goal is to provide some `scalacheck.Gen` Generators for problems that I run into a lot of times. Like generating urls, email or names. I am working with search so I also need random texts that make sense a lot, so this will go into there as well. If anyone has ideas or want to help out, I'd be happy :)
I've been working on a library for tracing the request flow easily (for Play), trying to get it working on filters to consider it as complete: https://github.com/AlexITC/play-request-tracer
&gt; We don't need those "bunch of theorems" Very little in programming is strictly necessary, but they're convenient. They let you refactor code in a guaranteed-safe way even when you don't know about the specifics of the functions involved. &gt; What does this mean? `User` might be costly to load because of LOB columns or requiring other joins. You're using `User` containing a `?[Car]` field so that you can have a `?[User]` value that will behave like a `?[?[Car]]` (the thing you actually want). &gt; You're still not showing any use case I need to be afraid of. Well if there are no use cases for `?` in the first place then why do you want it? &gt; ?[T] works every time. Except when it doesn't, when `T` is itself a `?` type. &gt; Then show me those rules if you're pretty sure. E.g. addition distributes over multiplication, `A * (B + C) = A * B + A * C`. Associativity, `(A + B) + C = A + (B + C)`. I mean fundamentally you can't even form the type that `?` is supposed to be a sum of, `String?` is `String` + a pseudo-type that doesn't properly exist in the type system. &gt; No, it's a natural value in programming. But None and Nothing are magical There's nothing magical about `None`, it's a plain old Scala class. You can define your own: `case class MyNone()` (indeed [scalaz does so](https://github.com/scalaz/scalaz/blob/5bf262da8f8d859b24a38d6da34cb5678f157871/core/src/main/scala/scalaz/Maybe.scala#L154)), and it'll behave the same way as the "official" one. `Nothing` is not a value, and I'm not advocating using it at all (if anything you're advocating it more than I am, since it's the closest thing to a reasonable type for `null`). &gt; I said "You know what's bizarre? Expecting an Option[Int] and receiving null..." and ?[Int] would fix it since we'd know if it's null. How? What's the scenario in which you currently expect an `Option[Int]` and receive `null`, where the introduction of `?[Int]` would fix it? &gt; But if it can "infect" the interop pointer then you can bring it to the "real" pointers, right? No, I don't think so (the language doesn't even have "real" pointers). &gt; Well, if you want to avoid java interop and want scalaz and monads so badly then why use scala? I want a language with ML-style types, HKT and Shapeless-style data structure traversal; nothing else has Scala's level of maturity and general tool support (Haskell maybe comes close, but I don't want lazy evaluation everywhere either). I'll probably move to Idris sooner or later, but it isn't really production-ready yet. &gt; NPE for example. The only scenario where NPEs currently happen is where a Java interop call wasn't expected to return `null` and did. `?` doesn't help with that, because it's still possible to do the wrong thing at the Java interop boundary. &gt; Plus decrease memory consumption thus putting less work on the GC and making the app "fitter" for real-time computing. That's a performance optimisation only; that's better done at the implementation level, it's not worth compromising the language design for. &gt; Second, we can forbid ?[?[T]] and evade all your problems. How? Forbid ever using`?[T]` as a type parameter? Have some kind of whole-program analyser that runs every time a class gets loaded and checks which generic parameters get put in `?`s? &gt; Third, the only use case when ?[T] would be problematic is Map.get and the Map having nullable values - but that case can be easily avoided or refactored. In concrete code yes, but not when you want to be able to reuse generic code.
Also it sounds like what you're doing is OS dependent. What OS are you using?
Pc name and CPU brand memory, temperature, etc
Are there any examples of hosting my Play Framework project's API documentation as a part of the app? I'd love to be able to hit localhost:9000/documentation or something like that to see the generated ScalaDoc
Thanks, quite make sense. But I would like to see the examples in the book are cats instead of scalaz.
2. You can do nanos / 1000000 on your dashboard side to display milliseconds(we use datadog that supports modifying data to display).
I only have the JMX console (jmc, jconsole).
Using swagger you could see the API docs instead of the ScalaDoc.
This issue has nothing to do with akka, so you may ask in kamon mailing list how to customize the view in JMX console. https://groups.google.com/forum/#!forum/kamon-user
this CAN work but is platform specific: println(System.getenv("PROCESSOR_IDENTIFIER")); println(System.getenv("PROCESSOR_ARCHITECTURE")); println(System.getenv("PROCESSOR_ARCHITEW6432")); println(System.getenv("NUMBER_OF_PROCESSORS")); 
Take a look at java bindings for https://github.com/hyperic/sigar
Scala is a scam for selling InteliJ, since even simple ScalaJS program is not working with eclipse plug-in.
I do use Swagger, but that's only for the REST API. I'm looking to show the ScalaDoc API separately.
Why is that? Typelevel says that Cats does include a good and "friendly" documentation, that makes me feel that if one of the libraries need more documentation and introduction material it is definitely Scalaz (that being said, I did not review Cats documentation to explicitly compare). Plus according to the author interest for the ecosystem, I wouldn't be surprised if it get updated for Scalaz 8 at some point... that would be great. In any case, most examples should be easily adapted from one to the other.
The language is definitely not for dumb people.
Most of the issues you have raised can probably be solved with an appropriate dash-boarding tech stack, e.g Datadog, Graphite/Grafana, Promethues, Cloudwatch etc. Kamon just pulls and sends the metrics, something else can aggregate and display them appropriately.
You can use the Ruby [Faker](https://github.com/stympy/faker) for ideas.
I added firewall and tag support to my DigitalOcean API client. https://github.com/shawjef3/digitalocean
ah nice thanks!
Is there a particular reason you don't want to make `log` in MyClass implicit? class MyClass(implicit log: Logger) extends BaseClass { callFunctionThatUsesImplicitLogging() } should work just fine.
Is there a combinator/way to go from `A =&gt; Future[B]` to `Future[A =&gt; B]` ? I have a feeling not, but I'm open to be surprsied.
An implicit protected field: class BaseClass(implicit protected val log: Logger)
OP says: &gt; Assuming that MyClass is a class that doesn't want to use an implicit parameter for some reason ;-)
I'm the author of Scala.js and I exclusively use Scala IDE (the Eclipse plug-in), so I'm pretty confident when I say that it works. Perhaps you tried to *run* your Scala.js from the IDE? That is not supposed to work (and I don't think it works in IntelliJ either, although I could be wrong). Running and unit testing must be done in the sbt console (via Node.js, PhantomJS or Selenium) or in a browser.
Does some reason conform to a particular reason? Or is there some implicit conversion I don't know about?
I've been writing in Scala full-time since about 2009. I'm doing big-data job-wrangling in Scala now, before that it was distributed systems and web UIs.
consider to use ammonite as your console :). It is simple &amp; powerfull tool (works on mac &amp; linux). There are some additional things to do on windows right now. http://ammonite.io/
"but it Works on my machine" is a common problem software clients and customers face from their developers, so my experienced advise to you is to find out why things are not working; best solution will be to create a wizard in eclipse for bare bone ScalaJS project. But beware, you will be illadvised to do so by those affiliated to JetBrains, but doing so will propel ScalaJS like no other type safe Framework ever rose against JS. Such a wizard can set a trend for the rest scala ecosystem to follow in its success path. Typesafe was supposed to do this 10 years ago but they are all academics who never worked on real world software.
I'm a big fan of [*Functional Programming in Sala*](https://www.manning.com/books/functional-programming-in-scala) but I think this book will be an excellent read too. One of the more challenging aspects of the aformentioned book is its inductive approach: It has you build small tools so you can later use them in a larger context. I absolutely believe the exercises presented are valuable, but it is not a 'jump into FP' type book. I'm very much looking forwards to having something that takes a more deductive approach to an overall application as well as something that *justifies* some of the "bizarre at first look" patterns of the FP world. 
That's 'sequence', right? The issue here is A =&gt; B doesn't have a Traverse instance. :/ 
That might be an interesting idea. 
A particular reason is always "some reason" :) Also, I'm aware *(implicit log: Logger)* works fine in *MyClass*. I was just trying to handle the case where you choose to not keep something implicit. For instance, you might have a code base where you don't use implicits but you may want to extend from another codebase that uses implicits liberally.
http://engineering.monsanto.com/2015/09/24/better-spray-metrics-with-kamon/
Thanks for being an early adopter! Indeed, the set of changes is pretty small. https://github.com/lihaoyi/fansi/commit/75bbbb555fe232c1928c0270bd16c21b6d6adcc7 https://github.com/lihaoyi/PPrint/commit/005a51a509bae088dc69c4427a27280249ce8cf6 https://github.com/lihaoyi/fastparse/commit/350486ffa27ba81870da70d4a3ad59f30d424ec7 I'm curious about the use of `_root_.sbtcrossproject.CrossPlugin.autoImport.crossProject`; is it for compatibility with the previous ScalaJS builds?
Just wrote a deploy script in Ammonite yesterday and it worked like a charm. Thanks for all the work you've put in to make this happen! I'm eagerly waiting for the day when the scala-native ecosystem is good enough for us to write Ammonite-native scripts and have sub millisecond startup time :)
Lol. This guy comes into the Scala subreddit spouting how Scala is a scam. I'm not sure what you expected.
IntelliJ's paid edition can run NodeJS and npm commands much like their webstorm product.
It would be great to see some code samples showing differences to e.g. `akka`.
I've ported a few trivial C applications without much issue. From the other side of the spectrum scala native is looking great as well. The one facility where I'm unsure of is debugging. It doesn't help though that I've never used LLDB before scala native, I was never able to figure out how to set up breakpoints.
It's to shadow Scala.js 0.6.x's own definition of `crossProject` (which only supports JVM and JS) with that of `sbt-crossproject` (which supports combinations of JVM, JS and Native). Scala.js 1.x directly uses `sbt-crossproject`.
Ok, thanks!
How would that be better than on the JVM where collections are based on JVM-native arrays and objects? Ultimately there's a lot of new functionality in the scala collections. You could write a facade for immutable.js and just use that instead of the scala collections.
Maybe with the collections redesign this won't be such a big lift?
Unfortunately, even if you *do* find or write a better collections library for Scala.js, it will not be viable. You will isolate yourself from the rest of the ecosystem of Scala(.js) libraries, which means you will have to reimplement a lot of things. IMO, it is not worth it.
&gt; One thing I don't like in ScalaJS is lots of javascript added to compiled bundle because of Scala Collections bulky design. How will immutable.js eliminate the scala collections "tax" in scala.js?
If anything it seems that 2.13 collections overhaul has been *adding* to the library (in order to hide CanBuildFrom from the user) more than removing. I'd be shocked if there was even a 10% reduction in generated Scala.js application binaries with Scala 2.13 collections. Basically @160KB is the baseline for now, getting miniscule Bucklescript-like binaries isn't in the cards without a major overhaul on the JVM side of the fence.
Do you think it would be possible to achieve the opposite, use scala collections from vanilla javascript? Similar to https://github.com/swannodette/mori
No, I don't think so. Most of the Scala collections library relies on implicit parameters that are quite involved. Called from JavaScript, you would have to provide those by hand, which defeats the purpose and makes for a very ugly API.
You should have a look at http://opentracing.io/ We also rolled our own request tracing library first, but it makes sense to concentrate the efforts and OpenTracing gives you a lot on top!
Kinda weird that they are shadowing `Int` in the `SortableList[Int]` trait and extending case classes, imo. In fact maybe they shouldn't be using a type parameter in the first place if their mergesort implementation is tied to `Int`. That, or keep the type parameter (and make it covariant), and make `merge` polymorphic on type of the `SortableList[T]` and have your mergesort implementation take an implicit `Ordering[T]` aka def merge[T](sortableList: SortableList[T])(implicit o: Ordering[T]): SortedList[T] = sortableList match { case s @ SortedList(_) =&gt; s case unsorted =&gt; ??? } 
I really wonder how far are we from being able to run something like http4s ?
Great stuff. thanks a lot!
In the way you implemented MergeSort, I think you can be sure that leftTail is always equal to Nil, which could reduce the size of your Merge function with ignoring leftTail. Correct me if I'm wrong! 
The only one here who sounds like never worked on real world software is you. ScalaJS' documentation on its website is clear and concise. If you can't read properly and follow the instruction, I highly doubt you're able to solve problems on real world software.
The idiot was probably fired/humiliated by a Scala dev in real life.
Correct! I'm actually planning on releasing some folloup posts regarding some performance benchmarking and optimization, and code cleanups. I tried to be slightly more verbose and explicit where possible to help newer folks follow along.
Dunno, maybe it's smaller? I mean at the end of the day, OP has to decide if he wants to use collections-like features. If he isn't using them then scala.js won't bundle them. It does DCE. And if he is then he can't get them completely for free.
&gt; If he isn't using them then scala.js won't bundle them If only that were the case. It's almost impossible not to touch the collections library in any non-trivial application; as soon as you do you quickly hit the 160KB baseline. DCE does an excellent job of eliminating unused code, but the collections library, complex as it is, is unable to be DCE'd due to everything depending on everything else. It's an unfortunate situation. Ironically enough on the JVM side, today they announced their intentions to add more features.
Spark maintainers didn't show much love for Spores. If you are interested in them (and other mechanisms to avoid run time errors with Spark), I have this issue for you: https://github.com/typelevel/frameless/issues/80
- Albert Einstein
Yeah, it's pretty damn hard to find *any* non-trivial code that doesn't use at least Seq/Vector... which basically force almost all of the collections library on you. (Well, I guess it doesn't force the 'parallel' collections, but who knows.)
I think sjrd said something about there being some potential for reducing size of generated binaries with 2.13, but it didn't sound like a drastic improvement. Would be most welcome to cut the collections tax in half or more (and get the baseline down to under 100KB). Maybe by the time Dotty lands collections will be more amenable to being DCE'd.
Hello there! &gt; 1. I am looking at mailbox size metric and its showin in percentile. But I don't want percentile I want to see (and chart) the absolute size of the actor mailbox overtime. Kamon sends the mailbox size metric as a histogram because it is actually sampling the min/max/current mailbox size periodically and then reports all these measurements.. take a look at the Core / Metrics / Instruments / MinMaxCounters section [1] in the docs for a bit more info on why we have that. If you are using Datadog I would suggest plotting min and max mailbox sizes since those are the only values that you can really trust. &gt; 2. All the time is shown in nano second (ex. time-in-mailbox). What if I want to see the time in milli second and not nanosecond. Look at the time-units settings in the kamon-datadog module [2]. btw, better to post questions in the mailing list.. he is less likely that we will see it unless someone points us to it. [1] http://kamon.io/documentation/kamon-core/0.6.6/metrics/instruments/ [2] http://kamon.io/documentation/kamon-datadog/0.6.6/overview/
For someone motivated: try out https://github.com/scala/collection-strawman (I made it work with Scala.js) on some small examples. Try and see if you can use some typical functions like `map`/`filter`/`foldLeft` etc. and see what you get as size output. Try and figure out if there are things that are pulled in that shouldn't be. Report results as issues on the collection-strawman repo. You can make it happen! (I unfortunately don't have time to do this; I need to spend it on the compiler and core toolchain)
But my point is _that means you're benefiting from them_, albeit indirectly. Now, you mean not be _gaining_ from them -- something could have been written in a different way. Except, it sounds like you're saying that the collections library "breaks" DCE and you end up with code that will never be called. Is that what you mean?
Ok, but if I only use (even indirectly) e.g. Seq.map but not Seq.filter (to give a silly but simple example), doesn't DCE mean that it won't include the code for .filter?
What is the complexity of such implementation of MergeSort? Is it `O(n * log n)`? Btw, I really liked the videos.
I tried but never managed to get it to properly aggregate actors by class, which is something we *really* wanted to do for our use case. Trying to do it in graphite/grafana didn't really work well either.
Closure compiler is unable to DCE the vast majority of the collections library. Why? Look at the Scala collections hierarchy. You can see for yourself, create a hello world scala.js project and then `fullOptJs`. Will be around 20KB. Now, create an empty `List`. Should be around 40KB. Create an empty `Map`, 60KB, and so on. Then once you start using `map`, `filter`, `zip`, etc. it's game over, you pull in the whole thing, 160KB.
https://github.com/typesafehub/scala-logging
Scalding co-author here. Spores sound pretty cool, but in practice the approaches we use very rarely cause problems. In scalding we try java serialization on the function and if that fails try Kryo serialization, and if both fail there is a technique to debug. Spark is quite similar in this regard. In practice this very often works. Maybe 99% of the time (at Twitter and Stripe where I have observed). Spores are cool, but put a significant cost on users, many of whom are not experts in serialization and even scala. To pay that cost for 1% of cases can be a pain. I like spores but convincing people to adopt them when runtime serialization errors of this sort are very rare and very quickly detected is a hard sell. Edit: to be clear I would love to see spores as a supported part of the standard library to control closures better (maybe even give more usable equality on functions or to aid in avoiding GC leaks).
Use SLF4J as a base API, and wrap it however you like. 
On top of slf4j, logback. 
Based on the track record with Scala 2.12, it will be at least six months.
&gt; For Scala.js that's not an end goal anymore. It's a living fact ;) It's been like that for 2.5 years. A truly stunning accomplishment!
Glad I expanded the comment. I needed a laugh.
It's about to be done: https://github.com/playframework/playframework/issues/7261
First of all, I was wondering why the bottom types (like `Null` or `Nothing`) doesn't have all methods above. Here is the [answer](https://softwareengineering.stackexchange.com/a/195807/191603). It says that **Subtyping** and **Inheritance** not the same. Subtypes don't inherit all methods above. So my question is - isn't it dangerous? I can put an instance of subtype without method in supertype with a method. It seems that Subtyping without Inheritance is dangerous. val user: User = null user.getName() On the other hand, Wikipedia keeps [saying](https://en.wikipedia.org/wiki/Subtyping) that Subtyping is safe. &gt; If S is a subtype of T, the subtyping relation is often written S &lt;: T, to mean that any term of type S can be safely used in a context where a term of type T is expected.
I am a bit confused here. What does scala have to do with SEO? Assuming SEO stands for search engine optimization.
&gt; Scala is a scam for selling InteliJ Umm. What??? Scala sold Intellij? I cant even.... &gt; since even simple ScalaJS program is not working with eclipse plug-in There is a plugin called eclipse plugin? 
&gt; so my experienced advise to you is to find out why things are not working My experienced advise is to not come and rant here and find a solution. Many people have developed successful projects in it. &gt; Such a wizard can set a trend for the rest scala ecosystem to follow in its success path. A wizard in eclipse can set scala ecosystem in success path? Well, it already is in its success path. The last thing we want in this sub is mindless rants.
&gt; but still not supporting SBT 1.0 Why didn't they just wait for [this PR]https://github.com/playframework/playframework/pull/7830 to get merged? They're literally right on the doorstep. I guess they wanted to get in all of the latest changes and publish a separate release for sbt 1.0 support (which will hopefully be very soon).
Well written, clear. Thank you.
Looks really useful, thanks!
not yet
I'm going to assume you mean the front end is React and your backend is a Scala API. If that's the case, your best bet is to see if you can do it with ScalaJS because you should be able to call `React.renderToString` and embed that in your initial page load. 
since many years I use "grizzled-SLF4J a scala-friendly SLF4J Wrapper" - small, efficient (uses inline) and easy - just extend Logging trait and you have info, warn, debug etc methods available!
Nice article and use of intersection types. An alternative to using type parameters that is often overlooked is to use type members. The advantage is that we can safely ignore that the phantom types even exist, in case we're is doing something unrelated. So you can make it: class Chef(ingredients: Seq[String]) { chef =&gt; type Pizza &lt;: Chef.Pizza def addCheese(cheeseType: String) = new Chef(ingredients :+ cheeseType) { type Pizza = chef.Pizza with Cheese } // ... } Also, from my personal experience, type members are often more robustly handled by Scalac than type parameters. A minor nitpick: case class Chef { This is not valid modern Scala. You need a parameter list for case classes.
This! We are working right now to support sbt 1.0 (the PR is a working in progress). But we want to avoid it to hold all the issues there were fixed already. Of course, we can do another release as soon as sbt 1.0 supports lands on 2.6.x branch.
Official announcement here: https://blog.playframework.com/play-2-6-5-released/
Yes, it takes time. It is not just Play itself: there are sbt-web plugins, twirl and a number of other plugins which we depend on to build and release. James Roper has made an incredible amount of contributions so the whole Play's ecosystem can support sbt 1.0. Anyway, as I said in another comment, we are already working to finish all the necessary tasks to support sbt 1.0. You can subscribe to [this PR](https://github.com/playframework/playframework/issues/7261) and get notifications about the progress. Best.
Would be also interesting if its possible to alias certain collection types to their native javascript variant. i.e. the implementation of `Array` in Scala.js is actually just a Javascript array (ideally without any boxing overhead). Same deal with Javascript map and `scala.collection.mutable.Map`. This should help in reducing the collections tax. iirc There was some work in this area but I can't remember what happened?
Well DCE still does a very good job, its just that the design of the Scala collections (being so generic) it makes it hard to eliminate the code. The alternative would have having not so nice collections library (that happens to eliminate much better)
On top of logback, [logstash-logback-encoder](https://github.com/logstash/logstash-logback-encoder).
I'm actually building a scala-powered static site right now. I don't have a very fleshed out example to show you, but I'm using [scalatags](https://github.com/lihaoyi/scalatags) to render the HTML and representing page elements as case classes. So I might render a page like so (hacked together, non-idiomatic POC example): trait View { def render: Frag } case class ShoopingCartView(user: Option[User], items: List[Item]) extends View { def render: html(body(h1("hello ", user.getOrElse("guest")), p("you have ", items.length, " items in your cart"))) } And then you can call `.toString` on your `Frag` then you can send it down the pipe with the right content-type and voila. At any rate - because it's a static site you avoid many of the SEO pitfalls associated with SPAs. You can also call on `script("""console.log("hello, world")""")` etc. And have a default view that summons your jQuery or Bootstrap or whatever. I'm looking into writing minimal JS using SBT as the build tool but don't have an answer for that yet.
That would be wrong in general. Scala's `Map` uses `==` for key equality, but JavaScript's `Map` is essentially an `IdentityHashMap` which always uses `eq` for key equality. The same issue applies to `Set`.
category people / matryoshka people... If I am folding over TREE strcuture, and each fold function is receiving in addition to Node of the tree it's Parent, how would such fold be called? type Parent = Node // just for this sample def howtocallme[A](tree: Node)(acc: A, f: (A, Parent, Node) =&gt; A): A = ... 
http://software.clapper.org/grizzled-slf4j/ Removes all boilerplate beyond `extends Logging`. 
Sure will try to update them asap. Thanx for the interest. 
That is what my last 2 employers do.
Has scalajs-react added support for server side rendering?
[Scalatags](http://www.lihaoyi.com/scalatags/) of course. You can use it in any way you'd like: Scala, ScalaJS, render them dinamically(e.g. in Play server), statically ([sbt-hepek](https://github.com/sake92/sbt-hepek)).
Yep, scalatags. The only real alternative is Twirl but scalatags compiles faster. But scalatags is not HTML which complicates testing CSS on static HTML pieces.
Using `Logging` trait with domain-specific methods with great success. Do not using logging api directly really helps with logging unification.
Scalatags is great, but imho it's more of a HTML builder/construction library than a templating engine for HTML. If the webdevs/designers in your company produce ready to use HTML, it doesn't help much (at least not if you don't want to translate that HTML back to Scalatags code).
I've read the article, thought about it a bunch, and think that while it does explain what phantom types are, I feel the specific examples used here don't work (for me). Door: if open and close are not valid calls for all values of type Door, why even expose them? Wouldn't it be better to: sealed trait Door object Door { case object Opened extends Door { def close: Door = Closed } case object Closed extends Door { def open: Door = Opened } } I feel this brings the same compile time guarantees without the type level trickery which, while fancy and frankly rather clever, is going to cause some serious head-scratching for beginner to intermediate Scala developers. Builder: first, if you have so few values to use when you build, why not simply create a Pizza.build method that takes one parameter per compulsory ingredient? I initially thought it was to allow *other* ingredients to be added, but Chef doesn't let you add anything but toppings, dough and cheese. Second, I feel the example is completely undone by the fact that it's proposing to use very cool type level things to offer compile time guarantees - and then uses string for the type of all ingredients. Maybe using proper types wouldn't break anything, maybe it would - but the current state of the example feels like it's taking shortcuts that rather defeat the point of what the author is trying to do. I'm actually very interested in how to write a proper builder pattern with compile time guarantees. This article hints that phantom types are the way to go, and I'll definitely play with it a bit, but I must admit I'm not as convinced as I hoped to be.
Oh, nice. I don't often use type members, but they'd definitely make things easier (from a library user perspective) here. Thanks for pointing it out!
&gt; User might be costly to load because of LOB columns or requiring other joins. You're using User containing a ?[Car] field so that you can have a ?[User] value that will behave like a ?[?[Car]] (the thing you actually want). Queries usually return with `Seq[User]` and not `?[User]`. And as I've said we don't need `?[Car]` but `Seq[Car]`. &gt; Well if there are no use cases for ? in the first place then why do you want it? I've already told you what are the benefits of it: lower memory cost, less CPU cycles wasted on allocation/garbage collection and integrates better with existing systems - if you use bindings. I'm also aware of its shortcomings however, my point is that those shortcomings are not serious enough to care about them. &gt; Except when it doesn't, when T is itself a ? type. It still works. Not as you want it, though. Usually, if you have nested optional types you already have more problems than you should. &gt; E.g. addition distributes over multiplication, A * (B + C) = A * B + A * C. Associativity, (A + B) + C = A + (B + C). How do these relate? &gt; I mean fundamentally you can't even form the type that ? is supposed to be a sum of, String? is String + a pseudo-type that doesn't properly exist in the type system. Because it's not a type-system thing but a machine/compiler thing. &gt; There's nothing magical about None, it's a plain old Scala class. You can define your own: case class MyNone() (indeed scalaz does so), and it'll behave the same way as the "official" one. None is defined like `object None extends Option[Nothing]` and it IS magic - it's a new `null`. &gt; Nothing is not a value, and I'm not advocating using it at all (if anything you're advocating it more than I am, since it's the closest thing to a reasonable type for null). You do advocate it because that's how None is implemented. &gt; How? What's the scenario in which you currently expect an Option[Int] and receive null, where the introduction of ?[Int] would fix it? In scala, Option[Int] can be Some(x), None or null. ?[Int] can be x or null. &gt; No, I don't think so (the language doesn't even have "real" pointers). Of course it have pointers, how would it work otherwise? &gt; The only scenario where NPEs currently happen is where a Java interop call wasn't expected to return null and did. ...and when someone uses null in scala which is not that rare... &gt; ? doesn't help with that, because it's still possible to do the wrong thing at the Java interop boundary. Unless we create wrappers while doing java interop(we should because java APIs are terrible to use) and 'mark' the interop functions as nullables. &gt; That's a performance optimisation only; that's better done at the implementation level, it's not worth compromising the language design for. You can't do that - sometime languages need to change for better optimization. &gt; How? Forbid ever using?[T] as a type parameter? Have some kind of whole-program analyser that runs every time a class gets loaded and checks which generic parameters get put in ?s? Yep - the typechecker should complain. Which means no more shortcomings just a little "inconvenience" at certain rare cases. &gt; In concrete code yes, but not when you want to be able to reuse generic code. Or use `Either[KeyNotFound, ?[T]]` and reuse the generic code.
Well, yes, but you can extract a function, use for loops etc... That's more than enough for me. xD
Great post as always!
I can't seem to reproduce your initial code. Worksheet in scastie: https://scastie.scala-lang.org/gXQ0R8wVTTixYZoJVmKgDQ 
.... Why do you want to produce HTML from scala?
Can you update the tutorial and also show mocking and Dependency injection?
Oh.... its IntelliJ. Sorry. IntelliJ shows red squiggly on the assignment. it compiles in SBT. I just pulled my hairs out for so many hours. because of an IDE quirk. I'm sorry. https://imgur.com/a/Uk6FP
- Wayne Gretzky
* /u/pgrizzay
IntelliJ has a few of those. When you can't seem to find the problem try to compile and read the errors in there. 
Just s thought, a lot of the times this is caused by competing implicits.
Twirl. Used in Play: https://github.com/playframework/twirl#sbt-twirl https://www.playframework.com/documentation/2.6.x/ScalaTemplates But you can easily embed into Scala with the html""" syntax: https://github.com/playframework/twirl/blob/master/api/shared/src/main/scala/play/twirl/api/package.scala 
I'm guessing he/she is returning html from a web server.
too bad that it can only be used with sbt
It won't help your workflow directly, but Scala can be a fun language to learn and can teach you ways to think about programming that will improve your R code.
What is your workflow?
I stream recordings to and from a database, and the fs2 library is very useful there. Racket seems like a fairly sane language, so I'm not automatically recommending to switch. Check out doobie and fs2, see if that appeals
https://github.com/JakeWharton/twirl-maven-plugin
I use scala on spark for pulling data (from various sources - relational sqls, bigquery, elasticsearch, raw parquet/avro, etc) doing various transformations and feature engineering, and doing modeling. I think it's a particularly elegant language for expressing that sort of data pipeline type series of functions. It's a lot less fluid in java (or even in python with pyspark)
R is a fantastic language if your data set is small enough to exists in memory and computations are light enough. Alternative tools start to make sense when data sets are large, computation is large or there is a need to integrate a process into some production environment. I feel scala/spark will be the 'next' data science tool and thus I'm trying to teach myself. 
I find that when I get in tough situations I have been able to use [data table](https://github.com/Rdatatable/data.table/wiki) as opposed to the usual [tidyverse](https://www.tidyverse.org/learn/). This was useful for some data that was larger than .5 gb. I have never had to load more data than memory. I think scala is elegant, but right now things are not broke so I don't think I want to fix it. I still would not mind learning it in the future.
Is it common to use play or whatever to be a full stack web app serving html/css/js as well? I've only ever used play as an api returning json which is then consumed by a front end team using whatever js framework they have all started arguing about this week.
Its amazing what can be done with data tables and foreach. Add on top of that a ML package like XGB that will multithread and most problems can be solved. I'm right with you on that. For scala, I actually needed to take a short Java course in order to understand the scala book I wanted to read. Scala seems to be taught like this - 'In java we use this thing (that we wont explain) but in scala we change it a little like this'. I found this completely useless as I didn't know java. 
I was under the impression that having a more granular effect system than "pure function" and "is affected by or has an effect on one or more things" was desirable. Say there's a `ConsoleRead[_]` which is a subset of `IO[_]` and thus implicitly convertable to `IO[_]`, but it would be known that a `ConsoleRead[_]` couldn't write to the file system or read the system clock or anything other than one or more reads from a console and that being able to know that would be useful for the exact same reason that knowing that a function is pure is useful. Or to restate, `ConsoleRead[_]` is less powerful than the `IO[_]`, but in the exact same way that a pure function is less powerful than a `IO[A] =&gt; IO[B]`. Is this orthogonal to the "One IO per Program" section? Does this suffer from diminishing returns? Am I flat out wrong? 
I would say it's more practical to move the choice of granularity of that from library developers to application developers. E.g. as an app developer, you might want to separate console reads from console writes, file reads from file writes from querying file stats, s3 uploads from crawling reddit, etc. and it's not like the library could provide you with granular enough API. Libraries provide you, however, with an ability to create such types for your particular need, e.g. Free monad &amp; friends. That is not only expressive to the point you are asking for, but also comes with the advantage of being able to substitute implementation easily e.g. for tests and to describe one type of effectful computations in terms of a different constrained effect set. The section is not related to that. If you're doing pure FP, your program ultimately composes up to `List[String] =&gt; IO[Unit]`. As an application developer, it's way more convenient to stick to a single type for `IO`, because, at least from expressiveness perspective, it's just easier to work with. E.g. if you have `val f: A =&gt; IO[B]` and `val g: B =&gt; IO[C]`, you can compose them easily (`Kleisli(f) andThenK g`). It's less straightforward if you have `val g: B =&gt; Task[C]`instead, and since `IO` and `Task` are there for the same job, why have both if you can just pick a single one?
I'm definitely on board with the tragedy that is "scala for java developers". Scala is not better java, it's a programming language that can interface with java code since it runs on the jvm.
I think it's a very good practice to abstract fine-grained effects so each section of your program only requires what it needs to perform its function. You can do that using free monads or effect type classes (AKA *mtl*). Yet at the end of the day, all these fine-grained effects will be translated and expressed in terms of a single effect type, which is the root effect of your application (and also the most powerful, by definition, since all other effects could be translated to it). In FP, the only meaning we can ascribe to a whole program is a value, and this value must be composed from all the "sub programs" in your application, which are themselves values, which means that no matter how you represent effects, your "main function" is going to be expressed using a single effect monad which can be interpreted by the runtime (or interpreted by you, if you are using `unsafePerformIO()` to integrate with Scala's main function). So it's really orthogonal to the "One IO per Program" section, like Oleg explains below. By all means, divvy up the effects of your program into fine-grained bundles, but also, recognize that everything will be translated into the most powerful effect monad required by your application, since your entire program must be a single value.
The bare style isn't very extensible, so although it is slightly more compact, it's not a good choice if you ever want to break your project out into subprojects. Rather than trying to offer first class support for two different paradigms I think it makes much more sense to nudge people in the direction of the more extensible solution. 
At the expense of making the syntax ugly and confusing? I may as well stay with Maven, then.
sbt is a tool, not a rule. do things how you want.
With the entire Scala community adopting SBT and mostly ignoring other build tools, I'm rather worried about it becoming a rule. That's why I decided to brush up, only to encounter yet another reason to avoid it.
I don't know exactly what you mean. I programmed a little bit in Scala, so I don't understand what Scala does, but in Kotlin you would use: //you can use extension functions, but just for keeping it simple class Resource: AutoCloseable{ override fun close() = Unit } fun acquireResource() : Resource? = Resource() fun &lt;A : Any&gt; access(f: (Resource) -&gt; A) : A? = acquireResource()?.use(f) use is a lambda that calls close at the end (in a finally block) and returns the content of the lambda
The phantom types condense multiple verbose type definitions into just a single one with multiple possible marker traits. They also (in the form shown) allow builds to proceed in any order. Your example would need multiple methods per 'state transition' type to allow that. Finally, the ingredient string list is an implementation detail, shown for the sake of providing runnable code. More to the point, you can build type-safe workflows using phantom types on top of even stringly-typed code. See e.g. https://gist.github.com/yawaramin/552c25a23549f15556d54954e39f946d (F#) which outputs a (SQL query) string at the end but forces you to build the query in valid SQL syntax.
It's not a rule. It's a tool. Do what works for you.
It really depends on how you view the syntax change. I personally prefer the new syntax because it's much more explicit, whereas the old syntax did a lot of things for you implicitly. However, if this is a dealbreaker for you using sbt by all means continue using Maven.
&gt;It really depends on how you view the syntax change. I personally prefer the new syntax because it's much more explicit, whereas the old syntax did a lot of things for you implicitly. Could you elaborate on this? &gt;However, if this is a dealbreaker for you using sbt by all means continue using Maven. Well, thing is, SBT is popular among the Scala community, including the wise minds that created the language itself. There must be a reason. Surely its design is actually sane, and I'm just misunderstanding somehow?
I used the bare style with a project first without really understanding the downsides or that there was even another valid way of doing it. Then I realized I wanted to plug in some compile-time code generation to my project using Slick. That was simple enough but then I wanted to customize it, so I had to make a separate project. I was pretty new to sbt (still am somewhat) but after I did a lot of reading I finally figured out how to have a proper multi project build that did exactly what I want. Now my build is much more sensible and extensible and I realize that I probably should have had it the other way to begin with instead of having to figure out that there were even multiple styles in the first place. That's just my experience though.
I don't know any build tool with a sane syntax. SBT is the least bad option; and I have more problems understanding how various third-party plugins interact than to convert a flat build.sbt to the new lazy val style. Keep using Maven if it suits you!
&gt;I don't know any build tool with a sane syntax. Sadly, I completely agree with you on this point. I have yet to meet a build tool that's actually elegant. Maven did try, with its mostly-purely-descriptive, plain-XML syntax, but it failed rather hard by making the schema so rigid, forcing all manner of random configuration stuff into `&lt;plugin&gt;` elements. This would be so much nicer: &lt;!-- Required plugins are determined from the namespace bindings. The namespace URIs are their repo coordinates. No need to declare that separately. Plugin versions are not part of the namespace URI. They're given separately. --&gt; &lt;project xmlns="maven:org.apache.maven:maven-core" xmlns:java="maven:org.apache.maven.plugins:maven-java-plugin" xmlns:jar="maven:org.apache.maven.plugins:maven-jar-plugin" xmlns:surefire="maven:org.apache.maven.plugins:maven-surefire-plugin" &gt; &lt;id o="com.example" m="example-project" v="1.0"/&gt; &lt;name&gt;Example Project&lt;/name&gt; &lt;description&gt;This project does…&lt;/description&gt; &lt;!-- Declare this to be a Java project. Adds javac, surefire, jar, etc to the build. javac settings also go here. --&gt; &lt;java:project source="8" target="8"/&gt; &lt;!-- Can be combined with other project declarations for polyglot projects, e.g. &lt;java:project/&gt;&lt;scala:project/&gt; --&gt; &lt;!-- Declare the artifacts to build. These imply the build steps needed to create them, e.g. java:classes implies a javac run. --&gt; &lt;jar:jar content="java:classes" main-class="com.example.Main"/&gt; &lt;jar:jar content="java:doc" a="javadoc"/&gt; &lt;jar:jar content="java:source" a="sources"/&gt; &lt;!-- Implies compilation of sources in src/test/java. --&gt; &lt;surefire:test fork="never"/&gt; &lt;!-- Dependencies can be on modules (libraries), as well as on parts of the environment (like the minimum Java and Maven version). --&gt; &lt;depend g="com.example" m="some-library" v="1"/&gt; &lt;depend g="org.junit" m="junit" v="4.0" scope="test"/&gt; &lt;depend java="8"/&gt; &lt;depend maven="5.1"/&gt; &lt;scm type="hg" url="https://hg.example.com/example-project.hg"/&gt; &lt;artifact-repo type="ivy2" url="https://ivy2-repo.example.com" content="java:binary-jar java:source-jar java:doc-jar" filter="com.example:*"/&gt; &lt;!-- The attribute names here correspond to the namespace prefixes above. --&gt; &lt;plugin-versions java="1.2.3" jar="4.5" surefire="6.7" /&gt; &lt;/project&gt;
It depends. I use scala because it is one single language from which I can experiment/prototype and then take it to production. There are several libraries that comes with good performance out of the box with little/no tuning and one such is akka http. There will be a definite need to expose your work as APIs, in this aspect scala is much better. If you are already familiar with python, then stick with it. It has loads of libraries and toolkits. But if you have plans for production with some serious scale, then I would recommend scala and the jvm platform.
Personally I like to use the bare still whenever it's enough, I only switch to the "full" config when I really need to.
What do you think of gradle?
You might try [CBT](https://github.com/cvogt/cbt) and see if suits you better. I haven't used it myself so I can't vouch for it.
yes, a friend made a pluggin for it, he used bytecode tricks in order to inject a context everywhere, my idea is most like an experiment that should be simpler to use. Thanks.
Hi! First of all, thank you for this kind of threads. I've recently taken the Coursera Specialization on Scala, and want to go further with Scala. I'm deeeeeply in love with Scala. How can I acquire the knowledge to understand complex concepts like phantom types, and to eventually, be able to contribute to the compiler? Thank you in advance for your time, and for Scala, of course :)
Speaking as someone whose background in programming started with R and SQL: No, not really. With scala you can jump onto the spark hype train quite easily, and if you are working with really big data that might be nice. But the problem is, almost nobody has really big data. I regularly worked with terabytes of data at amazon, and I quickly learned that sqlite with R almost always got my analysis done way quicker than the headache of setting up clusters, sharding and uploading data, and then analyzing it. In order of preference: 1) Use R and analyze your data in memory 2) Use SQLite and the sqldf package to transform and munge your data, with R to analyze it 3) Find a creative way to sample your data, so you don't have to use your dataset 4) Spin up a spark cluster and use that. 
Awful. Can't even set the root project's artifactId without weird hacks. Also, I saw a malware ad on their website. That did not inspire confidence.
No, it's even messier. My almost-ideal build definition would look more like [this](https://www.reddit.com/r/scala/comments/70qm5e/why_are_bare_sbt_build_definitions_not_recommended/dn5shq0/). (I say almost-ideal because there is still some syntactic noise due to XML.)
I've maintained two production Play! web apps, i.e. front- (html/css/js) and back-ends. So, yes, my understanding is that it's common to use Play! as a full stack web app. You might enjoy this Play! post - http://blog.jamie.ly/software/2016/09/25/play-framework.html - from a former colleague.
Well, one of my big concerns is keeping the build definition clean, easy to understand, and with a minimum of boilerplate, syntactic noise, and non-obvious semantics. In your example, it's not immediately clear whether the various `val`s have special meanings to the build system, or are just variables used elsewhere in the build definition. Also, having to say `val` and `Seq` everywhere is noisy. That's why I liked the bare style so much. It keeps that to a minimum. It doesn't pollute the build definition with magic value names. It's just a list of expressions that evaluate to SBT settings.
CBT is a hell of a lot less messier than SBT. Doing CBT plugins (which are just bare Scala traits) are often 3-5x smaller than SBT. Actual builds are also far easier to understand (you just extend a class and override your settings) and the tool itself is around 2-5x faster than SBT (with almost instant startup time). Although SBT is nice in that it has a slightly less verbose syntax its a lot more confusing and complicated, especially in how configs/settings work
Great stuff! I always thought the scala website was missing an interactive playground.
I'm sure there's gotta be a pretty easy solution to parse HTML and render Scalatags, if that's something you'd have to do pretty frequently.
Very nice article, but i actually found first example(state) way more informative and applicable than the builder pattern. What is the point if result is immutable? You might as well stick all the ingredients in class parameters and curry the contractor.You can even guarantee order of application that way
&gt; I know about the compositional 'issue' and I was the one who mentioned it to @m50d. However, composition is rarely useful and we can easily avoid it and provide better solutions. You need all your data in some normalized form without it. I don't know about you, but this can be pretty annoying to *have* to do, like working without tuples. The compositionality in my example makes it easier to understand each part in isolation. "Composition is rarely useful" is something I can't agree with. &gt; Why would we care about the functor laws and how did I break them? `?` (like [java.util.Optional](https://developer.atlassian.com/blog/2015/08/optional-broken/)) could not respect the functor laws due to its nesting behavior. &gt; Also, this is wrong from the beginning: The example is, given a bunch of user IDs, get the corresponding user objects. From the user objects, get users' cars where they're available, or "errors." Maybe I could've made that more clear. &gt; this also looks nasty: You can replace the folds with pattern matching if you want (like you did). &gt; But your methods can easily avoid it by unions These won't have functor or monad instances, unlike `Option`.
Don't you just do a cata and treat the node you get as the parent? I.e. normally `cata` is `(NodeF[A] =&gt; A) =&gt; Node =&gt; A`, but there's no reason you can't set `A=NodeF[B]`.
There shouldn't be one, just because at evaluation time there isn't going to be a way to do that, as far as I can see.
I find the best way to learn is solving real problems that you have. Just work on an actual program, and work your way up gradually. Or if you really want to work on the compiler, find a simple improvement that you want to make to the compiler to start with.
Maybe, but probably not. R is a fantastic language to do data analysis in. If your data fits in memory, then you just want to stick with what you do. Unless when the data extraction/pre aggregation with SQL is giving you headaches. If that's the case, it *might* be possible to have your data in some streaming database and work on it with Spark, bringing the fetching /preparing of the data and the analysis on the data into a single language. That would probably mean an overhaul of your entire storage model. I could imagine cases where that might be worth it, e.g. when you spend 90% of your time in SQL. I could imagine overwhelmingly more cases where it's not worth it.
&gt; I've already told you what are the benefits of it: lower memory cost, less CPU cycles wasted on allocation/garbage collection and integrates better with existing systems - if you use bindings. No, I mean what's your use case for an `Option`/`?`-like type at all? Every time I give an example of a nested use you say you can use `Seq[T]` or `Either[KeyNotFound, T]` instead of `?[T]`, which is true as far as it goes, but it's just as true for non-nested uses. &gt; Usually, if you have nested optional types you already have more problems than you should. That's a completely backwards way to think about it. Awkward, convoluted code is precisely when you most need the language to stay predictable and consistent. &gt; How do these relate? You asked me about the rules for sum types. &gt; Because it's not a type-system thing but a machine/compiler thing. Right, so it isn't a sum type. &gt; None is defined like object None extends Option[Nothing] I prefer the ScalaZ implementation `final case class Empty[A]() extends Maybe[A]`. The standard library implementation does behave equivalently in all reasonable code, so one can argue that it's an implementation detail, but fundamentally the use of `Nothing` is a performance hack and not a style I endorse (indeed I'd say it's more in line with what you're advocating - the use of `Nothing` is there to reduce memory usage and allow faster comparisons). &gt; it IS magic - it's a new null. Wtf? What does that mean? If I write `class MyClass` is that a "new null"? No, it's just a plain old Scala class. &gt; In scala, Option[Int] can be Some(x), None or null. ?[Int] can be x or null. Almost all ordinary classes in Scala can, at least in theory, be `null` at runtime. `String` can be `null`. `UUID` can be `null`. `Date` can be `null`. `Seq[Int]` can be `null`. If you write a `class MyClass` then that can be `null` as well. There's nothing special about `Option` here, and making a non-nullable replacement just for `Option` is effectively zero progress towards fixing this, unless you're planning on writing non-nullable replacements for every other class in existence, one at a time. What's the use case where `?` helps with the *general* problem? &gt; Of course it have pointers, how would it work otherwise? Wtf? Plenty of languages don't have pointers, you just... don't put them in the language. Implementations may use pointers, but those are an implementation detail. A language that needs to interop with C will have to have ways to handle C pointers, but that doesn't mean allowing pointers in the language proper. &gt; Unless we create wrappers while doing java interop(we should because java APIs are terrible to use) and 'mark' the interop functions as nullables. Right, but if you're going to be creating wrappers and checking nullability in any case, then you can just have the wrappers represent anything that's nullable on the Java side as an `Option` on the Scala side. So whether you use `Option` or `?` makes no difference to how safe it is. &gt; Yep - the typechecker should complain. Which means no more shortcomings just a little "inconvenience" at certain rare cases. You're not thinking this through. How can the typechecker tell whether a generic function is safe to pass `?` into or not? If you have some `def foo[T](x: T): T` you have no way of knowing whether `foo` is going to form a `T?` somewhere in its body. You might even compile against a version of `foo` that doesn't, but then you run against a different version of `foo` (library upgrade) that does. So either you get very surprising random breakages of your program, or you can't use `?` types in generic functions at all, which means you can never have a `Seq[T?]`, you can never have a function (value) that takes or returns a `T?`, which in turn means you can't ever use any higher-order functions like `map`/`fold`/`filter`... you'd lose all the point of using Scala in the first place. &gt; Or use Either[KeyNotFound, ?[T]] and reuse the generic code. What if you don't realise it needs to be a `?` inside until later? It's easy to switch code from `Either` to `Option` because they're both lawful monads and so lots of functions are written generically and will work correctly with either of them. But you can't write code that works with both `Either` and `?` because you can't tell when it's going to result in nesting `?` at which point everything goes wrong.
&gt; This myth rests on the (false) premise that different types of applications require fundamentally distinct capabilities that cannot be provided in a single effect monad. &gt; To dispel it, I will now present the ultimate IO monad, which can provide any capability required by any application whatsoever. All this shows is that it's *possible* to build an ecosystem on a single IO monad. Doesn't mean it's the best way to build one, nor that the resulting implementation will be as good on all fronts, e.g. performance. The post talks about how some programs might require e.g. cancellation, and demonstrates that it's possible to encode cancellation on top of a monad that doesn't offer it. But it hasn't shown that that encoding is efficient. It's entirely plausible that e.g. one IO monad (side note: can we call it something else? The name IO is an accident of Haskell history and not an accurate description of what it means) could offer an efficient implementation of cancellation for programs that frequently need to cancel asynchronous tasks, while another could offer a more efficient implementation of non-cancellable asynchronous tasks for programs that never need to cancel asynchronous tasks. Indeed I'd be extremely surprised if there weren't the kind of implementation tradeoffs that lead to such differences.
&gt; By all means, divvy up the effects of your program into fine-grained bundles, but also, recognize that everything will be translated into the most powerful effect monad required by your application, since your entire program must be a single value. Sure, but for different applications this could be different. E.g. maybe application A accesses the network and application B doesn't, so maybe application A has to be represented as a `GeneralIO` whereas application B can be represented as a `ConsoleIO`. Now of course I can inject my `ConsoleIO` value that represents application B into a `GeneralIO`, but I don't think it's desirable to do so (if anything the opposite; I'd rather have a visible distinction between the two that goes all the way up, so that e.g. the published package for application B would be visibly in a smaller sublanguage than the published package for application A is).
My annoyance with it has been the laissez-faire attitude of dumping files into root folders. Take a slightly larger project, and your root folder is filled to the brim with random files. I liked having all project config files in one place, instead of having them spread around across multiple directories, including the root folder. Slightly related to this, SBT still dumping files into $HOME is a complete disgrace. (There are other offenders, too. I'm close to hooking into the `write` syscall and just `SIGKILL`ing offending processes.) So glad that I don't have to deal with stuff like this anymore.
Not play, but i've built a few sites running on akka, slick for DB and twirl as a template engine. It's pretty solid.
cats (nonetheless matryoshka) is banned on project, so I have to reinvent nongeneric wheel ocasionally, and not always I know what is good name (would like to avoid creating something thats completely differntly named in FP, so one day when bosses decide its time for cats it doesnt mandate that of difficult rewrite) thanks though.
&gt; Indeed I'd be extremely surprised if there weren't the kind of implementation tradeoffs that lead to such differences. Agreed, in fact I would go as far to say that such a statement is most likely to be wrong. I posed questions in the previous thread, and none have been directly answered (he only mentions that its possible to encode cancellation using this IO monad, but doesn't demonstrate an actual example) Not saying that this is impossible with the proposed IO monad (using the cancellation as an example), its just likely to either expose an impractical API or have really bad performance (or both). I mean this comment is already implying this https://github.com/scalaz/scalaz/issues/1399#issuecomment-315550397 In other words, there are tradeoffs
Anything that can be done in templating engines can be modelled in Scala, i.e. the concept of "mixins" or "inline bodys" or "implicits" can all be represented in Scala with basic Scala language constructs 
[removed]
Even in this case, you're still going to need `ConsoleIO ~&gt; IO` in order to actually run the computation. More likely, the program can be generic in `F[_]` and just require `MonadConsoleIO`, and have an instance available for `IO`. This way you can build `ConsoleIO` or `IO` or whatever else you like. I do wish people did more "program composition" instead of the monoliths we have today...
&gt; All this shows is that it's possible to build an ecosystem on a single IO monad. Doesn't mean it's the best way to build one, nor that the resulting implementation will be as good on all fronts, e.g. performance. Indeed, I explicitly state performance is the _only_ reason to specialize an `IO` monad. `IO` is a great name IMO because it denotes program input / output with the external world. If a program does not interact with the external world, but is pure computation, it doesn't strictly speaking need an `IO` monad. &gt; while another could offer a more efficient implementation of non-cancellable asynchronous tasks for programs that never need to cancel asynchronous tasks. Indeed I'd be extremely surprised if there weren't the kind of implementation tradeoffs that lead to such differences. That's actually not true. Expanding the cases an interpreter has to handle has no effect on performance. The only thing you can argue is that there are an unbounded number of different low-level features requiring specialization that different programs will utilize. IMO that's not a plausible position to take, but I'd love to hear an argument for it.
Fine-grained interruption is the only effect I can think of which has some small effect on performance (but even it is negligible when optimized). Other features are added as new cases to the interpreter, which has no effect on performance.
Sure, I've been there. The actual fixed-point trick at the heart of matryoshka is pretty simple though, writing it yourself and your own `cata` is a good exercise and well worth doing IMO. (I mean, it can be as simple as this: https://github.com/m50d/tierney/blob/85cabeb0592fa14fc5adeaf861173dbcb8b50ca3/core/src/main/scala/tierney/core/package.scala#L10 )
I'm equally worried. I find SBT awful, avoid anything that relies on it, and firmly expect it to undergo the same popularity trajectory as e.g. the cake pattern did. But ultimately, the only thing those of us who want other build tools to be supported can do is step up and do the work (or I guess pay people to or something). A lot of the time a project or tool works fine with maven and it's simply a case of adding to the documentation or making a minor change; I've sent a couple of PRs along those lines and they've always been welcome.
&gt; Even in this case, you're still going to need ConsoleIO ~&gt; IO in order to actually run the computation Once we're talking about how to "actually run", our program isn't a value any more, and in a sense we're not talking about a programming language any more (rather we're talking about implementation) - in JVM Scala there will ultimately be a `main` method invoked, but from a pure functional perspective it's hard to regard `main` as something reasonable, so we probably regard `SafeApp` or equivalent as being part of the language implementation (and in principle there's no reason other implementations of Scala couldn't have the equivalent of `SafeApp` built into the runtime/build tool). In that sense, there's no reason the language couldn't offer other entry points with smaller effect types; at an implementation level we could have `SafeNonNetworkApp` or some such, and at a semantic level the language spec could just say that `run` is allowed to return one of various effect types. (And at implementation time `SafeNonNetworkApp` would probably just use a `ConsoleIO ~&gt; IO` internally). &gt; I do wish people did more "program composition" instead of the monoliths we have today... Yeah. I think the problem is that the small-program-composition style exists already in the unix tradition, but only in a very unstructured way, but then all of the efforts at a structured replacement make too strong a "closed world" assumption so there's no reasonable migration path. If everything you're doing is inside a spark (say) shell then you have a bunch of nice structured combinators which all compose, but then as soon as you want to integrate a non-JVM component into your pipeline then you're down to parsing arrays of strings and then printing for output. So people gravitate towards having as few of those interfaces as possible (I mean fundamentally, if two Scala functions want to call each other they can do so in a nice structured way, whereas if two Scala programs want to call each other it's a mess) and trying to make these environments as big as possible and do all their composition within them, but that only makes it even harder to interface with the outside world.
&gt; IO is a great name IMO because it denotes program input / output with the external world. If a program does not interact with the external world, but is pure computation, it doesn't strictly speaking need an IO monad. But a program that only wants to do input/output doesn't need anything like the full power of today's "IO" monads - most of the gnarly bits of the choice of an "IO" monad aren't about input/output at all, they're about async/parallelism/..., whereas it's perfectly reasonable to have a program that performs I/O but never does any kind of forking. &gt; The only thing you can argue is that there are an unbounded number of different low-level features requiring specialization that different programs will utilize. IMO that's not a plausible position to take, but I'd love to hear an argument for it. I think there will continue to be new ideas. We've talked about cancellation, pre-emption, prioritization of separate thread pools. What low-level integration points might a program need to run as efficiently as possible on an asymmetric multiprocessing system like we're starting to see used in phones (big.LITTLE)? I just don't feel we're at a point where we can say we've solved efficient task scheduling and only need to implement the well-known best solution, and until we reach that point there might be an unbounded number of approaches that work better or worse in different fields.
I don't like the notion of "templating"; for websites in Scala I tend to use Wicket (there's a wicket-scala library that adds some useful utilities for combining them), which I find feels a lot more compositional to work with.
I guess that Scala can help R with automation tools and java Swing GUIs for exploratory analysis. For instance, Matlab GUIs are built with Java Swing.
Tl;dr guy who used to work at lightbend uses lightbend stack, speaks well of it. kek.
Wow! Actually I have to say, I haven't looked at http://scala-lang.org/ in a while and really love the improvements already there. The IDEs section, the new flat logo, the Scaladex part, it's all looking really nice!
Yeah exactly. One thing for sure is that fiber model (which is what this `IO` seems to be based on) is great for backend style web servers but its completely the wrong model for native desktop apps/games/UI's where you basically need to have different dedicated "threads" because you have to deal with resources allocation and prioritisation (i.e. we need to buffer as much as possible off the UI thread so we don't make our GUI apps laggy, and in an async context). This problem is hard to solve in Haskell. When I dabbled in Haskell some time ago it wasn't really possible with their IO type, now it is possible by using machines (i.e. https://hackage.haskell.org/package/machines thanks to /u/aloiscochard for pointing this out here https://www.reddit.com/r/scala/comments/6vb9mr/why_im_excited_about_scalaz_8/dm0c3a2/) however after studying the slides it basically breaks modularity/api. i.e. with `Future` and `ExecutionContext`, on any `map`/`flatMap` operation, you can specify a different `ExecutionContext` (lets say the UI thread). In doing so you aren't actually modifying any of the business logic of your async computation (the `map`s and `flatMap`s that run on your futures), you are just adding an explicit parameter in one of your steps saying "I want to execute this somewhere else" where "else" is a different threadpool or executor of some sought. In Haskell (or in Scala) if you want to use machines, then you have to split out your logic completely and you aren't working in the IO monad anymore. You now have to work with `Plan` and `Machine` types. Saying that the proposed `IO` is the best general purpose solution is the equivalent of selling snake oil in software industry. 
The multi-project build syntax is identical to the bare project syntax, with two exceptions. 1. You must now chain your build functions together with dots 2. defining settings must be done within `.settings(...)`. Besides those two things, they are the same. I prefer the new way, since it's very clear that all the settings apply to the thing that is connected by the dots. Before you could just have everything anywhere in the build file willy nilly and you're just supposed to know that it happens to pertain to the build definition for the project in `./src/`
Thanks, This is awesome work from * [Javier de Silóniz Sandino](https://github.com/jdesiloniz) from [47deg](https://www.47deg.com/) * [Heather Miller](https://github.com/heathermiller) from [Scala Center](https://scala.epfl.ch/) * [Travis Lee](https://github.com/travissarles) for [Scala Center](https://scala.epfl.ch/)
I'm a fairly big Scala fan, and yet I agree with this answer at a large extent. Scala is great when you know exactly what you want to do with your data, and you are building large scale applications with data pipelines, using data for application behaviour etc. But when it comes to the data analysis part, it is best to stick with R/Python-with-pandas and plain old SQL.
Where possible avoid them. In cases where God forsake you, you can use mapping/for comprehension within mapping/for comprehension, try effect monad, monad transformer or free monad or write your own wrapper that will apply all the transformations the way you like. Cause in the end the biggest pita are composition and transformation of those types. You won't have issues with just passing them from one place to another?
There is little reason to use `IO[Try` because `IO` already knows how to trap exceptions (see the many combinators on `IO`). So assuming you have `List[IO[A]]` you can say `.sequence` to get `IO[List[A]]` which is probably what you want. If you do really want `List[IO[Try[Int]]]` you can sequence that to get `IO[List[Try[Int]]`. It's just weird. We don't really use `Try` in FP-style Scala.
I wrote the un-recommendation of bare style in 2014 in here (https://github.com/sbt/website/pull/68). My thinking then, and now, was that having three flavors to express a build (multi-project style, bare style, and `Build.scala`) would be harmful to the community. Especially for new or infrequent users, people have hard time mentally translating one style from the other when they come across different projects or snippets from Stackoverflow. Auto plugin, which orders your settings automatically, and multi-project style introduced in 0.13 I think reduces the confusion, compared to the earlier days without them. 
[jawn](https://github.com/non/jawn) can parse around this. It's a bit hacky since the feature is (mostly) intended for asynchronous parsing of stream-like data, but it works equally well for your use-case. I use it together with my favourite JSON-library, [circe](https://circe.github.io/circe/). Here's an example that parses two lines of text each containing one single JSON object: https://scastie.scala-lang.org/EVlhRwpORsq0U8G8ASSHvw
`Try` in this case is superfluous as an effect, so I would get rid of that first. That said, you could be more specific about what you want to try to do. The idiomatic pattern you're referring to in are most likely Monad Transformers. However you don't typically use Monad Transformers with IO, instead you typically try to make IO the outermost effect. You'd want to use the `sequence` method on `List`'s `Traverse` instance to turn your `List[IO[B]` into a `IO[List[B]]` and work inside the IO effect.
you mean Activator, are you nuts ?
&gt; From late 2011 to early 2014 I was a lead developer at Nurun Toronto, a digital agency focused on e-commerce solutions. He went to work at Lightbend afterwards.
Magic `val`s are supposed to be *less* confusing?!
Well, of course it does. What else would it pertain to? Meanwhile, there is some kind of magic happening in the new style. [The manual talks about](http://www.scala-sbt.org/1.x/docs/Basic-Def.html) how a build definition is laid out, but there is that mysterious `lazy val root` in there. It seems to aggregate all information about the root subproject, but how does SBT know to evaluate that specific `lazy val` to get the build definition? Because of its name? Because of its type? Because it involves `project in`? What if I write `def root` instead? What about [SBT's own `build.sbt`](https://github.com/sbt/sbt/blob/1.x/build.sbt), which calls it something else? There is zero explanation AFAICT. It's magic. It is the opposite of “very clear”.
If you do something other than lazy val, sbt will tell you that you have to change it. Any project definition is automatically picked up, I'm positive sbt documents that somewhere. It could definitely go with some better documentation, but it still makes more sense than the single project build. Also makes it easy to add more sub projects later on. 
&gt;If you do something other than lazy val, sbt will tell you that you have to change it. What if I don't assign it at all? What if I write just `(project in …).settings(…)` with no `def` or `val`? &gt;Any project definition is automatically picked up, I'm positive sbt documents that somewhere. So, yes, magic. In Java/Scala, classes/traits/objects/interfaces mean nothing (they aren't even loaded) unless they are explicitly referenced somewhere. Similarly, a Scala `lazy val` does nothing until it is actually evaluated, and may still not do anything interesting until some other part of the program uses it somehow. If I were to type `lazy val root = …` into a block and never mention it again, it would never be evaluated or used in any way. SBT, apparently, completely inverts what `lazy val` means, and in fact *eagerly* evaluates them (if they are a “project definition”, whatever that actually means). Besides the syntactic noise of writing `lazy val` in a build definition, this is also horribly counterintuitive. Frankly, if build definitions are supposed to be purely functional (this being Scala, I should hope they are), then they should work the same way that functional programming languages usually work: the build is defined by whatever the last expression in `build.sbt` evaluates to, and that last expression aggregates all information about the build. *That* would make sense. In the bare style, instead of the last expression defining the build, the whole file is a list of expressions, each of which contributes to the definition of the build. That, too, makes sense. Magic `lazy val`s don't. &gt;it still makes more sense than the single project build. Also makes it easy to add more sub projects later on. You could deal with that by giving them their own `build.sbt` files, then referencing them by path from the root. The root then defines common settings for all of them. That's what Maven does, and it works nicely.
I came from R and work mostly in Scala now. Its great for putting stuff in production or working with huge data (with Spark of course). Jar files are fun. Interactive and exploratory? Nope. But my current job requires stuff to go to production environments and R just isnt gonna fit for that here. Ive done it in more casual environments though. Also Scala is a gateway drug for functional programming if you find that interesting. R has some functional stuff too (apply functions are cool) but not nearly the same. I like strong and ststic typing, and Scalas general attitude. It feels very 'pro' compared to R. Also, I guess more importantly, I work with much bigger data now and R would either have to have a major package helping it out in a narrow scope or it would just not be an option.
Im kind of thankful that I went to Scala with zero Java experience. Id used python and R mostly and stepping into Scala was very fun. Java would have made me recoil.
I'm not saying it isn't magic. I'm just saying that taking the time to read the docs would help you. In the end, I don't really like sbt, but I like it more than anything else we have available. I don't like maven because you can't do enough with the builds, and it has its own dsl and magic properties which you have to go read the docs for anyway. 
&gt; So assuming you have `List[IO[A]] ` you can say `.sequence` to get `IO[List[A]]` which is probably what you want. And if you got there via `xs.map(f)` you can do `xs.traverse(f)` instead of `xs.map(f).sequence` to get there in one step. 
I find defining aliases can be helpful: type MyAction[A] = WriterT[EitherT[Future, MyError, ?], MyLog, A] Others have mentioned monad transformers if that's what you're looking for.
Indeed, you can do some stuff with `machines` in that regards but the reason I was really mentioned it originally is that it's a elegant way to get back-pressure. OTOH, I don't think it's possible to achieve anything like what you can do with tuning `ExecutionContext` in Haskell... It's quite a limit for the usecases you mention, luckily not much for web-services but this debatable. It's interesting to think about how such feature could be supported in a non-strit language based on a IO monad like this... a design space to explore. Clearly, we can do better!
It looks like you're already using Spark. Spark by default parses the JSONL (JSON lines) format which you have there anyway. So a spark.read.json(lnd_hdfs) might just work in your case.
How do you deal with Future effects encapsulated in an IO since you can run the IO asynchronously? 
I'll try to ask [one more](https://www.reddit.com/r/scala/comments/6xzz3x/fortnightly_scala_ask_anything_and_discussion/dn17ict/) time: First of all, I was wondering why the bottom types (like `Null` or `Nothing`) doesn't have all methods above. Here is the [answer](https://softwareengineering.stackexchange.com/a/195807/191603). It says that **Subtyping** and **Inheritance** not the same. Subtypes don't inherit all methods above. So my question is - isn't it dangerous? I can put an instance of subtype without method in supertype with a method. It seems that Subtyping without Inheritance is dangerous. val user: User = null user.getName() On the other hand, Wikipedia keeps [saying](https://en.wikipedia.org/wiki/Subtyping) that Subtyping is safe. &gt; If S is a subtype of T, the subtyping relation is often written S &lt;: T, to mean that any term of type S can be safely used in a context where a term of type T is expected.
**Subtyping** In programming language theory, subtyping (also subtype polymorphism or inclusion polymorphism) is a form of type polymorphism in which a subtype is a datatype that is related to another datatype (the supertype) by some notion of substitutability, meaning that program elements, typically subroutines or functions, written to operate on elements of the supertype can also operate on elements of the subtype. If S is a subtype of T, the subtyping relation is often written S &lt;: T, to mean that any term of type S can be safely used in a context where a term of type T is expected. The precise semantics of subtyping crucially depends on the particulars of what "safely used in a context where" means in a given programming language. The type system of a programming language essentially defines its own subtyping relation, which may well be trivial. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Yeah thanks I am trying that as well its just would like to have 2 ways to do it. Because if the json is too nested I find it difficult to parse with read json. But will try that as well.
Lookup scalaz lenses
Well in the case of `Null`, you're absolutely right, it is dangerous and that is why except for Java compatability or performance reasons, people don't use Null's at all when programming in Scala. `Nothing`, on the other hand is perfectly safe because there are no values that inhabit `Nothing`, so since there are no instances, there's no danger. The only thing you can do if you are required to provide an instance of `Nothing` is to throw exception, and if you're throwing exception, you already know that what you're doing is dangerous.
Ah yeah, I forgot that Scala has it's own equality checking
You could use http://www.scala-lang.org/api/2.12.3/scala/collection/immutable/TreeMap.html, and when defining Ordering for your type, have it be ordered on obj.key.
Hey that reminds me, I made a little library for almost this purpose, called [MultiIndex](https://github.com/joshlemer/MultiIndex). The difference here is that instead of a `Set[A]` of elements, I store a `MultiSet[A]` of elements. And can also take multiple functions (up to 4 I believe). Feel free to take inspiration from it :-)
Why not this, and use a regular map? If you have the key, you can get the hash string, and if you have the instance, you can just call hashCode. case class ThingToStore(fieldToStoreBy: Int, otherField:String){ override val hashCode = ThingToStore.hash(this) } object ThingToStore{ def hashAlgo(i: Int) = i.hashCode def hash(t: ThingToStore): String = hashAlgo(t.fieldToStore) def hash(i: Int): String = hashAlgo(i) } val map: Map[String, ThingToStore] = Map(anInstance.hashCode, anInstance) 
I was trying to use |+| on my list to get List[IO[B]] =&gt; IO[B], but B isn't a monoid. I have written a append method for B, so it should be a monoid. Trying to figure out how typeclasses work now. 
If no solution exists, probably easiest to just wrap a `Map`? Starting from something like this: class KeyedSet[V,K](val underlying: Map[K,V], toKey: V =&gt; K) { def +(v: V) = new KeyedSet(underlying + (toKey(v) -&gt; v), toKey) def get(key: K) = underlying.get(key) override def toString() = underlying.values.mkString("KeyedSet(", ", ", ")") } object KeyedSet { def apply[V,K](vs: V*)(toKey: V =&gt; K) = new KeyedSet(Map(vs.map(v =&gt; (toKey(v) -&gt; v)): _*), toKey) } 
&gt; First of all, I was wondering why the bottom types (like Null or Nothing) doesn't have all methods above. &gt; It seems that Subtyping without Inheritance is dangerous. Yeah that's about it really. Using `null`s or throwing exceptions are dangerous. Still very useful sometimes, or at least unavoidable, but if you use them be careful! Wild Pokemon live in the tall grass!
It's been integrated into slick 3.2 You could've read the release notes (first bullet) or ask in the slick gitter, this subreddit is a bit of a weird place
+1 to "runtimes" like `SafeConsoleApp`. &gt; I think the problem is that the small-program-composition style exists already in the unix tradition, but only in a very unstructured way, but then all of the efforts at a structured replacement make too strong a "closed world" assumption so there's no reasonable migration path. Do you know if anyone has explored a "typed OS" such that programs can communicate with each other as easily and safely as functions? In that world, the signature of main could be anything you want, maybe even explicitly coinductive.
I don't know; I'd be interested to hear of such a thing myself. I'm excited about what e.g. Sandstorm is doing (protobuf-style interfaces between application-like components), but of course that's a tiny baby step in terms of how much is expressible in the interface.
Well, you can also loop forever. The point is that in well-behaved Scala you can never form an instance of `Nothing`.
Hm. This article is from early 2014. _A lot_ has happened since then. I think people should prefer to read something more recent instead of this. E.g. In the article, he recommends everyone to follow Martin's "Scala levels guide." from 2011 (http://www.scala-lang.org/old/node/8610). Wow, that Scala guide is so outdated now that some of the points on the list are actually _strongly discouraged_ these days (e.g. "cake pattern"). Perhaps Martin should update it and cross out things like "cake pattern"; or just remove the article altogether so newcomers don't get confused..?
I enjoyed this talk very much. I have done half of the mistakes mentioned there and have seen people doing the other half. It's good to see some slight counter-FP position as well, so we don't end up in echo chamber.
That was a very good talk. I have run into some of the problems in the past, you definitely need to define early how far down the functional rabbit hole your code will be.
`writeData` is not in lexical scope where it's being used, so what you're expecting here won't work. One option would be to make it a method on `Channel` and then use the loaner pattern to allow something like `withChannel { c =&gt; c.writeData(...) }` or even `withChannel { implicit c =&gt; writeData(...) }` but you can't make `c` invisible this way [yet … implicit function types in future Scala will allow this] and, more importantly you can't prevent it from being leaked using this approach. A more principled way to do this is to hide the channel in a monad. This is how doobie and Slick pass database connections around without providing a direct reference that could be leaked. The free monad is one way to do this that I have used with some success. I realize it's a lot to bite off. [This video](https://www.youtube.com/watch?v=M5MF6M7FHPo) is a bit out of date but it introduces the mechanics of this approach.
You cannot do this with plain implicits, though Dotty would allow it, as tpolecat has noted. Depending on how much you *desperately want* that syntax, you can make it work with a macro. Here's the strategy: - Define `writeData` to be a static method that takes an implicit `Foo`, where `Foo` is the actual thing that keeps track of the dependency - Define a globally available `@compileTimeOnly implicit dummyFoo: Foo = ???` in `Foo`'s companion `object`. You can give the annotation an appropriate error message people will see if they use it outside the desired scope without the proper implicit available - Define `withChannel` to be a macro. Using the macro, convert the `withChannel{expr}` into `withChannelImpl{c =&gt; expr}`, and walk the `expr` to replace any usages of `dummyFoo` with `c` - Profit! This sounds a bit roundabout, but it works. uTest uses this technique for it's `TestPath` implicit ([dummy implicit](https://github.com/lihaoyi/utest/blob/93bf9002d1ffc35441295b4ac667f38d64aaf5b9/utest/shared/src/main/scala/utest/framework/Model.scala#L12), [macro replacer](https://github.com/lihaoyi/utest/blob/93bf9002d1ffc35441295b4ac667f38d64aaf5b9/utest/shared/src/main/scala/utest/Tests.scala#L63-L71)) and Scala.Rx uses it for it's `Ctx` values ([dummy implicit](https://github.com/lihaoyi/scala.rx/blob/cf15b7095407c3a6fa656073b56dafb1362a572f/scalarx/shared/src/main/scala/rx/Core.scala#L304-L312) [macro replacer](https://github.com/lihaoyi/scala.rx/blob/801757fee9f3d660519f0066374ede0712b00795/scalarx/shared/src/main/scala/rx/opmacros/Utils.scala#L32)) This seems a bit crazy, but it works. If you want that syntax badly enough, this can tide you over the next few years until "implicit function types" make their way into mainline Scala
Scala developers don't like Java EE because they think that it isn't the scala way.
Probably because it's garbage
When was the last time that you used Java EE and what application server was it? EDIT This is the problem with the Scala community. This is why Scala will remain niche forever.
Of course there can be only one IO Monad. Its name is `scala.Function0`.
&gt; being available 24/7 to spoon feed your documentation to random dudes on the internet who feel like you owe them something, and don’t know how to use a browser. If you don’t decorate your advice with salutations and graciously thanking them for trying out ENSIME, be prepared to be called arrogant, lazy and rude. My advice is to shut these people out of the project and not to tolerate them, but you will be ostracised by other groups in the scala ecosystem who wish to encourage such privileged behaviour. You will need thick skin to deal with both of these types of people. I don't know if its just me, but this comes across as passive aggressive/grandstanding. What is the point of mentioning something like this?
Maintaining a project other developers use is like running a daycare centre. People are rude, arrogant, expect you to spoonfeed them, and will then turn around and shit all over you because your hobby project doesn't perfectly solve their completely different use case. And then you get the idiots trying to play politics and take over your own project. You start something as a hobby project to scratch an itch, and then all of the sudden people think you owe them something. I can definitely understand where he's coming from. 
People are really shitty to each other in open-source communities sometimes, it gets to some people more than others.
ETL Framework
Sure thing although I believe this differs from project to project. There is a saying that the customer is always correct, and it doesn't mean that the customer is literally always correct. What it does mean, however, is that if you find a **lot** of people complaining (or being rude or an idiot) usually it means that something is wrong. If you get "idiots" every now and then, this is expected, but if it happens constantly (which is the implication since it's brought up in such a way on the website) then something is usually amiss Either that, or people in Ensime are very vocal. In any case it doesn't come across well
You've never worked I hospitality I take it. The Majority of people are rude under normal scenarios. 
2 years ago and it was JBoss. JavaEE is a shit standard that promotes shit design patterns, hiding objects behind beans, factories and managers so hard you lose yourself in the mud your application becomes. Scala has gotten significantly more popular over time but if Scala served as a gateway to keep all the deluded JavaEE programmers out, it would already be serving its purpose
What version of JBoss was it? Yes, if you have a less abstract programming language, you have to use patterns to provide some form of abstraction. It is not about deluded programmers. Imagine a large company that has a lot of projects in Java on Java EE. If they could have a smooth migration like this: * imperative Java on Java EE with gradle * imperative Scala on Java EE with sbt * imperative Scala on Play/Lagom with sbt * functional Scala on Play/Lagom with sbt It would be great for them and they could have an incremental process. Instead, now they have to rewrite a project if they want to use Scala.
This library looks great -- it's nice that it's smaller than ScalaTest while providing the nice parts like the assert macro, intercept, etc. Small suggestion: I wish that Scala test libraries didn't come with DSLs that need you to essentially learn a new mini-language before understanding the tests. Someone with a basic understanding of Scala can't reliably interpret what each token does in this: 'endOfCommand{ * - assertComplete("{}") * - assertComplete("foo.bar") * - assertComplete("foo.bar // line comment") * - assertComplete("foo.bar /* block comment */") * - assertComplete("va va") // postfix * - assertComplete("") // ... } Questions your coworkers will ask: How is the 'endOfCommand symbol able to take a function parameter? Is that a bulleted list inside (does Scala support Markdown literals in addition to XML literals now?) or some other magic? Is there subtraction on assert result codes going on? How do you find the definition of any of those symbols? Why not use alphanumeric identifiers that are more likely to be found by your company's code search tools? If you work at a company with 100+ developers you can't expect people outside a two-pizza team to understand DSL-heavy code, to easily contribute to it when projects intersect, or to help debug it in case of a production issue. Obviously people can learn it, but there's no need to create that friction when more obvious language constructs would do just fine. Scala is great, but it's hard to see the value of Scala's test libraries over functions + assertions in libraries like JUnit (plus maybe "import org.scalatest.junit.AssertionsForJUnit._").
It's not just you. If you read his Twitter or his comments in some threads on GitHub you'll see that he is usually like that. Also read the point about a code of conduct in the linked document. And in the end he says that "applicants need not apply" – so he apparently just wrote this to vent a bit and to express his views on things.
I actually agree with this entirely. I've stripped down ScalaTest's 8 mini-languages and Specs2's 2 mini-languages to uTest's 1 mini-language, but it would be nice to strip it down to zero mini-languages. Empirically, I've seen plenty of more-than-two-pizza teams understand and be able to work with ScalaTest, so it's not the end of the world. Baby steps in the right direction. I've played around with various possible syntaxes, and haven't settled on one "normal" syntax I'm happy with. The closest I've found is test("endOfCommand"){ test{ assertComplete("{}") } test{ assertComplete("foo.bar") } test{ assertComplete("foo.bar // line comment") } test{ assertComplete("foo.bar /* block comment */") } test{ assertComplete("va va") } // postfix test{ assertComplete("") } } This is a topic worth discussing, and if people seem to like it we could easily switch (and leave the old syntax present-but-deprecated, eventually removing it entirely)