Like maybe using union types for the left hand side of an either for all the exceptions? Not that different from what we have now except that we can see all the exceptiona being thrown more clearly.
Well, it's one of the points of implicit function types. The better encoding of checked exceptions has been found already.
&gt; The pessimist in me believes it was simply that Scala didn't care about principles until very recently. Well, that's a very bold claim to make. Never mind that Scala was developed by PL researchers (researchers don't care about principles?), was the subject of plenty of scientific papers, and is more grounded in theory than the vast majority of other popular languages, with the only notable exception of languages in the ML family. I'd understand better if you meant that Scala did not care about the _same_ principles as you. &gt; it encourages a usage and pattern that are frankly, only a thing because of a lack of support for better features If you're talking about union types, then no, they are not a thing because of a lack of support for better features. The main reason they're in Dotty is that they make the type system more uniform, as they are the dual to type intersections.
Both are useful for different reason. * Whenever I call Java code, I use `Try` (for the reasons stated before, people in Java use exceptions for business logic flow in error handling) * Whenever I use Scala code, I use `Either`, because typical Scala code either doesn't throw exceptions, or when it does, its really in exceptional circumstances.
Investigating to use the GraalVM at work, but we may not be able to use the enterprise edition (which contains the profiler) due to licensing restrictions (same issue as using OracleJDK in Docker) Looks promising, but I think it also depends on the code. If you have a look at https://twitter.com/lukasz_bialy/status/989091065033625606, Graal was actually giving worse runtime performance until the profiling feature was provided (which is part of the Graal enterprise edition)
To answer it: The introduction of union types was one of the things I considered when making `Either` right-biased. The name `Either` could have been adjusted/improved, but that wouldn't have been practical given the state of development in Scala at that time. The idea was to nudge it slightly toward more obviously distinct use-cases that are easier for users to pick: - If you want to retain a distinction between the two cases, even if the types are the same (e. g. `Either[Int,Int]` vs. `Int | Int`), use `Either`. (`Either` is a tagged union.) - If the distinction is not important, or it is guaranteed that the types cannot collapse (e. g. when working within a sealed hierarchy, use union types. (Union types are untagged.) Please bear in mind that this was done assuming a certain level of language quality. At this point, union types are barely implemented and null handling is completely unimplemented. They also seem to have chosen a non-working approach to handle `null`s coming from Java. 
&gt; I'd have to defer to someone else to answer exactly why it turned out this way. The name wasn't changed when the right-bias was added, because at that time I was just cutting down every non-essential change to at least some minimal changes merged without causing another year of bike-shedding. If you would go down that path, the question whether you can have some practical approach to treat `Option` as some specialization of `Either` comes up really really fast. I tried to avoid that question, because I didn't feel that there weren't enough interested and qualified people to discuss the options. &gt; The pessimist in me believes it was simply that Scala didn't care about principles until very recently. Too little too late. Not sure anything has changed recently. The only substantial improvement in quality was when Paul was in charge of scala/scala. Before and after it's largely EPFL throwing stuff at the wall, seeing what sticks and expecting other people will clean up the failed experiments for them. One thing that annoyed me during my stint in scala/scala was that their research on prior art ("stuff not invented here") was pretty weak, leading to them repeatedly throwing the same, known-to-be-broken stuff at the wall.
&gt;Well using that logic, Option also isn't a monad (even though everyone uses it as one) because Option.apply(null) gives None instead of Some(null) Bingo. I agree.
Wow! `--pgo` - should be a killer feature of EE!
Right, but my point here is that `Either` is also not a Monad if you use the same line of reasoning, which is contrary to what it seems you seem to be arguing. Or to put it differently, if you are going to claim that something is a Monad but ignoring certain things (such as exceptions), then you have to use this same claim for all Scala types, in which case `Try`/`Option` and `Either` are as "monadic" as eachother as long as you don't account for `Exception`'s or `null`
&gt; We should be deferring to MonadError or an Exception monad as a better way of handling error DSL's and Throwables. Can you explain what you have in mind, and why it's better than using `Either` directly for error handling?
What an interesting idea! A combinator `def check[A, B, E &lt; Throwable](f: A=&gt;B) = a =&gt; try { Right(f(a)) } catch { case e: E =&gt; Left(e) }` sounds useful, especially as an extension method so you can just specify the E.
`Some.apply` is your `return`. `Option.apply` is a convenience function. You can do without `Option.apply`, but you cannot do without `Some.apply`.
Nah, I get a type mismatch if I do that. Thanks though!‚Äô
Let's say that you can't change the definition of `type SuitableFood` from what I suggested. As a working programmer, how would you solve that type error?
It seems to me that if enclosed values are immutable, the fact that they are in there shouldn't have to be obvious for most cases (referential transparency and all that). Maybe if the closure is intended to be serialisable?
At the unit test level you shouldn't worry about testing the functionality of libraries. Assume that they implement their APIs as documented. If the concern is that you're using the cache as you intend then you can mock out the underlying resource and verify that multiple calls for the same key only hit that mock once. Outside of unit tests, I'd recommend capturing metrics on the cache such as counts of hits, misses, and how often the key is found/not found in the underlying resource. Those could be checked explicitly in integration tests but they could also be monitored when the code is in production to verify to cache's parameters continue to meet production needs and that upstream callers aren't making fruitless look ups, if avoidable.
Technically you're right, but it's good to know what the dependencies of your code are when reading it later. Say I closed over a value so I can pass a function into a `map` or something, and six months later I'm changing this code, I'd very much like the closure to be near the map operation so I can see that the function I passed to `map` depends on whatever I closed over, for whatever reason. But it is very subjective, and circumstantial. To dig into it a bit further, serialization is a great example. Let's say I have a function that receives a value and a color, and renders some HTML. It doesn't care how it gets the value, or the color. It's also a super contrived example, but let's run with it ;) If I'm planning to call this function over a list, it still doesn't really care how it gets the value, but _I'm_ starting to care about how I pass things around. For this contrived example, where my `renderThing` function already exists, I probably wouldn't use a closure manually, I'd probably just use partial application: ``` things.foreach(renderThing(_, Orange)) ``` But if `renderThing` didn't already exist, and I didn't know what color I'd need until just now, maybe I'd use a closure: def renderThisList(list: List[Thing], color: Color): Unit = { val toHtml = (thing: Thing) =&gt; s"""&lt;li style="color: ${color.value}"&gt; ${thing.toString()} &lt;/li&gt;""" println(s""" &lt;ul&gt; ${list.map(toHtml).mkString("\n")} &lt;/ul&gt; """) } Granted, this is a _poor_ example. And terribly ugly. But however long `renderThisList` is, I'd want this `toHtml` function to be defined pretty near where it's used, so that I'd be able to see at a glance that it's where my `color` went, and how it gets into the output.
 Thanks for your reply! So let me clarify that I'm not trying to test that Play's Async Caching function is actually working. I do want to test to see if I'm actually _using_ the cache correctly. As you mention: &gt; If the concern is that you're using the cache as you intend then you can mock out the underlying resource and verify that multiple calls for the same key only hit that mock once. What I'm not sure about now is how I would go about doing that. Here's what I have so far in my tests: class MyServiceSpec extends FlatSpec with MockitoSugar { // If someone could weigh in on best practices when using Implicits.global, // would love some guidance on proper usage/convention, etc. import scala.concurrent.ExecutionContext.Implicits.global behavior of "MyService" it should "use the cache" in { implicit lazy val app: Application = new GuiceApplicationBuilder.build() val repo1Mock = mock[MongoRepo1] val repo2Mock = mock[MongoRepo2] val service = new MyService(app.injector.instanceOf[AsyncCacheApi], repo1Mock, repo2Mock) val testInput = "test" service.findAll("test") service.findAll("test") service.findAll("test") service.findAll("test") service.findAll("test") service.findAll("test") } Running this misses the cache every time.
I don‚Äôt know. I am new to Scala. I tried to plug in Either as method parameter and I get Scala.util.Either.type doesn‚Äôt take parameters. ü§∑‚Äç‚ôÇÔ∏è
As a newcomer to the language, you should keep some open documentation and reference tabs handy, so you can quickly look up how to do unfamiliar things. For example, the [standard library documentation for Either](https://www.scala-lang.org/api/2.12.6/scala/util/Either.html) has several examples of how to create Eithers.
No, I looked at that already. It didn‚Äôt help me. 
No, I looked at that already. It didn‚Äôt help me. 
Just curious, why is there so much discussion regarding json libraries? I see a new post here every couple days. Is there really that much difference between the various libraries in terms of performance and usability? And is json parsing really the part of most applications that needs the most optimization? Not complaining, just curious.
This is awesome. I'm glad macros have a bright future in Scala 3. For those of us worried not about the future but about the *present* of macros, here is a quick overview of the current situation. Please correct me if I'm wrong on any of my points, I have no inside knowledge. * v1 (aka "scala.reflect"), even though experimental, is currently the only working solution we have. Don't expect any new features or improvements, although we were promised they will keep it working in 2.13 and, I presume, by extension 2.14. That is in spite of the fact that some features seem to be [kind of abandoned already](https://github.com/scala/bug/issues/10755). * v2 (aka "scala.meta"), which was promised as the real deal, was relegated to be basically a glorified parser. The Scalameta Paradise subproject, which offered most of the advanced macro features, is dead, dead, dead. * v3 (aka "scala.macros"), which was promised as the *real* real deal "[but this time we mean it, for sure](https://www.scala-lang.org/blog/2017/10/09/scalamacros.html)", is DOA. The [last commit](https://github.com/scalacenter/macros/commits/master) was over five months ago and last week they finally [declared the project officially dead](https://github.com/scalacenter/macros/blob/master/README.md)
Do you configure your underlying data sources to return a value? It's possible that Play's cache won't hold onto a value if it gets a null (the default value for a mock method call that returns an object)
I think the most important announcement is actually the (unimplemented) third item in the list of possible TASTY use cases. It's probably way too late to make an impact, but at least the limitations of using jars with class files as the primary device of distributing code have become exceedingly obvious in the meantime.
Akka Streams are tricky to learn, but once you get the hang of it, it's not so bad. This is admittedly trickier to standup than your JS version, but I believe this more or less does the same thing. ``` import akka.http.scaladsl.model.ws.{ BinaryMessage, Message, TextMessage } import akka.http.scaladsl.server.Directives._ import akka.stream.scaladsl.{ Flow, Source } val websocketRoutes = path("socket") { val outflow = Source.single("send") val webSocketFlow = Flow[TextMessage] .mapAsync(1) { // Collect the pieces of a split message, if it comes in parts. _.textStream .runFold("")(_ ++ _) .map { rawString =&gt; logger.info(s"received: $rawString") } } // Output nothing given any possible input .filter(_ =&gt; false) // Output our individual outgoing message .merge(outflow) .map(output =&gt; TextMessage(output)) handleWebSocketMessages(webSocketFlow) } ``` I'm assuming you want to have that outgoing message triggered by some event elsewhere in your app. For this, you could take advantage of [Akka's event stream](https://doc.akka.io/docs/akka/current/event-bus.html?language=scala#event-stream) and replace that `Source.single` with a `Source.actorRef` like so: ``` val outflow = Source.actorRef[Update](updateBufferDepth.value, OverflowStrategy.dropHead) .mapMaterializedValue { self =&gt; system.eventStream.subscribe(self, classOf[SomeDataType]) } .map(_.toString) // Or however you want to serialize it ```
Hi! I'm trying to select a case object depending on the "real type" of a value. Given a Request, I want to get a Consumable. Let's say we have these types: trait Request case class FoodRequest(name: String) extends Request case class DrinkRequest(name: String) extends Request trait Consumable case object Food extends Consumable case object Drink extends Consumable The caveat is that the Request types cannot use the Consumable types. I.e. doing something like trait Request { def toConsumable: Consumable } case class FoodRequest(name: String) extends Request { def toConsumable = Food } case class DrinkRequest(name: String) extends Request { def toConsumable = Drink } is not possible for this use case. My best solution so far is pattern matching on the "real" request type to select a Consumable value. How can I avoid having to perform this pattern match on the request type? trait Request case class FoodRequest(name: String) extends Request case class DrinkRequest(name: String) extends Request trait Consumable case object Food extends Consumable case object Drink extends Consumable val request: Request = FoodRequest("pasta") val consumable: Consumable = request match { case _: FoodRequest =&gt; Food case _: DrinkRequest =&gt; Drink } I would like to just be able to say something like val request: Request = FoodRequest("pasta") val consumable: Consumable = consumableFor(request) // == Food I've been trying to figure this out for days now, but haven't been able to find a solution that removes the need for the pattern matching. Any help is greatly appreciated!
&gt; It's probably way too late to make an impact, but at least the limitations of using jars with class files as the primary device of distributing code have become exceedingly obvious in the meantime. Its been this way for a while, using JVM bytecode binaries for package management has caused a lot of grief and problems with the Scala ecosystem, its both stifled language development as well as creating hellish build matrixes for non trivial libraries.
&gt; The macro part of Scalameta has been spun off into a separate project funded by the Scala Center, and it seems to have been discontinued in favor of Dotty macros. &gt; You are absolutely right. Scalameta is a very important pillar of Scala's tooling and I expect it will gain further in popularity in the future. Same goes for projects building on it, like `SemanticDB` and `scalafix`. They are quite complementary to Scala 3 reflect macros. The main difference as I see it is that reflect macros are integrated in the language and compiler (and the challenge for us is to sufficiently abstract from that fact!), whereas Scalameta is a largely compiler-independent toolbox with its own parser, tree representation and so on. Because in my mind the two systems are so complementary, and there's no wish to have one supplant the other, it did not occur to me that I should have mentioned Scalameta in the blog post. But I see now that it can give rise to misunderstandings. I'll add a paragraph explaining this point.
&gt; The introduction of union types was one of the things I considered when making Either right-biased. The name Either could have been adjusted/improved, but that wasn't practical given the state of development in Scala at that time. Okay. Makes sense, but ‚Äì in my humble opinion ‚Äì changing the semantics of a type (from a non-biased generic tagged union to a right-biased monadic-like construct) without changing the name / adding it as a new element of the type hierarchy, might not be the best choice. (I see, however, how it might be sensible given the amount of pre-existing code using the former in the meaning of the latter.)
In Scala, closures aren't a particuarly intriguing concept per se, but the very fact they exist allows for writing terse and expressive code. Imagine you're writing a short snippet of imperative code ‚Äì for instance, asking a service for a list of users and printing each of them ‚Äì like this for-comprehension: for { user &lt;- service.getUsers() } println(user.name) At compile time, the above code is transformed to something like the following, using a lambda function literal: service.getUsers().foreach { user =&gt; println(user.name) } (In simple cases, you can perform such a rewrite yourself. Sometimes, when the control flow gets more complex, it's more convenient to write in lambda-style rather than for-comprehension-style from the very beginning.) Now, let's imagine we want to extend the functionality by, say, adding a prefix before each printed name: val prefix = "user: " for { user &lt;- service.getUsers() } println(prefix + user.name) When compiled, this wil turn into: val prefix = "user: " service.getUsers().foreach { user =&gt; println(prefix + user.name) } What happens here is: we've just created a lambda function that encloses a variable `prefix` from the outer scope, which then is passed to the `foreach` method. The existence of closures ensures that such a code is working right and saves us rom the necessity of working around it.
I added a section to the blog post highlighting the relationship between Scala 3 macros and Scalameta.
You can't define toConsumable because you can't change definitions of request case classes, right? If that's the case, you can always use typeclasses (implicit classes) , though I think pattern matching would be cleaner. BTW you don't need to match on types, you can write case FoodRequest(_) =&gt;
I've got a play app that I've been tooling with for a while now. I have a base controller trait that has a bunch of functions to manage database interactions, serialization and data models for the endpoints. This has resulted in both cyclic and bloated type signatures, which I think is having a significant impact on compilation times. Surely there is a better way to do this: trait Controller[IDTYPE, STATEMODEL &lt;: StateModel[IDTYPE, EVENTMODEL, STATEMODEL], EVENTMODEL &lt;: EventModel[IDTYPE, STATEMODEL], REQUESTTYPE &lt;: Request[IDTYPE, STATEMODEL], RESPONSETYPE &lt;: Response[SERIALIZATIONTYPE], SERIALIZATIONTYPE, SERIALIZER &lt;: Serializer[IDTYPE, STATEMODEL, EVENTMODEL, SERIALIZATIONTYPE], RRSERIALIZER &lt;: RRSerializer[IDTYPE, STATEMODEL, EVENTMODEL, REQUESTTYPE, RESPONSETYPE, SERIALIZATIONTYPE], FILTERTYPE &lt;: Filter[_], CHAINTYPE &lt;: FilterChain[FILTERTYPE]]
You just need `inline` for that, which is already supported. See https://github.com/lampepfl/dotty/blob/master/tests/run/inlineForeach.scala
Neat! This also inlines the function argument? (I.e doesn't need to construct a function value to delegate to?)
Yes, if you declare the `foreach` decorator like this: implicit class intArrayOps(arr: Array[Int]) { inline def foreach(inline op: Int =&gt; Unit): Unit = { var i = 0 while (i &lt; arr.length) op(arr(i)) } } the function argument is inlined and all overhead is eliminated.
Does Scala ever plan to expose a macro system for untyped ASTs that are spliced before typing? Honestly as someone with a small scheme/racket background I have a hard time getting excited about this macro system. Perhaps it is the best Scala can do though given that the language was never designed with macros in mind. From my research in the past the nemerle programming looks To have the kind of macro system you'd get if macros were integral to the language: http://nemerle.org/About
Nice to see a well-presented entry level fun project in Scala! Higher-kinded types and Spark-jobs are all nice and good but doesn't lend itself well to everyone.
I've been thinking about doing something like this for a while. I see you've implemented more or less the basic functionalities? Like raw block data, proof of work with fixed difficulty etc. Are there any specific things you want to do with it? I'm not sure I'm parsing correct since I'm on mobile and can't run anything. But does your block hash effectively contain the entire history of your chain (including raw data)?
Bad bot.
Thank you! 
&gt; I would like to just be able to say something like &gt; val request: Request = FoodRequest("pasta") &gt; val consumable: Consumable = consumableFor(request) If that's the interface you want to support and you have no control over the `Request` instances then you simply have no recourse - you must check the runtime types. The trouble with not having control over the instances is, as you note, that you cannot create a method in the trait which returns the correct `Consumable` for the instance itself. The trouble with using `request: Request` is you no longer have any static type information available to you in order to associate the correct `Consumable` at compile time, e.g. with typeclasses. If you could keep the information that `request: FoodRequest`, then you could define a method that gives you the appropriate `Consumable`. If you are creating the `Request` instances at some point, you can at that time pair each instance with its correct `Consumable`, e.g. `val (req: Request, con: Consumable) = FoodRequest(...) -&gt; Food`.
Code generation might be close to what you're after: https://github.com/scalameta/scalagen
Ah, I see, that makes sense. Thank you!
So I'm new to Scala and I'm learning to work with the language without really *learning* the details of the syntax (normally I'd pick up a book to learn a new language but it's for work and I'm against the clock), but there's something I keep seeing and don't understand. I'm using the Scala Play library, and here's a line of code from an example: import play.api.libs.json._ import play.api.libs.functional.syntax._ implicit val locationReads: Reads[Location] = ( (JsPath \ "lat").read[Double] and (JsPath \ "long").read[Double] )(Location.apply _) Syntatically, what exactly is the Reads[Location] ?
`Reads[Location]` is the type of `locationReads`. While many languages list types before the variable, e.g. `Reads[Location] locationReads = new ...`, Scala places types after the variable.
&gt; * We even took a stab at runtime macro expansion, but that was just crazy. Out of curiosity, what did this do, and what were the main challenges? In [Squid](https://github.com/epfldata/squid), we supports both compile-time and runtime metaprogramming without much trouble, but that is mostly thanks to the amazing work that you put into scala-reflect ‚Äì the impact that the project had on Scala and its community cannot be overstated! An example Squid session: scala&gt; def pow(base: OpenCode[Double], exp: Int): OpenCode[Double] = { | if (exp &lt;= 0) code"1.0" | else code"$base * ${pow(base, exp-1)}" | } pow: (base: Embedding.OpenCode[Double], exp: Int)Embedding.OpenCode[Double] scala&gt; val myVar = Variable[Double] myVar: Embedding.Variable[Double] = Variable[Double](myVar$251950120) scala&gt; code"{($myVar) =&gt; ${pow(myVar.toCode,3)}}" res0: Embedding.OpenCode[Double =&gt; Double] = code"((myVar_0: scala.Double) =&gt; myVar_0.*(myVar_0.*(myVar_0.*(1.0))))" scala&gt; res0.close.get.compile Compiling tree: ((myVar_0: scala.Double) =&gt; myVar_0.*(myVar_0.*(myVar_0.*(1.0)))) res1: Double =&gt; Double = ... scala&gt; res1(0.5) res2: Double = 0.125 
Since the definition only allows for one type to be assigned to `SuitableFood` you need to make both Grass and Cookies a subtype of this type in order to be allowed to pass both to the method. This is a natural limitation of the type definition and an effect of object oriented solutions in general.
&gt; i was going to ask about this too. this is the rumored fix to the problem of version incompatibility in major versions of scala right? it sounds like with tasty, one could make a library, compile it to tasty 1.0, and have it work theoretically on any scala compiler that could parse and compile that version of tasty (wouldn't this be possible for most scala versions with a compiler plugin too?) That is roughly correct, but for this to work the ecosystem needs to start distributing tasty trees rather than standard java JVM jars.
Well I would argue that the primary constructor for some type that is a monad to conform to the laws. I mean I know that the way ADT's are constructed in Scala are different to Haskell, but this is orthogonal to the laws themselves. To be clear, the `Option.apply` was manually and deliberately defined to do null checking, it didn't have to be done this way.
As I mentioned in the description, this is a simple blockchain implementation. There are a lot of things not yet implemented, and I have a plan to develop api and p2p.
Tasty trees are not sufficient for this. You would at least need to: - Make the runtime an explicit dependency. - Stop implicitly importing `java.lang` into every source file. - Make the dependencies on the JDK explicit. - Split the JDK dependency into smaller modules on which people explicitly need to depend, so that you have a fighting chance of being able to define a subset that works reliably across platforms. - Add versioning, linking and dependency information to some future Scala module format. - Start distributing this module format instead, or in addition to, jar files. - Have "backends" that can use these modules to link and emit code (bytecode, native code, etc.). Even if this was a priority, it would take years to implement.
You could certainly give `Option.apply` a different name, but given that `null`s exist, it's useful to be able to decide whether you want to treat `null`s as `Some(null)` or `None`. If `Option.apply` didn't ship with the standard library, people would have just reinvented it over and over.
Thanks!
It's got some meat now. I've finished about 10 tickets' worth of work - I would love some input!
With Akka Streams it's pretty easy. Something like: val dbParallelism = 5 // or whatevs val postProcessParallelism = 10 // or whatevs // Whatever operations you want the DB to do should be baked into the query action val query = for { year &lt;- Parameters[Int] row &lt;- DataTable.filter(_.year === year) } yield row def postProcess(row: DataTableRow) = { Future.successful(row.toString) } Flow[Int] .mapAsync(dbParallelism) { year =&gt; db.run(singleYearQuery(year)) } .mapAsync(postProcessParallelism)(postProcess) .runWith(Source(1980 to 2012), Sink.seq) .foldLeft("")(_ + _) If your reducing step is commutative, then you might choose `mapAsyncUnordered` for greater efficiency. 
There are two very different things in this third item, which I will repeat here for convenience: &gt; A build tool can use it to cross-build on different platforms and migrate code from one binary version to another. Migrate code from one binary version to another is definitely feasible, assuming TASTY is sufficiently decoupled from said binary version (for example, the current version of TASTY assumes a bit too much about the encoding of default parameters or super calls in traits IMO). In fact that's part of the reason Scala.js has stayed binary compatible for so long: we already had typed ASTs before they were cool (our .sjsir files), and our deserializer can automatically "upgrade" old sjsir files to the new format. It's totally transparent so no one knows about it, but it happens every time you use Scala.js with libraries compiled with sufficiently old versions of Scala.js. The "cross-build on different platforms", however, I'm not buying. And I'm the author of Scala.js so I have some advanced technical knowledge about this. The main reasons are actually not what you write about (we *have* some reimplementation of the JDK so avoiding all dependencies on the JDK is not required) but rather platform-dependent source code, which necessarily must lead to platform-dependent TASTY files. And then, consequently, this means we need to encode platform-dependent transitive dependencies. I have explained this many times to my colleagues who develop dotty and TASTY-related stuff, but they are quite stubborn about this. Maybe they will eventually prove me wrong, but I won't believe it before I see it.
&gt; You could certainly give Option.apply a different name Exactly. My point is not about whether or not `Option.apply` exists, but the fact that `Option.apply` is the default constructor and so in context of making things lawful (if this is your argument) then the default constructor should follow this. The thing is, if you make a proper basic ADT in Scala, your `.apply` methods get automatically generated. `Option` isn't coded this way, they did a custom encoding of an ADT.
&gt; Scala doesn't scale on bigger projects. Sbt loading and Scala compilation takes time, i won't deny it. Please allow me an analogy. It only takes a few minutes to take your car, then you can stop whenever your want, for example to ask your way, and resume your journey in no time. Cars are made to be stopped quickly and restarted just as fast. You don't have to plan your journey, just take your car and make as many stops as your want until you reach your destination. On the contrary, taking a plane takes much more time! You have to go to the airport, register and landing and taking off takes ages. Making many stops by plane would take a huge amount of time. You just can't stop somewhere, ask your way and go again. So should we say that planes are not a good fit for big distances, the ones where you have to make many stops to ask your way? Do planes scale? Scala does scale very well for big project, at least when you take the language for what it is and what is has to offer. Every language is different, don't try to code in one like you know in another. If your way of coding is by running your code as quickly as possible to immediately see the changes you made, then that's fine. Mine if by precisely modelling my domain with types and designing my architecture with algebra in mind. Both of our approaches are good! Both can scale! But as true as my approach is not good match for dynamic languages or languages with limited type system (no offense, just a fact), quick-reloading programming style is not very efficient in Scala, because of the problems you mention. It does not mean Scala doesn't scale, it does mean quick-reloading is not a good match for Scala.
I'd say the law is: `f(v) === unit(v).flatMap(f)` (where `===` is "equivalence" which I won't define in detail) *for all f and `v` which are legitimate in the given context*. `Either` is a legitimate and useful monad in contexts like the scalazzi safe subset of Scala: it conforms to the monad laws *and* provides valuable functionality. There is no context in which `Try` is both legitimate and useful. Either you use it in a context like the scalazzi safe subset of Scala, in which case it's a worse version of `Either` (since it restricts the left hand side to the useless `Throwable` type, and also throws away any more detailed type information about it), or you use it in a context that permits `throw`, in which case it doesn't conform to the monad laws. When people say "`Try` doesn't conform to the monad laws", there's an unspoken "in the only contexts in which it provides any functionality". (I take a similar view on the legitimacy of regarding log statements as pure on the grounds that they're unobservable: if not logging is equivalent to logging then why have a logging call at all? Or of parallel `ap2` on `Future`/`Task` implementations: you can say that it doesn't violate the monad laws in a context where you consider parallel-evaluation and serial-evaluation equivalent, but in those contexts `Future`/`Task` is just an overcomplicated `Eval`).
&gt; I wouldn't agree that Scala should be encoding Either as Right | Wrong etc because that's not the fundamental point of Either, which is the naive binary Coproduct, and it encourages a usage and pattern that are frankly, only a thing because of a lack of support for better features. I would disagree with the notion that `Either` for error handling is a bad usage pattern. By using `MonadError` you're essentially introducing [an extra existential and level of indirection](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/). There's a time and a place for that, but there's also a (more common) time and place for returning plain, concrete values. Ultimately, whatever we call the two sides of `Either` it will have the same properties, so our choice of names should be about conveying the right information to the human reader of code that uses it. The use cases where it represents success/failure are more common than the use cases where it represents balanced left/right, IMO rightly, so it would be better to have names that reflect that.
&gt; we have some reimplementation of the JDK so avoiding all dependencies on the JDK is not required It's not about avoiding dependencies, but making it explicit on which parts you depend. The reason for that is that the frontend generating the module files should not have to know which platforms have reimplemented which parts of the JDK, so that the frontend can either guarantee that an emitted module can be compiled to JVM/Native/JS, or reject it at that point. It's not a hard requirement, but makes a lot of sense from a user experience perspective. &gt; but rather platform-dependent source code, which necessarily must lead to platform-dependent TASTY files. And then, consequently, this means we need to encode platform-dependent transitive dependencies. That pretty much reproduces what's done at the source-code level and is part of what I meant with "linking and dependency" information. Either users have modules with platform-specific sections with implementations (replicating the different source directories for different platforms), or they have depended-on modules with platform-specific sections (not possible today), or the backend selects different modules during linking, depending on the platform chosen (like it happens with scala-java-time). The benchmark on whether this is fully implemented is the ability to take a single "application module" (without explicitly restating the modules it depends on) and have it emit fully linked and runnable code without requiring build.sbt (or SBT at all). &gt; Maybe they will eventually prove me wrong, but I won't believe it before I see it. I agree on that.
In what sense is it a default constructor? Haskell doesn't even have this default constructor (only constructors for the individual data types) and it's working perfectly fine.
&gt; Well I would argue that the primary constructor for some type that is a monad to conform to the laws. Properly a monad is a triple; `(Option, Some.apply, Option#flatMap)` is the monad, and every monad typeclass instance I've seen for `Option` called `Some.apply` rather than `Option.apply`. Just as in pre-2.12 Scala, the monad for `Either[String, ?]` is `(Either[String, ?], Right.apply, {_.right.flatMap}) `. It's unfortunate that the monad operations for these types do not have the names that we'd expect, but most people understand "`Foo` is a monad" as "there exist an obvious definition of `point` and `flatMap` for which `(Foo, point, flatMap)` is a monad" rather than "`(Foo, Foo.apply, Foo#flatMap)` is a monad".
&gt; The thing is, if you make a proper basic ADT in Scala, your .apply methods get automatically generated. Option isn't coded this way, they did a custom encoding of an ADT. For a `sealed trait`/`case class` ADT you don't get an autogenerated `.apply` on the `trait`, only for the specific cases, no?
You're looking for `catching(classOf[E]) either f(a)`, from `scala.util.control.Exception._`.
I don't know about play, but maybe you can use composition rather than inheritance? E.g. maybe the serializer stuff could move down into a serializer interface, and some of those types could be internal for that, and the controller could have a serializer, filter and all those things rather than be them. Sometimes using higher-kinded types (`F[_]`-shaped types) can help ensure everything gets wired properly. E.g. something that needs to be deserialized to provide an `A` can be represented as an `F[A]` if you want to operate on it before it's deserialized somehow. Honestly your query is a bit too general to answer specifically - maybe you could explain what it is that this `Controller` does, and then we can try to suggest simpler ways to solve the same problem?
We should get rid of slf4j and create a Scala library for logging. MDC is a terrible concept for asynchronous programming.
Thanks for the comment - So, the reason that the empty method returns an IO, is because it also needs to create a new ListBuffer, which makes it not referentially transparent - and it can lead to unintended sharing of mutable state. Perhaps you have a point about where I run the usafeRunSync though - do you have any suggestions on how to improve it?
I would be good if they can fix the type errors when you say Right(x) and don‚Äôt declare the type of the left, which defaults to Nothing.
FYI I wrote an educational Scala miner. https://github.com/rlavolee/scala-educational-miner
Thank you, any plan to develop more?
I stay way from it because it's more of a framework than a library. It's more probable than not that it spreads through the codebase. That's why it didn't make it into production when I tried it so I don't really have the answer you want. 
Give NeoVim a try [https://neovim.io/](https://neovim.io/) NeoVim \+ Scala [https://medium.com/@alandevlin7/neovim\-scala\-f392bcd8b7de](https://medium.com/@alandevlin7/neovim-scala-f392bcd8b7de)
TTFI should not have (much) overhead - you are basically injecting type-class(es) for your algebra and call it the same way you would call e.g. `Future.successful`, `Task.pure`, etc. It would be harder though to do things like: creating AST, modifying it before running, and then interpreting. (There was some blog post about how you can optimize in runtime with TTFI, but all-in-all it is more difficult to understand). Free monads will always have this 3 steps: lift to monads, build program as an AST, interpret it. If you don't do anything with AST, but use it to feed interpreter then it is pretty much useless overhead. However if you do some interesting things with e.g. recursive schemes, then free monads are the way to go. Both concepts should not be understood as a "FP-feature I use in code" - they are architectural decisions impacting whole codebase, similarly to: * deciding that you will encode services as `Request =&gt; Future[Response]` * replacing Futures with Twitter's Futures, Scalaz's or Monix's Task * running whole domain on top of Akka Actors, Typed Actors or Akka Streams except both TTFL and FM want to decouple implementation from particular algebra (so you say you want to use monad or applicative or whetver, but don't bind whole codebase to particular implementation). The more I work with FP and domain modelling, the more these concepts make sense to me... except I wouldn't recommend them to people who don't worked with functional-style for a while. I also wouldn't recommend it if the decision wasn't approved by more senior members of the team that already wrote a whole project with them (even a toy project). Otherwise it would be like giving GoF Design Pattern book to people who just switched from assembly to C++ and expect them to write enterprise-ready project. There has to be someone that already feels like it should work guiding the rest, otherwise you'll end up with crappy code thinking that these patterns are shit. Whether it is freestyle, cats or scalaz implementation is irrelevant. Prototype some code and show it the the rest of the team. If they cannot get to the point where they could contribute in a reasonable amount of time (e.g. 1 day?), then the jump is too huge, so stick to something they already can maintain.
LoyaltyOne | Senior Software Engineer, API Services | Toronto, ON, Canada | ONSITE | Full Time Several of our teams are looking for Scala devs. We're mostly using Akka and its ecosystem to build microservices that should scale up. Among other things, we host the Scala Toronto Meetup, here's a video of our last one: [https://youtu.be/EPNBF5PXb84](Deploying an Akka Cluster to AWS EC2 Container Service) Apply at https://www.linkedin.com/jobs/view/536892724/
BondLink | Junior/Senior Software Engineer | Boston, MA, US | ONSITE | Full Time We are a small team of Scala engineers using advanced functional techniques to ship reliable, rot-proof software. We use lots of shapeless and scalaz, a free monad based program/interpreter architecture, monadic logging, and more. If these things are familiar you will feel right at home, if they are interesting to you and you want to learn, you will find a safe and supportive environment to grow on the job. About the company: BondLink provides a SaaS platform to bond issuers in the $4 trillion municipal bond industry. We have recently raised a large Series A, we have strong revenue growth, and we are already working with many of the largest issuers in the country. The company was founded by a former Massachusetts Assistant State Treasurer who raised nearly 25 billion in funding for the state and set the gold standard in the industry for transparency and investor outreach, partnered with a multiple-time startup founder with several successful exits under his belt. We have deep expertise and credibility in the industry we serve, great product-market fit, a strong footing with a ton of momentum, and a great handle on our software delivery. If this sounds like something you'd want to be a part of, shoot me a PM.
I created an Scala and sbt Docker images. The difference between mine and others are: * My images only use JVM passed TCK. * I uses Azul Zulu and AdoptedOpenJDK's OpenJ9 for now. My plan is: * If AdoptedOpenJDK releases HotSpot's version that passes TCK in the future, I will add that too. * Add Dotty. Any suggestions are appreciated! :\) [https://github.com/jiminhsieh/scala\-sbt\-docker\-images](https://github.com/jiminhsieh/scala-sbt-docker-images) [https://hub.docker.com/r/jiminhsieh/scala\-sbt/](https://hub.docker.com/r/jiminhsieh/scala-sbt/)
True Fit | Senior or Principal Scala/Java Engineer | Boston, MA, USA | ONSITE | Full Time | Not the hiring manager, but I'd guess ~100k (very rough guess) We're looking for an engineer on our engine team, which develops the core of our business. Candidate should be comfortable with mathematical equations, and transforming them into software. Should be experienced in multi-threaded software and distributed architecture. For Men it's less of an issue, but for Women especially, buying clothing online is difficult, and hit-or-miss at best. In one brand you could be a 2, in another you could be a 6. Even within a brand, different lines can fit differently. Our software equalizes these discrepancies between brands. We are also working on Discovery -- think Pandora for clothing. We're a Scala shop, but more than 50% of our hires were Java Engineers. I had only heard of Scala a week before I got a call for the interview. We have a lot of great Scala engineers and do a lot of training, send people to NE Scala symposium, etc.
I avoided it just because I don't trust macros and don't feel the benefit is worth the cost. I use (plain Scala) free monads in production and have for years, and am starting to use final tagless style in one corner of a production system.
Thanks for explanation!
Hi! This is my very first attempt at a blog post. I very much welcome your feedback. Cheers!
Interesting, however I believe it's just a more complicated way of doing what \[an STM already provides\]\([http://scala\-stm.org/](http://scala-stm.org/)\) \(e.g. versioning\).
If I had to interoperate with an existing API I'd convert at the boundary, just as with e.g. `null` or Java collections. I wouldn't ask an FP library to offer e.g. a `Foldable` instance for java `List`, because although java `List` can be used in a pure-functional way, it doesn't make any sense to use it within a pure-functional codebase.
Thanks. Indeed it's a somewhat similar. There are fundamental differences however. STM is a higher level abstraction. STM is based on transactions, with automatic rollback in case of concurrent modifications. On the other hand, the library I built is lower level, and does not expose the concept of a transaction. Users of the library have control over what should be done in case of concurrent modifications, contrary to STM. It is more flexible in this aspect. For some algorithms, such as the one presented at the end of the article, this flexibility is useful.
Hey so by the way, the monthly "Who's Hiring" threads are actually on a timer and get posted automatically by /u/AutoModerator. I'll leave this one up but let's not do this too much because now I'll have to go and delete the one that is scheduled to appear in about a week from now.
I apologise for the complications. It's a great idea to have a recurring hiring thread. But could we change the timer so that it posts on the *first* of each month? It felt silly to me to post into last month's thread :) Thanks
hmm yeah I'll take a look if you can schedule it that way, thanks for the suggestion!
That is a good distinction thank you
Confused by &gt; As an example, here is how one could, using atomic primitives, Atomics in Java are not primitives, so I'm not sure what you mean.
The term 'concurrency primitive' seems to be in pretty wide use to mean something along the lines of "an abstraction that provides some kind of guarantee, which can be used to build libraries/apps on top of"
Leapfin | Software Engineer | San Francisco, CA, US | Onsite or Remote | Full Time | $120k+ We are an enterprise b2b finance software startup based in SF SOMA. Because we work with finance, we deal with lots and lots of data. We have found product/market fit and are growing rapidly. We are looking for a technical leader who can help us scale. We are looking for someone able to collaborate well in a team environment and who always puts the customer first. Job Posting: https://leapfin.com/careers About: https://leapfin.com/about Let me know if you have any questions!
&gt; So those 2 traits will be very often compiled together, and Zinc incremental compiler will not help with that. Additionally changes to any of the components in a cake will trigger recompilation of stuff above it in dependency graph. This is no longer true as of SBT 1.x (when it turned on name bashed hashing of classes/traits to detect if files needed to be recompiled).
I argued in the past that we should decouple Scala from JDK, but I think its just way too late for that and its too much work (this was also talked about in a Scala center meeting since someone brought it up). Scala is just too reliant on JDK (or some implementation of JDK) that it would require too much work to just move away from it. This has turned out to be both a blessing and a curse for the language. Tasty does solve one half of the problem, which is the binary compatibility problem (if designed well). Its the other half, i.e. completely decoupling Scala from JDK/Java which is not likely to happen.
I agree that decoupling Scala from the JDK would be a good idea. I worked on this years ago, but the most important piece of the puzzle was shot down back then. The problem is that the ideas I have seen not a long time ago were poorly thought out, lacking a lot of the lessons learned over the past few years about what works, and what doesn't: Let's not forget that Scala already had some (abandoned) "pseudo"-decoupling from the time it also wanted to run on the CLR. Most of the proposals I have seen were more along the lines of "let's reinvent this random java.something package and put it in the scala namespace, people will surely flock to it and rewrite all their code, instead of just using the existing JDK stuff", which are just wrong and misguided based on existing evidence and practical considerations. The first thing that would have needed to happen, _before_ people go out, reinvent stuff (that probably no one is going to maintain), and dump&amp;abandon code into `scala._` was to take back control over the existing Java dependencies: For instance, if your global namespace, that is implicitly imported into every Scala source files, consists of 400 things, coming from three different sources of which one is not even under the control of the language ... there is an issue, that needed to be solved. Leadership blocked those efforts, and the rest is history.
&gt; I don't think this is a bad idea, the Scala namespace is considered blessed and people would rather use a library from such a namespace (assuming its good enough quality). It wouldn't completely solve the decoupling problem, but it would at least make libraries more portable and less reliable on Java (the idea is that eventually you get enough people depending on Scala namespace so transitioning is less painful). That's exceedingly unlikely to ever work out the way you imagine. You are severely underestimating the effort to even reinvent stuff, and overestimate the amount of people available to maintain such code. There is a reason why my total for scala/scala sits at -17K lines of code. &gt; Honestly you need to do both though, you have to decouple Scala from JDK/Java but you also need to provide libraries for common functionality, i.e. reading files or a DateTime library. This can be done, but the amount of bike-shedding on whether the package is named `java.something` or `scala.something` is a huge waste of time looking at the least important issues. If people feel they can do better than Java on some topic, they should go out, implement their library in their own namespace and prove it. After that it's perfectly fine to have a talk on renaming the package. But as long as I'm not seeing anyone doing this work, it's pointless to discuss the desired results. (Just have a look at the JSON situation to see how this worked out.)
&gt;plain old manual argument passing - no need to explain that Should be in bold, size 72 font and covered in blinking lights. The lengths developers go to not pass arguments is _astounding_.
&gt; BuT mY ReFleCtIOn BasEd EnTerPriSE IoC ConTAInER! \- Java programmers
Container based DI is so gross.
What about partial application as DI? Seems much cleaner, reusable, caveat-free.
To be fair: if you have a small function you'd like to pass another dependency to, and it has an enormous call hierarchy you would have to change, it would be a lot easier to autowire that argument in (or use a global). I still think it's worth simply passing arguments around, but I don't think the tooling we have to make it easy is the best. If there were a refactoring tool that traversed the call hierarchy and passed in an object of a specific type at each point in the call hierarchy, it would be a lot easier. Instead (unless using IoC/globals) I do it manually.
Writing my thesis ... which is about the design of Scala.js, so it's almost relevant üòÖ
Its also astounding how old this gets, i have better things to do then pass things like `requestId: RequestId, httpClient: HttpClient` in every single function in application webservers
&gt; That's exceedingly unlikely to ever work out the way you imagine. You are severely underestimating the effort to even reinvent stuff (or improving on Java), and overestimate the amount of people available to maintain such code. There is a reason why my total for scala/scala sits at -17K lines of code. Sure, but I also think its similar amounts of work to just reinventing the current `java` namespace (thats not withstanding the current legality issues in implementing the JDK with what happened with oracle). I mean other languages have also implemented their own standard libraries from scratch, yes it takes time and effort but its not impossible &gt; This can be done, but the amount of bike-shedding on whether the package is named java.something or scala.something is a huge waste of time spennt looking at the least important issues. There is unfortunately bike shedding but its not on this specific point. So far the bike shedding has been on everything apart from this point (its already been decided that things will not go into the scala namespace because they are trying to move libraries outside of it to make the core as small as possible)
I'd absolutely use it in production, free Monads are great, but using them requires a ton of ceremony. Of course, I personally prefer tagless final myself, but if you want to do free Monads then freestyle is a good idea :)
Freestyle now also supports tagless final. I was actually thinking of using Tagless Final pattern instead of FM.
I think you make a good point about not bringing in something that doesn't make everyone productive or hasn't been tested before. On that note, I have used these patterns before in both Haskell and Scala. Bringing in the patterns and features that come with the pattern are less of a concern. It was more around using Freestyle as a library to make life easier and gain the optimizations that the library has implemented at the same time.
that is argument passing.. just partial.
y
Java programmer should try avoiding reflection because it's slow and hard to read, but with annotations it can make some really neat api's (by Java standards).
If you feel comfortable enough to guide your team through learning new things, then why not? If I had codebase that was Cats/Scalaz heavy (or I was planning to have) I would simply go with Cats/Scalaz (and maybe used https://github.com/Thangiee/Freasy-Monad if I went with free monads). However if didn't want my codebase to be biased towards any of them I would use freestyle probably. But IMHO at this point it is mostly a matter of taste, preferences and habits, more than some clear advantages of one of the solutions.
Well, sorry for my poor style. If you have some idea how to improve readability, I welcome any (meaningful) feedback.
Freasy looks nice, and also using plain Cats (which is what other code is using in our application) would be great. I think the biggest benefit for us is the performance optimizations that Freestyle has implemented. Performance is of utmost importance in this application. But yeah, thanks for your suggestions!
I haven't looked at that, but I'm not quite sure what Freestyle buys you when doing Tagless Final
Literal type support is exciting! I've started doing some stuff with building a shapeless records Queryable interface for our db, and this will be nice! 
It's nice that at least something with Scala plugin is progressing. Last time I tried using trivial map from Shapeless, it resulted in red squiggly lines all over the place (4 years after report and bug is still not fixed). Other libraries suffer from this as well - e.g. ScalaCSS or Udash. Without macro support, Scala plugin is a bit "meh". I don't want to hear about macro support for Dotty, like few years back when JetBrains employee responded to my critique of current (lack of) support of macros. It still won't be here for like a year and even then I fear it won't be anywhere close to 100%, since IDEA fails handling even basic Scala code after so much time (e.g. "Good code red: pimp method call in trait with self-type" or "Good code red: overloading with implicit class"). Don't get me wrong, it is probably still the best Scala IDE, but using it one cannot use language and libraries fully (or at all), or one have to leave IDE and start using dumb editors without any dev features. I will probably be moving from Scala partially for this reason.
To what, though? My experience is that anything one might move to is likely to have even limitations than Intellij enforces on you with Scala baked into the language (especially if it's one with good IDE support). Scala is a flawed language but it's one that's great at ruining every other "practical"/popular language for you!
You know you can change inspection levels, right? I've used Intellij for Scala daily for years and I've only experienced one issue with code displaying red squiggles when it was correct (Finch with Shapeless). My issue was fixed in the plug-in within a week or two.
So your solution is to disable all advantages IDEA has? How is it different from using a free dumb editor like Atom? Also, years? I am just a hobbyist when it comes to Scala and my list of reported unfixed bugs is surprisingly large: https://youtrack.jetbrains.com/issue/SCL-11704 https://youtrack.jetbrains.com/issue/SCL-12098 https://youtrack.jetbrains.com/issue/SCL-12401 https://youtrack.jetbrains.com/issue/SCL-11488 https://youtrack.jetbrains.com/issue/SCL-12626 https://youtrack.jetbrains.com/issue/SCL-13686 Note that I have experienced many more bugs. The list above are just the bugs **I** discovered. I wonder if it is even worth reporting bugs, starring them, because rate of bugs is like 3 times higher than a rate of JetBrain's fixes. You were probably very lucky your bug got fixed this fast. Shapeless bugs are reported without a fix for years: https://youtrack.jetbrains.com/issue/SCL-8035 https://youtrack.jetbrains.com/issue/SCL-11192 https://youtrack.jetbrains.com/issue/SCL-7723 I was expecting a bit more from a **paid** service...
IntelliJ + Scala is one of the most pleasant programming experiences I've had. I hit bugs from time to time, but never anything that makes me regret using it.
In my case, there was a fix to add Shapeless and another dependency as a global import, I don't remember the details because it's been a while. I get that you're bitter and frustrated, but questioning whether or not I've _actually_ got 4+ years of professional Scala experience makes me wonder if your bugs have no activity because of how you represent the problem.
Annotation-based anything makes it really hard to compose software. I.e think of how java ee does stuff with servlets and filters, you have to undermine type safety if you decide to turn your `HttpRequest =&gt; SomeAugmentedRequestType`. If you do though and you forward it through a filter, you will have to check with `isInstanceOf` or unsafe downcast at runtime anyway. Annotations shit on type safety in a lot of places, and make it hard to reason about certain things. I'd rather not use them at all.
Can you speak more to what you're doing?
mapK and auto implicit factories for your algebras plus many built in utility algebras are provided as part of the core and other modules. 
&gt; but questioning whether or not I've actually got 4+ years of professional Scala experience I am not questioning your experience with Scala, I am amazed you virtually haven't hit any bug in Scala plugin in years. I am working on my side project in Scala for like a year, maybe a bit more, but nowhere near full-time (I would guess like 0.5-2days per week) and already hit so many landmines. Compared to you, who spent with Scala plugin like **15 times** more time and have perfect experience... Also those listed bugs are not all Scala plugin bugs I suffer from. There is another like 4 bugs which were reported by someone else and a few which I gave up on reporting, because how sluggish their reactions are (there is a workaround, so I am just using slightly worse code to "fix" IDEA's mess). Honestly, I am not surprised they don't care about Scala. After all it's a competition for their Kotlin and the Scala language is quite complex, so it would make sense they don't want to spent too much resources on it (e.g. proper macro support). But maybe I am reading too much to it, because they also failed to fix critical bug in TypeScript plugin for months, even though that plugin is clearly paid (I don't think it has a free variant). &gt; makes me wonder if your bugs have no activity because of how you represent the problem I doubt it, since in most bugs I don't write anything but the broken code itself and versions. It's a waste of my time and theirs writing too long descriptions of an issues which are apparently broken and can be summarized in title like "good code red: ...". Honestly, I find it amateurish that a paying customer has to wait weeks before support even responds to a ticket. And that response is only because customer spammed the hell of social media, not because they finally get to it. I don't bother with non-critical bugs, because they are not managing to fix even those critical ones in timely manner.
Can you upload your project to GitHub so someone could actually look at the setup?
Say, I'd like to check whether a certain point in time is part of set of potentially infinite many intervals. So like, I want to know whether a point of time is christmas. Mathmatically, Christmas is an infinte set, covering a certain span of time each year. Of course, when I have several of such sets, I can use set operations on them. So Christmas.intersect(December) should yield Christmas, while Christmas.intersect(Easter) yield the empty set. I think, I have seen a library for handling such sets once, but alas, I can't find it anymore. Ideas?
This is more of a fault of Scala than Intellij or Ensime, macro behavior (specifically whitebox macros) is basically completely undefined, which makes it very hard for IDE's to work with
Interesting, maybe I am just unlucky. I thought I was not using many special/advanced features - mainly just composition via traits (almost no generics or type fields), extension methods and from libraries most advanced are Monocle lenses (I had to try several libraries before I found one compatible with IDEA and Android/ProGuard). I tried using Shapeless to map over tuples, but that was such a pain to work with because of how broken everything around that code in IDEA has become. In the end I gave up and just wrote few helper functions. Scala is a beautiful language (with many tiny flaws, usually not worth mentioning, because the advantages heavily outweigh those minor issues), but tooling is definitely very far from good. Compared to Java or probably even TypeScript, Scala tooling has many unsupported cases of language use. The tooling, in my opinion, holds Scala back.
This thread provides a lot of explanation about macros and why they turned out that way (https://www.reddit.com/r/scala/comments/8g359q/macros_the_plan_for_scala_3/?utm_content=comments&amp;utm_medium=hot&amp;utm_source=reddit&amp;utm_name=scala). The tl;dr is that macros opens up a huge can of worms, and this is why historically they have always been "experimental". It turns out that even though they are experimental, its considered required functionality for certain parts of libraries/programs. Also there is a bit of contention of macros vs more expressive types when dealing with a strongly typed language like Scala. I agree that the situation we are in right now isn't a good one, on the one hand macros have really complicated the language, and a lot of language designers (including Martin) have some regret about even releasing macros. On the other hand, libraries like https://github.com/getquill/quill aren't even possible without macros. Many years were spent in researching to try and figure out a really nice macro system that works well with a strongly typed language but all of the macro systems have their issues, they are either well defined but very limited (current Dotty proposals) or they are extremely powerful but ill defined (current macro system which really is just exposing the internals of the compiler)
Dotty itself has proper IDE support backed right into the compiler, the compiler exposes LSP (language server protocol) which any IDE can use (whether Intellij will use this is up to debates, currently they rely on their own internal typechecker which in some cases is out of sync of the compiler). That plus semanticDB should makes things a lot easier.
I hope that open critiques like this one directed at specific technical decisions (not people or projects!) allow the community to collectively move forward. I also hope that others heavily critique the Scalaz 8 design, on every aspect, not just performance, because honest feedback is essential for improvement (and I don't take it personally). Also note that Luka, who is a Cats contributor, has been doing excellent work in his own repositories to workaround some of these limitations in Cats (mtl-special, cats-bio, etc.), and a critique will hopefully help encourage these contributions to be folded back into core, where they can benefit the maximum number of people. We are not our code.
IDE's have become such a crutch with young developers as well, so if you want the mindshare tooling IMO is more important then anything else...
&gt; If the type class syntax is not free I didn't quite follow this. "Free" in what sense?
The cats syntax uses AnyVal classes which do not cause additional allocations. The lack of these allocations is the ‚Äúfree‚Äù part.
Ohh, I misread then. I interpreted "...not free, as in the cats library" as "the cats library is not free" instead of the intended "...not free the way the cats library is".
Where do you live? Everyone we work with can't hire quickly enough.
+1 on clojure. And I never once pictured myself saying that but afai tested it, C# is pretty nice. Linq and PLinq will not let you down when operating on collections. The OO features are cool too and won't give you up. Caveat: no filters/pattern matching.
If you can code well, and have done enough to educate yourself on the basics of computer science, plus demonstrate the self-awareness that you know there may be gaps in your knowledge due to the lack of a degree, I have never found it to be a problem in practice. Everyone asks for it as third party proof you can code, or have a certain base skill set, but if you can demonstrate that yourself, then the value gained by a university vouching for you is minimal and people just overlook it. TLDR, don‚Äôt let a request for a specific degree stop you from applying for a job you feel you are qualified for. Let the company meet you and decide for themselves how important that really is to them.
1. You can avoid the overhead of the wrapper class (although not always by using `AnyVal`), but that's a small part of the overall overhead and won't make a significant difference in performance. 2. Yes, it's possible, but I recommend never to define orphan instances, so when a programmer knows a type, they also know the instance that will be used. I also encourage people to define instances in the type class companion object (for "standard" types) or in the type's companion object (for "user-defined" types). IMO changing imports should not change the semantics of a program, although I know not everyone agrees with this idea.
Come to New Zealand! We can‚Äôt find enough Scala devs over here :(
Oh this is a great idea! I completely forgot about Scala days
&gt; I tried using Shapeless to map over tuples, I mean, shapeless is (imo) about the most advanced thing you can get into in scala. It's not super surprising to me that intellij has issues with that. But ideally it should be a small, self contained section of your code. So if intellij is highlighting it all red, while not ideal, shouldn't be _that_ big of a deal. The red underlines doesn't prevent compiling, and doesn't show up in the "problems" section. So the only impact is red lines, and possibly unreliable auto completion and stuff. &gt;Scala tooling has many unsupported cases of language use While true, I would say that those unsupported features aren't even available in other languages. So, you basically lose nothing, even if you were prevented from using them by the IDE (which you're not)
I see your points, and yeah I also strongly prefer capturing the instances in the companions. Maybe a nice middle middle ground is the ‚Äòlow-priority implicits‚Äô trait approach. It provides the same benefits as instances directly in the companion objects, but also allows the users to override the instances.
yeah, but that's like rubbing salt into the wound...
\+1 for Clojure from me as well. I faced a situation similar to the one described. I went from mainly Java in university to working with Scala and Javascript. I really enjoyed using Scala, but Javascript not so much. I was worried about leaving Scala but took a shot and ended up changing jobs to a place where I use Clojurescript full time and I love it. Working with Clojure really made me enjoy the simplicity of it, and working with the REPL is amazing. Scala as a language is much, much more complex to work with. But at the same I do miss at times the assurances that come with a type systems such as the one Scala has. The thing is that there's no perfect language, and having experience with different languages just broadens your horizons. Main advice: Try to pinpoint what it is that you like about Scala, and search for languages that have similar features. Maybe one of those languages will lack something Scala has, but make up for it by having something Scala doesn't have. You mentioned Java and Python, but both are far from providing the main things that draw people to Scala. If I were to change jobs now I would certainly aim to keep working with Clojure, but if that weren't possible I would try going for one of Scala, Haskell, Ocaml/ReasonML, F# or at least Kotlin, Elixir.
I'm noticing more and more remote Scala jobs if that's a possibility for you.
I went back to Java and barfed so then i switched to Python. Happy again. Honestly your coworkers make the job not the language. A good team in COBOL beats divas in Scala.
Have you looked at Typescript?
&gt; I also think its similar amounts of work to just reinventing the current java namespace The problem is that it's not either-or: You'll need to implement/support the Java packages anyway, as long as people are not explicitly made aware of their dependencies. Regardless of how good some new thing is, `java.something` will always win because it is imported by default. As long as this doesn't change, further considerations are pointless. &gt; I mean other languages have also implemented their own standard libraries from scratch, yes it takes time and effort but its not impossible Not in general, but for Scala it is. You have a team that can't even manage to keep minor releases compatible anymore, and most of the packages in the standard library have completely bit-rotted over the years.
From my haskell knowledge, which is limited, either: - They don't or - The same way using newtypes + mtl. Also, realize that `ghc` does a _LOT_ more for optimization for indirection (and the pointer chasing) than scalac/jvm does. i.e `StateT`, as someone pointed out to me earlier during the week, is rewritten in ghc into a continuation passing style happy path which runs really fast under that runtime (See [benchmarks](https://github.com/fosskers/scalaz-and-cats)). Haskell and optimizations around FP are in another world compared to what we have under scala. 
I think scala native has already fallen quite behind. I can compile a kotlin native app and run it on my windows 10 machine. I cannot do that with scala native. It's unfortunate - I prefer Scala to Kotlin... but it has to be available or I can't use it. I can use Kotlin native, therefore, that is what I will use.
&gt; Regardless of how good some new thing is supposed to be, java.something will always win because it is imported by default. As long as this doesn't change, further considerations are pointless. Well in Scala this has been slowly changing, people aren't using less and less `java._` libraries. Of course you will always have to depend on stuff like `java.lang.string`, but its been moving away from this slowly &gt; Not in general, but for Scala it is. You have a team that can't even manage to keep minor releases compatible anymore, and most of the packages in the standard library have completely bit-rotted over the years. Well the worst example of this is actually the `java` namespace, its the worstcase of bitrot in standard library because of their backwards compatibility stance.
Offer an interview :) 
I took up a java job after doing scala for 3 years. I had similar problems of no fitting scala roles here in Singapore. Hated it at first, my first 2 months were spent looking for scala jobs. But now after a year I don't hate it so much. Intelij does a good job of generating boilerplate and hiding it when you are reading the code. Again I focus on solving the big problem at hand. Also I don't miss the slow compilation times and breaking changes. Give the java role a try maybe the problem will be interesting enough for you to forget about the language flaws. My colleague who did the same introduced kotlin in his java team. Now he prefers kotlin over scala. I would suggest go for the role and the problem the company solves. That should bring you greater joy! My 2 cents. [ BTW I also like JavaScript when no one is watching :) ] 
Do u know any good book or tutorial to learn jdk10? 
Actually, there is pattern matching in C#, starting with C# 7.0, so there's that. 
Yeah, it's totally reasonable to write Scala without an IDE. I've been doing so professionally for nearly a decade.
&gt; Well in Scala this has been slowly changing, people aren't using less and less java._ libraries. Yes, that's why I have been suggesting hat the Java dependency is made explicit. You can't give people a "free" dependency and expect them not to use it. &gt; Of course you will always have to depend on stuff like java.lang.string, but its been moving away from this slowly That's only _one_ class of roughly hundred that are imported by default, and one of thousands available. Scala can, and should have, explicitly exposed the few classes commonly used instead of having a blanket import-everything approach. Hell, `String` is even half-way there with the type being `scala.String`, and only the value being `java.lang.String`. &gt; Well the worst example of this is actually the java namespace, ... That's why Scala should stop implicitly importing it into every single file. &gt; ... its the worstcase of bitrot in standard library because of their backwards compatibility stance. Oh, they have actually started to cleanup and deprecate things recently. From removing CORBA over deprecating `java.lang.Compiler` to the deprecation of the constructors of wrapper types ... some solid work there ‚Äì and more trouble for Scala, because what is in its global scope can now be very different based on the JDK version. 
Ill definitely consider doing what youre saying. I am afraid though that the code I'm working on doesnt have the best engineering practices and Spring actually makes things such a clusterfuck in attempt to make things more "simplistic". But I'll definitely make an attempt.
Did you feel like there were more Clojure jobs than scala? My fear is learning another niche language and then not being able to find a job in it. Although scala is hardly niche these days.
Go for it! It's definitely growing, specially if you also take ClojureScript into account.
The good thing about being niche is that there are few candidates who already have experience. I had only 2 to 3 months hobby clojure programming when I joined my current company. I think knowing functional programming already puts you ahead of a lot of other candidates.
I find the comparisons here to Checked Exceptions very unconvincing here. &gt; That's right, you'll have a Throwable on your hands. I mean sure, if you choose to. You could also choose a better type to capture all three error types. You could a coproduct with all three cases. You could make an your own sum type for describing the errors. This is the same weak argument people who misunderstand Either make, nothing changes because they're exceptions. &gt; So we are executing 3 operations in sequence and each of them can fail, we don't know which or how. &gt;Does it matter? Most of the time, you don't care. I mean, I do care how many different states of error my program can be in. I don't care to have those states interfere with describing with what my program is doing, but in the end I do care in how many states I can end up in for any input. &gt; Empirical evidence suggests that most checked exceptions in Java are either ignored or rethrown, forcing people to write catch blocks that are meaningless and even error prone. Yeah, what they're doing in Java is not convincing me at all they I should not care. &gt; The Error Type is an Encapsulation Leak Just use a better type to describe the error like the first problem. Parameterize over the the error type when you know or care about what the total type of the error is, and use `Inject` when you do care to handle a specific error out of a larger set of possible errors. Handling errors really any different how you describe the rest of your program. Maybe this would be more obvious to people if Sscala had a more ergonomic coproduct. &gt; The Bifunctor IO[E, A] looks cool, but what happens downstream to the types using it? Monix's Iterant for example is Iterant[F[_], A] I don't know, does `Iterant` care about the type of an error in any way? If not, what's wrong with just applying `IO[E, ?]` to `F[_]`?
Take a look at seek.co.nz lots of banks hiring - big data stuff mostly
Primary resources to fuel brain has been videos, books and online documentation.
 Hopefully this should give you enough to get started with a real API.
I am interested have DMed you. Thanks.
I spent the weekend playing around with Monix Task and wrote this blogpost http://justinhj.github.io/2018/05/05/hacker-news-api-4.html 
Except Tony Morris was already blogging about this in 2007: http://blog.tmorris.net/posts/the-power-of-type-classes-with-scala-implicit-defs/index.html
The original paper was called tcpoly.pdf by Odersky and others. While misattribution and other forms of deception are a long time tradition in Scala, this is not such a case :)
The last link is an odersky link from 06
I said it before, but I think the comparison to checked exceptions isn't all that convincing. Most of us are already using `Either` to represent failure in a typed way, there's `Try` as well, but using `Either` to be more precise seems to be the most accepted way of doing error handling without `IO`. If the idea behind checked exceptions (modeling errors as types) is generally a failure, then so are `Either` and `EitherT`, which I fundamentally disagree with. That said, I do agree with a lot of different points made in the article and you can see further discussion in this issue: https://github.com/typelevel/cats-effect/issues/189
You missed the point. If you have one operation yielding E1 and another yielding E2, you will need an E3 that describes both. Compose enough operations and pretty soon you‚Äôll have an error type that‚Äôs as specific as Throwable. 
Iterant is nothing more than a lazy List and all operations on Iterant are nothing more than simple folds powered by monadic binds. I bring it up because it represents a real world sample. If you can‚Äôt address a public, we‚Äôll establish, real world sample, one has then to wonder what the target audience is üòâ Errors in Java don‚Äôt compose less than EitherT, quite the contrary, the throws statement in method signatures giving you union types for free, unlike Scala‚Äôs type system. Checked exceptions in Java are as a matter of fact more composable, I challenge you to prove otherwise. All your disagreements are already addressed in the article. And your reaction is precisely why I wrote it.
I get the argument but to be honest I never found `Either` to be a good type for expressing typed errors. The problem is that an error is either an expected response type that you can act upon, or not. If you can't act upon it, you usually only want to short-circuit the rest of the computation until the point where you can trigger a retry of the whole routine, log the error, return an error code to the user and so on. The error itself is important for the developer to understand what happened and maybe to transform it into something that is immediately actionable. So that said, for error types that are actionable, I personally tended to come up with my own ADT that models not just the error type, but the successful result(s) as well. As an example of such a type, you could talk about having: sealed trait HttpResponse[+A] { def status: Int } case class Http20xSuccess[A](status: Int, body: A) extends HttpResponse[A] case class HttpError(status: Int) extends HttpResponse[Nothing] This is the kind of ADT that captures both successes and errors. The errors are actionable and you don't necessarily need an `Either` for it. `Either` can actually get awkward in such a case, because you might end up with multiple successes. In the case of HTTP responses, an HTTP 201 is just as successful as an HTTP 200. And it's debatable if HTTP 404 is an error, after all, you did a query and the server responded successfully to that query. Do you need to short-circuit an `HttpError(404)`? Maybe, but the short-circuit will be very specific, the branch that you pick reflecting the error condition ‚Äî i.e. if it's a web server, you'll probably pipe the 404 status to the user. But maybe you don't want to do that, maybe you want to treat `HttpError(404)` like any other kind of error, in which case you can easily transform that `F[HttpResponse]` into an `F[Either[HttpError, A]]`, or into an `F[A]` and treat it like any other kind of unexpected error. Personally I don't find the arguments pro `Either` to be very convincing for one because in my mind `Either` is nothing more than a way to build an anonymous, tagged, disjunction type and in expressing the distinction between successful results and errors, it's a pretty limited type. We wouldn't need alternatives to it, if this weren't true. And `EitherT` and the bifunctor `IO` are specializations on `Either`, specializations that don't help, because we're talking of `MonadError` here, aka a way to short-circuit the bind chains when errors happen, until the nearest recovery routine or the nearest finalizer. `Either`-imbued logic making use of `MonadError` is basically placing itself between a rock and a hard place, because if an error is important enough for the user to notice it, it's probably important enough for it to not be passed via `F.raiseError`, because users in most cases will ignore it. This is one argument against Java's checked exceptions btw ‚Äî it turns out most checked exceptions are not important enough to be checked exceptions and those that are important, users end up ignoring them as being irrelevant, raising the question ‚Äî why not return those errors as plain / `pure` values (and let's be clear about it, due to `MonadError`'s behavior, errors returned in `EitherT` / bifunctor `IO` are not behaving like plain values being returned from functions, they are behaving like exceptions).
Well, /u/tsec-jmc is not known for being constructive. See for example: https://www.reddit.com/r/scala/comments/88qlze/dotty_proposal_typeclass_traits/dwr9rzk/?context=3
It's impossible to have an error type that's as in-specific as throwable, so long as you stay within sealed trait hierarchies. Throwable is infinite, sealed trait hierarchies are not. It is _always_ possible to do something with _every_ specific case when using Either with Error ADTs, even if that space gets large. The same is not true with throwable. Also, if your error ADTs are getting massive, you may need to re-think what you're doing. You shouldn't have a specific error type for every error. You should have a specific error type for each error that you can reasonably handle. Others should go under some umbrella. If your error ADTs are huge, but you're handling every case, then your ADT is _necessarily_ huge.
bad bot
Thank you, Jacoby6000, for voting on agree-with-you. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I‚Äôve been reading a lot about Monix and other effect types recently and wrote a blog post about my experience so far: http://justinhj.github.io/2018/05/05/hacker-news-api-4.html
Hi, the best place to ask this question is on the Coursera forum.
[Firebase4s](https://github.com/firebase4s/firebase4s). If anyone knows, or would like to learn, about macros, I could use some help implementing support for case classes.
I contribute far more to the cats ecosystem than Scalaz. By far, I've PR'd way more to http4s, fs2 and tsec than I've ever PR'd to scalaz-anything. /u/alexelcu That said, I think your remark is fair. I can be sort of a dick when something annoys me (like that typeclass proposal did) and I shouldn't have written a reply out in anger like I did at the time. I apologize. That said, there's literally _way_ more than one discussion in which I'm involved in which I'm not writing it in that tone (more like the majority of them).
This month, I am putting into production three small web services written in Scala with Scalatra!
What do you mean reaction? Aside from my comment on Odersky, there's no personal attacks in there at all.
In making a portfolio for my gf with customized Bulma (which I just discovered, and I love).
&gt; Bulma What is Bulma?
https://bulma.io/
I assume it is because macro roadmap has scala supporting annotation macros.
I've been writing some tests recently at work and wanted to give a shout-out to https://github.com/commodityvectors/scalatest-snapshot-matchers , kudos /u/grillorafael for this gem of a library that makes writing tests easy and fun. I was looking for a snapshot testing solution for Java last year ( https://groups.google.com/d/msg/scalatest-users/yrEJmt7RBXk/mjQzKQdpDAAJ ) but couldn't find anything approaching Jest's ease of use ... until this. For people who don't know, and are wondering why you should bother, see the above message: &gt; I'm finding my aging and ever-weary eyes struggle to see what went wrong when I get failed assertions. With a snapshot matcher, you get a textual diff that tells you exactly what the difference is.
I agree on a fundamental level. That said, we cannot always afford autonomy: In the extreme, if you design and implement everything yourself, then you have autonomy, but on the flip side you are also condemned to stay autonomous because you will find it difficult to find help with your made-at-home code. 
Hi [/u/red_atlantis](https://www.reddit.com/user/red_atlantis). I'd pass the `HutRepository` as a parameter. In your example is not that clear cause you have everything in one place (HttpService and StreamApp), but you should be able to do something like this: Define your `HttpService` to receive the `HutRepository` as an argument: def service(hutRepo: HutRepository) = HttpService[IO] = { ... } Use `flatMap` to access the `HutRepository` and pass the argument on creation of the app: def stream(args: List[String], requestShutdown: IO[Unit]) = Stream.eval(HutRepository.empty).flatMap { hutRepo =&gt; BlazeBuilder[IO] .bindHttp(8080, "0.0.0.0") .mountService(service(hutRepo), "/") .serve } } Note that you need to import `fs2.Stream` for `Stream.eval`. You could also use a `for-comprehention`.
So what version would that be in, 2.13? Or a dot release in 2.12?
What do you mean remove actor? As in the akka stuff? That's just the wider context of the scraper/indexer It's redundant here I guess. 
I think you'll only get a certain distance while using Play - most functional-oriented people tend to move on to something else. (I've heard good things about https://www.slideshare.net/GaryCoady/http4s-doobie-and-circe-the-functional-web-stack but can't look at it at the moment). &gt; Even if we are using map, foldLeft and constant values with as less side effect as possible, we are not using functionnal design pattern, currification, etc. This is the wrong way to look at it IMO - the whole point of the functional patterns is to let you write your functions in simple input-output form, if you can do that you're doing functional programming. Complex patterns are a liability rather than an asset. A better starting point is: what's a function that you can't figure out how to write in a functional way, that seems to inherently need a side effect? Then you can look for a functional technique to solve that problem, and you'll be using that technique for a practical reason rather than for the sake of it.
&gt; in my mind Either is nothing more than a way to build an anonymous, tagged, disjunction type and in expressing the distinction between successful results and errors, it's a pretty limited type. We wouldn't need alternatives to it, if this weren't true. Either is indeed small and limited - that's a feature rather than a bug. Either is nothing more or less than a disjunction between values we want to short-circuit and values we don't. That's an extremely common pattern IME - even in a domain where you have several different error states and several different success states, you can usually draw a line between those states for which you want `f1 &gt;=&gt; f2 &gt;=&gt; f3` to carry on and apply `f2` and those where you don't.
The milestone for this PR is set to 2.13.0-M5.
Sort of, but not what that ticket describes. The goal there is to avoid having to list every participant in the dependency graph when they can be inferred at compile time.
&gt; you can try to introduce kotlin in a Java team. The transition shouldn't be hard, and it's a mid step between java and scala Protip: Don't do this. Java devs will happily stay with Kotlin as it introduces just a little sanity compared to Java, but keeps all the Javaisms.
thx for this info
Time is money. If they had a million dollars for the project they could hire more staff (to a degree, mythical man month yada yada)
&gt; With a snapshot matcher, you get a textual diff that tells you exactly what the difference is. ScalaTest has native support for something conceptually similar (wrt visualization): http://doc.scalatest.org/3.0.1/index.html#org.scalatest.DiagrammedAssertions
There aren't that many people able to develop a compiler, certainly not with very good understanding of JS, the JVM and able to garantee semantic preservation between the two.
More money cannot make me do anything more than I am currently doing. I am already devoting all my work time *and* some of my free time to Scala.js. If I want to keep a life, and I do, I cannot do more. What I'm saying it precisely: one doesn't need to be me to make this particular thing happen! It is a bit time-consuming and requires some experience with writing compilers, but it does not require a PhD in writing a compiler to JS.
&gt; Unfortunately, while it compiles and runs, the route doesn't resolve and I can't connect to it. Sounds like your problem is nothing to do with websockets or the code you're showing, and something at the routing level. Try swapping out just the one class/method between Java and Scala (i.e. have a mixed Java/Scala project). There must be something you're doing differently in the two cases. You can worry about the lambda stuff (under 2.11 it's probably a matter of compiling with `-Xexperimental`) after you've got the basics working.
I had a go with it, I didn't find it quite as intuitive as Jsoup but I think I probably should give it another go again now I'm starting to get the hang of this stuff. As said I'm in the learning phase of Scala right now. 
Even if we gather and buy you infinite amont of coffee and cocaine? On serious note: After you're done with PhD money-sharks (companies) will run to get you and pay you heft fat checks so you can write Scala for them, essentially leaving Scala.js
I think problem solving and exploring with `inspect` should be greatly expanded, and used repeatedly in the other examples. Want to add a resource directory when compiling? Use inspect on `compile` to find out how sbt finds resource directories. Add a bunch of plugins. One of them has added a dependency to my project. Use inspect to figure out which plugin it is. Use inspect to find why a setting has a different value in two different configurations. Teach a mad to fish and all that would go a very long way in this context.
Yeah. We all know monad transformers are inefficient. Recently I proposed another approach for extensible effects in [Dsl.scala](https://github.com/ThoughtWorksInc/Dsl.scala/). Would you like to contribute a benchmark in https://github.com/ThoughtWorksInc/Dsl.scala/wiki/Benchmarks:-Dsl.scala-vs-Monix-vs-Cats-Effect-vs-Scalaz-Concurrent-vs-Scala-Async-vs-Scala-Continuation , to compare the Scalaz 8 with Dsl.scala?
Note that Monix / Cats Effect / Scalaz Concurrent is implemented as `sealed trait` / `case classes`, along with some delicate optimization for the inner loop. What is surprise is that simple vanilla continuation-passing style functions have comparable performance to those delicate implementations (won in some scenarios, loss in others scenarios). On the other hand, Dsl.scala beats `for` comprehension and other direct style DSLs in computational performance, especially when you want to use different types of effects in one direct style control flow. 
I'll cut straight to the chase - what on earth is going on here? scala&gt; class A defined class A scala&gt; class B extends A defined class B scala&gt; val a : A = new B a: A = B@4ac68d3e scala&gt; val b : B = new B b: B = B@768debd What exactly is `a` - if `B` had methods would `a` have access to them?
This is really cool. Being able to script with scala is excellent for automation. 
&gt; What exactly is a - if B had methods would a have access to them? a has been upcasted from B to A, so it will not have access to methods from B, like so: scala&gt; class A defined class A scala&gt; class B {def func: Int = 1} defined class B scala&gt; class B extends A {def func: Int = 1} defined class B scala&gt; val a : A = new B a: A = B@6a98f353 scala&gt; a.func &lt;console&gt;:13: error: value func is not a member of A a.func ^ scala&gt; val b: B = new B b: B = B@615b5480 scala&gt; b.func res1: Int = 1 you can downcast it using pattern matching or by calling asInstaceOf like so scala&gt; a.asInstanceOf[B].func res2: Int = 1 scala&gt; a match { case downCastedA : B =&gt; downCastedA.func } res4: Int = 1
Ah I see, I didn't realise that : also acted as syntax for casting Thanks very much!
They will also learn how to pass options to the Scala compiler (scalac) from the Scala Build Tool (SBT).
Great talk, thanks! 
@SystemFw the main reason to do compile drain is to return a singleton Stream of Unit, this usually makes more sense when composing inside our applications. Otherwise we found ourselves doing `.fold(()){ (_,_) =&gt; () }` but it is definitely something we would like to know if there is a better solution.
@SystemFw the main reason to do compile drain is to return a singleton Stream of Unit, this usually makes more sense when composing inside our applications. Otherwise we found ourselves doing .fold(()){ (_,_) =&gt; () } but it is definitely something we would like to know if there is a better solution.
Eh, I don't think it should matter whether a `Stream[F, Unit]` is a singleton or not in most cases, I'd be curious to know why you need that :) In any case the code above doesn't guarantee that the result is a singleton of Units anyway (you are still `evalMap`ing which happens on each element, so you are going to have fewer Units being emitted, but not necessarily one). That being said, you can replace `.fold(()){ (_,_) =&gt; () }` with `.foldMap(_ =&gt; ())` or `.void.foldMonoid`, which should be nicer on the eye. In any case feel free to hit me up on Gitter if you need, and again, cool project!
That's fantastic! I'm enthusiastic about ammonite since it offers for scala what I consider one of the main arguments for using python: a seamless path from script to application! I was bummed out when it didn't work well with intellij back when i tried, but will certainly try again now :)
So I can't use Intellij's Scala worksheet?
Note, it's for _up_ casting only because a subtype always safely conforms to its supertype.
Scala function compatibility with Java lambda requires Scala 2.12+. I would guess somehow that means your app.ws call doesn‚Äôt do what you intended.
I was hoping /u/lihaoyi would chime in the comments, but then saw that he posted this to scallit
When you are using Ammonite inside an sbt project, you may want to use classes in the project. [sbt-ammonite-classpath](https://www.reddit.com/r/scala/comments/8i3x14/sbtammoniteclasspath_an_sbt_plugin_to_export/) could help in the scenario. ### Generate the classpath file at `target/scala-2.12/fullClasspath-Runtime.sc` ``` sbt Runtime/fullClasspath/exportToAmmoniteScript ``` ### Import the classpath by adding the following statement in your Ammonite Script ``` scala import $file.target.`scala-2.12`.`fullClasspath-Runtime` ```
I think the main result of the performance difference is just due to how bad Scala's `for` comprehension desugers into `map`/`flatMap` calls, and this is already a well known limitation of current Scala. There are even plugins to rectify this (see https://github.com/oleg-py/better-monadic-for). Would be interesting to see bench marked results against hand optimized `map`/`flatMap` rather than just `for` comprehension
Yeah I suspect that this is due to boxing, I was just making a point that the use of `for` seems to be creating the biggest slowdown (at least from what I can interpret from the benchmarks)
The slowdown of `for` comprehensions is due to two reasons, as I wrote in the benchmark report: &gt; In general, the performance of recursive monadic binds in a for comprehension is always underoptimized due to the inefficient map. and &gt; Despite the trivial manual lift calls in for comprehension, the monad transformer approach causes terrible computational performance in comparison to manually called traverseM.
Yes I read this, in general * Monad transformers are terrible for performance (aside from their design problems). There is already a lot of material on this and is becoming more well known, http4s recently has changed their entire project to use tagless final (or in more concrete terms, they encode against a `[A: Async]` to avoid using monad transformers when possible) * Scala doesn't do any kind of map/flatMap fusion to avoid unnecessary map's/boxing/allocations (unlike Haskell) because it doesn't have information on what is safe to transform and what isn't (this is what I meant in my earlier remark, sorry for not being so clear)
I'm not sure if ensime is actively maintained at all; certainly the previous maintainer is no longer maintaining it. If your concern is maven integration specifically, I've found eclipse/scala-ide (with m2eclipse-scala) handles that significantly better than any alternative (though bear in mind that's also in a not-great maintenance situation). But it sounds like your issue is more about general IDE performance? Many (maybe all) IDEs are slow when working with large Scala codebases. I find IntelliJ is mostly ok in speed terms but sometimes inaccurate with error highlights; eclipse/scala-ide is accurate and fast most of the time but then has moments when the entire UI locks up for seconds or more. I haven't worked with VS Code. Honestly I'd suggest trying all of them on your codebase, as different people's experiences seem to differ quite a lot.
Easy to check, on github ensime server was updated 7 days ago, ensime-emacs 8 days ago, hardly looks like something that is abandoned.
Where is the correctness and maintainability when you compare this: ``` // Cartesian product for based on `ListT` transformer, in one function def listTask(rows: List[Future[Int]], columns: List[Future[Int]]): Future[List[Int]] = { for { taskX &lt;- ListT(Future.successful(rows)) taskY &lt;- ListT(Future.successful(columns)) x &lt;- taskX.liftM[ListT] y &lt;- taskY.liftM[ListT] r &lt;- ListT(Future.successful(List(x, y))) } yield r }.run ``` and this: ``` def listTask: Task[List[Int]] = reset { List(!(!Each(inputDslTasks)), !(!Each(inputDslTasks))) } ``` , in spite of the performance issue.
https://www.scala-lang.org/api/current/
No additional comments to the article, but I must say that this thread has been really refreshing to read. I see a lot of mutual respect and good discussion on contentious items. Its really interesting to follow the conversation! Ya'll are good people. :)
This setting is project-wide, so you may have them in different projects. Also, there is an option to enable Ammonite in test source roots only.
Seconded. Would be great to see some deliberate benchmarks. While the logic all makes sense here, my attempt to test this approach against a real-world transformer stack introduced too many new factors that cloud the results (eg synchronized mutable state to replace StateT in a context that supports concurrency) and my first experiment actually performed 1.5x slower than the equivalent transformer based code. A minimal example comparing performance stats between a believable transformer stack and the equivalent solution following the approach proposed in this article would make it far more valuable and actionable.
Autoscout24 | Scala / Software Engineer | Munich, Germany | ONSITE | Full Time | Competitive I'm working as a developer there and am looking for future team mates to help with migrating some key components of our platform to Scala. The company has a huge non messy AWS infrastructure and a lot of developers dedicated to produce good software. Tech we use (non exhaustive): Play2, Cats, Shapeless, Monix, Circe, PureConfig, AWS (Lambda, most other available services) Message me if you're interested.
"Learn Scala the Hard Way". I'm Ok with dat. Is there a way to search for help locally? 
Since when reading API documentation is "the hard way"? Try a tool like https://kapeli.com/dash or the community edition of IntelliJ.
&gt;class Service\(requestId: RequestId, httpClient: HttpClient\) { &gt; &gt; &gt; &gt; def foo = httpClient.POST\(requestId, ???\) &gt; &gt; do bar = httpClient.GET\(requestId, ???\) &gt; &gt; &gt; &gt;} Done. But feel free to use implicit parameters, macros, annotations, reflection, Dotty's implicit function types; whatever you need to avoid passing in those arguments. I just haven't personally found a non\-contrived example of real benefits.
Even with your code, correctness and maintainability favour the first over the second, since it's clear what everything's doing and normal refactoring still works. But I think you've unnecessarily overcomplicated the code to distort the comparison; one would normally write something more like: def listTask(rows: List[Future[Int]], columns: List[Future[Int]]): ListT[Future, Int] = for { x &lt;- ListT(rows.sequence) y &lt;- ListT(columns.sequence) r &lt;- List(x, y).liftM[ListT] } yield r And really the assumption that you'd have a `List[Future[Int]]` in the first place is pretty dubious; unless you're switching between transformer- and non-transformer styles in the middle of your application, you'd likely have `ListT`s already, and then the function is just `(rows ‚äõ columns)(List(_, _))`.
Since always. Reading a highly technical description of a language to learn it is not an easy way to learn --some would say its stupid. It's like trying to learn Chinese from a Chinese syntax treatise. Thank you kindly for the pointers.
Yeah you should be able to do that, and there's no reason in principle why you can't. The pieces just aren't assembled that way for whatever reason. +1 for Dash 
&gt; It's like trying to learn Chinese from a Chinese syntax treatise. No, it is not. It's nothing like that. API documentation is not a syntax treatise, that would be the language spec. If anything, they are like a dictionary. And no one learns a new (human) language without a dictionary at hand. Stupidity is not using the best resources we have available. Stupidity is ignoring the only official source of truth we have.
Thank you, I think the reason why it was too slow was that I tried to create `.ensime` using maven. Emacs ENSIME with sbt project provide satisfactory experiences with even huge code base like Apache Spark. Maybe for now I can stay with Emacs
Sbt by examples was really helpful. It shows me some tricks. 
Although we have differences in the question of what is good code. But `List[Future[Int]]` in the first place is not dubious. When you are creating server side applications or web services, it is very common to collect data from multiple remote endpoints / multiple SQL queries. In addition, if those requests are independent, you may want to perform them in parallel. This parallel execution behavior can be achieved by replacing `Each` to `Fork`: // Cartesian product in Dsl.scala, in one function def listTask: Task[List[Int]] = reset { List(!(!Fork(inputDslTasks)), !(!Fork(inputDslTasks))) } However, the monadic approach for parallel execution is not that simple. Scalaz's approach is providing a tagged type `Parallel`, requires the user to manually marked `Parallel.apply` and `Parallel.unwrap` everywhere, and code in this approach is difficult to be placed in `for` comprehension. We wrote [a lot of code like this](https://github.com/ThoughtWorksInc/DeepLearning.scala/blob/d380d18/plugins-TensorLayers/src/main/scala/com/thoughtworks/deeplearning/plugins/TensorLayers.scala#L171), and we felt the pain. 
late to the party, but I think [Exceptions Are Shared Secrets](https://existentialtype.wordpress.com/2012/12/03/exceptions-are-shared-secrets/) by Bob Harper is relevant here
&gt; Eh, they were the state of the art at the time, and honestly I still find them clearer than tagless final style in a lot of cases. I don't know why'd anyone write a large application with naked transformers, even Haskell community recognized that using transformers without abstraction is a huge maintainability risk (inability to change underlying monad without rewriting the app) which is why since inception transformers were shipped together with tagless final abstractions (mtl == tagless final)
In the interest of a more fair outcome, do you mind running the benchmarks with a more up to date [Scalaz IO effect](https://github.com/scalaz/ioeffect)? 
Outside of Akka, the only other thing I know is scalable (because it‚Äôs from and used by Twitter) is Finagle: https://twitter.github.io/finagle/ I personally haven‚Äôt used it, but I think it has a solid and simple design, a client and a server are are mirror images of each other and both modelled by the same type: `Request =&gt; Future[Reply]` Just did some research and it looks like there‚Äôs a service discovery solution for Finagle servers: https://github.com/ncbi/finagle-consul
Is there something about Akka in particular you are trying to avoid? It is a good tool for building sharded statefull applications As mentioned before finatra works quite well but unclear what service discovery would look like as I've never had to handle that myself in finatra
&gt; But List[Future[Int]] in the first place is not dubious. When you are creating server side applications or web services, it is very common to collect data from multiple remote endpoints / multiple SQL queries. Sure, but I'd usually move the `Future` to the outside at the point where I was forming the `List`. E.g. rather than doing `userIds map fetchUser` I'd do `userIds traverse fetchUser` and get a `Future[List[User]]` at that stage. &gt; In addition, if those requests are independent, you may want to perform them in parallel. This parallel execution behavior can be achieved by replacing Each to Fork: The choice between eagerly parallelising as an optimization and requiring explicit parallelization is orthogonal from the choice between monad transformers and other approaches. There are arguments for both sides: if you do ad-hoc parallelisation as some `Future` implementations do then you may get better performance, but run the risk of losing that performance when you do even simple mechanical refactors. (The idea of my [tierney](https://github.com/m50d/tierney) is to make it easier to combine parallel and sequential operations while making the steps between the two as lightweight as possible, but it will always be true that you can make the syntax even more lightweight by not requiring any explicit distinction at all - my experience is that doing that always comes to bite you as the application is maintained and evolves, though).
Nothing particular, I'm just trying to learn something new.
Ok, I have to check. I thought Finagle is kind of http framework for stateless microservices. I'm interested how is pub/sub pattern is implemented and where do they store states.
Hi /u/yang_bo, Some notes on your Monix testing: - you're not testing with the latest 3.x release, which has optimizations for `map` for example, or the latest cats-effect 1.0.0-RC1 (although this one is the same as 0.10) - when benchmarking Monix, you need a `Scheduler` configured for `ExecutionContext.SynchronousExecution`, otherwise you're comparing apples with oranges, unless the other abstractions also deal with fairness concerns and they don't - in the context of IO/Task implementations, testing a bind chain of a small size, like your size=50 is ... less than ideal, because values of these data types end up being composed in large ones that are preferably evaluated only once in a `SafeApp` - testing with `ListT` and then seeing that Cats doesn't have a `ListT` should highlight that we probably don't care about `ListT` ‚Äî actually in such cases you're testing Scalaz vs Cats, which isn't the test that the benchmarks are claiming to do, so either build your own data structures that can work for all, or drop such tests - that goes for other helpers, like `traverseM`, `flatTraverse`, `liftM` or others, because you're effectively testing the performance of the provided type class instances ‚Äî which in the case of Monix 2.x for example, the technique used to provide Scalaz instances is very inefficient Also, the "*Dsl.scala's !-notation*" is cool, but in the context of IO/Task data types it's irrelevant. That's because for such data types it's only `map` and `flatMap` that matter, due to the way they end up being used. You can't use anything else in recursive data structures. I also don't understand if you're abstraction suspends side effects or if it's a pure data type. In general when you test performance, it needs to offer a comparable feature set, unless it's an upgrade. For example Monix's `Task` can be shown how it compares against `Future`, because it's a higher level type than `Future` and usable in more contexts. On the other hand you can make an eager `Future`-like data type be very fast, if you let go of certain assumptions or behaviors, see [trane.io](http://trane.io/), which can be faster because it doesn't really do the same thing. And notably absent from this tests is the performance on error handling.
It's because the bulk of large data tasks are usually somewhat asynchronous compared to the needs of real time applications, so most of the examples are going to revolve around that. I'm not saying it's impossible, I'm not knowledgeable enough to know if it's a completely bad fit. 
&gt; even Haskell community recognized that using transformers without abstraction is a huge maintainability risk (inability to change underlying monad without rewriting the app) "rewriting" is a gross exaggeration. If you want to change the type you just... change it. Our languages make that easy. Public libraries written to be reused in third-party applications should use mtl-style abstractions or equivalent. But within an application (or even an internal library, if it shares maintainers and release cycles with the applications it's used in) I don't see anything wrong with just using the monad transformers directly; doing that makes the code simpler and clearer and lets you save your indirection budget for places where you'll get more out of it.
Sam may have stepped down as a maintainer, but that doesn't mean improvements, pull requests, or development has stopped. I use ensime for work, and it's been fantastic. In many cases, more reliable than Intellij or Eclipse.
I think akka-cluster is one of the best tools out there. https://vertx.io/ is another JVM friendly option you could look at?
Pull request is welcome. The only concern is that I am not sure whether Scalaz 8 has enough features to complete the benchmark.
Cool. I used to think Scalaz IO only works with Scalaz 8.
It's been fairly recent, but everyone was wanting a new IO to rival @alexelcu's work on Cats Effect. Performance is roughly the same, and even does slightly better in some cases when compared to Cats Effect (which does slightly better in other use cases). That being said, your benchmarks need some serious review - Alex is right when he says the benchmarking strategy is nonsensical in many ways.
/u/alexelcu said I should not use `traverseM` in Monix, and /u/m50d proposed a input type that requires `traverseM`. Intersting...
I guess, /u/alexelcu would suggest a `map`ing `List[() =&gt; Task[Int]]` to `List[Task[Int]]` then calling `sequence`. However this seems too weird when other effect systems use simple `flatTraverse`.
&gt; It sounds like you thought I did not configure Scheduler for Monix. I do not see the `ExecutionModel` being specified and by default it is always `BatchedExecution`, which as the name suggests does execution in batches, that no other implementation is doing, trading some throughput for some fairness. See: https://monix.io/docs/3x/execution/scheduler.html#execution-model
&gt; I do not see the ExecutionModel being specified and by default it is always BatchedExecution, which as the name suggests does execution in batches, that no other implementation is doing, trading some throughput for some fairness. Does that mean the performance of `BatchedExecution` will slowdown a little due to internally count the number of tasks executed, in comparison to `SynchronousExecution`?
Well if you're benchmarking readability you should compare code optimized for readability, and if you're benchmarking performance you should compare code optimized for performance; it's not that surprising if how you do those is different.
What is it you're interested in doing? How you do something at "scale" greatly depends on what it is you're trying to do. You mentioned websockets, so let's pretend you want to build a chat application at scale (and pretend you don't want to use any actors). You could do this simply with a Scala native http sever that supports websockets, and a Kafka cluster. An aside as you mentioned online games, most online game servers are still run from one stateful process. Network latency is just too high to manage really real-time distributed state.
Have you worked through the requisite [hacks](http://ensime.github.io/editors/emacs/hacks/) and troubleshooting guide for Ensime? 
Just download it and rename to `amm.bat` :\)
Well, I‚Äôm dumb af. r/lostredditors
Finagle has no implications on statefulness of your Microservice. You bring whatever state management you want to it. Finagle has very good service discovery, load balancing, back pressure and kill switch built in. It is also heavy on Future. If you want pub sub at scale (like, really really big scale), what you want is probably a message broker, Kafka comes to mind, RabbitMQ is another. There‚Äôs also Kinesis and google pub sub if cloud lock-in is not a concern. 
Agreed. Let me be honest: there's a portion of the Scala community I've actively avoided in the past decade just because of some aggressive and negative behavior that I didn't want to be anywhere around. My mistake \(among many\) was to lump John De Goes in with them. He's a real boon to the community: his criticisms are constructive, and he contributes a lot of excellent code. He makes things better. And, I'm really enjoying this [IO](https://github.com/scalaz/scalaz)/[Tasks](https://github.com/monix/monix)/[Arrows](https://github.com/traneio/arrows) arms race that seems to be happening. I haven't seen anything like it in a programming community: several excellent libraries, neck\-and\-neck, just constantly one up'ing each other in a friendly competition. &gt;I still hate the pervasive use of operators over named functions Also agreed :\)
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/scalaz/scalaz) - Previous text "IO" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Boxing. Boxing everywhere. Java checked exceptions were supposed to do basically this. Unfortunately, nobody bothered integrating them with Java generics, making them a mostly-useless irritant in modern Java code. I'm of the opinion that Scala should reintroduce checked exceptions, but integrate them into the type system. A method that returns `A` or throws `E` would have the return type `A|throw E`: def readAThing(in: InputStream): AThing|throw IOException = ‚Ä¶ These could be used in generic types. For instance, this returns an `Iterator` whose `next()` method may throw `IOException`: def iterateOverThings(in: InputStream): Iterator[AThing|throw IOException] = ‚Ä¶ Thrown exceptions could be passed around as values, like `scala.util.Try` but more transparently (and preferably without boxing): def processThingOrException(t: AThing|throw IOException): Unit = t match { case thing: AThing =&gt; ‚Ä¶ case e: throw IOException =&gt; e.printStackTrace() } // This will read a thing, then pass the thing or the thrown exception to processThingOrException. processThingOrException(readAThing(in))
[Exceptions Are Shared Secrets](https://existentialtype.wordpress.com/2012/12/03/exceptions-are-shared-secrets/) In the blogpost, Bob Harper is stressing the importance of exceptions as "shared secrets", shared by the thrower and catcher. On the other hand, the intermediary code is oblivious of such exceptions that could be thrown "through" it. Could something like that be achieved here, if I have a type `IO[E, A]` for some abstract type `E`, so that nobody can interfere/interact with the exception `E` potentially being thrown though it? As far as I understand it, the existence of `attempt` (`IO[E, A] -&gt; IO[E2, E \/ A]`) would break this principle of non-interference, since it enables the intermediary code to "mess" with the exception, even though it doesn't know the *class* of the exception object (ie is not supposed/can't decipher the exception).
This is admittedly the first time I'm hearing about unrecoverable errors and a supervision hierarchy for fibers. Has this been the intended design all along, or is it a more recent development? I guess my other question is, what can you do with supervision functions? Can we implement restarting with an exponential backoff, for example?
Lagom
This looks pretty cool!
This is the exact same issue we have in our project, when the article states that &gt; This myth is false because users themselves define how error types compose, and they are not required to use common supertype composition. For example, I can easily combine two effects IO[E1, A] and IO[E2, B] with different errors in a lossless way using a disjunction type, such as E1 \/ E2. In reality this myth is not false, because if we were to do this in our application we would probably have something like `E1 \/ E2 \/ E4 \/ E5 \/ E6 \/ E7 \/ E8 \/ E9 \/ E10 \/ E11 \/ E12 \/ E13` all the way up to E20 in our program (and yes, all of these `E`'s are actually sealed traits, non trivial applications in our case usually have ~80 types of recoverable errors) In our case, we just made all of our `E`'s subclass a `GeneralError` trait, although I doubt this is what people would call purely function. This really isn't a myth, sure in your trivial program you may only be dealing with one or two error types but this isn't the case in real world programs.
I am having issue with sbt 1.1.4 and sbt 1.1.5 that it exits immediately after printing [info] Loading settings from idea.sbt,plugins.sbt,global.sbt,watch.sbt ... [info] Loading global plugins from /Users/aterefe/.sbt/1.0/plugins [info] Loading settings from play.sbt,plugins.sbt ... Does anybody else have these issues? I have tried it with both play and http4s projects. 
Very precise and thorough explanation of what's been my largest gripe with the bifunctor approach. Something like row types could ease this situation but of course also have additional complexity cost (I've written a lot of PureScript and `Eff` has caused me some frustration with unhelpful error messages). 
It's no less possible with `IO[E, A]` than it is with `IO[A]`, since `IO[E, A]` subsumes all capabilities of `IO[A]`. That said, you can probably do better by using existential types, which hide information on the exact error but allow (safe) recover for parts of the application that know about it.
This decision happened early on. In an early (unpublished) draft of 8 IO, it was possible to lose errors, but many programmers pointed out how difficult this makes it to diagnose problems. The supervision hierarchy directly came out of those discussions, with a goal to ensure no errors are ever lost. With the change to bifunctor IO, the model was cleaned up a lot because of the new precise separation between recoverable errors and non-recoverabler errors.
My high-level response to this objection is as follows: 1. Bifunctor IO doesn't force you to model errors precisely. You're still free to use Throwable. Moreover, you don't have to make a single choice for your whole application. You can vary the choice depending on what component and layer you are working in. 2. Most applications do not want to preserve all errors up the stack. It's much more common to expand and contract error sets throughout an application. For example, a bunch of low level code to import a file may fail in many ways, but at a higher level and beyond just logging, you only care if it succeeded or not, you don't care if it failed due to a file being locked or permissions being wrong. 3. There are already techniques in Scala to model sets of errors in a relatively pain-free fashion. For example, you can use subtyping to model your hierarchy exactly, because a single error class can participate in multiple subtrees. Also, there are ways of dealing with sets of errors, such as Jon Pretty's use of intersection types to model sum types, and HList-like solutions. If module-style programming were more common in Scala, you could also use lenses or at least abstract error types to solve (3) in some pretty novel ways. These options aren't available in Haskell. Given you don't have to use precision anywhere you don't want, given most apps both expand and contract error sets (not just expand, like your example suggests), and given there are better ways of dealing with error composition than just wrapping in `Either` or endless custom sum types, I think there's hope that bifunctor IO will work for a lot of people skeptical of statically-typed errors. Certainly, a lot of pure FP shops in the wild are already using `EitherT`, so for them, it's a no-brainer. Those not using `EitherT` will have to see the benefits outweigh any inconvenience, which I think is true in a lot of cases with the best approaches.
I think this is an area where subtyping may help, given all of its problems. Row types is of course a more limited subset of subtyping
In the docs there's a "plot catalog" which demonstrates a few more plots. https://cibotech.github.io/evilplot/plot-catalog.html
You're in the wrong Reddit channel, try here -&gt; https://www.reddit.com/r/java/
&gt;If module-style programming were more common in Scala, you could also use lenses or at least abstract error types to solve (3) in some pretty novel ways. These options aren't even available in Haskell. What do you mean by that? Could you perhaps link to something about this? 
http://lmgtfy.com/?q=%7C%3E+scala This is an annoying meme.
But it needs a Java language change. Or at least a JVM change. And if we allow for that, then we can imagine that the same change makes Either[E,A] zero-cost implementable too.
Don't limit the list by 1000 items, just filter out all which are not developed last year
Great, now just a few more for Google to go. (And that'll surely make all the other alphanum optimized search engines of the world [github, stackoverflow] smarter too.) &lt;~&lt; Liskov, 6th place (a typelevel blog post), but no link to docs :+:, coproduct 1th place ==&gt;&gt;, &lt;=&lt;, et al., nothing on the first page \&amp;/ scalaz on the first page, but that's it, no link to the doc
It may be possible to model effect handlers purely at the type level, i.e. with only compiler changes.
I did post there but couldn‚Äôt get the problem solved. Thanks for your suggestion!
... what?
I built an in memory single site crawler/command line search using Akka for an interview for first scala job that I'll probably start extending to using hadoop and crawling multiple sites in personal time that I'd love to have pulled apart by people who know what they're doing [https://github.com/TRReeve](https://github.com/TRReeve)
Hope you get the job, good luck! 
I have other irons in fire with python but thanks. 
Please post jobs in one of the forums mentioned in the sidebar under the section 'Scala Jobs'.
It is not a bug if it is documented: &gt; Unicode conversion is done early during parsing, so that Unicode characters are generally equivalent to their escaped expansion in the source text https://www.scala-lang.org/files/archive/spec/2.12/01-lexical-syntax.html There should also be something about it here: https://docs.oracle.com/javase/specs/jls/se8/html/jls-3.html
Yep same situation for us but our error is called `BaseError` :D
 ¬Ø\\\_(„ÉÑ)_/¬Ø Reported this 8 years ago. Decision was to not improve over Java.
&gt; For example, you can use subtyping to model your hierarchy exactly, because a single error class can participate in multiple subtrees. I can see it working. For business logic errors exhaustive checking is important, so you will need to define your sealed (related) errors all in one place, which is acceptable. Jon's [Totalitarian](https://github.com/propensive/totalitarian) library looks pretty cool too! &gt; EitherT When using EitherT, upcasting a subclass of `E` to `E` is a huge overhead and is one of the primary reasons why I stopped using it. However, with some macro that can generate constructor functions for your E subclass instances, this should no longer be an issue. (Assuming IO is invariant in both type params here which I think is a fair assumption :)) I'm imagining something like this, implementing the example in my original post. @generate object Errors { sealed trait ErrFuncA1 sealed trait ErrFuncB1 // This feels wrong - C2 should not need to know about its caller A1 sealed trait ErrFuncC2 extends ErrFuncA1 sealed trait ErrFuncD2 extends ErrFuncA1 with ErrFuncB1 sealed trait ErrFuncE2 extends ErrFuncB1 case class AnErrInC2(str: String) extends ErrFuncA1 with ErrFuncC2 case class AnotherErrInC2(str: String) extends ErrFuncA1 with ErrFuncC2 case class AnErrInD2(str: String) extends ErrFuncD2 with ErrFuncB1 with ErrFuncA1 case class AnErrInE2(str: String) extends ErrFuncE2 with ErrFuncB1 // macro-generate functions to make errors to avoid needing to upcast def anErrInC2(str: String): ErrFuncC2 = ??? def anotherErrInC2(str: String): ErrFuncC2 = ??? def anErrInD2(str: String): ErrFuncD2 = ??? def anErrInE2(str: String): ErrFuncE2 = ??? } import Errors._ object BusinessLogic { def funcA1: IO[ErrFuncA1, Unit] = { for { // Still need to upcast errors here _ &lt;- funcC2().upcastError[ErrFuncA1] _ &lt;- funcD2().upcastError[ErrFuncA1] } yield () } def funcB1(): IO[ErrFuncB1, Unit] = ??? def funcC2(): IO[ErrFuncC2, Unit] = { if (true) { IO.raiseError(anErrInC2("oops")) } else { IO.raiseError(anotherErrInC2("another oops")) } } def funcD2(): IO[ErrFuncD2, Unit] = ??? def funcE2(): IO[ErrFuncE2, Unit] = ??? } Not too bad. I've prototyped the macro a while ago using scalameta (I called it the "adt" macro) for single sealed parent case. Will be more complicated for multi-parent case but not impossible. Overall, this looks promising and hopefully we can move towards better error handling without making our eyes bleed and head hurt :) If you want to compile my code above you can use my better implementation of IO :) case class IO[E, A]() { def flatMap[B](f: A =&gt; IO[E, B]): IO[E, B] = ??? def map[B](f: A =&gt; B): IO[E, B] = ??? def upcastError[EE](implicit ee: E &lt;:&lt; EE): IO[EE, A] = ??? } object IO { def raiseError[E, A](e: E): IO[E, A] = ??? } 
I've got a tiny example compiling with John's suggestions. Not too bad in my books
Yes, the bytecode is the important for performance.
Here it is, including using of a native transport (epoll in my case): https://github.com/plokhotnyuk/play/blob/master/src/main/resources/application.conf#L50-L51 For GET request I get up to 120K req/sec with GraalVM on my notebook. Results are here: https://github.com/plokhotnyuk/play/tree/master/results/wrk2
This is absolutely fantastic! Would it be difficult to support in https://scalafiddle.io/ ? scalafiddle supports medium, so it would open the possibility of making the code examples interactive.
It's not executing comments as code, it's reading newlines as newlines.
This is no longer the case (at least for thrush) https://github.com/scalaz/scalaz/pull/1739 If you see a symbolic operator with no alias, but you have a good alias in mind, make a PR!
Seems like union types in Dotty might provide a way to avoid type widening during composition. But from what I've read, it sounds like the union type would have to be explicitly declared as the error result of the composition. 
Merged 2 days ago, sneaky! Also thanks for the comment! 
I think this is one of the problems that's solved by using functional programming. I'd use `cats.effect.IO` or `monix.eval.Task` to model your program, and you would be able to tell by reading the code that no threads / executions will be fired off without you being explicitly in control. I'm currently working with the horrible `aws-java-sdk` that uses Java Future and I'm wrapping everything in a parametric `F[_]` that is interpreted to `IO` at the edge of my program. With `CompletableFuture` is much easier to wrap it in `IO`.
Well, with functional programming in Scala, you would assume this was correct, but it's not 100% correct. Java allows you to build the full list of CompletableFutures using functional methods, but it's not truly functional. I agree with you, though, that Cats/Task is a good idea. I have created my own threading model and job/task building model using functional, and it works great. (I even have several tests outlining this, and will have real-world tests soon.) I encourage you to take a look and see - you may find it helps you out. I'll be writing a Quick Start guide this weekend and more documentation. There's just a few more touch-ups before I issue 0.0.1 release of the lib.
You could use `new CompletableFuture&lt;Void&gt;()` as a trigger and use the obtrude method to fire it. If you add a trigger guarding each task, you could inspect a pause flag and queue up the work instead, firing when resuming. This way you can delay the task dependencies from being executed until you‚Äôre ready.
I love a good Future[Either [E, A]]
Beware, the code doesn't match the plots. e.g. the code for "Clustered Bar Chart Demo" is used four times in different graphs. The contour plot code also doesn't look right - surely is uses more than 100 data points.
Would you please explain the difference between the "combinator pattern" and "monadic composition" or monads in general? I'm new to FP.
&gt; Well, with functional programming in Scala, you would assume this was correct, but it's not 100% correct I meant functional programming as in "pure functional programming" which in Scala is accomplished by using either `Cats` or `Scalaz` and a lot of discipline :) &gt; Java allows you to build the full list of CompletableFutures using functional methods, but it's not truly functional. You don't need to use the Java API as they designed. I'd rather threat the building of a `CompletableFuture` as a side-effect meaning that I'd suspend the effect in `IO` or `Sync[F].delay(buildCompletableFuture)` being generic. &gt; I have created my own threading model and job/task building model using functional, and it works great. (I even have several tests outlining this, and will have real-world tests soon.) That'd be interesting to see. There have been a lot of competition recently between different `IO` designs and I think this only brings awesomeness to the community :) [These benchmarks](https://github.com/ThoughtWorksInc/Dsl.scala/wiki/Benchmarks:-Dsl.scala-vs-Monix-vs-Cats-Effect-vs-Scalaz-Concurrent-vs-Scala-Async-vs-Scala-Continuation) for example. 
I forwarded that on to the team. It might be a good idea to build an issue for that?
Sure! Both of these are compositional techniques for building complex structures from simple pieces. Combinators are a fairly general class of things whereas monads are a more specific technique. The big difference between them though is that combinators, by design, do not include "name binding" whereas monads include name binding. The big difference here is that "name binding" allows you to structure a piece of code like a directed acyclic graph and without it you can only make a tree. Generally, simpler construction techniques lead to more power in "interpretation", so the combinator pattern choice here is part of why it's easy to run EvilPlot on both ScalaJS and Java2d. Defining all of those terms would take a little time, but that's a high-level overview.
According to the [contributing guidelines](https://github.com/scalafiddle/scalafiddle-editor/blob/master/CONTRIBUTING.md#supporting-new-libraries), one should just add the library to the [libraries.json](https://github.com/scalafiddle/scalafiddle-io/blob/master/libraries.json) configuration file and submit a pull request.
Thank you! If I understand it correctly, the main difference is that monadic interface would allow not only combining primitives, but also their splitting, which probably wouldn't have much practical use anyway.
Well, remember that I'm trying to solve a problem by being a lean API, not the best, or fastest. :) (But if my design turns out to be faster than these, I'm happy!) The other things I could do is create a trigger with a ReentrantLock (as other libs do) to control pause and start with sync. Other things I could do is create a file lock to do the same thing, but that requires FS access, which is messy at best. I may do similar to IO/Monix and create an internal DAG and fire-and-forget tasks, but this seems to be more controllable using the PauseableThreadPoolExecutor that I found. Besides, I can also implement better control over the pause/resume code. I'll have to do some benchmarks of my own too, as now I'm curious. :) But either way, I'd love another set of eyes on the code to see if what I'm doing is actually worthwhile, as it sounds like it might be! (Especially if I make it distributed and give it the ability to run in a server environment.)
Yup, I've got pause at two different levels planned. One that blocks on the thread pool execution level, and another that blocks at the thread level. I'll have to see which works best, as I'm thinking the thread pool execution level is a better way to manipulate it. My "runTask()" method has a check in it to see if the task is queued, and runs the method if it is. I can also put a TaskPaused value with the previous state stored, and use that to control the task, but that's a lot more work, especially as I have to do a lot of synchronization between other running asynchronous threads. I'll do more research; I'm working on getting a 0.0.1 release done. Once I do that, I'll add some additional functionality, then work on the server code.
Just pass the particular orderings directly to the PQ: ``` val data = Seq("a" -&gt; 3, "c" -&gt; -3, "b" -&gt; 0) val keyOrd = Ordering.by { t: (String, Int) =&gt; t._1 } val valOrd = Ordering.by { t: (String, Int) =&gt; t._2 } val pqk = collection.mutable.PriorityQueue(data: _*)(keyOrd) val pqv = collection.mutable.PriorityQueue(data: _*)(valOrd) pqk.head // "c" is highest pqv.head // 3 is highest ```
I'm not sure if you understood my reply. I answered your question for how to achieve this without blocking threads. A very simple test shows the ability to delay your dependent futures until you manually complete the parent. That gives you the control at the future level that you asked for, without resorting to blocking threads and locks. var trigger = new CompletableFuture&lt;Void&gt;(); var output = new ConcurrentLinkedQueue&lt;Integer&gt;(); IntStream.range(0, 10) .forEach(i -&gt; trigger.thenRunAsync(() -&gt; output.add(i))); assertThat(output, hasSize(0)); trigger.obtrudeValue(null); // fire! await().until(() -&gt; output, hasSize(10));
FYI the link is broken, here is the fixed up one: http://appddeevvmeanderings.blogspot.ca/2018/05/cats-effect-and-webservice-composition.html
wow that's crazy!
Thanks for pointing that out. Must be cut and paste error.
Ah, gotcha. I was trying to look into obtrudeValue, but wasn't seeing much. Appreciate the update. :)
That means if the dependent task was already running, it will observe the value it was executed with. The term obtrude is meant as a warning that it is an offensive method to use generally. However, in the case of a manually controlled future it is fine and a fair usage of the method.
thanks
The final approach might as well hardcode `Stream` rather than giving it as an extra parameter, if it's only ever going to be used with `Stream`. What I suspect Pilquist was getting at was that if you do want to be able to test with lists, you can pass `ListT` for `S`. The full tagless-final approach would, I think, be to require a `Hoist[F, G]` rather than a `join` function, and require a list/stream-like operation to be available on `G`. `process` can then be implemented by lifting `save` to return `G` and then just using `flatMap`; it will return `G[Unit]` which is the correct return type for that operation.
did you try http://www.eclipsecolorthemes.org?
I don't know why but Ordering.by[A, B] erases information from Ordering[B] and then recovers it by invoking it twice. I recommend Ordering[B].on[A](f) instead. i.e. Ordering[Time].on[process](_. startTime)
The first one is a total function, the second is a partial function. Please don't use partial functions.
I don't see the difference if the value inside the flatMap is an Option? The match is as much a partial function as the version without the match.. 
I would welcome a scalafix rule that converted partial functions to their total counterparts where possible :)
It might. I'm skeptical reading the source for `PartialFunction`, considering every `apply` I see is overridden to include `orElse`-type functionality. But the point about totality still remains - `=&gt;` is much easier to reason about, and if the only benefits are syntactic, then I'd say it's probably not good to encourage.
As it currently stands, partial functions are only generated when the expected type of the expression is a `PartialFunction`. So in this case it will be compiled to a simple `Function1`.
It's amazing that they spent so much effort converting over to akka-http. Netty is so much better. 
Is akka http really that much worse in prod?
Depends. If your application serves lots of short requests at high volume and low latency, you'll definitely feel the pain of using the akka backend because the majority of your time on CPU will be spent inside the framework parsing requests and converting responses. If your application does a lot of CPU work anyway, then you won't notice much of a difference but you'll still take a slight hit compared to netty.
[This blog]([https://www.beyondthelines.net) has been everywhere recently &amp; with good reason; the content is truly excellent. Anyways, I was reading [this article](https://www.beyondthelines.net/programming/leveraging-the-type-system-to-avoid-mistakes/) and it talks about using wrappers around primitives to leverage the Scala type system to avoid trying to coerce, for instance, a `ProductId` into a `UserId`. I'm trying to use the same thing for a field of type `BigDecimal`, for example: ``` sealed trait Purchase def purchase(purchasePrice: Price[Purchase]) ``` The article indicates that I need to write a custom json marshalling function. Here's what I got: ``` case class Price[A](v: BigDecimal) extends AnyVal object Price { // Here's where I'm having trouble. implicit def jsonFormat[A]: Format[Price[A]] = { // Not sure if this is right. // My reasoning is that the data will come in as a String (json) and it must be // marshalled into a BigDecimal? val value: String =&gt; BigDecimal[A] = ??? Format( // Don't know what to do here. ) } } ``` For `Reads`, the compiler is expecting a type of `String =&gt; NotInferedB` (so it's expecting a function that maps a String to type `NotInferedB`, not sure what `NotInferedB` is). I can't simply put in `BigDecWrapper.apply[A]` as in the example because I think that apply method converts a `BigDecimal` to `BigDecimal` wrapped in `BigDecWrapper`, i.e, `BigDecimal =&gt; BigDecWrapper[A]`. Is the solution to use `Reads.FloatReads`? If so then won't I need to create a BigDecimal from Float? Not sure what the best course of action is. 
I like to think that Scala learned me
It started by deciding _not to_ learn Scala. A few (6? 7?) years ago, I wanted to learn some language making concurrent stuff easy. The JVM drove me off, and I went with Go (also sharpened my Python meanwhile). I avoided it for a while thanks to PySpark, but then, a couple years ago I was in a greenfield Spark project and we decided to go for Scala for performance. I liked it a lot, dove more into the funny stuff (the T word, the M word, etc), and here I am, still learning and still enjoying it a lot.
They told me I had to at work, and I really enjoyed how much depth there was in the language - and the concise syntax really drew me in. I get mad having to type `;` at the end of Java lines...
I learned Scala for work. And because I was a Perl programmer. Perl programmers usually have such an impoverished view of design and types. And I thought it would be good for me to learn a statically-typed language. I'm so glad I did it. I agree with another poster, you could look at it as a better Java or a worse Haskell. Both are very illuminating views compared to Perl.
It's actually not a partial function, it's a normal function written in cases. You can use `reflect.runtime.universe.reify` to see how it's desugared. @ reify { List(true).map { case true =&gt; 1 case false =&gt; 2 } } res8: reflect.runtime.package.universe.Expr[List[Int]] = Expr[List[Int]](List.apply(true).map(((x0$1) =&gt; x0$1 match { case true =&gt; 1 case false =&gt; 2 }))(List.canBuildFrom)) But if you call `collect` it does construct a PF. @ reify { List(true).collect { case true =&gt; 1 case false =&gt; 2 } } res9: reflect.runtime.package.universe.Expr[List[Int]] = Expr[List[Int]](List.apply(true).collect(({ final &lt;synthetic&gt; class $anonfun extends { def &lt;init&gt;() = { super.&lt;init&gt;(); () }; final override def applyOrElse[A1, B1](x1: A1, default: Function1[A1, B1]) = (x1.asInstanceOf: &lt;type ?&gt;): @unchecked match { case true =&gt; 1 case false =&gt; 2 case (defaultCase$ @ _) =&gt; default.apply(x1) }; final def isDefinedAt(x1) = (x1.asInstanceOf: &lt;type ?&gt;): @unchecked match { case true =&gt; true case false =&gt; true case (defaultCase$ @ _) =&gt; false } }; new $anonfun() }: &lt;type ?&gt;))(List.canBuildFrom)) Unfortunately you don't always get exhaustiveness checking, which sucks. It's better with Typelevel scala ‚Ä¶ hopefully the improved checkers will get merged into Lightbend Scala at some point. 
Kind of like cats. 
I started working with Play 2 in a Java world, and Scala intrigued me. A few years ago, I joined a company that made the decision to go Scala for new web services and projects. Now I actively dislike Scala, or at least the cult of it. Oh well. :)
Because writing Spark code in Scala is so much nicer than Java. 
I wanted to apply for a job that wanted knowledge of functional programming and JVM languages. Scala looked less foreign than Clojure. 
Because I liked the whole JVM write once, run anywhere approach but Java was just so verbose. I compared other JVM languages and Scala won me over with type inference and the whole functional stuff, especially type classes. All while still staying somewhat readable, unlike certain other functional languages.
Back then I was looking to improve my programming skills \(only knew C and Java\), and I had read that learning a functional paradigm coming from imperative would be beneficial. Flipped a coin between Scala and Groovy \-\- but looking back it didn't matter which one I chose. The more I learn about FP the more I enjoy it.
I've been working on an API to help quickly build a Job with a DAG of Tasks that can be run (a)synchronously. It's still got a lot of work to do, and I've got a huge roadmap of functions, but it's been fun so far! Anyone interested in helping out, please feel free to fork and contribute, or write me! I'm hoping to turn it into a distributed job coordination server (soon) that can coordinate jobs to Mesos, Spark, and [datafusion-rs](https://github.com/datafusion-rs/datafusion) amongst others! [Scattersphere](https://www.github.com/KenSuenobu/scattersphere) Not quite at a 0.0.1 yet, as there's no Quick Start guide, but I'm working on one. It just takes a lot of time to write that guide! &gt;_&lt;
I wanted to learn FP and the red book seemed to be pushed as one of the best resources, as well as the courses on Coursera. So Scala won by having the best learning resources.
Trying to get into data science. Learned some python and sql, but I'm not getting any offers where I apply. Hoping this sets me apart from the rest of the field a bit.
Same here. I got tired of my python UDFs being so slow due to all of the extra serialization (but suppose arrow might help that part out these days).
Why did YOU decide to post this question in 6 other programming languages subreddits?
The easiest way is to use https://github.com/spray/spray-json#providing-jsonformats-for-case-classes , see the paragraph beginning 'If your case class is generic....', but in your case a little easier because you don't need to provide a context bound: import spray.json.DefaultJsonProtocol case class Price[A](value: Int) extends AnyVal object Price extends DefaultJsonProtocol { implicit def priceFormat[A] = jsonFormat1(Price.apply[A]) } This will produce JSON like the following: {"value": 1} If you want, you can further optimise this to encode and decode just the value `1` by customising the format. See https://github.com/spray/spray-json#providing-jsonformats-for-unboxed-types for the way to do that.
We started using Apache Spark at my job. Most of our previous codebase was in Java, but at the time the resources for Spark in Scala seemed better. I also prefer writing Spark code in Scala because it's much cleaner.
Because you have a better chance selling Scala to Java devs than Haskell
Like Neo would say - ‚ÄúBecause I choose to ‚Äú
Perl doesn't have a strong type system or any parameterized types. All you have is ARRAY, HASH, SCALAR, and then class instances which are just special versions of HASH usually ("blessed" for the initiated). That and the object system was bolted on. There was a lot of ceremony, a lot of leaky abstraction, and a lot of room for error. And a lot of data design was passing around arrays and hashes with no mention of mutability. Privacy was by convention only. And forget exercising constraints like set uniqueness. That was always done by building hash with duplicates and then taking the keys. Really ghetto stuff. Writing really good Perl requires a level of self-discipline that most people are incapable of exercising.
Effects and Monads aren't introductory subjects, so I don't know what they were expecting introductory course. An introductory course should be focused on first principles like equational reasoning and substitution, functions as values and higher order functions. These first principles are applicable in every functional programming language, even the weakly typed ones. Monads however matter more in strongly typed functional programming languages. Implicits don't have anything to do with functional programming, they're a Scala language feature. The course's aim is to teach just enough Scala to instruct on the first principles mentioned earlier. It's not a course to learn Scala the programming language. It's not a course to go go from nothing to proficient in using scalaz. It's an intro to functional programming, using Scala. It's a great place to start. It's not where you should end. It sounds like you and your coworkers have some misaligned expectations about what the course is supposed to be.
Spark.
I took the course along with the rest of the courses in that sequence and liked them; that being said, the initial Principles course is more about FP application in Scala and less about the theories behind it. I can easily see how people would end up with the sort of gaps you mention. While I do think the course helps people get into a more functional programming\-oriented mindset, I don't think it's sufficient to prepare people for working on real life FP projects.
I wanted a more powerful, safer language for frontend development that I could also use on the backend. Scala and Scala.js fits my criteria much better than Typescript, Flow, etc. Unlike most others here, I've never used Java professionally.
I just thought it was an interesting question, and I wanted to ask it to people who use some of my favourite FP languages :-)
I was working on a team doing Python and we wanted to share code with another team doing Java. I had a background in Erlang (and a little Haskell) so I was pretty comfortable with functional programming, but Scala taught me about static typing and the JVM.
Work decided to use Scala for a Big Data project.
I was too much of a pleb to learn Haskell
RPG/Delphi programmer. I want to know a modern language! Also, I'm a mathematician at heart so the functional style massively appeals.
I have taken this course, and currently taking up the other courses in the specialization. More advanced FP concepts are touched upon in the [second course](https://www.coursera.org/learn/progfun2) \(but not extensive\). I think going through the first two courses should help in getting comfortable with a\) Scala, and b\) thinking functionally on a high level. For grokking FP \(and the type systems\), I think a good approach would be 1\) go through the red book, 2\) go through learning scalaz or herding cats by [eed3si9n](http://eed3si9n.com/), and 3\) reinforce it by using it, either on practice or real\-world problems. There are other great resources and communities out there, but this is the strategy I'd go for \(especially #3\).
I used Lombok and the Checkers Framework and realised that while they offered very useful improvements to Java, they didn't work unless every tool you were using (IDEs, code analysis, profilers, that kind of thing) supported them, and support in those tools was always breaking because it was seen as an optional extra. So I tried Scala as a language where those improvements were built in rather than bolted on.
Why Play is said to be non-blocking, isn't JDBC call in Future is blocking ?
The red book is really awesome
Yes but you don't have to use JDBC with Play, you can use something like postgresql-async or another datastore that has async drivers (e.g. Cassandra).
Wow, thank you for the quick response! Have a good one! :)
In my last workplace we worked in PHP... Some of my mates were start learning Clojure and they were talking every lunchtime about how magic is functional programming and Clojure. So I decided to learn a similar language but with a chance of being paid for work with it :troll: This was 4 years ago aprox... Nowadays some of my mates work with me in another company with Scala. Some others, the most stubborn ones, now work with Java 6. ROFL
I identify with this so much.
A job I was applying to used play framework.... and I continued learning because I really like functional languages (maybe I should give Haskell a go when I have the chance, I did quite a bit of LISP while I was in scool)
Is this approach any different from using a monad transformer?
It may reassure some that SAP is currently using Scala.js for some of their projects. Unofficially they already said that they will invest people/momey into maintaining a fork if it's required.
We switched from PHP to Scala at work for "the big rewrite". No one in the team had every written more than a toy project in it. It was a horrible idea (I told them as much). I took us years to get back on track - but I did fall in love with Scala on the way.
I've always been curious about different programming languages, and one day I just stumbled upon Scala and it was just love at first sight. It really had everything I wanted in a language and still today makes me very happy programmer. Being a former Ruby fan, Scala had everything ruby had, but with excellent tooling thanks to a strong static typesystem. Also they've got "monkey patching" right (implicit classes) so there was no looking back for me. Yep, I'm a huge Scala fan ;) I wouldn't trade it for anything. (Although the compiler could be a tad faster)
Just published a queue batch tracking service: https://github.com/paradoxical-io/carlyle It helps you track queue fanouts: one message creates N other messages down the queues and you want to know when all N are done determistically
Neat! I did something analogous with a workflow engine https://github.com/paradoxical-io/aetr
Nice! I'm working on something that will turn into a workflow engine eventually. Something that can create jobs on the fly, or can programmatically fire off other jobs after one completes. That's definitely in my list of things to do. :)
That‚Äôs the core of aetr, it calls other jobs. But jobs in this case are just API calls to external services. The idea being people can stand up endpoints themselves and then use aetr as a coordinator to call these services and transform/reduce calls between services 
I became interested in functional programming while writing JavaScript full-time, so I decided to start learning Haskell. After a while, I decided to spend my time on something more *practical*, so I moved to Go. I appreciate the simplicity of Go, but it's not fun to write. I finally decided to take a look at Scala, which I had avoided up until that point because I associated it with Java, and thought, "Ah, this is what I've been looking for."
pattern matching, pattern matching all the way...
Thanks for your suggestion. I ended up sticking it out with the play json api. This is what ended up working for me: ``` case class BigDecimalWrapper[A](wrapped: BigDecimal) extends AnyVal object BigDecimalWrapper { implicit def jsonFormat[A]: Format[BigDecimalWrapper[A]] = { Format( Reads.bigDecReads.map(BigDecimalWrapper.apply[A]), // Same as in the blog, just with bigdec (o: BigDecimalWrapper[A]) =&gt; JsNumber(o.wrapped) ) } ```
Can you give an example?
hmmm.... let me ask this. Lets say i have 10 functions (one(), two(), ..etc) Lets say that each of these functions returns a (Boolean, MyObj) where the first boolean is wiether or not it passed, and MyObj gets passed into the next function. How would I write a function that calls each function one after each other, but stops early if any of the functions fail? 
You have it exactly right.
Oh, sorry, I saw `jsonFormat` and thought you were using spray-json. Anyway, looks good. There are a couple of libraries available that automate these derivations btw.
thats correct. if you rather one() takes MyObjOne, two() takes MyObjTwo, etc thats fine too. The important part is that each function has a validation, and produces the next functions input.
BTW Reddit markdown does not accept code blocks with triple backticks. Indent each line by four spaces instead.
I can die trying ;) first of all: the point about nulls is quite valid, proper scala code will not deal in nulls. BUT (there's always a but..) let's assume that we're dealing with a json parsing library that comes from the java world. it's riddled with nulls and we want to do the aformentioned guards: object App { def method(one: String, two: String): Unit = { (one, two) match { case (null, _) =&gt; println("to be or not to be null") () // I would strongly suggest coming up with a return value case (_, null) =&gt; println("I am the null that knocks") () // I would strongly suggest coming up with a return value case _ =&gt; println("that's weird. never expected to get this far") () } } def main(args: Array[String]): Unit = { println("run 1") method(null, null) println("run 2") method("a", null) println("run 3") method("test1", "test2") } } Result: run 1 to be or not to be null run 2 I am the null that knocks run 3 that's weird. never expected to get this far Having said that, that's just one of the possible solutions, a 1-1ish translation of the java code. There's more to scala then that.
For comprehensions are very useful for making that code read better, for { one &lt;- One(input) two &lt;- Two(one) three &lt;- Three(two) } yield three For Option that would short circuit out with None. If you wanted the specific failures you you use Either, which instead of None has a failure case. You'd write your validation functions as returning `Either[String, GoodResult]`, and the for comprehension would look the same. If you have a straight pipeline of functions returning Either like that, you can get it smaller, like this; `One(input).flatMap(Two).flatMap(Three)`
Not a lawyer, but I'm pretty sure Clause 2 of the CLA gives EPFL the right to do literally anything with their copy of the contribution, including removing the contributor's name and slapping their own copyright/licensing terms on it. The difference between this and outright assignment is that the contributor still maintains ownership of _their_ copy of the contribution, so the EPFL can't (for example) prevent the contributor from giving the same patch to other projects on different terms. 
In those situations `Option(_)` is handy.
Not really. Scala compile times are notoriously slow, and while progress is being made, esp. as now we've had compiler benchmarks for a while, we really need a N times improvement for them to be impactful. I don't see that coming unless the compiler is completely rewritten. That being said, there is some work being done in that front along others: https://github.com/twitter/rsc https://github.com/scalacenter/bloop 
&gt; I'm pretty sure Clause 2 of the CLA gives EPFL the right to do literally anything I think this is not what the text says. It explicitly refers to a _license_ being given out, not an _assignment_ of copyright. If CLA's would facilitate the transfer of copyright ownership, then CAA's (copyright assignment agreements) wouldn't even need to exist. The language of CLA's and CAA's differs substantially (and the requirements for the latter are usually considerably higher, i. e. a button saying "I agree to the CAA" is not sufficient). Compare CLA's: &gt; You hereby grant to X and to recipients of software distributed by X a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright **license** to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. with CAA's: &gt; You **assign** to Us all right, title, and interest worldwide in all Copyright covering the Contribution or &gt; Beneficiary **assigns** to X the Copyright in computer programs and other copyrightable material world-wide As another piece of evidence, even Microsoft doesn't seem to believe that CLA's transfer ownership, otherwise they would write &gt; Copyright (c) .NET Foundation instead of &gt; Copyright (c) .NET Foundation _and Contributors_ in their [LICENSE](https://github.com/dotnet/corefx/blob/master/LICENSE.TXT). As you mention, I guess it comes down to whether a license allows stating "Copyright (c) ..." without being the owner of the copyright ‚Äì because the CLA clearly doesn't do that. 
Great! Good luck with it. Let's fix sbt
It's coming with Dotty, but that's quite a ways off (begin 2020). [Odersky explaining the dotc compiler architecture](https://youtu.be/w1ca4KL9UXc?t=11m55s) [In the same talk he says it will at least double the speed](https://www.youtube.com/watch?v=w1ca4KL9UXc&amp;feature=youtu.be&amp;t=58m42s)
Bloop and SBT with Coursier have been great!
And this is why no one should ever sign a CLA. I won't at least. Even if I really wanted to contribute to scala compiler or Play framework for example. Just fork and keep your code changes.
Somebody in this subreddit said this is unlikely to be the case anymore, but presumably there are compiler benchmarks in the dotty repo?
I guess you mean [this](https://lampepfl.github.io/bench/)? I'll have to admit I'm not sure how to 'read' this yet. :/ I'll try to tell you what I think I know: * Odersky explains in his talk how they restructured the compiler and how they achieve this speed. ([This 5-minute talk by Dimitry Petrashko briefly explains one of the design choices, so can judge for yourself](https://www.youtube.com/watch?v=9xYoSwnSPz0)) I don't expect this to suddenly 'go away'. The compiler architecture hasn't changed as far as I can tell and there's been no announcements about it. * It's been while since they've made these claims about the Dotty compiler, though. Last was in 2016. This kind of worries me. I hope they'll give us an update about this soon. * The Dotty compiler hasn't been completed yet. And most optimalisations usually happen in the last phases of development. It's hard to tell at the moment what the result of these optimalisations will be. * But for what it's worth: I've been rewriting Scala projects to Dotty projects for my internship the past few months. In my personal experience the Dotty compiler is usually faster. * I have also read on this sub that the speed is unlikely to be achieved anymore. But I don't remember any concrete sources or evidence for it (or who said). I couldn't find the comment anymore either, sadly. :/ 
IMHO your last paragraph is the reason - they have no reason to ask anyone and things would be fine (legally). But just in case some hotheaded moron jumped in and started bad mouthing left and right, they wanted to have such person write to them, so that they could address his concerns and defuse the bomb.
What is the bad thing that you think has happened here?
It's an introductory course, alternatively a more comprehensive introduction would be to read and complete the exercises in the "Big Red Book" aka Functional Programming in Scala. The Red Book does eventually get on to Monads, Monoids, etc. However, the exercises are harder than the Coursera ones, so doing the first Coursera prior to the book may be beneficial.
I would add there is nothing wrong with using `return`; it's in the language after all. Basically, in between curly braces you can do whatever. Just look at the methods in the standard collections. A lot use procedural code for modifications, then return an immutable result. It's just the for comprehensions over Option, Either etc. are often nicer to read for your fellow programmers, especially when the conditions get complex.
&gt; scala.Seq is now an alias for scala.collection.immutable.Seq (no longer scala.collection.Seq) üéáüéÜüéá
Do you have a running sbt session and perhaps you do testOnly some.tastyTest? (Or maybe play with auto reload?) On a non-trivial codebase on a 4 core i5-4310U I don't feel Scala slow. Sure, doing a full sbt stage takes ~3 minutes. 
Has anyone use this new vim feature with SBT ? https://www.vim.org/vim-8.1-released.php How's your setup and experience?
Would having more cores help?
Yes. The compiler uses all the cores it can put its grubby tentacles on. Probably there's a diminishing returns effect, but the JVM is pretty efficient, so given enough cores it can likely give them a lot of work.
We are using Scala/Playframework at [GIVE.asia](https://GIVE.asia), deploying to Heroku, and using Postgresql. This month \(or next month\) we hope to open\-source some of our in\-house libraries: * An SBT plugin to compile Vue's single components for both \`sbt stage\` and \`sbt run\` * A basic background job framework for Heroku that uses Postgresql as a queue.
&gt; * The scala-xml library is no longer bundled with the release, and scala-compiler no longer has a scala-xml dependency Finally! I really hope that in the future we can produce small JAR files using just sbt-assembly.
Shadowing does not involve any mutability at all. It may look like updating a mutable variable, but it's really not; there is no state involved. For example, consider: let name = get_name() in if true then let name = name |&gt; capitalize in print_string name; print_string name The second `print_string` still sees the old `name` value. For this reason, shadowing is compositional, and does not have the gotchas of mutability. 
I'd prefer as little shadowing as possible. Being able to rebind a symbol to a different value brings with it much of the same cognitive load as using variables.
I'd say that's mitigated a lot by the IDEs we have. If I can put my cursor on where a value is used and immediately get a highlight of where it's (immutably) defined, that puts me in a much better place than using variables that can be reassigned.
Yes, please.
&gt; The compiler performance in 2.13 is 5-10% better compared to 2.12. Most important sentence, clearly ;)
Now the code is very incorrect. Quasiquotes are not hygienic, with unpleasant implications: * All references to top-level symbols **must** begin with `_root_`, e.g. `_root_.org.scalacheck.whatever`. * `implicitly` is not guaranteed to have its usual definition; you need to say `_root_.scala.Predef.implicitly` first. * Importing inside a quasiquote is unsafe, because the imported names may shadow names from the surrounding context. What if `c.prefix` contains a reference to some unrelated symbol named `Arbitrary`? Note that 2.10-style `reify` *is* hygienic, and therefore doesn't have these problems. I suggest using that instead of quasiquotes where possible.
_If scala did not have var_ I think it would be a good idea. Unfortunately Scala does have variable reassignment, and that means shadowing needs a distinct syntax so as not to be confused with it.
I know this is just a toy example -- but why would you not just let name = get_name() if true then print_string (capitalize name) else print_string name Do you have any particularly compelling examples for where this would be necessary for useful?
I think the question is good. I'm not sure if there are really good use cases in Scala. You have a lot of room for making the code as readable as possible. You can refactor functions to extension methods so you could always have something like: &gt; val name = getName() &gt; .capitalize &gt; .doSomethingElse Which is better than &gt; val name = getName() &gt; val name = capitalize(name) &gt; val name = doSomethingElse(name) 
This sounds like a really nice and well-motivated idea!
Good points on all three. I remember \`reify\` is difficult to use. I can understand why \`reify\` is safer though. Because there is no huge amount of literal string.
Not everything in the language is a good idea. Scala does have 10 years' worth of accumulated cruft; some parts of the language really are worth avoiding entirely.
I've been thinking that I would try and keep notes and turn them into a series of blog posts about the sort of problems that we worked on after. 
My goal is to understand the problems others have to become a better teacher, to support the scala community generally. I'm probably an A3/L2 per https://www.scala-lang.org/old/node/8610 . Happy to learn something from smart folks or teach something to people who are learning. Both are great for me, I just want to be someone in the scala community people feel they can reach out to. 
&gt; https://www.scala-lang.org/old/node/8610 Speaking of this rubric, does anyone want to give a shot into making an updated one?
I'm not sure what this macro achieves. Why not just do: import org.scalacheck.Arbitrary import org.scalacheck.Arbitrary.abitrary object Maker { def apply[T: Arbitrary] = arbitrary[T].sample.get }
This is an intriguing opportunity. And I‚Äôm always looking to build skills. I‚Äôm a java developer myself with a keen interest in FP.
Un justificatif llvm un nul jkjk lm km MML mjj lmk l km km m my What mm ml I'll km Km l Kleenex j in no position lol km km looo Ppl m mm it'llk uh ppl Hu kllll ll ok ym l L ok k up to l k ppl ok my mm ml yn Momopl I jkjkp loo jmh ppl klll N mm lK pllnot nu p llp mlm loo j.k.chan@live.comkln Mm Yl LL
Wanted to learn a new language and was introduced to Haskell in college. Thought Scala seemed like a practical alternative where I could move over slowly from my older Java mindset. Now I'll never look back, though I still code Java at my day job.
Really bad sound :\(
It isn't as terse. Your code requires the user's code to provide implicit `T` .
That's what scala-check shapeless is for?
I'm not sure I understand what you mean. For your code, invoking `Maker.apply[User]` would complain `could not find implicit value for evidence parameter of type`; We would need to import `org.scalacheck.ScalacheckShapeless._` in the same part of code \(or in the same file\). Using Macro like I did doesn't need the import statement when invoking `Maker.apply[User]`.
What is your style when chaining multiple methods? doSomething(..) .map { a =&gt; b } .somethingElse() Or doSomething(..).map { a =&gt; b }.somethingElse() Or doSomething( c = d, e = f ) .map { a =&gt; b } .somethingElse()
What constitutes a value in functional programming? Is an object a value ? Is a function a value ? Is an object with setter methods and mutable states a value ? Is an object with mutable states and works as a state machine a value ? I am asking myself of these questions after seeing a sentence: &gt; Task has a constructor that captures side-effects as values,
I think using `return` comes with enough gotchas (the scope of the return, accidentally-caught exceptions) that it‚Äôs better to just avoid it.
The concept of an online Scala Genius Bar is interesting. The Earl of One could be the beginning of something big.
Definitely the first one for me.
Mods, can we please have a sticky thread where people can offer or ask for pair programming sessions ? For a while now i was thinking about contacting some active members of the community to ask if they can do a pair session with me or mentor me, but my shyness got in the way. I already sent a request to Earl, and i would love to send you one, but i am in Europe so finding an hour when we're both free and awake would be a challenge, but thank you for this for the community!
OP you are ducking awesome for doing this.
I'm curious if there is anything I can do to help with work on the Dotty Compiler. Creating my own programming language from scratch, while a cute little exercise, doesn't really help anyone where as I think that working on a fresh new Scala might be both educational and helpful. I only have a bachelor's degree and a copy of "Compiler Construction: Principles and Practice".
I suggested it was more an inspiration of what they do, more than what they solve. I know there's lots of libraries out there to do things that I do - all I'm doing is making it simple and easy-ish. It's more about utilizing CompletableFutures and making them work well. The composition is that of a CompletableFuture, nothing more. I just add a bit of control using a pausable executor. I'm also going to add some more features to it. Criticisms aside - as it sounds like you have a few - how would you implement it without using () =&gt; Unit and tasks not composing in an algebraic way?
Just PM me when you want to work on something.
I expected the link to point to an Admiral Akbar meme. Instead, I just don't get it; who cares what other subreddits OP posts to?
Can anyone tell me what the difference is between the following function calls: def double(i: Int) = i * 2 val double = (i: Int) =&gt; i * 2 They both seem to do the same thing. Is there an advantage of one way over the other?
it is not about where he posts, but *what* he posts. op has a long history of creating accounts, posting way more about his private life than recommended, and deleting the accounts only to create new ones soon after, always posting shitloads of personal information (name, github, resume, diploma, employer, salary, etc.), which totally defeats the purpose of creating new accounts, if you ask me. in a recent post, for instance, he suggested he should get laid more often because he makes 160k/yr. he might have good intentions with this offer, but i would otherwise advise caution to anyone thinking about accepting it.
The first is a method of a class and the second is a field whose value is an anonymous subclass of `Function1`. The `FunctionN` types have an `apply` method within them that allows you to use parentheses when you call them. With the method, a call like `instance.double(2)` will effectively be interpreted by the JVM similar to `Class.double(instance, 2)`. With the field, a call like `instance.double(2)` will eventually be interpreted similar to `Function1.apply(Class.double(instance), 2)`. Those are pretty similar, but the actual implementation in the second is in a different class and the second has an extra indirection to access the field. Now, this is all before optimizations happen. It's totally possible that the compiler and/or the JIT remove the indirection, and it's unlikely to matter to you, but in principle it's slightly less efficient to use the field version. Note too that if you had `def double = (i: Int) =&gt; i * 2`, then you would allocate a new `Function1` instance with every call. I don't have time at the moment but I'll attempt to come up with some bytecode later that shows all this. I think the moral of the story is: if both will work, prefer methods.
Just released 0.0.2 - includes a few changes to the API and the Wiki. More to come soon!
I believe every arg in this case becomes implicit.
That's correct
Stop discriminating against people with mental problems.
What does this do? Not sure about the implicit `class Something[T[_]](value: Int)(implicit T: Stream[T]) ...` and why would T be used twice here for AnotherThing? `class Something[T[_]](thing: AnothingThing[T, Stream[T]]) ...`
Forgive my psuedo code. If it's confusing I'll try to fix it...trying to express the simple stuff so the idea might take more shape fore people not exactly sure what I am talking about. Something I think I'm being super clear and I'm really not. Sorry.
In this video we will learn about the majority of use cases for traits \- interfaces on steroids. We will learn how to add functionality to existing classes, even the ones whose code we cannot access! We will learn how to modify behavior with the "Stackable modifications" pattern, which can also be used to intercept behavior. Most importantly we will learn about linearization \- a solution for the issues which commonly arise with multiple inheritance and diamonds.
First of all: select is not equivalent for filter. Select in sql performs *mapping* from one set to another so fp quivalent will be the map. Zip is also not eq to join. Its usually called lookup or smth. As for idea i think it is very ambitious and many scientists tried to do that. For example C# + Entity Framework. They offer sql-like syntax for collection traversal and sql queries but there is defferent interfaces for inmemory collection and external-database collections, so methods s are slightly different for them (and its kinda limited for sql collections so you have to switch to plain sql for complex queries) if you are looking for a simple scala persistence libs you should have a look at http://slick.lightbend.com/ or http://getquill.io/ I think none or them have an ability to join collection from dirrerent data sources. But they both at least can deterime which actions must be performed as sql queries, and which must be done inmemory. (for exaple if you want to apply some fuction to table row, it will be applied in memory and will not produce an not-supported-in-sql-exeption) If you are willing to try you idea anyway i can suggest have a look at free monad / Typed Tagless Final Interpreters theory to be able to construct abscruct syntax trees from your syntax and then pass it to some interpreter which will run is agains appropriate data targets.
You might look into Anorm if you haven‚Äôt already. It‚Äôs Play Framework‚Äôs SQL library, essentially just a wrapper over JDBC that lets you write plain SQL in your Scala code by using string interpolation to properly escape named parameters. The justification for it is much like what you describe: &gt; SQL is already the best DSL for accessing relational databases. We don‚Äôt need to invent something new. Moreover the SQL syntax and features can differ from one database vendor to another. And it‚Äôs a stand-alone library, so you don‚Äôt have to pull in all of Play to use it. 
As I mentioned before I‚Äôve used absolutely every scala library for persistence. I even came up with a fake story!!! I guess you missed that part... I‚Äôm not going to take this response too seriously because. I do t think you understand WHY I want to do something like this. I want to explore making persistence in scala much much easier and I want some feedback from the community as to how to do so. If that makes sense to you I‚Äôd love to hear it. As for the existing libraries out there the only one that actually makes sense to me is doobie. The others (for me) were roads of dsl jargon and my coworkers giving me shrug emoticon when I‚Äôm submitting my PRs. I think the focus of a sql library should be dead simple query generation as if it were flowing like sql out of our heads. There are some other follow up ideas that I‚Äôd love to get into for testing, documentation, and even optimizations...they‚Äôre definitely ambitious ideas but aren‚Äôt those the best ones???
What don‚Äôt you like about the strong interpolation? It does exactly what you‚Äôre describing. 
Well for me at least and the way I LIKE to think about queries what I want to do is represent the idea with the smallest footprint I can. The strings don‚Äôt lend themselves as being small composable ideas. They‚Äôre just huge blocks of string. Bleh... But I did just think of something that might make both string interp and a sql dialect dsl play nice together. I know this approach is really popular. It does make sense to me on a small scale.
List of scala projects which are released for 2.13.0-M4: https://index.scala-lang.org/search?q=fullScalaVersion%3A2.13.0-M4
&gt; As for the existing libraries out there the only one that actually makes sense to me is doobie. The others (for me) were roads of dsl jargon But Quill looks similiar to what you outlined above? query[Person].join(query[Contact]).on((p, c) =&gt; c.personId == p.id) Anyway, it's not a scala lib but have you tried jOOQ? create.select(AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME, count()) .from(AUTHOR) .join(BOOK).on(AUTHOR.ID.equal(BOOK.AUTHOR_ID)) .where(BOOK.LANGUAGE.eq("DE")) .and(BOOK.PUBLISHED.gt(date("2008-01-01"))) .groupBy(AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME) .having(count().gt(5)) .orderBy(AUTHOR.LAST_NAME.asc().nullsFirst()) .limit(2) .offset(1) 
pick a ticket from the [Github issues](https://github.com/lampepfl/dotty/labels/exp%3Anovice) and work on it.
In intellij you can try these : * open the main class file, and click the ```Run``` icon at the left the object definition. * open the main class file, right click the editor and click on the ```Run``` option. Are you having a specific problem? or you're just wondering ?
Hi all, I have a question regarding adding code to a class without having to create a definition ... that's about as best as I can describe it. For example: class MyCode extends RunnerClass { override def run(): Unit = { ... do some block of work here ... } } val runner: RunnerClass = new MyCode() runner.run() But ultimately, I want to do define the code this way: def myClass = RunnerClass { ... do something here ... } myClass.run() I'm not sure how to code something like that. You can see that in the second example, the code is much more concise, and easier to read. Ultimately, it means less typing for the programmer.
Agreed - I prefer to align it against the method name or variable name.
[removed]
Is there a reason you are asking for a Twitter handle but not a Github handle? Isn't Github/Gitlab/Bitbucket much more relevant here?
Just if they want me to follow them on Twitter. I don't follow many people on Github (more repositories that I watch). :)
Oh okay, makes sense. I thought it was meant to judge past experience or something.
Thanks for the detailed reply. I didnt give much to work with ha. You're right my question should have been much more specific and I should have explained the part that was confusing me but with help of your answer I'm begging to understand.
Man, I'd love to do this, especially as a lead. Going to conferences, doing design, and working with you would be great. Sadly I'm just not sure I have the time to spare and wouldn't want to commit to something I can't finish :\ I will think on this the next few days. Maybe I can convince myself to give it a go.
No pressure, but yes, it'd be fun...hopefully. ;)
You might just want an `apply` method in the companion object. object RunnerClass { def apply(block: =&gt; Unit): RunnerClass = new RunnerClass { override def run(): Unit = block } }
Thanks! I had a much more complicated way of doing this with a series of callbacks, but this is by far easier to deal with.
Your videos are super cool sir...The way you explain each concepts deserve a big round of applause. 
Apache Spark is becoming a must tool for big data engineers and data scientists. 
Wow, this is a great opportunity here. I recently started learning Scala because I'm drawn toward it's type system but I've been finding myself drawn toward the functional aspects as I go. I don't know Scalaz, and I'm not sure what most of the projects are doing (let alone _improving_), so I'll take a thorough look through the list of projects before signing up :)
Questions &amp; comments: Are you using IntelliJ Community? Or the professional edition? If you're like me, your company pays for a lot of fancy tools like IntelliJ Professional. A lot of the cool features of IntelliJ like code desugaring and automated type insertions/disambiguation is amazing. But I worry about my productivity with Scala if those features were to go away. I learned to program using Python and VIM so needless to say I don't like my reliance on a lot of the IntelliJ magic. \&gt; I would say the biggest hurdle is learning FP. Once you know say Haskell \(and you have already learned Java\), Scala isn't that hard. My first object oriented programming language took six months to learn. It makes sense for someone to take a good chunk of time to learn their first functional programming language as well. Also yes. Totally agree that FP is the biggest hurdle. \(Although you don't have to learn Haskell if you want to pick up Scala.\) I wanted to do everything FP for about 4 months before I realized, as you said, that sometimes mutable state is the way to go. But Scala does give you the option of never using mutable vars again, if you want it. 
You don't need the pro version of IntelliJ to get the desugaring feature. 
[removed]
Thanks for the help, and my hope it provides a lot of value to everyone who decides to participate. :)
Ah jooq... This is almost exactly what I‚Äôm talking about. Quill still missed some of the sqlness of their querying API so it is close and maybe could be used as a base and this kinda thing extends behavior like that. But the real sauce behind this is that multiple datastores means multiple persistence clients which means allocations that are hard to follow / reason about. I want to just talk persistence and have some sort of schema configuration that allows me to traverse all the things using a standard API no matter if it‚Äôs Cassandra, MySQL, etc ... The end game is to make things easy. Ruby on Rails' ActiveRecord level of easy but within a strictly typed, cacheless, managed execution kinda way.
Just look at that beautiful list of projects coming to scalaz!!! 
These are indeed pretty good reasons why Scala rocks. One small thing to note, explicitly annotating the type doesn‚Äôt mean the compiler skips inferring the type. In fact it still goes ahead with the inference and then doubles back to checked the annotated type against the inferred type. You can check this yourself by annotating with a wrong type, e.g.: val x: Int = "Hello"
Yeah you `extend SbtModule` (or `CrossSbtModule`) and it'll configure the source paths like that. Ammonite's `build.sc` file (linked above) does that and that repo is still using the old SBT/maven-style file layout.
In my last job I used ensure-vim plugin with vim to recreate my old python/per envs. It‚Äôs hard to give that up after 10 years. Like any code base, once you‚Äôre comfortable in the language and the code base itself, the need for IntelliJ largely goes away.
I like Scala more than any of typed language but some of the features are really painful things that only exist to confuse. Like it‚Äôs really difficult to mock a function whose type in an anonymous inner classes type. Classes inside other classes shouldn‚Äôt be possible.
Everything inside the brackets becomes implicit.
Ok thanks. I was having a specific problem, I needed to run sample programs and I was thinking how to do that. I found that I can create objects for that.
Such great oportunity, thank you! I suppose we will get some more information after 5/25?
Can you format your code so it actually appears in code blocks? It's a bit bit hard to follow without it :/ Paste the code, highlight it, and press the code button in the editor (`&lt;&gt;` icon). Anyway, I think your problem is that `OFormat`/`OWrites` are typeclasses belonging to the ReactiveMongo library, while `play.api.libs.json.JsObject` is play's JSON type. [This SO post](https://stackoverflow.com/questions/31142366/no-json-serializer-as-jsobject-found-for-type-play-api-libs-json-jsobject?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa) might help
I think there is hope for the future: https://twitter.com/gkossakowski/status/991222495779864576
Thank you so much! 
Company did both Java and Perl. Couldn‚Äôt find Perl devs so we switched to Scala, supposedly a happy medium. I personally felt a bit forced and have never been overly keen on Scala. Turns out the company now can‚Äôt hire Scala devs and I actually do like it. (I mean it was always going to be better than Perl)
Kotlin worse than Haskell
Coolio 
 Elixir, Elm, F#, Rust All past top 50. Tells you something about the HN/reddit programming communities. 
Weirdly, you're the second person to tell me that the code isn't properly formatted, which is odd b/c it appears formatted in my browser. Sorry about that; I added the spaces. Thanks. Also, I've read that post before and it doesn't seem like the solutions apply. 1. The first recommendation is about version consistency between play and reactivemongo. My reactivemongo version is: "org.reactivemongo" %% "play2-reactivemongo" % "0.13.0-play26"; the post mentioned concerns "org.reactivemongo" %% "play2-reactivemongo" % "0.11.0.play23". 2. The second concerns using `OFormat` versus `Format`; trouble is, as you can see in my models, I am using the implicit OFormat. I think this is where the error originates though. Because the compiler is having trouble finding a way to translate the nested case classes. 
To rephrase what everyone else has said, the `implicit` keyword applies to an entire parameter list, rather than to an individual parameter. That's a little confusing, but I can't think of a syntax that would accomplish this, while still making it clear that the parameter list works just as well with explicit arguments provided.
Please take TIOBE with a huge grain of salt, there are better rankings like the redmonk one.
I responded [below](https://www.reddit.com/r/scala/comments/8kb89n/question_would_you_be_in_favor_of_allowing_local/dzbznpv/). Below is the relevant part of my answer copy-pasted: &gt; I don't think you would argue that a big chunk of code used inline (and possibly copy-pasted in several places) is better than extracting it into a local variable with even a mildly-satisfying name. IMHO even imperfect names that only hint at what the expression represents are better than no names at all! &gt; &gt; For the record, one of my use cases goes like this: &gt; &gt; val pgrm1 = { ... } &gt; log("Initial program: $pgrm1") &gt; val pgrm2 = pgrm transform { case q"blah(x)" =&gt; q"foo(x)" } &gt; log("Instrumented program: $pgrm2") &gt; val pgrm3 = c.typecheck(pgrm1) &gt; log("After type checking: $pgrm3") &gt; // etc... &gt; &gt; The funny thing is that even while writing this short example, I actually (accidentally) made a copy-paste-induced mistake. Did you see it? 
That's just called type-checking ;)
Yup, I'm aware. :) However, there's a bunch of new features coming soon. Check my issues tab as well. I'm keeping that up-to-date, and I've got a Roadmap as well. Lots of ideas coming down the pipe.
Sql is a programming language? That said this is huge. A big community is really important for library support and hiring ease. I think most business should stick to the top 10 or so languages unless they have a compelling use case, but how much it takes to convince me to use a language is a lot different from slot 20 vs 50.
SQL is Turing-complete.
Bash is both #30 and down in the noise, tells you something about TIOBE.
But as far as I know there‚Äôs no runtime environment for MtG source code.
There is, it just requires two humans. I was told only asymptotic run time mattered. Also: https://www.xkcd.com/505/ 
Kotlin is also still very new. People who use it, love it, but the quantity of people using it isn‚Äôt super high yet 
TIOBE is not a serious index, though. Everything beyond the top-10 shows the craziest oscillations. So while it's great news, I'd be cautious.
This is a first draft and probably left some parts unexplained but let me know your thoughts and if you are using Kafka Streams on a Scala code base I'd be really interested to know.
Yes, that's right, I'll send out a confirmation email with next steps and regular weekly schedule.
Speaking of which, I've just added pause/resume. Making some really good headway. :)
Why should anyone use Kafka Streams if you can use Storm, Samza, Flink, Spark Streaming, and Spring Streams?
That was my first intuition when Kafka streams came out first. But it turns out that for light weight stuff, kafka streams work better than spark streaming. 
And how does it compare to other streaming engines like Storm, Samza, Flink, Akka Streams and Spring Streams?
Which still places it in the top 20, if I read their chart correctly. 
Way rad. Thanks for taking the time to make great docs!
I deeply enjoyed the course. It's the only Coursera course I've properly completed. It gave me a taste for Scala, and I've been programming the language ever since. At roughly the same time, I was working my way through the early chapters of the [Red Book](https://www.manning.com/books/functional-programming-in-scala), and I found the two resources complementary in their very different approaches. The Red Book gets much more into monads. I think I learned about your points 3, 4, and 5 via other resources, but I'm not sure I remember which ones. Certainly, algebraic data types were a revelation for me. The one piece I kind of had to learn on my own was how to structure entire systems. And I think this is also the place where the community is all over the place between free monads and other approaches.
Appreciate the comment :)
The question might be asked the other way around. Why should anyone use Storm, Samza, Flink, Spark Streaming if they already use Kafka? Why adding more complexity to your system by adding one more component? Here is a non exhaustive list of things I find better in Kafka Streams: \- Kafka Streams unlike Spark and Flink doesn't run in the Hadoop ecosystem but leverages the Kafka Topics instead for the stateful operations. So it's much simpler to manage. \- It doesn't have a system of save point based on Java serialisation that works sometimes but instead use topics with KTables in a streaming fashion with your custom serialisers \(Serde\). \- It doesn't have a closure cleaner that keeps you on older version of Scala \(2.11\). You can use 2.13 if you want.
I mean in terms of resource usage. One simple (but common) use case we use kafka streams is for using them as sink to store the data into elasticsearch and hbase. Writing spark jobs for these use cases seems an overkill in terms of memory, one more job to maintain, development/debug times. Kafka streams makes such tasks ridiculously simple. Spark (and others) will have an upper edge for doing anything more complex. In our infra we have almost all sink/sources on kafka streams and all the complex processing on spark. 
I‚Äôve had to go through this argument several times with businesses who have brought in the massive consultancies who have body shops of Spark developers. I‚Äôm not a fan of Confluent streams I prefer using FS2 although that involves effort integrating, I have also used Akka‚Äôs Kafka stream library successfully, again not the biggest Akka fan but it worked well here, was fast and I think the service last I heard had been ticking away in the background for months without issues or restarts it was kind of forgotten about as was just doing its job. What these all have in common is they are light weight, they do streaming only and have simple APIs and combinators, some better than others. Great for writing light weight standalone services consumers/producers and deploy as a container to a cluster. No spark scheduling/scheduler to worry about, no waiting for spark cluster version updates to see that 10x performance improvement. Confluent KStreams try to hide a lot from you for simplicity but I dislike this as yes it joins streams and handles state stores etc for windowing but as it hides it makes it hard to see what‚Äôs going on and can see very high IOPs. Its api is java centric to and it tries to be developer friendly on default options but local machine developer friendly does not always mean production friendly. Unfortunately tweaking buffers / batches etc is hard to get a one size fits all for defaults. I think it is now not the case but Spark was not streaming it was mini batch which is something far different. Anyway playing with all these implementations in big corps with TB to PBs of data. The non Spark options always turned out simplest, least overhead better performing. Spark vendors still carried on using Spark and trailed behind on delivery.
How about using Flume with a Kafka Channel with ElasticSearch and HBase sinks?
Thank you very much for the explanation. It is very useful.
From the top of my head: * FlinkML: machine learning * Gelly: a Graph API for Flink * an official open source RabbitMQ connector * an official open source Cassandra connector
Indeed those are missing from Kafka Streams. Although this article might give an overview of some prediction you can do with Kafka Streams: [https://www.confluent.io/blog/predicting\-flight\-arrivals\-with\-the\-apache\-kafka\-streams\-api/](https://www.confluent.io/blog/predicting-flight-arrivals-with-the-apache-kafka-streams-api/)
This is a great opportunity for someone interested in learning more about FP with Scala. Just out of curiosity, what's the motivation for this?
Well, I tongue-in-cheek [declared war](https://www.youtube.com/watch?v=wi_vLNULh9Y) on non-functional programming at Scale by the Bay last year. Can't wage a war without an army. ;)
Kafka gives a duarble linearized persistence backend for Akka Streams. And basically that's it. Kafka is low latency, high throughput, persistent with fine grained durability (single message or batch commitable, in Kafka or external commit/ACK store), with scalability and fault tolerance (redundancy) built in from the very first versions (group conusmers, topic partitions - needed for scaling/throughput, partition replication, replication leaders - which are needed for low latency). Compared to that Spark is a multi-purpose computational engine. Kafka is a buffer-DB-thingie. Kafka is the firehose to feed other things (Storm, ES, whatever). And of course, over the years the barriers became blurrier, Flink was nowhere when Kafka started. (Samza builds on Kafka, so that's basically the Hadoop connector for Kafka.)
Looks like a reasonable improvement over the status quo. Mixing default parameters and implicits has been considered to be a bad idea in the past though, not sure if this proposal has any impact on the number and severity of interactions and corner cases.
Something I think is missing nowadays: continuous query processing/processor for ES. (Sure, we could store shit in Mongo, or KairosDB - on top of ScyllaDB if we want to be really hip-hop-fancy-rock-ninja-stars. Or ... even in PgSQL. But ES is good for full text, and it's pretty easy to manage/shard/scale. Kibana/Timelion and Grafana both work well with it.) So, enter Kafka. We feed ES from it anyway, therefore putting a stream processor seems trivial, but Storm/Samza/etc are really heavy and too general, compared to something that periodically crunches ES queries on the last few minutes/hours/days and writes results back.
Btw, is there any discussion on why implicits + defaults is bad? I've seen it mentioned sometimes and avoided it as well, but I don't know of rationale.
I think this is much more sensible than calling `.explicitely`on a function, as Martin Odersky has proposed lately.
I'm interested too. I only use implicits with defaults to disambiguate overloads that wouldn't be allowed due to having the same type after erasure: def foo(as: Seq[Int]): Int = ??? //def foo(as: Seq[String]): Int = ??? //won't compile, def foo(as: Seq[String])(implicit discriminator: Int = 42): Int = ??? 
I hope it's "explicitly" and not "explicitely".
Can't see the ad. Post description here.
I hope it's one of those situations where having simpler semantics actually helps obviate corner cases.
Sounds like you made it to the other side of the mountain and into the valley of success :) It seems like most people who stick with it long enough eventually do. Although they may eventually end up on the far side of the valley where some of those inconsistencies and limitations live.
I could comment on English orthography in this place, alas I think more eloquent people have done so and native speakers at that.
Your mistake is either `typecheck(pgrm2)` or `val pgrm2 = pgrm1`. This is a better use case example for sure. However there are still other options, writer monad or something like this: implicit class Helpers[A](val a: A) extends AnyVal { def |&gt;[B](f: A =&gt; B) = f(a) def log(msg: String) = { println(msg); a } } val pgrm = List(1, 2, 3) |&gt; (pgrm =&gt; pgrm.map(_ + 1) .log("add 1 to everything")) |&gt; (pgrm =&gt; pgrm.sum .log("sum elements"))
There is the aptly named `DummyImplicit` value in the stdlib for that as well. It is a bit sketchy, but what can you do?
Haha, I didn't even noticed I had made the `pgrm`-instead-of-`pgrm1` mistake. &gt; val pgrm = List(1, 2, 3) |&gt; &gt; (pgrm =&gt; pgrm.map(_ + 1) &gt; .withLog("add 1 to everything")) |&gt; &gt; (pgrm =&gt; pgrm.sum &gt; .withLog("sum elements")) Well, congrats, you've rediscovered let-bindings :\^P Wouldn't it be better if first-class let-bindings (a.k.a `val` bindings) in Scala behaved like they are theoretically supposed to work? (i.e., by shadowing previous bindings.) If you look into it, you'll realize this semantics actually makes a language's definition _simpler_, so I wouldn't even call it a "feature" in itself.
For Eclipse: Open the main class's .scala file, then right-click and choose Run As -&gt; Scala Application.
What's the interplay of that new notation with implicit function types?
What do you mean by main class Scala file? 
The .scala file where the main class (the thing with the `def main(args: Array[String])` method, or that extends `App`) is defined.
 object MainObject { def main(args: Array[String]) = { functionExample(25, multiplyBy2) // Passing a function as parameter } def functionExample(a:Int, f:Int=&gt;AnyVal):Unit = { println(f(a)) // Calling that function } def multiplyBy2(a:Int):Int = { a*2 } Im on [https://www.javatpoint.com/scala\-higher\-order\-functions](https://www.javatpoint.com/scala-higher-order-functions) and it says the output is 60 and I am so confused on how can somebody please explain this to me, I understand the second example but this one I don't get
I've worked with Spark with supplementary help from Akka for most of my Scala development. I wouldn't say I'm well equipped for every situation but I'm comfortable working with it having docs at my side. I've done what I consider to the "Big Data" stuff with it using all types of datastores within a microservice architecture. Right now I'm trying to find a place with a strong presence in OSS and community support. I'm tired of giving my all to a companies that don't give back.
Stripe is probably hiring and they use Scala: https://github.com/bazelbuild/bazel/wiki/Bazel-Users#stripe
Morgan Stanley's always hiring. Here's evidence of Scala there: https://www.reddit.com/r/scala/comments/480ral/scala_at_morgan_stanley/ https://www.linkedin.com/jobs/morgan-stanley-scala-jobs/?country=gb
Barclays is probably hiring. https://underscore.io/jobs/2017-11-02-barclays/
Twitter is probably hiring https://github.com/twitter?utf8=%E2%9C%93&amp;q=&amp;type=&amp;language=scala
They were hiring a little while ago. Possibly still are looking. I know spotify is looking for Scala engineers to work on their data aggregation pipe. Both seem pretty cool. Thanks for the posts!
Searching for Scala on the website for jobs at the New York Times shows 10 positions: https://nytimes.wd5.myworkdayjobs.com/Tech 
The Guardian is known for using scala. Here's a job listing mentioning it: https://jobs.theguardian.com/job/6718753/fellowship-programme-2018-digital-development/?LinkSource=PremiumListing
Dang. You're really good at this. Very helpful and much appreciated! Do you work in Scala?
We‚Äôre hiring (Apollo agriculture) and I‚Äôd be happy to offer resume feedback.
Could you share a little bit info about the position? Also you can check my resume at: http://yordan.dk/resume.html (would love to hear some feedback too if possible, as I just built my site. CV can also be downloaded from it) Thanks :))
Hello! I appreciate the help. It's been a while since I did the hiring thing \(I kinda skirted around my last interview process\). Here's my resume oh GH [https://github.com/somethingconcon/career](https://github.com/somethingconcon/career) As far as your company. This is exactly the type of org I'd like to work for. I work in #fintech right now and I'm so sick of the money grubbing vaporware for ransom model. I'd love to know more.
agree. It is growing so fast
You look young for a CTO! This is sounding better every minute.
&gt; The one piece I kind of had to learn on my own was how to structure entire systems. I am currently struggling with this. Any thoughts on this? Or pointers to materials you have found helpful in this regard?
Looking sharp my dude! :D
Very high level feedback: * include your location somewhere obvious and at the top. Do you want to stay in Denmark? Belarus? It matters a lot to most places, that‚Äôs the first thing people filter on. * timelines of professional experience and education are confusing.it seems the two overlap? * I‚Äôd probably remove the gymnasium and automotive transport operator section, it sends the wrong message. * linearity the education section, 2 columns is hard to follow. * Give me some axis on which to evaluate your skill level. If you tell me you‚Äôre an expert at 10 different environments, from Wordpress to scala and everything between I‚Äôll assume you only have very shallow knowledge! Bolded skills for expert level is a great technique ( note it near the top) * I‚Äôd include way more detail about your five years of professional experience and much less about courses. Overall, your page is very visually appealing but doesn‚Äôt answer my fundamental questions about whether you are the right person for the job all that quickly. Was that helpful? Happy to expound more on site feedback. Not sure if you have a pdf/paper format resume sitting around but I‚Äôm happy to give feedback there too. 
I found it after you insisted it was there, but footer links definitely get glossed over. I‚Äôd suggest https://www.caseinterview.com/star-method for describing your work experience section. Knowing what your role was more and what you delivered is more important than the partners you worked with etc. CV is a little long, but not out of ordinary for Europe. If you‚Äôre applying to US jobs it‚Äôs one page hard cap‚Äîit‚Äôs just a rule, I don‚Äôt know why. 
oleg-py One example: if you call a method that takes an implicit, the method calling it needs to take the implicit as an argument as well, as you know. If you forget to do this, the code will not compile, except with a default parameter it will, and pass the default instead. A prime example of this being a problem is the old scalaz Task.apply, which took a Strategy ( = ExecutionContext) implicitly, with a default parameter ( = Global). You would forget to propagate an implicit constraint and now your code is running on the shared threadpool. 
Too late. Years has an engineer has my hair on the retreat. I've come to appreciate it as a token of my hard work I guess.
Foursquare is hiring in NYC, SF, and Chicago, and uses Scala as its primary language! https://foursquare.com/careers
Thanks for the reply. You wouldn't happen to know anything about working there would you?
eero is hiring, and we use Scala for our entire cloud environment!
Lucid Software is hiring. We make https://www.lucidchart.com and https://www.lucidpress.com. All of our backend services are in Scala. Frontend is TypeScript, Angular, and WebGL. If you're interested in applying, our careers page is here https://www.golucid.co/careers/ Feel free to shoot me a DM as well.
Can you tell me a little bit about what types of applications you deploy?
Kafka streams is a great lightweight way to do stream processing. It falls over once you need to do data analytics. IMO that's where spark and flink really start to make more sense.
The high level stuff sounds right up my alley. Is your team central to SF or do you employ remote as well?
Remote only if you live in the US. We are looking for a highly skilled senior consultant with a proven track record for high quality and impactful delivery. A true craftsman with solid experience using elements of our platform (Java/Scala, Spark, Kafka, Akka - especially Akka Streams, Flink, HDFS, and other Streaming technologies)who also enjoys breaking down tough problems, and enabling clients to deliver critical solutions. Our team is fully distributed and we will consider applicants from anywhere in North America!
sup matt \(at\) credit karma, just some q's What kind of services are you targeting with Scala? Are any of your services currently in Scala \+ how many Scala engineers do you currently employ? How do you like your commercials!? :D
I am jackson user too but circe also lets you parse the json without having to define to case classes. class JawnParser extends Parser { final def parse(input: String): Either[ParsingFailure, Json] = fromTry(CirceSupportParser.parseFromString(input)) } I have not used circe in PROD but I like their functional way of telling the decoded value as `Either[ParsingFailure, T]` 
What exactly does "competitive" compensation mean? Can we just talk dollars?
We're based in the city. Would say that at the moment that is required, but we're open to relocations. 
The Broad Institute \([https://www.broadinstitute.org/careers/software\-engineering](https://www.broadinstitute.org/careers/software-engineering)\) frequently is hiring software developers for scala based positions. They come and go and not all software positions are scala based so you do need to read the individual reqs
It doesn't, it fails at compile time (this is current behavior btw). Implicit parameters are only distinguished by their *type* in the same way that normal variables are only distinguished by their *name*. Implicit params are designed to provide context, for example a configuration object or an instance of a service. For this use case, you won't need to have more than one implicit param of the same type in scope.
Am I not understanding this right or does there very first example show that this syntax is broken? Doesn't it imply there are two different implicit Ints in scope? How does that resolution work?
I know all that, but the example is this: def f(a: Int, implicit b: Int, implicit c: Int) I don't see how that can work. Nobody in the thread has called it out, so I assume there's a solution.
Yes I should have mentioned I‚Äôm an engineering intern. I only started on Monday but so far have been very impressed with their engineering quality. Check out the engineering blog for examples of the types of problems they‚Äôre solving! They also have open source projects published under fsq.io. They‚Äôre pretty dedicated to code quality (eg uniform style guide) and lots of teams are also using ML if you‚Äôre interested in that. They‚Äôre also expanding quite rapidly and do more than the consumer apps most people are familiar with, including for example powering part of Snapchat and Apple Maps. Happy to answer any specific questions you might have!
Sent a message your way. Cheers!
If there is an implicit Int it will be used in both places. I think the article says so. It's also what I guess should happen, if you are to do that. 
Their client is almost certainly Sky/NowTV 
Man, you're a pretty stellar dude. This is great!
Denoting it with "?" looks counterintuitive though. 
Which company is it that you're hiring for?
Indeed, even Java does that. In fact, it's possible to get a different result: there are cases where type inference wouldn't know what to infer, or infer one of two possible solutions, and if you specify the type, it will use that. Maybe an analogy would be the difference between a student taking a science quiz (not multiple choice) and the teacher grading it. There is some work that is common (they both have to understand the question), but obviously the student's job is both harder and less deterministic.
We do a lot of data access layer and data eng work in scala. We have an internal framework based on some twitter tech that we‚Äôve built up over time. It does everything from low latency model execution (based on akka) to all of our data management for credit data. I run our internal product platform and most of it is in scala with a bit on typescript. I‚Äôd guess we have ~150 scala engineers and probably 40ish in my team. I think the commercials are amusing, especially some of the newer ones. I really liked the Canada commercial. I am honestly not that opinionated about it tho :o
Saved you a click: Advertising and retargetting job 
I don't understand what's problematic in your example. Calling `f(0)` for example would have exactly the same semantics as if you had defined `f` in the old way, as: def f(a: Int)(implicit b: Int, c: Int) What's the problem?
Nope, it's not. I address that [here](https://www.reddit.com/r/scala/comments/8kb89n/question_would_you_be_in_favor_of_allowing_local/dz6bzdb/) and [here](https://github.com/lampepfl/dotty/issues/4525#issuecomment-390147314).
If new release contain only bug fix, maybe it will be time to go 1.0 ?
&gt; Naming in this example at least seems quite straightforwards to me As you probably know, naming is one of the hardest things in software engineering. You can always find better names, but finding the optimal names requires an unreasonable amount of time and energy. The vast majority of local variable symbols out there does not need a great name. This is why it's _fine_ to use `x` and `xs` and `xss` and `i`, etc. in many contexts. Using names like `currentIndex` instead of `i` would actually make things harder to read IMHO. In my simple example, sure I could have spent more time finding good names for every intermediate variable, but I just don't _want_ to, as I am fairly sure it wouldn't have significantly helped anyone figure out the meaning of the code ‚Äì in many cases I think it's just not worth it. &gt; if you move `foo name` out of the `if` branch, it will suddenly acquire new meaning This is by design. Note that **this is already true in Scala today**: val name = "abc" if (true) { val name = "def"; foo(name) } bar(name) Moving `foo` out of the `if` of course changes the meaning of the code! So what you're arguing against really is shadowing. But removing shadowing (and some would argue for it) is not possible, because it makes code so much less composable ‚Äì you can't paste a self-contained expression in an arbitrary context anymore, for example (it's already not always possible in Scala, but this would make it way worse). Instead, how about making shadowing simpler and more consistent, in line with other functional languages?
We've been working towards 1.0.0 for the past year. The `master` branch is for 1.x development. We have also already published 3 milestones of 1.0.0 so far, and should publish M4 in the coming weeks. There are actually a few new features in 1.x, which cannot be provided in 0.6.x because of compatibility concerns. See the [release notes of Scala.js 1.0.0-M3](https://www.scala-js.org/news/2018/02/01/announcing-scalajs-1.0.0-M3/) for some more details. We can't just "declare" the status quo to be 1.0.0, because of binary compatibility concerns. 1.0.0 will be a binary breaking release compared to 0.6.x, and after 1.0.0, we will *never* break binary compatibility again (for some reasonable definition of "never", for example 10-15 years). So all the binary changes we want (mostly simplifications of the internal representation that we use, the .sjsir files) need to go into master before we cut 1.0.0 final.
3.0 miles = 4.83 kilometres. _____ ^(I'm a bot. Downvote to 0 to delete this comment.) ^[Info](https://www.reddit.com/user/bot_metric)
Thanks. Constraint propagation is a good example that didn't cross my mind yet.
How does &gt; (implicit b: Int, c: Int) get resolved?
&gt; Remote only if you live in the US. &gt; we will consider applicants from anywhere in North America! canada exists
But that's the same value used for both parameters. The equivalent default values example given is this: &gt; def f(a: Int, b: Int = 2, c: Int = 3) 
Fair point! Their clients are across the USA, and the benefit of people being able to travel onsite if they want to means they are currently focussing their search on people based in the USA
[removed]
Interesting. As a manager are you also coding yourself? I'll be quite honest just hearing the words "our own internal framework" doesn't make me feel great. I've worked at 3 companies that attempted to do a framework of their own and it was a road of sadness and frustration. That being said I would love to hear more about your team and your process. Can I DM you?
Any info on easing the pain of infinite retention? Maybe I am drawing too much from random complaints on the internet but people make out Kafka to be an operational nightmare
&gt; This is why it's fine to use `x` and `xs` and `xss` and `i`, etc. in many contexts It really only is fine if the naming convention is so overwhelming that the programmer does not require additional context ‚Äì `i` means "current index" as universally as `currentIndex` means "current index". But it only works because the meaning of `i` is obvious. It's not shadowed, there's no `i1`, `i2`, etc. for inner loops, etc. Once you start adding complexities like that, variable names start losing their conventional meaning. Picking names is hard but reading code with poor names is also hard, and we read more than we write. I see shadowing as a somewhat-necessary evil. For me it's more or less ok to shadow variables that are otherwise unused in a given file (e.g. shadowing some unused vars from `import foo._`). Anything beyond that starts to complicate readability.
Ok now you're talking my language. I tend to think of those things as tool\-kits and not really frameworks. What you're describing is an absolute must if you're doing anything involving large orgs with lots of big ideas floating around. My current company I'm focused mostly on internal libraries and tooling. I enjoy working with people to make their lives easier. Also a plus it means I got exposure to almost every single library we use. A big plus for getting a handle on what's available. cats, scalaz, akka \(all of it\), http4s, spark \(bleh\), spire, and even java libraries with proper functional wrappers. One of the big reasons I'm looking for a new gig is the pursuit of a company that can realize the effectiveness of their developers when they acknowledge that sharing and talking about code, code quality, testing, and documentation all revolve around a good team vision with a proper structure. Younger me wanted to be left alone in a room by myself to get my work done. Now most of my time I want to fully discuss our development process as a whole before making large decisions. Code is way more fun when the team is on the same page. Much less stressful as well.
I got into Scala the same way I got into Common Lisp. A friend of mine was into it and sent me some code, and I said that looks ridiculous and gave a list of reasons why. Then went down a huge rabbit hole that's lasted seven years trying to prove him wrong and ending up liking it a lot. My friend left Scala for Clojure.
I did not understand the purpose of never breaking binary compatibility in Scala.js. Considering Scala itself breaks compatibility per 1 or 2 years, `xxx_sjs0.6_2.11` and `xxx_sjs0.6_2.12` are incompatible per se. Why not simply release stable Scala.js versions at the same time of Scala release? If this approach is possible, there will be a chance to introduce incompatible features per one or two years. 
How do I install SBT/Scala on a Raspberry Pi 3? The SBT instructions for Linux are not working.
Is the way some people use shadowing an issue today? Because shadowing _is_ there, just with a few annoying restrictions. xs match { case Some(xs) =&gt; xs.map{ x =&gt; val xs = x.split // does it feel like updating a mutable variable? xs.sum }}
Infinite isn‚Äôt a big a deal in the sense of having infinite retention. What I would say is a smell is having consumers reading from the beginning of time frequently. A once off to build a snapshot then tail the stream is ok but constently reading all history, that's not streaming that‚Äôs batch querying, use a DB optimised for batch querying. If you constantly read all history you are going to be hitting disk rather than utilise the page cache. The second issue is cost. Say you currently have 400GB of data, multiply that by 3 for replication and you are over a petabyte already. If you care about your data you probably want 3 x replication with 2 x in sync replicas as a starting point. 
If we broke the binaries along with versions of Scala, you would need *a different version of sbt-scalajs for every version of Scala*. Indeed, the linker is tight to that binary format, and has to support the appropriate .sjsir files. This would *increase* the overhead of cross-compilation for library authors. So you really don't want us to do that. I also don't want to do that for maintainability reasons: in all likelihood, we will need to continuously support 3 or 4 versions of Scala at the same time. If we break binaries with each version, it means we need to maintain 3 or 4 versions of the Scala.js linker. Not good. Moreover, the Scala.js IR is not just about Scala anymore. We have a prototype of compiling Kotlin/JS to the Scala.js IR to exploit its better optimizer. Those are as many reasons not to break compatibility of the .sjsir files, even across versions of Scala. Breaking the *library* with Scala versions is possible (and sometimes inevitable, as obviously between 2.12 and 2.13 given the new collections, and the fact that we have our own collections), but that's a different story. (I actually *advocated* leveraging Scala breakages for general Scala libraries in my Scala Sphere talk on binary compatibility.) We also of course compile .scala files to .sjsir files differently in different Scala versions, as a direct consequence of Scala/JVM doing the same when compiling to .class files. We leveraged that once in the past to fix a bug that could only be fixed in a binary breaking way ([#2628](https://github.com/scala-js/scala-js/pull/2628)). But that doesn't mean breaking the .sjsir format (and indeed, it was untouched in that PR).
Really? How do you know? More info!
I did not mean the compatibility of Scala.js IR **should** be broken along with Scala version. I meant it **could** be. Especially, I would like to see removing of Scala 2.13+ support in Scala.js IR 0.6, and removing of 2.12- support in Scala.js IR 1.x, so that both library authors and Scala.js authors can maintain less cross versions. Of course, this approach will require an sbt plug-in to switch Scala.js compiler version according to Scala version for cross compilation. 
Around 4 years ago, I joined a company where the architect promoted FP every opportunity he could get. The problem was we were a Java shop, and couldn't switch since our customers also used Java. So instead, the architect would force us to use FP patterns in our Java code, even the specific pattern wasn't very applicable to our code. The result was a mess. Our types were verbose and difficult to reason about, and forcing immutability everywhere made many classes 4x longer than your normal verbose Java class. While I may have been highly annoyed at the architect, he was the reason I discovered Scala. He also exposed me to many FP patterns, and even if I thought the way he used them was wrong, it was also somewhat obvious to me what the value of these FP patterns would be if they were used correctly. A pattern being applied terribly doesn't mean it's a bad pattern. It also took almost no time for me to recognize Scala was a much superior language. Taking the immutable classes example, a 50\-line java class with getters and setters might be 50 lines, the immutable version in Java would be around 200 lines, and the Scala case class was one line. From there I moved onto another job where I thought I would be writing Scala, but it turned out that I spent only 1 week out of an entire year working in Scala. Thankfully, at the job after that I finally found a full time Scala position.
I will be sending out an email shortly, after the form closes on 25/5. :)
my dream is to have a rich set of tools (richer than current scalaz) so that we can then use it as foundation to build "better spark", "better cassandra" etc... thats why I see all of those projects you mentioned as very important, and I think if it works out well it will be game-changing. No more saying "FP needs killer product!"
This kind of makes me want to learn scalaz. I'm wondering what makes their new IO-monad allegedly so much faster than cats-effects (I think he mentioned this in some other talk), and if cats will catch up.
The talk claims the new typeclass encoding is "way better than Odersky's proposal". While I like the effects of the encoding I saw in the video, what is Odersky proposing and how is this better? I imagine Odersky is proposing language-level changes in Dotty, which should be a good thing, right? The same sentiment is echoed later in the video that better language support is desirable here.
If I recall correctly, this was the controversial GitHub Issue: https://github.com/lampepfl/dotty/issues/2047
Oh I assumed it would be about e.g. a `typeclass` keyword, not coherency. Hrm. Thanks for the link, though I won't have time to read through all that for a bit so I'd still like to hear what others believe makes this encoding superior. Regardless, with Dotty so far away, I hope the practical experience we get with the next versions of scalaz/cats typeclasses influences the language design for writing both kinds.
The [Dotty typeclass proposal itself is here](https://github.com/lampepfl/dotty/pull/4153). It's gone through a lot of changes since it was first posted and I haven't been keeping up with them, but in it's original form its lack of support for things like type parameters and higher-minded types made it largely insufficient for what the people it was supposed to be benefiting would have wanted to do with it.
In this whirlwind tour, maintainer John A
cats-effect IO has already caught up, they now have very similar speeds
That's good to know, thanks. I'm curious what kinds of optimizations made the difference there.
Thanks!
Can you please post more information, which commands have you ran with their error messages ?
as /u/zzyzzyxx said the site is wrong. A great tool that allows you to try scala code on the fly is the scala repl, if you have scala installed just type ```scala```in the command line, if not you can use https://scastie.scala-lang.org/ which a web based repl.
Wow. Thanks a lot, kind stranger.
I love my job, it‚Äôs the best job I‚Äôve ever had. I‚Äôm extremely happy. And despite that I should say that the reasons you are leaving sound like common problems at a growing company. My usual spiel is that every single company on the planet is fucked up, cause people work there. You are just looking for the sort of fucked up you like putting time into and not the sort that is life draining to you. It‚Äôs hard but the best orientation is built around knowing and loving why your company is fucked up and empowering yourself (and learning how) to fix it.
I'll bet that next underscore's book will be about ScalaJS ! I'd love to read this book.
This code sample is a good example why shadowing is bug-prone, confusing, and shouldn't be allowed at all IMHO.
Interesting. I can finally give my [type-safe metaprogramming](https://github.com/epfldata/squid) types the (generalized) applicative functor instances I always felt they deserved :\^) trait Functor[F[_]] { type FromObj[A] type FromCat[A, B] type ToCat[A, B] def map[A : FromObj, B : FromObj](f: FromCat[A, B]): ToCat[F[A], F[B]] } def codeIsFunctor[C] = new Functor[Code[?,C]] { type FromObj[A] = CodeType[A] type FromCat[A, B] = Code[A =&gt; B, C] type ToCat[A, B] = A =&gt; B def map[A: CodeType, B: CodeType](f: Code[A =&gt; B, C]): Code[A, C] =&gt; Code[B, C] = a =&gt; code"$f($a)" } I'm not sure what to make of the `ToObj`/`obj` definitions that are shown in the slides as part of `Functor` definition, though. Are they required? The only sensible thing I could do is to make `ToObj = Option` and have `obj[T]` return the code representation of zero value of `T` if `T` is a primitive type. 
Scala does not have an enum keyword (yet). To create an equivalent make a `sealed trait Foo`. A sealed trait can only be extended by objects and classes in the same file. The compiler can thus check for exhaustiveness.
`sealed trait`, huh? Awesome, thank you! I'll go read up on those :)
Personally I stopped after part 1 and went on to read Learn You a Haskell for Great Good and am partway through reading Scala with Cats. I have no doubt that I'd be comfortable now going back to the red book. Scala with Cats in particular is nice \(and free\) with exercises to practice as you go along, highly recommend it.
For production, I prefer to use a scala library like ¬¥Enumeratum¬¥ in favor of Java enums.
This is actually a really great question. I think that part II as recommended is a MUST. While doing so take a look at scalaz and cats. These are the two top FP libraries in Scala. The red book won‚Äôt get you fully prepared to jump right in but the concepts like type classes and data classes are thing you should start to investigate. The red book walks you through mock impls but here you can start to see how FP is done in the real. Also highly recommended is to check out the other Type Level projects. Doobie, http4s, Pureconfig. These are taking those concepts and applying them to their own domain. Good luck and ask as many questions to knowledgeable people as you can!
Many things. Most important to me: - You easily get all values - You can enrich the enums with attributes - You have a sane API to e.g. convert a String to an Enum (e.g. by getting ¬¥Option¬¥). - Lots of support for many libraries like for serialization or database
All of the above are supported by Java enums, or close enough to be equivalent, so personally I wouldn‚Äôt feel the need to reach for another library. Look at it another way, I don‚Äôt need a library because I have the functionality at the platform level.
nuance: `case object`s by default inherit from `Product` and `Serializable`. `trait`s do not. So the better way imo is `sealed trait Foo extends Product with Serializable`. Otherwise, datastructures involving multiple derivatives of `Foo` will be inferred strangely: ``` scala&gt; sealed trait Foo extends Product with Serializable; case object A extends Foo; case object B extends Foo; List(A, B) defined trait Foo defined object A defined object B res2: List[Foo] = List(A, B) scala&gt; sealed trait Bar; case object C extends Bar; case object D extends Bar; List(C, D) defined trait Bar defined object C defined object D res3: List[Product with Serializable with Bar] = List(C, D) ``` [Source](https://typelevel.org/blog/2018/05/09/product-with-serializable.html)
To add /u/LPTK's response: I don't know what /u/lihaoyi thinks about this proposal, but his [classic essay on naming](http://www.lihaoyi.com/post/StrategicScalaStyleConcisenessNames.html) makes some really thought-provoking points on where and how names add value. I personally think that there are places where new names can become a distraction, no matter how accurate they may be. A name is a request that you remember the purpose of something, because at some remote point, you'll need to know the meaning of a value. When that distance converges to one line, a name takes up mental and lexical space it simply doesn't justify. Shadowing can actually be a useful way to note that the previous value can be safely forgotten altogether. This is reminiscent of linear types, which denote values that must be referenced _exactly_ once. It can actually be very useful for avoiding certain types of errors.
I get that Enumeratum has its benefits, but no matter how mature, well-documented, and maintained it is, it's not more so than the enum implementation that comes with the Java platform. Re: unsafe Java methods: now, 'unsafe' is really quite a strong word. I would like to clarify that if you're talking about exceptions, they are actually quite safe; in fact exceptions are the runtime safety valve that prevents undefined behaviour. Now, specifically about enums, the only throwing method that realistically we might want to use is `valueOf` ( https://docs.oracle.com/javase/7/docs/api/java/lang/Enum.html#valueOf(java.lang.Class,%20java.lang.String) ), and that can be quite easily mitigated by any number of exception-handling techniques in Scala.
Can Scala with Cats help if I feel like I'm just a little bit too stupid for the red book right now?
I use Scala daily, but in the Hadoop ecosystem so the code is "fine" to be kind of procedural, but I don't want to rest on my laurels there. I guess I'll take a look. Between Cats, Scalaz, Shapeless, etc. which one is more or less the standard now? Or is one better than the other for dummies?
Shapeless is a completely different thing from Cats and Scalaz, and you won't learn Shapeless from either the Red book or the Cats book. Try the Shapeless book from underscore and the youtube video by the author, they are great. From my understanding, I would say there is no practical difference between Cats and Scalaz from a learner's perspective. If there is an article about scalaz, as long as it does not say it is specific to it (say, the new IO type for Scalaz), it will most likely apply to Cats also, and vice versa. You would have to learn the few differences in names, and that is all. In practice of course there is a difference, but what I am suggesting is that the basic concepts and structure, and therefore the learning materials, are compatible.
&gt; it's not more so than the enum implementation that comes with the Java platform. It is more. It's not only that I don't have to write wrappers for e.g. ¬¥valueOf¬¥ myself. I can also use ValueEnums. That allows me to describe a bunch of Strings or Ints while guaranteeing, that every of my Enum instances has a different String/Int value - at compile time. If someone changes my code and adds another case (by doing c&amp;p for instance), if he does not make sure to use another String/Int value it will fail at compile time. You can't do that with native Java Enums.
Thank you for the response! Maybe you misunderstood me. I am planning to finish the book either way, just wasn't sure if it's better to dive into Part 2 or do some project first. The problem with pet projects is that I never have idea that is large enough (more than a few 100s LoC) or small enough that I don't have to invest my time in learning some "business logic" (which drags me away from my main goal of learning the language). Nevertheless, I think I'll go for Part 2 and use your advice when I finally start pet project. Thanks!
I think I took it. It‚Äôs the Martin Odersky intro one? Incredibly painful and delivery is boring as hell. I don‚Äôt think I learnt anything. It takes times to get into functional programming. The best book is the read fp in programming book.
That and the helper methods to read in Strings in CamelCase or similar variations. And 3rd party support like play json out of the box. It's just convenient in production 
haha, I think it can be a little intimidating to look at those libraries before the FP stuff REALLY makes sense. You have to keep in focus that these libraries exist and an FP replacement for Scala's lacking standard lib. They're the most generalized forms of FP concepts. You'll see things like Monad\[F\[\_\]\] all over the place. What helped me read through these things was to take what cats and scalaz have done and make implementations out of them \(most of the stuff is build from subtype or general type classes\) so it's meant to be the interface \(in the classic Java sense\). If you've not done this level of stuff before it all seems greek/ Ask questions!!! Keep learning :\)
Great advices, thanks! üòÄ
I think Ray tracers are good medium sized projects. Plus you can make cool images with them. The math gets a little complicated but that keeps it interesting
I have it installed, but never really used it and dug into it so I'm not aware of its benefits. I'll look more deeply üòÄ I use IntelliJ with Scala plugin and sometimes use its REPL. What are main differences when working with amm?
There's a lot actually. The amm repl is better for editing code in blocks (in standard repl you have to do :paste and it gets annoying to always forget and need to jump around. amm makes these super easy by having it as a default. You can import ammonite scripts into other scripts. Say you're not super sure about pacakge structure or don't want to make a big decision. So bringing in small scripts or classes becomes much easier. the use of the $ivy command as the gist "kinda" show is the main reason I use it. You just need amm and 1 file to try out code and they can be completely independent. there's also built in support with ammonite.ops for reading / writes files (this is HUUUUGE) and also a few other libs for easy use scalaj http client. there are definitely others I think help but this is really why I like it. Intellij support of Scala is lacking and the tooling isn't even remotely correct. I like to work in amm before I move code into a project because it helps so much to flesh out ideas before I even think about project structure and whatnot. This might be a stretch but if you've ever worked with Ruby... an irb repl is just the language's repl and doesn't do much to support complex projects BUT the rails c repl will bring in dependencies and provide you with a better, more accurate place to run functionality by hand for like benchmarks or whatever.
What do you mean with "unwrapping those functions and using good variable names"? I don't really see how use good variable names is the opposite of using local functions. If you are talking about local defs, I am pretty sure they are compiled to private instance methods, so there should be no effect on performance.
Great! I think I saw somewhere that Ammonite can be integrated with IntelliJ so I'll look into it
like /u/yoohaemin said shapeless is different entirely IMO Cats is easier to learn/use than ScalaZ because it uses pronouncable names for all of the methods whereas \(last I checked\) ScalaZ had some weird\-ass names here and there like \&lt;\*\&gt; and \\@/ There's nothing inherently wrong with that, I just find it harder to internalize and googling for docs when you use names like that can be.. a pain in the ass.. As far as standards go, can't say either one is that dominant right now. It's still all relatively niche AFAIK.
I decided to run a test. I built this silly test class: object Test { def fun1(a: Int): Int = { def fun1Impl(x: Int): Int = x + 1 fun1Impl(a) } private[this] def fun2Impl(x: Int): Int = x + 1 def fun2(a: Int): Int = fun2Impl(a) } And disassembled it: Compiled from "Test.scala" public final class Test$ { public static Test$ MODULE$; public static {}; Code: 0: new #2 // class Test$ 3: invokespecial #12 // Method "&lt;init&gt;":()V 6: return public int fun1(int); Code: 0: iload_1 1: invokestatic #18 // Method fun1Impl$1:(I)I 4: ireturn private int fun2Impl(int); Code: 0: iload_1 1: iconst_1 2: iadd 3: ireturn public int fun2(int); Code: 0: aload_0 1: iload_1 2: invokespecial #25 // Method fun2Impl:(I)I 5: ireturn private static final int fun1Impl$1(int); Code: 0: iload_0 1: iconst_1 2: iadd 3: ireturn private Test$(); Code: 0: aload_0 1: invokespecial #26 // Method java/lang/Object."&lt;init&gt;":()V 4: aload_0 5: putstatic #28 // Field MODULE$:LTest$; 8: return } At least with this toy example, the nested function should be minutely more performant. It doesn't have the extra `aload`. Is that because it's reusing the caller's stack frame? I'm not sure.
That makes perfect sense. I like a bit of sugar (cons list :: , infix, etc) but since this goes to production at work where people are on call Id like legibility and googlability. Thanks! 
Ah, of course. `fun1Impl$1` is static while `fun2Impl` is not. How'd I miss that?
I think they get lifted into method parameters.
No problem! The approach Cats took is to add some of that syntax sugar but it's always aliased to a method name you can pronounce out loud.
I feel like there's a missed optimization opportunity here. I'm no expert in this domain and the JIT would probably de-virtualize these calls easily, but could an `object` with no super traits and only definitions in its body could be encoding completely with static fields/methods? That seems like it could be a net performance win. I suppose there's a binary compatibility concern should one add a super trait or statement where the encoding has to change, but that also seems like primarily a concern for libraries, so maybe an off-by-default compiler flag would be a good idea?
local functions, if you're not assigning them to `val`s should have no difference in performance. they encoding of them in bytecode is exactly the same. 
Except if you do this for 100k lines and have terrible perf, you are in for a 6 month project to fix perf. Better to know perf goals up front and frequently measure and test. 
Not wanting to sound snarky, but you should read the sidebar. Also, /r/lostredditors... Beyond that, there is no concrete question in your post.
It might okay question just lacks so many details, what and how is he parsing, what is he actually trying to achieve and whats wrong with current solution/state 
Don‚Äôt use Scala and performance in the same sentence. You combine difficult to spot implicit class conversations, maps that duplicate data structures and just the overhead of unboxing Options, you realise that asking questions like ‚Äúis there a performance hit with feature X‚Äù is always a losing argument. You‚Äôve been slapped in the face with a hundred performance-impacting cocks. You can‚Äôt justify not taking one more to the chin. People who say that‚Äôs bad code because of the performance impact are writing programs in the wrong language here.
java.nio.file.WatchService? https://www.agileand.me/fs2-java-nio-watchservice.html
Might i ask two questions? * How many years are you a programmer? * How many years are your coworkers already programming? 
Straight out of upwork
Here: [https://github.com/lauris/awesome\-scala](https://github.com/lauris/awesome-scala)
These are all pretty small datasets but would be interesting to see spark mllib compared as well. I think you'd have to do job time against an already running cluster though or else spark startup time would definitely kill you.
Hi, I am looking at your results and I have a question. How do you pick the -R parameter, i.e., the throughput? In your results the value of -R correlates with the "Requests/sec" reported value. Do you manually increase it until you reach a point where the value of "Requests/sec" starts to decrease, i.e., you have reached the maximum load that the server can process? Thanks!
doddle-model provides in-memory implementations (like scikit-learn) so datasets need to fit into RAM. Nevertheless, I agree that benchmarks with larger datasets should be added.
I started my Open Pair Programming project about a week ago and have already done 4 sessions with people. It's been really great, I think it's been helpful to others, and I've definitely learned a couple things. I'm super excited to write up some of the results.
Nice question! -R is a throughput argument (in total requests per second). Please see docs and sources of wrk2 to understand it in details: https://github.com/giltene/wrk2 Yes, I've increased it manually from 50% of expected max value because Play (as most of others contemporary Java/Scala HTTP frameworks) cannot provide SLA (acceptable response time ~10ms for 99th percentile and ~100ms for max) for me after a short-term spike of requests with too high rate when CPU usage is greater than 95%, so for these tests (as in production too), you should restart a whole service and warm it up at 50% of max. load rate.
Please use sbt-jmh for benchmarking - it will help to got stable results and realistic performance model. Also it simplify running of parametrized benchmarks, profiling (through JFR or async-profiler) and visualization (by uploading JSON output to JMH Visualizer). For more info about how to write benchmarks with JMH please see samples: https://github.com/ktoso/sbt-jmh/tree/master/plugin/src/sbt-test/sbt-jmh/jmh-asm/src/main/scala/org/openjdk/jmh/samples ...and Shipilev's articles, starting from: https://shipilev.net/blog/2014/nanotrusting-nanotime/ and 
Moving code blocks into named functions has the effect of documenting the intent of that code block, which loose code does not, unless you explicitly add comments. I prefer my code to express it's intention to human readers through variable and function names rather than comments (when practical), so I would probably side with your coworkers. How about showing us an example that illustrates your case? As far as performance considerations, modern compilers already inline functions and unroll loops, etc. when they believe it will help performance, so you won't be able to use performance considerations to support your preference.
Having a read through over morning coffee now. I noticed the link to the examples is broken: https://github.com/KenSuenobu/scattersphere/wiki
How do you determine what is an "acceptable response time", which in your case, if I understand correctly, is " ~10ms for 99th percentile and ~100ms for max", although looking at the numbers I see that the measured max is around ~30ms. This may be common knowledge, but I am new to measuring server performance. Are there some general industry guidelines regarding SLA? Any reference to more information is welcome. I already watched Gil Tene's videos that you mentioned in your other response, and that already gave me a lot of insight into what is worth measuring. I would like to run some experiments comparing GraalVM and GraalVM-Native running Netty and I was wondering if it would make sense to compare the two VMs under the same workload, e.g., 100K req/sec. I would like to compare the latency distribution between the two, i.e., the Hdr Histograms. At peak performance I expect GraalVM to achieve better throughput. However, I think that GraalVM-Native will probably exhibit less outliers, i.e., a lower max, as it has a more predictable performance since it doesn't need to JIT.
Couple of questions regarding SparkConf/SparkContext lifetimes. 1. How long do SparkContext/SparkConf objects live? Are there timeouts, and if so, is there a way to ping/refresh the objects so they stay live? 2. Can multiple SparkContext objects be used at the same time? Thanks!
Don't put managed state in your objects. Create config in you app main and pass it around (that would be one option). Objects are for constants and stateless functions.
Different business require different SLAs. It can be from microseconds in High Frequency Trading up to seconds for the transcontinental messaging. In my case it is Real-Time-Bidding on OpenRTdB exchanges which require from responses to come in less than 100ms. The 99% percentile is where most of valuable bids occurs, because nobids are ~95% of all responses and are much faster due don't have any payload. In reality response times for bids are much greater because transfer time and checking of targeting with CTR prediction adds to delay. Usually industry guidelines regarding SLA contains only availability numbers and the Incident Response Time. 10 year ago Amazon reported that every 100ms of latency cost them 1% in sales. Also, there was lot of links about how latency influence on business at the http://highscalability.com/blog/category/hot-links I wish good luck in your experiments, and if you will have time, please check it on different rates and types of workload (different size of messages, different serialization libraries, different business logic to handle them, etc.). Also please do profiling with async-profiler, it will allow you to build more realistic performance model and spot lot of interesting internals which are matter for latency: https://github.com/jvm-profiling-tools/async-profiler 
Cool, that was really easy to understand! Thanks!
I wouldn't draw big conclusions, optimizing compilers are a moving target. It's more interesting to ask the graal developers why certain code patterns are slow &amp; what prevents them from being optimized. The slowdowns caused by the high cost of polymorphic dispatch on small lambdas (like summation) is being worked on. 
 Traditional Object oriented way of passing config object in constructor to another object creation works but i want to inject the config object dynamically like i want to avoid boiler plate code to pass every time config object to other companion object . Any other way how experience Scala developer handle such scenarios.
Or pass it as argument to function. Why do you consider it such hassle? On how many places do you need config?? Looks like you are trying to solve wrong problem. Depend on cfg in your outermost layer, once. If ypy ate resolving it in random places you will have hard time (you already do). Do not try to introduce some magical solution. Then people complain Scala is hard. No. Its simple. Just do the right thing.
Spark session is actually grandfather of spark context. There is builder function from which can create or get spark session and finally give spark context. sparkcontext can be stopped and can be again generated from spark session . if you have seen streaming context you can stop it and agin create it at every window timeout operation. I don't think multiple spark context is possible as it is given by spark session on which you don't have any control.
Okay, from what you're saying, it sounds like a session generated from a spark context cannot be shared. But a spark context can create multiple sessions, and if that's the case, it solves #2. I'm just wondering if there's an "infinite lifetime" to #1? Reason I ask is, I've used JDBC drivers in the past that have lifetimes, and they often don't timeout until they receive a SQL query. I'm hoping SparkContext/SparkSession doesn't behave the same way. :)
This is a perfect use case for implicits.
Very nice. I always struggled to understand what so special about \`Kleisli\`. In the end you can do all same stuff with \`for\`\-comprehensions. But in fact what \`Kleisli\` gives us is a "point\-free" notation, much nicer and we're eliminating one of two hardest problems in programming \(which is "naming things"\).
I'd add that while, on one hand, it's true that "`Kleisli` is just a wrapper for an `A =&gt; M[B]`," IMO the key thing is that, whatever typeclass instances `M` has, `Kleisli[M[_], A, B]` also has. That means in code that wants an `M[B]` where e.g. `M : Monad`, you can pass a `Kleisli[M[_], A, B]` instead, and still operate on it with _its_ `Monad` instance, just like you could pass an `M[B]` and operate in it with _its_ `Monad` instance. And that's how it's the `Reader` `Monad`: you get to ignore the `A` in code that only cares about the `M[B]`, but when you need it, it's right there in scope for you to take advantage of.
How does the performance compare to jsoniter-scala?
I have a question about tagless final encoding. If I understand correctly \(and correct me if I'm wrong\), its main purpose is to abstract over the effect type used throughout our application. But what are the benefits of using that style, if my effect usually ends up being a Future/Task, and I don't care about the boilerplate related to using *Future.successful\(...\)* o*r Task.n**ow\(..*.\) in tests \(instead of the Id Monad\)?
Okay, got it. :)
How I wish this article existed before I started learning SBT! I had to learn all these the hard way by digging into the SBT source code.
Does the test result tell you which cases violated the laws? Maybe you could explain what you're doing with the eval/defer stuff? I don't really get it. Isn't your `foldLeft` actually just doing a `foldRight`?
&gt; Maybe you could explain what you're doing with the eval/defer stuff? I don't really get it. Isn't your foldLeft actually just doing a foldRight? Sure! So the defer is due to me experimenting, because more laws were failing (there's a law that states that foldRight should be lazy). For the implementation, I followed cat's documentation : implement `foldMap` in terms of `traverse`, then `foldRight` in terms of `foldMap` and `foldLeft` in terms of `foldRight`. FoldMap requires a monoid, and doing some type-driven development, it looked like having an endomorphic monoid for `Eval[B]` was the only way to provide a monoid in order to call `foldMap` in the `foldRight` implementation. &gt; Does the test result tell you which cases violated the laws? Yes. [Here is my test](https://gist.github.com/Baccata/0b972953cd65421d6a475b37aadad65b#file-foldablefromtraversespec-scala). Here are the failures : OpF.traverse.foldM identity *** FAILED *** GeneratorDrivenPropertyCheckFailedException was thrown during property evaluation. (Discipline.scala:14) Falsified after 9 successful property evaluations. Location: (Discipline.scala:14) Occurred when passed generated values ( arg0 = List(-313854204, 0), arg1 = Set(0), arg2 = org.scalacheck.GenArities$$Lambda$1306/1615822671@5ea502e0 ) Label of failing property: Expected: Set(-1) Received: Set(-1839344464, 51468362) OpF.traverse.collectFirstSome reference *** FAILED *** GeneratorDrivenPropertyCheckFailedException was thrown during property evaluation. (Discipline.scala:14) Falsified after 18 successful property evaluations. Location: (Discipline.scala:14) Occurred when passed generated values ( arg0 = List(2147483647, 316586117, 1), arg1 = org.scalacheck.GenArities$$Lambda$1195/2039479486@5bd1ceca ) Label of failing property: Expected: Some(Set(1)) Received: Some(Set()) 
&gt; What's your foldLeft actually doing to reverse the direction Thanks a lot man, that was the key question that helped me solve. I reimplemented foldLeft using directly `foldMap` and using a different `Monoid` (that calls upon `andThen`rather than compose). The `defer` appears to be needed though. I am really not sure why, but removing it breaks the laziness checks. 
I find tests are a lot more reliable if you can avoid futures in them - apart from anything else you don't have to define a timeout, and it's easier to get a meaningful stacktrace if something goes wrong. Generally I'd find tagless final more useful for intermediate level effects like in the "second layer" of http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html . Being able to swap out a test implementation of a more semantic/business-level interface like `UpdateUser` is quite nice. You're right that something like `Future`/`Task` only really runs, so there's not so much value in swapping it out because there's not really any more structure to it - even in a test you're not going to swap out a very different implementation, because the only thing you can do with those types is to run them.
The website is broken when visited through Android's Firefox
Regular firefox too.
Useful as always
Looks really cool! I'm wondering if it helps webdevelopers to integrate with Outwatch, to use Hepek Objects in outwach components and vice versa.
This is probably an FP more than Scala, but I figured it's the right forum... foo\(x\) { int y = \&lt;somevalue\&gt;; return bar\(y\); } If we consider "bar\(\)" to be/have a side\-effect to foo\(\), is it possible to rewrite foo\(\) in a more pure FP way? What if you had code littered with this kind of stuff \- is it worth rewriting or would you take on writing only new code in an FP way? I'm seeing this all over our code base \(Java\) and some kind of an FP bug has gotten up my butt to the point where I want to rewrite it all \- clearly an impossible task w/millions lines of code.... Here's one more question \-can TRULY pure FP code be written to the point where you have no 'side\-effect' type of calls, but . everything is a call to a function, e.g. f\(g\(h\(i\)\)\) so on and so forth, embeded ad nauseum.... And here's one more bordering on FP zealotry, the idea has come to my mind \- I know how to take any for/while whatever loop and pretty much transform them into recursive functions equally... but I have trouble with conditionals... how do you FP\-ify something like "if/then/else"? Dictionaries? Maps? Just curious as to how extreme one can get in FP, and specifically to this thread, Scala. Thanks!
Thank you for explaining!
Thanks, I appreciate it! Those libs are used for DOM manipulation in JS. Hepek is more of a server\-rendering thing. One way of using it could be for HTML templates like in Angular 2\+. You could reference them directly from your component, and vice\-versa. That would be a great thing I think! :\)
no
Thanks guys, will pass on to our dev team!
Thanks for the heads up guys, will pass on to our dev team!
Thank you for pointing that out, an issue has been opened in the repo.
Thanks, I just skimmed their README. For now I'm staying with Scalatags. Next thing I wanted to integrate is Playframework because Twirl is so ugly... :DPlease report any problems you find with hepek. Feature requests, contributions are very welcome! \^\^
_ReceivedJobStart()_ gets called once by _preStart()_ at the very beginning, then again every 10 seconds by scheduler. That's correct isn't it? We don't wait for doWork() to complete, it runs every 10 seconds by the scheduler. Since the duplicates are immediately after each other, we think it's from the same job instance, rather than two job instances 10 seconds apart processing the same records. 
I think (I maybe wrong though) there are two places where an event may be processed twice. 1. A scheduled task is started before the previous one is complete. Since you do not wait for completion of *doWork()* there may be a case when an event is *to be* processed by the first *doWork()* (but it is not processed yet). So, the *getUnprocessEvents()* in the second *doWork()* may return this event. Such event will be processed twice by two *doWork()*s. You can fix this by waiting for completion of *doWork()* before scheduling a task. 2. Actors are asynchronous. So, even if the previous *doWork()* is completed when the next scheduled task is started, there could be events that are *to be* processed by *EventListnerPoolOfActors*. Completion of *doWork()* only means that it completed sending unprocessed events to *EventListnerPoolOfActors*, but not that these actors completed processing them. That means that you need some kind of confirmation from *EventListnerPoolOfActors* for each event it processes if you want to avoid processing same event twice. A simple solution would be to just count the number of replies from *EventListnerPoolOfActors* and when it reaches *x.size* schedule the next task. This all is IMHO, I am not a very experienced hakker myself. 
Apparently it's because of [trampolining](https://github.com/typelevel/cats/issues/122#issuecomment-73435779) capabilities
Thanks!
Why do you dislike Scala?
What does the db call do? Maybe add some instrumentation and logging that could help you get more information next time it happens. For example where possible you could add on both the sender and receiver logging statements. If also make the publishers in this case wait on the future using a simple fsm/context.become
&gt;It isn't clear to me how to deal with multiple Options in a nice way. You could do it like this: override def degf: Int = { (cluster, strada) match { case (None, None) =&gt; df.count().toInt - 1 case (Some(c), None) =&gt; df.select(c).distinct().count().toInt - 1 case (None, Some(s)) =&gt; df.count().toInt - df.select(s).distinct().count().toInt case (Some(c), Some(s)) =&gt; df.groupBy(s).agg(countDistinct(c).alias("total")).agg(sum("total")).head().getLong(0).toInt - df.select(s).distinct().count().toInt }
All of those if/else could be match: if (strata.isEmpty &amp;&amp; cluster.isEmpty) Could be: (strata, cluster) match { case (Some(s), Some(c)) =&gt; case (Some(s), None)) =&gt; case (None, Some(c)) =&gt; case (None, None) =&gt; }
The TsDesign methids are selecting behaviour based on the Strata and cluster members, however these are fixed at construction so it seems uneccessarily complicated and inefficient as it stands. I would be tempted to replace it with a type that's the union of 4 case classes, one for each None/Some combination of strata and cluster. Then the methods don't need the big if/then/else expression.
Writing for the Lightbend stack, others can tell you about Monix / http4s. I don't see anything in here that points to FP specifically, though. Play is now built on top of akka\-http by default, and uses Akka throughout. It supports streaming through Akka Streams. Kafka integration is done through Reactive Kafka, which is again Akka Streaming based. You can add and remove publish / subscribe to a stream dynamically through MergeHub/BroadcastHub. Play does support Macwire. * [https://github.com/playframework/play\-scala\-rest\-api\-example](https://github.com/playframework/play-scala-rest-api-example) * [https://github.com/playframework/play\-scala\-macwire\-di\-example](https://github.com/playframework/play-scala-macwire-di-example) * [https://github.com/playframework/play\-scala\-chatroom\-example/](https://github.com/playframework/play-scala-chatroom-example/) * [https://doc.akka.io/docs/akka\-stream\-kafka/current/home.html](https://doc.akka.io/docs/akka-stream-kafka/current/home.html) So you'd set up a controller to write into the Akka Streams Flow, which would go into Reactive Kafka, get published, and then provide an HTTP result. The data store is something you can manage through [akka distributed data](https://doc.akka.io/docs/akka/2.5.4/scala/distributed-data.html) if you want to get fancy \-\- you can set up multiple Play servers to load balance, and then set up them up as an Akka cluster.
For the life of me, I can't understand how map is written. .map\(x =\&gt; x \* 2\) I know that this example will iterate over a list and multiply each element by 2... but why is it x =\&gt; x? \(I know it can also be written as .map\(\_\*2\) The specific example I have is: .map\(blahblah =\&gt; *ColumnNum*\(game.indexOf\(blahblah\)\) Why can I use blahblah, why can I use anything in those places?
`val EventListnerPoolOfActors = ActorSystem().actorOf` Why are you creating a new actorsystem inside your actor?
Yes, but also get rid of all the unnecessary `()`.
I second u/amazedballer's recommendation. Especially when it comes to what you've mentioned as your previous experience with the ecosystem. You would be right at home, and likely appreciative of all the development in all of those techs. With one other person who had no prior Scala experience, I successfully launched a project similar to what you're describing in about 6 months and it was a big win for my company. That's not to say anything bad about going the functional route. I'm just enthusiastic because my experience went really well.
Scala.js happened and has grown into a mature platform with a good variety of libraries available from both JS and Scala land.
Not Scala, but for the caching side of things definitely take a look at Apache Ignite. It's Java, but I've used it in multiple Scala projects with great results. It has a lot of power under the hood and goes way beyond (distributed) caching.
How ready is http4s do you think? I remember using it six months ago and I had to give up after a while. So far I've been sticking with Finch.
Hmm. I've been using it for years now without issue (in high traffic production environments). What about for you find non ready 6 months ago?
Thanks for spotting it, was inherited like this. Will change to `context.actorOf` instead.
The db call returns a `Future[Seq[Event]]`. I'll add some logging to sender and receiver to see try get more info. Using FSM makes sense, will see the result of the logs. Just not convinced the current problem is caused by lack of FSM yet.
At my project we keep the backend interfaces in sync with our home-grown [scala-tsi](https://github.com/code-star/scala-tsi), which exports Scala types to Typescript interfaces on compile. This keeps things in sync while being unobtrusive in the front-end toolchain
you are not missing anything, scala.js workflow is in that case slower then ts
4 seconds sounds quite good actually. So after using Scala for general programming for almost a full year, I finally got time to look into ScalaJS. The experience was OK, the compilation time was slow as you described; it's even slower for me as I'm using cats and shapeless for better type safety. I tried to cut down the feedback time by simply relying on the compiler in that if it compiles, it probably works and will look alright. I'm using [Bindings.scala](https://github.com/ThoughtWorksInc/Binding.scala) for the presentation and in almost all cases, things just work. If you're interested, the project I wrote is [here](https://github.com/tom91136/jmh-view/) if you want to take a look . 
These methods are useful if you're willing to deal with a couple of things, such as Graal, which sometimes show regression against their conventional alternatives. However, if you can cut it by using tools like Bloop and Coursier or build tools like Mill then that should already give you very reasonable speed-ups.
FWIW I have found scalatest easier to use than specs2. I spend less time reading the docs and more time writing tests.
Just wondering how "diff" objects will work with eventually consistency and other chaos. My 2 cents on lib recommendations: 1) Scalacheck or something other from the quickcheck family. 2) If you are going to scale efficiently then consider using of Blaze or Colossus instead of http4s or akka-http: https://www.techempower.com/benchmarks/previews/round16/#section=data-r16&amp;hw=ph&amp;test=json 3) These days, for serialization you can pick an efficient JSON parser/serializer which competes on par with best _binary_ serializers for Java and Scala: https://github.com/dkomanov/scala-serialization/pull/8 
Try not to repeat similar pattern, extract method if you could. Just by relying on IntellijIdea without knowing what you're doing I found this pattern repeated 17 times and that it actually can be extracted as method within surveyDesign trait. ``` private def thisIsRepeated17Times(est: Column): DataFrame = { df.withColumn(pweight.toString(), when(est.isNotNull, pweight).otherwise(0)).withColumn(est.toString(), when(est.isNotNull, est).otherwise(0)) } ``` 
Implicits and macros can slow down compilation time significantly. Have you tried [React4s](http://www.react4s.org/)? (shameless plug) It's tiny and uses no implicits and no macros. In my experience, it's pretty quick to incrementally compile React4s projects with sbt ~fastOptJS.
Are you using [sbt-revolver](https://github.com/spray/sbt-revolver)? It helped my workflow significantly, but it's still not as fast as a js setup. I've found that leveraging the compiler - i.e. using more expressive types - helps a lot because I'm not waiting on the js to render, I'm spending lots of time lining up my types comparatively.
remote?
"a few seconds" is pretty much as good as it gets with Scala.js. In my opinion if you want to be happy in Scala.js you have to choose libraries which let you write correct-at-compile-time code. Needing to check your code in the browser every 5 seconds is a Javascript developer's curse. If you use Scala.js and you still have to do that, you're not really getting the benefits of Scala.js. As a corollary, you should probably keep CSS generation outside of the Scala world ‚Äì feed LESS or SASS files to webpack for much faster hot reload instead of using ScalaCSS or inline styles. I'm saying this because those tools guarantee syntatic correctness of CSS, which is most assuredly not enough for what you actually care about with CSS ‚Äì visual correctness. And really, if all you need are shared models, there are plenty of tools that let you define a model in a common format and generate Scala case classes and Typescript classes / interfaces out of that. 
`x =&gt; x * 2` is a function that takes in a parameter named `x` and returns `x * 2`. You can pick any name you like for the parameter, eg. `blahblah`. `.map` is a method that applies the supplied function (eg. `x =&gt; x * 2`) to each element in the collection. 
It's all on a case by case basis, no hard yes or no. 
The `map` function takes another function as a parameter. Scala allows you to create anonymous functions with the syntax `(param1: Type1, param2: Type2, ...) =&gt; { /* function body */ }`. It also allows you to simplify certain common cases, like omitting the `{}` when the body is a single expression, omitting the parameter type when it can be inferred, and using `_` while omitting `=&gt;` when a parameter is used once and a name is not particularly useful. So `.map(_ * 2)` is the same as `.map(x =&gt; x * 2)` is the same as `.map( (i: Int) =&gt; { i * 2 } )` is the same as `.map( (someParamName: Int) =&gt; { someParamName * 2 } )`. All you're doing is naming the parameter so, really, it's not functionally different than if you had written out a proper function; you can freely choose your parameter names there too. In fact, given `def double(i: Int): Int = { i * 2 }` you can write `.map(double)`
This seems to be out of date. Spring dropped Scala support years ago.
Hm, this looks impressive. I should definitely take a closer look to React4s, thanks.
Is revolver usable / needed for frontend? I'm using it for JVM backend and it provides for some really fast turnaround. Type system is great for making sure things you write are *correct* logically, but for visually nice things it's... eh :D not even close.
I don't know of a tool that can give types as expressive as true Scala. Our backend uses refined types and newtypes wherever they make sense. I figured that thing about CSS generation, and it's really nice with Slinky, but coming from something like `glamor` writing styling separately from components feels like a downgrade. There are a plenty of benefits we can get if we switch to SJS - right now there's even a plenty boilerplate to parse the JSON out into correct types, and with `circe` on both ends that would be free. And HKTs and nice abstractions might be super helpful for some features we have planned. But a significant chunk of effort is now still spent with markup and styles, and a significant slowdown in this area might be not worth it, so I'm asking if I'm missing something :)
Thanks for sharing it, this actually looks like it could be of great help!
I meant using revolver on the backend so that your shared models get turned around quickly. I feel you on the pretty. Recompiling every time I change a css class in scalacss when I'm just trying to center the freaking text... 
I can't even do Scala without cats and shapeless anymore :) But last time I tried Binding.scala it was super slow (but it was two or so major versions ago). I'll take a look at the project and try tinkering it to see if something like your setup would work for my needs, so thank you for your response.
That's good to know. I just mentioned Spring here to follow the history of DI frameworks.
Is remote work a possibility? I live in northern New England but work remotely for a place in Cambridge now.
Oh don't get me wrong, I love what the Scala language has to offer, I'm just trying to warn you against treating Scala.js as a better Typescript. Typescript is not nearly as safe or as expressive as Scala, but it's well optimized for integration with tons of different JS libraries. While Scala.js the language has great facilities for JS interop, specific libraries that provide Scala.js interfaces to complicated JS libraries like React leave a lot to be desired, at least the one's I've used. JS programming style for which those libraries were invented simply does not always translate well into Scala. On the other hand, Scala.js is more powerful the more you use pure Scala solutions. However, I personally find Scala UI libraries lacking, enough so that I had to write my own (Laminar). Writing a library is obviously not an option for anyone who's in it just to get things done though.
&gt;We also considered *getUnprocessEvents\(\)* returning duplicate records, but it's getting them from the database and there're no duplicates in it because the event's Unique ID is a database constraint. But you make multiple calls to getUnprocessEvents\(\). What ensures that each call returns a distinct set of records? 
Been doing this myself, not quite on an Enterprise^TM level project and it's such a joy. Makes me never want to touch raw JavaScript again. My biggest gripe with the JS ecosystem, even when using TypeScript, is the extremely poor runtime type support. Scala.js fixes this for me almost entirely and makes it possible to design composable systems that not only compile correctly but also infer things from types at runtime somehow. Using a JSON library in Scala is so, so much better than having to manually validate and parse it like you have to in JS/TS. It's a common misconception that in TypeScript you can just parse something and say "as MyType" and it'll validate against it. That's simply not true, and can lead to runtime errors, whereas in Scala.js I can be completely sure that the incoming and outgoing JSON are properly formatted.
I think you hit the nail on the head with that, thanks so much. I'm doing a university degree and I'm used to other languages having to define variables before they're used. It tripped me up hard when I saw Scala using random "values" (in Scala's case) that I got confused. 
Why not just using implicits for dependency injection? I don‚Äôt see anything benefit of these libraries, IMO they only make sense in Java world...
I am having an issue when trying to add unmanaged jars to my project.. Has anyone seen this error: build.sc:19: type mismatch; found : mill.util.Loose.Agg[_1] where type _1 &lt;: ammonite.ops.Path required: mill.util.Loose.Agg[mill.eval.PathRef] if (!ammonite.ops.exists(millSourcePath / "lib")) Agg() ^ build.sc:18: `T.command` definitions must have 1 parameter list def unmanagedClasspath = T{ ^ Compilation Failed This is how my unmanagedClasspath looks like (from the docs): def unmanagedClasspath = T{ if (!ammonite.ops.exists(millSourcePath / "lib")) Agg() else Agg.from(ammonite.ops.ls(millSourcePath / "lib")) }
Workaround: Agg.from(ammonite.ops.ls(millSourcePath / "lib").map(PathRef(_))) 
I kind of stopped using coursier when sbt 1.0 was released. It does fetch deps faster, though once cached they don't have to be fetched again, so I haven't felt any urge to get back to using it. I guess coursier would make or of an impact if you had a build time around 5 minutes - then change from 5 to 3 minutes would be more noticeable than reduction from 45 to 43. Mill might be a good idea, but I doubt many people would try to convert existing codebases (especially enterprises), more like an option for greenfield projects. My main takeaway was that while sbt and scalac are slow, quite often slowness might come from misconfiguration and suboptimal structure of: project, code, CI run... which adds up and results in a death by a thousand cuts. Then using just one trick might not be enough.
lol @ username to job-seeking ratio actually I don't think I'll be asking about jobs with mine either...
&gt; My main takeaway was that while sbt and scalac are slow, quite often slowness might come from misconfiguration and suboptimal structure of: project, code, CI run... which adds up and results in a death by a thousand cuts. Then using just one trick might not be enough. Well yes and no. Scalac is slow just because it does a lot, and scala's typesystem is quite complex (majority of time spent by the compiler is in typechecking). This is why tools such as zinc make such a big difference. Regarding SBT, SBT is slow for more fundamental reasons 1. Caching with SBT is done in a very ad-hoc way because unlike tools like mill and bazel, there isn't any standard interface for cache invalidation (tools like mill/bazel checksum everything on the form of files, and so cache validation is whether the checksum has changed or not). This means that in SBT, when you need to cache something you have to come up with your own method of cache invalidation (which is often wrong). 2. SBT's entire build model is stored at runtime in a giant immutable graph that uses a state transformer monad to track alterations on the graph. On many levels this is very slow, and probably explains why even doing simple things such as even updating settings can take seconds. Build tools like CBT/Mill store the build model in the constructs of the language and/or performant datastructures which makes a big difference because they are highly optimized. 3. Typical usage of SBT loads huge amount of classes and churns through heap a lot because it is coded in idiomatic Scala style. Tools like graal can help in this area, but this is particularly visible on laptops/slower machines. It also puts excessive reliant on JVM JIT to do any kind of speedup. 4. Zinc isn't completely accurate (although its getting a lot better), because even the tiniest of changes can invalidate the incremental compilation due to how brittle/complex Scala's bytecode format is. And then you have really silly things, like relying on Ivy which downloads things in parallel, or the fact that SBT needs to check internet for updates on your project even though your just loading your project and you aren't compiling anything (and you have have preloaded your updates before).
I agree with all of that, I am just saying, that even though I am experiencing this slowness, more slowness in my use case came from the code came from how project was organized, and how tasks were run. Sure, had I used Mill (which didn't exist at a time) from the start, I would save a lot of time on calculations related to the build. But it wouldn't help with time lost on unnecessary shapeless operations, slow integration tests, installing dependencies etc. Actually, I estimate that shrink from 45 minutes to just 17 has, about 10 minutes of time saved by things related to sbt and scalac, and the rest was related to what I was feeding builder and compiler with or in what context operations would be run. So even if I embraced Mill (and coursier and bloop), I wouldn't experience this big gains had I just stopped there. In this posts I wanted to show all the tricks I used, especially for people who just cannot afford switching a whole build tool, but can incrementally apply some fixes, that will save time a few minutes at a time.
how long do you compile the Binding.scala project ? It take me 10s ~ 30s after sbt warm up. It is too slow for me.
I've found [Elm](http://www.elm-lang.org/), and I love it. Elm is purely functional, and has a strong type system. It follows the react style, in that it inspired the react style. So far, it's my favorite frontend development language. Easy to get started with, easy to refactor, because the compiler will tell you when you did it wrong.
Or, *shudder,* pass parameters explicitly instead of relying on half\-assed gimmicks that is DI frameworks
The first clean compilation took around 15s, after that incremental compilation is around 4 ~ 6 seconds, make sure you are using fastOptJS and not fullOptJS for development. I'm sure this can be sped up a bit more with something like [bloop](https://github.com/scalacenter/bloop) but I'm still looking into those things.
Hi, what is a good alternative for scala cats OptionT in terms of performance (memory) ? Last few days i did read about some alternatives in scalaz that look cool but i dont wish to migrate right now. thx
MacWire, hands down.
I've recently started using bloop, and that seems to have worked wonders.
Same question. Is remote work an option?
This is may be unthinkable but why use or why do you need a DI framework. What does not work with a main method and constructor injection? I say that after using Spring back since Spring was released. The problem it solved isn't such an issue these days with Scala being more concise.
Injecting dependencies to a constructor is only one aspect of DI frameworks. The other aspects include: * Finding dependent objects. For sharing the injected objects between classes we need to store the singleton somewhere. In Airframe, a session holds such objects. * Life cycle management: Initializing and discarding injected resources \(e.g., server thread, database connections, etc.\) * Building objects. As you mentioned using constructor injection itself has no problem \(although Google Guice has several pitfalls because it requires @Inject annotation to constructors\). If a DI framework can support bindings in traits, we can use trait mix\-ins to build more complex services. FYI. I described several examples here. [http://wvlet.org/airframe/docs/use\-cases.html](http://wvlet.org/airframe/docs/use-cases.html)
&gt;Finding dependent objects. For sharing the injected objects between classes we need to store the singleton somewhere. In Airframe, a session holds such objects. This is just passing values around? If you wire up your dependency graph in main I don't get why you need a singleton around. If you do want to pass that around it's simply a case of doing the below in main: `case class AppContext(obj1: ..., obj2....)` `void main = { val appContext = ...; ...}` Or even `Reader[AppContext, ?]` Granted if using case classes you are limited by max number of args. &gt;Life cycle management: Initializing and discarding injected resources \(e.g., server thread, database connections, etc.\) So: * Singleton \- `val myObj =` * Session \- `val myObj: () =&gt; MyObj = ...` * Connection \(simplified\) \- `val withConnectionManager: Connection =&gt; A = f =&gt; { val con = ...; f(con); con.close() }` 
&gt;Granted if using case classes you are limited by max number of args. Not anymore. This hasn't been the case since Scala 2.11 arrived in 2014.
Alex, great work to you and the cats\-effect contributors.
I'm very excited about this release. In my eyes, addition of things like `Ref` and `IOApp` greatly increases merit of `cats-effect` to application developers, shifting it from being just "slf4j for effect types". This has actually motivated me to contributing to cats-effect. And together with /u/alexelcu's amazing work on `cats.effect.IO` cancelability and performance, and changes I pushed to its API (no longer needing to `IO.shift` everywhere for parallelism, mostly), it is also well-suited for the apps where I don't need the full power of Monix. Unfortunately, we'll have to wait for a bit for ecosystem to catch up, since it's a bincompat breaking release, but I know there's already work being done on it and some exciting changes are coming in as well. (SPOILER: fs2 will be way less reliant on `Effect` and `ExecutionContext` :)
I am a backend dev and like OP have used Scala in backend projects (mostly Apache Spark though). Am trying out some ideas and need to build a full stack web app and if things go well may be a mobile app. Would you suggest using Scala.js as part of stack? I am not very comfortable with JS so would have to learn it first but was hoping something like Scala.js may ease the introduction. 
I've looked all over the map for interoperability between Scala 2.11 and 2.12 with Spark. Can anyone offer insight?
This is a fantastic release :) From the point of view of fs2, we pushed important abstractions down (Ref, Deferred and Semaphore) so that people can use them without depending on a streaming library, and the changes to Concurrent mean that the fs2 api will soon get rid of ExecutionContext and Effect in most places.
What are the changes done to allow not having to call .shift?
I found the hot reload compile/bundle under the web pack dev server to take longer than the scala compile. Across several areas, the compile or bundle time varied quite a bit depending in the mix of ts or scala involved. I found the compile cycle due to twidddling to be less of the overall cost of development since I did less twiddling and made fewer mistakes so my productivity went up. But, I had to invest the time before my scala js productivity was higher...did not happen quickly until I learned new patterns. I use a css-in-js solution from fabric that is exposed in scala and with that I also had an improvement in css productivity. The style funcs are in separate files with little if any compiler magic involved so scala compilation time is smaller. I‚Äôm the author of a reason react based scala js react facade. With a robust UI 3rd party framework, once my components became css-in-js driven and the components more self contained, I seemed to twiddle less on the UI side. I also found that doing some small twiddling in the browser gave me the css that I needed to change pretty quickly at that point. If your key measure is twiddling time and you cannot put in place some other things that would offset twiddle time and your front end does mostly presentation of server provided/computed results, I would not change.
I [proposed](https://github.com/typelevel/cats-effect/pull/232) that we require Timer for Concurrent and Parallel instances and all related operations (start, race, parSequence, etc.) and do a `shift` inside. Alex [followed up](https://github.com/typelevel/cats-effect/pull/250) with optimizations so in case of truly async operations extra thread switches are avoided. You still need `shift` for fairness reasons and for thread-pool switching, but operations that imply parallelism will not suddenly behave sequentially.
Javascript the language is really simple. It has its quirks, yes, but it's a small language to learn (very much unlike Scala). I would say the browser environment (meaning the entirety of the DOM API, including elements, styles, event system, etc.) is a bigger and more complicated domain than Javascript itself. And by using Scala.js you're not going to avoid the browser environment. Similarly for the UI library (primary source of complexity in your app). Say you go the popular route and use React. Hundreds of thousands of JS developers go through this every day, there are plenty of resources online. Now if you want to do React in Scala.js ‚Äì _you still have to learn React.js_ ‚Äì AND also a library like Scalajs-react. On the other hand, if you use a native Scala.js library, you wont' need to learn React. It might be easier or not depending on your Scala / FP experience and the library that you choose. So generally speaking "easyness" is not the reason to use Scala.js. If you want to optimize for output, Scala.js makes sense for bigger projects where you want safety and powerful language features. If those are not a priority or a necessity, it's a less compelling proposition. As for mobile, it's a whole other story. A lot of people build cross platform mobile apps in Javascript using Cordova, React Native, NativeScript, etc. ‚Äì Scala.js is possible for some of those, but you will be one of very few people doing that with Scala.js.
&gt; Javascript the language is really simple. It has its quirks, yes, but it's a small language to learn (very much unlike Scala). I'm not sure how true that is since ES6
Here singletons mean session\-scoped singletons. So basically it's the same with initializing objects in the main method. But managing such objects in the main while allowing reuse \(in the other contexts\) and resource deallocation is not so simple: [http://wvlet.org/airframe/docs/internals.html#comparsion\-with\-a\-naive\-approach](http://wvlet.org/airframe/docs/internals.html#comparsion-with-a-naive-approach) Reader monad has its performance overhead since each nesting will create a new function object, and to allow referencing outer variables, created function objects \(= closures\) will be big classes that hold reference to these outer variables. To avoid such overhead, you need to be really smart in writing such hand wiring. One of the big advantages of using DI frameworks like Airframe is you can separate a concern for writing applications that use several modules from a concern for wiring these dependent modules. If you are not wiring tens or hundreds of classes, pure\-Scala approach will work. But as you write such hand\-wiring code both in main and test code, you will easily end up writing similar code everywhere. Writing reusable functions for wiring modules is also challenging. When you have 100 objects that need to be created in your application, some module often requires only a subset of them. For example, module 1 needs A, B, C, D and module 2 needs B, D, E, etc. To reuse the initialization code you need to find the intersection of dependencies \(= B, D in this case\). I think if Scala programmers need to find such common patterns by themselves, it's already complex enough. If you start to feel such a pain in wiring classes, DI framework can help it.
Still true. ES6 made the language nicer, not really bigger. Mostly it just brought syntax sugar such as fat arrow functions, "classes", `...rest` args, destructuring. For anyone curious: https://github.com/lukehoban/es6features
How big is scala if syntactic sugar doesn't count? Is pattern matching syntactic sugar? (Javascript is getting pattern matching, you know. https://github.com/tc39/proposal-pattern-matching/pull/65) AFAICT the main difference in footprint between javascript and scala at this point is the type system. (And if you go with Typescript, its type system is more complex than scala's in many ways.) 
&gt; Scalac is slow just because it does a lot, and scala's typesystem is quite complex (majority of time spent by the compiler is in typechecking). This is why tools such as zinc make such a big difference. I would be careful making such statements! The inefficiency of the compiler seems to come from its generic, non-parallel design. The research in Kentucky Mule backs up the idea that there's no reason why compiling Scala should be slow, and the big progress in optimizing scalac over the past months also confirms that. Claiming that the slowness comes from the typechecking properties of the language is a stretch...
I'm glad to see Bloop mentioned! The speedups you get from using bloop do not only come from the fact that you keep hot compilers around, but they are also architectural: we see around 20% better compilation performance in projects like guardian/frontend, apache/spark, sbt/sbt, etc than compiling these projects with sbt. We're working on a new design of the compilation pipeline that will yield much better speedups. I'll be presenting this work in Scaladays New York and gave a sneak peek in Scaladays Berlin two weeks ago.
I know, it gets worse when your on a microservices based project and people insist on a DI framework for an app with a single controller and a single service...
Only issues have I've had with http4s were the rather frequent library changes, e.g: Scalaz -&gt; Cats, Fs2 -&gt; Cats IO.
Hi! We have a new project about making a control panel to monitor the infrastructure of the client \(server health, response times, put servers up and down, etc\). We plan to do the back\-end in scala and the front\-end with scala\-play. Do you know any graphical libraries that can come at hand? Thanks!
This should be somewhere very prominent in the main cats(-effect) docs.
Thanks :\) We might do it but anyway, take into account that this is a generic concept in functional programming and doesn't only apply to Cats Effect.
Do you know maybe how they (both Scalaz and Cats) compare with other concurrency stories (implementations) in other languages? If this really is that much faster (and easier to reason about), I guess it could be a good FP selling point. Sorry if I don't make sense, just thinking out loud what would I tell someone when arguing OOP vs FP or Scala vs other languages.
&gt; The research in Kentucky Mule backs up the idea that there's no reason why compiling Scala should be slow, and the big progress in optimizing scalac over the past months also confirms that. Claiming that the slowness comes from the typechecking properties of the language is a stretch... Well Kentucky mule is also a subset of Scala that has a lot of corner cases/functionality removed, so it will be interesting how much of scalac they can implement in a truly parallel fashion. Note that I was just stating that the typechecking phase in Scala is one of the most expensive phases, afaik this is the case.
&gt; Well Kentucky mule is also a subset of Scala that has a lot of corner cases/functionality removed, so it will be interesting how much of scalac they can implement in a truly parallel fashion. Actually, no, it doesn't have so many corner cases/functionality removed. There is only one concrete case (imports on local terms with no explicit return type) that is prohibited, and that is not fundamental to the language... &gt; Note that I was just stating that the typechecking phase in Scala is one of the most expensive phases, afaik this is the case. Ah, alright. The way it read, it seemed you were claiming that Scalac is slow because of its natural complexity, which as I already described is not completely true. The typechecker does lots of stuff, for sure, but that doesn't imply it has to be slow. Also, we need to distinguish here between how slow scalac actually is and how slow people perceive it to be because of other reasons (e.g. abuse of macros and implicit search).
What is this symbol `*&gt;`?
`fa *&gt; fb` is `fa.flatMap(_ =&gt; fb)`
More specifically it is `(fa, fb).mapN((_, b) =&gt; b)`. I say more specifically because the above is still equivalent, but requires a `Monad` instance instead of an `Applicative` instance :)
`Reads` https://www.playframework.com/documentation/2.6.x/ScalaJsonCombinators
Write case classes with native types. Write companion objects with an implicit val of type Reads[Whatever] or Format[Whatever], use Json.format[Whatever] to create the serializer for common types or write your own. For types without a Json.format[T] already defined, define one. For example if your case class has a java.time.Instant, you‚Äôll need to provide an implicit conversion that, for example, converts the instant to a JSNumber(BigDecimal(...)). Json.parse(string).validate[Whatever] will find your implicit formatters (and the ones defined for base types) and use them to recursively transform the input 
It's the cost of manual parameter passing. There's no point when your project is small, when your project has at least 200 independent modules, the cost of passing parameters makes adding a new module prohibitively expensive. Add to that the need for runtime configuration via config and different configurations in tests and you'd quickly find it impossible to scale without a DI framework, but trivial to scale with one. Algebraically, you can think of a DI framework as an entity that transforms your uninstantiated modules of form `(A .. N) =&gt; M` into instantiated modules `M`, recursively, until you're left with one program `M`. That is, it's an entity that lets you `foldMap` your large collection of modules into a single program, generically. Given a generic way to compose programs, why would someone prefer to write ad-hoc instantiation code? We don't write [MinUint64](https://qph.ec.quoracdn.net/main-qimg-dd2dc3bc72b058b85774ee804a521165) and we don't write `List_Functor.map(_)`, same as we shouldn't write monomorphic instantiation code when there's a principled way to compose programs generically.
Teads.tv | Backend Software Engineer | Montpelliers, France | Onsite | Full time I'm not directly affiliated with HR in my company, I just relay the offer in case some French scala dev is on the lookout. I currently work there and I'm loving it. [Job Description](https://boards.greenhouse.io/teads/jobs/188474?gh_jid=188474) You can apply under the job description directly, or you can send me the contact you want me CV + ML that I'll relay to the relevant persons.
BAMTECH Media | Scala Engineers at all levels (Jun, Mid, SNR) | New York, Manchester (UK), London and remote (US). BAMTECH Media is a leader in building DTC streaming platforms for Live and On-Demand customers, and with Disney backing the business in the summer of 2017, we have already launched ESPN+ and EuroSport, as well as commencing work on the marque Disney SVOD, due for release in 2019. If you're interested, either visit the careers page https://www.bamtechmedia.com/careers or contact me on sam.facer@bamtechmedia.com
Which means, that you basically make the sbt&lt;-&gt;bloop relationship similar to configure&lt;-&gt;make. ;) Did you measure if using bloop on CI makes any difference? I mean surely it uses less memory that the whole sbt, but does it affect compile time?
Well, Reasonable Scala works on a subset of Scala, so maybe you mean RSc? Triplequote's Hydra uses some parallelism and they achieved some speedups. On one of posts about them I saw very optimistic forecasts (like 10x speedup possible), as well as less optimistic (1.5x-2x speedup). Still, it is worth trying IMHO. *The shoemaker's children go barefoot* - it's a pity, that the language used for building fast, high-throughput applications with parallelism and scalability et all does't practice what it preaches in its own compiler.
Very interesting. How does it compares to kafka's [MirrorMaker](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/tools/MirrorMaker.scala)?
Comparing the MirrorMaker, Kafka Snow White should be easier to manage: - The configuration can be reloaded dynamically via files or Consul - Each mirror is defined independently with no need to restart or affect other mirrors when a given mirror is changed
Thanks for the suggestion! I was originally leaning toward this stack based on things I've read over the months, but my biggest concern is the learning curve. Given I haven't written Scala in years, so I need libraries with great documentation, stability, and examples so I can start getting things done. More importantly, nobody on my team knows Scala (but they are onboard with using it for this project), and they are not experienced with FP, so I will be the initial developer and eventually train the rest of the map as well about Scala + FP. It's hard for me to know how good the docs are until I actually jump in and start writing code. I've briefly done some very basic things with http4s and circe and the docs and examples I've found for http4s seem light compared to Play. Also concerned about the migration from http4s 0.18 ("stable") to 1 (development). With that said, I'm still hoping to use this stack because I'm a big fan of FP and have been really impressed from resources I've read/watched. Are there particular projects, articles/blogs, documentation, etc. that you recommend (other than project websites like http4s.org or typelevel.org/cats)
Doesn't that mean the semantics may be different? That actually jumped out at me when reading the gist, since in a parallel context keepRight doesn't imply a given sequence whereas flatMap would. Am I misunderstanding the behavior of keepRight?
Would projects without shapeless and macros benefits from this tool?
That‚Äôs true in general but a lot of places (cats in particular) Applicative instances are consistent with an implementation in terms of Monad.
Unfortunately, I'm not aware of a comprehensive survey of approaches. For the most part, the question boils down to how (or if) you want to encapsulate data inflow and outflow into your system There's a lot of material out there in the blogosphere/conference talk sphere if you're inclined to go the free monad or tagless final routes. The more mundane OOPish methodologies tend to be captured in project templates for things like Akka and Play. My personal approach has been to wrap inherently stateful services in traits that capture their interfaces and then business logic goes into case classes that take those traits as parameters (or in some cases, the parameters are functions that match the signatures of individual methods from those traits). That way, my business logic is pure and easily testable.
Also, very important: `*&gt;` is stack unsafe because the right hand side is not lazy.
Serious question: why is this useful? How does it improve the readability and maintainability of my code?
No, because by law `ap`, `map2` and `product` (these are the same operation basically) ... have to be consistent with `flatMap`, so you need to be able to say: product(fa, fb) &lt;-&gt; fa.flatMap(a =&gt; fb.map(b =&gt; (a, b))) If this doesn't happen, then it's a broken implementation.
Thanks for clarifying this.
I have a non\-shapeless non\-macro project right now that is non\-deterministic and will alternate between a build that takes \&gt;60 seconds and one that stack overflows during type checking, with a smattering of other errors in there too seemingly dependent on whether it's a clean build and a whether it's an incremental/batch build. This tool has potential for me, except I'm stuck on 2.11 for the time being.
Generally if you want to write a performant alternative to a monad transformer, you just have to write your own monad combining both effects. For example if you want to write an performant alternative to `OptionT[List, A]` you'll either write a monad instance for `Monad[List[Option[?]]]` or you'll write a new data type that represents lists with optional elements. Given that you're mention memory specifically, I'd guess(not a very scientific guess) you'd want to go with the later technique, but I would write both and test them in benchmarks to be sure you get the results you're looking for.
I'm also interested in this. We've built a tool that allows management of an akka cluster but we don't have any graphical components, just regular HTTP lists and so on
Either to Try is not that common, so I would be surprised if there was such util already. But you could still implement it yourself in a minute e.g. class EitherTryOps[E &lt;: Throwable, A](either: Either[E, A]) extends AnyVal { def toTry: Try[A] = either match { case Left(throwable) =&gt; Failed(throwable) case Right(value) =&gt; Success(value) } }
With cats `e.liftTo[Try]` will get you a `Try[Option[A]]`. If you want to treat `None` as a failure you might consider using `.unique` instead of `.option` on your doobie query.
It's a symbol made up by someone trying to be clever to make their code harder to read.
Great stuff!
In Scala 2.12 Either have the method def toTry(implicit ev: &lt;:&lt;[A, Throwable]): Try[B] it only needs to know how to go from A to Throwable
Scattersphere 0.2.0 is under way with Spark support. Currently has rudimentary Spark support, with the ability to submit jobs and collect results. As I write this, I am currently in the process of converting a "real world test" to a Spark implementation which is about 80% complete. Could use some extra eyes on the project, would love additional feedback, and more ideas on what to do with it. This project is great fun, and a way for me to experiment with writing a public API. I hope someone out there finds it useful. Check out the project [here](https://www.github.com/KenSuenobu/scattersphere) Thanks!
Scala has a lot more concepts than Javascript, OTOH: - Parallelism - Advanced inheritance features (traits / abstract classes / linearization / etc.) - Implicits (params, conversions, implicit/typeclasses) - Collections (much more complicated and advanced than what's available in JS) - Immutable data structures - Pattern matching / apply / unapply / case classes - Type system / HKTs / type lambdas / etc Javascript on the other hand is pretty much: - Functions - Simple hashmap-like objects - Prototypical inheritance that is usually wrapped into a simplistic class-like syntax - setTimeout which is usually wrapped in a simple Promise syntax Except for weird inheritance / "this", everything in Javascript is also available in Scala, and in much more advanced forms. For someone who knows Scala learning JS should be trivial ‚Äì it will be more about learning coding patterns and the browser environment than the language itself. That said, JS is indeed gradually adding new features, and a lot of devs choose to use new ES7 / ES8 features, but that's a choice that you don't have to make. Browsers still understand even plain old Javascript 5.1, and will for a long time, it's not going anywhere anytime soon.
First of all, it's important to understand what are the benefits of Functional Programming. One of the most important concepts IMO is `Referential Transparency`. Being able to substitute an expression for its bound value is a simple and powerful thing at the same time. Consider this: ```scala val expr = 123 (expr, expr) ``` Now I should be able to replace `expr` by its bound value without changing the meaning of the program: ```scala (123, 123) ``` The meaning of the program remains the same. How about this? ```scala val expr = println("hey") (expr, expr) ``` Performing substitution: ```scala (println("hey"), println("hey")) ``` The meaning of the program has changed. The first program prints "hey" once whereas the second program prints "hey" twice. ### Shared State WRT Shared State the concept remains the same. You want to keep the referential transparency capabilities while sharing and mutating state. Consider the following case (similar to the examples given by @SystemFW in the Gitter channel): ```scala var a = 0 def set(n: Int) = a = n def get: Int = a ``` This is an imperative and impure way to mutate state, so we can try wrapping things in `IO`: ```scala class IORef { var a: Int = 0 def set(n: Int) = IO(a = n) def get: Int = IO(a) } ``` This is better but we are now pushing the mutability to whoever creates an `IORef`: ```scala val ref = new IORef() ``` Two or more references to `ref` in your code will be referring to the same mutable state. So we want to make sure this doesn't happen and a way to achieve this is to wrap the creation of `IORef` in `IO`: ```scala private class IORef { var a: Int = 0 def set(n: Int) = IO(a = n) def get: Int = IO(a) } object IORef { def apply: IORef = IO(new IORef) } ``` Now when you create an `IORef` you'll get an `IO[IORef]` instead. So when you invoke `flatMap` on it twice you know that you will have two different mutable states, and this is the power of `Referential Transparency`. Concluding, `flatMap` once and pass the parameter as argument wherever the state needs to be shared. It gives you way more control than having a `val ref` hanging around in your code and gives you ***local reasoning***. 
If we bring in libraries, then it's hard to make a fair comparison at all because javascript doesn't have a standard library, so any scope other than either "just the language itself" or "the entire npm ecosystem" is arbitrary. So I think that forces us to either compare just the javascript language to just the scala language, no libraries included, or else factor both languages' ecosystems in. Since the latter is nearly impossible, let's just focus on the language. That takes parallelism, collections, and immutable data structures off the list. Also, I guess I wasn't clear before, but I'm not arguing that scala is not a bigger language than javascript. I agree with all your examples. I'm just saying that it's not bigger by a large enough amount that it's a "black and white" kind of difference, like it used to be. Meaning, I would find it hard to believe nowadays that someone would choose javascript over scala due to its *actual* (vs. believed) simplicity. 
Also, it's not literally true that everything in javascript other than dynamic `this` is available in scala
&gt; That takes parallelism, collections, and immutable data structures off the list. I don't know what technical definition of "library" people are using when they're calling these features libraries (not the first time I hear it), but it seems a bit disingenous to not include those. Those are not platform-specific features, or optional third party libraries, in practice they're as much part of Scala as anything else. I mean if you want to call out ES6/7/8 and stage0 proposed features as examples of JS growth / complexity, not including the basic building blocks of literally every Scala app in existence is not fair. &gt; I would find it hard to believe nowadays that someone would choose javascript over scala due to its actual (vs. believed) simplicity. In a narrow, specific case that I was making ‚Äì native JS vs Scala.js ‚Äì plenty of people will certainly make that choice, and they will not be wrong. And it wasn't the only argument I was making in favor of plain JS, that included the ecosystem too. Which isn't in Scala.js favour either.
Hello, thank you so much for posting the job listing here. Do you know if I can send my CV and cover letter in English? Or do HR prefer French? 
Thanks.
At the end of the day all the benefits from referential transparency boil down to being able to understand and build code compositionally. That is, understand code by understanding individual parts and putting them back together, and build code by building individual parts and combining them together. This is only possible if local reasoning is guaranteed, because otherwise there will be weird interaction when you put things back together, and referential transparency is _defined_ as something that guarantees local reasoning. In the specific case of state sharing, this gives rise to a really nice property: since the only way to share is passing things as an argument, _the regions of sharing are exactly the same of your call graph_, so you transform an important aspect of the behaviour ("who shares this state?") into a straightforward syntactical property ("what methods take this argument"?). This makes shared state in pure FP a lot easier to reason about than its side-effectful counterpart imho. Anyway, I'm preparing a talk on this, in the meantime you might want to have a look at these explanations, they all start from slightly different angles and at different levels, so there's probably something you can get from them. https://gitter.im/functional-streams-for-scala/fs2?at=59ee827df7299e8f53240557 https://gitter.im/functional-streams-for-scala/fs2?at=59f16cf9e44c43700a7bde70 https://gitter.im/functional-streams-for-scala/fs2?at=59fc66e7614889d47535da02 https://gitter.im/http4s/http4s?at=5ad22e27270d7d3708c90e39 P.S. Thanks /u/volpegabriel for writing that down in a useful format :)
@Judheg It may be useful if you use lots of implicit searches that are not well-organized in your codebase. To find out where your bottleneck is add `-Ystatistics` to your build, warm up compiler and see what you get. Based on the info you see, you'll see if you can use scalac-profiling to profile macros and expensive implicit searches. Another thing you may want to look into is in preserving as many hot compilers as you can. For that we created bloop https://github.com/scalacenter/bloop
Sure, CI should see the same speedups we get in our benchmarks. One thing we want to work on is in making bloop a server that handles remote compilation. The idea is that no matter what build tool you use, you can use workers with bloop servers running to compile all your CI submissions with hot compilers, in a concurrent fashion.
You can send them in English, but since the work is onsite in France, French fluency is expected. 
Hi somethingconcon I'd recommend checking out [Functional Works](https://functional.works-hub.com/#utm_source=reddit&amp;utm_medium=de&amp;utm_campaign=lukas), they are recommended in the scala jobs section in the right hand column too. 
_getUnprocessEvents()_ marks the selected records as 'in progress', so subsequent calls do not select them. The problem has disappeared now, I'll update the post.
DI frameworks were created to compensate for the deficiencies of Java, the language, and constitute a run-time, "stringly-typed" interpreter on top of the compiled language. It's not uncommon for a Spring project to just be dangling pieces of "bean" and imperative code, wired together and "enriched" via annotations. Shudder.
Thanks! Through extensive googling I happened upon functional works. It seems to be a really great service. I just don't know much about it :/ I applied to a few places listed on their site and expected the usual from a job site. Surprisingly I was contacted (by a human) and we've started looking for prospects. ++ can recommend.
I know all that. I‚Äôve just never found this to be an issue. Admittedly I‚Äôve only been programming for less than a decade, but not once have I ever needed to inline a print statement and gotten bitten by it 
Etsy, Inc. | Senior Engineer for Search / ML Infrastructure engineering | San Francisco, CA, USA | Onsite | Full\-time I am the hiring manager at Etsy for Search/ML Infrastructure Engineering teams. Looking for Scala experts with experience building large scale distributed systems. You can apply [here](https://www.etsy.com/careers/job/34772b87-55db-4c77-a6ed-7899ec2f2701) or send me your resume or Linkedin profile for review.
It‚Äôs summer - why would the features freeze? ü§î
Temperature overflow.
i really appreciate those tutorials you do, keep it up! one question: what are the best resources to learn proper FP? im trying to write FP style scala, but as im from OO world and surrounded mostly by java devs it's very hard to move forward and abandon java practices. what i do id call writing java in scala with FP elements. how to break free?
These days I'm thinking that asynchronous way of coding \(e.g. Future, map, flatMap\) is counter\-intuitive and easily getting complex. I understand its benefit \(e.g. performance\), and I hope there's a better of writing asynchronous code. The code should look like: a = doA(); doB(a); As opposed to doA().flatMap { a =&gt; doB(a) } Anyone is thinking the same thing?
At a new company \(mostly Python\), with a brand new team \(varied experience: C#, Clojure, PHP\), we've just successfully built a proof of concept and are now beginning to move it to being a production system to iterate on. We built it using Scala and Akka, and it feels like a really good fit as there's a lot of parallelism going on with some longer running tasks, and a bunch of external integrations that're nicely handled in an event driven manner. For context, the company is a non\-profit, and we're building a system to manage research data with the ultimate aim of making it open and free \([a recent relevant news article](https://www.theguardian.com/science/blog/2018/may/29/why-thousands-of-ai-researchers-are-boycotting-the-new-nature-journal)\). Does anyone have any good links or info specifically about making a scala codebase production ready? E.g. static code analysis, style guidelines, logging, monitoring, performance testing, etc. Vague request as I'm basically trying to find out what I don't know.
Great tutorials. Thank you.
No idea why people bring Java DI approach into Scala if it has powerful mixin composition. Just using cake pattern. 
Climate change 
&gt; It‚Äôs summer Stupid Americans don't know about the Southern Hemisphere
Not just Americans, but about 90% of the world population lives in the northern hemisphere üòä
Interoperability in what way? In general if there's a serialization boundary in between, one JVM could be running 2.12 and another 2.11 just fine. If you are talking about having a project that depends on some 2.12 artifacts and some 2.11 artifacts, that \_can\_ also work, but since binary compatibility is not guaranteed across major Scala versions you are much more likely to run into an issue.
I wanted to get into FP and a had an opportunity to write Scala at a cool company.
Functional Programming in Scala and Scala With Cats are both excellent places to start. FPIS focuses more on fundamentals of FP (recursion, immutable data structures) working up to category-theoreitcal abstractions, while SWC mainly works through the Cats typeclasses.
There's some syntactic sugar that is more convenient in Scala 2.12 vs. 2.11, and the fact that Spark is built on an older version of Scala makes it harder to adopt new libraries. Specifically, those built for Scala 2.12.
Sounds like you're after the mixed\-library case, so I think you're at the mercy of binary compatibility. If you happen to always use things that are binary compatible then you can get away with it, but it's somewhat risky as those issues won't show up until happen to execute the incompatible codepath.
Appreciate the words :)
Appreciate it :) I've also made my way into FP from the Java world and it wasn't easy I must say but it was fun (be aware there's no way back XD). I started writing code with Play! Framework and Akka, done the Coursera Scala course, read the Red Scala book and didn't get most of the stuff but I continued to learn by writing code and reading, reading a lot. But it wasn't until I had to work in a team doing pure FP in Scala surrounded by people with FP knowledge and where I was exposed to a lot of new concepts using libraries such as Scalaz, Scalaz Stream (now Fs2), Shapeless and Monocle to name a few, when the click happened. Then I read two Haskell books and I got better understanding of FP concepts and my skills in Scala leveled up as well. Then I got back to the Red book and finally got it. Writing your thoughts in a blog (beware you're going to be exposed to a lot of criticism but it's okay to make mistakes as long as you learn in the process) and especially contributing to open source projects help a lot. Nowadays I normally hang out in the Gitter channels (Cats, Cats Effect, Fs2, Http4s, etc) and I recommend you to do the same regardless of your FP level. I learned quite a lot just by paying attention to what was going on around. Keep it up! :)
Well... I have nothing left to say.
This is a very practical and well thought out article. This should be sbt 101 for anyone starting off on a multi-module project. I wish I'd seen all this when I was starting off.
I do rather like the Camel API m, though I agree about the issues of type safety. I will say I would rather see Camel get improved and support the more advanced features over adopting a new framework. Perhaps Camel could learn quite a bit from Alpakka, but I think the userbase and community proves it has more staying power.
Using DI in Scala doesn't prohibit you from using cake pattern and trait mixins. Actually I'm using Airframe with trait mix-ins as a much simpler form of cake pattern. Basically it's a choice between using type abstruction in cake pattern or using DI inside traits. 
yeah, this is why \`for\` comprehensions work the way they do. This would be written as: for { a &lt;- doA() } yield doB(a)
You could write `doA flatMap doB` for this simple example. You could also use a for comprehension. for { a &lt;- doA b &lt;- doB(a) } yield { b } The core of that is similar to what you want but there is the necessary syntax for the comprehension itself.
Aside from for comprehensions, which have been posted below, I often times need to do multiple asynchronous operations in a row. In this case, the flatMap is so much easier to comprehend for me because it draws a clear distinction between an async operation versus a synchronous one. In your example, the code would be the same between async and synchronous code, which would be a bit difficult to grok.
Interesting project! Is there an online DEMO for the JMH viewer?
Interesting project! Is there an online DEMO for the JMH viewer? 
I used it for showing results in this [benchmark](https://github.com/tom91136/scala-parser-benchmarks) repo, there is a link to the report page which uses the JMH view. The direct link to that is [here](https://tom91136.github.io/scala-parser-benchmarks/report.html), be sure to click on the left side bar and play around with the options. This is my first project using your library so let me know if I did something wrong. Thanks again for the brilliant library!
Your viewer looks better than http://jmh.morethan.io/ . I really like the feature of switching grouping. I feel difficult to distinguish which bar corresponds to which parameter set in [some of my benchmarks](https://github.com/ThoughtWorksInc/Compute.scala/#benchmark) when using the viewer of jmh.morethan.io .
Yeah, that was my concern. I'll stay on Scala 2.11.11 for the time being. It seems there's already work under way to get Spark 2.3.x converted to Scala 2.12, so once that happens, all should be okay.
EveryLIFE Technologies | Scala Developer | Farnborough, UK | Onsite | Full Time I'm the Head of Engineering at a small, lean and agile healthcare startup. Our vision is to continuously improve the quality of care given to people by making care more visible, consistent and reducing the possibility of mistakes in administering care by maintaining a single view of truth. In my experience it's rare to find a small, disrupting company outside of London, who operate with agility at its core, and are brave to use new technologies to best deliver value. As a Scala developer, you'd be working as part of a team developing the APIs that drive our mobile applications as develop the web front-end. You can find more details on our careers page here https://www.everylifetechnologies.com/about-us/careers/full-stack-developer/ If you're interested either apply directly to careers@everylifetechnologies.com or connect with me on LinkedIn and we can talk more https://www.linkedin.com/in/adamlondero/.
I have just bad experience with cake pattern the same as author here https://kubuszok.com/2018/cake-antipattern. TLDR; not specified initiation order, lot of boilerplate, circular dependencies.. I find constructors as better solution. Or then FP solutions `ReaderT` or free monads.
https://docs.scala-lang.org/overviews/collections/performance-characteristics.html do you know this site? This may help you
It would be easy to build this on top of Hinze and Paterson's finger trees, of which there are several Scala implementations. The resulting complexity for indexing and insertion should be logarithmic. Finger trees support many operations that you may not need, in which case you may be able to get faster results with a custom data structure (faster by a constant factor, not with a better big\-O). However, unless this is really your bottleneck or you're just doing it for fun, I doubt a custom data structure would be worth the effort.
Interesting! I'll check that out. The inserts would be particularly common, so it could be worth the effort. My problem is that I don't know where to begin with Scala. If it was C++, I could manipulate pointers to get the desired effect, but Scala is so abstract that it's not clear to me how to creat a realistic data structure from first principles. Can you point me in the right direction?
Much appreciated! Maybe I can re-use a standard collection.
Have you heard of elasticsearch? I'm may be just what you need. It supports horizonal scalability and sharding. I'm not sure if it supports fuzzy searching though
Autocomplete is a particular kind of prefix matching that's different than what's done when searching for term matches. Elastic has both. https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html
How many items are we talking about?
The system would still be useful even if unpopular enough to fit on one box. But if popular it would be in the hundreds of millions range or more. 
Thanks, I will look more into this. That and Spark seem like better options than anything else I've thought of. 
I'm not sure if Spark is good enough for this because you need to get the results fast and Spark is not intended for that.
Why not return everything as `Future[T]`, using `Future.successful(value)` for non-asynchronous cases?
remote?
If you want to write mutable data structures you can write them as you would normally. Scala does have pointers, they are just called references. If you are trying to learn to write immutable data structures you could check out https://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504
Skedulo| Senior Backend Engineer | Brisbane, Australia | ONSITE | Full Time Skedulo is the leading mobile workforce scheduling platform. We help companies around the world transform the way they manage and schedule their mobile workforce teams. We are looking for a Senior Backend Engineer with the following skills - 3+ years experience in backend development - Ideally experienced with one or more of the following: - Scala - Functional programming languages. - Java 8 and the latest features and functionality - Other open source object-oriented languages, with aptitude and interest in Scala - Solid computer science fundamentals in data structures, design patterns, and ideally complex scheduling and optimization algorithms. - Experience with cloud hosting (AWS, Azure or Google Cloud) - Understands the importance of contributing to the CI/CD pipeline and automation - Full stack experience in either mobile (ideally C#/Xamarin) or web (ideally React/Typescript) would also be beneficial Full Position Description: https://hire.withgoogle.com/public/jobs/skedulocom/view/P_AAAAAACAAADNo85h2ncrA_ Contact: russelladcock@skedulo.com
Where did you define \`SyncEventMapperFunc\`? I think your definition there is off. What you're doing is generally correct, it's called "tagless final" approach, and the \`C\[\_\]\` bit is called an object algebra. [https://typelevel.org/blog/2017/12/27/optimizing\-final\-tagless.html](https://typelevel.org/blog/2017/12/27/optimizing-final-tagless.html)
No. Elasticsearch is what you want.
:+1: on `elasticsearch`
Ah, good to know. Thanks!
Did you have a repo for us to look at your project?
I second Elasticsearch. I love the product. It‚Äôs based on Apache Lucene which itself gives you the search/autocomplete without having the additional overhead of elastic on top. Both are easy to configure from an API point of view. 
Oops, it's _ObjectMapperFunc_, a mistake in posting code. 
That would work of course. The point is to abstract the execution context, the caller does not care whether the function is async or not. 
You don't "run" your Scala JS app. It builds a Javascript file which you then include in your Play index.html file and use accordingly. Lots of working examples here: [https://github.com/search?q=playframework\+scalajs](https://github.com/search?q=playframework+scalajs)
I made this https://github.com/scala-jsonapi/scala-jsonapi in my previous job for working with [jsonapi](http://jsonapi.org/). It does work (is used in production) but it is very opinionated and you need to carefully follow the instructions (what little there is) to get it to work. Also the macros generate some dead code which annoys me greatly but never had the time to take a look what causes it (also my first time using macros and it kinda shows...) In my experience most of the "normal" ways to write serializers/deserializers for jsonapi kinda don't work as you need to flatten out the structure (all the things behind the nested relationships are in the included array flattened out). But your thing seems to be in a single level without any relationships so it should still be possible without using a specific library.
`OutputAMapperFunc` probably doesn't need to know what kind of effect it wants to return, you should change it's signature to: class OutputAMapperFunc[F[_] : Applicative] extends ObjectMapperFunc[F] { override def mapFunc(input: InputObject): Id[OutputObject] = Applicative[F].point(???) } Next, if the caller of `mapInputObject` doesn't know, or care what the effect is, it should parameterize over the effect as well. def mapInputObject[F[_]](contentType: String, input: InputObject): ObjectMapperFunc[F] = { contentType match { case 'A': OutputAMapperFunc case 'C': new OutputCMapperFunc } } Your only problem now is `OutputCMapperFunc` returns a `Future` where `F` is expected. You may concede that OutputCMapperFunc can't change and have `mapInputObject` return Future as well. If you do this then then at least `OutputAMapperFunc` will still be generalized, requiring only it's effect have an Applicative(or something that just `point`/`pure` if your functional programming library has that) instance. You may be able to generalize `OutputCMapperFunc` using a typeclass like you did with `OutputCMapperFunc`. Instead of applicative you would constrain `F` to be some effect that is `Async` or `Sync`, or just any `Monad/MonadError` depending what you're using from Future. The third option is to add another parameter(maybe implicitly) that will be a function `f: ObjectMapperFunc[Future] =&gt; ObjectMapper[F]`, and applying `OutputCMapperFunc` to that function. This is more or less just generalizing over the effect like you would with a typeclass in the previous option in a more naive way, but may work in this particular narrow case if this is your only use for this, and do not want to depend on a specific effect library.
Not at this time sorry
What this error is saying is that `ObjectMapperFunc` needs a type constructor but you've only given it a `_` as an argument (e.g. the type supplied is `ObjectMapperFunc[String]`, but what is needed is something like `ObjectMapperFunc[Future]`). But we can't just use `ObjectMapperFunc[_[_]]`, since we can't use wildcards in the type signature. It looks like you're trying to use an [existential type](https://dzone.com/articles/existential-types-in-scala), and it can be used like this: ```scala trait ObjectMapperFunc { type Container[_] def mapFunc(input: InputObject): Container[OutputObject] } object OutputAMapperFunc extends ObjectMapperFunc { override type Container[A] = Id[A] override def mapFunc(input: InputObject): Id[OutputObject] = ??? } class OutputCMapperFunc extends ObjectMapperFunc { override type Container[A] = Future[A] override def mapFunc(input: InputObject): Future[OutputObject] = ??? } class ObjectMapper { def mapInputObject(contentType: String): ObjectMapperFunc = ??? } ``` While this compiles, extracting an actual value becomes very difficult, since we don't know anything about the `Container` used in `ObjectMapperFunc`. You'll probably need to cast or use dependent types (e.g. shapeless) to get the actual value out. 
You can find my code on below URL https://github.com/pksuthar/AndroidApp
Yet another vote for ElasticSearch. It scales well, has trivially handled replication and sharding. You can also tinker to go beyond hash-based auto-magic by doing things like controling sharding and routing requests for different servers by a custom key. See routing in Elastic stack docs. Also auto-complete examples. I used to have typeahead in JS using bloodhound (or some such silly name) to not flood server yet provide near instant feedback. It amortizes amount of requests per second and keeps local cache.
I've done the exact same thing before on ElasticSearch and it works wonderfully!
From the source code you give it looks like you haven't really built the abstractions you need to make this easy to do. For example, you should have a `Scope` trait/class to represent the current evaluation context. And `Function` should also be a trait/class. I just implemented the exact same thing for a JSON query/transformation language I've developed. For comparison, the call method there looks like this: public JsonNode call(JsonNode input, JsonNode[] arguments) { // build scope inside function body Map&lt;String, JsonNode&gt; params = new HashMap(arguments.length); for (int ix = 0; ix &lt; arguments.length; ix++) params.put(parameters[ix], arguments[ix]); Scope scope = Scope.makeScope(params); // evaluate body return body.apply(scope, input); } (Yeah, I know it's Java. This is going to be a very widely reused library so we want minimal dependencies. Only runtime dependency is Jackson.)
Thank you for your response, but I want to keep it as slim as possible and thus I don't want to add new classes...
The problem with that is it's going to make the code much harder to write. To make an interpreter you really need a very clear mental model of the interpreter's structure, and the new classes are your formalization of that. Having those classes makes a huge difference.
What is it about the standard collections library that isn't sufficient for your use case? What exactly is your use case?
An exercise is asking for the following return values: &gt;Right(NumberType.Deficient) and &gt;Left("Hello World") How on earth do I construct them?
If you add a new entry with the same key as one that already exists in a Map, it will replace the old value (see [additions and updates](https://docs.scala-lang.org/overviews/collections/maps.html)). This should allow you to shadow variables.
[removed]
Thanks! But how do I interpret Blocks? I don't have a clue at the moment...
I need to upgrade a project from play 2.3 to Play 2.6. The switch gets rid of the old global play application so it‚Äôs a pretty big change. Because the mongo drivers are unsupported as well we decided to move some of the data to an external system at the same time. So I wrote a library using Play standalone ws client. Long story short, play 2.3 and he standalone ws client don‚Äôt play nicely together. I was able to shade the standalone ws client library to make everything appear to work, but I then ran into issues with Akka versions later on. Long story short. Keep your apps up to date!
I think `interpret` should return an updated map, too. Then, you can recursively call interpret with an updated Map. You won't do it with `for`, but with fold, starting with `varEnv` and updating the map at each step. A bit like this: def interpret(env0:Map, expr:Node): (Map, ExpValue) = expr match { case Block(exprs: List\[Node\]) =\&gt; { newEnv = exprs.fold(env0, (expr, env) =\&gt; { (newEnv, _) = interpret(env, expr); newEnv}) } } } From your post, the semantics of your interpreted language is not clear (is it rather statement-like or a series of expressions? Are variables names that are bound to values or are they aliases for memory locations? etc.) Depending on the semantics, you would like to have a different interpreter implementation.
Mentioned about this in the article Thanks!
Mascoma Bank | Senior Software Developer | Windsor, Vermont, USA | Onsite | Full time We're looking to expand our small internal development team at Mascoma Bank. We build software to support business functions within the bank, primarily in Python and Scala (although we're currently ramping up with Scala \- we'd love to add more Scala experience to the team). You can apply at [mascomabank.com/careers](https://www.mascomabank.com/careers). *Essential Functions:* 1. Collaborate with the rest of the engineering team to design and produce software that meets Mascoma Bank‚Äôs needs with regard to functionality, safety, and performance. 2. Participate in team planning sessions concerning creation or modification of software. 3. Write code (independently or with team members) that is clean, robust, and maintainable. 4. Write automated tests to ensure continued proper functioning of software. 5. Log work, decisions, and other relevant information in team tracking software. 6. Participate in code review sessions. Give and receive feedback and suggestions in a respectful and team\-oriented manner. *Requirements:* * Experience consistent with 5\+ years of professional software development * Strong understanding of software development concepts and practices * Strong understanding of how design decisions may impact performance, security, scalability, and maintainability of an application * Strong understanding of networking concepts * Expert familiarity with Python and/or Scala * Experience with front\-end client application development * Experience with REST API development * Experience writing automated tests * Ability to take general functionality requirements and use experience, best practices, and clarifying questions to implement them effectively in code * Passion for learning new things and applying them to create a better product As a Certified B Corporation, our vision is to be a force for positive change for our customers, communities, and employees. Mascoma Bank is an Affirmative Action and Equal Opportunity Employer, M/F/D/V Feel free to message me (I'm the hiring manager), or email [careers@mascomabank.com](mailto:careers@mascomabank.com)
I don't have a ton of links for you, but I can offer a little bit of guidance from personal experience working with Scala. * Logging \- I like the [scala\-logging](https://github.com/lightbend/scala-logging) library a lot, integrates neatly with SLF4J and the logging backend of your choice. * Monitoring \- This really depends on which monitoring backend you choose and how you want to integrate with it. I'm partial to statsd and like [Datadog's Java client](https://github.com/DataDog/java-dogstatsd-client) (I do not use Datadog, though the client has some features unique to their product). * Style guidelines \- This is really up to personal preference but I use [scalafmt](https://scalameta.org/scalafmt/) to enforce rules. * Performance testing \- This depends on what your system does and what kind of performance you want to optimize for, so it's hard to offer generalized advice for that. Hope that helps!
There's probably a more general way to frame this, but here goes: Say I have an actor system with 4 instances of an actor, all running within the same service, running in debug under IntelliJ. Is there any way I can separate the logs from these 4 instances into 4 different terminals, instead of having them all in the same page mixed together?
Going to play around with [scala-parser-combinators](https://github.com/scala/scala-parser-combinators) due to a possible use case for them at work. Hoping it could lead to something interesting! 
https://github.com/pksuthar/AndroidApp
Hey Gabriel! Which Haskell books did you read? 
[Learn you a Haskell for Great Good](http://learnyouahaskell.com/) and [Haskell Programming from First Principles](http://haskellbook.com/). The latter is quite new, very complete and easy to follow but hard to finish if you do every exercise. The first one is great to get started.
What if the structure was not single level? But rather: C (base level) B (composed of a sequence of Cs) and A (composed of a sequence of Bs (which are itself, composed of a sequence of Cs)) What isthe best way to de\-serialize from jsonai in such a case?
Nice, I'm reading the second one and it's really good! There's two-three follow up books to it as well but they have yet to come out
assuming your logs are being written to a file, just do `tail -f logfile | grep actorid`, if they aren't, just redirect the stdout to a file.
There's kind of a lot going on here. - Is this a school assignment? - If not, what is your intent? Why are you doing this? - There's not enough detail to implement this. I would suggest you look at an introductory book on programming language theory like [PLAI](www.plai.org) to understand how to structure an interpreter. There's a lot of subtlety to it.
It is a code challenge I am running together with my brother... We want to implement it with just enough details as needed
I suggest you look at Li Haoyi's [FastParse](https://github.com/lihaoyi/fastparse) or [Atto](http://tpolecat.github.io/atto/) (which I work on) ‚Ä¶ the Scala parser combinators are hard to use and have some problems.
 ExecutionContext.fromExecutorService(java.util.concurrent.Executors.newFixedThreadPool(10))
Oh, neat, I needed this for a project a few months back and didn't find many good options. I'll check it out.
Biggest one is to use `logstash-logback-encoder` with `LogstashMarker` for structured logging in JSON. Monitoring you can do with Lightbend Telemetry (Cinnamon) or `scala-metrics` -- I like Cinnamon because all of the setup can be done through Typesafe Config and 90% of it is done out of the box. I'd also recommend getting into distributed tracing with the Opentracing API, and taking a look at Honeycomb.
You should use a custom execution context: https://www.playframework.com/documentation/2.6.x/ScheduledTasks#Using-a-CustomExecutionContext and set up the dispatcher as a thread-pool dispatcher with fixed-size=10. https://www.playframework.com/documentation/2.6.x/ThreadPools#Using-other-thread-pools
Please don't do this. You'll leak threads if you don't shut down the pool in a shutdown hook, and it's much better to set up an akka dispatcher.
It should be noted that having 10 threads does NOT limit something like the http client to 10 concurrent requests (unless the requests are explicitly blocking). If you aren't performing any kind of asynchronous calls you can safely ignore this advice
When reading about tagless final in the end usually this approach commit to one effect, while here OP wants to mix Id with Future. Is it still tagless final?
I have an http client that hits an external api and does some business logic, that part is pretty synchronous. I was going to wrap that function in a Future then run it over an array of Strings for example so that'd be a `val tasks = Seq[Future[Unit]]` then I'd put that into a `Future.sequence(tasks)` and hopefully let the newly defined threadpool handle the details after that
Right. What I'm getting at is that if you are using the default async client‚Äîws I believe it's called‚Äîyou're likely to saturate the API even with a single thread. You can"avoid" this by blocking on the threads, but then you're driving in a nail with a screwdriver. What you want is to tell the client that it should at most have ten concurrent requests in transit at once. Using this approach instead of blocking lets you use a shared thread pool, which means less context switching, less memory overhead and likely more performance. You are also not painting yourself into a corner by trying to control concurrency by limiting resources. I'm not very familiar with Play but I'd be surprised if there wasn't a more idiomatic way to solve this. It is supposed to be an asynchronous framework after all
Yes \-\- \`Id\[\_\]\` is essentially the "do nothing" implementation, while \`Future\[\_\]\` and \`Try\[\_\]\` give an actual effect.
&gt; I'm not very familiar with Play but I'd be surprised if there wasn't a more idiomatic way to solve this. It is supposed to be an asynchronous framework after all If you set `play.ws.ahc.maxConnectionsTotal`, that should do it. [https://www.playframework.com/documentation/2.6.x/ScalaWS#Configuring\-AsyncHttpClientConfig](https://www.playframework.com/documentation/2.6.x/ScalaWS#Configuring-AsyncHttpClientConfig) The AsyncHttpClientConfig controls everything, so you can [look at that](https://github.com/AsyncHttpClient/async-http-client/wiki/Connection-pooling) for more details.
How is limiting this in your app going to work in practice, surely it‚Äôs going to be deployed to more than one node?
I'm really confused by Java's new [non-overlapping free support periods](http://www.oracle.com/technetwork/java/javase/eol-135779.html). How is anyone going to stay sane without paying thousands of dollars to Oracle (or someone else)? Updating the JVM version (with potentially breaking changes) every half year does not seem feasible for a project that has even a moderate number of dependencies. I _must_ be misunderstanding something... Is https://adoptopenjdk.net/ supposed to address this somehow?
Android adopted it after the lawsuit. https://developer.android.com/kotlin/
If it's a big deal, that's why there's an LTS version 
To clarify, unless you pay Oracle, you will not be getting support for that LTS version from them beyond the 6 month period. This is a significantly worse deal than the current situation.
Thanks for the heads up! Will check out both of them.
The situation is confusing, I plan on writing a guide on this too. In brief there are two release tracks, long term support (LTS) and feature releases. Feature releases roll out every 6 months and are only supported until the next one comes. If you want support beyond that then you need to pay for it. If you are on the LTS version then there is a release every three years that is supported. 
Different tools for different people. Kotlin is after Android and your classic Java projects: Spring, Hibernate, etc. Those who actually enjoy that stack are looking at Kotlin and enjoying it as a better Java. Whereas Scala is a different ecosystem with a stronger FP focus that no-longer is seen as just a better Java -- similar to Clojure.
kotlin is so envious of scala. most of us here don't give a shit about kotlin, yet kotlin is always measuring itself by scala. kotlin: scala for those who love to hate scala.
The survey used in the title post is from JetBrains, the creator of Kotlin. 
While openjdk itself is unclear, adoptopenjdk provides 3 years for openjdk LTS releases, azul provides 8 years.
Don't be so sure, I was working on a profitable Scala app that ran a single node for years ;)
Similarly to how JetBrains enabled Kotlin in IntelliJ via a plugin, they also have a Scala plugin. Do most Scala developers use the Scala IntelliJ plugin or something else? Perhaps this survey is representative of the usage patterns of developers that use JetBrains products (6000 developers). Note that this same survey showed 12 other languages that are more popular than Kotlin.
Lts version support will not be available for free though
I mean, it's the official Android language.
&gt; Similarly to how JetBrains enabled Kotlin in IntelliJ via a plugin, they also have a Scala plugin. Do most Scala developers use the Scala IntelliJ plugin or something else? Only difference is that I have to manually select to install the scala plugin (though there is a page for it in the setup). The Kotlin bits are there by default and I have to into the settings the rid of them.
Oh. Sure. Add a system shutdown hook and call shutdown on the pool you pass to fromExecutorService then -- but I suppose you have to do it when the application reloads in play... Playframework' s hot reloading is what causes the leaks. If you don't hook into that lifecycle you could be creating a new threadpool every time it reloads, as Will says, ExecutionContext or newFixedThreadPool are fine to use if you do that. Akka uses them internally in the dispatchers itself that he is suggesting you use. The CustomExecutionContext forces you to use akka and the ActorSystem, which will shut it down for you. You can attach to the lifecycle of the application and add a shutdownHook there, and be safe, I believe: val pool = java.util.concurrent.Executors.newFixedThreadPool(10) val ec = ExecutionContext.fromExecutorService (pool) lifecycle.addStopHook { () =&gt; pool.shutdown } I assume stop gets called in reload, or had that changed to something else now too?
I don't know if https://blog.scalac.io/2015/05/28/scala-modes.html remains to be relevant, but it did seem to be a very good solution to this type of problem.
scalac on JDK 10 crashes when compiling our project with scalac optimizations enabled. Haven't had much chance to dig into it yet, so for now it's JDK 8/9 with optimizations enabled, or optimizations off for JDK 10.
Continuing on to this, how do you handle people changing the configuration and reloading while the program is running? Merging changes when the config had live changes in memory that potentially conflict or need merging with the config files? Saving the config back?
_an_ official language
Stop gets called in reload. Also, in 2.7, Play's going to be using Akka's CoordinatedShutdown, which will allow for "graceful shutdown" aka request draining so you have more than just the one stop hook.
I do believe there's something inheritely wrong in using unpronouncable symbols liberally. For me, Cats shows that's not necessary. 
Perhaps you could expand on a bit on the safety thing: in general `Ref` _is_ safe for concurrent access, that's part of the point, and doesn't require a Semaphore. You do need a Semaphore where the modification function is `A =&gt; F[A]`(because you can't express it with `update` or `modify`, but separate `get` and `set`), but the two very common cases of `A =&gt; A` (modify the state), and `A =&gt; (A, F[B])` (modify the state and run an action, typical of state machines) can be done using `Ref`only.
Also note that in fs2 we have `once` for lazy memoisation /** * Lazily memoize `f`. For every time the returned `F[F[A]]` is * bound, the effect `f` will be performed at most once (when the * inner `F[A]` is bound the first time). * */ def once[F[_], A](f: F[A])(implicit F: Concurrent[F]): F[F[A]] = Ref.of[F, Option[Deferred[F, Either[Throwable, A]]]](None).map { ref =&gt; Deferred[F, Either[Throwable, A]].flatMap { d =&gt; ref.modify { case None =&gt; Some(d) -&gt; f.attempt.flatTap(d.complete) case s @ Some(other) =&gt; s -&gt; other.get }.flatten.rethrow } }
&gt; Also note that in fs2 we have once for lazy memoisation &gt; /** &gt; * Lazily memoize `f`. For every time the returned `F[F[A]]` is &gt; * bound, the effect `f` will be performed at most once (when the &gt; * inner `F[A]` is bound the first time). &gt; * &gt; */ &gt; def once[F[_], A](f: F[A])(implicit F: Concurrent[F]): F[F[A]] = &gt; Ref.of[F, Option[Deferred[F, Either[Throwable, A]]]](None).map { ref =&gt; &gt; Deferred[F, Either[Throwable, A]].flatMap { d =&gt; &gt; ref.modify { &gt; case None =&gt; Some(d) -&gt; f.attempt.flatTap(d.complete) &gt; case s @ Some(other) =&gt; s -&gt; other.get &gt; }.flatten.rethrow &gt; } &gt; } 
Cool. Thanks for the correction! 
honest question: you said remote is not an option *at this time*. given apple's tight control and secrecy (I mean no offense or disrespect) do you think remote will ever be an option?
That attitude doesn't help. The Kotlin team at JetBrains aren't envious of Scala -- they have different goals for their language and they still continue to support Scala in their IDE.
Cool! I'm still quite happy in Scala land. 
Each package declaration imports the symbols from the declared package. So more stuff is in scope in the first example, which can see everything in `server` *and* `server.api`. In the second example you can't see stuff in `server` and would have to import it explicitly.