Don't presume I'm doing smart things with `case` classes. I'm only using them because they let me hold my stuff in a sort of "structured record" with little effort :) I had seen that "pass-by-name parameter" option but indeed it doesn't work with `case` classes. It seems that dropping the `case` and going "pass-by-name" would be simpler than using lambdas. Would you suggest doing that ? I'm rather inexperienced so I'm trying to see what's the best option here. Thanks!
You're right -- unfortunately that one is required for any operation on scala futures. It makes ramping up new hires on futures more painful and it creates uncertainty around whether an appropriate execution context is being used in each `future.map` call.
These are imports used for all examples in their tutorial. I believe tho they only need `cats.effect.IO` and maybe `cats.effect.Effect`. We have a http4s server using monix 3.0.0-RC1 and we literally have only one import from cats.effect (c.e.Timer) at the whole project, none at the server mount point. (A lot of places import `monix.eval.Task` though) We also ended up with just one wildcard import of server itself - `org.http4s._`, which was auto-converted by IDEA since the class itself issues a bunch of HTTP requests (using http4s' client) so it needs to construct them (`FormBody`, `Uri`, `Request`, `Method`, etc.). No import of `org.http4s.implicits._` can currently be found. We rely on IDE to autocomplete necessary things and all our controllers extend a base class with `Http4sDsl[Task]` mixed in. Http4s DSL just provides extractors (`def unapply()`-s) for `Request` case classes and other case classes they contain. You can write your own or just match on objects directly. It's way less convenient though, you'll just end up reinventing most of DSL. Tho I can't argue with buying into `cats`. Simply because with: type HttpService[F[_]] = Kleisli[OptionT[F, ?], Request[F], Response[F]] You won't be able to avoid THAT thing when defining a middleware.
Type classes. I see typeclasses as a way to make `scalac` write the code for you, or fail at compile time if it cannot. E.g., Semigroup (`def combine[A](a: A, a2: A): A`). I tell scalac these things (already done in scalaz/cats) - For two `Int`, `combine` is addition (+) - For two `Tuple3`, `combine` is elementwise`combine`. Semigroup for all three element types must be available - For two `List`s, `combine` is concatenation (:::) - For two `Map`s, `combine` combines values at the same keys, leaving rest unchanged. Semigroup for values must be available. And then I go like: — Hey, scalac, can you `combine` two `Map[String, (Int, Map[String, Int], List[Int])]` using those rules (which you have to learn once)? — Yep. *compiles fine, runs as I expect* — Hey, scalac, can you combine two `Map[String, UnsignedInt]`s? — Nope. *implicit not found* — Oh, I forgot. For my own class `UnsignedInt`, `combine` is addition too. — Fine now *compiles fine, runs as I expect* — *after few hours of writing code* Hey, scalac, how about `(String, Int, File)` that you just inferred? — Nope. *implicit not found* — Ah right, wrong variable. Should be `(String, Int, List[Int])` — Fine now *compiles fine, runs as I expect* The feedback loop I get from this is just ridiculously fast. I can be writing code for hours and only run the app afterwards to do a quick check and things tend to Just Work a lot of time. (You still should have unit tests tho!) BTW, I also get some other nice methods. — Hey scalac, sum up that `List[(String, Int)]` for me. — Here you go, that's `Option[(String, Int)]`. — Ah, I probably need to learn a `Monoid` to avoid that `Option` thing. And other libraries take advantage of this. — Hey scalac, can you encode a `Map[String, (Int, Map[String, Int], List[Int])]` into JSON for me? — Yep — How about that `(String, Int, File)`? — Nope And with shapeless' implicits those libraries can support my own `case class`es, intertwined with standard types: — Hey scalac, can you decode that JSON string into a `Map[String, Person]`? — Ok, here you go, `Either[Error, Map[String, Person]]` *compiles, runs, does not blow up into my face* --- Then comes the syntax. Eventually you'll end up `combine`ing a lot of `Map[String, (Int, Map[String, Int], List[Int])]` and writing `Semigroup[Map[String, (Int, Map[String, Int], List[Int])]].combine(mapA, mapB)` kind-of defends the point of having a type inference. So you define an extension method, `|+|`, and you can write `mapA |+| mapB`. And it works on tuples, too, since it uses `Semigroup`, impicitly. `(42, mapA) |+| (killCount, mapB)` just works, but if you swapped the tuple arguments, it would not compile. And then you notice that `list.map(i =&gt; thing(i)).reduceLeftOption(_ |+| _)` appears quite often in your codebase, so you write an extension method on a List: `list.reduceMap(i =&gt; thing(i)). But hey, it works on `Vector`. And `Option`, too. You end up writing less code, while still getting the compiler working for you. And often you end up doing the same thing over and over which are specific to your application. Like converting a `Future[List[A]]` into `Future[A]`, failing the future if the list is empty or has more than one element. And it needs to happen in a middle of chain of `map` / `flatMap` calls, so adding a function call would look like that: ``` requireSingleton( future.map(f) .flatMap(g)) .map(h) ``` This is not so easy to read, and hard to spot on which `Future` `requireSingleton` operates. Would read much better, had it been a method: ``` future.map(f) .flatMap(g) .requireSingleton() .map(h) ``` So you define an `implicit class` with a method `requireSingleton`. You get code reuse while maintaining readability, and IDEA can highlight / jump to definition on extension methods. And it goes like this. A lot of operators provided in `cats` work in so many settings that learning them is worth the time. E.g. `mproduct` works for `Option`s, `List`s, `IO`s, `NonEmptyList`s, `State[Int, ?]`, etc. and you can figure out at a glance exactly *how* it's gonna do the thing. Then you transcend mortality and begin searching API docs for type signatures instead of names. You name your method parameters like `fgab: F[G[A =&gt; B]]` because there's no good name for that function inside two very abstract thingies, taking an abstract thing and returning other abstract thing. Your colleagues begin to agree with you, but stop understanding anything you say, because you're talking about monoids in a category of endofunctors and your backend being a Kleisli function from a streaming request to a polymorphic effect of a streaming response.
I don't have one at hand. Macros are definitely *not* going away, but scala.reflect macros, which are the current macros that work with the scalac API aren't going to work in scala3/dotty, since that doesn't use the scalac API. I don't think scala 3 will have white box macros either, but that's not entirely set in stone I think. There will be an update from Olaf on a transitional macro system soon I believe. 
Macros are a tool of last resort. Are you sure you have reached that point?
Runtime DI's are braindead easy to work with and I've never had any issues with getting it to work. Implicits on the other hand can appear out of nowhere. Accidentally import the wrong thing and a bad implicit comes into scope.
There will be an update soon! :)
I too am interested in the use case...
The community is very helpful when it comes to this, and there's [a lot](http://lmgtfy.com/?q=typeclasses+scala) of information about typeclass encoding in Scala and the benefits. Also you'll see them mentioned a lot in community posts and conferences and whatnot. Really, unless you work purely at a "scala as a better java" shop and read 0 blogposts from the community and don't interact at all, it's almost unavoidable to come across it (or even spot it in the stdlib: see `Ordering` or `Numeric` for example)
Wayyy over engineering this and absolutely no need for micro services. Just build the website with the features you need in a single app/database. Figure out what a *minimal* set of useful features are and build those first. What you’re describing abstractly above could take years to build. Start small and build up over time. 
There are still an amazing numbers of people who don't read blog posts or surf Reddit, and have never even heard of a monad.
And there's an amazing amount of people who don't give a shit about advancing their career or learning something new, or attempting to digest a new way of thinking about programs. My help extends out to those who do.
Thank you. That was really helpful!
https://www.scala-lang.org/blog/2017/10/09/scalamacros.html https://github.com/scala/docs.scala-lang/pull/57#issuecomment-239210760
Well first I got excited for a moment, and then I got disappointed. The headline is overpromising, don't you think.
Just tried it: This is everything I was hoping for. Thanks again! :)
Why does this code String.format("%02X", 2.toByte) generates this error? overloaded method value format with alternatives: (x$1: java.util.Locale,x$2: String,x$3: Object*)String (x$1: String,x$2: Object*)String cannot be applied to (String, Byte) 
+1 to `sttp` if all you need is an HTTP client. If you also need a server side API, I'd go for `http4s` that also comes with an HTTP client.
You should be prepared that your macros will break in some future Scala version. They have always been "experimental" (which never stopped people from using them quite a bit) If possible, avoid using macros. Maybe implicits / shapeless will work for your use-case, thus you can push maintenance burden to other developers. Shapeless in particular likely to be supported in one way or another in newer Scala version. If you need `def` macros and not annotations (e.g. functions that are inlined at the call site). Also if you need actual type information and not just syntax transforms, meta is not an option at all. Use reflect macros. If you need good Intellij support, use scalameta / paradise. I had written two scalameta macros, one for tagless algebras, another for defining enumeratum enums without repetitive boilerplate [(here)](https://github.com/oleg-py/enumeratum-macro), and both writing and using them is fairly decent in IDEA.
Do you think it would be better if you had to provide it explicitly? 
Scala doesn't allow primitives to implicitly box when the target type is `AnyRef` or `Object`, because it's felt that this masks errors. Use something like `(2.toByte: AnyRef)` (or `java.lang.Byte`) to do the boxing explicitly.
This might solve my issue with autowire. I need to look into this!
I'd prefer if future transforming operations had defaults similar to the behavior in twitter futures, java completable futures, js promises, rust futures, f# async, swift promisekit, etc.
Is anybody here using Scala as a better Java? What is your opinion on Scala so far?
That was where I started out. I've crept up to a very typelevel style over the years, but always in incremental steps driven by particular use cases. I found the language proper is very good for the better-Java use case, but brought down by the lack of support from the community (e.g. if someone asks how to do something with Spring in Scala they tend to get "why are you using Spring?" rather than help). Of course no-one (myself included) wants to help someone write code in a given style when they know a better style, so I can sympathise with those on the experienced side too.
I think a good way to do this might be to hang out on [Gitter](https://gitter.im/scala/scala) and chat with other developers, go to conferences, listen to talks online, get involved in an open-source project, etc.; so you can get to know other good Scala programmers and learn how they think about things. A lot of it will come down to style and interest. I'm an ok functional programmer but would get tossed out on my ass if I tried to interview for an Akka job. Nobody is good at everything … Scala is a big language.
&gt; Judging from open source code, even the top professionals tend to be frugal in their use of, say, implicits, currying, and other more specialist functions. At the same time, I can't help but feel that I am basically using scala as java with Option and much much better lambda functions. If you are looking at "most" as the spark apis, sure. That's because of the databricks style guide, which explicitly states to avoid implicits: https://github.com/databricks/scala-style-guide Many other open source projects, such as fs2 and Monix and Play! use implicits liberally (about 1% of the sloc): https://github.com/monix/monix -- 868 lines in main code of the word implicit out of 73946 lines of code according to sloc https://github.com/functional-streams-for-scala/fs2 -- 262 lines of main code with the word implicit out of 12636 according to sloc https://github.com/playframework/playframework - 1015 lines with the word implicit in main code out of 82585 lines according to sloc. This is opposed to Spark (note that it is a much larger project in terms of lines of code, it hand 534707 objects in git!): 1051 occurrences of the word implicit in main code. It has 615813 lines of code according to sloc. Note that implicit is mainly used for paramaters to functions, in monix, out of 4578 defs in main, 868 of them take implicit arguments, for example. That's ~25% of the methods. The thing is, unless you are writing your own applications outside of a framework, you should be following the conventions of the framework you are using. So in Spark -- use fewer implicits. In https -- yeah, probably using currying and implicits. If you follow the convention of your framework, and can test and change things easily, you are writing quality code for your domain.
There are lots of teams using Scala. They use it for microservices, streaming applications, data science etc. Some teams use it as a better Java, some use Lightbend's Akka stuff, others prefer the Typelevel ecosystem. I used to work there for 3.5 years. The interview process is pretty standard for a big company that hires a lot of people. Short tech screening, then interviews with ~3 engineers with some live coding. 
Play has a Futures trait that provides delay and timeout operations that use the default execution context...
Awesome. Now if only IJ didn't freeze for multiple minutes when trying to `alt+enter` autocomplete stuff sometimes...
Sounds bad. You can help us debug this by creating an issue on https://youtrack.jetbrains.com/issues/SCL and adding information as described here: https://intellij-support.jetbrains.com/hc/en-us/articles/207241235-Reporting-performance-problems
I tell people Scala is like a swimming pool -- you don't have to dive in the deep end to learn how to swim. Martin Odersky put together a [list](http://www.scala-lang.org/old/node/8610) of Scala skills for application developers and library developers. Generally, I'd say the learning curve of Scala are the bits not covered by Java: learning how to use functions, and then how to use implicits, and then how to use the type system. Don't get discouraged if you don't get it all at once -- the contributors have been doing this for years and sometimes forget how long it took to wrap their heads around. 
Did you give more memory to IJ? Default configuration is quite horrible for Scala.
Is there a reason you guys don't use github issues? Having to log on to submit a ticket to a platform I'd rarely ever check while your repo is hosted on github isn't too great.
 Yay! An intellij person! Any way to make the sbt shell error output navigible by click and F2 like the errors that appear in the gui? Any way to handle cross-builds yet? Any way to read managed sources from the sbt configuration? Any way to recognize extra settings (like handling IntegrationTest sources separately from Test sources)?
Because they make their own issue tracking software and eat their own dog food it would seem. 
Is this an intellij plugin?
didn't know about Gitter. Thanks
It's a standalone program but IntelliJ has built-in support for it. If you add a project/scalastyle_config.xml file to your project root then IntelliJ will highlight violations in your scala code as configured in that file. You can then use https://github.com/ngbinh/gradle-scalastyle-plugin to check for violations at build time if you use Gradle, or http://www.scalastyle.org/maven.html for Maven, or http://www.scalastyle.org/sbt.html for sbt.
[wartremover](http://www.wartremover.org/) inspects your code a bit more deeply than scalastyle. You can avoid more javaims with the OptionPartial, Throw, and TryPartial rules documented here: http://www.wartremover.org/doc/warts.html
fs2, Monix and Play! are *libraries* (or frameworks, whatever), not *applications*. Libraries tend to define more implicits because that's how you write generic code in Scala. Applications tend not to define implicits often, if at all. They of course call APIs with implicits, so they *materialize* implicits all the time, but don't *define* them. This distinction can even be visible in code written *by the same developer* in *the same repository*, if there are parts of the repo that are more library-like and others that more application-like. For example, in https://github.com/scala-js/scala-js you'll find that dichotomy being strongly present. Under [`library/`](https://github.com/scala-js/scala-js/tree/master/library/src/main/scala/scala/scalajs/js), you'll find (unsurprisingly) a library-like area with lots of definitions of implicits. But under [`linker/`](https://github.com/scala-js/scala-js/tree/master/linker/shared/src/main/scala/org/scalajs/linker), you'll be hard-pressed to find a single non-local implicit definition.
Awesome!
You don't actually have to use a wildcard import - that's just the way the example does it to keep things simple. In production code you'd normally just import the things you want to use.
Please provide an option for hiding module names. It’s been on Youtrack for years and is incredibly annoying if you’re doing ScalaJS development. I literally could fix this issue in 15 minutes. Closed source software really sucks sometimes. 
My personal experience with bug reports for IntelliJ has been terrible to date. Poor response rate and timeliness, and a LOT of back and forth for basically no satisfactory resolution. I'm a paying customer with licenses for my entire team, historically. Rather than me expending lots more of my own time for this, i'd be potentially willing to let y'all clone my repo and experience the scala IJ sadness firsthand if you're willing to sign an NDA and keep the code private. This should give you no end of problem cases you can test against. I can even point to the most egregious examples, and you'll be able to debug it directly using whatever internal tooling you have around this. If interested, please shoot me a reddit mail and let's discuss.
I agree and that's a very important distinction. Most apps are separated into a library and a program, certainly. Also, I call play and spark frameworks because they mostly call your code and have a prescriptive way of integrating with them. You can also use their libraries as libs in your applications - play-json, twirl, etc. Something like akka and fs2/monix are libraries because they are smaller and you call them. It's subtle, but you can definitely tell the difference. Fs2's streamApp might actually blur the lines a bit, there, too, I suppose.
Just keep looking at the code behind popular Scala libraries and looking at idiomatic Scala code and it will come naturally. If you can, contribute to OSS Scala projects in small ways and benefit from the community code reviews. Also helps to try and build hello-world or better apps using interesting scala frameworks. In no particular order: - akka - play - cats/scalaz - finagle - algebird Each of these give you exposure to new patterns and paradigms. Worth noting that w.r.t Spark specifically, most of the contributors are writing (very good) Scala that looks like Java for a variety of reasons. Especially the memory management, concurrency and performance optimization code. As a result, the source code tends to be less focused on idiomatic Scala and more focused on code that works as fast and correctly as possible.
Also consider code generation. I've become a big fan of using some form of data as input to a code generator that runs and produces source code before your application code compiles. That way, you get output you can actually see and work with with all your fancy IDE tooling. You can use tools like string interpolation or templating for simple stuff, or Scala Meta for complex code generation.
That's a good question. I think part of it is that typeclasses in Scala are a pattern, built on more fundamental languages constructs. Arguably, there should exist somewhere official a mapping of some common PL patterns to Scala constructs.
Thanks for all the efforts put into Scala plugin!
I'm in a similar place. I have been using Scala less than a year and have fallen in love. I recommend getting the "red book" and trying out some of the middle chapters when you're feeling ambitious. It helps give you a grounding on the type system and what is possible. Finally I will say that I dislike job interviews that focus on obscure details of languages to test your knowledge. I find that language quirks can be quickly learned as needed but having an intuitive sense of how to solve problems in a given language is much more important.
For reference: This is the idea64.vmoptions I use: -server -Xms1G -Xmx16G -XX:ReservedCodeCacheSize=1024m -XX:+UseG1GC -XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:SoftRefLRUPolicyMSPerMB=50 -ea -Dsun.io.useCanonCaches=false -Djava.net.preferIPv4Stack=true -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Dawt.useSystemAAFontSettings=lcd -Dsun.java2d.renderer=sun.java2d.marlin.MarlinRenderingEngine 
Scala Competency levels by Odersky: http://www.scala-lang.org/old/node/8610 This kind of thing is subjective of course, but it's a good list of topics to explore.
When is the IJ team going to get around to fixing their scala parser? Ensime does better parsing cats and other code, and it's maintained by only a few people.
The [lambdaconf ladder of functional programming](http://lambdaconf.us/downloads/documents/lambdaconf_slfp.pdf) got a lot of mockery and criticism when it came out, but I do think it's a good effort in this direction (though of course specific to one particular style of programming) of making you aware of what skills and techniques are out there that you might want to learn.
I get the frustration, but let's try and be polite? The amount of customers intellij has that are on the scala + cats warpath is probably pretty small. 
It does though. There are better ways to use it, but it's a really good language for that use case (support could be a lot better though).
Thanks for all the efforts put into Scala plugin! SBT shell is so great!
&gt; Any way to make the sbt shell error output navigible by click and F2 like the errors that appear in the gui? Not sure what you mean. File paths in the sbt shell can be navigated by click. &gt; Any way to handle cross-builds yet? We're well aware of this pain point and looking into how to get them to work. Related issues are aggregated here: https://youtrack.jetbrains.com/issue/SCL-12945 &gt; Any way to read managed sources from the sbt configuration? This should already be happening, if the managed source output is in `sourceDirectories`. However generated sources without corresponding configured source directory will not be imported since they are only known at build time. &gt; Any way to recognize extra settings (like handling IntegrationTest sources separately from Test sources)? Not yet.
IntelliJ Scala plugin is [open source](https://github.com/JetBrains/intellij-scala) and pull requests are welcome.
Why compare with 2.11.12 instead of 2.12.4?
I think you could do some PR:)
Hey /u/jastice, i want to thank you and thank the Intellij for all your efforts &lt;3
I held out with just a text editor for about a year and finally buckled
Just curious, why do you use Eclipse over IntelliJ? I haven't used Eclipse in a professional setting, but IntelliJ seems a lot better.
please please do. ensime leverages the presentation compiler, but I often get quicker results with it than I was getting in Intellij. I have had to abandon Intellij for now because the typechecking was frequently giving errors on files that were properly typed and was also slow to update at the same time. 
He means [this one](https://www.manning.com/books/functional-programming-in-scala)
IntelliJ error highlighting is unreliable with both false positives and false negatives (e.g. anything with higher-kinded type members and it just gives up completely). IntelliJ has plenty of advanced features that I miss when using Eclipse but that's no good when the basic fundamentals are so unreliable.
inb4 Simon's comment that Scala is dying, that "leadership" is not doing anything about it, and that Kotlin will prevail – oh, and by the way the Scala website really sucks.
This is exactly why I use Eclipse. All my colleagues using IntelliJ frequently waste time on spurious red squiggles.
The closest thing you get to an actual discussion of type classes in Scala is in the [Context Bound FAQ](https://docs.scala-lang.org/tutorials/FAQ/context-bounds.html): &gt; Context bounds were introduced in Scala 2.8.0, and are typically used with the so-called type class pattern, a pattern of code that emulates the functionality provided by Haskell type classes, though in a more verbose manner. 
this is not 100% true. Eclipse is also way more unreliable with false positives and false negatives. and since recently IntelliJ even supports `ProductArgs` which basically means that a big chunk of the shapeless ecosystem has good syntax highlighting in IntelliJ
&gt; e.g. anything with higher-kinded type members and it just gives up completely That's not true, it handles higher kinded types just fine. The trouble it has are with type projections and for example I could not get it to work with the [kind-projector](https://github.com/non/kind-projector) although other people did not have problems. So it has problems for example when operating on `EitherT[F, L, ?]`, shove a complicated param in that `?` placeholder and it will choke. But at that point the Scala compiler itself chokes. I have not used Eclipse, but I used [Ensime](https://ensime.github.io/), which uses the same engine and in my experience IntelliJ IDEA is much better in all aspects, except for resource usage.
Interesting, thank you. Considering all teams use it so differently, is it still a "company standard", so to speak, or did a lot of people end up using it because it's nice and interfaces easily with their existing JVM stack?
Genuinely curious are those "various reasons"?
By the way if I interpret OP correctly, the "failure" here is the failure of Spark to upgrade to 2.12, not any kind of failure on Scala/Lightbend's side.
I have the opposite experience. I fled intellij for ensime cause of the massive problems I was having and ensime has suited me much better.
IntelliJ IDEA is the best IDE right now, in general. I use it for Scala mainly, but it handles just about everything I care about ... JavaScript, TypeScript, Python, Clojure, Ruby, Haskell. I use VS Code as well and I'm a long time Emacs user, but it's not the same level of smartness.
I'm glad that Ensime works for you, it's free software and it enables Emacs (my favorite text editor) to work as an IDE for Scala. My problem with it is that when it fails, it fails hard. The difference is that with IntelliJ IDEA you can usually ignore errors, as the IDE is pretty tolerant and keeps working just fine. It's not the IDE's job to prove correctness, but to guide you in writing code, while being tolerant of real time changes. This is a property that of all other mainstream IDEs, it's only Visual Studio that gets it right.
This library is something my friend and I started some time ago. Since we finally managed to put together some documentation we decided to share it, and gather larger feedback :)
- How is Intellij support? I'm assuming autocomplete and other stuff works fine? - Any noticeable impact on compile time? I've been using shapeless records for this purpose, but we have to "make faces" at IDEA for shapeless code to not produce false highlights and compile messages are scaring the hell out of my colleague. Would gladly try the library out.
Yeah, but any kind of Scala-related failure is good when you're advocating Kotlin.
&gt; this is not 100% true. Eclipse is also way more unreliable with false positives and false negatives. I don't believe you. Every time I say this people claim Eclipse isn't reliable, every time I ask them to give examples they can't. I've done some pretty advanced Scala in it (all of tierney, e.g. [this file](https://github.com/m50d/tierney/blob/master/free/src/main/scala/tierney/free/package.scala)) and the "Problems" tab has always been 100% reliable (the presentation compiler occasionally shows spurious error highlights, but never false negatives, and the spurious highlights have a subtly different icon from the real ones).
&gt; That's not true, it handles higher kinded types just fine. Higher-kinded type *members*, i.e. path-dependent types of higher kind. It used to show any use of them as an error, I filed a bug report and in the next version it was "fixed" so any use of them always shows as success.
Cloudera is the top player in the Big Data space. If you have a Cloudera cluster, the default Spark version is 1.6.0. If you want to upgrade to Spark 2.x, you will have [many known issues](https://www.cloudera.com/documentation/spark2/latest/topics/spark2_known_issues.html), like the lack of access control layer, dynamic allocation, HBase support. So I guess that the Scala version of Spark 2.3 is not the main issue.
This looks interesting. Could've used it last year while transforming data from a legacy access database which had many tables with 40+ columns. Shapeless records worked but like with /u/oleg-py 's colleague my colleague feared compilation error messages and the red squiggles.
But most of those are not the Upstream Spark issues as far as I understand.
Umm, AWS - EC2/EMR are the biggest player and it provides Spark 2.2.x already. 
That is correct. My point is that even if some Spark 2.x will support Scala 2.12 tomorrow, developers will have to wait distributions to support it, and that could take years.
Can somebody provide the source of that quote, please? I'd like to see the context.
IntelliJ’s highlighting is more of a suggestion I trust the compiler for the final checking 
There’s little incentive to upgrade to Scala 2.12 vs . investing in additional Spark features. Keep in mind that a lot of people come to Scala *because* of Spark, not the other way around.
Can this library convert from class FooInner(name: Option[String], address: Option[String]) class Foo(inner: Option[Inner]) to class BarInner(name:String,address:String) class Bar(inner:BarInner) or return errors where option is none but suppose to be some ?
There's no type signature in that lib that returns something like Try/Either/Validated, so I'm pretty sure the answer is no.
[It has](https://www.javadoc.io/doc/io.scalaland/chimney_2.12/0.1.8) but use cases are better explained in [these docs](https://scalalandio.github.io/chimney/).
Well, to be honest we haven't though of such use case - we'd rather though of one where you would fooInner .into[BarInner] .withFieldComputed(_.name, _.name.getOrElse("default")) .withFieldComputed(_.address, _.address.getOrElse("default")) .transform though it makes sense only if you have a lot of cases and relatively little need to provide defaults. You might post a feature request though! 
Looks amazing! Thanks!
Sure, docs are very nice and easy to understand. I just often look at types involved to understand what function does instead.
As a user and library maintainer I can't blame the Spark folks. Scala 2.12 is a thoroughly underwhelming release in terms of user-facing features - indeed since it drops JDK6/android support it's kind of a downgrade. If switching my libraries to 2.12 had involved substantial work I probably wouldn't've bothered either. The flipside of not involving many user-facing changes is that migration to 2.12 should be easy, so I imagine spark will get to it sooner or later. But really, what's the rush? What's the compelling thing that you get from 2.12 that's not there in 2.11?
I think the time where you could point out issues and work on solutions in Scala is gone, it's all "shoot the messenger" now.
It is a truly great library! I started using it and just like that I've burned a lot of useless boilerplate. Thank you guys! Now my code is so clean I could make soap out of it!
hey there simon, i come to recognize you from your comments on here and on /r/programming. &gt; I think the time where you could point out issues and work on solutions in Scala has gone. This is just my two cents, people do not respond well to an individual who is always against something, a healthier attitude is to be for something else. Paul Philips had a healthier attitude towards Scala, he wasn't always against the Scala collections, but he was for a saner implementation of the collection which he worked on, he wasn't against the Scala compiler as it is now but he was for writing specifications for the Scala compiler... and people listened to him, and i agree with him a 100% on everything he said. Maybe you do have the for something else attitude, but when you start a comment on /r/programming with &gt; Scala is done. And I'm saying this as someone who spent half a decade working on the compiler, standard library and documentation. and then go on to praise Kotlin as a replacement for Scala, then it's really hard for people who know both languages to take you seriously, and you just come off as someone who is trying to scare people away from Scala.
Finally java.lang.Process is supported :)
The optimizer, increased compiler performance, reduced jar sizes, probably better interop with Java 8, improved Future API, right biased Either...
&gt;Fixing Java interop issues around enums Do you have a link to the PR / issue that you worked on? I'm curious what happened.
Performance improvements are always nice to have, but a ~10-30% improvement is not compelling, and that goes for all these changes really. The Java 8 interop improvement is only for calling Scala from Java which seems like a narrow use case (Scala shines at high-level orchestration/coordination/sequencing so it makes more sense for Scala to call Java than the other way around, IMO). Improvements to Future and Either are pretty trivial and were already offered by libraries. None of it's bad, but none of it's functionality I'm rushing to upgrade so I can use - indeed none of it seems worth giving up android compatibility for, frankly.
I think the author makes the point that it's awkward to try to access Scala code from Java code in a seamless way. I've never tried this myself, but I've heard this claim enough times to believe it's true. But beyond that, if you're only goal is some nice boilerplate reduction within the same patterns you'd use in Java, I think Scala's a bad choice. Sure, it works great for this, but the problem is that almost all the commentary around the language is about patterns that aren't really common or feasible in Java. So you'd find yourself wading through a sea of stuff you find confusing, because all you want is Better Java.
Sure, 2.12 is not revolutionary. On the other hand, if Scala would not take advantage of JVM improvements people would complain too, if Scala adds a lot of incompatible language and library changes every other year people complain too. And I'm sure even you have to admit that right biased Either is pretty great for integration of the std library with FP libraries.
&gt; On the other hand, if Scala would not take advantage of JVM improvements people would complain too, if Scala adds a lot of incompatible language and library changes every other year people complain too. Well there will always be different constituencies with different needs. Scala 2.12 delivers a lot of useful functionality for people who are writing frameworks in Scala for use from Java (i.e. Play), because that's where Lightbend's focus is. I don't think that approach really serves the interests of the wider community or is the best thing for the language in the long run (as I and many people said 2 years ago when the roadmap for all this came out, and the typelevel fork was created partly as an expression of that view. IMO the typelevel scala features would be important enough to be worth breaking compatibility for, but so far they've all been implemented without breaking compatibility with scala 2.11 and therefore android), but ultimately it's Lightbend's call to make (and them who have the most to lose if the language declines). &gt; And I'm sure even you have to admit that right biased Either is pretty great for integration of the std library with FP libraries. I mean don't get me wrong I'm glad it happened, but the actual practical impact is that I get to remove one import from some of my source files. Big whoop. 
&gt; I think the author makes the point that it's awkward to try to access Scala code from Java code in a seamless way. I've never tried this myself, but I've heard this claim enough times to believe it's true. Scala does let you express APIs that are hard to call from Java. But the same API written in Scala is no harder to call than that API written in Java. So I guess it's a question of how you're comparing them. &gt; Sure, it works great for this, but the problem is that almost all the commentary around the language is about patterns that aren't really common or feasible in Java. So you'd find yourself wading through a sea of stuff you find confusing any time you're looking for help, because all you want is Better Java. I think we're in agreement: the language itself is good for it, but support has a lot of room for improvement.
Which parts? I took a look at it before and I haven't seen anything that really stood out.
Reminds me of: https://github.com/kailuowang/henkan
Well that looks awesome. Question regarding Scala.js usage – any insight into how much code is added to runtime? Not in KB but just conceptually – is it pretty much as lightweight as writing out similar transformations by hand, or is the generated code significantly larger than that?
Simon, I enjoy reading your posts, always technical and factual. Moreover, I agree with many of your views on Java, Scala and Kotlin. And I have no doubt many others do as well. Keep it up, mate!
Updated the scala-native-libpcap article &amp; source code accordingly: - https://www.scalawilliam.com/scala-native-libpcap/ - https://github.com/ScalaWilliam/scala-native-libpcap/commit/eaa8117926c9e148e70868c3c1d29694b117e519
libpcap, fascinating!
There is a little overhead, but it shouldn't be an issue. Additionally we want to optimize it in future releases by migrating some type-classe derivation from Shapeless to macros.
The IntelliJ-internal representation of the code enables all the inspections and refactorings and is best supported by the IntelliJ platform. Changing to using the Scala presentation compiler would invalidate a lot of the investment in this and be a large undertaking, and increase overall complexity.
&gt; At some point, you've got to make peace with it and move on. Oh, I did, and it didn't work out so well for me, judging from the incoherent ramblings sent to my university after I announced I wouldn't work on documentation anymore. &gt; these particular issues seem all but anecdotal to me I debate works by either acknowledging or refuting points that have been made. Dismissing the points you got handed and instead criticizing the person who has made them is not the way to go. &gt; You are not at the center of the Scala contributor community. Strawman. &gt; it seems that you are not even a notable or important element of it, despite your early contributions to the main Scala repo [...] So there is no reason that you should have any particular say on the future of Scala. The merit of an argument should not depend on who is making it. But if that's you standard, please explain why a person who hasn't done a single line of maintenance in the last 6 years _should_ have a say. &gt; So there is no reason that you should have any particular say on the future of Scala. No one is required to accept your viewpoints as fact and to follow your directions. Strawman. &gt; On the other hand, you do have to respect people's right not to, especially when those people are respected members and even founders of this community, who have earned to have a say in these matters. Respect is earned, not demanded. There is no central bank of respect that makes up respect out of thin air after it is gone. &gt; What's wrong is constantly complaining that people did not necessarily agree with you, no matter how strongly you believe that they were misguided (and no matter if they actually were). Scala's technical problems are symptoms of non-technical problems, and I'm happy to point them out. &gt; When you make your own language, or become a major contributor of Kotlin (or whatever), then you'll be entitled to having strong opinions and to steering the directions of a project. People can have strong opinions whenever they want. The only thing I care about is the merit of the argument, not the person making it.
This is what happened with enums: - Discussion: https://groups.google.com/d/topic/scala-internals/8RWkccSRBxQ/discussion - Spec draft: https://docs.google.com/document/d/1mIKml4sJzezL_-iDJMmcAKmavHb9axjYJos_7UMlWJ8/edit?usp=sharing (you can see how the proposal evolved since 2013, and when the draft was not updated anymore, because at that point scalac got almost enough fixes to implement `@enum` without a language change) - Pull requests: https://github.com/scala/scala/pull/3247 https://github.com/scala/scala/pull/4898 https://github.com/scala/scala/pull/5352 https://github.com/scala/scala/pull/5521 This is what happened with implicit resolution: https://groups.google.com/d/topic/scala-language/ZE83TvSWpT4/discussion
charts for 2.12.4 looks mostly like for 2.11.12 except a string concatenation which has the same throughput for last two versions of 2.12.x 
&gt; Incoherent ramblings? Care to share what was sent to your university? Or PM me the details, if that can help me understand some context. I won't share any private communication I have received, but you got a PM with a rough outline of the events. &gt; your personal problems as a former contributor If I say "here are the facts that are demonstrably true" I get "that's just your opinion!". If I say "here is my view based on the experience of having worked on it" I get "you are making everything about yourself!". You can't have both. :-) Also, how many people with "personal problems" are required, before it's even considered that the problem is not with those people leaving, and therefore blaming them does not fix this? There was a number of people leaving over the years, all of them having largely the same reason for leaving. I don't think describing this as "personal problems" and dismissing everything they have to say as "anecdotes" is the right way to go. &gt; It's just the overall negativity and unnecessary doomsaying that puts me (and many other people) off I'm not sugar-coating things, I never have and I never will be. My intention is to give an accurate assessment of the situation. If the situation is bad, I will say so. What I'm saying is not unique, in fact pretty much the same I said was pointed out in the last 24 hours by someone else https://news.ycombinator.com/item?id=16708783. &gt; Yes, there are problems to be addressed. This has been going on for roughly a decade, with no progress made. People have left because they realized that. &gt; you should not be surprised when people start shunning you or not showing you too much respect As some already did from the beginning? :-) I assure you, voicing an opinion was never a requirement for receiving disrespect.
This is a fantastic idea! I wish I had had this when writing a compiler in school. I wrote it in Haskell and had 5 or 6 data definitions representing the same abstract syntax tree with slightly different items attached to the same basic values ('expression', 'conditional', 'application', etc.)
Sometimes you need to understand what someone really needs/wants, if someone asks how to do something in a particular style, it's more likely they want to learn about the style than the particular problem. So with those questions it wil likely be better to talk about the style and what the strengths and limitations of it are.
Kotlin let's you program the same way better, scala let's you program in a better way
Working building the second part of the [XSN Block Explorer](https://xsnexplorer.io/), at the moment it works querying the RPC server directly, a database is being integrated to be able to perform complex queries, it is [open source](https://github.com/bwang22/xsnblockexplorer).
You can do the same way better in Scala though, if that's what you want. 
This makes no sense what so ever. Why do you have both G1 and ConcMarkSweep ? Quite sure it won't even use G1.
Hey. This seems like a good idea. Gotta hate unchecked stuff. On your README, on the example where you change the desugared code to: getCounts .map(_ match { case (x, y) =&gt; x + y }) why not change it to the following: getCounts .map{ case (x, y) =&gt; x + y } Just wondering if you think the current is better for some reason I'm missing. Keep up the nice work, will follow the repo. Kudos.
You can use DL4J / ND4J or their corresponding Scala wrappers (ScalNet and ND4S). It's got support for GPUs.
It's actually a bit reverse. In parser the expansion is already `(_ match { case ... })`, so two are exactly same. It's typer that chooses (based on context) whether it's becoming a `PartialFunction` (generating optimized `isDefinedAt` too) or a regular one.
I'm on the same page about SIP. It has a chance to be included in Dotty, and /u/lihaoyi has already provided a very good proposal here: https://github.com/lampepfl/dotty/issues/2578 This keeps both behaviors, just the strict one is the default, and the unsafe requires a `case` keyword. I also would not want existing code to be silently broken, even if I think it's for a good cause. Somebody out there definitely relies on the `withFilter`-based desugaring. I hope that if plugin gets enough users, it will push Dotty to include a feature at the language level. Like it happened with [kind-projector](https://github.com/non/kind-projector) - type lambdas [made it](http://dotty.epfl.ch/docs/reference/type-lambdas.html) to Dotty, after all. And in the meantime Scala devs can now destructure all the `Task`s they want :)
I’m still not a huge fan of it but I think it’s somewhere to start 
Wrote up a blog post about uJson, the new standalone library backing uPickle. Although I already posted the new version of uPickle on this board, I think uJson is cool enough that it deserves some attention of it's own. Questions, comments &amp; opinions welcome! 
Could we back-port the `case` behavior described that issue using a `@case0` annotation? That would make for a wonderful prototype
I think it's a Spark issue not a Scala issue,and databricks is a commercial company. As for Scala itself,I think Scala 2.12 have improved a lot. For ktolin , hehe. I have to say, Kotlin +++++ and Scala -----, then they finally should be the same.
Ok, thanks for your suggestion. I looked over it and also found [Breeze!](https://github.com/scalanlp/breeze/wiki). Now, I'm confused which one to go with. Althouth, Breeze looks more feature-rich than ND4S at a first glance.
It seemed to me that, after `parser`, the short form has (&lt;empty&gt; match { case ... }) whereas the long form has (_ match { case ... }) and so I guessed that the `&lt;empty&gt;` was used as a marker for `typer` that this was eligible for the `PartialFunction` treatment. After `typer`, however, both trees were identical (modulo identifier renaming). But I'm really guessing here, so your expertise probably wins. To me, the world before `erasure` is an obscure beast better left untouched :P
Bravo! 👍
I'll be honest, I love your work Haoyi but I come to Scala to _not_ deal with people treating code like python/ruby. It's great that you empower these types of people, but your claim that you need to become an "Expert in X library" is unsubstantiated. It doesn't take a lot of brain space to use (for example) circe efficiently, even manipulating jsons with lense-type solutions, and it's a bit of a stretch to say something along the lines of "X library is preventing you from becoming an expert in Y". All in all, it's great you're putting out a competitor in this same space. It's just a lot of people _prefer_ to not have the "_intuitiveness that someone coming from Python, Ruby or Javascript might expect working with JSON data types._", because I'd rather not have any code whatsoever that looks like the spaghetti that comes from passing mutable references in those languages.
I think in this example there should be no need for a `withFilter` since you know that the pattern match will succeed by just looking at the types. The compiler has machinery to check for "irrefutable patterns", I am surprised it did not kick in in this case. 
Henkan's intellij story is way worse though and it's a pretty complex lib internally.
It seems this machinery is kicking in in refchecks phase, but the error for missing value `withFilter` comes earlier from typer.
AFAIK in Haskell it's not a problem because of a ['Trees that grow' technique](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf) – Using type families to vary a field in a data type like data Ast (phase :: Phase) = Val (ValOfPhase phase) data Phase = Untyped | Typed type family ValOfPhase (phase :: Phase) where ValOfPhase 'Untyped = NamedVal ValOfPhase 'Typed = TypedVal data NamedVal = NamedVal Text data TypedVal = TypedVal Text Type data Type – a technique that's sadly missing from scala that I find myself constantly reaching out for
This is awesome. Having a simple JSON parsing library with little code + effort needed on the programmer's behalf is a must. Having multiple options on how to do things is one of Scala's largest benefits.
Why not just use `.toString` / `.toInt` to cast types? `str` looks like an API designed 50 years ago, similar to `strcmp`, `strlen`, etc. 
This is really neat. Our data science team is thinking about migrating slowly from python to Scala and so this sort of library is a wonderful addition to the ecosystem. I’d love to see an additional example in the sbt docs. For example, if I want to package this with sbt-native-packager, how do I sort out the right platform specific reps for a different prod/development OS?
&gt; I won't share any private communication I have received, but you got a PM with a rough outline of the events. Thanks, but what you sent me does not substantiate much of anything – it's basically just a rant on gitter that remains completely vague about what was actually said/written by Martin. I note that vagueness seems to be a defining characteristic of your rants about people mistreating you. All I can gather from what I've seen so far is that you were super serious about improving some minors aspects of Scala that get overlooked but are important for beginners (which is good), but that you would sometimes not take "no" for an answer – as in discussions such as [this one](https://github.com/scala/scala-lang/pull/359), where the main disagreement concerned something entirely petty. After some misunderstandings during a chat that you had at ScalaDays, you created some ridiculous drama [on the mailing list](https://groups.google.com/d/msg/scala-internals/r2GnzCFc3TY/x2s5b0xVAwAJ), where you publicly disrespected people and questioned their good faith, using your usual periphrases like "well-known community leaders" and "famous Scala people, like those participating on this list," as if that somehow made it okay. Apparently, you got some (it would seem deserved IMHO) backlash for this affront. Since then, it seems you've been on a crusade against anything Scala. Regardless of your personal spite towards a few people, what you do is not nice because it hurts the community as a whole, and thereby people who have nothing to do with this. 
Compute.scala is a lower-level library than BigDL and Spark ML. Compute.scala(or ND4J, Breeze) is used to create your own algorithms.
&gt; Redmonk Language Rankings Sees Rapid Rise for Kotlin and Swift Why was the full title obscured?
Exactly! Scala should appeal to scientists and people with mathematical background. The ecosystem will benefit from it a lot. But those people need something to start with.
A critical feature for scientific computing is resource management. Unlike C++'s RAII or Python's reference counting, garbage collectors in JVM are not very good at managing data buffer or other native resources. I am proud that I finally find out a determinate approach to manage resources in Compute.scala (and still easy to use).
What does your benchmark measure? And what's the unit? And how does it compare to [ujson](http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html)? 
&gt; I note that vagueness seems to be a defining characteristic of your rants about people mistreating you. I have a minimal amount of decorum, unlike Martin, who seems make things up in great detail whenever truth is too inconvenient. &gt; but that you would sometimes not take "no" for an answer – as in discussions such as this one, where the main disagreement concerned something entirely petty. At that time I have seen the homework pattern repeat a few times too often. It's the right of ever contributor to define the limits of his or her contributions, especially toward people who have displayed bad faith in their behavior more than once. &gt; After some misunderstandings during a chat that you had at ScalaDays Really? Is gas-lighting the best thing you can do? If I had any trouble remembering, I can consult my records about the conversation. There was no misunderstanding. Martin and Heather – in separate chats – were very consistent with each other on their position. Cheers, Simon
Have you looked at PlayJson? It does have an api that is this simple val name = (parsedJson \ "counts" \ 2).as[Int] 
why ate you away from python to Scala? our shop is Scala first, and I would love to drop our python services for Scala ones, but it just doesn't seem possible atm 
That's exactly why it is a good API for **un-typesafe-ly** casting a JSON object to a string.
Even `stringValue` is better than `str`.
If you look in the network panel on chrome, can you out show what all the headers are?
I like it, but does the rest of Dotty really need to be held up for this?
It would be fastest if jsoniter-scala wasn't existed https://github.com/plokhotnyuk/jsoniter-scala
Such a shame Jsoniter even exists, since it's basically just a copy of DSL-JSON. You can easily find that via DSL License headers in it's source code. It would be so much better if Tao improved DSL-JSON instead of copied it under a different name ;( Now it's just confusing having multiple DSL-JSON copies with different names with similar performance characteristics ;(
They are. And I would love it if someone else helps with the library.
No, it's not. In general, it's more or less obvious once you now that you're dealing with json. It has even more sense if you consider that this library main usage, at least at the beginning, will be scripting(via ammonite).
Anyone knows the reason for "The kind of annotation macros required by simulacrum will almost certainly only be supported in Dotty as code generation tools, requiring a separate build step." ?
I tried adding jsoniter-scala to https://github.com/shawjef3/java-json-benchmark with some wrapper classes, but I can't get it to run. It wouldn't really be fair anyway since jsoniter would slow down due to java/scala conversions.
sounds familiar. which frameworks are you using for data science and that sort of thing so far? we're mostly on spark ml ourselves, but it's often quite a bit of overhead to get simple stuff done 
Sorry, Jef, I've never tried to build Scala projects by Gradle... As an option we can add support of Java classes and collections to jsoniter-scala macros or at least write custom codecs for them. 
I personally program in an overarching FP style, but frequently fall back on imperative structures. I still avoid the big no-nos like side effects, but I have absolutely no issues with basically just switching my brain to Java-mod whilst implementing individual functions if I think that's the easiest way to write it.
In [React4s](http://www.react4s.org), the Attachables often use imperative style and mutable variables on the inside, but expose a declarative interface on the outside. See the [Spotify example](http://www.react4s.org/examples/spotify-search) for instance. In general, when exposing effects, imperative programming is fine on the inside, as long as it doesn’t leak out into the interface.
Sorry, I cannot access it as it's private.
Logging. It's much easier with an imperative side-effectful called to `logger.debug` or something. Using a monad transformer or some other pure construct sounds horrifying. On the other hand, logging is the only side-effectful thing I would even consider approved *a priori* in a Scala application. Another occasion where it's "fine" to do so is when working with an imperative Java library, but that's generally out of necessity. 
What do you mean held up? Dotty currently had no planned release date. And for Scala there is still 2.13 to come out this year and most likely 2.14 next year (or 2020). Anyway Dotty is pretty much the only place where they could make big changes like this. Once Dotty 1.0 (Scala 3.0) comes out a big change like this would have to wait for Scala 4.0 (so another 5+ years)
How is this not literally a regression compared to what we have now?!?! &gt; It does the job, but at a cost of lots of boilerplate! The required boilerplate is very technical, advanced, and, I believe, frightening for a newcomer. The complexity of typeclasses does not end with their definition, either. It continues to the use sites, which typically need one extra type parameter per typeclass argument. Scala is a language set out to eliminate boilerplate and promote the simplest possible style of expression. But it seems in this area it has utterly failed to do so. So you go from three steps, a trait, an impl and an optional syntax object, to: trait SemiGroup extends TypeClass { def add(that: This) } trait Monoid extends SemiGroup { common def unit: This } How is this more coherent? You add essentially three keywords (Typeclass, common and `This` type), of which it's not even clear what the fuck it's supposed to be doing because I don't get how a monoid returns a `This`. This is going from a relatively simple definition of typeclasses in pure scala, which is what we currently have, to some weird OO conglomerate shit where you have `.impl`s and a bunch of different keywords that translate to the same shit we have now. Jesus Christ I hate hopping into the Scala pessimist crew because I actually enjoy the language and what it gives me access to, both in the community and the code it enables me to write, but this is some convoluted shit for the sake of keeping some weird as fuck OO style fusion with typeclasses. Why not just typeclass SemiGroup[A] { def combine(l: A, r: A): A } Is that so hard?!?!?!?
&gt; Martin and Heather – in separate chats (one very short, one very long) – were very consistent with each other on their position that there is neither interest or necessity in improving the website/documentation/marketing. I think the key word here is _necessity_. You can consider that something is not strictly necessary (or urgent), and still be open to select pull requests that improve specific things that do matter. In that light, if your accusations of people "not taking responsibility for their decisions" and "not standing up to their word in public" are really based on this misunderstanding, they are pretty much unfounded, and borderline slander. &gt; At that time I have seen the homework pattern repeat a few times too often. So, your version of the story seems to be "they kept moving the goalpost without telling me that it would lead nowhere." But reading what you said on [the PR](https://github.com/scala/scala-lang/pull/359) and on the mailing list: &gt; Because after I had done a single thing, people found new requirements or simply pronounced that things were not technically possible, which led to me investing even more time showing that in fact the issues are fixable it really, _really_ looks like you just _took it upon yourself_ to do some work _people had told you **not** to do_ – a very different story to me. What apparently happened is Heather told you "no" about merging the two repos, but because you're so incredibly stubborn you just went ahead and put a huge amount of work just to show her you could do it. Of course, it was still a "no" from Heather, and you blamed her for wasting your time, though it was the result of your own obstinacy. Correct me if I'm wrong here. The worst part in this story is that the core of the problem is (from my point of view) majorly irrelevant; almost nobody cares if the Scala website repos are merged or not – mere Scala users in particular won't see a difference.
I have added couple of benchmarks for dsl-json in this branch: https://github.com/plokhotnyuk/jsoniter-scala/compare/add-dsl-json-platform-benchmarks?expand=1 Please validate if all is done right, because results for small messages are most slow: Benchmark Mode Cnt Score Error Units AnyRefsBenchmark.readCirce thrpt 5 1627537.062 ± 162271.909 ops/s AnyRefsBenchmark.readDslJsonScala thrpt 5 233522.492 ± 2154.097 ops/s AnyRefsBenchmark.readJacksonScala thrpt 5 2567685.939 ± 98209.955 ops/s AnyRefsBenchmark.readJsoniterScala thrpt 5 9366231.528 ± 1319617.561 ops/s AnyRefsBenchmark.readPlayJson thrpt 5 627947.411 ± 24701.902 ops/s AnyRefsBenchmark.writeCirce thrpt 5 1908907.018 ± 168961.304 ops/s AnyRefsBenchmark.writeDslJsonScala thrpt 5 232092.701 ± 12277.523 ops/s AnyRefsBenchmark.writeJacksonScala thrpt 5 5809741.462 ± 513132.319 ops/s AnyRefsBenchmark.writeJsoniterScala thrpt 5 27473943.888 ± 3194654.984 ops/s AnyRefsBenchmark.writeJsoniterScalaPrealloc thrpt 5 26999162.628 ± 2314934.870 ops/s AnyRefsBenchmark.writePlayJson thrpt 5 1244444.876 ± 84318.596 ops/s While for medium size messages it is better but definitely are not fastest: Benchmark Mode Cnt Score Error Units GoogleMapsAPIBenchmark.readCirce thrpt 5 8025.703 ± 404.539 ops/s GoogleMapsAPIBenchmark.readDslJsonScala thrpt 5 10522.919 ± 60.681 ops/s GoogleMapsAPIBenchmark.readJacksonScala thrpt 5 13381.444 ± 1021.156 ops/s GoogleMapsAPIBenchmark.readJsoniterScala thrpt 5 22141.195 ± 424.687 ops/s GoogleMapsAPIBenchmark.readPlayJson thrpt 5 2287.320 ± 29.606 ops/s GoogleMapsAPIBenchmark.writeCirce thrpt 5 8516.226 ± 336.177 ops/s GoogleMapsAPIBenchmark.writeDslJsonScala thrpt 5 37843.091 ± 1314.111 ops/s GoogleMapsAPIBenchmark.writeJacksonScala thrpt 5 41475.254 ± 2511.897 ops/s GoogleMapsAPIBenchmark.writeJsoniterScala thrpt 5 61740.239 ± 844.927 ops/s GoogleMapsAPIBenchmark.writeJsoniterScalaPrealloc thrpt 5 104994.132 ± 1231.734 ops/s GoogleMapsAPIBenchmark.writePlayJson thrpt 5 5127.813 ± 350.282 ops/s 
&gt; still be open to select pull requests that improve specific things that do matter Yep, that was offered. &gt; In that light, if your accusations of people "not taking responsibility for their decisions" and "not standing up to their word in public" are really based on this misunderstanding, they are pretty much unfounded, and borderline slander. I was there, you weren't. There was no "misunderstanding". &gt; [...] Yes, you are wrong. Cheers.
It's an interesting construction, one has to read all .md to understand how to parts come together. I'm not putting a reply below /u/tsec-jmc 's comment, because I think it should be either edited or removed, being unnecessarily _full_ of swearing. But I'll refer to it, as it seems a natural reaction when you only read half through the proposal (which is an experiment now). I agree that `This` is a confusing type member name. So you have trait TypeClass { type This } instead of trait TypeClass[A] The same with "OO" style `add(that: This): This` (I think the return type was accidentally omitted) instead of `def add(a: A, b: A): A`. Ultimately, in my understanding, the reason behind that is to avoid the duplication of old-style type class (implicit class, `def (A, A): A`) and syntax-extension methods (`implicit class HasTypeClass[A: TypeClass](x: A) { def add(that: A): A = implicitly[TypeClass[A]].add(x, that) }`). This becomes clear when looking that the use of `extension` to define instances. extension IntSemiGroup for Int : SemiGroup { def add(that: Int): Int = this + that } This collapses the FP type class and the OO extension syntax into one construct. It may look weird at first, but then after groking it, I think it's actually quite nice. (Again, not sure that `This` is such a great type member name). ---- The same goes for `common`. At first, it looks weird, and yes, do we need another reserved keyword? What it does, if I read it correctly, is to "spread" a method across class plus its companion object. If you read a bit in the current (2.12) collections library, you can see all sorts of boiler plate for getting from a collection instance back to its "factory" via `companion`, which must then be overriden by sub-classes. The `common` construction seems to solve this thing in a quite elegant manner, if you discount the costs of adding another keyword. 
I edited it now, are you happy? It's family friendly :)
&gt; We trade two keywords for like 7 That's not true. Most additions are types/classes, not keywords. The new keywords are `extension` and `common`. &gt; why we have to fuse OO style with FP style Because that's the basic idea of Scala. Looking at 'factored-instances', I think it shows that the new construction is quite elegant. trait Functor[A] extends TypeClass { def map[B](f: A =&gt; B): This[B] } (BTW, here is clear why `TypeClass` doesn't have a type parameter, because it can be of different kinds). How would you write that using `typeclass`? typeclass Functor[F[_]] { def map[A, B](fa: F[A])(f: A =&gt; B): F[B] } Is that any better? Now you might have to fiddle around with the partial type application for something that becomes `F[_]`.
You keep saying mentioning that the syntax comes from OOP style, but I believe this proposal is heavily inspired by Rust's traits, and Rust cannot really be considered OOP, at least not in the traditional sense. Yet they manage to use typeclasses for their main way of sharing code, while also including method-style syntax when you want to use that. Maybe it could work to have something like typeclass Semigroup[A] { // compiler understands that This == A def combine(x: This, y: A): A } which could automatically allow for methods to be used like `1 combine 2` / `1.combine(2)` I think there's enough precedent set in Rust and Python for explicitly taking a "self" parameter as a first argument for enabling method syntax.
&gt; That's not true. Most additions are types/classes, not keywords. The new keywords are extension and common. My bad then. That's not clear to me reading the proposal. &gt; Because that's the basic idea of Scala. Even when it makes something like the typeclass construct much harder? You don't have to be faithful to this for every single aspect of the language, especially when it's a construct that comes from functional programming. &gt; Is that any better? Now you might have to fiddle around with the partial type application for something that becomes F[_]. Yes it is. It's more concise. It's expressed entirely in terms of the parameters fed to the class. What on earth tells me, from inheriting from `TypeClass`, that `This` is higher kinded? at least with `typeclass Functor[F[_]]` it is clear that it's a typeclass for some `* -&gt; *` kinded F. How about expressing `Profunctor`? How about `MonadTrans`? `This` is then this all mighty type that is surprisingly whatever-kinded to fit the definition? I don't like this. It's less clear than the haskell and the scala definitions that are currently in place.
&gt; What on earth tells me, from inheriting from `TypeClass`, that `This` is higher kinded? As far as I understood the examples, it becomes entirely unimportant at that point. It only ever matters for the instances. &gt; It's less clear than the haskell and the scala definitions that are currently in place. I think it appears less clear because it's an unusual way to seeing this construction, and the perception will change as we get used to it.
I'm saying it's OO-based because, from what I gather, it relies on inheritance from some god-trait called `TypeClass`. &gt; Maybe it could work to have something like.... Why would this be necessary to use `This` at all, when even right now this is expressible in pure scala? [simulacrum](https://github.com/mpilquist/simulacrum) basically does this already in an annotation. Moving that sort of construct to have first class language support is really all we need. 
&gt; I think it appears less clear because it's an unusual way to seeing this construction, and the perception will change as we get used to it. but why do we have to get used to triple the machinery than we have now :|. This could be so much more concise following what haskell does if we can stomach the reality that maybe haskell is onto something when it comes to brevity of typeclass instance syntax.
&gt;Why would this be necessary to use `This` at all Hmm well I thought that that could be used to tell the compiler that you want an infix method to be made available, since that is kinda how this works in Rust. But I guess you don't actually have to have that level of control, it could just generate infix for all methods that have a first argument of type `A`.
&gt; This at all, when even right now this is expressible in pure scala? simulacrum basically does this already in an annotation. simulacrum is not expressing this in pure Scala, but through a macro annotation, basically a compiler plugin. So if the question is between typeclass Foo vs trait Foo extends TypeClass vs @typeclass trait Foo then these are simply three syntax variations of the same degree of complexity.
I know simulacrum is a macro. I'm just saying that what simulacrum does, with or without annotations, is essentially all we would need, if it supported higher kinds with more than just `* -&gt; *`. Simulacrum is expressible in pure scala though, which is my point. &gt; then these are simply three syntax variations of the same degree of complexity. One is much more clear than another when dealing with higher kinded types.
I tried to running it but the setup failed (on my Win) ;( So I can't really tell whats the problem there, but from the quick look I can say that it's never a good idea to allocate in such a microbenchmark. I did not expose all the relevant APIs in Scala. Some of them could return an array plus a size to avoid return allocation. But I suspect the problem is somewhere else... which a quick profiling would reveal. Also, the Scala version of the library is not the fastest one, since it still relying on reflection or some unbenched APIs. And as I said it would be great if someone else helped with stuff. It's probable that there is some setup overhead in your bench I'm not aware of (which would be great if fixed) but all I can do for now is point you to various other 3rd party benchmarks where it is the fastest or to what the Jackson guy says: https://twitter.com/cowtowncoder/status/753068282442227712
Red book is the best
/u/phi_array since I'm no longer on my phone it's this'un: https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653/ref=sr_1_2?ie=UTF8&amp;qid=1522637974&amp;sr=8-2&amp;keywords=functional+programming+in+scala 
Scala for the Impatient
You don't even need a new keyword. This seems even more elegant: type class SemiGroup[A] { ... } 
Finatra supports scala futures, we use it in production and it’s great 
Programming in Scala, 2nd edition.
Hi, zapov! Have you managed to run benchmarks on Win? Please check if your PATH and JAVA_HOME are directed to JDK 8, because Scala 2.12.5 compiler cannot properly work with macros: https://github.com/scala/scala-dev/issues/480 Then just run `sbt clean test` and if all of them pass use following command to run benchmark suits: sbt 'benchmark/jmh:run .*AnyRefs.*' sbt 'benchmark/jmh:run .*GoogleMapsAPI.*' I've added benchmarks with a preallocated byte array that covered by a preallocated output stream: https://github.com/plokhotnyuk/jsoniter-scala/commit/8e9f17485dab180538a7f395ff6dc0dd3c2d3fd2 It helped to get greater throughput to beat Jackson that serialize to a new array (without preallocation), but these results far from hitting jsoniter-scala: GoogleMapsAPIBenchmark.writeDslJsonScalaPrealloc thrpt 5 43048.424 ± 2418.695 ops/s I think that there are a lot of room for improvement in JSON parsing with Scala, and thing are changing faster these days... if you will not manage how to parse and serialize efficiently from/to buffered bytes to Scala data structures to be competitive with jsoniter-scala, then you can try to beat ujson (from upickle) in parsing/serialization of JSON strings. 
I did manage to fix sbt, but then JMH didn't wanna run, so I ran it on a different project to see whats going on. I've updated my post with more efficient way to encode/decode stuff
FWIW, I made a proposal of my own for consistent type classes in a comment: https://github.com/lampepfl/dotty/pull/4153#issuecomment-377865231
Current encoding of typeclasses in scala is so awful that it barely justifies its usage. I love the `a.func(b)` syntax and find it infinitely superior over `func(a, b)` and current encoding leaves me with latter or forces me to use implicit conversions, which are too strong abstraction for this particular usa case.
Although it's not really about learning Scala, but about learning functional programming.
I worked ona big scala project for 2.5 years with many people joining and leaving. The first architect was a Haskell guy and guided us to use scalaz and shapeless. the next scala guy was a OO guu and started removing and simplying the types we had. After 2 years the code base was a total mess of FP and OO patterns. I'm not sure if the OO bit is such a benefit for the language. I do love Scala in general, but even though I come from a Java background I rarely use any OO patterns, except for the core bits that come from playframework itself.
never coded python, but that sounds like scala and it's type inference.
What do you mean by ProductArgs?
Yes, that seems reasonable. I guess thats partly expected overhead due to reflection/not using gen code for Scala. I didn't claim that it was the fastest lib for Scala :D just that it supports Scala now :)
I did not imply that python does not have it's problems, but i think it's a very important thing to note when talking about python. Just for completeness, in Python 3.5 type hints where added, which can remove some of the dynamic typing. Obviously that will not help with type checking before runtime, but it does make refactoring easier, and static code analysis tools also benefit from it. These analysis tools needs to be run proactively, which may or may not happen, and they may not catch all type errors, so i choose scala over python in 98% of the time.
You could say that, in some sense, but there may or may not be a compilation step, and even if there is, it does not do type checking, it's only done at runtime.
Are you inspiring me to add a full support of Java classes and collections to jsoniter-scala? 
&gt; so i choose scala over python in 98% of the time. That is the same reason I have switched to Scala and why it has become my new Python as I can develop in the REPL in almost the same way as this language due to the type inference. Python is still better for some numerical and scientific programming due to many bindings to C and C++ libraries.
As I said. Jsoniter copied a lot of code from dsljson which makes it fast. There is a comparison at Jsoniter articles and on https://github.com/fabienrenaud/java-json-benchmark/blob/master/README.md I also added a new bench there https://github.com/fabienrenaud/java-json-benchmark/pull/18 
The fastest way to learn the basics.
My "Modern Web Development with Scala" will work if you also know the basics of Web development. Odersky's "Programming in Scala" is also a good choice.
In the past, Scala was able to ship roughly one major change per release, and it usually took that whole release to iron out all the issues associated with it. I'm not sure how having a dozen, mostly half-baked ideas shipped in a single release and saying "well, if that doesn't work out, let's just try again in 5 years!" is a winning move...
Found this awhile ago https://gist.github.com/d1egoaz/2180cbbf7d373a0c5575f9a62466e5e1
The way I write Scala, I very often interleave *layers* of OO/imperative with layers of functional code. You could even say that's one of the architectural patterns I use. At one layer, you'll provide a functional API, but somewhere inside you want to use some imperative approach for performance/caching/incrementalization/etc., but inside that one, you use functional programming again, at a smaller scale. And you can go as many layers deep as you want, with this game. For example, the Scala.js `Linker`--the thing that takes the .sjsir files (kind of .class files but for Scala.js), optimizes them, and emits one .js file--has a pretty functional API at the top-level: https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/Linker.scala#L17 It basically has *one* functional method: def link(irFiles: Seq[VirtualScalaJSIRFile], moduleInitializers: Seq[ModuleInitializer], output: WritableVirtualJSFile, logger: Logger): Unit (OK, it writes to a file instead of returning the string, but still: the output in the file is a *function* of the `irFiles` and `moduleInitializers`) Somewhere inside that, though, I have several phases, which are all functions. The first one is the [`BaseLinker`](https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/frontend/BaseLinker.scala#L39-L46). The main job of that phase is to compute a so-called *reachability analysis* and return a trimmed-down codebase (what is often called dead code elimination). Now, even if at a high-level view, this is a pure function, the *implementation* would be dead slow if it were implemented in a purely functional style (it's basically a giant fix-point computation on a big graph). So inside of that thing you'll find the very imperative [`Analyzer`](https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/analyzer/Analyzer.scala#L25), with plenty of mutations everywhere. At an even smaller scale, the `Analyzer` uses again some functional constructs, like [`map`s and `flatMap`s in for comprehensions](https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/analyzer/Analyzer.scala#L191-L196). And guess what? Those `map`s and `flatMap`s are, in turn, implemented with mutation inside the Scala standard library. There are plenty of other examples in this linker, it's amazing! One other is the optimizer, whose main thing is [`GenIncOptimizer` and its `update` method](https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/frontend/optimizer/GenIncOptimizer.scala#L99), which is a purely functional interface. But inside, it uses very fancy caches and incremental computations, all within parallel computations using shared state (I'm not kidding). And down below all that, to actually the hard job of optimizing *one* method in isolation, there is [`OptimizerCore.optimize`](https://github.com/scala-js/scala-js/blob/134c8e456a4ade626091c74de780e03eb22aee83/linker/shared/src/main/scala/org/scalajs/linker/frontend/optimizer/OptimizerCore.scala#L123), which again has a purely functional interface (which is why the caches at the upper layer are sound). But hey! Inside, there's mutations again! And it goes on an on ... I think you get the idea. To me that's where Scala shines: at each layer of the architecture of your program, choose the style that applies best.
Can you recommend a good online resource to learn scala? Pdf or videos. Something from this year or last.
The hybrid approach in Scala is actually heavily leans towards OO and has a lot of shortcomings, e.g.: 1. Immutability is only reference based, no guarantee that object the reference points to is immutable. "Use immutable structures, Luke"? (c). Sure, but what's wrong with Javascript then? Just don't add a string to an integer. Besides it can be hidden, like a closure which uses mutable object, you never know. So no referential transparency in a general case, unless you check every object you use, 2. No native sum types (hopefully we'll have them in Dotty), 3. Typeclasses are ugly (they kind of work, but too much boilerplate and magic). Probably it's not the end of the list. So doing clean FP requires a lot of care, and given that Scala is touted as a gateway to FP for Java developers, i.e. those who have no or little experience with FP, it doesn't usually end well. Most companies hiring Scala developers I applied to (possibly not a representative sample) sounded like "we had a guy who wrote all this Scala mess and we need somebody to rewrite it in Java". Ironically Rust is much more FP language than Scala (except for the lack of HKT). 
Having recently moved from Java to Scala, I'd say just dive in, start coding and when ever you get stuck look up for the "Scala way" of doing it. The Scala cookbook by Alvin Alexander as well as the supporting blog post are all very helpful.
https://www.scala-exercises.org
 I'd love to, but the best recommendation would take into account your background. Where are you coming to Scala from? What's your experience as a programmer like?
Absolutely
&gt; And it would not break existing code. What if someone used a `typeclass` variable somewhere? Or do you want to introduce [context-sensitive keywords](https://docs.microsoft.com/en-us/cpp/windows/context-sensitive-keywords-cpp-component-extensions)?, a kludge IMHO. &gt; Overloading an existing keyword is more likely to break existing code. How so? 
Web-Platform. I want to learn more functional programming, trying to pick this over Haskell for usable real-world experience. A great deal of JS, HTML, CSS, plus C#, C++ and Java. My goal is a mix of become a better programmer by learning functional programming(design patterns) outside of javascript. The other goal is to learn to leverage the Java world in Scala or build services in Scala for frontend web platforms. 
FP by default, imperative when necessary. Even though I love FP and pure functions, there's still a lot of places where imperative / side effects are easier to both write, read and reason about. I often find that having all the "decision making"-doe as pure functions, and have some imperative effectful code is at the edges is the perfect eat your cake and have it too. You can unit test easily, while still having easy-on-the-mind imperative code at the edges (without having to deal with the complexity that comes with certain monadic approaches of writing your code). My only wish for Scala would be a clearer distinction between pre-op functions and effectful functions (or procedures). E.g. Pyre functions starting with `fn` instead of `def`.
Do you have any rules of thumb for yourself, or guidelines to keep things sane? How do you know where the boundary should be between your functional and imperative code? If a newbie joined your team, and knew how to program in both styles but didn't know where each one was best used, what would you tell them?
Assuming you mean pure vs. impure. Side-effects are acceptable in the small, locally, unobservably, as necessary to accommodate limitations of the JVM. Otherwise programs should be purely functional because this is how you gain compositionality, which is how you manage complexity.
Thanks for the reply! Would you say that purely functional approach is better at managing complexity than the OO-minus-inheritance approach? I've often use composition in imperative code, and I feel like it managed complexity quite well...
Fascinating, I never would have expected Rust to be more FP than Scala, tell me more?
By the way, [the 3rd edition](https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687) has been out for quite some time now.
I just jumped in and built a toy project. I had solid knowledge of Java and C++, made a number of embarrassing decisions, but it was a learning experience... Others have made good suggestions, but I'll emphasize that you don't have to learn any category theory to use Scala, and the "pure functional" style of cats / scalaz is completely optional. At the start, you can program in an imperative OO style, and add more functional principles as you learn.
Thanks, that's the one I meant... I have updated the post...
Basically the reverse of what I wrote about Scala: 1. True immutability, 2. Native sum types, 3. Convenient typeclasses, 4. Still some confusion here, closures and functions are separate, but it's explicit and methods are not sold as functions (i.e. closures) as in Scala. Rust does lack immutable structures, but it's an interesting point. If you _move_ arguments to a function it's gone for a calling method and there is no way you can mutate them, so programming with moves only (probably not cool, but as efficient as using references) fits FP paradigm, only without automatic cloning (and of course you can clone if you need to): fn f(mut xs: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { xs.push(4); xs } // xs is defined as immutable even though // it will be passed to f(mut xs...) let xs = vec![1, 2, 3]; println!("{:?}", xs); let xs_new = f(xs); // xs is gone here, moved, compile error // and above this line xs _is_ immutable println!("{:?}", xs) Very similar to creating a _new_ vector inside `f`, only the old one is not preserved. No side effects even though function explicitly mutates its parameter. And if we consider referential transparency, it's another interesting point. If we exclude I/O for a second, even Java can be considered referentially transparent if we include object itself as an argument (which it is), unless I'm missing something. Rust makes it explicit (calls to `xs.push(4)` is in fact syntactic sugar for `Vec::push(&amp;xs, 4)`). 
Compositionality in FP terms isn't quite the same as composition in the OO model; the idea is that you can take two small programs (or other structure) and combine them to create a larger program without increasing the surface complexity. So you can understand the whole by understanding the parts and the means of composition, and can keep building without accumulating a ball of mud. This falls apart immediately in the presence of non-locality and other side-effects. My friend Rúnar gave [a good talk](https://skillsmatter.com/skillscasts/10746-keynote-composing-programs) on these ideas that you might enjoy. 
Also I have Intellij + Scala working on my machine. 
Removed for being really weird and offtopic
Android development in Scala is _possible_ (I did it a couple years ago) but it's far from a first class citizen. Support is generally only via third party libraries that wrap the Java APIs in Scala code, and that is often incomplete so you end up having to drop into Java-style code anyway. You also _must_ run Proguard or you end up with too many symbols just from the Scala standard library. The Scala designers/developers don't seem to consider Android a priority (which I think is a huge missed opportunity, if not a total mistake), and until Kotlin the Android designers/developers historically wouldn't consider anything other than Java. So this situation was kind of a mutual dismissal, where the language didn't care about an expanding its market and the market didn't care about expanding its language support. If your goal is to learn Scala, it's probably better to do so outside of Android. If your goal is to learn Android programming, it's probably best to choose Kotlin. If you really want Scala and Android, just know you're likely to have to figure more out on your own and you won't have as smooth a learning curve with either, though you can succeed in the end.
Personally, I recommend the [scala-exercises](https://www.scala-exercises.org/) after getting past the Programming in Scala textbook, and the Coursera course. It will help get familiar with some of the libraries and current EcoSystem
There's no OO, no extension things, no objects. Just data and traits (very close to type classes). I like its approach. Then there's very cool ADTs with clean an clear syntax, matching, deconstruction of data types is very similar or as powerful or more than scala. The only downside is that currently and most likely won't ever happen for it to have true higher kinded types support as we know it, because the type system has to deal with low level details that don't match well with the abstraction. But something similar may be possible.
Monix's streams can handle this. Initiate the stream by telling it to get/generate the token at a specified interval and then use `replay(1)` to have it multicast the last fetched value every time a subscriber connects. To use the token in requests you pull the head from the stream which yields a Task/IO/what-have-you with the most recently fetched token, and flatMap it into a request.
Can you also incorporate the case where the token is found to be expired and you need a new token to be generated now, rather than whenever it's next scheduled?
I'm glad to hear your explain of "I think it's a bug that should be fixed.". What advantage Scala provides over Python at data science? Many people want low learning curve and quick prototyping. 
What advantage Scala provides are not only the Scala language itself, but also the way we programming based on the language features. Have you noticed that Compute.scala supports immutable and lazy operators, which is dynamically compiled to a kernel program? By combining the immutable programming with metaprogramming in Scala style, we aim to achieve two goals at once: Expressivity and Efficiency. In the [case study of fast matrix multiplication via split and join](https://github.com/ThoughtWorksInc/Compute.scala#case-study-fast-matrix-multiplication-via-split-and-join), we created a matrix multiplication operator from`map` / `reduce` in Scala, instead of hardcoding a CUDA kernels, and still runs fast. I guess data sciences would be happy if they are able to create a efficient implementation of their custom algorithms in a higher level language like Scala, instead of raw CUDA programming.
I think the https://www.manning.com/books/play-for-scala is pretty good. It'll give you a really clear set of examples of how to do some stuff in web. 
I use fs2 for this, which I think does a fantastic job of combining purity and IO. Monix is another option which I've only heard good things about. 
The pure-functional way to do this would be to represent "get the current token" as a monadic command using either a free monad + interpreter or tagless final style. Then you could have multiple implementations e.g. a naive one that always fetches the token, one that caches, one that supplies a fixed value for testing... (If you can express your logic in a free applicative way then doing this kind of "optimization" using `analyze` is more or less standard. One of the things I'm hoping to achieve with Tierney is to make it possible to do that kind of thing in the context of a computation that involves some dependent (i.e. monadic, non-applicative) steps as well, but I haven't got that far yet)
No. That flag was in the first iteration of TLS (pr #1, actually), which has vanished now.
&gt; Immutability is only reference based, no guarantee that an object the reference points to is immutable. "Use immutable structures, Luke"? Sure, but what's wrong with Javascript then? Just don't add a string to an integer. The problem is that you don't have compile-time checking of that, nor an ecosystem that supports it. In Scala I can write a method that accepts a `scala.collection.immutable.Seq` and a) that method will be usable with the stuff one gets from common libraries b) I have a reasonable level of assurance that it will never be called with a value that will get mutated underneath it. &gt; No native sum types (hopefully we'll have them in Dotty) Sealed trait sum types are good enough. Honestly I'm worried that Dotty will undermine all our algebraic goodness with its union types. &gt; Typeclasses are ugly (they kind of work, but too much boilerplate and magic), &gt; Constant confusion between a method and a function. Eh up to a point. These are genuine problems but they're pretty small IME. &gt; Ironically Rust is much more FP language than Scala (except for the lack of HKT). That's a big "except" though. You miss out on so much by not having HKT. 
Thank you very much for your input, guess I will have to read through some documentation!
This is one of the few resources that goes beyond what features Scala has. The book does a very good job of showing how Scala is more than just a better Java, and how idiomatic-but-not-hardcore-functional Scala still differs a lot from Java.
All features of old TLS are dropped and now it has to be made a PR into LBS first. It's unlikely they will accept a language fragmentation flag, like there was before, but they might accept e.g. a change in typer directly.
Does `Option.apply`'s method implement *Factory method pattern*?
A monix-Observable describes a uni-directional flow. If the token can be validated in a pure manner it can be integrated into the source Observable. On the other hand, if asynchronous invalidation of tokens is desired the setup gets more complicated. I'm fairly confident it can be done by racing a scheduled Observable against something like a ConcurrentSubject acting like a "refresh"-button.
As a user of Scala on the backend and Kotlin on android, I would probably stick with Kotlin for android. There's a huge benefit to working on a tool chain that google supports. 
I've been playing with Shapeless to try building generic web forms. It's funny, but so much of programming is still quite boilerplate wiring of fields to values, and I was wondering how far and how much of it all I could replace with a few specific type classes. I made a "Table" typeclass that lets you render any case class out as a normal nice looking table. I think something like a "Rails Admin" approach is definitely possible and is something I think I'm going to start building towards. 
Atomic Scala is also good, but is a bit dated. The O'Reilly Scala Cookbook is also pretty good. 
Depends on what OO parts you're referring to. Some parts like classes, universal equality etc. doesn't add much value except Java compatibility. Other parts are quite novel and powerful like the DOT calculus implemented in Dotty (and to a lesser extent in current Scala). That's basically what sets Scala apart from Haskell, ML, Rust etc.
What is OO-minus-inheritance? Composition is not something exclusive to OO, it's used very often in purely functional programming. The problem arises with functions that are not [referentially transparent](https://en.wikipedia.org/wiki/Referential_transparency), i.e. they have observable side effects which are not specfied at the type level. This makes them much harder to reason about and to compose with other functions.
&gt; Ironically Rust is much more FP language than Scala (except for the lack of HKT). This is so not true thats not funny. Rust's first priority in language design is not pure FP, its zero cost abstraction and Rust appears to cherry pick features from different languages. For example, all collection work in rust is done with interfaces on mutable collections, almost zero rust code is referentially transparent (the same cannot be said about Scala, where if you are exposed to cats/scalaz, they do their best part to make the code pure FP). Rust is also missing the biggest feature that is needed for any real pure FP, HKT (this is no small point, its almost as bad as missing first class functions). Its also highly unlikely that rust will ever get a generic HKT's that we are used to in Haskell and Scala, because HKT's need to box under certain circumstances. Rust also does not support full TCO, which is also necessary in pure FP. Instead people code imperative state machines/loops.
And also Scala's implicit parameters and sealed traits are way more powerful than corresponding Rust constructs (traits and enums). Hopefully Rust's type system will continue to improve though.
Removed for being crude/sexual, off-topic, and weird.
&gt; In Scala I can write a method that accepts a `scala.collection.immutable.Seq` Yes, it's true. But this is not _enforced_ by Scala, it's a library. I can probably create a similar library for Javascript and claim that Javascript is a functional language (which is partially true by the way). I was talking about language features.
&gt; In Scala I can write a method that accepts a `scala.collection.immutable.Seq` Yes, it's true. But this is not _enforced_ by Scala, it's a library. I can probably create a similar library for Javascript and claim that Javascript is a functional language (which is partially true by the way). I was talking about language features.
Are we talking about ecosystem or a language? Rust was not designed to be functional (or let's say it wasn't its main purpose), but it's easy to write nearly fully functional code and it will be enforced by the compiler. Just use moves everywhere and you have a perfect referential transparency (except for I/O, but this part is magic even in Haskell, and Scala is no different, and even I/O mutability is explicit when using moves in Rust). You can certainly write functional code in Scala, no questions about that, but it will not be enforced and you need to remember which collection is immutable, and which is not (and not just collections, everything). So basically you are relying on documentation and library maintainers' guarantees to make sure you have referential transparency. HKT and TCO - agree, it would be good to have them there, but I didn't claim Rust is pure functional. Also TCO is usually replaced by calls like `map` or `filter` anyway (which by the way are not implemented by using TCO in Scala AFAIK).
&gt; Are we talking about ecosystem or a language? Rust was not designed to be functional (or let's say it wasn't its main purpose), but it's easy to write nearly fully functional code and it will be enforced by the compiler. No it really is not, at least not for anything that is non trivial. It doesn't even contain the correct abstractions to do something mildly complicated. &gt; Just use moves everywhere and you have a perfect referential transparency (except for I/O, but this part is magic even in Haskell, and Scala is no different, and even I/O mutability is explicit when using moves in Rust) Which no one does and is besides the point of using Rust. I mean you can say the same about Scala, you can just use Cats/Scalaz if you want, it will provide you all of the machinary. &gt; You can certainly write functional code in Scala, no questions about that, but it will not be enforced and you need to remember which collection is immutable, and which is not (and not just collections, everything) Uh, Rust doesn't even have immutable collections in their stdlib apart from List. Last i checked they don't have an equivalent of Scala `Vector` which uses structural sharing. I wouldn't expect them to either considering its a terrible data structure when it comes to performance. &gt; So basically you are relying on documentation and library maintainers' guarantees to make sure you have referential transparency Same as Rust. Try to do any IO to be RT. &gt; but I didn't claim Rust is pure functional. Its also not any more functional than Scala, its actually a lot less functional than Scala (considering both the ecosystem and the language itself).
How can I do that without relying on a type which is supposed to be immutable? If it's true I will take back #1.
The point is that it's possible to a) write a type that will always be immutable and b) have your method only accept that type. In JavaScript you might be able to do a) but you have no practical way of doing b). (You also would have to build your own library ecosystem, because hardly any JavaScript libraries support working with immutable data - it's not just the fact that it's possible to write immutable datatypes in Scala, it's that those types exist as standard and are integrated into the whole library ecosystem).
I have to apologize, I didn't mean to start a flame war here. I was simply pointing out Scala shortcomings which IMO makes Scala less functional than I would want and Rust was only an example where those _particular_ problems are solved. It wasn't meant to be Rust vs Scala discussion.
I would probably use an algebraic datatype together with a rule like (S, M) -&gt; S, that consumes a message and updates that state. This allows you to pattern match on both the current state and message. Instead of stashing in the framework, I'd probably store deferred messages in the state value itself, just so it's more explicit that the deferral is related to the state.
Akka has finite state machine support built in: [Documentation](https://doc.akka.io/docs/akka/2.5/fsm.html)
If you give it a try, you'll see you can't initialize Options or Futures with new, only with static apply methods that return the instance to you. That's exactly what the factory pattern describes, so yeah, I'd say it fits.
You could separate initialization/authentication and request into separate actors. Your actor that handles the request can itself have children on its own for each request or for each request type. Something like this: class ConnectionHandler extends Actor { def receive: Receive =&gt; { case Authenticate =&gt; val handler = context.actorOf(Props[RequestHandler]) context become authenticated(handler) } def authenticated(handler: ActorRef): Receive =&gt; { case r:Request(...) =&gt; handler forward r case _ =&gt; ... } } 
The FSA support is simplistic at best. For any real production work I've found that it just doesn't _quite_ cut it.
Looking at the direction Akka Typed is going, they definitely want you to put all state into the receive method. It makes it easier to avoid closing over the actor's state, Also, with if-else and class members, you often end up with uninitialized members that don't make sense in all states, clogging up your class. I think that Akka FSM is overkill for almost everything.
It's perfectly valid to use multiple receives with become here, although that may have scaling penalties down the road. However, don't let "down the road" blind you to "getting it done." That aside, a more scalable way to handle this is to combine the two methods mentioned in the other responses. Use a "registration" actor that is "guaranteed" to be up and takes a message like "I need a Foo Actor." The registration actor spins up and initializes a Foo. Once Foo is up and running, it sends a message to the original requester saying, "hey, I'm your Foo." The original requester can then take care of things like "that service isn't available" on its own. Note this violates the "fire and forget" model that says you shouldn't depend on a service being able to run -- the caller has to understand the service availability and store enough data to be able to handle the return of the service. Fixing that would require some persistence to get right, but is certainly doable.
&gt; My only wish for Scala would be a clearer distinction between pure functions and effectful functions (or procedures). E.g. Pure functions starting with fn instead of def. Do mean you enforced by the compiler?
OO-minus-inheritance is my ignorant way of describing something similar to Rust, or perhaps using Java without the "extends" keyword. I admit, I have no idea how referential transparency relates to composition, and no idea what questions to ask. Looks like it's time for me to start reading tutorials.
&gt; I would probably use an algebraic datatype together with a rule like (S, M) -&gt; S, that consumes a message and updates that state. Interesting, are there any inherent advantages to this over switching contexts? It seems to me that this method will just bunch up all the Receive messages into a single block. &gt; Instead of stashing in the framework, I'd probably store deferred messages in the state value itself, just so it's more explicit that the deferral is related to the state. In my specific case, I don't think I need to distinguish messages that are deferred due to state, but in general, I'm not sure I'd want to replicate the Actor's mailbox functionality by creating my own queue. I feel like that just complicates the state unnecessarily.
&gt; It makes it easier to avoid closing over the actor's state Could you expand on what you mean by this?
&gt; although that may have scaling penalties down the road What is the reason for this? Is context switching an expensive operation? &gt; the caller has to understand the service availability and store enough data to be able to handle the return of the service. So does the caller figure out service availability on its own, or does the child do it and pass on this information to the parent?
It looks like you're trying to read it from the filesystem, but the file will be bundled in the jar and won't be on the filesystem. So try removing `Paths.get` and using `getResourceAsStream` to retrieve the contents directly. You _might_ also need a `/` at the beginning of your resource name.
Is the new example on http://www.react4s.org/ appetizing?
Hi, I usually use context.become and stash for handle simple states such are “loading data from db” or initializing clients.
&gt; What is the reason for this? Is context switching an expensive operation? No, become has almost no cost. The problem is that you may have a slew of requesters trying to hit that one actor, OR you have a metric boat ton of that actor killing whatever they’re a state full client of. Service discovery is indeed a sore spot for Akka, at least in the clustered world. You have to watch for node messages and decipher node roles from them, etc. that said, doing that work gives you a mostly-self-healing cluster that can scale automagically. Keywords to look for: ActorPath, Node Roles, ClusterSingleton. Beware of Split Brain — the default implementation in the free Akka release really sucks. 
Will vouch for SftI - I'm currently going through it. My background is in Ruby and Python - haven't even looked at Java code in a decade. It's (so far - about half through it) 95% accessible if you don't know Java or C++ (its target audience) - however, sometimes the author dedicates space to subjects that don't make a lot of a sense if you're not already a Java developer - but it usually just takes an extra half hour to look up what he's talking about it and why he's brining it up. Next after this book you'll need something on sbt - AFAIK SftI doesn't cover sbt at all - which is like learning Ruby without rubygems or bundler. You can write Scala with SftI - you just wont be able to do anything with it. So, find something on sbt next. 
I'm learning scala from "Scala for the Impatient" but it doesn't cover sbt or how to work with and use resources out there in the Scala community. What should I go through next to learn that stuff? Edit: Note - I have no Java background. Coming from Ruby.
I'd recommend SBT in action and frequent use of https://gitter.im/sbt/sbt You can also ping me on gitter, happy to answer what I can.
I think the example is interesting, but it really blends in with the color scheme. I have a very hard time reading the code. 
Thank you for the feedback - is it easier to read now?
On your proposal: &gt; allow type class bounds in general type parameter position That would explode the complexity of the types involved. The `SortedTree[A:Ord]` example is nice as a pedagogical example of why *coherency* is needed, but it's not what we care about in practice. In practice we care about MTL-style, with MonadMagic, without the compiler chocking and complicating those type parameters is a recipe for disaster. Coherency in Scala isn't a problem of the compiler forcing uniqueness, far from it. As a matter of fact Haskell's GHC doesn't force uniqueness either. No, the actual problem is that the compiler can't accept conflicting instances in scope, conflicts which are inevitable due to subtyping. Your proposal doesn't solve the actual problem and you're not addressing it in any way. &gt; we still want OrdInt and RevInt to be transparent synonyms of Int most of the time No, we really don't want that ;-) &gt; type SemigroupInt: Semigroup = Int &gt; ... &gt; The required implementations are found in the current scope. Not sure if you're aware, but that makes no sense. --- Now don't get me wrong, I love the creativity in these proposal, and I'm sure that something good will come out of it. But I find it funny how the community gets creative all of a sudden with dozens of proposals, just to prove the author wrong. Something to think about 😉
Are you serious? Just because I don't contribute to the compiler/dotty doesn't mean I don't do my part in other aspects of open source scala :| I posted that two days ago. Maybe at the beginning it wasn't clear that the kind of `This` was the same as the kind of the trait extending `TypeClass` (which is compiler magic and not expressible in scala) and that stuff like `Injector` was a class and not a keyword. That said, I was one of the first to ask about how you could express a shape like `MonadTrans` in that same issue (you can't under this proposal). If you want to hit @ the fact that I wrote that from a negative pov, yeah I probably did and I was irked at the time I read that initially, since it seemed to very directly ignore the elephant in the room in regards to the most obvious encoding for typeclasses (The one from Haskell). I wrote that initially out of anger. However, even if my understanding of the notation is better now than it was @ the time of posting this, I stand by the fact that this is convoluted, confusing and has a lot more machinery than necessary to express typeclasses, to the agreement of many.
Here's some specific examples: * [adal auth renewal](https://github.com/aappddeevv/dynamics-client/blob/master/adal/src/main/scala/dynamics/client/adal.scala) * [retry policy](https://github.com/aappddeevv/dynamics-client/blob/c034fd01ec267b6dcf4e7965ea2f9b3fe99d6e94/http/src/main/scala/dynamics/http/retry.scala) * [unfoldEvalWithDelay](https://github.com/aappddeevv/dynamics-client/blob/c034fd01ec267b6dcf4e7965ea2f9b3fe99d6e94/common/src/main/scala/dynamics/common/streams.scala) which is kind-of like a monix observable. The key to slicing your token in to be a "middleware" in the HTTP call and add the bearer token to the header and then pass the immutable copy down the chain to something that actually makes the call. monix should also be straight forward. 
https://www.scala-js.org/doc/sjs-for-js/ learn scala from javascript.
&gt; &gt; we still want OrdInt and RevInt to be transparent synonyms of Int most of the time &gt; No, we really don't want that ;-) Why not? Isn't this the reason people in Haskell resort to hackish mechanisms like _coerce_ (that are a little frowned upon AFAICT). &gt; Not sure if you're aware, but that makes no sense. I am not. Care to elaborate? &gt; nobody actually cares about Int or its ordering So you never sort a list of integers? You never use an ordered tree? &gt; Again I invite you to use things we actually care about. We don't care about Group or Semigroup. We care about Monad and MonadError. For someone who claim to promote open-mindedness, you make remarks that feel rather arrogant and dismissive. It's a bit ironic when you portray the very behavior that you seek to chastise. I know you are neck-deep into these use cases, but not everyone cares about the same use cases. Personally, I use monoids and semigroups all the time, because they're the correct abstraction when handling queries with grouping and aggregations, especially in connection with distributed computing. For example, it's a central abstraction behind Twitter's algebird. &gt; I find it funny how the community gets creative all of a sudden with dozens of proposals, just to prove the author wrong. The stated goal of my proposal was to "explore the design space." It was never to "prove" anyone wrong. Can we discuss things like adult, without resorting to petty accusations?
I've looked again at your benchmark and figured out that the main difference on setups is that you are doing encoder/decoder loookups by using that api, while preloading them for scala ones. You can do something similar via lower level APIs: val decoder = dslJson.tryFindReader(classOf[AnyRefs]) val encoder = dslJson.tryFindWriter(classOf[AnyRefs]).asInstanceOf[JsonWriter.WriteObject[AnyRefs]] and then writer.reset() encoder.write(writer, a) decoder.read(reader) This is as fast as you can get with DSL-JSON
dslJson.deserialize() call do lot of work, and I'm not sure that all its implementation can be easy replaced by two calls, proposed by you above: val decoder = dslJson.tryFindReader(classOf[AnyRefs]) decoder.read(reader) It is not clear for me where I should get reader instance... Should I extend DslJson for that? Please fork our benchmarks and send pull request with your best solution.
Multiple receive functions is covered in chapter 2 or 3 of Akka in Action. They want you to be doing that.
&gt; I'm not sure if the OO bit is such a benefit for the language. I know the feeling. One of my current coworkers actually knows Scala reasonably well, but I frequently see him hiding behind both: * "This isn't Java, so the OOP Java clean-code practices don't apply." * "The people promote stricter FP practice in Scala are anal and OCD." I can't tell if he does the above deliberately or not, but the only way anyone would really be able to tell him, "No, that's a bad pattern in both OOP and FP." would be if they had about as much or more Scala as my coworker does. I mean, I'm no Scala expert myself, but if I've heard "don't return unit" over and over from Scala experts I trust. While this coworker may know more about Scala than me, I absolutely can recognize that people far more knowledgeable than him treat it as a code-smell.
It's a bit concerning that there is no mention of JVM value types. Pretty much all the reasons why Scala value types are "bad" are obsolete with Valhalla. (That Scala value types ended up this way was more or less an explicit decision to a) avoid "compiler fiction", and b) bet on Java introducing value types so that the "temporary" restrictions of Scala value types could be dropped.)
Yes, but the valhalla project seems to be quite far off from a stable release, they are still discussing how to change the JVM bytecodes on the mailing list.
Even if that took them 2 years and opaque types shipped right now, I don't think it makes sense to introduce a new features. It would be completely obsolete before most people would even be able to use it.
No, it implements **static** factory method, as named by Josh Bloch in his "Effective Java" book. In "regular" factory pattern you can swap different factories, that implement the same interface. But in static factory you just have a "helper method" for creating an object... :D The benefit there is that you aren't bound to a constructor, you could swap the return object for whatever implementing class.
That was my concern too. But luckily, Scala is taking steps to reduce GC overhead, like SIP-15: http://docs.scala-lang.org/sips/completed/value-classes.html I found many of this proposals easier to understand than they appear, so I think it's important to take them into account. After all, my code is a **representation** of what I actually want to do. I don't care if the generated Java bytecode still adheres to functional principles if it is faster, which always have to be.
I don't know if the speaker mentioned it, but [someone found a way to emulate opaque types in today's Scala with abstract types](https://failex.blogspot.com/2017/04/the-high-cost-of-anyval-subclasses.html).
Use the try Monad to on the smallest parts of the code and use Options, Eithers and maps for the rest.
I would read the first three chapters of Akka which is a unique to Scala, distributed systems framework that’s easy to use. Maybe make a website (using Akka-http) that can read a file off of data.gov.uk such as staff salaries csv and generate some reports from it.
Yah, I think trying to implement everything in onTransition would be really hard. Using when(state) { partial function} has worked really well for us.
Thanks a lot.
I'll try to do that later. But it should be something along this lines val reader: JsonReader = DslPlatformJson.dslJson.newReader() just bellow pealocated writer and then reader.process(bytes).read() decoder.read(reader)
Definitely much better. I'd still increase contrast a bit.
Maybe open source projects wouldn't be able to use it, but I would be able to use it at work instantly. In the age of continuous deployment where a service might get deployed to prod multiple times a day, a feature that's 2 years out is tantamount to vaporware.
`Option.apply` can be used as a factory method - there isn't an existing factory interface that it implements, but thanks to SAM conversion it would implement one if you wrote one, or you could just use the existing `Function1` interface. But really the notion of factory method as a "pattern" doesn't make any sense in a language with first-class functions - why use an object with a factory method when you can just use a factory function?
Opaque types are completely orthogonal to value types in Valhalla. `AnyVal`s are not orthogonal to Valhalla, and could be adapted in the future to match Valhalla semantics, hopefully. Opaque types are useful regardless of the existence of Valhalla for their alternative semantics. In particular, I am looking forward to be able to use them in Scala.js to replace the `sealed trait Foo extends js.Any` without any implementing class idiom. That's really a hack that should always have been opaque type aliases, except we don't have them. See an example here: https://github.com/scala-js/scala-js/blob/v1.0.0-M3/library/src/main/scala/scala/scalajs/js/UnicodeNormalizationForm.scala
&gt; I've heard "don't return unit" over and over from Scala experts I trust I'd say this is a good rule of thumb but unless you're doing completely pure FP style, there will be tons of methods that return `Unit`, even in the scala and dotty compilers themselves.
I agree with u/yang_bo that a couple extra characters to remove all ambiguity and achieve complete self-documentation seems like a good trade.
Assuming you already tried VisualVM to get a basic profile maybe a more detailed profiler like YourKit will help. One thing to look into is thread deadlocks. We have sometimes noticed high cpu when the server is relatively idle and it was caused by incorrect configuration of Slick. The latest version of Slick can warn against this. 
If you can reproduce the condition, JProfiler. If you want to instrument your servers to catch things happening in production, look at Takipi (and maybe also New Relic).
On a first-pass at “cpu utilization is high and we haven’t changed code”, how the GC is behaving is something I’d want to look at. But maybe you already have.
Yes it does, for many reasons 1. If anything, Scala needs to abstract away from JVM semantics rather than rely on them. Relying on JVM semantics when not necessary has caused a lot of problems 2. Opaque times are more concret with Scala's base design. Just like you use `type` in Scala right now, `opaque type` have no runtime representation. This is a very fundamental design feature of Scala, and hence `opaque` is just an extension of type, i.e. its a standard Scala type alias which also provides a zero cost constructor. Unfortunately `AnyVal` (or value classes on JVM if they ever happen) are kind of this mixed up, wishy washy class trying to be a class when its not a class. The point of AnyVal is that its not meant to box, except that classes (both on JVM but in other languages as well) imply boxing by definition, since this is also how vtables work and get carried around. So what we are stuck with on `AnyVal` is we have something that is trying to be a class, but is not really a class which is why we have these utterly confusing performance characteristics. Sometimes `AnyVal` does box, sometimes it doesn't. Opaque types are a clearly superior solution to this problem, simply because they have no runtime representation so you can always guarantee they won't box (at least for the domain of opaque types themselves). This is also extremely important, especially for projects like Scala Native and Scala.js. If anything this is a fundamental issue of the JVM, that tries to treat everything as a class when in reality this isn't the case (same problem with the method vs function distinction)
This, always start with GC GC going nuts means you have bad GC settings or are churning way too much memory. Once you rule out GC, then you can start to dig into what the code really is doing. 
We looked at some GC stats, but don't necessarily know all of the lingo. For sure, there was a lot of GC activity during that period, and memory utilization was all over the place. But it's unclear what was actually causing this, as the issue persisted through periods of almost zero activity and through restarts.
If you get a Lightbend subscription, you can use Cinnamon, their telemetry project. Instruments Akka and Akka HTTP automatically.
&gt; completely orthogonal Maybe we have deviating definition of "orthogonal", but my interpretation of orthogonal means "unrelated/undisturbed/having no impact". I think that a proposal - that was named "unboxed wrapper classes" first - tries to work-around the limitations of Scala value types - aims to replace value types can hardly be claimed to be orthogonal.
The only benefit opaque types seem to have in that regard is that they have poor performance on all targets. The point is that value types is a pretty universal well-understood concept. It's finally coming to the JVM, and "native" code had it from the beginning. Scala-Native already has `@struct` to emulate it.
&gt; a feature that's 2 years out is tantamount to vaporware So pretty much every Scala release? :-) The point is that language additions are extremely expensive and long-lived. Scala held out for 8 years, I'm not seeing the deal-breaker in waiting another year or two.
You should see GCs - but they should be very fast and they should mostly not be full GC (the app I work on full GCs about once an hour or so in production, if that) Can you tell from the metrics you have how long the GCs were taking and whether or not they were full GC? If it is a GC thing, you’ll want to get a heap dump while it’s having the problem, load it up in an analyzer, and look for what’s sucking up the memory.
Not strictly Scala-related, but the bulk of this is talking about how Uber handles batch and streaming huge amounts of data on Spark and Flink.
&gt; The only benefit opaque types seem to have in that regard is that they have poor performance on all targets. Huh? An opaque type has the same performance as the underlying type - literally the best possible - in an obvious, demonstrable, will-never-fail way. &gt; The point is that value types is a pretty universal well-understood concept. Disagree: there's no consensus at all on what circumstances a "value type" may box under. `AnyVal` classes' current semantics cannot be implemented on many targets without boxing under some circumstances (and even with JVM value types they will necessarily not always have exactly the same performance as the underlying type) and changing their semantics would represent an unacceptable backward compatibility break. Opaque types have very simple, clear semantics (i.e. they never box).
&gt; If anything, Scala needs to abstract away from JVM semantics rather than rely on them. Relying on JVM semantics when not necessary has caused a lot of problems. Scala will have to deal with JVM semantics either way. (Of course they could just ignore it until it's too late, but that didn't work out too well with Java's default methods, right?) I don't think Oracle will say "hey OpenJDK devs, did you see what Scala did with opaque types? Let's scrap our value types and do nothing instead! Problem solved!". I'd really like to see how JVM value types will be represented in Scala once Scala value types are gone. Or is this yet another thing where Scala just throws up its hands, like with annotations and enums, and tells people to write platform-dependent code thrice instead? &gt; Opaque times are more concret with Scala's base design. Compare this to: &gt; Scala is a language set out to eliminate boilerplate and promote the simplest possible style of expression. I'm not seeing how package entity object `package` { opaque type Person = (String, String, Int) object Person { def apply(firstName: String, lastName: String, age: Int): Person = (firstName, lastName, age) implicit class PersonOps(val `this`: Person) extends AnyVal { def firstName = `this`._1 def lastName = `this`._2 def age = `this`._3 } } } is "eliminating boilerplate and promotes the simplest possible style of expression", compared to: package entity class Person(firstName: String, lastName: String, age: Int) extends AnyVal &gt; Unfortunately AnyVal [...] You are beating down a strawman in the rest of your comment. Pretty much everything you point out will be addressed with JVM value types. Let's not pretend that Scala value types where some perfect implementation – everyone involved knew that it was a carefully restricted design that hedged its bets on JVM value types to get rid of the limitations. A more erased design like opaque types was considered and discarded at that time, because everyone was aware of the pain inflicted by introducing more leaky compiler fiction. People had just spent years on getting rid of compiler fiction. Looks like that lesson has been forgotten now. &gt; Opaque types are a clearly superior solution to this problem, simply because they have no runtime representation so you can always guarantee they won't box So do JVM value types, except when being forced to in very well-understood circumstances. And that worst-case scenario is basically opaque types' base performance. In the above example, value types eliminate one layer of reference chasing and one level of boxing. And this example is meant to make opaque types look good. Consider class Complex(real: Float, imag: Float) extends AnyVal that's 64 bits being directly passed around with value types, and - 64 bits: Tuple2 reference - 64 bits: Tuple2 object header + class pointer (let's assume Compressed pointers, otherwise it is even worse) - 64 bits: Tuple2 object fields - 64 bits: Tuple2 specialized float fields with opaque types. So the representation of a Complex instance is 8 bytes with value types, and **32 bytes** with opaque types, a four-fold overhead. 
For me, it is much easier to bump the Scala version than to update the JVM. I interface Matlab which was still on 1.6 a year or two ago. Still, 2.13 introduces a lot of changes, so we may have to wait before big ecosystems migrate to it. But nothing prevents backporting opaque types in e.g. the Typelevel compiler variant or as a compiler plugin.
AnyVal value classes are so confusing, when they do not lead to bona fide compiler errors due to bridge methods. My only concern is that opaque types require AnyVal classes for syntax enrichment, so that will lead to overlapping concepts being present in future Scala.
&gt; The opaque type proposal is really small. Did you _read_ it? It has language changes, additional implicit conversion rules, introduces a new keyword, new typing rules and syntax additions. &gt; they can be members of a class, whereas AnyVal types (and probably Valhalla value types) cannot Why not? Reading the draft spec, I can't find a reason why this wouldn't be the case. &gt; I bet their inclusion in the Scala ecosystem will take years. Well, if you are fine that Scala will stop working with Java 11/12(?). Because I don't think Oracle will decide not to ship Java because Scala isn't ready. Or that library authors won't be using value types. The point is that this is work that needs to be done either way. Going the introduce-opaque-types-and-removing-value-types will just make it way harder, because how will Java value types be represented in Scala?
Except that your Person example is not part of any of the SIP use cases. Note that opaque types play well *today* with all the Scala features. However we have no idea of the future Valhalla types performance characteristics, especially in generic code. Will we revive the pains of @specialized ? Note that all the Scala native performance niceties are lost when writing generic code, for example a Complex[A] that abstracts over the real field (Float, Double). I expect the same for Valhalla.
&gt; So pretty much every Scala release? :-) Yes, unfortunately. Every time I see a presentation about Scala 2.14 (2018? 2019?), 2.15 (2020?), and dotty/3.0 (???), I feel a bit ambivalent about it because it's all arriving at a time scale that won't apply to my current team, and I have no idea whether the JVM will be relevant to wherever I'm working in N years or whether that future team will be as comfortable with not-java (or not-kotlin). AnyVal is better than nothing, but it comes at a cost: all the little corner cases where it might or might not do what you expect. That cost over 8 years adds to the perception that Scala is difficult and makes it harder to get buy in from teammates to use it. Cleaning that up sooner rather than later would be great.
I read the proposal, watched Erik's talk, and stand by what I think. I see a single keyword "opaque", creation of straightforward implicit conversions (that avoids compile magic), and a change in visibility of type properties. Now, regarding future Scala being broken. Java 11/12 will anyway introduce compatibility bridges for value types at the price of boxing. There is no way they will break legacy JVM libraries, so your point is moot. Of course performance is not going to be optimal before Scala takes its sweet time to implement them. Opaque types are not intended to replace AnyVal, btw they use them for syntax enrichment.
At this time, there is no point of arguing. A complex opaque type is likely to be a pair of doubles internally, which 1) can be implemented today and 2) can have its underlying pair represented by a Valhalla value pair type when the implementation lands. I don't see why you oppose Valhalla value types and opaque types, at all!
I thought opaque types were mostly geared toward primitives, like the `opaque type Logarithm = Double` example in https://docs.scala-lang.org/sips/opaque-types.html Likewise you might use opaque types for ProductID, UserID, etc. For a Person type, it seems simpler to define a trait and an implementation case class (or just a case class).
&gt; Except that your Person example is not part of any of the SIP use cases. But that was pretty much the intended use-case of value types from the beginning. If opaque types are replacing them, they should demonstrate that they actually work _in general_, not only on cherry-picked examples. &gt; Note that opaque types play well today with all the Scala features. That's why the proposal contains language changes, additional implicit conversion rules, introduces a new keyword, new typing rules and syntax additions? &gt; However we have no idea of the future Valhalla types performance characteristics, especially in generic code. I think there is pretty good idea about those characteristics. That's why they are doing specialized generics. If you are unsure, have a look at the CLR. Not everything is perfect there, but it should give you a good general idea. &gt; Will we revive the pains of @specialized? @specialized should have never shipped. It will finally be unnecessary with Valhalla. &gt; Note that all the Scala native performance niceties are lost when writing generic code, for example a Complex[A] that abstracts over the real field (Float, Double). I expect the same for Valhalla. ??? Why?
Symptoms do sound like memory saturation causing very frequent and expensive GC. If you can't reproduce and profile you're left with guesswork. What could have caused the app to suddenly use more memory? It was the financial year end for many organisations in the last few days. Anything in the app behave differently around that?
Because generics are implemented through erasure on the JVM and in Scala as a consequence. As a bonus, we get easy higher kinded types. The CLR has a different model for generics, where types are not erased but code is specialized on the fly. I have no idea on how the JVM is going to manage generics+Valhalla value types. Scala native avoids the problem by boxing. I'd welcome any up to date discussion of Valhalla+generics. I don't find much on Google, which is not optimistic for a feature that supposedly lands in 2 years.
I don't oppose Valhalla value types. They are the right way forward.
&gt; Maybe we have deviating definition of "orthogonal", but my interpretation of orthogonal means "unrelated/undisturbed/having no impact". That is also my interpretation. From the point of view the language semantics, they are unrelated, do not disturb each other, nor have they impact on each other. Now, from a *social*/*ecosystem* point of view, some use cases for which opaque types should have been used from the beginning, but where their absence forced people to use value classes instead, will be impacted, sure. People might migrate away from value classes to the opaque types that would have fit their use case better from the start. In that sense, opaque type aliases will "steal" use cases from `AnyVal`s. My take is that it's more like reclaiming than stealing.
Because value types already exist in Scala, and will do everything people want from "opaque types" without breaking people's code or introducing gargantuan language changes/additions.
&gt; I thought opaque types were mostly geared toward primitives No, that's not true. In fact, if we are purely talking about performance, opaque type aliases do not buy much wrt. `AnyVal`s when it comes to primitive types (except in `Array`s). But regardless, they're equally useful for reference types than for primitive types, as far as their *semantics* are concerned. (The text of the proposal currently puts way too much weight on this example and on performance for my taste, which is probably why you have been tricked into thinking this. I have already requested that this be rebalanced.)
No they do not. And Valhalla types are years in the future. I would bet that they do not land in Scala before at least 5 years. And the opaque type proposal is small, no need to get dramatic. BTW, we discussed with Erik at nescala about the nice work you did in Scala, including your BigInteger implementation. You should touch base with him.
&gt; the opaque type proposal is small Again, please read the draft. This will take _at least_ a full major version to iron out all the bugs. By the time it actually works and people can use it, Valhalla will be shipping.
Note that play-json *already* has the syntax you write here, partly because I contributed it a few months back: @ interp.load.ivy("com.typesafe.play" %% "play-json" % "2.6.7") @ import play.api.libs.json._ @ val json: JsValue = Json.parse(""" { "name" : "Watership Down", "location" : { "lat" : 51.235685, "long" : -1.309197 }, "residents" : [ { "name" : "Fiver", "age" : 4, "role" : null }, { "name" : "Bigwig", "age" : 6, "role" : "Owsla" } ] } """) @ val name = json("residents")(1)("name").as[String] name: String = "Bigwig" This is great for looking things up in your JSON, and great for REPLs or scripts (which is the reason I contributed it). What is less great is updating/modifying/transforming JSON values. Play-Json's syntax is kind of awkward: json.transform( __.json.pickBranch( (__ \ 'location \ 'lat).json.update( Reads.of[JsNumber].map{ case JsNumber(nb) =&gt; JsNumber(nb + 10) } ) andThen (__ \ 'residents).json.update( Reads.of[JsArray].map{ case JsArray(arr) =&gt; JsArray(arr :+ JsString("delta")) } ) ) ) As are the syntaxes for the other libraries I referenced. Compare with uJson: @ json2("location")("lat") = json2("location")("lat").num + 10 @ json2("residents").arr.append("delta") 
Try Netflix FlameScope with perf-map-agent for JVM: https://github.com/Netflix/flamescope ...and you will able check any millisecond of what is doing your server: https://www.youtube.com/watch?v=cFuI8SAAvJg
Wow I really should be reading that book. Thanks!
&gt; A Complex(Double, Double) value will have a memory footprint of 40 bytes with opaque types. It will have 16 bytes of footprint with value types. You're comparing an imaginary perfect future implementation of value types with the present-day implementation of opaque types. A value type today takes 48 bytes (as soon as you do just about anything with it, e.g. putting it in a list). An opaque type in a future where `Tuple2` is implemented as a JVM value type (and it's crazy to imagine a future where value types are implemented for custom types but not for `Tuple2`) will take 16 bytes. &gt; Everyone who has actually implemented them has a very good understanding of it. The cases were boxing would be necessary is virtually identical when comparing the implementation in the CLR and the design on the JVM for instance. And even that boxing is something that can be rapidly eliminated at runtime, with optimizations Hotspot had for years. Most developers are not VM implementors. Ordinary developers need to be able to understand the performance characteristics if any of this is to be worthwhile. &gt; You are beating a strawman here. Well, you're claiming value types "will do everything people want from "opaque types"". What I want is: performance guaranteed to always be the same as the underlying type, on any platform. Or some equally simple performance characteristics. Not a long list of caveats that I have to know about the VM implementation to understand. &gt; Which semantics would change? If you made `AnyVal`s *never* box that would change their behaviour with regard to e.g. `getClass`, pattern-matching from an `Any`. &gt; And even if semantics would change, how is removing value types an "acceptable backward compatibility break" while dropping the restrictions on them isn't? 1. I never said remove `AnyVal`s. They have different semantics ("behave like a real class at runtime, have reduced memory consumption on a best-effort basis" rather than "consume exactly the same memory at runtime, provide class-like semantics on a best-effort basis") and presumably there are some people who want them. 1. Removing `AnyVal` would be a compile-time error which is an acceptable compatibility break in a major version. A silent change in pattern-match behaviour would be much worse. &gt; Opaque types are always boxed by default. Opaque types are never boxed. Tuples are boxed in the current implementation, but that's a separate issue. (You're thinking about the multi-parameter case but that's almost irrelevant, the single-parameter case is much more common and important; hopefully we can at least agree that single-parameter opaque types never ever box in any sense). In any case implementing tuples as JVM value types resolves your concern with the multi-parameter case, can be done without a language change (since tuples are part of the language itself), and is something you'd want to do anyway.
Thank you, Erik! A really good and entertaining talk. So is it fair to say that this is akin to the newtype in the languages that provde that?
&gt; You're comparing an imaginary perfect future implementation of value types with the present-day implementation of opaque types. Just like opaque types are compared to a present-day implementation of opaque types. &gt; Most developers are not VM implementors. Ordinary developers need to be able to understand the performance characteristics if any of this is to be worthwhile. I'd rather have them "fast, practically everywhere" than "slow, guaranteed everywhere". Developers will need to understand them anyway, as the Java ecosystem adopts them. &gt; If you made AnyVals never box that would change their behaviour with regard to e.g. getClass, pattern-matching from an Any. Boxing is completely transparent with Java value types and will in fact work much better than it does right now. I cannot imagine people rely on some behavior that was explicitly considered to be a temporary scheme, but I would be all for implementing a compiler warning that makes people aware. That should resolve all concerns you have. &gt; I never said remove AnyVals. But that's the plan, right? &gt; Removing AnyVal would be a compile-time error which is an acceptable compatibility break in a major version. Oh, ok. That will literally do wonders for future adoption, I guess. &gt; A silent change in pattern-match behaviour would be much worse. Do you have an example? &gt; hopefully we can at least agree that single-parameter opaque types never ever box in any sense What about `opaque type Meter = Double`. I'm pretty sure that double will get boxed in a lot of places. &gt; You're thinking about the multi-parameter case but that's almost irrelevant, the single-parameter case is much more common and important That's like arguing the recent fix for the broken implicit resolution rules is not necessary, because no one uses contravariant typeclasses – of course contravariant typeclasses aren't used ... because instances are not resolved correctly! Of course they are "more common". Value classes with multiple fields literally don't compile! But if you look at some codebases, I think 60-80% of the current reference types could (and should) be migrated to value types after they were supported. &gt; In any case implementing tuples as JVM value types So now that we are back to JVM value types, why not just support them and be done? Because now developers need to understand the "long list of caveats that I have to know about the VM implementation to understand" anyway.
What is Valhalla? The County???
You think `5.plus(6)` is nicer than `plus(5, 6)`?
Twilio uses scala?
Ah, true, I only thought of my own platform, when mentioning *one* use case which I said *I* was personally looking forward to! I'm so sorry! It's not like value classes on Valhalla at all, which are so much more ubiquitous and apply to so many more platforms!
Yah, they're listed as big users of Circe as well I believe.
Scala in Android just got updated to make it work with the latest version of SBT 0.13. Another update to make it run on SBT 1.0 is currently sitting in the PR queue. There is still some activity, but as others have mentioned, the sub-par treatment of the project sunk the chance of Scala on Android getting some mainstream adoption. So the paths you'll take will be less well-trodden than with other languages.
I'd probably look into GC to start with. If you enable the java opts to log GC you should be able to get an idea. Taking some thread dumps can be a good idea too. Generally kill -3 on the java process will produce a thread dump with stack traces on all your threads. It will also print a heap usage summary at the end of the dump as well. If you take several thread dumps spaced 10-30 seconds apart you can then review and see if them and see if you spot threads executing in similar parts of your code. Essentially the flame graphs are gathering the same info and streamlining the analysis. Taking a few thread dumps manually can sometimes get your the answer without having to set up any additional tooling though.
I seriously don't understand the point of Akka FSM. State machine can be modeled much simplier and better using only `become`. You can just model every state as a method that returns `Receive`. Method itself represents "state", its parameters serve as "data" and the `Receive` returned by it as a transition function for that state. Changing state is simply done by calling `become`.
&gt; Just like opaque types are compared to a present-day implementation of Scala value types. Do you have any serious doubts that the implementation of opaque types can be delivered with the claimed properties? To me it seems obvious that opaque types can be implemented with the given behaviour (hell, I'm confident that *I* could implement them if it came down to it), whereas JVM value types seem to be promising the moon and have been delayed multiple times already. &gt; I cannot imagine people rely on some behavior that was explicitly considered to be a temporary scheme – any stability or compatibility under reflection was never guaranteed by Scala in the first place. Oh come off it. Of course people rely on the behaviour that's implemented. &gt; I would be all for implementing a compiler warning that makes people aware if they hit some obscure corner case. That should resolve all concerns you have. What would you warn on? Every assignment of an `AnyVal` value to a trait it implements? Every pattern match against a non-sealed type? Type parameters in pattern matches that are unchecked due to erasure are already a warning but still a major source of confusion and questions. &gt; But that's the plan, right? There are many features I would remove from Scala if I were making it just for myself. If there's enough demand for types with the semantics of present `AnyVal` types then present `AnyVal` types should exist. If not, not. &gt; Oh, ok. That will literally do wonders for future adoption, I guess. What's your point? A compile time failure with an easy fix that's backwards compatible with the previous version is absolutely fine. Having to put parameter lists on case classes when going 2.9-&gt;2.10 was such a non-problem that hardly anyone even remembers it was a change you had to make. &gt; Do you have an example? sealed trait Id extends Any case class UserId(value: UUID) extends AnyVal with Id case class GroupId(value: UUID) extends AnyVal with Id val id: Id = ... id match { case u: UserId =&gt; something(u) case g: GroupId =&gt; somethingElse(g) } &gt; What about opaque type Meter = Double. I'm pretty sure that double will get boxed in a lot of places. Not that primitives are a relevant case either, but anywhere you use `Meter` will have exactly the same runtime behaviour as if you'd used `Double`, that's the whole point. If there's a case where it won't then that's a serious problem with the proposal, please do give details. &gt; But if you look at some codebases, I think 60-80% of the current reference types could (and should) be migrated to value types after they were supported, because they never needed reference semantics to start with. &gt; So now that we are back to JVM value types, why not just support them and be done? If value types really improve everything with no downsides the way you describe, why not just compile all `case class`es (or any that don't override `equals`/`hashCode`) to value types? No-one is against the JVM backend making whatever use of value types it can. Opaque types would give me a simple, useful feature that resolves a real pain point I have: they let me write a type that wraps another type for safety with guaranteed 100% identical runtime behaviour to if I'd just used the underlying type, no ifs, no buts. That's something I want, and something I'll still want whatever clever optimizations get implemented by the JVM backend.
Not completely sold on the idea yet. Can anybody tell how it compares with [newtypes](https://github.com/alexknvl/newtypes) that supports opaque types in today's Scala?
It's closer to the translucent variant, but without the visible bound `&lt;: Int` in code external to the companion object.
Numbers I got after fixing the test setup: [info] AnyRefsBenchmark.readDslJsonJava thrpt 5 6034401,846 - 68949,206 ops/s [info] AnyRefsBenchmark.readJsoniterScala thrpt 5 5890885,123 - 95926,133 ops/s [info] AnyRefsBenchmark.writeDslJsonJava thrpt 5 10073666,500 - 434053,846 ops/s [info] AnyRefsBenchmark.writeDslJsonJavaPrealloc thrpt 5 10435042,415 - 2304250,779 ops/s [info] AnyRefsBenchmark.writeJsoniterScala thrpt 5 14192939,072 - 385575,970 ops/s [info] AnyRefsBenchmark.writeJsoniterScalaPrealloc thrpt 5 17441103,176 - 512921,134 ops/s 
Thanks! Great input: https://github.com/plokhotnyuk/jsoniter-scala/pull/100 I hope that all these improvements will be available in your library for Java users. Also, please, contribute your bests for Scala users. BTW Here are results from my notebook (Intel® Core™ i7-7700HQ CPU @ 2.8GHz (max 3.8GHz), RAM 16Gb DDR4-2400, Ubuntu 16.04, Linux notebook 4.13.0-32-generic, Oracle JDK 64-bit builds 1.8.0_161-b12): Benchmark Mode Cnt Score Error Units AnyRefsBenchmark.readCirce thrpt 5 1656302.172 ± 101073.908 ops/s AnyRefsBenchmark.readDslJsonJava thrpt 5 9603513.592 ± 1478031.812 ops/s AnyRefsBenchmark.readJacksonScala thrpt 5 2494703.128 ± 148352.637 ops/s AnyRefsBenchmark.readJsoniterScala thrpt 5 9842344.345 ± 391884.902 ops/s AnyRefsBenchmark.readPlayJson thrpt 5 621175.896 ± 50391.858 ops/s AnyRefsBenchmark.writeCirce thrpt 5 2016789.498 ± 201426.559 ops/s AnyRefsBenchmark.writeDslJsonJava thrpt 5 18950616.383 ± 2044511.333 ops/s AnyRefsBenchmark.writeDslJsonJavaPrealloc thrpt 5 20280086.333 ± 143141.722 ops/s AnyRefsBenchmark.writeJacksonScala thrpt 5 6097592.491 ± 203816.741 ops/s AnyRefsBenchmark.writeJsoniterScala thrpt 5 27384240.926 ± 657494.558 ops/s AnyRefsBenchmark.writeJsoniterScalaPrealloc thrpt 5 27825415.764 ± 444248.689 ops/s AnyRefsBenchmark.writePlayJson thrpt 5 1078746.173 ± 27447.825 ops/s 
Or Kamon.io, which is free! We're still using version 0.6.x though, because we need the StatsD plugin for Grafana. This way, you can get the same information of Cinnamon for free, with a bit of additional hassle
I believe I get what you're getting at but there's some other interesting uses for opaque types that would be more cumbersome to represent with value classes—though I'd prefer to have both. Being able to differentiate between for instance a FooId and a BarId—where both are backed by a Long—without having to tuck them in under a member field like `value` is great. Same with being able to easily represent things like NonEmptyString.
I have some scala code compiled in a jar and am trying to pass in a properties file via command line, but my scala code is unable to read it, it always gets null. cmd line: java -jar sweep-util.jar -p\sweep-util\conf\sweep.properties scala code: logger.info("ENV GET 1: " + System.getProperty("number.of.instances")) //logs null logger.info("ENV GET 2: " + System.getenv("number.of.instances")) //logs null logger.info("ENV GET 3: " + scala.util.Properties.envOrElse("number.of.instances", "cats" )) // logs cats None of these work! I am open to going about it another way - I just need to be able to access an external properties file. I also wanted to do it this way so that I didn't have to hardcode the name of the properties file in the scala code. Thank you in advance! 
Use the Properties class: https://www.mkyong.com/java/java-properties-file-examples/
Maybe just use TypeSafe Config .. it's pretty much the standard: https://github.com/lightbend/config
I don't recall ever seeing -p documented as an argument that java supports. System.getProperty reads properties set on command line with the -Dname=value convention. System.getenv will read a shell variable. You could use the mkyong example /u/vytah linked, but sometimes it can be a little complicated referencing the file you want from within a java process, especially if you are using relative paths or loading from the classpath. The TypeSafe Config library that /u/threeseed mentioned makes that a lot easier and has good options for default configs, overriding, and composing configs built in out of the box. It also handles reading into numbers, boolean, and list types for you instead of you having to read a string and manually parse.
I would highly recommend against VisualVM. VisualVM counts I/O blocked threads as if they are executing at 100% CPU. I've found that in practice this makes it all but useless for any profiling of a server app. I've found JProfiler to work fairly well, and I've heard others speak well for YourKit, but you need to at least use a profiler that is smart enough to not treat I/O-blocked threads as burning through CPU.
That's interesting, I've never heard of that problem. Will definitely watch out for it.
For whatever reason it's hard to find this limitation documented online, but here is a forum discussion I found: https://yourkit.com/forum/viewtopic.php?f=3&amp;t=3823 "These particular I/O methods are native. They depend on low-level platform synchronization mechanisms (on platform I/O functions), they don't use Java synchronization inside their implementation. These methods are runnable from the VM point of view, and are reported by VM as runnable." In short, whenever a thread is in native code, it is considered runnable even if it is blocked on I/O. It turns out that all java I/O operations are implemented at some point by native code, not jvm intrinsics, so profilers that rely on thread state will incorrectly see I/O operations as consuming cpu. I know little about yourkit, but jprofiler works around this by having a built-in list of methods that it excludes from counting as using CPU, and they include all the I/O operations in the standard library. Furthermore, you can extend this list yourself. For instance, when I was using jprofiler it did not have any filters for netty 4 operations, and I had to manually add them when profiling an application using netty.
Or pureconfig, which has a nicer interface around the typesafe one.
I have the same problem so bump. By the way, do you use the "space-instead-of-dot" syntactic sugar in IntelliJ? I find it very easy to read (and aesthetic) but IntelliJ will only autosuggest when a dot is used. Have you figured something out for this?
You could try marking your test directories as src directory in the create assembly task. Maybe you're better off creating a subproject specifically for the tests which depends on the main project so this sub project is treated as regular code by default
Signify Technology | Scala Developers | Across the World | Onsite | Freelance/Contract or Permanent/Full-time We are a Scala specialist where we do a lot within the community. We are working on a number of roles around the world for both contract and permanent. Have a look on our website at our roles - www.signifytechnology.com/job-search and if there isn't something for you on there, still get in touch as we might have something else. You can contact me on alexandra.staniland@signifytechnology.com or +44 (0) 203 865 0621 All the best! :) 
Using postfix is discouraged anyway :)
I gave up and let scalafmt win over scalastyle or intellij's formatter. I enabled auto-scalafmt on save in intellij. Here's my scalafmt config: align = none rewrite.rules = [AvoidInfix, RedundantBraces, RedundantParens] maxColumn = 120 runner.optimizer.forceConfigStyleMinArgCount = 3 
No, but I think `5 + 6` is nicer than either of those. A `plus()` method is a cherry-picked example of something that doesn't work particularly well, but I agree that in general having methods tied to an object is much more natural when writing code.
Looks way overcomplicated for no reason imo. When programming in Scala, best to just forget most of Java's design patterns.
I agree it's cherry-picked, but there are classes of functions that read better this way. I think it's best to just make `a.func(b, c, ...)` syntactic sugar for `func(a, b, c, ...)` so the programmer can choose what's best.
This method def build(isMaster: Boolean): Box[T] = { if (isMaster) buildMaster() else buildSlave() } is trying to shift information from run-time (when the *value* of the `Boolean` is known) to compile-time (when types are known). That isn't going to work. I'm not really clear on what you are trying to achieve. You described the component part, but you just kinda stopped when you got to describing the product part.
You don't really need to do any of this. You can use [path-dependent types](http://danielwestheide.com/blog/2013/02/13/the-neophytes-guide-to-scala-part-13-path-dependent-types.html) to achieve what you're after.
&gt; trying to shift information from run-time (when the value of the Boolean is known) to compile-time (when types are known). That isn't going to work. I'm not sure what I get what you mean here. Could you please elaborate? &gt; You described the component part, but you just kinda stopped when you got to describing the product part. I've edited the question to clarify. Essentially I want to wrap a concrete component in a Master/Slave context. I guess it's similar to Either[A, A] where we can tag the value as Left[A] or Right[A], but also enforce a bound on Left and Right so you can't mix up 
I think it's complicated too. Just an experiment for me to see how far I can push with scala...
hi how to build something transaction with AKKA actors:- for example if amazon user buys product with lightning deal but if by the time he reached at checkout if any product has expired lightening deal he want to role back to stage where he can again add those expired products that means remaining product inventory also should not be blocked.
Thank you. Learned something new and useful with Scala today.
&gt;IntelliJ will only autosuggest when a dot is used Have you tried Ctrl-Space?
Certainly not all the time. 1 + 1 is better than 1.+(1)
[removed]
I'm not sure I understand the root of the problem being solved. There's too many accidental complexities and things wrong with the example to begin addressing the root problem. Why is `MasterSlave` not sealed? Why is Covariant on `T`? why is `Box` Covariant on `T`? Why have both `Component` and `MasterSlave`? How married are you to this nonsensical pattern? Why not just use a type class?
&gt; Have you thought about adding an IntelliJ plugin? I'm sure everyone *thinks* about these things. I think the rate limiter is time/commitment, not lack of thinking.
Thank you. It's great. Helped me bridging some gaps between actors and streaming.
I've used factories in Scala with Finatra. They can use useful for testing with stubs if you have to do something like talk to an external service to create your object. But in general I find that creational patterns like builders and the "fluent" style are not necessary in Scala. Between case classes, apply methods, named parameters, and the copy method, I get along just fine without builders. I don't think I agree with the OP that you can forget about OOP design patterns, but it is true that design patterns are created to fill in missing language features. Scala just has a lot more power to work with in the native language features, so the Java patterns you know and love will begin to feel clunky once you begin to trust Scala's design philosophy.
&gt; I'm not sure I understand the root of the problem being solved. There's too many accidental complexities and things wrong with the example to begin addressing the root problem. I want to be able to call val component: Component = builder.build(false).product &gt; Why is MasterSlave not sealed? Why is Covariant on T? why is Box Covariant on T? MasterSlave can be sealed, not an issue Covariant because it makes sense to have a Master[Cow] in place of a Master[Animal], MasterSlave[Animal] &gt; Why have both Component and MasterSlave? Component is the concrete type put into a MasterSlave context, totally decoupled from each other. &gt; How married are you to this nonsensical pattern? Why not just use a type class? How does one use typeclass for this? 
Thanks, I was able to proceed with typesafe, though -D line arguments aren't working either. There must be something I am not importing or setting up correctly, my project is being built with sbt. Sending in arguments and catching them with args(0), args(1), etc does work, but its not very clean looking.
Thank you, this explanation was very helpful. I was able to use this and pass in the config file name via command line argument.
Thank you for your config anyways!
Actually I use this kind of notation very selectively, so not having autosuggest at this point was not a problem so far.
ah nice, I could need this! Will test it out soon :) I guess 1 simple implicit should be enough to let it work nicely together with better-files?
I don't think there are any substantial savings when using opaque types over value classes. Especially not if you need to define even a single method on that type. I expect that opaque types will be way more cumbersome than value types in 95% of all cases.
&gt; Do you have any serious doubts that the implementation of opaque types can be delivered with the claimed properties? No, not really. Plenty of other runtimes have shipped value types before, and many of them supported them form the get-go. It's not some exotic, experimental concept, it's a very basic, well-understood technique. The only thing that will be hard is to reconcile `equals`/`==` into something coherent, because let's face it – equality in Java is a complete mess. For Scala it would just mean moving `eq` from `AnyVal` to `Any`. &gt; To me it seems obvious that opaque types can be implemented with the given behaviour Sure, but consider why the given behavior can be implemented – because the whole design of opaque types is basically a workaround of current JVM limitations. I think nobody would ever _want_ this design in the first place, if it didn't promise less boxing complexity than the current value types. &gt; Of course people rely on the behaviour that's implemented. &gt; ... &gt; What would you warn on? Every assignment of an AnyVal value to a trait it implements? Every pattern match? Type parameters in pattern matches that are unchecked due to erasure are already a warning but still a major source of confusion and questions. &gt; ... &gt; sealed trait Id extends Any [...] I don't see _anything_ in what you brought up that needed or should behave differently with a native implementation of value types. Are you sure there is any actual problem? &gt; If value types really improve everything with no downsides the way you describe, why not just compile all case classes (or any that don't override equals/hashCode) to value types? One could probably consider how to achieve this. One of the plans I had was to make people explicitly decide between a reference type and a value types when defining a class (i. e. don't assume `AnyRef` be default), and make traits universal by default (`extends Any` instead of `extends AnyRef`). Not sure if it would have been ergonomically acceptable, but hey, you can measure these things and decide based on that data. &gt; That's something I want, and something I'll still want whatever clever optimizations get implemented by the JVM backend. If people would spend 5% of their efforts of adding new things on maintaining existing things, the language wouldn't be such a mess. Or do you think the rate of attrition of people who care about quality, who fix and maintain things is normal? Why do think people burn out and leave after a few years of trying to do this? Why do you think most parts of the standard library are not maintained anymore? Because everybody understands pretty soon that it is pointless to maintain anything – any improvement in quality one makes is completely irrelevant, as people just add their newest half-baked ideas, and one can start again from the beginning.
Sounds like a very complicated way to see such a simple thing. You should look into Phantom Types and Path Dependent Types if you want a more functional approach.
I am playing with SBT Microsite, and ScalaFiddle to document my journey while learning Scala: https://wibisono.github.io/fp-oops/ A bit random now, but it is just a bunch of markdown files interleaved with code snippet, sometimes I formatted it as exercise where people can try to code online in Scala fiddle. If you have ideas/feedback please let me know, it is still in an early stage now.
Awesome! Let us know what you think! Feedback/contribution is appreciated! 
I honestly feel that the Akka streams API is very crude at this point. Can anyone recommend good books/tutorials to get started with it.
Any good resources on learning akka streams. The documentation is very raw and hard to follow. Please don't suggest monix/fs2, I really dont have a choice on choosing the framework where I work.
The [FSM](https://doc.akka.io/docs/akka/2.5.11/fsm.html) ([ScalaDoc](https://doc.akka.io/api/akka/2.5.11/akka/actor/FSM.html)) trait is specifically designed to model finite state machines using Akka actors.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://doc.akka.io/docs/akka/2.5.11/fsm.html) - Previous text "FSM" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20dwzqx2j) 
Second Google result gets this: https://github.com/jakehschwartz/finatra-swagger This is a fork of xiaodongw's library, but updated for Scala 2.12 and newer swagger versions.
Perhaps you could model this more simply by having the request actor try to authenticate as soon as it’s spawned. If it successfully authenticates, then it’s obviously in a valid state and can make requests. If it fails to authenticate, it lets the failure propagate as an exception up to its supervisor, which can restart it perhaps with an exponential backoff strategy. The point here is to encode authenticated (‘ready’) state into the supervision tree itself instead of trying to shoehorn it into the state of a particular actor.
I think John is conveniently forgetting the main value proposition of an actor system: fault tolerance. With the IO type I’m manually implementing failure handling and recovery at the individual function level. With a supervision tree I’m designing my error recovery strategy top-down over my whole system.
Saying scalaz is “easier to understand” is like God telling you He doesn't understand why you're trying to make a warp drive, when it's so much easier to just alter the laws of physics.
You realize that fault tolerance can be implemented for IO, right? It's called principled error handling, and pure programs have been making use of it for decades.
This is an example of misunderstanding. If you implement robust supervision strategies for each parent-child relationship in an actor system, it's functionally equivalent to principled error handling with IO. If you're objecting to the fact that you have to define your error handling strategy manually, and you're willing to defer to Akka and all of its faults to avoid it, then I think your priorities are a little out of wack.
Some basic principles may be frontloaded, but scalaz is vastly easier to understand, work with, and reason about over the long term.
Too bad the frontloading involves years of study at a university.
sorry but very few things in programming come up to being as complex as akka codebase (that is older than few months, but everything is simple if it's 1000LOC). In that sense, I think it's fair to say that scalaz is simple.
Only if you understand scalaz's underlying concepts, which are not exactly accessible to the average programmer. Scala is arcane enough already, but scalaz takes the ivory-tower stuff up to 11. How many average programmers have you met that know what a bifunctor is?
Nah, I picked it up from a few study materials in my free time. I can link you some resources if you wish.
Not trying to be offensive but I think going for one extreme is a bit incorrect. Future/IO/Task are definitely a simpler and conciser approach. They are suitable for general use cases which are the huge majority, so it actually makes sense using them most of the time. On the other hand, Akka actors are a bit more flexible (and a bit more verbose at the same time). Just a few use cases that come in my mind: ES + Persistence (although this one is arguable, depending on what system you are trying to build you may want to use a distributed messaging system like Apache Kafka instead). Speaking of Apache Kafka, it feels like Kafka Consumer Actors (from [Scala Kafka Client](https://github.com/cakesolutions/scala-kafka-client)) are more flexible than Kafka Streams (although I must confess I haven't used it, this is mainly based on a quick research and general knowledge about streaming). Is it possible to have IO based Kafka consumers that are flexible enough to have thinks like an in-memory thread-safe cache? Another use case for actors is distributed computing that looks like it was covered in the presentation. It has its own disadvantages but it still covers some use cases that with IO/Task/Future don't. It is important to have a pragmatic approach and pick a tool that is most suitable for that job. Most of the time IO/Task/Future are the simpler and conciser solution but we shouldn't forget that sometimes Akka Actors are very handy.
&gt;It’s about isolating concurrent subsystems from each other and deciding when to stop or restart them. [*In the event of failure*](https://doc.akka.io/docs/akka/2.5/general/supervision.html). It is about error handling. It's certain sub-class of error handling strategies that deal with (in the case of `IO`, at least) error or cancelation, which pertain to the process which spawned node, be it an `IO` reference or a method. *There's no reason why you can't have this for `IO`*. Consider typeclassing the `IO` you're working with and adding the ability to fork work and cancel on certain types of error, or in the case of perhaps some timeout (with a Monadic timer, mind you!), or non-response, terminating it. That's roughly equivalent to supervision.
&gt; Opaque types just have slightly different feature set compared to value types exactly because it requires less boxing in some situations, the semantics of opaque types pretty much suck (no equals, toString, ...). They have much clearer semantics *with regard to boxing*, which is the whole raison d'être for any of these features. If `AnyVal` doesn't have clear semantics about when it boxes - and it doesn't - then it essentially has no semantics at all, and might as well be a no-op. Equals and toString are semantically unreliable in the first place - being universal means you can't rely on any given implementation of them to be correct. Codebases that focus on correctness avoid them already (e.g. they're banned in the Scalazzi safe subset of Scala). &gt; A design like opaque types was explicitly rejected at the time when the design space for value types was explored, because people just came back from years of undoing compiler fiction – it's not like anyone was unaware of the limitations of value types when the design was picked over something that was more fully erased like opaque types. The current design is worse than any compiler fiction that came before it - if `x` is a `GroupId` a typical developer literally can't tell whether it'll be a `GroupId` or `UUID` at runtime, because it's different in different contexts. Erased generics are confusing enough, but at least they follow simple, universal rules. &gt; Doing that is completely impossible with opaque types to start with – they cannot implement new interfaces, and it's unclear how to treat existing shared interfaces between opaque types. So you would probably need to box opaque types into an Either anyway. Exactly! When the developer wants to box they will choose to do so, explicitly, rather than having silent surprise boxing at runtime. &gt; Or do you think it wasn't always "other people's old unmaintainable features" vs. "my new shiny thing that I totally gonna maintain"? You're talking as though all features carry the same maintenance burden however they're designed and implemented, which is just not true. It is possible to design for maintainability, and that's something the maintainers should be enforcing. &gt; I think that's flat-out wrong, and I have seen this too often already to not call it out: You are completely overstating the issues with value types solely because you want to add a new feature. There is nothing that isn't fixable, and especially with value types it was clear from the beginning that it would require runtime support. No U. You've adopted multi-parameter `AnyVal` classes backed by Java value types as a pet feature for whatever reason, and are making a bunch of misleading claims to support that. You haven't even come up with a concrete definition of what semantics multi-parameter `AnyVal` classes are supposed to have, because all the possible answers have serious issues. And since `AnyVal` classes are in use in current code, they're unfixable: we can't silently weaken existing behaviour. Opaque types are taking the correct approach here: any cases where there isn't a clearly correct implementation are compile-time errors. We can always permit things that were previously forbidden, but once something has been permitted we can't go back and forbid it, and `AnyVal`s permit too much that should be forbidden. &gt; Value types are semantically sound – that's pretty much why it requires boxing in many places – and you are trying to replace it with something that is less sound, hoping that the semantics are cheaper to implement. To the extent that `AnyVal` classes have the same semantics as normal classes, there's no point in it - why bother with `AnyVal` if it doesn't mean something different from non-`AnyVal`? Semantically it's been hopelessly confused from the start - it's supposed to mean a value isn't boxed, but it doesn't mean that. &gt; From my point of view this the whole proposal is utterly bizarre, and just demonstrates how far Scala's quality standards have fallen. `AnyVal` was a new feature with inherent serious maintenance problems, so quality standards must've been lower in the past, not higher. &gt; It's like people have amnesia and have no recollection how bad things were in the past and which mistakes have already been made – people spent absurd efforts to clean up failed designs like what's proposed now. It's like a revolving door of bad ideas, but unlike in the past I just can't see anyone doing the cleanups anymore. If you want to clean up the language and codebase the best thing would be to deprecating and eventually remove `AnyVal`, but you're against that as well. You're not making any sense.
Are you saying concurrency is accessible, while typeclasses are not?
Catching errors at every level isn't sustainable. "let it crash"and have some to level error strategy is elegant and cleaner Also "it's dead simple" is never a good assumption
&gt;Catching errors at every level isn't sustainable. This is not what I said in the slightest. I suggest you reread the post.
I'm not really disagreeing with you there champ, you mention both approaches
I'm sorry you feel this way. If you need help, feel free to seek us, or typelevel's Cats folks out in our Gitter's to ask questions and learn. Don't feel down - it's just the start of the climb up the mountain.
That's kinda what I wanted to do, but doing this, wouldn't a crashing actor lose all it's pending messages? How do I retain those cross restarts? Push them to the supervisor actor before dying?
You're wrong. They are accessible to every programmer.
Download underscore's books for free. It's a good start
I did [this](https://www.coursera.org/learn/progfun1) course in Coursera 2 years ago. It's a little bit to academic but do not regret doing it.
Ask a question on gitter! Start with a simple one to test the waters, and don't be dissuaded when the topic is a little abstract 
I'd recommend this guide https://gist.github.com/d1egoaz/2180cbbf7d373a0c5575f9a62466e5e1 
See https://doc.akka.io/docs/akka/current/actors.html#what-happens-to-the-mailbox : &gt; If an exception is thrown while a message is being processed, nothing happens to the mailbox. If the actor is restarted, the same mailbox will be there. So all messages on that mailbox will be there as well.
Is there code somewhere? This seems like a comparison between vaporware and a mature platform currently being used in production at scale. Of course the fake interface will be cleaner.
Not sure why you wouldn't just link it. I've never seen anything on scalaz that was in particularly comprehensible.
Like you, I bought the red functional programming book to start out. I found myself getting quickly overwhelmed by the syntax. I decided to pick up Scala for the Impatient and I'm really liking it.
&gt; Supervision is not _built_ into Akka. It is simply _prepackaged..._ What is the difference for practical purposes? &gt; ... Akka boxed a few standard strategies for you ... as long as what you're doing doesn't require any sort of logic that doesn't come standard. Then you have to write them and include your custom strategies.... More than that, it gave me well-defined places to put handlers for any significant events that happen to my concurrent processes: lifecycle, supervision, and message reception. These are the core group of behaviours I'm concerned with anyway. What's the alternative? I structure my codebase from scratch every time using IO. I agree that everything you mentioned is doable, but it's still custom work and everyone will have a slightly different idea of what typeclasses and other mechanisms are needed. Or, I could structure my application in a standard way that's already well-defined, even if it's not the cleanest.
I recommend working with the red book second, after you are finished with the resource you chose.
I would suggest that it's because asking questions on gitter is not a methodical approach to learning a programming language. You need tutorials, videoscasts, exercises etc. It would be like teaching kids history by just letting them ask questions. It would be unstructured and not a very positive situation for eaither side..
Great summary. Thank you 
Second that suggestion. I came from PHP, too (although I had learned the trade with Java). It's a good introduction to the principles. Once you have that down, take a look at https://underscore.io/books/scala-with-cats/
Can't agree with this point more, not from the sense of correctness vs incorrectness (often these debates don't even fall into these lines), but the fact that the others parties don't even recognise the problems that other solutions are trying to solve. The whole point of akka actors is that it treats supervision and error handling as a first class system, and if you are doing highly concurrent async distributed system and tried to replicate it with IO/Future/Task, you will soon find that you will reinvent half of akka (the half that at least deals with all of these concerns) in a not so nice way. Don't get me wrong, akka definitely has problems (being very untyped is the main one, however this got mainly resolved recently). But slides that show trivial business logic being easier in Scalaz8 IO are completely missing the point, which is that akka's main strength is in complex distributed processing over networks, where error handling can happen anywhere and the domain in general is incredibly complex (heck this is how akka even got invented, its based off Erlang which was designed for telecommunications which required 100% uptime and were highly distributed in nature). If you are doing general CRUD/web based application logic than Akka is definitely overkill, and these are the majority of applications (and I do regret that Akka Actors were pushed so hard for this reason), but they definitely have their place which a lot of people are conveniently ignoring.
Yeah I have to say that the cats documentation is definitely a lot more approachable (coming from the POV of someone who has never done pure FP before)
This is a fair point. You can preview the current state here https://github.com/scalaz/scalaz/tree/series/8.0.x There's also a wiki showing the projects progress: https://github.com/scalaz/scalaz/projects/1
Indeed. Cats' documentation explains the various concepts in terms of what you can do with them (map over them, etc). That makes it accessible even to me.
&gt; ... fault tolerance being built into the core scheduling logic.... Does anybody do this? I don't see how this would be a useful approach. The whole point of Akka/Erlang-style supervision is that you get to override the strategy if you want. If you can't do that, it doesn't seem very flexible. &gt; ... Akka touts Backpressure as a feature of the architecture.... This is the first I'm hearing of it. In fact, didn't Akka Streams come about explicitly because Akka mailboxes can't handle backpressure? &gt; ... `IO` can be supplemented in the same with respect to supervision. It can, of course, but again that is left to every user. &gt; I see an argument for a library which standardizes fault tolerance for Effect types (including IO), less an argument for Akka. To each their own, it would certainly be interesting to see what a fault tolerance and self-healing architecture would look like in the context of the `IO` type. &gt; I'll propose this and we'll see if we can't get something written up for the next iteration of Scalaz's IO.... That would certainly be nice. I am looking forward to seeing these kinds of solutions provided by library designers instead of bare effect types and algebras for a change. The reactive streams interface was a good example of this–the designers did the hard design work of coming up with a simple, flexible API for backpressure.
I wasn't familiar with the idea. Very interesting concept. Thanks for sharing.
You are pretty much dead-wrong on everything, and I fear there is no fact I can provide to help you realize that. Do you understand that everything you pointed out was well-known when value types where designed – and the current design was still chosen? The reason why the current design was chosen is because it provided a clear upgrade path when JVM support came around. That's why value types are for instance boxed in arrays – because running things like `getClass` would keep behaving the same way before and after JVM support shipped. Despite getting this explained _multiple_ times already, you keep beating the straw-man of the current implementation. &gt; The current design is worse than any compiler fiction that came before it - if x is a GroupId a typical developer literally can't tell whether it'll be a GroupId or UUID at runtime, because it's different in different contexts. Erased generics are confusing enough, but at least they follow simple, universal rules. If you are unaware, generics are getting specialized. &gt; Exactly! When the developer wants to box they will choose to do so, explicitly, rather than having silent surprise boxing at runtime. If you are unaware, the JVM has supported inlining and elimination of dynamic dispatch for more than a decade already. You are literally arguing for having "consistent" overhead in opaque types that pretty much won't exist with value types in the first place. &gt; You're talking as though all features carry the same maintenance burden however they're designed and implemented, which is just not true. It is possible to design for maintainability, and that's something the maintainers should be enforcing. No, they are not. - It's pretty clear that the proponents of this proposal won't be around to actually implement "opaque types could use JVM value types under the covers" (as people here "optimistically" suggested). - Maintainability is best ensured by having actual maintainers, bot people who add things and then expect others to do the maintenance. - Using a feature that is maintained by a completely different organization (like JVM value types and specialized generics) is inherently cheaper than NIH-ing some new feature yourself. &gt; You've adopted multi-parameter AnyVal classes backed by Java value types as a pet feature for whatever reason, and are making a bunch of misleading claims to support that. If you are unaware, that was the whole point of value types right from the start. The current limitations were designed to make the removal of the restrictions as seamless as possible. &gt; You haven't even come up with a concrete definition of what semantics multi-parameter AnyVal classes are supposed to have, because all the possible answers have serious issues. If you are unaware, details of this have been discussed in 2013. What semantics are you looking for? &gt; And since AnyVal classes are in use in current code, they're unfixable: we can't silently weaken existing behaviour. What are you even talking about? &gt; We can always permit things that were previously forbidden, but once something has been permitted we can't go back and forbid it, and AnyVals permit too much that should be forbidden. If you are unaware, that's exactly why value types had so many restrictions in the first place: to allow permitting more things as JVM support arrives. &gt; To the extent that AnyVal classes have the same semantics as normal classes, there's no point in it - why bother with AnyVal if it doesn't mean something different from non-AnyVal? Semantically it's been hopelessly confused from the start - it's supposed to mean a value isn't boxed, but it doesn't mean that. It's about the semantics as an immutable value that value types provide. How they are implemented behind the scenes is largely irrelevant. And you don't need to take that from me, you can take that from the C#/CLR team. &gt; AnyVal was a new feature with inherent serious maintenance problems, so quality standards must've been lower in the past, not higher. Please name a few then. I don't think quality standards are higher now, as proponents of this proposal are completely unaware that more "erased" design like "opaque types" was rejected back then and why value types ended up the way they are. &gt; If you want to clean up the language and codebase the best thing would be to deprecating and eventually remove AnyVal, but you're against that as well. I don't think you clean up things by adding new features, not maintaining existing ones, and causing an even bigger mess of Scala having to "somehow" support both opaque types _and_ value types (because that's what all the Java libraries will be using in the future). &gt; You're not making any sense. Maybe this is because you keep rejecting facts? Of course this makes it harder to understand things. If you can't accept information from people who have cleaned up stuff for half a decade, and have seen the same disastrous, wishful thinking again and again, I really can't help you.
Yeah, too late to edit sorry :)
&gt; Does anybody do this? I don't see how this would be a useful approach. The whole point of Akka/Erlang-style supervision is that you get to override the strategy if you want. If you can't do that, it doesn't seem very flexible. Yes, people do this, and yes, I agree with you. It's inflexible. &gt;This is the first I'm hearing of it. In fact, didn't Akka Streams come about explicitly because Akka mailboxes can't handle backpressure? Ahaaaa yeah.... that was an egg on the face moment for them, considering they were telling people backpressure was supported out of the box via routing logic and mailbox customization. I can show you the slides I was asked to shill. Yes though, Akka streams is opinionated in that respect, because backpressure is difficult to implement for actors, regardless of what was claimed, or what was provided. &gt;IO can be supplemented in the same with respect to supervision. We should fix that! &gt;I am looking forward to seeing these kinds of solutions provided by library designers instead of bare effect types and algebras for a change. Perhaps we can sway John de Goes with a decent pitch.
Some more reading on this idea from 2012, with updates about Akka and Cassandra: &gt; In Dean’s experiments, BigTable’s 99.9th percentile latency dropped to 50ms when he sent out a second, redundant request if the initial request hadn’t come back in 10ms—a 40x improvement. http://www.bailis.org/blog/doing-redundant-work-to-speed-up-distributed-queries/ 
To get the basics down, exercism.io is good across most languages.
Scala IDE or Intellj? Which one for Scala development? Thanks
The Cassandra driver (well at least the Java one) has speculative queries built in though not enabled by default.
What is the output of `test:assembly`?
Don't bother with FPiS (i.e. the Red Book) to learn Scala the language, you'd be much better off with Programming in Scala (3rd edition) for that. If you want to learn functional programming, sure, by all means pick up FPiS, it's an amazing resource.
It may not be that hard, but hell Circe is so unintuitive !
&gt; Despite getting this explained multiple times already, you keep beating the straw-man of the current implementation. So the implementation that actually exists is a strawman, and the imaginary perfect implementation that is totally going to ship in two years despite having been delayed by two (or is it now three?) major releases already is what we should be talking about, got it. &gt; If you are unaware, generics are getting specialized. Which is relevant how? &gt; If you are unaware, the JVM has supported inlining and elimination of dynamic dispatch for more than a decade already. You are literally arguing for having "consistent" overhead in opaque types that pretty much won't exist with value types in the first place. To the extent that the JVM can eliminate it, it can eliminate it equally well for opaque types. If the JVM is so good at eliminating boxing overhead then why are you so bothered about having unboxed types at all? Either boxing matters - in which case developers need precise control over it which `AnyVal` can't provide - or it doesn't - in which case there's no point to `AnyVal` at all. &gt; It's pretty clear that the proponents of this proposal won't be around to actually implement "opaque types could use JVM value types under the covers" (as people here "optimistically" suggested). But the people who implemented `AnyVal` 5 years ago will definitely be around to implement the JVM value type integration without which it's pointless, when those JVM value types are finally implemented in 2 or more years' time? &gt; Maintainability is best ensured by having actual maintainers, not people who add things and then expect others to do the maintenance, as it happened again and again. Doesn't that mean you're the last person who should be commenting about maintainability, given that you took part in adding features and now don't take part in maintenance? &gt; Using a feature that is maintained by a completely different organization (like JVM value types and specialized generics) is inherently cheaper than NIH-ing some new feature yourself. Not always. Sometimes integrating with a feature maintained elsewhere is more difficult than implementing the same functionality directly. To the extent that JVM value types can be used as a pure-backend implementation-level functionality that doesn't carry maintenance implementations across the whole language, from the AST and the root of the type hierarchy right down to the bytecode generation, I'm in favour of using them. To the extent they require compromising the whole language design with semantically confused constructs like `AnyVal`, they're not worth the trouble. &gt; What semantics are you looking for? The semantics of `AnyVal`. What does it mean for a class to be `AnyVal` as distinct from one that isn't, particularly in the context of a `case class`? Why are both possibilities necessary/useful? &gt; It's about the semantics as an immutable value that value types provide. How they are implemented behind the scenes is largely irrelevant. And you don't need to take that from me, you can take that from the C#/CLR team. But `case class`es already have the semantics of being values (they're not necessarily immutable but nor are C# structs). I agree that the implementation is largely irrelevant - so why have a special-case language-level feature to control the implementation, particularly when that feature doesn't even let you control the implementation?
You don't have to know what a bifunctor is though, since it's just plain old code; if you click through to [an implementation of bimap](https://github.com/scalaz/scalaz/blob/v7.2.21/core/src/main/scala/scalaz/std/Either.scala#L52) in your IDE it's almost always straightforward to understand. Whereas I wouldn't even know where to click to to find out how a given bit of akka config changed the behaviour.
Brian Goetz felt sorry enough about the state of discussions in Scala to comment with some information. Now some people are slowly figuring out some of the things I said from the beginning. Why don't you join them? (Or just stop arguing with me, and instead explain to him why he is wrong?)
How does it compare to the existing forums/channels in the side bar?
I created simple Scala lib called [Tschingt](https://github.com/PaulNoth/tschingt). A reimplementation of [five.js](https://github.com/jackdclark/five) to Scala during my Scala/SBT learning process.
What does it offer over the users.scala-lang.org discourse?
I think, users.scala-lang.org is a question/answer forum, like stackoverflow in some way. I want to make the community on Spectrum more like a place for free communication on any topic about Scala, like reddit, maybe, but with different UI. :)
Anyway, I don't think we have enough space for communication.
If you're going to make these claims on /r/scala it's not my responsibility to follow up your random allusions to whatever other forum, it's your responsibility to back up what you're saying here. I take it from your lack of a substantive response that you've realised you can't.
Paidy | Scala Engineer | Tokyo, Japan | ONSITE | Full Time | Competitive I'm currently working here: https://engineering.paidy.com/
What is the name of the extension? 
I think it clearly demonstrates your complete lack of research and facts. If you followed the discussion even remotely, you would have known that I referred to the discussion on the opaque types PR. I intentionally didn't try to hand-hold you this time. I hoped it would help you would realize how far your denial has put you into fantasy land, in which you solely rely on dismissing publicly available information to protect your ideas from reality. I have nothing more to say, but I hope you will learn to also consider information that disagrees with your beliefs in the future. Maybe you will be able to come back to this thread then, concede how misinformed you have been and apologize for the way you failed to contribute to this debate. Good luck on your journey.
The usual way to do this is with the cake pattern, mixing in traits that have type members, something like: sealed trait Mode case class Master() extends Mode case class Slave() extends MOde trait ModalContext { type Mode } trait Component[M &lt;: Mode] class MasterComponent extends Component[Master] class SlaveComponent extends Component[Slave] trait ComponentBuilder extends ModalContext { def buildComponent: Component[Mode] } // Box doesn't need to know about master/slave-ness, that's all in Component, right? // Certainly you don't seem to need two layers of sealed trait - either you have two different // kinds of boxes in which case there is no need for a shared Component trait, or you have // one kind of box and two different kinds of Component case class Box[M &lt;: Mode](component: Component[M]) trait BoxBuilder extends ComponentBuilder { final def buildBox: Box[Mode] = Box(buildComponent) } class MasterComponentBuilder extends ComponentBuilder { final override type Mode = Master final override def buildComponent = new MasterComponent } class SlaveComponentBuilder extends ComponentBuilder { final override type Mode = Slave final override def buildComponent = new SlaveComponent } def boxBuilder(master: Boolean): BoxBuilder = if(master) new MasterComponentBuilder with BoxBuilder else new SlaveComponentBuilder with BoxBuilder boxBuilder(true).buildBox Bear in mind that even this much is often considered unnecessary overengineering. As others have said, maybe clarify what you're trying to achieve with all these layers of components. Often a "factory" is overkill in a language with first-class functions, a "builder" is unnecessary in a language with named parameters, and so on.
&gt; I intentionally didn't try to hand-hold you this time. I hoped it would help you would realize how far your denial has put you into fantasy land, in which you solely rely on dismissing publicly available information to protect your ideas from reality. Bullshit. You make these airy assertions to avoid actually committing to anything, because as soon as you actually gave a link it would be clear you're full of shit, just like upthread where "Please please please just read up on the Valhalla documentation" immediately turned into "Yes, the wiki is annoyingly out of date" as soon as someone actually called you on it. Constructive people are happy to link to "publicly available information". &gt; I have nothing more to say, but I hope you will learn to also consider information that disagrees with your beliefs in the future. No U &gt; Maybe you will be able to come back to this thread then, concede how misinformed you have been and apologize for the way you failed to contribute to this debate. No U. Do you never wonder why you end up with so many downvotes? How many people will it take to tell you before you see that you're the problem here? &gt; Feel free to have the last word, but if you do, please include the list of features I added to the language. I don't know or care what you did but you try to present yourself as some kind of Scala contributor here - but again you never give specifics (hmm, pattern?). So maybe you were bullshitting then, maybe you're bullshitting now. Probably both. Whatever. 
I prefer Scala IDE, because IntelliJ's error highlighting is unreliable, with both false positives and false negatives. It's a shame because IntelliJ has a lot of great features, but I need the basic error highlighting to be 100% reliable before I can even think about anything more advanced.
I use IntelliJ and it's mostly been a great experience. My main complaint is that errors don't always match scalac, typically when heavy inference and implicits are involved, but it's correct 99% of the time for the style of Scala I write. I also use the nightly channel for the Scala plugin. I did recently run into an issue where integration test resources are used when unit test resources should be, but I don't know if that's a Scala plugin issue or the fact that IntelliJ only has a notion of "test resources" indistinct from "integration test resources".
As much as possible try to do things with plain old code - in Scala what you can do in plain old code goes a lot further than in other languages. Can the builder just be a function? If you need a slave/master distinction can that just be a method with a type parameter? It would only ever make sense to do something complicated if the underlying problem is just as complicated.
``` case class MasterSlaveBuilder[T, M &lt;: T with Master[M], S &lt;: T with Slave[S]]( buildMaster: () =&gt; MasterBox[M], buildSlave: () =&gt; SlaveBox[S] ) { ``` from the original post, Builder here is just a wrapper around 2 functions and nothing more, isn't it? :) 
Fwiw you guys have a typo on your FED position - Ctrl-f "Fronend"
&gt; Builder here is just a wrapper around 2 functions, isn't it? :) It's awkwardly in between - I'd either pass the two functions around as functions, or make it a proper trait with abstract methods. &gt; But I want to generify Component from Builder, hence the Box with a hole to fill in Component/Reader/Writer. I don't think this follows? What's the Box adding? You can define Builder generically so it can build Component/Reader/Writer without needing any intermediate Box, or am I missing something? &gt; Then I want to differentiate 2 kinds of Box, 1 only contains Master and 1 only contains Slave, hence SlaveBox and MasterBox So is the Box actually solving a problem? I don't understand why you've added all these generics and self-types. Which things have different implementations between master and slave? Does a MasterReader have different code from a SlaveReader, or is it just something you want to keep track of? In the former case you could probably have non-selftyped interfaces for master/slave, just something like: trait Master trait Slave class MasterReader extends Reader with Master { ... } ... trait Builder[C &lt;: Component] { def buildMaster: C with Master def buildSlave: C with Slave def build(master: True): C = if(master) buildMaster else buildSlave } In the latter case you probably want the master/slave-ness to be a type parameter on your components: sealed trait Mode case class Master() extends Mode case class Slave() extends Mode class Reader[M &lt;: Mode] { ... } trait Builder[C[_]] { def buildMaster: C[Master] def buildSlave: C[Slave] def build(master: True): C[_] = if(master) buildMaster else buildSlave } 
Yeah, unfortunately so for people who can't or don't want to be engaged in a completely unstructured collective mindstream in real time. Any information created on gitter for all intents and purposes never leaves it, and gitter's search is horrible even if you do know to search there.
Unpopular opinion: don't bother with scalaz / cats / red book / other hardcore functional stuff until you've felt the pain of needing those things first hand. Otherwise you'll just end up cargo-culting complicated constructs with no benefit whatsoever. The need for things like e.g. typed effects is entirely subjective. Only a subset of the Scala community values those things, while everyone else simply doesn't care to say anything, which is why it sometimes seems that there is a general agreement that haskell-in-scala is the only way to do Scala. But it's not. Simple functional features like passing functions as values will take you very far, with a much gentler learning curve. Plenty of Scala teams work just fine without ever touching cats / scalaz / shapeless / eff / etc or being able to recite various FP definitions and laws from memory. The coursera course linked in the comments is a great resource for things you should actually be learning as a beginner.
Akka actors seem extremely useful to me for protecting state from concurrency, much like a concurrent version of encapsulation. I find them pretty useful when doing GUI work, but I've started using streams more often for that. 
Streams?
Thanks. I was able to generate swagger.json with the jakehschwartz library.
akka streams
I think many algorithms when done "pure" are much less efficient then combined approach. Scala library proves my point ) I've checked changes and they updated some modules.
Essential Scala and Scala with cats are the two books I recommend to people new to Scala at work The red book is fantastic but it's not a good intro to the language, and it's extremely demanding of the reader, which not everyone is ready for
I agree that just using parameters and functions will get you extremely far. It's important to keep in mind that the abstract fp concepts are all used to solve problems that come with you use parameters and functions. Once you feel the need, they make a lot more sense
Thanks for your patience &gt; So is the Box actually solving a problem? I don't understand why you've added all these generics and self-types. Due to the way I wrote Master/Slave as marker trait. I've no ways to get the interface out from the build product (master/slave has no methods). `Box` alone will solve the problem of making 2 kind of product: `MasterBox` and `SlaveBox` but then compiler lets me stuff a `SlaveComponent` into a `MasterBox` which I don't want too &gt; Which things have different implementations between master and slave? All: Component, Reader, Writer &gt; Does a MasterReader have different code from a SlaveReader, or is it just something you want to keep track of? Different code. trait Master trait Slave class MasterReader extends Reader with Master { ... } ... trait Builder[C &lt;: Component] { def buildMaster: C with Master def buildSlave: C with Slave def build(master: True): C = if(master) buildMaster else buildSlave } this builder is tightly coupled to Component... The last snippet would solve my problem but I'm not sure * is accepting any C[_] too general? * the base class Reader needs to take type parameter hence I can't provide master/slave for someone else's class.
nvm, this works great trait Master trait Slave trait Component class MasterComponent extends Component with Master class SlaveComponent extends Component with Slave case class Builder[C]( builderMaster: () =&gt; C with Master, builderSlave: () =&gt; C with Slave, ) { def build(isMaster: Boolean): C = if (isMaster) builderMaster() else builderSlave() } object Hello extends App { val componentBuilder = new Builder[Component]( () =&gt; new MasterComponent, () =&gt; new SlaveComponent ) val component: Component = componentBuilder.build(false) }
People should have choice, in my opinion.
Leapfin | Software Engineer | San Francisco, CA, US | Onsite or Remote | Full Time | $120k+ We are an enterprise b2b finance software startup based in SF SOMA. Because we work with finance, we deal with lots and lots of data. We have found product/market fit and are growing rapidly. We are looking for a technical leader who can help us scale. We are looking for someone able to collaborate well in a team environment and who always puts the customer first. Job Posting: https://leapfin.com/careers About: https://leapfin.com/about Let me know if you have any questions!
Demandbase | ML Engineer, Principal Data Engineer, Data Integration Architect, others | SF &amp; Seattle | ONSITE | Full Time | Very competitive We have a handful of engineering openings in both San Francisco and Seattle. Check out https://www.demandbase.com/company/careers/#current-openings and feel free to message me on here or email vramesh@demandbase.com if you have questions. 
Thanks, reported to the "Fronend" team XD
Hey, this looks interesting for me however I am look to pick up only an extra 10 hours a week remotely. Do you guys have any room for that? I currently a data engineer using scala and spark.
Cake Solutions / BAMTech / Disney | Scala Engineers (mid-senior) | NYC | Onsite if Cake, Remote + Onsite if BAMTech | FT | Competitive The NYC team is a fun, competent, and close-knit team of functional programmers (as in ref. transparency purely functional) working on large-scale distributed systems (think millions of users for your services). We work with libraries such as Cats, Scalaz, AWS (including libraries built to make it usable, like Scanamo). There are opportunities for internal talks, training other teams, as well. feel free to take a look, or pm for more info - [Full listing](https://cakesolutions.bamboohr.co.uk/jobs/view.php?id=6)
Snowplow Analytics | Scala engineer intern, consulting data engineer | London, UK | REMOTE | Full time | Competitive We are looking for a remote Scala intern for the summer to work on [our open source codebases](https://github.com/snowplow/). The internship will be centered around migrating our libraries from scalaz and json4s to cats and circe. This is a paid internship and we're looking for candidates in UTC+/-5. Job posting: https://snowplowanalytics.com/company/careers/?gh_jid=1107068 Additionally, we're looking for a Consulting Data Engineer to work with our customers to solve their data processing problems using batch and stream (Spark, Flink, Beam) on AWS and GCP. Job posting: https://snowplowanalytics.com/company/careers/?gh_jid=1083969
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/functionalprogramming] [\[Blog\] The Science Behind Functional Programming](https://www.reddit.com/r/functionalprogramming/comments/8b7btp/blog_the_science_behind_functional_programming/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
OpenLaw | Full Stack Developer | Brooklyn, NY | REMOTE | Full Time | Competitive ConsenSys is a venture production studio and the leading technology firm in blockchain globally. OpenLaw is a ConsenSys spoke. At its core, OpenLaw is a multi-module blockchain-based protocol and markup language to prepare, manage, and execute smart legal agreements. We use Scala as our main programming language with JavaScript / React for our frontend. We have a strong focus on functional programming in our codebase. Experience with Akka and CQRS a strong plus. Job Posting: https://new.consensys.net/careers/?gh_jid=1061221 Website: https://openlaw.io 
It's a bot, not you. It automatically linked back.
For what worked for me, I got the hang of it pretty quickly reading the docs. My personal takeaway in building a Web Socket server using Akka Streams is that it was much easier to use all of the built-in stages and their combinators than the graph building DSL. I'm not sure if that is less performant, but I find it much more intuitive.
Confusing science with math is a huge pet peeve of mine. Math is rooted in philosophical reasoning, not empirical reasoning. It is not developed via the scientific method, but through proof and refutation. The scientific method is enabled by mathematics, but mathematics is not science. Functional programming has its roots in category *theory*, not empirics. Just because something is technical doesn't mean it is a science. 
Was thinking the same thing when I read the title.
One more way to fuck someone's brain :) So Computer Science is not really science but philosophy? Computers are not natural but atrificial. So Computer Science is not science at all. If philosophers can prove that philosophy is science - they can prove mathematics is science and CS is science too :) Scala people, stop grinding my gears, i've chosen Scala because it is expressive, easier to express real life relations.
Fighting with bots... That's how this war started. I'll be back
&gt; So Computer Science is not really science but philosophy? Computers are not natural but atrificial. So Computer Science is not science at all. Depending on how you define computer science, it could be a misnomer. The study of the mathematical theory of computation? Yes, that is philosophical reasoning, and a subset of mathematics. The study of computation as a practical result of the von neumann architecture? Science. http://blog.codalism.com/index.php/computer-science-is-not-real-science/comment-page-1/
&gt; the abstract fp concepts are all used to solve problems that come with you use parameters and functions This is true. The problem is that there's not an obvious connection between problem and solution until you've taken a leap of faith and dedicated real effort to understanding the solutions. There's a danger in ignoring the abstract stuff because when you start to run into the kind of problems you describe, you won't know what solutions exist, or even that they do. You just end up with a combinatorial explosion of specialized implementations and think "I guess this is what scala looks like," shortly followed by "scala is awful."
Been grinding out Mill bugs recently and released version 0.2.0: - https://github.com/lihaoyi/mill#020 There are a lot of fixes in this one; perhaps the biggest is first class Java support. The build files remain in Scala, but you can now build/test/publish pure Java modules &amp; projects using Mill just as easily as Scala modules.
Hey thanks. Those articles are indeed helpful.
Also fair
Well, it could be the same thing (according to Max)[https://www.scientificamerican.com/article/is-the-universe-made-of-math-excerpt].
Why do I seem to be the only person who has 0 issues working with sbt? I've got some pretty complex builds with lots of source code generation and multi - projects and cross building and plugin development... What am I missing? I get the problems of understanding the internals of the tool, but in actual use I have almost never had an issue with it. Maybe I've just been using it for so long that I've forgotten the issues I had when I started. Maybe I just had really good teachers when I learned it. I just don't feel the pain that everyone else seems to feel with it. From Maven users at my company, the problem mainly seems to be that they just miss Maven, because they know Maven.
Sometime,I have to make use of Akka and Monix's Task both,Akka for the DDD modering and message passing, but yes, Task do the work some time. I think scalaz's IO and Akka just complete each other,not one for one.
I have a small follow up from my post on [Akka actors - how to model different states](https://www.reddit.com/r/scala/comments/89fqox/akka_actors_how_to_model_different_states/). One of the many things I learnt is that it's a good idea for error states to be handled by the parent actor (eg child actor hits an exception, gets according to parent actor's supervision strategy). I don't understand how to do this if the child actors are created dynamically (in response to a message), and need some dependency. Taking the example in my post, the client actor has a dependency (the client itself) that is supplied by it's supervisor. How does this work? Every example I've looked at shows a more simplified scenario where a single child actor is statically defined and has no dependencies, so I dunno how to make it work.
APS Payroll | API Engineer III | Shreveport, LA | REMOTE | Full Time | Competitive We are a payroll SaaS provider looking for a Senior-level scala engineer to help us build the next generation of our RESTful API services with the AKKA framework. We've not yet publicly posted this job, so you guys can have the first crack at it. Responsibilities include providing technical leadership for our RESTful API platform, mentoring junior engineers, and working with me, the engineering manager, on creating a roadmap to meet the needs of the applications that rely on our API. [company website](https://www.apspayroll.com/) Questions, or to apply: jcraft@apspayroll.com You can also PM for more info.
 That I can understand - it's very extensible so I do end up having to actually read much of the build to be sure I understand it. Especially task dependencies. You can definitely end up with a lot of time getting familiar with a build. I guess I'm just used to that coming from make/rake before entering into the scala/java world long ago. I think that's always going to be a problem with "build definitions that run code" rather than build definitions that describe data types to be interpreted. I realize that sbt does that internally, but with extensible tasks You can pretty much do anything and in any order. You can end up with that problem in Maven as well once you start adding enough plugins and build customizations, but there's tighter limitations on what you can do where, and the documentation is way more standardized, for sure.
Yeah, I think we're on the same page. What I'd emphasise - and what I think Maven's plugin model does very well - is that if you're going to run code as part of your build then that code should be first-class code: it needs unit tests, linters, code reviews, a versioning and release cycle and so on. This is a hobby-horse of mine that comes up in all sorts of areas - testing, building, deployment, user configuration - it's fine to have complex business logic if that's what you need, sometimes nothing else will do, but the appropriate place for complex business logic is first-class code written in the project's primary programming language.
&gt; The study of computation as a practical result of the von neumann architecture? Science. Most uni's that teach computer science teach it in such a way, there is then a subset of subjects (i.e. discrete math, category theory) which only focus on the "mathematical" parts of the degree.
I'm new to macros. I have a similar problem to this: https://stackoverflow.com/questions/36344046/scala-macro-inspect-tree-for-anonymous-function If there's a macro I can work with the AST of a lambda defined as the argument of the macro, but if the macro gets a lambda which is defined elsewhere, it only shows something like this: "Ident(TermName("f"))". Is there any way to get parents in the AST or somehow get a lambda if we only know the TermName(name) or some other way to solve this problem ?
A macro can only mess around with the AST body that was passed to it, not with random other parts of the program's AST - that would make them even crazier and more incomprehensible than they already are. What is it you're trying to achieve? Maybe you can make your macro apply some transformation to the lambda without needing access to the content of the lambda (e.g. you can do `f andThen myFunctionThatChangesWhatFDoes` without necessarily needing to know about the internals of `f`).
&gt; I assume for business logic cases in sbt builds we use a library tested with a testing framework, and/a plugin that depends on the library with usage in builds tested with scripted. If not, I make it so. You can of course achieve this in any build system, but in SBT (or really any build system that allows embedding arbitrary snippets of code in your build definition) you have to spend some of your discipline budget to do so. It's just very easy to put an untested throwaway line in the build that does something weird (e.g. run the tests with different parameters around DST change dates) that fixes an immediate issue, and then forget about it, and when these things are done directly in the build script you often don't have the linting/code review tools that would flag such bad practices if they happened in regular code. (I don't know what this "scripted" is and it's hard to search for). &gt; Testing with bats is always an option if we are really stuck with any buld, too. I guess, but I really don't want to be writing bash if I can help it - would far rather keep everything in Scala where I have all the tools/standards/practices/workflows already. &gt; Also, it's worth noting that really complex business logic is usually time for a full cli program, obviously tested in its own right, in which case a task that calls it with correct arguments and a scripted test to check the results is the correct way to go, if that program isn't used in multiple builds. A CLI program means falling back to a stringly typed interface. I'd far rather have a library (or if necessary then a service invoked via something like thrift). Of course I might also offer a CLI frontend onto that same library, but when doing a build step I'll use the library directly rather than flatten the parameters into strings only to immediately parse them again. &gt; I don't think merely using Maven ensures that is taking place at a client in builds, however It doesn't quite ensure that much, but it does ensure that any standards you apply to code also apply to build logic. E.g. if you have code coverage reporting, your build logic (in plugins, because maven doesn't let you put logic anywhere else) will automatically be included in those reports, because your plugin is just another code module. Of course in an organisation where the first-class code isn't well tested that will still be true of the build as well. &gt; I don't necessarily think that publishing a plugin for everything is the answer, especially when the plugin may not be re-used outside of my current build. I find that having a build-specific plugin works really well - the plugin is just another module in the project, you can keep the releasing and versioning in sync. (If we structure the logic as a library that the plugin then uses, as above, this is also nice: the library can be one module, the plugin another that depends on it, and then other modules in the project can use the plugin and/or the library as appropriate). And if and when the time comes to reuse it in another project, it's then very easy to pull a module out of a project and make it a full project in its own right (this is another thing I like about maven).
Thanks for the confirmation! We tried to make a dsl for a sort of weird use case. There are a number of plain objects in varied shapes with some properties, read from JSONs. And we wanted a function which only gets a certain kind of lambdas, like the following: &gt; has(_.someProperty) // so the lambda is always just: _.something And inside the 'has' function we need to access the property name and check if it's not null. (The null thing is needed because of some java compatibility) The initial solution was that the JSONs were read into Maps instead of classes. That way it was easy to access their names, but the class solution would be nicer. My only idea so far was that with a macro we could get the name after the last '.' . 
You might want to look at shapeless `LabelledGeneric`, which lets you access `case class`es generically as though they were records. (Or possibly even using records).
You could just use Spark Job Server to manage jobs. Then use JGraphT to handle the dependent management. 
This would likely work in conjunction with Spark Job Server. A long time ago, we worked on something that could be controlled using a series of SQL statements, but that project went the way-side. As a result, the idea never really disappeared, only the implementation. Creating a graph in memory is trivial, I probably won't need anything to handle dependencies - at least not to start. I want to keep it simple.
Yeah, I'm wanting to do something, and make it completely open source. What I've found is other companies have created these, but never released them. As a result, I think it's beneficial for everyone, so I might as well create one. :)
Yeah, I looked into Flowable a while back, but it was complete overkill. I want something very lightweight.
&gt; (I don't know what this "scripted" is and it's hard to search for). Scripted is the plugin built into sbt for testing sbt builds. [scripted](https://www.scala-sbt.org/1.0/docs/Testing-sbt-plugins.html) &gt; A CLI program means falling back to a stringly typed interface. I'd far rather have a library (or if necessary then a service invoked via something like thrift). Of course I might also offer a CLI frontend onto that same library... Sort of, but so is invoking mvn for actual business logic in a build, in the sense that xml properties and node values are strings that are not typechecked by a compiler. Maybe you aren't doing the parsing, but mvn certainly is. The plugin configuration documentation seems to be simply a set of rules for how arguments must be [serialized](https://maven.apache.org/guides/mini/guide-configuring-plugins.html) into xml in order for maven's parsing to work. Having never written a maven plugin in scala, I would assume that means that you must have a private var in a class in order for maven to deserialize the xml config into objects you can use. Is it possible to write maven plugins in scala? I know Josh Suereth had some stuff for mojo builds a long time ago, but haven't looked at it in a long time. If I'm using a cli app for something that does complex logic invoked by a build, using something like [scallop](https://github.com/scallop/scallop) certianly ensures that anything you pass to it can be deserialized into the types you intend or error. If using a library in an sbt build, you define a Setting[A] that the build step depends on, and that can be as strongly typed as you would like. So it seems like it's at least as type safe as plugin configurations without schema definitions, and with settings more type safe in the build definition. As for having a build specific plugin -- that's not that much different from having a library that is invoked in a build task, and you could still also have a plugin that does build tasks for you, just as you do in maven. I ask these questions, because although I can tell people to go read the sbt manual cover to cover, few people ever do. I'd love to move to Mill, but think that regardless of how simple the design is, maven users would have a problem with anything that isn't maven, so moving to maven to avoid ever having to explain sbt again at a client is fairly attractive. How is it for cross-building for different scala versions? The [FAQ](https://github.com/davidB/scala-maven-plugin/wiki/Frequently-Asked-Questions#cross-building-with-maven) seems kind of bleak, but doable. It just doesn't feel as simple as `+ cmd`. Forget about targeting scalajs and jvm in the same build, I can't imagine . It's little things like that that I feel the scala maven plugin should just handle (especially testing in both versions). The way that maven resolves dependencies is fundamentally at odds with the way that scala publishes binary incompatible artifacts, and repeating myself in several profiles in verbose XML doesn't sound like a lot of fun, either. I did come across [scalor](https://github.com/random-maven/scalor-maven-plugin), is that something that you use? My main argument for using sbt is that every scala library uses sbt, so if you have to pull a library down and look at it and compile it locally, you need to use sbt. That's easier if it isn't something you use once in a blue moon. But the benefits of using it (it's flexibility and type safety in configurations) are outweighed by the need to explain it to people new to it (nearly every person in the java world who hasn't worked in scala, and artifactory/nexus administrators at clients).
One of the more important things I would want this job to do is this: Consider the following scenario. You have a job that's made up of multiple sub-tasks. These tasks could include: Job 1 - task 1: import data from website - task 2: process data using csv or some other similar tool - task 3: load data into a database - task 4: process data, retrieve specific fields, and calculate result - task 5: take calculated results and store in a database Now, let's say tasks 1, 2, and 3 take about 10-15 minutes to run. This means, each time you run the job, you can be guaranteed about 10-15 minutes of runtime. And let's say something in task 4 failed after tasks 1-3 ran. Task 3 is a very simple task. It's perhaps 30 lines of code. But now, to debug what's going on, you have to re-run tasks 1-3 and watch the logs for task 4. Let's further say that task 4 is assigned a payload ID as data moves down the line. So, task 1 has a payload ID, task 2, task, 3, and so on. Let's say that task 4 failed, and its payload ID was 195. Well, now I can recompile the task, and re-submit a request to run my job from task 4, with payload ID 195. Now it runs, and you see lots more debugging, but for some reason, it still fails, but only in a distributed setting. So, now, tweak the job setting so it runs on a single partition, resubmit the job, run task 4, payload 195. And on and on. You can see, now, how powerful a debugging tool this could be. Yes, an argument could be made that you can "run the task from the command line and watch it." But you won't have the same distribution available, and payload mechanism that this server provides. There's a lot to be said about being able to run a job at a certain point with a known dataset. For us, it would save many hours of debugging and testing, and would gain productivity back.
&gt; Is it possible to write maven plugins in scala? I *think* there's also a way to write the "mojo" (entry point with its magic javadoc annotations for where config gets received etc.) in Scala but I tended to just write that one class in Java that then delegated immediately to Scala for the logic. The part that's painful about mixing Java and Scala is having to deal with nulls, Java collections etc., and you'd still have to do that if you wrote your mojo in Scala. (Given that all the data you get is config from the user that you want to validate/sanity-check anyway, it's not too bad to have to do that stuff at the same time) &gt; If I'm using a cli app for something that does complex logic invoked by a build, using something like scallop certianly ensures that anything you pass to it can be deserialized into the types you intend or error. True, but IME it's still quite easy to have cases where a value doesn't quite roundtrip properly - e.g. if one string in a list contains spaces, it gets split into two strings in the list. Or an `$` in a string value gets accidentally substituted by the shell. Nothing you can't fix when you hit it, but just a bunch of tedious cases, especially if the build needs to be cross-platform. &gt; So it seems like it's at least as type safe as plugin configurations without schema definitions I think maven must autogenerate schema definitions from the magic javadoc annotations and package them with the plugin or something, because I've never seen a maven plugin where that documentation wasn't accessible in the IDE when configuring it. &gt; and with settings more type safe in the build definition. Yeah agreed. Being able to specify plugin settings in Scala proper is nice and I do miss it. &gt; How is it for cross-building for different scala versions? Bad enough that I generally don't bother. Scalor will hopefully fix that (and also include cross-building to scalajs), but I wouldn't consider it production-quality yet (though they were very responsive and helpful with my bug reports, and it looks like development is proceeding rapidly). It's worth saying that `sbt +` doesn't solve cross-building in general, only for scala versions. If you look at SBT projects that have to cross-build against multiple versions of scalaz, or against scalaz and cats, or multiple versions of some other library, their config is every bit as bad as a project that cross-builds with maven. But yeah if you're in that middle-ground of publishing a library that has to be compatible with old versions of scala but doesn't have to be compatible with old versions of foundational libraries it depends on then SBT works nicely and I have used it for projects that are in that situation.
Check out Apache airflow. Jenkins and other CI tools do this too. You might also check out What's available with Finagle, Akka or Play for handling asynchronous tasks if you want any of this to be event or request driven within a service.
I think this belongs here: https://www.reddit.com/r/scala/comments/8axynb/who_is_hiring_monthly_rscala_job_postings_thread/
Ah, my bad. Didn't know about that
Interestingly I have a need for a similar tool - executing a dependency graph (https://blogs.ncl.ac.uk/andreymokhov/graphs-in-disguise/). In contrast, my domain is related to data analytics and machine learning so context transferring could be done by storing the intermediate results in HDFS. Also, I need to give the user an ability to define a job using other (more primitive) jobs and job combinators similar to those used in Akka Stream API (https://doc.akka.io/docs/akka/2.5/stream/stream-flows-and-basics.html#operator-fusion). Also, I have PoC implementation that uses RabbitMQ (via AMQP) to store job definitions and execute them in async manner occupying all of the available resources in a cluster. Plan to use Akka Cluster later (to get rid of RabbitMQ dependency). Sadly I am unable to share that PoC work (because it's ugly) but would be interested to discuss and to see other ideas which try to solve similar problem 
Hi, We open sourced Orchestra (https://github.com/drivetribe/orchestra) recently, which is a job engine running on Kubernetes. The transfer of context can be done with return result of jobs, they just need to have a Circe Encoder/Decoder. We've been using this for all our DevOps tasks, it replaced ou Jenkins CI/CD and Ansible playbooks. Let me know if you need any help with it, I'm happy to help. Cheers
I haven't used it with Finatra, but I used Kamon to get New Relic reporting from a scala app that used (scalaz) async and it worked nicely. Kamon apparently has support for Finagle futures as well, so that ought to work correctly.
When you've sufficiently pulled out enough hair by its roots after learning about Akka's latency and RabbitMQ frustrations, please let me know. We abandoned Akka a _long_ time ago, and never looked back.
But thank you, this is _very_ close to what I want.
**Fallacies of distributed computing** The fallacies of distributed computing are a set of assertions made by L Peter Deutsch and others at Sun Microsystems describing false assumptions that programmers new to distributed applications invariably make. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Could you shed any more detail? Perhaps some pseudo code and an example job graph? 
asynchronous programming is a fad that would be best off dying. Most people aren't writing services where the performance difference between blocking and non-blocking matters. Are you writing a service that executes individual requests in microseconds, and does the majority of its work by performing network I/O operations against other services? If not, blocking code is going to perform fine for you. And, no, most finagle/finatra services I've seen aren't operating at that level of performance. On the other hand, writing synchronous code really *is* much easier. For those cases where you need to do things concurrently, it's *still* easier to write this code in a blocking style. Java/Scala programmers are generally wasting their time and money shifting to this paradigm. Maybe if the JVM gets continuations and fibers, then it may be worth revisiting, but at the same time those VM/runtime features usually come with a performance cost as well.
I wouldn't go as far as MercurialHacked and say that asynchronous programming is a fad, but it's definitely overused, especially in the Scala world (actors are probably the worst offender). Asynchronous code using futures or actors is orders of magnitude slower and more memory hungry than plain Scala code, and if the database is the bottleneck an asynchronous system will not scale better, just waste more CPU cycles. So before deciding on an asynchronous system make sure that what you lose in code simplicity and performance is compensated for in terms of scalability.
It’s a part of Play example projects.
Thanks. I will look in the example you provided. I don't use Play framework though.
I'm going to have to disagree about simplicity. Code wise it's usually just working with a Future/Task wrapping your value which is pretty straightforward. As for performance, check out scalaz 8 IO or Cats new IO. They are crushing a tonne of benchmarks in this space.
I know about macwire. I did some research on DI and finally selected Guice.... bitter sweet decision but I am in prod and it works. My only problem with that is that its very complex and something I have so spend hours just troubleshooting Guice.
I know Ariframe[https://github.com/wvlet/airframe] and Macwire. However I'm just using light cake pattern (learned from Macwire's Guide to DI in Scala)
Not really. Tech trends find interesting trends. Europe is Scala's home turf but elsewhere if people aren't seeking out Scala diligently it's easy to shift your focus elsewhere to a lang with a local user group, and even docs in Chinese. 
I think this thread has gone sideways. I was not asking for "generic" DI approaches for Scala. I was very specifically asking for Dagger2. 
There certainly is a warmup period. A year or so ago, I followed some prior art at a new company and used play+akka. It probably took a month or two before everything passing Future (and learning having to do things like Future.sequence or .flatMap(identity) to deal with odd nestings) started to really click. But then it seemed natural enough. And definitely our play apis can scale to handle many many more requests than e.g. django or rails apps we also have in our stack, esp when we run into network slowness (a bunch of microservices calling other microservices) or slow db queries.
&gt; asynchronous programming is a fad that would be best off dying. Most people aren't writing services where the performance difference between blocking and non-blocking matters. Are you writing a service that executes individual requests in microseconds of CPU time, and does the majority of its work by performing network I/O operations against other services? Actually, most people are writing these services. 90%+ of webservers written are IO bound and describe the situation you present.
+1for Macwire. It's really solid
Wait... Why the f*** do you need DI in Scala ? Companion object should be enough for that.
I currently like to just have components take all their dependencies as implicit parameters and leave the wiring to implicit resolution. 
One issue with this approach is that all low-level dependencies will "bubble-up" to the top and in a way break encapsulation
https://github.com/paradoxical-io/finatra-extensions Also a fork of the original swagger you linked. I contributed a few times to it before I pulled it into here and added support for value class wrappers with swagger too. It’s available on maven central
Humble beginnings
great find OP, I like what Odersky did with this pizza :D
I should have written, it "might" bubble-up, depending on how you approach this. If you don't bubble-up dependencies you still have issues you need to consider: Let's start with class C1(implicit db: DB) { def doIt = // use the db } Then when you want to use C1 in C2 you need to pass db: class C2(implicit db: DB) { def doIt2 = new C1 } Not so good, we shouldn't have to know about C1 using a DB. This is mostly this style I was complaining about. Let's try to improve it and let's pass C1 instead: class C2(implicit c1: C1) { def doIt2 = // use C1 } This is still a problem because C2 has access to a concrete C1 using a database, maybe with methods like `def closeDatabase`. And everytime you change C1 for implementation reasons, you have to recompile C2 so we'd better use an interface: trait C1 { def doIt: Int } class C1Db(implicit db: DB) extends C1 { def doIt: Int = // use the db } class C2(implicit c1: C1Db) and you wire everything with object Main { implicit val db: Db = new Db implicit val c1: C1 = new C1Db implicit val c2: C2 = new C2 def main(args: Array[String]) = c2.doIt2 } Now for testing you have to rewrite a similar object with *all* the dependencies: object Test { implicit val c1: C1 = new MockC1 implicit val c2: C2 = new C2 def main(args: Array[String]) = c2.doIt2 } It is possible but not very DRY. Maybe you can think of putting some implicit definitions in a object to reuse them: object C1dependencies { implicit val db: Db = new Db implicit val c1: C1 = new C1Db } But if you put C1 in a library how does this compose with other components using the same Db: object C3dependencies { implicit val db: Db = new Db implicit val c3: C3 = new C3Db } Now you cannot import both objects if you need a C1 component and a C3 component: import C1Dependencies._ import C3Dependencies._ You have other issues as well. What if you want to use a Db for one component using C1 but an in-memory persistence for another component using C1? With implicits you are left with one implementation per type. In summary I think that using implicits to define dependencies is not such a bad idea as long as you use interfaces. However it brings some other challenges when you want to make your modules more independent with easy ways to wire them as you wish when they come from different libraries.
Actually, this is not true at all in my experience, and I have 10+ years experience writing backend servers across multiple companies and software stacks. Most people *think* their service is I/O bound because that's what they've been taught to believe. No one actually profiles their service well enough to find bottlenecks. This is especially true in the cloud world of AWS, GCP, etc. Bandwidth for inter-host communication between servers in the same zone/region is cheap, fast, and plentiful, while the performance penalties of running in a hypervisor make services even more CPU-bound than normal.
I'd love to see some benchmarks comparing scalaz 8 IO to something imperative using netty 4 directly, for instance. I will continue to be downvoted for expressing an unpopular opinion, but hopefully I am at least exposing more people to the idea that the async PR they've been given isn't necessarily true. See for instance: https://www.techempower.com/benchmarks/#section=data-r15&amp;hw=ph&amp;test=plaintext Finagle/Finatra are at 30% of the throughput of a raw netty4-based solution. I doubt scalaz IO is going to be that much better.
A few things: &gt; class C2(implicit c1: C1Db) I guess you meant `class C2(implicit c1: C1)`? &gt; It is possible but not very DRY I don't find this violates DRY. DRY doesn't mean "don't write the same code multiple times" but rather "don't write the same code multiples times if the semantics is rather to do one thing multiple times". Building an object/dependency graph is not violating DRY here, but maybe I got you wrong? 
I think you meant to reply to the parent. 
Indeed, sorry
&gt; 20 years later, Java has generics Java generics came almost straight from Pizza: "In 1998, Gilad Bracha, Martin Odersky, David Stoutamire and Philip Wadler created Generic Java, an extension to the Java language to support generic types. Generic Java was incorporated in Java with the addition of wildcards." https://en.wikipedia.org/wiki/Generics_in_Java
&gt; Two Ways to Bake Your Pizza (1766) Over 250 years later, still the same. Pizza or Foccacia? To pineapple or not to pineapple?
It's been working pretty well for me in both simple and large projects, so I still don't believe you need an external DI library to achieve this :) Given those 3 algebras, you can have the following implementation (and here's: class CatalogProgram[F[_]]( repo: CatalogRepository[F] )(implicit F: Sync[F], L: Logging[F], A: Authentication[F] ) extends CatalogService[F] Regarding dependencies substitution I guess you mean to have different implementations / interpreters of these algebras, for example for testing purposes. You could test the `CatalogProgram` by having dummy implementations of Logging and Authentication implicitly if you only care about the catalog program's logic or pass the implicit dependencies explicitly, for example when you want to test how the authentication logic affects your catalog program's logic. Here's a [gist](https://gist.github.com/gvolpe/149b1df55e47564e680f7d9a345d3953) with this example showing a possible implementation (using `Cats Effect`) cause you know Reddit's code parser is not really good ;)
I think this wouldn't be a problem if both `C1` and `C2` are defined as tagless final algebras. Let me re-write it in this way: // -- algebras trait C1[F[_]] { def foo: F[Foo] } trait C2[F[_]] { def fooBar(foo: Foo): F[(Foo, Bar)] } trait DB[F[_]] // -- programs class C2Program[F[_]: Monad](c1: C1[F]) extends C2[F] { // No notion of DB here override def fooBar: F[(Foo, Bar)] = for { foo &lt;- c1.foo rs &lt;- c2.fooBar(foo -&gt; Bar()) } yield rs } class C1Program[F[_]](implicit db: DB[F]) { override def foo: F[Foo] = db.getFoo // some logic here } // -- your app object Main { implicit db: DB[IO] = yourRealDB val c1 = new C1Program[IO] // implicit resolution for DB val c2 = new C2Program[IO](c1) // explicit parameter, no DB needed c2.fooBar.unsafeRunSync } As shown above the `C2Program` can make use of `C1` (and any other algebra you might need) without having to know about the use of `DB`. That comes down to the edge of your program when you glue all the components together. For testing, you would only need to have an `implicit val testDB` in scope for all the tests needing access to it, including testing `C1Program`.
I have implemented a persistence layer for my [Patterns](https://github.com/Sciss/Patterns) library, an adaptation of SuperCollider patterns, that will be available in the computer music system SoundProcesses. Now the streams created from patterns can have their state preserved, which I think will open a lot of new possibilities for the compositional process. (that's work in progress) Meanwhile, I realised I don't really like editing Scala code in my system (with its front-end Mellite), because the Scala editor is just too weak. Once again, I looked into alternatives of getting strong editing capabilities for Scala. The situation still seems desolate if you want anything beyond syntax highlight. So I warmed up again to the idea of utilising the Scala plug-in for IntelliJ. After looking into the IDEA-CE source code, it became obvious that there is simply no chance to create a stripped down "library" that would provide the editor. So I decided to give the opposite direction a go, see if I could instantiate Mellite from within IntelliJ. It basically seems to be possible, although I anticipate a lot of problems, e.g. my app uses its own database persistence and transactions which would need bridging to IntelliJ's state model, plus the windowing system is fundamentally different (docking vs. individual frames). Here's my proof of concept that the app can be embedded: https://files.gitter.im/JetBrains/intellij-scala/3Woc/Screenshot-from-2018-04-15-13-38-13m.png The big next question is if and how the Scala editor could be used from my program.
When is 2.12.6 hitting maven? The milestone has been finished.
I have actually benchmarked this, and this is not the case. Internetwork communication may be fast, but it's not faster than local CPU instructions. Most apps, especially ones that serve websites, are IO bound. 
Interesting. Let me share with you some fun profiling experiences I have had * An app can't exceed 500 rps because it is using log4j 1.x, which has terrible synchronization problems * An app can't exceed 2000 rps because of an internal metrics reporting library used by thousands of apps across the company. When I asked a principal engineer about it he said most apps have low rps per host. * An app regressed from 4000 RPS, CPU bound, to 2000 RPS because someone swapped the use of a guava cache with a cache that serialized to native code. The reason was to avoid heap pressure as Java gcs don't handle long lived data well, but for our service guava worked just fine and had way better throughput. * An app is cpu bound because it uses hibernate, which is way way slower than jdbc, and here's a hint, it's not because hibernate was doing extra I/O operations. * An app is cpu bound because it makes multiple copies of the request body throughout the lifetime of the request. This is a very common performance killer in automatically memory managed runtimes. People get sloppy with memory allocations because they can. I wonder where you are hosting your servers, what kind of services you are writing, and what your code hot paths look like where you actually see request rates bottlenecked on a saturated NIC. 
We have an akka-http service under docker (we use the hseeberger/sbt image as base) and the service throughput is essentially the same as native from what we could see (at least under test loads).
Well yes, if you are using crappy frameworks (spring/hibernate) which are all blocking along with JDBC, you are going to have these problems. FYI, we use none of these libraries, if you are going to use async, you need to do it all the way down. Stop using outdated java libraries and start using libraries which are built to be async first.
Have you tried it without the flags ? I never did any official benchmarks, but have run several things on the JVM and didn't notice a slow down ( not to say there wasnt one ).
I still don't trust that you've actually profiled an app and saw bottlenecks at the network layer. The 8000 RPS app *was* nonblocking all the way through. I know because I wrote it initially as blocking and then rewrote it as blocking. This was my first experience learning first hand that non blocking code throughout is almost identical to blocking throughput in many scenarios. Look at the techempower benchmarks involving json serialization: https://www.techempower.com/benchmarks/#section=data-r15&amp;hw=ph&amp;test=json Int this benchmark the difference between the top performing frameworks is relatively small, and the blocking frameworks actually outperform the non blocking ones! For instance the servlet one, which is tomcat via blocking API, outperforms all the nonblocking frameworks.
Well duh, these benchmarks are misleading. You are benchmarking JSON serialisation, which is CPU bound, not network bound. Of course there isn't going to be a big difference. To be clear, there are certain things that are CPU bound, these are * Anything network based (database requests, HTTP calls, RPC, SOAP etc etc) * File operations (most OS's provide async file operations, although this story with Linux is a bit sketchy) Anything else is CPU bound, i.e. typical CPU bound tasks are * Hashing (passwords, cryptography, security) * Serialization/deserialization (this includes JSON) * Client side algorithms that are **NOT** done on the database If you want to do a proper benchmark, you need to benchmark some operation which is async, i.e. typically database requests or http calls (which is a large chunk of your logic). Providing a benchmark of JSON serialisation is the completely wrong benchmark to show anything Also FYI, good IO frameworks provide a way to distinguish between CPU bound and IO bound operations, i.e. if you look at Monix (see https://monix.io/docs/2x/execution/scheduler.html#execution-model), you see there are multiple execution models. If you want to fine tune performance, you use asynchronous execution for completely IO bound tasks, synchronous execution for CPU bound and batched if you aren't entirely sure. BTW, this is one of the reasons why node.js is popular (and I think the only valid reason why they are popular). node.js managed to target the niche area of async programming where it mattered, backend web servers, and they also targeted the biggest issue with async programming in general, i.e. if you do async, it needs to be **async everywhere**. Also to be clear, I am not dismissing what your personal benchmarks may be showing. It may be the case that your apps are actually CPU bound, not IO bound. The thing is once you start mentioning frameworks like hibernate/spring/JDBC, then your entire argument gets thrown out of the window because none of these frameworks are async. If you want to be completely async (and really care about performance in this area), I would look at the following frameworks (catered for Scala obv) - https://github.com/twitter/finatra (akka-htttp also works if you use it in a very light manner) - https://github.com/getquill/quill (if you use postgres, make sure you use the postgres-async driver since JDBC is blocking, as mentioned before) - For logging, you can use logback but make sure you use it in async mode. Also if you are using it in async mode, avoid using MDC since this has a bad locking mechanism (uses synchronized on an entire map) Finally, make sure you use an IO type that is heavily tested and benchmarked for these scenarios. The only one I can really recommend right now is Monix 2.x (https://github.com/monix/monix), 3.x is slightly slower but its still not released yet. Twitter Future is also really fast. There is also this Future implementation here http://trane.io/ Scala's own `Future` unfortunately isn't that fast, but that is because its mainly optimized for fairness rather than throughput, although Viktor is doing good work to improve its performance (which should show in Scala 2.13).
Do you have any CPU restriction on kubernetes? The company I work for is using and we are having no problems so far. Flags that we use: `-J-XX:+UnlockExperimentalVMOptions -J-XX:+UseCGroupMemoryLimitForHeap -J-XX:MaxRAMFraction=1`
What web server are you writing that doesn't do some kind of serialization, whether it's JSON-based, thrift-based, or some other format? How much CPU do you think your database client driver uses or doesn't use? What are you are saying reinforces my point and makes it airtight. Your average web application is doing so much that is CPU-intensive, just from deserializing request and serializing responses alone, that the performance difference between a non-blocking and blocking matter is a matter of less than 10% throughput. If you are writing high-level functional code on the JVM then you've already admitted that you care far more about developer productivity than you do performance. Why then do you chain yourself to a paradigm, non-blocking code, that is basically proven to be significantly harder to write than blocking code? And node.js is a complete travesty of the backend, useful only because frontend developers don't want to educate themselves on anything but javascript. With node.js you are again coding in a paradigm that is way harding than blocking code, and you're doing it on a VM and dynamic language whose performance is far worse than you'd get on the JVM. So you are again at the worst of both worlds; a super-difficult-to-use programming model combined with a super-slow runtime.
Ive been trying out scalaFX by building a small gui to encypt plain text using AES. Still figuring it all out but FX does look much nicer than swing imho! https://github.com/RobertLemmens/AES-Encrypter
&gt; What web server are you writing that doesn't do some kind of serialization, whether it's JSON-based, thrift-based, or some other format? The point was that the benchmark was only benchmarking JSON serialization and nothing else. Most webservers do not just serialize static JSON data, they also do database/http requests &gt; If you are writing high-level functional code on the JVM then you've already admitted that you care far more about developer productivity than you do performance. Why then do you chain yourself to a paradigm, non-blocking code, that is basically proven to be significantly harder to write than blocking code? This has nothing to do with JVM and everything to do with the fact that Java is an incredibly unexpressive language that doesn't have a proper language construct for async code/concurrency (apart from event based driven UI frameworks which isn't a good abstraction for web servers). So yes, I agree with you here. If you have an app written in Java style with Java frameworks that are blocking by default, then your problem is actually Java. &gt; If you want to do non-blocking code in a way that doesn't affect developer productivity, then you need some form of delimited continuations baked into the runtime like with what GoLang has. Otherwise, you are for the most part making your life harder for no reason, unless you are writing a high-throughput app in low-level C/C++, something like it, or else Java code that is very imperative and C-like. I was excited long ago when I read that Scala had some support for delimited continuations, but, like most things in Scala, it was a research project abandoned by its researcher and never turned into anything of value. We'll have to put our hopes into project loom, I guess: Or you can use the solutions I posted before, it doesn't have to be backed into the JVM. The problem here is that Java is so inexpressive that the language byitself can't represent async computations in an expressive manner. Scala (and other languages such as Haskell) have been doing this for ages with `Future`/`IO`/`Task` types + `for`/`do` comprehension (which gets rid of callback hell) &gt; The sad thing is, most people are trying to pre-maturely optimize their code by cuffing themselves to an inferior programming model but without having done any benchmarking to show how they might be benefitting from it. Chances are, they aren't benefitting at all, just making their lives harder. No, the sad thing is that Java is an inexpressive language that can't represent these constructs. As it was stated in the quote you posted, using asynchronous programming (for the things that it should be used for) is much more performant, but the language can't program in it because its not expressive enough &gt; Again, the fastest jvm-based framework, jooby, is running under a blocking API. Check out this beautiful, blocking, JDBC-backed code. JDBC by the way still appears to be the fastest way for a jvm-based app to talk to most databases.... False, try using the driver I told you about before and compare the differences. Heck, even the getquill framework provides both a JDBC and the async driver, so you can compare the results directly. Note that if you use HikariCP, this is basically bolting on a reactive/evented system ontop of a blocking system, you will still get faster results using an async system (such as postgres async) though.
I don't know what else to say here, you admitted yourself (indirectly) that async non blocking is faster for IO bound but that its hard to express in Java because it kills developer productivity. This also has nothing to do with monads, it was just an example. Go has channels, Javascript has callbacks (and Future/Promise), Ruby/Crystal has fibers. &gt; Also if you think composing async operations with monads, cofree, or whatever category theory construct is the current fad in the typelevel community, is as easy as writing blocking code, then I'm afraid we will never agree. I am sorry, but you can't have your cake and eat it. You made a statement earlier that async code was worth in performance, this was shown to be not entirely correct. If you want to improve your codes performance and blocking IO is the bottleneck, then you need to do async programming. Then you need to do a cost benefit anaylsis vs productivity, and if you do code in Java, then yes your cost benefit analysis is going to be bad. This just means that you accept this problem, or you stop coding in Java, or you code it in Java while hurting developer productivity. And actually, if you are dealing with purely functional languages, there is (almost) no difference between blocking and non blocking code because in both cases you code against type classes that have the same implementation in both cases. You mentioned stacktraces, which means you should stop relying on exceptions. In this style of programming you treat errors as values and hence you stop throwing exceptions (which means you don't deal with stack traces) &gt; Haskell lost a long time ago, and there are precious few people who think using Scala as a worse Haskell is good idea in any decent sized software organization. I don't know what you expect writing such statements in the Scala subreddit. I mean your entire argument is on a presumption of writing Java, not Scala (tbh it sounds like you are a Java programmer) and the point I am making is there are solutions to this problem in Scala (which you ignore for what personally appears to be personal reasons). Honestly the real elephant in the room here is the fact that Java is severely lacking in async non blocking programming, and almost every modern language has an ideal solution, whether that may be IO/Task/Future/callbacks/actors/fibers/channels etc etc.
You misstate my point. My point is most people aren't writing code that is performant enough where non blocking I/o makes a significant performance difference, whereas on the other hand it significantly lowers developer productivity As an example, if you are writing high level functional code on the jvm, then the perf difference between blocking ans non blocking styles will be virtually non existent.
Im using the exact same flags. In order for autoscaling to work in kubernetes i've got to set a CPU restriction. I've tried setting 1, 2 and 4 cores with 256m, 512m, 1G and 3G memory restrictions. The total increase in IO matches the increased resources but never matches the old production numbers. It may be something in my config for the service.
I've tried it with -Xmm and -Xms vs the -XX:+ UseCGroupMemoryLimitForHeap flag. Using wrk didn't show any increase in throughput.
It looks like he uses the same docker image with an earlier version of java. I've been using sbt assembly to build a fat jar and running in the raw java container with java -jar so it should be the same.
I'll probably be creating something for this based on my needs. I have a project that I've had in mind for a while, called Scattersphere. As a result, I've created the project on http://www.github.com/KenSuenobu/Scattersphere Feel free to take a look and follow it. I should have drawings and such up soon, and should have some feedback from a work colleague regarding some job/task scenarios for the design. Hope to have something design in the next few weeks.
/u/volpegabriel and /u/valenterry. I am objecting to building large object graphs manually over and over, especially when I just want to tweak one little thing. It is definitely possible but for me: - it is not DRY because I am repeating almost the same code - as a result it gets in the way of refactoring because if I change the structure of my graph I have to change it everywhere in my tests where I have declared a slightly different graph - it makes it longer to write some integration tests where I want to mock some part of the system - it gets in the way of experimenting because if I want to try out a new combination (to chase integration bugs or get a better understanding of system performance - "what if authentication was instantaneous?"), it takes me longer to write the code I value the last 2 points because I observe that we tend to write less tests (or much bigger ones) and we test less hypothesis when there is more friction in doing so. 
Very nice and a good proposal. I find it really annoying to have `.attempt` return just a plain `IO` which could still contain an error. I have one remark though (explaining it on IO). In the beginning we just had IO which could contain an error or no error. With the new approach we now have two "types". The old IO which can contain an error or no error and the new IO, which never contains an error. But there is still one case missing: what if we *know* for sure, that there is an error? E.g. even with this new approch `raiseError` will give us a type that says `I may or may not contain an error`, but it definitely contains an error. We know that for sure but it is still not explicit. I cannot really think of many usecases yet, other than `IO[A].raiseError(...).unsafeRunSync` should not return `A` but directly give the error because we know it *must* fail. Maybe it is worth to model this explicitly, maybe the added complexity makes it not worth it. However, this new approach still feels to be missing something if we really want to make all the things we know for sure to be explicit at compiletime.
I disagree. The code is only WET (not DRY) if you repeat to write the build graph again and again, but why would you do that? It's a graph so you can easily reuse any part of the and plug together what you need. So, you also don't have to change the tests if you reuse parts of the dependency graph *unless* you changes also change the semantics of the test. In that case it is *good* that you have to change the tests to make it compile again. Same goes for integration tests - there is no difference to unit tests really. Let me make it more concrete. If you have `class X(a:A, b:B, c:C, ...)` and you always use different combinations of dependencies in your tests with mostly some defaults, just use a factory function. `def apply(a:A = defaultA, b:B = defaultB, ...): X` and then in you tests do `X(a = mockA).testSomething` or `X(b = mockB).testSomething` and so on. Just because we are talking about components with depencies, there is really no difference to other "normal code".
A known error is just an `E` - you could define `type H[A] = E` but I'm not sure why you ever would? Presumably the only reason you'd ever want to call `raiseError` is to "forget" that the value is definitely an error, just like the only reason you ever call `Left(x)` is to forget that `x` is definitely an `A` and treat it as an `Either[A, B]`.
&gt; You misstate my point. My point is most people aren't writing code that is performant enough where non blocking I/O makes a significant performance difference, whereas on the other hand it significantly lowers developer productivity For example, if you are writing high level functional code on the JVM, then the perf difference between blocking and non blocking styles will likely be virtually non existent. Performance is just a metric like any other, which you need to balance depending on what your scaling requirements are. If you have a web application that is taking significant load (and you are facing basically what is the 10k problem https://en.wikipedia.org/wiki/C10k_problem) then making your code non blocking **is the answer**. If you don't have this problem, then yes its a premature optimization, but this falls into the same bucket as "coding your web server in C/C++ when high level languages like Java/Scala are good enough". The thing is, if you are getting this amount of load at this scale then making your code non blocking will easily save more AWS costs then the developer time taken to fix the problem. If done properly in non blocking style, a single machine can handle multiples of load more then using blocking style. Heck I have seen single machine node.js applications with the same business logic handle as much node as 4/5 JVM blocking style (with spring/hibernate/JDBC) just due to thread starvation. This is also very easy to verify when using something like gatling.io &gt; For example, if you are writing high level functional code on the JVM, then the perf difference between blocking and non blocking styles will likely be virtually non existent. If you are getting 10 requests a second, then sure (but in this case you could code your webserver in python/ruby and you are clearly not even caring about performance). If you are getting 5k+ requests a second, then this isn't the case. &gt; Your point is valid though. If you are already using something like monads everywhere to track multiple kinds of effects, then what's another effect on your effect stack going to matter in terms of productivity? I personally think effect tracking though higher rank types in Scala is an evolutionary dead end, and IIRC the Scala team agrees and is still looking for alternative effect-tracking solutions in Dotty, but obviously many in this subreddit disagree. Its not a dead end because other languages (including really famous ones, i.e. Javascript) do the same. Its still the case that in Javascript, the most reliable way to work with evented code is Future/Promise, and yes people in Javascript when using these libraries are doing monadic composition, they just don't know and its not tracked by a type system Also I have translated completely blocking code to non blocking and its not as a big deal as you make it out to be (assuming you are not doing it in Java). Yes it took some time, but with Scala's own language constructs (main one being `for` comprehension) the process was quite mechanical. Initially we just used Scala's own `Future` inside for comprehension, and then later on we iterated.
I think `FailedIO[A]` is not the same as `E`, because it still has to be run to know the concrete instance of `E`. At least if I got it right what you mean by `E`.
In theory it makes sense, though I don't think it's worth the complexity as you say :)
&gt; I think FailedIO[A] is not the same as E, because it still has to be run to know the concrete instance of E. Well, at the point where you're calling `raiseError` then you have the concrete `E` already. If you want an IO action that you can run to get an `E`, isn't that just `G[E]` (or `F[E]` if it might return some other kind of error)? What's the use case where you'd actually want this `FailedIO[A]`? &gt; So when it comes to Either, we also have 3 types: Either, Right and Left Many people regard the fact that `Right` and `Left` are types as a flaw in the language (similarly the fact that `Some` and `None` are types), because a `Right[A, B]` is just a pointless wrapper around a `B`. `Either` should be the type and `Right` and `Left` should just be constructors/functions that inject into that type, not types in their own right.
Yeah I think that's pretty spot on, `IO` is isomorphic to `UIO[Either[Throwable, A]]`, this type class allows us to reason about that relationship with a bias to the right side. We usually don't need to care about the left side.
In my experience there is usually a default implementation for each interface which gets used in production which often depends on some configuration. You can put that in the companion object: trait Bar case class BarConfig() class BarImpl(implicit config: BarConfig) extends Bar object Bar { implicit def default(implicit config: BarConfig) = new BarImpl } trait Baz case class BazConfig() class BazImpl(implicit config: BazConfig) extends Baz object Baz { implicit def default(implicit config: BazConfig) = new BazImpl } trait Foo case class FooConfig() class FooImpl(implicit bar: Bar, baz: Baz, config: FooConfig) extends Foo object Foo { implicit def default(implicit bar: Bar, baz: Baz, config: FooConfig) = new FooImpl } You can then have several configurations: case class ApplicationConfig(_fooConfig: FooConfig, _barConfig: BarConfig, _bazConfig: BazConfig) { implicit val fooConfig = _fooConfig implicit val barConfig = _barConfig implicit val bazConfig = _bazConfig } object ApplicationConfig { val test: ApplicationConfig = ??? val prod: ApplicationConfig = ??? } And when you want to change a component's configuration for a test, you can just put it into the implicit scope: class IntegrationTest { val config = ApplicationConfig.test import config.{barConfig =&gt; _, _} implicit val testBarConfig = BarConfig() val testFoo = Foo.default } or we could exchange the whole component: class IntegrationTest2 { val config = ApplicationConfig.test import config._ implicit val testBar = new Bar {} val testFoo = Foo.default } I haven't used this in a large application yet, but I think this should work nicely. 
I'm curious how you decide whether to take a dependency explicitly or implicitly. Why is `CatalogRepository[F]` explicit but `Logging` and `Authentication` are implicit?
&gt; If done properly in non blocking style, a single machine can handle multiples of load more then using blocking style. This is what I don't think you've measured. My already-optimized blocking 4000 RPS service saw maybe a 20% throughput increase after switching from blocking to non-blocking paradigms. Had I not already heavily optimized it I believe the throughput increase would have been ~10% or less. The techempower benchmarks I shared 100% agree with me. You can argue they are measuring the wrong thing, but the numbers they report bear out what I have seen in production and practice. &gt; Heck I have seen single machine node.js applications with the same business logic handle as much node as 4/5 JVM blocking style (with spring/hibernate/JDBC) just due to thread starvation. I don't believe you actually figured out what the JVM apps were bottlenecked on. I guarantee you it wasn't thread starvation. It was likely CPU usage or lock contention, or you didn't configure your thread pools correctly. I hate hibernate and Spring as much as the next guy, but let's not blame performance problems on blocking paradigms. &gt; Its still the case that in Javascript, the most reliable way to work with evented code is Future/Promise It might help you if you read this article: http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/. It is written by a dart developer and explains how futures/promises don't fundamentally make writing blocking code much easier. And, I think the easiest way to write non-blocking code in javascript as of late is generators, not futures. 
I tried but I couldn't make it work so far. However it indeed seems like shapeless should be able to solve this problem, so I'll learn more about it and try again. So far shapeless seems to be the weirdest part of scala, but maybe it will change once I understand it more. 
Hello there! I currently got a bug for more than two weeks that I decided to spend a hours to delve into the root cause. I shortened my code to this and I can't understand why Scala is doing this, though I have some idea: val test = { if(1 == 1) List("A") else { List("B") } ++: List("C", "D") } Is returning A, and: val test = { (if(1 == 1) List("A") else { List("B") }) ++: List("C", "D") } Is returning A,C,D. (I also used +: and works the same, using scala 2.10.5, in console line) Anyone with a clear explanation would be welcome. Thanks. 
Putting performance aside, as I tried to argue in the article (maybe unsuccessfully), the async/Future-style *does have* important benefits. One example mentioned there is working with computations which happen in parallel and manipulating their results. The code to do that is much easier and readable (at least for me) than the synchronous version. In general, the ability to get a "handle" on the computation - represent the computation as a value - is where you get all the extra power from. That said, also as I mentioned in the article, Kotlin coroutines or scala-async since we are on the scala reddit do bring readability improvements in some cases. 
As I replied to MercurialHacked (https://www.reddit.com/r/scala/comments/8c011r/synchronous_or_asynchronous_and_why_wrestle_with/dxg9rym/), the main benefits for me are outside of performance - which is an important factor as well, but not the only one. Though I would definitely challenge the "orders of magnitude slower" claim.
why ?
Ok, I'll bite :) Parallell execution is a different subject than scalability of I/O-bound applications. It can be implemented efficiently if you use work stealing and if your algorithm can easily be divided up in rather big work tasks. We're talking hundreds or thousands of CPU-cycles per task for process local stuff (for parallell execution over the network we're talking about millions of CPU-cycles per task), otherwise the overhead will outweight the benefits. Just randomly wrapping CPU-intensive code in futures and mapping over these will not in general increase your overall performance. But the benefits of increased parallellism is also reduced if your CPU-cores are already busy doing other work. So, for example if you run a CPU-bound web server with as many threads as available CPU-cores and keep them busy handling client request there will be basically zero benefit in parallelising you otherwise sequential code (you will rather lose CPU-cycles to increased thread synchronization).
&gt; Yes I am aware of this. The biggest problem is actually exceptions I very much disagree with this. And one reason I can do so confidently is that I have written significant amounts of both blocking and non-blocking code in C, a language with no exceptions, and blocking code is waaay easier. When you write non-blocking code you are coding continuation-passing style. Your callback receives the continuation to the async operation. Hand-writing CPS code is waaaaay way harder than writing it in a blocking, synchronous style. You can try to make it look slightly less harder by using monads, which are actually very similar and have a strong relationship to CPS (https://stackoverflow.com/questions/4525919/continuation-passing-style-vs-monads), but you're still fundamentally much worse off than you would be writing synchronous style.
I think we're talking about different things - I'm talking about benefits *other* than performance, and the fact that they do exist and in some use-cases significantly improve the readability/maintainability etc. of the code. Of course, you need to be careful where you insert async boundaries, no question. But that's again back to the performance discussion.
*if* expect 2 expressions one after *if*, one after *else*, you gave it 2 expressions. { List("B") } ++: List("C", "D") is a valid expression by itself. It looked confusing to me too, but if you think about it, there's no reason Scala should consider your expression finished if you don't specifically mark it somehow. And *()* is a way to mark that inside the braces there is one expression, which is what you did in the second case. 
Yeah, I suspected that, but what's the second expressions it expects then? In Java, the { and } is sufficient to delimitate the *if*, and ++: being an operation, it seems weird to me that it take it has another expressions for the *if*.
This seems to work if there is a direct dependency between `Foo` and `Bar` but if you have a dependency chain `Foo -&gt; Bar -&gt; Boo` and you want to switch `Boo` how does that work? It seems to me that the implicit for `Bar` will look for an implicit for `Boo` in the companion object for `Boo` and will not retrieve the local definition in the test.
It seems to me that factory functions with default values help but are still quite verbose. If you have a dependency graph A -&gt; B -&gt; D -&gt; C -&gt; D and you want to replace `D` you need to write: val d1 = makeD val a: A = makeA(makeB(d = d1), makeC(d = d1)) If your graph gets deeper and wider, the method calls become more complex.
Shapeless is a fiddle to use - it's a way of hacking in things that really ought to just be language features IMO. The underlying concepts are very clean and nice, but the implementation can get very hairy, which shows in things like poor error messages when you get something wrong.
Sorry, I don't understand your graph. Do you mean `A(B(D), C(D))`? In that case you would usually want to have `A(MockB, MockC)` right? If you really want to Have `A(B(MockD), C(MockD))` then yes, what you do is correct. But you only need to do it once as in `bWithMockD = makeB(d = mockD); cWithMockD = makeC(d = mockD)` and now you can also do `X(bWithMockD, cWithMockD)` and you are done. No need to create the same thing again and again. You only do it once and then use it whereever you need it.
delete
I think you are right.
Ah ok. Finally understood your first post. Thanks a lot. But this seems pretty hard to adapt coming from Java, damn. 
This one is super general and vague. I've been looking into the fundamental FP structures (Functor, Monad, etc) and for the most part, they're either straightforward, or there are lots of good examples of how to apply them. But I'm having trouble seeing where Applicative Functors fit in. What does `apply` get me that `map` doesn't, and what is a common use for it? It seems to me that both `map` and `apply` let me take an `F[A] -&gt; F[B]` if I have some way to go from `A -&gt; B`. Where do I find a `F[A -&gt; B]`, for example? I can easily imagine having an `A -&gt; B` that I want to use, and then I suppose I could lift it into my functor with `unit`, but then, why wouldn't I just use `map`?
Does shapeless have anything like a type constructor list? I've been looking, but I've only been able to find hlist
Tbh I pretty confused about this myself but have you seen scalaz Validations/cats Validated?
Hi, has anyone been able to get Ensime + VSCode Scala Language Server extension to work as shown in the readme? I’ve done the following: - Set JDK_HOME correctly in both Windows environment and Git Bash - Run `sbt ensimeConfig` successfully on my project - Started VSCode in my project riot directory so that the `.ensime` file is in the project root I’m seeing the following issues: - Hover tooltip message is consistently ‘Loading...’ - Jump to definition doesn’t work However, I am seeing error messages prefixed with `[Scala]`. So it looks like errors from the presentation compiler are showing up, but not the messages from Ensime. Would appreciate any thoughts on this.
Can you tell me how your serving docker in production. I have a sneaking suspicion that its kubernetes related.
Can you tell me how your serving docker in production. I have a sneaking suspicion that its kubernetes related.
I hope it's ok to quote here since the book has been available for free, but this snippet from [Scala with Cats](https://underscore.io/books/scala-with-cats/) might be useful: &gt;This demonstrates the classic trade-off of power (in the mathematical sense) versus constraint. The more constraints we place on a data type, the more guarantees we have about its behaviour, but the fewer behaviours we can model. &gt;Monads happen to be a sweet spot in this trade-off. They are flexible enough to model a wide range of behaviours and restrictive enough to give strong guarantees about those behaviours. However, there are situations where monads aren’t the right tool for the job. Sometimes we want Thai food, and burritos just won’t satisfy. &gt;Whereas monads impose a strict sequencing on the computations they model, applicatives and semigroupals impose no such restriction. This puts them in a different sweet spot in the hierarchy. We can use them to represent classes of parallel / independent computations that monads cannot. &gt;We choose our semantics by choosing our data structures. If we choose a monad, we get strict sequencing. If we choose an applicative, we lose the ability to flatMap. This is the trade-off enforced by the consistency laws. So choose your types carefully!
Oh, not Kubernetes. N-dockers in the same instance (depends on the service and what colocation we might require), behind nginx. Maybe is the Kubernetes load balancer/ingress system?
My preferred way of understanding and explaining Apply is as a Functor with product operation `(F[A], F[B]) =&gt; F[(A, B)]`. Applicative is that + `pure`, the most useful thing ever :) I haven't used a `&lt;*&gt;` operator, but I imagine it might be useful if I say have a List of functions and a List of values and I need to apply every function to every value.
Ah, yeah, I just read that `apply` can be derived from `product` and `map`: I'll have to work that out on paper. I can definitely see how `product` is useful.
Looking at the type of the Functor `fmap`: (A =&gt; B) =&gt; F[A] =&gt; F[B] And at the type of the Applicative `liftA2`: (A =&gt; B =&gt; C) =&gt; F[A] =&gt; F[B] =&gt; F[C] You can see that they are related in a way - in an alternative history, they might have been named `lift1` and `lift2`, respectively lifting a function of one argument and a function of two arguments to the functor level. Another way to think of it is that `fmap` is the generalization of the list `map` function, and `liftA2` is the generalization of list `zipWith` function. Similarly, monads can be thought of as the generalization of the list `flatten` function, with `o.flatMap(f) = o.map(f).flatten`. For `Futures`, the `Monad` instance is for sequential operations, while the `Applicative` instance is for operations that may run in parallel.
If you think of monads as giving you sequential ordering, i.e. 'first carry out this action, then carry out that action, then finally return that result', then applicative functors give you parallelism: 'carry out this action, and at the same time carry out that action, and finally return the result once both actions are done'.
Resolved! Here's some SMTLIB to solve GI, which can be backed out into scala pretty easily. This intsantiation tries to find an isomorphism between a graph that has one node being the root, and two children in a complete graph. ```scheme (declare-fun I!3 () Int) (declare-fun I!2 () Int) (declare-fun I!1 () Int) (declare-fun I!0 () Int) (declare-fun I!6 () Int) (declare-fun I!5 () Int) (declare-fun I!4 () Int) (declare-fun adj-g1-!7 (Int Int) Bool) (declare-fun adj-g2-!8 (Int Int) Bool) (declare-fun inj!9 (Int) Int) (declare-fun invinj!10 (Int) Int) (assert (distinct I!4 I!5 I!6 I!0 I!1 I!2 I!3)) (assert (=&gt; (adj-g1-!7 I!4 I!5) true)) (assert (=&gt; (adj-g1-!7 I!4 I!6) true)) (assert (=&gt; (adj-g2-!8 I!2 I!3) true)) (assert (=&gt; (adj-g2-!8 I!2 I!0) true)) (assert (=&gt; (adj-g2-!8 I!3 I!0) true)) (assert (=&gt; (adj-g2-!8 I!0 I!3) true)) (assert (=&gt; (adj-g2-!8 I!2 I!1) true)) (assert (=&gt; (adj-g2-!8 I!0 I!2) true)) (assert (=&gt; (adj-g2-!8 I!3 I!1) true)) (assert (=&gt; (adj-g2-!8 I!1 I!3) true)) (assert (=&gt; (adj-g2-!8 I!1 I!2) true)) (assert (=&gt; (adj-g2-!8 I!0 I!1) true)) (assert (=&gt; (adj-g2-!8 I!1 I!0) true)) (assert (=&gt; (adj-g2-!8 I!3 I!2) true)) (assert (forall ((x!1 Int) (x!2 Int)) (=&gt; (adj-g1-!7 x!2 x!1) (adj-g2-!8 (inj!9 x!2) (inj!9 x!1))))) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!0 true) I!1 true) I!2 true))) (= (select (store a!1 I!3 true) (inj!9 I!4)) true))) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!4 true) I!5 true) I!6 true))) (= (select a!1 (invinj!10 I!0)) true))) (assert (= (inj!9 (invinj!10 I!0)) I!0)) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!4 true) I!5 true) I!6 true))) (= (select a!1 (invinj!10 I!1)) true))) (assert (= (inj!9 (invinj!10 I!1)) I!1)) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!4 true) I!5 true) I!6 true))) (= (select a!1 (invinj!10 I!2)) true))) (assert (= (inj!9 (invinj!10 I!2)) I!2)) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!4 true) I!5 true) I!6 true))) (= (select a!1 (invinj!10 I!3)) true))) (assert (= (inj!9 (invinj!10 I!3)) I!3)) (assert (= (invinj!10 (inj!9 I!4)) I!4)) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!0 true) I!1 true) I!2 true))) (= (select (store a!1 I!3 true) (inj!9 I!5)) true))) (assert (= (invinj!10 (inj!9 I!5)) I!5)) (assert (let ((a!1 (store (store (store ((as const (Array Int Bool)) false) I!0 true) I!1 true) I!2 true))) (= (select (store a!1 I!3 true) (inj!9 I!6)) true))) (assert (= (invinj!10 (inj!9 I!6)) I!6)) ```
We use scala in production in a docker container, also from the hseeberger/sbt image as base. No performance drop, by packaging it as a fatjar and then running with java -jar. Try debugging performance with perf, see if you're making a ton of syscalls that docker is choking on.
In algebra and also referencing the Spire library, if multiplication of alike types is defined by the typeclass `MultiplicativeSemigroup[A]` and multiplication of unalike types yielding one of the same types is defined by `Module[A,V]`, what is the name for multiplication of unalike types yielding a third (Out) type? As you would use for say, Voltage = Current * Resistance, or Mass = Volume * Density. 
Good idea, ill use perf tomorrow.
Yes, I see that, and I agree your example using the executor service is not pretty. However, when the application is mainly CPU-bound it's perfectly fine to just write sequential code (with minimal amount of thread synchronization) and have multiple, mostly independent threads run concurrently. I would argue that that code will be very simple, scale very well and run at pretty much optimal speed.
We are using DC/OS \(Mesos \+ Marathon\)
I find it easier to define applicative in terms of the function `(A, B), C -&gt; (F[A], F[B]) -&gt; F[C]` which is variously called `ap2`, `apply2`, `map2` or `liftA2`. It's essentially a two-argument version of `map`: it gives you something stronger than what functor does (you can combine two effectful values) but weaker than what monad does (because the second effectful value, the `F[B]`, can't depend on the `A` like it could for a monadic `flatMap`). A good example is `Validation`. The idea here is that rather than fail-fast like `Either`, `Validation` should give you either success or *all* your validation errors. So you're not allowed to have two validations that depend on each other, because if the first validation fails then you have no way of knowing whether the second validation would succeed or not. But you can have a validation that's built out of smaller validations as long as they're "parallel", e.g. if you have `case class User(address: Address, phoneNumber: PhoneNumber)` then you can form a `Validation[MyError, User]` from a `Validation[MyError, Address]` and a `Validation[MyError, PhoneNumber]` by doing something like `(validatedAddress, validatedPhoneNumber).mapN { (address, phoneNumber) =&gt; User(address, phoneNumber)}`. A slightly silly example is the constant type `Const[B]#O` (defined as: `sealed trait Const[B]{ final type O[A] = B}` i.e. `Const[Int]#O[A]` is always `Int`, whatever type `A` is) for some `B: Monoid`, e.g. `Int`. You can form an `Applicative` for this type which just adds up all the values using the `Monoid` instance, but you can't form a `Monad`, since you would have to implement `flatMap` for e.g. `Const[Int]#O[String], String =&gt; Const[Int]#O[Float] =&gt; Const[Int]#O[Float]`, but you have no way to "get a `String` out" to pass into the function, because your `Const[Int]#O[String]` is just an `Int`. Applicative's "parallel" nature also makes it useful for future-like types where you want to run operations in parallel and then combine them. Since you're never allowed to have one result depend on each other in an effectful way, you have the full structure of the computation available from the start, so `Applicative` is also more suitable to some kinds of "analyze and then run the computation" operations.
You might find it useful to try working in a functional style in Java, so that you're not changing style and language at the same time. Basically you avoid having control flow as such: all `if`s must have `else`s, you only ever use `return` at the end of a function, you replace loops with transformations and recursion. Alternately you could try writing Scala in a non-functional style to start with - just write the same style of code you'd write in Java, until you get used to the syntax. In that case the only reason you'd ever use `if`/`else` like this is to replace a `?:` construct in Java, and in those cases just remember to always put brackets around it: `a ? b : c` translates into Scala as `(if(a) b else c)`.
&gt; What's possible and what's not, what are the tricky bits (perhaps closures)?. How difficult would it be? Everything is possible but horrendously difficult, and even if you get something that works it probably won't work in future versions of Scala. Macros should be an absolute last resort. &gt; The things flowing through the function graph then need to all be Expression[A] or containers thereof, and not plain old Case Classes, which is what I initially envisaged. I might be able to neaten or hide this, but think I'd have to abandon regular scala functions and define some lovely new operator like =====&gt;&gt;&gt;&gt;&gt; to do so, which I'm not that enthusiastic about. Any suggestions here? See whether you can leverage existing functionality and operators, e.g. if you can implement your `Expression` as an Applicative or Monad then you can make use of all of Cats' helper functions and existing syntax. If you need visibility over *all* operations then yes everything will have to be custom, I think, but if you can tolerate some pure `Scala` functions in your graph and only need a few particular nodes to be "optimizable" then maybe something like Free Applicative can help.
I am seeing the same. FWIW, since the scala tooling team has decided to roll their own lsp, and nobody stepped forward to maintain the ensime-lsp integration,(see this discussion - https://github.com/ensime/ensime-server/issues/1918) ensime-lsp has been removed from ensime. If you want vscode integration that does more than the official one currently does, you are probably going to have to either wait or start contributing to the projects to move them forward. Otherwise, you are going to have to use intellij. FWIW - learning emacs enough to be productive in it will take you about a month of practice for an hour at a time at night, and using it with ensime is pretty straightforward, even if you have to kill the server process when it hangs.
In practice, Apply is the typeclass that is useful in most application code. Especially when used with `.mapN` (Option(5), Option(5), Option(5)).mapN((a, b, c) =&gt; a + b +c) // returns Option(15) This means that in order to combine any Functors that have an Apply, you don't need to do this: Option(5).flatMap{ a =&gt; Option(5).flatMap{ b =&gt; Option(5).map{ c =&gt; a + b + c } } } Which is quite a bit more tedious to type. In addition, mapN allows its arguments to be evaluated independently, while the functor/monad map/flatmap chain requires the first argument to evaluate before the second and third, etc. For Applicative, it allows you to write generic code, using `.pure` to lift a value into the applicative, since Functor doesn't provide a way to get a value into the F to start: def mapGenerically[F[_]: Applicative, A](a: A)(f: A =&gt; B): F[B] = implicitly[Applicative[F]].pure(a).map(f) mapGenerically[List](1)(_ + 1) // List(2) mapGenerically[Option](1)(_ + 1) // Option(2) You need to be able to put a value into an Applicative fairly often. It's used in sequence to turn F[G[A]] into G[F[A]], for example: def traverse[G[_], A, B](fa: List[A])(f: A =&gt; G[B])(implicit G: Applicative[G]): G[List[B]] = foldRight[A, G[List[B]]](fa, Always(G.pure(List.empty))){ (a, lglb) =&gt; G.map2Eval(f(a), lglb)(_ :: _) }.value def sequence[G[_]: Applicative, A](fga: F[G[A]]): G[F[A]] = traverse(fga)(ga =&gt; ga) This is usefull when traversing a list of functors, and you want a functor of a list, among other things. 
&gt;I still don't trust that you've actually profiled an app and saw bottlenecks at the network layer. This confuses me. The story I hear about why async is preferred is that it prevents you spinning up a thread for each inbound and outbound request, where each thread eats about 1MB of stack space. In that sense, apps that are hitting scaling difficulties because of blocking IO are actually hitting a memory problem as the number of threads eats all the heap space. This is a problem I have definitely seen before. Your reference to "bottlenecks at the network layer" seems to suggest that NIO approaches are meant to literally improve the number of TCP connections that can be held? Because that's a new one for me.
I replied to a few of your comments downstream, but then decided it'd be better to just write a single one. Sorry if I ended up spamming your mailbox. I'm a little confused as to what exactly you're saying, and your conversation with mdedetrich hasn't done a ton to clear things up. It sounds like you're talking about throughput in regard to blocking vs non-blocking IO? As in, when you talk about "performance differences" you're discussing how much work a given logical thread (which would also be a physical thread, in the blocking case) can get done in a certain amount of time? That may well be true, but I think it misses the actual arguments for NIO approaches. The first is scale - every thread requires 1MB of stack space by default, and with enough inbound and/or outbound requests that can blow your heap. You might argue that most services just don't need to handle that level of scale, and I might agree with you, but it's important to acknowledge. And even if we switch to throughput, that a logical thread isn't as fast as a physical thread doesn't necessarily mean that throughput is slower. The ability to parallelize work has to be treated seriously, especially when dealing with heavily IO bound applications where parallelism is "unlimited" (in contrast to CPU bound applications, where you run out of cores eventually). I agree that blocking code is much nicer to work with, and often argue for that in my professional life, but in order to do that effectively you have to understand why smart people often use async approaches. I've personally seen both situations described above (scale problems because of thread count and throughput issues relieved by parallel IO), so they're not just myths or hypotheticals.
kind-projector is not a sbt-plugin. It's a scalac plugin.
I appreciate that you are interested in understanding my viewpoint. So for performance I mostly focused on throughput, which is how much work your app can get done on a given piece of hardware, which these days is always multicore. So when I talk about throughput I mean something like X requests handled per second on Y CPU cores on some generation of CPU. The argument that non-blocking I/O scales better is true to some extent. The reason for this is I believe that Linux's thread scheduler is O(log(n)), so it gets slower the more threads are contending for CPU, and on top of that I imagine that context switching becomes more frequent. I don' t know for certain, I've never used a tool like perf to try to analyze that. The reason it often doesn't matter, is that most web servers in Java that expose a blocking API, nowadays, actually manage the underlying connections with non-blocking I/O, which means you can write code in blocking style and still receive the same scalability benefits. How does this work? Tomcat is the example I'm most familiar with so I'll use that. In tomcat's default configuration, it has a separate thread pool, called the NIO worker pool, that manages all HTTP connections. When a request comes in, this pool manages executing the I/O operations to receive the request bytes and also handles parsing the bytes into a valid HTTP request representation in Java memory. Once it has done so, it tosses the HTTP request over to a separate thread pool in accordance with the servlet API spec,. The HTTP request handling is done in this separate user thread pool and is allowed to block. Once the request handler returns, its response is tossed back to the NIO thread pool, where the response is then converted into bytes and sent using NIO operations. Using the above model, a tomcat server can handle 10k connections just fine, since those connections are mostly idle of they are comet or long-lived HTTP connections, whereas when a connection becomes temporarily busy the application logic is handled on a blocking thread pool. This technique scales beyond Tomcat. We are using a similar approach with Finagle/Finatra right now where we let the web server use NIO to handle HTTP requests and responses, but where we handle the actual business logic in a separate blocking thread. There is virtually no throughput or scalability difference, but the app is worlds easier to code.
&gt;The argument that non-blocking I/O scales better is true to some extent. The reason for this is I believe that Linux's thread scheduler is O(log(n)), so it gets slower the more threads are contending for CPU, and on top of that I imagine that context switching becomes more frequent. I don' t know for certain, I've never used a tool like perf to try to analyze that. I've seen this batted around, but surprisingly I haven't found it to be accurate. Saturating your CPU might be a problem, but the overhead of context switching is less than you might expect. Now, I'm not confident enough to declare that context switching is *never* the reason to use an async approach, but in my domain the strongest argument is still the memory cost of using lots of physical threads to handle IO. &gt;Using the above model, a tomcat server can handle 10k connections just fine, since those connections are mostly idle, whereas when a connection becomes temporarily busy the application logic is handled on a blocking thread pool. Yeah, NIO Tomcat adaptors are pretty nice. But I don't think this addresses the async usecase unless your application's business logic is basically a pure function, with no IO wait time. The second your application starts making outbound IO calls (even to a datastore), you suddenly have a blocking thread pool that's piling up waiting threads. I guess my point is that while some function `doSomeBusinessLogic()` should of course not be async (what would be the point?) `callThirtyEndpointsInParallelThenWriteToPostgres()` is much more likely to be. Otherwise, we're talking about 31 blocking threads (one for each outbound call, one for the server's thread) or one blocking thread and zero parallelism. That's the case where async IO begins to show advantages in both handling scale and increasing throughput.
Indeed, I'll fix that, thanks.
This is great!
So I'm a developer and I'm learning Scala on the side and I'm trying to understand what exactly sbt is. Is it possible to separate Scala from sbt, because I could only install Scala with sbt which was a little strage. It's a build tool I get that, but it's also a package manger? Am I understanding that correctly? I'm working through a tutorial of Scala and I'm seeing: sbt new scala/scalatest-example.g8 What exactly are .g8 files? I didn't find anything sufficient through Google. Also, why *wouldn't* I use sbt if sbt is build tool that scala provides? Thanks! 
Thanks for the info, I’ve set up Emacs now and it’s working fairly well. I’ve used Emacs before, but I was hoping to standardise all my dev work on VSCode. I’ll file an issue with the VSCode Scala extension about the removal of LSP support from Ensime. At the very least we might be able to add a disclaimer to the readme.
You have a point, but in practice I've found that services tend to make calls to other downstream services very quickly. I've also found in practice when profiling these services that they are CPU bound such that non-blocking I/O provides little benefit. YMMV, but this is at least my experience. Another way to look at it is, imagine you didn't set active request limits on your non-blocking service and you accepted 5000 in flight requests on a host with 4 cores, all blocked on a super slow downstream service. If that downstream service suddenly starts returning values, your service suddenly gets completely overloaded trying to finish all 5000 requests. In both non-blocking and blocking worlds, you need to set limits on both maximum number of in-flight requests you allow and maximum number of requests you'll allow yourself to queue. I've found in practice again that non-blocking doesn't buy you much here in the kinds of limits you can set.
Just a small correction you stated that `Cons` takes a `Cons` as it's tail, but it actually takes a `List` as it's tail.
Thanks, these are helpful pointers that give me more confidence in the direction I was going. I've looked a little at cats.Apply/Applicative (esp. map2 and map2Eval look useful) and also Free for separating Expression and Interpretation/Evaluation. I'll have to slug through understanding those more.
Great article, I enjoyed reading it and just retweeted it! PS: As I went through it I noticed some small typos and mistakes you might want to fix :) &gt; all three notations does the same do &gt; As said, the kind-projector is simply a sbt plugin scalac plugin is the answer as you fixed it at the beginning of the article :) &gt; Here, we need more parameter, a Comonad, to be able to run the Free[S[_], A] one more parameter? parameters? &gt; With lefty, it would resolved [T, U]InferToLeft.V[Tuple3, Boolean, T, U] &lt;=&gt; [T, U](U, T, Boolean), NOT our definition! would have
Thanks TinBryn. I have corrected it.
Side question: Has anyone measured the performance / flexibility of the [sbt-docker](https://github.com/marcuslonnberg/sbt-docker) plugin versus using the [scala-sbt](https://hub.docker.com/r/hseeberger/scala-sbt/tags) base image? We use the plugin for building our runtime docker images after sbt-assembly builds our fat-jar. In theory using sbt-docker instead of scala-sbt should let you a) not bundle sbt within your docker, b) use the [slim](https://hub.docker.com/r/library/openjdk/tags/) java images, and c) upgrade to jdk versions beyond jdk8. I'd guess it would be faster to build and smaller to download but I don't know if there would be other performance differences.
I'm not sure I know what you mean. Are you saying that you have lines that can be either "2_label2_1" or "2_label2_2", and that you want to split them by the last character? In that case you could do something like this... val Array(one,two) = scala.io.Source.fromFile(filename).getLines.groupBy(x =&gt; -x(x.length-1)).values.toArray Where one is an Array of the lines that end in one and two the lines that end in two. Probably not the best way but if your file is relatively short then this should work well enough.
Sorry for those typos. :( I will fix them asap, thanks!
Just allowing multiple type parameter lists - like we allow multiple value parameter lists - would replace almost all uses of kind-projector, and make the language more consistent in general.
A workaround would be creating a `withFilter` method annotated as `@compileTimeOnly`
Well, we have always used the "fat" sbt docker image (and we rebuild the application on docker build as a last "test" step to be sure the container is going to be as correct as we can tell easily) but for local purposes we can override the building (saves a ton of time) and use an image that just copies the jar into the docker and runs it (no building). I would expect to be no performance gain either way (aside from the building phase) since they should all use the same jdk and AFAIK there are no weird settings in either base image
Technically it's possible to use only the Scalac compiler, but SBT is much simpler. It can also manage your dependencies, handle multiple projects, it can package your app, you can even add other tasks to it (like code formatting, db migration or anything you want). It can do quite a lot that's why it's somewhat complicated, but the basic features are simple (compile, run, test, update dependencies). There are alternative build tools, most of them are listed in the r/scala sidebar. g8 files are project templates, so you can initialise a project easily with sbt new. So if you want to make a basic empty scala project it can save you a little time, but if you want to make something like a play or http4s project it saves you more time. It's usually listed at the start of the documentation if a library/framework has a g8 template. 
For dividing a collection into two parts, you can also use `.partition(predicate)`. 
Excellent! Tysvm for investing here.
It's not as though Scala Native was started as a project with ignorant of what Graal and Truffle has been up to. It's really not a replacement. Scala Native still offers a way to build scala programs without a GC, which Graal does not offer. It also provides a way to express C structs and primitives in Scala, which Graal does not. In the case of the linker and whole program optimizer, it's has still needed on the JVM even without graal, it's also needed in Scala.js still. &gt; As a bonus GraalVM also makes polyglot interop with languages like C++ and Rust much easier than was previously possible. That was a pretty easy thing to before using the LLVM toolchain, you just had to leave the JVM runtime behind. Now you don't have drop the JVM with the Graal toolchain, however there's no shortage of people who want to use Scala without the JVM none the less.
While I like the idea behind Graal, I think you are proposing is dangerous. We do not know how long it will take for Graal to achieve its goals. We do not know when companies will consider it production ready. Whole program optimizer - that I am certain - will do a better job then even AST-based optimizer for Graal. It will supports non-Graal JVM, Scala.js and Scala native. IMHO it would only result in a similar issues like with game developers and graphic drivers developers: game devs want to optimize things, but drivers are doing smart things, so developers try to guess what driver will do and write the game they predict driver will try to opimize things, then driver devs, try to guess how game devs changed their flow... As a result instead of one, solid, predictable driver you get game-specific hacks by NVidia and AMD, than no new game can benefit from unless they cooperate with corporation to add new hacks for them. Except here Scala compiler devs would have to cooperate with Oracle. GraalVM has a fuck up in optimization? You cannot fix anything, only wait for next update. Then you could end up with situation where GraalVM opitimizes wrong way, but Scala is blamed for the result. If with time Graal prove to be reliable enough, they might use their experience to use Graal for achieving their goals. Right now it is JUST a hype and at best Scala team can draft a PoC for Graal, but not support it officially.
I would be very wary. Rumour has it that Oracle, the company behind Graal, has just taken an app off the App Store [for having the word Javascript](https://twitter.com/BrendanEich/status/986605049987002368) in the title because Oracle owns the trademark on Javascript. The trademark is believed to be not defensible in this case, but good luck fighting Oracle's lawyers... Who knows what other shit they will pull once you rely on Graal. Oracle has no morals.
&gt; We do not know how long it will take for Graal to achieve its goals. Which goals are you refering to? It already supports the features I mentioned. &gt; We do not know when companies will consider it production ready. One of the biggest Scala users in the world, Twitter, already considers it production ready. &gt; Whole program optimizer - that I am certain - will do a better job then even AST-based optimizer for Graal. Possibly, but then we need to see some benchmark numbers to back up that claim. And also why not improve the existing Graal optimizer instead of spending time on a Scala-specific whole program optimizer? &gt; It will supports non-Graal JVM, Scala.js and Scala native. Scala.js is a short term advantage, yes. But I would be surprised if Graal won't have a WASM backend in the near future. That will beat the performance of any Scala.js optimizer. &gt; GraalVM has a fuck up in optimization? You cannot fix anything, only wait for next update. Then you could end up with situation where GraalVM opitimizes wrong way, but Scala is blamed for the result. It's an open source project, you can contribute patches. Of course it can take some time before a fix is released but that's true for any open source project. &gt; Right now it is JUST a hype Eh? They just released v1.0 and it's part of the JDK release. What part is hype?
I think we agree on most important points - especially that async style is a pain to code. Futures, reactive streams, and functional programming do a lot to make it a lot less painful but it's still not as simple as direct style. You also have to worry about "does this block" and what thread pool you're using if it does, which is always a headache. When it comes to backpressure I'm a little surprised to see you say that non-blocking approaches aren't particularly helpful. The only fluid, cross-application backpressure-aware pattern I'm aware of is reactive streaming, which while it's not necessarily required to be non-blocking is almost always implemented that way. The Reactive Manifesto explicitly calls out that particular solution to the concerns it raises (IIRC), and it's often a reason why frameworks go NIO (see Spring 5 adding support for NIO to allow for reactive streaming over HTTP). That's a long way of saying yes, it's totally the case that mindlessly increasing throughput (whether directly or by increasing scale) can end up saturating downstream hosts and eating all your gains. But that's kinda true for almost all increases in throughput, isn't it? And I think it makes the point that NIO approaches can increase your throughput and/or scale for certain classes of problem - even if that's not always what you want. :p I really do think this is because of different domains - you're talking about CPU bound apps where NIO is obviously not going to have a lot of benefit. Meanwhile, I'm sitting here trying to find any application that gets above 1% CPU usage for application work or 5% CPU usage for the whole JVM (eg. including garbage collection). In the kind of environment where all your application code really does is make an IO call (split about evenly between datastore queries, calls to internal apps, and calls to external APIs) and wait around for the results, not eating a whole MB of memory during that wait can be really nice. And while you can parallelize in the blocking model it's often a pain that involves configuring and injecting a blocking thread pool and suffering N * 1MB of memory overhead. There's definitely a faddish vibe to some of this stuff (make your app's config parsing non-blocking and reactive! :p) but I hope I've made clear that there are places where it has value. Whether it's worth the complications...well, that's another question. Good choice of framework is absolutely key to making it work (Spring 5, Play, Vert.X, etc.) Wow, that's a real essay I wrote. Sorry about that. :p
&gt; Scala Native still offers a way to build scala programs without a GC, which Graal does not offer. It also provides a way to express C structs and primitives in Scala, which Graal does not. But Graal provides interop with C++ and Rust so it must have support for these features already. It's just a matter of exposing them in Scala.
whats the usecase for something like this?
Hey thanks for reaching out. This project aims to make it very easy to deploy Spark computations as fault tolerant web services. Now if you use Spark, you can flip one line of code and deploy your large batch map reduce job as as either a streaming application or a web service. We have also integrated deep learning into spark so now you can easily deploy deep networks as web services for machine learning apps. 
&gt; To me it seems like it's time to scrap the Scala whole program optimizer, linker and Scala native projects, and instead focus on contributing to the Graal project. Scraping these projects is a little bit over the top. While they might not be useful for you, these projects are doing fine and ate useful for a lot of people. But of course you are totally free to contribute to GraalVM, e.g. provide better integration with Scala. A plug-in for sbt could be a start (or cbt or mill). Some competition between the different Scala projects is always good. Even the scalaz / cats schism sparked some nice projects.
I am running my dev env with GraalVM since yesterday to see how it works. * as JVM it is slightly faster than OpenJDK8 - :+1: * when I run AOT optimization, it blows up in my face - for me it means that the tech still need to mature As for what one do and one do not consider production ready... When I will be able to use all of that features without modifying my code, and a lot of overenthusiastic people try it out on production and describe all the pain points and pitfalls on blog posts and presentations... then I will consider it production ready. The part of the hype is when you expect core devs, that already heavily invested into some damn good research, to throw it all away the day after RC of an unproved technology is released. I worked at Opera, where the reasons for going away from their own Presto engine to basing their browser on Chromium were exactly the same as you mentioned. Why spend time on doing something, someone else is already doing. Result? * product got few years behind the competition as programmers had to relearn everything, change all their habits and reimplement basic features, * instead of focusing of only their own stuff they had to react to external changes, how Google rewritten things, refactored and so on, * while it is OSS and it should *make you just post a patch* - there is a process for posting a patch, it might be a length one. And you are forced to spend some resources all the time for keeping codebase in sync with upstream, * your model is no longer your own. Actually each time you go astray from original, you introduce dirty hacks for removing unused things, and adding own things, * at some point you might be able to get back to where you were earlier on... when it comes to development speed. But frustrated users moved on to the greener pastures. While all devs slaved like dogs to get it back to work, users haven't noticed anything of it. They just saw that something they used to use lost half of its features and needed years to recover some (not all) of them. I might have worked as a (costly!) business decision for Opera, but I wouldn't like to see it happen to Scala. Perhaps one day Scala will indeed move into Graal, but it should be done carefully, one step at a time, without abandoning current ways of doing things until it certain that most of users already adopted alternative.
Anybody know if Graal will work with wasm? That would interest me a lot for Scala, and wasm is declaredly not going to happen with Scala native.
Literally the day after I replied, this was opened: https://github.com/ensime/ensime-server/issues/1935 So some people are still talking about working ensime-lsp. I guess if you are interested yippy should keep a watch on that issue and maybe help contribute. 
As far as scala -&gt; wasm is concerned, it is worth noting gc is still some time away before landing in wasm. And so in the case of scala would require shipping a compiled gc (possibly not competing performancewise with js). See also http://teavm.org/
We're already heavily reliant on Java. How is Graal any worse?
Is this meant to replace HotSpot?
Thank you, I never had a good mental model of how call by name worked in Scala (I've only been looking at it for a few weeks), but your article made it click. It's a function that takes no arguments and returns the value.
seriously. graal is even under the same license as openjdk, and built into some versions of openjdk. the original post is just fear mongering
so is java...
In the long term, [yes](http://openjdk.java.net/projects/metropolis). It's already included for testing in JDK 9.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://openjdk.java.net/projects/metropolis) - Previous text "yes" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Well it wasn't and they never got the whole Java, Sun was smart enough to give the most to the community before being acquired. Oracle bought Sun, started a patent war, closed source the TCK, Gosling resigned, etc... Are you actually saying that you are happy with what Oracle did to Java? Luckily for us they couldn't take control of everything...
Now, it makes be sad, since Monix Task that I use looks the slowest. Hopefully Scalaz 8 is released soon, and Monix optimization will resume (because it will have still target to benchmark against).
you realize that graalvm is under the same license that java is right?
Huh, I didn't realise. Never mind then :)
It's just an indirection to separate the execution from random generation to avoid noise on the benchmarks. They compare the results exactly as it'd happen in a real-world scenario where someone converts a Task/IO computation into an arrow instance that is reused. I hope that clarifies, please give more details if still think it's suspicious
Thanks.
Monix is pretty close to the other Task impls, I wouldn't ditch it just because of these benchmarks
I am not going to, I use it for everything 😀
I mean, in your benchmark: ```scala def flatMap(t: Int =&gt; IO[Int], f: Int =&gt; IO[Int]) = t.andThen(_.flatMap(f)) ^ ^ -- 2 extra allocs of closures, also andThen is not stack-safe ``` vs regular ```scala def flatMap(io: IO[Int])(f: Int =&gt; IO[Int]) = io.flatMap(f) ^ -- a single alloc of IO frame ``` And the resulting `IO[Int]` is generated inside `@Benchmark` by calling `gen(1)`, so this overhead might be skewing the results. Since these effect types are lazy, to measure raw performance you probably want to pre-generate a set of `IO[Int]`s outside, so the only thing you're testing is the run loop (`unsafeRunSync`). Same is true for Monix Task, and _should_ be true for Scalaz 8 IO, but I didn't check.
I tried a couple of plugins for vim for awhile and eventually gave up. I now use vim bindings in Intellij. 
This code runs before the benchmark, not on every iteration (see the `State` annotation and `val`). It creates a function or an arrow that can be executed later. def flatMap(t: Int =&gt; IO[Int], f: Int =&gt; IO[Int]) = t.andThen(_.flatMap(f)) ^ ^ -- 2 extra allocs of closures, also andThen is not stack-safe The allocations don't happen during the benchmark, only when the benchmark class is created. Also, stack safety isn't really a concern atm since the generator has depth 100.
IntelliJ has an option to use Vim style controls if you want. Personally as someone who used Vim for a long time before picking up scala and intelliJ I went for their interface and keep Vim as something for small scale modifications. 
The setup describe in this article is pretty nice: https://medium.com/@alandevlin7/neovim-scala-f392bcd8b7de
I'm using Spacemacs with Ensime and it's great. Spacemacs supports all of the vim keybindings but with its own shortcuts that are a joy to use as well. (and it's prefixed with a spacebar so there are no conflicts)
That's what I use now, but i've had memory issues with Intellij especially when doing anything with type classes and inferred implicits. Just trying to see if there is a better way of doing things. I may end up sticking with Intellij for its plugin support for the build system my team uses, but I like to try to optimize my dev setup as much as possible.
Scala made me ditch vim. The spacemacs scala layer is nice in that you can run an sbt-mode buffer, let it compile, then step through the errors in your code. It also has built in ensime support, which basically boils down to: * Add ensime to your ~/.sbt/1.0/plugins/plugins.sbt * Run ensimeConfig in your sbt project * M-x ensime in spacemacs to connect to the server. Ensime is its own beast though and comes with its own set of troubleshooting considerations.
IO / Task do not define withFilter because there's no sensible "empty" value to return. Best case, you would get a failed or a non-terminating IO at runtime if you did something wrong (which is not too hard to do in polymorphic code, where Scalac will "helpfully" infer Any for you). This sort of defeats the point of having a strong type system, so all effect types in FP ecosystem have decided to live with a syntactic inconvenience of not having the ability to destructure on the same line as generators (you can still do it using temp variable with `=`, see e.g. [here](https://github.com/typelevel/cats-effect/issues/174)). Another thing you cannot `withFilter` is `Either`, because to get a `Left` value you need to supply something. The best you could get would be a runtime exception, again.
Maybe Scala folks should proactively reserve TastyTruffle as a name for something that runs Tasty on top of Truffle.
I have a basic guideline listed at github.com/DeveloperMeier/dotfiles on setting up Vim to work as a Scala IDE but as most others have already stated it just ended up being easier to use IntelliJ with Vim bindings overall. 
Vim bindings in intellij is a nice thing. Can't install any vim plugins though which is quite frustrating.
Hey we open sourced recently our CI/CD as we do all our DevOps with Scala at DriveTribe. Don't hesitate to ping me if you have any questions. Thanks
Derek Wyatt has a Scala plugin for vim. Ensime is good , also there are ctags things out there, on my phone atm. Also I've been using neovim lately, pretty nice
I suggest you to keep an eye on [Scala Metals](https://github.com/scalameta/metals) which is a Language Server for Scala. It's not production ready yet but I think it will be one of the best solutions out there in no time. I wrote some instructions and the features that I got working on `NeoVim` at the moment [here](https://github.com/gvolpe/vim-setup).
Any `Semigroup` can be extended to a `Monoid` by adding an identity element right? Is there, a `Monoid` instance for any `T` for which there is a `Semigroup` instance by wrapping that in `Option` in `cats`?
I always wonder if posting your own content is spam or not. Hopefully not :\) Endpoints: [https://github.com/julienrf/endpoints](https://github.com/julienrf/endpoints) Slides: [http://w.pitula.me/presentations/scalar\-2018/#/](http://w.pitula.me/presentations/scalar-2018/#/)
Nice work -- this library looks great. It would be interesting to see how interaction from non-Scala consumers would look like. For example, suppose a different team working on Android and iOS apps needs to call your service -- where should they look for your service definition and which type-safe clients would they use? The comparison to Thrift and Protobuf (or protobuf-based RPC like gRPC or Twirp) discards them based on the fact they use a custom IDL and code generation, but would be nice to also discuss how Endpoints solves the problems they tackle: cross-language service specs, compact data on the wire, type-safe clients in a variety of languages.
Thank you. We will definitely take a look at this.
Are you familiar with the [Sangria GraphQL server](http://sangria-graphql.org/)? You define a GraphQL API there [in terms of Scala traits/classes](http://sangria-graphql.org/learn/#macro-based-graphql-type-derivation) and in addition to it serving GraphQL requests, it also [renders a GraphQL schema](http://sangria-graphql.org/learn/#schema-rendering) in the [schema definition language](https://graphql.org/learn/schema/). You could do something similar here and render Thrift or Protobuf IDL (with [HTTP API annotations](https://cloud.google.com/endpoints/docs/grpc/grpc-service-config#annotations_http_rules_only)) from endpoint definitions. Take a look also at the [Armeria](https://line.github.io/armeria) server from Line Corp. They support REST, Thrift, gRPC, and "[unframed gRPC](https://line.github.io/armeria/server-grpc.html#unframed-requests)" (really sending protos over http post) in the same [server](https://line.github.io/armeria/server.html). Their server speaks HTTP 1.1 and 2, both as either HTTP or HTTPS. They support content as JSON or compact binary encodings.
You can use vim but WHY?
When is https://github.com/scala/scala/milestone/72 hitting maven ?
I am happy to take any questions people might have on this. 
Will algebraic effects be in it?
Do You already have any \(even vague\) ideas what will land in Scala 4? Or any dreams which route it may take?
No question, I just want to share that "Dotty will become Scala 3.0" is fantastic news. Thank you for all of your work, as well as all of the work from all of the other people pushing towards Scala's future!
I hope you have some good news about macros. It would be the end of my project if they get dropped.
The macro and generative programming roadmap is still evolving, so I'd prefer to wait a bit before giving a definite answer. But there are two main elements: First, macros will have to be rewritten. Second, the main language will provide itself some of the fundamentals of generative programming such as typeclass derivation. That will hopefully make these parts easier to use and faster to compile.
Not a question, but i would like to thank you for the fantastic coursera course you did that got me into scala and help iron out a lot of fundamentals for me as a programmer. 
There is a typo in the first signature, I think you mean: `(A, B) -&gt; C, (F[A], F[B]) -&gt; F[C]` 
Hey Martin! I think that some compiler flags are essential to avoid silly mistakes like for example -Ywarn-unused:locals, -Ywarn-unused:params... i actually always use [the compiler flags recommended](https://tpolecat.github.io/2017/04/25/scalac-flags.html) by /u/tpolecat. Will these compiler flags be on by default?
I'm improving the documentation a lot currently. Let me know if something is not clear I will refine the doc. The aim is a DevOps with programming experience like Java but not specifically Scala should understand.
What's the status of the Dotty linker and whole program optimization (also automatic specialization)? Will it be part of the Scala 3 release?
My preferred way as well. If `F[_]` was a table, then product would be the cross join. 
That part is not quite clear yet. The specific Dotty linker project was discontinued with the departure of @DarkDimius. We now see whole program optimization more as a possible add-on, not as a feature of a core release. But there are some alternative ideas how to do specialization that still need to be tried out. 
Thanks for the answer and for your hard work! I am looking forward for the future.
Python 3 was released in 2008, it fractured their language community for a decade, it's still not clear that a majority of users have yet migrated to Python 3. Will the same thing happen to Scala?
1. Do you think there will be big overhauls to the standard library including the new collections after Dotty comes out, to take advantage of the new features in Dotty? 2. Does it look like Records will make it into Scala 3? Cheers! 
We want to keep the standard library as stable as possible for the time of the migration Scala 2 -&gt; 3. Records are not in the list of planned features. But there's still enough time to consider proposals. 
Thanks, fixed. 
One vim plugin that served me well while I was using it was ctags. It doesn't natively support Scala, but you can use a few simple regexes to get it configured properly. It enables a JTD that doesn't rely on any language server, and complements the vim style much better than ensime IMHO
I used Vim with Scala for a while. I even got a ctags definition that worked pretty well. But after about 6 months I abandoned it to use the Vim keyboard layout in IntelliJ. The main reason is that there is just too many useful features in IntelliJ that you need in order to use Scala effectively (finding implicits, jumping to definitions, etc). If you're having memory problems, the first thing I would do is give the JVM more memory. IntelliJ is a _hog_. Go to _Help_ -&gt; _Edit Custom VM Options_ and give yourself more memory. Here's what I'm using. -Xmx4g -Xmx4g -Xms2g -XX:ReservedCodeCacheSize=240m -XX:+UseG1GC -XX:SoftRefLRUPolicyMSPerMB=50 -ea -Dsun.io.useCanonCaches=false -Djava.net.preferIPv4Stack=true -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Dawt.useSystemAAFontSettings=lcd -Dsun.java2d.renderer=sun.java2d.marlin.MarlinRenderingEngine -Dide.no.platform.update=true Then if you go to _File_ -&gt; _Settings_ -&gt; _Appearance &amp; Behavior_ -&gt; _Appearance_ and check the _Show Memory Indicator_ checkbox, you'll get a display in the lower right hand corner of the window of your current memory utilization. Bonus: If you click on the indicator, it appears do force some sort of garbage collection. 
What is your opinion on dotty (scala 3) vs kotlin? I am asking, because usually people are reluctant to adopt kotlin/scala because of aspects like slow compilation speed and also bumpy tooling. It seems like dotty is improving on both fronts. Are there some areas where dotty will be ahead? My motivation behind this question: i am currently working on a typical java monolith. But while i would love to rewrite some small modules in scala, i oberserved that developers are usually picking kotlin because its made by the company creating their favorite ide. As such it would help a lot to identify some areas where scala could shine. Language constructs are most often not relevant, because when java is your starting point, both languages look super fancy. But compilation speed, quality of compiler errors and IDE integration are immediatly visible to developers.
Hey, cybernd, just a quick heads-up: **immediatly** is actually spelled **immediately**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
First, thank you for Scala; I'm very much looking forward to version 3! The Dotty compiler has a significantly different architecture based on "fused mini phases" and was claimed to be [~2x faster](https://www.youtube.com/watch?v=9xYoSwnSPz0) than the then-current Scala compiler. As Dotty's evolved and features were added, how has that performance changed, and what should we expect for Scala 3? Is there any place we can track compilation performance over time? The [Scala benchmark dashboard](https://scala-ci.typesafe.com/grafana/dashboard/db/scala-benchmark) only shows scalac releases.
I am beyond pumped for this news. Thank you Martin for the crazy/wonderful language you've given us.
I can't wait! I really appreciate the amount of effort that goes into maintaining the Dotty reference interesting and up to date, I like to read it in the evenings again and again and I'm so excited for Scala 3! 
 &gt; 3! 3! = 6 
&gt; Does multiversal equality offer any path to a language where unsafe comparison can't look like safe comparison? Yes, if you `import language.strictEquality` (or pass it as a compiler setting). That will check all `==`, `!=` and pattern matching comparisons, for all types. It won't touch pre-compiled library code of course. So you could still put keys of incompatible types in a hashmap. &gt; Is there any equivalent on the way for pattern-matching It would certainly be nice to have it. A simple way to do it would be to treat `val P = E` and `P &lt;- E` as strict and have a possibly failing equals `val P =? E` and a possibly filtering generator `P &lt;-? E`. or something similar with different syntax. The most difficult aspect is cross-compilation. how can you define a common language subset that works the same for Scala 2 and Scala 3 here? &gt; Do language-level type lambdas work the way I'd expect when involving higher-kinded types? Yes &gt; Will everything in Dotty still be monokinded No, we have `AnyKind` as a top type. &gt; Will better-monadic-for or equivalent functionality be present in dotty? Not sure what you mean by that. Monads are expressible of course. Implicit function types are a good alternative for some monads that don't involve control of code flow (i.e. great for replacing reader monad, no use for replacing futures, at least not without support for continuations). &gt; Is there any way minutes/information on Scala compiler development could be available in text form going forward? I'm very interested to know what's going on but I struggle with videos. For the moment there's only the videos, I am afraid. 
What a shitty bot. 
https://github.com/oleg-py/better-monadic-for/ &gt;&gt;Will better-monadic-for or equivalent functionality be present in dotty? &gt; Not sure what you mean by that. I think he means https://github.com/oleg-py/better-monadic-for/ 
Curious whether we'd see better support for non-IDEA and non-VS-code editors. Scala has been my favorite and go-to language since a few years, yet the lack of stable Emacs support makes me sad. Ensime tries to do its job, but it crashes/gets stuck way too frequently :/ I know I'm supposed to investigate and file reports, but I'd love if there was a common and awesome tool which works across editors.
There's a benchmark dashboard for dotty as well: http://dotty-bench.epfl.ch/ You can track the performance there.
Thanks! I struggled finding that on my own. Both of these seem more geared for tracking performance of each separately and I can't tell if these two sites are apples-to-apples comparisons. They don't have many tests in common, only "vector" and "scalap" from what I see, and those look more than 2x slower in Dotty than in Scala. That difference surprises and concerns me a little. Does that mean Dotty has currently lost its performance edge? Or are the tests actually not comparable? If the former, I hope there will be good effort to regain it. If the latter, I think it'd be great if some representative comparison tests could be created.
1. The blog post mentions IntelliJ integration, but when I tried that integration out it didn’t even have syntax highlighting. Are there plans to improve it? 2. Quite a bit of scala tooling is currently being built around Semanticdb. When I last inquired neither LightBend nor EPFL had plans to support Semanticdb from Tasty. Does this mean we will need to re-implement the current listings and refactorings to be Dotty compatible? 3. Is the intent for Dotty to be the LSP server for Scala 2.x as well, or will all users need to migrate to 3.0 to benefit from it? 3.a If the intent is for Dotty to be the language server for Scala 2.x, will there be a commitment to be 100% compatible with all 2.x constructs, or do we risk another IntelliJ “Good code red” situation? I am very excited about Dotty, just trying to figure out what the migration plan will look like. This is scary to all of us out there who can’t even update to 2.12 because we happened to use Spark, one of the most popular Scala libraries out there. The thought of have to migrate to Scala 3.0 to get great refactoring and language server support is concerning and I am hoping you can help put my mind at ease.
It's planned to integrate Dotty in the Scala-2 benchmark tests. Hopefully that will happen soon. As they are now, they are not comparable because they are run on different types of machines.
Does it have Eclipse integration? Skimming through the docs suggests it does the other things I'm looking for pretty easily, but Eclipse integration is important.
Do you plan to release Scala3-compatible Scala.js together with Scala 3.0.0, or do you think it will be delayed? Basically, going forward, will Scala.js be treated as a core part of the language, or as a library that needs to catch up?
Will there be scalafixes or anything published to help this migration?
What resources on SBT have you been looking at already?
no. But you'll all find out why the hard way if you don't listen to the person with the most experience of building scala language servers.
Hi. Thanks your all of your work on Scala/dotty. I'm curious: what is the plan for tooling migration? This seems to be what the discussion is around.
I've looked through the documentation on the SBT website of course, but also various sites that come up on Google searches or on this subreddit. Examples: https://jazzy.id.au/2015/03/03/sbt-task-engine.html https://dcsobral.blogspot.ca/2011/06/very-quick-guide-to-project-creation-on.html https://www.beyondthelines.net/computing/understanding-sbt/ I'm not sure where else to look for good resources.
What would you have to cut in order to release 3.0 in 2018? And could those things be added incrementally in 3.x?
You're very right ! (And BTW, thanks for all your work on ensime! It is a pleasure when it's working, especially with small projects). It's just that somehow it appears that Scala became too complex of a language for such tools. I've been writing a lot of Haskell recently, and am amazed at how incredible and stable the new tool 'intero' is.
Regarding IntelliJ, it's best to ask the Scala plugin team directly. Regarding SemanticDB I agree it would be good to integrate with it, in particular for supporting large projects. We do rely on community support here. Regarding Dotty as an LSP server of Java 2.x, that's a possibility. It would never work 100%, but I believe it would work overall better than what we have now. But I am open to other solutions in that space as well.
Yes. TFA explains this in depth.
&gt; Yes, if you import language.strictEquality (or pass it as a compiler setting). That will check all ==, != and pattern matching comparisons, for all types. Excellent, that takes away one of my biggest worries. &gt; It won't touch pre-compiled library code of course. So you could still put keys of incompatible types in a hashmap. But if someone compiles a hashmap using that flag, their hashmap will have to take an implicit that says equality is valid, so *that* hashmap will then only be usable with types that have equality, right? &gt; It would certainly be nice to have it. A simple way to do it would be to treat val P = E and P &lt;- E as strict and have a possibly failing equals val P =? E and a possibly filtering generator P &lt;-? E. or something similar with different syntax. The most difficult aspect is cross-compilation. how can you define a common language subset that works the same for Scala 2 and Scala 3 here? I was thinking just actual `match`/`case`, in which case existing Scala already supports `@unchecked` there (i.e. I'd propose to make non-exhaustive matching on a non-sealed type a warning without `@unchecked` just like it is for sealed types), though maybe you'd consider that too cumbersome for people who do want the possibly failing match. I don't have any good ideas about the `=` and `&lt;-` cases, unless `@unchecked` would work there as well. &gt; No, we have AnyKind as a top type. Oh, cool. I'm not quite following the documentation - can I have an anykind type that is known to take some parameters but might take more? (Motivation: I want to write a kind-polymorphic variant of my fixed-point type, i.e. `case class Fix[F[_] &lt;: AnyKind](inner: F[Fix[F]])` that I can use with types shaped like e.g. `F[_[_], _]` (e.g. `Free`)). Someone else already covered the other one. As always, thanks for the language.
I have barely followed Dotty - but I thought one of the biggest benefits of Dotty would be significantly faster compilation. It didn't seem to be emphasized in the blog post at all. From what I understand this will also improve performance in the IDE as well.
Thanks for at least confirming that SBT should be able to do the things I want it to do. I've looked through both of those pages before, and remember hours of frustration trying to understand why my builds still weren't doing what I expected. I'll give both of these another try tomorrow (busy for the rest of today) and come back with further questions if I'm still struggling. Thanks again!
Mill is brand new so I'd be surprised if it's integrated with anything.
&gt; Scala has pioneered the fusion of object-oriented and functional programming in a typed setting. Have you heard of [OCaml](https://ocaml.org/)? I've been doing OO FP since before Scala existed. OCaml was the gateway drug that got me from OO, to OO FP, to mostly FP with a little OO. One of the things I really miss when programming in Scala, is that OCaml doesn't allow uninitialized member access, where Scala does. In Scala, that leads to a bunch of rules people make for themselves to avoid falling victim to unitialized access (resulting in an unexpected null or 0). Since everyone follows a different set of rules (none of which are enforced by the compiler) this gets rather awkward. One area I hope Scala could get better than OCaml is type inference. There is much room for improvement regarding type inference involving the combination of subtyping and polymorphism. 
I'd argue that Scala's OO capabilities are closer to OCaml's module system than to OCaml's OO capabilities – which are a unique mix of structural typing, parametric polymorphism, mostly global inference and a layer of estranged OO concepts. From a type-theoretic perspective OCaml's OOP is probably the "right way" of doing OOP in a typed functional language, but it's not what people actually mean by OOP nowadays (which is closer to Java/C++), and interestingly it seems that very few people find the system useful and actually leverage its great expressive power (see for example how it enables patterns like [this](https://dl.acm.org/citation.cfm?id=3110272)) in everyday programming. &gt; OCaml doesn't allow uninitialized member access, where Scala does. I think trait parameters will help here. AFAIK this is still an important issue being worked on, because it can also introduces unsoundness in the type system. &gt; One area I hope Scala could get better than OCaml is type inference. If you're really talking about OCaml's type inference of OO code, there is no way that Scala can catch up with it – it's in an entirely different league (relying mostly on global inference and row variables). If you're talking about type inference and checking of module code (particularly first-class and recursive modules), then yeah Scala already does better in that respect.
Hi! scala written(based on) in rust? 
I don't think Scalac is slow. It does a lot of things! How much time would it take to write by hand all that Scalac does a compile time? I don't want to waste time writing getters and setter by dozen, copy pasting code because language X does not have mixins, spend hours writing boilerplate code for Json (de)serialisation or fighting annotations or debugging. At the end is the day, I am more than happy to give one more minute to Scalac to compile than wasting hours or days writing and maintaining it by hand. Scala is not a die and retry language where you have to write, compile and run many many times to find a way to achieve your goal. Honnestly I don't mind compilation time, even if on big projects it can be more than 10 minutes. Because it runs in the background while I'm busy thinking about how to model my domain or solve my problem. And also because incremental compilation works very well. 
It does not matter if scala does a lot or not. It is fact, that developers will avoid programming environments with slow roundrip times. Products like jrebel where specifically invented because it is a heavy burden if you are unable to see your change immediatly. Programming languages like go where invented, because compile time matters. As such, it is in scala's best interest to have fast compile times, because otherwise it will simply lack the necessary adoption. A good incremental compiler might already do the trick (rememeber, eclipse had an incremental java compiler, and this was one of the most important factor for javas adoption rate). 
I'm not familiar with libsvm format. Why dynamic columns instead of rows? Rows would almost certainly be faster.
I'll admit, the more I used OCaml, the less I used the OO features. In general, I found pattern matching on variants tended to lead to better code structure than overriding methods in subclasses. (The visitor pattern was a notable exception, and I gather UI toolkits are too.) I do like the flexibility Scala offers in letting you choose whether to use pattern matching in a function vs. method overloading on a per-function/method basis. In OCaml you have to choose when defining the type, because can either define it as a variant or as a class. There is no overlap like Scala's `case class`es are classes too. &gt; not what people actually mean by OOP nowadays (which is closer to Java/C++) It is funny when a Java programmer wants to talk in OO terms, so you talk about basic things like sending messages to objects and watch the look of confusion come over their face.
Language server protocol support should potentially be able to support any editor that has a client for the protocol. So, all emacs would need is an LSP client implementation (which I believe it already has several).
Forcing developers to target a specific version and release features in lockstep with (much slower) compiler releases doesn't strike you as a poor idea? I don't even think I've mentioned how licensing would work in this case - where Ensime is currently GPL3, what would `dotc` be released under? Apache 2? I'm assuming so, and it would destroy the ability enforce an extra set of hurdles for FOSS devs to build against. Not to mention there's currently only 1 person maintaining the ensime LSP (Iulian Dragos), who is a single PhD student. His life is about to get much tougher. So yes, you will have support, which is good, but at what cost? If only Odersky-senpai would notice me and answer my mildly stated, well-toned and respectful question. Then we'll know!
Thanks. I think I am starting off with some misconceptions, but I'll run my original thinking by you, if that's ok. Suppose I have word count per sentence for a million sentences. So there are,say, 6000 columns, one per term (after data reduction). Each column head is a word, each cell is word frequency per that word(column) in that sentence (row) . So if I want to find all sentences which have the word "teapot", it's a simple select. If I have a table with three columns word, wordid, sentenceid, then I need to pull data from two tables and potentially multiple partitions within each table. My original understanding of Cassandra is that we design our data model with our queries in mind. But it seems that I will have to do some kind of join here. Any input is more than welcome. And please feel free to correct me. I'm on day one :) 
Excellent news that every version of Scala now has or would like semanticdb support, I think that will help put many people in the tooling community’s minds at ease. Regarding yet another language server for Scala that does not work 100%, this would be sad news. I’ll keep trying to think of alternatives, and encourage others in the community to do so as well. With IJ we know they had to create their own compiler for Scala which will always be out of sync to match their tooling, but as a community if we can’t even create our own Language Server typechecker that works 100% that might be a little embarrassing.
It seems you are beginning to approach to sbt. I will suggest you can read this slide \- [Sbt baby steps](https://www.slideshare.net/MarinaSigaeva/sbt-baby-steps). I think this slides is really great for anyone using sbt in the beginning. BTW, I notice a lot of people still use SBT to indicate this build tool. I just learned recently that it's only called **sbt.** &gt; the name sbt doesn’t stand for anything, it’s just “sbt”, and it should be written that way. There is [more detail explanation](https://www.scala-sbt.org/1.0/docs/Faq.html#Frequently+Asked+Questions) at sbt web side.
Performance benchmarks would be nice. It would be nice to make this API as fast as fastparse.
Hi Martin, I'm wondering how `T | Null` will interact with the type system, specifically if flow analysis will be done like in Kotlin.
That’s managed by @srd and isn’t part of the core Scala “product”. I can’t imagine it changing anytime soon. 
I believe some flow analysis will be needed, yes.
&gt; But if someone compiles a hashmap using that flag, their hashmap will have to take an implicit that says equality is valid, so that hashmap will then only be usable with types that have equality, right? &gt; Correct.
When I took a look yesterday I couldn't find clear description of what piece of my stack is this supposed to replace. I could definitely use some quick start that doesn't explain any details and just shows input and output of using this software.
Thank you for giving us scala. Will scala 3 continue the 6 weeks release cycle that is happening now with dotty?
I was compiling against GraalVM not Hotspot. Hotspot version - dotc version took ~7 sec and scalac took ~9 sec. However, as you mentioned because of JIT, I will try to benchmark the preformance using sbt-jmh.
&gt; It does not matter if scala does a lot or not. It does. You've got to be more nuanced than that. It seems that you missed the point of your parent comment. As a thought experiment, let's take things to the extreme and imagine that the Scala compiler were as fast as possible. It would still be "slower" by a Java compiler (using a naive line-per-second metric) purely because a one-liner case class in Scala is actually dozens of equivalent Java lines – accessors, `toString`, `equals`, `hashCode`, etc. – and those things _have_ to be generated at one point or another during the compilation phase.
Performance improvements and measurements are my current focus. Not sure though that it’d be as fast as fastparse due to trampolining on all recursive monad bindings. Will definitely perform some comparison tests. 
&gt; &gt; It seems that you missed the point of your parent comment. &gt; You have clearly missed the point of my comment. You guuuyyys. You're never going to resolve your differences like that!
&gt; You can remember it by [remembering it] Yeah. Not exactly useful.
so it would be two queries. One to find sentence ids with "teapot" and one to pull all sentences with sentence ids in ( find sentence ids with "teapot")? The little talk I am finding online does seem to suggest this. 
I'm exploring Akka and the utility of actors as a concurrency model to help build a small application that can take advantage of event sourcing. I have some questions about the intended "patterns" I should try to use with actors in my situation. A brief description of my problem domain is as follows: I'm writing basically a glorified to-do list that works like a directed graph. Each node is a "goal" that has some attributes and references some data related to some media item (movie, book, game, tv series). All of the top-level nodes will refer to the set of all "current" goals that are being worked on, and goals are removed from the graph when they are finished, setting the next goal nodes as "current". I'm wondering if it makes structural sense to model this kind of graph structure using actors, or to see if it would make sense to use some other concurrency abstraction. I'd like to do things like, say, execute reminder notifications at a certain time for each working node, or to be able to communicate with other nodes in order to distribute the graph structure and keep track of changes. At the very least, using actors for the currently-active goals and persisting the others until necessary seems like it's reasoable enough, albeit less flexible in terms of asynchronous messages coming from nodes (like, what if a goal you haven't started yet wants to send a notification or reminder to hurry up?) With that out of the way, here are my questions: * With akka actors, am I wasting my time if most of my actors do not have any kind of internal state? It seems to me like akka excels at stateful concurrency by enforcing a single task to execute at any given time for some data. Which of course is why I think it would make sense for my use case, but if I had a different use case, would I maybe be better off with just using futures? * Where would reactive streams come into this? I'm familiar with Reactive Extensions, Project Reactor, Monix, etc. but I've never used Akka Streams. Can I just use a library I'm already familiar with or does Akka Streams somehow play better with actors? * I've looked into Lagom quite a bit and it seems fascinating, albeit a little "magical" in how it handles things behind the scenes. One thing I find interesting is how "Persistent Entities" in Lagom are gotten rid of if they haven't been used in a while, for efficiency's sake. With Akka Persistence, is such a mechanism easily doable on my own? Thanks in advance if anyone can point me in the right direction on this. I'll continue playing around with Akka, Lagom and other frameworks and abstractions but I thought I'd put this out there to see what people have to say.
Whats the use case for having them (fault tolerant spark computations) as web services?
This is rude and not the kind of welcoming attitude we should be showing to beginners.
Yes, please elaborate on these programming idioms. I think it will really help for new adopters if we have opinionated guideline on how to properly use the language.
I see how it's managed right now. My question is about 2020, when Dotty will be released. Sebastian will probably finish his PhD before then. I assume (although don't know) that Scala.js 1.0 will be the final result of that PhD. What will happen with Scala.js after that? Will Scala.js still receive the same attention it does right now? Other than ScalaCenter's [Proposal SCP-005: Ensurance of continuity of Scala.js project](https://github.com/scala/scala.epfl.ch/blob/master/minutes/_posts/2016-06-06-may-9-2016.md#proposal-scp-005-ensurance-of-continuity-of-scalajs-project) I am not aware of any statement of commitment to Scala.js. I personally am really confused what level of commitment that is. From what I understand, ScalaCenter is currently funding work on scalajs-bundler, an important tooling for Scala.js, whereas Sebastian's work on Scala.js itself is currently being done without ScalaCenter funding. So it is not clear to me if ScalaCenter will have the resources to continue with SCP-005 until 2020 and beyond. So if Martin or Sebastian or someone who knows the situation could provide some confidence in this, it would be great. Scala.js is a great tool that we don't deserve, but it is hard to convince employers / clients / partners to use it because it doesn't appear to have significant corporate backing. Even for myself and my own private projects, this is the #1 doubt I have about my technology choice.
Hi Krever, Thanks for having a look. This is a good replacement of Jenkins (as I wrote in Related projects of the readme). But as with Orchestra we are able to use libraries, we managed to replace Ansible and the AWS command line tools with Scala code using Kubernetes Client for Scala and the AWS Java SDK. Some people told me I should add in the doc a comparison table with Jenkins.
Scalajs is superimportant to many. Its very underestimated I guess.
That question was debated on Gitter with Guillaume Martres (@smarter), the PhD student working on LSP support in Dotty: https://gitter.im/scala/tooling-contrib?at=5ad9e43f5f188ccc1585e4cf Also, correction: Iulian Dragos finished his PhD years ago and is now CEO of TripleQuote: https://www.linkedin.com/in/iuliandragos/. I understand his project is basically an interface between LSP and Ensime, I'm not sure how much code would be shared between that and interface between the LSP and Dotty (apart from any LSP implementation).
No, Scala predates Rust by years.
&gt; I'd argue that Scala's OO capabilities are closer to OCaml's module system than to OCaml's OO capabilities Let me emphasize this. Support for ML modules and objects was already explicit in the ECOOP 2003 paper on Scala's foundations (http://lampwww.epfl.ch/~odersky/papers/ecoop03.html). Regarding type inference, in Scala everything is an object, that is a module, and OCaml has indeed very limited inference for modules.
Longer term, I'd hope we'll use Dotty as LSP for Scala 3 itself and that people migrate. The alternative would involve large refactorings to Scala 2's presentation compiler that nobody volunteers to do?
&gt; Scala predates Rust by years i want the detail, can you specific?
OCaml is de facto "the" compiler for ML family, and the OO parts are *virtually* ignored in projects using OCaml. 
No eclipse integration, but you could always make it, and open a PR. And if not, there's always intellij which imo is light-years ahead of Eclipse anyway, so I highly recommend making the switch sooner than later. Learning a new tool is always tedious in the beginning, but it's definitely worth it imo! As for mill, I think it is much better and easier to understand than SBT,so I can warmly recommend it for that. Sbt is still "the standard", but I expect that could change in the near future.
Never mind mill vs. sbt (I've figured out the sbt problems now), but the feature list on IntelliJ is very tempting! Built-in version control integration, and the Scala plugin has a built-in sbt shell! I'm going to have to give this a try...
See here for an example use case we worked on. We trained a network on spark and deployed it as a web service to help create a snow leopard detection system for leopard conservation. https://news.microsoft.com/transform/snow-leopard-selfies-ai-save-species/ 
Are you talking about the running time of the code generated by each compiler? Your example using foreah is likely to be slower in Dotty currently because we do not implement method specialization yet.
Thanks, I'll take a look. Iulian is still listed as the maintainer for Scala's LSP support, so my information is dated. 
I think he still is the maintainer of one VSCode plugin based on ensime (not that I have special info on this), it’s just a different project. I’m also curious, but I think that http://dotty.epfl.ch/docs/reference/overview.html is relevant, and that it was added just last week. Not sure what exactly it refeshas in mind
Just the compilation speed on my I7. Trying to use nailgun project created by Scala Center to benchmark (slightly buggy now but this is a great step towards productivity. IntelliJ is too heavy for small things.)
&gt; Sebastian will probably finish his PhD before then Well before, iirc he should have already finished -- must love Scala.js too much :) Unfortunately neither ScalaCenter nor any other potential backer has stepped up and proposed hiring anyone from the Scala.js team. It would be huge to have at least one developer working full-time on the project.
We used RabbitMQ a long time ago for message queueing, and we had problems with it (high latency) when we started overloading it with messages in a highly distributed environment. We were getting read/trigger times of larger than 500ms per transaction, which was unacceptable. Now-a-days, it sounds like people are using etcd to handle most of these things. My server will be using etcd for distributed transactions, and I'll definitely being a DAG when starting job tasks. Check out my project at http://www.github.com/KenSuenobu/scattersphere/. I'm currently working in the initial-layout branch, but I'll be transferring things back to the develop branch once I get a working simple DAG graph working for jobs, which should be in about a week or so.
Check out my project and see the potential job tests. I have a "simple" job test that draws a linear DAG. It does not (yet) create an execution plan, but it will soon.
What's wrong with Alka http?
So 500K/minute = 8333 requests/second. From these benchmarks: https://www.techempower.com/benchmarks/#section=data-r15 You should be able to hit them with any of the frameworks provided you use them appropriately e.g. lots of async/futures etc. But I guess http://fintrospect.io is the fastest.
Akka http path matcher is insanely cpu intensive. At 100k rpm of noop traffic it consumes 80% of the cpu on an 
Do you have a link?
With the new `http4s` backend, we (meaning myself and [@jmcardon](https://github.com/jmcardon) were able to push a [minimal application](https://github.com/jmcardon/IOPort) to 60,000rps on a single `m4.large` node using different backends, including the new Scalaz 8 IO bifunctor. Now, while this wasn't sustained for long periods of time, it shows that in Scala, we've reached a point where purely functional code is now outperforming more imperative solutions such as `play` or `akka-http`, and I would urge you to consider looking to `http4s` for performance. That being said, performance isn't the prerogative of functional programming in general. Correctness and composition are the point, less the naive interpretation of "speed" people tend to use to describe a program.
The built-in version control is superb! And the Scala plugin for intellij is constantly worked on and improved. Also you'll love how keyboard centric intellij is ;) There's a hotkey for absolutely everything. Also: multiple cursors which is has many use cases.
This sounds like too vague of a statement. You should consider backing it up with a code example.
The details of Valhalla and how value types will work is not set in stone, so its not entirely clear how this will work out. Furthermore, immutability is just one example, there are many others. The fact is that Graal only works with bytecode, which means a lot of info about the original Scala program is lost.
If we get value types and specialization at the JVM level I don't think much information about the actual executing code is lost really. Of course a lot is lost on the type level, but that shouldn't affect optimization possibilities much. One thing would be side effects if an effects system is added to Scala, but functional purity can be inferred even at the bytecode level.
&gt; Alternatively, any suggestions on easier-to-understand build tools that work with Eclipse (and mixed Scala/Java projects) would be welcome. Maven. Maven has excellent integration with eclipse (e.g. you can add library dependencies from a UI in eclipse, and they'll be added to the "pom" (build config file) and simultaneously updated in the eclipse project (no need for a manual regenerate step or anything like that)), is simple and well-documented, and builds Scala fine (you will want to add the scala maven plugin to your pom, and install m2eclipse-scala in eclipse). If and when you get to the point where you want to "cross-build" libraries (i.e. release and publish libraries for multiple versions of Scala), then it might be worth looking at SBT. &gt; Also, I'd like to be able to add external JARs to the classpath from a third-party library (in this case, JMonkeyEngine for a game project). The best way to work with "custom" jars in maven is to install them into your local repository: https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html (I mention this because one trap people sometimes fall into is trying to use "system" scoped dependencies for this; don't do that). But really you should avoid needing "external" dependencies entirely, and instead find the jars you need as maven dependencies. In the case of JMonkeyEngine: https://jmonkeyengine.github.io/wiki/jme3/maven.html .
Wat
Ok. 8k per second should be relatively easy. If you are relying on your rest server to be read after write consistent, stop. Writes need to be asynchronous -- you post, it goes into a queue, and gets handled in due course by a worker. Your client app should manage any state you post internally, and reconcile that with the server. That's what the entity manager does in Spring, for example. If you delete, and the delete comes back successfully, remove the object from any internal client app storage without immediately reading via get. If you don't control the clients, put a cache in front of the backend storage, read from the cache and only refresh it after each worker queue process success or some small period of time. Basically, don't do processing in post, delete, or update, just shove it onto a queue and return success. Your reads should be cached for some period of time, generally your average queue processing latency. Any writes you do will eventually show up on the read side, but they shouldn't really do any processing either. Just read from storage. Now your concern becomes latency -- how quickly you can process your worker queues. The nice thing here is that, since each event is separate, you can pull huge batches off the queue, group events that have to do with the same object together, and process the groups concurrently. Fs2/akka streams/scalaz/monix are great at this stuff. You can use a separte cluster. By and large, you can scale workers horizontally by queue depth, or shard on event contents and scale horizontally with the different events you are getting, meaning you can scale more or less infinitely, and very reactively depending on overall load. Just make sure your memory or cpu (whatever yor processong bound is) is being fully used on your workers. Otherwise you aren't using as much of your box as you could be and are cost inefficient. You are free to reject requests while scaling your queue, just indicate that the client should retry the request via rate limiting or some other status code -- (420 enhance your calm status from twitter is an example of rate limiting, google it). This means you have separated your concerns -- handling requests quickly is the webservers' job. Processing requests into domain data quickly is the queue workers' job, and that's easier when you don't have to boil the ocean on each request and can do 100s at a time. If you stop waiting on io and post processing to return success, your servers will handle more requests per second. If your queue processing scales, your latency will be really small, meaning your reads will be consistent. Happy hunting.
We use Finagle and have had good success, though I would have expected similar from akka_http. Some questions for you to consider: * what are the performance characteristics of the business logic code separate from the REST framework? Could the bottleneck be there? * Have you verified that you're not blocking any threads, spiking on CPU, thrashing memory with GCs? * How large are the bodies of the request and response, and how does that compare to the bandwidth available to the machine? * Are you able to get enough connections from the OS or is the process hitting a ulimit? * Are there other processes running on the same node that might be cannibalizing resources? * my assumption is that you'd set thus up as multiple instances operating behind a load balancer to make it HA if nothing else. Does the total number of resources make sense for your load? For example, do you have enough cores for a reasonable requests per second per core?
Replied to your post. I think what you are seeing is expected and actually shows how scalable the router is. You are no-opping. Nothing blocks. The router is free to use as much cpu as is available, which is almost all of it. You need to report the requests per second and latency to measure the performance of the routing dsl. 
Replied to your post. I think what you are seeing is expected and actually shows how scalable the router is. You are no-opping. Nothing blocks. The router is free to use as much cpu as is available, which is almost all of it. You need to report the requests per second and latency to measure the performance of the routing dsl. 
What matters is productivity (and fun! ;) ). This is a trade off. Seeing your changes immediately often means very few static analysis, no guidance from the language, no help with error-prone boilerplate code, etc. All of this cost CPU cycles but here is the point: CPU cycles are damn much cheaper than brain ones! The more Scalac does to address boring value-less code, the more i can concentrate on features, business logic and tricky parts. Go compiles fast because it is a very limited language. Scala has: * *algebraic data types*: it is a very nice tool to express the business domain. Sum types (aka Distinct unions, sealed traits, variants, etc) is a wonderful tool to express cases. Most languages do not have it! * *pattern-matching": both the visitor pattern and pattern matching serve the same goal. But using the later is faster, shorter, safer and more flexible. * *mixin traits*: who have never swear at Java because the lack of default methods. * *first-class functions*: Just a `(x: A) =&gt; ` and i can express any behavior as a value! * *first-class objects*: Just write `object X {` and you're done! No need to write a class, then implement the singleton pattern with static methods, etc. * *uniform access*: it simplifies so many things! * *lazy values*: that's just 4 characters! I can type it in less than 2 seconds :) Productivity wise, that's great! * *Options, not null*: I lost to many hours tracking null pointer exceptions! I prefer working on a feature than guessing where that null value comes from. * *for comprehensions*: I can write asynchronous code as if it was just a simple synchronous for loop. * *implicits*: thanks to it, I can make Scala create automatically complex values like serializers and deserializers with my own rules. It's safe and compared to me writing and maintaining it, it's super fast! * *Type classes*: copy-paste programming is just not productive. Every time you want to modify a bit of code, you have to remember to repeat the modification on every copy. That's error-prone and just wasted time. * *macros*: yet again, Scalac does the boring job for me. * *Scala type system*: the more properties i can enforce in types, the more i do. I would have to write many many tests to cover all that Scalac type-checker can verify (non empty list, variance, etc). So i can write more meaningful tests. Instead of seeing in the compiler just as a foot ball, you can see it as a something writing and checking code for you.
From what you linked: akka-http could only handle 6,753rps. Fairing slightly better: play2-scala-slick at 19,990rps.
Wow my comment was killed! It sounds like your pipeline has moved from being CPU-bound to being IO-bound, which is great! But there are times when you can't but help but be CPU-bound. In those cases idiomatic Scala is not your friend. It allocates too much, kills your CPU cache, generates polymorphic code, tends not to vectorize, etc. Just simple iteration in a tight loop is enormously expensive. Still though, let's look at https://www.techempower.com/benchmarks/#section=data-r15. akka-http could only handle 6,753rps and fairing slightly better was play2-scala-slick at 19,990rps. I stand by those as being representative of idiomatic Scala, and they barely handle the load the OP states they need to process. Assuming any real work needs to be done per event/request then you'll have to throw massive amounts of hardware at the problem.
Thanks for this nice post, I will try out http4s because of this!
OP was asking in terms of requests per minute. Scaling those request per second measurements up gives 405,108rpm and 1,199,400rpm, easily meeting OP's requirements. &gt; It allocates too much, kills your CPU cache, generates polymorphic code, tends not to vectorize, etc. Do you have evidence to support that these claims, especially when compared to using just Java as you earlier suggested?
These were great, thanks for posting them
Well I might be wrong, but the goal for Akka HTTP was not handling many requests fast, but many requests without fail, so perhaps some of these would be answered just not in 1s margin.
Great! You can use the app I linked as as base reference for architecting your program. In addition, i forgot to post several screenshots from results of our tests (pre-60k test, but at least there's some proof so it's not just my word ;) - here's [35k rps, &lt;.01% error](https://josecardona.ca/gatling/httpapp_3/) ).
&gt; is this the idiomatic way of making the actor system/materializer available to the http client? Yes (though remember you can use `&amp;` as a more concise way to combine multiple directives). (I get confused by `async`/`await` but I'll assume they're doing the same thing as a `for`/`yield` here. I don't think `r` needs to be its own variable but that's subjective I guess). &gt; should i use the same thread pool as my server's actor system when making these service calls (i am now using import system.dispatcher inside the TotoService If you trust that TotoService is implemented efficiently, or if your web service isn't usable at all without it, then yes; if TotoService is unreliable and/or secondary then no. Using the same thread pool for all operations is more efficient and avoids the risk of priority inversions etc., but at the cost that if you accidentally block inside TotoService (e.g. because you made a non-async HTTP call - not a problem if you're using akka-http client all the time) then you've blocked a web dispatcher thread and if you block all of those then your server can't respond to requests until one of them unblocks. If you give it its own thread pool then it's more compartmentalized, but the cost of that is that threads from both thread pools will be competing for CPU (or you'll have to run half as many threads in each pool).
If you have a tight loop, then this Scala for (i &lt;- dataArray) { ...some logic...} can be much slower than this Java for (int i = 0; i &lt; dataArray.length; i++) { ...the same logic...} because a for loop in Scala does extra work that for loop in Java doesn't do, like potentially allocating, boxing, method dispatch, etc. That overhead tends to add up and limit your performance if the work done in-loop ends up comparably expensive to just iterating through the loop! In Scala the fix is to use a while loop, but I don't think that's "idiomatic". I could also point to the performance of collections like List and Vector vs regular Java Arrays, deep call stacks, the overhead of lambda's pre-2.12, the terrible throughput of Future, and the complex rules around boxing. All add small amounts of overhead that accumulate. For a moderately large code base this can mean a performance difference of 50 to 100% compared to idiomatic Java. Sure, you can throw more hardware, but if that isn't an option then you have to abandon Scala-isms and actually start to optimize your code. Now most of the time you don't care and you aren't pushing your servers to the edge. But it adds up. If you want links: http://www.lihaoyi.com/post/MicrooptimizingyourScalacode.html http://www.lihaoyi.com/post/ScalaVectoroperationsarentEffectivelyConstanttime.html http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html
Don’t use await. Chain the futures together. Using await is going to block at least 2 threads to handle your request. “Idiomatic Akka” would have you send a message to a worker to handle the client call, then when the client is complete send back a message to the original caller, which then completes the original request. At no point would await be used. 
&gt; is this the idiomatic way of making the actor system/materializer available to the http client? I'd say no, because you don't need to instantiate a new client on every request. You can construct it once and reuse it. I'd say better would be to - have `TotoService` as a constructor parameter for whatever class is creating this route - have the `TotoService` constructor take the `ActorSystem` and `ActorMaterializer` as constructor parameters - create the `HttpExt` instance inside `TotoService`, e.g. `private val http: HttpExt = Http()` - call `http.singleRequest` inside `listTotos`, which now only accepts the parameters needed for the request - remove the `extract*` directives from the route Depending on how you're using this you might even be able swap out `singleRequest` for a cached connection pool instead of creating one each time, though `singleRequest` certainly is simpler. &gt; should i use the same thread pool as my server's actor system when making these service calls If you do any kind of heavy processing when the future is done, no; use a different thread pool. However unless this is a super high-demand route, chances are you won't interfere with other actors and it won't make a big difference, plus you can always swap it out later if you find it is in fact an issue. On the other hand, putting things on different named thread pools can help debugging and be a nice conceptual separation of concerns.
True enough, the OP didn't include any latency requirement. But my working assumption is that the system should be able to process 400-500k requests at steady state, not that the load will burst to 500k and then back off. From what I can tell Akka just can't handle 500k/m (8k/s) at steady state - it just doesn't perform at that level.
I assume they would like some processing free at the end to actually do something with the request. So no, I don't think akka would suffice. Play might, but again if the routing logic is appreciably expensive compared to your business logic then you might have an issue. Neither leave much room, which is my point. As for evidence, I think that [this blog](http://www.lihaoyi.com/) should have enough to convince you. If you want more, look at some of the discussion around Scalaz 7 and 8. Quite a lot of the improvement is from moving away from abstractions that allocate and from deeply nested abstractions, like Monad Transformers (https://corecursive.com/009-throw-away-the-irrelevant-with-john-a-de-goes).
As far as I know, there is only one non-Ruby solution that is similar to Sidekiq - that is Celery for Python. No Java library provides the same set of features. Mike Perham (author of Sidekiq) wrote Faktory, but it also doesn't have ability to run jobs written in Java.
That does look like an easier way to load in the JMonkeyEngine jars!
Multiple cursors? I've seen that in the documentation, but I can't quite imagine yet how it would be useful... Have to play around with it a bit!
Spark has a Job Scheduler, but that might be too large a dependency. As an aside, why do you care if the job scheduling library is Akka based? Not sure if I understand your concerns about dependencies. It's a Java library, but there's [Quartz](http://www.quartz-scheduler.org/). Could also be considered a large dependency. 
Because you'll need to handle tons of details to make your solution production-ready. Using Sidekiq is as easy as creating a class with `perform` method.
That's quite an important question, in the presentation about dotty linker there was that example about really slow operations on scala collections due to multiple pack\-unpack operations that has to be performed every time lambda is invoked. This may be fixed for jvm backend when project valhalla finally lands, but do we need to be dependent on it?
https://www.quora.com/Is-mathematics-a-science
This is an illiterate setup (no attempt to be an IDE) I use: [sctags](https://github.com/luben/sctags) [fzf](https://github.com/junegunn/fzf) [vim-scala](https://github.com/derekwyatt/vim-scala) [vim-lsp](https://github.com/prabirshrestha/vim-lsp) (with sbt 1.x) Did try ensime several times, but it was either broken in vim or sublime and resource hog. I use gvim in combination with Zeal, tmux and zsh. For refactoring, heavy typed stuff or implicit foo I use idea. 
Sidekiq uses threads to handle many jobs at the same time *in the same process.* OP specifically mentioned distributing the work. 
You can run as many instances of Sidekiq workers as you need (on different hosts obviously).
Check out OCaml for inspiration. It has been using a hybrid functional/object/imperative approach and was a direct influence on Scala. I would reiterate the good advice you got here that side effects should be kept contained inside function boundaries as much as possible; they shouldn’t be observable to the outside world. However at runtime you of course need to run all your effects and get your actual work done. Effect types are a great way of managing that. Scala comes with one, `Future[A]`, but it has some drawbacks so many people like the `IO` or `Task` types provides by cats and Scalaz. Your mileage may vary, but I find `Future` to be good enough to start with for effect management as long as you remember that futures are executed immediately as soon as they’re evaluated, so the trick is to delay their evaluation, by putting them inside methods instead of vals. Then you control when they’re executed, by calling the methods.
This is due to https://issues.apache.org/jira/browse/SPARK-14540. It would have been resolved long ago, but it requires additional support from Scala compiler.
As someone who has to write loops that run hundreds of thousands to millions of times per second in a Flink cluster, I do find it kinda annoying that when you want to drop down to optimize code in scala, all you have are while loops. I get that we don't want to encourage for-loop imperative programming as first approach, but maybe we could put some imperative features behind a language import or something? The result of _not_ having these constructs often results in code that's even harder to reason about. val iter = javaList.iterator while(iter.hasNext) { val next = iter.next val otherIter = otherJavaList.iterator while (otherIter.hasNext) { val innerNext = otherIter.next .... } }
It depends on what on want you want to do in the background. For the vast majority of the things I would push to Sidekiq in a Rails app I'd instead just execute in a Future inside the same process in Scala and use that work queue. If it needs to be executed after a delay or repeatedly, and the task can be recreated if needed or it's ok to lose, then a scheduled thread pool can be used to create ScheduledFuture instances. Usually this the thread pool is restricted to a single thread, and all the actual work is done in another future on the default ExecutionContext. The tasks that remain tend to be more specialized and so are less likely to have a generic solution. If you want to execute the work on a remote process but do so immediately then the task would be put on a message queue of some sort, like RabbitMQ, SQS (if on AWS), or Akka mailbkxes. If you want to persistently store a delayed task then persist the details of the task in the data store of your choice and have a scheduled future that checks that, either in the same process or on a separate worker.
This is vastly understating things. Maybe you worked on an eng team with a separate platform/infra team that maintained sidekick for you? You need a persistent storage mechanism for the queue and you have to manage the processes and or node to run them. Not unlike running nodes running scale and a message queue
I am not super familiar with DBIOAction, but if you look at how the conversion between DBIOAction and Future works, it's probably possible to adapt the code so that it instead returns an IO. While this is probably better than going DBIOAction -&gt; Future -&gt; IO, you'll still want to be careful to understand the semantics of how your conversion works. 
&gt; is this the idiomatic way of making the actor system/materializer available to the http client? No, you should probably be giving those to your client via some kind of dependency injection (see constructor dependency injection or the cake pattern). Another HUGE thing: do not use `await`. Try val r = for { barData &lt;- ... totoData &lt;- TotoService.listToto(code) } yield barData match { case Some(c) =&gt; Toto(c, totoData) case None =&gt; StatusCodes.NotFound } instead
Thanks. I'll take a look now
There are various 'value class' or 'newtype' options available for Scala, but I don't think I've seen the 'opaque type alias' trick wrapped up in an easy-to-use way before. Detailed usage is in the gist, but quick example: object Main { import newtype.Newtype val Km = Newtype[Double](0.&lt;=) type Km = Km.T def main(args: Array[String]): Unit = { val dist1 = Km(1) val dist2 = Km(2) val totalDist = Km(Km.value(dist1) + Km.value(dist2)) } }
Here, go nuts and never touch `Future` - def fromTheFuture[F[_], A]( ff: F[Future[A]] )(implicit F: Effect[F], ec: ExecutionContext): F[A] = ff &gt;&gt;= ( f =&gt; F.async { cb =&gt; f.onComplete( r =&gt; cb(r match { case Success(a) =&gt; Right(a) case Failure(e) =&gt; Left(e) }) ) // continue on ec } ) Whenever you have anything returning future, wrap the call in either `Effect[F].pure` or `delay`, and pass it to this function and you don't have to deal with `Future`'s api pollution, and you can keep your codebase relatively clean for any `Effect`
The sbt dependencyTree command does include transitive dependencies. Given that output, you only have two direct dependencies and I was able to cofirm neither have transitive dependencies. Is the project set up to have submodules? If so and if you didn't configure the root module to aggregate the other moduels, then running `sbt dependencyTree` by itself will only show the dependencies of just the root. You can run the command against a specific submodules I stead if you give it a prefix. For example, if you have a submodules named "foo" then you could run `sbt foo/dependencyTree` to run the command in the context of that submodule.
The IO also has a [fromFuture](https://github.com/typelevel/cats-effect/blob/a4468ba715efba1581a79db7c29d1711fedbcfb8/core/shared/src/main/scala/cats/effect/IO.scala#L918) method. its just that I don't want to call it in each and every of my slick methods because there are simply too many of them.
A library implementation of the JOSE suite (the standards JWT are built on) for scala. I'm running up against trying to balance making things extensible but still type safe when parsing them. Does anyone have patterns they like for parsing things into one of potentially many classes, and allowing the user to define some of those classes?
If someone made a PR to Slick abstracting away Future, without breaking normal usage, I think a lot of people would be really happy about that. That said, one big challenge is what to do about the fact that most of the DBIO APIs take an implicit ExecutionContext, to use when it's turned into a Future. Maybe those operations could be written independently for the different monad types. Or you could replace ExecutionContext with an abstract type, and in the IO implementation use DummyImplicit or something. Or maybe someone has a better idea.
If you use sbt-coursier there is `coursierDependencyTree` https://github.com/coursier/coursier#printing-trees
The idea simply was to minimize the devops "overhead". We already have a reliable database / data storage. Goal was to be able to just spawn as many worker nodes as we want and they would be using the existing infrastructure to organize. Without this restriction a MQ would be the ideal solution I guess.
You're right, but: We already have the persistent storage mechanism (a MongoDB and a Postgres instance). The ideal case would be to just throw in a Docker container running the worker processes and spawn as many of them as we want on different servers. With something like Sidekiq that would actually be a 30 minute task, that's why I was trying to find something in that direction.
Actually AWS. I will check if this is an option, thanks!
It's more like we don't want to actively use Akka ourselves. If Akka is a dependency of an existing framework / library and the framework / library does integrate with our codebase without problems, we don't care about it's dependencies (well, obviously, we care about the resource usage of the framework / library and through that of its dependencies).
Oh nice, I didn't know that Quartz actually has a clustering functionality. Will definitely look into that, thanks!
implicit conversion?
What is the benefit in comparison to [refine](https://github.com/fthomas/refined)?
Beautiful! Thanks! I recently had to do this and my solution was to use `implicitly[LiftIO].liftIO(IO.fromFuture(IO(f)))` to lift a future inside an IO into some arbitrary Effect F. But your solution cuts that trip by half.
Refined is meant to model refinement types and as such implements conversions between types that are compatible. Newtype is meant to enforce incompatibility between all wrapper types (i.e. they are nominal types) with an extra benefit of being able to add on runtime validation rules for the new types.
`Try(Km(-1))`
Does Quartz allow to have two different types of instances - one only submits the jobs and another one only executes them?
With Sidekiq I only need to deal with operations tasks, with custom solution it's both operations and coding. When using Heroku or AWS Elastic Beanstalk Sidekiq "just works". Sidekiq has lots of stuff figured out and tested and allows the developer to concentrate on the product. It deals with multiple queues, priorities, error handling, restarts, scatter/gather, has a UI, integrates with existing monitoring tools, has plug-ins to deal with complex scenarios etc.
I think `refineV`can be used for runtime validation as well. What's the difference between your runtime validation and `refineV`?
Added a hyperparameter search functionality to \[doddle\-model\]\([https://github.com/picnicml/doddle\-model](https://github.com/picnicml/doddle-model)\) \(an ML library I've been working on\). The API looks like this: `val search = HyperparameterSearch[LogisticRegression](crossValidation, numIterations)` `val bestModel = search.bestOf(x, y) {` `LogisticRegression(lambda = gamma.draw())` `}` For other examples see \[doddle\-model\-examples\]\([https://github.com/picnicml/doddle\-model\-examples](https://github.com/picnicml/doddle-model-examples)\).
Check out [https://github.com/estatico/scala\-newtype](https://github.com/estatico/scala-newtype)
Step 1. sbt Hmm, is this avoidable? Maybe Mill, or Gradle, or Pants, or Bazel instead?
Is it avoidable? Of course. Is it advisable for a beginner to use something else when starting out? Probably not. sbt is still the de facto standard build tool for scala and all other options are only used by a fraction of the community, so there's less help when you need it and possibly more potential problems only you encounter. Personally I don't see why using sbt here is a problem, anyway. 
I’m throwing an exception (illegal argument) on validation failure; refined is returning an Either. The key difference is really about type incompatibility, though, not about validation.
Not sure exactly what you're asking. But I'm gonna bet that whatever type of setup you're looking for is well supported by Quartz as it has been around for a while and a battle tested. Basically you define jobs that are run in separate threads. You then define triggers which spin off the jobs. It's very customizable. See [Quartz Tutorial ](http://www.quartz-scheduler.org/documentation/quartz-2.x/tutorials/).
Well, if you're not in the mood for a rewrite, then there's not much we can do. Converting your code to something that is more performant and more compositional is never a quick fix unless you start that way on principle. The best you can do working with legacy code (which is what you're working with - `Future` is legacy), is to forge ahead and start using the new style, and hopefully the changes over time touch enough of your base.
One small suggestion - change the IceCream class to a case class - its a better default type for data holders - as you are a probably going to want to pattern match on flavour?
Spoiler for my next blog post :) 
I think the most complicated thing I've ever had to do with SBT was figure out the changing syntax between older versions and 0.13 -&gt; 1.x when going through some outdated tutorials. Second most complicated thing was using that newer syntax to specify how to run a task Gradle-style using a subproject in the folder. Neither of those things are anywhere near what most people will need today for their projects. SBT has its problems, as does any complicated build tool really, but I agree for this use case it's just easier to set something basic up so people can get familiar with it. The majority of what most people will ever need to do with SBT anyway is just add their dependencies and change the project version. Maybe configure package publishing or docker but that's easy with a couple plugins.
Honestly starting with Mill probably wouldn't be so bad. Apart from being simpler/faster, it also comes with a bunch of things out of the box (fat-jars, publishing to sonatype, ...) that SBT would have you fumbling with plugins to setup. There's less docs, plugins &amp; tribal knowledge, but there's also less *need* for docs/plugins/tribal-knowledge, so even that comparison may be a wash. Disclaimer: I wrote Mill
&gt; more imperative solutions such as play or akka-http I am not sure what you mean by "more imperative", but akka-http being slow has very little to anything to do with imperative vs functional. Actually akka-http just uses akka-streams under the hood, which about as functional as something like monix task (in the sense that the implementations try and expose a functional API but underneath they are very imperative and low level to try and improve performance) The reason why akka-http is slow is simple, its because its new, and the underlying streaming library, akka-streams is also new (its in fact newer than fs2 or monix). Which means basically it hasn't been optimized yet, and its something that publicly the akka guys have stated is not their focus until recently (focus being performance).
Using SBT in your tutorials because it's popular and there's more help available for it is self-perpetuating. I'd advise beginners to start with Maven; it's much simpler, much better documented, has a bigger overall community and better IDE integration. (It can't easily cross-publish libraries for multiple versions of Scala, but beginners don't need to do that, and that capability isn't worth all the downsides of SBT IMO).
Downside is no integration with IntelliJ... unless I'm wrong?
The readme actually tells you how to generate Intellij project files with a simple command. I haven't personally tried it out though.
I wonder how much additional overhead this would really have on a warm JVM: ``` import scala.collection.JavaConverters._ for { next &lt;- javaList.iterator.asScala innerNext &lt;- otherJavaList.iterator.asScala } { ??? } ```
In code, there are often patterns repeated several times that you'd like to edit together as if it was juste one. As a basic example, say you have a parameter list: (foo: Foo, bar: Bar, baz: Baz) And you want it without the types. Select the first `:` as in: (foo[:] Foo, bar: Bar, baz: Baz) Press cmd+D twice, ending up with all `:` selected: (foo[:] Foo, bar[:] Bar, baz[:] Baz) Then press shift+alt+right to select the space and the type following each `:`, and press delete to remove them all at the same time: (foo, bar, baz) 
It works, just remember to regenerate the project files whenever you edit the build file. 
Thanks a lot. i have followed your advice. It did get a bit ugly by inserting the actor system and materializer into implicit context using the constructor parameters. 
yes that's what i thought (see my reply as well to /u/kag0 above). Thanks
You're right that async/await doesn't block threads, but I'd try to avoid it for a few reasons. In general I avoid macros in logic code, it can be unclear what the code is doing and hunting bugs can become impossible if the bug is really in the macro. While the library is not unmaintained, it is on a slow development pace. There are limits to the macros and some language features can't be used, resulting in inconsistent mixing of macro and monad. One big one is compatibility, any code using async/await will probably block your ability to do minor version updates for a while, and likely need to be rewritten to work with dotty/3. Not world ending stuff, but the kind of things that make life harder down the road for not much short term gain. (And if I'm totally wrong and async/await got mainlined into the language at some point then well, f me)
You should do streamlined video tutorials as well. That might take more time and editing but it will be much more beneficial.
I suggest that you use a giter8 template that is suited for the project you are trying to do, like `sbt new scala/scala-seed.g8`. Many big libraries/frameworks have dedicated or community-created templates, and you can figure out how things are done right away by reading the build file. I do recommend that you read the guide though. 
oops .... one thing I would say is do you want to mix behaviour and data in your case class? There is no correct answer but I would suggest a companion object is correct if you are converting or a trait if you are doing something related to ice cream
in my experience the majority of commercial and open source scala projects use sbt. Some use Maven due to familiarity from previous JVM projects. The other tools you use never seen on Scala projects (In the UK that is)
My goto example for UI frameworks is the tree editor ([like this](http://www.react4s.org/examples/tree-editor)), because it's a relatively short example that demonstrates: - How to compose UI together from smaller parts recursively. - How to update a small subset of the model (eg. a node). - How much of the whole tree that must be rerendered (to virtual DOM or otherwise) on each edit. - How much the framework leaks into the model (in Binding.js, is it full of mutable Vars?). How would that look with this approach?
We built our version of a solution to this on top of rabbitmq and quartz. Quartz just fires the scheduled events and about 100 lines of code manages the task processing. Retries and permanent failures are all managed via rabbit dead letter queues, and the rabbit management UI handles all the common steps like requeuing failed tasks on demand, etc. Don't be afraid to roll your own, it's not that bad.
Don't futures run eagerly though? Does this really work?
Yes forgot you can define implicit parameters on a constructor :) much better now. tx again
Agree with all the points, but for me the "fun" is going away when I try to debug something and even with the incremental compiler it take 4mins to compile. Scala doesn't scale on bigger projects. I love Scala but the slow compilation is its biggest issue.
A number of people have been asking me on how I made MDC work with Monix Task in my http4s app, so I put up a post with working code and some nuances explained for people willing to try it out.
Why wouldn't this work, and what does `Future`'s eager eval have to do with this code? `Futures` are evaluated upon instantiation, yes, but then, you may choose to lift them via `delay` or `pure` depending on your use case. It changes the semantics only insofar as your evaluation strategy lifting them into `F`. It has nothing to do with whether the function works or not. The logic is sound. In fact, that's how it's done natively, in [Cats' IO](https://github.com/typelevel/cats-effect/blob/master/core/shared/src/main/scala/cats/effect/IO.scala#L330). I simply abstracted to any generic `F: Effect`
Nice, I ended up using `Logger.takingImplicit` to solve the same problem (as a bonus it actually enforces that you log this value otherwise it won't compile). This is definitely a much better solution than what I had to do previously, i.e. manually having to update the maps whenever the context switches. I suspect this solution will have better performance than the others, due to avoiding contention on the MDC's underlying map. This is speculation though, haven't benchmarked anything
Akka-http is more concerned about latency stability and delivery of requests rather than raw throughput.
You may want to look at this https://scalafiddle.io/sf/KEznYyM/2 
Are you not able to scale horizontally? Do these requests have to return sync or can they be dispatched as async tasks?
As Scala is compitable with Java you could use Java libraries for such tasks. Something to look into for this would be: - Gson - library made by Google (https://mvnrepository.com/artifact/com.google.code.gson/gson/2.8.2) - Jackson (https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind/2.9.5) - Json (https://mvnrepository.com/artifact/org.json/json) Gson and Jackson are quite powerful libraries.
Thought about this when reading the OP: http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html
This was something else I forgot to ask. Is this a routine/best practice thing to do to use the Java libraries or is it seen as a backup? 
I don't see this as a bad practice. Yes, this is not really a functional approach since Java is imperative, but if it was a really bad practice, maven repository wouldn't have added a section for Scala build tool, would they? I believe this is totally fine to use this if you need to do something quickly. In this case I don't see a reason to learn a whole 'Play' framework just to read a JSON a reasonable approach. If you are going to use the framework later, you can just convert the code to use the framework approach, but for now I'd just stick to Java libs.
Which kind of brings me back to original Q what is the core/ a couple of core FP library for this? u/nambitable presented a code example for initialising an object using lift. Granted the actual specific problem for me is extracting from a document full of JSON files then initialising an object to simulate a stream but there seems to be a lot of different approaches which worries me that I'm missing something obvious that a professional would do in a few seconds and move on to less trivial matters. 
Take a look at this blog post - http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html. You can use uJson, but also explore the alternatives mentioned. And btw, Play and Lift are full blown frameworks, but json is just a part of it i.e the project play json is just a module within the play framework. This is the actual source code for the project - https://github.com/playframework/play-json
Appreciated. Someone else posted that as well. I think a big part of my problem is caused by the fact I'm still getting used to the scala/java ecosystem at the same time.
Use Circe, it is by far the best json library IMO. https://circe.github.io/circe/
Just this!
Try Circe
I'm very fond of Monix to the point I became contributor. I've switched to its `Observable` from `Akka Streams` and I didn't look back. :) What I like about it is that it feels pretty high-level but exposes so many useful operators that it still allows me to express complex transformations with precise control without getting into internals. Not to mention performance which in my use cases was way above expectations.\\ Unfortunately [documentation](https://monix.io/docs/3x/) isn't quite complete and is still in process of being updated to upcoming 3.0 release but that's something we are aware of and are actively working on. I hope to give it even more focus after we're done with 3.0 release so we actually have more "real world" examples.\\ On the bright side, implementation for streaming data types (`Observable` and `Iterant` ) is actually quite simple to understand most of the time + ScalaDocs are rich in information so you can find a lot of information there. Just [skim through that](https://github.com/monix/monix/blob/master/monix-tail/shared/src/main/scala/monix/tail/Iterant.scala) to know what I mean. The `Observable` is based on ReactiveX so any documentation for that will be useful too. Also we are also happy to answer any questions and help out on monix/monix gitter channel. \\ If you decide to check out Monix and ecounter any issues related to documentation, learning curve etc. please either let us know on gitter or through issue on github (monix.io or monix) - any feedback is appreciated. :)
I imagine most json libraries use some form of reflection to accomplish basic pojo ser/de. You can write custom serializers for classes it has a problem with if I remember.
The trick is that you delayed the thing that creates the future inside the `F`.
I found this setting in IntelliJ IDEA today. I am not sure which IntelliJ IDEA/Scala plugin added this feature, so maybe you need update your IDEA version if you want this feature. Though I guess a lot of people here are get used to with type inference, it may still good for beginner or reading someone else's code.
Sad to see this downvoted so much. ..Especially since I fully agree. Everything is very basic and straightforward here -- but SBT with its extremely cumbersome folder structure (inherited from Maven I guess) sticks out like an eye sore. I think the worst part is that SBT is nothing like regular Scala. There rules are completely different. Assignments that are actually immutable datastrutures just casually floating around in the "global scope" -- somehow interpreted and executed sequentially like an imperative program when SBT "sees" it. I still to this day don't know exactly how it works even after working with SBT for 5 years. Would be much cleaner with just a simple `build.sc` file with regular plain old scala code -- and a preferably a much simpler folder structure. It's great to see tutorial like this though :D We need to cater for the newcomers if we want to grow as a community.
The typelevel libraries have mostly converged on a mono-build style that works pretty well for me. A simple one is [atto's build](https://github.com/tpolecat/atto/blob/series/0.6.x/build.sbt) which does all the things you ask.
It's nice to be able to see the type, but it makes editing awkward, since it inserts "voids" in the source code where your text cursor skips over. It doesn't feel finished to me.
So for comprehensions are generally syntactic sugar for `flatMap`, the method that characterizes a Monad. `Option`, `Either`, and `Future` are all monads. You can read about the flatMap desugaring [here](https://docs.scala-lang.org/tutorials/FAQ/yield.html). [doobie](https://github.com/tpolecat/doobie) is one of several libraries that lift database operations into a monadic type, but what's cool about doobie is that you can take actual sql, interpolate the values you're passing as inputs, and build up a program in a particular monad - `ConnectionIO` - which can then run as a single transaction and return a more general effect type like `IO` or `Task`.
Just weigh in on the pro-sbt side: I'm looking to get up to speed on some basic Scala so I can read, and offer basic pull requests on, the code of a team that I work with. Stuff like tweaking API behaviours, extending options etc. I've done plenty of backend FP, just never in Scala, or Java for that matter. Because they are a typical Scala team, their projects ALL use sbt, so I'm very glad to have an intro that uses sbt. It's just a practical, pragmatic thing for me.
Doobie looks really useful, I utterly detest orms and it might be just the right level of abstraction. Thanks!
Try jsoniter-scala: https://github.com/plokhotnyuk/jsoniter-scala In most cases it works on par with the best binary serializers for Java and Scala: https://github.com/dkomanov/scala-serialization/pull/8
 val serialized = MessagesToBytesSerializer.serialize(RemoteActionsResults(List(ErroneousRemoteAction("")))) MessagesToBytesSerializer.deserialize(serialized.right.get) is a Left, also in Main. The test is failing because it expects all deserialisations to be Right(command) 
Thank you for testing the code on your machine. On my machine, in the main class all deserializations are instances of right and in the test class the first two deserializations are instances of right and the last is an instance of left. This code actually runs in productions without a problem, but when I wrote the unit test for it, the test failed.
Appreciated. If you're feeling nice I edited my post and put in the solution I came up with to the issue using the lift utilities (they were the first ones that clicked for me personally) 
I tried pretty much all the libraries suggested here and in the end lift was the first one to click. I edited my post to include the solution I came up with. Which is also my first publically accessible scala code so any feedback appreciated. 
Not really idiomatic use of for-comprehensions although I'm certainly no scala expert. I would just use maps val perfdata = json.childern.map ( acct =&gt; acct.extract[LogEntry ).map( _.record.values) println(perfdata) println(perfdata.size)
I kept trying to use maps but It wasn't playing ball I'll try your solution when I get home. Im definitely out of my depth thats my first piece of "scala code" that does anything as a python data engineer. 
I'm not 100% it'll compile, sometimes it can't infer the types and you have to be more explicit. I'm actually sure it can be written even more concisely but I don't have your case class on hand to throw it in a REPL and test it out. If the maps aren't playing ball, the best bet is to try to understand why. That will give you a better understanding of how scala and functional programming works. That's what I did. This map or flatMap doesn't work. How do I fix it? 
We rolled our own macro\-based serializer a few years ago: [https://github.com/frugalmechanic/fm\-serializer](https://github.com/frugalmechanic/fm-serializer) I think its worth checking out, and it supports JSON. import fm.serializer.json.JSON case class Hello(name: String, list: List[Int]) val hello = Hello("World", List(1,2,3,4,5)) val json: String = JSON.toJSON(hello) val hello2: Hello = JSON.fromJSON[Hello](json) require(hello == hello2)
Next stage for me is the date type fields. Id like to insert this data into postgres at some point and im not sure castinf timestamps as strings will play nicely. 
I just retested the code on a third different machine, with different OS, and i still have the same problem. executing the Main class, the code is able to serialize and deserialize. Right(SuccessfulRemoteAction()) Right(FailedRemoteTestAction()) Right(ErroneousRemoteAction()) Right(RemoteActionsResults(List(ErroneousRemoteAction()))) but running the test give me, on the last serialization/deserialization of the class RemoteActionsResults(List(ErroneousRemoteAction())): Left(java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field com.example.StandardOutputPrinter$RemoteActionsResults.resultList of type scala.collection.immutable.List in instance of com.example.StandardOutputPrinter$RemoteActionsResults)
Duly noted. Live was first framework that worked reasonably (solution to a basic etl problem in original post now) but ill play with them again this wknd. 
2.12.6 has now made it to Maven Central.
I would have to disagree, at least personally I’m not a big fan of implicit magic 
[Quill](http://getquill.io) is a LINQ based on this [paper](http://homepages.inf.ed.ac.uk/slindley/papers/practical-theory-of-linq.pdf). It allows users to express queries in Scala (including for-comprehensions) and run them on a target language like SQL and CQL (cassandra).
Legit question: why is Circe the best? [Spray-Json](https://github.com/spray/spray-json) (which is the default [recommended by akka-http](https://doc.akka.io/docs/akka-http/current/common/json-support.html) unsurprisingly) seems to outperform Circe on most of the things on Circe's own performance page: https://circe.github.io/circe/performance.html Spray json is what I usually go to for JSON parsing in scala, so I'm wondering why Circe is the best.
I think they are useful when used properly, but I’m just not a fan of the magic they create
[Twitter have abandoned **C2**!](https://www.youtube.com/watch?v=pR5NDkIZBOA)
Currently, download page doesn't tell about CE or English editions, just give me links for Linux and Mac OS binaries: http://www.oracle.com/technetwork/oracle-labs/program-languages/downloads/index.html
Nice spot! I've updated the initial post.
I really wish Jebrains would finally fully support Scala, now it's only half baked. When using (even popular) libraries IDEA shows completely broken code (red everywhere) while in reality it's a valid code (e.g. ScalaCSS, Udash, Shapeless). There are even issues with Scala code without macros :-(.
Look here: http://www.graalvm.org/downloads/
You can use the java websocket (Here)[http://www.baeldung.com/java-websockets] you have a guide in java,but it is easy to adapt it to scala
I'm confused. It is really enough to just enable that flag and you have Graal running?
This works: https://www.playframework.com/documentation/2.6.x/ScalaWebSockets 
Ive never had issues, probably because all of my models are immutable ADT's
Here's 3 reasons (although I could probably list more) 1. I'm using spray-json at the moment (I require ScalaNative support) and the biggest gripe I have is the error handling. In Circe, Encoders and Decoders can gracefully handle errors in the stardard monadic way, you can easily recover and ignore errors etc. . In spray-json you rely on exception throwing. This is really annoying to deal with when managing malformed input, it's also far slower. 2. Circe's use of cursors is great for debugging, if a parse fails, you know exactly where it was, always. Id sacrifice a small amount of performance for that any day. 3. It's functional first. With support for Cat's and Scalaz Circe fits really well with libraries like Monix (Tasks) and Monadic/Applicative error handling. Because I tend to write in that style it suits me well.
That would be fantastic. This is my first time dealing with fs2 and the documentation seems to cover very basic cases only. I tried to figure out how to create a simple function wherever these streams exist that would just let me send a message and could not for the life of me figure it out.
What's the use of having `HutRepository.empty` return an `IO` when you immediate call `HutRepository.empty.unsafeRunSync()`? Otherwise nice post.
Thanks for putting that together, but I'm still missing something. The clientStream publishes one message that goes to the client when the connection starts, but then never again. The scheduler is sending out messages. How can you send "Hello," or some other argument as a string, again?
Judging from directory layout, it looks like GraalVM release is based on java 8 and it might have slightly newer version of Graal since java 10 released 3 weeks earlier than GraalVM
Yes, the CE versions are in the [GitHub releases page](https://github.com/oracle/graal/releases) but there's not binaries for Windows or MacOS at the moment. They've said that they are working on that. It would be interesting to see your benchmarks on both, the CE and EE versions, and see the difference in improvement. In either case, there seems to be a perfomance gain when running Scala code on Graal.
Graal, the compiler, can be used for Just-In-Time compilation or Ahead-Of-Time compilation. In the recent JDK versions, they introduced a feature that relies on AOT to help with the start-up. That AOT feature is very different from the AOT one in GraalVM, that does the native-image generation. But both depend on Graal, the compiler. Since the feature that helps with the start-up in recent JDK versions uses AOT compilation, a version of the Graal compiler is also included in the JDK. That is why you can enabled it with those options, so it does JIT compilation, replacing the default compiler. But you only have the Graal compiler, nothing else. In the GraalVM binaries, you have by default the Graal compiler turned on, the Truffle component that involves the different languages, and the SubstrateVM component that does the native-image generation with the help of Graal for AOT compilation. And probably more things that I'm not aware of.
The problem is that the answer depends on what you're trying to do. For example, your batch process could offer a topic that it publishes events to, then the code in your http4s service would subscribe to that: batchProcess.topic.subscribe(10).map(Text(_)) I did something like that on a recent project that needed to push updates to data when it was changed in a store (database).
What would send the updates in the batch boils down to this (super simplified): { //... startup stuff val data = extractData(args) sendMessage("Data extracted") val calcResult = runCalculation(data) sendMessage("Calculations complete") //... more stuff } It's a lot of CPU-bound work, and we just want to send status updates to our UI. Anyway, I'll take a look at the link. I've scanned it a lot in the last week, and someone in the http4s gitter noted that the end of it, which happens to be the topic part, is 'all wrong.' Maybe it'll click, though :) 
Looks like I have a working implementation brewing with using topics. Just need to fiddle with it some more. Thank you! 
If you run with `sbt run` you will see that the error is here too. From here you can conclude that the difference is not run vs test, but sbt vs without sbt. If you search a relevant part of the exception (like `cannot assign instance of scala.collection.immutable.List$SerializationProxy`) with the keyword sbt with your favorite search engine, you should find some explanation of the issue, and a workaround: add `fork := true` in build.sbt.
It sounds like what you're trying to do is combine some sort of synchronous action (imperatively send a message whenever I want) with a stream interface. There's usually two ways of having synchronous actions combine with streams: convert the synchronous action into a stream itself and use the normal set of stream combinators or use an intermediate "store" between your synchronous actions. This is a common theme in all streaming systems, all the way from single-machine streaming libraries, to message brokers such as Kafka. I'll give an example of the latter. In this fs2 case there are a lot of choices you could use for your store. The most straightforward is probably a queue. Here's the most straight-forward translation of your Node code. ``` // Probably doesn't quite compile, but should be close; I don't happen to have a compiler handy unfortunately val wss: IO[HttpService] = HttpService { // We'll do the exact same thing for every incoming HTTP handshake request so we're ignoring it _ =&gt; for { // This number determines when to start blocking if your websocket client isn't // consuming quickly enough. // Change the kind of queue to change the behavior (e.g. you could have a circular // buffer that throws away responses if the client isn't consuming quickly enough // instead of blocking) ws &lt;- Queue.bounded(5) dealingWithIncoming = Sink.lift(x =&gt; IO(println(s"Received: $x"))) httpHandshakeResponse &lt;- WebSocketBuilder[F].build(ws.dequeue, dealingWithIncoming) _ &lt;- ws.enqueueOnce(Text("something")) } yield httpHandshakeResponse } ``` You might ask why don't `akka-http` or `fs2` provide an equivalent of the `ws` object in your Node example? It turns out that this intermediate store is exactly the equivalent of `ws` (which is why I've named it such) and you gain added flexibility in getting control by creating the store yourself and as I said before, it's a common idiom you'll see in streaming libraries and frameworks that is language-agnostic. As an example of this flexibility, apart from the kind of queue that I mentioned earlier, you might want to have the store persist across multiple websocket connections so that if a client disconnects and later reconnects, it sees all the notifications it's missed in the interim. In that case you'd lift the `Queue.bounded` out of the scope of `HttpService {...}` and build it outside `wss`. Another way is to convert your synchronous actions into streams, which can be nicer architecturally depending on what you're doing, but it's harder to give an off-the-shelf example since this depends more on what exactly your synchronous action is.
Interesting. I've seen the Queue used to implement these streams but didn't think I'd be able to use them for my case. I'll try to whip something up based on your notes!
This is where I wound up: https://gist.github.com/rcornell/785713f212bc56f9391f85f69aac7648 I can't say for sure if I'm committing unholy sins or not, but it's working for my use case. Those 'unsafe...' calls probably needs to be converted to non-unsafe versions.
Came up with this: https://gist.github.com/rcornell/c48b86273b9897955c33637781955900 How bad is this?
Cool. Looks good to me. 
I'm on mobile, so it took me a while to find the slides. To spare everyone else the hassle: [the slides are on GitHub](https://github.com/changlinli/types_presentation_slides/blob/v0.2.0/slides.md).
`UserAlgebra[F[_]]`, by its signature _does not know the meaning of `F[_]`_. The fact that F might talk to a database, and that database might be down, is a responsibility you have explicitly disowned by parameterising on `F[_]`. From the `UserAlgebra`'s point of view `F[_]` might just as easily be a `State[List[User]]` (and in your tests, it may very well be), for which "can't talk to the database" is only possible if your JVM has disintegrated. That said, you might want to make your program aware in a general sense of the fact that you expect your `UserAlgebra` to be backed by some external service that might fail. You'd encode that into your types with something like: trait UserAlgebra[F[_]] { def get (id: Long)(implicit E: MonadError[F, ServiceFailure]): Option[User] = ??? } The potential error condition is still a property of the effect, but it's now an explicit property that any code that interacts with that effect can interact with.
So basically if I don't want the `native-image` binary then I'm OK with just using `-XX:+UseJVMCICompiler`? 
Ah that makes sense. Is this similar to how mtl encoding works?
Exactly.
Yeah that's exactly why I was pretty unsure about my 'implemention.' Which gitter? Fs2?
Thank you so much, that did solve the problem! i'll leave this here for anyone reading this in the future. https://www.scala-sbt.org/release/docs/Running-Project-Code.html
This is a very cool idea, called 'lightweight static capabilities'. The 'labels' that Changlin talks about are the capabilities. I summarised and spoke about Oleg Kiselyov's paper on LSC: https://github.com/yawaramin/lightweght-static-capabilities
It would be very helpful if you could give a minimal complete example that we can copy and paste and see a compile error.
I tried your example. If we ignore the last two lines, the relevant error is this: &lt;pastie&gt;:18: error: class Cow needs to be abstract, since method eat in class Animal of type (food: Food)Unit is not defined (Note that Food does not match Grass: class Grass is a subclass of class Food, but method parameter types must match exactly.) class Cow extends Animal { ^ &lt;pastie&gt;:20: error: method eat overrides nothing. Note: the super classes of class Cow contain the following, non final members named eat: def eat(food: Food): Unit override def eat(food: Grass) = {} ^ I think this is the critical part of the error message: method parameter types must match exactly You need the override to have the exact same signature as the abstract method: override def eat(food: Food) = {} This will solve the compile error but it is actually also incorrect in terms of this design. The correct method signature for `Animal` and all its subtypes should be: def eat(food: SuitableFood) &gt; What if cow can eat two types of food. Grass and cookies? Under the current design, this should not be possible. Each animal should have _one_ suitable food, not two or more. If you want two or more suitable foods for each animal at the type level, the best way to do that is to use a different technique called a typeclass. P.S. you can format code in Reddit messages by prefixing each line with four spaces. E.g. here is what I have after playing around with your code a bit: class Food class Grass extends Food class Cookies extends Food class Fish extends Food abstract class Animal { type SuitableFood &lt;: Food def eat(food: SuitableFood) } class Cow extends Animal { type SuitableFood = Grass override def eat(food: SuitableFood) = {} } val bessy: Animal = new Cow bessy eat new Fish bessy eat new Cookies
Ah okay. Thanks! I will look into typeclass. 
&gt; The left/right bias allows you to treat Either as a monad, and it works just fine via fold, bimap, and the usual monad operations. Okay, I admit I was loking at pre-2.12 docs writing that. As I see, 2.12 removed the need for using `.right` to access the operations in the *conventional* (right-biased) case, but there's still the mismatch between what the `Either` nominally represents (a simplified union of two types) and how it is primarily used (to represent generic completed/failed results). With the inclusion of union types proper in Scala 3, I was wondering if there is a notion to replace `Either` with something that has a more descriptive name and a more friendly interface (say, `Right | Wrong` instead of `Left | Right`, inclusion of `.failed` etc.)? &gt; In general (or at least, production), I stay away from Try because it's not referentially transparent. It's entirely off-topic, but isn't `Try` referentially transparent (as opposed to `Future`)?
&gt; but there's still the mismatch between what the Either nominally represents (a simplified union of two types) and how it is primarily used (to represent generic completed/failed results). I definitely agree there. That's partly the reason why the notion of generalized Coproduct types (i.e. your standard Sum type) has taken off in Shapeless, Iota and other libraries geared towards either principled sums or Coproduct derivation. The answer is complicated, and I'd have to defer to someone else to answer exactly *why* it turned out this way. The pessimist in me believes it was simply that Scala didn't care about principles until very recently. Too little too late. &gt;I was wondering if there is a notion to replace Either with something that has a more descriptive name and a more friendly interface (say, Right | Wrong instead of Left | Right, inclusion of .failed etc.)? The reason it's called `Left | Right` is that that's exactly what it is. It might be that we use it for error handling, but only because there is a lack of support for `MonadError` in the standard library, unless one adds Scalaz or Cats as a dependency. I wouldn't agree that Scala should be encoding `Either` as `Right | Wrong` etc because that's not the fundamental point of `Either`, which is the naive binary Coproduct, and it encourages a usage and pattern that are frankly, only a thing because of a lack of support for better features. We should be deferring to `MonadError` or an Exception monad as a better way of handling error DSL's and Throwables. &gt;It's entirely off-topic, but isn't Try referentially transparent (as opposed to Future)? Neither are referentially transparent because i can `Success(t : Throwable)` in the same way I can `Future.successful(t : Throwable)`. The monad laws are broken in almost [the exact same way as Future](https://gist.github.com/ms-tg/6222775). 
I don’t see how any function is lawful when you admit `throw e` as a method body. I don’t think Either is lawful in that case either. I think the preference of Either to Try is mostly a cultural shibboleth. The fact that any method can in principle throw in Java and Scala makes the catch NonFatal semantics of Try.map and Try.flatMap very convenient while at the same time not violating any of the laws that exist in cats, for instance, which does have MonadError instance for Try.
My point was that the use of `Either` as a way of handling errors is just a consequence of there not being good language-level support for principled error handling, or, at least, there wasn't until Scalaz and Cats came along. I've seen a few instances of people using `Either` properly (for instance, you can see its usage in contravariant Applicative (`Coapplicative`) and `Codivisible` deriving schemes - see [scalaz-deriving](https://gitlab.com/fommil/scalaz-deriving)), but in general, yes, the use-cases are limited. That's one of the reasons why it's not included in the standard Haskell Prelude, I'd imagine. They have an Exception monad and `IO` :)
A more direct translation is this: package javalintest import io.javalin.Javalin import io.javalin.embeddedserver.jetty.websocket.WebSocketHandler object Main { def main(args: Array[String]): Unit = { val app = Javalin.start(7000) app.ws("/websocket/:path", { ws: WebSocketHandler =&gt; ws.onConnect(_ =&gt; println("Connected")) ws.onMessage { (session, message) =&gt; println("Received: " + message) session.getRemote().sendString("Echo: " + message) } ws.onClose { (session, statusCode, reason) =&gt; println("Closed") } ws.onError { (session, throwable) =&gt; println("Errored") } }) } }
Got it going :) import io.javalin.Javalin import io.javalin.embeddedserver.jetty.websocket.interfaces.{CloseHandler, ConnectHandler, ErrorHandler, MessageHandler} import io.javalin.embeddedserver.jetty.websocket.{WebSocketConfig, WebSocketHandler, WsSession} // "io.javalin" % "javalin" % "1.6.0" object JavalinSocketServer { import org.slf4j.{Logger, LoggerFactory} implicit val logger: Logger = LoggerFactory.getLogger(this.getClass.getName) def main(args: Array[String]): Unit = { val app = Javalin.create() val c = new MyConfig() app.ws("/:path", c) app.port(1337) app.start() } } class MyConfig extends WebSocketConfig { override def configure(webSocketHandler: WebSocketHandler): Unit = { val handler = new HandlerDefs() webSocketHandler.onConnect(handler) webSocketHandler.onClose(handler) webSocketHandler.onMessage(handler) webSocketHandler.onError(handler) } } class HandlerDefs extends ConnectHandler with ErrorHandler with CloseHandler with MessageHandler { @throws[Exception] override def handle(session: WsSession): Unit = { println("Connected") session.send("Oh hello") } // Error override def handle(session: WsSession, throwable: Throwable): Unit = { } // Close override def handle(session: WsSession, statusCode: Int, reason: String): Unit = { } // Message override def handle(session: WsSession, msg: String): Unit = { println("Message received") session.send(s"You sent: $msg") } }
Which law is cats lacking on MonadError that would rule in Either but Try out? I don’t know of any except something like: fa.map(throw e) must throw. But if you do that you’d also rule out lazy types like Eval. Can you share a law so people can see what law cats has left out of MonadError?
Aah thanks. I wasn't sure how to properly scala-fy the implementation, so I went the long way around. Thanks for this.
Cool. The Scala 2.12 series brings compatibility between Scala and Java 8 lambdas, and since Javalin heavily uses Java 8 lambdas, you really need Scala 2.12. Fortunately, it's easy to tell sbt to use it. Put this line in your `build.sbt`: scalaVersion = "2.12.4"
Scala 3 has union types, making some uses of `Either` unnecessary.
I assume this was meant for me, but you missed. &gt;Which law is cats lacking on MonadError that would rule in Either but Try out? Consider [this](https://gist.github.com/ms-tg/6222775).
I have read that. However I don’t see how this changes if you replace Try with Either. Either would still fail in many of those same cases (either due to lacking a good equality on Throwable or due to a non-function throwing). So, again, what succinct law is missing *that Either passes* that rules out Try from being a Monad? The post you link to gives some examples, including using throw, which we virtually never assume when asking for lawful typeclasses, but it doesn’t show how Either would have somehow been better. As far as I can tell, it is just an example of how you shouldn’t use throw in functional programs.
&gt; I was rather thinking about replacing the right-biased version of _Either_ with a newly-named type, better encompassing the usage Result?