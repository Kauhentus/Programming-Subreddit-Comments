We're talking about **conferences and other professional settings** here. To answer your question about how to find a partner/get laid: 1. there are more apps out there (other than tinder) that helps you to find a relationship 2. have you considered the same sex? 3. go look up [r/incel](https://www.reddit.com/r/OutOfTheLoop/comments/5n4368/what_is_rincel/) 4. [indocardigan](https://www.reddit.com/r/scala/comments/9ysz5z/making_conference_a_safer_space_for_women/ea8jz8z) has a good point that it is really not as hopeless as what you said. Plus, we are talking about conferences and professional settings. STOP DEFLECTING THE ATTENTION TO NONRELATED ISSUES. At the end of the day, how you find your mate is not my or your other female coworkers' problem. We are not responsible to help you to solve it.
I'm glad you're making an effort, and I'm sorry for any harassment you've received due to your gender.
&gt; go look up r/incel Please do not lookup r/incel. It is a silly place.
&gt; And what exactly is "unsafe" about flirting? Annoying sure, but unsafe? Absolutely. For women, a man that they don't know flirting with them is a potential threat. They have no idea who this guy is or how he may take a rejection, and men are larger, stronger, unpredictable and entitled in general.
Alone on a street at night, sure. In a well-lit venue with tons of witnesses and probably cameras, I find that hard to imagine.
&gt; Alone on a street at night, sure. In a well-lit venue with tons of witnesses and probably cameras, I find that hard to imagine. It starts in a well lit venue. They have no idea what the man will do next. He could try making out with them in an elevator with no warning. He could show up outside their hotel room with a bottle of wine. He could send them facebook and twitter friend requests and then show up the next day asking why she hasn't friended him already. These aren't hypothetical examples. All of these things have happened before. Some men don't stop just because women don't respond to flirting or turn them down. They come back harder.
And I'm pretty sure there are already anti-harassment rules that cover any behaviors beyond taking "No" for an answer, without the need to demonize asking people out. It is my firm belief that men in tech don't deserve this level of fear and being treated as dangerous and lacking self-control, because of the actions of some minority. &gt; send them facebook and twitter friend requests and then show up the next day asking why she hasn't friended him already Oh the horror.
Why quibble over semantics? The word "safe" is overloaded in this political climate. You clearly know exactly what it means in this context.
Good observation based on unfortunately bad experiences. It’s almost incredible that such an unhealthy amount of ego and mediocrity can coexist so close. At the same, you will meet such proud people that will risk the whole project and put professional behavior aside just to satisfy their ego. And on another side, you will see “meh” people. Showing almost zero interest and motivation for new things. I met one person at small Scala conference that had a similar story. Hope she made it too. Good luck.
&gt; If a group doesn't want to collaborate with somebody in the first place, because they don't fit the mould, then no collaboration has been lost by "ripping on" them. I have no idea what group you're referring to, Scalaz? Typelevel? I don't have any insight into the politics of either because I stay away from them like the plague due to many of their toxic champions. I'm referring to mainstream Scala communities. &gt; It is the sign of a truly welcoming community to adapt to diversity; not to force diverse, colourful, and energetic characters to be homogenised into the ideals of those in charge, so that they may be blessed with the honour of being allowed to "collaborate" with them. Yeah, no shit. &gt; Personally I think we should be more open minded in our interactions, and assuming more good will in general, with less haste to label somebody "prickly", or even to consider that a bad thing. Personally I prefer to collaborate with "prickly" intelligent people. Btw, if a guy behaves the same way he is more likely to be called assertive. I don't care if they're a guy or girl. I was being polite in my term "prickly". If they're regularly making fun of other communities or non-FP conformist opinions on good software hygiene and best practices then I have better things to do then waste time with them.
I care
[doobie](https://tpolecat.github.io/doobie/)?
Beginner at Scala. But from the examples seems like it can map into a Class from a given SELECT query. So that would suffice for starters. Thanks.
The ego is indeed a major problem. I'm sad we are in a community where glorifying people is the norm. All the women i know, and this is of course true in computer science and software engineering, have lots of clever things to say. But while there are a few very popular women in tech, there is not yet enough of them (enough = parity), so most women are ignored. I'm sure eventually we will reach parity, but in the meantime, it is and will be unfortunately a tough fight for women to be heard. If you are the one i think you are, you have all the skills needed to be, one day, one these future very popular women. Until then i wish you good luck.
What's the point of visiting those conferences with boring talks if not to meet people and interact with them? 
I haven't used it for quite some time but Squeryl is pretty painless to pick up. Quill is also pretty great. I found that Doobie requires a fair amount of magic helper methods/implicits to get up and running in an easy to use manner, but once those are in place it is a dream to use!
This is a pretty good presentation. Overall some of his points are pretty good. I think Brian is wrong towards the end where he's saying that FP is bad at resource management. It really depends on what kind of resources you're talking. If we're talking about RAM memory, then yes, FP is pretty bad at working with memory. But if we are talking about managing file / socket handles, then the abstractions I've seen in the FP world are far better.
Those are good points. I also disagree with the presentation's sense that large-scale modular components need OOP at the borders. Opaque types hide implementation details well, and RPC interfaces (as seen in gRPC, Thrift, etc) don't require objects with dynamic dispatch either.
How many ways do you have to interact with people that are not sexual advances?
Thank you, she might be the same person :)
How adding another arm to *match* in *print* is better than extending objects? When you say "it doesn't modify existing code" it depends on a definition of "existing code". *print* **is** existing code and belongs to objects. If you really want to show how FP does it, you should probably try typeclasses. The big difference is **where** you define extension and how compiler helps you.
Composes well 
Hi reddit. It's pitched firmly at intermediate scala devs, hopefully some of you find it interesting. I've never written anything for the public before so any feedback is much appreciated.
Great article, thank you. I have a beginner question — how would you handle a failure in the Future using EitherT. Say you want to handle all 3 scenarios: - Validation error via Left(error) - Success via Right(pubs) - Exception thrown inside the Future From what I understand, the last one (which would normally be handled via ‘recover/recoverWith’ can no longer be handled. Are we losing that ability or am I missing something ?
Do you guys use Finatra at your work?
For #3, you can use recover, catch the exception, and return a Left (if you're using EitherT).
No one cares outside of pol 
Tupac cares, if don't nobody else care.
kind of got it, but kind of didn't. Would be less mental overhead to write articles without type alias'. Too much context switching to go back and figure out what it actually is and mentally substitute it every time.
I think it picks up a bunch of rails developers as well. It has a kind of similar style and attitude and I think a lot of rails programmers end up there if they start looking for something more performant.
Thanks! That's a really great point - EitherT has recover and recoverWith methods which you can use in the exact same way as with a regular Either, you don't lose anything. The tutorial page doesn't mention them though, you have to check the API docs.
Thank you so much! Really great to hear.
Thanks!
This is a first (pre) release of the Scala edition for Bartosz Milewski's "Category Theory for Programmers"! This edition contains both Haskell and Scala code one after the other. &amp;#x200B; Huge thanks to \[Typelevel contributors\]([https://github.com/typelevel/CT\_from\_Programmers.scala](https://github.com/typelevel/CT_from_Programmers.scala)) for the Scala snippets! &amp;#x200B; Please get the PDF and report any issues you may find :)
This is fantastic. : )
Great! I'll buy a print copy as soon as it becomes available. Just a question: why leave Haskell code in Scala version?
I see it as a plus. You can learn some Haskell along the way.
That's a good question. I ran an informal twitter poll, most voted answer was having Haskell/Scala code side-by-side somehow. I tend to agree, this seems the best. Also, I didn't want to change the original text, so I didn't want to replace various "in Haskell" with "with Scala". It's actually quite a testament to Scala that almost all snippets had straightforward translations. Some needed the Kind Projector plugin to avoid ugly type lambdas, others needed to define some functions that come in the Haskell prelude, so some code looks more verbose than it should. But overall, I think the result is great!
Hi all, I got few questions for ensime. 1. I am trying to use ensime and emacs. It works perfect, but I wondering does ensime provides auto completion for build. sbt? In other words, is intellij idea your only option for build. sbt complemtion? If not, could you provide some links so I can dig into? 
Amazing! 
Is printed version planned? 
Yes, hopefully as soon as I'll iron out all the possible formatting issues. Also, it had increased in size to 430 pages, so this may make it slightly more expensive to print in Hardcover.
I just went to this training in San Francisco and loved it. You will read and write a lot of code, in a structured format, where code builds up in complexity.
Awesome! Thanks for all your work.
Thank-you
What's the FP side of the community advice on private methods? Many of the techniques of pure FP improve testability, but I've never seen a private method being tested during my Java days. Should private methods be avoided? Tested by increasing the scope of the method-in-test so that it can be seen by the test code? Am I overthinking this? :)
Macro used for implementing a method cannot create anything outside a method. You might calculate the return type and make it "dynamic", and this way pass some information outside, e.g. with refined types, but running a `def method = macro Macro.impl` and having some value appear is not possible. You can only add things into the class/object using macro annotations (that's why we use them after all), but surely the way you want. And for a good reason. While normal macros can generate implementation using some compiler's internal information and decide a return value's type - they are encapsulated. The cannot pollute/break code outside of the place they are used. If I could inject new value/method just by running macro in some place, it would be really easy to break code in a way that is almost impossible to guess where it came from. Notice, that we use run a method implemented with a macro you see no difference between that method and a normally implemented one. That `macro Macros.impl` stays inside. Macro annotations on the other hand can mess around with the internals of your class, but they are more explicit about it. You have to have this `@IAmAnnotation` there which always warns you about trickery ahead.
Doobie is a lot nicer after it standardized on cats effect. Definitely worth looking at. 
I, for one, am happy to pay the extra to get this in hardcover. 
How about have 3 separate editions then? Scala/haskell/combined? (if that's not too much of work). 
When you don't have mutable state there's no particular reason to have private methods - in OO some methods of a class will be private because they can mess up that class's invariants if used incorrectly, but that's not a concern in FP style. Another reason for private is if a method returns e.g. a non-validated version of some datastructure that might be in an invalid state, but in FP style we'd make the validated and unvalidated version different types, so if you do get the unvalidated version you can't pass it to anything that expects the validated version. So I guess I'd say: see if you can achieve the same level of safety without needing to make the method private.
That seems reasonable to me, but how about the "usability" of the API? By this I mean, wouldn't the public API become too complicated to deal with? I've been thinking the same, that the methods should stay public and then rely on the trait to define the API contract and make it easy to understand, but I'm not sure whether the "trait+implementation" approach sounds too Java :)
I find you can be a lot more compositional in functional style, with small datastructures whose members are other small datastructures. You don't need to be scared of the "law of demeter" when working with datastructures (case classes). So it's fine to have e.g. the top level interface be the part that people are expected to actually use, but the nested details are there if they want them. In a way a field that's nested 3 levels deep behaves like "private" in that an end user is unlikely to call it, but it's accessible for testing or the like if you need it.
Don't forget about Expression Problem, Trivially: https://i.cs.hku.hk/~bruno/papers/Modularity2016.pdf IMHO this solution is far more elegant than what's possible in languages without subtyping.
&gt; RPC interfaces (as seen in gRPC, Thrift, etc) don't require objects with dynamic dispatch either. They _are_ objects with dynamic dispatch themselves. They encapsulate state and communicate by message passing, there's little you can do about 'objects' in your architecture as long as you use RPC across stateful services.
I am just waiting on web socket support on the Http4s client to completely remove Akka-Http. [https://github.com/http4s/http4s/issues/330](https://github.com/http4s/http4s/issues/330) Let me know what you think.
You're the dude who ruins fun things for everyone else.
How do you know.
seen a lot of press about Awair, would have been interesting to see how bad the air quality got in SF these past weeks. 
Thank you for your response. Your not wrong about the XY problem. I’m start with Scala, happy accepting the abstraction that futures is a great way to write concurrent programs in a clean, deadlock free, scalable and out-of-the-way fashion. When something goes wrong you have a few options: Complain the language is shit, hack random crap together or take a step back and try and understand things more deeply. This third option is what drove me to write the question. My problem was an exception, thrown from my test suite saying “The Future has timed out”. Pretty unambiguous stuff. I have eventually tracked it down to the fact my auth library is using Play.current which has a locking wrapper which causes the Timeout when I have parallel testing switched on. Anyway, thank you for your answers, I still feel there are some holes I want to fill and some tools I want in my box for other more trickier scenarios I might find myself in. I’m sure those tools exist for a reason. Thanks again!
&gt; why leave Haskell code in Scala version? This is such a great question. As someone in the midst of learning FP who is also a fan of Scala, sometimes it can be like... Ugh, why Haskell. For anyone that feels that way, I can tell you that the Haskell versions are ultimately more concise than the Scala versions. During the learning process this is probably a negative (since it makes the Haskell syntax inaccessible) but as one gets more an more comfortable with FP concepts, you'll find that the Scala versions carry a lot of "baggage" that Haskell opts out of. Insanely interesting.
Reading through it, seems like some of the text references the Haskell code and doesn't make sense for the Scala stuff. Great job, but voicing my support for the lesser voted answer of having just Scala code and changing the original text to correspond to it.
If `MyContainer` is invariant, why does this compile? I guess I don't understand variance after all... ```scala class Animal(name: String) case class Duck(name: String) extends Animal(name) case class MyContainer[A](value: A) def doStuffWithAnimals(animals: MyContainer[Animal]): String = "Stuff done" doStuffWithAnimals(MyContainer(Duck("Donald"))) ```
Thank you for doing more than posting a medium blog post and then just linking to it. 
This looks cool! We do a bunch with Neo4j at my work. I'll try this out soon.
I'm working on a project meant to reduce the boilerplate for REST/CRUD applications. First, using a DSL, one describes the data they are expecting to receive/return. For instance: ``` case class Person(name: String, age: Int) val personSchema = ( kvp("name", string(sv.matchesRegex("\^\[a-zA-Z \]\*$".r))) :: kvp("age", int(iv.min(0))) :: KvpNil ).convert\[Person\] case class Error(error: String) val errorDef = (kvp("error", string) :: KvpNil).convert\[Error\] ``` Next we describe the available operations: ``` val personService = ServiceOps.withPath("person") .withCreate(personSchema, personWithId, errorDef) .withRead(personWithId, errorDef) .withUpdate(personSchema, personWithId, errorDef) .withDelete(personWithId, errorDef) ``` The service description is the schema which can be passed to different interpreters. For instance the OpenAPI interpreter will print OpenAPI/Swagger compliant documentation of the service base on the schema. ``` val openApi = new OpenAPI() CrudOasInterpreter.jsonApiForService(personService).apply(openApi) println(io.swagger.v3.core.util.Json.mapper().writeValueAsString(openApi) ``` The http4s interpreter will generate HttpRoutes\[IO\] for each of the defined operations. The interpreter is responsible for generating a runtime which will unmarshal,validate and marhsall based on the schema. ``` val http4Service = service.forService( personService, middleware.createF, middleware.readF, middleware.updateF, middleware.deleteF ) BlazeBuilder[IO].bindHttp(8080, "localhost").mountService(http4Service, "/") .serve .compile.drain.as(ExitCode.Success) ``` Here is the complete [Example](https://github.com/OleTraveler/bones/blob/master/examples/src/main/scala/com/bones/PersonEndpoint.scala). I like this approach because updates to the schema will be reflected in each interpreter. (The doc stays up to date with the application!) The next interpreter I'm interested in writing is one which can marhsall/unmarshall protobuf based on the schema. It will also be able to generate a .proto definition. This would be integrated in the http4s interpreter so that a config switch would generate endpoints which could accept protobuf. I'd also like to experiment with Scala.js to see if a reasonable React application can be generated using the schema which would be compatible with the personService. 
No.
Would anyone recommend it to use with Scala?
No chance of remote?
Just to share some more light on this: This is the chance to work for a fantastic energy company who are using Scala in order to power changes in the energy market. They are doing this through a variety of products such as: Smart Meters and an app, Lumo where Pay as You Go customers can top up their energy via mobile and see real-time analytics on their gas and energy usage. They have a new product, **V-Net**, is a cloud-based platform helping to unlock smart charging capabilities for electric vehicles on a national scale. They have created a network that can redistribute energy amongst their customer base. What this means is that you are able to sell any unused energy stored in your car back to the National Grid, and then re-purchase when the demand on the National Grid is lessened, or when the energy is cheaper! They have a key team of engineers who are all using Scala alongside other technologies including but not limited to: Akka, Cats, fs2 and Spark. They are paying up to £85k for the right person and are a great group of people! &amp;#x200B; If you are interested or have any questions, please do reach out!
I really enjoyed this post. The fact that `[A, B &lt;: List[A]](b: B)` enables a more detailed type description than `[A](as: List[A])` was especially eye-opening. I'll have to digest this for a bit.
Unfortunately not. Be sure to keep an eye on the [Functional Works job board](https://functional.works-hub.com/jobs/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=k.cadima) for remote jobs. Should have a few coming up soon.
Yeah I’ve found an sbt module that can do exactly what I want. Thanks.
Nice! Which module?
Thanks I did not know that one
Thanks so much to those that worked on this. I had read several chapters into the original version of Category Theory for Programmers, but the C++ code snippets did nothing for me as I don't have background in that language. It will make a huge difference for me to be able to see the code in Scala as well as Haskell in this new version.
As long as you aren't cross-building, gradle and maven work fine. If you are, I think sbt and mill are your best bets.
Oh nice. I'll investigate when I have time. I also suffer from neo4j at work and having a bunch of Anys in my code :D
While accurate, I think this is too much detail for someone just starting with Scala.
Hello! I have a simple word counter spark application, and the source file `src/main/scala/org/apache/spark/Counter.scala` looks like this: package org.apache.spark object Counter { def main(args: Array[String]): Unit = { val sc = new SparkContext(new SparkConf()) val lines = sc.textFile(getClass.getResource("/Message.txt").getPath) val wordsCount = lines .flatMap(line =&gt; line.split("\\s", 2)) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) wordsCount.foreach(println) } } in line 6, the spark context is reading the text file in `src/main/resource/Message.txt` after building the source with sbt, I use spark submit to run the code, and it throws the following exception: &gt;Exception in thread "main" java.lang.NullPointerException &gt; &gt; at org.apache.spark.Counter$.main(Counter.scala:6) &gt; &gt; at org.apache.spark.Counter.main(Counter.scala) &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke([NativeMethodAccessorImpl.java:62](https://NativeMethodAccessorImpl.java:62)) &gt; &gt; at sun.reflect.DelegatingMethodAccessorImpl.invoke([DelegatingMethodAccessorImpl.java:43](https://DelegatingMethodAccessorImpl.java:43)) &gt; &gt; at java.lang.reflect.Method.invoke([Method.java:498](https://Method.java:498)) &gt; &gt; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775) &gt; &gt; at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) &gt; &gt; at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) &gt; &gt; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119) &gt; &gt; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) It seems that `getPath` is not providing a full path to `sc.textFile`, any idea why?!
`getResource` returns a `URL` which might well be a URL to a resource inside a JAR file, and `getPath` returns the path part of that URL.
You can learn a lot about that from projects like: https://github.com/plokhotnyuk/jsoniter-scala https://github.com/non/debox https://github.com/Sizmek/rtree2d
Does this cover Scala js or the bindings for it? I don't see how this is on topic here
That's amazing!!!!! Why is that not posted on Twitter?
What is the best way to learn scala. I tried it by the books of underscore.io but when I try that knowlegde on "real" world problems like AdventOfCode I still have the feeling I miss something 
Just a link to your careers page? Kinda lazy...
That neo-nazi reference at 8 minutes is hilariously out of place. I'm 100% certain Brian doesn't know that Rise Above is a classic neo-nazi slogan, and that exact tattoo is common with the Aryan brotherhood, which makes the juxtaposition even more funny.
Nice work! I briefly looked into Neo4j and hoped there was a type-safe driver like this. I'm gonna check it out!
I have to agree.
While reading your explanation I could not stop thinking about well-known _Command_ pattern from OOP world. I would appreciate if you explained how is it different from `IO` (if it is). Also, I don't really get the argument about being able to reason about your code. Once any function returns `IO`, you can't reason about following functions that also return `IO`. You now have a context — a huge world outside of your application that your IO mutates. Let's consider some simplistic database as example. And following side-effects. 1) write entry to a write-ahead-log 2) write a record to disk 3) if 2) succeeded, write entry to log, that operation succeeded 4) if 2) failed, write entry to error log You can't make a decision about your sideffects *before* performing them. How would you represent this with `IO`? How exactly representation with `IO` would help you to reason about the code?
Sorry everyone. This is the direct link to the job posting [http://app.jobvite.com/m?3ZVNfkwL](http://app.jobvite.com/m?3ZVNfkwL) 
Function composition is one way to chain functions together: timesTwo . addOne $ 0 -- =&gt; 2 Another way is pipe-forward like F# does: 0 |&gt; addOne |&gt; timesTwo // =&gt; 2 In Scala, it's more idiomatic to chain methods together using _fluent syntax,_ because of Scala's object/functional mix: 0.addOne.timesTwo // =&gt; 2 Of course, the above is a made-up example. A more realistic example is something like this: https://stackoverflow.com/a/44505067/20371
- Out of the box hot-recompiles on browser refresh - Familiar model-view-controller layout - (Relatively) thorough documentation including at least one book - Built on top of Akka/Streams so you can drop down to that if you need it - Stateless design so it can scale horizontally as well as vertically.
I also did something similar with [hepek-starter static site template](https://github.com/sake92/hepek-starter). Netlify is really great, you can make forms [only in HTML](https://hepek-examples.netlify.com/form/form-example.html), add "netlify" attribute, and it will provide a "backend" for you, store results in a CSV.. :D I think they also provide AWS Lambdas and stuff.
Yes, I work for an IoT company and we build all our scala projects using gradle. We are really happy with the results.
Love that they're using the DAG architecture at Constellation. Mush easier easier to scale than blockchain.
Wow, I'm glad to see a startup in Bay Area using Scala.
I think generally, you will lose the insertion order when traversing the nodes in the map. Otherwise, specific usecase should be considered.
There is no inherently superior option - Do you need to routes to be ordered? use a `List` - Do you not care about ordering but need string-based lookup? Use a `Map` - Do you need both ordering and string-based lookup? Use a `LinkedHashMap` - Do you need index-based lookup? Use a `Vector` 
More “data artisans” spam!
Good news, although Spark was still the first :)
It'd be nice to see a post-mortem on whyit took so long for this implementation. And what actions they will take to prevent this for the ongoing releases of Scala.
I think flink depends on Sparks closure cleaner.
The community needed to add proper closure cleaning of Scala 2.12 lambdas which is why Scala 2.12 support was added in Flink 1.7. Here is the corresponding JIRA issue: [https://issues.apache.org/jira/browse/FLINK-7816](https://issues.apache.org/jira/browse/FLINK-7816)
I mean seeing the headline is nice though. I'm glad to know 2.12 is supported. 
Also see the [thread on contributors.scala-lang.org](https://contributors.scala-lang.org/t/announcing-dotty-0-11-0-rc1/2630)
Oh, really? Didn't know that. Well, now that makes sense :)
Is there any way to get the opaque type to work in 2.13? Because that looks sweet 
Here's hoping there will be a dotty dot net too
Since this is likely homework wont give you the exact answer but check out the list collection api. https://www.scala-lang.org/api/2.12.x/scala/collection/immutable/List.html You should see methods that allow you to take something from the beginning of the list and return a collection without the first item.
Why have a data structure at all? case class Routes(maptoEndPoints: String =&gt; Option[Routes], endpoint: Option[String]) Perhaps you still want to know something about the structure of the routes? case class Routes[Find[_,_]](mapToEndPoints: Find[String, Option[Routes], endpoint: Option[String]) Sometimes it's worth deferring a choice unless a specific semantic is required.
you could also post to /r/gamedev ;o)
Yaaay. I'm pretty excited to see Dotty coming along so nicely.
There's probably a much more clever way to do this, but you could load the dictionary wordfile (distributed on a lot of unix based systems) into a set, then check to see the percentage of the words in the line that exists in that set?
Basically something like that. You could check for non-English characters first.
Yeah, good point--whizz through the line to calculate a percentage of non-English characters and if it's higher than some arbitrary cutoff then reject it entirely. 
&gt;Is this something that you've seen happening in the Scala community, or is it something more generally that happens in the industry? Yes and yes. In fact, I know some people who were invited to some of these secret rooms (containing some of the most prominent members of the community!) and were appalled at the idea. I don't know how to fix this problem, except to urge people to be inclusive and stay away from secret clubs, and stay away from loyalties to friendly groups who share common political beliefs. Good luck imo. It's probably never going to happen.
That only seems appropriate if you're representing a completely static tree. Most websites where you'd have to implement routing would need something more flexible. It's an interesting question, but I'm having a tough time thinking of a scenario where I'd want exactly nested maps, as opposed to one flat map or something more expressive. 
English doesnt really have a character set. While you can do a heuristic of character prevalence, hard rules will almost certainly be too strict
That's not really true. If your text contains a bunch of å or ø it's likely not english. Even a large occurrence of accented characters (é etc) is probably enough.
So, heuristics on character prevalence?
It's not enough to positively identify a text as english but it's enough to identify some texts as not english.
It really depends on the size of yout text. The larger the space, the more accurate you can predict. There are several NLP libraries out there, but there is a language detection facility in Apache Tika. It might be sufficient for your use case. If you really need high accuracy, you should probably look at Google's Api but they come at a cost beyond the free tier.
What you could do that is not based on a large dictionary file that you would have to carry around is a n-gram frequency method. For some hints check the wiki: [https://en.wikipedia.org/wiki/Trigram](https://en.wikipedia.org/wiki/Trigram) or [http://cloudmark.github.io/Language-Detection/](http://cloudmark.github.io/Language-Detection/) , google 'n-gram language detection scala' and you will even find some repositories with scala code. 
Bigrams or trigrams are easy, but you’ll need a corpus of non English as well.
&gt;you should probably try typeclasses That will be a second part. This is the only intro. &amp;#x200B;
Hmm. Thank you. I'll enqueue that article. It is familiar because The Expression Problem is a well known formulation with many solutions 
The easy and simple solution would be using something like [this](https://github.com/AyeshJayasekara/English-Dictionary-SQLite). You can then run queries on the database using any Java SQLite library.
this. opaque types are something i want badly. tagged types work, but they have a lot of boilerplate that you might not want.
Can opaque types be parameterized (have generic arguments)? It would be quite useful, for example as a way to implement monad transformers: opaque type EitherT[F[_], A, B] = F[Either[A, B]] object EitherT { // constructors, type-class instances etc. }
this is not a scala problem... it is a statistical language analysis problem
You're on the right track and hopefully when all is said and done you've returned a new deck with the top card missing instead of removing the top card from the original deck. 
You need https://docs.oracle.com/javase/7/docs/api/java/util/Deque.html
[removed]
‘Synthesis’ (putting the parts together into an overall solution) is a learned skill and it’s common to any area of knowledge. It depends on your learning style; some people learn top-down and some learn bottom-up, some are a mix; some learn by abstraction and some by example. So you’ll need to specify what kind of learning style you’re looking for.
If you don't want to use any dependency managers, then you can always add a raw jar file to your project. But how are you going to package your project? Ie- build a deployable?
Find it on [http://mvnrepository.com](http://mvnrepository.com), you can download it there. However at the next opportunity you should learn how to use a build tool like SBT etc. &amp;#x200B;
`libraryDependencies +=... ` in build.sbt. What comes after is usually shown on the library's github page. It's a three part information, called Ivy. You might recognize the parts as Maven. 
Yes. They are awesome.
If you're willing (or allowed) to use a third party API, you could use the Google language detection API. https://cloud.google.com/translate/docs/detecting-language IIRC Azure and AWS (among others) have similar offerings. You could also do your own ML approach with some training data, but that might be overkill. As others have suggested, simply comparing words against a set from an English dictionary would likely be pretty accurate. (This doesn't really have much to do with Scala - the solution would be very similar in any language)
To improve my Scala knowledge, I decided that I should read more Scala code and I started from \`monix/minitest\` (easy domain, has "mini" in its name and has Alex as its author: sounds like a great combination!). &amp;#x200B; Minitest targets multiple runtimes: JVM, ScalaJS and ScalaNative. However, when loading the project on IntelliJ I always get an error saying it can't resolve ScalaCheck Native (\`unresolved dependency: org.scalacheck#scalacheck\_native0.3\_2.11;1.14.0: not found\`). &amp;#x200B; \`sbt compile\` and \`sbt test\` work fine. This is the first time I bump into a project that produces artefacts for multiple runtimes and I am a bit lost. &amp;#x200B; Is there any recommended reading you guys suggest so that I'll be able to understand the build definition of this project? To be fair, I would just like the project to be indexed by IntelliJ as it makes a beginner's life easier :) &amp;#x200B; Btw, suggestions for other, maybe simpler projects I should begin with are also a good answer :)
Check out youtube- Scala - Your First Programming Language - Part 1 - Series Introduction by DevInsideYou. These got me up and running in Scala.
Yes.
I recently ploughed through Exercism's scala track. It's pretty nice, IMO, but it doesn't teach the more advanced stuff like how to make a web server.
Spam makes a subreddit impure
I believe circe also has fs2 streaming support. https://github.com/circe/circe-fs2
Yea, afaik circe fs2 is based on jawn async parser. basil is slightly different in that it does not attempt to parse the whole stream into a Json, instead it tries to extract relevant data and stop once the data is extracted. 
I personally don't use Scala-IDE but it seems it has a setting to specify library dependencies: http://scala-ide.org/docs/current-user-doc/features/scalacompiler/index.html#changing-the-settings-for-existing-project Look for the 'Libraries' tab in the 'Java Build Path' page of the application preferences.
Do you have a source for that you can share?
My personal feeling is that the courses which try to go "end-to-end" are very basic and in the end you don't learn much. SBT is quite a complex technology and you should learn it on-its-own rather than mix it with something like Play. Just my 2 cents.
Whenever you work with implicits, I can recommend import language.implicitConversions import scala.reflect.runtime.universe.reify reify { implicitly[Ordering[Int]] } to display what the implicit is expanded to. I think intellij can also help there. You could also look up "free monad" and/or "tagless final" which might suit your problem and are less hairy.
Cool!
Would you happen to know any that teaches it in detail? 
Yeah in particular, PBS recently had a feature on a particular subgroup of the Aryan brotherhood called the Rise Again Movement [here](https://www.pbs.org/wgbh/frontline/film/documenting-hate-charlottesville/).
This one is nice. There are others as well on youtube https://www.youtube.com/watch?v=PDhOv4NMK-Y I liked the SBT book a lot. it got me started and from then on I read the product documentation. https://www.amazon.com/sbt-Action-simple-Scala-build/dp/1617291277/ref=sr_1_1?ie=UTF8&amp;qid=1543803523&amp;sr=8-1&amp;keywords=scala+build+tool One problem you will constantly face with all open source products (including SBT) that the syntax changes very very fast. so all books and blogs don't work when you type code as is. So you will have to learn from the book and then google (or StackOverflow) a little to find the latest syntax of things. Lastly, today there are plenty of alternatives to SBT as well. Mill, Rage and Bazel. so if you are really interested, try to learn the broader eco-system as well. Who knows if you will end up liking Mill more than SBT. 
Check out lightbend's serving machine learning models, hope you find something useful there. https://github.com/typesafehub/fdp-modelserver/tree/7a8cf8bd5fe8476c36822e3520da794cf547eb76?files=1
I think your `TxnFilterF` just calls itself in an infinite loop. Implicits are resolved statically (i.e. at compile time), so when you call tf.filter(txn) then that call will use the static type of `tf`, i.e. `TxnFilter`, so you'll have a `FilterUtil[TxnFilter]` and the call to `.filter` must just be picking up `TxnFilterF` again. You probably want to either have the implementation of `TxnFilterF.filter` do a `match` on `tf` and delegate to the appropriate implementation for the specific type, or else avoid having an implementation for `TxnFilter` at all and instead have `TxnFiltersAND` contain a `HList` so that the specific filter types are available at compile time, and resolve implicits based on those specific types.
I am confused now. &amp;#x200B; isn't Either used to catch the "impure part" and make it "pure" again?
I think we can think it in this way: if the exception thrown is not observable, then from caller point of view, it does not exists (ie. it's an implementation detail), similarly have local state mutation in a function does not make it impure
Consider this function: ``` def foo: Int = throw new Exception() ``` Now let's check this function for referential transparency: ``` val x = foo x + x // Now let's inline 'foo' to see if the program behaviour changes foo + foo ``` These two programs are equivalent in that they never return. This makes `foo` equivalent to a function like this: ``` def bar: Int = bar ``` Now, I'm glossing over a few things here and Jakub's totally right that throwing an exception isn't pure in Scala generally speaking. Though a function that might throw one can still be fully referentially transparent as one can see here. There's a great answer on stackoverflow that looks at all of this in really detailed way: https://stackoverflow.com/questions/12335245/why-is-catching-an-exception-non-pure-but-throwing-an-exception-is-pure 
So that mean that that function would still be pure but not total (my point being that, if an exception is thrown, it means there's a value from the domain with no correspondence on the codomain)? :/ 
Thanks Luka. From your answer things start to become a little bit clearer now. I should definitely read that StackOverflow answer. &amp;#x200B; I guess I just need to decipher Alex's point on what a total function is (*Totality is about the static type system being able to do a “for all” proof, instead of “for some”.*) and map this into the [normal definition of a mathematical function](http://mathworld.wolfram.com/Function.html) that has always been what I believed to be same as a total function :)
My view: no, because throwing an exception isn't a value. We can't even ask whether two different invocations of the function evaluate to equivalent values, because they didn't evaluate to a value at all. If you introduce a bottom value into the language then you can say that an invocation that throws evaluates to that bottom value, and then you can consider the function pure if any given invocation evaluates to a value that's equivalent to that invocation. But at that point you're not really working in Scala any more, but rather in some custom superset of Scala (that may be more convenient for analysis).
While it's always interesting to get the thoughts of other Scala devs, annoyingly, it wasn't actually about how they use Scala at the BBC - it felt like the second half of the interview was missing!
I use kamon https://kamon.io/documentation/get-started/ with akka-http, but they have http4s support too
How would I go about writing a program that will balance a scale with a given weight? I want the output to look like this. Would I do this recursively? Scale: 1,3 Extra Weights: 1,2,3,4 The One To Balance The Scale: 2
I'm confused by your last comment. I was under the impression that in Scala exceptions had the type Nothing, which is the bottom type, so you're still in regular Scala with your bottom value. This is just how I had understood Martin Odersky's coursera course.
https://www.meetup.com/en-AU/Scala-in-the-City/
This feels like an interview/test/homework question so I'm not inclined to provide a full answer at this time, but I will say I don't see a need for recursion; consider a linear search.
There's a bottom *type* but no bottom value.
thanks!
I believe the motivation is that since a given input always results in the same exception being thrown, the function is pure. For example, if we call our hypothetical function ```divide(4, 0)```, it will always throw the zero division error. 
Great guide. We do something almost exactly the same in our http4s apps. 
As @Falmarri said definitely take a quick look at kamon; The implementation mostly follows the opentracing standards and it already provides integration with multiple tools for multiple use cases ( zipkin, jaeger, datadog, influxdb , ...) It also provides distributed tracing which means that you can have trace id's being propagated even across application boundaries. (Because it also supports multiple frameworks you shouldn't have any issues if another service you use is written in play, akka-http, ...) Lastly, log lines produced by code not developed by you would also have a trace id added so if you were to enable doobie or some other libraries info/debug logging, the trace ids would be present. &amp;#x200B; 
Ah, of course. Nothing has no values. Thanks.
Agreed. It wasn't really about anything, and it seemed to be over before it started.
Hey, there's a new Ask-Anything thread now on the front page, you might wanna repost this there :-)
Plenty of Scala meetups: &amp;#x200B; LSUG: [https://www.meetup.com/en-AU/london-scala/](https://www.meetup.com/en-AU/london-scala/) Scala Central: [https://www.meetup.com/en-AU/Scala-Central/](https://www.meetup.com/en-AU/Scala-Central/)
Why did you guys choose to ditch Scala IDE and use IntelliJ? I am wondering if I missed something. I've been using Scala IDE for more than 5 years and recently tried IntelliJ (Ultimate), it's really hard for me to imagine myself using IntelliJ. Most probably I will have to fall back to Emacs+Ensime, as I learned Scala IDE has been officially ditched by the Scala people.
Thanks!
&gt; As /u/Falmarri said definitely take a quick look at kamon;The implementation mostly follows the opentracing standards and it already provides integration with multiple tools for multiple use cases ( zipkin, jaeger, datadog, influxdb , ...) I know `kamon` and I have used it in the past with `spray`. As I explained in the [motivations](https://gvolpe.github.io/http4s-tracer/motivations.html) section all the Open Trace stuff is way more complex and follows a different approach. &gt; It also provides distributed tracing which means that you can have trace id's being propagated even across application boundaries. (Because it also supports multiple frameworks you shouldn't have any issues if another service you use is written in play, akka-http, ...) It comes with a price though. Setting this up in a distributed environment with many microservices is not trivial + some dev ops need to look into it. We do this "manually". Every microservice is aware of a "Flow-Id` being transmitted by Http headers / Messaging headers (Kafka) and makes use of it. It has its own trade-offs but it's very simple to setup. In `haskell` is recommended to go live by using the `RIO` Monad which is `ReaderT` + `IO` basically. I tried to follow a similar approach while integrating it very well with `http4s` since that's the library we use at work. &gt; Lastly, log lines produced by code not developed by you would also have a trace id added so if you were to enable doobie or some other libraries info/debug logging, the trace ids would be present. You can do this with `http4s-tracer` + tagless final as shown in the examples. &gt; Particularly liked the fact that you don't really have to add implicits everywhere which I've seen in some attempts and it really makes code look particularly ugly! Thanks a lot, feedback is always appreciated :)
This is really nice. I'm a little over half way through the red book, and these forward looking problems can provide some extra motivation. I'm not quite to the point where I could do so sufficiently, but may give a few attempts anyway. Thank you.
I was looking at [Cats Type Classes](https://typelevel.org/cats/typeclasses.html) and there was some piece of code I couldn't understand. def combineAll[A](list: List[A], A: Monoid[A]): A = list.foldRight(A.empty)(A.combine) I don't understand the parameter `A: Monoid[A]`. The method takes in a parameter that is a type parameter? It seems like `A` is of the type of `Monoid[A]` which seems self defining. Could someone explain what this method signature actually means? I think I am missing a scala syntax, something like [context bounds](https://docs.scala-lang.org/tutorials/FAQ/context-bounds.html)?
You can also do something with the frequencies of letters in the words. Reference: [https://en.wikipedia.org/wiki/Letter\_frequency](https://en.wikipedia.org/wiki/Letter_frequency)
**Letter frequency** The frequency of letters in text has been studied for use in cryptanalysis, and frequency analysis in particular, dating back to the Iraqi mathematician Al-Kindi (c. 801–873 AD), who formally developed the method (the ciphers breakable by this technique go back at least to the Caesar cipher invented by Julius Caesar, so this method could have been explored in classical times). Letter frequency analysis gained additional importance in Europe with the development of movable type in 1450 AD, where one must estimate the amount of type required for each letterform, as evidenced by the variations in letter compartment size in typographer's type cases. Linguists use letter frequency analysis as a rudimentary technique for language identification, where it's particularly effective as an indication of whether an unknown writing system is alphabetic, syllablic, or ideographic. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Letter, letters, or literature may refer to:
I'm getting started with the same process. Looks like you are more familiar with the Scala standard library and I/O than I at this moment - but I do know what one of the big mantras of Scala is functional paradigm and immutability. Could you refactor some of the imperative statements/loops into functions and Array/List \`.map\`, \`.filter\`, \`.reduce\`, \`.foreach\` functional Array/List methods etc.? They kind of work similarly to the list comprehension Python, but share syntax closest to JavaScript with the fat arrow syntax. &amp;#x200B;
To get a sum you can call list.sum To verify a condition for one or more elements you can call list.exists(predicate) 
This isn’t idiomatic at all. You need to replace the vars with vals and either replace the for and while loops with recursion, or a method such as .map or .foreach 
Disclaimer, I've not run the code snippets below and I didn't read the Advent challenge, but from just looking at the code I'll suggest a couple things. We can build the list of inputs in the order we need and only call the parseClaim method once per line: `@tailrec` `def readInputs(prev: List[String]): List[String] = {` `val line = scala.io.StdIn.readLine()` `if (line != null) readInputs(prev :+ line)` `else prev` `}` `val claims = readInputs(List.empty).map(parseClaim)` It's also common to use \`case class\`s instead of large tuples `case class Claim(id: Int, left: Int, top: Int, width: Int, height: Int)` `def parse_claim(input: String): Claim = {` `val data = input.split("\\D+").filter(_.nonEmpty).map(_.toInt)` `Claim(data(0), data(1), data(2), data(3), data(4))` `}` I'd consider comparing the claims with arithmetic instead of building up the fabric matrix.
Your reasoning is correct, but not fully. If you inline then stacktraces inside exceptions change so the programs are not equivalent.
In scala the namespaces for values and types are independent, that function has a type parameter A, and a regular parameter named A of type Monoid\[A\]. It's a common pattern to name some type classes instances with the same symbol that the type parameter, but it could be other name and the logic will be the same: &amp;#x200B; `def combineAll[A](list: List[A], aMonoid: Monoid[A]): A = list.foldRight(aMonoid.empty)(aMonoid.combine)` 
Maybe and, please note that I don't know what I'm talking about :P, the thing is: if you can recover from the error or if the error is a business one (to make it clear from the function's return type that a business error can occur), return an Either. Otherwise, if you can't do recover, just throw an exception that will be wrapped in the effect type, be it a future or anything else. I \*\*think\*\* I've heard Runar saying (on the CoRecursive podcast) that, if you're not doing anything with the exception, it is fine to throw them. I may have to revisit that episode tough :)
Just echoing what others have said. 1. Dont use nulls 2. use vals instead of vars 3. itch the for loops and try to use existing methods on the collections api https://www.scala-lang.org/api/2.12.0/scala/collection/immutable/List.html
I'd say Scala is far from perfect language for such sort of tasks and shines much more in larger systems. Nevertheless, a couple of idioms you should try to pick up as you're doing these. 1. Try to initialize a variable in one go, instead of mutating it afterwards. Let's consider this piece of code: var overlap = 0 for (row &lt;- fabric) { overlap += row.count(_ == -1) } Here, a variable is created and calculated later. Idiomatic Scala would be to use collection operators to initialize it once in a `val`, not `var`: val overlap = fabric.map(row =&gt; row.count(_ == -1)).sum But wait, there's more. For example, let's consider line building: var input = List[String]() var line = "" while ({ line = scala.io.StdIn.readLine(); line != null }) { input = line :: input } Here, you introduce an unscoped variable and a mutable list, and populating it backwards. It helps me to phrase _what_ I am trying to achieve and then look for library methods to do it. So, say, I want to: &gt; Call `readLine` over and over until it returns `null` and get a list of results And I would translate it to code like this: val input = Iterator.continually(scala.io.StdIn.readLine()) // Call `readLine` over and over .takeWhile(_ != null) // until it returns null .toList // and get a list of results This list would already be in a proper order, so I would not need to call `.reverse` in later loops. But wait, there's more. We also always parse each line, so we might as well do it in one go: val input = Iterator.continually(scala.io.StdIn.readLine()) // Call `readLine` over and over .takeWhile(_ != null) // until it returns null .map(parse_claim) // then parse every string into a claim .toList // and get a list of results 2. Use classes and functions Side note: `x to y - 1` can be written as `x until y`. It's customary to use case classes instead of tuples wherever possible. You rarely even see tuples of arity higher than 2 used in Scala (whereas Tuple2 is often used to represent key-value pairs of map) Classes are very easy to write and require little boilerplate, compared to Python. case class Claim(id: Int, left: Int, top: Int, width: Int, height: Int) Now, every field is named, as is the whole thing. This is better, because having a bunch of Ints gives you no information and you have to investigate the code that uses it to figure out what they mean. But wait, there's more. We can add methods to classes, to encapsulate our commonly used patterns. In particular, there's a thing we do twice: for (i &lt;- top to top + height - 1; j &lt;- left to left + width - 1) { ... something with i and j ... } We can encapsulate these ranges at the very least, but we can also step up our abstraction game with a function accepting a function: case class Claim(id: Int, left: Int, top: Int, width: Int, height: Int) { def columns = top until top + height def rows = left until left + width // We take a function producing any A and collect all the results. // Ignore this for now if you have problems understanding generics def eachSquare[A](f: (Int, Int) =&gt; A): Vector[A] = for (i &lt;- columns; j &lt;- rows) yield f(i, j) } This lets us also get back to the part with `var conflict`, which becomes: for (claim &lt;- input) { val conflict = claim.eachSquare((i, j) =&gt; fabric(i)(j)).exists(_ == -1) if (!conflict) { println(s"No conflict in ${claim.id}") } } And for the array creation code, we can use the same method but just discard a `Vector`. Oh, and if-else can be used as a ternary operator: for (claim &lt;- input) { claim.eachSquare { (i, j) =&gt; fabric(i)(j) = if (fabric(i)(j) != 0) -1 else id } } Other notes: 1. Names are camelCase, not snake_case. So, `parseClaim`. See [style guide](https://docs.scala-lang.org/style/). 2. Avoid unnecessary `var`s. `fabric` can be a `val`, even though it's a mutable array, you never replace the array with another one. Usually you don't need something to be both mutable and reassignable. See [this blog post](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html#immutability--mutability) for more info 3. `List`s in Scala are singly-linked lists. That means getting size or iterating in reverse has O(n) complexity. The immutable alternative of Python `list` is `Vector`. The mutable one is `ArrayBuffer`. `Lists` are good for small collections, like 5-10 elements. 4. Contrary to what others here have said, I think your usage of `null` is perfectly fine, as you get it from 3rd-party API and specifically don't let it leak into the rest of the code.
I have the same feeling. Many thanks for this /u/oleg-py! Exercises are the best way to learn the material and really engage with it. Not only just that, but they give a taste of what's possibly to achieve with a library.
Is that the formatting or is tailrecdef one word? 
That makes much more sense. Thank you!
&gt; If you inline then stacktraces inside exceptions change Indeed &gt; so the programs are not equivalent. This one's a little more subjective. After all, if you inline any value then its memory address (and therefore the result of e.g. `System.identityHashCode`) will change, so you could say there's no such thing as a referentially transparent function (and for certain purposes that's true). Really we need to define equivalence in a way that's appropriate to the analysis we're doing. IME it's often more useful to consider exceptions as equivalent when they differ only in stacktrace.
I'd like to follow up on this idea. What if we said that every function A =&gt; B would be mechanically translated into A =&gt; Either\[Nothing, B\] where all exceptions are caught. Rather than every function it would probably have to be every expression. Then, also mechanically, all these results would be flatmapped, doing nothing else with it. My idea was, that this would be a different (but isomorphic) representation of a a program that works with exceptions without "handling" them. There might be minor things like rethrowing exceptions of having different types of exceptions compared to one bottom type, but it still makes me think that exceptions could be considered pure if their use could be translated mechanically as described.
This is awesome, thank you so much for the time to go through it so thoroughly.
At Skills Matter we have a few courses that you might be interested in that are coming up soon: * [Advanced Scala with Dick Wall](https://skillsmatter.com/courses/516-scala-advanced-with-dick-wall) * [Lightbend Scala Language - Professional](https://skillsmatter.com/courses/339-lightbend-scala-language-professional) * [Lightbend Apache Spark for Scala - Professional](https://skillsmatter.com/courses/524-lightbend-apache-spark-an-introductory-workshop-for-developers) We currently have discounts on these courses so feel free to contact me via [fraser.bradbrook@skillsmatter.com](mailto:fraser.bradbrook@skillsmatter.com) so we can discuss them.
Sorry about the shitty formatting. It's @tailrec :)
&gt; My idea was, that this would be a different (but isomorphic) representation of a a program that works with exceptions without "handling" them. Any representation where thrown exceptions are (ordinary, first-class) values can't be isomorphic to Scala because you will have constructions that you can't translate back e.g. a list of two or more thrown exceptions - when you try to turn that back into Scala then it can only throw a single exception. So like I said, at that point you're working in some custom superset of Scala.
Good explanation. I think your definition is easier to read with the aMonoid rather than the A. 
By "a list of two or more thrown exceptions" you mean an expression that might throw two or more types of exceptions? Btw, I think I made a mistake. It should be `Either[Exception, B]` as there are different types of exceptions. Of course, it can only be isomorphic, if the code that uses exceptions does not catch them except for immediately rethrowing an enriched exception. So it still would be a superset (or rather subset?) of Scala in that there can only be an isomorphism if only that super/subset is used. I just mention that (maybe possible) isomorphism because I find it makes thinking about a certain usage of exceptions easier and goes well with the idea that this specific usage of exceptions is RT or at least could be mechanically rewritten to be RT.
&gt; By "a list of two or more thrown exceptions" you mean an expression that might throw two or more types of exceptions? No, I mean a list of two or more actually-thrown exceptions. If you have a representation where `def f = throw Something` can be viewed as a function that returns an (ordinary, first-class) value like `Left(Something)` then you can form a list of those values e.g. `List(Left(Something), Left(Something))`, and this is clearly a different value from e.g. `List(Left(Something), Right(()))`. But you've got no way to translate that distinction back into the `throw` world.
&gt; Any caveats I should be aware of? Learning Scala can be perilous. You will invariably start to learn functional programming and Scala syntax at the same time. Most tutorials don't bother distinguishing between the two. And if you ask any Scala experts for help, they'll likely push for you to write something that is more FP than the level you're currently at. I'd say keep an open mind. You will end up learning and re-learning many many concepts over and over again. There is almost always a better, more FP way to write something. Start with the basics of "this style is more idiomatic than Java" (e.g. no `var`s). I'd say stay away from things like Cats and Scalaz to keep a mental cap on the things that you will learn. You can learn them in time, but if you don't know any FP at all, they will just be a distraction. They are worth learning in the long run though.
Not a Scala expert, but I'm leaning scala coming from .net. What helped me quite a bit was to use IntelliJ. It is free and has autocomplete to get some help on what is available in the java SDK.
&gt; Any caveats I should be aware of? Similar to /u/hyperforce, don't get sucked into FP libraries like Scalaz or cats or shapeless. Keep it to the Scala standard library and only venture out into FP territory when you feel it gives you a real advantage. Martin Odersky has an excellent series of keynotes that talk about Scala idioms and styles that you can find on Youtube, for example https://www.youtube.com/watch?v=iPitDNUNyR0 I also found [Another Tour of Scala](https://naildrivin5.com/scalatour/) to be a big help in getting familiar.
Thanks, this is very helpful.
I came from Python, VB and R background, so I’d say given your background, you’d be surprised how quickly you can pick it up. It is possible but like the majority of the comments - focus on learning the language and then the FP libraries. Underscore have a number kid freely available books to introduce FP that I’d highly recommend. 
Ah, that's what you mean. My idea is that the following program &gt;def f = throw Something &gt; &gt;val a = f &gt; &gt;val b = f &gt; &gt;List(a, b) Would be mechanically translated to &gt;def f = throw Something &gt; &gt;for { &gt; &gt;a &lt;- Try(f) &gt; &gt;b &lt;- Try(f) &gt; &gt;result &lt;- Try(List(a, b)) &gt; &gt;} yield result So, the `Either[Exception, B]` (here I used `Try[B]` instead for readability) can't be accessed or influenced from the original program. Therefore it will (for every expression but not definiton) be a huge chain of `flatMap` calls.
&gt; So, the Either[Exception, B] (here I used Try[B] instead for readability) can't be accessed or influenced from the original program. Sure. So you can embed the original program into this Either-wrapped form. But it's not an isomorphism because a lot of things that are possible in the Either-wrapped language aren't possible in the original Scala. So while you can do some forms of analysis this way, in the general case you can't get away from having to think about the awkward specific mechanics of exceptions.
How do you feel, btw? After VS's debugging capabilities IntelliJ looks miserable to me. 
I was in the same boat. I’d mirror the other commenters, stay away from cats and especially Scalaz - but with a twist - I had a great time learning about the stuff around cats in scala-exercises. Rapidly I found that many things were functors and monads, and operators like .sequence and .traverse are invaluable for turning a List[Option[Int]] into an Option[List[Int]], which cropped up for me during JSON deserialization a bunch. For the record, this advice is most useful in month 2, not week 1. For comprehensions are wonderful as well for correctly short-circuiting processing behavior and keeping track of things like mutable State. The most key thing to learn is how the error messages play out. I’ve seen hundered a of downstream errors because the typer borked on one line, sussing out what that error really meant was harder in the beginning, but now it just pops. Inevitably you’re going to have to use or want implicits. Debugging implicit derivation is hard, but implicitly will be your friend here. I primarily use the power of implicits for implicit classes, type classes, and type class derivation. Shapeless coproduct is also worth taking a look at, it makes dealing with multiple types in a collection so nice, though it’s runtime overhead is a bit sickening. 
I don't think I understand exactly what you're looking for, but if you just want to count all occurrences of each number then you could do something like this: https://scastie.scala-lang.org/QVRKf2scQayK9zbrJIK5Dg
Are there some courses in southern europe?
thanks for responding. i was too busy to google it at that time. but u made me go, *ive been doing this wrong all this time* LOL
Thanks, that's really cool. My attempt at explaining it: def f(v: Int): Int = throw new RuntimeException(v.toString) def f1 = f(1) + f(2) def f2 = f(2) + f(1) val v1 = try { f1 } catch {case e: _ =&gt; e.getMessage} val v2 = try { f2 } catch {case e: _ =&gt; e.getMessage} As long as we don't catch, f1 and f2 are fine and behave identically. When we add catch, we make operation `+` not associative on Int arguments - v1 and v2 will contain different values. 
VS is what I'm most used to and provides a phenomenal experience. So far I've just read a few books and tutorials so no professional projects yet. But here are the things that I enjoyed about Scala, sbt, and IntelliJ: * I enjoyed the consistency of the language * I liked having a repl (and in IntelliJ worksheets) where I already had references to my project to play with. Normally I'll use Linqpad for a similar experience. * I like that sbt would watch/compile/test my project as I saved files
Thank you, I hadn’t thought about the typeclass witness having the same name as the type param. It looks more succinct, I’ll try that out.
You don’t necessarily need to have a Java background to come into Scala. The problem is that your background also only includes OO experience in C++, which is a whole other world compared to the OO world of Java/C#/PHP/Python. Now, Scala mixes Java like OO with functional, and you don’t seem to have a background on any of those, which makes it 2 things to learn on top of the language syntax and idiosyncrasies. Since you have a C++ background, learning a more OO style of Scala might be a better option. On the other hand, if you’re feeling adventurous it might be worth it to jump into the functional side of things, in which case, Scala borrows a lot of the ideas from Haskell, and those ideas are implemented in a more clear and simple way in Haskell, so it might help to read something like the LYAHFGG to grasp the concepts and understand them in Scala and why they are implemented as they are (spoiler alert, it’s usually so that they still work inside of the OO paradigm).
Some great advice on this thread, I would add one more thing: you can use `scala.io.Source.fromFile` to read all the lines in one go instead of having to manually read lines. It’s like Python’s `for lines in open(my_file):...`: import scala.io.Source for (lines &lt;- Source.fromFile(myFile).getLines) { ... }
Hmm, it might be helpful to maybe try to pick up C# or Java or Ruby. It's just really that you're going to be learning two different paradigms at once, because Scala is by design meant to be a fusion of object-oriented and functional paradigms. I wouldn't say knowing Java is necessarily a requirement per se, but it helps to know what's going on under the hood when you're debugging stuff. For example, [Scala concurrency builds upon Java concurrency](https://twitter.github.io/scala_school/concurrency.html).
I didn't go through the same journey as I knew Java, but I will say this: Scala is quite different from Java as a *language* so just learn it like you would any other language. I highly [this book](https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=EAX84Y3S6254ZSVDJR52) written by the language's creator. It sticks to the language itself and doesn't go off the deep end on functional programming, which, as others have noticed, is what you're going to find via Google. You'll eventually want to learn FP (when that time comes, get the [red book](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1543965938&amp;sr=1-3&amp;keywords=functional+programming+in+scala)) but learn the language itself first. Scala works perfectly well as an imperative, OO language. Now, where you're going to be at a disadvantage is with the Java *ecosystem*. Even though Scala is an entirely separate language, it still runs on the JVM. That means it can interface with Java code, so if there's a library out there that does what you need in Java, you end up having to expose yourself to Java. Luckily, at this point, most libraries you'd want are either pure Scala or have wrappers written for the Java counterpart that keeps you in the Scala way of thinking. It also means that you will need to understand how the JVM works. In the end, all Scala code gets compiled directly to the same bytecode as Java to run on the JVM. You'll want to understand what is going on at compilation time, what the class path is, how to work with jar files (java archives), etc. For this, you may end up reading articles/tutorials written for Java. Beyond that, JVM runtime tuning is a specialty in and of itself. So, approach the language like you would any other language, but understand that you're also living in the Java ecosystem and don't be afraid to expose yourself to some Java if it means getting access to a library you need or better understanding the JVM where your Scala code ultimately runs. If you can code in C, C++, and Go then Java is cake.
Thanks for the tip - in my case I'm trying to read a piped in file (e.g. ./myprogram.scala &lt; input.txt). Is there an equivalent for this use case? I looked at scala.io.Source.fromInputStream but it wasn't giving me what I wanted. In fact it gave me a rather confusing error: " error: overloaded method value fromInputStream with alternatives:"
You don't need to wrap everything in `Task` as long as your functions are side-effects free. I'd define the first function just as `Option[Long]` but make sure you don't call `get `on an optional value, that defeats the purpose of using `Option` in the first place (`claims.find(_.id == id).get`). The second function is a good use case to ensure stack-safety. I'd probably go a step further and would not commit to using an effect type [too early](https://softwaremill.com/free-tagless-compared-how-not-to-commit-to-monad-too-early/) (`Task`). The more abstract the better. Note that there's also a `StackSafeMonad[F]` available in `cats` and `monix-3.x` is compatible with it.
Hmm, it might be helpful to maybe try to pick up C# or Java or Ruby. It's just really that you're going to be learning two different paradigms at once, because Scala is by design meant to be a fusion of object-oriented and functional paradigms. I wouldn't say knowing Java is necessarily a requirement per se, but it helps to know what's going on under the hood when you're debugging stuff. For example, [Scala concurrency builds upon Java concurrency](https://twitter.github.io/scala_school/concurrency.html).
Hmm, that's weird, although the `Source.fromInputStream` method _is_ overloaded, the overloads have quite different signatures, so it should be difficult to trigger that error. Anyway, this seems to be working fine: object SourceTest { def main(args: Array[String]): Unit = { import scala.io.Source val source = Source.fromInputStream(System.in).getLines println(source.mkString(",")) } } E.g. cat somefile | scala SourceTest.scala
Yep this a great post. Only things I would add or I guess emphasize on is to embrace the collection api of Scala, it's wonderful. This will help tremendously when focusing on immutability. There is never a reason in Scala to use var except laziness. Learn the Option and Either patterns. If you haven't already. Option means you should never run into a NPE. Using Either pattern allows for better error handling and controlling data flow through your application. Both the collection api and Either pattern will also help prepare you better for FP.
&gt;You don't need to wrap everything in `Task` as long as your functions are side-effects free. Usually the reason of wrapping non-pure functions in an effect monad is to make function pure and *move* the side-effect to the *edge* of your application (I'm not sure I used exact terms used by people when explaining this). In a modernt application this likely will be your network (f.ex. http) API. So some significant part of your code will be expecting to get effect monad instances from the underlying code. Since different types are not composable, wrapping your pure function in an effect monad may be your easiest (and only?) choice if you want to combine the result of non-pure functions with the result of pure functions and emit this to your API code. Is this a sign of a badly designed application? &amp;#x200B;
http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html#dont-over-engineer &gt; If your function only needs a single method from an object, and aren't sure if it will need other things later, pass in that method rather than the whole object so you can't use other things.
&gt; Usually the reason of wrapping non-pure functions in an effect monad is to make function pure and move the side-effect to the edge of your application (I'm not sure I used exact terms used by people when explaining this). In a modernt application this likely will be your network (f.ex. http) API. So some significant part of your code will be expecting to get effect monad instances from the underlying code. That's correct. &gt; Since different types are not composable, wrapping your pure function in an effect monad may be your easiest (and only?) choice if you want to combine the result of non-pure functions with the result of pure functions and emit this to your API code. When using tagless final you only need to worry about your concrete effect type at the edge of your application (eg. your `main`). You can always compose pure algebras and programs without worrying about purity.
Thanks! That is more or less what I expected to hear - would you mind giving a quick glance over to my updated version? https://pastebin.com/h1vPLzGv
Thanks again for the help here - would you mind taking a quick look at my updated version? I'm also not quite sure if I understood the way generics are being used in the claim.eachSquare function - did you use a generic to let us use List, Vector, or ArrayBuffer without explicitly choosing one? https://pastebin.com/h1vPLzGv
I'll give this a shot, but I'm not sure if others who have come to appreciate pure functional programming see things this way. Categories like Monad or Applicative Functor are the modus operandi of the FP paradigm in a similar fashion that classes, abstract classes, traits, and interfaces are in OOP. Much like a Java developer leveraging these things to get enough abstraction to wrap their head around a code base, and maybe write less code via inheirted polymorphism, FP devs use their abstractions to get composition, easy reasoning via purity, compiler-verified correctness, and sometimes parametric polymorphism. With type inference and FP libraries like Cats and ScalaZ, you're not paying a big boiler plate tax to use these abstractions, but it's a steeper learning curve than classes and objects. Maybe a better way to put it: would you ask a C developer if pointer arithmetic was really necessary? After all, you can pass by value; why all the mental overhead? The reason pure FP structures in Scala give the impression that they're something extra that you won't need is that you don't have to write Scala in a FP style at all. Like OOP, many FP languages including Scala give lots of leeway in how abstract and generic you make things. Ex: Lenses like Monicle vs. just plain mutable variables.
I’m not sure exactly what your question is, but none of the things you describe have anything to do with FP or category theory, and , specifically the second example, doesn’t even make sense. Regarding the first example, it doesn’t seem to relate to FP at all, as at its core it seems to deal with inheritance a lot, so it’s already a design problem at the OO level. I am not sure what you mean “merging two Employees”, but most developers I know (FP or not) would not recommend doing anything other than merging the Employees. Certainly the FP ones would not recommend trying to do it to a superclass, this being an OO concept, and also this sounds like the kind of problem that requires knowledge of the internals of this value. The second problem, doesn’t seem to make sense. Definitely if all you want is a function that takes two values and returns a tuple, you can easily do that, even though you don’t need it because you can already zip a list together which gives you exactly this. But even if for some unknown reason you still want to implement this, it would heavily depend on the use case. You can implement it using A and B as type variables, just because you don’t need to know the details of two types to put them inside a tuple. This has the advantage of giving you encapsulation, in the sense that you can’t do anything to those values because you know nothing about them. There’s no YAGNI here because encapsulation is always a good thing for internal code quality, and you’re not implementing any extra functionality. All of this is done in a type safe way, and whatever input type A and B turn out to be, will have to match the output types A and B, or your code will not compile, there’s nothing that you could use here with shapeless to make it more or less type safe. The reading I’d recommend would be Martin Fowler’s writing on YAGNI, on the OO side find some writings about composition vs inheritance. Regarding FP and category theory, I would recommend the LYAHFGG book (even though some people are not too keen of it), and maybe looking at some category theory intro videos in /r/scalaconferencevideos
Here’s a direct link to one such talk: https://www.reddit.com/r/ScalaConferenceVideos/comments/8uutd9/scala_days_2018_a_pragmatic_introduction_to/?st=JPAKZMOR&amp;sh=b1c0ad95
I think FP is just as prone to premature abstraction as OOP is and YAGNI is a pretty good guideline to keep that down in both paradigms. For every FP person creating a `Merge[T]` typeclass too early you'll find an OOP person creating a `MergeStrategy`/`MergeVisitor`/`Mergable&lt;T&gt;`/whatever setup too early. &gt; in _this application_ you will never find types other than `A` and `B`. This is a key distinction in my mind. Applications have different requirements than libraries. Applications know the entire state of the world. They are aware of other dependencies, requirements, infrastructure, etc, and can make good use of all that information. You actually can know things like how many types need to be merged. On the other hand libraries know almost nothing except how to do a small piece of behavior given relatively restricted inputs. They don't necessarily know the requirements any eventual application will have. They don't necessarily know what other libraries that application will have access too. A well-abstracted library where those details can be filled in after the fact suddenly becomes useful in many different contexts for many different people. Libraries like Cats and category theory and the ecosystems built on top of them take this to its inevitable conclusion. They describe "things" in very abstract but, crucially, well-structured terms. They don't care _what_ something is, only how it behaves, and sometimes only how it behaves relative to other "things". This allows you to write code that, once correct, is correct forever. And that means when you're hunting down your bugs you have fewer places to look. It also means that once a problem is solved, it's solved for everything; you can plug in any well-structured thing and you still don't have to write the code again. It's sort of the ultimate in re-usability. It turns out that _a lot_ of problems can be described in these pre-existing generic terms and consequently can make use of all the pre-existing code written to solve generic problems. So what happens to all of that code you would have written to re-solve such problems for your specific case only? YAGNI ;).
&gt;Much like a Java developer leveraging these things to get enough abstraction to wrap their head around a code base, and maybe write less code via inheirted polymorphism, FP devs use their abstractions to get composition, easy reasoning via purity, compiler-verified correctness, and sometimes parametric polymorphism. In OOP new developers very often do following mistake. Given simple model, even something like class Employee( var id: Long var firstName: String var lastName: String ) { def fullName: String = s"$firstName $lastName" } they will come up with long and highly branched inheritance chain: \`WithId\` , \`WithName\`, \`Printable\`, what not. The mistake is (was?) so common, that various learning resources (blog posts, talks, I think I even saw that in a printed book by some famous author) started offering heuristics like: if you have only one implementation of your interface, you don't need an interface (that's where they usually mentioned YAGNI). Also real life demonstrated, that avoiding code duplication via inheritance is often hurtful (in practice siblings very often diverge due to change in requirements), to the extent, that inheritance became an antipattern (very ironic because it's one of OOP pillars). My point is, in OOP YAGNI quite often refers to the OOP itself and suggests you to write simple procedural code. Would you say, that in FP you can also apply YAGNI principle to the FP itself (if language permits it, like Scala does)? You partly touched the topic I think: &gt;many FP languages including Scala give lots of leeway in how abstract and generic you make things. Ex: Lenses like Monicle vs. just plain mutable variables. it is just not clear for me, whether, in your opinion, developers should embrace the leeway and fall-back to other paradigms when necessary (even when purity and referential transparency are scraficed) or should they stick with FP code even if a language does not impose any limitations in that sense?
&gt;You can implement it using A and B as type variables, just because you don’t need to know the details of two types to put them inside a tuple. This has the advantage of giving you encapsulation, in the sense that you can’t do anything to those values because you know nothing about them. There’s no YAGNI here because encapsulation is always a good thing for internal code quality, and you’re not implementing any extra functionality I may be wrong, but my understanding is that YAGNI applies not only to functionality but to lines of code (does not matter functionality or boilerplate). Assume you're implementing such a function to operate on `ConcreteA` and `ConcreteB`. And in your part of application it does not look like anything other than `ConcreteA` and `ConcreteB` will ever appear. I see 2 options (please let me know, if you see more) 1) as `(ConcreteA, ConcreteB) =&gt; Result` to operate on concrete types 2) as `(A, B) =&gt; Result` and additionally implement abstractions `A` and `B` that expose only pieces of information from `ConcreteA` and `ConcreteB`. To me 2) looks like strictly contradicting YAGNI. Even if there's high chance, that both types and function will evolve I would still chose option 1). The only case where I'd prefer 2) is when `A` and `B` abstractions are already implemented for me and I know for sure, that I won't ever need to maintain them. Which option would you prefer and why?
YAGNI is a caution against spending time implementing code you aren't going to need. This isn't YAGNI so much as "you already have it available, so use it if it helps." In other words, if you already have a `combine` function for your `Employee`, cats gives you `combineN`, `combineAllOption`, and `maybeCombine` functions for free, in addition to whatever application-specific code you've written that can use a semigroup. If those already-existing functions don't help you in your particular case, don't bother with the small boilerplate to make an `Employee` a semigroup, but if you refuse to reuse them only because they are "too generic," then you are violating DRY, another important design principle.
Regarding the second example: you're right, has not much to do with FP, more Scala itself. Thank you for pointing this out. I put more effort into the next example :) Consider something like `Seq.sliding` (`Seq(1, 2, 3, 4, 5, 6).sliding(2, 2)` will give you an iterator over smthng like `Seq(1, 2), Seq(3, 4), Seq(5, 6)`. I've seen scala code, that goes as far as reimplementing `sliding` in a type-safe way, so if first argument to it is known during compile time, it emits an iterator over `Tuple2(1, 2), Tuple2(3, 4), Tuple2(5, 6)`. The motivation behind this was that by iterating over `Seq`s with 2 elements in each one, you can no longer reason about the code locally: in one place you know, that there are 2 elements in each `Seq`, but after passing this further you lose this information. The team even went farther and implemented a macro and implemented a generic type-safe `sliding` function that produces tuples of correct arity. The last part for me is 100% YAGNI. Would you agree? As for that less generic `sliding` that only produces `Tuple2`, I don't have an opinion yet. Trying to get one by asking people here. What do you think? Is it worth reimplementing a function from standard library to preserve local reasoning (sort of a feature) or YAGNI?
That’s not at all how it works. Implementation of this for concrete type: def f(a: MyType1, b: MyType2) = (a, b) Implementation using type variables: def f[A, B](a: A, b: B) = (a, b) You only use this when you don’t need to know the implementation of the types. YAGNI doesn’t apply to internal code quality, you always need quality. Knowing less about the types you’re dealing with means less coupling, equals better internal quality.
I feel there’s a misunderstanding of how type variables work here, and that somehow you seem to think there’s some extra work you need to do here. There isn’t. In the example I provided above, at compile time the compiler will replace the type A and B with the types used in the context of the call, A and B only exist in the context of the function f.
Thank you for the link. It's an awesome read. My biggest confusion is deciding which function is more powerful. The one, that operates on semigroups and thus can operate on many many types, or the one that operates on concrete types, but knows much more about those types. From my experience the first one will likely require more boilerplate. Does "more code" mean "more power"?
Regarding this one, both approaches are type safe, using a tuple vs using a Seq doesn’t give you any more type safety. What the person implementing this might want was a more fine grained type, which is better than a less fine grained one. The reason for this is that using a more specific type means that you have guarantee that you have the correct values. If you require two values, and use a Tuple2 or some case class that holds those two values, then you’re guaranteed to always hold valid data. If you on the other hand use a Seq then you’re now doing the work that the compiler could have done for you and validate that you always have two values. In this case, if you had an odd number of elements (which means that even using Seq for the initial use case might have been a bad idea), your last Seq would only have 1 element and wouldn’t be a pair of values, so you would know be in a situation where you’re writing more lines of code to deal with edge cases.
In my example `Result` type was not a tuple of `A` and `B`. To make it more concrete, consider Twitter's `Future` and `Monad` from Cats. And your function actually only requires its argument to be `Monad`. So case 1) in my previous comment would correspond to `f(t: Future)` and case 2) to `f[F](f: F)(implicit F: Monad[F])`. If you go with case 2) you would actually have to write a lot of boilerplate.
This is actually one of the cases where abstracting the monad type is most useful. You don’t actually have to write any boiler plate and if you use the second form (the type parameter there should be actually F[_]), it means a few things: 1. You can replace the kind of effect you’re using (hint using Future is a bad idea because it’s not referentially transparent, so you’ll want to change it at some point) 2. Encapsulation. Because you have a generic type you can’t do the wrong thing and let the effect leak into your pure code (I.E. in the case of a future you can’t run things like Success or Fail, in the case of IO you can’t run any of the unsafe* functions) 3. Principle of the less powerful abstraction. By using a generic effect F there it means that you can also now contain your F to the least powerful abstraction you need on each step. So you probably won’t event constrain it to a Monad unless you need to flatMap on F
I'm learning Scala right now and also don't have a background in Java. The piece of advice I would give is to pick up a copy of Programming In Scala, 3rd Ed., preferably a ebook version so you can have it open side-by-side with a Scala REPL, and read a chapter or two each day. Make sure you type out the examples and play with the code to get a feel for what's going on. Seriously, this book is amazing. It explains the language extremely well with intuitive examples. It's a massive book, but it's absolutely worth the time investment required to read it. &amp;#x200B;
I'm very happy with the way this conversation unfolds :) 1. Let's eliminate this by assuming, that I'm using my own custom type `MyIO[E, A]`, which is referentially transparent. Why would I want to change it at some point? 2. Just to clarify, do you mean, that I can do `MyIO.onSuccess(a: A =&gt; ???).onFailure(e: E =&gt; ???)` while inside my function (essentially adding more side-effects)? Would you still consider this critical if `MyIO` execution was delayed like for example cats `IO`? Why? 3. This to me looks very similar to 2, frankly
The fact that inheritance is today considered a pillar of OOP is ironic because Alan Kay is repeatedly cited as saying that’s not what it’s about, it’s about objects sending each other messages. Anyway, in general—yes, it’s possible to go down the path of implementing too much with FP, the same as it’s possible with OOP, or any other style. This is something you need to develop a sense and taste for, after experience with FP codebases and their different requirements.
I think the OP has a point. For example I can't see myself using tagless final ever. I probably won't need that parameter. 
Afaik, gradle scala plugin only supports zinc up to 0.3.x. Because of that I couldn't figure out how to build a project with circe using gradle (maybe it's possible, but I decided it's not worth the effort). Otherwise I'm happy with it. I think its plugins ecosystem (except for scala plugin) is much more mature than sbt's.
Yeah that’s great. I would call that code idiomatic Scala for sure. Good job.
YAGNI and the principle of least power are indeed useful but they have one big issue: they rely on a subjective referential. From JS or any other untyped language, you may ask: do I need types? Are types over-engineering my code? From the Scala side, you may think on the contary that types take some power from you by restricting what you can write. So do you need the power of untyping? In my code I use mutable variable only when there are strong properties I can not have with immutable variables. I consider mutation as a strong power from which I must flee whenever possible. But Java deleopers may think differently. They could consider mutation a normal thing, and immutability an extra power. I often ask myself: do I need to be specific? Because I know specific code has to be maintained when specificity changes. Others may ask: do I need to be generic? Because they know generic code is more difficult to write. I depends so much on who is asking that personnaly I consider this principles as under specified and so useless.
&gt;So while you can do some forms of analysis this way, in the general case you can't get away from having to think about the awkward specific mechanics of exceptions. Right, that's exactly why I like it: it allows me to mentally "check" if a function could be considered RT. If I have no way to mechanically convert it into the Either-wrapped form, it's probably not RT. As you mention, Either-Wrapping could offer a lot functionality that can't be translated to exceptions in the general case, so it is only isomorphic if it is used as in my example (wrap every expression and then flatmap on it. No traversing or certain MonadError functionality for example). But I was unsure if exceptions can always be translated into the Either-wrapped form at all, so thanks for confirming my assumption.
Is Option and Either similar to what Rust has? I have familiarity with Rust but I have programmed anything moderately complex in Rust. I don't consider myself to be a good Rust programmer perhaps beginner is the right term. 
If you are dealing with side effects, then yes, it needs to be delayed and execute every time it já ran, otherwise it stops being referentially transparent (exactly like cats.effect IO). This is one of the reasons you don’t want your code to have the power to actually run the effect. Using your own custom type for this would be a bad idea, because there are libraries out there that have years of work invested into creating this type of effect monad, so there’s no need to reinvent the wheel. Also, if you’re using your custom type then you’re probably better off not abstracting over it, because sooner or later you’ll need to do something with it, at which point you either find yourself creating a bunch of instances of cats type classes for your custom type, or reinventing cats/cats.effect.
At the risk of totally digressing, for me the hardest part of programming in Scala is working in a team with different levels of experience. The fact that Scala is un-opnionated means that people create opinion for themselves, can lead to a lot of debate. Therefore, in order to keep the debate healthy and as far away as possible from statements like "I feel like ...", finding objective arguments to support your view is absolutely necessary. Here's a few observations I made for myself, that I hope to be objective, and that I use to support my opinion about why abstraction is necessary. * Features are assets, **code is a liability**. The more code you have, the more you're likely to have a bug, but also your velocity (time it takes to add/amend features) will decrease because the likelihood of a new feature impacting old code will increase. * So you need to keep code size to a minimum. In order to reduce code size, you need to **minimise interface surface** of your components. This also is good because it helps parallelise work between members of a team : the less likely you are to concurrently touch the same source file than a coworker, the faster your reviews will go. * Reducing interface surface is directly tied to **separation of concern**, and to "least power principle", which I tend summarise, in the context of Scala as as "if a function doesn't use something, it must not have access to it". It's probably a terrible way of summarising it. * NB : by interface surface, I'm not just talking about traits, but also data models, function signatures, etc ... * Interface surface can be quantified, for instance using [cardinality](https://typelevel.org/blog/2018/11/02/semirings.html). Cardinality of traits is not really computable in Scala without using macros, but it's easy to see that a trait is isomorphic to a TupleN of functions. * One effective way of reducing interface surface is abstraction. If a function receives a List\[Person\], does it really need to know that the list contains Persons, or could you just have it receive a List\[A\] where A is abstract ? Doing so minimises the interface surface. * But sometimes, abstracting away some piece of knowledge isn't enough, you need to declare that A is associated to some capability, be the ability to be combined, transformed to a string, to json, etc ... this is where **typeclasses** come in, which are abstraction of capabilities. * People generally suck at expressing new abstractions. We're concrete animals, and most often reasoning in abstract terms is quite hard. For instance, people suck at math as soon as letters/variables are introduced. Moreover, people coming up with their own abstractions are extremely often only re-inventing (or converging towards) existing stuff, and making it shitty. Avoid creating new typeclasses cause chances are gonna be bad. * So the way of writing software that I find the best is learning existing good abstractions/patterns, and using them in you use-case. **What makes an abstraction good** ? The presence of **laws** . These are invariants that any concrete implementation of the abstraction MUST ABIDE BY. In Scala, the problem is that the compiler cannot verify that you're not fucking up the laws when you implement an abstraction. Libraries mitigate it by kindly offering you **reusable property-based tests**. They're giving you tests for free ! * Laws are also used to build a personal framework of reasoning. It frees you from un-necessary questions you might have about the way the software works, which usually leads to less experienced developers adding un-necessary code to protect against non-existant problems, such as : what if (1 + 2) + 3 != 1 + (2 +3 ) ???. So lawful typeclasses (like the ones provided by libraries such as cats or scalaz) **protect you against accidental complexity.** It's a lot digression on my part, apologies for it, but I think the tl;dr of what I'm trying to say **"when it comes to abstractions, the question ain't whether you're gonna need the abstraction, it's whether you need to know the concrete details"**. The learning curve is hard, but training is quantifiable, whereas complexity creeping up on you because the business asking you for more and more features while you're failing to minimise the code efficiently enough to retain a decent velocity is a cancer. 
I think you are correct here. You might want to switch from `cats.effect.IO` to ZIO or Monix Task, but this is not the reason why you do this. Points 2 and 3 are very similar, but they are the reason why you abstract over the effect type. If you see a function like `def f[F[_] : Sync](x: Int): F[Foo] = ...`, you know that `f` will not do any asynchronous/concurrent business. If it's `def f(x: Int): IO[Foo]`, all bets are off. It could do whatever it wants. It's the same reason why you should prefer generic functions over concrete ones. Consider `def f[A](as: List[A]): Option[A]`. Just by looking at the signature, you know that the `A` in the output can only come from the input. And you also know that `f` cannot look into any of the `as`. This is incredibly helpful, even if you only ever call `f` with the same type `A`. As a benefit, functions like that are easier to test, because you don't need to summon a value of some complicated domain class. You could test `f` with a list of `Int`s.
Yes, Rust and Scala Options are similar: an option is either None or Some(value). Rust's Result is similar to Scala's Either. Either can be a Left or a Right, and the convention is to use Left for errors and Right for values.
Not sure if this is what you mean, but when you want to combine an effectful value with a pure value, you can map over the former. E.g. ``` val fa: F[A] = g(x) val b: B = h(y) def f(a: A, b: B): C = ... fa.map(a =&gt; f(a, b)) // has type F[C] ``` so there is no need to wrap `h` to return `F[B]`.
[removed]
The semigroup one is strictly more generic and less powerful, where "powerful" != "useful".
Aside from as an entertaining exercise, why would you ever do something you don't have to? I think that's a very reasonable approach. Sometimes people do need to abstract over things inexpressible in concrete types, like when writing a library (also—in my opinion—good tests should be using your code as a library, as far as possible without using mocks). At that point the tagless final or Free monad approach may make sense, depending on the user-base you're catering to.
This is a fascinating discussion. I also think that http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html#dont-over-engineer and its sub-parts should be required reading for every team doing Scala programming. One thing that stuck out to me is how to keep the balance between YAGNI and DRY. "Don't Repeat Yourself" calls for abstracting out duplications whereas YAGNI warns against premature generalization. To tell when to use which I have seen a rule of thumb saying that when you have 3 or more occurrences of the same pattern you should try to find a generalization That works generally well for me. But then comes another question: How do you tell "you ain't gonna need it" when it comes to types? Or more precisely: when it comes to decide what aspects of your program should be captured by statically checked types? Just the minimum to avoid manual casts? Or should types also capture the length of a list, the side effects of a function, the running time and resource consumption of computations, the physical unit of a quantity, the localization of a string, etc? Right now there's a strong tendency to answer this with "ideally, all of the above", but it would lead to very large types, and will likely give error messages that are very hard grok. Complexity in the types is just as bad as complexity elsewhere. I am always surprised by the cavalier nature in which some programmers just add another type parameter to an already long list without regard for the legibility of the resulting program. The problem is, when it comes to types, there's no YAGNI that could balance a DRY. How do you know you will _not_ need the additional safety afforded by a more complicated type? Ideally you'd look at run-time failures. If there are at least N (for some number N to pick) actual run time failures that could have been prevented by a more complicated type, it's worthwhile to invest in the type. But this is not a very concrete or practical guideline and it's retrospective: something bad has to happen before one takes action. I would love to find out better guidelines here. 
&gt;From my experience the first one will likely require more boilerplate. Maybe this is, why you are skeptical and it is very to be. Indeed, if you are writing more code when using more generalized functions (like from cats) this is not a good sign. But sometimes the reason is just that the way of using the generalized function (and the related declarations to use it) are not the best one. After using e.g. cats a lot, you get better and better with that. So, if you have such a case, please write it down and let's see if it can't be solved more concise. Usually cats implicits and sccala kittens (based on shapeless) are sufficient. To make that more concrete. Say you have two two options of strings and want to combine them, so: `Option("hello")` and `Option("world")` become `Option("helloworld")` (or None if one or both are `None`) You might do something like `(option1, option2).mapN((a, b) =&gt; a + b)` but you can also write `option1 |+| option2`. The more you know the better it gets, but sometimes it's worth to use a custom function instead of defining lots of typeclass instances. &amp;#x200B;
Good questions! 1. Multiple reasons. Your company just bought another one and you need to merge that code. Unfortunately, they have \`MyIO2\` which is similar, but has some extra functionality and misses some, so can't just directly be replaced/refactored. Or maybe in some place you want to replace your \`IO\` type with a streaming \`type\`, using the same function to work on it. At least in tests, that's common! In a test, I don't care about effects, so I will just use \`cats.Id\` instead of an actual effect-type. So my test now looks like \`doSomething\[Id\](myInput) shouldEqual expectedResult\` instead of \`doSomething\[MyIO\](myInput).unsafeRunSync shouldEqual expectedResult\` (or having to rely on the test framework to support async tests. The other questions have been answered. But here is a different perspective: use the least powerful tool, right? So Why would you use \`DoubleLinkedArrayList\` (madeup) instead of \`Iterable\` if all you need is iteration anyways? It makes people question why it must be a \`DoubleLinkedArrayList\` or if that's just accidential complexity. Same goes for the input of functions: make them as general as you can and as specific as you have to.
Have you ever head a list that you \_knew\_ could not be empty? You still had to deal with \`headOption\` returning an option or using code like \`.head\` which could become unsafe in the future if someone changed other parts of the code. Giving more precise types prevents that and thus adds complexity at the definition side, but reduces it at use side. So depending on how often it is used, it might payoff. YAGNI is not the correct term here in my eyes.
I want to ask about how can I quickly start with Scala and contribute simultaneously on a open project or start a small worthy project on my own. Because until I start building a project, I can't learn more. 
&gt; once correct, is correct forever Mmm, just wanted to crystalize this. Happy Holidays!
The second function seems to be stack safe anyway (it's tail recursive without the \`Task\`). Wouldn't that make the trampolining redundant?
if you want a numeric (meaning based on math and not blog-post-dogma) approach to determining "power", have a look at https://alexknvl.com/posts/counting-type-inhabitants.html in this context you might use "power" interchangeably with "knowledge", meaning that the more a function knows about its inputs , the more it can do with those inputs. That also means there's more ways you can implement it and typecheck, which also means there's more ways to implement it incorrectly. Types with fewer inhabitants are almost always better than types with more if you care about correctness, and parametricity lowers inhabitants. Implementing a function in terms of a Semigroup constraint simultaneously gives you a more generally useful function, while giving you less ways to write the function such that it is broken.
Well, I can say for sure I like it more than previous one. The main question is — do _you_ think the code has become better? :) I used generics to let me pass function from two ints to _anything_ and return a vector of _that exact thing_. This allowed the `eachSquare` to be used in both cases where we return an `Int` and where we return nothing (which in Scala is equivalent to returning `Unit`)
A progression for learning scala: * Start with [Programming in Scala](https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=7VR5FHA6ACETMV1RM963). * Read [Effective Scala](http://twitter.github.io/effectivescala/) by Twitter. * Read the [sbt manual](https://www.scala-sbt.org/1.x/docs/index.html). * Do the Scala Exercises for the [Scala tutorial](https://www.scala-exercises.org/scala_tutorial/) and the [standard library](https://www.scala-exercises.org/std_lib). * Read [A crash course from C++ to java](http://webcache.googleusercontent.com/search?q=cache:MN3-19ZT7OkJ:www.horstmann.com/ccc/c_to_java.pdf+&amp;cd=10&amp;hl=en&amp;ct=clnk&amp;gl=us). * [Read Functional Programming in Scala for Mortals](https://leanpub.com/fpmortals). * Do the scala exercises for [Cats](https://www.scala-exercises.org/cats). * Do the scala exercises for [Doobie](https://www.scala-exercises.org/doobie/). * Do the scala exercises for [circe](https://www.scala-exercises.org/circe). * Read the [Red Book.](https://www.manning.com/books/functional-programming-in-scala) And do the [FP In Scala exercises](https://www.scala-exercises.org/fp_in_scala/). * Read["Becoming more functional."](http://m50d.github.io/2017/01/23/becoming-more-functional.html) * Read ["Neophytes guide to typeclasses."](https://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html) * Read [Exploring Tagless Final](https://blog.scalac.io/exploring-tagless-final.html) * Read the [fs2 guide.](https://fs2.io/guide.html) * Work through the [http4s getting started guide](https://http4s.org/v0.18/) * Read the [Type Astronaut's Guide to Shapeless](https://underscore.io/books/shapeless-guide/) * Read [Akka in Action](https://www.manning.com/books/akka-in-action) Make stuff throughout the learning process. Refactor stuff as you learn more and when it is appropriate. This reads sort of a OO -&gt; FP list, but I think I organized it from simple to advanced features, ending with shapeless and akka. You can stop at any point where you feel comfortable, and jump around once you've built some things with the scala standard library. I recommend reading the Java crash course because you will inevitably end up using java libraries in many projects, and you'll need to know how to call them (not that much different than scala, but it is helpful). I recommend FP in mortals first because it's more of a how to use fp in scala than a how to build the standard fp abstractions from scratch book. Both are good, but if you are a doer and not a thinker then mortals is more practical. By the end of this list you should be familiar with most of the concepts in the scala ecosystem. You don't have to memorize all the content in the books/articles. Just read them. Then, when you hear something discussed or see it in code you'll have a reference to learn more about a given topic. 
Power refers to what you can do with an abstraction. Weak abstractions tend to have the most valid implementations, so they're often more generically useful than stronger abstractions. For example, a List is a more powerful abstraction than a monoid. You can't sort a monoid, but you can sort a list. You can't get the head of a monoid, but you can get the head of a list. But if you write a function that's generic over monoids, it's more useful than one that's generic over lists.
This is more for motivation than anything else: I worked with someone that moved from Siebel (I think it has a JavaScript-ish language) - his first professional role - to Scala and he was great :) 
I advice you to hang out on the scala gitter so we can poison your mind with dangerous ideas such as contraFunctors and comonads
I also learned Scala without bothering much with Java. I learned some Java at college, but I never did any professional work. Scala is lots of different things at the same time. It is a clean, fun, productive language. It is a powerful functional language that enables cool, math-heavy abstractions. It is a fully object-oriented language with lots of OO patterns already baked in. It is a JVM language, able to build upon the huge amount of libraries available for the Java platform. The main issue when doing Scala is that, at every moment, you have to be fully conscious of where you are. Scala is the C++ of the JVM, and like C++, there's no real need to use all the features all the time. Pragmatism will be your best ally. Another key issue is to know every person is different. If you end up working in a team, be very clear on your thought processes. Comments are essential, and whoever says Scala is so expressive that comments aren't needed is wrong. Comments are essential, because there's so many ways to solve every problem that it's not always obvious whether the code can be improved. In short: Install Ammonite, Read "Programming in Scala", bash your head against sbt until you grok it, then be free to explore all the awesome libraries. You'll have holes in your knowledge and the hole pattern will be unique to you, so write your code so anyone that isn't you can follow it. And have fun. It's a cool journey :)
While I appreciate the sentiment, skipping the typelevel stack is a grave mistake. Just don't feel like you have to start there, but you should have it as a goal 
Just wanted to say Thanks. Wow, so many helpful responses without being demeaning to a newbie is greatly appreciated. I came for a technical query but I may stay for the community. :)
I've used tagless final, and I can't see a benefit in production code. Then again, I could be misunderstanding the benefits of Tagless final style. Production code rarely changes libraries used (in my experience), and it seems like making it a generic DSL using tagless final is a wasted exercise. For example, if I know I'm using library that always returns a `Future`, where's the benefit of using tagless final? I can have my interfaces return `Future` instead of `F[_]`. In testing, it might have some benefits so that your stubs can return an `Id` or something. But using `whenReady` (in scalatest) or `Await.result` is probably good enough for tests. The only other case I can think of is if libraries start using tagless final so I can pass in whatever Effect monad I need. But currently, I don't think any stable libs are doing that. 
Thank you.
This is the correct empirical answer. Cyclomatic complexity is the way to identify implementations that need to be more tightly restricted by types.
Hi, You cannot add a typeclass constraint to a `trait` so that needs to be an `abstract class`. Here's how I might do it: ```scala trait FaultManager[F[_]] { def notifyAlarmRaise(alarm: Alarm): F[Unit] } abstract class DefaultFaultManager[F[_]: Monad] extends FaultManager[F] { private def isThrottled(alarm: Alarm): F[Boolean] = true.pure[F] def raiseAlarm(alarm: Alarm): F[Unit] = isThrottled(alarm).ifM(().pure[F], notifyAlarmRaise(alarm)) } class HttpFaultManager[F[_]: Sync] extends DefaultFaultManager[F] { override def notifyAlarmRaise(alarm: Alarm) = Sync[F].delay(println("Sync HTTP")) } class SnmpFaultManager[F[_]: Async] extends DefaultFaultManager[F] { override def notifyAlarmRaise(alarm: Alarm) = Sync[F].delay(println("Async HTTP")) } ``` I'd say you need 4 interpreters for those 4 combinations. All you can re-use is the logic which is defined in the `abstract class`. The rest is an implementation detail.
Yeah, that’s something that kinda bugs me sometimes too. The points list is generated based on the claims list. There has to be a claim with the same id as the point. I don’t think I can model this using types since a missing claim would mean an invalid state in the first place. In my revised version of the first function I also removed the points parameter and instead generate those based on the claims list.
I just realized I can include the size of the claim in the point also doing away with the find operation. 
&gt; skipping the typelevel stack is a grave mistake It isn't, though. There are many libraries and frameworks that don't use the typelevel stack. If, once OP has learned about implicits, type class pattern, higher kinded types, type variance, effects, tagless final and IO, OP decides that maybe cats / cats-effect will make his life easier, then great... but it's a long road to that point.
It certainly seems cleaner, but for me it's hard to say if it's better. It feels very alien after years working in Python. For example chaining things together like `fromInputStream().getLines.map().toVector` seems very powerful but also easy to abuse. I think I need to build up my sense of how Scala is supposed to feel and read.
Normally I'd recommend using Ammonite: http://ammonite.io/#ScalaScripts
That looks pretty nice, I will give it a try. Thank you!
Your environment at home is probably setup incorrectly if option 2 is failing. Details of the Errors experienced need to be provided to troubleshoot. How about a quick starter from [https://developer.lightbend.com/ibm/?group=scala](https://developer.lightbend.com/ibm/?group=scala) ? It shows a simple sbt project. The other option is to use one of the IDEs which support Scala , either Eclipse or IntelliJ would do. It provides the "terminal view" in addition to code view. You will need to be able to debug your programs eventually and good environment can help.
There is no error when option 2 fails, it hangs for a minute, and then the process returns 0 with no output, despite there being numerous println statements. Thanks for the sbt starter!
Quickstart: 1. Add your Scala code to `MyFile.sc` 2. In a terminal run `amm --watch MyFile.sc` 3. Make changes to your file, save, and it will auto-compile and run Also, one of my favorite features is support for inline [imports](http://ammonite.io/#ScriptImports), makes trying out new libraries easy.
You can figure out whether that’s an installation/environmental issue or specifically an issue with your Scala file, by running a very minimal script: echo 'println("Test")' &gt;Test.scala scala Test.scala
Not sure if that's the same use case but to practice the language scastie is awesome: https://scastie.scala-lang.org
For online fiddling I prefer https://scalafiddle.io/ It support typelevel libraries (cats, cats-effect, etc) and I like to play with canvas drawing there, here's a little dragon :) https://scalafiddle.io/sf/ZrB58oO/7
The first problem in the workflow itself. The correct workflow should be write tests -&gt; run tests -&gt; write code -&gt; run tests -&gt; refactor [https://en.wikipedia.org/wiki/Test-driven\_development](https://en.wikipedia.org/wiki/Test-driven_development) &amp;#x200B;
**Test-driven development** Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the software is improved to pass the new tests, only. This is opposed to software development that allows software to be added that is not proven to meet requirements. American software engineer Kent Beck, who is credited with having developed or "rediscovered" the technique, stated in 2003 that TDD encourages simple designs and inspires confidence.Test-driven development is related to the test-first programming concepts of extreme programming, begun in 1999, but more recently has created more general interest in its own right.Programmers also apply the concept to improving and debugging legacy code developed with older techniques. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thank you for your answer! Now I got another question :) Does your implementation design can be classified as subtyping? Because I heard that pure FP does not allow subtyping. If no, can you please elaborate why - because in Scala a lot of patterns look similar? If yes, how could I go on writing it without subtyping and dynamic dispatch? I'm guessing `DefaultFaultMnaager` should be `sealed abstract class` but I'm not sure I could have methods in it (?).
Subtyping is fine as it is the only way to define interpreters and programs for your algebras in Scala. Yes, you can make it `sealed`.
Both 1 and 2 should work. For either to fail suggests there's something significantly wrong with your install that is probably worth fixing. 3 should work and there shouldn't be any need for the top-level object's name to match the file name. Just make sure to pass the name of the top-level object as the argument to `scala` (i.e. `scalac myfile.scala` followed by `scala myobject`). You might need to pass `-cp .` or some such also. 4 Yes it is. Honestly most of the language tooling is set up around working with larger projects; the language experience when running a small snippet is, uh, not the best. If you want to depend on libraries, build projects to distribute etc. then I'd definitely recommend using a build/project manager of some sort (personally I prefer maven) and it's certainly possible to run a single file with these (with maven once you've added the scala plugin it's just a case of `mvn exec:java -Dexec.mainClass=myclass`, I assume there's a similar thing in SBT), but possibly more overhead than you want when you're just getting started. It might be easier to get started with an IDE's "worksheet" mode, or Zeppelin (a web-based tool that works similarly), or Ammonite as has already been mentioned.
Hello World always comes before TDD
The tagless final approach would be putting `notifyAlarmRaise` on the typeclass constraint on `F`, making it some kind of `MonadNotify` (analogous to `MonadWriter` etc.). Part of the point is that you don't need a `FaultManager` trait with multiple implementations, only a parameter: `final class FaultManager[F[_]]`. If `isThrottled` is implemented in the "manager" as shown here then it shouldn't need any `F`-context at all and you don't need the `Monad` constraint either, so really all you need is a typeclass for "can raise alarms" and this is just the ordinary typeclass pattern. Of course in this example you don't even need that much, just a function for `notifyAlarmRaise`, so you can make it a class (or better, a function) that takes a `Alarm =&gt; Unit`, but I assume the real example is more complicated. &gt; How to model these combinations without repeating myself? To have the e.g. SNMP implementation but still not committed to concrete F and then separately have the F implementation? Write the HTTP and SNMP implementations generically in terms of `F[_]: MonadNetworkRequest` or something on those lines (some trait you define), and then you implement `MonadNetworkRequest` for `Id` and async.
You timing is great :) Just when I was going to ask Gabriel if I can define `FaultManager` as a typeclass since it is some kind of a behavior. Anyway, this is how I'm going to (try to) implement it. The only thing I'm not sure I get it is `MonadNetworkRequest`. Would it be defined as a constraint on that same `F` with the `MonadNotify` constraint? Like this: final class FaultManager[F[_]: MonadNotify: MonadNetworkRequest]
&gt; Would it be defined as a constraint on that same F with the MonadNotify constraint? Like this: I'd say no, `MonadNotify` and `MonadNetworkRequest` are at different levels of abstraction. Rather you'd want something like: trait Notifier[F[_]] { ... } final class SNMPNotifier[F[_]: MonadNetworkRequest] extends Notifier[F] { ... } final class HttpNotifier[F[_]: MonadNetworkRequest] extends Notifier[F] { ... } final class FaultManager[F[_] /* possibly :Monad */](notifier: Notifier[F]) { ... } Note that at this point we've got rid of `MonadNotify` in favour of an explicit `SNMPNotifier` and `HttpNotifier`: when there's more than one possible implementation for the same `F` it's probably better to be explicit. `SNMPNotifier` and `HttpNotifier` know about network requests, but `FaultManager` only knows about `Notifier`, not the low-level details of how it's implemented.
Find a small problem that you have. If you've programmed before, half an hour might be enough time to be doing something basic but useful - some calculation that you want to know, or some repeated command. Then build out from there.
It's much more commonly needed in library code than in application code. You're right that an application will usually know what the concrete type is, but in a library that's less common - and it is relatively normal in a larger application to have code that gets reused with multiple types. E.g. in one application I worked on we had some per-client reports and one report had to make a callback to a (slow) client-provided HTTP endpoint, and another needed to accumulate some statistics, so I wrote the generic report in `F[_]` style and it was `Future` for one client, `Writer` for another, and `Id` for most of them. We could've always used `Future` or always calculated the statistics, but it would've made debugging harder in the common case. Another use case is when you have big systems that genuinely do need multiple backends. I worked on a system that had two datastores because old data was stored in the (expensive, close-to-capacity) single-server database that the system had been originally designed for, while new data was stored in a distributed system. Building up a query language with different interpreters for each system felt like a much nicer way to structure that code than trying to have a common facade. `Await.result` is fine when your code succeeds, but `Future`-based code is much harder to debug when it's broken (stack traces are a lot less useful etc.). So personally I'm happy to pay the costs of tagless final to avoid having to have `Future`s in my tests.
You should be using SBT. Will greatly "simplify" your workflow, despite its... unwieldiness.
If you decide Ammonite scripts are too small and you need a real build too, using [Mill](https://github.com/lihaoyi/mill) and `mill --watch foo.run` is a pretty nice experience too
Anyone else tried using graalvm with scala? I've found that the generated native binary crashes if string interpolation is used. I did file a bug but was wondering if there are workarounds I could try https://github.com/oracle/graal/issues/822
We currently are only based in London, however a lot of people travel from all over Europe to attend.
There's one coming up on the 10th if you're keen [https://skillsmatter.com/meetups/11322-london-scala-users-group](https://skillsmatter.com/meetups/11322-london-scala-users-group)
So I COULD keep the `MonadNotify` if I'd want to, but it's better not to? I'm asking because I'm curious how you'd implement it in Haskell for example, where there's no subtyping. Sorry for bothering, but I want to make sure I 100% understand this before dive into it. Also, I'd appreciate if you could point me in right direction (some article/blog post) that explains the connection between classes and their respective monads (e.g. `EitherT` and `MonadError`)
&gt; So I COULD keep the MonadNotify if I'd want to, but it's better not to? You could, but it wouldn't really be a typeclass any more: you'd have "incoherent instances" in the Haskell terminology. &gt; I'm asking because I'm curious how you'd implement it in Haskell for example, where there's no subtyping. In Haskell I'd either have a typeclass with instances for `SNMPNotifier` and `HttpNotifier` (basically just replacing our `Notifier` trait with a typeclass), or more likely just pass the notify function around as a bare function. I can't emphasise enough that these complex techniques are only for when a simpler approach won't do: plain old functions and values are the way to go as much as possible. &gt; Also, I'd appreciate if you could point me in right direction (some article/blog post) that explains the connection between classes and their respective monads (e.g. EitherT and MonadError) I sort of picked this up bit by bit, but look for things on tagless final or "mtl-style" typeclasses. https://typelevel.org/blog/2018/10/06/intro-to-mtl.html looks like a reasonable introduction on a quick glance.
I was wondering if someone had a good example of connecting to an oath service with scala. I need to connect to an api using oath, and I'm a bit unsure of the best way of going about doing it. I'm also not using akka or spring, and almost every example I find is using one of those two in some way. &amp;#x200B; Thanks
&gt; the Advent of code challenges every day Do you have a link? &gt; I'm wrapping almost every function in a Task monad This sounds like a solution looking for a problem. If you're doing typical in-memory problem solving type work, you probably don't need an effect-managing solution like an effect monad.
&gt; I can point to plenty of codebases out there of varying size and complexity that are very very real (as in, deployed in production, massive scale, users, SLAs, bug reports, etc) which have maintained referential transparency throughout and derived extremely significant value from it Please do! I couldn't find any open-source apps, that use `IO` moands at scale and benefit from referential transparency. Seeing real code would help a lot.
Ok, I've lost you again :( Some of your sentences seems contradictory to me, so I'm missing something. &gt; You could, but it **wouldn't really be a typeclass** any more: you'd have "incoherent instances" And &gt; In Haskell I'd either **have a typeclass** \[...\] basically just replacing our Notifier trait with a typeclass I will probably use just functional since you said it's better for simple use cases, but could you please write the full example with typeclasses because I'm lost. Sorry again for reiterating, I hope this is my last (sub)question :D
If you write it like class FaultManager[F[_]: MonadNotify] { ... } and you have multiple instances of `MonadNotify` for the same `F`, it's not really a typeclass. If I had to write this in Haskell I'd have to do something like the equivalent of: class FaultManager[N, F[_]: Monad](n: N)(implicit isNotifier: IsNotifier[N, F]) where `IsNotifier` is a multi-parameter typeclass and `N` is going to be some token that marks HTTP/SNMP. (I'm not an experienced Haskeller though, there might be better ways) So to do that in Scala you'd do something like: trait IsNotifier[N, F[_]] { def notify(alarm: Alarm): F[Unit] } case class Http() object Http { implicit def HttpIsNotifier[F[_]: MonadNetworkOperation] = new IsNotifier[Http, F] { override def notify(alarm: Alarm) = ... } } case class SNMP() object SNMP { .... } Obviously in Scala this is just unnecessarily complicated compared to just defining `notify` directly on `Http`/`SNMP` - we do this kind of thing when `Http`/`SNMP` are third-party types that we can't extend directly. But in Haskell it's the only way to do ad-hoc polymorphism at all, so any kind of interface-with-multiple-implementations is done that way.
&gt; You could look at run-time failures. If there are at least N (for some number N to pick) actual run time failures that could have been prevented by a more complicated type, it's worthwhile to invest in the type. But this is not a very concrete or practical guideline and it's retrospective: something bad has to happen before one takes action. I think this is a very practical approach that shouldn't be dismissed. Having spent a lot of time implementing static analyzers for dynamic languages, it's always a trade-off between strictness (how much real issues you catch) and convenience (how many false positives you have to workaround/whitelist). Questions like "will this check actually help us" are a constant background to the work being done. In Scala, you can also make things more or less strongly typed: - Do I often forget to escape Scala identifier `String`? Wrap it in an `ammonite.util.Name` that sanitizes it on creation and I can pass around without issue - Do I often mix up concatenating path `String`s, whether forgetting to add `/`s in between, or accidentally adding multiples `/`? Wrap them in `os.Path`s and `os.Relpath`s that ensure they're always concatenated properly - Do I often forget to sanitize HTML strings, or accidentally sanitize them multiple times? Wrap them in `scalatags.Frag`s so they're always sanitized once. All of these are optional: you could use a more specific type, or you could not. There is a cost of inconvenience and benefit of safety, and you have to consider if it's worth the tradeoff. This applies to tests as well: they have benefits, they have costs, and I have been in multiple conversations about finding tests which have never failed so we can delete them or run them less often (e.g. on-commit -&gt; nightly) to reduce the cost. Large efforts on new tests for a stricter test suite are often prompted by "which production outage would this have prevented". I don't see why large efforts making stricter types should be any different.
Can't agree more with the basic premise of this, we do the same in our code (in regards to distinguishing between business logic errors and exceptions where exceptions map to 500 in context of REST APIs) however we are still using monad transformers to do this (i.e. we have an \`EitherT\` with a Monix \`Task\` and \`Either\`). We had to resort to using subclassing due to a lack of union types in order to sanely compose errors, so our \`Left\` of the \`Either\` has a \`GeneralError\` (which represents business logic errors) Hopefully one day we can have a look at migrating but our codebase is pretty massive and the change isn't really trivial, glad to see that classy optics are getting somewhere in regards to solving the composing of multiple errors from different free algebra's sanely but I think more work needs to be done in regards to the ergonomics.
&gt; Anyone who went through same journey as me? I came to Scala from C++. I never programmed in Java before that, but I knew it's capabilities. I also had prior experience with non-C++ features like GC and lambdas from other languages. The point is, *the language itself* wasn't a problem. However: &gt; Any caveats I should be aware of? You will find that documentations in Scala world pretty much always assume that user is already versed in Java ecosystem and infrastructure. The very first encounter will be the build tool of your choice. Next encounters will emerge by need, as soon as you start using Java &amp; Scala libraries. There is simply no centralized guide. If you decide to publish a library, you will need to go through the process, that probably have been designed by Vogons. [Helpful video](https://www.youtube.com/watch?v=RmEMUwfQoSc) 
something like https://circe.github.io/circe/cursors.html cursors in circe?
I did see this from a Gitter recommendation. This seems to satisfy my prerequisites with regard to searching the document. If using circe as something that would do the actual data fetching for me how could I compose the search terms into my own DSL? I think that's the part that I'm hung up on the most. I want to make search terms that compose with each other cleanly so that other devs can just use it without thinking much about it.
You might get something meaningful from fp for mortals - https://leanpub.com/fpmortals/read It does speak to some of the ct basics, but my impression is that it's leaner on the fundamentals and tends to focus more on language/ecosystem specifics like scala patterns for leveraging fp, typeclasses, final tagless, and the contents/layout of scalaz. My guess is that once you have implicits and either cats or scalaz mapped out things will be pretty familiar. Until you want rankn... 
[removed]
[https://www.manning.com/books/functional-programming-in-scala](https://www.manning.com/books/functional-programming-in-scala)
take functional programming with scala and then scala with cats
I am exploring something related recently, I was trying to create a parsing DSL for json that expresses which data in the JSON to be read. Recursion scheme is a good way to express such recursive structure, and you can make them composable using FreeApplicative Here's the code https://github.com/qingwei91/basil
I'm betting base and height are Int's so the JVM is doing integer division here resulting in a truncated result of 12 instead of 12.5. Because area is typed as a Double, "12" is printing as "12.0".
THANK YOU, that was it! I was reading in user input as "int" instead of "double", so Scala was double int division.
If base and height are of type `Int` then it will always truncate. If you will keep your divisor as a constant, just explicitly indicate that as a floating-point number: val area = (base * height) / 2.0 // or 2f println(area)
Wow this is very impressive. I'm thrilled you shared this and I will be looking into using.
It's never isomorphic. The point is that information is lost from your either wrapped version to the version that throws, and that cannot be recovered. 
Nice work!
I think this book will probably not teach much to the op since he already know some haskell 
The prayers of many have been heard! 😍 Thank you guys!
Great! Is support on the table for showing an expression's type, finding all references, and renaming symbols? These are what I really need and use all the time. With them, I would seriously consider switching from IntelliJ. It seems to work well with macros! Support for jumping to symbols by name and running the main class and the tests from the editor would be the cherry on top. I don't know if it's on purpose, but I like that it only checks for errors when I save the file. It really bugs me when editors go nuts with squiggly arrows just because I _haven't finished_ typing my expression. Maybe it works well in languages like Java where a program is a bunch of loosely-connected statements, but in Scala where programs are essentially made of big expressions I find it really annoying. 
Find references, rename and "jump to symbol by name" are on the nearer roadmap. It's difficult to estimate more, I prefer to focus on one item at a time and make sure the released features work OK. &gt; I like that it only checks for errors when I save the file I like that too :) I get nervous with many unsaved buffers so I save frequently by habit anyways.
Awesome, thanks guys!
Last time I used the default format, it tried to line up method arguments with the open paren of the method, like def someReallyLongMethod( a: Int, b: Int, c: Int, d: Int ) which I just cannot stomach. Every time I change the name of a method, the alignment changes! It's horrible for git diffs! Now I use style = IntelliJ maxColumn = 120 continuationIndent.callSite = 2 continuationIndent.defnSite = 2 align.openParenDefnSite = false align.openParenCallSite = false assumeStandardLibraryStripMargin = true
The defaults have changed. Currently (v1.6) the ones corresponding to yours are: * style: (undocumented) * maxColumn: 80 * continuationIndent.callSite = 2 * continuationIndent.defnSite = 4 * align.openParenDefnSite = false * align.openParenCallSite = false * assumeStandardLibraryStripMargin = false I think the customization that you have is not really worth it any more now that your main complaint has been taken care of. That's the nice thing about sticking with the defaults too: you automatically get better defaults when you upgrade.
Try this: https://scastie.scala-lang.org/9ZQkSa5ITxiiSZJb5mHZFA
Unfortunately, I think that has the same problem. What I'd like is for the append expression to not compile due to incompatible types. Is that possible in Scala?
Because defaults change it's extremely important to declare the chosen style in the config so it keeps consistent. Nobody wants to change the tool version and suddenly it trying to format the codebase in a totally different and unexpected way. Also other people using different versions of the same tool would keep trying to push different coding styles on commits. It's already painful not being able to use some options on latest version because intellij bundled scalafmt is a previous version than current. So, I'd say it's much more important to choose a style, even if it's the current default and make sure it is written explicitly in the config file.
&gt; Because defaults change it's extremely important to declare the chosen style in the config so it keeps consistent. Defaults may change _when you upgrade to a new version,_ not arbitrarily. You can prepare for a change by dedicating a single commit to a scalafmt version bump and subsequent codebase reformat. The key point is not that it's eternally consistent, but that you don't have to worry about enforcing consistency, you just make sure you're on a reasonably recent scalafmt version. &gt; Also other people using different versions of the same tool would keep trying to push different coding styles on commits. It's already painful not being able to use some options on latest version because intellij bundled scalafmt is a previous version than current. The solution is to specify the scalafmt version in your build file and set it up to auto-run that version before every compile, not rely on users running scalafmt from their various IntelliJ plugin versions.
Type aliased only the type (and its constructor), but it doesn't alias companion object. If you partially apply some arguments, Scala won't be able to guess how to translate it into companion object calls. But you can do it manually: object FreqMap { def empty[A]: FreqMap[A] = Map.empty[A, Int] def apply[A](pairs: (A, Int)*): FreqMap[A] = Map(pairs: _*) }
Yeah actually a type is just a type and not a class (which introduces a type in the name space and associated constructor (s) ). 
Perfect, this is what I was looking for. Thanks! One follow up question: Is there a way to prevent a Map from updating if the value is not the correct type? For example: scala&gt; val foo = Map.empty[Int, Int] foo: scala.collection.immutable.Map[Int,Int] = Map() scala&gt; foo + (1 -&gt; 'b') res0: scala.collection.immutable.Map[Int,AnyVal] = Map(1 -&gt; b) I would expect a `type mismatch`error to be thrown, but instead it changes the signature from `Int` to `AnyVal`.
The signature of + is def + [V1 &gt;: V](kv: (K, V1)): Map[K, V1] What's happening is that for the GuardMap you created like this val gmap: GuardMap = Map.empty when you try to add (1 -&gt; "asdf") to it, the compiler looks for a suitable V1 to infer as the type. V1 has to be a super type of both a String (the type that you're adding to the Map), and a super type of FreqMap[Int]. It finds Object as a super type of both these things, so it allows you to add the element and returns a Map[Guard, Object] as a result. You could define your own method similar to + that does not have this behaviour using extension methods if that's what you really want.
Got it. Thanks again.
AFAIR many methods (Map's update, Future's recover, etc) explicitly allow making result type more generic. If that is not what you want just use type ascription/type annotation: (map + (1 -&gt; 'b')) : FreqMap[Int] // should fail compilation if types doesn't match val x: FreqMap[Int] = map + (1 -&gt; 'b') // same here
From your responses it appears you are looking for a more typesafe solution. You might want to take a look at newtype (https://github.com/estatico/scala-newtype) and google for scala opaque types. 
It looks like it doesn't fail, but instead tries some implicit conversion: scala&gt; val bar: Map[Int, Int] = Map.empty[Int, Int] + (1 -&gt; 'a') bar: Map[Int,Int] = Map(1 -&gt; 97)
You might check few flags: https://tpolecat.github.io/2017/04/25/scalac-flags.html namely: * `"-Xlint:infer-any"` * `"-Ywarn-numeric-widen"`
Thanks for the tips and pointers. I didn't realize that implicit are resolved statically. So this was fixed with pattern match statement for base class filter. The actual count of case classes is small enough so this should be manageable for now. implicit object TxnFilterF extends CanTxnFilter[TxnFilter] { def filter(tfABC: TxnFilter, txn: Transaction): Boolean = { tfABC match { case tf: TxnFiltersAND =&gt; TxnFilterANDF.filter(tf, txn) case tf: TxnFilterName =&gt; TxnFilterNameF.filter(tf, txn) } } } If anyone is interested in Plaintext Accounting, the code in question can be found in tackler which is plaintext accounting engine written in scala, with Server and Client API for JVM/JS (by ScalaJS): [https://github.com/sn127/tackler](https://github.com/sn127/tackler) [https://github.com/sn127/tackler/blob/v0.10.0/api/src/main/scala/fi/sn127/tackler/api/TxnFilter.scala](https://github.com/sn127/tackler/blob/v0.10.0/api/src/main/scala/fi/sn127/tackler/api/TxnFilter.scala) [https://github.com/sn127/tackler/blob/v0.10.0/core/src/main/scala/fi/sn127/tackler/filter/package.scala](https://github.com/sn127/tackler/blob/v0.10.0/core/src/main/scala/fi/sn127/tackler/filter/package.scala) Again, thank you for the help! &amp;#x200B;
I believe sbt is the best way to run scala programs from source. It is easy, create two files in a directory: - build.sbt with single line `scalaVersion := "2.12.7"` - Main.scala with main method and type `sbt run` 
Cats-effect is based on cooperative yielding using thread pools on the JVM, i.e. you schedule tasks that execute on one of the threads and yield when they've finished to let other tasks run. Then there's several ways to create those tasks: &amp;#x200B; \- \`IO.async\` allows you to wrap callback-based code that is already asynchronous. You're given a callback that you need to invoke on the completion of the task. It's useful for interop with existing asynchronous Java code. \- \`IO.delay\` wraps a synchronous piece of code (that can still be forked... look below). \- \`IO.shift\` shifts the execution of the given program to a different thread pool. It's basically a fork - you can use it to keep your thread pools only do things they're meant to do, e.g. you can reserve a thread pool for blocking IO. What happens in practice is that those tasks get scheduled to run on a different set of threads, so they can run at the same time as you process other things on other threads. &amp;#x200B; As far as I know, Concurrent was made a separate entity because it's meant to be used to schedule tasks that support some form of cancellation (which is useful to prevent resource leaks). &amp;#x200B; Fiber is just a representation of a task running asynchronously (the task being created one of the methods listed above). You can spawn one of those using \`start\`. By default your IO code will execute sequentially - you can use \`start\` to do parallel programming for example.
[JLS](https://docs.oracle.com/javase/specs/jls/se9/html/index.html) and [JVMS](https://docs.oracle.com/javase/specs/jvms/se9/html/index.html)
Might be worth it to clarify a bit on the difference between IO an Future, as IO is not exactly lazily evaluated, but rather the effects are suspended until the unsafe operation is ran. Unlike futures which are eagerly evaluated but then the result is memoized, on IO effects are evaluated every time they’re ran. The reason for the existence of Async and Concurrent is because they’re different things. Maybe it’s worth a read on those type classes and also this page: https://typelevel.org/cats-effect/concurrency/basics.html From the cats.effects.Asyc docs page: An asynchronous task represents logic that executes independent of the main program flow Form the concurrency basics page, the definition of Concurrency: Multiple tasks interleaved.
&gt; detection of misspelled parameter names I’m a little embarrassed to admit how much debugging time this would have saved me over the past year.
The reason behind IO is to provide a way to handle effects in a pure way, which Furure doesn’t allow because it’s eager (doesn’t suspend the effect), and memoizes the result (a side effect in itself, and also changes what happens at the first execution and subsequent ones, so it’s not referentially transparent. Regarding the Async[F].delay, it’s actually inherited from the Sync type class, so delay just suspends the effect, for asynchronous calls you need Async[F].async, or jf using IO directly IO.async. IO itself is not tied to a thread, but it does have instances of type classes that allow you to deal with threads.
So according to you, a Scala dev should completely master Java. 
I think that it might be enough to know the main API interfaces, like Collections and IO for example.
You should bold that help things - the ability to debug missing configs out-of-the-box is a huge selling point against pureconfig. The other is overriding things with command line arguments - half the initial setup I am doing is combining scopt with pureconfig with some lenses just to be able to: 1. take defaults, 2. implicily override then with env vars, 3. explicitly override that with CLI parameters. Doing it all at once would be a huge win.
I would probably like to read the article, if the author could just unfuck his text layout.
:D 
It is pretty horrible on mobile. The content seem interesting but couldn't drag left right much longer. 
lol, it's like he copy-pasted from word, and didn't even look at it after he posted
Holy fuck. I’ll just skip answering your question if you’re going to be a dick on answers you don’t like.
&gt; the ability to debug missing configs out-of-the-box is a huge selling point against pureconfig How so? pureconfig is pretty vocal about what's missing
It might be, but I bet they won't link to your project with a comment: "they did it better". You have to advertise yourself ;)
Don't be so radical, it is just a discussion, I am not "being a dick on answers I don't like", I don't see something in my words that can make one think that I don't like the answer, I just though it could be more accurate, and I made a proposal on that direction... Anyway, sorry. 
I don't think so - I wish there was such a book, but realistically most Scala developers learnt Java first so the audience would be small. In terms of general Java books I've heard good things about Java in a Nutshell, but that was several years ago and I didn't read it myself in any case.
10.0 inches ≈ 25.4 centimetres ^(1 inch = 2.54cm) ^(I'm a bot. Downvote to remove.) _____ ^| ^[Info](https://www.reddit.com/user/Bot_Metric/comments/8lt7af/i_am_a_bot/) ^| ^[PM](https://www.reddit.com/message/compose?to=Ttime5) ^| ^[Stats](http://botmetric.pythonanywhere.com) ^| ^[Opt-out](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=Don't%20reply%20to%20me&amp;message=If%20you%20send%20this%20message,%20I%20will%20no%20longer%20reply%20to%20your%20comments%20and%20posts.) ^| ^[v.4.4.6](https://www.reddit.com/user/Bot_Metric/comments/8o9vgz/updates/) ^|
You can be completely fine without knowing any Java (well, just a bit to understand basics). However you should know in advance that a lot of positions require BOTH - scala and java knowledge. Sometimes with some very java things like spring or hibernate. So it will narrow your job opportunities like in half.
Thanks. I will go for [Core Java (8) for the Impatient](https://www.amazon.com/Core-Java-Impatient-Cay-Horstmann/dp/0321996321/ref=sr_1_8?ie=UTF8&amp;qid=1544438677&amp;sr=8-8&amp;keywords=core+java). Unfortunately for me I did Java back to 2012, I gave up because of the plethora of frameworks to master (Spring, Hibernate, ...) and I switched to Javascript/Node. Now I comeback because of all the design patterns that Scala/Akka make possible...
I wouldn't look to Scala for patterns - indeed I'd say a great advantage of the library is that most things are just libraries (of normal classes and functions) that you can use, rather than frameworks or patterns.
Do you have fellow java programmers in your company? If so, ask them for their Nexus/Artifactory instance, sbt can use that to download packages.
No, we don't generally develop code for JVM, I wanted to introduce Scala for the next project, but to do that I need to figure out if we will be able to develop in offline mode.
Maven has a `dependency:go-offline` task that you can use to download all of a given project's dependencies in preparation for working offline; SBT might have something similar?
So, you're going to have a few issues. First, of course, is just getting the software you need - scala, SBT, etc. But, more importantly, unless you plan to *only* use the standard library, then you're going to need a way to get 3rd party packages into your code, which is what maven/sbt/gradle/etc. (basically, the build tools) handle for you. In most companies with strict policies such as this, there is an in-house repository (a la Nexus/Artifactory as /u/Scf37 mentions) which contains approved versions of 3rd party libraries Otherwise, you're going to be entirely on your own and miss out on one of the benefits of being in the Java ecosystem. What languages are generally used at your company? How do they deal with 3rd party libraries, etc?
*usual* approach is to set up Sonatype Nexus instance in private network so it can work as caching proxy for repositories in the Internet as well as storage for your artifacts. If you can't arrange that, I believe running sbt on machine with Internet connection and then copying `~/.ivy2` folder to your machine is the only option.
Effective Java (3rd Edition?) - the newest edition is relatively up-to-date with lots of the recent changes such as the "streams" API. It might not be the exact thing you're looking for, but if there's a particular bit of the language you're learning about it's a good collection of technical issues/smells/problems and solutions. 
C/C++, C#, Python (we just download libraries by hand from PIP, Nuget or other sites and install them from local file system).
You shouldn't need sbt (or maven) and half the internet they want to download Most basic thing would be to bring a JDK, and download Scala https://www.scala-lang.org/download/ (at the bottom) then just use a text editor and scalac from the command line. Old school java development. You could also try using a JDK, IntelliJ Community, and the Scala plugin https://plugins.jetbrains.com/plugin/1347-scala which you can download and just import into IntelliJ I'd try it at home, just download then disconnect from the internet and see how the setup goes. Might need to fiddle with some stuff but one of these should generally work. If you want to add 3rd party libraries just download jars and stick em in your classpath.
\+1 for Effective Java. I wouldn't say it's the best book for scala developers specifically and I don't have a recommendation for that situation but I do think it's the best java book period. It's a book that engineers of all levels can pick up and learn something. As long as you can read and understand basic Java code then you'll be able to pick up this book and gain a lot through it. The writing style is one of my favorites for a technical book. 
Lunatech | Scala Developer | Amsterdam, The Netherlands | ONSITE | Full Time &amp;#x200B; We’re looking for a passionate developer who is always looking for personal growth and the best for our customers. Because we don’t have managers to draw charts or go wild in Excel, we expect that you pick up responsibility of leading projects to successful completion. &amp;#x200B; [**employment@lunatech.com**](mailto:employment+website@lunatech.com) **or** [**https://lunatech.workable.com/jobs/301491**](https://lunatech.workable.com/jobs/301491)
A service that turns a git repo into a wiki. It's very alpha right now. https://webdocs.io
Not too sure about SBT &amp; Scala compiler, but for libraries, you can probably download the JARs manually and instruct SBT to include that folder. http://flummox-engineering.blogspot.com/2014/06/sbt-use-jar-file-for-librarydependencies.html Another way to do it:https://stackoverflow.com/a/40414593
I doubt that you need a book. Taking into account that you know the theory you can just start using cats. Go read few articles on the internet about tagless final, take a look at http4s, doobie and fs2 - you should be good after that. &amp;#x200B;
Lucid Software | Software Engineer in Application, Dev Tools, or Site Reliability Engineering | Salt Lake City, UT, USA | Onsite | Full Time Lucid Software is the creators of Lucidchart and Lucidpress - world-class web applications that push the boundaries of what is possible in your browser. The entire application back end is written in Scala. The frontend stack uses TypeScript, Angular, WebGL, and the Google Closure Compiler. The build system is Bazel. We are hosted in AWS. We are a ~400 person company outside of Salt Lake City Utah, that is growing like mad. We are looking for - Application engineers to help build applications that users love. - Development tools engineers to help build a development experience that engineers love. - Site reliability engineers to help build a scalable, reliable, and performance production system. More information and the application can be found here - [Application Engineer](https://www.golucid.co/careers/f9cb83be-e016-4be5-91dc-598e17e8d04c?team=Engineering) - [Development Tools Engineer](https://www.golucid.co/careers/c8de139c-6da7-44dc-a4af-95052aa2a086?team=Engineering) - [Site Reliability Engineer](https://www.golucid.co/careers/be16cacc-4cd3-4be0-9ec0-070646f9ec69?team=Engineering) Please DM me if you would like to chat about any of the positions. Please mention this Reddit thread if you apply :D
**Hivemind-Technologies | Scala Developer | Cologne, Germany | ONSITE or REMOTE (EU/EEA only)** **Contact:** [jobs@hivemindtechnologies.com](mailto:jobs@hivemindtechnologies.com) You can also write me a private message if you have any questions. :) **About us:** Hivemind Technologies specializes in the development of high-performance and scalable big data systems. We help our customers gain more from their data and advise them how to use state-of-the-art technology to process, store and analyse their data. We are a small team of skilled Scala developers which like to write pure functional and clean Scala code. We are working mostly fullstack with little frontend but quite a bit of DevOps. Our stack includes a lot of OSS, including Jenkins, Spark, git, kafka, elasticsearch and libraries including cats, circe, monix, fs2, akka streams, http4s and shapeless. We make heavily use of AWS and cloud computing. It's nice to work with us, we have flexible working times and embrace remote work. We also sponsor visits to conferences and are offering free coffee and fruits in the office. Remote work is possible for all EU / EEA nationals or when having permission for EU / EEA and live in one of these timezones: GMT, CET or EET. (No Visa sponsorship) Also visit **our website** for a more detailed job description: [http://www.hivemindtechnologies.com/jobs](http://www.hivemindtechnologies.com/jobs)
FYI you can also download jars from maven and do the same. although its a bit painful.
Perhaps not helpful but perhaps you could develop "online", as in, build the code outside the secure network then build the jar/docker/rpm whatever and publish that internally.
Hmmm.... &gt; scala&gt; List("a", "b", "c").toSet + "d" res0: scala.collection.immutable.Set[String] = Set(a, b, c, d)
Ha, great write-up, and good advice about turning on the built-in adapted args lint. In fact I’d go further and say just turn on most of the built-in lints, they’ll save you from a lot of these kinds of quirks. Rob Norris has the canonical listing up on his blog.
I have been working on improving performance of the new mutable/immutable [BitSets](https://github.com/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+author%3Ajoshlemer+archived%3Afalse+BitSet), and the new immutable [ListMaps](https://github.com/scala/scala/pull/7509) and [VectorMaps](https://github.com/scala/scala/pull/7508)
Thanks ! Thats awesome ! 
You are missing `()`. It is just bad design of scala collections: someone thought that Set[T].apply(T) is great alias for `contains(T)` 
you're missing a (). Try toSet() ;)
Maven uses a local repository "cache." Once you have downloaded things into that, you can continue to use Maven in offline mode. I would assume SBT has a similar capability. Storing the dependencies purely locally may be simpler for you than maintaining a Nexus instance. No matter what, you will need to be online sometimes. You need to get those build dependencies from somewhere, and occasionally you will want to add new libraries or update to newer versions of ones you already have. As far as IDEs go, I would go with IntelliJ first, then Eclipse. 
To see it in action the toSet needs to be called I.e List("a", "b", "c").toSet() + "d".
Ahh, thanks.
If it’s already a C# shop then IMHO it doesn’t make sense to go with Scala and JVM. Stick with C#.
Depends on a lot of factors, like whether you prefer compile-time or runtime DI, which frameworks you may be working with, etc. In Play framework, for example, Guice is the default. 
Guice is the default, but there are many DI implementations in Play https://github.com/playframework/play-scala-macwire-di-example https://github.com/playframework/play-java-dagger2-example https://github.com/playframework/play-scala-compile-di-example/ and there's a discussion about Macwire in [DI in Scala](https://di-in-scala.github.io/).
I was certain he would say something about implicit conversion. The appearance of `+ "d"` forces the compiler to deduce the type of the left operand of +. The compiler knows "d" is type string. 
I haven't felt the need for one in Scala like I did with Java. They provide value in managing object lifetimes, but when code is more functional there isn't that state to manage. Aside from that, it's just instiating some objects, which I'd rather do in code. It might be worth talking about what you want a DI library to do for you.
I've been using this technique for a while now and have decided that I like it better than using a 3rd party library: http://www.michaelpollmeier.com/2014/06/29/simple-dependency-injection-scala
I prefer Java's built-in DI: the `new` keyword. 
**"None"** is my personal preference. Pass in dependencies directly, to the function or constructor. Or better, if you attempt to achieve pure functions (referential transparency), the number of dependencies you'll have should significantly decrease.
None. I'll either use good old `new` with constructor dependency injection or the cake pattern. I imagine if you're using some inversion of control framework then you might need a DI library, but I also suspect that in that case it comes with a specific framework already chosen, so use that.
None of the above, just constructors and implicit parameters.
We used the cake pattern before, but felt it was overly verbose and cumbersome to maintain. I recommend https://github.com/adamw/macwire as a great compromise between simplicity and convenience.
Let me say it like this : Ouch. Are you fluent in Java? Packages are the same. There are certainly talks on YouTube for the basic syntax. I Iike the one by Venkat Submaranian. Look for Scala Indian Guy. A small web framework is Scalatra. There are others. I have no idea how you might learn the collections library in that time frame. Google Scala collections probably. 
I do talk about implicit conversions, see the any2stringadd section ;)
I also favor to use None. Currently we use Guice but are getting rid of it. Run-time injection circumvents the advantages of a strong static type system, and I think it actually makes code a lot less clear.
Good luck. Maybe you should have started earlier. 
Look into reactive streams with back pressure.
My friend, would you kindly tell me, how would you feel about not having a free day from October to now, because that has been pretty much me. I would do this months ago if I could, but I couldn't and so im here with the best and worst of my class trying to survive this thing we call uni. 
It's fixed now.
I've tried implicit passing, inheritance based, and guice based DI _principled_ guice usage has been by far the most enjoyable! Obviously people have different preferences. Small tips that I've picked up working with guice: - inject everything, not just a large "config" object - make everything Singleton as default unless it shouldn't be - limit your application to only 1 injector.instance[MyApp] call
The fastest way to learn as much as you can about Scala is probably this video: https://www.youtube.com/watch?v=DzFt0YkZo8M I didn't watch the entire video, but it seems it'll get you up to speed with basic stuff. You can write a port scanner in Scala (that's what I did back in school when I was in a similar situation). That would be quick and easy thing to do, I guess.
The coursera class on Scala is pretty good too. And this extension help you go through videos faster: [https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk](https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk)
We've all been at uni and enjoyed the drinking and partying. The work load is light and the preasure low. Take it from someone who dropped out the first time and had to go back at 27 that the only reason for starting an assignment w4 hours before is laziness. Not that I judge you, I did the same first time around but you can't kid a kidder.
Look at F#, maybe?
You are right, but the fact that most things are just libraries is also a pattern, I mean way of solving well defined problems, and which is possible because Scala is how it is.
Thanks
Thanks, I will definitely read it.
F[_] 
Why akka logging use default dispatcher by default? Most of logs backend are blocking, this cause default dispatcher thread to have a little block for every log.
I worked with Scala for building APIs and Apache Spark jobs and I must admit this technique is really awesome. It's so simple that I feel kind of silly that I didn't think of it. For the APIs I was using Play which already has Guice. But for the Spark jobs we were using reflection to create objects and we were storing the objects in a global application context. I hated that :) Thanks for sharing.
What kind of college did you go to? I want in. Today, I got 2 final programming projects due (finish the other one yesterday). Final exam last Sunday, another one last Friday. Before that i had 2 assignments and 4 quizzes for the end of class last week. Not to mentioned the back to back midterms I was in the week before that one. God I miss the taste of barfing out too much vodka compared to this. Seriously, where did you go bcs I want to transfer. 
Scala in 24h? Impossible, go rest ;)
You could also use F-bounded polymorphism: ```scala sealed trait Seat[S &lt;: Seat[S]] final case class NormalSeat(row: Int, column: Int) extends Seat[NormalSeat] final case class VipTribune(tribuneId: String) extends Seat[VipTribune] sealed trait Standing extends Seat[Standing] final case object Standing extends Standing final case class Ticket[S &lt;: Seat[S]]( verificationCode: String, price: Int, seat: S ) case class Reservation[S &lt;: Seat[S]]( userTicket: Ticket[S], guestTicket: Ticket[S] ) ``` See https://tpolecat.github.io/2015/04/29/f-bounds.html and http://logji.blogspot.com/2012/11/f-bounded-type-polymorphism-give-up-now.html 
Spring
[Akka Streams](https://doc.akka.io/docs/akka/2.5/stream/) has a bit of a learning curve, but once you get going they're very powerful.
You have weeks for all this. The work you need to do for assignments and study is less than the hours you would need to put into a full time job. When your next semseter starts try this. Get up at 7am and start your work at 9am. Interleave your two or three hours of lectures with assignment coding. Finish your day at 6pm. I don't think you'll struggle to complete things on time this way. Going to uni was awesome. My first uni was in the uk my second the usa. But in terms of hours I wish my real jobs in my life had had the uni hours.
Already did, 10 am to 8 pm every day, every week since the start of the semester till now. Look, I get it, everybody assumed that for this situation the students must be lazy, but it is really not. Especially for where I come from (SEA), there we study from 6 am to 4 pm everyday and that's just middle school. You guys may have it easy for your schools, which is why I wish I went there instead, but we do not have that here. Hell, I'm already one of the mid to top students in my class and I'm barely getting things done in time. That's balancing school with a relationship, part time job and boxing. Honestly, some times I don't know how I survives half the stuffs I do just for fun. 
Removing because there's already a who's hiring thread on the front page right now
I am so glad I am not alone in thinking like that. Thank you, Daniel W.
Thanks, you wouldn't happen to know where I can learn to make a port scanner in Scala? I found how to make one in Java, but nothing for Scala yet. 
Tangent: I never really understood what the big hub-bub with dependency injection is. I've probably not built a system which needed such fine-grained control about what gets put into where and how and why. What's insufficient about plain-old constructor-based injection? Does it matter if it's Scala or some other language? (i.e. is there something unique to Scala that makes constructor-based injection more favorable?)
Reminds me a lot of 'Simple made Easy' talk of Rich Hickey
Logging is usually assumed to be quick and cheap. Local disk I/O, for example, is "blocking" but reliably low-latency to the extent that it's usually more efficient to do directly than to introduce a task switch by doing it async.
It is frustrating that seemingly simple static invariants aren't easy to define without resorting to repetition or advanced type features. It seems like a "strict subclass" type bound would help here, and is essentially what the Shapeless approach does. Is this viable? Even if it is, it concerns me that it would still be easy to make the type-level version of the fencepost error by accidentally using \`&lt;:\`. Is this a problem in Haskell or ML type systems? Without \`Seat\` being an independent type unto itself, it seems like it might not be an issue. Lastly, I feel I run into this situation when trying to model state transitions, but I'm unsure whether a strict subclass type bound would be the answer in that situation. Haven't thought it through. I just wish there were an easy answer.
Thanks, this piece was an excellent read.
Thans to everyone for your replies.
Guice for work, None for home projects. Scala features make configuration much less painful
It's purely convenience. If you have a large program with multiple entrypoints, adding a new dependency to a component can require maintaining a dozen places where that constructor is called. If that new dependency itself has dependencies, that can become a lot of tedious work pretty quickly. Often, that work can cause merge conflicts in the entrypoints even when the real changes are isolated. DI, the pattern, is important for a host of reasons. DI, the frameworks, provide tools to ease the maintaince burden of following that pattern. Personally, I feel that frameworks mask what is really going on and shouldn't be used until the pattern is well established and the program is large enough for the convenience to justify the complexity.
I'm not sure an article making an example of simplifying server side code by writing HTML rendering instead of json rendering has the same sense of simplicity I do.
Personally, I'm partial to macwire. All the goodness of constructor based DI with a lot less boilerplate and a lot less compiler breakage when adding new deps.
well, it depends. SPA requires proficient API and frontent devs.
And HTML generation also requires frontend devs provicient in server-side HTML generation. &gt; But imagine the unthinkable for a minute: What if we did server-side rendering of HTML instead? There wouldn’t be any need to design an evolvable JSON API, which, unlike simply exposing your domain model, is not trivial, and there would definitely be no need for defining any JSON codecs. And that’s just what you get on the server side. certainly does sound like it implies that server-side HTML rendering and either cobbling that together on the client side, somehow, or doing a full render for every DOM change, is a lot less complex than designing an versioning an evolvable JSON API. I don't think that's true.
Like /u/acjohnson55 suggested, the complexity is because of subtyping of the seat types. The post doesn't actually examine why it needs this subtyping. Why not make the seat types completely unrelated? Then we don't need anything extra, type parameters work nicely to guarantee the seat types must be the same. Nevertheless, let's assume we really need subtyping for the seat types, the most obvious reason being that we want exhaustive pattern matching. Since /u/skaalf suggested F-bounded polymorphism, I'll suggest typeclasses instead, since the former should always be converted to the latter: case class Reservation[SeatType: SeatClass]( userTicket: Ticket[SeatType], guestTicket: Ticket[SeatType] ) case class Ticket[SeatType: SeatClass](verificationCode: String, price: Int, seat: SeatType) sealed trait SeatClass[A] sealed trait Seat case class NormalSeat(row: Int, column: Int) extends Seat object NormalSeat { implicit val seatClass: SeatClass[NormalSeat] = null } case class VipTribune(tribuneId: String) extends Seat object VipTribune { implicit val seatClass: SeatClass[VipTribune] = null } case object Standing extends Seat { implicit val seatClass: SeatClass[Standing.type] = null } // Good: Reservation(Ticket("1", 1, Standing), Ticket("2", 1, Standing)) // Type error: Reservation(Ticket("1", 1, Standing), Ticket("2", 1, NormalSeat(1, 1))) The trick here is to provide `SeatClass` (the typeclass) instances for the seat subtypes, but not for the `Seat` trait itself, and a typeclass bound instead of a subtyping constraint for the seat type parameter. Of course, someone can just do `implicit val seatClass: SeatClass[Seat] = null`, but there's only so much you can do in the presence of 'bottom'...
Against all odds, I actually just finish the project. Thanks for the suggestions, they caught me up fast. 
&gt; A great technique for documenting these decisions for future reference is to write Architecture Decision Records (ADR). Great process, will be applying this for past decisions and the many small ones that continually come up.
Just use the node API. Scala-Js compiles to Js. Node is a js engine which runs Js (targettable by Scala.js directly) and some other API that interfaces with system (not targettable by Scala.js directly, needs you to call node directly using Js interop)
Looks like you're busy. Honestly- don't waste a single second and stop working on this project as soon as you see this message. Focus on rest of your work.
There is Vaadin and JSF for easy development.
The Reader monad.
I think he misrepresents the reasons why it makes sense to abstract over the effect type. He writes: &gt; So the main reason that remains for people to abstract over the effect type in application development is the desire to make their effectful code unit-testable. &gt; How does this work? You have a function that mixes logic as well as effects, and in your production code, you execute it in your favourite effect monad, talking to a real external system. In unit tests, on the other hand, you execute it in the Id monad, using hard-coded test data. This is false. First of all, when your function returns an `IO[A]` you would usually want to perform an effect in that function (e.g., delay a synchronous side-effect). So, `def f1(s: String): IO[Int]` would turn into `def f2[F[_] : Sync](s: String): F[Int]`. At this point you cannot test the function with `F = Id`, because `Id` does not have a `Sync` instance. So why abstract over the effect then? One important reason is that it is better to program to an interface, not an implementation. When I look at `f1`'s signature above, I can tell that the function might execute an effect, but I have no idea which one. It could delay a synchronous side-effect, an asynchronous side-effect, start another thread, etc. With `f2` however, I have the guarantee that only functions defined in `Sync` and its superclasses will be called. I.e., `Sync` is the interface that `F` implements. This is the same reason for which a function should take its values by interface type and not by the concrete class type (e.g., the function would take an `ExecutionContext` and not a `BalancingDispatcher` if all it wants to do is to call `ExecutionContext#execute`). Totally agree on the JSON/DTO bit though. ;-D
&gt;So why abstract over the effect then? One important reason is that it is better to program to an interface, not an implementation. Thanks for pointing this out. Or, in other words: [http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) [https://en.wikipedia.org/wiki/Rule\_of\_least\_power](https://en.wikipedia.org/wiki/Rule_of_least_power) &amp;#x200B; I think Daniel (the OP) is still not completely wrong. Using final tagless or free monads has a huge overhead. On the other hand, I think that the harder tests are to right, the less tests are written. And this is dramatically underestimated. This is similar to FP and referential transparency. If you tell people that FP/RT help with refactoring, to them that sounds like a small thing. However, the hurdle to do a small refactoring while developing on a feature must be as low as possible. Otherwise, people just don't refactor because it takes time, delays their features, gives them little immediate benefit but high immediate risk of failure, problems and blame if they introduce a bug through their refactoring. This, together with the broken windows theory, is heavily underestimated, unfortunately.
wow thanks this is refreshing read. The problem is that people get hyped so much about new techno / techniques that they want to apply it everywhere. &amp;#x200B; I think, part of the problem is that some speakers that evangelize those techniques (and I very much thank them for doing so) do not clearly state when the technique / techno should be used, instead they just go with: "This is my latest shiny toy project, whick works fine in production. This is awesome, just use it !" looking for some traction for their own project. They don't put disclaimers such as "warning, this is a very advanced practice, you can do it, but mind the risks / costs for your project" &amp;#x200B; Overall, I find blatant the lack of pragmatism among the community. We should always ask ourselves first: "which problem am I trying to tackle ?" rather than "which problem can I solve using this technique / techno". &amp;#x200B; Don't get me wrong, I really enjoy very advanced techniques such as Free / Tagless final but there is a fine line liking them and using them at work. &amp;#x200B; PS: I HATE automatic derivation for json encoders / decoders aka. change one variable name, break all your clients. No one seems to realise how dangerous it is, for what in the end ? Taking 1 or 2 minutes to writes them by yourself. &amp;#x200B; PS 2: I love Scala, thank you everyone for making this language / community great
Not really cross-compilable. From what I found on the internet, java.io is supposedly supported (even though it is in the java namespace), I just can't get it to work completely (it complains of missing functions).
ReaQta | Senior Scala Developer | Amsterdam, The Netherlands | ONSITE | Full Time &amp;#x200B; We are looking for a scala developer to join our backend team. At the backend we are using scala, cassandra, elasticsearch, akka(http,streams,actors,persistence, cluster), http4s, doobie, fs2, cats. Codebase is new, all Scala. No java at all. &amp;#x200B; Our solution is an endpoint threat response platform. You can find more on [https://reaqta.com/](https://reaqta.com/) or drop in our office in the center of Amsterdam for a demo. &amp;#x200B; [info@reaqta.com](mailto:info@reaqta.com) or [https://reaqta.com/contact-us/](https://reaqta.com/contact-us/) &amp;#x200B;
I don't find the argument for using this instead of protocol buffers convincing. In either case you have a canonical representation of the serialized thing; if you use protocol buffers you write a dedicated language specifically designed to make forward compatibility easy and where it's simply impossible to accidentally write a nonserializable type, which seems much nicer than writing `case class`es in full Scala and generating a protobuf-like representation from those. I don't see how this approach is supposed to be "more lightweight" or "simpler". What am I missing?
Agreed, 'use the right tool for the job' applies here.
It generates the representation for you. You don’t need to learn the intermediate language. You write Scala a
&gt; It generates the representation for you. So does protobuf? Either you write the definitions once in protobuf and generate the scala and other language representations, or you write them once in scala and generate the other representations (via an intermediate representation); I don't see one as any simpler or easier or more lightweight than the other. &gt; You don’t need to learn the intermediate language. I guess, but the protobuf language is very simple (since it's just inert datatype definitions). I think it's worth having a representation that's not just "scala case classes" because the things you can do in a serializable datatype are so much more limited than what you can do in a scala case class, shrug. &gt; For many apps protocol buffers is a layer of complexity you may not need or want given the scope. Where's the complexity though? We still have three representations (scala representation, generic representation whether that's `.proto` or `DeclF`, and javascript representation). We still have the same amount of serializing/rendering. What's the difference?
Unfortunately not. But I suppose it's already too late anyway.
Very relevant to this discussion: Daniel Westheide's excellent recent post, [The Complexity Trap](https://danielwestheide.com/blog/2018/12/07/the-complexity-trap.html). He examines how in programming we often jump to technological solutions instead of properly thinking about the problem or even without asking _why_ a particular tech is the best solution.
[Buzz Killington said it best.](https://youtu.be/HgY-xvyIlzY?t=37)
OK, but to Daniel's general point–why finally tagless style? Why not program to an interface in the traditional way, e.g. `def f2(sync: Sync, s: String): Int`, given an actual interface e.g. `trait Sync { def apply[A](a: A): A }`? Your reasoning: &gt; With `f2` however, I have the guarantee that only functions defined in `Sync` and its superclasses will be called. Doesn't really hold up, you don't really have any such guarantees in Scala. What you have is an _indicator_ to the reader that this is what's _meant_ to happen, which can be achieved using my above example too.
Suppose I have a class class MyClass(val s: String){} How can I make it so that when I create a new instance of MyClass, s is only set to the first 5 characters of the input to the constructor? E.g. someone says "new MyClass("Scala Language")", I want the object to actually be "MyClass("Scala")".
Nope. Still bad. The whole page is covered by his bio.
Your version cannot suspend side-effects, i.e., it's not referentially transparent, so it's really a different thing. And sure, you can always lie and throw an exception, use reflection or whatever, but you don't do that in functional programming. Objects can also be null in Scala, but I never check if they are (only when calling Java code), and never had a problem with NPEs, because everyone agrees that we don't work with nulls in Scala. 
 class MyClass(value: String) { val s = value.take(5) } &amp;#x200B;
If anyone else has run into this, I was able to work around this problem by using a traditional main method rather than extending the App trait. Putting the code in a method within the object and then simply calling it, while extending the App trait, also avoids this bug. I updated the issue on github, it looks like they have improved the segfault message a lot on RC10 where it was able to show where the code crashed (within App.scala)
I like this. Monad transformers are painful.
For me, having used this in a project and as a Scala dev, I write the Scala case classes and don't need another language (albeit a simple one) involved in the build process. If I need to change the protocol in some way I can do it on the server without a generation step until its needed for the front end. As Pere said, "We are writing code in Scala and obtaining the front-end representation automatically, so any change to our Scala ADT is automatically picked up by the type-checker for our front-end codebase."
In the beginning he complains about json derivation not being an external API, but in the end he admits that usually you're just doing it for an SPA anyway and you don't actually need an API at all. Exactly, which is why auto derivation is perfect for that scenario (especially with autowire or Sloth). 
Yes! Implicit arguments make this easy.
So my original question still remains: why finally-tagless style? Why abstract over the effect type? And a new question: why do we need to do pure, referentially-transparent FP as an absolute? I get why it's useful in general, but why is that the blessed architectural style instead of say Daniel's example of a layered architecture?
I think I explained above why I think it makes sense to abstract over the effect type. As for why FP in general, that has been discussed elsewhere. I recommend /u/systemfw's posts here: https://www.reddit.com/r/scala/comments/8ygjcq/can_someone_explain_to_me_the_benefits_of_io/
But isn't "value" going to be an instance variable that is stored with the object? I don't want it stored.
I think I'd use the companion object for this instead of making the class do the work itself. object MyClass { def apply(s: String): MyClass = new MyClass(s take 5) } Then you can use `MyClass("Scala Language")` to get the truncating behavior and `new MyClass(...)` to get non truncating behavior. If you make the constructor private with `class MyClass private (...)` then the companion object will be the only way to construct an instance and you will always get truncation.
Yes, you explained it in terms of programming to an interface, not an implementation, but didn't explain _why_ we want to do that universally? Is that applicable in every possible situation? What if I don't need to swap out another implementation? What if I want to use an interface that's not monadic, but applicative, in some cases? The OP already asks all this, btw. I just a had a look at https://www.reddit.com/r/scala/comments/8ygjcq/can_someone_explain_to_me_the_benefits_of_io/e2jfrg8/ , it's very well-written. My final thought on this would be that–though the benefits look great–we should also ask: what are the costs of a wholesale move to this IO-based, fully referentially-transparent architecture? Could we mitigate the costs by separating the logic layer into a purely functional data layer, and a separate effectful service layer? Again, OP asks this exact question.
Could you explain? If I don't have an API, why I am doing JSON derivation?
Nope! It's a subtle thing. Putting "val" in front of a class argument makes it an instance value. Removing it makes it just an argument. Maybe naming it "value" is confusing. &amp;#x200B; class MyClass(toTruncate: String) { val s = toTruncate.take(5) } &amp;#x200B;
&gt; Yes, you explained it in terms of programming to an interface, not an implementation, but didn't explain why we want to do that universally? Is that applicable in every possible situation? This is a well-known principle in software engineering. I don't think I need to explain it here, when it has been explained elsewhere before. &gt; What if I don't need to swap out another implementation? I don't know, I never said anything about different implementations. &gt; What if I want to use an interface that's not monadic, but applicative, in some cases? That is exactly what this style enables. You write def f[F[_] : Applicative](s: String): F[Int]) = ... Done. No way to `flatMap` in `f`. &gt; what are the costs of a wholesale move to this IO-based, fully referentially-transparent architecture? It's good to ask this question, but it was not what I wanted to address. I wanted to address what I felt was a misrepresentation of the reasons for the choice to abstract over the effect type. There are definitely downsides to doing this, and there are also downsides to pure FP in general, whether you abstract over the effect type or not. &gt; Could we mitigate the costs by separating the logic layer into a purely functional data layer, and a separate effectful service layer? Could we? I don't know, I think we are trying! I'm not sure how separate you can keep those layers. Usually it's a back and forth between effectful code and pure business logic. You read some value from the database, perform some logic, depending on some criterion read something else etc. I would assume the desire to keep the business logic pure would eventually push you into the direction of writing something like a free monad. 
&gt; This is a well-known principle in software engineering. There are lots of well-known principles in software engineering, including YAGNI :-) &gt; I never said anything about different implementations. That's interesting, because you're defaulting to an approach–programming to the interface–which is explicitly designed for being able to swap out implementations. If you don't care about doing that, you have no reason to program to an interface. &gt; I wanted to address what I felt was a misrepresentation of the reasons for the choice to abstract over the effect type. I don't believe that's a misrepresentation. When people talk about the benefits, they talk about testing as the biggest example. No one talks about 'programming to the interface' as a benefit because that's not actually a concrete benefit, it's a technique to realize other benefits ... like pluggability of implementations.
Ahhhh! That's so awesome!
&gt;Cofunctor I seriously hope you are meming. `Cofunctor` is a common joke among category theorists. Duality (Co-things) is not the same as Contravariance. Please redo your sections on variance and the Yoneda lemma as well. You missed vital setup when you were copy-pasting from the wikipedia page.
I would call it an internal API, it's an implementation detail of how the React etc. frontend talks to the backend. 
What about if I need to do something in the constructor that needs a temporary variable? How do I do that?
If it's something simple I would put the temporary stuff inside of its own block. For instance, &amp;#x200B; class MyClass(toTruncate: String) { val s = { val l = toTruncate.length.toString toTruncate.take(5) + l } } &amp;#x200B;
Makes sense. But what if I wanted to use a temporary variable in multiple instance variables?
That's quibbling on semantics quite a bit :-) an internal API shouldn't require encoding and decoding data across a network.
Radix Labs / 1. scala programmer and 2. Marketing Director / ONSITE (Boston) / NO REMOTE / VISA SPONSOR / ENGINEER / 90-150k / EQUITY\*\* 1. Full Stack Scala Programmer At Radix, we build a compiler and operating system for the computer architecture expressed by a biology lab. We do this to allow biologists to disseminate their work in a runnable form to other labs. We're currently hiring Scala engineers to help us with this. Our entire stack is in scala, from our compiler to our runtime implementation to our device drivers. We interface with sensors and lab equipment to provide abstractions that allow biologists to write their lab protocols as formal programs, constraining the acceptable environmental conditions in their protocol, which we check at compile time to provide error logs and insight into why protocols may not reproduce. We've just cleared our Seed round, with investors like MIT's The Engine, Y Combinator, and Firstminute capital. We're a rapidly growing team that could use a person like you. Even if you don't meet all of our requirements, we're happy to talk and see if we can work together. &amp;#x200B; \### Required experience \- Scala/Matryoska/Akka/Cats/Scala.js \- Functional programming \- Has built DSL's and DSL compilers before \- Formal logic \- Scheduler and malloc implementation knowledge \- High-performance algorithms (Only-once, streaming, distributed, approximation) - Git in a monorepo \### Nice to have \- An interest in biology / biology lab exposure \- An interest in driver development \- ETL with Kafka experience \- Spark \- Transactional systems implementation expereince \- Declarative language cost models and planners \- Graphical programming language design \- Design thinking &amp;#x200B; 2. Marketing Director We are Radix Labs, and we build an operating system for biology labs. Biologists describe the mechanistic work to be done to perform an experiment in a "lab protocol" - often natural language on paper. We build a declarative graphical/textual programming language that looks a lot like a lab protocol to allow biologists to transparently use automation, log sensor data, and manage the workflow of a protocol. This abstraction allows biologists to "download chemicals" in the same way CAD software and 3D printers allow us to "download shapes". We're looking for a salesperson with experience in selling deeply technical products that provide business value, such as a database or log parsers. Experience in Enterprise SaaS is welcome, with a keen interest in research biology and the pharmaceutical industry. This person would manage our sales process to entities such as research labs in universities, small biotech companies, large biotech companies, and big pharma. We would rely on them to interpret user needs and deliver prioritized lists of features requested by customers for our technical team to build. Furthermore, this person would assist us in Market Size, Go-To-Market, and business development questions that we field from potential investors. &amp;#x200B; Please contact lucas@radix.bio with your resume if interested. &amp;#x200B; &amp;#x200B;
What you want is memoize, you can find it in scalaz, :[https://scalaz.github.io/scalaz/scalaz-2.9.1-6.0.2/doc.sxr/scalaz/Memo.scala.html](https://scalaz.github.io/scalaz/scalaz-2.9.1-6.0.2/doc.sxr/scalaz/Memo.scala.html) which should give you an idea of how to implement your own. &amp;#x200B; Here's an example of how to use it: [http://eed3si9n.com/learning-scalaz/Memo.html](http://eed3si9n.com/learning-scalaz/Memo.html) &amp;#x200B;
Implementing business logic in a case class seems like a deal breaker to me. 
Is there support for emacs?
At 34:19, for the opaque types example where he shows an implementation of immutable array, there's this line: def apply[A: ClassTag](xs: A*) = initialize(Array(xs: _*)) Can someone shed some light on what `ClassTag`, `A*`, and `initialize` are in this context? I've never seen an asterisk behind a type parameter, there is no `initialize` in scope that I can see, and I'm just unfamiliar with `ClassTag` though I believe it has something to do with recovering type information erased at runtime? 
There's a couple options. You can declare two values at the same time, or use another constructor. class MyClass(toTruncate: String) { val (s, t) = (toTruncate.take(5), toTruncate.takeRight(5)) } class MyClass private (val s: String, val t: String) { private def this(toTruncate: String) { this(toTruncate.take(5), toTruncate.takeRight(5)) } }
How else can the part running in the user's browser talk to the part running on my server?
Its called varargs. Similar to java's repeated arguments. `public void foo (int... args)`
hm, I don't think memoization would help me here. &amp;#x200B; I do not want to memoize results of "crunch" function. I want to memoize single value (val y). But I want to do this in such way so that this value is local to my function. I do not want to put it into outer scope.
D'oh. I don't know how, but I misread it the first time and thought it was on the type *parameter* itself, a la `[A*]`.
By exposing a network API ... aka an external API.
Methods have no persistent storage, the only places you can put the closure are in a named variable in each class instance or in an outside object (like the companion).
This is the best quick intro explanation to spark I've ever seen, although it does rely on some prerequisite mapreduce knowledge. I wish I saw this when I didn't know what spark was
I don't see how that story is relevant at all, but the broader point is you're using words with a different meaning than me. You seem to be using "internal" and "external" to mean "secret" and "public" (respectively). However, when I used them I had meant "for use within this codebase" and "for use outside this codebase" (respectively). This should be obvious from the context, namely, whether it is bad to make the json a direct reflection of your internal models (with the same sense of "internal"). Again, my argument is that if the json is only for my SPA, which my case classes are cross-compiled to, and the json is just to send that case class across the wire, then auto derivation is great. 
OK, so your context is an app where you control both the backend and the frontend? Then sure, it makes sense to auto-derive or even just use an 'isomorphic stack' like NodeJS+whatever client framework. But if your service is being consumed by frontends from other teams? Then I hope you're not tying together your domain models and your frontend APIs like that.
The simplest two options are to make `y` a private instance member or a private static member (in the companion object): trait trait MyTrait { def crunch(s: String): String } class MyClass extends MyTrait { def crunch(s: String): String = s"$y $s" // Instance member private val y = "somestring" } Alternatively, static member: object MyClass { private val y = "somestring" } ...which you'd import into `MyClass` for use in `crunch`. Whether to go with instance or static depends on your needs.
To be fair, the generation step should be automated away by your build process, e.g. https://github.com/sbt/sbt-protobuf
Go back and read my original comment. I think you misunderstood something. Here is the part in the end of the article I'm referring to: &gt; But the maximum zoop trap also applies to our issue with the JSON codecs. If we do a root cause analysis here, at some point, we may ask ourselves why we have a JSON API in the first place. Often, the sole consumer of such an API is a single page application (SPA) running in the browser. Again, my point is once he conceded that the SPA is the sole consumer -- to the extent that his next move is to suggest moving to server-side rendering and removing the API -- then his earlier argument against auto derivation falls apart. Of course it's still a valid argument in other scenarios but he made it sound like auto derivation is always bad.
this is exactly what I want to avoid. In your code function body and val-y are separated. Which is not a problem when there is only one function in this trait and class. But becomes a readability problem when there are dozens such functions and values. And in my initial post I showed how it can be solved: val closureVal = { val y = "somestring" (s: String) =&gt; { s"$y $s" } } as you see here value-y is lexically enclosed into closure-valued function. which makes it readable. If I don't need to override then I'm happy with this code -- I can use this closure just as any other method. Unfortunately I failed to make it override def from inherited Trait.
Your requirements (1) and (2) are mutually exclusive. Also, if you have dozens of functions and values like this in a single class, your main problem is the design of the class, not this specific issue of one-time initialization.
Sure, you’re right in that sense. But if you have a SPA, soon enough you’ll want to do SSR enough to get a performance boost and now you’re back to square one :-)
She did a video on mapreduce as well, that should be sufficient prerequisite for anyone interested.
&gt; If I need to change the protocol in some way I can do it on the server without a generation step until its needed for the front end. I guess, but you need to build your Scala to use it, so it seems like a distinction without difference - either way changes are picked up when you compile your Scala project (even if that's IDE compile-on-save, protobuf support is common enough). And surely there's no meaning in making a change that you can't use from the frontend?
Classtags are necessary for arrays, jvm quirk and not necessary for understanding 
That timeline sounds absurd to me. 
 enum Option[+T] { case Some(x: T) case None def isDefined: Boolean = this match { case Some(_) =&gt; true case None =&gt; false } } Isn't that inherently slower than virtual method as it implemented now?
I think what you actually want here is a class? class Cruncher(y: String) extends (String =&gt; String) { override def apply(s: String) = s"$y $s" } trait MyTrait { // you can't implement a method with a function val // so have to make it a function val at this level instead val crunch: (String) =&gt; String } class MyClass extends MyTrait { override val crunch = new Cruncher("somestring") } 
The most important part of my blog post are the three proposal for improvements that I copy-pased below: ### Proposals for Pattern Matching Improvements #### • Pattern Variables in Alternatives It should be possible to bind pattern variable in the branches of pattern alternatives, just like in OCaml. For example, I want to be able to write: case Left(a) | Right(a) =&gt; ... The type given to `a` in the right-hand side of the `case`, assuming we are matching an `Either[S,T]`, should obviously be the least upper bound of `S` and `T`. In Dotty, that corresponds to `S | T`. For instance: def get[S,T]: Either[S,T] =&gt; S | T = { case Left(a) | Right(a) =&gt; a } This is a seemingly trivial change, but it would make so many uses of pattern matching in Scala more concise! For one thing, instead of the awkward: try ... catch { case er @ (_: E | _: F) =&gt; ... } we could write: try ... catch { case er: E | er: F =&gt; ... } but this also really translates to more expressive power. Consider the following OCaml pattern matching code, which I wrote in my last project: let rec normalize = function | TUnion(TBot,t) | TUnion(t,TBot) | TInter(t,TTop) | TInter(TTop,t) | TUnion((TTop as t),_) | TUnion(_,(TTop as t)) | TInter(_,(TBot as t)) | TInter((TBot as t),_) (* the above is a single pattern! *) -&gt; normalize t (* and here's the right-hand side of the match *) ... In order to write that in Scala, one would have to split the pattern into many cases, and duplicate the right-hand side. When the right-hand side is non-trivial, this can become a problem. Instead, I just want to be able to write: case TUnion(TBot,t) | TUnion(t,TBot) | TInter(t,TTop) | TInter(TTop,t) | TUnion((t @ TTop),_) | TUnion(_,(t @ TTop)) | TInter(_,(t @ TBot)) | TInter((t @ TBot),_) =&gt; normalize(t) As a generalization of this, once we have type-enforced null-safety, we could also say that if a pattern variable appears in only one of two pattern alternatives (something that is illegal in OCaml), in Dotty we would give it type `T | Null`. Or even something like `T | Unit` could already be useful, in the absence of null-safety. #### • Generalized Pattern Binding In Scala and Haskell, `@` is used to bind a name to a subpattern. However, I don't see a reason why it couldn't be used to bind two or more arbitrary subpatterns: case FirstPattern(a) @ SecondPattern(b, c) =&gt; // ... use a, b, c ``` For example, consider hypothetical character extractors `StartingWith` and `EndingWith` for `String`, which could then be used as: def isPalindrom: String =&gt; String = { case "" =&gt; true case str @ StartingWith(a) @ EndingWith(b) =&gt; a == b &amp;&amp; isPalindrom(str.tail.init) } (Of course, the above is inefficient because it destructures a string inductively, incurring lots of copying. We could make it more efficient using some kind of `StringSlice` or `StringView` type, though.) #### • Partial Destructuring in Guards _**Preamble: boolean blindness of conditional guards.**_ Have you ever seen or written this pattern? case Var(name) if boundVariables.contains(name) =&gt; foo(boundVariables(value)) This is terrible: it queries the `boundVariables` map _twice_: once to find out whether the `name` key is in it, and another to actually access it. In addition, this suffers from "boolean blindness": the type system does not track whether you made the `contains` check before doing the access, and a refactoring could easily break that invariant silently. There is a better function for querying maps in Scala called `get`, which returns an option in one go. But that method cannot be used here, without refactoring the pattern match expression in a disruptive way: we would have to move the conditional branching out of the conditional guard and into the right-hand side of the case, which means we'd have to modify the other cases in the match too. A typical example is when calling `collect` as below: exprs.collect { case Var(name) if bv contains name =&gt; Left(bv(name)) case Const(v) =&gt; Rigth(v) } which can't be easily adapted to use `get` on the map, instead requiring a complete refactoring: exprs.flatMap { // can't use collect anymore! case Var(name) =&gt; bv.get(name).map(Left.apply) case Const(v) =&gt; Some(Rigth(v)) } Things would get even messier if we wanted to test something on the extracted `value` in the guard: case Var(name) if boundVariables.get(name).exists(value =&gt; value &gt; 0) =&gt; foo(boundVariables(value)) This is just an instance of the general problem that conditional guards in Scala suffer from boolean blindness. _**Solution: destructuring pattern guards.**_ We should allow several conditional guards headed by `if`, taking either a boolean expression or a destructuring statement with an `=` sign. For example: case Var(name) if Some(value) = boundVariables.get(name) if value &gt; 0 =&gt; foo(value) There is no simple way that I know of doing something like the above in current Scala, without defining a custom extractor, which is very verbose... This is very similar to [the pattern guards](https://wiki.haskell.org/Pattern_guard) that Haskell added in 2010. It's also related to Rust's more restricted [`if let` idiom](https://doc.rust-lang.org/rust-by-example/flow_control/if_let.html). _**Similarity and unification with partial matching in comprehensions.**_ There has been a lot of talk about [separating total from partial destructuring](https://github.com/lampepfl/dotty/issues/2578) in `for` comprehensions. One of the proposed syntax was: for { x &lt;- eithers; if Left(y) = x; if Right(z) = y } yield z // equivalent to: for { if Left(Right(z)) &lt;- eithers } yield z ...instead of the current Scala code below, which is possibly surprising because it hides the fact that the destructuring is partial and actually performs filtering: for { Left(Right(z)) &lt;- eithers } yield z // filters // NOT equivalent to: for { x &lt;- eithers; Left(y) = x; Right(z) = y } yield z // fails with MatchError! I like the proposed `if` syntax, and suggest to generalize it to pattern guards. 
Well depends on your background... I knew java and learned scala. Have no idea about haskell by the way. Maybe it is easier but I really can't say.
You can just start with the Functional Programming for mortals book. Haskell is not a must to learn to learn functional scala. However if you like the functional approach there is a high chance you will want more and learn Haskell. Scala is the entry drug for functional programming.
+1
What makes you think that? Have you tried measuring it? Usually a branch between two known paths is much better for processor performance than virtual dispatch (i.e., the processor has no idea what code will execute next). The JVM is pretty good at _removing_ virtual calls to recover that performance characteristic. Still, in principle, not having a virtual call at all in the first place is even better.
As a Haskell developer currently diving into Scala: If Scala is what you'll be using, then I don't see the point in learning Haskell first. Haskell is more elegant, but all the abstractions you'll be using in Scala are available in Scala. 
No. Haskell way of programming is not idiomatic Scala. If you want to learn idiomatic Scala, Programming in Scala (3rd edition) by Scala author (Odersky) is probably the best book on the subject (and skip all the "pure" FP books like the red book and the one you've mentioned).
For virtual calls with two concrete targets, the current JVM/Hotspot/C2 generally inlines it into if-else checks, so it would probably be exactly the same machine code being generated in either case. IIRC C2's heuristics for inlining are roughly: - Not more than 9 levels deep after inlining - Not more than 35 bytecodes in total after inlining (~2000 bytecodes for hot paths) - Not too polymorphic: - 1-target virtual calls are inlined raw (+guard) - 2- or 3-target virtual calls are inlined with if-elses (+guard) - &gt;3 target virtual calls are never inlined ("megamorphic") - Basically all other optimizations depend on inlining; no inlining means no constant folding, allocation sinking, escape analysis, anything It's good to keep these numbers in mind; the JVM JIT isn't "magic", as many people treat it as. It has a pretty small number of pretty straightforward rules, and has plenty of cases which it handles pretty badly overall: highly-polymorphic code or code with lots of higher order functions. Basically idiomatic Scala! Graal probably does something different, but probably not that different.
But they explicitly asked about the FP side of Scala. You cannot learn this from "Programming in Scala". 
I gave 100% emphasis to "My goal is to learn Scala."
Haskell is a difficult language to learn, to the point where the easiest way might actually be to learn Scala first. If you're looking to force yourself into a more functional style in Scala by learning from another functional language, I found Standard ML a good starting point - it's rather like Scala but without mutable variables or classes.
As i understand: &gt; 3 target virtual calls are never inlined ("megamorphic") Is major drawback of current typeclasses in Scala, mainly implementations that in fact are known at compile time are never properly optimised at runtime. &amp;#x200B;
If your goal is to learn a more functional Scala, you don't need Haskell. You can read *FP for Mortals* or *Advanced Scala with Cats*. *Just learn Haskell* is just a response from some lazy developers, that don't bother with providing you with tutorials/documentation.
A reporting system for financial reporting in Spark/scala for a bank. I like a lot scala and everyday we do more things with just a few lines of code. It is impressive how scala and functional programming resolve problems with a simple and elegant solution. 
No
Writing them is painful, and performance is painful, but using them shouldn't be.
Cool stuff. I love the synergy between extensions methods and typeclasses. It just feels right. &amp;#x200B; He presents the C#-like, "this" modifier for extension methods. But the stuff that gets merged appears to use a different syntax. Which one are they moving forward with? Or is it still in flux? I really hope it's the this modifier.
Is there a function inside designer that is blocking the ability of thumbnails on content manager for templates being created? Cannot seem to find why my thumbnails are not showing up.
The idiomatic functional side of Scala is not Haskell-style FP, but ML-style FP. That FP style is shown in all the Scala books. Haskell-style FP can be done too, that shows how powerful and featureful Scala is, but it's not the only way to do it.
Back to square one how? &amp;#x200B; IIUC it just means moving more code to the cross-compiled module.
It's not necessary but IMHO Haskell is clearer syntactically about basic functional concepts such as Functors, Monads, and Algebraic Data Types. So if you want to really get into functional programming, learning Haskell won't be wasted time. 
My biggest usability gripe is that they're stacks. F\[G, \_\] is distinct from G\[F, \_\], so functions involving one aren't composable with functions that involve the other. They should be sets.
I would recommend it. Not learn it in depth but the basics. The way Scala started making sense to me as an FP language was by reading Learn You a Haskell and starting to understand why some things were like they are in Scala without the taint of OO.
Not really. They should come out roughly the same on HotSpot after CHA. The bigger issues are that this encoding doesn't support the API `Option` currently has, gets further away from ever being compiled down to a cheap and efficient value type, and makes union types even less useful (you would need to write `new Some(...)` everywhere, and `None` is completely unsupported because it is considered to be a singleton type.).
Generic extension methods is pretty bad though, because they create a syntax where a type is used before it is defined. It's a bit surprising that nobody looked at Ceylon and Kotlin, which both suffer(ed) from this exact issue.
No. It should have been deprecated an removed altogether. It tries to do too many things at the same time, with poor results and high costs. I kind of regret even proposing the `Stream` to `LazyList` rename: although it is a net improvement, it created the wrong impression that this piece of functionality is salvageable. 
I am sympathetic to the cause. I believe "Pattern Variables in Alternatives" has a small enough surface area that it would have no problem to get in, _provided someone makes the effort to implement it and prepares a PR_. The other two require more thought since there are larger language changes. So we'd need to make a SIP and discuss it thoroughly.
Could you give a specific example of a potential issue? I've used C# extensions methods and liked them a lot, but I'm not as familiar with the Ceylon or Kotlin equivalents.
&gt; it would have no problem to get in, _provided someone makes the effort to implement it and prepares a PR_ Great! I can give it a go at the Scala spree on Saturday.
If you're going to learn a different language to pick up FP concepts, I'd recommend SML over Haskell, because it's a much smaller language, and closer in spirit to Scala. &amp;#x200B; The Little MLer is a good quick starting point to see if it's valuable to you: &amp;#x200B; [https://mitpress.mit.edu/books/little-mler](https://mitpress.mit.edu/books/little-mler)
&gt; Just learn Haskell is just a response from some lazy developers Haha I see what you did there.
Applicative will let you work with validateds. 
You mean releasing 3.0 by 2020
 Iterator#toStream shouldn't force evaluation of the first element.
This sort of thing: ``` case FirstPattern(a) @ SecondPattern(b, c) =&gt; ``` can be written as ``` case FirstPattern(a) &amp; SecondPattern(b, c) =&gt; ``` If `&amp;` is defined as ``` object &amp; { def unapply[A](a: A): Option[(A, A)] = Some(a, a) } ``` 
I think you have posted in the wrong subreddit.
Use *take(1)*
Isn't this for SCALA support?
This sub is for the [Scala Programming Language](https://scala-lang.org/). I'm guessing you're asking about the [SCALA digital sign company](https://scala.com/). I do not know if there's one for that.
head and tail are important concepts in functional programming. it's never deprecated
Alright
I’ve done something like that with Slick. Check out this documentation http://slick.lightbend.com/doc/3.2.3/gettingstarted.html#schema For loading the data you can parse the csv or json and map to a tuple/case class for smooth Slick interaction. Or just use the psql copy command with the csv 
If the name of the variable is secret and the comments are your biggest concern, you have already lost whatever you are trying to protect. Environment variable names do leak things, but for most use cases, it’s the value that’s the prize. ‘DATABASE_URL’ for example, DOES give away the the fact that you have a database with a URL. This is potentially useful to an attacker, but most apps have one and they still need to know the value of that variable to access said database. ‘SECRET_MILITARY_BASE_LOCATION’ might be a bad call, because you are now a unique target. You asked a specific question, my answer is ‘not really, but it depends’, and what you should read about is called threat modeling. Know your adversary, because you won’t keep the KGB at bay with good comment hygiene.
yes that was a bad example from me
Here is Pere's Scalax 2018 lightning talk about Bridges [https://skillsmatter.com/skillscasts/13117-lightning-talk-bridging-the-gap-between-front-end-and-back-end](https://skillsmatter.com/skillscasts/13117-lightning-talk-bridging-the-gap-between-front-end-and-back-end)
Thanks for your reply!! I don't really have env var's names that indicating how serious/significant the value is, but you make a good call, in general this approach may leak info unintentionally. 
if we accept this argument, is acceptable that we can do blocking operation in any Actor when we assume that it is "quick and cheap", like a local disk "I/O". Without even getting into discussion if a cheap IO exists, in my opinion its goes again they own best practice. &amp;#x200B;
How do you deploy the key to your app?
&gt; if we accept this argument, is acceptable that we can do blocking operation in any Actor when we assume that it is "quick and cheap", like a local disk "I/O". Sure, I'd agree with that. I tend to be quite anti-akka in general though, so maybe akka advocates will take a different view. 
Good point! I had forgotten about the pattern-splitting trick. The proposal to generalize `@` is the least compelling of the three since we can already have the same behavior with a custom operator, as you show. It's mostly about what I consider to be an unjustified and unnecessary restriction of the language. It would be nice if `@` just did the general thing and if we didn't have to use a custom operator.
akka is nice, whot you don't use actors :)
Yep. You’re right. 
&gt; This looks good, in terms of the original intent, it does capture every possible scenario, but it is very complicated to work with this deeply nested structure. It is, but the complication was already there in the problem: code that can receive an invalid input, can fail, and can have side effects *should* look complicated because it *is* complicated. Your top-level code will have to work with such constructs (or more likely their monad transformer stack equivalents), but part of the point of making these aspects explicit is that you can now work to disentangle your business logic from them - the so-called "functional core, imperative shell" model. So you would generally then try to separate the parts that do validation, the parts that can fail, and the parts that have side effects into distinct functions (as far as possible) that can be tested separately etc. I'm going to write about monads from now on; note that `Validation` does not form a monad so this isn't directly applicable to your example, but imagine working with three or more monads that you wanted to compose. In an application I'd be tempted to just define a top-level transformer stack type and specific functions for lifting `Either`, `F` and `Validation` into that stack type. But if you're working on a library or want to go full-generic there are two competing approaches: [free coproduct style](https://underscore.io/blog/posts/2017/03/29/free-inject.html) and final tagless style (aka MTL-style typeclasses), where you'd write functions in terms of typeclass constraints that describe the effects that those functions need. E.g. a function than can error would be written in terms of `F[_]: MonadError[E]`, a function that can log would be written in terms of `F[_]: MonadTell[S]`, and a function that composes those and does both would be written in terms of `F[_]: MonadError[E]: MonadTell[S]`. At the very top level you form a concrete monad transformer stack that has all of the capabilities you need, but all your lower-level functions are written generically in terms of smaller sets of possible effects.
And here I thought we were getting C's best feature 
Looks to me like it was "fast" because you never actually consumed the iterator. I don't know what `childrenIter` is, but `(x: java.util.Iterator[_]).asScala.toSeq` creates a lazy collection.
Lol )))
Nothing prevents you from using `List`, but I'd wager that &lt; 1% actually use the additional features thrown into `Stream` ... even if you're only interested in one of features, you are still paying for all the others in terms of complexity. It's a poor package deal. 
Ive got a webapp (akka http). Id like to have an endpoint that executes a shell command in the background. Id like to enque the commands it runs so it only ever runs one at a time. I see how to run a command with sys.process._ but I'm not sure about the queue. I can use a mutable list but i keep reading that immutable is better. Also i can run async things in a future but im not sure how to run one in parallel for as long as my server is running. Any pointers? Ive done something similar in python by having two scripts and beanstalkd for message passing. Im looking to solve this as a scala learning exercise without beanstalkd.
Same here
And today one of the most important milestones was achieved :D With some caveats but accepted into live use. Great xmas gift after a very hard year. 
Apart from not forcing the iterator, as mentioned below, but you can also try using uPickle, when benches at ~3x faster than play-json for reading and writing JSON to strings 
Learning some Haskell after working with Scala for awhile really put the pieces together for me. I had a lot of aha moments about things in Scala that were previously a little confusing to me. Something useful is that I couldn't keep falling back to the OO stuff while learning FP since I was "stuck" in Haskell. I would definitely recommend it.
Try jsoniter-scala - serialization of UUIDs and nested JSON objects is where it shines the most. Just see results of benchmarks where it is compared with different JSON serializers in Scala: https://plokhotnyuk.github.io/jsoniter-scala/ There are 2 main options how to do that: 1) convert the source graph to some tree for wich an automatically derived codec can be generated by macros; 2) write a custom codec that can serialize your graph more efficiently using low-level API. Also welcome to our chat or raise issue with some shared code as input - we will happy to help you find the best suitable solution: https://gitter.im/plokhotnyuk/jsoniter-scala https://github.com/plokhotnyuk/jsoniter-scala/issues 
This is a bad article, the problem it describes is inexistent : the author creates a new "ReaderT"-like datatype for each dependency that he has, stacks them, then defines a new typeclass reflecting each of them, and complains that he has to do it. First of all, all the boilerplate is reduced to NIL of you're using the actual ReaderT and MTL-typeclass (ApplicativeAsk) associated to it. Second of all, yes, stacking reader transformers is terrible performance-wise. I don't think anybody ever claimed it was a good idea. The point of MTLs is to work with the typeclasses first AND THEN find a monad that fits your constraints. If you have to pull both network and db you don't have to stack several readers, you can provide a monad that does it with a single layer by simply reading both dependencies from a case class that contains them. Now that being said, tagless interfaces do make the code more readable usually. But guess what ? MTL are tagless final encoding of monad transformers ! So saying that tagless solves a problem created by MTL is misleading. Saying that it's a good idea to define additional, domain-specific algebras rather than the low level MTLs directly in your domain logic is however a good idea. I agree with tagless being a good thing, i disagree with the explanation given by the author as to why it's a good thing. 
What's the difference between **Scala** By The Bay and **Scale** By The Bay?
I think as a start you could leverage the graph database better. Here is a query that will get you all of the data you need in one transaction. I believe the slowdown is going back through all of the child vertices and querying the graph again more so than any conversion to JSON. \`\`\` g.V() .has("id", parentId) .has("type", parentType) .out(edgeType) .has("type", childType) .dedupe() .valueMap("name", "id", "type") \`\`\`
Can FS2 be used to stream GZ files stored in Amazon S3? The files are very large 100G+ 
I mostly finished to update videos and programming assignments in my course "Introduction to programming with dependent types in Scala (2018)" [https://stepik.org/course/49181](https://stepik.org/course/49181?fbclid=IwAR1gj99YYeoiKYiojw7QNi_W9iKhO8_iF09OlXE-jBOjNaRt-twKFwNh_mo) Slides and code snippets are still in progress. Currently there are 89 assignments (including new ones). All assignments are run online at server (Scala + Shapeless + ProvingGround).
I think this is exactly the problem. I'm not leveraging the graph functionality correctly. I'll try this and get back to you. Also, happy cake day!!
Correct. Using `toList` would evaluate the collection immediately &amp; give me the slow down that I was expecting. Thanks for this. Still need to figure out how to improve the speed.
I recently released version `0.11.0` of [AckCord](https://github.com/Katrix/AckCord). AckCord is a Discord API library using Akka.
Okay, today I made a WIP implementation and opened the corresponding issue on the Dotty repo: https://github.com/lampepfl/dotty/issues/5620
this is super interesting IMO for undergrad students. I love the content. Get it in front of students asap! :)
Thank you for making my heart melt! :) 
Wasn’t the question just about the fact that he was passing a dependency implicitly rather than providing a type class instance for his type class? In which case the alternative would be to pass it explicitly.
Possibly :D Maybe that should've been my question: "Did I understand the question?". I may update the post tbh. Thanks.
Hi, Gabriel here, thanks a lot for your feedback :) I do pass the instance of `Console` implicitly because I consider it a standard effect in the same way `cats-effect` embraces passing `Timer`, `Clock` and `ContextShift` as a "typeclass" instance, even though these are not typeclasses. So it is clear that only a single instance should be in scope (anyway you could still pass another instance explicitly as you could do with any other lawful typeclass but you'd be breaking the rules and defeating the idea of using typeclasses in the first place). If I decided to pass it explicitly instead: ```scala def program[F[_]: Monad](console: Console[F]): F[Unit] = for { _ &lt;- console.putStrLn("Enter your name:") n &lt;- console.readLn _ &lt;- console.putStrLn(s"Hello $n!") } yield () ``` We would not be enforcing the existence of a single instance in scope but still it'll be a valid approach. TL;DR: I prefer to pass instances implcitly whenever they represent clear "effect algebras" and explicitly when working with "algebras" that represent the business domain.
Thank you very much Gabriel :) I think I got the idea!
I use basically only aws-sdk see [gist](https://gist.github.com/btd/49079c38ddb0e06b0641c1d0d74ae3bb). But i do not publish too much metrics. Just to collect some relative usage stat.
Thanks!
So there's two parts to a metrics API -- collecting them, and reporting them. For collection, I use Lightbend Telemetry aka Cinnamon, because it's got the configuration layer and instrumentation magic built in for Play, but it's really a wrapper on top of Codahale/Yammer Metrics. You can also use [Kamon](https://kamon.io/documentation/1.x/core/basics/metrics/) or [metrics-scala](https://github.com/erikvanoosten/metrics-scala) for collecting metrics. Reporting is typically done through Prometheus or some sort of statsd reporter. It sounds like you want to publish custom metrics directly to AWS cloudwatch, which would writing a reporter which would connect to Cloudwatch [using the AWS SDK](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/examples-cloudwatch-publish-custom-metrics.html). If you're not into that, then many of the reporting services have exporters, i.e. prometheus has a [cloudwatch_exporter](https://github.com/prometheus/cloudwatch_exporter). This can be easier to handle, as you're only worrying about collecting metrics and reporting to Prometheus, and then everything after that is downstream from you.
Haven’t looked into too much detail at your code but please replace `Optional.fromNullable(null)` with `Optional.empty()`.
thanks, you're right!
Why return Product in ShopDao? 
Man, that was exactly the reply I was looking for! Big big thanks!
you mean fetchProducts and fetchProduct? these methods give you full information about the product in shop (joined with an intermediate table named shop\_product)
If you just want to learn functional programming, Haskell is way easier to learn from my personal experience. I do both Scala and Haskell at my job. &amp;#x200B; Scala is a much bigger and more complicated language. It offers more than one way of doing things and tries to please a wider group of users. Haskell on the other hand is a lot smaller and more focused on the FP paradigm. &amp;#x200B; If you have to learn Scala and want to learn FP as a bonus, then go with Scala, there's no point in learning Haskell then Scala if you aren't gonna use Haskell. They are very different languages. I don't suggest learning them both at the same time because I have to work on some Scala and Haskell projects at work. Sometimes I get confused about syntax because of the constant switching between the two, and this costs time.
I added more info on a project that I recently started working on - scala idiomatic wrapper for the neo4j java driver. Today, completed the first version of a documentation site - [https://neotypes.github.io/neotypes/](https://neotypes.github.io/neotypes/) Project - [https://github.com/neotypes/neotypes](https://github.com/neotypes/neotypes)
Unrelated to your code, but avoid committing IDE related files/folders (`.settings`, `.vscode`) and files that get auto-generated on compilation (Files like `.classpath`and `.project` are auto-generated right?). Add them either to your global or project `.gitignore`. *PS: Not really a Java person, so the files I've pointed out could be actually incorrect and have some value to your project. I usually have a site like [https://www.gitignore.io/](https://www.gitignore.io/) to generate the `.gitignore` file for me.*
Hi. Thanks a lot for your comment. It's indeed a possibility that one can use a predefined ReaderT and experience way less pain. However defining new monad transformers is painful. Article also concentrates on completely different problem - defining fine grained algebras and building kinda bottom-up dsl for your program, clearly defining boundaries - what is using what subset of effects. That's what this article series is about. Not how to do MTL, tagless final, free or eff, because other authors described it in a better way. What I wanted to do is to build up a nice intuition for every approach and to see how can we come up with that approach. About using ReaderT for kinda dependency injection through single case class - not exactly possible as the constraint is to have a fine grain control over effects subsets. Which is not possible if we smash them all into single case class. Tagless final on the other side gives me an easy way to control which effects will be used in the function. I would appreciate if you have some links for codebases that use MTL with ReaderT to have that fine grained effects control. &amp;#x200B;
Had a look through the first one, the ini parser. * Lots of confusing comments - I suggest finding some way to not have to turn some formatter off and on every two lines. * Rather than `map` to something with `orElse(null)` and then immediately filter out `null`s, better to use some kind of `flatMap`. * I don't understand what you're doing with the `Either` * `reduce` immediately after a `map`/`flatMap` should usually be some kind of `foldMap` instead * You call `getOrNull()` and then immediately call a method on the result, isn't that unsafe? If you want to do an unconditional `get` it's better to do it directly. * Rather than creating a `Parser` and then mutating its state field, would be better to have the constructor accept the value you want to put in it and put it in a `final` field. * Are you sure all the `getOrElse("")` is a good idea? One of the cornerstones of functional style is for the program not to continue in an invalid state; looking up should probably either return some kind of `Optional`, or throw if the thing is not present; returning an empty string is generally just as bad as `null` if not more so (since it can propagate even further, and make it harder to tell where the program got into an invalid state).
Should be fine - there are a couple of implementations of http on fs2 (particularly http4s) and there's https://github.com/slamdata/fs2-gzip for doing the decompression in a streaming way.
&gt; Id like to have an endpoint that executes a shell command in the background. Id like to enque the commands it runs so it only ever runs one at a time. I see how to run a command with sys.process._ but I'm not sure about the queue. I can use a mutable list but i keep reading that immutable is better. If you want something to happen one at a time that's going to have to involve a minimal amount of mutable state somewhere, even if only on a thread's stack. The most natural way for akka-http would probably be to have an actor that runs the processes (since by their nature actors have queues of messages that they process one at a time). I might be tempted to just have an explicit queue in code (`ArrayBlockingQueue` or similar) and a single thread that pulled tasks off it (effectively recreating an actor "in userspace", which would mean you could keep type safety). The functional-purist way would probably be to use iteratees, but I'm not sure they'd be easy to connect up with akka-http. &gt; Also i can run async things in a future but im not sure how to run one in parallel for as long as my server is running. Any pointers? You *can* abuse the futures API by just creating a future whose body runs forever. But it would be better to use `ExecutionContext#execute` or just explicitly create a `Thread` and `start()` it. One gotcha is that the JVM keeps running as long as there are any non-daemon threads running. So you either need to make sure all your threads complete when you want to exit (not just the main thread), or else exit the JVM a little more forcefully (e.g. by calling `System.exit`). Though if the server is intended to just run indefinitely until it gets stopped by the OS then you can manage without either of those things. Another gotcha is that by default if you have an uncaught exception in a non-main thread it'll just log the traceback to stderr and that thread die. You probably want to either define an uncaught exception handler for the thread you create, or a global uncaught exception handler, or both.
hi :) &amp;#x200B; `style = defaultWithAlign` `maxColumn = 120` `continuationIndent.callSite = 2` `continuationIndent.defnSite = 2` `align.openParenDefnSite = false` `align.openParenCallSite = false` `danglingParentheses = true` `indentOperator = spray` `project.excludeFilters = [".*\\.sbt"]` `rewrite.rules = [RedundantBraces, RedundantParens, SortModifiers, prefercurlyfors]` `unindentTopLevelOperators = true` `importSelectors = singleLine` `spaces.afterKeywordBeforeParen = true` `lineEndings = unix` `newlines.penalizeSingleSelectMultiArgList = false` `newlines.alwaysBeforeElseAfterCurlyIf = false` `binPack.literalArgumentLists = false` `runner.optimizer.forceConfigStyleMinArgCount = 1` 
If one doesn't want or need to import any dependencies, like ScalaZ, here's another fairly simple way which includes nested unwinding of resource closes per the actual Java specification for AutoCloseable. https://stackoverflow.com/a/34277491/501113
Also planned to be released in 2.13 is a new "Using" class which will nicely handle auto closeable things and similar https://github.com/scala/scala/blob/2.13.x/src/library/scala/util/Using.scala
Thanks for your review, it's one of the valuable right now among others. You cheered me up thanks! `Lots of confusing comments - I suggest finding some way to not have to turn some formatter off and on every two lines.` eclipse formatter lame for lamdas in my opinion, so I had to apply third-party solution (Java-Prettier) and after it everything goes well, but I forgot to remove these comments. \---- `Rather than map to something with orElse(null) and then immediately filter out nulls, better to use some kind of flatMap.` Are you talking about `findAny().orElse(null) ... filter(r -&gt; r != null)` ? well, you're right, I have rewritten this to `flatMap(value -&gt; value)` . Consice and clear! \---- `I don't understand what you're doing with the Either` I can explain. Ini parser consists from sections `[SECTION]` and keys `hello=world` (I call them lines) . `List.of(`[`r.group`](https://r.group)`(0).split("=")).map(String::trim)` this expression returns so called keys. [`r.group`](https://r.group)`(3)` and this returns section if you will take a look at regular expression it's combined from two: (linePattern and sectionPattern) \---- `reduce immediately after a map/flatMap should usually be some kind of foldMap instead` in Java `reduce` is the terminal operation, so it comes in hand for me. \---- `You call getOrNull() and then immediately call a method on the result, isn't that unsafe? If you want to do an unconditional get it's better to do it directly.` Agree. Will take into account. \---- `Rather than creating a Parser and then mutating its state field, would be better to have the constructor accept the value you want to put in it and put it in a final field.` so you talking about overloaded `public static * fromFile(String filePath)` where \* - multimap, and where I call this method from `public static IniParser fromFile(String filePath)` and `parser.state = fromFile(filePath)` ? \---- `Are you sure all the getOrElse("") is a good idea?` Good point, thanks!
Awesome, this was a feature from C#/F# that I always felt was missing from Scala. Is there a full writeup with examples for the changes in 2.13? 
In projects that use spark and scala. How do you manage the Decimal fields? The spark framework returns and reads java.math.BigDecimal so we are using in the UDFs the java BigDecimal instead of the scala BigDecimal. &amp;#x200B; Thanks
My take on the issue: https://www.reddit.com/r/scala/comments/a4dq44/psa_use_the_default_scalafmt_config_even_if_you/
Is there a guide that gives an in-depth info about best practices with packaging and compiling?
Resource handling is seemingly one of those things that everyone feels they can do better than the default :-) nevertheless, I quite like Java's try-with-resources for its simplicity. Here's my (partial) attempt to port it to Scala: object AutoClose { def apply[A &lt;: AutoCloseable](a: =&gt; A): AutoClose[A] = new AutoClose(() =&gt; a) } class AutoClose[A &lt;: AutoCloseable] private (val acquire: () =&gt; A) extends AnyVal { def flatMap[B &lt;: AutoCloseable](f: A =&gt; AutoClose[B]): AutoClose[B] = foreach(f) def foreach[U](f: A =&gt; U): U = { val a = acquire() try f(a) finally a.close() } } As you can imagine, this is meant to be used in a for-comprehension, so that resources can be closed in the reverse order of acquisition. Here's an example usage: object AutoCloseTest { import java.io.{BufferedReader, FileReader, PrintWriter} def run(): Unit = for { input &lt;- AutoClose(new BufferedReader(new FileReader("README.md"))) output &lt;- AutoClose(new PrintWriter("README.markdown")) } copyLines(input, output) private def copyLines(reader: BufferedReader, writer: PrintWriter): Unit = { var line = reader.readLine() while (line != null) { writer.println(line) line = reader.readLine() } } } 
Resource handling is one of those "one minor detail" things that is not always as simple as it looks. For example, if you've got async handling that depends on that resource, you can't close the resource until all your tasks have finished processing on it (or errored / timed out). If you've got a blocking resource, or even if closing the resource is a blocking operation, then you can take the thread out of commission. 
You should think domain-wise (for lack of a better term). Your Product DAO should be consistent in returning Products, same with Shop. Don't just mix and match. If you have a third entity (or is it a ManyToMany?) then it should be ProductShop, and have that contain anything for that entity.
To me the capitalization of `Using` in combination with the actual name looks awkward. Like it's imitating a keyword/built-in feature but at the same time making sure you know it's an object wrapper and losing clarity in both aspects. Something like `Managed` would just make more sense to me, or lower-casing the u (like with `require`/`assert`), but maybe I'm missing a bigger picture
Thanks for the info! I think an actor will be my first attempt based on what you've written here.
yes, it's ManyToMany, thanks for the note, I thought about it myself but end up placing in one DAO
Perhaps https://www.scala-sbt.org/sbt-native-packager/ , although it is focused on a single tool.
There is an implicit conversion from java BigDecimal to scala BigDecimal, so going that way works transparently. If you call \`.bigDecimal\` on a scala BigDecimal, it will return a java BigDecimal. &amp;#x200B; Not sure what you mean by "manage", but as far as conversions and passing it as a parameter it's pretty transparent.
Recording of the fireside chat is here, free to watch (registration required), along with all the other videos from Scala eXchange: [https://skillsmatter.com/skillscasts/12961-fireside-chat-martin-odersky-and-simon-peyton-jones](https://skillsmatter.com/skillscasts/12961-fireside-chat-martin-odersky-and-simon-peyton-jones)
I am showing my team the resuls of this config. In general people like scalafmt, my problem is that I would like class definitions, if too big, to be one line for explicits dependencies, one line for implicit dependencies and one line for traits 
&gt; Ini parser consists from sections [SECTION] and keys hello=world (I call them lines) . &gt; &gt; List.of(r.group(0).split("=")).map(String::trim) &gt; &gt; this expression returns so called keys. &gt; &gt; r.group(3) &gt; &gt; and this returns section So how does it know which to use? Does it try the first one and then use the second if the first errors, or something like that? I'd rather have that logic be more explicit, and certainly not rely on error handling in normal operation. &gt; so you talking about overloaded public static * fromFile(String filePath) &gt; where * - multimap, and where I call this method from public static IniParser fromFile(String filePath) &gt; and parser.state = fromFile(filePath) No, I'm saying that `state` should be `final` and set in the constructor, and you should do `new Parser(state)` when you have a valid state, not before. Just because it's the class your `main` is in, doesn't mean it shouldn't follow the normal rules for good class design: make invalid states unrepresentable.
*aot Please lets not abuse this term since in Scala's context it can get confusing
&gt; The idiomatic functional side of Scala is not Haskell-style FP, but ML-style FP. Idiomatic functional side of Scala is Scala-style FP. If you ignore Haskell-style HKT and category theory abstractions, you're doing it wrong, but if you ignore ML-style modules and abstract data types, you're also doing it wrong.
&gt; For the record, we can get full type reification in Scala using the much more heavyweight/expensive TypeTag[T] type class (not recommended as it basically pulls Scala’s entire type system into your runtime), but such evidence cannot be used for pattern matching use cases such as the one above. Hmm, no? `TypeTag`s definitely do work with pattern matching.
A similar trick I just used is - since the affected cases is basically where you do not have a qualified import of Seq - to just add a package object for your library with an alias like ```scala package object mypackage { type Seq[+A] = scala.collection.Seq[A] } ``` then in the IDE of your choice (I use IntelliJ), to rename that alias, say to `Foo`. Then remove it manually from the package object. Now if you do a fresh recompile, you will get `Foo` not found in all the affected places, and here you can decide whether you want to use `scala.collection.Seq` or `scala.collection.immutable.Seq`, or indeed just `Seq` (generic in Scala &lt;= 2.12 and immutable in &gt;= 2.13). The other symbol affected in the same way is `IndexedSeq`.
I know what you mean (`&lt;:&lt;` in guards), but: 1. that's not pattern matching and is not safe, and 2. you can't write my example using it.
Are there any updates on the release timeframe for Scala 2.13? 
Hmm, no, that's not what I mean. AFAIK Scala _will_ use the typetag to match in code such as this: def x[T: TypeTag](l: List[T]) = l match { case _: List[T] =&gt; true ; case_ =&gt; false }
I agree with you, the uppercase looks weird. But there's still time to make a lowercase alias that forwards to this class.
Its interesting that they did not need to pass reflection information to graalvm to compile (or maybe they do that somewhere and I missed it) I would have thought using something like jackson would need reflection info to be passed to graalvm?
Prof. Odersky, not Mr.
God emperor Odersky. 
[Benevolent dictator for life](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life) Odersky
Professor is not a title, it's a job. 
Jobs can be titles.
It loooks the same on Firefox and Chrome, so I'm assuming this isn't a browser-specific rendering issue, but there's a *lot* of code in that article, and the grey line highlighting makes it somewhat difficult to read. The extra wide line-spacing doesn't help either. I would like to say something useful about the content, but TBH my brain is still choking on the rendering.
Heh, sorry, I wish I could help, but I am clueless when it comes to styling. I did point out the issue to the right people though :-)
Oh, now I see what the problem is. Highlighter went awry. Will fix that.
I don't know of any official word, but RC1 is currently scheduled for end of January according to [the milestone](https://github.com/scala/scala/milestone/67). Scala 2.12 had two RC releases separated by about a month, with 2.12.0 released a month after that. Assuming 2.13 follows a similar path, a reasonable estimate might be end of Q1.
Much better, thanks.
“Serverless”
Hmm, no? You know you can just try it in the REPL, right? This is my example using a `TypeTag`. It doesn't even work in the simpler case where a `ClassTag` would work: &gt; import reflect.runtime.universe._ &gt; def first[S:TypeTag](ls: List[Any]): Option[S] = ls.collectFirst{ case s: S =&gt; s } &gt; first[String](List(1,"a",2,"b")) res3: Option[String] = Some(1) 
You could also use Dr Odersky, of course.
Good call on `IndexedSeq`. I updated the post. 
Easiest would be to use `classOf[EndpointWhateverType]` in the tuple, then your call will look something like `executeWithType(action._2.cast(action._1), batchRequest)` But that's not going to solve all your problems, because that `.as[A]` call in `executeWithType` requires an implicit `Reads[A]` which it has nowhere to get from. One way around that is to declare a class that will hold a reference to the endpoint and to a matching Reads object `case class Endpoint[A: Reads](action: () =&gt; Action[A]) {` `def invoke(batchRequest: BatchRequest) = action()(request.map(_ =&gt; batchRequest.body.map(_.as[A]).get))` `}` Then the rest of your code looks something like: `val actions = Map(` `Seq("POST", "/endpoint1") -&gt; Endpoint(() =&gt; controller.endpointOne)),` `Seq("POST", "/endpoint2") -&gt; Endpoint(() =&gt; controller.endpointTwo))` `)` `val res = actions.get(Seq(batchRequest.method, batchRequest.endpoint)) match {` `case Some(endpoint) =&gt; endpoint.invoke(batchRequest)` `case _ =&gt; Future.successful(NotFound)` `}`
Reminded me of ["Batman's a scientist"](https://vimeo.com/12065366).
What is the font you use for the code snippets? It looks really nice to read.
I omitted the implicit reads by mistake - I'll update the OP. `EndpointOneType` and `EndpointTwoType` etc all have implicitly defined readers, which is where they would get the Reads from. Does that change your answer at all? I really appreciate the help, I'll see what I can do from what you've written so far.
&gt;So how does it know which to use? it doesn't. &gt;Does it try the first one and then use the second if the first errors, or something like that? Everything goes like a waterfall from the stream, the error can only happen if it's empty or an incorrect string in the ini file. We're only relying on regex groups. And the rest logic concentrated in reduce. &gt;I'd rather have that logic be more explicit, and certainly not rely on error handling in normal operation. If regex were safer in Java it would be much easier to write explicit code, so I had to do some "save my ass" error handling.
Managed to get it working using the case class method you've described. Do you mind explaining how it works? It's basically like magic to me. Two years of Scala and I'm still surprised what you can do with it. How can it tell what type it needs to convert the JSON to? Is it through the definition of the endpoint itself, because the endpoint is an `Action[EndpointOneType]` or `Action[EndpointTwoType]`?
You need to be careful to also capture the effects in laws, not just the result. A good litmus test is to see if the law specifies a possible refactoring that doesn't break anything. For example, I would not be able to blindly substitute by this law: save(e) &gt;&gt; known(e) &lt;-&gt; pure(true) because it completely removes the effect of saving stuff. The correct law would be save(e) &gt;&gt; known(e) &lt;-&gt; save(e) &gt;&gt; pure(true) And this is ignoring concurrency and stuff like errors arising from network issues (it might be reasonable to ignore)
!!This only works with java 9+!! This is some sbt code I wrote up tonight to get ```jdeps```, ```jlink```, and the [SBT-Native-Packager](https://sbt-native-packager.readthedocs.io/en/stable/index.html) working together to make a bundled .zip file for MacOSX / Linux that has a built in JRE. ```jdeps``` is used on your code and the dependencies you've declared in your project to find the jvm modules you depend on (such as ```java.base```). Then jlink is called with this information to generate a JRE that only contains the modules you need to run your application. Finally, this minimal JRE is put inside a zip package which contains a launch script generated by sbt-native-packager, with some minor modifications to get the launch script to use the bundled JRE. With this I was able to take an application depending on openjfx and bundle it with a JRE for a .zip file of the size of 70MB. I hope to use the jdeps utility to try to do some shading that strips unnecessary classes from this package and gets it to be even smaller.
&gt; you automatically get better defaults when you upgrade. Yeah, and huge history-destroying diffs.
Maybe to try to use it with sbt assembly to combine all your jars to one. In theory this should reduce size also.
sbt-assembly would work, but it doesn't reduce the size much. Plus I wanted the launch script that sbt-native-packager provides i'm going to see if I can use jdeps to find the list of used classes in a project and feed that to sbt-assembly's shade command though. That would reduce the size a good deal I feel. 
He isn't one though. There are the Scala Center, SIPs etc.
Thanks, for example this code fails: &amp;#x200B; sparkThings.spark.sqlContext.udf.register("udfSuma", **( a: BigDecimal, b: BigDecimal) =&gt; a + b)** val a = sparkThings.spark.sql("SELECT myDecimalOne + myDecimalTwo, udfSuma(myDecimalOne , myDecimalTwo) FROM myTable").head &amp;#x200B; Failed to execute user defined function(suiteTest$$Lambda$1966/0x0000000840d7c840: (decimal(38,18), decimal(38,18)) =&gt; decimal(38,18)) ... Caused by: java.lang.ClassCastException: class java.math.BigDecimal cannot be cast to class scala.math.BigDecimal (java.math.BigDecimal is in module java.base of loader 'bootstrap'; scala.math.BigDecimal is in unnamed module of loader 'app') &amp;#x200B; While this code works perfectly, selecting the java.math.BigDecimal in the input parameters &amp;#x200B; sparkThings.spark.sqlContext.udf.register("udfSuma", **( a: java.math.BigDecimal, b: java.math.BigDecimal) =&gt; scala.math.BigDecimal(a) + scala.math.BigDecimal(b)** ) val a = sparkThings.spark.sql("SELECT myDecimalOne + myDecimalTwo, udfSuma(myDecimalOne , myDecimalTwo) FROM myTable").head &amp;#x200B; Thanks in advance. &amp;#x200B;
&gt; group 0 is placed into the left side of either** &gt; &gt; group 3 is placed into the right side of either** But if a regex matches then you're gonna have both groups, aren't you? How does that work? The whole point of the standard functional programming "either" is that it's left or right but never both, if this thing doesn't behave like that then it should definitely be called something different. &gt; If regex was safer in Java it would be much easier to write explicit code, so I had to do some "save my ass" error handling. In that case you should encapsulate that into a separate safe wrapper around Java regex. Keep the pure-functional business logic separate from the messy interfacing with external libraries.
\&gt;But if a regex matches then you're gonna have both groups, aren't you? &amp;#x200B; it can hold only one group (it determined by my regex, but I haven't covered it fully with tests, so there's maybe arise some mistakes and it can hold both) &amp;#x200B; \&gt;The whole point of the standard functional programming "either" is that it's left or right but never both, if this thing doesn't behave like that then it should definitely be called something different. I wonder about this too, it was some strange third party library but in my case it worked out perfectly 
Yep when you create an instance of `Endpoint` it infers the type A from the Action, moreover it finds the right implicit `Reads[A]` for it (at compile time) and makes that accessible at runtime when `.as[A]` is called. If you try to pass it something that doesn't have a matching implicit Reads in the current scope it won't compile.
One diff commit per upgrade, if that.
They are referring to scala-native which is a native compiler for Scala. Graal is a ahead-of-time JVM.
Ah, right. Using these laws as specified you would not be able to correctly reason about more complex expressions. For instance, you'd conclude that: `save(e) &gt;&gt; known(e) &gt;&gt; save(e) &lt;-&gt; pure(true) &gt;&gt; save(e) &lt;-&gt; save(e)` , whereas it should lead to: `save(e) &gt;&gt; pure(true) &gt;&gt; save(e) &lt;-&gt; save(e) &gt;&gt; save(e) &lt;-&gt; save(e) &gt;&gt; pure(Left(Error))` (given that double save is *not* just pure error, it's the effect of save plus error). Does it look ok now? I think I will rewrite this part with due credit. Thanks
I don't know what I'm talking about, but I'd guess that if your SQL is using JDBC underneath, then the values you're pulling out of myTable are being mapped from a SQL number type to java BigDecimals. The implicit conversion from java to scala bd is at compile time, so this is not being done based on your runtime query. I'd look for a spark configuration somewhere where you can change the type mappings, or else just use your second code snippet (it may be handy to `import java.math.{BigDecimal =&gt; JBigDecimal}` to cut down on typing.
It'll lead a lot of noise in git blame, is my point.
I'd doubt a list is the most efficient collection to turn all these results into. What are your other options?
Where it the bottleneck? Did you profile?
Git blame noise is a fact of life 🤷‍♂️
Yes, it is very relevant and expresses the same concerns I had. Thank you.
I need to get all these results into a collection that can be converted to JSON and sent to a front-end.
It is, without a doubt, at the `toList` stage. For example, I can get all the vertices by issuing: graph .V(vertex1, vertex2) .out("parent-&gt;child") .dedup() .next(5000) // Pass in a number larger than the number of vertices in the graph to ensure // that all the nodes are retrieved. &amp;#x200B; Replacing with `next` speeds up significantly. But the question remains--how do I retrieve the properties associated with each vertex? One way is, if all the vertex ids are unique I can cache the data associated with each vertex id in a map. A simple lookup table. But I don't want to increase complexity to the system like this unless I'm sure this is the only way. &amp;#x200B;
Thanks again for the help!
I see you want to forcefully evaluate the the query into a collection, by why a `List` specifically? Why not a `Vector`, `Array` or something which that can be built in constant space and much less overhead per element than `List`.
Beautiful! I have an on going project where I'm ambivalent about going full `cats-effects`. That settles it.
Not where I'm from, but "okay", I guess... We do keep a *very* clean git history graph with ff-only merges, but maybe you don't find value in that. (Which is fine, it's just not workable for us to have a messed up blame/history.) Honestly, the the git noise isn't even the biggest problem with scalafmt in general[1], it's the fact that any mostly trivial refactor of cross-cutting code can cause a huge cascade of merge conflicts because of the formatter applying a full reflowing of huge chunks of the code. FWIW, I really *wanted* to use scalafmt, but reflowing code because it broke some arbitrary N column barrier is a deal-breaker because of the conflicts. (You could argue that this is a problem with git not being able to handle/recognize 'reflowing' of Scala code, but realistically that's not going to change any time soon.) [1] It *would* be a problem with just 'keeping up with the defaults', at least for my dev shop.
Well the reason for that is because `toList` is a function that is available through the JanusGraph API. I don't see a `toVector` or `toArray`.
I like a clean git history. I rebase all my stuff and try to avoid merge commits whenever possible. The reality is most workplaces don't do that. They end up with huge messes. You see auto-format as not worth the costs, I get it. Personally I see it as worth the cost of reading a few more diff lines to have a consistent code style throughout the codebase. The 'biggest problem' you point with scalafmt isn't even a scalafmt problem, really. It's just a general problem with a reflowing formatter and plenty of languages have those nowadays so programmers can avoid arguing about style and just get on with it.
It's not about reading diff lines. It's about huge MERGE CONFLICTS which git cannot handle.
/u/p220, just finished the profiling. The property map (`valueMap`) is taking _so_ long. 
They switched off for editorial reasons? Not technical? What’s up with that?
&gt; After this, the Guardian’s migration to AWS became that bit more urgent. We decided to purchase [OpsManager](https://www.mongodb.com/products/ops-manager) – Mongo’s database management software – along with a Mongo support contract – to help with the cloud migration. We used OpsManager to manage backups, handle orchestration and provide monitoring for our database cluster. &amp;#x200B; Here's where the "we're not telling the whole story" starts. OpsManager is included with a support contract. Why? Because a support contract automatically includes MongoDB Enterprise Advanced Server. In fact, part of the onboarding involves sending a link to download the server as well as set up your OpsManager account. &amp;#x200B; &gt;Due to editorial requirements, we needed to run the database cluster and OpsManager on our own infrastructure in AWS rather than using Mongo’s managed database offering. This was non-trivial, as Mongo didn’t provide any tooling for getting set up easily on AWS &amp;#x200B; On-prem OpsManager is a nightmare. For this reason it's only pseudo-supported. There are instructions, and people do it, but not one of them gets the benefit from it that they should. &amp;#x200B; &gt;Each of the issues could warrant a whole blog post in themselves, but the general take away points were: &gt; &gt;Clocks are important – don’t lock down your VPC so much that NTP stops working. Clocks are indeed important. So important there's a log message that fires off as soon as clocks begin to diverge. And it keeps firing off until you fix it. &gt;Automatically generating database indexes on application startup is probably a bad idea. Yep. And support recommends turning that off/not doing it. &gt;Database management is important and hard – and we’d rather not be doing it ourselves. Yep, that's why OpsManager exists. &amp;#x200B; This is a case study in "decisions made because admitting our mistakes is hard." MongoDB has warts, definitely. But The Guardian's use case is almost exactly tailored to MongoDB's strengths. That the transition then encountered so many problems leads me to believe there's a LOT more going on than a simple "this technology doesn't work for us at all." &amp;#x200B; Sources: Worked for quite some time at MongoDB support. Saw many instances of issues just like the article reports. Encountered many "but it must be your fault" responses. &amp;#x200B; Secondary source: worked with the OpsManager team supporting the product. Would not personally recommend the product, but there's literally nothing else. &amp;#x200B;
Were these asked? Maybe post answers?
The exhaustive match feature only works on sealed traits I believe. The word sealed means that you cannot extend the trait outside the file, therefore, the compiler knows all the possible match scenarios. I don't know what type A is but it may just be a non-sealed trait.
Thank you so much!
\&gt; About using ReaderT for kinda dependency injection through single case class - not exactly possible as the constraint is to have a fine grain control over effects subsets. Apologies for replying so late. I think maybe I made a confusing criticism. I'll try to rephrase. Usually, people start using Monad Transformers fairly soon after encountering their first computations that involve several stacks of effect. The solution however fails to scale with additional effects, so the natural way of progressing is going from Monad Transformers to MTLs, which is a set of typeclasses that abstract over the various effects provided by a bunch of monads and their transformers. I assume you know all this, but in terms of cats , it goes like : * Either becomes Applicative/Monad Error * State becomes MonadState * Option becomes FunctorFilter * Reader becomes ApplicativeAsk * ect When you say in the article : &gt; MTL is cumbersome - for any effect we should define an extra monad transformer. &amp;#x200B; you're making things confusing for the reader : you haven't even expressed what MTL is wrt Monad Transformers. And the thing is that the MTL typeclasses have more implementations than just the datatypes/transformers they are inspired from. In particular, ApplicativeAsk have more implementations than Reader. For instance, because typeclass instances are mere values, if you have an \`implicit val a : A\`, it's trivial to have \`ApplicativeAsk\[F, A\]\` for any Applicative F. There's no transformer involved, it's simple typeclass instantiation. Similarly, if you can have a pure \`A =&gt; B\` function (which is the case if A is a case class containing a field B), you can derive an \`ApplicativeAsk\[F, B\]\` from \`ApplicativeAsk\[F, A\]\`. So you can have a fine control and declare that a given function depends on B and call it from a site where you have access to a dependency on A. &amp;#x200B; You can even come up with ways to do this derivation automatically. I suggest you have a look at : [https://github.com/oleg-py/meow-mtl](https://github.com/oleg-py/meow-mtl) &amp;#x200B; &gt;... defining fine grained algebras and building kinda bottom-up dsl for your program What I'm saying is : the goal of your article is great, but I think your argumentation needs revising...
Did you surpress any warnings? Just ran the code snippet in scastie and the warning appears. https://scastie.scala-lang.org/zHopHqPkQNqsg5T0Bkr4ew
Just IMAGINE. The AUDACITY! I think you should unfriend /u/ScottSkillsMatter on MySpace and complain about Skills Matter to your therapist.
Just opened an issue for DoS case in Circe: https://github.com/circe/circe/issues/1040 Will anybody request a CVE number for it?
A law that lies to you sometimes is not a law. Similarly, I think it's important to not call something that depends on the network a law. s/law/test and I'll agree with you. Minor tidbit, but it feels like a misnomer to call something that may fail for many reasons a "law".
Do you have control over the source data-structure? What I found to be VERY fast for traversing graphs is this: \- Store your Vertices in an Array, so they have an Index. \- Have one mutable.HashMap to for looking up the array-index by VertexId \- Translate all your Edges into an interleaved Array\[Int\]. So that every two array entries represent an edge consisting of source-vertex-index and target-vertex-index. \- Then build up a neighbourhood-structure by iterating over the edge-Array TWICE. Yes, iterating twice is faster, because you completely avoid reallocating arrays. 
&gt;But The Guardian's use case is almost exactly tailored to MongoDB's strengths. Can you give a bit more insight into this?
Good article. All that time and effort really highlights the cost of making a bad initial decision.
I'm not aware of any. TBH I'm not sure how much in-depth detail there is to go into? Package libraries the normal way. Package applications as shaded jars (or if you need something specific to your deployment model then do that). Use your build tool's release plugin to do releases. Have a CI build and require it to pass before allowing merges to master.
Thanks, it is a spark SQL, no jdbc. We cannot find the mapping config :(. We've discussed the JBigDecimal alias but some members of the team feel this creates a bit of confusion. Thanks again for your help. 
In swiss universities, Prof. is a title.
I don't see how this is relayed to Scala. Should the windows sandbox news be a post too? Because I can now run Java stuff in the VM?
Read the lagom quickstart/tutorial and write up an event driven domain. Lots of good explanations there
`a` is a `List` though, so it doesn’t matter what’s inside it. 
Sure, although the Guardian did a good job of that already [here](https://www.mongodb.com/customers/guardian). But that’s all marketing speak, really. Here’s my take. Note these assume they’re using MMAPv1, which is deprecated in 4.0 but was the only beast around in 2011: - Guardian has far more reads per document than writes. Mongo excels at this workload, even outside of a clustered config. - if you need to scale up for increased load, you really can just add machines. Yes, you need to know what you’re doing, but it’s not a heavy lift by far. - with proper planning, mongo will almost always reuse a data record on updates. And by reuse I don’t mean “keep the same if or row”. I mean it will reuse the physical disk space. - when properly organized, event streams (such as comments on blog posts) are extremely fast both in write and read. - in a non-deleting setup like this, mongo never needs to update data tables, so maintenance over is small. Note that index will still need to be updated, so it’s not a huge win. None of this applies for a WiredTiger setup. WiredTiger is much more RDBMS-like. 
Find a problem that you actually have. Some number that you want to calculate. Some task that you want your computer to do 10 times over and don't want to have to click through each time. The most important part of programming is making programs that are actually useful; everything else will follow from that.
I hope you're joking... :D
Thanks so much for the detailed response! However, in another comment, I did a profile realizing that the \`valueMap\` is what's causing the slow down. From a graph traversal perspective, janus is crushing it at around 226ms (\~quarter second).
Meanwhile, I thought of something like refactoring every bit into small helper functions, yet, is it not too much? ``` class metricNames { val builder: ListMetricsRequest.Builder = ListMetricsRequest.builder() def buildRequest(token: Option[String], namespace: String): ListMetricsRequest = { if (token.isEmpty) builder.namespace(namespace).build() else builder.namespace(namespace).nextToken(token.get).build() } def processResponse(response: ListMetricsResponse): List[String] = { response.metrics.asScala.map(_.metricName).toList } def makeRequest(cw: CloudWatchClient, request: ListMetricsRequest): ListMetricsResponse = { cw.listMetrics(request) } def getData(cw: CloudWatchClient, namespace: String): List[String] = { @tailrec def loop(token: Option[String], prev: List[String]): List[String] = { val request = buildRequest(token, namespace) val response = makeRequest(cw, request) if (Option(response.nextToken).isEmpty) prev ++ processResponse(response) else loop(Option(response.nextToken), prev ++ processResponse(response)) } loop(None, Nil) } } ``` Here `buildRequest`, `processResponse` are trivialy testable, and `CloudWatchClient` may be extended with some `CWCMock` class, that will implement `listMetrics` method. Yet it seems I'm implementing some testing framework over again.
PlaceIQ | Senior Software Engineer - Data | New York, NY, USA | ONSITE FULLTIME PlaceIQ is looking for experienced Senior Software Engineers with Hadoop, Scala and Spark to join our team at our NYC headquarters. At PlaceIQ, we empower our team members to extend beyond traditional functional boundaries so that we can build game changing products. As a member of our engineering team, you will own the design, development and integration of the next generation Location Analytics Platform. If you are looking to join an exciting and growing team at PlaceIQ, take a look at our official job description and apply now! PlaceIQ has been voted Crain’s Best Place to Work in NYC for the 5th year straight! [http://app.jobvite.com/m?3gPehkwp](http://app.jobvite.com/m?3gPehkwp)
I really really hope he is joking
You got it backwards, Scala is the gateway drug to Haskell.
I copy pasted your code into a worksheet and definitely get errors. Are you sure you're looking for compiler error output? I see it in a tab labeled "Worksheet errors".
Here's something interesting to mull over: their *initial* decision was to use Oracle. Then they migrated to MongoDB. Then they migrated to PG. &amp;#x200B; \- Time on Oracle: 11 years (1999ish- March2011) \- Time on MongoDB: 6.5 years (2011-2018) \- Time on PG: ??? &amp;#x200B; I'd posit that their problem isn't the decision. It's something deeper, like not-invented-here syndrome. &amp;#x200B; Source: [The slidshow they used when they moved to Mongo](https://www.slideshare.net/tackers/why-we-chose-mongodb-for-guardiancouk)
Thanks for the insight.
It depends. If you have a strong background in Java, you are way ahead. The Odersky book is a must-read, and you pretty much need to know everything in that book. There are some good videos on YouTube that explain some of the philosophy of the language, and you will need to internalize that to produce idiomatic Scala. If it isn't sticking, maybe one of the on-line courses from Coursera or others, ones that include an introduction and a project? Scala is not Python. The learning curve is pretty steep. Everyone I know who works with it, including myself, had to put in significant study and then significant work time before we felt comfortable with it.
Btw, likely you want None rather than Some(Nil)
Thank you for this! I love the interactive notebook programming style, especially for learning, and have been wanting to dig into Scala more.
Scala worksheets are pretty bad, you'll be better off making an extra Main module and putting the code in the entry point
How do you do "real-time" logging (e.g. to log the program state) in a functional way using Cats ? So far I've wrapper "logger.info(...)" into an IO, is there a better way ? I know about the Writer monad but that doesn't fit my need since I don't want to treat the result of my thread after its execution. &amp;#x200B; Thanks :)
It's not that bad :-) No _law_ depends on network (or anything external) as every _law_ abstracts over effect type. What it can only do is to demand that the said effect has certain properties eg. be a Monad or Applicative etc. You might be arguing that some concrete implementation lies to you about being _lawful_ because it, for instance, depends on network. More precisely, this implementation would usually depend on something that, in your eyes, lies about being a Monad (for instance you can't really show that `fa.flatMap(a =&gt; F.pure(f(a))) &lt;-&gt; fa.map(f)` for `DBIO` because you could observe some glitch during evaluation of both sides). While this would be a correct argument in my eyes, I can't help but notice that you would be able to disprove almost everything using this argumentation. What matters here is whether it is an acceptable way of writing tests, that is, if tests generated from these laws can be considered good enough. 
Although this behavior is not what I'd expect - this seems to be what the Spark UDF code does the way it is written (as of Spark 2.2) - [specifically this line](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L334) . When you call the UDF, it looks like this is what happens: 1. you end up in the ScalaUDF class [here](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L113) 2. From there you have the `CatalystTypeConverters.createToScalaConverter` calls [here](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L116) for each of your 2 arguments 3. which takes you [here](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L409) \- `getConverterForType(dataType).toScala` 4. `getConverterForType(dataType)` evaluates to [DecimalConverter](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L318) as per [this](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L65) 5. and the [toScala method on DecimalConverter](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L332) strangely (at least to me) calls [catalystValue.toJavaBigDecimal](https://github.com/apache/spark/blob/fa0d4bf69929c5acd676d602e758a969713d19d8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L334) Now, I'm not sure if this is by design or a bug. Also, not sure if this actually helps you :D
It looks like IDEA did not properly import the project. In your screenshot it is visible the app directory is not marked as a source root. What I find always works is after "sbt new" in IDEA I go File -&gt; Open -&gt; navigate to build.sbt file and select that. You should see [this prompt](https://imgur.com/a/r4kiozq). Select "Open as project" which leads you to [this screen](https://imgur.com/a/jI83a0F). Make sure the options "Use sbt shell" for imports and for builds are both checked. By default they are not. Click OK and after IDEA settles (it is known to take a while to do the initial import) you should see something [like this](https://imgur.com/a/VAyD906). Notice that the "app" directory is marked blue which means it is treated as a source root, which gives different options for the "New" menu. &amp;#x200B;
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/8CXQBQM.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ec8gcjk) 
As for examples, have you looked at https://github.com/playframework/play-scala-starter-example so far? It has a complete Play app showing off a few concepts to give you a feel for how things are implemented. Please also feel free to ask lots of detailed, specific questions. People here will be happy to help.
[removed]
This is awesome. Thanks for sharing! 
Yeah something is not right, because it should look like this: https://imgur.com/eTg1vrg Try and reopen the project by opening the .sbt file.
Hi. Yeah, sbt is a bear since it helps that you already know Scala if you want to use it properly. :( Have you looked at Lightbend's activator? [https://www.lightbend.com/blog/typesafe-activator-what-is-it](https://www.lightbend.com/blog/typesafe-activator-what-is-it) It's not a bad tool for getting yourself bootstrapped in the beginning. Unfortunately, it doesn't really help with IntelliJ integration, but if you want a quick &amp; dirty way to get yourself going with Play apps, it's decent. Regarding sbt, assuming that you use the IntelliJ sbt shell, make sure that you give it plenty of memory. I run mine with an 8GB heap -- it doesn't normally use this much but the compiler really thrashes the JVM garbage collector and so it helps to have a large heap for those peak moments. Happy to share more detail with you, i.e. specific settings. Let me know. &amp;#x200B; &amp;#x200B;
Why can't you just sign up and watch the videos?
Which bank do you work for, if you don't mind disclosing?
That worked, thank you so much! 
You should start from https://www.playframework.com/getting-started and download one of the applications from there. 
Activator has been discontinued for a long time now. The closest thing to it is the [Lightbend Tech Hub](https://developer.lightbend.com/start/?group=play).
There's a discussion about Play JSON conversion in the [REST API Guide](https://developer.lightbend.com/guides/play-rest-api/part-1/index.html) if that helps.
I don't mean to be rude but this isn't helpful at all. I feel like it should be quite clear from my post that I understand the basics laid out in this link. 
Very interesting. Let me see if I understood it, this framework allows the teams to assess the quality of their tests by mutating the real code and running the tests aganist the mutated code. &amp;#x200B;
&gt; Also, note that the "source" data structure is just the vertex. I don't really have any control over that. (Unless I'm misinterpreting your question. Which is very possible.) What I was asking is if you have one fixed (never changing) graph and do a lot of traversals, or if the graph different for every traversal. Building up the indexing structure with NestedArray is maybe not worth it, if you are only doing one traversal. I do not know your exact use-case so my recommendations might not be suitable. &gt; The way I was thinking about doing this is...since all the vertex ids are unique is to create a key/lookup of vertex id -&gt; properties. Then use janus (which is fast) to get the vertex ids and then just do a fast lookup (single array) on the properties based on that id. Keep in Mind that Map lookups are WAY slower than array index lookups. (http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html#lookup-performance) So if you can avoid maps in favor of arrays, this will improve your performance. Also note that mutable.HashMap lookups and constructions are a bit faster than immutable.Map. Another trick: If want to work with a set of vertices, where each vertex has an index: Instead of a Set[Int] or Set[Vertex] you can use an array of integers with the same size of the vertices array. Then at each vertex-index you store either a 1 or a 0 depending on wether the vertex is in the set or not. This is more efficient than hashset-based deduplication. For this you can use the ArraySet of my flatland-library.
Absolutely right. I meant to use an ArrayIndex lookup. Probably offset by # of fields (as you had suggested with interleaved arrays). 
I know others are giving you great tips for getting started with play, but if you understand generic scala, you might have an easier time grokking a framework like http4s: [https://http4s.org/](https://http4s.org/). Since you come from OCAML, play probably will fit in better with your background, but just thought you might look into a framework you have more control over than play.
That is very correct :) So basically the mutated code consists of "bugs" in that sense so your tests should pick that up of course :)
TLDR: you should use case classes. a case class is considered a functionally pure datatype, in my opinion at least. That being said you shouldn't put anything mutable inside it (in other words it doesn't magically make something immutable). In my opinion, idiomatic scala code should rely on case classes and sealed traits in the overwhelming majority of cases. the regular \`class\` keyword is primarily used for dependency injection (like say you had a UserRepo that required access to a database object, UserRepo would not be a case class because a database object is a mutable connection to a database. Therefore you could use a \`class\` in this case and pass the database object in as a constructor argument). &amp;#x200B; That being said, the purely functional camp would probably suggest using monads/and-or streams to represent your game loop. &amp;#x200B; &amp;#x200B;
&gt; is there even a difference in Scala? No. &gt; I think that I can still write functional code using objects as long as I have no state and I don't edit the board every game tick, but rather redraw it every time, is this correct? Yes. /thread
Some advice: - Use case classes and stick with Immutability - Extract behavior to typeclasses - Use a library like Cats or Scalaz for advanced fp stuff - You might look into State Monad - Lenses / Optics might help you to immutable mutate parts of the game state 
I looked into Scalaz, but I'm not sure I really understand it. I thought Scala is already functional, how does Scalaz make it more functional? What is the purpose of the framework as opposed to just using the functional aspects built into the language? After reading these responses, I think I need to become more comfortable with Scala (book or youtube tutorials) before I just dive in and learn by doing. There's a lot of concepts, like case classes, monads, etc, which I've read up on and understand, but not very deeply at all. I'm really excited to start this project and get better at functional programming (and Scala itself), but there's a lot stuff that is kind of overwhelming and I'm not sure I know how to start. If I were to do this in any OO language, I would know immediately how to start, but in Scala I'm not so sure, and I don't want to just dive in and create a clusterfuck of spaghetti code lol.
The way I understand it, specifically the mutation is done to branches—in other words, if the ‘mutator’ sees an ‘if’ or ‘match’ expression it replaces it with unconditional code paths to each of the branches and runs all the tests against each of those mutations. This is definitely thorough but from what I understand, very slow.
As Scala is a functional language, we chose to focus on mutating functions first. For example, a `filter` to `filterNot`. Among other types of mutations. You are correct that it can be very slow, as your tests have to be run for every mutation. But there are optimizations like coverage analysis (which we don't use), or mutation switching ([which Stryker4s is built on](https://stryker-mutator.io/blog/2018-10-6/mutation-switching))
First, you may be interested in this writeup: [https://prog21.dadgum.com/23.html](https://prog21.dadgum.com/23.html). It's about exactly what you are doing, (in erlang though it doesn't show much code, so don't worry about that) focusing on general fp techniques. Be sure to read the follow up that is linked in the last part as well. Second: Your data should be represented as case classes or as [contraband classes](http://eed3si9n.com/contraband-an-alternative-to-case-class). These are immutable record types. You use them to store data. You change them using .copy(case classes) or through the generated .with&lt;PropName&gt;. DO NOT put methods in them. &amp;#x200B; Use [typeclasses](https://blog.scalac.io/2017/04/19/typeclasses-in-scala.html) to provide behavior. There are two standard typeclass libraries available, [cats](https://typelevel.org/cats/) and [scalaz](https://scalaz.github.io/7/). Because you are new to scala FP, I reccomend cats -- the docs are more accessible. Scalaz's documentation is good in the scaladoc, but cats's microsite is easier to use. They have a bunch of standard typeclass definitions that you can implement for your data structures: Functor: provides map(container:F\[A\])(f: A =&gt; B) -- take a container, operate on the thing in the container using f, and return another container Semigroup: provides combine(a: A, b: A): A -- take two things of the same type and combine them into one thing of that type Monoid: extends Semigroup and provides empty: A -- adds an empty element that you can use. combine and empty form a monoid. Traverse: foldLeft(a: F\[A\])(z: B)(f: (A, B) =&gt; B) and sequence(a: F\[G\[A\]\]): G\[F\[A\]\] -- foldLeft provides iteration of data structures using fold (there's also foldRight and some special purpose iterations), sequence takes a container containing containers of another type and swaps the order of nesting; Option\[List\[Int\]\] becomes List\[Option\[Int\]\] Apply: extends Functor, provides mapN(a: F\[A\], b: F\[B\], ..., n: F\[N\])(f: (A, B, ..., N) =&gt; F\[B\]):F\[B\] and ap(f:F\[A =&gt; B\])(a: F\[A\]): F\[B\] -- (ap) apply a function in a container to a container to produce a container with the result of applying the function to it, and (mapN) take a variadic number of containers and combine their contents into a container using a function that takes the same number and types of things in the input containers. Applicative: extends Apply, provides pure(a: A):F\[A\] -- put a value into a container/construct a container. FlatMap: extends Applicative, provides flatMap(a: F\[A\])(f: A =&gt; F\[B\]): F\[B\] -- applies a function that produces a value in a container to a container, un-nesting the result. Monad: extends FlatMap, provides iterateUntil(a: F\[A\])(f: A =&gt; Boolean) -- loop over an action until the value in the container is true. Basically, do-while in using stack safe recursion. ApplicativeError: extends Applicative and provides raiseError\[A\](error: E): F\[A\], and handleError\[A, E\](a: F\[A\])(f: E =&gt; A): F\[A\], attempt(f: F\[A\]): F\[Either\[E, A\]\] -- for things that have both a failure state and a successful state, handleError allows you to handle the error state by producing a recovery value, raise error allows you to put a container into a failure state containing the error to be raised, attempt will try to run an action and return the container with an Either with any raised errors as a Left and any successes as a Right . MonadError: extends ApplicativeError and Monad, provides ensure(f: F\[A\])(error: =&gt; E)(pred: A =&gt; Boolean): F\[A\] and rethrow(f:F\[Either\[E, A\]\]): F\[A\] -- ensure turns a success into a failure if the value in the container does not satisfy a predicate, rethrow will turn a successful container containing a left into a failed container. State\[A,B\]: provides a way to store state and return values from functions based on that state -- basically an alternative to passing tuples containing some state A, and producing values B into every function: ((A,B) =&gt; (A, C)) becomes B =&gt; C. Has instances of at least up to Monad. To extract the values of the state and the produced value, call run: (A, B) which returns a tuple where the first member is the accumulated state and the second is the final value of any computations. Cats also extends the standard library Either to include .catchNonFatal, which will catch runtime exceptions on the hunk of code you write and put them into a Left\[Throwable, A\] if an exception is thrown, and a Right\[Throwable, A\] if your code succeeds, where A is the result of the hunk of code enclosed in .catchNonFatal. Monad's iterateUntil is a simple do/while loop. You will probably need more sophisticated loops and synchronization. For that, you will need [cats-effect](https://typelevel.org/cats-effect/) or [scalaz IO](https://scalaz.github.io/scalaz-zio/). For even more sophistication, you can model things as a stream of computations using [Monix](https://monix.io/) or [FS2](https://fs2.io/). Monix and FS2 play nicely with cats. For custom behavior that doesn't fit in the generic stuff in the libraries (pacManIsInvulnerable(game: GameState): Boolean), you can simply create modules (objects) with the appropriate methods if there are no side effects, or write Effect typeclasses using [tagless final](https://blog.scalac.io/exploring-tagless-final.html) if they do some sort of effect (throw exceptions or do disk/network io/thread context switching). You probably won't be doing a TON of that with Pac-Man, as most of the state management is just changing a global data structure, and most of the rendering will be setting properties in scalafx (see below) according to the game state produced in your game loop. For rendering and interaction, use [ScalaFX](http://www.scalafx.org/). With the above, you should be able to store and interact with your state AS IF IT WERE MUTABLE (fs2 Ref, cats-effect MVar, State), and control the timing of your game loop (FS2 scheduler/the scalafx clock), write most of the business logic in simple functions that take in your game state at time T and the interactions from the player at time T and produce a new game state at time T+1, and compute those functions in a game loop (fs2 stream repeat) and draw them to the scalafx window by changing scalafx properties containing your game model (see: [http://www.scalafx.org/docs/properties/](http://www.scalafx.org/docs/properties/)). If you want to run in a browser, use the scalajs versions of the libs and scalajs-react to do your rendering. This should be a fun project. All the libs needed may seem a bit intimidating, so for your first go, just put everything in a mutable IO loop using the Monad.iterateUntil with thread sleeps for your tick. It won't be very performant, but you will be able to start refactoring it to be more maintainable and less god-functiony from there. [Here's a stub for your main](https://gist.github.com/jackcviers/0b52350f7c5a62305414194bb3b2480e). &amp;#x200B; And presto changeo, you're there. This is absolute overkill for a simple game like pac-man (you could write it on one page, probably, using a jpane), and you could just do it in pure scalafx without the functional libs and the standard lib, but if you want to learn the functional part of scala, the above should give you some ideas. The main benefit I see is that all of your state management is separated from the concern of rendering, so you could switch it later, so you could just model everything in tests (the game loop could be a separate class, instantiated and run in main). &amp;#x200B;
Wow! Thank you for all the advice, and even a place to start. I found the same link to the guy making Pacman in Erlang, it was really useful. I recognize this is gross overkill for such a simple game, frankly I wish I had a more practical use for functional programming. I know I can make Pacman in Java/python/etc, it's really just a matter of time. I'm sure it would still teach me and be useful, but I am sort of over engineering the problem to force myself to learn functional programming just as a means to become a better programmer generally. It's definitely a lot to take in, but I am down for the challenge. &amp;#x200B; I already got a simple ScalaFX thing going, so I have that down for display. &amp;#x200B; I'm looking more into typeclasses, you said cats and Scalaz are both standard libraries, so are these 3rd party, or just simply a part of the language? Not that it matters, but I only ask because it seems I can't do fp well without these extra things, but as a novice to Scala, I thought it was just part of the Scala package, because it's a hybrid language. I guess to reword it, I don't fully understand how you can be sure that what you're programming in Scala is actually functional, because you can just throw for loops or something imperative and now the application isn't functional. It's a lot of power to the programmer coding in a hybrid language, so I'm just trying to make sure I understand, and stay strictly or at least primarily functional for the challenge of it. &amp;#x200B; cats-effects and Monix also go over my head. I've read both their pages, and I get the gist, but I don't see why I would need them. Monix's site says: "Monix is a high-performance Scala / Scala.js library for composing asynchronous, event-based programs.". It's clear to me how this relates to my game, it's asynchronous, and an event happens every game tick where new information needs to be fed into a function. What does Monix (or a similar tool) do, that recursion, or some loop doesn't do? Really cats-effects and Monix are the only things that went over my head I think. Once again thank you so much for the comment! Scala is a lot to take in though for sure. I'm 1 semester away from graduating, and odds are I'm sure I'll get a strictly OO job (just based on amount of jobs), but I'm really interested in fp nonetheless. 
Trying to get more familiar with the Scala compiler and work on some easy beginner bugs. Also reading and implementing scalaz for mortals.
Good point, branching logic can be nested deep inside helper functions. It would be quite a task trying to trace control flow back through to our own codebase.
&gt;I'm looking more into typeclasses, you said cats and Scalaz are both standard libraries, so are these 3rd party, or just simply a part of the language? PART 1 Yes, they are third-party, but they provide some common functional patterns not present in the stdlib, and only work with referentially transparent data types -- so you know your code will actually be functional if you use them. With the built-in stdlib, you can run into some weird gotchas that you might not expect. That's also the main purpose of libraries like cats-effect, scalaz io, and Monix. Here's an example below: &amp;#x200B; val f = Future{ Random.nextInt} val x = Await.result(f, Duration.Inf) val y = Await.result(f, Duration.Inf) println(x != y) &amp;#x200B; Really, x and y should be different. However, Future runs its body as soon as it can and caches the value. So you get the same number. If you were to inline f, you get the result you expect: &amp;#x200B; val x = Await.result(Future{ Random.nextInt}, Duration.Inf) val y = Await.result(Future{ Random.nextInt}, Duration.Inf) println(x != y) &amp;#x200B; As you know, since you learned ocaml, for code to be functional both snippets I just pasted are required to behave the same way in order to be functional. If they don't, they are side-effecting and thus imperative. &amp;#x200B; With cats-effect, you switch out Await with unsafeRunSync and Future with IO, and you will get the expected result: &amp;#x200B; val f = IO{ Random.nextInt} val x = f.unsafeRunSync val y = f.unsafeRunSync println(x != y) &amp;#x200B; If you inline f, you still get the expected result: &amp;#x200B; val x = IO{ Random.nextInt}.unsafeRunSync val y = IO{ Random.nextInt}.unsafeRunSync println(x != y) &amp;#x200B; So you get more consistency from a type that is almost the same as scala's standard Future. The same is true of Try, and for that you add \`import cats.syntax.either.\_\` and replace \`Try{something}\` with \`Either.catchNonFatal{something}\`. \&gt; it seems I can't do fp well without these extra things, but as a novice to Scala, I thought it was just part of the Scala package, because it's a hybrid language You can do fp well without these things. But, not everything in the standard library is functional. Lots of stuff is, though! Without cats, you have to learn, on your own, what is and what is not referentially transparent in the std library. You can't rely on the type signatures to help you. You have to actually read the code and test it. But with cats you are guaranteed that you are only using functional data types. You really only need to know 15 methods: combine, empty, map, mapN, pure, flatMap, foldLeft, sequence, catchNonFatal, raiseError, handleError, rethrow, unsafeRunSync, iterateUntil, IO. That's not that much to memorize. You can look up pretty much everything else you'll use with the apis after you know these. You can learn 15 methods in a weekend. So it is a small, but powerful subset that helps keep you safe while you are learning. The hybrid language part of scala is great! When I need to make something really fast by using a mutable data type in a hot loop, I can. No hoops are necessary to jump through. Modules are first-class citizens in the language. Typeclasses can be combined with other typeclasses using \`with.\` I can talk to Java libraries without foreign function interfaces, and java libraries can talk to scala libs. It's extremely powerful. But when you are starting out, you don't need power. You need consistency and conciseness. You get that with cats-effect and cats. The libraries are much smaller than the standard library. When people talk about the complexity of cats and cats-effect, they are talking about the complexity of \*how they are built and how hard the source code is to read without knowing all the features of scala, not how ergonomic the actual interfaces are or how consistent their behavior is\*. Here's the type signature of Functor's main method: **map**\[A, B\](fa: F\[A\])(f: (A) ⇒ B): F\[B\] It's easy to read, outside of, maybe, the F paramater. What does F mean? Whenever you see something like that, it's handy to replace F with List when you are starting: **map**\[A, B\](fa: List\[A\])(f: (A) ⇒ B): List\[B\] Then to replace it with Option: **map**\[A, B\](fa: Option\[A\])(f: (A) ⇒ B): Option\[B\] Then to replace it with Future: **map**\[A, B\](fa: Future\[A\])(f: (A) ⇒ B): Future\[B\] &amp;#x200B;
PART 2 Now you should get what F is: It's a container. Map takes a container, and returns a container of the same type. What's more, instead of an interface for Future, List, and Option that is different, Functor is \*one interface for all things mappable\*. So you don't need to memorize List and Option and Future to use map. You just need to know map. The interfaces are also small, so are easy to memorize, so you can restrict yourself to just including what you need to implement your method/function later, but when you are starting out it is helpful to just stick to importing all of cats and letting the ide help you. When you learn how to restrict yourself later, you can do that and just change your imports and type signatures and not have to change your implementation at all. That's more difficult to do with the standard library, as there are so many traits for everything, and they aren't broken down into easily memorizable interfaces. That's somewhat by necessity, since they work with parallel and mutable types as well as immutable ones, but it is a barrier to learning them. And god help you if you ever want to implement your own TraversableLike from the standard library (things that can be mapped over). With cats, if you want to implement map for your own data type, you implement only one interface, Functor, and only one method in that interface, map. However, you can do FP without these libraries. You will probably end up doing very similar patterns to what is in the library, or throw up in frustration when you run into typechecker problems while composing two things that just won't compose when using vanilla fp. Plus, when you are learning, it's helpful to have common code that others have tested and used to lean on. So you should use the std lib or cats. Cats will restrict you to the smallest possible footprint of scala to learn to be productive. Especially if you stick to the basic 15 methods I told you about above. Later you can branch out, but by that time you'll already have a grasp of the syntax and the most scala of scala idioms, and the other parts of scala that you learn will be easier to understand and you will also know when you absolutely must reach for that more powerful thing you read about on the interwebs. \&gt; What does Monix (or a similar tool) do, that recursion, or some loop doesn't do? Nothing. It just does them in a standardized, optimized, tested, and reusable way. The question when using a tool like Monix or FS2 is: what goal am I trying to solve? Am I trying to learn how to write my own event-driven recursive tool? Or am I trying to learn how to use recursion to solve a more general problem in my domain of choice? If you aren't trying to learn recursion, you should choose a tool that already implements recursion. If you aren't trying to learn event-driven programming, you should use a tool that already implements the basic parts of event-driven architecture. You can learn recursion by implementing a recursion library to build stuff with, but now you have to learn two things -- how to do raw recursion and how to use the library you have created. You can remain productive at your domain problem, and once you see how an industrial strength library handles the problem you can then go back and build your own from first principles with the additional knowledge of how it might behave by comparing it to the library you already know. You can learn from other's mistakes that way too. There will be more code written against the ecosystem library than in your from scratch implementation. \&gt; I'm 1 semester away from graduating, and odds are I'm sure I'll get a strictly OO job Probably. But learning these things (especially typeclasses) will make you a better OO programmer. Typeclasses are more SOLID than vanilla class heirarchies. They miss encapsulation. But they are definitely a key to understanding how to extend and maintain objects and types from libraries you didn't write without having to extend classes you either cannot (because final) or are externally maintained. They aren't an FP thing. They are a code organization thing. &amp;#x200B; Hope all of this helped, and if you have any more questions, or I wasn't clear, feel free to ask.
I'm sure you understand this to some extent, but you're in for an uphill battle. There's a reason why game programmers are stuck with OOP: it's incredibly well suited to the problem. Game programming is inherently stateful. Not just stateful...mind bogglingly stateful. And if you don't understand that that means mutable, you're gonna experience a lot of pain. Functional programming has tons of applicability in lots of domains. But while stateful programming *can* be used for stateful programming, it is incredibly clunky doing so. For the most part, you'll be "mutating" state by functional processing of immutable data in purely functional containers like monads, which have extreme restrictions on how they are operated upon in order to maintain their functional purity. I have been using functional programming from the very beginning...every language I've ever used was a functional programming language. One day at work I decided that I needed to do an industrial simulation of pick path optimization in fulfillment warehouses. Simulations, much like game programming, are incredibly stateful programs. Being the functional programmer, I used every trick I could muster up...monads, actors, whatever. It took me a couple of weeks to come up with a working prototype. A few nagging bugs got the best of me so I took my program to a principal engineer I worked with. He couldn't understand my code, so he rewrote it in pure OOP...in 1 hour. It was then that it clicked: if you're manipulating stateful objects en masse, *OOP was basically created for you*. Now if you're just doing this as an academic or learning exercise, good luck. It's going to be a challenging problem and you'll learn a lot. But understand that you're trying to fit a square peg into a round hole, and don't be surprised when the results aren't pretty. 
Yeah I'm definitely under the impression, if I want to go through with this, it's going to be an over-engineered piece of garbage odds are. Thing is, I already know I can do this in OOP, so there's not much point of me doing a project this simple in a style of programming I'm already familiar with. After my programming languages class where we learned OCaml, I decided I wanted to get better at fp because it's such a struggle beginning with it, but it's very satisfying and I think will lead me to write better code in the future. This is definitely a learning exercise for me, I recognize this isn't the best tool for the job. ' &amp;#x200B; In a perfect world, I would think of a project that actually lends itself to fp, but thinking of ideas is not my forte haha. I think I'm going to try to go through with this, but really at the end of the day I agree with you. Games are stateful, and there's going to be a bunch of weird work-arounds doing it functionally. I'm not sure exactly what I want to do, because the whole project in mind is really just a means to practice fp, I can't say I'm that passionate about Pacman specifically, but I'm not sure what else to create. I might be biting off more than I can chew, and really just using the wrong tool for the job, but I am going to try to go through with it just for the learning exercise. &amp;#x200B; I agree with you though, this all seems over-complicated, and I'm a bit overwhelmed. Odds are my first job is going to be strictly OOP too, I just want to do this for myself you know?
I've tried to like and use Play. I really have. My initial forays into Scala were with Play. And I've pushed multiple projects into production on Play. I can honestly say that I've tried and used Play successfully. And yet every time I try to use Play I feel like I get bit. It's the degree frustration that I just can't get past. And yes, some of the issues I'm sure are not Play's fault. But this last time...trying to get guice to work sanely...well that was enough of that. I give up. If the Play framework, and the templates built on there-on, can't actually be productive to work with, then I give up. So I'm using http4s for my most recent side project. And I'm very likely to ship http4s into production. And I've yet to run into a single issue that made me feel like I was walking through a fireworks factory the way that Play did. 
It's the dependencies, reliance upon convention, runtime di (yes I know that compile-time di is possible, it's the way I used it) the configuration overhead that gets you. http4s/doobie/bindings.scala is a better stack, and reactivemongo if you need mongo (though it brings akka with it but config overhead isn'tas bad, ugh) is way easier to control and scale, but requires you to know some fp.
I wrote this a *really* long time ago, and its only 80% finished(code works but my tutorial isn’t finished) but it should help get you pointed in the right direction: https://github.com/vmarquez/PureBomberMan/blob/master/README.md
I understand that this mutation check it is not necessary for every build. Just perform a periodic check every sprint to add technical debt. For example my project right now takes 25 minutes to perform the build with all the rest suite. If we add every mutation times can skyrocket. 
One of the main retail banks in UK. Most of them are doing the same, moving the BI solutions to Big Data to reduce costs. 
This video is a great start to begin learning how to handle stuff like this in a functional way in Scala: https://www.youtube.com/watch?v=sxudIMiOo68
Quill is pretty decent for Cassandra 
Is there a good constraint programming library in scala that is easily usable with a simple import via sbt ? I found some searching by myself, but I can't seem to find a simple one to add ass a libraryDependency
Will this make my program run? Asking for a friend 
Monads and streams do not exclude case classes. For instance if you use the a state monad the 'S' could be a case class. Bottom line is, whatever you want to do, a case class ain't wrong. 
For the record, my GA library has dibs on the name "scalapagos".
For the record I'm calling dibs on "scalapagos" for when my GA library is ready for primetime 
Regular classes and traits are great for enforcing invariants and decoupling the interface from the implementation. For example: case class ReducedRational(numerator: Int, denominator: Int) ReducedRational(6, 8).numerator // =&gt; 6 With a regular class, you can enforce that the rational is reduced: class ReducedRational(numerator: Int, denominator: Int) { val numerator: Int = ... val denominator: Int = ... } ReducedRational(6, 8).numerator // =&gt; 3
Your reasoning process is pretty good, that's basically how ReactJS, Elm, and other functional-style single-page app libraries work. They call this 'uni-directional data flow', i.e. you provide a pure data model (e.g. `val count: Int = 0`), you provide a pure rendering function that converts the data into a view (e.g. `def render(count: Int): Button = Button(text = s"Clicked $count times", onClick = incrementCount)`), and pure event handlers (e.g. `def incrementCount(count: Int): Int = count + 1`) embedded in the view that return an updated model. The render function doesn't care about efficiency; it just draws an entire view from an entire model. The event handlers don't care about efficiency either; they return an entire updated model (of course, with case classes and immutable updates, the actual update is quite efficient). Also, they're all obviously pure functions. The library code that _runs_ the render and event handlers, though, does care about efficiency; it compares the old rendered view with the new one, calculates a _diff_ between the two, and _patches_ the old one with the diff to update it to the new one. It's also not pure-functional because it manages state internally. The important thing though is that the impurity is pushed outside the _application_ code and down into the _library_ code. Having said that, I don't know of any GUI library for Scala that actually implements uni-directional data flow like this. ScalaFX, which is an opinionated wrapper for JavaFX, has a two-way binding model, and so does Binding.scala. There is of course `scalajs-react` which is an opinionated wrapper for ReactJS, but then you'd have to use Scala.js, and that would be another thing you'd have to learn. Uni-directional data flow for JVM game/GUI programming is certainly a very interesting area to explore, but right now the support isn't there in Scala.
&gt; I can't find good examples of apps that use play online I have some open source projects that hopefully can help: - https://cryptocoinalerts.net/ -&gt; https://github.com/AlexITC/crypto-coin-alerts - https://xsnexplorer.io/ -&gt; https://github.com/X9Developers/block-explorer
Unit tests for your unit tests. Programmers are hilarious :')
Looks to me like `Cofree[Box, Cat]`
I just want to never use sbt ever again. 
Or, approximately 60% want more language features and better tooling compared to 30% wanting a faster compiler. Also true. We can make many enjoyable narratives consistent with this data.
still don't get it 
'Most people want more language features and better tooling' can be true but it isn't really a thematically consistent narrative compared to 'faster and better tooling'. Anyway from the wording of the survey question it seems like it was a single-choice question only. I'd agree that a multi-choice or a ranked-choice survey style would have been much better at capturing the respondents' true preferences.
You've gotten the key to journalism
TIL From that twitter there's a new language server called MetaLS
I've put a lot time into gradle for us for local dev and our Jenkins pipeline. No one is using sbt anymore on the team now. There's obviously tradeoffs. But I can't stand sbt. Time well spent 
It has a big learning curve, as it’s just Scala once familiar it becomes really powerful. I heavily use auto plugins now to apply project defaults on multi module projects, custom tasks in Scala run by sbt instead of a mixture of shell scripts etc. Lots of warts however it gives you a great deal of power when you dig in. I used to hate it to but with experience I can see it’s merits. 
I have 0 affiliation with sbt and I would actually recommend mill for new projects because it is even better thought out than sbt even if the support and ecosystem is small. But I was in your boat- hated sbt with passion. Then I read the documentation. Once. Then I tried it on a complex project. Then I failed. Then I read it again, the pre-0.13 version and then the post 0.13 version. I learnt something. Then I tried my project and failed again. So I still hated sbt. But then I realizes that its an amazingly well thought and well-engineered product whose single fault is that all keys are available for all build configs, even when not applicable. Yet, its an amazingly designed system. Then once I let that small amount of love in my heart fade out some hate, that's when the gods were pleased. I had the inspiration to read the docs again. And when I did. I managed to build my project. Somehow. A bit of q hack. But I did. I still love sbt. I hate it too. But I love it too. I will always have fond memories of the first time I realized that a build system can be some amazingly powerful. Even if it has its faults. Kind of like the first time you have sex with a woman. Mine was a sloppy girl who barely did anything but it beats using the hand. So much that i will always be in love with her.
I know how to use sbt, mostly, ...I think; it's a bit hard to judge when one masters sbt because of how complicated it is, but it builds my projects well enough most of the time. But everything I do in sbt feels like duct tape and custom this and custom that in sbt-specific DSL or plugins and so many plugins and so many tabs of docs open and sometimes it won't resolve that funky maven jar's and your build sometimes breaks because some cache somewhere had some old stuff that never got cleaned out and it does all that very slowly. I just want to write scala man. 
I'm trying to migrate to Gradle too. We have a large multi-project build, which is going to be a challenge. Another challenge is that Gradle uses a pretty old version of zinc (the sbt incremental compiler) and misses out on the latest performance improvements. I'm trying to figure out how to upgrade it ( https://github.com/gradle/gradle/issues/2158 ), but the zinc API has substantially changed so still playing around with it...
It's not even about power. It's about speed for me. Startup should be almost instant, resolution should be manually-triggered i.e. it shouldn't happen on every project load, incremental compile should take less than a second, test runs should be automatically incremental and should be less than a second to start running the test. A lot of people are talking about alternative build tools now. Sbt has been great in a lot of ways but it's just too much of a time waste.
Jesus, dude. Is this a programming forum or an erotic fiction forum? Can you tone it down with the comparisons?
I haven't used SBT for over a year now. Work is on Bazel, personal stuff is on Mill. The post-SBT Scala world is pretty awesome; there is nothing from SBT that I miss
I also invoked gods. But you took no objection to the religious imagery not relevant to programming. Beauty is in the eyes of beholder you see. :(
Oh my god. I thought you were a pro user. Why would you use mill and not sbt? Also how big is your bazel firm and how do you like it?
My question is why do people always defend sbt saying that it's powerful? Like what are you comparing it to? Every build tool is extensible at that level. Yes, even maven can use in-project subprojects as plugins. A build tool's goal should be 'build as quickly as possible', not 'be as powerful as possible'. People will find ways to customize their build pipelines to do what they need, but they won't find a way to make the inherently slow internals faster.
We have ton of multi project builds, let me know if you need any help/tips. One our biggest challenges was spark based apps for EMR and spark based internal dependencies for Nexus. Easiest resolution is custom configurations for the spark deps and using fat byte jars on EMR again with custom configurations. Yeah the scala plugin is a bit slower. But I'd rather have slightly slower build times than dealing with sbt.
What size is you're bazel using firm? How do you like bazel? And his do you like mill? 
&gt; Startup should be almost instant, resolution should be manually-triggered i.e. it shouldn't happen on every project load, incremental compile should take less than a second, test runs should be automatically incremental and should be less than a second to start running the test. (this is how Mill works)
He wrote mill so I imagine he loves it :)
As a library author the only reason I use SBT is for cross publishing. At work it's gradle all the way now. We used to have an sbt build and it was painful. That project needs to die.
I want fully working and less verbose gadts and pattern matching that does not box everything, and fast collections, and an easier way of defining type level computations. Oh and a full specification of the implicit resolution algorithm.
Cool 😊
https://data-flair.training/blogs/scala-case-class/
Sbt is actually easy. Read the manual. Read the Api docs. It isn't maven. And maven is just as slow. Sorry, but I'm really tired of people who don't do anything actually difficult in sbt saying that maven is A) easier and B) simple. An sbt build is shorter than an equivalent maven build. An sbt build uses fewer plugins than a maven build if you use testing. Maven phases and task reordering is just as hard as in sbt. Plugins for freaking everything is easier than tasks? Get off my lawn. &gt; sbt new scala/scala-seed.g8 &gt; mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false Really? As for understanding complex builds in maven vs sbt, the first line of this article is "don't." https://blog.rubiconred.com/how-to-use-maven-for-complex-build-processes/ Almost nobody reads the sbt manual. It's like 80 pages. That's because the same nobodies don't really read the maven manual, either. It's like 800 pages. People read the getting started guides and copypasta builds. I've watched over 50 devs that I've trained ignore the part of the reccomendations section I give them that says: read the sbt manual. It's the first line. They don't. Almost nobody really understands how maven internals work ANY BETTER than they do sbt internals. Li and Martin are right. The data structure that describes an sbt build can just be a simple acyclic directed graph with an index pointing to named vertices that execute all the code in their values and their parents' values. The abstraction can be lighter and clearer and less leaky and more ergonomic than in sbt. They actually understand what they are talking about. But that's not maven or gradle. Those tools are just as complex. Users are just familiar with maven and copy and paste more without actually understanding what they are doing in a declarative build than copying and pasting without understanding what they are doing in a task-based build system like make or sbt or gradle. To top it off, the tooling Sam is talking about in his tweet isn't sbt. It's the COMPILER and the overhaul to compiler plugins and phases that makes tools with autocomplete very difficult to maintain. They were already hard to build. But now you will have to essentially create your own copies of the standard compiler phases in order to add additional hooks for a code analysis system. Or you have to basically build your own compiler and guess, and you will have a very hard time with live updates in non-compiling projects. That's risky to do, makes tools very heavyweight, and nobody is sure how it could work. It affects lots of existing tools that actually work, and tries to fake improving baseline compiler performance by just making it difficult to hook into the existing compiler, which, currently, a lot of compiler plugins do at the typer phase and doing more things there other than typing artificially slows everything down. &gt; build as quickly as possible Sorry, but a build should produce a correct build product, and be repeatable. Performance is a non-functional concern. And in sbt, that performance is actually governed by IVY and the general slowness of compiler plugins, macro expansion, and the compiler itself. 
It doesn't have autocomplete, can't have autocomplete, and only works for fully compiling codebases. The demo was promising, but it's inferior to intellij and existing eol ENSIME. We're in trouble, as a community, if we don't have a reliable ide. Nobody new will adopt, and existing users will be cast adrift. I don't use autocomplete to discover apis that much. I use jump to source and inspect type at point and show errors at point and show implicits at point and scala/javadocs. But those won't work either in explorative builds.
Hey. Sorry for lag - was busy this week. This meow-mtl thing looks promising - I will give it a try, thanks! My problem arrised from need to manually instantiate instances (like `ApplicativeAsk`) for my transformers. E.g. I would like to have `MonadDb` for any transformers stack that contains `DbT`. Without something like meow-mtl I need to write it by hand, and in case if my stack changes - I need to write another one. That is quite annoying and indeed can be solved by transformes that are already supported by the library. Like ReaderT that you pointed to - we usually would use `Ask` with it. And Ask already has defined instance for any stack that contains `ReaderT`. We can try to write same recursive definition as Ask has - for any stack with `DbT` to have `MonadDb`. I found that to be not very trivial - and amount of boilerplate also increases in that case. If meow-mtl automatically generates that - than we have no problem with boilerplate. But we still will have performance implications because each `flatMap` will need to unroll whole stack and then roll it back. We indeed can solve it by defining huge reader that will bear our `Db`, `Net` and `Log`. And then instantiate `MonadDb`, `MonadNet` and `MonadLog` for it. And indeed if we take this approach to extreme * simple stack - one monad that can do it all, e.g. IO * define all instances for that stack - `MonadDb`, `MonadNet` and `MonadLog` * functions work with any stack `F[_]` for which desired effects are defined (e.g. `foo` needs `MonadDb` and `MonadLog` We will get tagless final. I am going to write another blog post(or maybe alter existing one). Thank you very much for this conversation - it was very enlightening for me. 
&gt; full specification of the implicit resolution algorithm. There is 9ne in the spec, but you are better off trying to read the compiler. Here's what actually works: 1). Put implicit definitions you write in the companion object of one of the types that the implicit refers to. The compiler will look there last, so it's easy to override in tests. 2). If you don't control (didn't write) any of the method's arguments, make a typeclass that wraps the implicit value type and provide an implementation in its companion object, and assign the method call that provides the value to an implicit val inside your method body: def useConfig(x: String)(implicit configProvider: ConfigProvider[Config]) = { implicit val config = configProvider.config ... } Don't import implicits unless the ones provided by the library you are using can't be received any other way. You can't override imported implicits. 
Scala's "new" extension method syntax was abandoned by Kotlin years ago: def (x: T) foo [T]: T = ... // "new" syntax They thought the syntax was so bad, that they rather changed the language's declaration syntax of methods than be stuck with: fun T.foo&lt;T&gt;() { ... } // removed syntax Instead, the migrated the whole language to: fun &lt;T&gt; T.foo() { ... } // current syntax So if you ever wondered why generics are placed before the method name in Kotlin (instead of having them after the identifier, which is generally better) – they found the extension method syntax _that_ ugly.
&gt; Sbt is actually easy. Read the manual. Read the Api docs. Will reading the manual and API docs make sbt start up faster, resolve dependencies faster, and trigger builds faster? If so, I'm happy to read the manual. &gt; Really? Yeah? You do this setup literally _once_ for every project. The cost is negligible compared to the ongoing costs of using the build tool. &gt; Almost nobody really understands how maven internals work ANY BETTER than they do sbt internals. Yeah, because we _literally don't care_ how the internals work. For the same reason we don't care how the internals work of almost every other tool we use, it's the wrong level of abstraction for people who're trying to use the tool. &gt; To top it off, the tooling Sam is talking about in his tweet isn't sbt. Sam's tweet is about this survey question: https://typesafe.co1.qualtrics.com/results/public/dHlwZXNhZmUtVVJfNlB4cWNSMXdub0liVExmLTVhZjMwZDc4MjAzMGVkMDAxNDhkOTc4OA==#/pages/Page_e4247943-5c33-43a4-bb3a-0ea8d39babfa One of the choices is clearly, 'Better tooling (IDE/build tool/...)'. If that doesn't include sbt, would you kindly explain what it means? &gt; a build should produce a correct build product, and be repeatable. Literally nobody on the face of the planet disagrees with that. &gt; Performance is a non-functional concern. But it is an _actual_ developer concern. &gt; in sbt, that performance is actually governed by IVY and the general slowness of compiler plugins, macro expansion, and the compiler itself. Then how do other build tools (like Mill) manage to eke out better performance using the same basic ingredients?
Thanks for the reply. Feel free to ping me when you write some other stuff. Also I'd gladly take a look at some code if you want to share ideas / have some external eyes on it. Regards, and apologies for the harshness of my initial comment 
Ok, so: * "main" project as "project aggregating all others subprojects" is not the same as root project. If you make root dependOn/aggregateOn subprojects you cannot use dependsOn root in subproject (or you'll create circular dependency). It is important because * you cannot make A access B and B access A in the same time - AFAIK hardly any other build tool would let you do that (by that I mean I cannot name any build tool that allows circular dependencies), * you can split the interface from the implementation to break the circle, having something like (example) `abcInterfaces`, `a.dependsOn(abcIntefaces)`, `b.dependsOn(abcIntefaces)`, `c.dependsOn(abcIntefaces)`, * of course if you want to run them you would have to both provide all of them in classpath, as well as use something like reflection to initialize program to look for implementations from other projects, * if you don't want to use reflection, your only option is to initialize in compile time - IMHO good idea, but this way you have to maintain strict hierarchy and perhaps something like dependency injection to provide implementation later on, as it might not be visible in your current module yet, but is in module that will depend on it. By that I want to tell that requirement 3 is virtually impossibly to implement. Assuming, that you want to have some main app with 2 optional plugins you could arrange things in a way to make it possible, but separating interfaces and implementations (and adding more submodules) - I merely guessing it is a matter of plugins, because if you want to have one app, one uberjar would make things much easier. But it will be much easier to advice something if you tell something more about what you want to achieve.
I am not trying to make a circular dependency, I want to make a project in a form of an exe + dll files where the main project can import the sub-project classes and data
Well, JVM doesn't have such requirements. You can build 3 separate JARs (again - B, and C cannot rely on A and access its classes). What you would build could be something like // build.sbt val root = (project in file(".")) .aggregateOn(a,b,c) lazy val a = (project in file("a")) .settings(assemblyJarName := "a.jar") .dependsOn(b, c) val b = (project in file("b")) val c = (project in file("c")) // project/plugins.sbt addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.9") and then sbt a/assembly This way you generate a.jar - it already contains everything from b and c (and all of their dependencies). If you want to expose the rest as libraries (which only makes sense if you want to reuse them in another project) then you should [publish them](https://www.scala-sbt.org/1.x/docs/Publishing.html) either to a local repo or a public one. And in that other project also use uberjar. If you want, you can go with the flow of building separate files and running them but it is suboptimal, hacky and not supported out of the box by sbt and, I believe, neither by many other tools.
&gt; Then how do other build tools (like Mill) manage to eke out better performance using the same basic ingredients? The main strategy is to not start with a 8 year old monster codebase written in a great hurry that literally nobody understands. Mark Harrah was incredibly productive, and for its time SBT was amazingly advanced. But the field has progressed since 2010, many lessons have been learned, new technologies and techniques have been developed, better libraries and building blocks made available. You don't need to implement your own [type-level programming library](https://github.com/sbt/sbt/tree/develop/internal/util-collection/src/main/scala/sbt/internal/util) these days when you have shapeless, or your own [parser combinator](https://github.com/sbt/sbt/tree/develop/internal/util-complete/src/main/scala/sbt/internal/util/complete) library, or ... Unfortunately the one thing the early SBT developers did most poorly of all was knowledge transfer, so much so that nobody today even knows why SBT 0.1x/1.x is the way it is, and what the problems with SBT 0.7x were that prompted the rewrite. The current maintainers do the best they can, but even they can only chip away at the ocean of complexity left in the project. I doubt I would fare any better. If you glance through SBT's git-blame, you'll see that apart from minor reformattings a huge fraction of the code is exactly as Mark wrote it, almost a decade ago. If you ask yourself "why would well-maintained code written in 2018 using modern tools, techniques and conventions have better runtime characteristics than semi-abandoned, experimental code frozen in time from 2010", the answer should be clear.
[removed]
S3 supports notifications so you can listen to changes in S3 bucket e.g. update/write/delete. You could then have service which listens to those events and streams the data into Cassandra.
I don't want a to have b and c in it, I want there to be 3 Jar file (a.jar, b.jar and c.jar) and when running a.jar it accesses b and c
Set up your project with dependsOn and use sbt native package manager plugin 
You could use SBT native packager, which will give you multiple jars in a zip file https://sbt-native-packager.readthedocs.io/en/stable/formats/universal.html
&gt;can't have autocomplete &amp;#x200B; Why do you say Metals can't have autocomplete? (Genuinely don't know).
Are you working in AWS cloud? In that case an AWS lambda function can probably make the job. I think Google and Azure have similar services 
Check out @fommil’s Tweet: https://twitter.com/fommil/status/1076551642223427584?s=09 There isn't an interactive compiler post 2.12
oh, I see. Hmm.
AWS Lambda max execution time is 300 seconds (5 minutes). Don't use it.
https://www.reddit.com/r/scala/comments/a8bmbh/switching_over_to_functional_programming_after/ecca9vo/
Then, it's gonna be difficult. First, you will have to add to MANIFEST.MF something like Class-Path: ./b.jar ./c.jar (Or maybe some wildcards \`./\*\`). Then, you will have to resolve few dependency issues: * let's say you B depends on Circe and C depends son Circe (or Play Framework or whatever) * in which JAR you put dependencies so that you won't have duplicates? `a.jar`? `b.jar`? `c.jar`? Or maybe you should allow duplicates and put dependencies in all of them? * if implementations are all the same it shouldn't matter - class loaded just picks up the first it finds - but if dependencies land in either `b.jar` or `c.jar` it no longer can be used as a library (or you'll get nasty conflicts if someone will have the same dependency but in a different version) * other approach would be to not put dependencies in any of them, and instead have like 50 JARs in some directory, all listed in `a.jar`s MANIFEST file I believe sbt native packager supports the part about packaging all JARs into one zip file. It should put each dependency as a separate JAR and have `a.jar` refer them though MANIFEST.MF thanks to `ClasspathJarPlugin`. But it will be much more than just 3 JARs there.
&gt;Mark Harrah was incredibly productive, and for its time SBT was amazingly advanced. But the field has progressed since 2010, many lessons have been learned, new technologies and techniques have been developed, better libraries and building blocks made available. You don't need to implement your own &gt; &gt;type-level programming library &gt; &gt; these days when you have shapeless, or your own &gt; &gt;parser combinator &gt; &gt; library, we can re-use the Zinc incremental compiler developed as part of SBT rather than writing our own, there are all kinds of good serialization libraries you can simply drop in, and the list go on. I am fairly sure that SBT could use these libraries, but because SBT is a build tool they tend to inline all dependencies (if possible) in order to * Keep the SBT binary as small as possible * Have minimal issues with binary compatibility Dependencies such as shapeless in particular are very large, especially seeing as SBT is already having enough problems with startup time
Paraphrasing from what I understand, the reason why metals doesn't have autocomplete yet is because its incredibly hard to do it correctly and they are setting a very high bar for only including features which are actually "properly working". Personally I would prefer something over nothing, and to date actually no IDE/build server (Ensime/Intellij) has autocomplete that has worked flawlessly (when compared to Java for example).
What it's the reasoning for (2) compared to accepting the implicit value directly and relying on callers to have the import they want or providing a value in a package object?
Spin up an EMR cluster and use Apache Spark.
You get it exactly right. Ensime autocomplete will simply bail after 30 seconds, and often ends up just crashing out. ENSIME does cache the last good compile state as it goes, though, and uses that for autocomplete. Metals doesn't. ENSIME tends to be slow, though, and synchronous. It's a real conundrum. I think ENSIME'S cached strategy is probably the better compromise, and I imagine metals could use that strategy in the future, even with the batching compiler. The batching compiler will always be slow, though, so I don't know if autocomplete can be viable except for when you really have no clue what you need and are willing to wait. I think autocomplete is the first thing an ide needs to make it an ide, though. It's awfully hard to discover apis without it. But it's hard. I use it and type inspection the most when I first use a library. But it has too be faster than loading the api docs or reading the source code. Error highlighting is secondary, and honestly really easy to achieve. Compile in the background with zinc and find the errors that refer to open buffers and highlight them. The feedback can be less immediate. It just can't be incorrect. Debugging is harder, and I never seem to do it anyway; I guess I'm an outlier there. 
actually that would be better since that will reduce the overall size of each file, can you provide me with a working simple project showing file structure and the build.sbt file and the MANIFEST file you mentioned and the sbt native packager I am still relatively new to scala and still having trouble with working with complex sbt files
~~simple~~scary build tool 
An option that would help with organization would be to have everything that is potentially common to all projects in some kind of "common" or "core" sub-project, and have them all depend on that. Then everything that is unique to projects A, B, or C would be the only things in those projects (i.e. your main method, maybe a few classes that only make sense for those sub-projects if any). Try to organize your code in such a way that none of it is too project-specific unless absolutely necessary. Separate things out to be as generally applicable as possible and then you'd only have two JARs per project: A and Core, B and Core, C and Core respectively. For your external frameworks, those can be shared dependencies of the subprojects that need them, or your core project (where they would be transitive dependencies by default).
Li, first, I think your tools and libraries are great. I want to use Mill at work, as I've said. Thank you for all your hard work. But I was taken aback by this: &amp;#x200B; \&gt; Then how do other build tools (like Mill) manage to eke out better performance using the same basic ingredients? \&gt; The basic reason is that that claim isn't true - SBT is plenty slow even doing nothing at all, let alone any ivy/scala-related things - and so there's plenty of things that can be sped up. At work when we moved off SBT onto Bazel, one of the big prompts was that SBT was taking 2+ minutes just to boot up, before we even told it to do anything! That has not been my experience with sbt, ever. Even with a fresh install. I'm willing to be wrong, though, so I ran some experiments and screen-capped them. &amp;#x200B; I started by creating a new sbt project with \`sbt new scala/scala-seed.g8\`. Then I deleted my .ivy2 and .sbt directories. Then I ran \`sbt\` on my new project. That took about 1:19: [https://www.youtube.com/watch?v=UJQJyk1E\_Uw](https://www.youtube.com/watch?v=UJQJyk1E_Uw). Then I exited sbt and ran \`sbt\` again. This second startup was much faster. It took 12 seconds: [https://www.youtube.com/watch?v=5Eeqp9vnVRc](https://www.youtube.com/watch?v=5Eeqp9vnVRc). Then I deleted my .ivy2 and .sbt directories, and created a new from-scratch project using \`sbt new scala/scala-seed.g8\`. That took about 1:46: [https://www.youtube.com/watch?v=w6xZFnGTKRE](https://www.youtube.com/watch?v=w6xZFnGTKRE) &amp;#x200B; I thought maybe the complexity of the build might be a problem. Then I went and found a fairly complex open-source build: the quasar project ([https://github.com/slamdata/quasar/blob/master/build.sbt](https://github.com/slamdata/quasar/blob/master/build.sbt)). I cleared out my .ivy2 and .sbt directories again, ran \`sbt\` and waited for it to complete, and exited. I then ran \`sbt\` again, and it started in 14 seconds: [https://www.youtube.com/watch?v=J9bIymYU\_n0](https://www.youtube.com/watch?v=J9bIymYU_n0) &amp;#x200B; From this, I can glean that whatever you are running your builds on, you are working with a completely clean environment where the sbt version you are working with had never been installed before. It jives with my 1:46 second wiped computer sbt startup times. This may be true in a common build environment where you are spinning up a new build slave or a docker build environment. It is not true when using sbt from a normal desktop situation, or when running builds on slaves that don't wipe their .ivy2 and .sbt directories after each build. &amp;#x200B; Here's what happens when I make the same build with mill. I made the analogous mill project to the scala/scala-seed.g8 project. I then ran mill -i to ensure that I had everything installed correctly, and deleted my \~/.mill and \~/.ivy2 directories and ran \`mill -i\`. The startup time was approximately 15 seconds: [https://youtu.be/Shk7zljFn8c](https://youtu.be/Shk7zljFn8c) This is about the same as running an sbt project that already has it's dependencies resolved. I figure that must be because mill hasn't resolved any dependencies yet. I couldn't figure out how to make mill just download the dependencies. There's no question that mill is faster to start, and that sbt does things when it starts that make it slow. But after the initial download of the sbt version, it starts up in 15 seconds or so, when it doesn't have to do anything. Sorry for the nitpicking. I just wanted to support my claim. &amp;#x200B; &amp;#x200B;
As the tester of \`useConfig\` I can override the config provided. If calling code imports \`mypackage.implicits.\_\` to get an unwrapped Config object, then they'll always get that config object, even in tests. It may be clearer with code: &amp;#x200B; object Foo{ def useConfig(str: String)(implicit configProvider: ConfigProvider\[Config\]): List\[String\] = configProvider.config.getList(str) } object Bar{ def bar(i: Int)(implicit configProvider: ConfigProvider\[Config\]): String = Foo.useConfig("myList")(i) } &amp;#x200B; If I test bar, I can give it a fake config. Contrast with using an import: object Implicits{ implicit val config: Config = ConfigFactory.load() } object Foo{ def useConfig(str: String)(implicit config:Config\]): List\[String\] = config.getList(str) } object Bar{ import Implicits.\_ def bar(i: Int): String = Foo.useConfig("myList")(i) } Now I have to use the default config in Bar, no matter what. This isn't usually a problem, but if you use a dynamic config library (like archaius or something) that pulls from a database or some other source, you have to be able to talk to your backing store when testing Bar, for example. You could also encode the second Bar like this: object Bar{ def bar(i: Int)(implicit config: Config): String = Foo.useConfig("myList")(i) } and import the Implicits.\_ at the call site. This works just fine. But now my callers have to know \*where\* Implicits.\_ is to import them. Creating a typeclass as a provider and following the advice of putting the implicit val declaration in the companion of the typeclass means that they can always override with their own implementation, and that they will also always know where to look for the default implementation of that implicit, even without an ide. Additionally, people working on your codebase only have to know one way of using implicits, rather than one way for typeclasses and one way for things you don't own. It's one more thing that you have to explain. Finally, it follows rules of the clean code book on boundaries, which is that you should wrap external dependencies in your own interfaces.
You could try measuring time to finish first compile from scratch with both. That should necessarily include downloading dependencies.
&gt; But I was taken aback by this I don't think you need to be so surprised. Here we can see the Ammonite SBT build taking 7s to perform *no operation*. Mill takes 2s without the worker daemon, with the worker daemon it's now more like 0.3-0.4s. Ammonite$ time sbt ops/compile real 0m7.122s Ammonite$ time mill ops[2.12.4].compile real 0m1.967s Ammonite isn't a particularly large project; a handful of submodules with a bunch of cross builds. 7s *no-op compile* Here are some numbers for my fork of the Spark repo (basically the same as [Apache spark](https://github.com/apache/spark)), from the last time I tried upgrading it from Ivy to Coursier: | | Ivy | |---|--:| | Clean No-Cache EC2 `sbt core/compile` | 10m 19s | | Clean No-Cache Laptop `sbt core/compile` | 61m 41s | | No-Op Incremental EC2 `sbt core/compile` | 12s | | No-Op Incremental Laptop `sbt core/compile` | 15s | Here, we can see that SBT starts up in about 12-15s on the spark repo; again a moderately sized repo with maybe 20ish subprojects. (For now let's ignore the fact that the initial ivy resolution takes *over an hour* when run from Singapore! much of it is unavoidable, because SBT has it's own huge dependency tree it glacially resolves using Ivy. In comparison Coursier takes ~20min (with SBT's own jars still being resolved using Ivy), and the numbers are 10min/4min when run from an EC2 box in US-West, but that's a different issue) Presumably that means Spark has a build with about the same complexity as Quasar's build. This entirely matches your experience. Given: - A tiny hello-world build with 0-1 subprojects takes ~2s to boot SBT, - A small build like Ammonite with ~8 subprojects takes ~7s to boot SBT, - A medium-sized build with a ~20 subprojects, such as Spark or Quasar, ends up taking ~15s to initialize Why should we be surprised at all if a large build that may be 10x as complex as spark/quasar, with 100-200 subprojects, takes ~120s to initialize? This seems like the most unsurprising thing in the world! 100-200 subprojects isn't really a lot. In my own open source work, I alone probably have 100ish subprojects split over my various repositories; any organization with a bunch of engineers that has been around for a while can easily hit that number if they're trying to keep things modular.
&gt; Keep the SBT binary as small as possible SBT's binary already isn't small, and already pulls in 37 third party (non org.scala-sbt) dependencies (see https://coursier.github.io/coursier/#demo, search for `org.scala-sbt:sbt:1.2.7`). I don't think binary size is one of the top-10 issues facing SBT users today. &gt; Have minimal issues with binary compatibility If SBT needs libraries internally, and it doesn't want those things to affect binary compatibility with code loaded onto the SBT classpath, it can shade those libraries, classloader-isolate them, or subprocess-isolate them. These are techniques we've had for a while now. &gt; Dependencies such as shapeless in particular are very large Shapeless, at ~2mb of jar, really isn't that big all things considered. &gt; especially seeing as SBT is already having enough problems with startup time SBT's total classpath isn't all that much smaller than Ammonite or Mill's total classpath: - `./coursier fetch org.scala-sbt:sbt:1.2.5 | xargs du -hc`: 47m - `./coursier fetch com.lihaoyi:ammonite_2.12.8:1.6.0 | xargs du -hc`: 30M (Mill is about the same size as Ammonite) Sure SBT is a bit bigger, but I do not for a moment think classloading/JIT time is the bottleneck for SBT's initialization process (in contrast to Ammonite, [where it is](https://github.com/lihaoyi/Ammonite/issues/135#issuecomment-302470086)) 
I'd suggest you to look into [fs2](https://fs2.io/). It's the best tool out there. I just wrote a CSV parser for [Spatial Key sample CSV Data](https://support.spatialkey.com/spatialkey-sample-csv-data/) to remind myself of how powerful and amazing this library is! This is the main parser: ```scala val parser: Stream[IO, Unit] = io.file .readAll[IO](Paths.get("/path/to/sample.csv"), blockingExecutionContext, 4096) .through(text.utf8Decode) .through(text.lines) .drop(1) // remove headers .map(_.split(',').toList) // separate by comma .map(parseSample) // parse each line into a valid sample .unNoneTerminate // terminate when done .evalMap(x =&gt; IO(println(x))) ``` Seriously, it doesn't get simpler than this. And it does so in constant memory. In your case, you'll need to plug in the Cassandra writer instead of just printing each result to the standard console. Maybe worth taking a look into [fs2-cassandra](https://spinoco.github.io/fs2-cassandra/) but you can use any library of preference, just need to wrap the side-effects in `IO`. Find the full example [here](https://gist.github.com/gvolpe/40b1f38ebbcbb76266dc40cad587c469).
Rewrite your build in mill. If that is not an option, rewrite your coee in rust. 
If you think your requirements make for a complex build, you've yet to see to anything. Producing an assembly, either as a single fat jar, or as a native package is next step above packaging "hello world" compared to some builds I've seen. I've seen SBT builds that make sure of source dependencies, produce artifacts for multiple versions of Scala and the JVM, multiple backends for the compiler, package database migrations, websites, and manage deployments. SBT may be daunting at first, but the complexity actually scales pretty well(the performance on the other hand is a different issue), which I think is a property lost on many who haven't had to do extremely large and complex builds.
Are you talking about optimization with constraints or a library like Refined for refined types?
[https://bitbucket.org/oscarlib/oscar/wiki/Home](https://bitbucket.org/oscarlib/oscar/wiki/Home) \-- Looks like they have stabled version on the artifactory
Makes me laugh when people talk about functional vs non functional features. I've developed an amazing product ... but it's slow as donkies. So then it's not functional is it.
At work when I change a dependency in my 10 module sbt build it takes TEN minutes for intellij to refresh. It takes 5 seconds in the equiv gradle build. This might be because it's harder to parse sbt than gradle I don't know but sbt is an utter waste of everyone's time.
If you break up your source into several Gradle sub-projects, then you'll get incremental builds at the granularity of sub-projects. You can then have your master build server (jenkins, circle ci, etc) [write to a remote cache](https://docs.gradle.org/current/userguide/build_cache.html#sec:build_cache_configure_use_cases) and have laptops read from that cache to skip a significant portion of their builds.
Good idea. I could also probably just clone the source code and wrap some timers around the task for sbt and the coursier calls in mill. Wouldn't be that difficult. I bet /u/lihaoyi already has all those numbers, though. He posted some in reply to my post. 
Thanks for the example–that is cool. Btw Reddit doesn't support triple-backticks for code, it only supports 4-space indent.
The most effective way to bulk load data into Cassandra is to use bulk loader https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/tools/toolsBulkloader_t.html that streams SSTables. As far as I understand `COPY` command will trigger compaction at some point in the process that will likely interfere with reads from Cassandra cluster. That was the case with Datastax connector for Spark so we had to use a custom tool that worked the same way as Cassandra bulk loader.
I already found this lib, but was not able to locate their version in the factory. But it seems I was not searching in the right place. Thanks, I will try that.
I know that I've said this before, but sbt with intellij is bad. You need to be on the latest community edition, the latest scala plugin, go into sbt settings, set the sbt launch jar to be the same one as your cli uses, change the settings to build with sbt for builds and code instead of intellij. If you can, update your project sbt version to 1.2.6. I've seen this happen when using intellij's internal sbt launch jar. Using the same one as the cli one and building with sbt shell instead of intellij makes it better.
Hi reddit. I was taken aback by the great feedback I had on my first blog post, thanks to everyone who read it. You inspired me to do another one, and again, any feedback will be much appreciated.
Could probably just use glue for this, honestly. Check out https://aws.amazon.com/glue/. It is built for exactly this purpose.
This is not a proper CSV parser
Great artical. I remember reading, when I started to get I to Scala, that at some point functions would become my go-to as opposed to def.. I kind of get the impression that maybe Scala was always meant to work with functions and not defs.
Hmm, could you explain how you would end up with 150? If both threads credit (i.e. deposit) 50, wouldn't you end up with 200?
I mean, the very special situation where: 1. Thread 1 reads 100 2. Thread 2 also reads the same 100 3. Thread 2 adds 50 to its 100 copy 4. Thread 1 adds 50 to its 100 copy The end result would still be 150 as the step would be overridden. This is a very contrived example but I believe it would be possible, even if remotely. I've been thinking about a different, more subtle, scenario where immutability might help and that might be the advantage. If the class is mutable and if I pass the instance to multiple threads, I lose my local reasoning as the value of the class can be updated for both threads. When I have immutable case classes, even if a thread updates the values of the class, that won't affect other threads. Maybe this subtle difference has a huge impact and maybe this is all this is all about :/
That's exactly it :-) Immutability is one way to protect yourself from [data races](https://docs.oracle.com/cd/E19205-01/820-0619/geojs/index.html). Another way is what Rust does–a compile-time ownership check to make sure only one thread has write access to some memory at one time.
Immutability doesn't make synchronization problem (like yours) go away, unforunately. What it does do though is completely rule out a whole class of error because once you have a reference(pointer) to something, **no one** can change it from under you. Consider that your `Account` class consists of both a `balance` and `currency` (currency the balance is denominated in). Your shared state is the whole `Account`. In a mutable world you can grab the `Account` and accidentally pass it to a function which mutates it `def changeToUSD(acc: Account): Unit`. Now you have a concurrency problem. In the immutable world, you will always need to make a copy of the `Account` (`def changeToUSD(acc: Account): Account`), which means any state synchronization across threads will always need to be explicit. If I have a reference to an `Account`, I am 100% sure that it'll always be the same, so I can freely use it, store it, pass it to anyone without any worry.
I like SBT, at least in theory, but I think there's a lot of problems with the implementation. Speed is definitely a problem even for small-mid sized projects. Here's a comparison between using SBT and Bloop to build various projects - goes to show the overhead SBT adds to compilation. https://youtu.be/O3H0U2BjUq8?t=1759
Simple explanation. If you have an immutable Account object that is being immutably copied by two threads, and then that result is written back into a shared item, there is a mutation in there somewhere no matter how you code it. That's not a fully immutable operation.
This doesn't ring true for me. Your example where Bar.foo has no implicit is a completely different structure than the one with an implicit ConfigProvider, so I don't find it comparable and thus do not find the drawback of non-overrideable config compelling. That's an artifact of the different structure, not a lack of the provider class. The example with the implicit Config is much more a direct translation and what I had in mind. It seems to have all the benefits of the provider, except without the proxy class. It seems your complaint is only that the caller has to import an implicit (which I would generally consider a good thing and arguably makes usage I'm tests more consistent with usage in actual code) or that you have to locate the default instance somewhere other than the companion object (because you don't control that object). I personally don't see that as substantially better or worse than using a package object for the default instance (or anything else in the default search hierarchy). I'm remain unconvinced by the remaining arguments. I see more value in making use of the built in implicit resolution and/or common conventions like an Implicits object than a forced "companions only" set up. Trying to maintain that level of consistency feels less natural and prone to creating unusual constructs just to adhere to a guideline without a clear benefit. You still have to explain to newcomers why you use that setup instead of a more common one they're more likely to have experienced. And it doesn't wrap anything in your own interface; the user must still interact with the Config class directly so all you've done is create an indirection to get to that instance rather than making it an implementation detail. Ultimately this'll come down to design preferences but I don't think this policy carries enough benefit to warrant the creation of the proxy vs using the type directly via built-in features or more common conventions.
&gt; a forced "companions only" set up Where would you look for any implicits when you use a new library? If you don't set a common place for where to find implicits you will just end up with them all over the place and people will complain they don't know where to look and let's just go back to spring because scala is too hard to understand. In addition companion objects are the last place the compiler looks, so they are always possible to override. You could say - put them in a global implicits object (like scalaz or cats). But then you have to say why. And there isn't a compiler enforced reason. So people think you are just making stuff up and complain about complexity and want to go back to java 7 and spring. For a company that has gone all in on scala and is convinced that it's worth the time to train people to write scala in a particular style that fits the company's culture, you can write it in cake (train about early initializers and the bakery of doom) or macwire or spring-boot as long as it is consistent and people understandwhat they are doing. You are adding additional cruft on top of the language features that exist for this stuff, but ok, you do you. However if you use implicits as a feature for dependency injection, a built-in dependency injection feature, and you don't have opaque and extern yet, at a company that is constantly bringing in outside help and shifts teams around to train people, this is the way to follow the compiler enforced lookup locations that is consistent and trainable with a citeable source (suereth's implicits talk and the spec and stack overflow) for the skeptics to read/watch that isn't just my advice. I didn't want to write a full config lib wrapper for the reddit comment, sorry I left config exposed, and maybe config was bad and Int would have been better. All this arguing in public in the scala community, and the tacit TMTOWTDI ideal espoused is a real struggle to manage when you are responsible for trying to keep new scala devs on the rails producing maintainable code. At some point we have to choose and commit to a path that is teachable, and reasonable and repeatable and gives compiler errors. 2) is exactly that. It has 0 conditions in which it won't work or requires a non-structured workaround that doesn't fit in its style. Every other compile-time and runtime DI requires too much experience to teach when to break out of the pattern, and the cuts can be very deep.
Cheers :) I guess you were reading this from mobile? There's a known bug. Reddit supports markdown (backticks included) on the browser but it doesn't render it properly on mobile.
Hey, maybe you can shed us some light and show us how to do it properly? I'm just a newbie :)
Hmm, funnily enough I was reading it from my laptop browser ... in the new design. But now I've switched back to the old design and triple-backticks are rendering correctly! Figures...
test ```scala a b```
Your example doesn’t handle quoted values and values containing commas. Just use an established library like Apache Commons to parse CSV
Something that other commenters haven't mentioned is essentially JVM-specific, though similar problems exist in other environments. Mutual exclusion isn't the only challenge when writing multithreaded code. You also have to ensure that a change made by one thread is "promoted" to be visible in other threads (i.e. you want to ensure that those other threads don't read a stale value). Your environment's memory model describes the situations under which a change will definitely be visible to other threads. On the JVM, immutable objects are inherently "consistent" across all threads. Any thread that inspects the fields of the object will see values that are consistent with all other threads. This guarantee does not exist in the face of mutable objects. If an object is ever mutated, you will need to somehow establish a "happens-before" relationship between the mutation action and subsequent read actions. This is commonly done via explicit locking, though other tools can be used to establish the "happens-before" relationship. Note that this is an even lower-level issue than the issue you're describing. In your case, you're questioning "what happens when two threads try to update the same value?" That's essentially an application-level concern. In some cases, you want to employ commutative updates. In other cases, you want to serialize the updates. In still other cases, it's fine for the latest update to "win". But I'm pointing at something that's more fundamental: "when some thread *does* successfully update this value, when will that update be visible to other threads"?
&gt; Your example doesn’t handle quoted values and values containing commas That wasn't mentioned in the requirements. Sometimes all you need is a solution as simple as just separate each line by comma and that's the case with the Spatial Key sample CSV data shown above. In any case, if that's needed you can just plug in any parser function (might be exposed by an external library) on the streaming data transformation using `map / evalMap`. 
Thanks! Yeah it's interesting, most of the code I write (and see) seems to use def by default and only switch to functions when needed, just for convenience.
When you say "CSV" it carries certain baggage of requirements with it. If you think you can hold assumption that data is not ever quoted, or will never contain comma as part of data, it's not a CSV anymore. It's some simplified format and you will enter world of pain if somebody actually throws real CSV at you.
&gt; When you say "CSV" it carries certain baggage of requirements with it. And what are such requirements? [The CSV file format is not fully standardized](https://en.wikipedia.org/wiki/Comma-separated_values). &gt; If you think you can hold assumption that data is not ever quoted, or will never contain comma as part of data, it's not a CSV anymore. Source? 
Immutability is a more restricted version of the rule "at any given time you can have either one read-write reference (and no read-only references) to a value or you can have multiple read-only references (but no read-write references)". This rule avoids data races in multithreaded applications. The [Rust](https://www.rust-lang.org/) compiler enforces this rule so it's a bit more flexible and safe than Scala when it comes to mutability and multithreading. In your example even if you have an immutable `Account` class, you will still have one read-write reference per thread to the variable that stores the reference to the `Account` object. So the only thing you've done is push the mutability one step up in the object hierarchy. To be able to have multiple read-write references to the same value you have to use runtime checked locks, for example a mutex or STM.
Yep. There are libraries that provide STM - scala-stm. Another safe state container is cats-effect Ref, which you modify via modify and access via .acess/.get. fs2 provides semaphores and queues. All of these things help with threaded state management.
&gt; compose is only nice to use when we can make use of the infix notation, and infix is only available for methods with a single parameter You could get the infix syntax you want without cats Arrow by defining an implicit value class wrapping `Function1` which would contain your `&lt;&lt;` method.
Nice work! 
Thanks :) I may still make some major changes since the design has evolved quite rapidly since the initial version, but I'm fairly satisfied with the current result :D 
Released a new version of [https://github.com/neotypes/neotypes](https://github.com/neotypes/neotypes) : * Added scala 2.11 support * Created cats-effect integration. This is how it looks like [https://github.com/neotypes/neotypes/blob/master/cats-effect/src/test/scala/neotypes/cats/CatsAsyncSpec.scala](https://github.com/neotypes/neotypes/blob/master/cats-effect/src/test/scala/neotypes/cats/CatsAsyncSpec.scala)
I can tell you from experience it's very worth it if you want to get in on that Big Data action, Apache Spark's native language is Scala. Additionally, since it runs on the JVM at my firm we package a lof of self contained analytics software in a fat JAR for our client's use even if we're not using Spark. I'd say it's still a niche language but it definitely isn't a waste of time to learn properly. 
It's a gateway drug to statically typed functional programming
Yes
off course it's worth, at least because it has a quite unique combination of "statically typed functional programming"/"classic imperative stuff"/"industry adoption" - which will allow you to get unique experience by working on actual problems with functional programming usage. But maybe for the first job you need choose a more standard language (like Java, C#, python ...), to get some pain industry experience that will allow you understand better why things like FP is important.
what if I am not getting into the big data field but will just be doing some general software engineering?
It's worth learning because it's a good language. It won't be the only one you learn, but it has concepts that will carry through to other languages easily. By learning Scala, you're picking up skills that will serve you through your career.
I honestly wouldn't know what to tell you then. I'm an AI/ML data guy.
what are some main use cases of Scala in the industry other than the big data area?
what are some main use cases of Scala in the industry other than the big data area? I would be doing general software engineering.
I guess learn it if your company uses it. Or if you want to do data engineering or big data with Spark (PySpark is still messy and inefficient). Or if you want to explore typed object-functional fusion programming (if that sounds potentially messy, it can get there if you’re not careful). So I guess it depends on what you’re looking to get out of it. If you’re looking for a go-to language that you might enjoy for personal projects, you might well end up picking something else.
what are some main use cases of Scala in the industry other than the big data area? I would be doing general software engineering. &amp;#x200B;
It's a joy to write if you ask me, that matters
Outside of data engineering/big data, Scala makes the biggest impact in building web applications and APIs (e.g. with Play Framework, Akka HTTP, Finatra, http4s, ...), and in building massively-concurrent applications with the Akka actor framework. See e.g. https://blog.pragmaticengineer.com/distributed-architecture-concepts-i-have-learned-while-building-payments-systems/ Scala is also emerging as a well-supported tech stack for event-sourcing style applications (which treat current internal state as an aggregate of immutable events over time), with tech like Akka Persistence and Lagom available. Scala's core goal is to be a 'scalable language'–it's even in the name. This means that it can take you all the way from 'more succinct Java' to purely functional, statically-typed architecture. A general use case is if you want a more flexible language, with a high potential to grow alongside you, on the JVM (obviously with access to all the JVM has to offer).
I like scala a lot but honestly probably not. If the fp stuff gets you going, or spark + akka ecosystem is needed for your problems, scala is great. Otherwise (i.e., normal crud webapps) probably not worth it.
It's a great answer. Thanks! Why is functional programming important though? Other than the reason that it's neat (considering the functional programming functionality of Python), can't think of a reason that comes into my mind.
Yes, it is useful even if you are not interested in big data. You can use it as the „better Java“. Other than that, it’s a very nice intro to functional programming, as you can slowly use functional concepts inside OO code. At work, we use scala for „normal“ things as well, not only for big data stuff. 
Hey, good job! I think there are a lot of things that could be improved but you're on the right track. Here are some red flags I identified in the `Heap` implementation. 1. Avoid calling `head`. Prefer `headOption` or something safe. ```scala private def swap(index1 : Int, index2 : Int)(implicit array : Array[T]) : Unit = { if(isValidIndex(index1) &amp;&amp; isValidIndex(index2)){ val temp = array{index1} array{index1} = array{index2} array{index2} = temp } } ``` 2. Use pattern matching or `fold` instead of `isDefined / get`. ```scala private def trickleDown(index : Int)(implicit array : Array[T]) : Unit = { val leastChildIndex = getLeastChildIndex(index) if(leastChildIndex.isDefined) { val wasSwapped = swapIfConditionMatches(leastChildIndex.get, index) if(wasSwapped){ trickleDown(leastChildIndex.get) } } } ``` 3. Avoid side-effects in pure functions. ```scala private def swapIfConditionMatches(index1 : Int, index2 : Int)(implicit array: Array[T], condition : (T, T) =&gt; Boolean) : Boolean = { if(condition(array{index1}, array{index2})) { swap(index1, index2)(array) // side-effect true } else false } ``` 4. Avoid mutation and function that return `Unit`. (*). ```scala private def swap(index1 : Int, index2 : Int)(implicit array : Array[T]) : Unit = { if(isValidIndex(index1) &amp;&amp; isValidIndex(index2)){ val temp = array{index1} array{index1} = array{index2} array{index2} = temp } } ``` (*) Mutation is okay if you keep it encapsulated ONLY for performance reasons. But since the idea of the project is to write functional code as a learning exercise might be better to avoid it at all cost or at least having the two implementations. Also, what are your thoughts on adding some dependencies (eg. `cats`, `cats-effect`)? Keep on FP'ing! :)
You are right. But for Heap creation and extracting top of the heap, can you do it without mutation? So I decided to make it private private val heap : Array\[T\] = createMinHeap( arr.clone) . I mean the actual heap
And the mutations are done only by private functions
I know cases of applying it at Fintech and Advertising (RTB ...), besides that Scala useful for any not trivial web projects which has some backed functionality and APIs. 
The immutable way would be to create a new `Array` without the head of the previous one. But of course. in terms of performance local mutation is preferred. Depends on what you are looking for.
Thanks to Scala, I learned three type of polymorphism. Which is not possible being coexisted in current mainstream languages. And I am looking for learning type-level programming in Scala 3. 
As you said creating a new array out of an old one will require you to copy the whole array unlike Lists. But unfortunately we cant use Lists in Heap &amp;#x200B;
I can't use languages like java or javascript any more &amp;#x200B;
Since you don’t seem willing to learn new languages, I’d say no. Even though Scala has really good tooling for doing http (like http4s for example), I’m pretty sure that whatever language (I’m going to guess C# here) you’re already familiar with can also make websites. PS: assuming http since this is the most generalised case of programming atm, so “generic CS stuff” is either this or warming up the cpu.
C++ Dev, Not a Scala Dev. Used it for a few months. It's a very productive language and allowes for a nice trade-off between OO and functional programming. In the end, knowing multiple languages is always a good thing. Remember that fundamentals count, not fads
It's worth learning if you use Spark or love the JVM, otherwise I wouldn't bother. I used it at my last job, the issue is that Scala has so many language constructs it's really fun to write, but since it has so many language constructs it's a PITA to read someone else's Scala code. 
Sounds interesting, but there is no word what is it and where can i use it )
you're asking this on a scala sub. 
Scala uses JVM. So you can get backend work pretty easily. For example, Play framework is very popular. There are tons of others out there from Twitter. But JAVA is used more than Scala when it comes to these stuff. 
There was a [survey](https://typesafe.co1.qualtrics.com/results/public/dHlwZXNhZmUtVVJfNlB4cWNSMXdub0liVExmLTVhZjMwZDc4MjAzMGVkMDAxNDhkOTc4OA==#/pages/Page_e4247943-5c33-43a4-bb3a-0ea8d39babfa) recently. There's a question "In which application domain do you primarily use Scala?". Looks like the main use case is web backend.
If you don't already know how applied category theory relates to typed functional programming, and you learn the kind of scala programming that captures this relationship (scalaz, cats, software/community that uses things like functor, monad, applicative etc etc), this is a path to becoming a _much_ better programmer (orders of magnitude better). If you already know this stuff, it will just be reckoning with new syntax, language and jvm quirks. If you don't and you learn "scala as better java," it has no transferable benefit beyond a talking point in interviews for jobs that might credit past experience working with scala.
Today most people would say the biggest benefit is it helps with concurrent programming, see e.g. how successful Erlang has been at achieving concurrency by restricting the language and runtime to functional programming style. The other big benefit is easier modularization by breaking problems down into small functions and abstracting their behaviours out into smaller, more general functions. There’s a paper by Prof. John Hughes, one of the creators of Haskell, that explains the modularization argument. I know it’s a cliche to say ‘read the paper’ but this one is really aimed at people new to FP. I would give it a look. PDF, 23 pages: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
Have you read [Why Functional Programming Matters](https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf)? Python is actually a good example of a language that was originally intended as an imperative language (in this case, object-oriented as well) but has picked up FP features over time (e.g. `lambda` was supposedly added in by an old Lisp hacker, presumably while Guido wasn’t paying attention). I hear that Java and C++ have picked up lambdas in the last few years as well. You’ve heard of React? Web front end rendering library, based on FP principles. (Vue as well, I hear, although I’ve not used it myself.) The point being that FP features and styles are becoming more widespread and more widely used, even if the languages we use are not really functional. Does this mean you should learn Scala? If your goal is to learn FP, then I’d argue that learning something like Haskell would be a better bet: Haskell will force you to learn FP, and if you need to use Scala for work later on, it’s pretty easy if you already know Haskell and Java. Sorry, I’m rambling.
I think for me the most important thing is that functional code expresses intent better (in most cases). When you read something it's easier to understand what it does and easier to see that it works like that intentionally. In imperative code many times it's hard to decide if edge cases are handled intentionally or not. You should probably always strive for writing pure functions, even if you don't go too functional and have local variables inside the function. They are easier to test, reason about and run in parallel, because there's nothing else to consider just the arguments and the body of the function. I also think functions are easy abstraction, what you can achieve with functions is probably simpler than the alternative (annotations, oop patterns or whatever). So overall for me functional code is easier, faster and more enjoyable to write and read. 
Interesting. I'll give this a try and write an update. Thanks silentraquo.
For reference, see the before/after meth heads mugshots
Webapps &amp; microservices, that alone would cover most of the industry :)
Depends. If your concern is purely finding a job then no. Orders of magnitude more work in Javascript and Java. But if you want to learn a JVM language that attempts to bring together object and functional styles then yes. Scala is a really good language and you will learn new things.
Defintely. Even if you never use it, it will train you to think differently. When you only know a single language, you often fall into routine and fail to try and compe up with newer and better solutions to problems. Learning a langunge such as Scala, which forces you to write immutable classes, and functional code, makes you realize how maybe some of the things you were doing in Java or other lanuages aren't actaully the best solution. It definitely heled be grow as an engineer.
This is amazing, thank you. I knew there would be some implicit way to do this, but didn't know how to get started. Now I do :D 
Scala is used in big data. But i've used Scala professionally for over 5 years and used it for all sorts of purposes, mostly REST services and other applications. 
Yeah, I supposed a few words of background would be apt. Thanks :)
Wrap with IO and discard the result (Unit) by combining with either `&lt;*` or `*&gt;` depending on if the logging happens before or after
They said use braun trees? [http://alaska-kamtchatka.blogspot.com/2010/02/braun-trees.html](http://alaska-kamtchatka.blogspot.com/2010/02/braun-trees.html)
scala hyperspace is welcome to everyone. you come from land of reacts js and node js , you welcomed by scalajs. you came from jvm land you can still connect to then. it is all, all it is. In 21st it chosses you when you are ready. After one yr in scala land you definitly experience ego death. 
It is IMO, the best general purpose language out there. A number of modern languages have been adding similar sets of features (Swift, Rust, C#, Kotlin). It is also heavily tied to the JVM (although there is some work happening to change this). For me, the JVM is an advantage. The last 5 years of my career have been using Scala. It is helpful when job hunting, as companies that use Scala have a hard time finding experienced Scala developers. This immediately moves your resume to the top of the pile. I have been using it for server back-end work, generally "REST" API stuff... Many people here will hype the advantages of pure functional programming. You absolutely do not need to use pure FP, in fact, none of my employees have done it (my last job did use a little Cats). That said, learning about pure FP / category-theory will make you a better programmer, and it is fascinating stuff. Also, mainstream languages are adding monads, but they do not call them monads (e.g. Javascript's Promise, Java's Optional and Stream). The designers of said languages either do not understand category theory, or are afraid of it; the result is a half-assed implementation.
I strongly disagree with that. It’s a hybrid of oo and fp and best used this way. 
Scala helped to educate me on better ways to write JavaScript. Also, the FP principles I picked up from Scala bled not only into JS development, but also C# and Rust. Never a bad deal to learn multiple languages and paradigms, they tend to help reinforce each other.
Yes. It's a big language, though, and you should start with the least powerful subset then move to broaden your capabilities. That means pure scala and function composition, then categorical fp (cats) then mixed oop/fp (cats and std lib and java libs) then capabilities and effects (IO) then frp (Var with a capital V) then actor systems and streaming and finally macro building. That goes from least powerful to more powerful. Also fewer features to most features.
What you write about sbt is mostly true. Having recently spent a lot of time working on improving the performance of sbt, I would add that I believe that much of the code base is more than 8 years old because it was using svn before switching to git in 2010. That being said, I do think that it's somewhat disappointing that the community has seemingly thrown up its collective hands at trying to improve sbt. There is an enormous amount of legacy code using sbt and it can be a lot of work to translate a build from one tool to another. &amp;#x200B; I'll admit that I have a dog in the race because I have made numerous performance and usability fixes that are pending in sbt 1.3.0. There was a lot of fairly low hanging performance optimization fruit throughout sbt -- mostly related to io and class loading. After my changes, I'd be surprised if mill is generally as fast as sbt for most tasks once sbt has warmed up (i.e. the second time you do anything in a shell session). For example, I can generate simple projects in which sbt detects a source file change, recompiles the file and runs a test all within O(150ms) of the file being modified. &amp;#x200B; The sbt startup time is bad. It also regressed between 0.13 and 1.0 (4s vs 7s) for a no-op compile like you described. Some, but not all, of that time is spent in task initialization and is difficult to optimize (class loading and dependency resolution are still significant bottlenecks that will impact any build tool on a cold start). The fact that there was a regression also suggests to me that the performance issue is not intrinsic to the sbt data model. That being said, I think that for many use cases this is a bit of a red herring since sbt is designed to be used as a shell. For the most part, sbt can be made to run fast once it has started up. &amp;#x200B; My experience working on the sbt code base is that it is generally quite difficult to figure out how the hell anything works, but once you do figure it out, it's usually fairly straightforward to actually implement the change. It wouldn't be nearly as hard if there was good documentation for the high level design as well as the actual implementation. Much of the struggle is just figuring out where the implementation of a particular feature is. &amp;#x200B; I understand that there isn't a lot of enthusiasm for digging through an old legacy code base and making improvements but the potential impact is huge. Given how much work it is to actually implement all of the features that sbt has (including those added via plugins), it is almost surely less work for the overall community to improve sbt than to migrate everyone to mill or gradle. It's an even tougher sell because there is so much angst around sbt. Even people who generally like it (like me) tend to be apologists. &amp;#x200B; It seems to me that the frustration is rooted in the fact that sbt imposes a dsl on users that has a nontrivial learning curve and doesn't always work well whether because of bugs/nuances of the task definition macros or because of poor performance. Because the codebase is so inscrutable, it is difficult to debug when a user does get suck. Moreover, because the task definition dsl is implemented in a subset of scala, it is very confusing when idioms that work in regular scala code do not, or vice versa. On the flip side, when everything is working, I think the dsl is quite pleasant to work with. Make no mistake, I'm not claiming that its build syntax is better than other tools like mill, but I don't think it's objectively worse. It is, however, widely used in most scala projects so its quality matters. &amp;#x200B; How could we improve the code base to address some of these pain points? Right now, I think of sbt as being effectively split into three parts: task definitions, task initialization and task execution. For the most part, I think task definition and task execution work well and can be improved without undue effort. The task initialization codebase (along with the code for accessing the definitions) is quite complex and difficult to keep in your head. My intuition is that the internal data structures for initialization could be simplified and that this could make it easier to optimize start up times and would certainly make it easier to maintain. This is a nontrivial project but could be done by one or two people in less than a year, I think.
I had to learn Scala at work. Initially, I disliked the language for its complexity, but after some a couple of months I found the journey rewarding. The language exposed me to different programming paradigms, which in turn revived my love for different learning programming languages and paradigms.
🤔
Scala has probably the most advanced typing system and the most advanced concepts. The disadvantage is that the language is complex. It takes time to learn. But, after learning Scala, you'll probably learn other languages faster. You'll get used to many advanced concepts like lazy, covariants, Scala macros, the recursive nature of SBT, and etc. (Maybe Rust's lifetime concept is an exception because Scala doesn't allow us to do memory management) Scala also takes a different direction from other languages. I remember Odersky mentioned that language was a tool, and programmers could choose to use some features and not to use some features (or something along that line). So, that's why Scala is biased to have many features. Other languages focus more on providing one right way to implement given a certain situation. Golang is the extreme example of this direction.
For me the only reason to stay with sbt is native packager plugin. It just gets shit done and is very approachable - after I created an initial build file for deb package with native libraries for multiple platform (that took like two hours with no prior experience) my sysadmin colleague with no Scala experience took over it and started customizing it in no time.
What about haskell's type system?
Those are described in the Expressions section of the [Scala Spec](https://www.scala-lang.org/files/archive/spec/2.12/06-expressions.html).
Is there a more concise resource that describes what methods have special effects I don't know about? While the language specification probably has them all listed in different places, the page you linked is over 10,000 words.
https://stackoverflow.com/questions/1483212/list-of-scalas-magic-functions
&gt; What are some main use cases of Scala in the industry other than the big data area? Scala is a JVM based language with OO roots, so anywhere that you would use Java, you can use Scala. It's a pretty straight drop in replacement. So you can ask /r/java "what are the use cases for Java" and see what they say. Beyond that, Scala has the following advantages over Java: * It works far better with multi-threaded programs. It has better concurrency primitives, and facilities for creating immutable data objects and data structures. * It removes many warts from Java. There are no checked exceptions, and nothing returns `null` -- instead, you'll be given an `Option`, which is like `java.util.Optional` but far easier to work with. * The type system allows for much richer structures. Java's generic support for covariance and contravariance is awkward to work with, especially in collections. So, Scala is good at concurrency, it's fairly low-latency, and it's good at setting up strong type containers for processing work. With this in mind, Scala's good at frameworks, where you want people to work inside of a given box, and put together "work packets" that can be processed as fast as possible once work is handed to the engine, and where work (CPU processing) can be "swizzled" -- sent from one thread to another, one core to another, one socket to another, one machine to another, one data center to another. The following frameworks all want the above, so they are the use cases: distributed databases (Kafka), data processing (Spark), work management (Akka), http applications (Play), microservices (Lagom), etc. Anything where you would want to be able to throw more CPUs / docker containers in while your service is running and have it scale up on the fly.
Thank you! I tried looking around for a list of "magic" functions to avoid but could not find anything
Lol wut r u even saying
I think he had a stroke 
dont think like dmt
I am glad that Mill exists, but for now I'm very happy with SBT. It does have some quirks but overall I've been happy with it. It's the de facto standard in the Scala community and there's great value in picking the most popular option, because people know how to build projects with sbt and plugin authors are targeting it first. With enough uptake Mill could become that de facto standard and I'll use whatever that standard is, so until then I'll happily use sbt as a personal preference. &gt; A lot of functionality is built in (such as dependency graphs, fat jar generation, and run script generation) This isn't necessarily OK. With sbt this is handled by plugins and sbt's plugin ecosystem is very rich. For example sbt has [sbt-assembly](https://github.com/sbt/sbt-assembly), but for easy deployment purposes it also has [sbt-native-packager](https://github.com/sbt/sbt-native-packager) that can also compile Debian (deb) / Red Hat (rpm) packages, Docker images, etc, it's absolutely awesome and other build tools can only dream of such comprehensive support. The advantage of having functionality built in is that you don't have to find and configure yet another plugin. But that's a cost you have to pay only once per plugin and you'll end up with a dozen plugins in complex projects, no matter the build tool — if it has good plugin support, you'll use plugins. Personally I will happily copy/paste configurations from my `build.sbt` files and I have no problems with my workflow. Granted I'm a senior Scala dev, but alternative build tools improve the need to copy/paste very much. &gt; `mill clean` on my project takes .323 seconds to complete, while `sbt clean` takes 8.329 seconds That's nice, however `sbt` will usually be active in the background and doing `sbt clean` is not the way `sbt` is meant to be used. &gt; `mill myproject.compile` takes 34.911 seconds to complete from clean while `sbt compile` took 39.483 seconds to complete from clean. Again the difference is explained by the cold startup time. Otherwise in both cases the actual compilation is probably managed by [sbt/zinc](https://github.com/sbt/zinc), the incremental Scala compiler. 
&gt; Sometimes it caches just a bit too much, and I have to change by build.sc file to get a project reload to happen. There's probably a better way to do this, but I just started using it. If you can reproduce this, open an issue with a minimized repro! This should not happen. &gt; As far as I can tell, no -jvm-home, or settings to pass in arguments to mill's host JVM. It has good defaults, but I don't know how to modify them well. The standard `JAVA_OPTS` env var should work. Otherwise `java -cp $(which mill) mill.MillMain` works (though this runs mill without the background daemon). I don't remember if we exposed a way to pass JVM flags to the background daemon, but if someone wants it it should be easy to add
&gt; That's nice, however sbt will usually be active in the background and doing sbt clean is not the way sbt is meant to be used. I'm aware sbt isn't meant to be used that way. Like I said, I've been using it for years. What I like about mill is that it can be used that way, and I brought up these numbers to highlight that. SBT's interpreter mode isn't that bad to deal with, but at the same time if I do something like run a program that hangs, or run a benchmark that's taking too long, usually the act of halting the program takes sbt's instance with it. Plus, I've had issues with an sbt interpreter instance being open for too long and my VM starting to hit memory limits due to class unloading/reloading (this usually happens with play). This quick startup time and quick execution of tasks is a strength of mill's IMO. Yes, sbt can catch up some with interpreter mode, but that has weaknesses too. And mill has its own interpreter mode, so it can enjoy the strengths of that approach while not being required to stick to it. 
&gt; If you can reproduce this, open an issue with a minimized repro! This should not happen. The main way it happens is I publishLocal a snapshot of my module, use it in another project, and that other project's build.sc doesn't recompile in response to the new snapshot. Since the version isn't changing I'm not surprised that it's not recognized that the snapshot.jar it's depending on has changed and it needs to recompile. IIRC, this is a problem with coursier and there's a way to fix that in SBT. I just need to look that fix up and see if I can apply it to mill. &gt; The standard JAVA_OPTS env var should work. Otherwise java -cp $(which mill) mill.MillMain lets you pass in whatever JVM stuff you want (though this runs mill without the background daemon). I don't remember if we exposed a way to pass JVM flags to the background daemon, but if someone wants it it should be easy to add this will be helpful, thanks!
&gt; How do you do "real-time" logging (e.g. to log the program state) in a functional way using Cats ? You kind of have to pick whether you consider the logging important or not. If you've got a log as a first class part of your program's functionality, you do it like anything else, e.g. using doobie to log to a database. If you don't consider logging worth representing that way, then random uncontrolled calls to log functions are probably fine - but in that case I'd question how much value there is in doing the logging at all.
You could use a number of Java libraries, such as [JaCoP](https://github.com/radsz/jacop). I wrote [my own Scala wrapper](https://github.com/Sciss/Poirot) for it, slightly differing from the one written by the original authors, and slightly behind upstream HEAD. It's published as maven artifact.
If you just want to use the most popular thing with the largest plugin ecosystem, wouldn't you be better off using Maven? I think everyone knows much of the Scala community uses SBT, but at some point a tool is bad enough that the benefits of using a less popular alternative outweigh the costs. /u/lihaoyi gave a bunch of specific criticisms (whether you agree with them or not) in his post announcing Mill. You talk about "personal preference" but I'm not getting a lot of sense of what it is you think SBT does well - is it just that it's popular and plugin-based, or is there more to it than that?
It's a good general purpose language; for me that's a lot of the appeal. There's no particular "use case"; rather Scala is a good language for almost anything you want to use a computer for.
Haskell lacks subtyping, and vanilla Haskell has no support for dependent types at all.
If you're trying to have some kind of plugin system you'd be better off opening the plugin jars at runtime via reflection (constructing a new classloader etc.) rather than trying to have your build tool do that - the reason people usually have plugin systems is to let them add/remove plugins at runtime rather than at build time. Particularly when you've said you're new to Scala, I'd strongly recommend following the normal Scala conventions and finding a way to achieve your requirements while working with the grain of the tools, at least to start with. Once you understand the tools and the tradeoffs they make you'll have a sense of when to step off the beaten path. But for now, what is it that you're trying to get out of "having the main project import the sub-project classes and data"?
I think even for that kind of case I'd sooner use a `case class` with a `private` constructor and an `apply` method in the companion, and remember not to call the `.copy` methods. It's just too easy to forget to implement `equals`/`hashCode` otherwise, and avoiding ever comparing for equality is harder than avoiding ever calling `.copy`.
&gt; What is the purpose of the framework as opposed to just using the functional aspects built into the language? It's not a framework, it's a library - just a pile of useful functions and data structures. You don't have to use them - indeed I'd say implementing the things that cats/scalaz do "by hand" is a useful learning experience, and maybe even that you shouldn't use a function from there that you don't understand well enough to implement yourself. But since a lot of things in the functional world are very generic and reusable, rather than e.g. writing your own `Validation` type and implementing your own `traverse` function for a `Vector[MyValidation[...]]`, it's easier (and more maintainable, especially if you want to work with other people who will be familiar with these standard constructs) to use the popular library implementation of those things.
&gt; Sbt is actually easy. Read the manual. Read the Api docs. Which change radically every other version and are still incomplete and hard to search for. I never had to read a manual or docs for maven. I can get context-specific help about build settings from my IDE. Very occasionally I had to search for something, which was easy because everything had a plain-English name. &gt; And maven is just as slow. Actual compilation takes the same time, but SBT takes much longer to start up and load a project. (Given how much slower compiling Scala is than parsing XML, how could it be otherwise?) &gt; Maven phases and task reordering is just as hard as in sbt. No it isn't. In maven there's a single static list of what phases happen in what order. (Many people complain about that not being customizable enough, but IMO if you need more phases than that then it's time to split your build into smaller modules and order those module builds in the normal way that everyone understands). SBT has nothing like that. &gt; &gt; sbt new scala/scala-seed.g8 &gt; &gt; mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false &gt; Really? Either you're dishonestly comparing interactive with non-interactive, or the SBT version will generate an app that has the same group and name as anyone else using the same generator, which seems decidedly unwise. &gt; As for understanding complex builds in maven vs sbt, the first line of this article is "don't." https://blog.rubiconred.com/how-to-use-maven-for-complex-build-processes/ And they're absolutely right. The problem is that SBT naturally guides developers towards agglomerating complexity in their builds. Even if a good SBT build is better than a good Maven build and a bad SBT build is better than a bad Maven build, the real problem is that SBT builds always end up bad. http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html &gt; Almost nobody reads the sbt manual. It's like 80 pages. That's because the same nobodies don't really read the maven manual, either. It's like 800 pages. People read the getting started guides and copypasta builds. Yep. So either you can try to make better people, or you can judge tools on how they work for actually existing developers. No-one reads the maven manual but they somehow manage to come up with maintainable builds that other people can understand. SBT builds have to be rewritten every few years and even then a developer from one project will often have difficulty understanding the build of a different project.
Reddit in general supports 4-backtick markdown. 3-backticks works but in the new design only.
Thanks, I didn't know that! Yes, I started using the standard markdown with the new design only to find out that it doesn't work everywhere. The previous code formatter is terrible...
Shrug, I always hated writing 3 (odd number) so have always been happy with the old one. 
&gt;I'd question how much value there is in doing the logging at all. Not much indeed. The information contained in the logs can already be found somewhere else. It's juste easier for me to have them in my terminal from time to time. Thanks for the answer ;)
&gt;no-one reads the maven manual but they somehow manage to come up with maintainable builds that other people can understand. They don't understand them. They only understand what they've had to fix. If you can use anecdotal evidence, so can I. I end up fixing just as many maven builds as I do sbt builds. Look at any spring-boot build. Or Camel. Or Jaxb contIning project at a large company. I think we can agree that builds tend towards complexity period. &gt;, the real problem is that SBT builds always end up bad No they don't. Did you look at the quazar build? Your claim of always is hyperbole and I just refuted it with one example. &gt;Either you're dishonestly comparing interactive with non-interactive, or the SBT version will generate an app that has the same group and name as anyone else using the same generator, which seems decidedly unwise. I'm comparing the terminology and the lack of interactivity to not. Archetype:generate? Whats wrong with new? &gt;In maven there's a single static list of what phases happen in what order. &gt;SBT takes much longer to start up and load a project I just screen capped sbt. Again if 12 seconds a day kills your productivity, you are in the wrong language. &gt;SBT builds have to be rewritten every few years and even then a developer from one project will often have difficulty understanding the build of a different project. That's true of lots of maven builds, and rake, and maven. They don't change apis like sbt did from 13 to 1.0, but it was a major version change. That change in api has occurred twice over ten years. &gt;maven there's a single static list of what phases happen in what order. Yep. But every plugin has its own tasks that hijack that simple list and do things out of order. And to configure them you have to nest xml several layers deep. I'm in the crowd of what the hell was wrong with make. You are right, We can't fix a people problem with a tool. But unless the community switches en-masse you need to understand sbt, and my point is that sbt builds are plenty easy to understand compared to rake, make, and autoconfigured builds with g++. But people are pretty productive with those. Maven and sbt have similar abstraction problems. Mojo is magic juju. If we're going to fix the toolkit let's not rely on an Ant extension.
hey, the 2 point was a good catch. I have changed it to fold and pushed to the repo! Regarding the 1st point, where exactly am I using head?
&gt; Look at any spring-boot build. Or Camel. Or Jaxb contIning project at a large company. I think we can agree that builds tend towards complexity period. There's a difference between a build being complex because it's doing some complex framework-specific processing and a build being complex even if it's just compiling normal code. I've seen plenty of maven builds grow but stay organized and tractable, because the tool guides you towards encapsulating your logic in plugins, which will naturally be first-class code that you'll apply your normal standards to. Whereas unless everyone is very disciplined an SBT build will accumulate 1-liner hacks until it becomes incomprehensible, because that's the path of least resistance whenever you need to do anything in SBT. &gt; I'm comparing the terminology and the lack of interactivity to not You're comparing interactivity with deliberately turning off interactive mode. Did you not see the `-DinteractiveMode=false` in your command line? &gt; Archetype:generate? Whats wrong with new? We're talking about a rarely-used command, so a long descriptive name that makes documentation easy to search for is better than a short ambiguous name. &gt; I just screen capped sbt. Again if 12 seconds a day kills your productivity, you are in the wrong language. You're moving the goalposts. 12 seconds is very slow. Could I put up with it if there was a compelling advantage for that tool? Yes, of course. &gt; That's true of lots of maven builds, and rake, and maven. They don't change apis like sbt did from 13 to 1.0, but it was a major version change. That change in api has occurred twice over ten years. Two drastic incompatible changes (to the point that old tutorials can't be followed with new versions, basic syntax and even filenames are different) over a time period where maven has had zero such. &gt; I'm in the crowd of what the hell was wrong with make. It relies on running random uncontrolled executables in the host system; being non-cross-platform is the least of it (as are fundamental usability issues like significant tabs). There's no hope of reproducible builds. It assumes that all dependencies/inputs/outputs are represented by files with timestamps... &gt; But unless the community switches en-masse you need to understand sbt Not really? I keep all my own projects on maven. If I'm working on an SBT-based project I won't try to fiddle with the build; if the build is too problematic then I won't contribute to that project (or I'll make a private fork that switches to maven, if I really want to work with it). &gt; Maven and sbt have similar abstraction problems. I don't think that's true at all. Maven doesn't have the mutable-programming-logic issues that lihaoyi describes. It's a lot more declarative in terms of how you use/work on it. &gt; Mojo is magic juju. True, but you only have to deal with it if you're writing a custom plugin, whereas SBT is incomprehensible magic even if you're just writing a build. &gt; If we're going to fix the toolkit let's not rely on an Ant extension. Huh? What ant extension are you talking about?
sbt for work and mill at home. I really like that I can read through the mill source and understand things. I'm hopeful that it's a few plugins away from seeing adoption skyrocket because that's sbt's main advantage
I think you'll be in for a surprise if you let scalac generate `equals` for you and then try `ReducedRational(6, 8) == ReducedRational(3, 4)` :-)
I am familiar with other languages like C++ (advanced/intermediate), Haskell (intermediate) and Python. I've programmed in Java a decade ago but I'm not familiar with things added after around when Generics appeared. &amp;#x200B; I would like to learn Scala quickly and use it to write readable and maintainable production code for a complex big data processing pipeline. Any suggestions for an efficient learning path? "Programming in Scala" has good reviews but is 800+ pages. I'm hoping for a faster route leveraging on experience in other languages.
Nice work! I guess someone more knowledgeable could give better answer but for benchmarking you should use [https://openjdk.java.net/projects/code-tools/jmh/](https://openjdk.java.net/projects/code-tools/jmh/) \- it is maintained by JVM developers so it takes care for all these pitfalls you could normally make (cold JVM, etc). I am also want to clarify one thing - you said you developed \_proprietary\_ framework, while now it is released under Apache Licence - so you mean it \_used to be proprietary\_ during development and before the release?
This is fascinatingly idiosyncratic
I meant proprietary, as not following existing standards. But, of course, it is free for use under Apache Licence 
Good job. Looking forward to see some reviewing from the community
Agree. Reading the introduction though, a lot of right decisions have been made.
Hi. It's always very interesting to see new work being done in Scala. Looks like you've created a mini-ecosystem for your commercial projects. I would ask, are you looking for opportunities to cross-pollinate with the wider ecosystem? There may be opportunities for learnings and improvements in both directions.
Good point, originally it was a product ecosystem, and not long ago I started opening it up to work side by side with existing API. But, now I try to rethink it as a Scala extension for cases where it is a better fit. And "Yes', I would love to integrate with wider ecosystem and need a lot of advice and help. I am a bit single focused and do not have a good general picture.
I like `mill` a lot and used it in a few personal projects. Unfortunately, I find that I often had to go back to an `.sbt` file after a while, having hit one limitation or another. Most of these limitations are non-essential and probably only temporary, though. For example: * there were some limitations in IntelliJ-generated projects, and it was smoother to just use an `.sbt` file to import the project in IntelliJ; * the input of my console app using Jline2 does not seem to work when using `mill run` for some reason, so I had to use `sbt` to run it. I'll open issues next time I encounter problems if I find the time. Also, one thing that really disappointed me was the reliance on macros to define tasks and the like. From having tried to define my own tasks (a few months ago), I found that the macros are very brittle (as expected for Scala-reflect macros), yielding terrible compiler crashes if you're not following implicit undocumented conventions. That was a big turn-off for me. Also, I never understood (but did not look into) why when I define settings that have a default implementation (which I can see in IntelliJ navigating to their declaration) I don't need an `override`. What's going on there?
He's using a custom compiler plugin that makes override optional
I like mill a lot. I'm using it on all my personal projects, and have used it in one project at work. &amp;#x200B; What I don't like about it : * it hasn't reached a state where backward compat is guaranteed. This is fine when working on personal stuff, but I can't really push it more at work for this very reason. * doesn't parallelise the call graph when possible * the popularity of the "Haoyi" stack ironically creates some binary compat problems between mill and other libraries that haven't updated the latest versions yet. It happened to me with mdoc a month ago ago : [https://github.com/olafurpg/mdoc](https://github.com/olafurpg/mdoc) . This lead me to run mdoc in a "main" rather than in a task, which is pretty much the road that Fury is taking for plugin support. * I wish changing the build didn't necessarily invalidate the cache, but it is very much a non-trivial problem * doesn't have autocompletion yet (I do rely on it a lot in sbt) * sometimes macros yield errors and newbies are put-off by it &amp;#x200B; What I like about it : * interoperability with other tools from a sensible sdtout/stderr usage, the adoption of json as an output format and quick/instinctive call-ability from bash * simple/familiar semantics, and this is the big one. If you have a maven jar, you pretty much have a mill plugin. The sole reason for most SBT plugins to exist is that the semantics of SBT are so unintuitive that people feel like they have to translate library functions to SBT tasks for their users, because their users don't necessarily know how do it themselves. This is extremely empowering. * sensible path design in file outputs * uses coursier * running the IDEA generation command is actually waaaay faster than updating the view of an SBT build from IDEA's built-in feature. 
What I'm suggesting is something like https://scastie.scala-lang.org/LrwKLxxXS1GKq7aNWhX7nA .
I might try it if someone can tell me how to port a project that's using scalajs-bundler
&gt; the input of my console app using Jline2 does not seem to work when using mill run for some reason, so I have to use sbt to run it. You need to use `mill -i run` if you want stdin to be wired up; otherwise wiring up stdin to the background daemon is difficult so I haven't done so &gt; I would have much preferred a clean applicative-style way of defining the dependencies between things, rather than some brittle magic reminiscent of SBT's darker inclinations. There's `T.zipMap` if you want to program in an explicitly applicative style. Personally I can't stand it, and wrote the macros after spending a few weeks `zipMap`ing all over the place, but if that's what makes you happy it's all still there (the macros simple generate `zipMap` calls)
&gt; This lead me to run mdoc in a "main" rather than in a task, which is pretty much the road that Fury is taking for plugin support. This is prettymuch the "standard" way of integrating non-trivial external code into the build. Unfortunately there isn't a write-up somewhere, but basically all the built in heavy lifting done by the scala compiler, scalajs optimizer, scala-native, etc is all run in isolated classloaders. We provide some helpers builtin to help with the common task of "run thing in classloader". And you can always shell out to a subprocess running the `java` command (there are helpers builtin for that too). &gt; I wish changing the build didn't necessarily invalidate the cache, but it is very much a non-trivial problem At the very least your zinc incremental compile caches should stick around, but yeah given the current state of "general purpose scala language for build file" it's impossible to compute what needs to be invalidated in a more fine grained way.
Have it any tests or I missed them?
Did not realize that. Pity because it was a decent, lightweight tool to get introduced to Scala. One of my colleagues is retrofitting our build process with Mill but too early to say how much of an improvement it is over SBT. Have you tried it? Kind regards. 
The test are below: [Regular Benchmark](http://scalqa.org/doc/Quick_Collections/Stream_Performance/Benchmarks.html) [Specialized Benchmark](http://scalqa.org/doc/Quick_Collections/Stream_Performance/Specialized_Benchmarks.html) The code part can be inserted in any App, or even run in interpreter (given the library JAR is in class path) The 'for-comprehensions' trigger following ops: filter, flatMap, map, count (or sum) &amp;#x200B;
I pasted the wrong code on (1) but if you search for `.head` you'll find out :)
**Amazon** **|** **Software Development Engineer, AWS IoT Core** **| Seattle, WA, US | ONSITE | Full Time** # DESCRIPTION Are you a software developer who wants to make an impact and connect billions of smart things to AWS eco-system? Do you want to design and build back-end services that route, transform and enrich data from and to devices on a global scale? **The team:** The Rules Engine of Amazon’s Internet of Things platform (AWS IoT) enables customers to filter, enrich, process and route data from devices to the full eco-system of AWS services in a fully managed environment. Companies from all over the world can build their smart features and apps without having to worry about building scalable infrastructure that scales with their message traffic from billions of users and devices – because we do it for them. From simple sensors and actuators that barely have enough memory to run an OS and implement connectivity - to full-blown industrial gateways – they all can benefit from AWS IoT’s Rules Engine to receive, store and process data without having to touch the device’s software itself by using the power of the AWS cloud. **You:** We’re seeking software developers with industry experience who are passionate about enhancing customer satisfaction, operational performance, and growing Amazon’s IoT business. If highly concurrent systems, distributed functionality, programming language processing and difficult synchronization problems make you want to take control, we have the right challenge for you. The Rules Engine of AWS IoT is a high performance, distributed system that connects customer data with the AWS eco-system by enabling customers to create rules to filter, pre-process, transform, enrich their message data before calling to other AWS and external services to extract value. We are building and operating a highly efficient instructions interpreter to process device data at high rates. All of our development is very customer-centric, and you should feel strongly not only about building and releasing good software, but about making that software highly reliable under extreme load. You should be someone who enjoys working and having fun with some of the smartest software engineers in the industry - building complex system software to deliver significant impact for the rapidly growing AWS IoT business. # Key Responsibilities * Design, build and optimize the software to run a highly scalable communication protocol engine and front-end to connect billions of devices to the AWS eco-system using Scala. * Define intuitive web-service APIs and use AWS’s cutting edge technologies to develop and deploy new features quickly * Empower developers from around the world to use our SDKs to connect their devices to AWS IoT to enable intelligent systems * Automate deployment and monitoring of your service to track down performance issues before our customers get affected * Recruit, interview and hire software developers * Mentor junior software developers and grow their skills * Drive development process improvements and establish best practices to ensure highest quality software ### Basic Qualifications * Master’s or Bachelor’s Degree in Computer Science or related field of studies * Minimum of 2 years of industry software development experience using JAVA, C#, OR C++ * Possess an extremely sound understanding of basic areas of Computer Science such as Algorithms, Data Structures, Object Oriented Design, Databases * Be able to write high quality, testable and maintainable code * Must have good written and oral communication skills # Apply at * [https://www.amazon.jobs/en/jobs/688437/software-development-engineer-aws-iot-core](https://www.amazon.jobs/en/jobs/688437/software-development-engineer-aws-iot-core)
I agree with you on most points, somewhat less on something being worthy of your time only if its the standard no matter how good the competition and much less on the shell part. I don't want to keep the shell running, at what point do we accept that users have limited resources? Yet another jvm and a few electrons and some Firefox tabs. 
I like Eugene. He's given a better UI to sbt's amazing engineering from harrah
You have to load some data into memory to keep track of that stuff. if the data you're processing is unbounded then you will sooner or later run out of memory if all elements are *supposed* to be unique. the task ahead of you is to put off when you'll run out of memory. I notice you are taking `Color` items, which almost assuredly involves strings according to the data you posted. Is there any way you can avoid storing strings in this Stream? Maybe map strings to ints to compress your data. Also, you could use a fold operation with (Set[Color],Boolean). When an item comes into the fold operation, you add it to the set and see if the original set is a subset of the new set. If it isn't, you set the boolean output to true instead of false, and stop updating the Set, turning the fold operation into a no-op.