The number of pull request is pretty good in Github for Scala #12: [GitHut 2.0](https://madnight.github.io/githut/#/pull_requests/2018/2) far from Haskell.
&gt; I guess I'm one of those people who don't exist. My personal definition of "Java as nicer Scala" Devs is "Devs who pick and choose to use FP or OOP at will." I don't know if the speaker shares that definition. Every Scala Dev I've ever met does apply at least some of the Scala FP toolset. My biggest frustration as someone who prefers "pure" (or more pure) FP, is that I've run into many people who do pick and choose FP or OOP based on their subjective preferences. On my current team, the hybrid FP/OOP has made it impossible for me to discern any standards or best practices. I can't enforce either FP or OOP best practices, because our "Standards" have no consistency and are made up at will. &gt; but then it'd be worthless for anything else I see a lot of "Java as nicer Scala" devs making this claim. Without necessarily starting a detailed debate, there is a major difference between "I don't know how to solve this problem with FP" versus "FP is worthless for this problem." 
Thank you, did you by chance mean immutable data structures?
The "new" meta programming is already dead/pivoted to other use cases, though. There is now the new new thing, which is very different and changing weekly. Also compiler plugins just got neutered (plugins cannot run before typer anymore) so people who ported their stuff from macros (because they are unmaintained and will not exist in a similar way in Dotty) to compiler plugins, like Sam's `@derive`, now also hit a dead end.
Yes, sorry I meant the opposite of everything I said. Immutable.
Thank you, I will make sure to use that link as a source. From a quick scan it looked helpful. I have a follow up question, I keep hearing all the great things about FP, mainly being pure functions, immutability and reducing Boilerplate code. Are there any times when OO is preferred over FP?
&gt; C++ has an actual standard. Yeah, you can say a lot about C++, but every other language (including Java) would love to operate on C++' level of quality control and judging features based on merit (and not who proposed them). &gt;&gt; On top of that, all features, removals and additions will be reviewed by the Scala Improvement Process Committee (SIP Committee) in a rigorous review process. The process where people just started dumping bulk language change proposals into it and tried to get the acceptance criteria down to a simple 50% vote. Doesn't really instill confidence.
Wasn’t built in staging supposed to replace black box macros and transparent functions supposed to replace white box macros? For type level stuff the latter seemed like a good improvement to me. The proposal also mentions deriving being an open topic but it sounded like tackling that was still considered. In any case what’s on the table seems to be quite powerful.
I'm sure I callously offended by saying it's a joke and I apologize for that. That said, watching folks like the spire devs actually try to make it work felt a little comical. I think it would be hard to argue that Scala specialization isn't orders of magnitude harder to use than c++ templates, and I say this as someone who has attempted to use both. Add on top of that issues like the fact that, as implemented specialized classes store both a reference field and primitive field, making them highly space inefficient for things like specialized tuples, and I find it hard to believe anyone takes Scala specialization seriously as a general language feature.
&gt; For those people like this guy, he should do ETA or just Haskell. I wonder what forces are preventing certain people from just doing Eta or Haskell.
I like uncontrolled imperative things... not all effects are evil.
&gt; What macros are commonly used other than shapeless? Doesn't cats (and all cats projects by extension) use macros to generate typeclasses? (`simulacrum`)
Let's see how much actually survives in the end. With every iteration of macros the expressiveness was cut down, almost to the point where you skip a macro system, do codegen and be done with it. In the end, I think it's just a waste of resources that would be needed direly elsewhere. Macro authprs are probably 0.1% of the whole user base. They have survived bugs, limitations, poor APIs and changes in minor versions for years. It just doesn't matter much how pretty the new API looks like, they work with what they get. I just think the language would be in a much better state if that amount of effort would have applied to interfaces actually used by people.
Devil's advocate: what parts of what John's already specific do you find to be not concrete? There seems to just be a general lack of alignment, like all innernet debates...
&gt; "I don't know how to solve this problem with FP" versus "FP is worthless for this problem." I meant that such an FP-focused language will only be good for solving problems using FP techniques, not that FP techniques are only good for solving a certain set of problems. My point is, not everyone even wants to write pure FP. I personally am not productive trying to think about the world in terms of monoids / applicatives / semigroups / etc. I used to remember and understand these terms but it never stuck. I just use more basic, perhaps less powerful but more intuitive (to me) abstractions, and I'm happy that Scala lets me do that.
Wow Haskell is pretty low on that list, its wear coffeescript is, which is an abandoned language that is no longer maintained.
&gt; Scastie Was written _not_ by Scalacenter, and was better when it was a project on its own. Olegych's work there was great. I wish it was still how it used to be. &gt; New Collections These are awful. They adopted many of the same bad design principles of the previous collections, and perform worse. This is a subject that's been discussed to death, but there's no reason for the Scala collections to be as bad as they are. It's a disappointment. &gt; Scalafmt, Scalameta, Scalafix Are absolutely great ideas. Glad to have them around. The rest I cant say much about because I haven't done much with them. Any sbt and zinc improvements have certainly been marginal, because I haven't noticed them. The MOOCs I've seen for scala in the past have been awful (notably, Odersky's FP and FRP courses) granted, those were Pre-scalacenter Sprees to get new external contributors wouldn't be a big deal, if the compiler was actually maintainable, so I wouldn't call this a good thing. What does bsp provide over lsp? Seems like it only exists for bloop. 
Scalac is so unmaintainable, that it is _usually_ better to just use a different language, if you have the choice. If you don't have the choice, you then have to ask yourself "is spending the time contributing this feature going to pay back the amount of time I spent?" If you're a person who doesn't feel chained to Scala, then the answer is no. The philosophy of Scalaz aligns somewhere along the lines of providing familiar FP concepts to people who want them. The people who find that philosophy appealing are not likely to waste time writing Scala in their free time for long. Especially with the recent egress of very good FP Scala developers.
They have an inaccurate, in-house type checker. 
Thank you for your persistent attempt to shake up opinions.
People who are using `@specialized` without inspecting the bytecode at every stage are doing it wrong. It's one of the features that should have been jettisoned years ago in favor of cooperating with OpenJDK developers on the design of value types and specialized generics. Even if Valhalla ships with exactly the features and semantics Scala could use – there a tons of required changes to profit from it. Otherwise it will exist as something grafted onto the side like so many other half-working things in Scala. That would mean accepting that `Option` can never be turned into a value type. Deprecating the parts that cannot be sustained should probably have started years ago. (I planned on having `@deprecatedType` along the lines of `@deprecatedInheritance` a while ago to help facilitate the migration.)
For a role for an experienced Scala dev, how about "Write me a typeclass to add method .*some-name* to *SomeType"*, where you pick something relatively easy, e.g. add a `.reverse` method to `Int` \- 123.reverse = 321. An essential concept in our world, I think.
Great talk.
Maybe? Certainly there was a time when it didn't use macros, so it could do without them if need be.
I wonder how Miles did it then, there was a long withstanding type inference limitation that people were complaining about for literally years. If you check his commit what he did wasn't completely out of the ordinary
The current OO support is OK,Scala 3 should make FP more easier to make Scala code simpler.
Sounds like Sabin is on it after all? And glad to hear it's sorted in Dotty in any case. Thanks.
That true ,so.do I,but better supporting for FP will make the code more simpler,like the extension method. 
I don't see it as so much of an either/or, because I think a lot of FP style comes from following through on the same principles that OO endorses - separation of concerns, single responsibility and all that. The only part of OO that I think FP really rejects is hidden pervasive mutable state. And the only case where I'd say it makes sense to use that is when the business problem you're solving - the process you're modelling - really does look like that. I don't go all-in on immutability because I do think that some processes are best described in terms of mutation and in those cases direct mutability in your programming language is easier to work with than emulating it via a state monad or similar. But that's only true if you can be certain that logical time will always correspond to wall clock time and you'll never ever want to go back and look at a previous version of the state, apply the same set of changes to a different initial state, etc.
&gt; Ensime has to recompile your code constantly in the background, while ghcid does not. The typechecking feedback cycle is much faster. Also, Ensime isn't maintained and will break completely with Scala 3 (Sam is happy about neither of those things). This is false, Ensime uses Scala's presentation compiler the typecheck a subset of sources in your Ensime project
copy kotlin,fix bugs 
Miles did it by simply currying type application. His patch actually wound up being astoundingly small for what it accomplished. Yes I asked for help. I am not incompetent. 
I think it uses it for code generation. I'm not sure if there are more macro-based tricks to get around an extra allocation, I might be making stuff up. Just saying at worst it would be a "transparent" change to have a no-macro version of cats typeclasses.
keep things out of a language is harder 
&gt; The difference here being that Scala encourages free use of Java libs without protecting you That doesn't match my experience at least at a community level. I've found most of the Scala ecosystem to be pure-Scala these days, particularly with the rise of Scalajs. Java interop is there when you need it - and there are a few gaps in the native-to-Scala library ecosystem where it's the least bad option - but it's by no means the first port of call. It's also worth saying that even if we're talking about Java, reputable libraries in recent years are careful about `null` and looking to better approaches to error-handling in a way that isn't happening in C, IME)
Pure FP will.never be mainstream, funding do help, but better is an ICO . 
&gt; Miles did it by simply currying type application. His patch actually wound up being astoundingly small for what it accomplished. Yes, which was precisely my point.... &gt; Yes I asked for help. May I ask what you were actually attempting to do?
At least all macros will need to be rewritten. Everything that uses macros seems a bit wide, depending on what you mean by uses macros.
You can say that C++ has a standard, but if thats your argument its not a good one. The C++ standard last time I checked is around 1k pages long and even with such a descriptive standard there are still differences between the different compilers
I aggree with you, If Scala can not solve the industry problem, then why would them invest it. The words on the homepage show no benefits about the business objectives. If I am promoting Scala, I will say, Scala is a Cloud Native language,helps you to build and maintaining modern inderstry platforms quickly and easily. 
Logging libraries often have macros to expand `debug(...)` into `if (log.debugEnabled) { log.debug(...) }` so that you can use things like interpolation without creating garbage that will just be ignored by the logger.
https://github.com/scala/scala/pull/5744 I was trying to finish that. Yes I asked questions in channels other than that thread. In the end, I did not give up because it is hard. I gave up because there is _no sane way_ to debug scalac. It's not like I'm saying anything controversial. It is well understood that Scalac is very hard to maintain.
Really not much, Alibaba is one of the biggest company based in China, which are mainly using Java,our stream platform are building on top of Blink ,and there are nearly no Scala code. If you want to say Scala is an inderstry language, it is been used, but Scala never be a main language here. there may be 10 Scala developers here？ What I want to see is we could make the Scala more like an better Java and Better Kotlin, make people buy it. improving the maven and gradle plugin to make it accessible to more people . advising it for simple and productive. Building fancy Scala libs for hot topics,like services mesh, cloud native, graphql,serverless, and providing good Java and Kotlin API,and advertising them too. If the top companies are using Scala as their main language, then Scala will go mainstream. See docker ,react, tenserflow .
The only way I see to make Scala more popular is let the guys to be CTO of Google and Facebook, and then make sure Scala as main language of that company. that's more easier. 
Miles wrote up how he did it [on his blog](http://milessabin.com/blog/2016/05/13/scalac-hacking/).
if you want to look at it that way, yeah. graalvm ce is not as fast as EE, but still shows speed improvements in areas over standard java c1/c2 optimizer. 
Flunk should not be listed here ok.If you could make the spring rewritten by Scala ,then I will buy.
Flink should not be listed here ok.If you could make the spring rewritten by Scala ,then I will buy.
You are right again, they are using Scala to quick prototyping and shipping, but getting more Java or rewriting to meet the market. We do not need to deploy our own Spark or Kafka any more, there are clouds there. Scala should be, simpler ,better fp support,the OO part should be enough now.And help it to be a dumb simple language to write CRUD,yes,the business logics,which is not cool,but make a lots of money. 
vavr 
Thank you very much, I love them all 
Scala is for everyone 
was
You want to see something more comical than Spire? There you go: https://github.com/denisrosset/metal Basically using macros to get around the fact that `Map[Int, AnyRef]` cannot be `@specialized`. BTW, I'm a Spire maintainer, I use it for exact computations (`Rational`, `Algebraic`, ...) where it really shines.
Are you aware of any progress since http://cr.openjdk.java.net/~jrose/values/value-type-hygiene.html , where each classfile had to declare the types it would use as value types? (Thus kill any `Option` prospects?) 
Yup, I did admit that in another comment somewhere. I confirmed with Sam about the actual behaviour.
That's good to hear! :)
It's the culture that you find where tools like Mockitoo are prevalent that has people up in arms, not the tool itself.
Could you elaborate? What culture problems do you see at places that use Mockito?
Historically: Mockito =&gt; TDD =&gt; Spring =&gt; Over Engineered Java apps
All Java libraries can be used in Scala because both languages use the JVM. Almost all of the types in Scala are also in Java, or just renamed (eg `scala.collection.immutable.List` is the same as `java.util.ArrayList`). While it would be nice to have Scala-specific documentation, it's unnecessary to port an entire library.
Wouldn't be nice if Scala SDK is non-blocking / Async based?
I personally like Mockito. I guess to each their own. Mockito is not without its issues, but every mock library has some. specs2 has a nice [Scala version](https://etorreborre.github.io/specs2/guide/SPECS2-3.4/org.specs2.guide.UseMockito.html) of it also for ScalaTest, so there is definitely support out there for it and not completely frowned upon by the community. It makes code more more Scala like: val daoM = mock[SomeDao] daoM.foo1.returns(2)
This is good to know, will try using this at work. I've all but forgotten about Mockito or any mocking library
Nice! As someone with a slight Racket background I am fond of macros and its nice to see more practical uses of them.
I find people overdo it with Mockito and miss bugs in code that would otherwise be tested, but was instead mocked. I find if you only mock at the boundary of a service (ie mocking the database or mocking something upstream) it’s much easier to use Mockito in a way that doesn’t inhibit correctness of your program.
&gt; I have not yet seen a serious proposal coming from the Scalaz community We did it years ago. It's called ermine. It does Java interop properly.
Any differences between the compilers that are not *explicitly* due to "undefined", "unspecified", or "implementation-defined" behavior is a bug in said compiler(s). However, that's a sacrifice made to the Great God of Performance and his twin the God of Portability (to different hardware). There's no reason all the behavior could be specified like e.g. Java does. That's not to say that compilers or the standard doesn't have bugs such that compilers will behave differently in edge cases, but they're considered just that: bugs, not just differences of opinion.
I've found huge security holes in software because Mockito was used so the Mocks where instead being tested not the real code. It seems a common smell with Mockito, people mock everything and it ends up what is really happening is you are just testing your mocks from your mocking library. Also with a language like Scala you can avoid the need of a mocking library completely, just pass functions in as args. With that i see Mockito as a code smell as it can be completely avoided in Scala, it tends to only creep in from people clinging to the Java / Spring world as it's familiar. If you can test your code without a mocking library it is almost certainly far simpler code composed togethet underneath...as you don't need a mocking library to test it.
Re the Scala 3 fearmongering, does anyone remember the SBT 1.0 fearmongering? Or we all forgot it because it was executed so well?
For mocking in Scala, I personally prefer \[ScalaMock\]([https://scalamock.org/](https://scalamock.org/)), but I still love using Mockito for Java. Like any other tool, tools are supposed to ease our work. Problem is, many developers are using tools without fully understanding them, eg. due to company mandate, which leads to them to use it wrongly, defeating the purpose of the tool.
The problem is that `Option` has subclasses, that outright kills any value type prospects. You could either - throw tons of compiler magic at it to keep the user interface unchanged and emit completely different, value-type-able bytecode (there are plenty of examples how this turned out in the past), or - rewrite `Option` completely (as described above).
although, trait parameters mitigate that to a certain degree
One thing I never understood about akka's supervisors... In the example given, one actor pipes a future to itself, which fails, and so it gets a Failure(e). At this point the actor chooses to throw to let the supervisor take over. Why not just send a regular message to the parent (and maybe terminate yourself too)? It seems like an artificial distinction between communicating errors and non-errors. Ok now one answers is: what happens when when an actor does throw? There needs to be some way to handle that... So you need something like supervisors. But in Akka, all io is going to happen in a Future, so that might fall, but that's different than the actor throwing, and sure a programmer might make a mistake and get say a null pointer exception, but a programming mistake won't be able to be handled by the supervisor. And, of course, anything synchronus and dangerous can be wrapped in a try. Anyway it seems to me that the *authors* of Akka need to assume that an actor might throw and so created supervisors, but the users of Akka should probably just never throw and ignore supervisors. 
I didn't watch or hear the actual talk but I read that slide totally different. It looks to me that it's saying that in contrast to Pizza, which did zero innovation in OOP relative to Java and instead added some functional elements on top of Java, Scala did include plenty of innovation on the OOP side. It then goes on to say that Scala is more functional than Pizza too, and more seamless in its integration of the two.
What would it take to make scalac easier to debug? In any case your work is definitely appreciated. Meanwhile it looks like Miles took on that particular PR but hasn't gotten to it yet.
I don't think that refutes /u/joshlemer's point, that abandoning the OO side of Scala would be very harmful. The truth is, I'm not entirely sure what you're advocating for. Surely you aren't being serious about removing subtyping from Scala's type system? I mean five minutes ago you were complaining that Scala 3 is a new language (which perhaps could be true in a very academic sense, but for anyone using it, it's just removing things people complain about (which will be done gradually) and adding new features. But if you remove subtyping, surely it would alter the language completely beyond recognition? So, I'm assuming you don't mean that literally. I suspect you mean that language changes should prioritize improving the experience of FP purists somewhat, or something like that. But it would be great if you could clarify.
&gt;it's natural for people to want to move on at some point For the record, I didn't want to move on. It's not like I was tired of the work. But I could see the handwriting on the wall. Indeed, here we are almost five years later, and (in my estimation) nothing of importance has materially improved, and opportunities like the collections rewrite are being squandered.
If you can "quote numerous issues in which suggestions are rejected on grounds they are 'too FP'", then please do. Saying "I think you know what I'm talking about" and linking to the entire Dotty issue tracker is just disingenuous. I follow a lot of those issues. I've also read and/or followed a number of issues on the C#, Typescript, and ES6 repos. I don't see any difference in approach that are relevant to your implications. (I just recently linked to the opaque types SIP on a typescript ticket where they were discussing solving a similar problem. So far at least one person found it interesting. :) ) 
Less mutable state, less cake pattern, more traditional (or at least principled) compiler design. This is generally true with all software projects though, so this doesnt help probably. Lol
Even with scala that's not entire true. You should be isolating and testing your resources out individually. 
Umm no, Scala is not a research language. It's true that its designer has more of a background in language design than many other languages, but the design goals are always practicality. Sometimes that doesn't work out in the long run (XML literals are a good example), but that doesn't justify false assertions. Some people might remember that just a few years ago, a number of people were complaining that innovation in the language had all but ceased, possibly due to Lightbend's focus on solving industry problems -- perhaps too much. To me it seems almost as a response to that, projects such as Dotty were formed, so that both camps could be satisfied. So yes, after a while Martin started once again putting more effort into researching how to evolve the language. So, yes Scala began in an academic institution, but its aim and focus was always (though not always successfully) to solve actual problems in the industry. And yes, Scala has always had a terrible PR problem. (Which giving talks like this both help and harm.)
It can’t be completely avoided, like when passing in a function that returns unit as an argument then verifying that function is called. 
Mockito also behaves weirdly when dealing with default arguments (you’ll see double on some of the call counts). I don’t think the library itself is hated, the technique can be incredibly useful in some cases but I have memories of Java code with about 10 layers, mocking at every level, refactoring that code was a nightmare.
IMO one of scala's biggest achilles heels is the fact that its staunchest supporters are its most vocal critics. Scala users love to criticize Scala. (They love to criticize most other languages even more, but they don't usually do so unprovoked.) In contrast, for most other newer languages, I always hear people going on about how great it is. I think it's because Scala tends to attract people that have seen it all, many have the perspective that software in general is terrible (which is true in many ways) and Scala is just orders of magnitude less bad than $SOME_OTHER_LANG. On a more practical level, there are tons of talks at lots of conferences with people advocating any technology you can think of, Kubernetes, Kotlin, Sprint Boot, React, and so on. While Scala developers give "teaching" talks at Scala conferences, but how often do you see talks at Java or general programming conferences about how awesome Scala, Play, or any other Scala-related technology is? Similarly, the state of documentation would be a lot better if the Scala camp had a similar percentage to other languages' of people who love blogging about existing features. (I feel like most blog posts in the Scala community are about sharing new ways to do things in FP, which is great, but doesn't address the issue at hand.) Maybe I'm exhibiting grass-on-the-other-side syndrome. But there are a lot of ways people in the community can make Scala more interesting to people who haven't gotten too acquainted yet. Anyway, one thing to keep in mind is that in the last few years, there has been a sort of explosion in programming languages. The pie is being divided into many more slices. (The good news is the pie is growing too...) And of course, most of the popular languages are backed by a lot of muscle. C# and Typescript have MS (no longer taboo in OSS quarters), Kotlin has Jetbrains, Go has Google (which also has Dart, and supports Typescript and Kotlin), and Java has Oracle. And Javascript has the web itself. So most of those aren't fair competitors. There are a lot more points in the talk I could counter. But the bottom line is, I agree that we need to get better at this game, but I don't think giving talks about how dire things are is going to attract more people to Scala. I mean if you really tried every other channel and it's the only way to get people's attention, fine. So again, if you want Scala to improve, you need to engage in dialog. That means open, bidirectional settings (like gitter, etc). And yes, that means you are forced to phrase it in a more civilized and less theatrical way. And you (along with everyone) need to accept that your view, like everyone else's, is only part of the picture, and issues are complex and have multiple sides to them and are usually subjective. And if you want Scala's popularity to improve, then give talks about how awesome Scala is, at conferences where people aren't already in the Scala choir.
This is a great question. I've been using Akka for a few years so I can answer this from my experience but I'm not sure I really know the right answer. I tend to let an actor handle all of its success responses and all of its *expected* errors. That way unexpected errors crash the actor and the supervisor can restart it or take different action. An example would be trying to fetch a database row. The success response is just the row. Expected errors might be the row doesn't exist, the database times out, a transient network error and so on. The actor can handle all these scenarios. If you get unexpected errors like the database table doesn't exist or you don't have the right credentials, that's not something the actor can recover from and the supervisor is better placed to decide what to do.
Came here to say that. There's so much work going into this series; very informative and well written.
While I have great respect for jdg there are some issues with this talk: * Typelevel ecosystem is thriving and has great fp projects including Cats, Monix, Doobie, Circe and Fs2. This is not mentioned. * Having Oop is a great gateway so we can convince our pointy haired bosses and mainstream colleagues that Scala can be gradually adopted into a project. * Reactive and actor based patterns, Lightbend and so on don’t get a mention and yet are used throughout industry and are very valuable tools * ZIO maybe 100x faster than Future in certain benchmarks but can I get a 100x speed up on my real world oop code, my actor code, even my future based code, by switching to ZIO? I feel like that's false advertising.
Side note, I have seen too many times where people give a talk about some amazing Scala technology, and they undersell it. Often, in the Q&amp;A segment someone in the audience asks a question and the speaker interprets it in a defensive sort of way and gives a minimalist answer. A recent example, to pick a scapegoat: in a scaladays berlin 2018 talk on scala-native, someone in the audience asked why use scala-native vs. C. The speaker, who in the talk was focusing specifically on how to get as low-level as possible when necessary in scala-native, seems to have understood the question to mean, if one is only doing such low-level operations why not just use C, and gave (at least at first) a pretty milquetoast answer, that almost sounded like, no real reason not to just use C. He did eventually throw in something about that you can use higher-level Scala features in scala-native too (besides having mentioned earlier in the talk how much better the scala idioms are for expressing low-level things). Perhaps the speaker actually was right about the questioner's intent, but I doubt most people watching got the right message. Again, sorry to pick a scapegoat -- this is the sort of thing I see way too often in talks. I feel like if people gave more enthusiasm-filled talks to more technologically diverse audiences, it might help a lot.
So it's not hating Mockito, but constructive critics of an idea of mocking. I agree. Tests, that just verify the order of invocations on mocks are not very valuable and they require big effort in maintenance. But mocks are useful while doing unit tests, especially with TDD. 
I don't think anyone is implying that if your code, which is X% time spent in async libraries, would switch async libraries, your code would as a whole run 100X faster! But the implication certainly is that lightbend's concurrency operations are suboptimal, possibly inherent to their design, and perhaps the X% of your code would be 100X faster. Now, whether that's true or not, I can't say -- if indeed those are microbenchmarks that don't reflect real-world usage, then indeed that might be false advertising, or at least misleading advertising (not that much better!). You probably meant that, but why so skeptical? Anyway, definitely agree with the other points. Although, IMO, OOP is more than a gateway drug. It's a toolset that is often useful. But of course that is an old debate...
I prefer scalamock to mockito as well but rarely use either. I believe much of the mocking hate comes from the values love. Even in java, nobody mocks values. You don’t mock an Int, or a String. I think in general if you have a community advocating programming with values, they are not going to be advocating mocking, because they won’t really see much value in it. That said, if I come into an existing codebase that is hard to test I might reach for mocks, but would probably think this is a crutch I am reaching for to deal with a design issue rather than a nice solution. Also, I don’t think there is much hate in the Scala community for test implementations, just mocking libraries which are historically used for code that is hard to create a test version of. If you have an interface that typeclass based for example, making a TestDb interface shouldn’t take any more time than mocking your RealDb interface and there will be zero magic behind the implementation and no need to read docs on a new tool.
I heard Jonas on a podcast once talking about the origin of Akka. He said he was doing a lot of JVM contracting, and also playing with Erlang and OTP on the side and was really enamored with Erlang’s Actor programming model so he created Akka so he could use that programming model on the JVM. Which makes Any =&gt; Unit make a lot more sense when you realize it was meant to emulate Erlang’s dynamic typing and runtime plugability. Whenever people talk about the different groups writing Java in Scala, Haskell in Scala, or Ruby in Scala I like to bring up the large number writing Erlang in Scala :D 
I'll normally use an AtomicInt for this, pass in a function which increments it per call then assert the count is as expected. Save's bringing in Mockito for verification and i only see these use cases when generally wrapping Java libs and not enough for the 2-3 lines of boilerplate to bother me.
&gt; The process where people just started dumping bulk language change proposals into it Language changes that make sense together, and for which the discussions were likely to be very similar, so they could be factored out. When it turns out that discussions are in fact not similar, batches get decomposed to accommodate that. This has already happened, I'm not just hoping here. &gt; and tried to get the acceptance criteria down to a simple 50% vote. Absolute nonsense. *One* person tried that, and the process stopped the idea. I think the process did really well here!
Sure, if i needed to use a mocking library i would almost certainly go with Mockito, it's been around a long time, people are familiar with it etc etc. At the same time I try to design code so I do not need a mocking library, i think that is the underlying argument against Mockito.
You probably have powermock in mind. Mockito is great...
FYI, Haskell development is primarily done on GHCHQ's Phabricator instance.
I find it hard to come up with other professions where this practice is as common as in software development. It's a bit creepy and unsettling, the way I see it.
&gt; I can't enforce either FP or OOP best practices, because our "Standards" have no consistency and are made up at will. Where's the conflict? To my mind good FP practice and good OOP practice are largely the same thing - ensuring that your classes have separate responsibilities, favouring composition over inheritance etc. is the right thing from both perspectives.
Its funny that a lot of people here think Kotlin will be the next Java and beat Scala. Nothing can be far from the truth. I have been observing kotlin since its launch, it has been brought up many times in the company that I work for(not by me) and it has been declined every time for back end development since Java is fast catching up. It's making a lot of noise in the android space since Java 8 and higher versions cannot be used there. Yes, Scala is niche, but its usage is steadily increasing. I think its best for Scala to be in the FP + OOP form. If Scala goes down the path of pure FP, it will definitely lose. Eta will at-least take two to three years to be production ready and will be much better in pure FP in comparison with Scala since it is a Haskell port. And also, what is preventing people from moving to Haskell if its so appealing? Pure FP becoming mainstream is far from reality. It might not even be that way in the foreseeable future. Scala covers a lot of middle ground i.e become a better Java + ability to do decent FP is what many people want. 
We hate because those that love it, uses it wrong. People that like it use everywhere and in every invalid way of use it. Just take a look on they own documenation, they describe many of those anti-patterns. For instance: "Don't mock type you don't own!". https://github.com/mockito/mockito/wiki/How-to-write-good-tests If you use it correctly, is very handy tool. So is better to hate it and use only when it need to be used, that love it and use it wrong :D
Rules of the process are here to be enforced. I am optimistic with how we're all handling this topic, /u/simon_o.
Yes but what is important here is the development of the ecosystem, not the language itself. I mean Haskell may be on Phabricator but Coffeescript is an abandoned language.
&gt; First, some facts: Flink is almost entirely Java, Kafka is mostly Java (despite starting as 100% Scala), Play is 1/3rd Java, and Akka is nearly 40/60. In other words, many of the libraries you cite as "Scala libraries" are actually Java libraries, and 100% of them have Java APIs and are nearly as usable in Java as they are in Scala. Using an expressive language to iterate on a system and get the architecture right, and then eventually rewriting it into Java as a long-term stable form once the design has ossified, is a common pattern for many languages, not just Scala. The likes of Kafka are very useful systems that deliver a lot of value to people and would not have been possible without Scala; that's a success for Scala regardless of what language the current implementation ends up being written in. &gt; As a result, there are fewer and fewer reasons to choose Scala. Even if you choose Scala, if you want major adoption (like Spark, Akka, Play, etc.), you still have to target the Java crowd too, which leads to a lowest-common denominator design for the API. Akka, Play, etc., may as well be written in 100% Java. &gt; If Spark were started today, the authors would not choose Scala (IMO), because Scala is not the best "Better Java" language anymore. If they didn't need the JVM (the JVM is experiencing some decline!), they'd choose Go, or maybe Rust. If they needed the JVM and could wait on a few features like pattern matching and data classes, they would probably choose Java, which has become tolerable and is getting better. If they couldn't choose Java, they'd probably choose Kotlin. To the extent that that's true, mission accomplished, and maybe Scala should go gentle into that good night. I don't think any of those languages are quite good enough yet though. Even for something as basic and common as JSON serialisation, the API one can offer with Scala (based on typeclass derivation) is head and shoulders above any of the languages you mention, so even Play would become less useful if it were written in Java. Even for a user who doesn't write their own higher-kinded types, having that functionality in the language is what makes the likes of treelog possible. You're right that the bar for what functional features are ordinary and commonplace has risen. But by the same token, the kind of advanced FP features that Scala offers are now less of a leap than they were - that "lowest common denominator" is not so low any more. Systems like Spark and Kafka are written in, essentially, mainstream programming style with a few cautious extensions - anything more would be too much for a large system. As the mainstream advances, Scala remains ideally placed to offer "mainstream programming style with a few cautious extensions". If Kotlin is the new Java, Scala is the new Scala. &gt; I have thought about that a long time and my own answer is that Scala is better at FP than the other JVM languages. But Scala was never the language that was best at FP, even if we restrict ourselves to JVM languages for some reason. The thing that's unique about Scala, that no other language does, is that it offers a path upwards from a Java-like style to a Haskell-like style that lets you remain productive every step of the way. &gt; I think focusing on FP will inspire FP developers to stay and contribute libraries that are much more powerful, principled, and type-safe than Spark, Kafka, and many other libraries that helped push Scala into industry so long ago. Existing FP developers may stay, but for many of us Scala's support for both OO and functional is what made it possible to become "FP developers" in the first place. &gt; Pursuing Kafka, Spark, etc., is a distraction. Recognizing that, in my opinion, is the key to success. The goal is not language design for the sake of it, the goal is making people's lives better by making it easier to write high-quality, maintainable software systems. The likes of Kafka, Spark, etc. are the great successes of Scala - more subtly, so is the wider adoption of pattern matching, local type inference, and case classes. I think we may agree that we want Scala to be the language of the next Kafka/Spark/etc. - but if you think the way to achieve that is to focus on pure FP and away from mainstream programming culture, then I'd ask: why is it that Scala has produced widely-used systems like Kafka/Spark/... and Haskell has not? Whatever factor it is that enabled Scala to produce those systems, I want to keep it, and I don't think it's just the JVM.
"Improve the process" is not actionable - or, to the extent it's actionable, it's already been actioned (e.g. the new SIP process).
&gt; does anyone remember the SBT 1.0 fearmongering? Every time I check out a project that's still using SBT. &gt; Or we all forgot it because the transition was executed so well and all the dire predictions didn't come to pass? Speak for yourself. I got off SBT for good and I'm never going back to it.
I would not recommend using Mockito in Scala for technical reasons - Mockito is heavily based on runtime reflection and sometimes may be very surprised of what it finds in Scala classes. For example: try mocking a method that takes or returns a value class.
The immediate, if subtle, problem with mocking like example 1 is magic that you can't refactor according to the normal rules of the language. In normal Scala your code would be obviously equivalent to: val p1 = when(daoM.foo1) when(daoM.foo2).thenReturn(4) p1.thenReturn(2) assert(testSubject.bar() == 8) But in fact this will fail. In a small example like this it doesn't matter so much, but in a larger test it can be harder to see; what tends to happen is you get a chilling effect where people simply stop trying to refactor their tests to remove code duplication etc. (because you can never tell when a simple refactor will mess up the mocking) and the code quality of the tests goes down and down. Whereas example 2 is plain old Scala that follows the normal rules for Scala and can be refactored normally. (And as you say, interfaces should normally be smaller than this, so the problems with example 2 are much less of a problem in real Scala codebases; for "repositories and daos" you might be able to write a generic method, or avoid needing a separate service at all). There are a couple of other issues that make mockito worse than other mock libraries: * It has poor support for strict mocking (the default is to return `null` for calls that weren't explicitly mocked and in practice nothing will work properly if you try to change this default), meaning tests often pass by accident or fail miles away from the actual failure. * It has explicitly no support for multithreaded use. If you use the same mock from multiple threads it will break nondeterministically and bugs will be closed as WONTFIX. 
&gt; I just think the language would be in a much better state if that amount of effort would have applied to interfaces actually used by people. I disagree, macros are used heavily by shapeless, slick, cats, circe, scalatest, sbt itself and our company's projects. Almost every big Scala library right now is using macros right now, with exception of maybe Akka. Losing macros means losing some of the bigger scala sellers like `slick`.
&gt; What macros are commonly used other than shapeless? `slick` is macro based for example, and `sbt` is chock-full of macros.
I don't even see how removing subtyping makes sense from `jdegoes`' own point of view – he's been listing ot as a big reason why Bifunctor IO can work in Scala, whereas it doesn't work so well in Haskell.
&gt; What does bsp provide over lsp? BSP complements LSP by providing abstractions over the build tool in the same way LSP abstracts over the editor. At the moment the barrier of entry is really high to implement a Scala language server because you need integrations with several build tools (sbt,gradle,maven,...). &gt; Seems like it only exists for bloop. Bloop is the only available server at the moment and the IntelliJ Scala plugin is the only available client. Any build tool are welcome to implement the spec if they want a better integration with IntelliJ https://github.com/scalacenter/bsp/blob/master/docs/bsp.md
Yes but you are missing the point I am making, which is that the standard is kind of pointless. Haskell has its standard (the Haskell98/2000 standard) but its irrelevant because everyone uses GHC, there aren't any real alternate compilers. Also the standard pales in comparison to what typical Haskell code does with all of the GHC extensions. If there are alternate backends which need to be targeted its much more feasible to extend the current GHC backend (i.e. GHCJS for Javascript) rather than to implement a separate new compiler that follows the spec. The same deal is with Scala, whether or not the spec exists is kind of besides the point because there is only one real mainline compiler. Just like with GHC we have scala-native and Scala.js. The standard only really makes sense for C/C++ because such languages have to target a lot of OS's, architectures, but even with this standard in place it still hasn't completely improved the situation. So really I have no idea what you are trying to argue on this "Scala's standard" point.
I'm not talking about whether you like SBT or not. I'm talking about the suggestion that the ecosystem of plugins would get stuck on 0.13 and the SBT community would splinter.
Why do you need to test that a function was called instead of the results it provides? Internal implementation details shouldn't be exposed and definitely shouldn't be tested.
Ok, so I guess you don't mean specifically debugging, but working with the codebase altogether - understanding and modifying code. My understanding is Dotty ticks those boxes to a significant degree, but obviously you want to fix it in Scala 2. So, given the current codebase, is there anything that would make it easier that you can suggest?
Well any compiler for any mainstream language is hard to maintain, even GHC has its own issues (the compiler was written in the 90's in a style that is very different to modern Haskell). I mean also putting things into context, you are trying to work on a fairly advanced feature (type inference of high order GADT's). Also as pointed out, the issue isn't debugging but the fact that the compiler is written in a different style that you are used to.
I don't think people specifically hate Mockito, but if you write in a more FP (or pure FP) way, mocking is basically almost never done, you just end up passing different arguments to functions (or set up your `case class` in a different way). Mocking is something that you need if you mix state with data, which is something that typically you shouldn't be doing in idiomatic Scala.
&gt; ZIO maybe 100x faster than Future in certain benchmarks but can I get a 100x speed up on my real world oop code, my actor code, even my future based code, by switching to ZIO? I feel like that's false advertising. Its basically the exact same style of advertisting that MongoDB does when they said that their database was 100x times faster than PostGres (or w/e the figure it was), completely ignoring the fact that MongoDB wasn't properly ACID (and thats putting it lightly, in the early days MongoDB would silently lose your data. In Task/I/O/Future types, you often have a spectrum. On one end you have performance/throughput and on the other end you have fairness. Cats/ZIO are optimized on the extreme end of the performance/throughput spectrum (at the cost of fairness), where as `Future` is optimized completely for fairness (at least with the default `ExecutionContext` which is backed by a `ForkJoinPool`). Task types like Monix `Task` sit in the middle, and try to intelligently/automatically move up/down the spectrum depending on the type of tasks you are executing. So yes, when this figure is being thrown around, it just reminds me of the sales tactic that people use to sell their software, devils are always in the details.
A lot of modern Java libraries also use the `@NotNull` annotation which is quite helpful when it comes to wrapping Java libraries, i.e. you don't always have to wrap values in `Option.apply`. Exceptions are annoying, but this is stuff like `scala.util.Try` exist
Same, I really tried to go down the deep end but it wouldn't stick.
What is being squandered in the collections rewrite? There's still time to make changes if you want something to make it in!
This is turning into /r/programming... all blog spam no community
&gt; less theatrical way Things that we're not going to get for 500, Alex.
&gt; opportunities like the collections rewrite are being squandered Could you provide an example about how it is being squandered currently?
That completely missed the point.
TIL that everyone hates Mockito. I've used it for several years with great satisfaction.
I don't think you'll see a lot of advantage from switching to monix `Task` if you're not hitting problems with scala `Future`. (There are issues with scala `Future` being implicitly started, but if those are a problem for you then you'll know they're a problem). What is the problem you're trying to solve here?
To be on the safe side, I would double check on that. Miles has lots of things on his plate so he may welcome some help :)
&gt;Any sbt and zinc improvements have certainly been marginal, because I haven't noticed them. Many improvements on the Zinc front. Check the issue tracker, the PR queue and the announcement of Zinc 1.0 featuring the new incremental algorithm started by Grzegorz Kossakowski. &gt; Was written *not* by Scalacenter, and was better when it was a project on its own. Olegych's work there was great. I wish it was still how it used to be. Nobody has said so. Re-read the message. 
Hi, I will join a team using scala and spark next month. I have background in Java, javascript/es6, python. Have some knowledge about FP but not using extensively. As a scala newbie, what you think i should focus to be a good scala programmer.
I agree but sometimes you have to, normally interacting with some imperative Java API but sometimes you have to in a Scala first API to. An example which is still fresh in my head is FS2 Sinks where a Sink is a `(`[`Stream`](https://oss.sonatype.org/service/local/repositories/releases/archive/co/fs2/fs2-core_2.12/0.10.1/fs2-core_2.12-0.10.1-javadoc.jar/!/fs2/Stream.html)`[F, I]) ⇒` [`Stream`](https://oss.sonatype.org/service/local/repositories/releases/archive/co/fs2/fs2-core_2.12/0.10.1/fs2-core_2.12-0.10.1-javadoc.jar/!/fs2/Stream.html)`[F,` [`Unit`](http://www.scala-lang.org/api/2.12.4/scala/Unit.html#scala.Unit)`].` Given some `I` value I want to do something, probably side-effecting, write to a DB, log a message, post to a web API etc. For other `I` values I want to do nothing, how do I test the action only happens on the correct case? Sink is a stream of \`Unit\`. You could do something like: def myStream(sendToDb: String =&gt; IO[Unit]): Sink[String,IO] = Sink[String, IO] { case str @"Hello" =&gt; sendToDb(str) case _ =&gt; IO(()) } You can then unit test the above by passing in a function that increments a counter when it's called and assert the number of invocations match the expected from your test data, you could also collect the invocation count and arguments called with by appending them to a list. It also means it's trivial to change the implementation of the side-effect function while the logic remains the same. It's a trivialised example but it does demonstrate sometimes there is a need for verification. Obviously where you can avoid this and return some value as it makes things far simpler evaluating to a value you can assert against.
We are also hiring at Tray.io London. We work to help big clients automate! We have a very talented team of engineers and we're looking for top team players to join us. Hit me up at luciano(at)tray.io(dot)com or check our open positions here: https://tray.io/jobs
&gt; dirty mutability tricks are just as possible in Haskell I'm not so sure about that. What I'd call a mutability trick, for example, would be: I have this algorithm that maps some function over some structure. Now new requirements come in and I can just add a dirty effect to the function in order to aggregate some new info on the side. You can't really do that in Haskell, where you have to _plan_ your mutability in advance or refactor everything to accommodate it. Now don't get me wrong, the trick above is terrible from an architecture and maintainability perspective, but it gets the job done with a fraction of the headache. It's a compromise. I think this is one of the concrete reasons Scala is often more practical than Haskell.
&gt; Sacrifice half the community, send them to Kotlin. This one bugs me. I think John and other FP/type aficionados represent a strong, but minority of Scala developers in this community. It's a subculture. If it was half the community or larger than organizations (for profit or otherwise) would invest. I think this is indication that the majority of Scala users are happy with language and ecosystem in general and the effort that fulltime organizations put into it. Instead of stirring controversy do what all healthy OSS communities do, fork the language or create a new one. The type-level compiler fork is a good start, but maybe to address the concerns with the maintainability of the compiler they can fork dotty. I anticipate that even if this subculture gets everything they want, that there still isn't a large enough community to support organizations investing money and fulltime engineers to.
Well, unit tests are written as a last resort on my team... we tend to reach for functional testing first. I feel that’s part of why overuse of Mockito is considered a Hard No. 
I work at [africastalking.com](https://africastalking.com) a kenyan SMS, Payment, Voice and Airtime company building a pan Africa solution, we use scala and Akka extensively(90&amp;#37; of our codebase). Scala is awesome, I bet you. we never regretted moving to it. yes there are some challenges for newbies, but the overall advantages out way them. 
John's argumentation make me think of Brexit argumentation. Keep bringing up pains everyone has and pretend people are not trying to address them (which is false, as other parts of this thread show), and push for impossible solutions (e.g., remove subtyping) with zero actual concrete plans moving forward.
&gt; scala.collection.immutable.List is the same as java.util.ArrayList It's actually not - `scala.collection.immutable.List` is a stack, not backed by an array.
Oh man, I would hate to be the speaker following up here. Hey guys... so uhh... I added another feature, the virtual inheritence monad! ... for my thesis 
Rewriting the system, like what the dotty team is doing. I haven't looked at the dotty code base, so I can't say anything about it. Scalac is way too far gone, probably because it was architected around the time that the cake pattern was hot 
I’m talking about unit tests here, with an integration test you can test the results of the two or more functions together. If you are passing in a callback (the function to be called) then the code you are calling should not be concerned with what that callback actually does, only that it was called correctly. 
That’s an option, but you’re essentially writing your own mocking library. It escalates if you need to pass arguments to your callback, and further still if that callback can throw an exception. I don’t think Mockito is always needed, but it’s nice to have it available at times. 
I don't want to ever be doing reflection, bytecode manipulation, dynamic proxying or anything like that on my codebase. I'll take a little extra verbosity in return for keeping everything in plain old scala.
Separate your business logic from your side effects: rather than having a sink that does something side-effecting for some values and nothing for others, have a sink that does the side-effecting thing unconditionally and have the logic as a pipeline stage. Then you can test the logic by checking its output is equal to its input, and there's no logic in the database-writing part to test; you probably want an integration test to verify that it really does write to a database, but there's no value in a unit test on top of that.
I'd strongly favour EasyMock since the explicit separation between setting expectations and running with the mock makes its error reporting much better.
Thanks. I realize it's `scala.collection.mutable.ArrayBuffer`, which is mutable, but I'll update it with a better example.
I quite like it, to the point contributed mockito-scala to the main framework which is soon to be released, this is to overcome many of the problems mentioned here. I know I could just use scalamock, but in some companies, for different reasons use mockito so let’s better have a more scala friendly one Please check out https://github.com/mockito/mockito-scala, binaries will be in maven central soon, comments, contributions and constructive criticism are welcome. As others said, some tools get hate because some people use them incorrectly, but in that case you have a people problem, and those problems don’t get solved that easily 
That's a great point. You should be able to understand the importance of types even if the language doesn't care about them. This will help you with all programming languages you will encounter. People in bootcamps are mostly being taught JS or Python which have no types at all. JavaScript is the most used language with professional developers according to [Stack Overflow](https://insights.stackoverflow.com/survey/2018/), even though it's one of the most loosely typed.
&gt; they can fork dotty The activation energy for this is too high. It's far easier to complain.
PhD scum! Side effect off with you!
That's your own limited vision of the world. The hard facts (check any trend / metric) are stating the exact opposite. There are also some not so subtle clues. The major Java frameworks and tools for example (Spring, Vert.x, Sparkjava, Gradle, etc) now come in Kotlin flavors. Scala will never recover simply because the barriers of entry are too high (scala tries to reinvent everything) while Kotlin embraces the stuff which is already there. Programmers out there don't want a baroque language which not even their own devs can agree upon how to use. They need pragmatic solutions for real life problems instead.
That's a very good idea. I really like both and Scala always felt like the elephant in the room.
Did this happen? Tuned in last night and there was nothing.
It’s today (12th)
I thought that the two groups did not have an intersection. I mean I love Clojure (and LISP in general) and after using any LISP on a real project programming in any other language feels like wearing a straitjacket. They are also at the far ends on a baroqueness scope: Scala is an abomination, LISP in general has virtually no syntax.
Isolation is only one property of a useful test. I find that many people mistakenly place it above all other priorities and end up with brittle tests that don't actually cover some of the most difficult parts of any code base (interacting with external resources, be them files, networked services, databases, etc). Ideally you should write both small, focused unit tests, and larger, more comprehensive functional tests, but in practice that rarely happens. Given the choice between which kind to focus on, I'll take functional tests the vast majority of the time - they are much better at not breaking when implementations change, and not hiding breakage from external changes (which, while not directly "your code's fault", are breakage either way).
&gt; The hard facts (check any trend / metric) are stating the exact opposite What hard facts? Don't tell me TIOBE, Redmonk, they are almost useless metrics. &gt; The major Java frameworks and tools for example (Spring, Vert.x, Sparkjava, Gradle, etc) now come in Kotlin flavors Agreed, they are adding kotlin support. That's because it is one of the official language in kotlin and it makes sense for them to add support. That didn't happen for Scala because it had its own sets of better frameworks such as Play, Spark, Akka etc., &gt; It will never recover simply because the barriers of entry are too high (scala tries to reinvent everything) while Kotlin embraces the stuff which is already there I am sorry but that is a load of plain BS. Scala does it different because in that view, its better. Kotlin is just trying to be a better Java which it will fail terribly since Java is adding features considering Kotlin. &gt; Programmers out there don't want a baroque language which not even their own devs can agree upon how to use. Twitter has a huge codebase on Scala. Many other companies including the big names such as Aws etc., have started more Scala teams. I really don't understand from where you draw your opinions. There will always be discussions and disagreements, but all of these doomsday predictions is unnecessary wastage of time. &gt; They also don't want to learn custom tools with inconsistent programming models (I look at you sbt). They need pragmatic solutions for real life problems instead. You do realise that we can use gradle and also maven with Scala right? How can one build tool define the whole language ecosystem? SBT is weird in its own ways and I am sure that there other tools such as Mill which can be used. 
I'd agree with you, if Scala's unmaintainability was some new idea that I just came up with. Ghc may be difficult to maintain, but debugging it via standard methods won't break it like it does scalac. 
Why not just use the existing NY Scala meetup group? Seems incredibly niche...
Hate it? No. It's just unnecessary. I mean, your average application has http and databases at the edges of the system and the popular libraries for those offer abstractions that are always going to be easier to use than some sort of reflective magic. All the rest of it can be composed of pure functions - why do I care which functions get called if the values are right?
So, that speaker was actually me ;-) It's a fair criticism. My goal isn't to downplay what makes Scala Native great, but to be realistic about its drawbacks, and let my audience make an informed decision whether it's a good fit for their use case...but I still sort of flubbed that question tbh. Can you think of any talks, off the top of your head, that deliver the right level of enthusiasm/pitch?
I think this is definitely a rather pragmatic "programs as data" approach that is too specific for the wide world of "anything scala". They might get more Haskell converts and specialization separating themselves from the, ahem, entry-level stuff. 
hey I organize the ny-scala meetup. We've got an (un)meetup coming up later this month, and this would be a perfect topic for the group! 
Yes, they are hard facts. Check Stack Overflow Trends, Google Trends, ITJobsWatch, etc. They are adding Kotlin support because it is easy and 95% of devs use those tools. Scala as I said is trying to reinvent the wheel that's why no support for major tools exists for Scala. Play / Akka are hardly better than anything. They are just as baroque as Scala is. That's no BS. It is empirical evidence as [Joel Spolsky](https://www.joelonsoftware.com/2000/06/03/strategy-letter-iii-let-me-go-back/) also stated in his article. Kotlin also won't fail in being a better Java since it has much less legacy to carry. It is obvious that you haven't coded in Kotlin and you don't have a clue about the ecosystem. I draw my conclusions (not opinions) form hard facts as I have said before. If everything points in the same direction you should also open your eyes to the truth. You should start facing the music instead of blindly defending a dying language. Even its own creators are leaving the ecosystem and talking about the inevitable demise of the language. Listen to [this guy](https://www.youtube.com/watch?v=TS1lpKBMkgg&amp;feature=youtu.be) for example.
Seconded! As someone in the UK, the times don't really work out.
A scenario in which you have to test optional side effects that are totally invisible from outside the system is pretty rare. Though, a mocking framework does help in that case.
This won't happen because: 1. type safety helps migration and refactoring 2. Scala 2 and Scala 3 share the same library, which python 2 and 3 didn't 3. Scala 2 and Scala 3 will share the same IR (tasty) and **there will be compatibility both ways**. For example, you'll be able to have Scala 3 and Scala 2 libraries in the same class path. Python 2 and Python 3 would dream of having this leverage because these features improve migration by a lot. Allowing people to slowly migrate to Scala 3 at their own pace, as they see fit, without feeling forced to it is something that didn't happen with Scala 3, for example.
Idiomatic Scala is its own thing. It's not pure FP or pure OOP. Essentially, you're classifying idiomatic Scala devs–the vast majority (including Martin Odersky)–as 'Scala as nicer Java' devs.
let it crash pattern 
Which company ？I more concern the server side
Oh yes, I kind hate it. Also specs2 and scalatest. After many tests written and trying to understand how to use the lib there's always some corner case you are constantly hitting with it where I've spent hours searching the web on how to do it, or how to solve the specific mock problem, mostly to read somewhere "oh it's not so well supported because you're doing something wrong." or the classic nulls being thrown everywhere or exceptions being thrown. Also the magical counting of method calls that fails sometimes but other times is ok... grrr, too much. Also it feels like a java api first most of the time and leaves you scratching your head constantly. I think I hate all those testing libraries. Always too many options and choices on how to do things, different words that all do the same, lacking documentation, assuming you know java, etc. I don't want to spend hours constantly learning the specific dsl of each of those libraries to just write a simple test. It's annoying.
Are you aware on any `Option` value type proposal in the Java community? How do they achieve that goal without subclassing? That could help to foresee a solution here.
Corporations do pay Lightbend for improvements and those are upstreamed. As for EPFL and Scala Center, it's not really in their charters to take money from corporate sponsors to do work as would be appropriate for a Lightbend or 47 Degrees type. Some of those corporations are rather private and just don't bother to tell everyone about those sponsorships even if it would paint them in a good light.
If having programs as values, in the same vein of having functions as values doesn't strike you as profoundly powerful, then I'm not sure anything is going to convince you. Also I don't understand what number 2 has to do with IO.
We already have programs as values. They're called *functions*. If I have a piece of code I want to wrap up, pass around, and use multiple times, I simply put it in a function. What does IO provide over that? &gt; Also I don't understand what number 2 has to do with IO. Idk, that's literally what one of my coworkers said
Hi there. Quick background for context: I came to Scala from Haskell, wrote Scala professionally for 4 years. Was sad that pure functions are left as a "best practice" in Scala, but very happy to see IO-based libs popping up (`doobie`, etc). With that in mind... &gt; It allows you defer side effects - ok, so why not just call your side effects when you actually need them? This is tied to (4). If your application is architected correctly, then the point at which you need the effects is precisely at the boundaries of your application. This lets you write most of your functions as pure code, the benefit of which is one of **maintenance burden**. The ability to point at a function and say "It's (near) impossible that this code is the source of the bug" is a strong one. Another way of looking at it: if I notice that most of my application is in `IO`, including business logic, and that I'm fighting my code to impose my will upon it, I think to myself that something is probably wrong with my architecture. In these cases I let the compiler tell *me* how everything should be laid out, and it's almost always right. &gt; We could change the implementation of IO and you would get a completely different execution semantics That's a weird one. You're probably right that this doesn't happen in practice. I trust the `IO` people to provide good defaults. &gt; It makes all your code referentially transparent - but that's only because it doesn't do anything. As soon as you actually execute the IO, you lose referential transparency I personally wouldn't call safely executing an `IO` action the same thing as losing RT. "Losing RT" happens when you have something like: def foo(s: Int): String = { println("hi!") s.toString } If you've executed an `IO`, you have its final result forever. It can't change willy-nilly on you, which is the usual fear of not having RT. &gt; It pushes side effects to the boundaries of your program - it doesn't, the effects still occur in the same order The "boundaries" bit refers to physically *where* in your code the effects are defined, not necessarily where they're run. Again, the goal is to try to maximize the number of pure, total functions we have in our code bases. **Summary** - `IO` let's us logically separate effectful functions from non-effectful ones. This lets us reason about our code better, reducing bugs and maintenance costs. - If we stick to `IO`, in means we have lots of pure functions too. These are easy to unit test when we know they don't have effects. - `IO` usually comes with bundled with a `Fiber` type, a `Future`-like construct with stronger safety guarantees. If you're looking for some sort of "`IO` will reduce your bugs by 87%" figure, I don't think anyone has such a thing. All I (or anyone) has is anecdote, having written in both styles for years and seeing the difference. Personally I can't live without `IO` anymore - I don't trust myself enough to write perfectly rigourous code without it being compiler-enforced. Cheers, hope that helps!
&gt; If I have a piece of code I want to wrap up, pass around, and use multiple times, I simply put it in a function. What does IO provide over that? Because if side effecting, or prone to non-deterministic failure, or any of the other things that violate referential transparency, then it is not a function, despite your loose usage of the word. IO turns that program into a value, so you can do all the things you do functions. Like compose them into larger programs while respecting the algebraic laws that allow to reason about composition without knowing anything about what a program does.
Please don't start a political debate
Thanks for your in-depth reply! &gt; If your application is architected correctly, then the point at which you need the effects is precisely at the boundaries of your application Why is this a correct architecture? Effects are needed when they're needed. If at some point in my code I need to make an http request, why would I push that to the end of my program, instead of calling it naturally where it occurs in the code? I have to do mental gymnastic to think about how that works. &gt; This lets you write most of your functions as pure code, the benefit of which is one of maintenance burden. The ability to point at a function and say "It's (near) impossible that this code is the source of the bug" is a strong one. I'm not so confident that pure functions are always bug-free. For example, let's say I have this function: `def foo(x: Integer): Integer = { println("Incrementing") x + 1 } ` How does turning that into `def foo(x: Integer): IO[Integer] = { IO(println("Incrementing")).map(_ =&gt; x + 1) } ` reduce the chance of bugs? The second is more complicated, and thus in my experience more likely to contain a bug. &gt; If I notice that most of my application is in IO, including business logic, and that I'm fighting my code to impose my will upon it, I think to myself that something is probably wrong with my architecture. Ideally yes, I would also prefer to minimize IO. But in my experience, most business logic is IO. Especially since micro-services are so common nowadays, http requests are littered throughout the code, not to mention logging code. I don't think this means something is wrong with the architecture. I think it means the code naturally reflects the business domain/logic. I feel like artificially contorting your architecture to fit a design pattern (IO) is the wrong approach. It's the same as when Java programmers try to model everything as inheritance hierarchies - it's often not the right design tool for the problem. &gt; If you've executed an IO, you have its final result forever. It can't change willy-nilly on you, which is the usual fear of not having RT. How would that happen? Taking your function as an example, I can "execute" it by doing `val x = foo("hello")`; then `x` is never going to change on me. That's the purpose of using `val` over `var`. In fact, if I call `foo("hello")` a hundred times, I always get the same result... actually `foo` is a pure function already, even though it has side effects! &gt; The "boundaries" bit refers to physically where in your code the effects are defined, not necessarily where they're run. Again, the goal is to try to maximize the number of pure, total functions we have in our code bases. I'm confused what you mean by boundaries. Aren't the effects still defined in the same place whether you use IO or not? They're just "activated" - so to speak - at different points in the code. &gt; If we stick to IO, in means we have lots of pure functions too. These are easy to unit test when we know they don't have effects. But we still have effects. Having IO doesn't mean we magically don't need effects in our programs. It just means they have a different type. We still need to test them the same. &gt; IO usually comes with bundled with a Fiber type, a Future-like construct with stronger safety guarantees. I agree that futures are great, but that's not really core to what IO is. Sorry, but I'm still not convinced! 
Lol. I mean it's the definition of a function as commonly used in programming. I know a mathematical function is different, but it's not what people usually mean. But if you want to feel superior over definitions then by all means. &gt; or prone to non-deterministic failure I don't get what non-determinism has to do with it? Non-determinism generally arises from concurrency. It seems rather orthogonal to side effects. &gt; Like compose them into larger programs while respecting the algebraic laws that allow to reason about composition without knowing anything about what a program does. I can already compose functions. It's called function composition. What algebraic laws do you refer to? Do you really combine arbitrary programs without knowing what they do? That seems... irresponsible. I would not want to work on your code.
&gt; Lol. I mean it's the definition of a function as commonly used in programming. I know a mathematical function is different, but it's not what people usually mean. But if you want to feel superior over definitions then by all means. If this is what you think, then functional programming is not for you and you're wasting your time here.
Ignoring your ignorance and rudeness, the last thing I'll say is that exceptions are, again, orthogonal to IO. If you don't like exceptions, then I suggest using `Option` or `Either`. They're really handy :)
First of all no matter how good a programmer someone is, if they scoff at anyone earning 80k a year they are an idiot. It sounds like your killing it career wise if not fulfillment wise, take credit for that you earned it. Second of all you are not a glorified anything, you are a programmer whose domain happens to be databases right now. You want to change domains, programmers do it all the time, people in other industries do it all the time. You wouldn’t think a carpenter who made tables was crazy for learning how to weld and churning out spiral staircases. Third, I dropped out of computer science when I left school because I couldn’t handle the maths. Almost a decade later when I decided to give becoming a programmer chance I chose front end and html and JavaScript because ‘compiled languages where especially hard’ and i thought I wasn’t smart enough to figure them out. 5 years later, because of the play framework tbh, I have found , and write for a living now, scala. I am well on my way to understanding and being able to utilize the more advanced corners of the language as well as getting a handling on the, frightfully mathematics filled, pure functional community of scala programmers. None of which I would have really thought possible 6 years ago. On top of all that they pay me great money to do it. I don’t know if scala is going to eat Java’s lunch and be the next next in Vogue language. Probably not. I do know that the syntax you write is just the tool you choose to do your job. With scala you can start relatively small and easy yet have so many paths to choose as you grow laid out in front of you , all the while learning up to date, foundational and advanced techniques that should serve you no matter what your next language turns out to be. If it’s what you want.... you got this, good luck, have fun.
&gt; the last thing I'll say is that exceptions are, again, orthogonal to IO Not on the JVM and thus Scala they aren't. If you change your mind on functions/programs/procudures then I'm willing to educate on the parallels IO provides for function composition if necessary.
Hi, I'm very much a blue-collar. My first job was one of a java dev, and I've (very slowly) converted to FP aficionado. Rather than trying to answer your question generally, I'll give you a small convincing example : implementing `def retry(fut : Future[Int]) : Future[Int]` is impossible. The reason is that when that method receives the Future, the Future is already running, and you've already lost the ability to re-construct what it does. So in order to retry a Future, you either need to : * have the knowledge of how the Future is constructed in the first place, and write a retry for that : `def myFun(x : Int, y : String) : Future[Int] ; def retryMyFun(x : Int, y : String) : Future[Int] = ???`. This doesn't scale. If you have to write retry logic for a few functions, you'll get annoyed at how you can't refactor. * use a by-name param `def retry(fut : =&gt; Future[Int])` but you'll have to wrap all the futures that you want retried during their definition, which pollutes your function with additional logic. * Express the fact that your retry function works on a deferred future : `def retry(() =&gt; Future[Int])`. Well, you don't even have to know it's working on an Int, you can do `def retry[A](() =&gt; Future[A])`. So it means that you now have an incentive to have your functions return `() =&gt; Function[A]` ... aaaaaand you've re-invented IO. This is only a simple example, but in general IO gives you a lot of really useful constructs (or the ability to implement them), and the ability to call to separate the calls to these constructs from your core logic. Another example that worked for me : I was able to write a piece of code (that our devops understand) that polls AWS to know whether a stack was up (timeing out after a while), with only 7 lines of code, using monix's Task which is another implementation of IO. My coworkers wanted to roll-in Akka, which would have costed a couple (annoying to test classes) and would have been much less readable. But the main piece of advice is : just code with the style you (and your team) are productive with. If you don't want to buy into this stuff, it's ABSOLUTELY FINE. People have created a lot of value for their clients using php, java, etc, and at the end of the day that's what matters. 
Thanks for your reply. Can you explain why you think `def retry(fut : =&gt; Future[Int])` is insufficient? I think a by-name parameter is exactly what you want here. &gt; So it means that you now have an incentive to have your functions return () =&gt; Function[A] ... aaaaaand you've re-invented IO. Lol. This is funny because I've already figured out that IO is just a thunk, but when I tell other people they insist I'm wrong and I'm missing some magic that IO does. So if you're saying that IO is just thunks, then yeah I agree they're useful. But I still wouldn't capture all my side-effects in thunks. That just makes control flow more difficult. I actually find your whole example quite odd. Future and IO are two different mechanisms. Futures are used to run parallel computations, and they work on either pure or impure code. IOs are used to reify side effects. The two aren't really related. I've noticed that a lot of Scala devs tend to confuse the two ideas though. Futures are useful, and if you just call futures IO, then yeah IO is useful, but they're really not the same thing. I want to know why the concept of IO is useful. &gt; just code with the style you (and your team) are productive with. If you don't want to buy into this stuff, it's ABSOLUTELY FINE I wish. I am much more productive in the normal "imperative" style, but my team forces me to use IO.
I think what your co-worker meant by (2) is that you can change things like evaluation order (e.g., eager or lazy evaluation), number of times something is evaluated (if something should be evaluated on every call like Future or never change it once fetched like the default IO monad does), or even retry strategy if someone is willing to go that far (I've never seen someone do that to IO monad, but in theory you could do that by changing implementation).
If you're doing unrestricted IO all over the place then of course the IO monad isn't really going to do much for you. You'd just end up wrapping everything in IO { ... } and adding an IO[T] to every function signature. That goesn't really provide much value. What *does* provide value is using *restricted* forms of the IO monad and embedding them in IO, e.g. a DB[\_] monad where you can run database queries (which obviously require IO at some level), but are only allowed to run DB queries -- not just do any type of IO. (I've seen this called the "MTL" style. It can also be likened to capability-based programming.). Another example would be a ReaderT layered over IO which could give you all the (static) dependency injection you want without the extreme amounts of boilerplate that e.g. constructor injection leads to. (The possibilities are endless.) Another scenario where this type of thing is incredibly useful is when writing libraries with callbacks. If you have a function f(callback: =&gt; DB[\_]) you know that that callback function can only do database queries and won't create random files and whatnot. (Of given that this is Scala it's unfortunately easy to cheat, but such is life.)
IO gives you referential transparency, as you noted. So now we have reduced the question to "why is referential transparency desirable?" … it's desirable because it makes it easier to maintain programs. You can refactor confidently because your program is guaranteed to be invariant under a large class of syntactic transformations. This ends up being very useful, in fact it's the whole point of doing FP. If you're not convinced of this then IO will seem silly. If you are convinced then IO is essential.
Reply-to-self for visibility: I can recommend watching Gary Bernhardt's talks: * https://www.destroyallsoftware.com/talks/boundaries * https://www.destroyallsoftware.com/screencasts/catalog/functional-core-imperative-shell which give an overview of this approach. (Well, maybe not *exactly* this approach, but the idea is the same.)
&gt; I tell other people they insist I'm wrong and I'm missing some magic that IO does I mean they are right, but a thunk is where it starts. There's usually a whole ADT of various cases that an IO can end up being, but if you want to implement a minimal IO construct, that's the starting point. &gt; I actually find your whole example quite odd You're right. I used Future in the example cause most side effects in the application I maintain originate from libraries that surface Futures, so it was to convey that example in question is likely to call upon side-effectful stuff, but you could very well replace it by anything. That's also why IO (in the case of Monix/cats/ZIO anyway) is more complex than "just a thunk", it also incorporates semantics that allow for async/concurrent calls. &gt; Can you explain why you think def retry(fut : =&gt; Future[Int]) is insufficient? because if you have `def myFun(x : Int) : Future[Int]`, you can do `retry(myFun(42))` but you can't do `val fut : myFun(42); retry(fut)` Using IO allows you to do that, and therefore allows to separate the concerns better because these instructions could happen at different points in the code. I think the point is that, with IO, you need less to know where the IO came from / what it does. With side effects, you have to keep a mental map of what the code does. It's fine for small codebases, but with bigger codebases it can be hairy and concerns start getting mixed together. IO also gives a more fine-grained control over how your thread pools end up being use. And in my case, in codebases where most side effects are `Futures` provided by libraries, IO prevents me from having to carry `implicit execution contexts` everywhere, which is quite a win in itself. &gt; but my team forces me to use IO Sadly, good teachers are hard to find, especially when it has to do with abstractions. John de Goes is usually pretty good at conveying motivation for this sort of stuff. Here's one interesting talk from him that might help you understand better than I'll be able to : https://www.youtube.com/watch?v=wi_vLNULh9Y
Hi, we have a bit over a hundred Scala jobs here at Functional Works: [https://functional.works-hub.com/jobs/?search=scala](https://functional.works-hub.com/jobs/?search=scala)
&gt; Futures are used to run parallel computations They are used to run _asynchronous_ computations. &gt; IOs are used to reify side effects. The two aren't really related. `IO` can be used to manage both synchronous and asynchronous effects, so they are not completely unrelated. There is no reason at all to use `Future` in a codebase that uses `IO`.
Be curious and humble. While a Java/Python background is nothing bad in my opinion it carries with it pretty strong prescribed ideas. I find subjects like null, exceptions and mutability are approached with much more care in Scala, and you usually model things more explicitly. How far it's taken is very much up to the team/project you are working on, but a lot of discussion happens in this space and you will likely come across concepts like effect encapsulation and monads. You can get by using Scala as a different Java, but you'll never really learn much unless you step out of your comfort zone.
I agree with others that it all boils down to referential transparency but I also recognize that it's not something convincing at first, it takes time to appreciate so I'll try to address few points regarding `Future`. :) &gt; Can you explain why you think def retry(fut : =&gt; Future[Int]) is insufficient? I think a by-name parameter is exactly what you want here. It's not sufficient, it depends on how this `Future` is created before calling your function. You have no guarantees whether it's already done or not. If you have a function that takes multiple `Futures` as an argument you have no way of controlling order of their execution which might be essential if you do any side-effects. &gt; I actually find your whole example quite odd. Future and IO are two different mechanisms. Futures are used to run parallel computations, and they work on either pure or impure code. IOs are used to reify side effects. The two aren't really related. Yes, they are different but they can both capture asynchronous processing and we have a lot of arguments to prove that it does much better job than `Future` even if you don't care about referential transparency. I assume you're talking about `IO` like `cats.effect.IO`, `monix.eval.Task` or `ZIO`. We can easily run those in parallel too, we can cancel them, they outperform `Future` in throughput by huge margin (and if you need fairness you can do that relatively easily by calling `IO.shift`), they have rich API, you doesn't have to carry `ExecutionContext` around, there is really cool way to handle safe acquisition and release of resources (`bracket`) and many other cool stuff. I think those are very good reasons to use `IO` even if you don't care about FP too much. 
&gt; If you've executed an IO, you have its final result forever. It can't change willy-nilly on you, which is the usual fear of not having RT. &gt; &gt; How would that happen? Taking your function as an example, I can "execute" it by doing val x = foo("hello"); then x is never going to change on me. That's the purpose of using val over var. &gt; &gt; In fact, if I call foo("hello") a hundred times, I always get the same result... actually foo is a pure function already, even though it has side effects! If it is pure you can safely replace its result with function call/definition. This is incredibly useful property when refactoring code. This is nicely explained on example here: https://www.reddit.com/r/haskell/comments/7ykrv3/xkcds_prediction_for_a_haskellrelated_cve_in_2018/dujbmid/?context=3
thank you, i figured that i have to unlearn to relearn along the way is there any resources i you recommend to start to model in scala way?
I started with the second edition of Programming in Scala (https://www.oreilly.com/library/view/programming-in-scala/9780981531687/), finding it approachable and interesting.
Sorry I accidently named it street. It should be address (edited it now)
Thank you for your answer. Currently i use a trait for the definition of common properties. This definition is used in my services. For persistence I than would use a MySqlCustomer with an id and maybe a create date or such stuff. I would probably have a CustomerResponse (with json serializers in its companion object) and whatever "Customer" representation I would need else. All the representations would extend my common trait. How would a more FP idomatic approach would look like? Would it look somehow like this: trait Customer[A] { def name(a: A): String def lastName(a: A): String def address(a: A): String } case class MySqlCustomer(id: Long, name: String, lastName: String, address:String) object MySqlCustomer { implicit val customer: Customer[MySqlCustomer] = new Customer[MySqlCustomer] { def name(c: MySqlCustomer) : String = c.name def lastName(c: MySqlCustomer) : String = c.lastName def address(c: MySqlCustomer) : String = c.address } } How would I use it? Like this? object CustomerService { def printCustomerName[C: Customer](customer: C): Unit = { val c = implicitly[Customer[C]] println(c.name(customer)) } } Sorry, I'am new to FP.
GraalVM CE will give you better performance for free and EE is even better. A big project like this needs funding...
Great question! Thank you for asking it! I am going to assume that most software written are "servers" of some kind. This means they need to process requests in a parallel fashion. Generally you will need to process more concurrent requests than are threads in the underlying operating system. For this to work you will need some way to turn long running actions (like HTTP requests or database queries) into asynchronous actions. If you don't your threads will block and throughput will plummet. This part in non-negotiable. You want to build servers on the jvm then you need this kind of abstraction, period. So you must use *some* kind of container. The question then becomes which container? You could use Future, but I think its shortcomings are well established. If we are going to use a container for effects, which we must, then we would like our contains to behave in an orderly fashion, your points go right to the heart of the issue. \- Not being able to defer side effects in one of the primary problems with future and why you should user a better IO monad. It makes code reuse considerably easier that you can just change the definitions with referential transparency instead of having to watch yourself every time to make sure a future is not eagerly evaluated when you did not want it to. \- Different implementations are in fact used in practice. Again once you wrap your code in containers, which you must, then if a better container comes along (say it is faster) then you want to migrate. \- I understand your conclusion, but the truth lies a little deeper. Again; you need a container. But some containers break referential transparency, like the Scala Future. Having a container that does not is a massive mental burden you don't need to carry. \- You are absolutely right. The effects still occur and in the same order, which is what we want. Except for the cases where they don't, again the Scala Future is a nice example. In summary; The reason for needing an IO monad can be many, but on the JVM we absolutely need an asynchronous IO monad if we are to build efficient servers. Once we accept that we must use an IO monad it becomes interesting what kind of other guarantees we cant get if we have to pay the price of using one. Monix, Cats and Scalaz all have different answers to this question, and I will leave it up to you to figure out which one is best for you. 
&gt; I'm not so confident that pure functions are always bug-free. For example, let's say I have this function: &gt; def foo(x: Integer): Integer = { println("Incrementing") x + 1 } &gt; How does turning that into &gt; def foo(x: Integer): IO[Integer] = { IO(println("Incrementing")).map(_ =&gt; x + 1) } &gt; reduce the chance of bugs? The second is more complicated, and thus in my experience more likely to contain a bug. Another way of thinking about it is that the second function is _more honest_ than the first. println grabs a global lock, so it turns a simple pure function into something that could bottleneck your entire program. Something that dangerous probably deserves a big red flag in its type signature. &gt;But in my experience, most business logic is IO. Especially since micro-services are so common nowadays, http requests are littered throughout the code… The IO isn't the business logic. The IO is the plumbing _between_ the business logic. When you conflate the two, the result is a system that is harder to reason about and harder to test. I've worked on far more systems built the "HTTP (/database/etc) requests littered throughout the code" way than I care to count, and you inevitably end up jumping between two testing strategies: * End-to-end tests that require you run a bunch of infrastructure either mirroring, or pretending to mirror the remote services you're talking to * Mock/stub tests where, because your business logic and I/O are so tightly interleaved, your are inevitably mocking out business logic, and thus your tests are making dangerous assumptions in the absence of the real code. The way to make these systems reliable and testable is to make the bits of your code that perform the I/O as small as possible (i.e. "push IO to the edges"), so that you can write tests that make the smallest possible set of assumptions about what they're doing. "Minimise the amount of your code that needs to run inside IO, lift your (pure) business logic into that IO" becomes a common goal _whether or not_ you're using IO explicitly. Which brings us back to the idea that using IO is more honest. IO is telling you which bits of your program you need to make smallest, and raising a big red flag when IO starts invading the bits of the code that it shouldn't. &gt; …not to mention logging code. Code littered with log statements is another warning flag. It's a sign it's so hard to tell what your program that you only feel safe if it keeps telling you. You rarely need to log in pure code because if you know what goes into it, you can precisely reproduce everything that happens next, making logging that information just in case kind of redundant. If you structure your code around isolating IO, the places where you need to log almost always cluster around the points where IO is happening anyway.
&gt; is that you can change things like evaluation order (e.g., eager or lazy evaluation) IO is always eager wrt effects, that's the reason it got into Haskell in the first place.
&gt; End-to-end tests that require you run a bunch of infrastructure either mirroring, or pretending to mirror the remote services you're talking to Yet tests that do IO, preferably against real services, are still necessary. IO is the most unpredictable part of your program, everything can go wrong and verification is nigh-impossible, only tests can give any, however low, degree of confidence. &gt; Code littered with log statements is another warning flag. It's a sign it's so hard to tell what your program is doing that you only feel safe if it keeps telling you. You rarely need to log in pure code because if you know what goes into it, you can precisely reproduce everything that happens next, making logging that information just in case kind of redundant. Completely dishonest rationalization. It doesn't matter how easily you can reproduce the result, your algorithm can easily be complex enough that you need to log intermediate values to understand it.
&gt; Can you explain why you think def retry(fut : =&gt; Future[Int]) is insufficient? I think a by-name parameter is exactly what you want here. You can't enforce that the by-naminess happens in the right place, and it's very easy to accidentally break your guarantees. Suppose you have: retry(doHttpCall("lalalaverylongparameter")) and someone wants to break up the long line: val c = doHttpCall("lalalaverylongparameter") retry(c) Then suddenly you've broken retry in a silent, invisible way.
"Physically" `IO` is just a bit of syntax sugar around passing functions around, sure. You can think of it as just a "tagged" function, the same way you might have an `EmailAddress` type that is really just a `String`. The value it offers is a) a little bit of syntax sugar b) distinguishing between pure and impure functions, which is very handy when testing and refactoring.
Not purity?
&gt; How does turning that into &gt; &gt; def foo(x: Integer): IO[Integer] = { IO(println("Incrementing")).map(_ =&gt; x + 1) } &gt; &gt; reduce the chance of bugs? The second is more complicated, and thus in my experience more likely to contain a bug. You now have a big red flag that `foo` is doing some funny business. This means you know you need to do some extra work when testing `foo` (because a naive unit test of your first example would completely miss that `foo` is supposed to print something and not test that). And you can work on separating concerns, refactoring `foo` into distinct pieces that do the incrementing and do the I/O; having the monadic operations available does help that, because now you can do something like: def increment(x: Integer) = x + 1 def foo(x: Integer) = IO(println("Incrementing")).map(increment) and you can unit-test the business logic of `increment` separately, and be confident that any I/O is happening elsewhere. &gt; In fact, if I call foo("hello") a hundred times, I always get the same result... actually foo is a pure function already, even though it has side effects! Only if you don't care about the side effects. Suppose you have some code like: val x = foo(3) val y = foo(3) You would expect to safely refactor that to: val x = foo(3) val y = x I might make that refactor without testing or even consciously thinking about it, because it's an obviously correct refactor. But for the definition of `foo` given, this refactor changes the program's behaviour radically (maybe. If we care about what our program prints, it's an important change. If not, then we can say that `foo` is pure. Using `IO` only makes sense for I/O that you actually care about, that affects the meaning of your program; if you don't care about when or whether a particular line gets printed, it's not worth using I/O for it - though that does then beg the question of why you're printing it at all). So the point of `IO` is to still be able to do this kind of obviously-correct refactor without worrying about it. &gt; I'm confused what you mean by boundaries. Aren't the effects still defined in the same place whether you use IO or not? They're just "activated" - so to speak - at different points in the code. &gt; But we still have effects. Having IO doesn't mean we magically don't need effects in our programs. It just means they have a different type. We still need to test them the same. If we keep the effectful operation as a value - an "action" - then it remains a normal value that we can refactor and test in the normal way. If we write: def foo(s: Int) = IO { println("hi!") } map { _ =&gt; s.toString } Then we really can refactor in the usual way: `foo(3) *&gt; foo(3)` really is the same program as `val x = foo(3); x *&gt; x`. 
&gt;The reason for needing an IO monad can be many, but on the JVM we absolutely need an asynchronous IO monad if we are to build efficient servers. I think the only thing required are asynchronous I/O operations. I can't see from your post why specifically a monad would be required for efficiency.
Not purity, but laziness. Haskell had rudimentary forms of IO before they came up with using monads. (main taking lazy list of console input characters and returning lazy list of console output characters. IO is not really `pure` in a real-world usable way. i.e. You can't run one IO action twice with same input and get the same output. Instead IO is a `model` that allows to use and reason about I/O in a pure and/or lazy language. In Haskell, IO is conceptualized as `type IO a = RealWorld -&gt; (a, RealWorld)`, where `RealWorld` is an imaginary value that can't be created or inspected. In Haskell, IO does two things: 1. It allows to model actions in the real world as function inputs and outputs. Each action depends on the state of the real world, and returns a modified real world. Of course, this depends on `RealWorld` being totally abstract, otherwise you could write a function like: `applyAndGoBackInTime f oldRealWorld = let (result, newWorld) = (f oldRealWorld) in (result, oldRealWorld)` and use it as `applyAndGoBackInTime destroyEarth -- everyone is back alive`. Which obviously would be impossible to implement. 2. It forces ordering and execution in spite of lazyness: Because each new action in `flatMap` depends on the RealWorld result of the previous action, the modifications to RealWorld will be applied in correct order. When the `main` function is called, the (imaginary part of) Haskell runtime will create a new RealWorld and pass it to `main`, but it will also `force` the `RealWorld` result of `main`, and as such force the entire sequence of operations on the RealWorld i.e. your program. `haskellRts main = let (newWorld, _) = main conjureNewWorld in newWorld `seq` exitCode 0`
Hi thx for answer, well most of the Scala is in basic pure sync code so exceptions are unhandled (or only much higher in the stack). I got the task from my boss (besides other user stories) to also improve our scala/team knowledge and i think to use Monix would improve this code base. So there is no specific problem (or problems are hidden or not obvious) I see code that could definitely benefit from the new knowledge, but its like java "you know that you were suffering only once you learned Scala" Anyway i made a 25 page presentation, will see how it goes.
I would start with representing errors you want to handle as values using Either or similar. That sounds like it would deliver more value quickly, and it will come in useful if and when you do want to make more use of async.
When using Tagless Final, I’ve used ScalaMock for mocking the returned values of my Algebras. What do you do in that case as you’re not using ScalaMock?
We are hiring a lot of Scala developers in Toronto.
Good point. The IO monad is not strictly neccesary. However in practice much software in general rely on synchronous IO. Not all databases has async interfaces for example. Thus you have to either limit yourself to an async subset of the scala features or you need some way to turn synchronous computations into async ones.
But in such cases you would have to write that function yourself, make it set some some state/flag and assert that that flag is changed afterwards, correct? This is exactly what mocking frameworks are for: working around the need to write these kind of implementations yourself. I don't know how well Mockito works with Scala functions, but if you're finding yourself writing mock functions over and over, consider using a mocking framework that generates the verification for you.
IO has a couple of major benefits: - Maintainability. You mention things like "just use a lazy value". That's great if you have the discipline to design your control flow with these in mind. Thing is, you probably aren't the only developer on the project. I've seen multiple bugs caused by bad declarations of Futures, which IO does not allow. Having that .run call really avoids problematic bugs like this, increasing efficiency across your team. - Performance, particularly for async applications. IO doesn't require an ExecutionContext, and as a result, its not tied to the overhead of one. Any function called on a Future (e.g. .map) requires the context, and thus can be much more expensive to call than the same on IO. - Functional fun. You can use IO from cats or scalaz to design a wealth of functional programs, leveraging all the fancy Monads, Transformers, etc that come with them. 
&gt; IO is the most unpredictable part of your program, everything can go wrong and verification is nigh-impossible, only tests can give any, however low, degree of confidence. Indeed, which is exactly why you want to isolate it in as small a code area as possible, which can then be heavily tested, rather than scattering it throughout your codebase. This is why people talk about "pushing I/O to the edges". &gt; It doesn't matter how easily you can reproduce the result, your algorithm can easily be complex enough that you need to log intermediate values to understand it. If your algorithm is reliable and consistent then you should be able to just re-run it to see what's happening. More generally, if you actually want the information you should be representing it in a structured way rather than dumping strings to standard output and hoping a human can figure out what those strings mean later on. And once you have a structured representation of whatever event trace it is you care about (you might want to use a library like treelog), it's just a normal value that you can test in the normal way and then store in a normal structured way in a database or what have you. So I do think there's an excluded middle around log4j-style unstructured ad-hoc logging: if information is worth recording, it's worth recording properly.
Do you know of any good resources to start out with purely functional design like this? I've been using Scala at work for the past 6 months or so, but we've been using it in a less pure fashion, especially as we're integrating with fairly impure APIs like Spark and Akka. I really like the idea of encoding a program's behavior in its typesystem, but I don't have a background in category theory. I get the individual components; e.g. I can do something with the IO monad in scalafiddle, but the architecture is still a little fuzzy for me.
Mr. De Goes, thank you, you have just reduced my chances to introduce Scala for our next project by 90%. And now I will have to do it in Java. You probably forgot how that feels like. And how fckn distant _any_ form of FP is to us... I don't understand the almost constant bashing on Scala by people that "care" about it. I really think you should program in Java every now and then - you will fast appreciate Scala as it is...
A good point. There's an ergonomic issue however, as using treelog/writer/etc will introduce monad syntax and the usual monadic boilerplate (pure/lift/fmap/etc) into your otherwise pure computation.
Real knowledge of Category theory isn't really necessary for being productive with the "pure FP" tools. If you're interested in learning more, check out [this section](https://github.com/fosskers/scalaz-and-cats#sec-2-1) of my Scalaz and Cats comparison.
Well, without the monadic "boilerplate" natural, common-sense refactorings will lead to accidentally changing the logging behaviour without noticing. To the extent that you care about the logging, you presumably don't want that to happen. So again I think there's an excluded middle: if you care about this stuff then you want it flagged up when you're working on that code area so that you can pay attention to it (which is what the monadic operators do for you), if you don't care about it then why do it at all?
It's a very good question. I was working on Haskell in Paul Hudak's group when IO got invented (in 1992). At the time, many people were trying to find ways to combine FP with mutable state and other effects. The trick of IO was to essentially split the program in two: A purely functional program that constructed an effectful program in the IO monad. This is somewhat a slight of hand: we simply declare that a function returning an IO type may have arbitrary effects without going into detail in how these come about. The scheme has two consequences: First, the type of a function tells you whether it is referentially transparent or has side-effects when run. Second, the syntax to compose programs is also different: normal function application for combining pure functions, monadic bind for combining effectful ones. I believe that the first distinction is very good, but the second is an unfortunate accident. Let's do a thought experiment. Instead of having a "magical" IO monad, let's have a magical *capability* `CanIO`. Then, instead of returning an `IO[T]`, a function would instead return a `CanIO =&gt; T`. So this says "give me the capability to do IO and I return a `T`". Assume the `CanIO` capability is produced only by the runtime that executes a program. Then every program and every function that has side effects has to be of type `CanIO =&gt; T`, since there is no other way to get the capability. This is exactly the effect of an IO monad. However, instead of opaque IO components which are cobbled together with cumbersome monad operations, you now have regular functions that you can compose functionally! No more pains to express recursion, need to trampoline, and so on. To avoid boilerplate, you want to make the capability parameter implicit and abstractable; that's exactly what Scala 3's implicit function types are about (to find out more: google for "Simplicitly, POPL 2018"). In the end, this gives you a system that can run exactly like a regular side-effecting system, except at each step the types tell you whether something belongs to the pure or the effectful half. Also, pure and effectful functions are composed in the same way, no more need to go monadic, no need to do monad transformer acrobatics to achieve any sort of composition. Aside: I have never understood the fascination with monadic syntax (e.g. in Scala's for expressions). They are fine when they are needed, but I find regular composition with `val` and function application much preferable. In a way, computing the monadic binds/for expressions is like writing assembly instructions, where every step has to be spelled out in detail and sequenced properly. So why did Haskell do it that way? It has to do with laziness. In a lazy language, function application is not a practical way to sequence effects, so something else was needed. But Scala is not lazy, so this argument does not apply to it. The other argument for going to monads and IO is that it makes composition of effectful code more cumbersome than composition of pure code. So some people like this for the educational aspect: Using effects should be painful, so that you are pushed towards referentially transparent solutions instead. I understand the point of view but don't subscribe to it. A good analogy is `var` and `val`. Scala makes you declare your intentions (and prefers the functional `val`), but there's no syntactic penalty for using `var`. Some people argue this is bad, too, and that writing imperative code should be different (and more convoluted) than writing functional code. So, in summary, IO is a very good solution for a lazy language like Haskell. For a strict language like Scala, it's currently the most common solution if you insist that the distinction between pure and impure should be reflected in the types of your program. But it is far from ideal, and I believe that in Scala 3 we can do a lot better. My personal choice is simply to not reflect effects in the types now, but at the same time taking care to minimize them in my programs, and wait until implicit function types are generally available. But other teams might well make different choices.
&gt;So, the reason you can't run the same IO action twice with same results is because there is no way to capture or create the imaginary RealWorld value that is the input that defines the result of IO! Yes, so the IO monad enables everything to be pure, and does so without exposing RealWorld to the programmer, i.e., does so safely — since the developer can't capture old values, it is impossible to misuse them. But I find this alone requires IO monad. Laziness seems entirely irrelevant. In particular, there's nothing about the concept of a monad that requires RealWorld to be fully evaluated after main is done, and if Haskell used eager evaluation, it would still need IO monad to interact with the word while being pure.
It's sad that the answer from the creator of Scala isn't upvoted more. I hope the research into implicit capabilities bears some useful fruit! I definitely don't want to live in a world where effects are tracked via monads.
Ah, you caught me just as I was going to sleep last night. /u/m50d offered a lot of what I would say, but I'll still try to add something. &gt; Sorry, but I'm still not convinced! At the end of the day, nobody can really convince anyone of anything. The best I can do is lay out my experience and what I (think I) know for you to chew on. Quick questions for you: - is it that it isn't clear to you *technically* what `IO` is for? - is it a problem of *power*? Do you feel that `IO` restricts you? - do you feel a social pressure that is causing you to resent `IO`? For the Haskeller, there is just no way to not use `IO` when writing applications. We have no choice, so we adapt to the style and come to prefer it. Certainly new people who come to Haskell do at first feel that `IO` is a *restriction* (see [this cat meme](https://acm.wustl.edu/functional/io-monad.jpg)). Once you're used to it, you think of it more as a *constraint* that instead empowers to do things you couldn't before (i.e. think of chess. It's only an interesting game because each piece is limited in what it can do. A chess where all the pieces were Queens would get boring very fast). In Haskell there's nothing to be gained by hating `IO`, because it "just is". Metaphor: there's no sense in blaming the ocean for being big when you want to swim to Japan. It gets prickly when it comes to Scala, because `IO` is a best-practice in Scala. We can listen to people around us tell us how great it is, but in the back of our head we *know* it's something enforced upon us. We *can* blame the ocean for being big, because some human made it that way. So if we end up not liking `IO`, we can resent it, knowing that there's a way out (i.e. effects where ever we want). *Just let me write code the way I know how, dammit.* &gt; Why is this a correct architecture? To go a bit deeper, this is also a question of human will and coding philosophy. One of my programming mantras is: - For any given function longer than one line, there exists a series of refactors that approach a one-liner. This synergizes with another: - The language knows better than I do how it wants to be written. And so I use compiler errors and that feeling of "fighting my code" that I mentioned before to try to guide me to the "real form" of the code. Question for you: is the act of coding a process of discovering how the code wants to be written, or one of imposing our will upon it? In my opinion, Vanilla Scala lets (and encourages) you to impose your will upon your code. `IO` constrains that, but frees you in other ways that I and others in this thread are mentioning. &gt; I have to do mental gymnastic to think about how that works. Up front, putting an effect where ever we want it requires less mental overhead. However, if maintenance costs are the true devil of software development, then this approach kicks that effort down to future you. When you have a giant code base through which effects are completely interwoven, truly understanding what anything actually does becomes very hard for one human brain. As I mentioned, I don't consider myself smart enough to program in this way. &gt; I'm not so confident that pure functions are always bug-free. /u/m50d talks about this, but I'll add that you did the opposite: you made the function effectful, not pure. People argue about the definition of pure, but for right now I'll say it's "functions with no side-effects". Your example added a side effect, so of course we can't expect that to be bug free. &gt; Especially since micro-services are so common nowadays, http requests are littered throughout the code, not to mention logging code. I don't think this means something is wrong with the architecture. Sure, sometimes we just gotta make external web calls. However, if we notice that our entire code base is in `IO`, something is probably wrong. Referring to Mantra #1, there's almost certainly a cleaner refactor that's easier to reason about. &gt; I'm confused what you mean by boundaries. Aren't the effects still defined in the same place whether you use IO or not? Boundaries, as in your `main` function and things that talk to the world, say your database. The idea is that once your data has been called in from the outside, all your "business logic" can be continued in pure functions. Forgive me if it wasn't clear before that your composed `IO` action should only be run once - `unsafePerformIO` (or whichever you're using) should only be called in `main`. Otherwise, you abandon safety. This would be the same as using `Await` on a `Future` in-line. Does this help? Should I elaborate on anything?
I think you are right, error handling could be implemented by custom messages, however that's a common enough case that it got special handling. That is, doing synchronous, dangerous things, or failing a because a child failed, etc., is what happens often when you use actors - so with extra APIs for error handling, the code is clearer and more more readable.
&gt; people who learn the FP side of Scala and feel that paradigm empowering tend to eventually end up writing Haskell. I would add to the above: Scala markets itself as a non-verbose language that's Java devs can start using one little piece at a time. The idea you don't have to relearn everything is a huge selling point to "Java Devs." Since it's been part of Scala's marketing for some time, it should be no surprise that a good portion of Java-to-Scala devs invest very little effort into learning or applying correct FP, and instead want to pick and choose FP vs OOP vs other styles based on what is most convenient. If someone starts pushing correct FP, a percenative of these people are highly resistant because all they ever wanted was nicer Java, and now their coworker is being "anal" or "academic" by trying to push the team towards more correct FP practices. Conversely, the anti-FP "Scala as better Java" devs are really frustrating from someone who sees the value and is interested in advancing their FP skillset. If your team loudly complains every time you promote a "chapter 1 FP best practice," then your skillset and experience at work will never advance (Stagnate) that beyond Scala syntax. So if you are the ambitious sort (like me), you'll carefully select your next Scala job to be a team that actually cares about FP. After doing that for a while, these same people will start to realize *"Why am I spending so much time and effort fighting the `Scala as nicer Java` crowd, when I could use Haskell (and similar languages) and never have to deal with this ever again."* I'm sadly experiencing this right now, the amount of active resistance to basic FP principles on my team feels extremely anti-intellectual. My hope with switching to Scala was to advance my career and skillset towards being more than just "a Senior Dev." 3-4 years from now, it would be great to be mentoring others in Scala, contributing to Scala open source, and giving talks at conferences... and the reason I want all of that is because of the value it could produce for others (which I enjoy). /rant
&gt; if Haskell used eager evaluation, it would still need IO monad to interact safely with the world while being pure. Nope, there are many other ways to model IO purely. For example Clean uses uniqueness typing to ensure that an effectful function is, semantically, always called with a different argument. Then there's algebraic effects systems that do not require explicit monads, as currently used in ConcurrentML and FStar - https://www.fstar-lang.org/papers/icfp2015/full.pdf &gt; In particular, there's nothing about the concept of a monad that requires RealWorld to be fully evaluated after main is done Oh, but if it's not evaluated, nothing would happen! The unreferenced thunk would simply be discarded.
I’m skeptical because I know that ZIO won’t speed up my real world code by a factor of 100 because I already profile and I know that the overhead of future is not remotely close to being a bottleneck. So yeah, I know it’s false advertising, but a lot of people might take it at face value, which makes it a bit disingenuous. 
Effects aren't tracked via monads now. Effects are tracked via coproducts of some kind in the type signature. They're combined together via monads. Even with implicit capabilities, if you're going to combine effectful value A with effectful value B into effectful value C, where B depends on A, you're still going to use a monad. The only thing that changes is the which monad you're using; the monad for Function1 versus the monad for Kleisli for some coproduct constrained to only accept the effects in the coproduct. The former is vastly simpler, well understood, and more performant.
Great question. **The main benefit of IO is stronger static typing**. Look at the following signatures and think about which you have the most information on: \`\`\` def func(a: Any): Any = ??? def func(a: Int): String = ??? def func(a: Int): IO\[String\] = ??? \`\`\` We can express the same function in all three ways. But one of them gives you the best understanding of what's actually happening without having to read all the code. The IO type gives you more information about one of the most critical parts of any program. It also gives your compiler and your IDE that same information; now your compiler and IDE can help prevent you from making mistakes. This particular type also provides a bunch of helper methods (map, flatMap, filter, etc.) to help you cleanly compose instances of it with other parts of your code.
&gt; Assume the CanIO capability is produced only by the runtime that executes a program. How would/could you enforce this though? Some kind of second-class values (and presumably corresponding second-class types)? That would be a radical change to a language where all values are assumed to be copyable, storable etc. I went through exactly the same thought experiment when I was looking to represent database connections in my program. The obvious way is with a `Reader[DatabaseConnection, ?]`, but the trouble with that is that there's no way to prevent someone from just "smuggling out" the `DatabaseConnection` handle and then trying to use it when it's no longer valid (or, worse, when another operation is already using it). Using an opaque monad solves this problem: code that works on a `QueryTable(...)` value (say) has no way to accidentally leak a database connection - in fact it doesn't even have visibility of the `DatabaseConnection` type. (An alternate approach might be [the rank-2 type trick](https://apocalisp.wordpress.com/2010/07/02/higher-rank-polymorphism-in-scala/), but I can't imagine you'd prefer that). Observation: `IO` is kind of a special case of a resource monad, where we treat "the world" as a resource. &gt; instead of opaque IO components which are cobbled together with cumbersome monad operations, you now have regular functions that you can compose functionally! No more pains to express recursion, need to trampoline, and so on. What's the distinction you're drawing? Recursion and composition should be just as easy for monadic functions as for regular functions (the concept of an `Arrow` formalizes this - one can write compositions in terms of arrows that will do the same thing on sequences of pure functions or sequences of monadic functions. Generic-recursion libraries like matryoshka generally provide both monadic and pure versions of their operations). Indeed I'd say the easiest, most natural way to compose a `CanIO =&gt; A`, a `CanIO =&gt; A =&gt; B`, a `B =&gt; C` and a `CanIO =&gt; C =&gt; D` to get a a `CanIO =&gt; D` is to use monadic `for`/`yield` - certainly I'd rather be doing that than manually threading the `CanIO` handle through my operations - and at that point you still have the trampolining problem. &gt; To avoid boilerplate, you want to make the capability parameter implicit and abstractable I don't want implicit parameters because that loses me too much visibility over which functions perform I/O and which don't. It's exactly the same mistake as using checked exceptions instead of result types: there's no `=` versus `&lt;-` distinction between calls which can't fail and calls which might fail, only a great big `try` block within which any function might fail. Similarly, if I have an implicit `CanIO` in scope then it's in scope for the whole block, and there's no longer any way to see the difference between the `B =&gt; C` and the `CanIO =&gt; C =&gt; D`. &gt; Also, pure and effectful functions are composed in the same way, no more need to go monadic, no need to do monad transformer acrobatics to achieve any sort of composition. But effect composition *should* require a visible choice of which way around the effects compose, because effects don't commute. If I have a logging effect and a nondeterminism effect, do I get logs from abandoned branches or not? Do I get separate log traces or a single combined one? MTL-style typeclasses already give me a way to write a function that accepts a monad with logging capability and nondeterminism capability and defer that choice to the monad, if that's what I want, but I do need to make that choice explicitly sooner or later, which is as it should be. &gt; They are fine when they are needed, but I find regular composition with val and function application much preferable. In a way, computing the monadic binds/for expressions is like writing assembly instructions, where every step has to be spelled out in detail and sequenced properly. A codebase where you can refactor fearlessly is so much better than one where you have to carefully test each change. If you make sure all the cases where you explicitly care about sequencing order are explicit, then you can be confident that anywhere where there *isn't* a monadic context you have freedom to resequence everything as long as you return the same final result. This makes all your code maintenance so much cheaper and easier and eliminates huge swathes of what would otherwise become production bugs (it also means you can greatly reduce the size of your testsuite for the same level of confidence, which in turns makes further refactoring and maintenance much easier). &gt; In a way, computing the monadic binds/for expressions is like writing assembly instructions, where every step has to be spelled out in detail and sequenced properly. So why did Haskell do it that way? It has to do with laziness. In a lazy language, function application is not a practical way to sequence effects, so something else was needed. But Scala is not lazy, so this argument does not apply to it. The argument is still valid for a strict language because so much of the ordering of function calls in a program is incidental rather than deliberate, a consequence of having a single linear thread of execution. The programmer's intention becomes so much clearer, and code so much easier to understand, when you can draw a distinction between cases where it is important that `f()` is called before `g()` and cases where what matters is computing `h(f(), g())` and it doesn't matter which order they're evaluated in.
&gt; It's sad that the answer from the creator of Scala isn't upvoted more. Comments should stand or fall on their merits, not on who wrote them. &gt; I definitely don't want to live in a world where effects are tracked via monads. Why's that? Monads are easy to think about and let you keep working in terms of plain old values and functions. Implicit capabilities are overly magical and cause more problems than they solve as far as I can see.
&gt; Nope, there are many other ways to model IO purely. For example Clean uses uniqueness typing to ensure that an effectful function is, semantically, always called with a different argument. Cool beans. &gt; Oh, but if it's not evaluated, nothing would happen! The unreferenced thunk would simply be discarded. Yes, but it's not the monad that guarantees evaluation. At best it only makes it easier for the runtime to automagically guarantee full evaluation of IO effects and *only* of IO effects. (Maybe. I mean, you could imagine runtime forcing full evaluation of main's result other than IO, but then still we get possibility for user program to easily generate arbitrarily large thunk on heap until the program reaches the point when evaluation is forced. Requiring users to manually seq their functions all over the place for something as basic and prevalent as IO is hardly user-friendly. Yet possible.) I like Odersky's response better — it specifically mentions that using function application alone for this purpose in a non-strict language is non-practical, which I take as meaning that it can be done, but not quite as cleanly or easy to use efficiently. All in all, the way I understand it, monad is *not required* to deal with laziness. It seems to be just one way to do all the things: from imposing order of transformations to enabling purity. Laziness might make some alternatives more cumbersome, but not preclude them. Still waiting for someone to explain to me why I'm wrong :)
&gt; if Haskell used eager evaluation, it would still need IO monad to interact safely with the world while being pure. This is true, but the laziness is what makes the purity non-optional. Imagine that Haskell instead used a "magic function" `println` that printed when it was evaluated, like we do in Scala: in an eagerly evaluated language you can write programs that way and reason about what they're going to print (I'd argue it's not the best way to deal with it, but it's possible) whereas in lazy Haskell you'd have no way of knowing what would get printed when and what program changes would change the order of printing. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/mark.pdf has some of the history
Why are you talking about Haskell suddenly? I think you actually missed *my* point -- I was responding to a post that basically tried to use C++ as a proxy argument for "having Scalac 2.x and Dotty 3 won't be a problem -- because Clang and GCC exist". I was pointing out that the there's a *single* language that both Clang and GCC implement. There's no single language that both Scalac 2.x and Dotty implement. Btw, I *also* think Haskell is a worse language for not having an up-to-date standard. It just so happens that GHC Haskell is hugely superior to Scala (for me).
&gt; Effects are tracked via coproducts of some kind in the type signature. Would you mind spelling this out or explaining more explicitly?
When I attended Dutch Clojure meetup last Thursday (fewer people attend due to the fact that everyone will miss first half of England vs Croatia), from around 11 people there only 3 of them are pure Clojurian, the rest happened to be also Scala guys taking a break from Types :D
Sure, we'll look a few popular ways of doing effects in Scala right now. With the [Free Monad](https://underscore.io/blog/posts/2017/03/29/free-inject.html) you'll see `Coproduct` is used to present a list of possible capabilities(called algebras in this context). The `Inject` typeclass is then used to assert that an effect has all the capabilities in the list. The [Freestyle](http://frees.io/) project replaces `Coproduct` with it's own [fast tagging system](https://github.com/frees-io/iota) but contains the same idea. There's a coproduct which represents a lists of capabilities. `Inject` proves that certain capabilities are in the effect. With the [Eff Monad](https://github.com/atnos-org/eff) we'll see the same thing. Similarly to Freestyle, Eff provides it's own coproduct. But instead of listing capabilities, Eff lists other monads. In the [introduction](http://atnos-org.github.io/eff/org.atnos.site.Introduction.html) the very first thing you do is define a list using Eff's coproduct: type ReaderInt[A] = Reader[Int, A] type WriterString[A] = Writer[String, A] type Stack = Fx.fx3[WriterString, ReaderInt, Eval] With implicit functions, you no longer have a coproduct like data structure to compare capabilities. The compiler now knows how to do that comparison using implicit search. If implicit search fails, you're missing a capability. It succeeds then you are implicitly given a value that allows you to do what you want. This brings me back to where the monad comes in, in all of the above example, the monads do nothing to provide or check what the capabilities present or not present. In fact you don't need a monad at all to express capabilities in Initial algebras(*Final algebras purposefully ignored to pedagogical reasons), hence why Free Applicatives are sometimes used to combine effectful values instead, I've also heard of people combining effectful values using Monoids as well
&gt;This is true, but the laziness is what makes the purity non-optional. That is an interesting insight I haven't encountered before. Thank you. It might also explain people juxtaposing monads and laziness together.
Watch the talk that was posted some days ago: https://www.reddit.com/r/scala/comments/8xreuv/keynote_the_last_hope_for_scalas_infinity_war/ Even if it does not look like it, he also deals with your question. Given your current list of languages i would simply say: functional programming with immutable data structures. But in context of the talk i just link i would rephrase it to: the ability to write correct code while seamlessly reusing existing JVM libraries. (slightly reduced version of his wording at 39:55, because this 2 phrases where in my head as interpretation of functional + immutable, before i looked up his statement within the talk)
&gt; Comments should stand or fall on their merits, not on who wrote them. Agreed. https://yourlogicalfallacyis.com/appeal-to-authority
OP and myself do not see the value of "referential transparency", when what our function returns is a monad. This style essentially uses monads to generate an impure function by combining pure functions with an impure part. We can (and I do) program this way without the use of monads.
So the way I came to Scala: I was a ruby programmer, and I wanted something with the expressiveness of Ruby but the typing and speed of Java. Try one of the Play examples and see how features like Future and Option work. 
It's a moot point now, because the OP was upvoted appropriately, but its sad because it represents that the direction the creator of the language plans to take Scala, as well as the style he recommends we use it in, appears to greatly differ from that of this sub-reddit, and that disconnect is something that usually benefits no one. Also, if you think monads are "easy to think about" then I'm not sure we can come to agreement on the topic. Suffice to say I would rather not track effects in the type system at all than to use IO.
&gt; Idiomatic Scala is its own thing. What is idiomatic Scala?
&gt; I don't understand the almost constant bashing on Scala by people that "care" about it. It's cool to care about something so much you bash it.
Still waiting for someone to explain why I'm wrong :(
We are still talking about two orthogonal aspects here. The first one is to have different kinds of persisting a customer. The second one is that we sometimes deal with customers (regardless of where it comes from) where we need their id and sometimes we don't need the id (or it would not even make sense to have one). Let's not mix up these two things for now because it makes things more complicated. Let's stick to the first aspect. The functional approach doesn't use any traits at all! I created a fiddle, so take a look. You can also execute it and play around with the code, e.g. change the example json to see what happens if the json is invalid or if the data doesn't make sense (like negative customer age): [https://scalafiddle.io/sf/NwewC87/2](https://scalafiddle.io/sf/NwewC87/2) I only used the Json in this example, leaving to implement a csv-from-database-like-request to you. ;) I hope this shows that you don't need a trait. Just convert your persistancy-models into your domain model and from there on work only with the domain model, no matter if it was originally created from CSV or JSON or anything else. The code example that you just gave, actually looks almost correct. The method would look like that (your method signature is slightly wrong): \`printCustomerName\[C: Customer\](customer: C) = println( implicitly\[Customer\[C\]\].name(customer) )\` But you don't need that, because you have full controll over the code. No one else is calling that \`printCustomerName\` method, right? Only if that method is called with class instances that you don't have control over, then the typeclass approach is needed. Otherwise, it's just overcomplicated and you can directly use just the one simple domain case class like in my example fiddle. Does that make things more clear? The second aspect about the ID is: you will not have one domain case class \`Customer\` but you will need more then one - unless all your customers always have an ID that is used in every context - which is very unlikely.
Well, that was discouraging. It felt like he said "Scala doesn't really have a purpose." He certainly didn't recommend any types of projects Scala would be good for. 
&gt; Still waiting for someone to explain why I'm wrong :( No one who is on the losing side of an internet debate will come back and reply with "yeah you were right I'm an idiot".
&gt; having functions In the subthread that ensues, there seems to be a lack of agreement about what functions can or should contain. Can they contain side effects, etc.
And yet, he is still using scala ;o)
Neither. Create an `object MyConfig` and put all your `getString` and etc. there. Then, just call `MyConfig.myEnvVar` wherever in your code.
&gt; I've also heard of people combining effectful values using Monoids as well Yeah, it's called GoF Command Pattern :&gt;
A corporate sponsor doesn't have to pay for improvements, only represent a stakeholder: a user of the feature, a source of information about whether or not it solves a real pain they have. Without _actual customers_ using early software and providing _real feedback_, the odds of errant feature development is extremely high. And that just leads to more complexity and less chance of something being maintained, which is a common story.
What's the rationale for this?
&gt; Existing FP developers may stay, but for many of us Scala's support for both OO and functional is what made it possible to become "FP developers" in the first place. This is the argument that Scala is a springboard language, which I'm sympathetic to, but I'd also like to think Scala's "end game" could be competitive with Haskell or Eta—i.e. that functional programmers would be happy staying with Scala indefinitely. Beyond just helping functional programmers stay with Scala, I believe the relevance of a springboard language has passed. If you want lambdas, local type inference, etc., you can just use Java or another Better Java language like Kotlin—many of which are simpler, have fewer ways of doing the same thing, better tooling, and more industry backing. In fact, there are no features in functional programming that are attractive to the mainstream that are not already in all the other Better Java languages—and soon this will be true of Java itself (data classes, pattern matching). If all Scala has going for it is being a springboard language, then I don't think that's enough to stay relevant today. Personally, I think it can be more than a springboard language. I think it can be a destination language. &gt; why is it that Scala has produced widely-used systems like Kafka/Spark/... and Haskell has not? Whatever factor it is that enabled Scala to produce those systems, I want to keep it, and I don't think it's just the JVM. Kafka is now a Java project, and Spark's successors are Java. These are not compelling arguments. The mainstream reasons for choosing Scala 10 years ago don't exist today. All we have left are esoteric reasons like higher-kinded types or dependent object types. Note that new data infrastructure projects on the JVM are being written in Java (and many outside the JVM are being written in Go). Not Scala. There was a brief period when Scala made a lot of sense, because it was a Better Java at the time when there weren't any other choices. But now that time is passed. Spark and Kafka will not repeat themselves (well, not unless we make it happen).
**Amazon** **|** **Software Development Engineer II, AWS IoT Core** **| Seattle, WA, US | ONSITE | Full Time** # DESCRIPTION Are you a software developer who wants to make an impact and connect billions of smart things to AWS eco-system? Do you want to design and build back-end services that route, transform and enrich data from and to devices on a global scale? Come join the AWS Internet of Things (IoT) team, and be part of a revolution! **The team:** The Rules Engine of Amazon’s Internet of Things platform (AWS IoT) enables customers to filter, enrich, process and route data from devices to the full eco-system of AWS services in a fully managed environment. Companies from all over the world can build their smart features and apps without having to worry about building scalable infrastructure that scales with their message traffic from billions of users and devices – because we do it for them. From simple sensors and actuators that barely have enough memory to run an OS and implement connectivity - to full-blown industrial gateways – they all can benefit from AWS IoT’s Rules Engine to receive, store and process data to implement an ever growing set of features without having to touch the device’s software itself by using the power of the AWS cloud. **You:** We’re seeking software developers with industry experience who are passionate about enhancing customer satisfaction, operational performance, and growing Amazon’s IoT business. If highly concurrent systems, distributed functionality, programming language processing and difficult synchronization problems make you want to take control, we have the right challenge for you. The Rules Engine of AWS IoT is a high performance, distributed system that connects customer data with the AWS eco-system by enabling customers to create rules to filter, pre-process, transform, enrich their message data before calling to other AWS and external services to extract value. We are building and operating a highly efficient instructions interpreter to process device data at high rates. All of our development is very customer-centric, and you should feel strongly not only about building and releasing good software, but about making that software highly reliable under extreme load. Experience with web-services, especially at massive scale, will be helpful. You should be someone who enjoys working and having fun with some of the smartest software engineers in the industry - building complex system software to deliver significant impact for the rapidly growing AWS IoT business. # Key Responsibilities * Design, build and optimize the software to run a highly scalable communication protocol engine and front-end to connect billions of devices to the AWS eco-system * Define intuitive web-service APIs and use AWS’s cutting edge technologies to develop and deploy new features quickly * Empower developers from around the world to use our SDKs to connect their devices to AWS IoT to enable intelligent systems * Automate deployment and monitoring of your service to track down performance issues before our customers get affected * Recruit, interview and hire software developers * Mentor junior software developers and grow their skills * Drive development process improvements and establish best practices to ensure highest quality software ### Basic Qualifications * Master’s or Bachelor’s Degree in Computer Science or related field of studies * Minimum of 2 years of industry software development experience using Scala, JAVA, C#, OR C++ * Possess an extremely sound understanding of basic areas of Computer Science such as Algorithms, Data Structures, Object Oriented Design, Databases * Be able to write high quality, testable and maintainable code * Must have good written and oral communication skills ### Preferred Qualifications * Strong analytical skills with excellent problem solving abilities * Proven ability to learn fast and ability to adapt quickly to a fast-paced development environment * Excellent understanding of resource and design requirements for highly scalable systems * Experience with the Linux operating system environment * Experience with web service development, distributed systems, and/or networking * Knowledge of system performance and operational readiness monitoring * Strong desire to build, deliver results, take ownership, and drive projects * Proven ability to mentor other software developers and to improve development best practices * Ability to optimize code for memory and CPU utilization to support hundreds of thousands of simultaneous connections and tens of thousands of message per second per server # Apply at * [https://www.amazon.jobs/en/jobs/688437/software-development-engineer-ii-aws-iot-core](https://www.amazon.jobs/en/jobs/688437/software-development-engineer-ii-aws-iot-core) * [https://www.amazon.jobs/en/jobs/686690/software-development-engineer-ii-aws-iot-core](https://www.amazon.jobs/en/jobs/686690/software-development-engineer-ii-aws-iot-core)
See [here](https://github.com/lampepfl/dotty/pull/4153#issuecomment-379163554) for one example in which a distinction between an FP construct and an OOP construct is suggested, and then subsequently dismissed, with a view toward what can only be described as an ideological commitment to unification of traits and type classes.
&gt; Why would you use Scala as opposed to any of the languages listed above? For me, Scala's defining feature is its type system, and you're doing yourself a disservice if you do not attempt to take advantage of it. There are other benefits, to be sure, but I feel you don't really "unlock" Scala until you can work at the type level effectively. It's combination of features like higher-kinded types, path-dependent types, declaration-site variance, subtyping, implicits, and more, allows you to write code that is expressive, reusable, and correct. In this case I mean _guaranteed_ to be correct, because incorrect code would not even compile. &gt; What are some of the cooler design features I'd want to explore? Implicits and traits, eventually getting to the typeclass pattern. Among all the type system features I mentioned implicits are probably the most unique to Scala and also some of the most pervasive features you'll find. Be aware that implicits are very flexible and powerful, so not all usages of them are good and helpful; it's easy to use them in costly and confusing manner. &gt; Is more like a "glue" that holds other languages together? Or is it a self-contained language like C++/C#? I'd consider more like C# in this regard. Specifically, it is its own language and projects can be built in 100% Scala, but it can also make use of libraries written in other languages due to running on the JVM. In particular, Scala is designed to inter-operate nicely with Java. C# works the same way on the .NET virtual machine and how it inter-operates with languages like F#. &gt; I get that it must work on a computer that's running JVM... but, I don't know the significance of that Just like running something on .NET, it means you can develop on one kind of machine (say, Windows) and have the code execute effectively the same on another (say, a Linux server), or your co-worker's Mac. You also get the benefit of all the performance work that has been put into the JVM, which is pretty much best-in-class. I don't have a lot of input on projects. I've used Scala on Android. People use Scala to create frontend websites (via scala.js). It's starting to get into the high-performance space (via scala-native). I'd guess the most prevalent stuff you'll find in the wild are business-related since that's where a big portion of Scala use (and JVM use in general) actually lies. Data science is big right now due to the Spark project. But people are using it for all kinds of things if you go looking for them.
Abandoning OO means to stop chasing a failed dream of making Scala a better Java, and instead focus on making Scala better for FP. This would imply bias toward FP constructs and features that make life useful for functional programming, in any case where there are tradeoffs to be made. It would also imply that instead of testing unproven, theoretical ways of tracking effects or emulating type classes, for example, we work within the existing language to make the things functional programmers are already doing easier and more idiomatic.
Referential Transparency has been mentioned a lot here. If you wonder why should one care: [thread](https://twitter.com/impurepics/status/1017844414578905088)
&gt; This style essentially uses monads to generate an impure function by combining pure functions with an impure part. That's not quite true. The style uses monads to generate a *description* of an impure function. The distinction is really really significant here, because *descriptions* of impurity can be reasoned about, composed, etc, but impurity itself is not. When you compose *controlled* effects (using tools like `IO`), you gain the flexibility to compose and recompose those effects at will. You have the assurance that the effect is fully under your control, because it's not in a "already running omg I didn't mean it MAKE IT STOP" state, but is actually quiescently waiting for you to activate it. Effects become no scarier or less pure than integer literal `42`, which makes them vastly easier to move around and reason about. You don't get that same assurance by simply combining impure and pure parts of your code. The impure parts will still be scary and dangerous. Values of type `IO` are never scary or dangerous. They can't hurt you unless you run them, and you won't run them until everything is nicely in order at the end of the world.
This is a really interesting answer, so I'm going to rebut a couple points really quickly. :-) &gt; Let's do a thought experiment. Instead of having a "magical" IO monad, let's have a magical capability CanIO. I think we can probably abstract this notion away from magic in general. None of the `IO` monads in Scala are magical at all (they're all entirely user-land implementations with no compile-time tricks), but they still have the same usefulness of a "magic" `IO`. In fact, removing magic from the equation is actually a large part of why the `IO` monad *as a monad* is compelling for solving this problem: it can be done without magic. `CanIO` needs special language support to have any meaning (since you need to attach that constraint at compile time). &gt; Aside: I have never understood the fascination with monadic syntax (e.g. in Scala's for expressions). They are fine when they are needed, but I find regular composition with val and function application much preferable. Me too. The trick is getting those sequential effects into *data* which can be manipulated without magic. In other words, converting something like: ```scala val a = foo val b = bar a + b ``` …transparently into: ```scala for { a &lt;- foo b &lt;- bar } yield a + b ``` The only solutions to this are things like the old `async` block plugin and SBT's `.value` syntax, both of which have significant user-visible gotchas and caveats. Direct monadic syntax, even through the syntactic guise of `for`-comprehensions, is considerably more uniform and it allows us to accomplish the goal of converting *statements* into *data* without leaky magic. &gt; In a way, computing the monadic binds/for expressions is like writing assembly instructions, where every step has to be spelled out in detail and sequenced properly. So why did Haskell do it that way? It has to do with laziness. In a lazy language, function application is not a practical way to sequence effects, so something else was needed. But Scala is not lazy, so this argument does not apply to it. The argument which does apply though is the need to convert sequential composition into some sort of data structure (where it can be manipulated and recomposed and scheduled and re-flowed and so on). `flatMap` is just a really really good, user-land tool for doing that. Other tools are certainly possible, like `async` blocks, but as with all specialized language magic, they come with significant caveats. I would absolutely *love* to see nicer syntax for manipulating monads (or things that are isomorphic to monads) in Scala, but having *some* sort of syntactic construct is, I believe, fundamental if you want a general solution. &gt; The other argument for going to monads and IO is that it makes composition of effectful code more cumbersome than composition of pure code. So some people like this for the educational aspect: Using effects should be painful, so that you are pushed towards referentially transparent solutions instead. I understand the point of view but don't subscribe to it. A good analogy is var and val. Scala makes you declare your intentions (and prefers the functional val), but there's no syntactic penalty for using var. Some people argue this is bad, too, and that writing imperative code should be different (and more convoluted) than writing functional code. I actually don't think that effectful code should be painful or convoluted at all. Most applications spend the majority of their time in *some* sort of effectful context. I spend a lot of time trying to make effect systems nice and clean and composable and *pleasant.* I do agree that some people present this argument (disincentivizing effects by making them horrible to work with), but I certainly do not ascribe to it. &gt; One aspect that I did not mention was that IO is often used for expressing futures or tasks. That's a usage that cannot be addressed directly with implicit function types, because it would require also continuations. In other word, IFTs are good for "boring" effects like mutation, exceptions, or dependencies, but by themselves they are not powerful enough to express control effects. There was an interesting paper by Jonathan Brachthaeuser at the last Scala Symposium where he showed how to do general algebraic effects with IFTs and a continuation monad. This is an awesome point! Let's unpack this… First off, I would contend that continuations are fundamental to any *useful* effect system in Scala. I mean it. The JVM doesn't have green threading, which means we need to manage async continuations explicitly if we want to have high performance IO (in the NIO sense, not necessarily the monadic sense). In practice, any serious application is going to have a mixture of asynchronous and synchronous effects, which is to say a mixture of direct- and CPS-style control flow. It is 100% essential that any effect abstraction, monadic or otherwise, provides for these needs in a unified fashion (e.g. consider the `IO.async` constructor in cats-effect, monix, and basically everything else). We cannot have synchronous effects by themselves, because that results in massive thread blockage. We cannot have asynchronous effects by themselves, because that results in extremely slow straightline performance and issues arising from the lack of JVM tailcall optimization. We need both. Second, I need to look up this paper! It sounds fascinating, but it's important to remember that continuations are not algebraic effects. This is reflected somewhat elegantly by the fact that there is no algebra `F[_]` such that `type Cont[R, A] = Free[F, A]` (referring to the `Cont` and `Free` monads). Note that there are a lot of very common non-algebraic effects, such as `List`. These effects have been the bane of systems like `Eff` since the dawn of time. In general, I really think the most promising direction of IFTs lies in encodings of MTL, which is a Haskell style of effect encoding done entirely with typeclasses but ultimately backed by monad transformer stacks (which are invisible due to the typeclasses). You get arbitrary composability and subsetting of the effect coproduct just like with the IFT proposals, you avoid convoluted syntax and complex chains of `lift`ing (in fact, no user code ever does any lifting), and you get excellent performance without magic or corner-cases surrounding non-algebraic effects like `State` and `Cont`.
&gt; Also I don't understand what number 2 has to do with IO. The way it should have been phrased is that if you abstract an effect type `F`, you can plug in other effect types.
I also want to thank scala's community. They are not known to be open minded. And yes, that has behavior has just convinced me to stay away from scala.
Those images are fire! Who is the brains behind that operation?
I basically completely agree with you. I would much prefer to have an effects system (which I recently read about in *Advanced Topics in Types and Programming Languages*. Not only are they more fine-grained, but they allow you to write your program using normal syntax / function composition. Side effects *are* a problem, but IO is not the solution (and I would argue they're not even as dangerous as FP purists make them out to be). 
Maybe it's just me, but I pretty much never refactor in the way you describe. I've seen this argument multiple times, and while I understand it, it's just never been an issue for me. But YMMV of course
I see what you're saying. But isn't the solution just to to `def c = ...` ? And honestly, I think the language should distinguish between lazy and non-lazy expressions of the same type, so your second example wouldn't even compile.
&gt; I've seen multiple bugs caused by bad declarations of Futures, which IO does not allow. Having that .run call really avoids problematic bugs like this, increasing efficiency across your team. I've seen multiple bugs caused by bad use of IO's :) &gt; You can use IO from cats or scalaz to design a wealth of functional programs, leveraging all the fancy Monads, Transformers, etc that come with them. And make the code even more impenetrable!
But why not take that further? `func` could literally do *any* IO, from reading a file to wiping a database. Wouldn't more fine-grained capabilities be an improvement? 
&gt; Monads are easy to think about and let you keep working in terms of plain old values and functions. Implicit capabilities are overly magical and cause more problems than they solve as far as I can see. It's so weird, cause I would have said the exact opposite. Monads require you to program using an entirely different syntax composed of maps and flatMaps. Maybe our brains are just wired differently
I agree with /u/KagakuNinja. I understand referential transparency, but I think it's vastly over-rated. Many FP advocates think that referential transparency automatically leads to bug-free, easy-to-read, easy-to-test code, which in my experience is just not true at all. In fact, I would say there's almost no correlation. &gt; The impure parts will still be scary and dangerous. Values of type IO are never scary or dangerous. They can't hurt you unless you run them, and you won't run them until everything is nicely in order at the end of the world. This is silly. You always have to run your IO. If you don't, your program doesn't do anything. Maybe that's what FP purists would prefer? And I don't consider impure parts of my code to be scary or dangerous in the first place
I'm not convinced of this. I am intimately familiar with referential transparency. I have used it to prove toy programs correct. But it's just not that useful for real codebases. The vast majority of refactors I make do not take advantage of referential transparency; it simply isn't a concern. What I see is programmers sacrificing all other aspects of program design to achieve the holy grail of referential transparency. Maybe I just don't like FP /shrug
What if I use Future to wrap a pure expression?
What about it? 
That has nothing to do with IO, but I'm still using Future.
&gt; Many FP advocates think that referential transparency automatically leads to bug-free, easy-to-read, easy-to-test code, which in my experience is just not true at all. In fact, I would say there's almost no correlation. I'm not sure anyone would make that kind of absolutist claim. All code in the untyped lambda calculus is referentially transparent, but I would defy *anyone* to write a non-trivial program in it which is bug-free on the first try. Referential transparency gives you code that is *reasonable*, meaning that you can reason about it and move things around. Let's put this another way: ``` expr1 + expr2 ``` I want it to *always* be the case that I can replace the above with the following: ``` val x = expr1; val y = expr2; x + y ``` Those two should be equivalent. Always. Every time. Any time they're *not* equivalent, it's just one more special thing I need to remember about this piece of code. Special things that I need to remember are *hard*. Really hard. They're hard to get right, and they're easy to forget, and they're just generally fragile. So rather than remembering that special thing, why wouldn't I want a guarantee that I can always make that entirely-reasonable transformation? Referential transparency allows you to forget about a lot of special things. A very, very, very large set of special things. That's why it's valuable. &gt; This is silly. You always have to run your IO. If you don't, your program doesn't do anything. Maybe that's what FP purists would prefer? :-) Please don't ascribe straw-man arguments to "FP purists". The whole point is to write programs that do things, and no one would pretend otherwise. You do run your `IO`, you just run it *once*, at the end, once everything is set, and you don't need to change anything anymore. In languages like Haskell or Idris, the platform will take care of running it for you, and you *can't* run it yourself without using special functionality. In Scala, obviously we're always going to end up calling `unsafeRunSync` in our `main` method somewhere, and that's ok! That's exactly as it should be. What is useful here is what happens *before* we call that function. By putting our effects into the form of data (e.g. perhaps using a monad like `IO`), we can manipulate those effects and ensure that all is as we want it to be. We can reschedule things for fairness and thread utilization. We can wrap things in error handling and recovery logic, and that logic can be *localized* to a single part of our codebase so we're sure we did it correctly. And so on and so on. Maybe the strongest argument here, honestly, is controlling your effects allows you to test code much more easily. Effects are *hard* to test. Really hard to test. This is a reflection of how dangerous and hard to reason about they are, but in general it's much more easily seen through the guise of testing. When you write a unit test, you don't want to have to find a database connection, load some test data, run your function, inspect the database to make sure the right things happened, then roll it all back. That's insane and hard and painful and no one likes it. Even for simpler things like filesystem functions and such, you really don't want to do much of that sort of testing if you can avoid it. You have to do *some* of it, but the majority of your program's logic should be tested through much simpler mechanisms. Controlling effects and, more generally, referential transparency makes testing much easier. Much, much, much easier, because you can just test functions in isolation and know that they're going to do exactly what's on the tin, no more and no less, with no other dependencies. Imagine only having to write complicated (and error-prone) test harnesses and mocks for *integration* tests. Imagine a world in which every line of your code has extremely comprehensive, *fast* and reliable unit tests that were pleasant and easy to write. That's the world I honestly live in, and the majority of that world comes because of controlled effects.
I don't think I get your point. 
wish i could upvote more than once.
First of all, to be clear, it was not at all a criticism. And I'm not any better at this. Perhaps Scala just attracts people that are more thoughtful and honest. ;) Maybe there are a lot of enthusiastic-sounding talks and they're just not the ones I listened to because they didn't cover something I wanted to learn more about. :D Anyway my point really isn't about the tone at scala conferences, it's more about "evangelism" at non-scala conferences... P.S. I definitely enjoyed your talk! 
&gt; But it's just not that useful for real codebases. What is it about "real" codebases that makes it less useful? I can point to plenty of codebases out there of varying size and complexity that are very very real (as in, deployed in production, massive scale, users, SLAs, bug reports, etc) which have maintained referential transparency throughout and derived extremely significant value from it. &gt; The vast majority of refactors I make do not take advantage of referential transparency; it simply isn't a concern. Actually, I'd be willing to bet that the vast majority of refactors you make *do* take advantage of referential transparency when it can be inferred, and spend a considerable amount of time and logic (either human- or machine-driven) in sorting through the special cases when it cannot. Referential transparency gives you easy refactoring for free, because everything just becomes a trivial code transformation, rather than something you (or your IDE) have to think carefully about doing. &gt; Maybe I just don't like FP /shrug :-) I'm guessing it's more that you just haven't had the opportunity to witness the benefits you gain from the rigidity. Pure FP *does* force you to do things in a certain way. If you haven't seen how that methodology works at scale (in code terms), then it probably seems a) considerably more onerous than it actually is in practice, and b) considerably less useful. This wouldn't be an unreasonable position for you to have, btw. You believe the evidence of your eyes, and you haven't *seen* how it works yet. I really sympathize with your point of view, and it's one that I 100% shared about a decade ago. I'm just urging you to keep an open mind and understand that the people who are doing pure FP aren't doing it out of a sense of religious zealotry, but rather because it legitimately leads to better software that is easier to maintain, easier to test, easier to write, easier to refactor, and imposes considerably less mental strain to understand.
OK. I'll keep my mind open. Maybe it will take a few years to click. Maybe it never will. Right now I'm just going to grin and bear it :)
&gt; And honestly, I think the language should distinguish between lazy and non-lazy expressions of the same type, so your second example wouldn't even compile. For plain old values it doesn't matter - `def x = 4` and `val x = 4` have no substantive difference. All you really want is a different type for lazy values where evaluating those values has a relevant side effect - which is exactly what `IO` is.
Not true. There's tons of research into lazy types, and they are quite useful. See the chapter in **Practical Foundations for Programming Languages**. You should do more research into programming language theory - it might broaden your horizons. Haskell isn't the be-all end-all of language design
First of all, I'm very glad to hear that I was right in assuming you were exaggerating when you made it sound like you were suggesting removing things like inheritance from the language. Unfortunately I doubt most people realized that, which makes it much harder for people to "hear" your points and be receptive to them. Instead, what you are proposing is what a lot of people want, which is to put less emphasis on improving the ergonomics of OO features (though no one thinks of that in terms of "better Java"), and more emphasis on that of FP features. And a lot of people feel that Dotty is doing that. They've engaged in the process and found the other participants receptive. ADT/enum syntax, type lambdas, generic tuples, kind polymorphism, dependent function types, by-name implicits, all fall squarely into "making the things functional programmers are already doing easier and more idiomatic." I understand where your pessimism is coming from, but I think it's not justified today. Instead of giving up so fast, please become more active in constructive ways. Make concrete proposals on contributors.scala-lang.org, and on dotty's gitter channel and issue tracker. When people raise counterarguments don't make any assumptions about their intentions, just respond to them point-by-point. And so on. There is so much you can accomplish, but as long as you act like people are a priori opposed to your (FP) interests, people will react emotionally and they won't be able to "hear" you. If you stick to technical details, and speak with humility and respect, you will find people receptive to your proposals. (This is true regardless of whether people actually are opposed to being "too FP" or not.) Dotty is still (to a certain degree) a sandbox for experimentation. A lot things (especially typeclass syntax and effect tracking) are far from set in stone. So please, if you have a concrete suggestion for ways to "make the things functional programmers are already doing easier and more idiomatic," please get involved! I'm looking forward to seeing your ideas on contributors.scala-lang.org. 
&gt; Monads require you to program using an entirely different syntax composed of maps and flatMaps. That's one way of approaching them, but you can form APIs that look a lot more similar. If you're writing pipelines the difference between monadic and not can be the difference between `|&gt;` and `&gt;&gt;=`, or the difference between `&gt;=&gt;` and `&gt;==&gt;`. If you're inside a `for`/`yield` then it's just the difference between `&lt;-` and `=`. If you're already writing you control flow as expressions then the difference between a monadic conditional or loop and non is just the difference between `ifM` and `if` or `whileM` and `while`. You can even abstract over the distinction entirely and write in terms of a generic `Arrow` that could be monadic or not. This kind of stuff is possible because everything you do with monads is ultimately just plain old functions and values, so you don't have to rely on any language-level special cases. Whatever kind of operator makes the most sense to you, you can define and use that operator.
Well there's a chicken-and-egg problem here - you can only refactor in this way if you're working on a codebase where all your functions are pure. As soon as even 1% of your functions are side-effecting it becomes too unreliable. All I can suggest is: try working on a project that does this. You say your team is talking about whether to adopt `IO` - maybe you could agree to use it for one isolated component and look at whether it makes things easier or harder. I don't blame you for being skeptical about it - I was too for my first ~7 years of Scala.
Lol it's my first year of Scala. I was all excited to have case classes, ADTs, and immutable data structures (all of which IMO are many times more useful than IO), and then this gets thrown at me. It's look dipping your toe in the pool and getting pushed in. We've already adapted IO, I'm just the lone naysayer, but I have to accept it or else I'll lose my sanity (too late)
I agree with what teknocide said. Try and be as humble as you can, and really try to defer judgement until you are more experience. It's easy to come in with previous experience and preconceived notions about things, and let that affect your judgement. Try to remain open to the new ideas, instead of fighting them. When you have more experience with Scala, then revisit those things, and look at them again
You're right. And there's active work being done in finger grained tracking of all kinds of effects. See the Eff monad: http://atnos-org.github.io/eff/org.atnos.site.Introduction.html#first-example It hasn't really caught on yet because: 1) Their usage ergonomics aren't that great (...yet?) 2) Tracking IO is "good enough" The somewhat dirty little secret of functional programming is there is no absolute definition of what effects to track or even the ability to track *all* effects. It's all relative to your program and what's valuable to you. I personally have never received value from tracking println's and log statements. I'm sure there are domains in which tracking memory, temperature and fan speed are important, but most people doing CRUD-type apps will greatly benefit from just knowing when IO is happening. Knowing that the IO is to a cache, db, or remote service doesn't move the needle much. What moves the needle is knowing that my business logic is easily unit testable, parallelizable, re-runnable, cacheable, re-usable, etc. because it's pure. And that the IO is easily discerned from the aforementioned business logic. And that composing the two is just a little plumbing work with map, flatMap, and friends.
I'm not sure how you managed to read it that way. What I see is (from that comment on): 1. @etorreborre raises an objection to the desire to "be able to migrate fluidly from a normal trait to a typeclass without having to refactor code" 2. @odersky argues that the objection may not hold, and additionally Rust seems to illustrate that the objection doesn't hold. 3. @aloiscochard says that the Rust implementation has serious issues 4. @etorreborre encourages both alternatives to be explored then compared 5. @odersky is receptive to @aloiscochard's experiences but assures that one particular issue wouldn't apply to Scala 6. @Ichoran says all the Rust issues wouldn't affect Scala 7. @aloiscochard sounds appreciative of the feedback 8. @LoranceChen suggests using different keywords 9. @allanrenucci closes the issue, adding the "stat:revisit" label So in short, all I see is a bunch of people engaging in a discussion of technical merits/demerits of Martin's proposal, and concluding that it needs to be revisited later. Is Martin a bit on the defensive side of his proposal? Maybe. That's the role he's supposed to play in a discussion about his proposal. That doesn't mean he is insisting on it. (He's made that plenty clear too.) The way progress is achieved is not by handwavy "scala should be more XXX" statements. It's by coming up with concrete proposals, thoroughly analyzing all the tradeoffs, and comparing them. And as far as I can tell, that's all people are doing. I'm looking forward to someone opening a PR on the dotty repo implementing @LukaJCB's proposal, and the ensuing discussion. If someone does that, I absolutely expect it to have a strong influence on the final result. (I don't expect any single initial proposal to be the final result.) 
You can use Scala for building web apps(with Play framework and Akka) or use it for Big data apps(with Spark). Scala is a general purpose language, so you can build any app that you'd build with other langs. Scala is good for backend on JVM env as it deals with parallelism and stream processing effectively. You can also try ScalaFX which is a wrapper around JavaFX for building desktop UI app
&gt; I'd also like to think Scala's "end game" could be competitive with Haskell or Eta—i.e. that functional programmers would be happy staying with Scala indefinitely. I'm not aware of any really compelling advantage for Haskell or Eta, and certainly not anything that Scala's OO support interferes with. Laziness is not the right path IMO (e.g. Idris avoids it). Not having kind polymorphism is infuriating in the abstract but honestly I've hit maybe two solid use cases in my whole career. A less verbose approach to typeclasses would be extremely welcome as would improvements to some type-level programming constructs, but it sounds like those are already being worked on. And of course better type inference is always good, but again what we have is so rarely a problem in practice. There is always room for improvement, but people don't switch languages for the sake of a bit of boilerplate reduction or more elegant internals. Haskell and Eta are not a threat unless they've got some "killer app" that we outright can't do. &gt; In fact, there are no features in functional programming that are attractive to the mainstream that are not already in all the other Better Java languages—and soon this will be true of Java itself (data classes, pattern matching). &gt; The mainstream reasons for choosing Scala 10 years ago don't exist today. All we have left are esoteric reasons like higher-kinded types or dependent object types. "Attractive to the mainstream" is a moving target. Ten years ago you could've said the mainstream didn't care about esoteric features like data classes or pattern matching, but Scala proved there was an appetite for them. I think the same can be true for HKT and typeclass derivation. &gt; Note that new data infrastructure projects on the JVM are being written in Java (and many outside the JVM are being written in Go). Not Scala. &gt; Spark and Kafka will not repeat themselves (well, not unless we make it happen). That's not what it looks like from the side of the industry I've seen. There is still interesting, valuable work being done in Scala in the broadly "data infrastructure" area, and it's happening because Scala is the right combination of accessible to OO programmers while providing unmatched functional power. If that market ever vanishes entirely then I see no harm in letting Scala die with it - we're not a business that has to survive for its own sake, we're a tool for programmers, and if other languages have surpassed us then we should let programmers move on to those languages. There's no point in trying to be a better Haskell than Haskell or a better Eta than Eta. As you said, the relevant question is what is unique about Scala.
&gt; I also want to thank scala's community. They are not known to be open minded. And yes, that behavior has just convinced me to stay away from scala. What is the point of sarcastic, toxic contributions like this?
This was my reaction to the toxic community contribution regarding my other post. 
\&gt; That being said, I don't really know what Scala's niche is. Being able to entertain OOP and FP styles simultaneously. \&gt; Why would you use Scala as opposed to any of the languages listed above? If you want to explore FP. If you want to work on the JVM. \&gt; What are some of the cooler design features I'd want to explore? Type inference. Anything you find that's related to FP or making FP work (e.g. implicits). I'll amend this by saying certain kinds of folks might chime in with things like "the type inference in Scala isn't that great" or "implicits are a universal evil", but those are a matter of perspective. I think there's a lot to learn, if you look at it through a nonjudgemental lens. \&gt; If I were to encounter Scala, what kind of project should I expect? In my experience, Scala projects tend to be... Ideology driven. Anything from "I want to write a Scala wrapper around this well-established Java library so that it feels easier to use in Scala" to "I want to re-envision common conventions around some workflow and do it with the latest and greatest in FP machinery". \&gt; Is more like a "glue" that holds other languages together? Or is it a self-contained language like C++/C#? If its not any of the above, then what is it? It's more like a full-bodied language like C++ and C#. As someone else mentioned, you'll definitely see things that are written completely in Scala. It's not a convenient glue/scripting language. But maybe some people are trying to change that (Ammonite). \&gt; I get that it must work on a computer that's running JVM... but, I don't know the significance of that. Does that just mean its more portable? Or is there something more important going on there? The sell is that anywhere a JVM runs, your app can run. The JVM platform is popular among employers. Good support on AWS. Any performance enhancements that go into the JVM you get for free when running on it.
&gt; What if I use Future to wrap a pure expression? What does this buy you? Can you provide an example?
&gt; its sad because it represents that the direction the creator of the language plans to take Scala, as well as the style he recommends we use it in, appears to greatly differ from that of this sub-reddit, and that disconnect is something that usually benefits no one. It is indeed unfortunate to have such a divergence. I hope Odersky will come around to the community's way of seeing things - or, at the least, not make language changes that destroy the things that have made Scala such a valuable language for so many of us. But, to the extent there is disagreement, far better to have that disagreement in the open, and maybe be able to find solutions to it, than to pretend what Odersky is saying must be right because he's Odersky. &gt; Also, if you think monads are "easy to think about" then I'm not sure we can come to agreement on the topic. Suffice to say I would rather not track effects in the type system at all than to use IO or any of the other techniques that have been advocated by the Scala FP community so far, including free monads. Hah. *Relatively* easy to think about, then. Monads are not great but they're simplicity itself compared to continuations. Not tracking effects works great most of the time, and I certainly wouldn't want to lose the ability to just "YOLO it" and let side effects happen when they happen during program execution. But when you have multiple effects that interact with each other in a codebase that's too complex to keep in your head, that just becomes impossible to understand, and at that point even though tracking the effects imposes a huge additional overhead, it pays for itself by making it possible to decompose the program and reason about the pieces individually. At some point understanding 100 x-line functions becomes more plausible than understanding 10x lines of hopelessly intertwined code. &gt; Every effort I have seen to encode effects into Scala's type system has resulted in what appears to be a ton of boilerplate, a significant amount of unnecessary performance penalties, and complexity that only the most seasoned of Haskellers can ignore have programmed in the style for years. Well for that last part I'm only guessing. I do believe though that as a software developer I am far more productive not worrying about effects in my type system and the quality of my work is fine. I can't blame you for thinking that way. All I can say is, my own road to monadic effects is paved with production bugs I've experienced. NPE -&gt; use options rather than null. Production error that the test should have made impossible because a test exception was being caught -&gt; use eithers rather than exceptions. Production logs being completely wrong and confusing because no-one can really unit-test logging -&gt; use a treelog-like construct. A bug rather like [this story](http://thecodelesscode.com/case/211) -&gt; use a monad to track database transactions rather than an interceptor. Enough years of that and the monadic operators become so normal and second-nature - you use them so many times in so many different contexts - that the cost of using them to track IO drops a long way, and as you refactor more fearlessly and drop your level of test coverage you reach a point where *not* tracking IO becomes more expensive. Or I did, anyway.
&gt; Effects are hard to test. Really hard to test. Maybe it's the difficulty in effect management that the OP disagrees with because they lack experience. If you don't think effects are hard, then the sell to FP/IO is stupid.
&gt; But it's just not that useful for real codebases What is a "real codebase"?
&gt; what I care about in a language is a strong static type system Why do you care about a strong type system?
Did you reply to the right comment? I thought we were talking about programming practices in Scala. Theoretical notions of types for lazy things are neither here nor there, unless you have some way of retrofitting that distinction into Scala when the language treats them as being of identical type.
&gt; pure functions are always bug-free This is a goal post move, right? No one is making the claim that "pure = bug-free". But probably "pure = easier to reason about = less likely the source of bugs".
Check out the free chapters from the book Functional Domain Modeling by Debashish Ghost. https://www.manning.com/books/functional-and-reactive-domain-modeling
I agree with this. If effects are easy then FP is mostly pointless.
&gt; I was all excited to have case classes, ADTs, and immutable data structures (all of which IMO are many times more useful than IO), and then this gets thrown at me. It's look dipping your toe in the pool and getting pushed in. I don't disagree. I'd certainly adopt Option, Either, Writer, Future, State, Resource, custom monads, and transformer versions of the first three before I even thought about using IO; to my mind it only makes sense in a codebase where you're already very comfortable with the various monad operations and making extensive use of them. Even today my team allows unconstrained logging (for the benefit of team members who like it; I tell myself that I don't care about what gets logged, consider non-logging equivalent to logging, and try to make sure any data that I care about myself gets persisted in a more structured form) and only really uses our IO-like construct for controlling important network calls. I do think IO has value, but in many codebases it's pretty marginal and honestly I've seen a lot of cargo-culting in companies that adopt Scala. You're right to be skeptical. I wish I could give useful advice about within-company technical politics, but I've never been much good with that.
Look, there are plenty of negative videos about every programming language. If a python newbie asked questions like "What are some cool Python features?" and "What are some examples of good Python projects?", I'd respond with "Check out asyncio and Django" not "Check out one of these videos about why dynamic typing is idiotic". I actually enjoyed the critique you linked to and agreed with some of his points. But can we just chill out on the holy wars for a second? They're not usually relevant to someone new to the language.
I entered a project recently where instead of injecting the Conf or a subset of that conf as a case class it was using the load() directly everywhere on class constructors and object constructors... It's crazy. I'm still having a bad time just thinking about it. Love the true gods of side effects. There's no good reason to not pass the necessary conf as a parameter. As a plus, take a look into [https://github.com/pureconfig/pureconfig](https://github.com/pureconfig/pureconfig) . With it you could just read the whole config one time in one single place, having it all typed as case classes and then just insert the required parts wherever you need.
&gt; They're not usually relevant to someone new to the language. Are you certain? Experienced programmers will be careful when they attempt to adopt a new environment. They will try there best to identify potential traps before they are investing a lot of time. A video like that will shape opinions. A community acting immature against outsiders/critism will also shape opinions. Scalas community is well known for it's elitism.
Fair enough. I appreciate the polite and well-reasoned response even if we don't agree.
IMHO it's great at domain modelling: * case classes and immutable data structures are great for modelling entities, value objects, events * you have (Akka) streams and other forms of modelling your processes as piping the data (map and flatMap for Futures, and Taska, FS2 streams, Spark) * above + FP (pattern matching, foldLeft at all) makes it natural for writing event sourcing * once you learn a bit about type system quite often you are able to make invalid states unrepresentative - if you cannot compile invalid code it won't happen at runtime Additionally with metaprogrammning you are able to generate quite a lot of code that would be * trivial and repetivive * a bother to maintain, because you could make a stupid mistake In one of the projects I could go full DDD and I was almost sad I didn't had any opportunity to write a framework: * I could express all my read models, write models and events as case classes * all services were simply functions returning Monix's \`Task\`s - so async + error handling + control over execution * my hand were not tied by a uniform model (same models for API and for domain internally), yet with metaprogramming I could convert one representation into the other (library - [chimney](https://github.com/scalalandio/chimney/), by my friend and me ;) ) As for projects: Scala is very flexible, so you might meet projects that use it as OO Haskell on JVM, you can see projects where it is used as Java without semicolons, you can see projects where people use it only because of Spark or Akka. It uses JVM interoperability, so you might use it as a glue code, but you will feel \*yuck\* doing so: Java returns nulls and throws exceptions, so any interop with Java usually involves \`Option\`s and \`Try\`s that will be pattern matched immediately. (So people from Java that use Java libs intensively will consider then ove-rengeeniered \`if\`s and \`try\`-\`catch\`s). JVM implies that you can run your code on anything that has: Java SE, Open JDK, Zing, Zulu, IBM JDK, GraalVM etc installed. Actually you could use it on Android though it is uncommon. Also you have access to whole JVM ecosystem: Maven, Ivy2. There are ongoing efforts to make Scala also compilable to JS (Scala.js afaik it is consider production-ready) or native code (Scala Native, not as mature). My advises: * know your type system * don't stuck with Slick - it might look nice at first, but over time I see more and more limitations * nowadays you almost never need (untyped) actors - I had to write about 1 actor over last year, as a scheduler. now I would do away without it - learn akka streams instead, try out alpakka integrations. typed actors see like something reasonable to know but I had no use case for them so far * play is really nice, but it involves a lot of magic (IMHO), so more than few times you might think that you have no idea why it works (hint: code generation) * don't try to understant sbt from the day 0 - it does quite a lot of stuff, so it is easier to understand once you get more familiar with Scala (shameless self-advertising: [I wrote a post about sbt](https://kubuszok.com/2018/relearn-your-sbt/) :)) * learn one thing at a time. If you try to learn a syntax, a framework, few libraries, category theory etc at once it will look much harder than it actually is.
pureconfig ftw! If I test something I'd rather pass a case class than... what exactly? Use env vars or Java arguments and override them for each piece of code?
&gt; Scalas community is well known for it's elitism. Actually, what Scala is well-known for is progressively introducing everyday programmers to functional programming (FP). I'd call that _the opposite_ of elitism. If you want an example of elitism, I refer you to the video you linked above yourself. This guy wants to cut out the object-oriented part of the language, which is what provides a bridge between traditional OOP and FP and allowed so many people to transition in the past.
The original poster said it’s “near impossible” for an RT function to have bugs. That’s a pretty bold claim 
&gt; don't stuck with Slick - it might look nice at first, but over time I see more and more limitations Drive by Slick hate! What about it do you find limiting?
Wouldn't say hate. But I'd like it to be more flexible. I imagine some mappings could be abstracted away (ttfi?) or generated with type class derivation but it is virtually impossible with heavy reliance on implicit evidences in each operations, path-depended types, query and dbio being different things etc. When I wanted to DRY I had to use macros (macro annotation). Hardly composible solution.
Not quite what I claimed. Given the context of whatever your current bug is, you can scan over various functions, see what their type signatures are, and then declare with more confidence that "the bug is almost certainly *not* here". If you have some funky race condition say, then you know *at least* that none of your RT functions are to blame.
&gt;don't try to understand sbt from the day 0 So true
And how do you override that config in your tests?
Strong types actually make coding easier since everything is explicitly and you don’t have to guess what a variable’s type is. I’m still confused how IO makes my life easier. It apparently helps with refactoring and maybe it’s more suited to massive code bases. But I’m still at a loss. 
The other answers here are very complete. I'll reiterate the popularity within the data science community for production ML deployments thanks to Spark. Although I don't think that's necessarily a niche it's known for (I could be mistaken), performant data science work does come to mind for things Scala is used for.
&gt; a function would instead return a CanIO =&gt; T. So this says "give me the capability to do IO and I return a T". Assume the CanIO capability is produced only by the runtime that executes a program. &gt; Then every program and every function that has side effects has to be of type CanIO =&gt; T, since there is no other way to get the capability. This is exactly the effect of an IO monad. This is completely incorrect. However, I have retired from correcting Scala programmers. Just know that it is both wrong, unhelpful, and leads to a complete misunderstanding of the practical consequences. The remainder of the explanation highlights that misunderstanding.
Thinking of functions as first-class values represents a paradigm shift in programming. You could accomplish all the same things with GOTO statements and switches, but it's error prone and isn't very easy to conceptualize, reuse, or maintain. In a similar way, IO data structures allow programers to think of side-effects as first-class values. A function that returns an IO action encodes a whole program as a first class value. It can be passed around, manipulated, tested, refactored, inlined, factored out, abstracted, etc, just like first class functions. It took a long time (and a lot of convincing people) to achieve the benefits of replacing GOTOs with structured loop and procedure calls. Likewise, the ramifications of treating effects as first-class values have not yet fully panned out, and it'll take a long time to convince people of their utility. Now, think of first-class functions: if your language doesn't support them well they will be painful to use. Basically all classical OOP patterns arise from trying to model first-class functions in languages that don't support first-class functions. Since effects as first-class values is still a new concept, our languages don't really have the features to support it well. Haskell and Scala are really the only languages that let you abstract over effects (you need higher-kinded types and type classes), but doing so is still awkward even in those languages. While I love me some Haskell and Scala, I'm hopeful for the next crop of languages.
I wouldn't recommend that library, but if you must you should definitely have it injected into the constructor for all the reasons that are usually cited about why DI is good. 
https://www.artima.com/scalazine/articles/selfless_trait_pattern.html
It's a very good question, and one that I asked myself as well. The first step to consider is why would we want to wrap our computations in any kind of wrapper in the first place (think `Future`, `Task`, `IO`)? Why not just code synchronously like "in the old days"? Scala's definitely not the only one doing it (consider e.g. JavaScript and `Promise`s), so there might be good reasons. The long version of the answer is here: https://blog.softwaremill.com/synchronous-or-asynchronous-and-why-wrestle-with-wrappers-2c5667eb7acf, but to sum it up in 2 points: 1. most of the power comes from *representing programs as values*. This really changes the way you write code, but at least for me, it came with time. The simplest example is composing a number of asynchronous computations to run in parallel - it's so much easier and more readable when operating on values. But that's only a start. Another great example is working with SQL databases and transactions. Instead of relying on annotations, thread-local transactional contexts, or even implicitly passed transaction/connections, one can define values which *describe* a computation that should be run in a transactional context. Any number of these can be composed (at any point in the program - we can re-use already defined values) into the final computation description to be run, given an open connection. The logic that creates the descriptions of the computations is completely separate to how the connections, or transactions are managed, and doesn't need to be aware of it. That's exacty what Slick is doing with `DBIOAction` and Doobie with `ConnectionIO`. 2. the clear demarcation of methods which are side-effecting / perform I/O is also very valuable for code readability - the signature of a method tells you so much more, than when using synchronous functions. Having method signatures which reveal as much as possible about what the method can, and more importantly *cannot* do, is crucial for the "explorability" of a codebase. With more complex control flows, wrappers are more of an obstacle than help. Luckily, we have the `async`/`await` mechanism - available both for Scala's Futures (through scala-async) or for any cats/scalaz monad (through monadless/effectful), which allow you to locally use synchronous-like syntax, having the code automatically translated to the wrapped version. (As a side note, Javascript and Kotlin have very similar constructs, but baked into the language: async/await and coroutines). Now that we know that wrappers are actually useful, which flavor of wrappers to use? Futures, which represent an eagerly executing computation, are a very popular option, but what about the lazy version, Tasks/IOs? Even Akka recently introduced a lazy wrapper for computations (`Behavior`s in akka-typed), so the lazy wrappers seem to be an increasingly popular option. What do they give us? As you write, Tasks/IOs are essentially lazy computations. There isn't much more to them. But that's powerful enough - this, and being able to represent a computation as a value. In addition to freely manipulating and composing asynchronous computations, thanks to laziness you can do so *without fear*. It's a liberating change - calling a method and not worrying what might be the side-effects (e.g. getting a running future), as they are all deferred and encapsulated. Take for example the already mentioned `retry` method, or a rate-limiting function (described in detail in the blog I link later). Yes, you can implement them using by-name parameters `def retry[T](f: =&gt; Future[T])`. However, you need to be extra careful no to create the `Future` before passing it to the method (e.g. capturing it as a val, or passing a previously captured result of another computation). With lazy wrappers like Tasks/IOs, this whole class of bugs goes away. Another advantage is that you can specify the exact configuration of how a computation should execute after it's defined. In the Akka-typed case, that can mean specifying the recovery logic after the main actor behavior is specified. All captured as a value, which can be refined and used multiple times, on multiple threads, without a chance to get a threading bug! With `Task`s/`IO`s, we can fork computations to run in the background, retry them, handle errors, etc. This can be done on an arbitrary computation defined by arbitrary methods; thanks to laziness, we can instruct the interpreter how to execute about logic after it's created. Excuse me for linking twice to my own blogs, but as I said, I've asked myself the same question, and the blogs are the long version of how I arrived at a conclusion that using lazy wrappers *is* the preferable way to write code which performs I/O and runs asynchronous computations. So, here: https://blog.softwaremill.com/scalaz-8-io-vs-akka-typed-actors-vs-monix-part-1-5672657169e1 you can find the first part of a series which tries to compare the `Future`-based approach of Akka, with Akka-typed `Behavior`s, Monix `Task`s and ZIO `IO`s. 
Scalaz 8 IO (ZIO) looks neat. ...how can I try it out? There's a repo but it doesn't look like there are any releases pushed? I don't see much info about roadmap, but maybe I'm not looking in the right place.
&gt; unhelpful Just like your own comment, then...
What would you like help with?
&gt; I've seen multiple bugs caused by bad use of IO's This is pretty unfair. I know what bugs you're talking about, and most of them are related to switching from Future to IO. Switching between concurrency primitives is bound to come up with bugs, because the way they operate is fundamentally different. It's not as easy as just dropping them in like we did. With Futures, we were exploiting their eager behavior, and then abusing value discarding to get a fire-and-forget type thing. The value discards should not have been there in the first place. The other bugs I can think of which you may be referencing had to do with people just not reading the docs/not understanding how to execute an IO on a different thread. All I can say about that one is, sure... If you want a simpler surface API and you don't care about performance or lawfulness, then use Future. If any of those two things matter, then you have to use something which exposes the ideas of context switching. If you want multithreaded behavior, but you don't want potential context switching every single map/flatmap, then it has to be made explicit, so the api must have more methods, and therefore more to learn.
With understanding what exactly is so very wrong about what Odersky said.
I explained that to him nearly 15 years ago. He then became abusive. His understanding has clearly not progressed since then. I am not in the business of engaging this anymore. Many others have also been on the end of this and are no longer playing. That's not to say, I don't wish to help you (or anyone), understand why it is wrong, and generally share ideas openly. Just that this is not going to be happening here, right now. Instead, I'll just point to the fact that nothing has changed and it is still wrong. You're always invited to forums where discussions on this subject are open, and with no other agenda but to help each other learn more.
Configuration is historical. It predates Typesafe Config itself. The advantage of using Configuation is that it can do Option and make use of Typeclass patterns using as[], but if you are writing domain code or want to use libraries like Ficus, then Config is more appropriate.
Just FYI, since this seems to have a transparent background, the black text disappears against the black background when you click through the link. For everybody else, the text says "car salesman: *slaps roof of Java* this bad boy can fit so much fuckin boilerplate code in it". 
Thanks for sharing some details of how IFT can be used here! I'm still trying to wrap my head around understanding how they replace some of existing usages of monads/functors/... in Scala, so this definitely helps. However, I think I would order the purposes of `IO` radically differently: first and foremost, `IO`s/`Task`s/`Future`s are useful to wrap asynchronous operations, represent computation and values and compose them (e.g. to run in parallel). Having the side effects being represented in the type signature is **also** great, but not the main driving factor. (This might be different from the original design goals of `IO` in Haskell - but if it's so, it wouldn't be the first case where a language construct is used differently than originally designed :).) Take a look at any "application" Scala codebase (so I'm not talking about libraries or the compiler, just regular end-user code): it's full of `Future`s, sometimes `Task`s, `DBIOAction`s etc.! Wrapping I/O in values is just very useful (as I also tried to justify in my top-level answer to the question: https://www.reddit.com/r/scala/comments/8ygjcq/can_someone_explain_to_me_the_benefits_of_io/e2d6qhi). I agree that composing `Future`s or `IO`s is often painful, but that's what `scala-async` is for, and that's also exactly what coroutines in Kotlin or await/async in JavaScript do: they allow to locally create the familiar syntax, which is then transformed to operate on wrappers by the macro or the compiler. It has its limitations, and isn't quite the same as simply operation and composing functions, but is often quite useful. My main concern with `CanIO` is how it would combine with `Future`s or `Task`. It seems we would have two ways of representing side-effecting computations: one using IFT, one using wrapper types (monads). When to choose which? How to combine a computation which uses IFT with another which uses `Future`? Why have two constructs for the same purpose? I'm far from an expert on language design, but for me the laziness of a data structure like `IO` is on a different level than how the language is evaluated (lazily or eagerly). Laziness of `IO` allows, among other things, to instruct the interpreter how to run the computation after it's created, hence allowing to create descriptions of computations and *composable* transformations of them. Laziness of the language also influences when side-effects happen, and might be yet another factor in favor of IO. But a lazy-future in an eager language definitely makes sense as well. (btw: I tried to find the paper you mentioned, but all links on http://ps.informatik.uni-tuebingen.de/2017/10/23/talk-at-scala-2017/ are broken)
This is a post I put together for some friends that may help. http://codeninja.blog/2018/monads/ I don't get to IO until the end, mostly because there's nothing really special about it. It sounds like you generally have a working understanding about monads, so the post may be a bit too simplistic. But it may help explain what exactly IO is there for by first explaining some of the other monads.
I may be the \_real\_ odd man out here, because I have no problem with defining "logging" as "output the program doesn't need for meaning" and \_not\_ putting it in \`IO\`, wanting \_everything else\_ to be referentially transparent, but also wanting to acknowledge that if you try to mix-and-match \`IO\` and non-referentially-transparent code, not only will you not get the benefits of referential transparency, you'll get enough pain that doing \`IO\` "part way" \_isn't worth it\_. Basically, as far as I'm concerned, it's an all-or-nothing proposition. That means I start with http4s, Doobie, Circe..., my entire service is a \`StreamApp\`, and because it's Scala, I have to pay attention to what needs to be in \`IO\` or some other monad, when to use \`Applicative\`s instead, etc. Then I have to worry about how to compose monads. The results are worth it, but unfortunately, mix-and-match being painful and not offering many benefits, if any, tends to get blamed on FP rather than mix-and-match.
The condensed, and therefore obviously inadequate, answer is: Because `IO` is a monad, in other words, a type of value we can manipulate algebraically. Because types define legal operations on values, then, the compiler can make guarantees about the behavior of whole classes of operations if our code compiles. Because those operations obey certain laws governing composition, composition of these values "makes sense" and our defect rates go down. There's a lot in there I'm happy to unpack, if you decide to want to ask any questions about it.
&gt;Monads require you to program using an entirely different syntax composed of maps and flatMaps. You're overstating things here a bit. Implicit Function Types are literally a new, different synytax - they don't even exist yet in Scala. flatMap's and map's are just method calls on an object. They're similar to tons of existing libraries like Java 8's streams. They don't require that you use different syntax like for-comprehensions, async/await, etc. (all of which are used in other languages like Haskell, C# and Javascript).
&gt;Why do you care about a strong type system? ...I see where this is going :) &gt;Strong types actually make coding easier since everything is explicit and you don’t have to guess what a variable’s type is. And **referential transparency** makes coding easier since everything is explicit and you don't have to guess what a **function's behavior** is.
With most monads you'd be right, but with `IO` being a "sin bin" type it's not as clear-cut. You already mentioned allowing logging to happen outside `IO`; one can also e.g. have a program which reads from an essentially read-only filesystem, in which case one might usefully use `IO` for network calls but allow local filesystem reads to happen in an uncontrolled, ad-hoc way.
They may have overstated things a bit but you do need some form of laziness in your async operation in order to batch those operations up and achieve higher throughput. IO monads can provide that laziness. But I guess your async operation could - instead of eagerly evaluating - store a description of its operation to some queue.
I think nobody is questioning your optimism. I'd say people are rather tired of rate at which Scala's dev process can turn optimism into disappointment though. 
&gt; I think the process did really well here! Of course you do! [_The sacrificial duck kept the meddling manager away from the stuff that was important._](https://rachelbythebay.com/w/2013/06/05/duck/) 
&gt; it's natural for people to want to move on at some point Scala's problem is not the naturally occurring fluctuation, but the additional departures on top of that due to high levels of burnout, loss of hope, depression, harassment. &gt; if a project is healthy it should be able to survive this Well, Scala clearly isn't.
No. Definitely not. The problem is not the implementation, but the fundamental design. The design is irreparably flawed, just as it was on the day it was first proposed. It's no surprise that – given pretty much the same design constraints as the original implementation – they arrived at a rewrite that suffers from practically the same issues.
&gt; Are you aware on any Option value type proposal in the Java community? Well, Java has `Optional`, which is wrong and broken, but won't pose much issue at being turned into a value class. &gt; That could help to foresee a solution here. The solution is rather straight-forward, the question is how to migrate the existing code. 
&gt; If everything points in the same direction you should start facing the music instead of blindly defending a dying language. This. One shouldn't dismiss facts because one dislikes them. &gt; Even its own creators are leaving the ecosystem and talking about the inevitable demise of the language. paulp, the creator of "Scala, the good parts"? :-)
How is the SBT 1.0 support for Scala-on-Android coming along?
Is there something out there that will generate Scala classes from Avro that is just a file on disk? Choosing the name of `sbt-Avro` seems a little bit wide for a functionality that is rather specific. Also I am interested in this because I am actively doing stuff with Scala and Avro outside of the Spark ecosystem.
You said you can "quote numerous issues", but you've mentioned exactly one issue multiple times now, the typeclass proposal (which is still not settled by the way), do you have another example?
Is there any material you could recommend to understand this subject more? What should I google ?
Check out scavro although I don't think it supports DBT 1.x
There is http://github.com/data61/fp-course where during the introductory section, we do exercises to the extent that you'd be well-equipped to debunk the original claims. The problem, however, is that this material is run with instructor(s). It will be next run in Canberra, next week: https://qfpl.io/posts/2018-canberra-intro-to-fp/ The only other thing you could do is seek out people who know this subject well, and engage in those forums. I can point you to them. IMO, Scala is now in a bubble, because almost all the skilled programmers and teachers have left, for various reasons.
&gt;First, the type of a function tells you whether it is referentially transparent or has side-effects when run. `CanIO =&gt; T` can't actually accomplish this though. For example, with `IO`, it's impossible to define a function of type `(A =&gt; IO[B]) =&gt; IO[A =&gt; B]`. Consider why this is. An `IO[A =&gt; B]` is an action which (when run) does some IO, and returns a (pure) function of type `A =&gt; B` (i.e. one which doesn't do IO). But if you only have an `A =&gt; IO[B]`, this provides no way to produce a B from an A without doing IO. You can go the other way however -- i.e. you can define `IO[A =&gt; B] =&gt; A =&gt; IO[B]` -- and so `IO[A =&gt; B]` is a stronger type than `A =&gt; IO[B]`. But if you define `type IO[A] = CanIO =&gt; A`, then these two types are equivalent, as we can certainly define `(A =&gt; CanIO =&gt; B) =&gt; CanIO =&gt; B =&gt; A`, in addition to the inverse. Another example: suppose we have: def foo(x: =&gt; Int, y: =&gt; Int) = x + y In the body of `foo`, since neither `x` nor `y` has type `CanIO =&gt; T`, we might expect that the order we evaluate them in doesn't matter. Yet what if we then write: def bar(c: CanIO) = foo(doSomeIO(c), doSomeMoreIO(c)) What are the semantics of this? Does the order which `x` and `y` are evaluated in affect the order of effects? **tl;dr** modeling `IO[A]` as `CanIO =&gt; A` simply doesn't make sense, as the latter has properties which the former should not.
I'm working on a chrome extension created with Scala.js that gets rid of those annoyingly long preambles found on a lot of recipe websites. If anyone would want to try it out, let me know. I'd be happy to send a crx file.
I'm one of those crazy functional programming zealots and the architect of [ZIO](http://github.com/scalaz/scalaz-zio), a purely-functional effect system for Scala. I use the purely-functional `IO` in Scala to model _all_ my effects&amp;mdash;not out of any ideological commitment to functional programming, but because it makes my life easier and makes my programs faster, clearer and better. In general, we functional programmers prefer to use `IO` in Scala for all of the following reasons: 1. **Uniform Reasoning** 2. **Uniform Purity** 3. **Reified Programs** 4. **Performance &amp; Power** 5. **Flexibility** 6. **Industry Proven** Because of this question, I wrote a [blog post](http://degoes.net/articles/fpoop-vs-fp) to explain each of these benefits. Please check it out and let me know what you think!
&gt;An &gt; &gt;IO\[A =&gt; B\] &gt; &gt; is an action which (when run) does some IO, and returns a (pure) function of type &gt; &gt;A =&gt; B &gt; &gt; (i.e. one which doesn't do IO). But if you only have an &gt; &gt;A =&gt; IO\[B\] &gt; &gt;, this provides no way to produce a B from an A without doing IO. I thank you very much for this. I've been struggling a lot to understand what a F\[A =&gt; B\] actually meant and I think I got it from your explanation. Huge thanks. 
scala has better support exception handling which I don't see people using in my team either. (That's the thing I am not liking about scala world that people use it almost as java). I see people throwing `java.lang.Exception` all over the place. So, My recommendation would be to use `Either[ErrorHappened, ResponseIwant]` or `Future[Either[ErrorHappened, ResponseIwant]]`. And `recoverWith` if needed. If your issue is `scala.concurrent.Future` being started eagerly, define as a function or a `lazy val`. scala&gt; import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.ExecutionContext.Implicits.global scala&gt; lazy val processHttpRequest = Future { 100 } processHttpRequest: scala.concurrent.Future[Int] = &lt;lazy&gt; scala&gt; processHttpRequest res0: scala.concurrent.Future[Int] = Future(Success(100)) 
Something that really irks me with all these posts is they never show the best way to implement something without &lt;the way of doing things&gt; (IO in this case). They just show examples of how nice it is with IO. But that's not the argument they're trying to make, they're trying to say that this is **better** than... _something_? This needs a **comparison** to that anonymous _something_. And the reason for that, I think, is that the benefits IO provides over plain "FP-OOP" Scala are not nearly as significant as the proponents make it out to be, and so they are actually comparing it with imperative Java OOP programming style rather than a good non-IO Scala style. I mean, I can do `def retry[A](fn: () =&gt; Future[A]): Future[A]` I guess? Implementation would be longer, but how many retry functions do you write per year? If you write good "FP-OOP" Scala, the only benefit of `IO` is really "Type-Based Reasoning". And whether you personally want that is debatable. I honestly can't remember the last time my code was doing accidental DB access or network IO, it must have been years. If that happens to you often, perhaps you should look into organizing systems / modules / classes / methods better. I don't even care about IO that much – to each their own – but the author rejects the notion that he likes IO for ideological reasons, and I just want to point out that this post doesn't disprove that, even though I think it was intended to.
"After all, the growing armies of developers using IO to solve everyday problems can’t all be crazy! (Or can we?)" How many people banded together make up an army these days? I guarantee you the "army" of people not tracking effects in their type systems is much much larger. Are we crazy? I think not. Look at any well-used and highly performant code on the JVM. Does it use ZIO, IO, coproducts and free monads, or anything else similar to get things done? No, it doesn't, and here are some examples: * Apache Kafka * Apache Spark * any performant math or stats library on the JVM * any performant web framework or library on the JVM (see techempower benchmarks or any other benchmark) * any performant database driver on the JVM (see for instance http://lucidsoftware.github.io/relate/) At the end of the day, when I see these paradigms pushed by the Scala FP community I am entirely unconvinced of their usefulness. In my day-to-day I never wish effects were encoded in my type system. It looks like it takes far too much effort for far too little gain, and I have other more important things to worry about. 
I think the message here is somewhat derailed by the `ZIO` sales pitch. Functions with Side-effects are not functions. They're something different. It doesn't really matter what you call them, procedures, routines, methods, thunks, callables, or programs. The important thing they are not functions. Scala(and I should also mention Haskell, because comparisons to Haskell derailed the last discussion about this) does not make the distinction in the language. `IO` is that tool you use to compose programs like you do functions. Is that distinction worth always making? No, just like with any type, not always worth being more specific, but most of the time it is. If you function only returns uppercase Strings, should you go out of your way to create a `UppercaseString` type? The fact that we use functions to model `IO` doesn't mean they're still the same thing. Just like the fact that we would use an array of characters to model our `UppercaseString` makes them the same thing. A practical example is logging. Logging effects are not functions. However that doesn't mean they're a side effect. You can log a value and still be referentially transparent. You can log a value end up not being referentially transparent. Should you use IO on that logging effect? It really depends if your usage ends up being in the former or later category. In the standard library, under this philosophy I think `Future` and `Try` are still `IO` types. Even if I think they're bad at what they attempt to do. `IO` is not about eager or lazy evaluation. It's not about what concurrency or threads or execution contexts. Those are just details about what kinds of non-function programs they emphasize. `IO` is about representing non-function programs as values. I you walk away recognizing anything from this post, I would implore is be this. I think both /u/jdegoes's and /u/odersky's post on the subject touch on this, but don't emphasize this point as much as it should.
Scala Wars? Really? What a joke.
All memory allocation is side-effecting, and the side effect is visible for instance when you start seeing more GC activity or an OOM. Should we use a type to specify whether a function allocates memory or not? Technically all function calls are side-effecting. This is visible for instance when your program stack-overflows. Math is side-effecting potentially because if you divide by zero your program crashes. Should we use a type to specify whether a function divides by a number that we can't prove to be non-zero? At some point we draw the line as to what we do and do not track via types. To my knowledge Haskellers mostly draw the line when it comes to tracking values via types and leave that to dependently-typed languages (although haskell has a few extensions in this area? https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell) In the world of Scala and the JVM, the burden of proof is on the FP community to convince us that tracking effects in the type system is worth it, and so far I personally haven't seen that it is, at least with the techniques we currently have available.
I don't know, why?
Yeah, but there are more constructive places to bash it
&gt; All memory allocation is side-effecting, and the side effect is visible for instance when you start seeing more GC activity or an OOM error. Should we use a type to specify whether a function allocates memory or not? &gt; Technically all function calls are side-effecting. This is visible for instance when your program stack-overflows. Should we have a type to track whether a function uses a non-constant amount of stack space? So, to answer the question directly no. Those things don't make functions impure. Side effects only make a function impure if you observe that effect from your program. &gt; In the world of Scala and the JVM, the burden of proof is on the FP community to convince us that tracking effects in the type system is worth it, and so far I personally haven't seen that it is, at least with the techniques we currently have available. To me tracking effects means things like Odersky's implicit function proposal, or free monads. Where different kinds of effects are described, and tracked. That's different from what `IO` does. With IO, you are only differentiating functions from non-functions, with the assumption that everything that is a non-function is effectful. 
&gt; Also keep in mind that all of this advocacy comes from the Haskell community. Clojure and other Lisp developers aren't trying to track effects via any type system, be it runtime or compile-time. The ML communities don't bother either. There are more ways to consider yourself a functional programmer than one, and at the end of the day the Haskellers are a small minority. I think this last bit is needless divisive, and resorts to otherism and on top of that is factually wrong.
He made that point (partially) in his talk
It takes a few tries, every time I try I get a little deeper in ;)
Armies, wars, exactly what the world (of Scala) needs now? I really think the way some people advocate FP is the biggest reason for newcomers running away from Scala.
I don't understand why don't these FP people just show the applications they developed using FP instead of explaining what is FP every day. I want to play with FP but all the things I need to use (e.g. Apache POI or DocX) are OOP and I don't see how FP actually make development quicker. From all the Youtube videos on FP that I see are just FP advocates who tells you FP solves everything but produces nothing except SUPER ABSTRACTION. Even for Haskell, so far, the only wonderful thing that comes out of Haskell is Pandoc. Is it easy to port Haskell to Scala FP? And How? Scala FP people are also split: Why do we need FS2 and Monix and being recommended to know both? Why do we need both Cats and Scalaz (and when will the Scalaz 8 come?). If FP is so great, why is it SO UNPRODUCTIVE?
&gt; Being able to entertain OOP and FP styles simultaneously. Somehow this makes me chuckle :P Indeed, some of both side of these camp are entertained, but some are also not so entertained. I am just amused about this myself :D
Thanks!
Yep, you shouldn't dismiss the facts, that's what I'm saying.' Take a look at those videos and see them for yourself. They **are** talking about what I said.
Yep, I think we are in perfect agreement.
&gt; show the applications What exactly do you mean by that? I've written lots of industrial code in FP over the past couple of decades: * Xen toolstack that ran Amazon AWS * Stock exchange * Tradability analysis * Trading screens * Life insurance calculations * Business rules engine for the insurance industry * Business analytics * Scientific visualization * Market research AI * ... I can tell you broadly about the projects but obviously I cannot show you the code. 
Yet you still dismiss the facts \*\*and\*\* the things scala's \*\*own creators\*\* say. You are hilarious.
This is an excellent observation! I did not mention one crucial detail in the IFT/effect story: namely that effect capabilities cannot be captured in lambdas. We need to change the type system to enforce this. I like to say that effects are "perishable capabilties" because of this aspect. If effects cannot be captured in lambdas then I think `flip` is not definable. Indeed, let's try the obvious implementation: def flip = (f: A =&gt; CanIO =&gt; B) =&gt; (c: CanIO) =&gt; (a: A) =&gt; f(a)(c) This is illegal, as the final lambda `(a: A) =&gt; f(a)(c)` captures `c`.
This is a great comment, just one thing: &gt; If you function only returns uppercase Strings, should you go out of your way to create a UppercaseString type? In a type-system that cannot really distinguish types by their content fully, such a type really isn't all that beneficial, true. But then there's dependent types, which enable you to state arbitrarily complex things about your types, and where you really can have types that allow you to statically prove all values of `UppercaseString` are really uppercase strings (whether that included only alphabetical characters or symbols, you could make that distinction too). And this isn't done through run-time checks in constructors or a shield of abstract methods for mutating state, it's all in the types.
I fully agree about the aspect that IO is a practical solution for asynchronous programming, and that scala-async, coroutines, or full continuations (if Scala had them) are possible alternatives. I guess my point is that we should leave it at that. Don't conflate asynchronricity with other effects in one monad. I have the impression that a lot of applications are moving in that direction already.
Actually, that's what tends to put me off Haskell. 
&gt; Math is side-effecting potentially because if you divide by zero your program crashes. That's really not how it works. Math is just math, saying that it is or is not side-effecting is a category error. The point of referential transparency is to enable programs to be expressed in a way where it is possible to more easily reason about them mathematically. You *could* model say the C language (in some execution/hardware context), but such a model would be much, much more difficult to get any meaningful information out of, because it says so little about the things you care about the most as a programmer. A mathematical function written using very expressive types is way easier to think about than a sequence of weakly-related state-mutations that you hope most of the time have the trajectories that you really want.
In his latest keynote he bemoans a (perceived) exodus of good scala devs from the community and now he cites "growing armies of devs using IO" as evidence that his lib is great. Sure, whatever dude. Always bending words (or outright lying) to fit his narrative. Nothing new to see here... 
Given the subject of the blog post I suppose they mean industrial code written in *pure* FP Scala.
&gt; Don't conflate asynchronricity with other effects in one monad. I have the impression that a lot of applications are moving in that direction already. One generally does as much work as possible in contexts with restricted effects, sure - something like http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html goes into more detail. But ultimately one does have to write programs that will have multiple effects going on, and compose those out of smaller program fragments that might also have multiple effects, so we do need representations for functions that involve both async and I/O (for example). Furthermore the details of the interleaving of those effects are nonobvious and important - possibly the hardest part of async programming is figuring out how I/O, errors/exceptions, and the like interact with async aspects. Current `IO` monads are more cumbersome than anyone would like, but they offer something that no other approach to programming has ever been able to: clear, reasonable, refactor-friendly semantics for how async and I/O operations interact with each other. If the proposal is to keep using monads for async but use this novel effect system for other effects, how are developers going to be able to understand and reason about the interaction between the two?
&gt; How would/could you enforce this though? Some kind of second-class values (and presumably corresponding second-class types)? That would be a radical change to a language where all values are assumed to be copyable, storable etc. Yes, it will come down to that, basically. &gt; But effect composition should require a visible choice of which way around the effects compose, because effects don't commute. But a lot of the most common effects do commute! My proposal is to "keep your monadic powder dry" to express effects that don't commute (like the non-determinism you mention). These are what I lumped together as "control effects". I see the argument that one wants explicit syntax for sequencing effectful operations, but in the end that's a matter of taste, so it's hard to discuss and even harder to agree. I personally find the resulting monadic syntax a little bit ... overwhelming, and I also object to the fact that I then I have to restructure my whole program if I want to add a logging statement somewhere. 
Thank you for your answer. I'm not sure I would agree here - though, what do you exactly mean by "asynchronous effects"? I think [m50d's comment](https://www.reddit.com/r/scala/comments/8ygjcq/can_someone_explain_to_me_the_benefits_of_io/e2fcjf3) captures the problem well. The monadic syntax/for-comprehension is far from ideal, but it's a uniform approach to dealing with effects. As an example, let's suppose we have a client who wants to send an email. Two implementations of this are available: either using the fully synchronous `javax.mail` API, or using Amazon's SES through a fully asynchronous HTTP API. Should the client care which implementation is used? The way I would implement this today, is to expose both email clients with a `sendEmail(e: Email): IO[Unit]` interface. The `javax.mail` implementation would wrap the synchronous networking code in an `IO`, using a dedicated thread pool instance for blocking operations. The Amazon one would just operate on the level of composing `IO`s or `Future`s. As a bonus, as you mentioned, the signature of `sendEmail` clearly specifies that side effects are involved. But that could be of course achieved using `CanIO` as well. So maybe `IO` should be used for all I/O operations (as the name suggests :) ), not only asynchronous ones? Not sure if it's possible - probably there's some research on the topic that I'm not aware of - but maybe it would be possible to make `IO`-style programming easier in Scala. Maybe in some way using IFT to seamlessly convert between a wrapped and unwrapped representations. Finally, if you would have a link to Jonathan's paper, it would be great to read it.
&gt; I see the argument that one wants explicit syntax for sequencing effectful operations, but in the end that's a matter of taste, so it's hard to discuss and even harder to agree. I personally find the resulting monadic syntax a little bit ... overwhelming, and I also object to the fact that I then I have to restructure my whole program if I want to add a logging statement somewhere. There's of course a balance to strike, and as you write, to a large degree it's a matter of the personal or team preferences, or even better - depends on the particular problem being solved and how much effect control it requires. But it's great that Scala is flexible here as well: allows you to capture as much or as little effects as you want. I wouldn't in most cases trace the usage of logging and metrics. That would just create noise. But, I would often trace the usage of db or filesystem access. 
If you do that then functions that take `CanIO` are not remotely first-class and will not be practical to work with - again this feels like Java checked exceptions all over again. For example it becomes impossible to `map` over a list with a function that performs I/O, or `cata` down a matryoshka tree. Monadic I/O also disallows this but it's mitigated because we have a library of reusable functions - in this case `traverse` and `cataM` respectively - that operate on any monad (including the "identity monad" if necessary) and that we're already using to work with other effects. I just don't see switching to a novel representation that throws away all our existing libraries and techniques being worth the cost, and as such I don't see the community/ecosystem moving. The absolute best case is that someone will figure out a reusable representation that abstracts over the two (e.g. perhaps functions that use these capabilities can be represented as `Arrow`s) - but clearly any code written using such a representation will not be able to do anything that can't be implemented for monads. Using continuations as an implementation technique for monads might be worthwhile (Kotlin's arrow library does this; I find direct use of continuations utterly incomprehensible but I can accept that that might be a worthwhile tradeoff for low-level code that's used extensively across the community). Some kind of optimised representation for monads that don't do much with control flow (which is, as you say, the vast majority of them) would be great - there's no reason such monads should have to pay the costs of trampolines. Integrating with coming JVM work on continuations has the potential to make managed-effect code more efficient than what's possible in today's Scala by an order of magnitude or more. But if it means breaking compatibility with the current monad representation then it's a non-starter IMO. Since you've said these capabilities won't subsume all use cases for monads, we would need three representations for our operations (pure, monadic, and capability), when developers already find having monadic and non-monadic versions of `map` etc. confusing enough. And if the only way to avoid the performance issues of current scala monads is to rewrite all my code and change all the libraries I'm using to things built on some different abstraction, then frankly it would be less effort to port all my code to Haskell where at least I can keep using monads and the library functions I'm used to.
&gt; But a lot of the most common effects do commute! My proposal is to "keep your monadic powder dry" to express effects that don't commute (like the non-determinism you mention). These are what I lumped together as "control effects". I don't think that's true? The only effect I can think of that commutes with all the others is `Reader`, and even that's only sort of true if we have things like regions. Even the very basics like `Option` and logging don't commute: `someFunctionThatReturnsOption() map someFunctionThatLogs()` is a very different program from `someFunctionThatLogs() map someFunctionThatReturnsOption()`. A non-monadic representation for effects that commute with control flow might make sense, but hardly anything commutes with control flow - indeed not commuting with control flow is almost the definition of an effect. &gt; I also object to the fact that I then I have to restructure my whole program if I want to add a logging statement somewhere. If your functions are generally effectful and you're being agnostic about which order effects are composed in then you work with MTL style typeclasses and it's just a case of adding another constraint to all your function signatures going down to the one where you want to log: def myFunction[F[_]: MonadFoo : MonadBar] = ... becomes def myFunction[F[_]: MonadFoo : MonadBar : MonadLog] = ... which seems like exactly the same amount of restructuring you'd do under the model you're proposing (adding an implicit `CanIO` parameter to every function on the way down). If you're changing a function that was previously pure to one that now has an effect that you care about the sequencing of, then that *should* lead you to carefully think about whether the sequencing of evaluation is what you want. E.g. if you `map`ed over a collection with a function that was previously pure but now performs database transactions, you probably do want to think about whether you want those transactions to happen one after another or in parallel, and the compiler should flag this up to you. If you don't care about what order your logging happens in, then don't represent it as a monad. But in that case what do you gain from representing it as some kind of special capability-based thing rather than as an ordinary function? It seems to me that there's an excluded middle: either you want to control the sequencing of it or you don't.
What I have been working on, is not exactly a new web framework, but instead a Generalized Abstract Data Type Algebra to describe CRUD applications. I finally have a proof of concept where once the CRUD-GADT is defined, it is interpreted into a running REST service expecting JSON that is validated and then saved to Postgres. Another interpreter can create Swagger compliant JSON documentation and finally another interpreter that spits out the DB Schema. Each of these interpreters are incomplete, but the POC is there. I call this project Bones: [https://github.com/OleTraveler/bones](https://github.com/OleTraveler/bones) It is similar to [https://github.com/nuttycom/xenomorph](https://github.com/nuttycom/xenomorph) .
&gt; All memory allocation is side-effecting, and the side effect is visible for instance when you start seeing more GC activity or an OOM error. Should we use a type to specify whether a function allocates memory or not, or even how much memory the function allocates and whether and how that is a factor of its parameters? Purity is defined in terms of equivalence. If `f` returns the same result as `g` but uses a bit more memory, are they equivalent? Yes for many purposes, no for others. If we want to be very careful, we should distinguish between different kinds of equivalences; `f` and `g` have a particular kind of equivalence and we can use this to reason about the consequences of refactoring a call to `f` to call `g` instead, while still acknowledging that they are not the identical. In practice, a single language cannot be all things to all people. The language and community can only really work with a single shared notion of equivalence. I'd argue that it's not worth keeping track of the details of memory allocation because the overwhelming majority of the time, for the overwhelming majority of functions in the overwhelming majority of programs, no-one cares about allocation when it happens. But that's a subjective, pragmatic judgement, and conceivably some programmers working in some specialised areas could disagree, in which case those programmers would be best served by a language that did track this distinction. &gt; Math is side-effecting potentially because if you divide by zero your program crashes. Should we use a type to specify whether a function divides by a number that we can't prove to be non-zero? Honestly, yes. The ratio of production issues caused by dividing by zero : division operations in codebase is pretty high in my experience. Certainly a lot higher than the ratio of production issues caused by memory : memory allocations in codebase. &gt; In the world of Scala and the JVM, the burden of proof is on the FP community to convince us that tracking effects in the type system is worth it, and so far I personally haven't seen that it is, at least with the techniques we currently have available. All I can offer is my own experience. I started using `Option` instead of `null` because I kept seeing NPEs causing issues in programs. I started using `Either` instead of exceptions because I kept seeing production issues caused by incorrect exception handling. I started using a monad to track database transactions because I kept seeing production issues caused by incorrect transaction boundaries. I started using region/resource monads to enforce that files were closed whenever they were opened because I saw production issues due to running out of file handles. I started using a confidential data monad because... well, you get the idea. In terms of something like logging, I would argue: any effect you care enough about to deliberately write in your program is one that you should be tracking, because if you don't care about that effect then why write it at all? I'm not a fan of ad-hoc logging at all, but for cases where it's important to have a particular kind of trace for particular operations I do use a treelog-style construct, which lets the logging operations be a first-class value that I can test, refactor etc. in a normal. The way I see it, to the extent it's worth doing at all (and a lot of the time I do think people log too much without thinking and could stand to cut back and think about what they actually need), it's worth doing as proper, first-class code.
Which programming language has a total division operator? Haskell and Scala do not, so I'm curious. Can it be done without full dependent types? How practical is it to track relations between integers in a codebase? A related question is typesafe linear algebra (i.e. compile-time sized matrices). I was not able to come up with a satisfactory formulation for my use case (polytope transformations).
Quick answer, you don't need to go full FP. There is a trade-off between how much abstraction your code can handle, and how many bugs you can let slip. I just wrote a symbolic processing framework using advanced typesystem features; they reduce considerably the code readability for a newcomer, but compared to my previous projects, I had to debug very few bugs, and those were caught quickly using e.g. property testing of algebraic laws. The bugs were all in the parts programmed in imperative style for speed (mutation, primitive arrays). For this program, I don't see any point of using a `IO` monad, cats/scalaz (apart from typelevel/algebra and spire), as input/output is done in a single file.
&gt; There are more ways to consider yourself a functional programmer than one Part of the problem is that "functional programming" means different things to different communities. 
&gt; Which programming language has a total division operator? Haskell and Scala do not, so I'm curious. Idris does for naturals. Comments suggest it's taken from Agda. &gt; Can it be done without full dependent types? How practical is it to track relations between integers in a codebase? I don't think dependent types make anything possible that's not possible without them, since you can always cart the type-level evidence around by hand, but yeah a lot of things are practical with them that aren't without them. Speculatively, I wouldn't try keep track of the relations between integers in the general case, but I think a distinct type for known-nonzero might be practical (by analogy with lists: I don't use fixed-sized lists in general but I do use a `NonEmptyList` type). You could always call a check function that returns `Option[NonZero]` at runtime - indeed I'd expect that would be the most common way to use it - it would just mean forcing people to explicitly handle the zero case rather than surprising them with a runtime exception when the divisor turns out to be zero. &gt; A related question is typesafe linear algebra (i.e. compile-time sized matrices). I was not able to come up with a satisfactory formulation for my use case (polytope transformations). Yeah. That kind of thing is harder than it looks. I don't know how much difference something like Idris makes, but the last time I tried to do typed linear algebra in Scala I gave up. I think the difference is that matrices have a lot more algebraic structure than typical business objects. To be able to work nicely with them, you'd need to be able to lift those algebraic identities to type-level identities (or alternatively explain the proofs of those algebraic identities to the type system), and the tooling and techniques for doing that just aren't there yet. Whereas with a custom type the only algebraic structure tends to be what you've given it yourself, and so you will always have a constructive proof of any identities you want to use (you'll never want to use a theorem from a textbook that predates the Scala language, which is pretty common in linear algebra).
Well by `IO` he means `ZIO` which is new, so it has nowhere to go but grow. Also the exodus is of good contributors, not just any dev.
If you search scalaz-effect on maven central you'll find releases [here](https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.scalaz%22%20scalaz-effect).
How about [a dozen or more recent examples](https://twitter.com/search?q=jdegoes%20from%3Aadriaanm&amp;src=typd), including choice nuggets like this one: &gt; To influence Scala, you have to be willing to play ball and respect our core rules (e.g., OO + FP). Or how about [the time Odersky told Kris Nuttycombe](https://twitter.com/odersky/status/382631339188428800) that it was "irrelevant" how much work it was to encode functional programming abstractions in Scala? &gt; @nuttycom Irrelevant. This is not code I am particularly interested in making simple to write. There are plenty of other examples showing that "Team Fusion" is ideologically opposed to doing pure FP in Scala. I don't have a problem with this stance. I obviously don't think it's good for Scala—as I hope I made clear in my talk—but it does strongly deter me from being more involved with language development, because I know that anything which is not aligned with the "FP + OOP" agenda is not likely to be welcomed. Better to put my hours into library development, since that's where I'll have the largest impact on the future of Scala.
Wat? Where?
I call your bluff.
&gt; For example it becomes impossible to map over a list with a function that performs I/O. That would indeed be very bad. But it's not going to be a problem - map will be effect polymorphic. I realize that I am promising without backup of an implementation or a full paper here, so I completely understand you are skeptical. Let us do the work first and then judge it.
&gt; As an example, let's suppose we have a client who wants to send an email. Two implementations of this are available: either using the fully synchronous javax.mail API, or using Amazon's SES through a fully asynchronous HTTP API. Should the client care which implementation is used? Sure, if you foresee using an asychronous implementation then it's best to plan for this by using IO. I guess it depends on your environment whether most I/O is potentially asynchronous or not. In the first case, it does make sense to use IO for all I/O.
The effects I want to address are those things that a classical runtime model can express, yet that when invoked from a function render the function not referentially transparent. E.g. reading, writing, throwing exceptions, I/O, possibly non-termination. The things that a language like F* would classify as effects. In that model, I don't care about Option and List as effects, precisely because they can be created and decomposed in referentially transparent functions. I don't have to make my whole program an `Option` transform if I want to use Option, whereas I do have to make my whole program an IO monad if I want to do I/O. And, if I talk about these "boring" effects, then order does not matter. If I say a function mutates global state and throws an exception that's just as good as saying the function throws an exception and mutates global state. And if I say it mutates global state and then it mutates global state again, that's just as good as if I say it once. In other words my "boring effects" form a set. And I am after a model that exploits that structure. 
What bluff?
The biggest codebase that biggest pure FP advocates worked on that I know and can access is this: https://github.com/slamdata/quasar But I've never done any big research into how pure it actually is.
A good example I would like to add to help differentiate non-functions from side effects it a non-terminating loop. It doesn't do anything, it has no side-effects. However because it it never terminates it is not a function. With IO, you could take a non-terminating program, and compose with other program. You couldn't do that with just plain functions, you'll end up with another non-terminating expression, regardless of what evaluation model you use.
Maybe there's a good beyond-monad model out there to be found; that's certainly a worthwhile research area. But the nature of research is that negative results are possible. It sounds like you're already committed to including this in future Scala - and not just as a library available for those who want it, but as part of the language that would impact every user. And while of course that's your right, as someone who uses and loves (and, not to put too fine a point on it, makes a living from) Scala as a working, industrial language, I'd really not see changes that risk breaking the aspects that make it great.
&gt; And, if I talk about these "boring" effects, then order does not matter. If I say a function mutates global state and throws an exception that's just as good as saying the function throws an exception and mutates global state. And if I say it mutates global state and then it mutates global state again, that's just as good as if I say it once. But that ceases to be true as soon as non-boring effects exist? Once you have async going on, then you do care whether the global state gets mutated once or twice, and whether a state-mutation comes before or after an exception.
there's also a [series of youtube videos](https://www.youtube.com/playlist?list=PLly9WMAVMrayYo2c-1E_rIRwBXG_FbLBW) by Brian McKenna teaching this course
FWIW simon isn't accusing *you* of dismissing facts, he's accusing Scala fans.
You said: &gt; I can quote numerous issues in which suggestions are rejected on grounds they "too FP" With "issues" linking specifically to the Dotty issue tracker, yet you're citing twitter conversations. In any case, none of the conversations you cite are about specific suggestions being rejected for being "too FP". Adriaan isn't saying that every language feature needs to be OO+FP, just that the language itself is OO+FP. Martin's comment (from 5 years ago!) just says the he isn't personally interested in something. Even here, it's worth noting that this comment thread was about higher-kinded types in Scala 3, and since then, Martin has investigated an enormous amount of time into making higher-kinded types work well and be sound for Scala 3, I can attest to that because I was there and I helped. &gt; it does strongly deter me from being more involved with language development, because I know that anything which is not aligned with the "FP + OOP" agenda is not likely to be welcomed. I recently came up with a proposal for [polymorphic function types](https://github.com/lampepfl/dotty/pull/4672) which you'd probably agree is very much an "FP" concept, and yet Martin is happy with it and has no intention of rejecting it until we make it "more OO" or something like that. We'd love to hear your ideas! &gt; Better to put my hours into library development, since that's where I'll have the largest impact on the future of Scala. I'm afraid that right now your largest impact might be turning off people from Scala, e.g. https://twitter.com/herval/status/1017415127715332097, https://www.reddit.com/r/scala/comments/8ylnph/what_does_scala_offer_that_separates_it_from/e2c36nt/, etc. I think this could have been avoided by making your talk less adversarial. Being "brutally honest" is great, but it doesn't preclude being empathic, for example by not attributing negative ulterior motives to others like "they're just implementing this feature to write a paper, they're not interested in making Scala better" (how would you feel if someone said, "John is just giving this talk because he likes publicity, he's not interested in making Scala better" ?). Finally, I don't think waging war on part of the community is helpful, maybe it's a cultural thing: why are Americans always so eager to declare absurd wars on things ? ;) Wars tend to be bad for everyone involved, collaboration works better and is so much more fun!
That's a great idea!
Anyone can say anything is incorrect, if they don't have to say why
Another large Scala codebase is Ermine. Then there's also [Verizon](https://www.youtube.com/watch?v=oAu0MIe072M)
Scala and FP have made me a better programmer. This is exactly why i have a bit of a bias, and cheesy soft spot for Scala. I also have a full time job coding in Scala. Unfortunately, at the same time all this division/polarization/fragmentation between pure FP and OO-FP is absolutely fucking killing this Scala ecosystem. TBH, pure FP and general complexity in libraries over pragmatism has been pushing me away from Scala for some time now. While there are valid points in this article and in the "infinity war Scala" presentation, it's also exacerbating the division between writing OO-FP vs Pure FP. And there is a sizable negative atmosphere generated for newcomers. Lastly, I'm inclined to now believe that the hybrid model of OO-FP as the basis for the Scala language is incompatible with the purist/absolutist mindset of the vocal, leading external contributors of this ecosystem. 
I did say why, hundreds of times, 15 years ago. You'll never believe what happens next.
There are lots of other examples of things [like](https://github.com/lampepfl/dotty/pull/4153#issuecomment-377939928): &gt; I believe our goals are simply too different to be able to join forces. scalaz tries to import Haskell's typeclasses into Scala, effectively turning Scala into a Haskell-like language with a less convenient syntax. This isn't a new thing—the FP in Scala community has dealt with it, repeatedly, year after year, since the birth of Scalaz. I don't have the energy to deal with all this, and it's really not my problem to solve. I do what I can. The rest is up to others. &gt; I recently came up with a proposal for polymorphic function types which you'd probably agree is very much an "FP" concept, and yet Martin is happy with it and has no intention of rejecting it until we make it "more OO" or something like that. You're a hero in my book. &gt; Being "brutally honest" is great, but it doesn't preclude being empathic, for example by not attributing negative ulterior motives to others like "they're just implementing this feature to write a paper, they're not interested in making Scala better" I never said that or most of the other things attributed to me. As I hope you know from watching the talk. :) &gt; How would you feel if someone said, "John is just giving this talk because he likes publicity, he's not interested in making Scala better" I feel nothing. I'm attacked all the time. It just goes with the territory of doing anything remotely interesting. Take it as a good sign. :) &gt; Wars tend to be bad for everyone involved, collaboration works better and is so much more fun! I'm all for collaboration, but to quote Odersky on the issue, *"I believe our goals are simply too different to be able to join forces."* Are they too different to join forces? Should I abandon my efforts to train 300 new contributors to the Scala ecosystem? Should I walk away from Scala forever?
How is it factually wrong? - Clojure and Lisp community doesn't have any prominent effect tracking system - ML community has mainly OCaml's Lwt and Async which are used to manage concurrency monadically; they are not meant for generalised effect tracking - Haskell community is a small minority of the entire FP community Re: 'divisive', we are programmers, we tackle issues by breaking them into smaller portions. Re: 'otherism', can you explain how it applies?
&gt; Math is side-effecting potentially because if you divide by zero your program crashes. Not in a lot of languages; if you divide by zero they give you the value 'infinity', which obeys the mathematical rules of operations that work on infinity.
He does mention that devs in lots of programming languages are using IO types, and he even gives examples of these languages. That is perfectly consistent with contributors leaving the Scala ecosystem. Please don't accuse people of lying until you understand all the facts, it's very disrespectful.
Previous parts: * [Implicits, type classes, and extension methods, part 1: with type classes in mind](https://kubuszok.com/2018/implicits-type-classes-and-extension-methods-part-1/) * [Implicits, type classes, and extension methods, part 2: implicit derivation](https://kubuszok.com/2018/implicits-type-classes-and-extension-methods-part-2/)
ZIO is, without the slightest doubt, a truly impressive project. It's something that can and should be used to build solid, performant functional libraries and apps in Scala. At the same time, it doesn't preclude people from using OOP techniques like traits, classes, SOLID etc. for structuring their programs. I believe Scala's philosophy is 'OOP in structure, FP in operations'. If it helps, think of classes and instances as functors and modules. The fact that we can combine different styles gives us way more flexibility. It's a matter of perspective.
same exact boat. Very hard to chose between ignorance of OOP crowd and mountains of BS drama from FP crowd. Same people who laugh at "a better java" feel offended when Odersky tells then he did really envisioned scala as second-grade haskell on the jvm
and you show no evidence of that. I don't even know who you are
In the `sendEmail` example, you could use implicit function types with `type IO[T] = implicit CanExec =&gt; Future[T]` and `type CanExec = ExecutionContext`.
When will the results become available?
More to the point, you may have said why to someone else, not to the readers here, so again, anyone could claim that.
&gt; the growing armies of developers across many different communities using monadic effects to solve everyday problems can’t all be crazy! This is the exact quote. In his keynote he's talking about people choosing Kotlin over Scala and stuff like that. Last time I checked almost noone even bothers about pure FP (and IO Monads) in Kotlin, because most people use it as a drop-in replacement for Java (in Spring, Android and other frameworks). The only languages where you can actually reasonably use IO Monads in production are Haskell/Purescript and Scala. If he is referencing Haskell/Purescript in the "growing armies" part, I'd like to see data about that because they are used far far less than Scala in most rankings like Tiobe. But in the end it's fine because it's a sales pitch (for his library and himself) and sales people never tell you the whole story.
&gt; I never said that Fair enough, I misremembered your exact words because of https://www.reddit.com/r/scala/comments/8xreuv/keynote_the_last_hope_for_scalas_infinity_war/e254u8w/, you said that a lot of what we do is done in order to get PhD and doesn't necessarily make the language more useful. I don't think that's true and I don't think you would think that if you talked more with us. In fact, the length to which some PhD students at LAMP go to to make something useful at the expense of any academic consideration is amazing, consider how much effort Sébastien spent on making Scala.js a production-grade compiler, including publishing [48 releases](https://www.scala-js.org/doc/internals/version-history.html), maintaining binary-compatibility for the past 3 years, cross-publishing Scala.js with every possible Scala version (including every milestone pretty much immediately after their release because the ecosystem cannot bootstrap without Scala.js), being active on Gitter, etc. &gt; Are they too different to join forces? Maybe they're too different _on this particular issue_, but hopefully not in general. Anyway, you're still quoting from the same issue even though you said they were "numerous", so I'll keep saying basically the same thing: I believe the outcome of the typeclass proposal will satisfy you: either we get nothing in the language because we can't agree on something, or we get something that is good enough.
That's because I have retired from correcting Scala programmers. Let them drown in a bubble of ignorance for all I care.
Sorry. I was a bit tunnel visioning on the parent post and I misinterpreted your comment.
No problem!
Functional Dependencies or Fundeps(like `CanBuildFrom`) are an implicit pattern worth mentioning.
In my book, that would be going too far. I have plenty of places where I divide by a number that is never going to be non-zero, but proving that fact would be extremely cumbersome. In that case, there is absolutely no interest in handling the failure case, apart from writing `(x/y).getOrElse(sys.error("never happens"))`.
&gt; Lastly, I'm inclined to now believe that the hybrid model of OO-FP as the basis for the Scala language is incompatible with the purist/absolutist mindset of the vocal, leading external contributors of this ecosystem. People like tpolecat are excellent advocates of pure FP, but you don't hear as much from them as they don't stir up drama. Tut, for example, is brilliant, self-contained and uses the IO monad. https://github.com/tpolecat/tut 
I don't care what you care. But you aren't displaying any credibility, so I call your bluff.
And I do not care. I note that the knowledge of this subject on display has progressed by absolutely zero. This will remain constant whether I intervene or I do not. There is no bluff.
https://www.youtube.com/watch?v=LH75sJAR0hc
That's precisely what I've been trying to get you to notice. Saying that you dispute something without saying the reason adds zero knowledge.
You may believe it adds zero knowledge, but if I were to tell you you're wrong, then I'd be correcting a Scala programmer, which is unethical. Perpetual beginners, no matter what it is that I do.
Agree, but Optional would still wrap a pointer, so we don't gain much compared to e.g. an opaque type represented by a reference. Second problem: Optional cannot store null by design, compared to Scala's `Some(null)` (but that's easily solved by an extra "tag" field).
&gt;I have plenty of places where I divide by a number that is never going to be non-zero ಠ_ಠ
&gt; I don't consider impure parts of my code to be scary or dangerous in the first place This is very instructive. A central part of the FP thesis is an understanding that the impurity is one of the most complicated and complicating parts of programming. If you don't share that understanding then clearly FP is not going to be compelling. Most functional programmers I know have been imperative programmers for years, and have studied how systems work and evolve. The bits that are hard, and the source of the vast majority of the problems and bugs, come from the impure parts of the code. If you are a good enough programmer, then you may find that you don't have a problem with these parts. For me, I have never been a good enough programmer, or been fortunate enough to work with good enough programmers, that taming impurity has been easy.
 class Option[+T] extends AnyVal { val isDefined: Boolean val value: T ... } object Option { ...} object Some { ... } object None { ... } No wrapping with JVM value types, two registers to pass. Special hacks for `Option[T]` where `T is NonNull` may apply, could be compiled down to `T @Nullable` at the bytecode (one register to pass).
You aren't by any chance Tony Morris, are you?
Has Tony Morris explained to you over 100 times why this beginner mistake that Martin consistently makes, is wrong?
Yes, though it's also an interesting point. If using `Future`s, running the `sendEmail` method is side effecting, hence the signature `Email =&gt; CanExec =&gt; Future[T]`. If using `IO`s, the sendEmail method wouldn't be side-effecting, so the signature would be `Email =&gt; IO[T]`. Only the `IO.runUnsafe` method would need the `CanExec` capability.
&gt; Sure, if you foresee using an asychronous implementation then it's best to plan for this by using IO. I guess it depends on your environment whether most I/O is potentially asynchronous or not. In the first case, it does make sense to use IO for all I/O. I think at least in the domain of "business" Scala applications, almost always at least some I/O is asynchronous. All major HTTP clients and servers have async I/O, and almost every application does HTTP calls :).
avrohugger
What if he has and what if he hasn't?
I used to agree with you, but after encountering dozens of bugs with division by zero I started introducing a safe division operator in our code base and things have been going much better than I expected. It hasn't really been cumbersome at all and the few cases where it had, have now been fixed thanks to me complaining on twitter :D https://twitter.com/LukaJacobowitz/status/1015207154993844224
If he has, then he'd probably have given up explaining anything to Scala programmers by now. If he hasn't, he then he'd probably be wondering what Scala programmers would do with such an explanation.
TIL. I always considered CanBuildFrom as just a type class riding on the type inference.
&gt; If it helps, think of classes and instances as functors and modules. This is a nice way to think about Scala's OOP System because: 1. Scala's type system was explicitly [*designed*](https://stackoverflow.com/questions/47098538/objects-with-type-members-what-is-scalas-object-vs-module-system-trying-to) to unify objects and modules. 2. Scala programmers sealed traits &amp; case classes to define data models instead of abstract data types and class hierarchies. This already destroys textbook OO examples such as class hierarchies of Shapes and Animals. Instead, classes and traits are relegated to structuring code: ["Functions for logic, OO for modularity"](https://www.slideshare.net/Odersky/preparing-for-scala-3) which is just a nice way to say that Scala employs a module system based vaguely based on Java inheritance. I have to admit, that for Scala FP purists, the problem with OO disappears with change of terminology... Somehow [first-class modules](https://twitter.com/jdegoes/status/1017050063120420865) are good, but objects are BAAAD! Finding any difference between the two is left as an exercise to the read.
I'll like it if it's modular enough that you plug the resolver to another build tool, in order to benefit from what this advocates whilst retaining the goodies of whichever build tools we're most comfortable with. I appreciate that it delegates to bloop for compilation. That's a really good decision. What I'm less a fan of is the "hype-building" without even a partial demo ... 
Same with Cats people...
Which is kind of sad. A LOT of good (pragmatic?) thinking from that side...
Nothing he said is factually wrong, the term functional programming originated from LISP (way before Haskell existed), with functional programming being defined as having functions as first class values that you can pass around (in LISP these functions meant manipulating lists). Out of all of the mainstream languages, Haskell (and now Scala through Scalaz/Cats) are the only ones which actually tracked effects through the typesystem
I finished a school project two days ago. Our instructor jokingly described it as "Photoshop" - briefly, it's a command line image manipulation tool. It includes translatable opaque-to-transparent layers of other images, selections of rectangles on which to apply the operations, and various mathematical operations and functions, including user-definable composite or sequential operations or functions (this is where the "functional" part really paid off). Oh, it also can save or load the current context to a file. It was my first non-trivial Scala project, and I really loved it! Adding a GUI (Swing, I guess) might be a next step, but I'm open to extension suggestions for it.
Supporting code generation =&gt; This just became a build tool. Why? Because people will need to have some sort control over ordering if the code generator depends on code (which must be compiled) inside the project itself. I think the only way to really deal with this is to either: - *not* have the fancy fury init, etc. stuff, but to actually restrict Fury to just be a "publish a source project in a way that makes it easy to consume", very similar to how "sbt bloopInstall" works but at at a 'network' level. Include compiler versions, compiler switches, etc. in the published artifacts and bob's your uncle. * Actually embrace that it's going to have to become a build tool, and go with Applicative as the abstraction for building like e.g. Bazel, Mill, etc. It's really the only sane way to build[1] and if you need anything more complicated than that, then I think it's fine to force people to adapt (or just use a different build system which can publish "Fury artifacts"). [1] At when expect to be doing distributed builds, sane caching of build output for identical code (something which I think might become necessary when everything is build for source), etc.
I had to do this during my last interview wave. I prefer this to a blackboard. Much more realistic to real-life coding. Also, what is the problem with watching you: if they are hiring you, they will have around you all the time, review your code and other stuff. Better know how you are working, that seems a pretty safe and reasonable process.
If you use pure logging, you'll lose your entire event trace if your function dies or enters infinite loop – which is exactly when you need it the most.
There's also a Scala job board on gitter [https://gitter.im/scala/job-board](https://gitter.im/scala/job-board) . Not a lot of traffic but some good employers like Lightbend and others post jobs there 
&gt; I have plenty of places where I divide by a number that is never going to be non-zero, but proving that fact would be extremely cumbersome. This is a notion I'm always sceptical about: surely if you actually know why a number is non-zero then you can explain that to the compiler. I've certainly seen cases where a colleague "knew" a list would always be non-empty and then when we chased it through it turned out it wasn't. Maybe it's different in more mathematics-heavy codebases though.
Yes and no. You're forced to think carefully about your commit points and make your event-processing loop a bit explicit, but that's a good idea anyway. When you want to reproduce a failure case you start from the last commit point which is probably a bit further away than the closest log message, but you have a structured event and know exactly what all the relevant state is, which normally makes it very easy to find the problem.
IMHO there's some overlap between Monix and Fs2 but you don't need both; you can use the one you like best, or you can use parts of either. Personally I think Fs2 is very interesting from a design point of view, but Monix is much more practical it terms of being easy to learn and use.
I would say that J Degoes is the one being divisive. In his previous video "Scala Infinity Wars", he advocated removing all the OOP features from Scala, even if that meant losing half the Scala community...
It is even worse than that... Software functions are never truly pure, because they run on physical hardware (which can have intermittent faults), consume CPU cycles (potentially huge amounts), and may never terminate (see: the Halting Problem). To account for this reality, Haskell pulls a slight-of-hand in their type system, by adding the "bottom" value to all the types in the Hask category...
Thanks
Yesterday I worked on changing case classes so that their toString method displays the field name beside the field value https://github.com/scala/scala/pull/6936. 
With good compiler support, I realize it could actually be done. The benefits would be very small (from my use case): having written 50 kLOCs in Scala, I have yet to encounter a division by zero exception.
Ok! let me have a look. Where were those bugs located?
Let's not make it about the cats/scalaz community split, but about individual contributions.
This is a tutorial that explains how to get up and running with Spark &amp; Zeppelin - I talked about this at Scala Days this year! [https://www.youtube.com/watch?v=3HjjxP0-uFE&amp;t=1s](https://www.youtube.com/watch?v=3HjjxP0-uFE&amp;t=1s)
Not necessarily new, but this is interesting, and implies that the whole scala stdlib is available: http://www.scala-native.org/en/latest/lib/index.html
Unfortunately it's not open code, but a lot of them had to do with situations where we absolutely knew the divisor couldn't be 0, but after some unrelated changes came in, now we had edge cases where it could be 0 and result in a runtime error.
In principle you could do that, you could split your fuctions into steps that return a log and a continuation, and run them in effectful supervisor to output logs eagerly, and you would even get some decoupling in the process. But in practice, this is not only expensive - just adding a single logging concern means rewriting a straightforward function into a 'generator' function - but also greatly hurts readability, especially for novice java converts.
Yep, thanks. But with the current design of value types, `Option` would have to declare for which `T` it would be a value class, no? Are there plans to alleviate that problem?
&gt; Yeah. That kind of thing is harder than it looks. I don't know how much difference something like Idris makes, but the last time I tried to do typed linear algebra in Scala I gave up. &gt; I think the difference is that matrices have a lot more algebraic structure than typical business objects. To be able to work nicely with them, you'd need to be able to lift those algebraic identities to type-level identities (or alternatively explain the proofs of those algebraic identities to the type system), and the tooling and techniques for doing that just aren't there yet. Whereas with a custom type the only algebraic structure tends to be what you've given it yourself, and so you will always have a constructive proof of any identities you want to use (you'll never want to use a theorem from a textbook that predates the Scala language, which is pretty common in linear algebra). I think that projects like [frameless](https://github.com/typelevel/frameless) or [TensorFlow Scala](https://github.com/eaplatanios/tensorflow_scala) do allow you to do some operations on matrices/collections in a safe way via types (safe multiplication of matrices, safe access to indices, etc).
If your system is to be durable and scalable you already need to have some notion of a unit of processing where you can logically retry/"commit"/fail, you just perform logging at that level.
Any new benchmark on how this compares to JVM scala in real world apps?
I never deny the importance of FP in business world. I am referring the applications for personal computer, where I need to prepare report for my boss, process some data with basic statistics. I really don't see the power of FP in that respect. When I need to generate multiple documents, using Python and LaTeX to generate PDF get my job done. Unlike the business environment, I work in an education institute, there is no way for me to introduce FP to students if everything that FP claim to be great are close source and proprietary. Even the Scientific Visualization that you mention, I see D3JS being successful, but not Scala's FP.
I'm writing an Spark version of JRip, as much functional as I can, as part of my end-of-master thesis. It's a pain to read JRip's code in order to recreate it's behaviour, so I think this functional version is easier to read and much more elegant, so maybe it can be useful for didactic purposes.
Not a question, but I look forward to *toIntOption* a new *String* method in 2.13. [link](https://github.com/scala/scala/blob/4d39d4f849cbab5d6ad61d74e07aba41c08c6340/src/library/scala/collection/StringOps.scala#L806)
\&gt; We’re more interested in your approach than having lots of experience in specific frameworks and libraries. If only more employers understood it ! \&gt; You will be developing a Scala / ScalaJS / ReactJS A scala job which is not related to spark, great ! And with scalaJS ! I really wonder how many scalaJS is in production. At work another team created this project in full scalaJS : [https://frsa.nestincloud.io/#/](https://frsa.nestincloud.io/#/) That being said, they didn't used react JS for scalaJS, but monadic HTML : [https://github.com/OlivierBlanvillain/monadic-html](https://github.com/OlivierBlanvillain/monadic-html) Why using react in scalaJS while there is more lightweight way to create webapp in scalaJS ?
Would someone TLDR this for me? It was too slow even for 2x youtube speed but i sense that the speaker is intelligent and has important stuff to say so I'm curious to know what he said.
If we are to work with asynchronous I/O operations in a maintainable way, while still having fine control over the details of where our yield points are, then we need a composable representation of I/O operations that keeps any sequencing explicit. We would also like to be able to use an existing library of higher-order functions that do compositions, e.g. `traverse` or `cataM`. So far monads are the only approach that has achieved this in practice.
Perhaps. The part that I objected read as if IO monad was necessary to achieve asynchronous IO at all. I thought it's too inaccurate to be useful as a motivation for using it.
/u/Odersky 's post shows some common misconceptions about `IO`, which imho end up derailing the whole conversation because we argue about different things. In particular: &gt; First, the type of a function tells you whether it is referentially transparent or has side-effects when run. &gt; The other argument for going to monads and IO is that it makes composition of effectful code more cumbersome than composition of pure code. So some people like this for the educational aspect: Using effects should be painful, so that you are pushed towards referentially transparent solutions instead. &gt; So, in summary, IO is a very good solution for a lazy language like Haskell. For a strict language like Scala, it's currently the most common solution if you insist that the distinction between pure and impure should be reflected in the types of your program. The implication here is that IO is good at separating pure and impure code, and therefore it's a signalling mechanism, which is better done with capabilities. Except, *IO is 100% pure (referentially transparent)*, so let me unpack a few things. Referential transparency: replacing an expression by its bound value doesn't alter the behaviour of your program. This gives huge benefits, which I'll talk about later. Side effects: This is the main issue, because I think we definitely don't share the same language here, and that causes heaps of confusion. In pure FP, a side-effect is a breakage of referential transparency. That's it. Doing i/o, state, concurrency or what have you is not _inhererently_ a side-effect. Traditionally, these things are done in a way that breaks referential transparency, and are therefore side-effects, but they cease to be if you do them in a referentially transparent manner (hence why the term *purely functional IO* is not an oxymoron). Purely functional programming: programming exclusively through referentially transparent expressions (i.e. without side effects, as per the definition above). Value: a referentially transparent expression. That's what _program as values_ means. A value can be put anywhere, passed to functions, stored in data structures, manipulated, replced by its name, without altering program behaviour. That's only true in the presence of referential transparency, by definition. Now, this statement: &gt; First, the type of a function tells you whether it is referentially transparent or has side-effects when run. is wrong because it's a false dichotomy. `IO` is something that can have side effects _when run_, but that doesn't mean that it isn't referentially transparent. `IO` is 100% referentially transparent. This is very easily seen and I've shown it multiple times already, for example: https://youtu.be/x3GLwl1FxcA?t=3m9s https://www.reddit.com/r/haskell/comments/7ykrv3/xkcds_prediction_for_a_haskellrelated_cve_in_2018/dui4eya/ https://www.reddit.com/r/haskell/comments/7ykrv3/xkcds_prediction_for_a_haskellrelated_cve_in_2018/dujbmid/ So, given this definition of side-effects, `IO` is not about signalling side-effects, it's about _avoiding_ them altogether, so this &gt; it's currently the most common solution if you insist that the distinction between pure and impure should be reflected in the types of your program. is very wrong as well, given that `IO` is completely pure. &gt; The other argument for going to monads and IO is that it makes composition of effectful code more cumbersome than composition of pure code. So some people like this for the educational aspect: Using effects should be painful, so that you are pushed towards referentially transparent solutions instead. Same thing here: programming with effects (IO but also State or anything else) _is_ a referentially transparent solution, that's the point, and I also disagree that composing IO-based code is cumbersome. The examples shown are always about `for`, but they neglect the million combinators defined for `IO`, and more importantly the fact that `IO` can be built upon to create abstractions that are much more expressive than imperative-looking code, for example streaming/FRP-like like fs2. Matter of fact, most of the code I write is effectful (like I suspect the majority of server side programmers), and doing this purely (in my case with fs2 and cats-effect) it's not only not cumbersome at all, but in fact extremely convenient and powerful, and it allows me to build complex behaviour at a speed and correctness I could never achieve when writing side-effectful, non functional code. Effects aren't a weak point of Haskell-style FP, they're a strength. (I think /u/Jasper-M and others wanted some details on why the post was considered incorrect). 
Now, to the OP (/u/arbitrarycivilian), you say: &gt; 3) It makes all your code referentially transparent - but that's only because it doesn't do anything. As soon as you actually execute the IO, you lose referential transparency But that's not quite true. Referential transparency (or more correctly, a breakage thereof) is something that _the caller_ of your code observes. So it's true that when you execute `IO` you lose it, but if you use it correctly and execute it at the very last point of your code, there's no code left to observe the breakage, so all of your code enjoys the benefits of it. Haskell takes this the extra mile, running the `IO` is done directly by the runtime system, so there's literally no code to observe the breakage of purity. Note that that I can express _any_ IO computation, in _any_ point of the code (not just the boundaries, that's an architectural choice and not a constraint, and therefore another red herring), without ever having to "run" the IO (calling `unsafeRunSync`). This is what programs in Haskell look like. 
 Now, given that the benefit of IO is referential transparency, and not "signalling side-effects, "tracking effects in the types" (`IO` is in fact very opaque, there are ways to track effects in the type at the desired granularity, but this is a _different_ discussion), or "knowing when things can go wrong", the question is reduced to (like /u/tpolecat correctly said) : &gt; What are the benefits of referential transparency? First, a disclaimer: you're not going to be convinced by a conversation. You would need to write and read a good amount of pure code to appreciate it. This is a general point about language expressiveness, the same reason why in 2006 one couldn't convince Java programmers that first class functions were useful at all. Note that most pure FP programmers have written quite a lot of impure code in their careers, but the reverse isn't true. Anyway, all the benefits of referential transparency boil down to one thing, _local reasoning_. The fact that you can always, in any circumstance, replace an expression by its bound value, means that referentially transparent code is not intrinsically tied to a _context_, whereas if you can't make that substitution without altering behaviour means that you need to take something else into account apart from the expression you are dealing with, _and you can't separate the two_: either evaluation order, or whether something else has happened before, or the interaction with another portion of the code. That means that non referentially transparent code requires non-local reasoning, which eventually degrades into that feeling of having too many things in your head at once. Now let's see some of the nice consequences of referential transparency. ____________ Compositionality when understanding code, i.e. understanding code by understanding its individual parts, and then the global behaviour as the sum of the individual behaviours. To do this you would analyse individual portion of the code, see what they would reduce to given certain inputs, and then _put them back together_. Putting things back together means exactly replacing an expression by its bound value, and if you're not guaranteed that such a substitution doesn't change things, you are forced to understand things globally, which doesn't scale with complexity at all. An example using `iterator.next`, which isn't referentially transparent (imagine `it` is an `Iterator[Int]`) val a = it.next val b = a + 1 a + b Now imagine that `a + b` is a complicated enough subprogram that you aren't able to understand at once, so you would like to understand `a` and `b` in isolation, then put them back together. Well `a` is just `it.next`. `b` is `a + 1` , so `it.next + 1`, so `a + b` is `it.next + (it.next + 1)` &lt;-- wrong I can't really separate the two, and I'm forced to look at the whole block in its entirety, which is _non compositional_. Breaking referential transparency alwasy hinders compositionality when understanding code. Take it far enough, and you have spaghetti code, which is exactly code that you can't separate to its individual parts. -------------- Compositionality when assembling code. A nice way to tackle complexity is by creating small primitives that can then be assembled in a myriad different ways to build behaviour. This isn't possible unless your blocks can be passed around, modified, assigned to things freely without changing behaviour. You could see that in the `Future.retry` example, where you are forced to make the Future lazy to try and recover the advantages of having it as a value (i.e. of it being referentially transparent). The same phenomenon applies everywhere you break the property, and just `thunks` aren't the answer: they are error prone because you need to take care of them staying "lifted" at all times, which goes against a strict evaluation model, and they also can't really represent asynchronous computation. `IO` lets you do both, seamlessly, and with largely the same apis (algebras) you use for Option, State, List and a million other abstractions. Another objection I've seen is "how often do you write retry". My answer is, if you don't write programs that modify and manipulate other programs, it's because lack of referential trasparency doesn't allow you to (see the point about Java 6 and first class functions). Programs that modify programs are the base of compositional apis, and if you look at things like fs2 (disclaimer: I work on it) or Monix, you will realise it's not an accident that they are both based on IO-style abstractions that reify programs as values. You also said that functions are enough to represent programs as values. On a practical level, there are similar objections as to thunks, but I want to offer a broader perspective. Pure FP is really about _computations_, which you operate on using _algebras_. Functions are one kind of computation, with a nice simple algebra: you can apply them, you can compose them. `IO`s are also computations, and we can offer richer algebras, for example one for asynchrony or concurrency. `State` or `Either` or `Probability` are others. Why limit yourself to functions only, when there's so much more? That's even more true when higher-kinds and typeclasses allow us to have a uniform language to talk about all of them through abstractions like Applicative, Monad, Monoid, and so on. The fact that you can apply a series of validations to one input, or send multiple http requests, or collect all the values in a tree using the same abstraction really is a game changing possibility. ---------------- Inversion of control. The way effects are encoded in a referentially transparent manner is by returning a value to the caller (e.g. `IO`, or `State`, or `Writer`). Since nothing has happened yet, the caller is in full control: in the extreme, the caller could simply discard the IO value and _nothing_ would happen. With side-effects, the callee is in control: once you've called a side-effecting function, there's really not much you can do after. Let's see why the former is preferable with an example: test frameworks. The way most test frameworks work is by having you write a side effecting function (an assertion), which is then called by the framework and succeeds unless it throws. But now imagine you want to run an action after your assertion (e.g cleanup resources), or retry your assertion `n` times, or run certain tests sequentially and others in parallel. You can't. After you write the assertion, all you get is Unit + side-effects, nothing you can operate on. The way this is "solved" is by having the author of the framework anticipate these needs, and write custom commands to do all these things. This is non-modular and non compositional. Imagine the framework being another part of your codebase, you would need to go and modify it to achieve functionality that wasn't foreseen before. Now imagine the assertion returned `IO[Result]` instead. Now _you_, as the caller, have a value to operate on, that fully represents the computation. Running an action after it is a simple as `*&gt; cleanup`, retry is as simple as `.handleErrorWith(_ =&gt; action)`, you can run them sequentially by `List(ioA, ioB, ioC).sequence` and in parallel with `parSequence`. Note that this two latter operations give you another `IO`, so now you can `*&gt; cleanup` and run an action after the _whole_ suite, and reusing the same combinator you had for a single test. So you're able to build new functionality, which the original author didn't envision, compositionally, and with reusable pieces. That's only possible with effects as values. Imagine the impact on this on a real codebase, both in terms of adaptability and modularity. ------------------ Refactoring. That's already been mentioned a couple times, and you said you don't really use this. I just want to point out one of the many cases this is useful: abstracting away duplicated code. If referential transparency holds you can just put any duplicated code in its own definition without having to think about whether it needs to be a def or a val, at which point of the flow it needs to be, and so on. If you don't take advantage of this, you should imho, it's very liberating. ------------------ Separating evaluation order from execution order. This is more minor, but it's a thing that helped many junior members of my teams: you don't need to worry about `val`, `def`, or `lazy val` anymore, since when the `IO` is evaluated is not when the `IO` is executed. ----------------- Interaction with shared state. This is another _very_ important point, but this post is super long already, so I'm just going to touch on it, since I've explained it a million times before and I'm preparing a talk on it. Encapsulating shared (possibly concurrent) state in `IO` gives always rise to this really nice property: _the region of state sharing is the same as your call graph_. This transforms an important part of the behaviour of your code (who shares this state), into a straigthforward syntactic property (who takes this argument). More info: https://gitter.im/functional-streams-for-scala/fs2?at=59ee827df7299e8f53240557 https://gitter.im/functional-streams-for-scala/fs2?at=59f16cf9e44c43700a7bde70 https://gitter.im/functional-streams-for-scala/fs2?at=59fc66e7614889d47535da02 https://gitter.im/http4s/http4s?at=5ad22e27270d7d3708c90e39 https://typelevel.org/blog/2018/06/07/shared-state-in-fp.html ---------------- ... and so on ---------------- As I said, I don't expect essays to convince anyone (apart from the first paragraph, which is correcting some factual errors), but I hope it's enough to make you curious. If you are, and given that it seems you will have to work with `IO` anyway, I'm always available on Gitter (cats, cats-effect, fs2 and http4s channels especially) to talk and try to be helpful. If instead you decide that pure FP is not for you, well, that's perfectly fine as well. Have a nice day 
How does this save division operator look like?
Not many people pretend to be many other professions without actually knowing their things, i guess. Like you wouldn't apply for a surgeon position after being an plumber.
It'll also be interesting to compare it to a binary created by Graal.
TL;DR, he's building a build tool/resolver. The main difference is that he does not want to rely on published jars / central repositories as it creates maintenance burden for library devs, but rather build dependencies from sources so that you don't have to wait for a X.Y.Z release, but rather you can point to a specific commit hash (therefore the tool integrates with git rather than maven). Also the description of how to build a library could live separately, so that even if the maintainers of libraries don't have time to integrate with that resolution mechanism, people can still decide to do it themselves / publish the description of the build elsewhere for the tool to consume. 
There are some comparisons to JDK8 in [the blog post about Interflow](http://www.scala-native.org/en/latest/blog/interflow.html) though it is mainly about comparing native/native+interflow/native+interflow+pgo and it was run on 0.3.7 (and some development snapshots) 
[removed]
ZIO is very impressive and so does the Monix project.
Thank you
I have coded in large systems with Ruby, Python, Java, and Scala. Scala is in the middle of these languages. It's more terse than Java (because of complex type and etc.) and more robust than Ruby/Python (because of static typing). With the power of Scala, you can have a smaller team that can manage a large system (because of the type system and terse code). Now you might be able to guess the disadvantages of Scala. It's complex and has slower compiler. If you are onboarding engineers at quicker pace and have an extremely large codebase, Scala becomes problematic. Think of Google, Twitter, and LinkedIn where they hired 1,000 engineers in a year. In fact, these two reasons, I believe, are why Golang has been invented. It's simple (and more verbose, of course) and compiles quickly. Golang is like an opposite of Scala when you think about it.
Why do I need a framework for this? Between higher order functions, callables, and implicit parameters, I don't see why I would need this. Also, monads and kleisli. 
I think [https://gist.github.com/gvolpe/1454db0ed9476ed0189dcc016fd758aa](https://gist.github.com/gvolpe/1454db0ed9476ed0189dcc016fd758aa) is a good example of combining these FP approaches without using any framework (although it depends on Cats and cats-effect) \&gt; monads and kleisli If you are talking about ReaderMonad, it has some performance overhead for building closures for passing context objects defined outside of the functions. To avoid such overhead, you need to carefully combine functions. And also ReaderMonad makes difficult to control the scope of objects because closures in Scala always contain outer variable references. If your application is small, it will be a subtle thing, but for larger applications, it is a big concern. So despite that FP style is beautiful, it doesn't reduce what you need to think for writing efficient applications.
How would you? wrap it in a class with private constructor?
If I was doing it by hand, then either that or a tagged type. (There's also a trick where you can still use subtypes and typeclasses even if the type in question is actually final - I suspect one could work with `A &lt;: Int : Positive` even though `A` will of course always be `Int` at runtime - but for this specific case I'm not sure that's helpful). Someone else mentioned [refined](https://github.com/fthomas/refined) for a more thorough existing implementation.
IIRC the original keynote didn't highlight one of the principal possible reasons for Kotlin's success, namely that it (allegedly) significantly reduces the clutter when developing Android apps. I must say, having developed a relatively trivial Android app in Java, it was a boiler-plate-city and really a PITA. A possible corollary to this is that Android development in Scala is equally a PITA due to tooling issues, combined with (allegedly) very large library jars due to bloat in the Scala standard library. As ever, what is your market is the question, but why not include compact performant easy-to-develop Android apps in Scala's goal? Or is it to be 2010's J2EE 3 framework?
You don't. If you use implicit paramaters for injection, you don't need any framework. Readers have a performance overhead and you lose the IDE's (ENSIME/Intellij) ability to tell you what specific instance is being injected at a given call site.
This sounds cool. And thank you for answering. 
Map keys are unique, so when `.toMap` is building the map left-to-right it replaces `0 -&gt; 1` with `0 -&gt; 2`. You can illustrate this with `scanLeft`: scala&gt; relationships.scanLeft(Map.empty[Int, Int])(_ + _).foreach(println) Map() Map(0 -&gt; 1) Map(0 -&gt; 2) Map(0 -&gt; 2, 1 -&gt; 2) Map(0 -&gt; 2, 1 -&gt; 2, 3 -&gt; 4) So perhaps you don't want `Map` here?
You cannot have duplicate keys in a map. The second tuple whose key is 0 overwrite the first one.
Readers have a performance overhead, yes. Yes they do but is it in your hot loop and have you measured how big the overhead is? Also if you are constructing your depdency graph one time only at startup what's the difference between a few nano seconds here and a few nano seconds there? 
I'm an idiot. Thank you.
`Seq` is probably better, no? 
No worries, this happens to the best of us :)
Its because you have a duplicate key, 0.
I would've preferred if there was an example of using `Monad Transformers` in `Cats`, but still a good article nevertheless.
I would use `List` because it's very easy to reason about, but sure `Seq` is probably fine.
\&gt; I have retired from correcting Scala programmers. So you say, but if you *actually* just pissed off, rather than repeatedly popping back up like an unwanted skin condition, we'd all be very grateful.
lol go fuck yourself
It's sad how you just can't piss off. Yeah, yeah, you hate us Scala plebs, but for some reason you just can't resist coming in to sneer and snicker. Well, you're not wanted. You've contributed net negative value to the Scala community and human understanding of how to write software, and you'll *never* understand this. Just ... go away.
Haha never. Keep being sad. You waiting for your shitty program to finish compiling?
This sounds similar to the AUR for Arch Linux. [https://wiki.archlinux.org/index.php/Arch\_User\_Repository](https://wiki.archlinux.org/index.php/Arch_User_Repository)
It's not similar to that at all. It's similar to Gentoo overlays.
No, I'm too busy delivering value for my employer.
&gt; I started introducing a safe division operator [..] How does this operator work? Does it return just zero if it detects division by zero? 
Funny guy.
Hmmm, if Odersky doesn't get it right, maybe this is for, who exactly?
There are quite a lot of people who are aware that Odersky does not understand this subject, and refuses to, probably out of incapability :) I'm happy to help people who enjoy learning and sharing ideas. 
&gt; the same reason why in 2006 one couldn't convince Java programmers that first class functions were useful at all ahem http://javac.info/
Disclaimer: I'm a maintainer of [distage](https://izumi.7mind.io/distage/index.html) In your article, you talk a lot about a choice between Pure FP vs. OOP dependency injection, but one possibility that wasn't expounded on is – why not both? In distage, we have first-class support for tagless final style. Say, how [freestyle tagless example](http://frees.io/docs/core/handlers/#tagless-interpretation) looks in distage: ``` class Program[F: TagK: Monad] extends ModuleDef { make[TaglessProgram[F]] } object TryInterpreters extends ModuleDef { make[Validation.Handler[Try]].from(tryValidationHandler) make[Interaction.Handler[Try]].from(tryInteractionHandler) } val TryProgram = new Program[Try] ++ TryInterpreters // Combine modules into a full program ``` where ``` class TaglessProgram[F[_]: Monad](validation: Validation[F], interaction: Interaction[F]) { def program = for { userInput &lt;- interaction.ask("Give me something with at least 3 chars and a number on it") valid &lt;- (validation.minSize(userInput, 3), validation.hasNumber(userInput)).mapN(_ &amp;&amp; _) _ &lt;- if (valid) interaction.tell("awesomesauce!") else interaction.tell(s"$userInput is not valid") } yield () } val validationHandler = new Validation.Handler[Try] { override def minSize(s: String, n: Int): Try[Boolean] = Try(s.size &gt;= n) override def hasNumber(s: String): Try[Boolean] = Try(s.exists(c =&gt; "0123456789".contains(c))) } val interactionHandler = new Interaction.Handler[Try] { override def tell(s: String): Try[Unit] = Try(println(s)) override def ask(s: String): Try[String] = Try("This could have been user input 1") } ``` Notice how the program module stays completely polymorphic and abstracted from its eventual interpeter or what monad it will run in? Want a program in different Monad? No problem: ``` val IOProgram = new Program[IO] ++ IOInterpreters ``` Do you want a program in the **same** Monad, but with different interpreters? No problem at all: ``` val DifferentTryProgram = new Program[Try] ++ DifferentTryInterpreters ``` Distage makes tagless final style easier and safer, it makes your implicit instances explicit and even enforces typeclass coherence by disallowing multiple instances, so one wrong `import` can't ruin your day. Distage is still [in active development](https://github.com/pshirshov/izumi-r2/) right now and somewhat lacks documentation, but we've already been using it for months in production, it allowed to port our legacy code from Akka/Guice stack to pure FP http4s/cats without losing neither ease of configuration and variability of a runtime DI framework, nor parametricity and equational reasoning of pure FP tagless final style. &gt; The code below shows an example of reading configurations from a YAML file We also have first-class support for configs, so your first example with manual config reading is not necessary. Just put your config into typesafe-config and in distage you can request it in any module: ``` case class Config(different: Boolean) class ConfiguredTaglessProgram[F]( @ConfPath("tagless.program.config") config: Config, primaryProgram: TaglessProgram[F] @Id("primary"), differentProgram: TaglessProgram[F] @Id("different")) { val program = if (config.different) differentProgram else primaryProgram } ``` Where your config file looks like this: ``` tagless { program { config = true } } ``` &gt; If you prefer compile-time check, consider using MacWire Unfortunately, MacWire is fully static and cannot change dependencies at runtime. It would seem like the only choice is between unsafe, but flexible, runtime dependency injection vs. safe, but rigid, compile-time schemes. That's not true, however. [Why not have both?](https://github.com/pshirshov/izumi-r2/issues/51) By giving appropriate types to our modules we can allow runtime variation, while at the same time guaranteeing correct instantiation. The proposed typing scheme is currently a work in progress in distage, but already implementing a basic check is as easy as running the exact same wiring code in compile-time macro instead of at runtime.
What do you mean by it's easier to reason about? https://stackoverflow.com/questions/10866639/difference-between-a-seq-and-a-list-in-scala http://alvincjin.blogspot.com/2015/07/seq-vs-list-in-scala.html ^ This is what I found on the advantages of using list over seq. But the date on the post is pretty old. From what I understand though list is implemented as a linked list? So it's great for stuff like map and filter which needs to operate over the entire collection. Simply follow the money to the end (sorry couldn't resist). So it gives efficient access to head/tail? Also it appears that a list should be used where the item's position does not matter? The second article also recommends using Vector over List? 
I knew I wouldn't like this article as soon as I got through the first few paragraphs, which included fallacies like: * To fully understand the benefits of DI, we need to use an actual DI framework in parctice * Management if service lifecycles, such as start-up and shutdown processes has anything to do with DI Yes, those two bullet points are actually in the article almost verbatim. For point number one, what JVM-based software developer has been blessed enough by the powers of heaven to not yet have had a DI framework forced down their throats as they inherit code written by another team that chose to use Spring or Guice? I'm pretty sure everyone that dislikes DI frameworks has had plenty of experience using them already. DI is providing dependencies to code rather than that code explicitly accessing its dependencies. It is merely the difference between passing a value to a "function" or some block of code versus that code directly accessing a value which usually means it is using a global variable. That's it. No framework needed, thank you. A major problem I have seen with most DI framework usages is that the magic pieces are not modular, composable, or good software design. In fact, they often lead to code that is almost as bad directly accessing global state. For instance, autowiring by type or by name is essentially autowiring a global variable into your application. The guice devs say that they save hundreds of thousands of boilerplate lines of code at Google, but, I just don't believe them frankly. I've written several services without a DI framework and I never missed them. 
`List` is a strict, singly-linked list, nothing more. It has very well-defined behavior and known performance characteristics. Consequently it's easy to "reason about" and understand what you can do with it and how your program will behave. `Seq` could be anything, e.g. `List`, `Vector`, even `Stream` (which is particularly pernicious). So the runtime behavior of your code is in some sense unknown. You can discover and remember it, of course, but if you're just reading the code at first you can't see it. It's also easy to do something to the `Seq` that is not well supported by the implementation, e.g. `seq :+ element` will append to the sequence, which is quite slow if it's a `List` but reasonably fast if it's a `Vector` and may never complete if it's a `Stream`. If you really want an interface instead of a concrete implementation, I suggest `IndexedSeq`. At least then you know that it will be completely in memory, not lazily generated, and can randomly access an element with reasonable performance.
Ahah so true
Very interesting approach. Thanks for sharing. Supporting interpretation of tagless interfaces looks good. \&gt; Unfortunately, MacWire is fully static and cannot change dependencies at runtime I know and importing all dependencies into the current scope can be tricky if you need to wire many modules. I've also considered the dynamic + compile-time check approach in Airframe [https://github.com/wvlet/airframe/issues/37](https://github.com/wvlet/airframe/issues/37), but discarded this idea since the syntax will be unnatural to pass the module dependencies at compile-time. If you can find a better syntax for this, let me know :) 
I’m doing a play framework upgrade from 2.3 to 2.6. I have to change 60 test suite files BEFORE I can testOnly on the first one. Or I need to empty the entire directory structure...
Thanks a lot for your work.
You need to do it stepwise -- upgrade from 2.3 to 2.4, get all the tests passing, and then upgrade to 2.5 from there. The migration guides help much more then.
I’ve already done the whole jump. The app works and the test suite did too. They’ve made a serious jump from cake pattern to dependency injection that’s mind blowing in terms of the refactoring you’re asking for. Now I’m just improving the DI and cleaning it up. It’s taken about 3 weeks of absolutely solid concentration. Now I’m just in the tidy up phase. I know the entire Play Framework documentation including those Migration pages like the back of my hand. The MessagesProvider bullshit was the worst of it. The CSRF fuckery was trivial by comparison.
Been working on and off on a proper Discord API library in Scala, [AckCord](https://github.com/Katrix/AckCord), for quite some time now. I've tried to address many of the annoyances I've encountered while working with many of the current Java libraries, like mixing up different types of ids (solved using tagged types), difficult to reason about concurrency, invisible side effects, lot's of indirection, not being able to go more low level when needed and more. Been a long project, but I feel like I'm getting closer to something I can be truly happy with soon. I'm planning to release version `0.11` soonish (as soon as I can get back to my GPG key).
For GUI I strongly recommend JavaFX- its much more modern than Swing and includes easy support for styling with CSS and a XML-like language for declaring your UI(doing it though code is of course still possible). Besides it gets you great looks straight of the box whereas Swing looks a bit more dated IMO. The only downside is poorer support(26K questions on SO vs 70K for Swing) since fewer folks use it, but at least Oracle has some [good tutorials](https://docs.oracle.com/javafx/2/get_started/jfxpub-get_started.htm) for getting started 
From the [linked blog post about Interflow below](http://www.scala-native.org/en/latest/blog/interflow.html): &gt; On the other hand, Interflow (geomean 0.89x) outperforms Graal Native Image statically. With the addition of PGO, Interflow gets quite close to the throughput of a fully warmed JIT compiler (geomean 0.96x). 
You can use a multimap. val input = """|5 |0 1 |0 2 |1 2 |3 4""".stripMargin val lines = input.split(System.lineSeparator) val size = lines.head.toInt implicit class SetMultiMap[A,B](private val map: Map[A, Set[B]]) extends AnyVal { def addBinding(key: A, value: B): Map[A, Set[B]] = map + (key -&gt; { value :: map.getOrElse(key, Set()) }) def removeBinding(key: A, value: B): Map[A, Set[B]] = map.get(key) match { case None =&gt; map case Some(Set(`value`)) =&gt; map - key case Some(set) =&gt; map + (key -&gt; set - value)) } } val relationships = lines.tail.foldLeft(Map.empty[Int, Set[Int]]){ case (acc, line) =&gt; val Array(x, y) = line.split(' ') acc + relationshipsBuilder.addBinding(x.toInt, y.toInt) } relationships.get(0) // Some(Set(1, 2)) 
Thanks! Here is the full link: https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/dl4j-examples/tutorials/15.%20Sea%20Temperature%20Convolutional%20LSTM%20Example.json I'll ask Jaxenter to fix it on their site...
I've spent 3 months once upon a time migrating from 2.3 to 2.4 and trying to remove almost all the deprecated stuff in order to not have to deal with something like it again later... I still feel pain from those days. Do not recommend. 
Removing because this was recently posted here and is still on the front page and has 257 comments.
Because of the template interaction? Or because of all the implicits?
Was this recorded? 
Yes it was. Oli told me on Twitter that it will take a few weeks to create the video. I'll post it as soon as it becomes available.
Would you consider the passage of time a side effect? It's an observable change in state outside the scope of the function (on the JVM just look at System.currentTimeMillis). If so, does laziness make function calls "pure"?
I think worrying about the performance of a Seq is probably a red herring at this point, it will probably end up being a Vector at runtime, i.e. whatever the split method actually returns. If OP has actual issues they can always fix it up later.
Great, thank you! 
I'd argue it's more likely to be `List` since that's what `Seq.apply` returns, and the performance issues with linked lists are well established. Most other methods tend to preserve their original runtime type. Though I admit you're likely to get `Vector` if you call `toSeq` or use `collection.breakOut` on something that isn't already a `Seq`. But what it's likely to be is tangential to my point. Indeed, even the actual performance or lack thereof is tangential. I'm not arguing that the performance here is or should be a concern. I'm arguing about what you do and do not know. So it's the "probably" in your statement that is exactly my original point: you don't really know much just from looking at it and it's the not knowing that makes things less reasonable. If a method takes a `Seq` then there are more things it can do so you know a little bit less about what it will do. You can't look at the method and say definitively how it will behave because it depends on what gets passed in. While it works in this case, "fix it up later" is not always viable because you don't always have control over the offending code, e.g. if it's in a library. In the end it's all trade-offs. There's a lot of flexibility and re-usability gained by being able to pass in more types of things. But that flexibility often comes at the cost of reasonableness. One of the reasons I like typeclasses is because I think they strike a better balance of re-usability and reasonableness by allowing you to have highly generic but very small interfaces, so both the caller and the callee can understand precisely the operations available to the implementation. For instance, if your method requires a `Functor`, one of the few useful things you can do with that is call `map` (or things built on top of `map`). So the callee can accept anything that can be `map`ped and the caller knows _without looking at this implementation_ "this is going to call `map` so the only thing I need to care about right now is how what I have behaves when `map`ped". Contrast that with taking `Seq`, which has so many methods and disparate subclasses it would be impossible to know what's called, and so impossible as the caller to reason about what calling that method is going to do. Maybe it only iterates, and you're fine. Maybe it calls `size` and causes an out of memory error because the third party library you just updated happened to switch to a `Stream` internally and as it turns out you've been routing a `Seq` the library handed you through your code and what was once strict is now lazy and nothing was really prepared for that possibility.
Yeah I had dozens of functions in traits that do Messages() calls. Since they’re traits I had to explicitly add the implicit to all the individual functions. Then I needed to turn a bunch of vars into defs as well so they can take the parameter too. Then I had to add the implicit declaration to the top of every html page based on whether it called Messages directly or included something that did. Then I had to rewrite the test suite which was doing this: ControllerWasAnObject.func must_=== Messages(“deep.error”) Anyway coming together now. Another week and I’ll be completely finished. Like I said, I’m done, it compiles and has 100% just got some misc stuff to do (reads scoverage,rpm, sbt-release etc)
It expects a `N Refined Positive` as it's second argument, using the refined library https://github.com/fthomas/refined :)
Using git as an example of a friendly user experience is laughable at best.
I feel like you've ripped the exact words out of my mind! I share your pain, so much.
You're absolutely correct!
Actually, it's pretty simple, you don't need a new syntax! Since your syntax is an immutable builder, you can keep the exact same syntax, except also accumulate a type-level list for binding info. See our sketch: https://github.com/pshirshov/izumi-r2/blob/develop/distage/distage-static/src/main/scala/com/github/pshirshov/izumi/distage/model/definition/TypeLevelDSL.scala#L73 You can carry values directly, but you may also only carry at type-level – values of singleton types can be recovered with `Witness` even if you discarded the objects – https://github.com/pshirshov/izumi-r2/blob/develop/distage/distage-static/src/main/scala/com/github/pshirshov/izumi/distage/model/definition/TypeLevelDSL.scala#L46 Your macro can then analyze the enriched type of Design and provide compile-time checks, or even do partial evaluation (because you can directly access the values of singleton types if you have just the type) 
&gt; Mastery of the Giphy plugin in Slack Shit, I'm underqualified for this job.
People approach the same endpoint from different directions. The trouble with the command pattern as a pattern is that you have to manually implement composed commands for every kind of command you want to implement. If you try to write a generic version of a composed command, i.e. a chain of commands with functions to go from the output of one to the next, then it turns out what you've written is the free monad.
That’s indeed awfully specific/niche for a “requirement”.
I'm afraid to ask for a salary estimate. In the US, senior Scala devs make more than any other engineers, I believe. I a high cost of living city like NY, DC, or SF, this might approach $200k. Let's just say I've heard Spanking has low salaries all around, but especially low for engineers compared to US, UK, etc. I'm guessing Euro 50k unless this company is highly funded...
upgrading Play is always too problematic. More than once I've almost going with another framework just because of that. If I have to spend 1 developer's week on each upgrade, then it starts to become too much overhead. I hope the Play Framework team starts to consider that on newer releases
&gt; Somehow first-class modules are good, but objects are BAD! Finding any difference between the two is left as an exercise to the reader. Modules don't (or at least shouldn't) contain mutable state. They contain functions (which are values, you can copy them out of the module and use them as normal functions) rather than methods (which are inherently entangled with an object's state) or messages.
&gt; If you try to write a generic version of a composed command, i.e. a chain of commands with functions to go from the output of one to the next, then it turns out what you've written is the free monad. Going from monoid straight to monad, isn't that shooting sparrows with a canon? A free arrow is sufficient for composition. But it's still too costly, as you lose deep embedding (serialization) due to containing functions in your commands. Instead, just implement a stack machine and in your commands do not include argument values directly, but instead refer indirectly by their indices, same as for example Lua bytecode; Every free monad can (obviously) be represented that way, but unlike free monads, bytecode allows arbitrary transformation and serialization, at the expense of requiring more caution.
&gt; Modules don't (or at least shouldn't) contain mutable state. In ML they often do... [See](https://www.cs.ox.ac.uk/ralf.hinze/WG2.8/24/slides/derek.pdf). And with same implications as objects. &gt; They contain functions rather than methods No difference in practice, since functions can capture the module they're defined in on construction. JavaScript objects were made like that for ages! &gt; which are values, you can copy them out of the module and use them as normal functions Not necessarily, if they contain existential types, you won't be able to interact with them meaningfully without tagging along their module of origin. No, I don't think you can make a meaningful distinction, since if a language has mutable variables, nothing can prevent tangling with module's internal state, and abstract/existential types can easily force you to keep the module and its functions together up to total isomorphism with an OO system.
The Functor example you gave has the same issue—if you’re agnostic to the actual data type and just working against the Functor typeclass, you also can’t reason about what the actual implementation will be like (even if it obeys the Functor laws).
&gt; Going from monoid straight to monad, isn't that shooting sparrows with a canon? It is - like I said, you get there from the opposite direction. If `F[_]: Monad` then side-effect-only commands `F[Unit]` have a monoid instance (via the applicative instance for `F` and the monoid instance for `Unit`), so you can certainly do anything you can do with monoidal commands. You may well not actually need the full power of monad. But "this function, then this command, then this function, then this command, then ..." is the most direct/natural/intuitive way to implement a generic version of composed commands and take some of the overhead out of using the command pattern, at least for me, and then maybe you can constrain it a bit more later. &gt; But it's still too costly, as you lose deep embedding (serialization) due to containing functions in your commands. In some cases that's a major cost, in other cases not. Depends on what you're using the commands for. &gt; Instead, just implement a stack machine and in your commands do not include argument values directly, but instead refer indirectly by their indices, same structure as i.e. Lua bytecode; Every free monad can (obviously) be represented that way, but unlike free monads, bytecode allows arbitrary transformation and serialization, at the expense of requiring more caution. That's interesting but it sounds like a lot of work, whereas I came at this from wanting to simplify the command pattern and reduce the amount of custom code I have to write for each new command language (because with `Free` I don't have to implement my own composed-command type, I can just implement the "primitive" commands and then I get the composed one "for free").
&gt; if they contain existential types, you won't be able to interact with them meaningfully without tagging along their module of origin. Indeed, but that makes the coupling explicit. I'd far rather call a function and get a piece of state that is visibly an opaque handle that I can only use to interact with the module the function came from, than call a method that stores the equivalent state as an invisible mutation to the object it came from. &gt; if a language has mutable variables, nothing can prevent tangling with module's internal state Agreed that there is no "physical" distinction here, but there is a cultural one. Certainly when someone says a value is an "object" I expect that to have internal mutable state (that will change the behaviour of its methods), whereas when someone says a value is a "module" I expect it to be (as-if) immutable. &gt; abstract/existential types can easily force you to keep the module and its functions together up to total isomorphism with an OO system. Visible state is meaningfully different from hidden state even if they're isomorphic.
Another way to find usages is to comment out that particular item and see what breaks in the compile.
&gt; That's interesting but it sounds like a lot of work Ah, yes, it's not for program structuring, but for cases where deep embedding is necessary.
Not everybody in the world has the opportunity to work in NY, DC, or SF.
We wish we could though. Statistically I'm in the top 10% of earners for my country and in the higher salary bracket, 200k usd is nearly a 80% wage increase! Wouldn't be so bad but cost of living isn't that far behind the top US locations and actual interesting jobs are way behind what's in the US.
I sympathise. I'm also in a non-US market, but thankfully cost of living and salary compliment each other in my situation.
&gt; Parallelism is very tricky to get right, and Scala offers a number of libraries which make this task easier, by building abstractions. There are two main approaches: an actor-based, represented by Akka, and an FP-based one, represented by Monix/cats-effect and Scalaz/ZIO Finagle handles probably the most traffic in the Scala ecosystem (Twitter and its backend services) and it doesn't use either actors or monix/scalaz/cats tasks. Finagle's Twitter futures are fairly similar to `scala.concurrent.Future`, and futures are one of the main approaches to parallelism in Scala. I didn't see any usage of akka or tasks at Amazon either. See also "Parallel and Concurrent Programming" in https://docs.scala-lang.org/overviews
"However, I think that Scala is still the “better Java”, despite Kotlin." - 👍 The Scala community must emphasise this fact more in order to attract more users from the Java community. Because it's TRUE.
One business reason is that you get smart people eager to use a language other than Ruby/JavaScript applying for your job openings
We actually just don't upgrade play. We are sticking on 2.3 for the foreseeable future. For better or worse...
For more then one year I used Scala every day at work. Besides using it for Spark jobs on Hadoop, when I had to build an API from scratch using Play2 and Slick, it took a lot of time for every new feature. I followed the repository pattern for data access and I used ScalaTest for unit testing. I remember I tried to use Wiremock for integration testing (I was accessing an external service) and I can't because of Play2. I had a lot of issues when integrating Mockito at first. And for every new repo using Slick I had to write a lot of code every time. Everyone in the company used Scala so I had a lot of people to reach out when I felt stuck. Out of curiosity, how do you guys handling things like that? Even the error handler in Play2 took a lot to implement it. After this experience I'm switching back to Spring and Java. I admit that learning FP in Scala is pretty awesome. I don't mind writing code but reinventing the wheel every time can be a bit frustrating. :)
&gt; Indeed, but that makes the coupling explicit. I'd far rather call a function and get a piece of state that is visibly an opaque handle that I can only use to interact with the module the function came from, than call a method that stores the equivalent state as an invisible mutation to the object it came from. Note that Ocaml doesn't have a convenient .copy method, so more often than not, the handle itself will be mutated. &gt; Agreed that there may not be a "physical" distinction here, but there is a cultural one. Certainly when someone says a value is an "object" I expect that to have internal mutable state (that will change the behaviour of its methods), whereas when someone says a value is a "module" I expect it to be (as-if) immutable. This hasn't been my experience with Scala, 99% objects in Scala are immutable. And it's a good idea to drop shitty frameworks like Akka, that violate this convention. Kotlin encourages immutable objects too, so do most new languages except Go. IMO there is a much bigger chasm between java objects and scala objects than there is between scala objects and modules.
You can make it total by changing the return type (e.g. Try) 
&gt; I feel like you've ripped the exact words out of my mind! I share your pain, so much. rant continued.... For my last several years working with Java, I did a decent amount of FP. Over time, I'd find myself slowly building up a library of Java FP tools, and then I'd get the idea "Maybe I should add some polish, testing, and release this is an open-source library!" A couple days later, I'd usually stop working on it. The reason why I usually stopped was mostly around the realization that "Why am I spending so much time writing basic FP tools, when Scala is right over there, and supports 100x these feature naively? The wiser thing for me to do, would be focusing on learning Scala." You might notice that's quite similar to: &gt; *"Why am I spending so much time and effort fighting the Scala as nicer Java crowd, when I could use Haskell (and similar languages) and never have to deal with this ever again."* The frustrating thing for me is that I had the misconception that the majority of Scala Devs actually cared about FP. I had heard of the `Scala as nicer Java` types several years before my first FP job, but I thought those were probably only the laziest devs, or Java devs who had zero interest in Scala. I don't know what the actual `FP Scala` to `Java as nicer Scala` ratio is, but on my current team, it's a about `1:5 ratio`. The #1 people who will (and do) contribute the absolute most to Scala, and who are more knowledgeable, and the Scala eco system are from the "FP Scala" side. In /u/jdegoes's video, he talks about core contributors leaving Scala. All of the core contributors I've met, who have left, always leave because they're tired of dealing with the `Java as nicer Scala` bullshit. I strongly suspect I will make a similar move myself in the next 5 years.
The example, regarding Monad Transformers, that you ask for can be found in here http://www.geekabyte.io/2018/05/thoughts-on-dealing-with-having-another.html#use-monad-transformers :) This was also mentioned in this post by the way.
Scala is better than Java only because it's completely unlike Java, which may not be what they want to hear.
Play2 offers a lot, but I find it quite annoying. For most applications it's overkill and just slows you down, especially for backend-only. If you are learning FP anyways, try out \`http4s\` next time. Much more easy to get started and less opinionated, so you can use the libs you like. However, you should try not to use mocks if you want to learn FP. ;)
being a better java also involves tooling, from the build manager to availability of good quality IDEs, so definitely no.
Why not using mocks ? I don't see its problem.
I'm surprised you want to go back to spring. Spring to me is like javascript, completetly untyped, shit performance, and unreliable debug information because of the endless hidden hoops.
\&gt; How do you handle that Mostly by writing SQL and executing it rather than relying on Object Relational Mappers and Entity Frameworks. Really, use [Doobie](https://tpolecat.github.io/doobie/), here's a [quick demo/tutorial](https://www.scala-exercises.org/doobie/multi_column_queries), and here's the full-blown [tutorial](https://tpolecat.github.io/doobie-0.2.0/00-index.html), and one of its coolest [features](https://tpolecat.github.io/doobie-0.2.0/06-Checking.html). \&gt; I had a lot of issues when integrating Mockito at first. Either don't use mocks (use docker to set up your dependent services with [docker-it-scala](https://github.com/whisklabs/docker-it-scala) for integration tests, and create fake test instances for your unit tests), or use [ScalaMock](https://scalamock.org/). ScalaMock is painful at first (it's strict, meaning a mock is exactly once by default, and you have to annotate EVERY, SINGLE, TYPE in the method you are mocking, and suffers from the same problem with finals and private mocking as Mockito), but once you get used to its style, you can mock pretty much anything you need to. Judicious composition of objects and functions also helps lessen the things you have to mock by raising the level of abstraction, of course. \&gt; I had to build an API from scratch using Play2 If you don't have a GUI for your api, Playframework is overkill, and adds a LOT of complexity. [Http4s](https://http4s.org/v0.18/) is fairly easy to set up (it comes with a sbt template that will set you up quickly) and the [Http4s DSL](https://http4s.org/v0.18/dsl/) is simple. You'll have to learn IO, but it's pretty much like Future, except it is consistent, you control when tasks shift to an ExecutionContext, cancellable, and requires you to call unsafeRun\* to execute it. Creating common wrappers for simple routing services is pretty [easy.](https://http4s.org/v0.18/middleware/) It comes with [Rho](https://github.com/http4s/rho/blob/master/Rho.md), which outputs api documentation. I use [circe](https://http4s.org/v0.18/json/) for my json. I used to recommend and use akka-http, but its documentation is more comprehensive but not as concise and opinionated (user-friendly) as http4s', and it is a much heavier dependency for your project, which kind of defeats the purpose for a lightweight microservice. \&gt; reinventing the wheel every time can be a bit frustrating Any time you have a problem that others' will have, create a library (if your company lets you), and publish it. The above libraries are fairly comprehensive, of course, but there might be some missing feature that you need. Finding all these things as a scala newbie (one year is definitely newbie territory) and fighting a company culture geared toward one library or another is difficult. The [Scaladex](https://index.scala-lang.org/) is your friend, and so are the [Gitter](https://gitter.im/scala/scala) chatrooms attached to most projects.
If anything, the way the Scala involved parties are addressing the language evolution is very professional, they care a lot, and are doing a lot with the scarce resources they have, specially when compared to languages backed by companies such as Oracle, Google, JetBrains and Microsoft. Props to them.
You can reason _more_ about it compared to `Seq` because there's much less it can do. The worst it can do is call `map` a bunch of times because that's all it _can_ do. I guess in a world with exceptions it could throw those, or have an infinite loop in it, but even in those cases I would know that it's not part of `map`. I wasn't trying to say typeclasses are magical thing that is completely reasonable and completely reusable for both caller and callee. I was trying to illustrate how the trade-offs lie on a spectrum and say why I like where typeclasses fall on that spectrum.
Thanks for the reply, this is like gold :). I ended up using circle at some point, that's a nice library. &gt; Either don't use mocks (use docker to set up your dependent services with [docker-it-scala](https://github.com/whisklabs/docker-it-scala) for integration tests, and create fake test instances for your unit tests), or use [ScalaMock](https://scalamock.org/). ScalaMock is painful at first (it's strict, meaning a mock is exactly once by default, and you have to annotate EVERY, SINGLE, TYPE in the method you are mocking, and suffers from the same problem with finals and private mocking as Mockito), but once you get used to its style, you can mock pretty much anything you need to. Judicious composition of objects and functions also helps lessen the things you have to mock by raising the level of abstraction, of course. AFAIK tests should be quick so throwing docker for tests, even integration, will slow them down. At first glance the [Http4s](https://http4s.org/v0.18/) looks really good. I wanted to use Cats at some point but didn't have the chance.
Mocks are extremely unnecessary for testing well factored code. They're also very very easy to abuse and write tests that are a net negative on codebase health
I'm so sorry. For what it's worth, you've gone through the worst of it. If there's one thing I wish had been done with Play, it would have been to make rendering related functions take an implicit context. I wrote up some of the issues about designing with implicits (and some of the odder bugs) [here](https://twitter.com/i/moments/879509069995704320).
I was using DI in Play because otherwise how you are going to store all the code from different services and components? For example let's say you have a Controller that has a Service as a dependency and the service has a http client and a database access layer. How do you test this structure in FP way? My opinion is that you need to take the best of from both worlds OOP and FP when trying yo resolve a problem.
I can't tell you how happy it makes me to hear that there are some sane people out the who haven't chugged the akka koolaid.
For a new project / codebase that's being started, what can be done to ensure that migrating to scala 3 won't be a problem or will just work out of the box?
In a FP, a very famous way to do that is to use the final tagless pattern (or even eff). If you want to stick with more traditional like structure, take a look at macwire. It is compiletime DI and unless you need to change dependencies while your application is running, macwire is a very solid solution.
For testing with dependencies, it depends on how you structure your program. I would separate out the functional parts as much as possible from the imperative parts. Then the functional code can be unit tested without any dependencies. The state-changing part of the program should be simple as possible that they don't need to be unit tested. You can test the state change + functional parts during integration testing. Depending on the state, you can use mocks, embedded datastore, dockerized containers containing the dependencies, etc. 
I see. I got the idea. So by using HList like type composition, we can tell the compiler the actual binding types. 
I've used Play2 (I moved to a different company that uses Akka-http now), but I never had a problem with Play 2 to make backend-only REST services. I thought it was simple. You had your routes, and they pointed to the controller. It's not as simple as http4s since you had to deal with Actions, but I didn't think it was too bad. I think Akka-Http is worse. 
play 2 is really a piece of crap. don't blame scala for its shortcomings
\&gt; AFAIK tests should be quick .... docker Yeah, they are integration test only. Other than that you should definitely already have some interface wrapped around the things that actually connect to your db/disk/apis. Fake them or mock them. Faking is super easy. Just new it up with an anonymous class: `val fake = new MyService(a,b,c){` `override def myCalledMethod = "my result"` `override def anythingElse = ???` `}` If your interface is large, then you have issues, but that's a different design problem that can be quickly solved by some refactoring.
Intellij with Scala is amazing though... But then again, intellij is pretty much your only decent option for a Scala IDE; it would be better with more options...which is on the way actually. Build tool wise, SBT is pretty good, but hard to figure out and a incredibly slow to start up. Mill looks really promising; a couple of orders of magnitude faster, but is still perhaps a little to immature for enterprise adaption (from personal experience). I'm not sure where Mill will be in 1 year; but I'm pretty damn sure SBT will be around. Both are alright though.
Me neither, play is simple and imo much better to code in than spring. 
Well, simple is relative right? Compared to http4s I don't find it simple at all. It starts with the routes. The default in Play is that you have your own file with it's own language, which is parsed at compiletime. But you have to learn the syntax, known Scala doesn't help. You need an IDE plugin to get help, e.g. to know which controller is called when a request comes in for a certain route. You can's use Scalas language feature to factor out common route parts or subroutes and so on. Of course, having it's own language/syntax for routes also comes with advantages, but is it really worth the drawbacks? I think for most applications, especially small ones, it's not worth it at all. Next thing is guice as default for dependency injection (at least it was default for quite some time). Runtime di with a Java lib. Not really what a Scala dev wants to use, when he is going the FP way. Actions are another part that they designed rather bad. So all in all: Play2 has a lot of drawbacks and is usually not worth it, except if you have special requirements or use some of its special plugins. Most apps don't. But to be fair, I also think Play2 has a hard time because they want to support Java as well. Makes it hard to create a sane api. And I like it still better than akka-http, at least from the little impressions that I got from the latter.
&gt; Intellij with Scala is amazing though sorry, a lot of us think quite the contrary 😕
Being a better Java involves exactly what the author of your above article emphasized. E.g. Immutable data structures without boilerplate (annotations included) and other stuff that makes the code more readable and hence maintainable. And last but not least makes you as a programmer enjoy your work more since you don't have to deal with inventing hot water over and over again (just learning Scala had pointed me to many eye opening paths)
&gt;Of course, having it's own language/syntax for routes also comes with advantages, but is it really worth the drawbacks? I think for most applications, especially small ones, it's not worth it at all. I think the routing file is better for smaller applications. The "language" is pretty simple: Method path Controller. eg. GET /foo/:id controllers.Foo.get(id: Long) For simple routes, I don't see how it can get any simpler. It's easy to read and write. Also, I'm sure you're aware, Play does have it's own Routing DSL that you can write in Scala if you need to do more complex things. It's probably not any easier or harder to learn than http4s's DSL. You're right that Play's decision to have runtime DI out of the box was bad. They were catering too much to the Java crowd with that one. Although switching to other DI methods seem trivial. But since it's not out of the box, it's not easy as http4s, or even akka-http where you can do whatever method you want to use (constructor, macros, Reader, Free, etc). Speaking of which, Play was a hit at my previous company where there were more guys familiar with Java. The runtime DI and everything seemed simple and familiar to them. It helped them to get into Scala more. 
Sadly, I must agree. I have encountered many bugs even in "simpler" Scala usage (usually pimps, generics and self types). When you start using a library (even very common ones, like Shapeless) which use macros, then you are usually screwed - e.g. I tried Udash framework, I had many files riddled with squiggly red lines even thought the code was valid and compiled fine. There were even issues with imports... IntelliJ lacks macro support and only very few libraries based around macros are working nice in it. :(
That's because you already know what the syntax looks like and how it behaves. The question is rather: why need a new dsl for routing at all? And yes, Play has it's own Routing DSL, which is not default and not very nice. The thing is, if I know Scala in and out, with http4s I know how to refactor a bunch of routes which share the same path without having to learn anything but seeing the list of routes (as a list of \`case\`s). You can't do the same with a Play2 routes file without taking a look into the documentation. Don't underestimate that - sometimes it's hard to take the eyes of someone that never touched Play before. Same with DI. Sure, if you know Play and the options it's easy to switch. But imagine someone just starting. He has to read a lot of documentation to even know where to start to do manual DI. And then all the examples are with guice... it's actually not that easy to switch (and I talk from my own experience here) unless you did it before. So I'm not saying Play is bad. But for a small backend app for a beginner who wants to learn FP in Scala, I would recommend \`http4s\` over Play anytime. If you have someone who works with a Java team and wants to get things done fast and have good documentation and examples and doesn't care about FP and plans to develope a huge app, I'd say go for Play!
Too many people conflate parallelism and concurrency. 
This isn't the place for this. 
You're right. I am moving it to their inbox. Just gimme a minute.
I am not deleting this until they give me a phone interview.
They dropped me suddenly after a multi-week process where everything seemed to go really well. Maybe they have some issues with their hiring team.
&gt;John de Goes mentioned some of the OO features which are useful in the purely-functional approach as well: **first-class modules** (via object composition), **dot syntax** (calling methods on objects) and **first-class type classes/instances** I'm starting to think 90% of the hate for OO is really targeted towards sub-type inheritance and the brittle hierarchies they sometimes lead to - people actually really like the rest. Kind of like how 90% of the hate for implicits is probably exclusively for implicit conversions. Most people like Haskell's typeclasses and C# extension methods. But those concepts got combined with something that sometimes looks like Javascript-level "type safety". During my Spark courses, Scala newcomers are occasionally tell me how they hate "implicits". I don't really blame them, but this just might be a marketing problem. I'm excited by Mr. Ordersky's keynote about potentially fixing it.
I'm sure they can't wait to hire you now.
What do mocks have to do with well factored code? 
I’m confused by your comment, can you elaborate? Are you implying that Akka enables parallelism and Futures do not?
Removing this because it has already been posted today :-)
What I'm saying is that when your code is well factored you don't need to use mocks to write tests. Mocks are a smell and should be aggressively avoided. If your code can't be tested without them then your codebase will only grow more unmanageable over time
I might have worded that not very well. What I meant was to not rely on mocking frameworks that use reflection and alike to do the work.
It's nice you enjoy Scala as the rest of us. And I'm sorry about the bipolar/psychotic symptom you describe in that thread -- that sounds pretty scary when it happens - definitely check that out. Maybe it's nothing to worry about, we all got our cross to bear (mine is general anxiety), but that's just life sucking from time to time. Still good to be alive and a great time to be a (Scala) programmer! That said, I don't think you're "punishing" Lightbend by leaving this thread here. This is a forum for and by Scala enthusiasts, and have nothing to do with Lightbend the company afaik. Odersky is here once a month or so, but that's about it. You should probably try emailing them or something; and give it some time. Maybe they'll call you next week?
I couldn't agree more. I think there's a reason Rust tops the developer happiness survey, and I believe it's because its tooling is so wonderful to work in. By comparison, Scala's compilation is slow, it offers a single, sane way to edit code (intellij), its default build tool is complex and unpleasant to work in, and has the rather unexpected limitation of not working across minor version boundaries (2.12 can't work with 2.11).
"as competent, responsible software engineers, we are more than capable to choose how to best solve a specific problem. I don’t believe that using crippled tools (i.e. languages with fewer features) and taking away some of the freedom that we have in Scala is the way to go." I see this as a business reason to not use Scala. With any team, you will have many people choosing the "best" way to solve a specific problem and which will lead to divergence. In my experience at my workplace, even something as simple as picking a formatting rule in scalariform leads to divergence and wasted time. This is why I prefer prescriptive languages like Elm. Though, I will note this lack of prescriptiveness is an issue I see that not only lies with Scala.
&gt; the way the Scala involved parties are addressing the language evolution is very professional What is the context of this comment that one's professionalism needed to be highlighted?
&gt; Too many people conflate parallelism and concurrency. Since so many people do it, would some kind soul volunteer an explanation that unconflates (?) the two?
&gt; pimps What's the latest on alternative names for this? Something more PG. C# calls it "extension methods". I've seen old code refer to it as like `RichA`, enrichment maybe.
&gt; completely unlike Java Devil's advocate: You can still write awful, Java-like Scala, if one were so inclined or didn't know any FP conventions (and actively disregarded common Scala conventions).
&gt; One business reason is that you get smart people eager to use a language I don't know if this phenomenon has a name but this is the exact tactic Google used back in its early days by looking for Python developers. At the time Python wasn't popular, so the people that would apply were typically self-starting language enthusiasts.
&gt; But then again, intellij is pretty much your only decent option for ~~a Scala~~an IDE; A controversial FTFY 
I don't think this is true at all I think you could argue you can write Scala in a very similar syntax to Java and apply many of the same patterns, while utilizing the same libraries and frameworks.
&gt; In a FP, a very famous way to do that is to use the final tagless pattern (or even eff). Any recommended links for learning this style?
i'm a big fan of love island
Just opinions so take it with a grain of salt: * Play involves too much magic, kind of like Ruby on Rails - initially, you don't know what is done by Play libraries, what by code generator attached by sbt plugins and what is just a convention, if something doesn't work, you don't know the foundations because Play was supposed to do everything for you * Guice is put there only because many Play users migrate from Java, and for them it is something familiar. It as used to replace cake (anti)pattern, but many programmers more into FP find it atrocious * Slick is the only Scala library I used so far to deal with database from Scala, but over these 2 years it gives me more and more reasons to try out something less annoying. Lot of boilerplace, you cannot compose things using traits or type classes, because of magic that happens in each \`map\` and \`filter\`, so at best you can DRY it with macros :/ For routing Akka-HTTP is much better imho, you can define dependencies yourself. But there are many alternatives which also feel lighter that full blown framework like Play. Actually, quite a lot of people I know say that they don't do frameworks - composition of libraries is is simple that they can configure whole server in a day or two and they can understand and change every single line of it. (I wanted to search an example for Akka HTTP with Quill, but there is a lot of GitHub examples, which shows how you can set up a server where everything is easily testable, same for Akka HTTP + MongoDB - IMHO much easier to understand by newcomer and maintain than Play). It might have been that you didn't get burned by Scala as much as by Lightbend stack.
Half the community ? More like 95%. Most Scala users are silent. 
https://cs.stackexchange.com/a/19993/58979 
Concurrency is the term used for multiple tasks making progress in some amount of time. Parallelism is the term used for multiple tasks running at the same time. Concurrency does not require parallelism, but parallelism is a form of concurrency. One way to visualize this is by imagining two tasks, `A` and `B`, that get accomplished in units, `A1, A2, ... Ai`, `B1, B2, ... Bj`. Each piece must run on some set of resources, `R1, R2, ... Rk`. Even if you only have one resource, you can still have task-level concurrency by interleaving the work units. R1: A1 B1 B2 A2 A3 A4 B3 B4 ... Over time, both `A` and `B` make progress, but only one makes progress in any given moment. This is concurrency without parallelism. Parallelism requires multiple resources in order to have simultaneous progress. R1: A1 A2 A3 B4 B5 ... R2: B1 B2 B3 A4 A5 ... This can be extended to arbitrary numbers of resources, arbitrary numbers of tasks, and work units taking multiple time slices, but the core concept remains the same. There are multiple ways to map this model to the real world. A natural one might be to consider a "thread" as a resource, an "assembly instruction" as a work unit, and "program" as a task. Another might be that a "message" is a work unit and an "actor" is a resource. To tie this back to the thread: actors are a primitive for concurrency because their main functionality is serializing many possibly-parallel messages onto a single resource (the actor) to be processed serially. In the case of Akka this is done by putting messages into the actor's mailbox in a thread-safe way. To visualize, imagine a black box that somehow converts the streams of work units from my parallel example into the stream of units from my concurrency example then then executes them one at a time. That's actors. However people frequently use actors, a tool for managing concurrency, trying to achieve parallelism. More often than not any parallelism they're getting is because of the underlying thread pool being able to run multiple actors simultaneously and has nothing inherently to do spawning multiple actors. Multiple actors could just as easily run on a single thread and process one message before another actor gets to process one message, much like my concurrency visualization. Put another way, actors inherently give you concurrency and they do not inherently give you parallelism, but many people either don't understand the distinction or think they give you both; they conflate concurrency and parallelism.
They're probably talking about how actors are for concurrency, not parallelism. See [my other comment](https://www.reddit.com/r/scala/comments/905eo7/what_are_the_technical_and_business_reasons_to/e2p3rvw) for more detail.
Enrich my library pattern. Is what I've heard it being referred to in talks. But honestly, we should just call them extension methods. They have practically the same semantics.
Ah didn't notice the link, thanks!
It's called the [bedazzler](https://www.youtube.com/watch?v=9m766NNpwVg&amp;t=4) pattern. Don't be dull, be dazzling!
[The Python Paradox](http://www.paulgraham.com/pypar.html)
It sounds like you want something like val names: Seq[String] = all.flatMap { p =&gt; p.name +: p.children.flatMap { c =&gt; Seq(c.property1, c.property2, c.property3) } } 
Other way of doing it is like that, remember you can define variables in for comprehensions: for { p &lt;- all parentName = p.name // you can define variables in for comprehensions child &lt;- p.children } acc = acc ++ Seq(parentName, child.property1, child.property2, child.property3)
Perfect. That's the clean solution I was looking for!
/u/tpolecat! I see you on this sub a lot. Hi again! Thanks for your comment, but I feel it's stylistically cleaner to use a single for-comprehension here.
&gt; but the /u/tpolecat is more idiomatic and I would suggest you using his way Really? I thought that the preference was to use for-comprehensions whenever you have nested map/flatMap/forEach &amp; other collection operations?
\&gt; thought that the preference was to use for-comprehensions whenever you have nested map/flatMap/forEach &amp; other collection operations? I agree with you here, that is true, but in your specific case you end up needing a mutable accumulator and it's more preferable to not use mutable state, and minimize side effects , the example from /u/tpolecat is more consistent regarding this and can be improved further like with concurrency without resource to synchronisation or other concurrency primitives. Just to be fair, I don't say the way I suggested is "wrong" like you can use mutable state, but things tend to be easier if you don't use it :) But in this specific case, it's up to you, if you like this one more, please go ahead, no judging haha :)
If you want an alternative, what about some small one-liner functions for clarity: def enumerateChild(c: Child) = c.iterator.toSeq def enumerateParent(p: Parent) = p.name +: p.children.flatMap(enumerateChild) def enumerateAll(a: All) = a.all.flatMap(enumerateParent) Note: I have not tested this code. Side note: why All and not just a naked Seq[Parent]?
Offhand I don't see a better way to do it.
I kinda like "pimps" or "pimp my library" :). I saw people using "extension methods" when discussing Dotty, so I guess most people will understand what it is.
I don't indent to transform this thread into a "a vs b", for me it's much faster to build an app in Spring with Spring Boot. And I can use reactive programming , FP in Java when needed and async with CompletableFutures. I also like the documentation, the community and tooling. I know that FP in Scala is way better. &gt;unreliable debug information because of the endless hidden hoops. I remember one time when I was getting an unexpected null value from db in Slick and the error stacktrace was not helpful at all, it didn't point to the right repo. I lost like 2 hours trying to figure it out what was the value in what table. I was mapping the table to a case class.
I played a bit with vert.x and it's quite cool. Picture this: new job, new programming language, never really done FP, new team, all members without FP knowledge, had to build an API. Did some research about what framework to use, asked other colleagues in the company. Some of them with more then 10 years in Scala. Ended up using Play. At first I like it, async up to bottom. I wont be using Play again :)
Fp in scala for Mortals, Chapter 3. By Sam Holiday - https://leanpub.com/fpmortals 
You welcome ;)
As a starting point, I liked this article: [https://softwaremill.com/free-monad-or-tagless-final-pres/](https://softwaremill.com/free-monad-or-tagless-final-pres/) It compares traditional approaches to Free and Final Tagless. First learn them, then take a look at [https://github.com/atnos-org/eff](https://github.com/atnos-org/eff) to understand how it tackles some of their issues. (there is also nice talks about eff on youtube, easy to find)
I will look if it doesn't follow the "fluent" testing fad. 
Udash uses a lot of macros but these are all IDE-friendly _blackbox_ macros - IntelliJ does not need to be able to expand them, it just needs their signature. So it's surprising that you have problems with them. However, it's true that IntelliJ has problems with wildcard imports that bring many things into scope (e.g. `import io.udash._`). 
I've come to realize that it's often very desireable to split your play project in many small projects: Often big parts of the project don't actually have anything to do with play (db layer, webservice calls, business logic, ...). I make those separate projects, which compile very fast. This helps a lot with compilation times - and also helps me structure the code.
&gt; Also, I'm sure you're aware, Play does have it's own Routing DSL that you can write in Scala if you need to do more complex things. It's probably not any easier or harder to learn than http4s's DSL. The akka-http DSL (which I think is the same thing you're talking about?) is just as nice as http4s's DSL (indeed I'd say it's nicer, since it doesn't rely on unsafe-looking `case`). But the point isn't whether I have to learn another Scala DSL (fine), it's whether I have to also learn a non-Scala config file syntax that will break all my tools. E.g. "find references" in my IDE works perfectly with http4s or akka-http or Bob's Novel DSL That We Only Use At This Company, but won't work with Play routing without some specialised support.
&gt; For example let's say you have a Controller that has a Service as a dependency and the service has a http client and a database access layer. &gt; &gt; How do you test this structure in FP way? Ordinary functions and parameters. The constructor for the Controller should take the Service as a parameter. The Service should implement an interface. If I want to test with a stub of the Service, I call the constructor passing in a stub implementation of the same interface. 
IIUC when you use a router, you introduce parallelism
Idiomatic code is as simple as possible, but no simpler. `for`/`yield` is great for hiding away a chain of multiple `flatMap`s inside each other, but when the logic is a bit more complex then it's better to have the details visible. (It also acts as a red flag to a reader that this is a complex case, because if it were the simple case then you would have used a `for`/`yield`). It seems like you really are doing two different things for parents and children, in which case two different `flatMap`s seems appropriate. If this was a generic tree datastructure I'd suggest using matryoshka and a catamorphism, but when you have exactly two distinct levels that you make an explicit distinction between, it seems like the code should reflect that.
&gt; In my experience at my workplace, even something as simple as picking a formatting rule in scalariform leads to divergence and wasted time. This is why I prefer prescriptive languages like Elm. Though, I will note this lack of prescriptiveness is an issue I see that not only lies with Scala. I agree with you that this, at first sight, may not be a good reason to use Scala if you take it literally. But the flexibility of the compiler, the metaprogramming facilities and the tooling in general allows you to create your own subset of Scala, restricted to your codebase, that works just right to how you want it to work. This is a huge selling point because in my experience all companies need some degree of customization of the language to shape it to their culture and the use case they are trying to solve. The fact that you can have linters for specific operations (in a type safe way) or custom DSLs or custom compiler plugin phases opens up a big array of possibilities for downstream users. And Scala does allow that without the need to extend the compiler itself (like other people with the same need do with languages like Rust or C++, to name a few). 
Well, classical OOP (as done by most Java devs) also implies, that data and functionality must be bundled. And doing otherwise is discouraged and regarded as an antipattern. That is, you should not have data-holding classes and then (pure / static) functions working on these data, but always have some class that has behaviour. I always ask what the reasons are to *force* the bundling data and functionality and I never got any convincing answer.
Make use of the recommended compiler flags: [https://tpolecat.github.io/2017/04/25/scalac-flags.html](https://tpolecat.github.io/2017/04/25/scalac-flags.html) That will disallow some of the language features that will go away with Dotty / Scala 3
What about the following to get rid of that nasty mutable reference? for { p &lt;- all parentName = p.name // you can define variables in for comprehensions child &lt;- p.children item &lt;- parentName +: child.productIterator.toSeq } yield item 
Exactly what I did. But maybe that is not the FP way. Although I don't understand all this hate 😀
Yeah, that import bug is quite infuriating, since it is probably as basic from language perspective as it can get. &gt; Udash uses a lot of macros but these are all IDE-friendly blackbox macros - IntelliJ does not need to be able to expand them, it just needs their signature. So it's surprising that you have problems with them. Looking at the ticket now (which JetBrains has been ignoring for almost a year), I am not sure that it is a macro issue, might be a problem with implicits - https://youtrack.jetbrains.com/_persistent/idea_bug_scalacss.png?file=74-391516&amp;c=true&amp;rw=867&amp;rh=807&amp;u=1489080278975. Nevertheless, point with very incomplete Scala support in IntelliJ IDEA stands, because even trivial usage of Shapeless (mapping of tuple) leads to broken IDE. Also JetBrains doesn't seem to care that much about Scala - I have opened other 4 issues with Scala (with various complexity, some looks like basic lang. usage) which all are ignored, most of them months old without any response. I thought that they may be providing better support for paid plugins, but that's not the case either - have issue with JavaScript where all imports are broken, so IDEA provides almost no autocompletion or checking, is crippled to levels of free editors. It's now three months and they started reacting only after I started complaining on social media. Have similar experience with other issues as well - ignoring my bumps and questions for months and when I spam on social media then, suddenly, in a matter of days or even hours someone starts reacting. I am not satisfied with their customer service at all. I might even jump ship if other company comes with similarly "intelligent" IDE.
You might want to look the approach that I built for [playsonify](https://github.com/AlexITC/playsonify), the idea is simply to extend the [FieldOrderingParser](https://github.com/AlexITC/playsonify/blob/master/playsonify/src/com/alexitc/playsonify/parsers/FieldOrderingParser.scala) specifying the expected type and you get the validations, for example, I use the [FieldOrderingSQLInterpreter](https://github.com/X9Developers/block-explorer/blob/master/server/app/com/xsn/explorer/data/anorm/interpreters/FieldOrderingSQLInterpreter.scala) which uses the typeclass pattern to map the condition to SQL, this is an implementation: [BalanceOrderingParser](https://github.com/X9Developers/block-explorer/blob/master/server/app/com/xsn/explorer/data/anorm/interpreters/FieldOrderingSQLInterpreter.scala).
I found this presentation on the subject of finally tagless quite interesting https://youtu.be/1h11efA4k8E
The FP way is: everything in plain old code. Plain old functions, plain old values. The reason we do all these complicated things with monads and typeclasses and all that is so that we can use plain old code instead of reflection, global variables, or whatever kind of hidden magic. The whole point of the exercise is that you should be able to maintain any Scala codebase only knowing Scala; you should never have to learn anything specific to how some framework works. You might not know what a given library function does, but if you know it's a normal function then you can still safely refactor code that uses it (e.g. if you see two calls with the same parameters, you can replace that with storing the return value and reusing it, even if you have no idea what the function actually does). Reflection-based DI breaks that. You can't find all the places an object gets constructed, because your "find references" tool doesn't know about the DI framework. You can't rely on the object being in a valid state when you construct it, because it might rely on some "lifecycle method" like Spring's `afterPropertiesSet()` getting "magically" called by the framework. Of course you can avoid using those features, but it eats into your discipline budget. And if all your DI framework is doing is instantiating a bunch of classes passing previously-instantiated classes as parameters, is there really any value in that? I'd rather do it in plain old code where I can see what's going on. If you really can't stand to explicitly pass the fooService to the fooController you can make it an implicit parameter, but I take the view that explicit is better than implicit: if we're changing the fooController so that it also uses the barService now, I'd rather see that change in the PR diff, `new FooController(fooService)` -&gt; `new FooController(fooService, barService)`, than have it magically resolved in a hidden way at run time (or, if there isn't actually a barService available, fail at runtime instead of compile time).
&gt; all my code will be monadic I personally don't see a problem with this. We use Free algebras through a majority of our codebase, at a low-level over a co-product of algebras that model the different effects required for implementation, and at a higher level modeling specific domains, targeting that lower level. The vast majority of our code exists in for-comprehensions or applicative combinations. Where we have complex logic, we put in extra effort to extract "pure" functions for ease of testing, but the truth is that it's trivial to test even the monadic code using a substitute interpreter for our lower level algebras that just threads generated values through. Ultimately when you define your programs in terms of a free algebra, your code is all pure anyway. I think it's fine to use everywhere. If you're worried about type-plumbing adding noise to a codebase, there's ways to mitigate that. I think Freestyle aims towards this, and includes support for both Free and Tagless. [freek](https://github.com/ProjectSeptemberInc/freek) accomplishes it for Free, trading a little bit of verbosity for added flexibility in program composition.We use our own solution [composefree](https://github.com/mblink/composefree) which commits more heavily to a given source algebra in exchange for near-zero type plumbing noise, and gives you out-of-box support for arbitrary nesting of free monad/applicative based programs.
I'm trying to answer some of the same questions as I use some of this stuff in actual projects. I started to brain dump some things I've figured out so far, but I'm on mobile, and I quickly lost the thread of what I meant to say. Posting what I wrote in the hopes that there's something of value in there. *There's an extremely strong possibility some or all of this may be completely inaccurate. These are the thoughts of someone in the same position as you stumbling around trying to figure things out.* To address one tiny part of your question: I'm playing with keeping the core domain logic pure using techniques I got from articles about hexaganol architecture. If the core domain needs to, say, make http requests, then I might do something like: trait Requestor[F[_]] { def get(url: URL) : F[Response] } class Http4sRequestor[F[_] : Effect] extends Requestor[F] {...} class MockRequestor[F[__]] extends Requestor[F] {...} I'm still not 100% sure about this approach. It adds a lot of boilerplate, and reduces flexibility. (You have to use libraries through a narrow interface.) It does mean that during testing, or for any other reason, I can swap in a pure version of Requestor, and that most of my most important code isn't bound to IO or a particular monad stack. In general, I'm finding deferring specifying the context an algebra uses to return values extremely powerful. F is not constrained at all in the base trait, and even concrete requestors do not specify a concrete F. (Given F[_] : Monad, I can assign val X = Monad[F] and use X.flatMap and so on to access all the Monad-y goodness without knowing what the implementing Monad will be.) If you have a set of classes that accept some types with different constraints, you can define a custom typeclass that mixes together typeclasses that fulfill those constraints. For example, imagine I have: class Http4sRequestor[F[_] : Effect] class CryptoFunctions[F[_] : MonadError] ... class Main[F[_]](r: Http4sRequestor[F], c : CryptoFunctions[F]) To make Main work, I could define a custom typeclass instance that mixes Effect and MonadError: trait MyMonad extends Effect with MonadError //not actually this simple In general instead of thinking in terms of classes and whatnot I try to think in terms of: * Functions * Contexts (Holds value. Adds effects when you apply functions to this value -- for example Option adds an existence check to any function I map in). * Algebras (defines the shape of a set of functions that relate to each other + laws governing these functions) * Interpreters (inaccurate over-simplification: an implementation of an algebra) The functions that make up an algebra should focus on the core problem they exist to solve. Concerns like dealing with errors, absence, values that do not exist and so on should be pushed to contexts. Example: abstract class SomeInterpreter[F[_]] extends SomeAlgebra [F](implicit F : Async[F]] { // This function does explicity deal with asyncronicity, but most of the messy details are handled by the context def getFoo : F[Foo] = F.async { callback =&gt; callback(longComputation.asRight) } //Notice how focus is on the important //high-level logic, not async mechanics def someLogic : F[Boolean] = getFoo.map{ case foo if foo.bar &gt; 1 true case _ =&gt; false } } } actually gluing things together: // notice how program, again is mostly free from boilerplate dealing with non-core concerns. You operate at the algebra level, passing in a specific fulfillment with supporting contexts at the last possible moment //Monad constrain only necessary because we want to execute functions sequentially //You could, for example, use Applicative here to build a program that just fires off a bunch of commands def program[F[_] : Monad](i :Interpreter1[F] with Interpreter2[F]) = for { data &lt;- i.fetchData r &lt;- i.analyze(data) cond = (r &gt; 1).pure result &lt;- cond.ifM(i.computation1, i.computation2) } yield result program[MyMonad](new CombinedInterpreters[MyMonad]).runSync 
&gt; However, would this not mean that basically all my code will be monadic, complicating type signatures and hampering readability? I understand that it's bad to have IO littered all over the codebase and I'd guess having Free[T] (or an alias thereof) littered everywhere would be equally undesirable. http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html might help? Your domain-action code will be monadic (or, from another perspective, imperative), but performing actions should be a small proportion of your code: most of your business logic should be pure, well, logic. &gt; Writing this, I kind of realize that it's probably smart to only expose a sort of world-facing API as such algebras and compose the inner logic out of simpler functions. Intuitively I'd say such an API is by definition effectful. Am I way off here? It's normal for the outermost layer of your program to be effectful - your program eventually does something, after all. But ideally you'll have a thin imperative shell around a functional core, and your domain algebra is a way of shifting effectful operations out of the core. Layer 1 is outright imperative, layer 2 is your domain action algebra which, like any monadic code, you can equally validly view as effectful procedures or (somewhat more complex) pure values, depending on what questions you need to answer about it. But ideally most of your code should be right down in layer 3 where it's all pure.
I only have a couple of comments: &gt; However, would this not mean that basically all my code will be monadic Yes. &gt; complicating type signatures and hampering readability? No, but this is really an observation about familiarity, not anything more objective than that. At this point (after doing pure FP in Scala for the last six years or so), I can't readily read _non_ monadic code! First, I assume we're talking about a REST API today, in which case, as far as I'm concerned, we're talking about [http4s](https://http4s.org/). In http4s, we want to write an `HttpService[F[_]]`, which is a type alias for `Kleisli[OptionT[F, ?], Request[F], Response[F]]`. Usually you use some [syntax support for building `Response`s](https://github.com/http4s/http4s/blob/v0.18.15/dsl/src/main/scala/org/http4s/dsl/impl/Responses.scala), so you can just say something like `Ok("This is a perfectly good plain-text response.")`. But thanks to some cleverness with `EntityEncoder`s, such as [this](https://github.com/http4s/http4s/blob/v0.18.15/core/src/main/scala/org/http4s/EntityEncoder.scala#L185-L191), you can also say something like `Ok(IO("This is a perfectly good plain-text response.")`. The latter is a bit silly, of course, since the "effect" is a pure value. But the point is you can replace it with whatever effectful computation you like. So this is where your algebra and `Free` or finally-tagless program come in. Personally, I'm pretty drawn to [`Diesel`](https://github.com/lloydmeta/diesel) for defining finally-tagless algebras and, most importantly, their composition, with a minimum of muss and fuss. Just interpret into `F[_]` where `F: Effect[F]`, use that same `F[_]` to parameterize your `HttpService[F[_]]`, and things both read and work very well.
I would define a fold function for `All` and use that to compute your list of names. def foldMap[A : Monoid](all: All)(p: Parent =&gt; A, c: Child =&gt; A): A = all.all.foldMap(parent =&gt; p(parent) |+| parent.children.foldMap(c)) foldMap(all)(p =&gt; Seq(p.name))(c =&gt; Seq(c.name, c.property1, c.property2, c.property3))
This is in https://www.playframework.com/documentation/2.6.x/CompilationSpeed if that helps.
It has a modern type system: sum types (sort of), type inference (although limited) and no primitive types that need special treatment. It's hackable: Scala lets you take a lot of technical debt if the business needs you to do something quick and dirty, due to its weak treatment of effects. It has a sufficiently advanced IDE: Type-based auto-completion, go to definition and find usages. It has a production ready JavaScript backend. That means you can use Scala for essentially everything in most businesses. 
You do not. A router is a way to distribute messages among a set of actors. It doesn't give you any more execution resources. Those actors may still be running on a single thread.
I just finished putting together a finger-like tool using the GitHub API (and Scala, of course!): blog post: [http://codeninja.blog/2018/github-finger/](http://codeninja.blog/2018/github-finger/) web page: [http://finger.codeninja.blog/?q=massung](http://finger.codeninja.blog/?q=massung)
is there a list of such features somewhere?
Yeah, fuck play basically.
[Parallel and Concurrent Programming in Haskell](https://web.archive.org/web/20171014080407/http://chimera.labs.oreilly.com:80/books/1230000000929/ch01.html#sec_terminology) has a pretty good explanation: &gt;In many fields, the words parallel and concurrent are synonyms; not so in programming, where they are used to describe fundamentally different concepts. &gt; &gt; &gt; &gt;A parallel program is one that uses a multiplicity of computational hardware (e.g., several processor cores) to perform a computation more quickly. The aim is to arrive at the answer earlier, by delegating different parts of the computation to different processors that execute at the same time. &gt; &gt; &gt; &gt;By contrast, concurrency is a program-structuring technique in which there are multiple threads of control. Conceptually, the threads of control execute “at the same time”; that is, the user sees their effects interleaved. Whether they actually execute at the same time or not is an implementation detail; a concurrent program can execute on a single processor through interleaved execution or on multiple physical processors. &gt; &gt; &gt; &gt;While parallel programming is concerned only with efficiency, concurrent programming is concerned with structuring a program that needs to interact with multiple independent external agents (for example, the user, a database server, and some external clients). Concurrency allows such programs to be modular; the thread that interacts with the user is distinct from the thread that talks to the database. In the absence of concurrency, such programs have to be written with event loops and callbacks, which are typically more cumbersome and lack the modularity that threads offer. Of course, they are very closely related, because you often use concurrency in order to \*enable\* more parallelism. For example, when you write `a + b + c + d` (assuming integer arithmetic without overflow checking), that is not a concurrent program, but whether or not it executes in parallel is an implementation detail. A modern superscalar CPU would first execute `a + b` and `c + d` in parallel, followed by adding the intermediate results (also executing other instructions in parallel if it can). You can have parallelism without having to deal with concurrency. Alternately, you could write a concurrent program and even if it is executed completely sequentially, you still have to be careful to avoid introducing race conditions. (Because you don't know what order the code will execute in.) Purely functional programming helps a lot here, because if you have no writes, then you never have to worry about the ordering of writes. :-) The line between whether a program is concurrent or not can be a bit fuzzy though. For example, one might argue over whether a monadic program is concurrent or not (particularly when using the [Future monad](https://stackoverflow.com/questions/27454798/is-future-in-scala-a-monad) ). I haven't used Akka in a long time, but I can't imagine it's moved away from being very explicitly concurrent. (Though using it "right" does help you avoid most of the problems that typically come with concurrency.)
This is an interesting question. But I'm curious, why did you post it to /r/scala specifically, rather than somewhere else?
In the age of the Internet there exist such thing as phone interview? Wow.
The name `mtl` is an historical accident, at this point. The `mtl` library in haskell used to provide the concrete monad transformers types, which are now in `transformers`. What `mtl` is now is _ a final tagless encoding of common effects_: like state, reading from an environment, and so on. The term `mtl-style` is used to refer to an application of `final tagless` to encode effect, since `final tagless` is a more general term for a technique to encode DSLs (the interpreter pattern). I have an in-depth explanation of what `final tagless` means here, https://gitter.im/scala/scala?at=5a97f40de4ff28713adf49d7 and an example repo using final tagless + transformers to write a complex interpreter here, https://github.com/SystemFw/Befunge-93 This days, final tagless abstraction backed by fs2 + cats-effect implementations is my go-to technique for both business logic and custom concurrent abstractions, it's really nice. Btw, the article you were missing in Haskell is: https://blog.jle.im/entry/mtl-is-not-a-monad-transformer-library.html 
I would suggest you start with final tagless rather than Free, since: - Composing multiple algebras is easier. - It's easier to model things that are more specific than Monad (e.g. MonadError). Using any kind of interpreter pattern is a really, really powerful means of abstractions, it's worth investing your time there. &gt; However, would this not mean that basically all my code will be monadic, complicating type signatures and hampering readability? I'll flip this on its head: all of your code will be made of first-class _programs as values_, which you can operate on not just monadically, but with many other principled and powerful abstractions like Applicative or Traverse, as well as store in data structures, manipulate, and so on. This is a powerful concept, I have written down some of its advantages recently https://www.reddit.com/r/scala/comments/8ygjcq/can_someone_explain_to_me_the_benefits_of_io/e2jfrg8/ &gt; I understand that it's bad to have IO littered all over the codebase and I'd guess having Free[T] (or an alias thereof) littered everywhere would be equally undesirable. No, that's not correct :) The reason why is good advice to not have `IO` littered all over is not to avoid monadic code (and btw having `IO` all over is _a million times better than side effects_, as I explain in the link), but because `IO` is too powerful, it can do too much, so a more constrained power in your functions makes this easier to reason about. Having `Free[ConstrainedLanguage, A]` all over solves this problem, since you only have the subset the your DSL has defined. Final tagless is even more interesting in this regards, because even though your whole program will eventually be in `IO` (when you instantiate your `F[_]` to `IO`), all the individual functions are still constrained, since they only see `F[_]: DomainSpecificAlgebra1 : DSL2]` 
Thank you so much for the detailed explanation! I'll take a look in depth at the links provided for a better understanding of final tagless implemented.
I'm working on a bitcoin n-versioning system: https://github.com/lktkorg/bp2p the intention is to connect to the bitcoin cash or bitcoin core network and then relay messages to a set of client nodes developed by different dev teams. Then act if there are discrepancies found between them. The bitcoin network needs all of its nodes to react the same way in order to maintain consensus. The bitcoin core development team justify their monopoly due to this. this is why bitcoin core is the client used in 99% in the network. In bitcoin cash this is not the case there are two prominent dev teams that have a 70/30 ratio share, plus other minor ones. The intention of this software is to avoid potential forks if and when discrepancies happen while maintaining a diverse ecosystem in clients. Right now im struggling with FS2 and wondering if its the right tool. I will look into Monix as well
I also have the entire messaging codecs for the bitcoin network, there are some open issues there if someone is interested in contributing. The scala ecosystem is in dire need of making a headway into this industry. and not let Go, C++ and rust keep everything
Personally, I'd rather my codebase be littered with well understood monads than with custom types exhibiting unknown properties :). 
I don't think significantly more people confuse concurrency and parallelism in the context of actors than they do in any other context. At least, from what I've seen in the Akka and Erlang/Elixir communities, people are usually very clear that actors are for concurrency and you need specialised tools on top of them for parallelism, e.g. Akka Streams or Elixir Flow.
I usually call it static monkey-patching.
&gt; Although switching to other DI methods seem trivial. It's not. The built-in DI is a bit of a spider-web, and since Play is a framework, not a library, it expects various things to be set up which are not quite clear. Removing all the DI and trying to go manual involves quite a lot of work threading dependencies manually through the app.
Agreed. Tpolecat is awesome and a good example of someone from the pure FP camp with a good demeanor
Thanks for the info that they have switched to DI from Cake. Makes play-framework worth another look for me.
I’m joining your protest. I demand a telephone interview too!
You can use the same tooling (maven, idea...) and if you use Scala without the most advanced features, idea works just as good.
Interesting, especially the decision to make a synchronous HTTP client. The main two points seem to be simplicity (both in LoC and mental overhead). However, in practice the HTTP library does not add any. In the examples all of the "overhead" would already exist in your project (like ActorSystems, `unsafeAsync` etc). So at very little cost you gain async and/or pure by using the matching HTTP library. Seems useful in the context of scripting, but not something I would use in a continually running production system. 
Yeah, final tagless would have been my choice as well. It seems to be easier to grasp coming from a non-functional background, which probably would help with getting my co-workers on board. I've read about concerns of FT not being stack-safe, though, which as I understand might be mitigated by interpreting into an intermediary Free-representation. However I'm wondering if this usually is a problem in practice. I've used recursive functions a lot in Python (with doesn't have tail-call optimization) and I can't remember the stack-size having ever been a limitation for these applications. The point for final-tagless being more flexible by allowing tighter constraints than the Monad-interface is interesting. I've not seen this explicitly stated in other comparisons of the two approaches. Your answer to that other question I've already stumbled upon browsing around here :)
Thanks for suggesting diesel. I've been looking at freestyle, but am not quite convinced what either buys me when targeting tagless algebras. Any thoughts on differences between the two? TF seems to be straightforward enough to implement and combine as is. Yet while I'm kind of wary of heavy macro usage, especially in the early stages of building an understanding of and gaining practice with a new concept, I guess that in the case of Free I'd probably be using one of those libraries. 
I'm also working at a real tiny shop. Currently we're just three and a half developers. We currently build on OO Ruby and somewhat half-assed functional Python. We're enjoying a great amount of freedom in choosing our tools, though, and this might be the opportunity to maybe shift somehwat more towards typed FP. With Spark being an integral part of our business already, Scala seems like a good choice to make this move gradually, leaving no one behind. Having a convincing showcase to demonstrate the benefits of this approach would be great.
Thanks, that blog post turned out to be a very fruitful rabbit hole touching many of my concerns!
Thanks, that was very helpful!
huh, that is a good point :) I guess my question turned out to be a little more broad than I initially intended. However as Scala is the probable choice for me, I don't mind a somewhat scala-biased angle on this.
Thanks for the pointers to those libraries. Although in my experience trying to hide complexity behind heavy macros generally leads to inscrutable error-messages, especially when my understanding of that "plumbing" is yet somewhat incomplete and brittle.
&gt; I've read about concerns of FT not being stack-safe, though, which as I understand might be mitigated by interpreting into an intermediary Free-representation. Generally speaking you don't need this, because you would end up interpreting into a cats data type, or cats-effect IO, and we have gone through a lot of effort into making them stack-safe &gt; I've used recursive functions a lot in Python (with doesn't have tail-call optimization) and I can't remember the stack-size having ever been a limitation for these applications. You don't have to worry too much about this as an user, but as a library author (say someone that works on `IO` or `Kleisli`) this is a pretty big problem. Scala can't optimise tail-calls, but unlike Python mutual recursion is super common. For example in a chain of `flatMap` you have `flatMap` that calls a function which calls `flatMap` which calls a function. With a naive representation, something like traversing a List of 10000 elements with State overflows. But as I said, these days you should be fine as a user. &gt; The point for final-tagless being more flexible by allowing tighter constraints than the Monad-interface is interesting. I've not seen this explicitly stated in other comparisons of the two approaches. Yeah, it's actually one of the greatest strengths imho
I've wanted this for a while! Thank you
I can't speak for freestyle (I think it may provide annotations,) but I don't think freek is based on macros, and I can say for certain that composefree uses none in it's 150 LOC. It makes sense to want to understand the plumbing, but to your point of having IO or Free littered all over the codebase, I don't know of another way to handle that. For me one doesn't preclude the other, and on our team we don't really have the problem you describe wrt error messages. On the surface, the mechanics seem more obvious with the tagless final approach, so maybe that would work better for you. Personally I'm too attached to the level of reification that Free gives us to see tagless ever fully replacing it.
TLDR: responding to the arguments for monads and against effect systems, many of the objections in this thread apply equally to all effect systems, yet Conor McBride et al. at POPL (https://arxiv.org/pdf/1611.09259.pdf) and Paul Chiusano, Rúnar Bjarnason and Arya Irani in industry (http://unisonweb.org/2018-07-20/update.html) are working on them, respectively in Frank and Unison. That's because those arguments hold less sway than you think. They were pretty accurate when first made, but effect systems have progressed. They cannot do everything that monads can (continuations go beyond algebraic effects), but they're interesting in other cases. There are also differences between IFTs and algebraic effects, and we can talk about those, but that's a very different debate. And if you have an effect system, the distinction between side effects and "effects modeled via monads" becomes much less important. # Equational reasoning with effects There are different ways to consider these topics and represent programs with "effects or side effects". For brevity, I'll use effects for both. All those views account explain why `val x = readFromUser(); x + x` differs `readFromUser() + readFromUser()`, just in different ways. The essential point is that you don't duplicate/reorder *either* effects *or* side effects: with imperative programs you need to look at `readFromUser`'s implementation to really know they're there (or trust names and docs, if you have the immense discipline to keep them correct). Both effect systems and monads support equational reasoning: With effect system, you restrict inlining to computations without effects, and effects are visible in types; with monads, computations with effects are composed in a syntactically different way (do notation/monadic combinator), and inlining is not added for those. With monads, equational reasoning works purely on program syntax, while with effect systems case you need to consider types but can then do the same reasoning. With this summary, you could claim that people who like monads but dislike effect systems really dislike types — which can't be the case. The only issue I see is that access to types in IDEs/editors becomes even more important. The other issue seems that effect systems are *unfamiliar* — like monads to the mainstream — and not tested as much — which is indeed true. But we can't criticize something unfamiliar *just* for being unfamiliar! (See below for discussion) ### Scala 3 capabilities and IFTs vs algebraic effect systems What is IMHO a valid objection is that details about the Scala approach are scarce and open questions there abound, and some details are different. I agree with that argument. Some fear that you'll still need monads for anything nontrivial. But it appears many scenarios can be handled by algebraic effects, and Scala with IFTs seems raises to the challenge in early research (http://dl.acm.org/ft_gateway.cfm?id=3136007&amp;type=pdf and https://github.com/b-studios/scala-effekt), and to scale to more advanced scenarios. # Meta: Should we entertain the unfamiliar? IMHO, we can't progress here by just discussing the actual topic (the "object level" of the discussion). But we must also discuss about how to run these language-design debates (the "meta level" of the discussion), tho I realize that will bore many. So, a problem with effect systems is that they're unfamiliar to Haskellers (including Haskell-inspired Scala devs). But that's just like Haskell is unfamiliar to JS devs — we can't criticize the unfamiliar out-of-hand without engaging with it! But does it mean why must engage with everything new? Of course not, but we must be coherent — neither Haskell/Scala devs have a duty to engage with IFTs, nor JS devs have a duty to engage with Haskell. I strongly believe the same principle applies: we can't explore everything that is unfamiliar, but we should be open to it. Since we can't learn everything enough to evaluate it on its merits, we choose based on heuristics — even tho heuristics, by definition, are imperfect. And that's why I mentioned names at the top: I'm not making an argument from authority, but I'm appealing to a common and legitimate *heuristic* for whether to *engage* with an argument: *check who talks about X*. #### Meta: am I biased against Haskell? No If anybody is taking me for an anti-FP zealot: nope. I enjoy FP and Haskell and have programmed with dependent types in Agda and Coq. I just don't think there's some moral duty to use Haskell, and I'm very critical of overstated arguments — they don't help use get closer to the truth. ## Meta-meta: overwhelming evidence is an exception You need not engage with all new ideas. Sometimes people reject overwhelming evidence and consensus from experts on a topic — but there's no such consensus in programming language and software engineering. In fact, you could joke there's more robust evidence in favor of parapsychology (even tho it is bunk — http://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/) than in favor of, e.g., static type systems *for correctness* (https://danluu.com/empirical-pl/) or other claims by FP advocates. Mathematical facts are still facts, but theorems never tell you some process or language is more correct.
I get that you wanted a very simple API to interact with, but I have two questions: Why have it throw exceptions when the request fails? Why synchronous? The questions are because this design decision makes this library look much better for scripting than actual production applications. This may be intended and there's nothing wrong with that. Just trying to understand. 
Are you responding to the right comment? Because I made zero criticism of effect systems in mine, and much less said that we shouldn't research them (in fact I've been following work on Frank for a while, like most PL things that come out of my former university). &gt; So, a problem with effect systems is that they're unfamiliar to Haskellers (including Haskell-inspired Scala devs). But that's just like Haskell is unfamiliar to JS devs — we can't criticize the unfamiliar out-of-hand without engaging with it! Exactly! And it appeared that `IO` was also not super familiar to some, judging from the factual mistakes I tried to correct in my first comment (for example that it's a marker for non referentially transparent code). All I did in my second comment was presenting the arguments for `IO` from a Haskell-style FP perspective, which was the original request from OP, trying to substantiate them with concrete examples, which include code. Whether these arguments hold enough weight or not I explicitly left to the reader, exactly because there's no consensus in language design. -------------- P.S. Matter of fact, I explicitly left out any discussion about IFT or effect systems, and focused on traditional side-effects vs `IO`, to avoid muddying the waters. 
Honest question from a noob, whats wrong with throwing an exception for a request failure? How are you supposed to do it?
It's not that exceptions are "wrong" necessarily but if you're doing FP then there are "better" solutions to use such as Either[L, R]. That way you can model for the happy path and defer error handling to the edges of your program by chaining together computations or operations as if nothing is going wrong, and then declaratively specifying what to do if something goes wrong in the chain somewhere. It's what I'd personally do as I'm a big fan of FP but being this is Scala you can do whatever you want. It's a style thing mainly.
[Why is this @throws even here lol](https://github.com/scala/scala/blob/4d39d4f849cbab5d6ad61d74e07aba41c08c6340/src/library/scala/collection/StringOps.scala#L804)
 def x = List.fill(10); x(1) how to write that so it works?
&gt;That way you can model for the happy path and defer error handling to the edges of your program by chaining together computations or operations as if nothing is going wrong, and then declaratively specifying what to do if something goes wrong in the chain somewhere. To be fair, that's also exactly what exceptions are for.
While that's true, I more meant it in the sense that exceptions expect to be handled somewhere upwards of where the function is called, whereas Either, being a monad, allows you to push errors forward to be dealt with later as raw values. I'm not the best at explaining it TBH. I think this article does a way better job of explaining the benefits of this style of error handling: https://fsharpforfunandprofit.com/rop/
Diesel vs. Freestyle: I mostly think of Freestyle as providing monadic APIs for a bunch of popular libraries, in addition to providing some of the same boilerplate-reducing conveniences of Diesel. Freestyle also provides the same support for free applicatives, which may be important. I think of Diesel as being much more narrowly focused: it's _only_ for finally-tagless encoding of monads. Beyond that, all I really see it doing is automating the grunt work of defining the various smart constructors for the derived monad, but that's convenient enough to appeal to me, I guess.
If you want asynchronous, you can simply wrap the API call in a \`Future\`. If you want exception purity, why can't you simply wrap it in your \`Try\` or \`IO\`? I don't understand the trend of Scala library authors having to suffer the burden of wrapping it in everyone's favorite monad. 
Thanks for your answer, and for clarifying the open questions. I wasn't sure where to hang my comment, *but* here's the question. It seems that most (or all?) "factual mistakes" (quotes motivated below) are about side effects vs "effects". Is that distinction still important with effect systems? To me, that hinges on how you want to do equational reasoning, and that's what my comment is about. Also, again on the "factual mistakes", re-reading the comments and having talked with Martin, I guess when he talks about "impure" and "side effects" he's talking about the original non-monadic programs, such as `val x = readFromUser(); x + x`, where `readFromUser` is an impure side-effectful non-referentially transparent function. If you translate that to Haskell, its translation `ioReadFromUser` would be marked with `IO`. But the translated program has the same runtime behavior, once you account for the IO runtime. The distinction does matter in *another* scenario: when you have a Scala program with *both* effects and side effects. This reminds me a bit of incommensurability in scientific literature [as explained by Dick Gabriel](https://www.dreamsongs.com/Files/Incommensurability.pdf) — Gabriel talks about "attempts to understand the apparently nonsensical passages I encountered in that old scientific text", due to incommensurability, that is a different way to describe the same phenomenons. This is a much smaller incommensurability/misunderstanding, but the difference seems just gradation.
Throwing exceptions breaks parametricity and referential transparency. Parametricity is being able to tell the possible implementations of a function based purely on the types of the function's paramaters and being able to tell what it can't do. Referential transparency is being able to replace the call to divide with its body and get the same behavior out of your program. Let's take division. Scala doesn't have a Numeric type for division, and this example will demonstrate why that is, so we'll use Double. def divide(dividend: Double, divisor: Double) : Double The above function cannot be implemented. Why? Because in the case of the divisor being 0, you cannot encode the return as a Double. You might return infinity, but that is not a member of Double. You might be tempted to throw an exception, but then your function would return Nothing, which isn't really a Double. You might return 0, but then anything divided by 0 would be the same as 0, which it isn't. The above method definition forces your types to lie, which is confusing to people reading your code. The naive implementation of divide for the above method definition is to throw on 0, because the average programmer will just delegate to double division, and that throws. Your type signature doesn't warn you about this, so you might not handle exceptions for the throwing. This can be dangerous, and cause your program to crash or behave unexpectedly. It really depends on how you use the return and what try/catch blocks you have in the call stack surrounding your call to divide. This makes analysis difficult. For division, common sense tells you that you could throw, but what about calling some other external java library that internally throws deep in its call stack, and doesn't handle the error? You might not know about it till it costs you money in production. Anyway, we're going to stick to division because it is well-known and easy to demonstrate. A better encoding might be to return an Either around your return: def divide(dividend: Double, divisor: Double): Either[Throwable, Double] This is better, because you are announcing that it may fail, but it still breaks parametricity, because you aren't giving a type to the function that says it can catch exceptions. So you're limited to returning a Right and throwing anyway, which also breaks referential transparency. To call out that you can, and do, catch exceptions and that the method can fail, you need to place a restriction on Either: def divide(dividend: Double, divisor: Double)(implicit applicativeError: ApplicativeError[Either[Throwable, _]]): Double The above won't compile, because you need a type type constructor for Either[Throwable, A]. You can create it easily: type ThrowableEither[A] = Either[Throwable, A] def divide(dividend: Double, divisor: Double)(implicit applicativeError: ApplicativeError[ThrowableEither]): ThrowableEither[Double] There are ways to declare it inline with type lambdas or compiler plugins, but declaring the type uses fewer moving parts. Now you have a type signature that tells callers that the method can fail, and you are allowed to catch failure in your implementation and you can encode the failure and result in your return type. You can also use Try, but that doesn't declare that it can side effects, breaking another rule of parametricity. ApplicativeError allows the type passed to it to handle side effects, as it is also an Applicative. But you can go farther. You don't really need to specify at the definition site what the type of the wrapper is. By including that type information, you are adding to the number of possible implementations of divide. You can call lots of things on an Either. Ideally, you should make the number of implementations smaller: def divide[F[_]: ApplicativeError](dividend: Double, Double):F[Double] Now you are restricted to calling the methods on ApplicativeError, which means you can't be tempted to do any funny business in your implementation. Scala's Numeric doesn't include divide, because it can't be implemented for all numeric types, because of errors and because it isn't clear when and what rounding to use for precision on the Numeric types in scala without declaring them in a method signature that would be more complicated like the final one above, and standard library signatures are generally simple. You are trading off complexity at the definition site for simplicity in the safety analysis of your program when you do this. I am of the opinion that it is worth it, but not everyone agrees that having as strictly defined programs as possible at the type level is worth the tradeoff in training and learning required to achieve programs which don't lie in their types. Uncaught exceptions frequently cause errors in production, and are trivial to prevent, so it's a case where I'm usually willing to pay the price. The same is true of unconstrained IO, so I pay the price there as well. ApplicativeError is available in cats. Hope this helped. Any errors will be corrected by replys from fellow scala programmers, I imagine. To make divide completely parametric, you need to make Double an A and provide Rounding rule restrictions and Precision restrictions on A. It's overkill for the error handling example, but you could still do it. 
I'm honestly not sure how to unpack it better than my first comment. &gt; It seems that most (or all?) "factual mistakes" (quotes motivated below) are about side effects vs "effects". Is that distinction still important with effect systems? It's definitely important to understand what the point of `IO` is, which is what the discussion is about. I'm still unclear on how to answer that question with effect systems, because obviously it depends on the specific effect system, and because I didn't see yet a treatment of them from the point of view of ref.trans. , especially one that tries to motivate them like my second comment. &gt; Also, again on the "factual mistakes", re-reading the comments and having talked with Martin, I guess when he talks about "impure" and "side effects" he's talking about the original non-monadic programs, Can't comment on your offline interaction ofc :) but the text of one of the comments is: &gt; The other argument for going to monads and IO is that it makes composition of effectful code more cumbersome than composition of pure code. So some people like this for the educational aspect: Using effects should be painful, so that you are pushed towards referentially transparent solutions instead. I understand the point of view but don't subscribe to it. A good analogy is var and val And it's clear that with effects is talking about `IO` and saying that it's not a referentially transparent solution (note the word "instead"), which is incorrect. And in fact `var` and `val` is _not_ a good analogy, because from the point of view of referential transparency there is no difference between `String` and `IO String` &gt; If you translate that to Haskell, its translation ioReadFromUser would be marked with IO. But the translated program has the same runtime behavior, once you account for the IO runtime. That's irrelevant. It does not have the same behaviour with respect to substitution, which is what referential transparency is about, and which matters (to me) because of the reasons I tried to substantiate in my second comment. 
Now imagine that same answer if it were java: "You can use eclipse as long as you don't use anonymous inner classes, ah, in IDEA you can't use default methods at all, also refactor is kinda wonky in any IDE, it will usually leave symbols un-updated"
Love it. This could be what actually makes is feasible to create Scala scripts. I don't care about actor systems and other ceremonial crap. Just let me get an url. Finally.
Nice. For me it looks like this library is conceptually more locate on the "Kotlin side" than on the "Haskell side" of Scala. So it looks like it's a nice addition for the Scala ecosystem on a side where it's really needed: the pragmatic, engineering and scripting side!
Beta
As somebody who doesn't do FP but trying to understand what the real benefits are, I read your answer in awe. Are there people who do FP professionally and have the time to think about all these constraints without sacrificing speed of development? That's an honest question btw, I'm really curious. 
There is also: https://github.com/scalaj/scalaj-http for people looking for a simple Http Client written in Scala (not the author).
It doesn't matter if you are doing FP or not. You still need to think about what values your functions can act on, and what they can return.
Of you want it nonblocking, there's nothing at all you can do when it's written synchronously. Wrapping in Future isn't enough
Thinking about these makes development faster because you catch bugs while writing code instead of after a production deploy. After a while it becomes second nature and takes little effort. Flipping your statement around, Are there people who do imperative code professionally and have the time to think about all the ways their code can fail with no computer assistance, without sacrificing speed of development? 
See the slide ‘Removed’ in https://www.slideshare.net/mobile/Odersky/preparing-for-scala-3
"The point for final-tagless being more flexible by allowing tighter constraints than the Monad-interface is interesting. I've not seen this explicitly stated in other comparisons of the two approaches." That's because it's not strictly true. If you compare the Free Monad to final tagless then sure, but you can lift your domain into any free structure in theory. E.g. Free Applicative etc. You can also combine them in much the same way as Final Tagless programs. This isn't too hard to do conceptually but it's not as straightforward syntactically in Scala. The important thing to realise about free vs. final tagless is that they are isomorphic. This means they encode the same thing in different ways. Ultimately it comes down to a few things as for why you'd pick one over the other. Final Tagless certainly has less syntactic overhead for a majority of use cases so it's a good place to start.
On your last point - only if you compare it to the free Monad specifically (as opposed to free encodings in general).
So, the tldr pragmatic answer to that is that we use cats or scalaz, and IO, with Monix or fs2 for everything, because those libraries are inspired by abstractions present in other FP languages that, in many cases, have associated peer reviewed proofs or papers and years - decades of pragmatic use in those languages. As a user of these libraries, I really only need to memorize the basic methods and the purposes of these 8 typeclasses: Functor, Apply, Applicative, ApplicativeError, FlatMap, Monad, MonadError, and Traverse. The methods they provide are, semicolon-separated: map - apply a function in a context to get a new context with the result of the function application; mapN, apply the contents of several independent contexts as arguments to a single function; pure, construct a new context containing the pasted value; catchNonFatal, catch thrown exceptions and put them into a context in a failed state, raiseError, put a context into a failed state containing a value describing the error, handleError, convert an errored context to a new context by converting the error to the success result type, and recover, only convert to a successful context if the error is some specific error type; flatMap, apply a function returning a context to a context returning the resulting unnested context, allowing chaining that allows each call to depend on the results of the previous call; whileM, apply an effect repeatedly, collecting the results, iterateWhileM, apply an effect iteratively from some starting point until a conditional fails, returning the last result; ensure, fail a successful value in a context if it doesn't pass a predicate, ensureOr, provide a default successful value if the current value doesn't pass a predicate; foldLeft/foldright, convert a context containing a value to a value using the provided starting value and a binary function taking the starting value and the value in the context, sequence, reverse the nesting order of the contexts (F[G[A]] -&gt; G[F[A]]). So 14 methods get you a really long way to having gaurantees and capabilities in your type signatures. List alone in the std lib has many more methods and interfaces to memorize than that. Once you learn the typeclasses, this actually decreases cognitive lead. The methods' type signatures are as close to actually being self documenting as you can get, and the code is as locally reasonable as possible. Scala's type system and compiler is so rich, that it makes it possible to express type constraints that are only possible in a few languages, yet lax enough to allow Java style code when safety isn't a major concern. The language grows with your knowledge, which is something that helps productivity. Types and FP are tools that can make you more productive. Once you learn how to use a router or lathe, you no longer have to carve by hand with a chisel, and can make more reproducible products. You must learn the techniques, though, and that takes time. The same is true of FP constructs and libraries. They take time to learn, but are powerful tools to simplify making programs. So, yes, but like any skill, it gets easier with time and deliberate practice. 
 val x = List.fill(10)("hello") x(1) // "hello"
Railway-oriented-programming is a great article. His talk that included "functions in the large, functions in the small" is nice, too. I don't have the link handy, but it really made me think about how I teach FP at work, so it's worth a watch if you can find it.
Alternatively you could refine your divisor. 
You could indeed! Refined is a nice library, too. You'll still get Eithers from refineMV if you get the data at runtime. It only works for literal values at compile time, I think.
Please remain professional. Bashing people or organizations on the internet because of issues that happened between you and them is far from that.
 x(1) // "hello" what?` x(1)`? hmm the problem was that List.fill(10) should return a function (but it should be a `def`), and calling the second line should return `List(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)`
Wait, I've got plenty to learn, but you lost me at: &gt; This is better, because you are announcing that it may fail, but it still breaks parametricity, because you aren't giving a type to the function that says it can catch exceptions How is `Either[Throwable, Double]` not telling enough? Because we don't document through the type signature all possible Exceptions that may be thrown? Then how about declaring divide as `def divide(a: Double, b: Double): Either[ArithmeticException, Double] = if(b!=0) Right(a/b) else Left(new ArithmeticException("no good"))`? I guess we could even generalize that to: sealed class MyDivideException class ByZero extends MyDivideException class IncorrectSomethingElse extends MyDivideException and define as `def divide(a: Double, b: Double): Either[MyDivideException, Double] = ...` the caller would know how to pattern match against `MyDivideException` wouldn't it? I fail to see what the use of the implicit ApplicativeError buys me here? 
I'll reply to your longer comment :)
&gt; That's because it's not strictly true. If you compare the Free Monad to final tagless then sure, but you can lift your domain into any free structure in theory. E.g. Free Applicative etc. Say you have different operations with different contraints trait Foo[F[_]] { def foo: F[Unit] def bar[A](implicit F: Monad[F]): F[A] def baz[A](implicit F: MonadError[F]): F[A] } With an initial encoding you'd have to have Free for `bar`, `foo` will be overconstrained, and you'd need a custom `FreeMonadError` (or add an `Attempt` constructor to your algebra). Not only that, but you won't be able to mix these operations nicely, since they return three different datatypes (so in practice they would all return `Free` and you'd need to add `Attempt` to your algebra). That's inconvenient. Final tagless just works. Also, in some cases it's not quite clear how the free structure should look like in the first place (e.g. Free Applicatives have a few different encodings). &gt; You can also combine them in much the same way as Final Tagless programs. You can certainly combine them, but it's a lot more work. Typeclass constraints already combine (horizontally). You need Coproducts and Inject for Free. &gt; The important thing to realise about free vs. final tagless is that they are isomorphic. Sure, initial and final encodings can encode the same things. It just so happens that final tagless is more convenient and requires a lot less machinery imho (for this use case) :) 
Ah sorry. Like this def x[T] = List.fill[T](10)(_) x(1)
Well handling whether a String is null or not is a different question. Normally in Scala you don't have null values, I assume that's why these functions don't handle nulls. 
John Pretty (original author of the cake pattern) has a good conclusion: https://twitter.com/propensive/status/856150425304780801
My oh my, such a long time ago... Makes me feel old hahaha
You can definitely do the same/similar with Free structures though there's less library support for it and it's definitely more cumbersome than final tagless in Scala (especially to define the machinery that pieces them together). Check out `ExecStrategy` in this gist for an example of how you could do it if the relevant Free structures existed. You can see how they're used in user code at the bottom. https://gist.github.com/camjo/d2dd391b5a44b55d407f041477341242
Thank you!
Either doesn't have a catch method. Either doesn't have a raiseError method. You have to call out that ability in your type to be parametric. This idea comes from Philip Wadler, in `Theorems for Free!` in a paper. Bartowz Milewski has a good blog [post](https://bartoszmilewski.com/2014/09/22/parametricity-money-for-nothing-and-theorems-for-free/) on it. As I say later in the post, you could use Try here, as it is Referentially Transparent, catches errors, can raise errors, and you aren't doing IO or random, or any other shenanigans with divide. ApplicativeError[ThrowableEither] is Try, they behave almost the same. ApplicativeError also calls out that you can side effect, which is where most errors are thrown in scala - typically you only throw when wrapping Java code, and usually Java code that does something effecty. If the / operation didn't throw, you could just use the Either. It would imply that you are bounds-checking the divisor, as someone else mentioned in a reply earlier mentioning refinement. Not all keywords are effectual or necessary. We need the try/catch keywords, since we work with Java libraries that throw on an alternative code path. I don't think we need throw in Scala. There may be some Java interop reasons. We just also need to call out that we use them. We only care about calling out keywords in types that have an uncontrolled effect. try and catch cause effects. Var, while, and do are the others.
I agree, I think scala could do a lot better of a job at providing these pragmatic libraries in non-pure-fp style for projects that don't need to be {Reactive,Resilient,Elastic,MessageDriven}
&gt; I was not shown how to do this in the preceding chapter. There was no mention of currying being in the form of f(a)(b) This isn't currying, and you implement currying in 2.3, which I assume you had no problem with. This is an example of function application, which you've been doing all throughout this chapter. The what seems to be confusing you is how to apply two functions in a row for some reason. &gt;Am I expected to struggle throughout this entire book trying to figure out these little subtleties? The point of this section is not to learn how to apply functions, but that parametrcity only allows one correct implementation of the function, anything else you could possibly write would fail to compile. Which is exactly what happened in your case. The problem here might be that the compiler error was unhelpful instead of anything the book has done wrong. &gt; I get that this is a difficult book, but to me, I want to focus on the functional programming part and not the syntax. It's an intermediate level Scala book, so you shouldn't be having problems with syntax if you're prepared to see this book to the end.
As an aside, why did you expect your solution to work? I don't think it's just a matter of syntax like you claim, the solution doesn't make any sense semantically.
The book did not mention how to actually use application in the form f(x1)(x...n). Early on in the book, it was mentioned that Scala was merely 'the vehicle' that they are using to teach functional programming, and that the intent of the book was to teach functional programming and not scala. That is why I expect them to actually cover things like this and I drew the comparison to SICP. The book literally says no scala experience is required! Which is why I asked if there were any supplementary resources to use aside from the scala lang documentation.
Wow, thank you for that extremely thorough and helpful response! I have a lot of learning to do.
&gt; Implement uncurry, which reverses the transformation of curry. Note that since =&gt; associates to the right, A =&gt; (B =&gt; C) can be written as A =&gt; B =&gt; C. This note in the problem gives a hint that having f : A =&gt; B =&gt; C is the same as having f : A =&gt; (B =&gt; C). Applying f with this signature, to argument a, you'd have fa = f(a) returning a function of type (B =&gt; C). Applying the resulting fa: B=&gt;C to b i.e fa(b) a.k.a f(a)(b) (since here I set fa = f(a)) yield the C that was asked in the problem. And since you are asked to produce function accepting tuple (A,B) producing C, you're basically almost there. It is just that instead of trying to apply f to B like you did (which would not fit since original f expect A), what you need is to produce C, by using f applied to a producing fa: B=&gt;C, and applied again to b to get the final C. I hope that makes sense. 
Even if you did not get a syntax issue regarding tuples, your solution would not have compiled as you are trying to apply a `B` to a function that is defined over `A =&gt; B =&gt; C`. Note that `f` does not change when you apply `a` to it. When you apply `a` to `f`, you end up with an anonymous function that is `B =&gt; C`. You can then apply a value of type `B` to this to obtain a value of type `C`. Therefore `f(a)` gives you `B =&gt; C`. You have to apply `b` to this to obtain `C`. If the syntax confuses you, the following is the same as the solution: def uncurry[A,B,C](f: A =&gt; B =&gt; C): (A, B) =&gt; C = (a, b) =&gt; { val intermediate: B =&gt; C = f(a) val result: C = intermediate(b) result } Or if you want to be explicit in your types, we can change this as the compiler suggests. (Note the curly brackets) def uncurry[A,B,C](f: A =&gt; B =&gt; C): (A, B) =&gt; C = { case (a: A, b: B) =&gt; val intermediate: B =&gt; C = f(a) val result: C = intermediate(b) result }
&gt; if we're changing the fooController so that it also uses the barService now, I'd rather see that change in the PR diff, new FooController(fooService) -&gt; new FooController(fooService, barService), than have it magically resolved in a hidden way at run time (or, if there isn't actually a barService available, fail at runtime instead of compile time). Its always going to be visible in the PR, becuse `FooController` itself will be asking for a different implicit parameter. Implicits are also completely safe as long as you use `lazy val`/`def`. The scala compiler will make sure you only ever have one singleton instance at runtime and it will automatically form the dependency graph for you, and it will handle all initialization for you as well. Implicits are actually a subset of logic based similar to prolog, so when you are using implicit's as DI, you are basically asking the compiler to prove to you that it can provide an single coherent instance at runtime of this value. There is no magic, no spring boilerplate and every Scala based IDE will work with implicits since its a Scala language construct. It also doesn't have the disadvantage of parameter passing whos boilerplate gets tedious, especially if you need to replace the dependency of only of your nodes that your dependency graph forms (i.e. something that Grafter solves with using Macros + ReaderT monad, see https://github.com/zalando/grafter) 
I struggled also when I started to read and *work* through this book. Lately I discovered the book "Functional programming simplified" by Alvin Alexander. It takes a totally different approach: it tells everything to the reader and repeats it twice or more often. So if the former book is kinda *dense*, the latter is very in detail. I am almost through it right now and hope to acquire enough knowledge and competence to go back to "Functional programming in Scala" again and then having a better time with it. I believe it covers the theory behind many functional concepts more accurate and makes you reimplement many of them. So having read both and worked through their exercises will hopefully enabling me to really get into consistent functional programming finally. 
I was working on a library called `DMap` which allows for Python-like dynamic (but safe) heterogeneous Maps. https://github.com/joshlemer/DMap
Are you familiar with this [http://ammonite.io/#HTTPRequests](http://ammonite.io/#HTTPRequests) ?
This definitely makes more sense. Would it defeat the purpose of the book if I were to look to other reference books? 
Glad that helps. I am not sure if you need any. Perhaps you might want to get a bit more experience with Scala? Contrary to authors, I would say that one needs to know a beginner-intermediate level of Scala before attempting FPiS. Otherwise, it is one of the best programming books I have seen in the recent years regarding flow, pedagogy and having a self-contained story. Regarding your edit in OP: FPiS definitely needs to be read like a textbook, taking time to think and work through exercises and such.
&gt; Its always going to be visible in the PR, becuse FooController itself will be asking for a different implicit parameter. Yes and no. There will be a change to `FooController` but there's likely no change to the "wiring" code, so it looks more like a normal code change. &gt; when you are using implicit's as DI, you are basically asking the compiler to prove to you that it can provide an single coherent instance at runtime of this value. There is no magic, no spring boilerplate and every Scala based IDE will work with implicits since its a Scala language construct. Indeed; my point was that isn't true about Guice, but I phrased it badly.
I'll stick through with the book. I got it because for once I foudn a book that was heavily focused on exercises so this will definitely improve my skills. I'm entering 2nd year of CS this fall so maybe I am not the intended audience here but I find it interesting enough to persevere. Lastly, (off topic) what do you think about trial and error approach to learning? Do you think its bad practice to rely on the interpreter/compiler and not think everything completely through? I find sometimes I manage to get the right solution without completely understanding everything below the hood.
If it makes you feel any better, I am in the same spot of this book and feel exactly the same way you do. I find myself poring through sections 4 and 5 times to see if I missed something because the answers seem to be things that haven't yet been explained to me.
If you are reading this book before starting your second year of CS education, then you are definitely in the intended audience, albeit lacking experience. ;) Regarding your question: It depends. I am guilty of doing trial-error myself on some occasions. I think the most important thing is to not have a false sense of security regarding understanding after finding a low-effort solution to a confusing problem. A healthy approach would be to think as thoroughly as one can regarding a problem, in my opinion, but skipping over it temporarily if it makes you lose too much time. This way, progress will not be hindered and it allows one to come back to confusing problems with greater knowledge and a fresh mind. The important thing is actually coming back to them though. Iterative deepening depth-first learning! Perhaps others would like to chime in?
A curried function is a function that returns another function. A =&gt; B =&gt; C Represents two functions: type Second = B =&gt; C type First = A =&gt; Second Written this way, it is clearer that a curried function returns a function. You call it by calling apply on the first function, then by calling apply on the second function. first.apply(a).apply(b) In scala you can omit the word apply on Function objects: first(a)(b) uncurry takes a curried function of arity 2 (two arguments) and returns a regular Function2 object. So, you've got to return (a,b) =&gt; f(a)(b) This return type is (A,B) =&gt; C . 
I'm not sure which is more pathetic: that you talk about yourself in the third person, or that you still feel the need to comment on this subreddit to insult all Scala programmers (and get upvoted for it).
Isn't it an anti-pattern though? I tried implementing it and found it to be nasty
It is true that it's a bit pricey on the trade-off scale, but I wouldn't go as far as calling it an anti-pattern. That's just my opinion though.... 
Scala programmers themselves are more pathetic. I will ensure you all continue to pay a penalty for what you have done to others. You will continue to have a shit house language and get upset at being told you are wrong. Tell me more about pathetic.
I’m just going to drop this here... https://github.com/fpinscala/fpinscala
Does more harm than good. Why to teach it to young programmers entering Scala? The whole reason I code in Scala is to avoid boiler plate code. but cake introduces so much boiler plate code if done at a project level.
&gt; You might return infinity, but that is not a member of Double. scala&gt; Double.PositiveInfinity res2: Double = Infinity
Well, you can spin up a separate execution context for that Future, so obviously you can do something.
That takes a thread.
Great talk. Those students are so brilliant. 
Are implicit function types getting a backport to 2.x?
Thx for the feedback! It was not my intention to encourage the usage of the cake pattern though. Having multiple view angles on the problem is very beneficial for the learning process. Furthermore, I believe my viewers wouldn't want me to make choices on their behalf anyway.
For me implicit functions are gonna be great for tagless final programming style. As most are aware monad transformers are rather inefficient in Scala (question is, will opaque types help with that, would be great if there is someone who knows something about that). So the best solution we know so far is usage of libreries like [cats-mtl ](https://github.com/typelevel/cats-mtl)and [meow-mtl](https://github.com/oleg-py/meow-mtl). But unfortunatly to avoid performance overhead one have to write interpreter using quite cumbersome code, like: for { //setup MVars, Refs, etc res &lt;- Consumer(var1.put) runTell { implicit tell =&gt; ref1 runAsk { implicit ask =&gt; ref2 runState { implicit st =&gt; //run your program } } } } yield res And using implicit functions it should be possible to simplify it to something like this: for { //setup MVars, Refs, etc res &lt;- setupInstances(var1, ref1, ref2) { //run your program } } yield res Previous code snipet can't be simplified like this cause `{ implicit arg ... }` accept only one parameter.
Especially considering they aren't Scala programmers, that was pretty great engagement yeah
This divide function is already not parametric because it has no type parameters. What you're saying about catching errors matters if there is a parameter, but that leap isn't well explained in the post
It is an important distinction for parametric polymorphism. And you are right to call it out. That's what I say in the last paragraph of the post: &gt; To make divide completely parametric you need to make Double an A Do you think that should be explained earlier, or differently? Should I include an example? I thought by the end the reader would have seen how to add constraints and the the fully parametric version of divide could be visualized from that paragraph. Where should I have put it, and what else should I have added?
Yes, but it is a lie. ;-p Scala's Double has a range: &gt; Double 64-bit IEEE 754 double-precision float 4.94065645841246544e-324d to 1.79769313486231570e+308d (positive or negative) Really, I wanted to avoid arguments about division by 0 meaning infinity/undefined. The upper boundary of double is sufficiently large that it could serve as infinite, so I suppose we could say it works. 
Here's an exploration of that topic: https://gist.github.com/OlivierBlanvillain/48bb5c66dbb0557da50465809564ee80
I think it would be helpful to say that Either[Throwable, Double] is a perfectly valid encoding for divide, and then go on to say it could be more abstract, and talk about what the abstraction step buys in contrast to the non parametric version
How about thin cake pattern? http://di-in-scala.github.io/#modules
Does this shop have any more junior openings? Do they sponsor non-EU nationals? I have experience with about half of the named tech/patterns/paradigms and would love to relocate to Malaga.
This might help with a common mistake I've seen at some companies I've work with. When creating a route with akka-http (and spray), like the following: &gt;path("/things") { &gt; &gt; get { &gt; &gt;HttpResponse(???) &gt; &gt; } \~ &gt; &gt; post { &gt; &gt;HttpResponse(???) &gt; &gt; } &gt; &gt;} If you don't add the tilde, making the GET and POST a single expression, then only the POST gets registered. It looks minor but I've seen people spend hours not understanding what was happening. While very cool, I was still a little skeptical even after thinking about the table/row example and the "avoid passing arguments" example. This ScalaDays presentation helped me think of even more possibilities: [https://slideslive.com/38908156/applications-of-implicit-function-types](https://slideslive.com/38908156/applications-of-implicit-function-types)
\&gt; Does this mean through this mechanism we don't need pattern matching on the types? What does Martin mean by "that's a very deep realization"? What is pattern matching on types?
What was missing from [https://github.com/scalaj/scalaj-http](https://github.com/scalaj/scalaj-http) for your purposes? I've always used it for quick scripts. `import scalaj.http._` `val response: HttpResponse[String] = Http("r/`[`http://foo.com/search").param("q","monkeys").asString`](http://foo.com/search").param("q","monkeys").asString) vs `import "com.lihaoyi" %% "requests" % "0.1.2"` `val r = requests.get("r/`[`https://api.github.com/users/lihaoyi`](https://api.github.com/users/lihaoyi)`")`
How many times doing a web request was really the CPU throttle bottleneck of your application?
&gt; Also JetBrains doesn't seem to care that much about Scala - I have opened other 4 issues with Scala (with various complexity, some looks like basic lang. usage) which all are ignored, most of them months old without any response. This isn't exactly true, I have opened bug reports there which did get fixed eventually (albeit after some time) Intellij just had a really hard task at hand, i.e. they had to recreate the entire Scala typechecker (and obviously the Intellij typechecker has divergences with the Scalac one). They had to do this in order to provide non trivial inspections (i.e. ones that aren't just a basic refactor rename)
&gt; Yes and no. There will be a change to FooController but there's likely no change to the "wiring" code, so it looks more like a normal code change. Well yes, thats the whole point of DI/wiring code. This is even visible in Grafter. The Scala compiler constructs the dependency graph for you.
`Double`’s and other number types’ ranges are an implementation detail that’s irrelevant when we’re talking about infinity 😊
I wondered about that too. would be nice if someone can elaborate on that. 
I guess, but we're not really saying Either can catch then, right? Or is that part of my reasoning flawed, too?
Cake feel like a huge anti-pattern. You’re choosing inheritance over has-a relationships and losing control of how many instances of traits/objects/connections get created or shared, in order to get some ugly god objects with compile time DI. That’s how it feels to me. However I don’t blog articles on the internet nearly as harsh as my opinion. Why is this? 
Ultimately it solves a problem that people have. Traits are supposed to be usable as mixins that represent logical composition as well as for inheritance, so I don't see it as "choosing inheritance over has-a"; ultimately if you don't represent a has-a relationship with a trait then how do you represent it? Are you creating an ugly god object? "Physically" yes, but it generally doesn't look like it in code, and doesn't behave as one for maintenance purposes. Better a nasty big object than a nasty big hashmap, which is what alternative DI solutions tend to boil down to.
Sure; I think it's better for the graph to be explicit, because you care about the graph and want to be able to review changes to it.
It's not a CPU throttle bottleneck, that's precisely why async is so important. How many times have all my application's threads been blocked making a slow HTTP request to a third-party endpoint, leaving none free to serve client requests even for operations that don't need to call a third-party HTTP endpoint? Too many.
Thinking about this kind of thing is quicker/easier than thinking about exceptions, IME. `Either` is a plain old type written in plain old Scala, so you don't have to remember any special cases - if you forget what any part of it does, you can just click through to the definition and read the code. If I wrote some code that called functions that might throw exceptions, I'd have to worry about unit tests for whether my code handled those exceptions properly, and I'd probably get those unit tests wrong.
It was their choice to start their project to emulate Scalac, they could cooperate with community and build a reusable one. I think that they, at the start of Scala popularity, were really invested, but after Kotlin started to grow, Scala support started to degrade. After all, why support a competitor. One time even JetBrains dev responded to issues about macros, and from what he told was clear that they have no plans at supporting current macroes (that was like a year or two back). So they are either waiting for Dotty, which still has years to finish, or abandoning Scala entirely :/. I think a good reliable IDE(s) is one of the most important things for a language and when you don't have that, the language can no longer grow over some point (e.g. be a serious alternative to Java). There are many wonderful languages (like Haskell) which without proper IDE are not going to be successful (maybe Eta could change that). Scala had a good start, but now, at least from my point of view, it doesn't seem to be progressing at all - losing key people from major libraries, from that community driven linter, weakening JetBrains support, nothing good on a horizon for many years until Dotty arrives. Dotty which might scare many enterprises, because it a very different language and I don't think they are going to port their enormous codebases (their custom macroes) any time soon. I like Scala, but I probably lost my optimism in its future...
I think that's flawed. The return signature is part of the contract that the caller can expect. You can use a raw try/catch to implement that signature - it still presents a pure interface, implemented impurely
I'm not familiar with "Functional Programming, Simplified," but another option is [Scala with Cats.](https://underscore.io/books/scala-with-cats/) It focuses more on use than implementation. I went through the Red Book first, but I think that having a better understanding of the *why* would have been helpful to have before actually working through implementations.
I agree it's RT as long as you catch, and OP was asking why Either is better. The point was that Either doesn't catch, so you have to be careful how you implement it. Perhaps reordering like the following? I did point out that using Try was valid here too. Probably should reorder the argument so that we use Either, point out that Either doesn't catch, then use Try, because it catches, then generalize try for parametricity, and conclude with Double as A with restrictions. That would make it more coherent, and make the leaps between the sigs smaller.
&gt; It was their choice to start their project to emulate Scalac, they could cooperate with community and build a reusable one. I think that they, at the start of Scala popularity, were really invested, but after Kotlin started to grow, Scala support started to degrade. After all, why support a competitor. This wasn't a feasible option. The scalac compiler is a old style single pass compiler, and the only way to get it to typecheck is to actually pass scalac after a certain phase, this is how Ensime and ScalaIDE work and its incredibly brittle and error prone. Ontop of all of this, it still doesn't provide the ability to provide advanced inspections because in order to do this you have to store all type information into an AST (and that is oversimplifying things, really you need to build up a database full of inspections). This exists now, in the form of SemanticDB/Dotty, but this is 10 years after Intellij released their scalac plugin &gt; One time even JetBrains dev responded to issues about macros, and from what he told was clear that they have no plans at supporting current macroes (that was like a year or two back). So they are either waiting for Dotty, which still has years to finish, or abandoning Scala entirely :/ Or because macros are an experimental feature, and have always been advertised as such. 
Thats like saying its better to be explicit about memory management, because you can easily view and control how memory works. There is a reason why DI was invented, it was solving an actual business problem (just as GC was), which is that for a lot of code that resembles services*, its actually better to *not* have to be explicit about a services* dependencies, and rather have either the compiler (or runtime) wire it up for you, and the reason why is that its easy to isolate a single service from your entire graph and replace its implementation which you need to do in testing (i.e. in an acceptance test, I need to make sure that my UserService which depends on a Logging service actually makes the correct logs, DI allows you to create your entire application chain and only replace the Logging service with a Mock Lock service) Eric Torrobore (author of Grafter) did a very good talk on this which you can see here https://www.youtube.com/watch?v=vXCXQ8cLZm4.
&gt; There is a reason why DI was invented, it was solving an actual business problem Most of the time I've seen DI introduced, it was used was to work around dysfunctional policy rather than addressing a genuine business problem. The few cases it was genuinely useful was for constructs that were inexpressible in the Java type system. I don't believe that DI adds value in an expressive language; if it did, it would be popular in Python/Ruby/...
What I mean is that you can implement the function which returns Either by using a catch internally, and there's nothing wrong with that. There's even a combinator on the Either object for doing that
&gt; Python/Ruby/... Python/Ruby use metaprogramming to do the exact same thing For example, with Rails this style code code ` def initialize(model_klass) @model_klass = model_klass end ` Is basically DI, its just a different way of expressing it
One note: fs2 is a cats library, not scalaz
Swing is fine. There are some great look-and-feels.
Gradle doesn't have "reasonable support for scala" ? At least as a non-library developer, gradle has worked perfectly fine building anything scala. So I might dispute that one. And it's probably easier to stay away from SBT, as it's painfully complex.
Too bad it basically got rejected because people are using toString that has no contract enforced in the language to drive their implementation Oo I got really upset by that, I thought in a language like Scala I wouldn't see such an attachment to string based programming. Personally, I've never used that toString that way. A point about quotes for string parameters was also valid. I'd love to see both of those things implemented.
I'm pretty bummed about it too TBH. I think it would have been an awesome improvement.
I don't know of any such combinator on Either, and I don't see it in the documentation or source for Either.scala. Importing the syntax for cats.either gets you `Either.fromTry`, which you can perform on a Try, and `scala.util.control.Exception` has `allCatch.either`. But just Either in the return type does not imply catching in the implementation, which I think is the useful part of parametricity. If I have: def foo[A](a: A): A I'm not allowed to return any other instance of A than the instance of A I pass in. If I say I return Either, and include a B type, I'm not allowed to do anything other than wrap the A in a Right[B, A]: def foo[A, B](a:A):Either[B, A] = Right[B, A](a) Because I don't have a B to raise into the Either or a way to convert an A into a B, nor am I defining when I would return a Left[B, A]. In order to do that I have to at least pass a predicate from A =&gt; Boolean and a transformation from A =&gt; Left[B, A]: def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] The predicate implies that I will use a conditional, and the transformation implies that if the conditional doesn't pass, I will produce a Left. I don't think parametricity requires that I use every argument, so the number of possible implementations grows to five: def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] = Right(a) def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] = toError(A) def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] = if(pred(a)){ Right[B, A](a)} else {toError(a)} def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] = pred(a) match{ case True =&gt; Right[B, A](a) case _ =&gt; toError(a) } def foo[A, B](a: A)(pred: A =&gt; Bool)(toError: A =&gt; Left[B, A]): Either[B, A] = Right[B, A](a) match{ case Right(x) if pred(x) =&gt; Right[B, A](a) case _ =&gt; toError(a) } Though the last three are equivalent. For foo to be parametric, Either has to be a generic, and now I'm required to declare that it can raise errors and ensure: def bar[F[_, _], A, B](a: A)(toError: A =&gt; B)(isError: A =&gt; Boolean)(implicit monadError: MonadError[({type L[a] = F[B, a]})#L, B]): F[B, A] This has, I think, one implementation that uses all the arguments: def foo[F[_, _], A, B](a: A)(toError: A =&gt; B)(isError: A =&gt; Boolean)(implicit monadError: MonadError[({type L[a] = F[B, a]})#L, B]): F[B, A] = monadError.ensure(monadError.pure(a))(toError(a))(isError) In practice, to be useful, I don't think it is necessary that F be parametric. Restricting it to a concrete type like Either is enough, but I do believe you have to declare its capabilities, and no type in the divide example states that it can raise or catch errors. Try is a different case, though. It can raise errors (through Failure constructor) and can catch errors (in apply and its combinators), so just using it is sufficient to be useful for reasoning about the implementation: def foo[A](a: A, pred: A =&gt; Boolean, toError: A =&gt; Throwable): Try[A] It even restricts you from doing unconstrained IO, which is arguably better than using ApplicativeError, which doesn't. The type signature is also much simpler, and only three implementations exist that use all the arguments: def foo[A](a: A, pred: A =&gt; Boolean, toError: A =&gt; Throwable): Try[A] = Try(a).filter(pred).recoverWith{ case _ =&gt; Failure[A](toError(a))} def foo[A](a: A, pred: A =&gt; Boolean, toError: A =&gt; Throwable): Try[A] = Try(a) match { case Success(x) if pred(x) =&gt; Try(a) case _ =&gt; Failure[A](toError(a)) } def foo[A](a: A, pred: A =&gt; Boolean, toError: A =&gt; Throwable): Try[A] = if(pred(a) { Try(a) } else { Failure[A](toError(a))} Arguably, using Try is the correct choice if your B is a throwable and you don't want to allow other effects. That's how I understand parametricity, and its pragmatic use. None of the above says that just using try/catch is *wrong* to produce the Either. It is stating that defining a parametrically polymorphic method is a safer way of informing your caller that you handle the use case and that you are requiring yourself to handle the use case. Scala doesn't guarantee any of this, even when you include the restraints, which is part of what makes it so expressive. You are free to choose when this type of reasoning is pragmatic.
Gradle can satisfy most simple Scala builds, but it does lack a number of features, including common tasks used by library maintainers such as cross building to different versions of Scala. As painful as SBT may be it is where you're going to find the most support for building Scala projects and should be the default recommendation. It has a fulltime team maintaining it and is covered under enterprise licensing by Lightbend.
Hi, author here. Fs2 Redis is a purely functional Redis client built on top of Lettuce, Fs2 and Cats Effect. Expect more documentation to come soon! And here's the release tag: https://github.com/gvolpe/fs2-redis/releases/tag/0.1.0 Please give it a try if interested and report any bugs you might find, much appreciated it! 
&gt; immutable variables and while loop are discouraged I’m confused. I know while loops are discouraged but shouldn’t this be mutable variables?
Probably not what /u/RandomName8 was talking about, but I thought the justifications for how the Java 8 library team designed Optional were pretty insane: [Why java.util.Optional is broken](https://developer.atlassian.com/blog/2015/08/optional-broken/). "We are not trying to shoe-horn in an option monad" demonstrates lack of an understanding of what a monad is; adding monads to the language without adding applicatives or allowing library designers to create their own was pretty shortsighted and something I can only expect from people who have a shallow understanding of what they are trying to build and how it's going to be used. This may sound harsh, but I expect a lot more from people in charge of something with so much impact.
You could. Scala is more of a library language than either Java or Kotlin, so you can support more in libraries. [https://typelevel.org/blog/2016/08/21/hkts-moving-forward.html](https://typelevel.org/blog/2016/08/21/hkts-moving-forward.html)
&gt; This wasn't a feasible option. The scalac compiler is a old style single pass compiler, and the only way to get it to typecheck is to actually pass scalac after a certain phase, this is how Ensime and ScalaIDE work and its incredibly brittle and error prone. &gt; Ontop of all of this, it still doesn't provide the ability to provide advanced inspections because in order to do this you have to store all type information into an AST (and that is oversimplifying things, really you need to build up a database full of inspections). &gt; This exists now, in the form of SemanticDB/Dotty, but this is 10 years after Intellij released their scalac plugin It could have been something new, not necessarily based directly on scalac. It could have been versatile tool like e.g. TypeScript has, so integration to many editors/IDEs is quite easy. Trivial compared to writing their own compiler. All I am saying is if they had joined forces with community, we might have had better and more Scala IDEs. Maybe even proper macro support. &gt; Or because macros are an experimental feature, and have always been advertised as such. It doesn't matter what label it has, since it is used in almost all major libraries. It would be hard to develop something non-trivial in Scala while not using any of these libraries. Once macros (or any language feature) were adopted to such extend, they should be supported by IDEs and other tools.
I'm still very excited about Mill! We use Gradle at work for some of our hybrid projects in Scala/Java, but SBT is just easier (perhaps only for me since I have used it longer). In general it's a good idea to use SBT + Coursier, or SBT + Bloop for speed, which seems to be the biggest issue with SBT that I didn't experience with Gradle.
Would be worth addressing the FUD that Kotlin folks put out around `null`, as there are some subtleties. Is Scala less null-safe than Kotlin? No: pure Scala code is completely safe because it never uses null and uses `Option[T]` where Kotlin would use `T?`, Scala code that interoperates with Java can be unsafe if you don't explicitly check nullable return values (by wrapping them in `Option(myJavaMethod())`) just as Kotlin code that interoperates with Java can be unsafe if you don't explicitly check nullable return values (by assigning them to `T?` rather than `T`). The history of Typelevel given here is false, FWIW; Typelevel predates cats (indeed scalaz was at one point part of typelevel). I'd also dispute the "they boil down to don’t be a jerk" characterization; I don't think that's a fair reflection of the written CoC (I wouldn't have a problem signing up to their code if that was all it was) or how it has historically been applied by typelevel moderators. Would be worth being more concrete about what SBT does and doesn't help with. Personally in nearly a decade of Scala I've never found SBT to be worth the trouble. Maven didn't support cross-building against multiple versions of Scala until recently, but it's very rare that a newbie would actually cross-build: applications obviously only need to be built for one version of Scala, and unless you're actively working on applications that use multiple different versions of Scala, your personal libraries only need to be built for one version too. Even organisations of ~100 people often don't need to maintain internal libraries on an older version of the language. SBT is visibly in use by very core libraries like cats or shapeless (which *do* need to actively maintain their previous-version-of-scala versions) so I think people sometimes get the impression it's "the" Scala build tool, but what makes sense for those libraries isn't necessarily what makes sense for newbies.
Congratulations. I hope you find opportunity to work on these things in your post-doc life :)
Congratulations!
Thx for the link to the book! 
&gt; No: pure Scala code is completely safe because it never uses null Sorry, but this is just false. (One can have conventions and linters, and that's fine as far as that goes -- but *not* everybody does that, unfortunately. There's also the NPE issues that stem from initialization order problems.)
Hey! More than 50k€. NY, DC, or SF - these cities are 3x+ more expensive than Malaga. 
That's not too bad :). Can you sponsor a US citizen? No Portuguese, but I speak Spanish.
&gt; One can have conventions and linters, and that's fine as far as that goes -- but not everybody does that, unfortunately. I've found it to be true in practice: established Scala libraries do follow the conventions and/or use the linters. You're right that there's a bit more nuance than what I said. &gt; There's also the NPE issues that stem from initialization order problems. True, but I think also just as much a problem in Kotlin?
Many people agree with you, they just not write blogs. 
Just yesterday I've watched this thing, where this guy has a very strong opinion on driving the future of a language by a PhD projects. It is worth watching IMO. I have no strong opinion here, but maybe some one will be interested in this one.
I'am currently looking for a scala client for redis. I came across these two: * [Rediscala](https://github.com/etaty/rediscala) * [scala-redis](https://github.com/debasishg/scala-redis) The first one considers itself non blocking (how does this work for the single threaded redis anyhow?) while the second is favored on the redis website. Rediscala seems to lack maintenance... Can you recommend one of these clients or a different one? We are planning to run a redis cluster..
Ah, but the fundamental difference in this case is that the PhD thesis was driven by the design of Scala.js. Not the other way around. ;)
I can't/don't watch videos. But I'd say you have to ask what the alternative is. Language development is expensive and time-consuming and it's very hard for any entity that funds it to capture the value they're creating. We have a few languages that were created by platform companies and funded as part of promoting their platform, but that's not a great model (especially if the point is to be cross-platform as in this case). Honestly public funding seems like the right approach, the same way we do for basic research. And while I wish there was a structure to allow public funding for maintenance work, that structure doesn't currently exist. So we do the best we can with what we have.
I thing Kotlin kind of prooves it can be done . It was created by a professional software company, not on university. It can definitely work out without public founding. At least I'm not aware it used one. It is still young though, you might be right. We'll see.
&gt; Scala believes you know what you do, and lets you do things in your way. This. I'm new to Scala, but I've quickly found out you can use as much or as little Scala/FP sugar in your code as you like. There are dozens of ways to accomplish the same work. And thats what excites me about the language in general. 
Nice. I think yes. I've texted you via private message. 
It's tied to a different kind of "platform", the JetBrains IDEs. I think that causes the same kind of issues for the language as being single-platform in the traditional sense - a lot of good for Scala has come out of having multiple IDEs competing (in a loose sense) over the best way for particular features to interoperate with a human coder.
It's worth being nonblocking when you're communicating across a network, on the assumption that network latency may well be higher than time taken for the remote end to process your requests. The last time I used Redis from Scala we just used Jedis, but that was a while ago and there may be better options now.
If you have callbacks that emit a single Event you can use Future/Task/IO to model convert those. If you have callbacks that will get invoked multiple times, you need a streaming abstraction like Akka Streams, Observables, or FS2
I was afraid of that. So in this case I would need to publish the file watcher events to a stream and within my event loop check for emissions correct? If that is the case should I continually call the eventLoop without passing the event directly and check the stream’s contents within that? 
FYI: from https://doc.akka.io/docs/akka-http/current/routing-dsl/directives/index.html#composing-directives "Gotcha: forgetting the ~ (tilde) character in between directives can result in perfectly valid Scala code that compiles but does not work as expected. What would be intended as a single expression would actually be multiple expressions, and only the final one would be used as the result of the parent directive. Alternatively, you might choose to use the concat combinator. concat(a, b, c) is the same as a ~ b ~ c."
Ideally, you don't want to poll for new events but rather to listen for new events via the streaming abstraction. If you use something like Akka Streams then you would essentially spin up a FileWatcher Source which would emit events whenever those callbacks fire and you would use the rest of the streaming abstractions like Flow and Sink to see those events propagate and react to them. I'm not sure if you have used Akka Streams but it goes Source \~&gt; Flow (0..n) \~&gt; Sink Once you connect those components together, you get a graph that you can run. It works in a similar way for FS2 and Observables
I have used Akka streams. This does make sense. I was hoping I could just publish the events to a state monad and have the application cycle take events out of that. It seems though that is exactly the void streaming abstractions are trying to fill...maybe?
I have used Akka streams. This does make sense. I was hoping I could just publish the events to a state monad and have the application cycle take events out of that. It seems though that is exactly the void streaming abstractions are trying to fill...maybe?
You should take a look at FS2 in that case which is the functional approach. I don't believe State will give you what you need because you are looking at potentially unbounded usage right?
I believe so. The application wouldn’t be under high load. Maybe 10 events a day so I was thinking Statem would be fine. Wrong assumption?
I believe so. The application wouldn’t be under high load. Maybe 10 events a day so I was thinking Statem would be fine. Wrong assumption?
You can definitely try with the state monad as a start, just make sure that memory is not increasing over time. Don't think about it on a per day scope, think about it on a yearly scope. I would personally take this as an opportunity to use FS2 😉
Not disagreeing with the streaming suggestion. Just trying to understand where my assumptions will end up failing. Thanks for the replies!
Cheers mate! Good luck. Let me know how it pans out :-)
I kind of hope so, they will bring tagless final into very nearly no boilerplate. From the gist attatched to the other comment: def tf0\[T\](implicit e: Exp\[T\], m: Mult\[T\]): T = e.add(e.lit(7), e.neg(m.mul(e.lit(1), e.lit(2)))) becomes def tfm1\[T\]: Ring\[T\] = add(lit(7), neg(mul(lit(1), lit(2)))) It may not seem like much, but composing different algebras for injection will be a lot more concise.
Take a look at https://functional-streams-for-scala.github.io/fs2/guide.html#asynchronous-effects-callbacks-invoked-multiple-times. I used this pattern to build a fs2-based wrapper about Google's PubSub Java library, and it works well. 
Thanks! fs2 was suggested and it looks very promising
Just a note there is a weekly jobs thread here. Also, you haven't posted any information about the company or how to apply.
Indeed. The difference is that exceptions are a special-case language feature that break (or perhaps extend) the rules of the language; monadic error handling lets you achieve the same thing in plain old code that could even be implemented as a library rather than a core language feature.
Or both, if you're Scala. (Unfortunately requirement of JVM compatibility, but at least `Try { ... }` bridges the two.
We're still talking about `divide`, aren't we? There's nothing to be parametric about. There's no need to call-out any kind of catching because that's not part of the public API (as far as a caller is concerned this is just a function that returns a value and doesn't throw). So there's no value in using `ApplicativeError` over `Either`; they're just the initial and final encoding of the same thing, and very much equivalent. `Either` is a more direct, clearer API (at least in Scala) and so the better implementation; typeclasses and polymorphism are overengineering when returning a plain value will do.
Nice! seems like this is what [/user/somethingconcon](https://www.reddit.com/user/somethingconcon) is looking for :)
Looks interesting! But the job page says "We are not able to offer Visa sponsorship at this time". So just to confirm, is it still the case? I'm currently working in the US on an H1b visa, do you guys support H1b transfer?
Getting away from having to deal with exceptions is doable though; you can avoid them in your own code, use a linter to enforce it, and avoid using libraries that throw and/or catch explicitly at the boundaries. (Though fwiw I prefer using the combinators from `scala.util.control.Exception` to catch specific exceptions in `Either`, rather than using `Try` and having everything decay to `Throwable`).
I just talked with some of the folks in HR. Unfortunately, they said it is difficult for a company in our position, but not completely impossible. I'd definitely encourage you to apply and I'm sure they'd be happy to talk and discuss the circumstances. 
Hello, start earning money by paid surveys. [http://www.clixsense.com/?8172285](http://www.clixsense.com/?8172285)
if you still need someone late next year i might apply. I have work I have to get done here first though
Lol. Trump is the president. The government functions horribly. This is coming from someone with 15 years of government service. I quit government service last year and will NEVER go back. Be warned, fellow Redditors. 
That's unfortunate that you had a bad experience. We're fully private though. And we believe doing something to change things is more constructive than just voicing objections. 
Just voicing objections? Well that is a bit condescending. You realize you are on a public forum right? This is what people do here, voice opinions. You have no idea what I was doing for the government. I tried to help. The government is broken and has been for over a decade. This is my opinion: anyone considering this position should take another more interesting position away from the incompetence of the US government. There are far too many opportunities in the world to subject your self to it.
Also, if you have to say “not actually bad” legacy PHP, that means it is probably horrible. All PHP is pretty much horrible... this is why you had to write that caveat. 
I heard that cake patten is anti-pattern. Is thin-cake pattern an anti-pattern ? http://di-in-scala.github.io/#modules
Ok, I'll bite. What would you expect and what has been your typical experience? I agree that we can do much more with the internet than we do/did with phones, but how do you propose we make that compatible with the need for privacy in real terms? That is, how are you supposed to be discreet about your interviewing while you're currently already employed if you're sitting around with your laptop jabbering with someone via Skype or the like? Not only that, but there are real technical impediments still to even doing that reliably. Hell, getting a reliable phone connection for an issue can be a challenge depending on your location or network. Then marry that with technical issues with device drivers, latency, blah blah blah... and pretty soon you'll have folks begging to just show up for a face to face interview in a suit. Turns out, it's easier than you think.
Some people might not agree with your choice to post but as a person who also suffers from some of the brain bugs I can sympathize. And truthfully I feel that you are within your own right to post something like this. This forum represents the Scala community, yup that's you and yup that's Lightbend as well. Cheers my dude. Hope you get that interview :D
Hello! What kind of weekly jobs thread? You can text me via private message to apply and I'll share with you all information.
Hi! Okay, sure :)
I don't think this is a Scala version issue, since Apache POI is designed for Java, and, as far as I know, Scala isn't involved in its development. Have you tried a clean build?
&gt;(An empty list is actually a list of just Nil, correct?) No, an empty list IS Nil, it's not a List of Nil (e.g. it's not List(Nil)) &gt; Is the second case really necessary? I think you're right, it might just be to showcase match use case, and also might provide slight performance benefits
You defined it for List[Double], so List[String] is not a valid argument for this function. The implicit stuff you saw has nothing to do with the Nil here. An Empty List is Nil, that's different to "An empty list is actually a list of just Nil" the second case is just makes it return a value faster, since if you have 1 billion other numbers after the 0, you won't have to calculate those
&gt; Why exactly does case Nil return 1.0? I ran some tests in the REPL and an empty list of type List[String] results in an exception that hints that there is an implicit value available for an empty list of types List[Double] and List[Int] (I didn't test any others). Is it Nil itself that has the implicit value when it's part of one of these list types? (An empty list is actually a list of just Nil, correct?) No implicits are involved. `Nil` is the empty list, not a list of `Nil` (these are historical names). &gt; Is the second case really necessary? I did not run any tests, but the 3rd case should cover the second and give the same result, no? (If any element in the list is 0.0 you'll get 0.0 back unless I'm missing something.) Agreed. I guess it short-circuits the recursion since once the product is `0.0` it will always be `0.0` whatever else you multiply by.
Hi, it is fairly common to use the neutral element of the binary operation as default when folding/reducing over some empty collection. So for multiplication over reals it is 1.0, for addition it would be 0.0, for list concatenation the empty string, for boolean 'and' it would be true, for boolean 'or' it would be false. For the second case in your pattern matching example, it is the absorbant element of the binary operation that is used. It is a shortcut optimisation. The underlying pattern is fairly generic, you can use traits/implicits to factor it out. ``` scala&gt; :paste // Entering paste mode (ctrl-D to finish) /** Some algebraic structure. */ trait MyAlg[T] { /** binary operation */ def op(x1: T, x2: T): T /** property forall x, op(x, neutral) == op(neutral, x) == x */ def neutral: T /** property forall x, op(x, absorbant) == op(absorbant,x) == absorbant */ def absorbant: T } object MyAlg { implicit val doubleMultAlg = new MyAlg[Double] { def op(x1: Double, x2: Double): Double = x1 * x2 def neutral = 1.0 def absorbant = 0.0 } implicit val boolAndAlg = new MyAlg[Boolean] { def op(x1: Boolean, x2: Boolean): Boolean = x1 &amp;&amp; x2 def neutral = true def absorbant = false } } def reduce[T: MyAlg](xs: List[T]): T = { val m = implicitly[MyAlg[T]] xs match { case Nil =&gt; m.neutral case x :: _ if x == m.absorbant =&gt; m.absorbant case x :: xs =&gt; m.op(x, reduce(xs)) } } // Exiting paste mode, now interpreting. defined trait MyAlg defined object MyAlg reduce: [T](xs: List[T])(implicit evidence$1: MyAlg[T])T //now you can write this reduce(List(12.0, 1.0, 36.0)) reduce[Double](Nil) reduce(List(true, false, true, true)) reduce[Boolean](Nil) scala&gt; reduce(List(12.0, 1.0, 36.0)) res0: Double = 432.0 scala&gt; reduce[Double](Nil) res1: Double = 1.0 scala&gt; reduce(List(true, false, true, true)) res2: Boolean = false scala&gt; reduce[Boolean](Nil) res3: Boolean = true ``` 
Imagine you were doing a recursion. Eventually, you would have to do `something * product(Nil)`, when you get to the base case. Imagine you're just doing it on a single item list if it helps: `3 :: Nil` So, `product(3 :: Nil) = 3 * product(Nil)` If it were anything other than 1, it would ruin the rest of the calculation. If it were zero, the whole product would be zero. If it were 2, your result would be twice what you'd expect (in this case 6). `Nil.product` has no choice but to be 1.
Hi, living in Amsterdam, working as Scala developer at Tnt in microservices. Tech stack is type level ( cats, http4s, Circe, Doobie, cats-effect), strong functional background and passion. Also using Kafka /Kafka streams). Close to 3 years working professional experience with Scala. Would like to hear more :) 
&gt; I was hoping I could just publish the events to a state monad. [...] Wrong assumption? You can't "publish" to a state monad :) `State` models only one specific type of stateful computations: completely sequential state chains. Otoh, "publishing" always entails some form of concurrent state, which needs to be shared between producer and consumer. This kind of stateful computations is modelled with `IO` (or `STM`, but we don't have that in scala). In this case, the shared state is an `fs2/Queue[F, A]` (and ultimately implemented with cats-effect `F` and `Ref` + `Promise/Deferred`). You can then get a `Stream[F, Event]` out of that which is nicer to work with. I hope that makes things a bit clearer.
He has very strong opinions, period. You have to factor that in to understand what he really means
Note that fs2 already has a file watcher: https://github.com/functional-streams-for-scala/fs2/blob/series/1.0/io/src/main/scala/fs2/io/file/Watcher.scala 
sup fabio, thanks for the reply. Appreciate the clarification and (in your other comment) pointing out the Watcher. I found that yesterday and was like YUSSS I can delete all of my code that does this manually! Great stuff. I think you nailed what I was trying to explain. Still kinda new to wordsing FP stuff. If you could also help with some follow up. What's the difference between IO and STM? 
&gt; because it's completely unlike Java It really isn't though. Can you even name 5 languages that are more similar to Scala and Java than they are to each other? Kotlin, C#, maybe Groovy? Do you have any examples of Java programs that can't be directly translated into Scala? What features does Java have that you think Scala is lacking? I don't think there are many.
I had never actually seen this much in the wild since it is so generally lambasted. Thanks for sharing the cautionary tale. 
Hi :) So, this isn't relevant for Scala, because we don't have a nice STM yet (at least the kind of STM I'm referring to). You'll have to reach for Haskell to use it. I'm assuming you kinda already know about cats-effect `IO`: it's a purely functional abstraction to represent arbitrary effects (including concurrency), on top of which you can have concurrent primitives like `Ref` and `Deferred`, which can be used to build `Semaphores`, locks, queues and so on. The problem with a model of concurrency based on locks is that is not super composable, if you have a piece of code that's been carefully designed to be concurrency aware, and another piece of code that's also been carefully designed to be concurrency aware, just putting them back together doesn't mean that the resulting code will work correctly. STM, which stand for Software transactional memory, is an abstraction for composable concurrency. data STM a -- abstract instance Monad STM -- among other things atomically :: STM a -&gt; IO a data TVar a -- abstract newTVar :: a -&gt; STM (TVar a) readTVar :: TVar a -&gt; STM a writeTVar :: TVar a -&gt; a -&gt; STM () retry :: STM a orElse :: STM a-&gt;STM a-&gt;STM a throwSTM :: Exception e =&gt; e -&gt; STM a catchSTM :: Exception e =&gt; STM a -&gt; (e -&gt; STM a) -&gt; STM a STM basically gives you a monad for computations that are supposed to be atomic (which will have type e.g. `STM Int`) and a data type called `TVar` (transactional variables). Basically you can write arbitrary computations in STM, and when you translate to `IO`, you are guaranteed that the whole computation executes atomically. This is done with a transaction scheme that isn't too dissimilar from the one used in DB: a computation is tried, and then either committed or rolled back until it succeeds. What makes `STM` super nice are `retry` and `orElse`: `orElse` will execute an alternative computation if this one rolls back, and `retry` will block on an arbitrary condition (I'm simplifying a lot here). The really nice thing about STM is its composability, if you have to STM computations, they are atomic, and when you compose them together the resulting STM computation is _also_ atomic. This is a quick'n'dirty summary, you can find more here: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/beautiful.pdf or in the book Parallel and Concurrent Programming in Haskell 
I did unfortunately it seems to be a maven issue the poi class is not being loaded from my jar even though I am compiling with dependencies . It is being loaded from someone else which is causing a `[Loaded org.apache.poi.xssf.model.ThemesTable$ThemeElement from file:/usr/local/lib/radoop/rapidminer_libs-7.6.1.jar] ` `java.lang.NoSuchMethodError: org.apache.poi.ss.usermodel.Cell.getCellTypeEnum()Lorg/apache/poi/ss/usermodel/CellType;`
Future is a really poor abstraction when you need this kind of control. There are several popular streaming solutions (akka streams, fs2, monix) that can throttle and rate limit fairly easily, either using built-in features, minimal custom code, or add-on libraries. As soon as you start to want any more say in how your async code is evaluated, you would do well to look into one of these options, or you just start re-implementing things that they've already solved for you.
https://doc.akka.io/docs/akka/current/stream/operators/Source-or-Flow/throttle.html I started using streams for this kind of things
Ah, yes I should fix this :)
You can translate a BASIC program into Haskell, does that mean BASIC is close to Haskell? Scala is a superset of Java, but Scala's idioms are very different, so translating back is not possible. &gt; Do you have any examples of Java programs that can't be directly translated into Scala? Strangely enough, yes! For one, Scala can't create advanced java annotations (runtime-preserved, compile-time only, anonymous function argument annotations, etc.) There's a lot of nuance in Java annotation language now and Scala never cared to support it beyond just letting you use Java's annotation)
I would say: * *majority* of libraries made only with Scala (no Java deps) are not using nulls nor throws exceptions * Scala wrappers for Java libs *usually* handle nulls and exceptions internally * virtually *any* time you use Java lib directly you'd better assume that it can return null or throw I use null only if Java libs require it (some libs use nulls as some sort of defaults...) and exceptions (Throwable) are also used as fixed error type for many MonadErrors ( :( ), so I kind of have to use them as well (though not by throwing).
There might be, but the right thing is to separate concerns. Put a limit in the thing that actually needs to be limited (probably `barService`?) rather than trying to do something at the `Future.traverse` level.
The code I'm concerned with is a run once daily job on the off hours that finishes in a few minutes but runs into errors with rate limiting. The `barService` doesn't need any throttling with it's normal use though
The simplification is great. Usually I only need the high level breakdown of a type or instance to then reference to code and have the AHA! moment(s). I do see why this would be useful. What's preventing Scala from having something like this? side note - Funny that this conversation is a great primer for Parallel and Concurrent Programming in Haskell. It's next in my queue. As always, your explanations are extremely helpful. You're one hell of teacher.
&gt;The history of Typelevel given here is false, FWIW; Typelevel predates cats (indeed scalaz was at one point part of typelevel). I might fix the history of TL, though there are hardly any good written sources. There was a TL before cats (e.g. post from 2013), but it all gives a feeling that before CoC and whole Tony Morris drama (2014) it was just a club for a friends interested in doing pure FP Scala. Post CoC it become more of a movement/community lead by TL gang.
Where is the `Get(e)` from? how does `runForeach` know to turn the `e: Any` into the proper type, in this case String?
Sounds like it does, you're just not hitting the limit in normal use. I mean, the problem here isn't that you're running `Future.traverse` too fast, the problem is that you're calling this 3rd-party API too fast. So the code that talks to the 3rd-party API is the place to put the limiter.
No ones seriously advocated for the cake pattern for over half a decade now, and there's plenty of people, even Martin himself who say the the cake pattern works, but just doesn't scale. Some people think the pattern will have a revival after trait parameters are introduced, but I am skeptical of this, even if it may improve the situation somewhat.
it's just pseudo code, Get is supposed to be some kind of Future based async Http library which makes a get request on the url. e isn't any in this case then, it's the result type of Get (for example HttpResponse)
Congratulations on writing this thesis. Even more so since english is not your mother tongue I assume (unless you grew up bilingually). I hope you get a good offer from SAP or some other large company that uses Scala.js and wants you to keep working on it (unless you wanna do other things). I wanted to dive into the the inner workings of Scala.js and Scala Native out of curiosity since a while ago and this seems like the perfect starting point. Out of curiosity, are there requirements that a platform/runtime needs to fulfill in order to target it with a compiler plugin like Scala.js or Scala Native? * like having a GC available, or at least the possibility to provide your own like Native did with boehm and now immix? * would the reified generics in .NET/IL be problematic/make things more compilcated? I would assume for just running scala they can be ignored but that ignoring them would make interop painful/impossible? * what about interpreted languages like PHP which eliminate all state after a request is done? Also, would there be a clear winner between targeting a language like Java over JVM bytecode? Cheers
Oh, thanks :) As for scala STM, it's not easy to implement, especially with good performance - Haskell's version is implemented in C directly in the runtime system.
I work with Twitter Futures and not Scala Futures, so this may be subtly wrong, but you can do something like this: def traverseChunked[A, B](items: List[A], chunkSize: Int = 50)(f: A =&gt; Future[B]): Future[List[B]] = { val chunkedItems = items.grouped(chunkSize) chunkedItems.foldLeft(Future(List.empty[B])) { (intermediateResult: Future[List[B]], chunk: List[A]) =&gt; for { completed &lt;- intermediateResult processedChunk &lt;- Future.traverse(chunk.map(f)) } yield completed ++ processedChunk } }
Ah, thanks for the response. So for compile time know values you get a compiler error and for runtime values you get Either as a return type?
right now we have only Intelij though, no?
It's a dependency version conflict. You can try to **exclude** POI transitive dependency from rapidminer, and include POI dep version (which has this method) explicitly in pom.xml.
I tried modifying the pom to accomplish this but I am getting this error now `org.shaded.apache.poi.POIXMLException: java.lang.reflect.InvocationTargetException` subset of the pom `&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;package-uber-jar&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;org.apache.http&lt;/pattern&gt; &lt;shadedPattern&gt;org.shaded.apache.http&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;relocation&gt; &lt;pattern&gt;org.apache.poi&lt;/pattern&gt; &lt;shadedPattern&gt;org.shaded.apache.poi&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.spark:*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;`
What you want is to fail the future on rate limit and retry with exponential backoff. You can build it yourself, but akka streams, fs2, and akka all have retry as part of their libraries. Here's a gist that shows how retry with backoff works. Read the comments: [https://gist.github.com/viktorklang/9414163](https://gist.github.com/viktorklang/9414163)
Word. Sounds like I need to learn more! Going to crack open Parallel and Concurrent Programming in Haskell this weekend hopefully!
How did you learn Spring?
I don't know what it means either, but the maintainers of a library can probably help guide you if you want to study in depth. And that's usually where I start … read the doc and then talk to the maintainers if I have questions. Like, for [Cats](https://typelevel.org/cats/) I recommend reading [Scala with Cats](https://underscore.io/books/scala-with-cats/) and maybe exploring the [Infographic](https://github.com/tpolecat/cats-infographic) and asking questions on [Gitter](https://gitter.im/typelevel/cats). In any case I think most of those lists on job descriptions are aspirational … ideally you know something about some of these. But I wouldn't worry about it too much.
I've had serious problems with larger multi-module projects. I decided to give up on Gradle and use SBT instead.
&gt; in my opinion it brings no actual value This suggests you would not be happiest working somewhere that invests in functional programming. Is there a reason you feel inclined to learn FP beyond seeing job openings that mention cats? I believe there's still a reasonable job market for scala-as-better-java (my impression is that FP shops are still the minority,) and FP is a pretty deep topic that takes a reasonable commitment to progress with. Personally I think it's absolutely worth the effort, but I don't know if it's a terribly efficient way to widen your options for employment.
It's not like I am against learning it. It's more like the pejects I am working with are not FP oriented and thus I lack the motivation to actually improve in this area.
Pretty standard I guess. Reading docs and actually using the thing I learn. I have never worked on a FP project and thus I just don't know where should I aim when building the codebase.
Yeah but I am wondering more like how is cats actually used in commercial projects. Thanks for the tips. I didn't like the cats docs tbh. 
&gt; like having a GC available, or at least the possibility to provide your own like Native did with boehm and now immix? One of these two options is a must, yes. &gt; would the reified generics in .NET/IL be problematic/make things more compilcated? I would assume for just running scala they can be ignored but that ignoring them would make interop painful/impossible? It was already tried, years ago. The reified generics proved to be too painful indeed, and the effort was abandoned. Now, many Scala-to-JS efforts have been tried and failed before Scala.js, so it is no proof that there does not exist any way to make it work, somehow. But nobody has managed to do so at this point. &gt; what about interpreted languages like PHP which eliminate all state after a request is done? That is not an issue. Obviously the state of your Scala/PHP page would also be eliminated after each request. The important insight here is that you don't want to try to compile the application paradigms to any random platform. You want instead to adapt to the paradigms of said platform. So Scala/PHP would be used in a paradigm where state is lost been requests. Similarly, Scala.js is used in a paradigm where there is exactly 1 thread, and blocking is simply not possible. &gt; Also, would there be a clear winner between compiling to a language (let's say Java) vs compiling to the platforms bytecode to which the language compiles (in this case Java)? Absolutely not. This doesn't make any real difference. At least from the point of view of portability and interoperability that I have explored. In terms of performance, it is possible that for some languages, compiling to the bytecode allows the compiler to exploit some lower-level features that the language would not give access to. For example, Scala/JVM exploits the unrestricted `goto`s of the JVM bytecode, which Java does not expose. On the other hand, for some language/VM pair, compiling to the bytecode might require significantly more effort from the compiler, whereas reusing the language's compiler would allow more code reuse. This used to be the case for C vs x86/x86_64, but LLVM kind of solved that issue.
How different is Twitter Future from Scala Future? Does anyone other than Twitter use Twitter Future?
If you can solve the motivational issue, I found this to be one of the more instantly gratifying paths of the many I tried - https://underscore.io/books/scala-with-cats/ FWIW, for myself and many I've talked with, FP has a curve the size of OOP, where sometimes the benefits that a technique yields are not quite visible until after you've spent substantial time getting used to it. But on the other hand, some techniques are much quicker to pay off, like in the case of writing a substantial amount of monadic code based on a right-biased Either type, it becomes immediately apparent how valuable it is to have the compiler force you to tag every line of code with some kind of failure case value. I guess what I'm saying is you probably can benefit from a decent amount of faith and a reasonable suspension of skepticism, because there can be stretches where it's not really clear what you're going to get out of the effort.
They support (generally) the same functions, but the API and implications are quite different. They appear to have taken inspiration from one another. Twitter futures are used in the Twitter open source tools, while others open source (akka, play, others) tools typically use the Scala futures. You can use both in a single project but must be careful about converting between the two and managing the multiple resource pools required to execute the tasks
I still hold a torch for Eclipse as there are things it does better, though I appreciate I'm in the minority.
&gt;You can translate a BASIC program into Haskell, does that mean BASIC is close to Haskell? You can't *directly* translate a BASIC program into Haskell. For example, what's the Haskell equivalent of `GOTO 20`? &gt;it's completely unlike Java &gt; &gt;Scala is a superset of Java Which one is it going to be? Of course Scala is not a strict superset of *the latest version of Java*, but most Java users are not going to find the inability to create advanced java annotations (or other fringe features) particularly limiting.
The title should be Scala improvements, as the link is to the Scala anchor on the page
Backup your `*.iml` files, because this version re-generates them differently, in case something went wrong. (We keep those in `.gitignore`).
Ahh, this is making things click a little bit. I was thinking about this incorrectly (obviously). It's not that the product of the elements of an empty list = 1, it's that the method we're defining needs to return 1 (or 1.0 for Double) when the `List` is `Nil` in order to return the correct product of elements of any non-empty list. Is that right?
I just want to come back on this and hear your thoughts. The exercises are built atop of eachother using knowledge from the previous. Wouldn't it be hard to skip one and come back to it? I was wondering if you think it was a good or bad idea to perhaps work on a problem for 1-3 hours, and if I can't get it done by that time frame, then to perhaps ask it on here or stackoverflow? I am not sure if that would be against the authors intentions since I do not see any mention of how he would approach this book. Thanks again! I'm on chapter 4 now and its really fun(but rather challenging) so far.
Started to use 2018.2 today on Ubuntu (xfce with "focus follows mouse"): Whenever I switch the desktop workspace away from where Intellij is on and move back, Intellij has changed its z-order to be underneath all other windows (on that workspace). Somewhat annoying. It looks like a regression.
You can use `grouped` on myList to get N elements, where N is the number of operations per unit time. Then traverse over each group with a sleep in between.
I believe Twitter Futures use an event loop while Scala futures use thread pooling and a scheduler. However, they're both similar in that they execute tasks as soon as they are scheduled, instead of waiting to be run, like IO, Task, etc. 
Been using focus follows mouse for some weeks now and another thing totally broken is global search with shift shift, it just closes the popup after typing 3 characters. You need to click on the popup most of the time for it to work. So annoying. Would like for these problems to be improved but when I looked on the web they seem old and really hard to fix on intellij so not much hopeful.
For cats, I recommend ignoring the type classes for now and learning things one [data type](https://typelevel.org/cats/datatypes.html) at a time. These are all fairly isolated and serve a specific purpose (so you won't have a need for all of them). For me the gateway drug was [OptionT](https://typelevel.org/cats/datatypes/optiont.html). I understanding the motivation for this type is pretty easy. Do you ever have to call a web service that may or may not return a result? If you do you might end up with results that have the type `Future[Option[A]]`. If you have results of this type what happens if you want take that result and do some calculation so you end up with a new value of `Future[Option[B]]`? You end up having to do something kind of awkward: val apiResponse = ... // webservice response value val calculatedValue = apiResponse.map( _.map { actualResponse =&gt; // ... do stuff } ) You have to unwrap it twice to do anything. This gets even worse if you have 2 values of similar structure that you need as inputs to a calculation. This is where `OptionT` comes in. It allows you to treat these double layer wrappers as a single layer wrapper. So, the above example can be rewritten as: val apiResponse = ... // webservice response value val calculatedValue = OptionT(apiResponse).map { actualResponse =&gt; // ... do stuff }.value From there it is not hard to see the utility of `EitherT` and `Nested`. Once those make sense you will start to the utility of the type classes since that is the only way you can write those data types. 
Dear JetBrains, I never wanted to create Kotlin classes and never will. Please allow me to remove Kotlin support from IntelliJ.
This book has been mentioned here already and I have started reading it. I actually use eithers in my codebase a lot. But that's pretty much it. Have written some monoids here and there. But I didn't see anything above that. This book looks really cool as it allows me to see how should I structure my code. E. g. using read monad as a DI tool is a clear example for me.
Thanks for the excessive answer. It's not like I don't understand the benefits of monad transformers. It's more like - how do you actually know if this thing is actually still a monad. As an example - is OptionT[Future, Int] a monad or not? It is not but the code works anyways. While the abstraction layer keeps growing I am afraid to see unexpected behavior of such code.
amen
As long as you understand and remember the answer. :) Perhaps it would be good to try to solve it yourself again after a few days of receiving help.
me `for {}`
You're right, but we actually _define_ it to be 1. It _has_ to be 1 because of the above, and so we say it is defined to be 1 for all implementations. It's the same as why 0^0 = 1. It's technically undefined, but really, deep down, there are hidden reasons why it should equal 1.
Thanks for taking the time to answer my layman questions, really appreciate it. While reading your thesis I came across the link to: http://lampwww.epfl.ch/~doeraene/thesis/sjsir-semantics/ and was wondering if you did have to write all of that by hand, or could you maybe leverage the scala.js source code by analyzing it programatically and then auto-generating parts of the document?
Will they now let me open two projects in the same window.....It drives me nuts switching windows all the time
Cool post. I think more Scala libraries should do things like this: using clever type-system magic, but in a way that even an un-sophisticated user can benefit from without needing to fully understand the cleverness. Far too often, I see libraries end up forcing the user to learn everything the library-author had to learn, which is tough given library authors are almost by definition more advanced than the typical user. One rule of thumb I've used regarding implicits (everyone's favorite bugbear) is to only by-default use implicits present in companion objects, whether implicit conversions or implicit parameters. That way they are always automatically resolved, and a user can benefit from you e.g. using implicit constructors/typeclasses for method-type "overloading" at a callsite, but never have to deal with issues like missing imports or collisions themselves. Furthermore, if they do get confused by the unfamiliar type in the signature that is being implicitly-constructed, it's only one jump to go to the companion object of that type, which we have by-convention declared all implicits to reside within. So at least they can puzzle through the implicit behavior based on what's immediately in front of them, rather than having to go hunting high and low for the missing implicits they need to import all over a library's codebase.
&gt; You can't directly translate a BASIC program into Haskell You can as a DSL! http://augustss.blogspot.com/2009/02/more-basic-not-that-anybody-should-care.html &gt; Which one is it going to be? These statements do not conflict. "Scala can do everything Java can, but is a completely different language" - what's contradiction here?
&gt; It's more like - how do you actually know if this thing is actually still a monad. How do you know if a class that implements `java.util.Collection` is "actually still" a collection? You could say that any class that provides implementations of all the methods counts, but actually there's more to it than that - e.g. if `isEmpty()` returns `true` but `size()` returns 10, you'd say that's a broken collection. There's a formal version of this when people talk about "laws", but often the laws for a type are informal or rely a bit of judgement. E.g. is a Scala `Stream` "still" a `Seq`? It implements the interface, but a lot of methods that take `Seq` won't work for `Stream`, so for some purposes a `Stream` is a perfectly good `Seq` and for others it isn't. There's no one true answer here. And so it is for `Monad` (with the slight wrinkle that rather than implementing a `Monad` interface directly, we have typeclass instances for monadic types). The laws for monad are better-known and more formal than for most concepts, but they ultimately still rely on you defining which expressions are "equivalent" or not. For what it's worth I'd say that `OptionT[Future, Int]` is a perfectly good monad, as legitimate as any other that you'll ever find in Scala. Just like some methods that accept a `Seq` will sometimes misbehave when passed a `Stream` - or even when passed anything that isn't a `List` - some methods that accept a `F[_]: Monad` will sometimes misbehaved when passed an unusual `F` - or ever when passed anything that isn't `Option`. Monad is clearer than most interfaces though because the rules are well-known and any code that works with those rules will preserve your notion of "equivalence", whatever that is. That is to say, if a method that accepts `F[_]: Monad` does something that you think is different from what it should do, what the method did will be equivalent to what it should have done, in the same sense of equivalence that lead you to say that the monad implementation you passed conforms to the monad laws.
I did have to write it by hand, I'm afraid ;) Writing a smart enough tool to do anything useful would be longer than directly writing the document. There is tool for the formatting and the hyperlinks, though: https://github.com/bterlson/ecmarkup
I don't know if anything changed in the new version, but in 2018.1 you can simply remove the plugin (in Settings &gt; Plugins), that way you don't get your popup menus cluttered with useless 'New Kotlin class...' items.
&gt; another thing totally broken is global search with shift shift, it just closes the popup after typing 3 characters This seems to be a problem with the bundled jdk. Set JAVA_HOME to a jdk you've installed yourself and it goes away. I can't find the link but it worked for me. 
In the age of microservices, having two or more projects open at the same time. 
`InvocationTargetException` is just something that tells you the thing was invoked reflectively. The real error will be in the "caused by". There is no way to do relocations safely when code uses reflection, so that's probably your problem. Why is that rapidminer_libs jar on your classpath? Can you run your code without that? Failing that, can you build against the same version of poi that's on your runtime classpath? Fundamentally if you can't make your compile-time classpath look like your runtime classpath you're going to have a bad time.
Twitter Futures on the default pool are an abstraction on top of Java Futures (see their FuturePool source). It's not really an event loop, but more it guarantees where and when the code is executed. That is, Scala futures execute in a new thread every time you wrap something with `Future`, while this isn't true for Twitter Futures (FuturePool creates the Runnable). Twitter Futures rely on asynchronous behavior within the code you're calling. Scala's original Futures implementation was tightly coupled with the actor package and code. Twitter created a standalone Future implementation to address their own needs. Scala borrowed in part from Twitter's implementation and the existing actor implementation to create what's the today. You can use Twitter's bijection framework to convert between the two implementations. If you're using Finatra, latest versions support both types of futures as return values from controller methods. Source: I work at a large non-Twitter company and my team uses Twitter frameworks exclusively.
Neat trick. Unfortunately \`U#Value\` will no longer work in dotty/scala 3.
The new "show implicits" feature is insanity. It's like exploring a hidden world.
You can already do that by adding another module(what everyone else calls projects) to the project(what everyone else calls a workspace).
Thanks! 
No! I use ENSIME daily! I didn't do anything other than follow the documentation on the website: Here's my \~/.sbt/1.0/plugins/plugins.sbt `addSbtPlugin("org.ensime" % "sbt-ensime" % "2.5.1")` Here's my emacs ensime setup in my init file (\~/.emacs.d/init.el): `(use-package ensime` `:ensure t` `:pin melpa-stable` `:config` `(setq ensime-startup-notification nil)` `)` `(use-package scala-mode` `:pin melpa-stable` `:interpreter` `("scala" . scala-mode)` `:config (lambda () ;\;\ For complex scala files` `(setq max-lisp-eval-depth 50000)` `(setq max-specpdl-size 5000)` `(setq prettify-symbols-alist scala-prettify-symbols-alist)` `(prettify-symbols-mode)))` `(use-package sbt-mode` `:pin melpa-stable` `:commands sbt-start sbt-command` `:config` `;\;\ WORKAROUND:` [`https://github.com/ensime/emacs-sbt-mode/issues/31`](https://github.com/ensime/emacs-sbt-mode/issues/31) `;\;\ allows using SPACE when in the minibuffer` `(substitute-key-definition` `'minibuffer-complete-word` `'self-insert-command` `minibuffer-local-completion-map)` `(setq sbt:program-options '())` `)` `(use-package projectile` `:demand` `:init (setq projectile-use-git-grep t)` `:config (projectile-global-mode t)` `:bind (("s-f" . projectile-find-file)` `("s-F" . projectile-grep)))` `(use-package goto-chg` `:commands goto-last-change` `;; complementary to` `;; C-x r m / C-x r l` `;; and C-&lt;space&gt; C-&lt;space&gt; / C-u C-&lt;space&gt;` `:bind (("C-." . goto-last-change)` `("C-," . goto-last-change-reverse)))` I use sbt to build my projects, hit M-x ensime, wait till the ENSIME buffer says Indexed ... added ... rows and hit C-c C-b c to recompile the project, and I'm all set. I haven't been able to get the ensime server to work with vscode or anything, but metals looks promising over there. Projectile and goto chg are not neccessary, they just make searching and navigating your project easier.
Hey, jackcviers, just a quick heads-up: **neccessary** is actually spelled **necessary**. You can remember it by **one c, two s’s**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Out of curiosity, what OS are you on? On Ubuntu there's a different shortcut for switching Windows of the same application, which makes it a lot easier (as do its other window management features).
Don't try to do it when importing SBT projects with the shell enabled, it will cause all kinds of havoc (IntelliJ will often try to refresh the wrong project when the `.sbt` files change).
To build on what Daxten said, you can do this with Akka Streams to achieve a throttled version of traverse Source(myList).throttle(1, 1.second) .mapAsync(parallelism = 1)(barService.fooFuture) .toMat(Sink.seq)(Keep.right) .run() // Future[List[YourResult]]
for coding, windows
I thought it was one maven project per project. If I have two maven projects that each have their own set of modules, then I understand they can't be opened in the same idea window unless I merge the poms. Am I wrong....I would love to be wrong
I have updated the error message in the question . I ask myself the same question with regards to the rapid miner jar but there is nothing I can do about it unfortunately.
All I can suggest is figuring out what version of rapidminer it is, adding it as a provided scope dependency, removing your explicit dependency on poi and building against the one in the rapidminer jar instead. 
We were also looking at overops but their prices are insane, so we are now checking out google stack driver error reporting plus debugger. Also sentry looks decent. Of course overops looks great but as a startup we cannot afford 50k a year
Yeah the pricing is pretty insane. Especially because they're dropping their low-quantity VM plans.
&gt; You can as a DSL! That's an embedding, not a translation. Of course you can write an interpreter in Haskell. &gt; Continuations are directly equivalent. I guess at least sort of... &gt; "Scala can do everything Java can, but is a completely different language" - what's the contradiction here? Well, given that you've now abandoned *both* of your previous statements, the original contradiction is gone. Amazing. /s
\&gt; That's an embedding, not a translation. Of course you can write an interpreter in Haskell. It doesn't have to be a \*deep\* embedding via interpreter, it can be a shallow DSL that mimics BASIC without an interpretation step, since Haskell does have analogues for BASIC features. \&gt; Well, given that you've now abandoned *both* of your previous statements, the original contradiction is gone. Amazing. /s You were just interpreting too literally, I was talking about 'application' of idiomatic Scala vs. idiomatic Java, not whether programming languages as objects, themselves, intersect in features. Languages may intersect, but Scala is a completely different world than a Java-dev's world.
An elasticsearch+logstash+kibana stack is pretty easy to setup. 
To dump entire stack traces with variable values without needing to do it manually for each possible error point? The big part of OverOps we loved is its extremely minimal setup and deep stack snapshots.
That is the reason. I am trying to focus on this small area. Envision it as being more of a study group.
i have been trying to get some content together to share. that is not a bad idea - just not sure i will be ready that soon. 
Good bot.
Thank you, jackcviers, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Good bot.
https://medium.com/anyjunk/how-to-traverse-sequentially-a071afacc84d
Cats is set of tools. Not framework. Know how to use each tool. You need 5-10lines of code to try out eavh tool. Do it in console or worksheet. Once you understand multiple things what it does and how to use it, you "know" cats...
We used overops and dropped it after their pricing model changed. Now we use stack driver trace, debugger, profiler, logging, and error reporting. The debugger does similar variable state capture, but does require setting up the capture in advance and won't capture it automatically.
Aside from article’s approach (where U is abstract) - the overall trick is okay. I used type-projection on a regular class with evidences to protect my DSL from misusage. Frankly, I think T &lt;: Something is often overused due to copy-pasting.
Unfortunately, in the mobile version in Reddit's web-view, the header takes up the entire screen, without the option to close it.
How could you compute the difference of diagonals of matrix in a more functional way? I have tried this but feels really imperative, however I can't think of a better solution. Could you help me? def diagonalDifference(arr: Array[Array[Int]]): Int = { val dimensions = arr.length var diagonal1 = 0 var diagonal2 = 0 for (i &lt;- 0 until dimensions) { diagonal1 += arr(i)(i) diagonal2 += arr(i)(dimensions-1 - i) } return Math.abs(diagonal1 - diagonal2) } 