I am not familiar with kotlin, but I imagine some of the more advanced fp ideas, and things like higher kinded types, f bounded polymorphism, abstract type member, etc. But I don't know how much kotlin has in fp, it's my understanding that it's not a purely functional language though.
Ok could u please explain to me how and why I should use a dataset in this case . Thanks
The original question didn't mention FP, how do you sell Scala to people who don't know about FP?
I think flatmap/map would be better than a for comp here 
Not really, no. You can write it as an extension method, though. 
I'm aware. I just mean syntactically map and flatmap are easier to read for this, in my opinion 
Picking between for comps and map/flatmap is something I do case by case. Sometimes flatmap/map is easier to read, sometimes for comps are. If for comprehensions worked similarly to Haskell's "do" , I'd say to use it all the time. It'd be much nicer. I was only stating my opinion to make the OP aware that you shouldn't always necessarily use a for comp even when you can. Just pick what looks best and what the team likes most for the scenario. 
Honestly I don't think it's worth selling at that point. Scala is an advanced language which is good because it has oop and fp constructs. If someone is not an advanced enough programmer where they aren't even aware of fp, then I don't think it's worth spending the time winning them over, they aren't going to be using these advanced features in the first place.
How would that look like?
Yet 90% of production scala code is Spark/Akka/Play-based code that's about as far from FP as it gets. Scala has some limited support for FP, but it's not an FP-first language by any means, especially in commercial software development.
recorded: https://github.com/pheymann/typedapi/issues/2 I will work on that in the near future.
Interesting. The concern I had is how to abort the whole method early with `?`. A `return` in an extension method would only return from the extension method itself, but would have no effect on the method in which `?` is used.
Something like that could work: ``` (foo andThenK _.bar() andThenK _.baz())() ``` This is simpler and less powerful than `map`/`flatmap` as you cannot access the results of previous calls. You cannot do this with Rusts `?` operator either, so it's the same level of power. Bonus: it works not only for `Option` but other types like `Either` or `Future` as well.
Yes, op asked for exactly this.
&gt;If for comprehensions worked similarly to Haskell's "do" I thought they did. How does Haskell's `do` work?
Yeah, I agree.
I just tested a quick implementation with: scala&gt; val ls = List("1","2","oops","3").map(x =&gt; scala.util.Try(x.toInt)) ls: List[scala.util.Try[Int]] = List(Success(1), Success(2), Failure(java.lang.NumberFormatException: For input string: "oops"), Success(3)) scala&gt; ls.map(_.?.toDouble) res1: List[scala.util.Try[Double]] = List(Success(1.0), Success(2.0), Failure(java.lang.NumberFormatException: For input string: "oops"), Success(3.0)) scala&gt; ls.?.isSuccess res2: List[Boolean] = List(true, true, false, true) Here is the code: import scala.language.experimental.macros import scala.reflect.macros.whitebox implicit class QOp[A](private val self: A) extends AnyVal { def ? = new QDyn(self) } import scala.language.dynamics class QDyn[A](private val self: A) extends AnyVal with Dynamic { def selectDynamic(fname: String): Any = macro selectImpl def applyDynamic(fname: String)(args: Any*): Any = ... // etc. } def selectImpl(c: whitebox.Context)(fname: c.Tree): c.Tree = { import c.universe._ val tname = fname match { case Literal(Constant(str: String)) =&gt; str } c.prefix.tree match { case q"$_[$tp]($self).?" =&gt; q"$self.map(_.${TermName(tname)})" } } It's behaves a bit strangely in some cases, though.
In addition to what people have said about type inference, pattern matching, and case classes: if you wanted to get more advanced, showing things like json mappers with type classes is an example that's easy to see the power of. Something like play json's `Json.fromJson[MyModel](myJson)` that is compile and type safe is going to be uncommon in other languages.
Hi, I'm an active maintainer for http4s, and I can give you a few reasons these benchmarks aren't a great way to judge an http framework: For one this stuff is using [an old version of http4s](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Scala/http4s/build.sbt) that's been EOL for at least 6 months IIRC. The stack around http4s has improved _massively_ with http4s 0.18, cats-effect IO and doobie 0.5. In particular, the `IO` used now is far faster than the old task, and new fs2 is _way_ faster than old scalaz-stream. On top of the prior point, which is sort of a big deal, - JSON serialization isn't part of the framework directly. We could cheat and use jackson, which is not anywhere close to being as nice as circe but is fast as balls. That would bump up our bench quite a bit. - Single query, again, it not part of the http framework alone directly, it also counts the db framework/library you're using. We could also probably cheat here and use some thing not pure but really fast + jackson - Multiple queries, again, use something close to raw jdbc + jackson - Fortunes we performed well, but I still think this isn't a great test. - Date updates, same story... - Plaintext responses: here for sure, we could probably do better, but I'd love to see it on our new stack. This isn't testing the framework alone, but a common stack used with the framework, which was measured using an out of date version that's EOL for long enough that it's not even [in our website](http://http4s.org/versions/). However, despite it being out of date on a _much_ slower stack, http4s is not meant to be a speed demon that you use as a proxy server or to server http plaintexts at light speed. A good chunk of our users that choose our stuff have much heavier work going on behind the scenes, to where the http framework isn't even close to being the bottleneck of what needs to work. Http4s enables nice, principled design that encourages not treating your programs as balls of side effects using stdlib future. As such, there will always be overhead in pure FP Scala because we interpret operations into data structures that hit the heap, which is how we reify our programs and make them pure in scala. The only thing to "take to heart" about techempower, is that it's a terrible way of making a choice for your http framework.
That's an interesting suggestion! I agree that do-while is hardly ever used; I like the simplification of having just one primitive loop.
the benchmark is not updated. It's on an old version of http4s.
This. She knows da wae
 I've pinged the last person who worked on this to see if he can provide more info, but it's generally not something we focus on. From https://www.reddit.com/r/golang/comments/7zg8ds/techempower_round_15/duomxpe/ &gt; I will begin by paraphrasing Tim Fox, the creator of Vert.x, who, at some point, some years ago, said something along the lines: "you don't look at the Techempower Benchmarks results to argue over who does 1 million rps vs 1.1 million rps. You will most likely be bottlenecked somewhere else in real life. You go there to weed out the truly bad performers, and know not use them." Realistically speaking, you'll have a latency profile and you'll look at how many users you can support with 99.9% latency outliers. Typically that will be about 70% of your service utilization, because that point you'll start to see the CPU work queue back up, and your response time vs latency will start to show queued up work more often. So a useful devops rule of thumb is to start horizontal scaling around the 70% CPU mark. In a real life situation, a CPU also spends a lot of time doing other things than serving requests -- it serves metrics, logs, sends off work to backend servers for database and emails, and it spends lots of time waiting in between network traffic. Play is optimized to slot work in between the idle spaces in the CPU, using a fork/join thread pool with work stealing, making the best use of the CPU overall. I wrote about this in more detail in the [white paper](https://info.lightbend.com/white-paper-play-framework-the-jvm-architects-path-to-super-fast-web-app-register.html).
I think this exchange is a bit extreme, on both sides. first, your OP: &gt; People who don't know about fp people mostly know about FP, but only from college. Scala would be their first commercial FP experience. That was true for basically all of my (~10 people) coworkers, and our code takes advantage of functional aspects pretty well, I think. &gt; Honestly I don't think it's worth selling at that point. It's worth selling, even if somebody has cloudy, or purely theoretical, idea of FP. Show them good parts of Scala by showing good parts of FP.
Techempower benchmark mainly measures the IO performance rather than CPU and basically the frameworks are used to glue database to the browsers and that is why Node appears on top since the job is IO bound and the heavy lifting is done by V8 engine written in C++.
Nice. I'd highly suggest using [stags](https://github.com/pjrt/stags) over ctags. It works so much better.
I asked because I can compile build and install the plugin on my Idea, but no idea how to use it in vim :)
Mhhh, I'm not sure this captures what `?` does in Rust. In Rust you can have code like (sorry for my terrible Rust-Scala pseudo-code) ... def foo: Option[Int] = { val x = Option(1).? val y = None.? Some (x + y) } ... where the `?` acts as a kind of shortcut operator, which leaves the method immediately on failure (e.g. on `val y ...` in this example), returning `None`.
&gt; My first piece of advice: don’t get vim — get neovim. Neovim has support for asynchronous commands While I use neovim, this isn’t true. As of Vim 8, vim now supports async. If you stick with vim, FZF is an ezcellent fuzzy-find-everything plugin instead of neovim-fuzzy the author is suggesting. FZF is also written by the junegunn, the same dev who wrote the vim-plug manager author mentions.
i'm trying to use this and I get no error highlighting period. kinda kills this for me at the moment.
So what about my existing plugins, for example vim-airline? Do those work on neovim too? I've never tried neovim don't really know what I'm missing
What plugin are your referring to?
This project contains two plugin, one you have to build and install in Idea as a plugin from disk, this supposedly will listen to commands from Vim. And another vim plugin that will call and do autocomplete etc. I missed how to configure the second vim plugin, since there is barely any docs on how.
No wait, I did not see this issue which is sort of a docs itself : https://github.com/google/ijaas/issues/5 It is working nicely.
I have no direct experience, but people tend to like PostgreSQL's geospatial support and it's established enough that I'd expect there to be (probably unpleasantly Java-ey) libraries for it - notably postgresql-async has an open issue so presumably doesn't support it yet.
(This is also for u/BerserkerAstra): Neovim is vim. It supports vimscript and more(you can write plugins with the RPC API even in scala). The biggest difference is that the async API was first implemented in neovim and Bram(vim's author) didn't want them to be compatible. See [vim-differences](https://neovim.io/doc/user/vim_diff.html). Both FZF and vim-airline supposed to work with neovim. I prefer neovim because the integrated terminal is better and I can use any language to write plugins. For example: my entire config and all of my plugins are written in ruby. Instead of suffering with vimscript and its poor semantics and documentation I can exploit ruby and [its gems](https://rubygems.org/).
Others have said `for`/`yield` is probably the closest equivalent. If you really want to write Rust-style and are willing to use a macro there's https://github.com/ThoughtWorksInc/each , but I wouldn't really recommend it - it's not idiomatic Scala.
Would the subreddit be happy to see this kind of reply on posts about problems with IntelliJ or SBT or Play or Akka or ScalaTest (all projects I have those kinds of feelings about)? Everyone has their own preferred tools which is fine, but not an appropriate way to respond to people having problems with their own tools.
This is a solution to the above problem because it (and many others with Eclipse) do not exist in IntelliJ + Scala plugin.
&gt; Every so often a contributor will go in and clean it up and it'll magically improve That's actually the concern here, a ton of work was put into this round and the results are less than stellar. The expectation was that Play would perform better than in previous rounds, but actually the framework seems to have regressed performance-wise (in TE benchmark at least). This may be due to default akka-http, but more likely some new overhead has been introduced to the framework since the last round/play version. Benchmarks may not be important in the scheme of things, but you'd expect a JVM framework to blow away PHP based frameworks across the board; not mingle with them.
Well, maybe; corrupted installs and mysterious non- reproducible errors happen with IntelliJ just as they do with Eclipse. More to the point, I could say the same thing about all the problems people have with IntelliJ like error highlighting being broken: switching to Eclipse is a solution to the problem because it (along with many other issues in IntelliJ) does not exist in Eclipse. 
I just realised/remembered that I stopped going done the `:&gt;:` path because of another issue I cannot fix. Scala's desugars `:&gt;:` calls in a way that breaks Shapeless's `Witness` creation. See: https://www.reddit.com/r/scala/comments/7xp0r3/strange_shapelesswitness_behaviour_help_wanted/ And: https://github.com/pheymann/typedapi/issues/2
Shapeless does lend a lot of flexibility, and it made it a lot easy for me when I started working with JSON in Scala initially.
Shapeless is a dependency you need in any serious Scala project IME, and far better to reuse it than reimplement it (especially if the alternative is implementing your own custom macros - macro programming is inherently error-prone so it's especially important to rely on a few widely-used macros like the ones in shapeless rather than each project implementing its own macros). You also need proper dependency management whatever you're doing. Whether to use `Seq` is a difficult design decision because it's a lot of extra work to write your code in a way which will work correctly with `Stream` (which is a subtype of `Seq`), so some people prefer to use concrete finite sequence types. Personally I avoid this problem by banning `Stream` in my codebases, but a general-purpose library can't assume that everyone will do that, and it's not good to offer an API that appears to accept any `Seq` but will break when passed an infinite `Stream` (which presumably must be what play-json does). Personally I have a good impression of Circe because of its clear design document that articulates a philosophy that makes a lot of sense. Whereas I have a poor impression of Play in general (it encourages a programming style that relies on unsafe casts and custom build steps, and doesn't really make much use of the power of Scala) so would instinctively avoid libraries from it.
In what way did you find the API unintuitive?
Indeed, if you don't require `auto` or `semi-auto`, you could avoid shapeless. But, automatic derivation is the whole point of Circe (aside from, of course, JSON) for me: get the machine to do the dirty job for me, in a type safe way.
When you are working with the typelevel ecosystem then circe just fits right in. What about the API did you find counter-intuitive? 
`auto` certainly is useful but it does increase compile times and code size some. I know some large projects that have moved away from `auto` for these reasons. Semi-automatic doesn't have these issues AFAIK. Also, dealing with goofy formats from external sources can require writing decoders by hand. So I hesitate to say it's the whole point of Circe. IMO Circe is lovely to use even if you don't use automatic derivation.
I agree with your play opinion. The introduction of the runtime dependency injection by default is just an heresy in my opinion. But it is still possible to use compile time dependency injection with it so that's ok (they just don't advertise how to do it : https://www.playframework.com/documentation/2.6.x/ScalaCompileTimeDependencyInjection). Still this is the only complete web framework I know and it fits my usecase well.
Haha, nice! :-)
This is why I added *for me*, there are reasons to avoid auto or semi as you mention, thanks for pointing these. Luckily the projects I use Circe on are small enough to compile relatively fast. 
This is one of those features that I feel exists in Scala for Java interoperability and should otherwise not be used. It's insufficiently explicit and requires looking at the called method's signature in order to tell what's happening. 
With play json macro, it is only one line for each case class... It seems short enough for me.
Is Airstream tied to Scala.js, or is it usable in other contexts as well? How are transactions implemented?
It seems to do error highlighting I have to use ```:EnTypeCheck``` Aside from ```:EnSuggestImports" not recommending javax and javafx classes it seems to work pretty well.
Add circe, scalate, scalacss, and scalajs to http4s. No play needed. Down with FakeApplication! /s Seriously, you can do pretty much everything play does out of the box with those deps. Routes aren't generated for you, but they are in actual Scala files. Don't rewrite working apps in http4s, but try it on your next app. Play has become too idiosyncratic in catering to Java devs.it doesn't even feel like scala anymore. http://http4s.org/v0.15/auth/
One of the things a future language might consider in the syntax department is merging if expressions and pattern matching. E .g. instead of having both if (foo == bar) 1 else if (foo eq baz) 2 else 3 and foo match { case bar =&gt; 1 case baz if foo eq baz =&gt; 2 case _ =&gt; 3 } one would only have if foo == bar then 1 eq baz then 2 else 3 This requires both if expressions (not if statements) and probably indentation-aware syntax though.
The app is the sane as Jackson's low level api except for that you can chain cursor calls and you don't need to cast when reading fields. It's .hCursor.downField(fieldName) That's not terribly complex. They'd also have optics &gt; import io.circe.optics.JsonPath._ val _phoneNum = root.order.customer.contactDetails.phone.string val phoneNum: Option[String] = _phoneNum.getOption(json) That's pretty intuitive...
`Seq` is a useless type that doesn't describe anything. Why would you want an API that uses `Seq`?.
There's https://github.com/circe/circe-derivation which uses custom macros instead of shapeless (and has no dep on shapeless therefore)
Like this: thingDoer.doThings(1,2,3) thingDoer.doThings(List(1,2,3)) which is more explicit? def doThings(thing: Int*): Int def doThings(things: Iterable[Int]): Int What if it's really like this? def doThings(i1: Int, i2: Int, i3: Int): Int Then I have to wait until a compile to be told, "Nope." If I want to refactor, I have to change things around more. Maybe that array should have been a Set? I'll admit that I'm anti-varargs and don't think they have a place in my toolbox, and I cringe when I have to deal with them because of how obtuse they are.
Um, `Seq` codecs actually work. https://scastie.scala-lang.org/eUjEGLjWSQKmW7bZC9owJg
* But all the filters are off in [application.conf](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Scala/play2-scala/play2-scala-slick/conf/application.conf) * 
I have actually been in projects which have discouraged use of shapeless because of its exponential increase in compile times (talking easily &gt; 15 minutes here). Unless you really need it, I wouldn't recommend using it. There are macro based solutions to a lot of the problems which shapeless solves for the areas where you need it. Circe's automatic derivation definitely falls victim in this area.
&gt; But all the filters are off in application.conf The point is that it would be very easy for Play to game the system by removing a few lines of code in an inner loop -- but that wouldn't be a good indication of the overall system performance. How well does the system respond to a sudden spike? What latency outliers happen at what load? How well does the system scale when given more CPU or more memory? When the system is stressed, how does it fail? These are all great questions, and there have been projects like https://github.com/tenorviol/play-c100k-test which have explored how Play scales up and out.
In the end its about what is a bigger problem to people, and I suspect for a lot of people having sometimes incorrect highlighting is on the lower end of "serious problems" compared to a lot of disadvantages that Eclipse has (especially considering that most people tend to run a separate terminal to run compile in anyways)
&gt; This actually isn't true at all, I have migrated from Circe auto derivation to https://github.com/circe/circe-derivation and for the exact same code it has been much faster to compile. The README suggests it achieves that by deliberately offering a much more limited level of derivation. Fundamentally there can't be a free lunch - for a custom macro to do the same thing the shapeless macros do it will have to do the same things, and any clever speedup you come up with could equally be implemented in shapeless. &gt; also to support things like automatic snake case naming which afaik is not possible with automatic derivation, from the README.md It's certainly possible - the code has access to the field names as strings and can apply any transformations you can implement in code, and you can use another implicit to indicate special cases if that's what you want (spray-json-shapeless does this to let you configure how `Option`s are represented). What Circe chooses to implement is another question, but there's no technical barrier.
I'm the original author of play json (before Pascal Voitot made it so much better!) and I now mostly use Circe, so I guess I have some insights. At the core they have a similar philosophy, namely they're based on typeclasses to encode/decode json. So in that sense I think they're similar. However they have a few differences: - Play Json exposes its AST, along with a DSL to make it easy to traverse and modify Json. Circe hides its AST on purpose: https://github.com/circe/circe/blob/master/DESIGN.md to encourage to work with case classes - Circe is based on cats, and provides stuff like `Show` for example - Circe's result is an Either[Failure, T] while Play uses a custom sealed trait. The Either makes it nicer because it's part of the standard lib so it's composable, but the `Result` of Play Json comes with - Circe's macros are more powerful, in particular the @ConfiguredJsonCodec In short: - Play Json is easier to use if you're new to Scala, or don't used advanced Scala features (you can easily find a value inside a Json for example) - You'll see circe's value if you already use cats, or use Either a lot in your codebase - If you accept circe's philosophy that Json is only a transport and you should only manipulate case classes, it's fine - if you want to work directly with Json then Play Json will be more convenient 
Thanks for the correction - I have amended the article.
See also http://eed3si9n.com/sbt-server-with-neovim
Don't we already have language server protocol coming up in dotty? 
Show us your code and complete error
Whatever you do, stay classy!;)
What's circle?
For now, Airstream only works on Scala.js, because this is where its design is most needed. It potentially _could_ be made to work on the JVM, but I don't see that happening anytime soon due to lack of need/demand and lack of my time. See the "Limitations" section in the [README](https://github.com/raquo/Laminar/blob/master/src/main/scala/com/raquo/laminar/experimental/airstream/README.md#limitations) for a bit more detail. Transactions are run sequentially, not in parallel, because JS is single threaded. They don't ahve much in common with DB transactions. Perhaps they would better be named Propagations. Error handling is not yet implemented in Airstream (top of my todo list), but it is unlikely that there will be any sort of transaction rollback functionality (and without that, no true atomicity). You can't un-emit events that have already fired. Once implemented, error handling will probably work similar to existing streaming libraries, that is – errors are treated as exceptions that propagate downstream, and it's up to your code to use `eventStream.recover` or other operators to recover from such a situation. Similar to native Scala exceptions, this functionality is not to be used for business logic (use error values instead).
https://github.com/circe/circe
Except your two examples just move the var-arg call to a different spot. In the first example it's in your own `doThings` method. But in the second you're invoking `List.apply[A](xs: A*)`. So it's varargs either way! Without varargs we'd either need to have special syntax for creation of collection (ugh!) or use list concatenation (i.e. `1 :: 2 ...`).
&gt;You shouldn't ever use Seq. It's a silly abstraction in my honest opinion. You care about the runtime properties of your data structures and lookups between list and IndexedSeq are not the same. So? What if you're only going to ever traverse the sequence using map / filter / flatmap / foreach? 
But that's the thing... depending on the actual implementation of `Seq`, `map`, `flatMap`, etc have very different constant factors (i.e flatmap and appending for a list vs a vector). Since when do we as programmers not give a shit about the asymptotic properties of our data structures? [This](https://gist.github.com/djspiewak/2ae2570c8856037a7738#problems) is a great gist by Daniel Spiewak on it. Also see [this](https://twitter.com/emi1ypi/status/962763876323512320) twitter thread. 
&gt;But that's the thing... depending on the actual implementation of Seq, map, flatMap, etc have very different constant factors (i.e flatmap and appending for a list vs a vector). &gt;Since when do we as programmers not give a shit about the asymptotic properties of our data structures? There are tons of cases where I don't give a single shit about the performance of traversing Lists vs Vectors vs Arrays etc. Examples: * Seq of libraryDependencies in a build tool * Seq of any other small to medium configuration parameter basically * Basically, absolutely anything at all that is not in one of the hot loops of the application / library. So, constructors / factory methods for modules-like classes and traits. Maybe debug logging, etc. Second of all couldn't you say the same thing about any of the pure functional typeclass based abstractions? I could say for instance "What is the value of a {monad,monoid,semigroup,applicative,show}??? Doesn't anyone know that different monoids are going to have vastly different performance characteristics? How can you possibly get use out of the monoid abstraction when the implementation of `combine` could be anything from `Int.max` to `Map[K,V].++`? What a useless abstraction! 
That's fine. I was linking to an example that shows how competitive http4s' api is with play's api. 
this is f\*ing great. first we had scalameta, which was supposed to be the new scala macro system, being relegated to live life as a [glorified parser](http://www.scala-lang.org/blog/2017/10/09/scalamacros.html) then we have the *new* new macro not being touched [in over three months](https://github.com/scalacenter/macros/commits/master). and now this. scala has three macro systems, none of them compatible among themselves, none of them with any clear migration path, none of them in fully finished working condition, none of them supported, and none of them being properly maintained. i've spent the last many months working on a project that heavily relies on macros. i might as well thow it all in the garbage now, 'cos it doesn't matter what i do, it most certainly won't be supported by scala 2.13. scala macros is a dead end. today i feel like the only reason i'm still using scala as primary language is because i have nothing else to move to. i'm open to suggestions.
I maintain circe, so some of this comment is likely to be kind of defensive, but I also really am curious to hear more about your experiences and those of your team. About the Shapeless dependency: as other people have pointed out here, Shapeless isn't a core dependency. You can use circe without it, either by writing your own encoders and decoders by hand (which we try to make as convenient as possible through methods like `forProductN`), or by replacing circe-generic with [circe-derivation](https://github.com/circe/circe-derivation), which is a little like Play JSON's "inception" macros plus a few extra pieces of functionality (e.g. snake case key transformations, etc.). For what it's worth, our Sonatype download stats suggest that a non-trivial proportion of adopters (roughly 10%) don't use circe-generic. So you definitely can avoid the Shapeless dependency if you don't want it. You mention a "depenency conflict with another library", and I'd be curious to hear what that was. It's true that you can't avoid the Cats dependency, but that's the only dependency you can't avoid, and now that Cats has a 1.0 release this should be less of a concern. About the "`List` everywhere" part: I'm not sure what this means, since circe provides efficient type class instances for many collection types, and where it does use `List` we've chosen `List` at least in part with performance considerations in mind. We definitely care about performance, and [our benchmarks](https://github.com/circe/circe-benchmarks) suggest that circe is at least competitive with other Scala JSON libraries, and in many cases is much faster than most of them for common use cases. If you've run into situations where this isn't the case, I'd like to hear about them. About the comparison with Play JSON: [erwan's comment](https://www.reddit.com/r/scala/comments/814lyh/why_do_people_love_circe/dv0y70a/) does a good job of highlighting most of the big differences, but I wanted to point out a few other things: * While it's true that circe hides its JSON representation's constructors, it does provide ways to work with JSON directly that are arguably as comfortable as Play's. `Json#fold` is one of these, and circe-optics provides a wide range of others (including pattern matching, if that's what you're looking for). * circe is integrated with many, many other libraries (optionally, via small modules), and is more actively developed. * I'm not sure what the Scala.js experience is like for Play JSON. I see that it now has Scala.js artifacts (fairly recently?), but I've not really heard anything about people using it on Scala.js. circe has supported Scala.js more or less from the beginning, and has some Scala.js-specific utilities. * circe's just a lot faster than Play JSON for many common use cases (see the benchmarks linked above, and other people's benchmark projects linked from the main circe README). * circe provides lots of features that Play JSON doesn't (or that are third-party additions for Play), such as JSON literals. * Many people seem to find circe's approach to error accumulation useful, and the ability to choose the failure mode is much more difficult in libraries like Play (see [my original blog post](https://meta.plasm.us/posts/2015/12/17/error-accumulating-decoders-in-circe/) about this topic for a comparison with Play JSON specifically). * circe's approach to JSON numbers is much more robust. Try `play.api.libs.json.Json.parse("1e2147483647")` and you'll get a weird `NegativeArraySizeException`. Knock one digit off the end of that string and Play JSON will hang indefinitely. circe just works for large (or small) numbers like this, and it will do the right thing as you decode them into Scala's numeric types as well. None of this is to discourage you or your team from using Play JSON. I think it's a great library. There _are_ plenty of reasons to prefer circe, though, even if you don't care at all about functional purity or any of the Typelevel stuff. :)
Circe does allow for snake case though. You just need the extra generics dependency instead. From there you define your implicit configuration, and you're set.
I'm pretty sure that's implemented on Dynamic, which is a can of worms but I would recommend avoiding if you can.
Off topic but since Circe is based off shapeless, does stuff like full generic derivation work with tagged types?
If I remembered correctly, 2.10 macros will remain working in 2.13. Macros v3 is planned for 2.14 I suspect the focus right now is on 2.13 and the new collections, not macros v3
Dotty will have [its own macro](http://dotty.epfl.ch/docs/reference/principled-meta-programming.html) too.
https://www.slideshare.net/brikis98/nodejs-vs-play-framework page 150.
Actors are about work with shared state and are overkill for fetch-aggregate flow. Future is the best choice in your situation.
Cool. I think so too. The only thing comming to my mind is throttling (the third party limits me to a certain number of calls per seconds). But that could be solved with futures as well, by limiting the number of threads in the ExecutionContext right?
0.18 is using the typeclasses from cats-effect, which means that you can use Monix to power your http4s application. 
&gt; However, if macros v3 become finished for 2.14 everything could be ok. Then the question is what happens when macros v4 come around (I kind of lost track which one macros v3 is), not only because the one for Dotty seems to be extremely invasive in terms of language and syntax additions (`~`, `'`, `inline`). Things like inline def power(inline n: Int, x: Double) = ~powerCode(n, ’(x)) really look a bit like a different language to me. Plus, there was `macro`, then there will be `inline` ... how many keywords will macros cost in the end? There also seems to be some kind of movement to abandon annotations for new keywords, so I'm not sure how many additional changes there will be in general.
I am going to follow your page from now. I am very new to web programming. Could you please recommend nice books. I worked primarily for batch programming and optimization, primarily COBOL and C program.
One of our primary batch application is going to migrate to Scala for faster processing, just now started reading programming in Scala from Martin odersky. Seven months from now, we are going to convert C screens to Webpages and Enterprise architect wanted to develop in Scala pages as they are going to convert C programs to Scala programs. Your suggestions are highly appreciated.
[This is a nice overview of the current status and plans](https://www.scala-lang.org/blog/2017/10/09/scalamacros.html) Macros v1 = scala.reflect as known from scala 2.10+. Macros v2 = scala.meta, which was stopped because supporting def macros (code-generating macros) was not feasible with it. Macros v3 = scala.macros, which is in early development and targeted for 2.14 I wouldn't focus too much on dotty, it is a research project and afaik the macro stuff is still highly experimental. There hasn't been an final decision on whether the `inline`keyword and siblings will ever be in scala.
What about the macros _before_ scala.meta, reading the text I guess that's hat they mean with scala.reflect? So the old `scala.reflect` could be called "v0" then, I guess? &gt; There hasn't been an final decision on whether the inline keyword and siblings will ever be in scala or in what form. `inline` seems to be a SIP already (for Scala "2", Dotty doesn't need to go through this process because it doesn't count as Scala).
I agree about not using auto generic derivation. We banned its use right from the start because the compile time cost and lost of explicitness (potential source of bugs) is really not worth saving one line of code per case class.. 
Does anyone find F-bounded type super useful? I don't know if Im doing something wrong, but very often I find the need for F bounded types... could it be with relation that current object does not use typeclasses, as a way to get back some reusability with traits?
Throttling can be done with a stream. That’d be the minimally powerful tool for that usecase. In fact, most of what actors can do can be expressed more functionally and cleanly as streams.
Ok thanks, I'll check that out. You mean the built in Scala streams or akka streams?
Oh cool!
&gt; keywords And adding new keywords was supposed to be anathema to a "library-based language".
&gt; Macros v3 = scala.macros Where can I read about this? It's hard to Google at this point.
¯\\\_(ツ)_/¯ I kind of have lost track, but there at least three more now: `enum`, `inline` and `ghost`. From my point of view all of them can and should be annotations. Why they are not? I think I have a pretty good idea.
While this does address a real problem, I feel as though it wouldn't be a problem if we didn't have a proliferation of IO types in the ecosystem.
I was aware of `enum` and `inline`. Where are `ghost` and `opaque` coming from?
[Here](https://github.com/scalacenter/advisoryboard/blob/master/proposals/014-production-ready-scalamacros.md) is a good place to start.
`opaque` is intended to replace value types as far as I understand, so in the future you'll write package `object` entity { opaque type Person = (String, String, Int) object Person { def apply(firstName: String, lastName: String, age: Int): Person = (firstName, lastName, age) implicit class PersonOps(val `this`: Person) extends AnyVal { def firstName = `this`._1 def lastName = `this`._2 def age = `this`._3 } } } instead of package entity class Person(firstName: String, lastName: String, age: Int) extends AnyVal
This actually grew from a concrete usecase around Futures. To me, it has value as a monomorphic type :)
It's a pretty messy situation to be in, but remember that `scala.reflect` has always been in experimental status. You're saying it won't have mainline support in 2.13, and that's true, but they also didn't have mainline support in 2.12 or 2.11. Nothing changes in that regard. If you think that means throwing it in the garbage is the right choice, well, I don't know what you were thinking when making it for 2.12 with the same support status.
Quoting Seth Tisue: "I want to be 1000% clear with you and with everyone that there has been zero change in the maintenance and/or supported status of anything. we're just trying to not have so many stale tickets that nobody has looked at in years."
Yep. Abandoned libraries have a long tradition in the language, it's certainly not some recent change.
I don't know what you mean by Scala page, I have built cryptocoinalerts.net using Play Framework and you can look to the source code: https://github.com/AlexITC/crypto-coin-alerts
spark itself uses jersey for that ( presuming you mean apache spark )
Hey, There are a few. The most common one would be Play from typelevel. It is an MVC framework that is very Java like. Another is akka-http, a minimal framework built on top of Akka. Lastly, http4s is a minimal functional framework. If you like cats, fs2 and the sorts. Pick what suits you best.
&gt; Play from typelevel You mean lightbend :-)
Isn't it using Twitter Futures (which break referential transparency)? 
No one said about scala.js? Weird :) I'm using it from ~2 years without any problems (many libraries/cross compiled code/nice js interop).
To be honest many implicits/macros are not so lightweight as you said. There still is generated code that not always is optimal. Nevertheless those are really nice features. 
This is my one loop version import Math._ /* Helper function for powers of integers */ def ipow(n: Int, a: Int): Int = pow(n, a).toInt /* returns the integer n^(1/a) if it exists */ def integerRoot(n: Int, a: Int): Option[Int] = { val k = round(pow(n, 1.0/a)).toInt ipow(k, a) == n match { case true =&gt; Some(k) case false =&gt; None } } /* find all possible (x, y) with n = x^a + y^a */ def twoPartitions(n: Int, a: Int): List[(Int, Int)] = { val range = 0 to floor(pow(n/2, 1.0/a)).toInt range.toList.map { k: Int =&gt; integerRoot(n - ipow(k, a), a).map((k, _)) }.flatten } twoPartitions(1729, 3) List((1,12), (9,10)) &gt; twoPartitions(13, 2) List((2,3)) &gt; twoPartitions(10,1) List((0,10), (1,9), (2,8), (3,7), (4,6), (5,5)) &gt; (1 to 4200).map((n: Int) =&gt; (n, twoPartitions(n, 3))).filter(_._2.length &gt; 1) Vector((1729,List((1,12), (9,10))), (4104,List((2,16), (9,15)))) So the next number is 4104 ​ 
curious - do you think that h2o(+ sparkling water) is not good enough?
You need to install the Scala plugin. I use the EAP releases and they are almost always good, and they come with Scala. See https://www.jetbrains.com/idea/nextversion/
&gt; scala macros is a dead end. i feel like the only reason i'm still using scala as primary language is because i have nothing else to move to. i'm open to suggestions. I'm feeling exactly the same. Of course features no longer being developed in a language is not a big deal if they work and keep working. But in scala, esp related to macros, there now are 3 incompatible systems that libraries might use and keep using as there are no alternatives. I'd much rather have one library/solution with rough edges (or even breaking use cases, just add a clear "handle with care") that has the blessing as a "best effort alpha quality solution" than 3 incompatible solutions you get pulled in to one way or another that might not be supported at any time in the future.
I can't recommend http4s enough, it's super clean and really really fun to use!
i have been looking forward to having an excuse to try out rho 
Any gotchas on using this with an embedded spark client?
Finch is a great framework for writing APIs. I love the strongly typed endpoints. yes TwitterFuture is painful, but that's used only at the last layer. In 99.99 percent of my code I only use Scala futures. and then at the last step before returning the result, convert the scala future to a twitter future.
&gt; So the old scala.reflect package could be called "v0" then You mean the 2.9.x `scala.reflect`? That didn't have macros at all, just runtime reflection? from 2.10.x it does macros, and that's v1. &gt; Then there is this Squid thing. Yes, that's a research project on yet a different way of doing macros. I don't think you can really blame researchers for doing research and publishing their research. 
&gt; I'd much rather have one library/solution with rough edges (or even breaking use cases, just add a clear "handle with care") that has the blessing as a "best effort maintained alpha quality solution" Thanks for giving actionable comments. I might be wrong, isn't that scala.reflect macros? is anybody using the alternatives for macros? Because if this matters, it could be easy to clarify. Yet, scala.reflect is still experimental as it has always been — they'll keep best effort support on Scalac, but can't be ported to Dotty.
&gt; That didn't have macros at all, just runtime reflection. The main issue that it is _also_ still shipping (looking at you, `Manifest`s), not that they are also macros. I probably worded that very poorly. &gt; I don't think you can really blame researchers for doing research and publishing their research. I'm sure nobody is doing that. Nevertheless, I believe people have the right to be a bit skeptical about the fact that things tend to end up as language changes or ship with the standard library, often causing various issues. Every student has been smart and motivated, and wanted to make an impact. I think every Scala user is thankful for their work. The issue is that they cannot predict the future – just like the rest of mankind – and the well-intentioned "I will certainly keep working on this after I finished my degree" has almost never worked out if we look at the last decade. Nobody is blaming the students, but _someone_ has to have a bit more experience by now to avoid making the same mistakes for 10 years. Someone needs to reign in on the scope and complexity, and make sure that things don't get added to Scala without some kind of maintenance story. Paul did this during his reign, and the quality improved substantially. Now it's back to pre-2.8 levels. I only did the Scala stuff as a hobby, and even I have figured out that proposals that want to add new keywords to the language or add new stuff to scala._ are usually poorly thought out, with the respective maintenance woes down the line.
You could consider actors when you have mutable state that needs to be modified concurrently. Otherwise use something less complex.
&gt; Nevertheless, I believe people have the right to be a bit skeptical about the fact that things tend to end up as pre-mature language changes or ship with the standard library, often causing various issues. Yes, this has definitely been an issue. It's getting cleaned up, and things that shouldn't ever have been part of the mainline supported standard library are being either deprecated (e.g. parser combinator stuff) or factored out in their own repos (e.g. scala xml) However, that's not the case for scala macros, which always have been experimental. When you use an experimental feature, that you explicitly enable with `-Xexperimental`, it shouldn't be surprising that that its lifecycle behaves like an experimental feature. &gt; Someone needs to reign in on the scope and complexity, and make sure that things don't get added to Scala without some kind of maintenance story. Paul did this during his reign, and the quality improved substantially. Now it's back to pre-2.8 levels. What's been added to mainline scala or the non-experimental stdlib since 2.8 that you feel this is the case for? &gt; I only did the Scala stuff as a hobby, and even I have figured out that proposals that want to add new keywords to the language or add new stuff to scala._ are usually poorly thought out, with the respective maintenance woes down the line. Proposals, or accepted proposals?
&gt; It's getting cleaned up, I know, I spend a lot of time doing that. I have given up on this, because it's completely futile. The rate at which poorly thought-out things are added is way higher than the rate at which things can be deprecated. Having seen the last 6 years of deprecations and removals, my estimate was – under the unrealistic assumption that people would stop making things worse – that it would take at least another 4 years to get Scala into a maintainable state. &gt; and things that shouldn't ever have been part of the mainline supported standard library are being either deprecated (e.g. parser combinator stuff) or factored out in their own repos (e.g. scala xml) Which has worked ... not so well overall. See the situation with the parser combinators: Deprecated, modularized, undeprecated, deprecated. Or the partest module, which turned any fundamental cleanups into some byzantine mess of publishing custom versions of libraries and changing build scripts. This mess made me give up on multiple cleanups alone. &gt; However, that's not the case for scala macros, which always have been experimental. Experimental changes usually don't come with language changes. I'm not sure why people didn't see how this would turn out poorly from the beginning. Most parts of the ecosystem depend on this stuff now, and no amount of telling users "but it was experimental" will fix this. &gt; What's been added to mainline scala or the non-experimental stdlib since 2.8 that you feel this is the case for? You mean post-Paul, not post-2.8? SIP-Commas (broke compatibility in a minor version), `@showAsInfix`, `scala.async`, `scala.spores` ... and no, "but it's experimental" does _not_ count. If people make the decision to add things to the Scala namespace, they have to take the responsibility for it. I think the worst things have yet to come anyway. &gt; Proposals, or accepted proposals? The point is that under Paul's leadership this would have been a closed case after 5 minutes on scala-internals, because development was measured to substantially higher standards. Which leads us back to the first point: Getting the quality situation in Scala under control requires higher standards before things are added, not being proud of how fast people can deprecate things once they are in.
I have the Scala plugin install. But i don't see any intellisense on hover. I will install the eap.
you understood me wrong. first, i'm not complaining that the "api might go away"; i'm complaining that the api went away before the replacement was ready, and the replacement itself went away before the replacement replacement was ready, and the replacement replacement itself isn't showing any signs of progress. second, i'm a framework author. i'm not "just using libraries implemented with macros", i'm creating them! i'm maintaining them! i *could* keep using scala 2.x myself. however, my users probably won't want to after 2.x+1 and 2.x+2 are out there.
&gt; we're just trying to not have so many stale tickets that nobody has looked at in years that's just a verbose way to write "sweeping the dirt under the carpet"
Check out scalatra 
I wasn't talking about performance. I love the way you can compose endpoints. You can test the entire endpoint in a REPL even without running a server.
Which API exactly went away?
Had great experience with Finatra. Straightforward and easy to use, even exposes a detailed monitoring admin endpoint by default.
Ok. That's the thing. I thought, in this case, that the mutable state is the aggregated response I'm going to return, and each response from a sub actor (that performed one request) will be added to the aggregated response, until they're all done, and the parent actor can return it. Thanks, I'll consider it.
Corrections: 2.x+1 and 2.x+2 (2.13 and 2.14) will still have the same API. It’s Dotty/Scala 3 that can’t have them. And given how many people still use 2.10 and that every library out there uses macros, migration to Dotty will take a while, especially if replacements will be too late and if other macro users will be in the same boat. Tne only difference is that people are delivering on the “experimental” label, so it’s likely that evolving the current code is a bad idea. That is, if people are running into some obscure nightmare bug that is next to impossible to fix, now you know you probably shouldn’t work on it. But yes, it also sucks if you want to write new code with macros now. On replacements, I suspect those replacements should have been labeled as experimental as well.
Last time I checked, I haven't found a way how to use it without running some training service. 
&gt; no, "but it's experimental" does not count. There we disagree. I do believe that if you make use of an experimental feature, you shouldn't expect the same maintenance status of a non-experimental feature. 
Why are /u/davdonkin and /u/know_not_much getting downvoted? Is there a scecret agenda going on?
Right now i'm (again) confused, this post gave me the impression the current macro framework shipped with scala is eol (https://docs.scala-lang.org/overviews/quasiquotes/intro.html) (Note that this documentation still points to scala-meta as the future (https://docs.scala-lang.org/overviews/quasiquotes/future.html) while scala meta as far as macros go is over) I could be wrong but i have the impression that management would prefer that macro's left the language because they're not theoretically "sane" and difficult to maintain. At the same time macro's are so powerful that users and the ecosystem (libraries) have embraced them: Scala without shapeless is unimaginable. And scala-meta macro's are used in libraries to generate code to ease functional programming (for example http://frees.io/). Then there is scalamacro paradise, of which i'm not sure what the status is, but by my understanding it has been abandoned, replaced by scala-meta, and with scala-meta macro support has been abandoned. Either way, https://github.com/ThoughtWorksInc/Binding.scala depends on it, but also typelevel projects use it via simulacrum, by the looks of it even cats depends on it. It's clear macro's (in its many flavors, both def and annotation based) are so ingrained in the scala ecosystem that they won't (and i'd argue should not, for me they're as much (or even more) a strength of scala as its hybrid functional/oop design) go away. It also seems that currently all implementations hit a dead end in one way or another, while at the same time being so valuable they are used regardless. At this point, i think it would be best for scala if one macro/meta system was picked as "official" and blessed with some guarantee of conintued existince, which could very well mean a best efforts migration path when the time is there for macro/meta version x+1
What's the point of using an annotation instead of a keyword for something that is built into the language? By that logic, we could replace most of the language keywords by annotations.
&gt; How are Scala annotations different from language features? In Java you can implement annotations outside Javac, in Dotty you can't (yet). In Scalac you could, but I haven't seen success stories. The core point is that keywords should be restricted to the core requirements of the language. If people have done 10 years without something, the feature probably doesn't not deserve a new keyword. Keywords are extremely invasive and are expensive to manage: You need to migrate everyone off the new keywords the used as identifiers. Changing them is hard, deprecating is way more complicated than annotations. They cannot be extended. Case in point: sbt-datatype. I think SBT devs would have loved some mechanism to opt out of the parts of `case` that cause binary compatibility issues. Another case in point: `@deprecated`. If `@deprecated` had been a keyword it wouldn't been possible to extend it with a version string for instance. Without that version string, Scala would probably still have deprecations in the standard library from Scala 2.3. &gt; If annotations are implemented by the compiler, they're effectively language features. And that's fine. See `@SerialVersionUID`. Should that have been a keyword instead? Another example: `@inline` and `@noinline`. &gt; the only advantage of annotations is literally not using a new keyword, And that's a huge advantage. Keywords live in a global scope, annotations don't. They can be imported, or left unimported. Jumping to the declaration of an annotation allows reading the documentation of what an annotation does. Annotatins are cheap, keywords aren't. For instance, when I spoke to someone on the team about `inline` some while ago, he told me that `inline` replaces `@inline` and does everything better. But the current inlining approach doesn't only have `@inline`, it also has `@noinline`! So either the new inline proposal also adds `noinline` as a keyword, or the claim that `inline` is a clean upgrade from `@inline` isn't really true. &gt; and renaming conflicting identifiers is actually feasible for scalafix. Scala got promised migration tools ~5 years ago, and they still haven't shipped with the language. Even with migration tools people are forced to fork their libraries to support multiple versions, if they can't reconcile the differences. &gt; I don't buy the "count keyword" heuristic, just like I don't buy it from Martin The thing is, one can't lecture people about the simplicity of the language using "number of keywords" as a metric and then get emotional if people actually start counting. One can't extol the virtues of Scala being a library-based language and then turn around and sprinkle keywords on anything that looks mildly inconvenient to implement with other means. &gt; Not sure which issue you mean, except "nobody ever designed annotations, that's why they didn't work". Designing annotation would be a language change anyway. No, that's not what I mean. The implementation of annotations has multiple issues. The basic issue is that everything that introduces new members has to deal with the namer-typer interactions A good example of this is how `case` works, and `case` is a keyword. `case` needs to go behind the scenes and manually adjust the result of the naming phase. The code isn't pretty and I think there aren't more than a handful of people who understand that part of the code base. Annotations have more issues than that, because during parsing of the outline the annotations are not resolved yet. So having `@foo` doesn't really allow doing anything, because it's not even clear at this point whether `foo` refers to `bar.foo` or `baz.foo`. That's why you have some annotations where people have just given up and match _on the name_! val hasBeanProperty = tree.mods hasAnnotationNamed tpnme.BeanPropertyAnnot That's why you can't alias or rename the `@BaenProperty` stuff and the compiler just errors out instead! That's why you have annotations like `@SerialVersionUID` where people don't even bother to expose their effect to the front-end, which is why scalac can't see the static member. There are many more of these issues and they are annoying to address. But fixing this _once_ would allow cleaning up all the existing mess _and_ avoid the need for everyone who wants to add things (whether it be keywords or annotations) to rolling out the necessary namer-typer workaround on a per-feature basis. As a basic rule, if some feature simply does some codegen (`enum`) or acts as a marker (`inline`, `ghost`) it should be an annotation. As an example: I implemented enums first as a keyword, then as a macro annotation, then as a scalac-supported annotation. There is absolutely no reason why this needs to be a keyword. The current proposal pretty much replicates the bad ideas of `scala.Enumeration` (weird mini-DSL), while being only usable for type hierarchies with either zero (`case Foo`) or one level (`case Bar(...)`) of nesting. It also ignores the complaints people had about `case` for ages (does multiple things, no clean way to select the parts you want). So in practice, barely any ADT can benefit from it, while it still has no interoperability with Java (like `scala.Enumeration` and ADTs). Some of those things where `enum` could be used (like `Option`) need to be rewritten without the type hierarchy anyway if you want to benefit from value types in the future. So they are not a good usage example either. That leaves us with a keyword that introduces some weird mini-DSL, is not interoperable with Java, and has barely any use-cases. In the end `enum` is fundamentally flawed, because it tries to be two separate things and fails at each of them. Expressing ADTs is severely limited, expressing "classic" enums is not interoperable with Java, and the fact that you can mix both of these concepts together means proposed methods like `enumValues` are extremely misleading. In a sense, annotations are better because they severely restrict the creativity of people. You can have an `@enum` annotation that requires zero rewrites of existing code, introduces no new syntax, can be applied where it makes sense and provides clean interop with Java.
Yep, I know how this works. :-) &gt; To expand on organigrams ... I think what you point out is the core issue – people have their own motivations why they do things. That's perfectly valid and legitimate, but it matters if you have many people whose special interest are not necessarily aligned with making the lives of Scala users better. There is lack of awareness when people ask "why does nobody care about the issues users face?" ... D'oh! Because it is literally no one's job description! &gt; I'm still pushing for features to go through the SIP process I think the SIP is completely broken. It made things substantially less equitable, split the community into a large part of those whose arguments don't matter and a very small part of those whose opinions matter. It the whole process less transparent, and substantially lowered the quality requirements on new proposals. I'm sure this wasn't the original intention, but this is how things have ended up.
Let's mark the whole language as experimental then, given that most of the ecosystem relies directly or on indirectly on experimental stuff. Because that's the reality we ended up with. Of course one can fall back on some prescriptive stance on how users are wrong, but I always had a more descriptive stance on things – "this is the way it is now, what can be done?".
I’m not sure what is happening then. Are you opening an sbt project?
&gt; Right now i'm (again) confused, this post gave me the impression the current macro framework shipped with scala is eol (https://docs.scala-lang.org/overviews/quasiquotes/intro.html) See https://github.com/scala/bug/issues/10755#issuecomment-370068536. Nothing changes on whether you should use macros for now. In fact, you can keep using the same code! Not sure what happens on regressions. It's clear that macros are a crucial part of the ecosystem—for shapeless, Miles is improving Scalac itself. Other solutions might be appropriate in other places https://gitter.im/scala/contributors?at=5a9aa9d8888332ee3af49f53 &gt; I could be wrong but i have the impression that management would prefer that macro's left the language because they're not theoretically "sane" and difficult to maintain. At the same time macro's are so powerful that users and the ecosystem (libraries) have embraced them: 'not theoretically "sane" and difficult to maintain' is an understatement. They expose you to the internals of Scalac, which will be replaced by Dotty, with very different internals. They're better, but supporting the scala.reflect API in Dotty is impossible. Still, either there's a good API to migrate macros to when Dotty becomes the "official" compiler, or people will stick to Scalac. Not because this is decided by me or anybody here, but because of laws of nature (of community dynamics, but same difference). I'm sorry for the uncertainty on when and how, but for now it appears `scala.reflect`/macros-paradise is still the most supported approach. I'll try to get an official statement on this.
What's @noinline for if it is implied on everything? (Answers on other points might come separately after I digest them.)
&gt; I think what you point out is the core issue – people have their own motivations why they do things. I just meant "we literally won't be around in 5 years", so we need to take measures to keep the code maintainable. I'll accept evidence-based resources on helpful practices — I say evidence-based because I need evidence to even try to convince people, and because I don't want to waste mine and their time. https://twitter.com/Blaisorblade/status/969957868026892288 &gt; There is lack of awareness when people ask "why does nobody care about the issues users face?" ... D'oh! Because it is literally no one's job description! Everybody cares, but people have different opinions on what they are, how they should be fixed, and what's more important. You have correctly identified some biases, but you don't have an exclusive on "real issues", just like people complaining Scala isn't Haskell don't have an exclusive either. &gt; I think the SIP is completely broken. I just (re)learned a detail: Martin can veto proposals, but not force approval. Then you still need qualified reviewers. I remember your disagreement on trailing commas, but when I looked at your arguments they didn't seem so slam-dunk. My favorite process is scientific peer reviewing; people keep complaining it's also broken, and they aren't wrong, but the best programming languages we have (at least according to certain dimensions) come from there. With good reviewers you're forced to make a compelling argument, which I think is a great way to review a design.
You can opt out of a declaration-site `@inline` with a call-site `@noinline`.
Why do you need to use annotations? Because Scalas compile-time is more powerful, runtime reflection is rarely needed.
Squid is not really in that category. It's an independent, higher-level _metaprogramming_ system. It does _use_ `scala.reflect`, and has facilities to _generate_ code that uses `scala.reflect` to implement macros. So we can keep more or less the same general high-level API once the future lower-level macro API is chosen.
Yeah - Java annotations are basically the only way to go. You'll get a bunch of answers telling you you're doing it wrong if you need them present at runtime, but that's not an answer to the fact that sometimes you really do need that information there. An example is a argument parsing library I helped create - we used a pair of [Java annotations](https://github.com/fulcrumgenomics/sopt/tree/master/src/main/java/com/fulcrumgenomics/sopt/cmdline), and then [`type` defs](https://github.com/fulcrumgenomics/sopt/blob/master/src/main/scala/com/fulcrumgenomics/sopt/package.scala) to make them feel a bit more scala-ish.
Absolutely, there are trade-offs, but the principle of minimal power should always apply. I agree that making common cases easy is important, but if you look e. g. at `enum`, that's a completely new syntax/DSL. That's yet-another thing people have to learn, compared to making things work within the existing syntax. That's easy vs. simple. If a person has e.g. sealed trait Pet object Pet { case object Cat case object Dog } it is nicer to tell them to add `@enum` if he/she wants a list of values and Java compatibility, than saying "you need to rewrite your code with this new special syntax (oh, and no Java compat, maybe rewrite it as a Java enum instead?)".
Squid is pretty nice, I like its design. I found that it would be extremely useful to libraries like Quill. I hope you didn't interpret my comment on the overall state against a knock against Squid. The problem is not with the individual efforts, but how hard it is to keep any overview over what is happening, what's abandoned, what's not there yet, what's developed, what's deprecated, etc. and how this affects all the libraries and tools that rely on keeping these things working (e. g. with later proposals cutting more and more features people actually used).
What you're describing sounds just like akka-streams. It utilizes actors under the covers in the manner that you're describing. I think streams will do you well.
Just ran into https://github.com/scalacenter/macros/issues/4. In case you wonder why so many of these projects have given up, this quote and the linked Eugene talk are interesting: &gt; Unfortunately, the architecture of the macro engine of scalameta/paradise that relies on so-called converters seems be broken beyond repair. https://www.youtube.com/watch?v=FDtoQbn9ueU&amp;feature=youtu.be&amp;t=1404
&gt; Squid is pretty nice, I like its design. I found that it would be extremely useful to libraries like Quill. Thanks. Coincidentally, I'm working on a language-integrated query engine right now using Squid, but it's still very early stage.
You mean DBStage?
You can try julienrf/endpoints and select a backend of your choice (Akka http or play)
Since we're here. As of [0.16-M3](https://github.com/finagle/finch/releases/tag/0.16.0-M3), it's possible to return Scala Futures from your endpoints and have them automatically converted into Twitter Futures. That said, you can virtually avoid dealing with Twitter Futures within your code-based.
There was some work on supporting the definition of runtime annotations in Scala, but this never shipped. You need to define them in Java, and then you can use them in Scala.
Couldn't you just define those methods on an interface and mandate that the components implement that interface? That's how it's usually done.
Fascinating and practical
Jackson Jackson all the way. We had hart time choosing JSON library for our Scala project but had to stop on Jackson. Reasons: - performance - ability to fail on excess JSON fields (must have if you value backward compatibility) - ability to distinguish null and missing field (required for PATCH/partial PUT) - nice json tree manipulation (sometimes we have to parse JSON that cannot be represented as scala model)
Wow, you invented syntactic coke: An obscure, yet sweet solution. 
&gt; ability to distinguish null and missing field (required for PATCH/partial PUT) Here is a working example of how creation of format instances can be automated with "org.julienrf" %% "play-json-derived-codecs" % "4.0.0": https://github.com/plokhotnyuk/jsoniter-scala/blob/d7d1014bfe9dd555f5b29fa6ffc248b7bc23e9a6/benchmark/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/macros/PlayJsonFormats.scala#L87-L93
I have never seen a valid use-case for actual actors in a commercial environment. In all examples I've seen, futures would have been better, simpler and more easy to deal with. Obviously uses for them exist, and we wouldn't have a telephone network without them, but if you think there's even a chance it will work with something other than actors... don't use actors. Things that use actors under the hood like Akka Streams are a different matter.
That's not true. `ManagedT` (also fs2 `Stream`, and monix `Iterant` in the upcoming 3.0.0-M4) allow you to describe a computation that keeps resource open as long as it is necessary. For example, you can safely do something along the lines of def getFile = bracket(openFile)(file =&gt; file.pure[IO])((file,_) =&gt; IO(file.close)) def firstLine = getFile.evalMap(f =&gt; IO(f.readFirstLine)) using above types, but not with `IO` / `Task`. For the latter, the only answer to "when to close the resource" is "immediately after the `use` block". ManagedT gives you another option: when you call `apply` or `unwrap` (in OP's library), and in streaming types the resource is closed when the end of stream is reached during one of terminal operations. In that sense `ManagedT` is more powerful than `IO`, and more composable than `try/finally`, but it's not suitable to be a thing your pure `main` returns, like `IO` is, since it could keep resources open forever when used that way. See discussion on [that PR for cats-effect](https://github.com/typelevel/cats-effect/pull/113) too.
Yeah, you need a third-party library which has to be kept up to date, and apparently it's still boilerplatey. 
Can you upload a minimal project in github (or somewhere else) that reproduce the behaviour ? Hard to understand where it could come from without seeing your setup. 
The question isn't mine, i just found it and was curious for an answer..
It seems clear it's talking about the current design in https://github.com/scalacenter/macros (that is, v3 [in earlier comments](https://www.reddit.com/r/scala/comments/818qar/quasiquotes_are_no_longer_being_improved/dv21pjj/)). &gt; Some features of the macro system appear to be relatively simple to support across different compilers. In the scalacenter/macros repository, we have a prototype macro system that runs on both Scala 2.x and Dotty. In only a few days, we got some interesting macros working for both compilers: a JSON automatic serializer for case classes and most of the sourcecode macros. That pretty exciting news for code generation macros and a subset of developer ergonomics macros. Inspection on types and creation of new terms doesn’t appear to introduce deep technical road-blockers! However, we struggled to implement some other macros like utest assert since it requires the ability to transform trees. Yes, the article describes potential avenues for fixes by looking at alternative designs. I'd maybe call the result v4, maybe you'd still call it v3, doesn't matter, but the article concludes with &gt; There remain many open challenges in the design of the new macros. which sounds accurate to me, though lots of my colleagues are working on it.
I wish you success. I'm probably a bit jaded from seeing everyone working on their own third-party library, while everyone has given up on meaningful improvements to "mainstream" Scala, which has been left behind by Python, C# or even Java.
Yeah, it's supposed to be an imperative building block to build more declarative abstractions on top, like most Scala collections.
Jup, that's the other way which was presented to us. As e.g. Java EE and such do refrain from that and use annotations, aside from other downfalls of the interface method (e.g. the need to implement all methods, even if not all are needed), the assignment wants us to do it with annotations.
Makes sense. This is pretty great, how'd you start thinking about it? I haven't looked around but I'm surprised this wasn't a thing before. Slightly regret not going PhD every now and then.
Congratulations! You've discovered Iteratees. Fs2 and Play iteratee are built off of this exact idea. If you introduce an element holder, `El` and wrap that around your inputs, you can get rid of hasNext. Simply emit El.Empty when you have nothing to give. You can implement infinite streams by including El.EOF to indicate that the source ous done producing (and not terminating your computation until EOF is emitted). You can add errorhandling by putting in El.Error. Now, when you loop, you can be in several states and you need to handle them. Anyway, I appreciate the simple interface over the more complicated ones already in the ecosystem, and feel this really cuts to the essence of iteratees.
If not all are need then your interfaces are too big.
&gt; It is an MVC framework that is very Java like. I can't stress this enough. Play wants to do _everything_ for you. It spreads and spreads into every layer if you let it and removing it becomes very difficult.
wow!!! this is amazing. will give the .16 a try. 
I think you are right that we are approaching this from different perspectives. If you are approaching it primarily as a consumer of macros (what I'm doing), v3 looks like an incomplete design. If you are approaching it more from the language design side (what I suspect you are doing) you would probably see v3 as several successive designs each with its own set of trade offs.
&gt; But it is still possible to use compile time dependency injection with it so that's ok (they just don't advertise how to do it : Yes we do. From the main [dependency injection page](https://www.playframework.com/documentation/2.6.x/ScalaDependencyInjection), first paragraph: &gt; Dependency injection is a widely used design pattern that helps separate your components’ behaviour from dependency resolution. Play supports both runtime dependency injection based on JSR 330 (described in this page) and compile time dependency injection in Scala. There's an example on the [download page](https://www.playframework.com/download#examples): https://github.com/playframework/play-scala-compile-di-example and there's compile time dependency injection for Java as well, using Dagger 2: https://github.com/playframework/play-java-compile-di-example/tree/2.6.x
Play Slim: https://github.com/lloydmeta/slim-play#slim-play-app- with the PlayService module: https://www.playframework.com/documentation/2.6.x/Highlights26#PlayService-sbt-plugin-(experimental) and using julianrf/endpoints https://julienrf.github.io/endpoints/ or use the Play REST API example: https://developer.lightbend.com/guides/play-rest-api/part-1/index.html#routing-post-requests or use Akka-HTTP: http://reactore.com/generic-rest-api-services-using-akka-http/ 
I've just inherited a project which uses Play at every layer, including play-slick/evolutions and play-logging. It makes it very difficult to move modules around and make them API-agnostic. Even play-test seems to require _selenium_(!) as a dependency in every test, even if you don't use any play features in the test at all. That is serious bloat. Every one of our tests is a `PlaySpecification`. It's difficult to manage once it's everywhere.
Wrote a few paragraphs for my hepek static stuff generator [documentation](https://sake92.github.io/hepek/). Markdown is supported, prismjs for code highlighting, typesafe bootstrap rows/columns/grid, helpers of all sorts. Templates are just traits, contents are objects. Feedback welcoime. :)
Do you have a view for or against rho? I dislike the `case ...` syntax of straight http4s, but liked the look of rho.
I recently released 0.9.x, so I thought it was time for some more documentation and examples. Are there any more examples you'd like to see? General feedback is also very much appreciated!
Of course it all goes away if you just generate your PKs on the client. I switched a while ago and never looked back.
Wow, that seems like a lot of code for such a simple app! Thank you for the suggestion, I'll give it a go.
&gt; Wow, that seems like a lot of code for such a simple app! Welcome to Angular, the amount of boilerplate to do even the simplest of things is staggering. Typescript helps, to some degree, but I'd leap at the chance to port everything to Scala.js (cannot, due to lack of library support for anything beyond Angular 1.x).
Any problems, questions or feedback please email info@exceljava.com.
I did put @Inject as markers in the compile time dependency injection examples once, purely as an example. We got confused users asking why @Inject was required. I had to take them out again. All the compile time components you need are right there for both Scala and Java. With Java 1.8, you don't even need Dagger 2 to do Java compile time dependency injection. https://github.com/playframework/playframework/tree/master/framework/src/play/src/main/java/play/components I don't know of another framework that does as much for different kinds of dependency injection as Play. It supports pretty much everything, in any configuration. Then again, over the past year both the Scala and Java APIs have both been called "second class citizens" -- to each other. I suppose that's progress? 
Oof. Well, that's not how you're supposed to do it. Once you have several modules, you should be building trait only APIs and matching Play to those, with the implementations behind it. Play should only be in the root module, or at least only listed as a dependency when you're developing a Play plugin/module.
Fixed!
Circe gives you easy translations between case classes and Json. Play json makes you write format macros which you end littering everywhere. Circe makes me feel like I’m not littering my controller code into my deeper backend components.
`IndexedSeq` is only better in Seq in that it has less terrible implementations subclassing the interface, less so that the `IndexedSeq` itself promises something much more meaningful. `LinearSeq`is getting closer to something useful, if you don't mind being bound to the vague interpretations of "reasonably efficient".
&gt; but not with `IO` / `Task`. Maybe the `IO`/`Task` types we use today can't, I don't see why in theory a hypothetical `IO` shouldn't. Said hypothetical `IO` should encompass such considerations to prevent the proliferation we've seen already.
This is nice but really no information about license model ?
I am not a great excel user myself but what do you think a free tier (even for commercial use) based on size of the file (i.e number of rows, one tab only, etc) ?
Spark for excel?
Any tips for debugging compilation time? I'm working with the scalaxb library to interface with some rather old services, and the code it generates for my wsdl/xsd files takes over a minute to compile if it's successful at all, occasionally causes stack overflows during type specialization, and occasionally generates "file name too long" errors (apparently for anonymous classes). Given the stack overflow, I assumed it was something with implicit conversions/resolution maybe being recursive, so I've tried `-Ytyper-debug`, `-Ydebug`, `-Xprint-types`, and maybe some others but haven't been able to discern anything useful from the output (I have no idea what I'm looking for). Adding the arguments seems to change the errors, e.g. `sbt clean compile` will have different errors depending on whether `-Ydebug` was used. I also tried `-Ybreak-cycles` in case the types were recursive but that had no effect. I'm on 2.11 and won't be able to move to 2.12 for this project because none of our internal libraries are cross-built. There are some obvious deficiencies in the generated code like generating a 12kloc file full of implicits and anonymous functions, so I'm currently playing around with a clone of scalaxb to see if I can improve the generation and make it faster by accident, but to no avail yet. Most of the non-trivial functions and implicits have fully-specified signatures, so I don't _think_ it's spending too much time re-analyzing function bodies to figure out what all the types are. The fact that it's intermittent indicates to me that compilation order is important, but I don't know how to control that, or how to figure out what the problematic ordering is.
I want to write slick support for snowflake database. I tried reading through the code of the existing MySQL slick driver, but I could find any patterns which would allow me to extract rewrite something similar for snowflake. is there any guidance available anywhere to add support for new databases in slick 3?
Is there any good and maintained library to do machine learning on timeseries with Spark in Scala ? I've been looking around and found a spark-ts package which, unfortunately, seems unmaintained as of today. Cheers!
If I can make an off-topic suggestion, please enable https for the site. My previous ISP would inject scripts into http websites, that's why I have a. changed my ISP b. stopped enabling JS on http sites. My current ISP probably doesn't inject scripts, but I don't want to find out the hard way. And moreover, there would be other peope who might be using not-so-good ISPs.
Indeed - it's just that, with the current hosting setup I have, https is literally 10x the price (even though the certificate itself is free from letsencrypt). I'll look into an alternative setup.
If you are looking for alternatives (for a static site), I suggest netlify.com. Pushing updates is as simple as a git push. They have integrated letsEncrypt, CDN, etc. (I have no affiliation)
Probably because it's faster, and has a larger, more mature set of plugins.
i believe that part of being an Apache project is that it must use Maven to build artifacts. Jira is also the required issue tracker. sorry i couldnt find a reference for this info. 
'cos spark is the most "java without semicolons" project ever. just look at their [code conventions](https://github.com/databricks/scala-style-guide), the whole thing could be replaced with a single line: "just pretend it's still java"
I think what they really wanted was Kotlin.
What does maven do that SBT doesn't? Reliable backwards compatibility, reliable IDE integration, more readable build definitions (verbose XML is better than symbol-soup code IMO) that are more amenable to declarative reasoning, enforces that all custom build steps are properly encapsulated as plugins, conventions that promote consistent build processes across all projects.
Take a look at: https://www.reddit.com/r/scala/comments/7v515l/opening_oss_project_with_intellij/ Once again, shame on you reddit Scala for downvoting a perfectly valid question.
Then to my mind the question to ask is why SBT is there. I suspect it's because some people find `sbt ~test` a useful workflow, which is something Maven doesn't support AFAIK. Personally I don't think that running tests on file changes is an appropriate responsibility for a build tool, rather it should be done by an IDE that understands the build, but some people prefer to use a text editor to edit Scala. So being very speculative, I'd say that SBT is supported not for use as a build tool but as a pseudo-IDE support tool for people who like to edit Scala in a text editor, a use case that doesn't really overlap with what the project uses Maven for.
Maven can't do everything. Maven for example can't cross compile against Scala versions. Scala gets around this by having to manually rewrite the `pom.xml` file.
Have you checked out JOOQ? https://www.jooq.org/doc/2.6/manual/getting-started/jooq-and-scala/
I did a couple Scala + play 2.4 + postgres apps a few years back and had no issues. I did one with slick and one using standard SQL. At the time my biggest complaint was how slow slick builds were. I don't remember what I was using librarywise for the SQL app but it wasn't anything strange.
Softwarewill has a blog post where they compared some of the notable relational database access libraries in the Scala ecosystem... Find the blog here: https://softwaremill.com/comparing-scala-relational-database-access-libraries/ Maybe you want to look that up? Perhaps you find something that can help you with your task in there.
Have you seen [doobie](https://github.com/tpolecat/doobie)? It's purely functional and at the end of the day you just end up writing SQL.
I've used scalikejdbc and was alright, but never hear it being mentioned so not sure how widely used it is
https://github.com/rocketfuel/sdbc
Seconded. Also for the ability to typecheck SQL.
Thanks, that's a great summary. It's one of the few places where I could find non-trivial SQL examples from the different libraries.
I haven't looked at doobie before. But they just proved my point: their documentation only contains trivial examples.
Quite honestly, there’s a lot more Java support than Scala support for relational DBs. I would go with JDBC or anything else with a long and rock solid history over anything fancy/new. Your database is the heart of your application. Getting fancy there isn’t worth it. 
I disagree, just read the entire code convention and it's perfectly fine, specially for a project of its size.
The existing scala-maven-plugin can't, but the new scalor-maven-plugin claims to support cross compilation.
Just for the record, It's not purely functional if it returns `Future`, either twitter or stdlib, both break referential transparency. Nothing wrong with finch. Just want to point this out, since it does matter.
It's literally just typechecked sql. It's not an ORM, it's a wrapper over JDBC, that makes it nice to write SQL that the compiler can complain about, unlike raw jdbc which explodes at runtime. Anything more than a "Trivial example" is quite literally just whatever you can do in jdbc. What exactly is it that you're looking for? A full webapp written for you?
If you look at the sidebar there is a section with links to database libraries. It will come down to the features you want and the kind of code enjoy writing. There are a lot of good options in Scala.
It's a JDBC layer... if you want more examples you need to read JDBC documentation
I definitely prefer a DSL to a config file, Rho is a DSL too. I dislike the use of `case` specifically because it's usually used for unsafe code, so I tend to see it as a red flag in the codebase.
There is also Relate, which is what we at Lucid use. We also wrote it, but it's pretty nice to use: http://lucidsoftware.github.io/relate/
Haha, yes, in a sense... I'd like to gain a better idea of what my webapp would look like if written in doobie without having to first fully understand the library.
Relate was written because of frustration with the poor performance of anorm (but that was a few years ago, anorm may be better now). As such, performance is a high priority for it. It also uses scala string interpolation for writing SQL queries, and makes use of serialize/deserialize SQL columns from/to scala types.
&gt; because it's usually used for unsafe code Ok here we'll _strongly_ disagree. There's loads of uses of `case` with pure and safe code.
Sweet, thanks! That's the type of library I was looking for.
http://getquill.io/ is fantastic
&gt; I'd like to gain a better idea of what my webapp would look like if written in doobie Here, it _really_ depends on how you decide to write it and what your framework allows you to do. I have my webapps in terms of algebraic DSLs in a final-tagless style, for example. If you do it a bit more OO-FP, it will look different. If you use it with play framework, then you'd be more play-style. The wild variations of the way your app can look is part of what is a double edged sword in scala. I wouldn't use a sample webapp as what it will look like, but only what it _can_ if you embrace the style of the author. This is less prevalent in say, play, mostly because you're sort of forced into guice and injecting components via runtime DI, for example.
I believe it's a combination of legacy, need to support java, need to support maven publication, at the time the project started. 
Maybe "usually" was an overstatement. The point is that `case` can be used unsafely, and the unsafe uses don't look any different (locally) from the safe cases.
haha, loved the "previously on reddit". I still agree with his comment here https://www.reddit.com/r/scala/comments/2ze443/a_good_example_of_a_scala_style_guide_by_people/cpia4oq/ though. Perhaps kotlin was better for them, but I don't think that if you want to code scala like not-haskell, then you should stick to kotlin. I still think scala is a better kotlin than kotlin itself at the things kotlin tries to do well.
We generate our models from the sql itself: http://slick.lightbend.com/doc/2.1.0/code-generation.html This is worth you looking into as it solves the problem I think you are describing
That can't be true. What do they do when they adopt projects that are already using something else?
https://tpolecat.github.io/doobie-cats-0.4.2/09-Error-Handling.html By documentation you mean this book is not enough?
If it helps, I’ve been using doobie in various apps in my day job (finance), and it’s proven to be more than capable of being used in so-called real-world web apps. 
Anorm used to be the default for play. It's still supported pretty well. Now that they are pushing slick more I am thinking anorm isn't going to get a lot of updates in the future. I still use anorm though. I don't think slick really delivers very well on it's promises. It's syntax is super confusing and it's not really clear how to actually realize their promise of creating query fragments that are composable and reusable. We've had tons of issues with it generating completely retarded SQL and with bad connection management at my company. I'm trying to get everyone here to abandon slick.
Except in slick you get giant unwieldy nested tuples with the join API or if you use the for comprehension you get terribly generated SQL.
Been using ScalikeJDBC for an good while, and it's really nice to work with.
Why not break away from the past: https://neo4j.com/developer/graph-db-vs-rdbms/ ;-)
Generally, what I hear from my coworkers is "Why is spark built?" Apparently, dependencies are a nightmare every time we update Spark.
Yeah, maybe I'm being closed minded, but showing SELECT queries without any JOIN examples isn't sufficient for me.
Yeah, I'm definitely not opposed to considering a graph DB... I don't know much about them but will look into it more. Thanks.
Here's the code from a real application: https://github.com/SmartBackpacker/core/blob/master/api/src/main/scala/com/smartbackpackerapp/repository/PostgresAirlineRepository.scala 
Yeah there's a note [in the FAQ](http://tpolecat.github.io/doobie/docs/17-FAQ.html#how-do-i-handle-outer-joins). It should be more prominent.
I have some code I wrote once to flatten such nested tuples. Maybe I should publish it one day... BTW Quill has the same nested tuples behavior.
My experience with Slick so far has been best described as finding 1-2 features I do like, followed by finding 2-3 things that seem very unusual (or bad). My opinion may change at some point, but I have yet to find an ORM that I'd prefer to use over writing native SQL myself. That's coming from someone whose SQL skills are average at best for a Sr Engineer. edit: My opinion of ORMs so far is about the same opinion I have of those "Scala to Javascript" or "Java to Javascript" libraries. You're almost always better off writing code in the actual language that's used. Perhaps the #1 thing for me which defines why to not use an ORM (or Javascript generator, etc) is you'll eventually run into a scenario where you need to do non-trivial debugging. When you debug, you'll almost always have to start with the generated code, which is usually not very readable (because that's not it's purpose). Once you find the bug, you have to essentially reverse engineer the library to figure out how your code results in the buggy generated code. It also makes it VERY easy to write very bad (generated) code. It may look elegant in Scala, but the generated SQL might look like Satan's worst nightmare. 
it is testtime check.
Some peoeple may mislead it, as for typechecking with DB schema and SQL. Doobie is test-time-check. Slick is compile-time-check. IntelliJ is edit-time check. Most people use Slick for Play, I'm also using Doobie, because I'm familiar with SQL and Slick doesn't suuport all SQL (for example Postgresql's insert on conflict do ). 
Try path("") instead. That being said, I found running nginx in front of such services and doing reverse proxies much easier. It can do all the basic HTTP stuff for you in a much simpler manner and your akka code can focus on the real routes. https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/ 
You probably want pathSingleSlash. Check this out: https://doc.akka.io/docs/akka-http/current/routing-dsl/directives/path-directives/pathSingleSlash.html
I'd suggest default GPLv3 licensing + paid support + paid closed-source licensing.
I would highly recommend passing it as a parameter to retain type safety and avoid the situation where you found yourself without context cause something failed to initialise/propagate it properly (especially when you evolve in a multi-threaded environment which is gonna be your case) If you want to disregard this and are comfortable spending more time debugging, then `MDC` for `Mapped Diagnostic Context` can be a solution. A couple links : * http://yanns.github.io/blog/2014/05/04/slf4j-mapped-diagnostic-context-mdc-with-play-framework/ * https://github.com/lightbend/scala-logging Note that even Play has some notes in the documentation warning about the fact that MDC is hard to work with : *https://www.playframework.com/documentation/2.6.x/ScalaLogging#Using-Markers-and-Marker-Contexts 
Gotcha... Yeah, I looked at it, it is pretty complicated. Thanks.
&gt; verbose XML is better than symbol-soup code IMO I thought you're a scalaz advocate. How is scalaz's "symbol soup" better than SBT's? In the latter, the most used operators are `:=`, `+=`, `++=` and `%%` which are pretty basic. If you're against operator overloading and prefer verbosity then why use Scala?
Not /u/labyj but I would like to see it. 
&gt; I thought you're a scalaz advocate. How is scalaz's "symbol soup" better than SBT's? I advocate the techniques and abstractions from ScalaZ, not the symbols. I've repeatedly said here and elsewhere that ScalaZ (and indeed Cats) should replace the symbols with more readable names. (At my current job I maintain a library that implements similar functionality under more readable names; I'd like to open-source it but it's a slow process). &gt; If you're against operator overloading and prefer verbosity then why use Scala? What other language has higher-kinded types, typeclass-like functionality, and reasonably mainstream support? Only Haskell and the symbol situation there is if anything worse.
&gt; What other language has higher-kinded types, typeclass-like functionality, and reasonably mainstream support? I'd recommend [Nim](https://nim-lang.org/), see [concepts](https://nim-lang.org/docs/manual.html#generics-concepts) and [a functor sample](https://play.nim-lang.org/?gist=a01c9fd62e6271aa7fd598bc539d40fa). It doesn't have "mainstream support" but I don't think scala has that either. If you miss niché libraries(like me on the JVM) then the C/C++ FFI is easy in it. Nim doesn't have all the features of Scala but it's somewhat similar and it has cool stuff like the [parallel mechanism](https://nim-lang.org/docs/manual.html#parallel-spawn-parallel-statement) which checks the target part of your code to be free of data races at compile-time. It also has powerful and simple hygienic macros and templates and very fast compilation. Another one is Rust which is less advanced when it comes to abstractions but linear typing is very powerful when it comes to safety and efficiency. It has typeclasses but no higher-kinded types. It has a huge community.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://nim-lang.org/) - Previous text "Nim" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
&gt; I'd recommend Nim, see concepts and a functor sample. The functor example already seems to be straining at the limits of the system with that `stripGenericParams` (reminds me of attempts I've seen in C#, it seems like it doesn't have a nice way of working with the typeclass without an instance of the thing, which you need for monoidal zero and monadic point). It looks like the system is structural rather than nominal which I dislike (too easy to accidentally implement an interface, plus e.g. it seems like there would be no way to have the same thing be a functor in two different ways even with a newtype wrapper?). And it doesn't mention any way to talk generically about all the fields of a struct, so I assume there's no way to do typeclass derivation and using typeclasses for things like JSON serialization or database access wouldn't be practical at all? &gt; It doesn't have "mainstream support" but I don't think scala has that either. Scala has a good tooling and library ecosystem - multiple IDEs, support in profilers/debuggers/IPC systems/..., native-to-Scala libraries for most "normal" tasks you would want to do all available via Maven Central where they're indexed, there are multiple mature ways to run a local proxy repository etc.. And I've had no trouble finding full-time jobs in the language over the last ~6 years. &gt; If you miss niché libraries(like me on the JVM) then the C/C++ FFI is easy in it. C/C++ FFI terrifies me - one bit of undefined behaviour and your whole program can be destroyed, and almost all nontrivial C/C++ code invokes undefined behaviour. I've used maybe 3 bits of C/C++ FFI on the JVM (more back when I did Python) and it's always been a huge source of bugs that are difficult to debug/test. (E.g. you can't use a unit test to detect whether your C/C++ libraries are segfaulting, because the segfault brings down the entire JVM, killing every thread, and so all the tests that were running at the time will fail with the same symptoms, even ones that are unrelated to the issue you're hitting). I view the ability to stick to only JVM libraries, where the worst that can happen is an exception[1], as a huge advantage for Scala and wouldn't want to give it up. &gt; Nim doesn't have all the features of Scala but it's somewhat similar and it has cool stuff like the parallel mechanism which checks the target part of your code to be free of data races at compile-time. Shrug. I've looked at it a few times and it seems... fine? For years my go-to question when looking at any new language has been "Why would I use this over OCaml?" and as far as I can see Nim has maybe a few convenience features but nothing really compelling. (I feel similarly about D and Crystal). &gt; It also has powerful and simple hygienic macros and templates I view macros as if anything a disadvantage - I think they encourage code that's hard to maintain. Scala's cumbersome macro systems have perversely ended up being good for the language IMO, since they've discouraged unnecessary macros (while leaving them possible in the cases where they're absolutely necessary). &gt; Another one is Rust which is less advanced when it comes to abstractions but linear typing is very powerful when it comes to safety and efficiency. I like linear typing but I hate language-level special cases; the linearity in Rust is a language-level builtin that you can't control, which in turn leads to its control flow structures being embedded in the language and not things you could reimplement yourself in user code, which has knock-on effects on style and culture throughout the language. I'm glad Rust exists and is paving the way for linearity, but I'm pinning my hopes on something like Haskell's "linear arrow" proposals that would offer linearity in a more first-class way, as something more like a library, more like plain old code. &gt; It has typeclasses but no higher-kinded types. Yeah, I'm not willing to give up on higher-kinded types - I think in them these days, and it's far too cumbersome and painful to manually translate the code in my head into all the specific cases. If and when Rust gets them or an equivalent (concretely, I want library functions like e.g. Matryoshka's `cataM` - some people claim "associated type constructors" offer equivalent power to HKT but I'm not convinced, I'll believe it when I see a `cataM` implementation) I'd be open to taking a job doing Rust, though my understanding of some statements from the Rust developers is that Rust doesn't intend to ever support true HKT. &gt; It has a huge community. It has a *vocal* community but I'm not at all convinced that that corresponds to actual production usage. Certainly last time I looked for full-time Rust jobs in London I didn't see any (you sometimes see a few postings that mention it, but when you drill down it turns out they're seeing it as a desirable skill or maybe have one small component in Rust, not that they're looking to hire someone to actually work in Rust the whole time). [1] Or a bug in the JVM itself, sure, but the JVM is immensely widely used and goes through a much more stringent testing/verification process than a random C library you would FFI into.
Or perhaps `pathEndOrSingleSlash`?
As with any slow-running Scala program, my first move would be to throw JProfiler at it.
I'm also using this. It's relatively easy to use. The only thing that I don't like is that it supports string interpolation. Granted, it converts interpolated values to jdbc placeholders automatically, but it can create bad habits in junior engineer that don't understand why it's bad practice or that it's performing magic internally. I just directed my team not to use the feature.
Technically, that's embracing a much further past: https://en.wikipedia.org/wiki/Network_model
&gt; You could translate your example pretty directly into Scala (after implementing type-level library functions for stripGenericParams and typed) using structural types. Not really. But you see, Nim's concepts have more features to describe requirements and it's much nicer syntactically. Beauty is in the eye of the beholder, of course. &gt; Scala programmers prefer not to use that style because we prefer being explicit about which types implement which interfaces, but that's semantically contentful information, not just extra ceremony. It is extra ceremony. There's no reason to be explicit, unless you want to change the function's behaviour when linking the types but that's a bad idea. &gt; So what you're telling me is that this feature is still in flux and you can't build stable code on it yet? What I've shown to you is stable. But it'll be easier and concept will gain more "power". &gt; More "powerful" in the sense of being easier to hurt yourself with (and "behavioural" meaning "structural" here), right? The things that you can't implement with typeclasses are almost always things that are a bad idea to implement. More powerful by being able to tell what to check more precisely(you can call it "structural", but there's no need to touch structures and no need for a certain structure). &gt; Sometimes there are multiple functors with the same carrier datatype. E.g. Either you can implement map on the left or on the right and both of those implementations form legitimate functors. You want left *and* right-biased functors in the same codebase? Btw, you can do it by changing `map`'s meaning in the context - similary to how you do it in Scala but without implicits. &gt; Reflection means you don't have compile-time type safety (it's also much less efficient). I wouldn't go back to it. You need reflection in Scala for serialization. That's how you interact with products in Scala(as I remember). &gt; Fortunately there is a really good build tool for Scala (maven). I disagree entirely. I used maven for java and I hated it. &gt; True, but not a showstopper. It was for certain companies and it made a lot of noise. &gt; Eh maybe. It's fast enough, it's faster than most programs. I've never had a serious performance problem in my own code, and indeed I've ported code from other languages to Scala to make it faster. (I've even seen Scala code perform better than the C++ code it replaced) Depends on the case, of course. What I mean is that the JVM generates bytecode and is not competitive with native languages if everything is written similarly. &gt; This is pure FUD. I'd expect that Nim will do worse in this regard, it's just that the language hasn't been around for long enough for this to happen yet. No FUD here, java libraries tend to die as soon as they reach 1.0 - and they keep the bugs. I started to use Scala 6 years ago and I'd to abandon libraries - sometime, I couldn't update to compiler because the library didn't support it. &gt; Wrappers and exceptions won't help you when a C library invokes undefined behaviour, by, well, definition. Many useful JVM libraries heavily use C FFI. What do you do then? There are also cases when writing something in a language with a GC is a bad idea. Or when it isn't written in it. &gt; I hate exceptions but they're less bad than bringing down your whole program, which is what bugs in a C/C++ library generally do. C++ has exceptions too. And if your bug is fatal then there's no problem. &gt; OCaml has good async support for parallel I/O which is often enough (it can't yet do parallel computation, sure), Just like python - and it feels bad in practice. &gt; and its module system is widely renowned and ahead of most languages I don't like it and it's not that useful either. None of use is using it, right? &gt; OCaml has been around for 20+ years and has an established ecosystem and developer base. Nonsense. The OCaml community is dead and the ecosystem was almost empty a few years ago. Its developer base is mostly that one company which uses it. &gt; So being the same or even a little bit better isn't enough; as a newcomer the onus is on Nim to demonstrate a compelling advantage. You want advantages? Here: fast compilation, **useful compilation errors**, friendlier syntax, larger ecosystem, better macros, templates, more powerful abstractions, C/C++/JS/WASM backends and targets, parallelism, safe parallel computing with `parallel`, fast GC, better performance and small memory-footprint. &gt; Nonsense (unless you're counting shapeless' generic support, which is the rare good use case, but should just have been built into the language proper). I do DB communication without macros all the time, I think quill is the best way to interact with databases and it uses macros. &gt; and things which people tend to call "category theory" (though that term gets thrown around a lot and means different things to different people). Typeclasses without [simulacrum](https://github.com/mpilquist/simulacrum) is a PITA in scala. &gt; I think it's the best language going overall, Best for what - that's the question. There are tradeoffs everywhere. &gt; Even if memory is managed for you there are plenty of resources that aren't - think file handles, database transactions... And that's why it's important for linear typing to be the default. &gt; It isn't, yet. But I think it has better long-term potential than Rust's. Do you think it'll have larger adoption? Or that they'll have something better than Rust's borrow-checker? &gt; I've not seen any convincing use cases for macros in general-purpose code that can't be better replaced by other language features, and I've seen lots of terrible use of macros, whereas with typeclasses I've only really seen reasonable uses, and I haven't seen good alternatives proposed for the things I use them for. If you replace every use case of macros with language features then your language will be way too complex. Also, macros can help you with improving your apps' perf. and to avoid boilerplate. Can you replace them too? &gt; This is 100% the opposite of my experience. What on earth are you working on? Not webdev - and that's the problem. If you're on the JVM and leave backend dev/basic data processing you're on your own. &gt; Sure JVM libraries sometimes have issues but the quality is a lot higher than for native libraries If by quality you mean easier to use then I agree. If you mean stability or the lack of bugs or efficiency then I strongly disagree. &gt; many C build systems don't even have unit tests by default, whereas in Java it's at least expected Expected? Maybe. But not used properly. &gt; And it's much more common for there to be a non-FFI implementation on the JVM than in other languages. And there are plenty of cases when it's better to have an FFI-implementation - or when it's the only sane thing. &gt; though in the cases where you need it it's not really any worse than doing it from any other language. The last time I've tried to use domain sockets from the JVM I've tried plenty of libraries - and they were ALL shit. I also needed an implementation for a specific serialization protocol(msgpack) but it was rather beta and hard to use. The previous time I needed cross-platform streaming and the only one was almost useless because it was really shitty. I can go back, if you wish. These events may classify as anecdotal evidence but they did happen.
Thanks! Yeah, I think it would be better to place the JOIN examples alongside the other SELECT examples.
That worked great, thanks!
I can talk about Play Evolutions and Flyway: - Play Evolutions, it has worked good enough to me, the auto commit thing is probably related to databases without transactional DDL (I'm looking at you MySQL), I would use them if I work with Play Framework because they are already integrated. - Flyway, it has worked good enough to me, I would use them without Play Framework, you just need to write the code for performing the migration when the application starts, I had a bad experience with the SBT plugin. Last, using pure SQL has the advantage to let you write anything specific to PostgreSQL, like check constraints that I use a lot, on the H2 for testing, I recommend you to use a real database instead, you can get it using the docker-it-scala plugin ([example](https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/test/com/alexitc/coinalerts/commons/PostgresDataHandlerSpec.scala)).
We have had good luck with Flyway. It's very simple and reliable. Just as a side note: I *highly* recommend you use the same database for testing and production. Vendor differences are immediate, significant, and often quite subtle. 
Never wanted to be involved in this kind of discussions, but (given your interest) Finch is using something called [Rerunnable](https://github.com/travisbrown/catbird/blob/master/util/src/main/scala/io/catbird/util/Rerunnable.scala) internally, not Twitter Futures. `Rerunnable` wraps Twitter Futures and makes then "rerunnable" (think of `Task`). The `Rerunnable` to Twitter Future conversion happens at the very last step when `Endpoint` is run.
I use it too, it's pretty great in that you're just writing typesafe SQL.
ah, my mistake. Thank you.
Yeah, I've been leaning with Play Evolutions because of the integration. I am actually a fan of pure SQL - the libraries can get confusing. Thanks for the advice on the testing DB. We've had some performance issues with full DBs in the past, but we'll give it a shot and look to make changes if our database tests get too slow.
I've used flyway pretty heavily and highly suggest it. Currently working on a way to migrate the database prior to integration tests using the flyway system. 
What ticked you off with Slick? I used Quill on a large project and it soon became very hard to manage. The last time I used Slick, it was a bit more code that I would have wanted, but things worked the way they were supposed to. I'm asking because I'm thinking of switching to Slick.
The project that I linked contains 5 test suites where a PostgreSQL instance is created by docker, locally, my whole application tests run in less than 40 seconds (around 160 tests) locally once the docker image is downloaded, on travis-ci, it takes around 7 minutes to complete everything. The biggest advantage is that I'm confident that all my database layer works as expected in most cases (what's covered by the tests), on API tests, I'm free to use a dummy but fast in-memory implementation because I have already tested the database layer.
+1. Finch is by far my favourite library for writing REST APIs. I've used it in production too, and I like how the development is moving forward seriously in a good direction. It's super clean, and I personally find it much easier to use than http4s.
I **love** the changes to the enum syntax. Puts things back on my favorite side of the expression problem. It reminds me of the Rust syntax which I'm similarly a fan of.
Will be interesting to see whether people can come up with good use-cases for it. Feels like all the syntax additions are a hefty price to pay for something that can only support a tiny subset of the existing ADTs, has no Java interop, and cannot support `Option` once value types arrive.
Interesting... yeah, one data point I found was that Postgres is significantly faster than graph DBs for the types of operations our webapp will be performing frequently (e.g. mostly reads): https://www.arangodb.com/2018/02/nosql-performance-benchmark-2018-mongodb-postgresql-orientdb-neo4j-arangodb/
&gt; Will be interesting to see whether people can come up with good use-cases for it. Just about every business application I develop starts with modeling the business domain with ADTs. So there's tons of use-cases. I'll replace my sealed traits on day one. &gt; Feels like all the syntax additions are a hefty price to pay Isn't the old, unpopular enum syntax being removed? So we've broken even on syntax. &gt; for something that can only support a tiny subset of the existing ADTs What are the major deficiencies? I'm curious. Looks like it'll do most of what I need. Of course, I'm not typically writing shapeless/scalaz/cats type stuff. ;) &gt; has no Java interop and cannot support Option once value types arrive. That's unfortunate. But are those implementation details that can be improved on with time while providing this consistent, lighter-weight (vs. sealed traits) syntax? 
Provided the `enum` syntax desugars to a standard JVM hierarchy (abstract class/interface with implementation in subclasses), this is great news for performance while enhancing clarity (i.e. writing a particular method in one site). Having methods defined 3+ times in your code (the sealed trait + at least 2 implementations) is a pain and a source of bugs. Moreover, the proposal covers most, if not all, of my use cases. Java interop? Java enums are very specific (basically a collection of values), write them in Java and you can use them from Scala just fine. Value types? That's going to be a wild ride when they land whatever `enum` does. In particular, it's going to be fun to write a value-class optimized `Option` while preserving compatibility across JVMs (higher-kinded types play really well with erasure).
i hope the lsp hooks that dotty has are being backported to scala 2.13. I'm sick of Intellij's buggy typechecking and ensime doesn't seem to be doing well at the moment...
This also happens using just the normal play migrations. It needs a file and if someone is behind the current upto date migration file current on master or multiple people create a new same file on their branches it can get problematic. On another project I was it was all rolled by hand, a system very similar to play but done outside of it, using pure sql, it followed the same logic of a new numbered file, but the same NN.sql could be on multiple branches with different names like NN-feature1.sql, NN-feature2.sql and at the time of the migration it would aggregate the multiple NN files and apply it as a whole single transaction. Actually kinda cool, but all hand rolled. Also didn't have any downs/rollbacks, which I actually liked and prefer and I think downn/rollbacks are a stupid thing to have. You should never need to support and undo those kind of operations since they are destructive and we never want those kind of things to run on a production environment. It simply existing is a problem and I've seen it a more than a couple times causing problems to really dislike it.
My current project uses Entityframework (C#). Instead of numbering, they use date strings. It makes it easier to avoid conflicts, but you have to make sure that they are merged in the correct order or they could get skipped/error due to inconsistent state.
How often do you upgrade that? I've been using it today for the first time and am so blown away! Don't know if I want to spend $500 + $180/major release in upgrades though if it's going to be every year. Might just bite the bullet though since it's just so incredibly useful.
&gt; 1) I think the issue is that even if enums are fine for 95% of your use-cases, you will need to go back to sealed traits for the last 5%. I feel that's really inconsistent, especially because you also have to override all your `apply` methods now. Also, with enums you lose the possibility to write things like def add(rel: Relative, abs: Absolute) = ... which might be fine for some use-cases, but maybe not ok for others. And as soon as you hit such a use-case somewhere, you have to go back and rewrite the enum as a slealed trait again. &gt; 2) Yeah, not sure how the claims about making things simpler and more orthogonal works out when there is already 4 new proposed keywords, substantial syntax additions, and more stuff added to the global namespace.
I’ve used both systems in real life production. The integration for both is simple. The con of flyway is that rollback support is nearly nonexistent. The author specifically thinks you shouldn’t need them. Frankly, after a little bit of thought and integration test setup, I tend to agree. I’ve never actually seen a successful automated rollback in production. The con of the ANorm migrations in Play is that they don’t support parallel execution. You have to have a specific deployment step to migrate. It ends up being operationally a significant lift to make it work reliably. In the end, were I starting a new project, I’d use Play with migrations turned off, Slick for database access, and Flyway for migrations. For testing use either opentable’s embedded Postgres or whatever will embed for your dbms of choice. That pr bed to be a winning solution last I used it, although the spin up time for integration testing was _brutal_. Proper service database decoupling is crucial for sanity. 
I mean, with a sealed trait + implementation classes, you can implement stuff using pattern matching in the sealed trait, or implement/override methods in the children. In that second case, you end up splitting implementations across the code. Yep, most of my functional-style ADTs are one level deep; when I have several levels, it's OOP-ish style. Thanks for pointing out the `enumValues` stuff, that's indeed contradictory. For the Java compat, you'll need a compatible enum when you are interoperating with existing Java code, which is not cross-platform anyway. I cannot think an example where this will lead to major code duplication; do you have one?
You can do something like import play.api.libs.json._ import play.api.libs.functional.syntax._ object PersonalInfo { implicit val reader: Reads[PersonalInfo] = ( (JsPath \ "email").read[String](Reads.email) and (JsPath \ "password").read[String] and (JsPath \ "firstName").read[String] and (JsPath \ "lastName").read[String] )(PersonalInfo.apply _)
I think your last point hits on it. Actors are a low-level building block. They can be appropriate in some cases, but even then, it's best to wrap them as tightly as possible in future-based API.
I've only used it when working for organisations with licenses, never been the one paying.
Even with one level layer nesting, enum's are basically one of the most common structures we create in typical "business" apps. Because of this, we basically have to use https://github.com/lloydmeta/enumeratum in every project and in some cases we have had binary compatibility issues because of transitive dependency hell. This functionality really should be in stdlib, and actually I was arguing for something simpler in the thread but something is better than nothing (or the current Scala enum for that matter).
For most use-cases, the relational model will outperform the others. The other models are useful only for specific data problems.
&gt; it has a few but powerful features(the typesystem, concepts, macros, disjoint-check, exceptions and pragmas). And the feature we're talking about - concepts - will become easier to use. I'm still dubious about concepts as a distinct feature rather than typeclasses as a pattern implemented with plain old classes, but I guess I'll keep an eye on how it develops. What about the `stripGenericParams` and `typed` - are those functions I could implement for myself in the language, or magic builtins? &gt; Btw, you do realize this is r/scala, right? Scala isn't famous for having a small subset of stable and powerful features. It's the opposite. The reputation is wrong though. If you look into the things people call out as "bloated features" in Scala they're usually actually talking about library features, or things that are combinations of simple features. &gt; Then define flatMap like that. I do, the problem is that with concepts if I do that then `Validation` will magically become a `Monad`, which I don't want. &gt; Also, since the flatMap of Validation is not a "real" flatMap you can give it another name. At which point we're essentially saying that all function names share a global namespace and every function has to have a unique name. That's exactly my objection to structural typing. I'd like "this type implements this interface" to be a deliberate, explicit decision, not something that happens implicitly because the functions have the same name. &gt; How is it a problem here? If I defined a `Monoid` concept then the method on my statistics class would have to be called `plus`, or I'd have to pollute the main class interface with both `plus` and `merge` methods where one just calls the other. With typeclasses, the typeclass instance says that *monoidal* plus for `ReportStatistics` is `merge`, but this is more narrowly scoped. It gets even more important for things like serialization - secondary concerns that shouldn't pollute the primary interface of the class, but need to go somewhere - and that kind of thing is a really common and important use case for typeclasses. &gt; Static serialization is not what we were talking about. You can't create flexible and easy-to-use serialization libs without metaprogramming or reflection. You can consider typeclass derivation to be "metaprogramming" if you want, but it's very simple and safe: you can only do one very controlled thing with it, so it doesn't carry anything like the risks of general-purpose macros. &gt; Then here's the thing: don't rely on undefined behaviour. I use C at work and I fail to see the issue. Our project has hundreds of modules and hundreds of dependencies and things are simple(except the memory management). ESR posted a 200-line C snippet when he was struggling to use Rust - and the snippet had undefined behaviour in it. Even the lowest-level, biggest-name C libraries keep having these issues - think Heartbleed, the zlib double free, the libjpeg arbitrary code execution. Even code that's carefully reviewed by security experts has these issues (the Chrome sandbox overshift that was explicitly LGTMed by two reviewers). The Linux kernel invokes undefined behaviour so often that it sets a compilation flag to try to control how that undefined behaviour gets compiled (`-fno-delete-null-pointer-checks`). &gt; I've seen the FFI killing the java program - and as I've told you there are cases when you just need to use the FFI. Fundamentally this is a problem any language that uses C/C++ libraries will have, not just Java. On the JVM the cases where you need the FFI are rare enough that you can take steps to mitigate it (e.g. running those FFI operations in a separate process), and a lot of code doesn't need FFI at all. In other runtimes you don't have that problem. &gt; C++ can be used safely, especially modern C++. &gt; If you check for nulls and bounds you'll be ok. C/C++ fans keep claiming this, massive security vulnerabilities keep happening. &gt; Just because your java thread is running it doesn't mean it's in a correct state. Perfect is the enemy of good. Avoiding getting your threads randomly killed doesn't solve all your problems, sure, but it's a good start. &gt; You mean, not having parallelism is not important? It's a long way down the list of priorities IME. The space of problems where you need single-machine parallelism but don't need distribution is pretty narrow (it covers what, one order of magnitude? Two at most?). &gt; It's just ridiculous because over the things you don't find to be "pain points" OCaml doesn't have anything and Scala only has like 1-2 useful features(subjective) and many major drawbacks. Of course OCaml doesn't have anything over my pain points because I'm using OCaml (or really any ML-family language) as my baseline. And to a certain extent I agree that Scala is a small advance in programming languages. But HKT is really useful and powerful without compromising safety, typeclasses (with derivation-like functionality) are really useful and powerful without compromising safety, I wouldn't give either of them up (or at least, a language that was missing either would have to offer something really compelling). Present Scala has plenty of flaws, but those two things are compelling enough that I'll stick with Scala until something better comes along. I have high hopes for Idris, or as we've said maybe Rust will get HKT. Or maybe Dotty will be good. Or maybe there won't be a better language and I'll keep writing Scala 2.x. Who can say what the future holds? &gt; Gatekeeping No, just using appropriate tools for the job. Macros are too dangerous to use for minor syntactic conveniences; the extra testing and review needed to achieve the same level of confidence in code that uses macros outweighs the advantage. &gt; No one said that Well what are you saying then? If you have linear types by default then you'll end up tracking ownership of most or all values - it's the same thing. &gt; Again, if you don't like macros then why use Scala? Again, I think Scala's the best language available, I still think it has plenty of flaws. My hard-ish requirements are HKT, typeclass functionality (including derivation), and a reasonable level of library/tool support (also all the usual good things you get from ML-family languages, like ADTs and parametric polymorphism - I'm treating those as table stakes), and that already rules out most alternatives. &gt; Scala is famous for having nice and original DSLs. Most of which have no need for macros - indeed many of them predate macros being added to the language. Something like the Spray routing DSL (or Rho for a modern equivalent) has no need for macros, and IMO those DSLs that are so radical as to need macros are probably a bad idea. &gt; Also, you can't compare using DSLs with bloating the language. Macro-based DSLs are just as bad as language-level bloat - they have exactly the same effect on maintainability because they have exactly the same effect on code. &gt; you've no reason to use Scala - it has tons of very heavy features and there are plenty of them which you don't like. That's the reputation but it's not my experience; Scala has a few small powerful features and then most things are implemented in plain code on top of that. There are some Scala features that I think are true bloat and would rather see removed (`Dynamic`, structural types (except for kind-projector style use), `do`/`while`...) but I never see them actually used in practice. If I was having to maintain code that used them I'd be bothered, but so far I haven't. &gt; Then by your definitions, there's no such language. There's no perfect language yet, sure. Scala is the closest I've found (e.g. I've found I need less boilerplate in Scala than in any alternative, on the whole) - like I've said, I'll use it until something better comes along. &gt; The scala ecosystem likes choices but I don't think you do. Sure, I think that's fair. &gt; No need to give up memory-safety completely - there's RAII. But you can't rely on it. As soon as you use a C/C++ library you've given up on memory safety, because you can't rely on not hitting bugs in that library that corrupt your memory. &gt; You can still corrupt your stuff on the JVM. If you prefer absolute safety then you should use Rust or Idris. I've literally never seen memory corruption on the JVM, so it's not common enough to be worth giving up HKT for. I'm enthusiastic about Idris - I'd take a job writing it if I saw one - though I do think the library/tool ecosystem isn't there yet (and worry that a lot of things would end up relying on FFI into C libraries - Idris does have a JVM backend but it feels like kind of a second-class citizen). &gt; Btw, most of the cool stuff is written in C/C++. Most of the JVM libraries are either wrappers or just toy projects. Citation needed, not my experience at all. Plenty of cool stuff (e.g. spark, kafka or freenet) gets written on the JVM, and a lot of C/C++ projects end up as terrible code that can't be reused at all outside the original throwaway project. Your "Btw, &lt;controversial claim stated as if it were fact&gt;" style is really not constructive, please stop doing it. &gt; Streaming multimedia. Ah fair. Yeah, video encoding without relying on C/C++ is in a really bad state - that was actually one of the cases I was talking about where I did need to FFI from the JVM.
 &gt; Niches?! You mean most of the programming domains? Areas where language microperformance matters are niches, and they're getting smaller most of the time. Programming culture has this weird obsession with performance microoptimization where we ask "what's the fastest possible way to implement the first algorithm I thought of?" instead of "what level of performance is required and what's the cheapest way to achieve that?" Most of the time, performance is probably fourth on the list - development cost, features, and defect rate are generally higher priorities. &gt; Game development - how much serious games do you see written in java? Even the ones written in it(ex. minecraft) are famous for performing poorly - and let's not get into memory consumption... It's so bad for game development, only the best-selling game of all time was written on it... Seriously I found gamedev had the worst of that kind of performance culture, and even so it's slowly coming around to using higher-level languages. Tomorrow's AAA games will be developed the way today's indie games are, as has always been the way of the industry. &gt; Systems/OS programming - what can I say? The JVM is obviously a shitty idea here. By all accounts MS' "singularity research OS" (using the CLR which is similar) worked out pretty well. We haven't seen serious efforts at JVM-based OSes because we haven't seen serious efforts at systems/OS programming at all for decades. Niche. &gt; Command line apps - think about the JVM's startup time. There's a lot you can do to reduce JVM startup time, to the point where it becomes usable for these. But most people don't bother, because it's not worth it - interactive use of command line apps is a tiny niche. &gt; Embedded - think about the JVM's resource consumption and that how hard FFI is on it. I've actually done this one, it was fine. &gt; Networking systems - that's another place where performance really matters so, no java for you. Done this one too. It's fine. &gt; Cryptography - managed memory has no place here. True, real issue - though niche. &gt; Desktop apps - swing and FX are neither easy to use nor efficient. They're also pretty much dead. True enough (though honestly FX isn't that bad), but that's because desktop apps in general are pretty much dead. &gt; And don't even think about browsers... Why not? Browsers are written in C++ mostly due to accidents of history (no-one's written a major new HTML rendering engine from scratch since what, 1998?) &gt; Front-end - not relevant. Not even with WASM(no GC yet). Shrug, maybe - not a case I care about since I'd use ScalaJS for frontend. &gt; Back-end - we all know that java based webframeworks are not competitive when it comes to performance, scalability or safety. Nonsense. Just picked a random page from the latest Techempower benchmarks ( https://www.techempower.com/benchmarks/#section=data-r15&amp;hw=ph&amp;test=query ) and the top Java framework is actually ahead of the top C or C++ framework. If we're talking about REST APIs, having HKT makes everything so much safer (you can do session-in-view but in a principled, safe way where your type signatures reflect which functions access the database or don't, and know exactly what to test). If we're talking about HTML Wicket remains the safest, most maintainable web framework I've seen on any platform. &gt; Mobile dev - android devices need more and more computing power because java wasn't a good idea for small computers. ios with swift and objective-c is doing fine because its memory management is "better" for this case. Android is winning on marketshare, whatever better memory management ios may or may not have doesn't seem to be translating into cheaper phones which is what ultimately counts. &gt; At the end, the JVM got into "enterprise" apps and mobile dev - but not because it's good at those domains now but because 20 years ago most of the native languages were too hard to use for the blue-collar programmers. It's almost like js and node for today's coding-bootcamp people. Being easy to use to produce programs that meet the requirements in a given domain is the only meaningful definition of a language being good at that domain. Using a "harder" language to prove you're a better programmer is just stupid posturing.
I have used both, and Quill is just basically a much leaner/cleaner/nicer implementation of Slick (both or FRM or functional reactive mappers). Slick tries to implement FRM through Scala's type system and collections (somewhat) and because of this its very heavy, both with respect to the code you write and at runtime. Slick is actually not a very fast database library, and the defaults that is uses (dynamic queries at runtime) make this even worse. Even slicks compiled queries (which are lazy val's) have performance issues. Quill on the other hand does all of what Slick does with a Macro AST. Queries are compiled by default (dynamic queries are opt-in) and you can even see the generated SQL at compile time, in IDE's and the scala compiler. Basically Quill will (almost) generate your query into a verified SQL string, which is as fast as you can get for a database library. Also quill has a lot more integrations for both databases and IO types (such as Monix)
Have you looked into Quill, its basically a much nicer implementation of Slick that also supports many more SQL's (including non SQL's like Cassandra!). It also has really good support for inline (or raw SQL), similar to how Doobie works.
I also like the new enum syntax, I'm just worried about performance implications if you cannot define members in the sub-types, e.g. for Option which is arguably one of the most pervasive types in Scala, having to go through a pattern match for each and every method - isn't that incurring a performance hit compared to an abstract interface and implementations on the sub-types, for which I guess the JVM is highly optimised?
Did you use Quill in production, by the way? Maybe I'm not the best at designing databases and wasting my time on a library when there are other issues.
I created a bash script for my team that creates either a versioned or repeatable migration using a full date as the version. Only requirement in distributed teams is that you have out of order migrations enabled. I don't think flyway supported down migrations prior to 5.x. My team runs flyway repair if a migration fails, and we create a new migration if data gets mucked up by a poorly written migration.
&gt; Areas where language microperformance matters are niches, and they're getting smaller most of the time. Programming culture has this weird obsession with performance microoptimization where we ask "what's the fastest possible way to implement the first algorithm I thought of?" instead of "what level of performance is required and what's the cheapest way to achieve that?" Most of the time, performance is probably fourth on the list - development cost, features, and defect rate are generally higher priorities. Are you a webprogrammer by chance? Because it seems like you're trying to make performance matter less because your tools are usually bad at it. &gt; It's so bad for game development, only the best-selling game of all time was written on it... Tetris? Not java. Btw, minecraft is a pretty dumb game(cheap voxel graphics) and it has *still terrible performance* and huge resource consumption. Ms bought minecraft and they're rewriting in it in C++ - and it's far faster. It's not like you can come up with any game which is written in java and it was actually a good implementation. The JVM sucks for games and it's funny because minecraft showed it in practice. &gt; Seriously I found gamedev had the worst of that kind of performance culture, and even so it's slowly coming around to using higher-level languages. Tomorrow's AAA games will be developed the way today's indie games are, as has always been the way of the industry. Hahaha. Even if you use C++ and know a lot of "tricks" it's hard to create a complex well-performing game. With managed runtimes it's impossible. If you look around all the mono/.net based games are pretty primitive and have worse performance than what we could expect. But it doesn't matter because they're mostly platformers or indie games. &gt; By all accounts MS' "singularity research OS" (using the CLR which is similar) worked out pretty well. We haven't seen serious efforts at JVM-based OSes because we haven't seen serious efforts at systems/OS programming at all for decades. Niche. Pretty silly argument. Btw, systems programming != OS dev. Think about drivers and system utilities. Also, developing OSes in a managed language? Don't you feel how stupid that sounds? &gt; There's a lot you can do to reduce JVM startup time, to the point where it becomes usable for these. But most people don't bother, because it's not worth it - interactive use of command line apps is a tiny niche. No, you can't. Not even with proguard. Unless you can come up with something better. &gt; I've actually done this one, it was fine. You've wrote small programs for small devices with very limited computing power and real-time contraints? Doubt it. &gt; Done this one too. It's fine. You mean you wrote complex networking systems in java? Haven't heard about anything similar at all. &gt; True, real issue - though niche. Cryptography vs niche. Are you joking? &gt; True enough (though honestly FX isn't that bad), but that's because desktop apps in general are pretty much dead. Yeah, we're talking with the wind right now. What a bullshit argument. Are you talking to me through your chromebook? &gt; Why not? Browsers are written in C++ mostly due to accidents of history (no-one's written a major new HTML rendering engine from scratch since what, 1998?) Dude, it'd suck. Just admit it, the JVM has pretty bad performance when it actually matters. The engines written in java were just jokes. The JVM may beat script languages' runtimes but against native it's just not fair. &gt; Shrug, maybe - not a case I care about since I'd use ScalaJS for frontend. You forgot your main argument: "Niche". &gt; Nonsense. Just picked a random page from the latest Techempower benchmarks A "random test case", sure. On that chart Dart is the first. It's kinda funny how you try to defend the JVM's performance. What do you think, what's a VM? Have you ever seen algorithmic benchmarks comparing java and c++? &gt; Android is winning on marketshare, whatever better memory management ios may or may not have doesn't seem to be translating into cheaper phones which is what ultimately counts. Yes, but that's unrelated. Android apps are shit because of its VM. Not so niche, huh? &gt; Being easy to use to produce programs that meet the requirements in a given domain is the only meaningful definition of a language being good at that domain. Unfortunately, java is not easy to use at those domains. &gt; Using a "harder" language to prove you're a better programmer is just stupid posturing. Pushing an outdated platform and denying its problems won't convince anyone either. You seem to be sticking to this: &gt; (I'm likely to remain convinced that it's the best general-purpose option though) but by "convinced" you meant "religious", right? You call everything "niche" when all these areas are like 80-90% of today's software. The rest is just toy code on the web.
Yes I have used both in production. Quill's performance is much faster than Slick or Doobie (especially if you choose to use the PostgresAsync driver). Neither Slick or Doobie can do this (although Slick is driver agnostic it hasn't actually implemented any integrations with other drivers) and Doobie is just a JDBC wrapper (so its completely tied to JDBC). Even outside of the whole JDBC/Driver issue, Quill behaves much more nicely in IDE's (compared to Slick/Doobie), has a really nice FRM API (with the ability to fallback to SQL just like Doobie when you need to) and generates superior runtime code compared to the other solutions.
&gt; Are you a webprogrammer by chance? Because it seems like you're trying to make performance matter less because your tools are usually bad at it. Nope. Right back at you: it seems like you're trying to pretend performance is a lot more important than it is because it's the only thing your tools are good at, and just tossing insults ("toy code") at anyone not working in the same area as you. &gt; Btw, minecraft is a pretty dumb game(cheap voxel graphics) and it has still terrible performance and huge resource consumption. Ms bought minecraft and they're rewriting in it in C++ - and it's far faster. It's not like you can come up with any game which is written in java and it was actually a good implementation. The JVM sucks for games and it's funny because minecraft showed it in practice. It's funny because it shows how little that kind of "good implementation" matters. Performance just isn't important, almost all of the time. &gt; Think about drivers and system utilities. Those don't need to be C/C++ either. Hell I've got some running on my system right now that're written in Python (HP's printer drivers). &gt; A "random test case", sure. On that chart Dart is the first. Indeed it is. What's your point? &gt; Have you ever seen algorithmic benchmarks comparing java and c++? Yeah, I just don't think they're measuring something relevant. Hand-tuned C++ with infinite development time on compute-bound problems tends to be a small factor faster than Java (1x to 2x in the general case, occasionally 5x on small specific benchmarks). &gt; Pushing an outdated platform and denying its problems won't convince anyone either. Every year computers get faster and the impact of security vulnerabilities gets worse. C/C++ and anything that relies on them for libraries are a lot more outdated than Java. &gt; You call everything "niche" when all these areas are like 80-90% of today's software. Citation needed. Java is the most popular language in the world, and was only created in 1995 by which point there were lots of other options, so it can't be so bad for so many of the cases.
I wrote an answer.
&gt; I'm still dubious about concepts as a distinct feature rather than typeclasses as a pattern implemented with plain old classes, but I guess I'll keep an eye on how it develops. Concepts are not typeclasses. &gt; What about the stripGenericParams and typed - are those functions I could implement for myself in the language, or magic builtins? stripGenericParams is a macro, typed is how you define a generic type in a concept. &gt; The reputation is wrong though. If you look into the things people call out as "bloated feature" in Scala they're usually actually talking about library features, or things that are combinations of simple features. I looked into it and Scala has too much features - even according to you. I doubt that someone who knows scala and is not a scala-evangelist will agree with you. &gt; I do, the problem is that with concepts if I do that then Validation will magically become a Monad, which I don't want. You misunderstood. Change flatMap's signature or name. Done. &gt; At which point we're essentially saying that all function names share a global namespace and every function has to have a unique name. They don't share the global namespace unless you import them and they're importable. Btw, if Validation is not a Monad then define Monad so, that Validation won't be one. Or don't use the flatMap name. &gt; That's exactly my objection to structural typing. I'd like "this type implements this interface" to be a deliberate, explicit decision, not something that happens implicitly because the functions have the same name. It sounds more like bike-shedding and that you don't have a reasonable argument against it. &gt; If I defined a Monoid concept then the method on my statistics class would have to be called plus, or I'd have to pollute the main class interface with both plus and merge methods where one just calls the other. With typeclasses, the typeclass instance says that monoidal plus for ReportStatistics is merge, but this is more narrowly scoped. And you can do that in nim. &gt; It gets even more important for things like serialization - secondary concerns that shouldn't pollute the primary interface of the class, but need to go somewhere - and that kind of thing is a really common and important use case for typeclasses. I don't see how concept has any problem here. &gt; You can consider typeclass derivation to be "metaprogramming" if you want, but it's very simple and safe: you can only do one very controlled thing with it, so it doesn't carry anything like the risks of general-purpose macros. You can't do general-purpose serialization with typeclasses. Unless you want to write a bunch of useless boilerplate. &gt; ESR posted a 200-line C snippet when he was struggling to use Rust - and the snippet had undefined behaviour in it. Even the lowest-level, biggest-name C libraries keep having these issues - think Heartbleed, the zlib double free, the libjpeg arbitrary code execution. Even code that's carefully reviewed by security experts has these issues (the Chrome sandbox overshift that was explicitly LGTMed by two reviewers). The Linux kernel invokes undefined behaviour so often that it sets a compilation flag to try to control how that undefined behaviour gets compiled (-fno-delete-null-pointer-checks). And you think JVM libraries are perfect and can't have sec. bugs? Or just because you have exceptions you think you're safe? No, you're not. &gt; On the JVM the cases where you need the FFI are rare enough ...if you only do webdev and simple string processing and you don't care about performance and don't need cutting-edge graphics/multimedia/crypto libraries... &gt; C/C++ fans keep claiming this, massive security vulnerabilities keep happening. 1. I was talking about C++ and RAII. 2. I don't see that much "massive" sec bugs happening with modern C++ codebases. 3. It's not like you can claim Java is safe - especially not if you compare it to something like Rust. &gt; Perfect is the enemy of good. Avoiding getting your threads randomly killed doesn't solve all your problems, sure, but it's a good start. Good start for what? If you need fault-tolerance, you're on the wrong VM. &gt; But HKT is really useful and powerful without compromising safety, typeclasses (with derivation-like functionality) are really useful and powerful without compromising safety, I wouldn't give either of them up (or at least, a language that was missing either would have to offer something really compelling). If you truly like safety then you need dependent types and linear typing because HKT and typeclasses won't help you at all(except at making your apps' performance worse). &gt; Present Scala has plenty of flaws, but those two things are compelling enough that I'll stick with Scala until something better comes along. Better for what and when? Based on what? &gt; I have high hopes for Idris, or as we've said maybe Rust will get HKT. Or maybe Dotty will be good. Or maybe there won't be a better language and I'll keep writing Scala 2.x. *Who can say what the future holds?* Maybe a little objectivity? &gt; No, just using appropriate tools for the job. Macros are too dangerous to use for minor syntactic conveniences; I've already told you the advantages of macros. If you still don't know what they're for then I recommened for you to study them and form an opinion *after* you know their drawbacks and benefits. &gt; the extra testing and review needed to achieve the same level of confidence in code that uses macros outweighs the advantage. What extra testing and review? What do you think macros are? Magic? Nope, they just let you manipulate the AST a little bit *safely*. You know that scala implements some features through macros, right? &gt; Well what are you saying then? That it should be the *default* and not the only one. &gt; If you have linear types by default then you'll end up tracking ownership of most or all values - it's the same thing. Yes, and that's good. *Especially if you actually care about safety*. &gt; Most of which have no need for macros... Yes, not all of them need it but that wasn't the point. &gt; Macro-based DSLs are just as bad as language-level bloat - they have exactly the same effect on maintainability because they have exactly the same effect on code. Nope. Maintaining macros is easy - just like avoiding them. Avoiding macros is better than not having them when you need them. &gt; That's the reputation but it's not my experience; Scala has a few small powerful features and then most things are implemented in plain code on top of that. Like which ones? &gt; There's no perfect language yet, sure. Scala is the closest I've found (e.g. I've found I need less boilerplate in Scala than in any alternative, on the whole) - like I've said, I'll use it until something better comes along. If you don't like boilerplate then you're supposed to like macros and nim. &gt; But you can't rely on it. As soon as you use a C/C++ library you've given up on memory safety, because you can't rely on not hitting bugs in that library that corrupt your memory. You paint the situation of C/C++ so badly but IRL it's not like that. There are smart pointers, valgrind, cppcheck etc. which help us to avoid most memory-handling issues. &gt; I've literally never seen memory corruption on the JVM, FFI. &gt; so it's not common enough to be worth giving up HKT for. It's not like HKT is important. I'm pretty sure you can't show me how it'll "save the day" - maybe, help you to save 1-2 lines sometimes. I've used haskell and scalaz but meh. There are far more important features in a programming language semantically than HKT. &gt; Citation needed, not my experience at all. Plenty of cool stuff (e.g. spark, kafka or freenet) gets written on the JVM, spark is a niche library in its own domain and the ones you've mentioned are pretty small and replacable. &gt; Your "Btw, &lt;controversial claim stated as if it were fact&gt;" style is really not constructive, please stop doing it. First you. Not realizing how much useful stuff requires FFI doesn't make you look like a pro. I can't think about any case where something important doesn't require low-level access. Yeah, you've approximately safe sockets in java. But that's it. Once you want to *use* the machine you'll need to *access* it. All the domains I've mentioned in my previous post have very useful/specific libraries in C/C++. 
thank /u/m50d
&gt; Nope. Right back at you: it seems like you're trying to pretend performance is a lot more important than it is No, it's just important. Where do you wrote this comment? From internet explorer? Maybe from a java browser? Just look around your desktop and tell me: what would happen if everything would run on the JVM. &gt; because it's the only thing your tools are good at Hell no. "My" tools have: far faster compilation(nim), more backends(nim), nicer syntax(at least with nim), better FFI, more safety - like safer concurrency(linear typing - rust, disjoint check - nim) and safer resource management(rust and nim with templates), easier macros and better abstraction models(concepts). &gt; and just tossing insults ("toy code") at anyone not working in the same area as you. No, it was you who tried to make performance and FFI look bad and stupid because "your" tools suck at them. &gt; It's funny because it shows how little that kind of "good implementation" matters. Performance just isn't important, almost all of the time. Yeah, it's funny because the number 1 complaint about mc was the performance and ms is rewriting it now. Mc was only a thing in java because it's a minimalistic game when it comes to graphics - and it still sucks. Also, don't try to act like performance doesn't matter, it won't work. &gt; Those don't need to be C/C++ either. LoL. &gt; Hell I've got some running on my system right now that're written in Python (HP's printer drivers). If the backend is already written in C/C++, maybe. &gt; Indeed it is. What's your point? Is dart competitive with C/C++ now? It's just a matter of implementation. &gt; Yeah, I just don't think they're measuring something relevant. Hand-tuned C++ with infinite development time on compute-bound problems tends to be a small factor faster than Java (1x to 2x in the general case, occasionally 5x on small specific benchmarks). "Small factor"? Dude, just think about container data types... &gt; Every year computers get faster Is it 2005 again? &gt; and the impact of security vulnerabilities gets worse. Then choose Nim and Rust because java won't be useful where these issues can happen. &gt; C/C++ and anything that relies on them for libraries are a lot more outdated than Java. Haha. You do realize that the JVM as an ecosystem is dying, right? Many people started to hate because of oracle. &gt; Citation needed. Look at the domains I've mentioned so far - at most of them java is niche or useless. Java is present in two areas: webprogramming(niche because php and js rule it, unfortunately) and mobile dev(android only). On android you need the vm and people started to prefer kotlin. Did you know that resource-sensitive games(which are not something like candy crush) and utilities are usually written in C/C++? &gt; Java is the most popular language in the world, Citation needed(not TIOBE). I think C would be the most popular. Though, it'd be better top see an "importance" graph. &gt; and was only created in 1995 by which point there were lots of other options, In 1995 there were nowhere near as much languages as now. Delphi was expensive. script languages just started to appear. FP languages were only in the academies. &gt; so it can't be so bad for so many of the cases. But it's that's why it's niche or useless at most domains.
Dont use anorm. Very bad experiences with it. Used it at one gig. Plenty of bugs, shitty design and terrible generated sql. Also hard to extend to fix problems. Also not adhering to semver make it pain to upgrade.
I hope they desugar the pattern match into abstract method + implementation in subclasses, which is what the JVM expects.
&gt; Where do you wrote this comment? From internet explorer? Maybe from a java browser? Just look around your desktop and tell me: what would happen if everything would run on the JVM. Pretty sure it'd be fine. &gt; more backends(nim) Number of backends isn't an advantage, if anything it's a disadvantage. &gt; nicer syntax(at least with nim) Disagree, looking at the examples. &gt; better FFI Better how? &gt; safer resource management(rust and nim with templates) What do templates offer that something like Scala-arm doesn't? &gt; and better abstraction models(concepts) If you think structural typing is better than nominal typing at the concept level why aren't you using a language with structural types at the normal value level? If concepts make abstraction as easy as HKT does then why is there no Nim implementation of something like `cataM`? (You've shown `Functor` but that's about as simple as HKT gets and it's already relying on a macro and this language-level `typed` construct, so I'm not convinced it will work for more general HKT use cases - e.g. will `typed` work properly for a parameterized type parameter, which is needed to be able build a `MonadTransformer` typeclass?) &gt; it's funny because the number 1 complaint about mc was the performance And yet people kept buying it. &gt; Is dart competitive with C/C++ now? On web framework performance? Yes, evidently. Turns out performance isn't the most important thing though. &gt; Then choose Nim and Rust because java won't be useful where these issues can happen. As long as you're having to FFI into C/C++ to get anything done your code can't be any more secure than C/C++. Linear types in the language are great but they won't help when you hit a double `free()` (= undefined behaviour = security vulnerability) or other memory safety vulnerability in a library you're using. Whereas on the JVM it's practical to avoid doing FFI and so avoid having memory safety vulnerabilities. &gt; Citation needed(not TIOBE). I think you proved my point - you already know TIOBE shows you're wrong.
Wait, what? No Java interop? They don't compile to Java enums? That seriously sucks for interop and performance.
Serious question: does anyone actually use Lagom?
Think of the `@enum` I implemented as enumeratum, just without the unnecessary boilerplate and some no-brainers like Java compatibility thrown in. Add `@enum` to existing code. Done. No `extends Enum[T]`, no `extends EnumEntry`, no `val values = findValues`. The enum thing implemented in Dotty is comparable to the preliminary designs I did in 2013, which I all discarded, because I felt they weren't good enough in terms of language footprint and complexity. I have trouble understanding how someone could come up with something worse 5 years later.
Yes, all enum items are objects. `values` doesn't make sense if one lets users create their own values. Dotty's `enumValues` is the worst of both worlds in that sense.
`Option` should be completely rewritten to make use of the upcoming value types, so I think it's a poor example for `enum` anyway. It can't be both an `enum` and a value type, and I can't see the benefit `enum` adds for `Option`.
¯\\\_(ツ)_/¯ When I started working on this, Java interop was pretty much the first thing I wrote into the spec. That was in 2013.
Aren't we reading too much in the examples of the post? There is no standard library for Dotty now, there will be plenty of time to decide how Option evolves in the future.
If you don't need to do anything special a macro can do all of the heavy lifting for you import play.api.libs.json._ implicit val residentReads = Json.reads[PersonalInfo] https://www.playframework.com/documentation/2.6.x/ScalaJsonAutomated 
Probably a question for lightbend. I personally don’t see why not, it abstracts the http and event sourcing boilerplate into case class/object representations. It uses play and akka under the hood, both popular in their own roles. And it doesn’t limit you in any way in the code you write to handle your events. So I would say, if you’re planning on having command/query separation, Lagom seems like a good choice. In a broader sense, it seems like event sourcing or cqrs in general is not that widely used, yet. IMO that’s because a lot of devs still don’t feel too comfortable with those, maybe because the general ideia is that there is a lot more to it than it really is. And that’s actually where Lagom might be able to help.
FWIW, as a SIP committee member, I would vote "No" to a proposal for `enum`s that do not interop with Java.
[The current spec](http://dotty.epfl.ch/docs/reference/enums/enums.html) looks pretty much exactly like how Java enums work, right down to how it desugars to a sealed class and (if needed) subclasses. Compiling this to an actual Java enum should be embarrassingly simple. What the hell are they thinking?!
Interesting to hear! Sadly, Dotty is not Scala, so the feature will probably never see a SIP committee. It also should not even require a SIP, because it's possible to implement Java-compatible enums without any language changes or syntax additions. (That's why I dropped work on the SIP draft in 2016.)
Great question.
I do need to something special though. I'm going to give /u/mmmdaaa 's solution a try when I get a chance
I think by interop with Java, Simon means "Is a Java enum", not "Is easily usable from Java". Is that what you mean too?
This sounds pretty cool in that it allows for people coming from those other languages that rely on those constructs to use scala like they did in their source lang. That said, I personally write scala to _not_ every have to write DSLs like this, so it's a two sided coin the way I see it.
what version of java are you using? 
What version of java are you using? What operating system?
Yes, that is what I mean too.
&gt; It also should not even require a SIP, because it's possible to implement Java-compatible enums without any language changes or syntax additions. The fact that it's possible to hijack the existing compiler internals in a macro, and that it happens to be able to convince the back-end to generate Java enums, does not mean that it shouldn't require a SIP. The compiler internals could change in ways that still comply with the documentation of `scala-reflect`, and break the macro. A SIP, or at least some guaranteed support from the macros API, is necessary to have a *stable* way to do this. Moreover, IMO, anything that one can write with a macro should *also* be writeable without any macro. I.e., it should always be possible to rewrite a codebase without macro in a way that produces the same bytecode in the end. &gt; Interesting to hear! Sadly, Dotty is not Scala, so the feature will probably never see a SIP committee. It will not see a SIP committee before entering Dotty (in fact, it already *has* entered Dotty). However, by the time Dotty is supposed to become "Scala 3", IMO all its new features should go through the committee. Those that are rejected should be removed or deactivated from the compiler before Dotty can claim its "Scala 3" name. Disclaimer: this is not written anywhere, it is my opinion, but I intend to turn that opinion into fact, when the time comes.
java 8, Linux
is it oracle or openjdk? and what exact version number? I was having this exact problem with `jre-1.8.0-openjdk-1.8.0.144-0.b01.el7_4.x86_64` from redhat
&gt; What about @SerialVersionUID? What about it? It's an annotation known by the compiler and the language specification, not a macro. `@enum`, as implemented last I saw, was a macro that did something magical that could not be reproduced without macro. I am not saying what it did was wrong; I am saying what it did should be part of the language specification. &gt; Come on, we all know how this will work out: Well, maybe. If that happens, I'm definitely resigning from the committee, because it means that it has no meaning whatsoever. I am willing to stay confident that it won't happen that way, though.
The original issue https://github.com/lampepfl/dotty/issues/1970 contained many thoughtful comments and analyses of how enums could produce Java enums.
&gt; @enum, as implemented last I saw, was a macro that did something magical that could not be reproduced without macro. Ah, now I understand. The enum feature has existed in various variations over time as I cut down on footprint, this might have caused some confusion. I had type macros, macro annotations, and then just a compiler implementation like `@SerialVersionUID` in the end. &gt; I am saying what it did should be part of the language specification. I can agree on that, but I'd have rather preferred a discussion on scala-internals instead to achieve this. &gt; If that happens, I'm definitely resigning from the committee, because it means that it has no meaning whatsoever. You will certainly earn a lot of respect for that. &gt; I am willing to stay confident that it won't happen that way, though. Wouldn't be the first time where processes have been subverted by some to achieve their desired outcome.
Here is the subthread on r/programming about it: https://www.reddit.com/r/programming/comments/82wpiw/the_redmonk_programming_language_rankings_january/dvdiguw/
how do i find which one it is?
I understand why people complain about Scala but I just can't find a better language at the moment. One that I am actually excited about writing... When I write Scala I enjoy it. I haven't found any other language (especially one that usable in a production environment) that is the same.
Being at position 14 sounds pretty incredible to me. The competition consists of top mainstream languages which have been historically strong or newer languages pushed by very large corporations with infinite amounts of money (Swift, TypeScript, Go).
&gt; However, by the time Dotty is supposed to become "Scala 3", IMO all its new features should go through the committee. Those that are rejected should be removed or deactivated from the compiler before Dotty can claim its "Scala 3" name. Fully agree with this. &gt; Disclaimer: this is not written anywhere, it is my opinion, but I intend to turn that opinion into fact, when the time comes. I intend to do the same.
Agreed. Makes me wonder what language simon_o uses professionally (when he isn't trolling r/scala)
What really important in Dsl.scala is collaboration. Each user-defined keyword should be compatible with each others and normal Scala control flow. Collaborative keywords make the usage become non-intrusive and lightweight.
i'd take a type system over a dynamic language any day
Try haskell
Serious non-troll question, have you looked at http://mypy-lang.org/ ? It gave me the typing where I wanted it at the method boundaries. And man oh man, writing models and forms compared to the absolute crap I had to write in Scala Play + Slick? I think I save 30% lines of code with Django over Scala Play for CRUD views.
Java master race.
Having worked with Java programmers that could barely handle Lambdas or Streams, I probably wouldn't want a lot of those people writing Scala code. When these same people write Scala, I often find they act as if the standards they learned from Java no longer apply. Scala code is so powerful that making extremely shitty code is ridiculously easy, and fools people into thinking they don't need previous standards. "Single purpose principle doesn't matter anymore, because we can throw everything into a bunch of mixins!" Anyway, 14th seems like a comfortable position to me.
I think mypy is amazing for the evolution of python - I just can't wait for the type hints from pep 484 to have a static precompilation aspect. From a performance standpoint, it could have incredible impact. Sort of like what Hack did for PHP.
I think the main thing here is not that it's at position 14... The main thing here is that, for the third quarter in a row, Scala continues to drop... Which begs the question...Why?
Thanks for the link. To sum up: most people think that Kotlin drags away (potential) Scala developers what I consider to be realistic. I know some Scala developers which switched to Kotlin, because Kotlin works better in practise. And practise is not only the language but all the things around it like IDEs, build tools, libraries, Java compatiblity etc. Just look at the SBT monster and its documentation to get an idea how tooling in Scala looks like.This [post](https://www.reddit.com/r/programming/comments/82wpiw/the_redmonk_programming_language_rankings_january/dvdzvqt/?st=jejmfbie&amp;sh=2dfe838d) is a pretty good description of the problem.
Maybe a lot of programmers came to Scala for Spark, and now that bubble is deflating?
IMO because scala is a functional language, and people will not pick it up unless they are interested in doing functional programming. At the same time you have new languages like Kotlin, Go, TypeScript and Swift that share the same paradigms as the most populaire languages and come with new quality of life improvement.
&gt; Pretty sure it'd be fine. Yaay. &gt; Number of backends isn't an advantage, if anything it's a disadvantage. Jeez, are you trolling? So, scala became worse with the js and the llvm backend? You said you even use the js backend so what? It seems like you just want to disagree. &gt; Disagree, looking at the examples. You can look at traits vs concepts. Or macros+templates vs scala macros. &gt; Better how? Less pain, finer control. &gt; What do templates offer that something like Scala-arm doesn't? No overhead. No boilerplate. &gt; If you think structural typing is better than nominal typing at the concept level why aren't you using a language with structural types at the normal value level? I don't think one is better than the other. Concepts are just better. &gt; If concepts make abstraction as easy as HKT does then why is there no Nim implementation of something like cataM? Because nim is a systems programming language first and its users don't want to give up efficient resource management to spare a few lines and to obfuscate their code. &gt; You've shown Functor but that's about as simple as HKT gets and it's already relying on a macro and this language-level typed construct, so I'm not convinced it will work for more general HKT use cases - e.g. will typed work properly for a parameterized type parameter, which is needed to be able build a MonadTransformer typeclass? You said a language only needs a few but powerful features - like nim. With macros, nim's abstractions are easier to use and more powerful than scala's. Concepts more tricks with less advanced features. &gt; And yet people kept buying it. And this proves what? Nothing, because players already know that 1. mc is bearable because it's a simple voxel game 2. it'd be better in native - like how its native implementations perform better and almost have feature-parity. &gt; On web framework performance? Yes, evidently. LoL no. Maybe with better algorithms. &gt; Turns out performance isn't the most important thing though. For you? Maybe. So far, nothing is important to you if it's not scala's HKT with explicitly implemented typeclass-simulations. &gt; As long as you're having to FFI into C/C++ to get anything done your code can't be any more secure than C/C++. Hmm, is that why most crypto frameworks are written in C/C++? Hmm... &gt; Linear types in the language are great but they won't help when you hit a double free() 1. There's no double-free with linear typing, unless you call unsafe(you shouldn't). 2. no valgrind? &gt; (= undefined behaviour = security vulnerability) or other memory safety vulnerability in a library you're using. Whereas on the JVM it's practical to avoid doing FFI and so avoid having memory safety vulnerabilities. and avoid having decent performance and have the cryptographic issues of managed memory... &gt; I think you proved my point - you already know TIOBE shows you're wrong. TIOBE shows what? That a bunch of dead languages are more popular than scala? Or that Java is somehow more popular than C and Javascript despite the last two occupying the largest sections in the industry?
Is normal programming languages you write groups of statements to execute in chains of functions... transforming the data. In Scala you put data in monads that describe transformations and use a functor (called map) to sequence those transformations. Is that an lose description of how Scala works or their devs think? Is there a relationship between map and a Functor or is a functor more general?
I thought I was the only one thinking that way. I've written code in just a few languages: Java, R, C/C++, Octave, PHP &amp; Scala, and, from my point of view, Scala is the best tool I have to write close-to-bug-free code, enforcing the use of inmutability, recursion, Option and Try types. I'm very much productive right now and I'll definitively stick to Scala for all my own projects.
&gt; Well I said I wanted/needed typeclass-like functionality and you suggested concepts, do they let me implement the patterns I care about (e.g. type-safe derivation of serialization, safe AOP-like effect management) or not? Concepts can do more than typeclasses. They define "behaviour" better than any typeclass implementation in any language. In scala, you need to fall back to implicit hacks to even define a minimal typeclass. &gt; Both the signature and the name are as they should be. So, is it flatMap or not? &gt; You're telling me to rename Validation#flatMap because it has the same name as Monad#flatMap even though neither of them imports the other one, that's the same problem as having a global namespace. By that definition, everything has a global namespace. And if Validation#flatMap is not flatMap then 1. don't name it like that 2. define Monad *properly*. 3. define Validation#flatMap properly. &gt; Maybe I don't want to go into that much detail, or maybe the language doesn't let me express the distinction (it's pretty subtle). No, it's not subtle. It's probably a problem in haskell/scala but not inim. &gt; This is a common and important use case for types, and why structural type systems aren't good enough: And by "aren't good enough" you meant "I just don't want to acknowledge it because I've listened to haskell-nuts too much". &gt; often one wants to use types to distinguish between two things that are "physically" exactly the same (e.g. DraftOrder and SubmittedOrder might contain exactly the same fields with exactly the same types, but it's important not to mix them up, so we represent them as different types). It shouldn't be any different at the "concept" level. It **should** be. If you can't define your abstractions properly then why stick to them so much? &gt; How, when you just told me that the name on the thing has to match the name on the concept? If your class doesn't have the method from its typeclass then you're doing it wrong for no sane reason. You're just making it more ad-hoc than it supposed to be. &gt; I can and I do, all the time. Look at something like https://github.com/milessabin/spray-json-shapeless . Oh, I was thinking about *deserialization*, sorry. Btw, that library is 1. unmaintained 2. looks worse than any ~~nim~~ macro I've seen so far. Compare that to nim's [marshal](https://github.com/nim-lang/Nim/blob/master/lib/pure/marshal.nim) module. Also, just as I thought: you can't do serialization without reflection or without allocating field info at compile time - just as it's done in nim. &gt; Perfect? No. Many orders of magnitude better? Yes. If they're so better, then why people don't use it for security software? &gt; This mythical "modern C++" is only ever used as a No True Scotsman by C++ advocates - any time a C++ codebase has security bugs it's "well, it must not have been modern C++". There are no rules you can follow to actually write it, and no two C++ advocates agree on what it is (the C++ Core Guidelines might eventually become a true standard people could follow, but they're explicitly in early development at the moment). "modern C++" is not using it like C: use smart pointers, avoid manual allocation and use boost/std. Myth busted. &gt; But we're comparing to C++, since that's what the libraries you FFI into are written in at the moment. Then you've this problem: on the jvm you need ffi. Btw, Nim can catch C++ exceptions, can java do that? &gt; And compared to that, Java is orders of magnitude safer. Maybe at memory safety. But when it comes to real-time apps, fine controlled memory management and cryptography java is a no-go. &gt; Java is memory-safe, C++ isn't, and memory-safety bugs are the overwhelming majority of security bugs. LoL this bullshit. 99% of them can be catched by simply using valgrind. &gt; Not true. I have direct experience that HKT and typeclasses have helped a lot. I also have direct experience with it and when I see people advertising that technique it's just nonsense. Also, HKT and TCs still won't help you to achieve high-levela and zero-cost safety. &gt; General-purpose programming, based on my experiences. And by "General-purpose" you meant "web programming and simple data processing", right? Because at most of the domains I've mentioned the JVM has two problems: 1. languages with managed memory is a bad idea. 2. performance matters 3. memory-efficiency matters. 4. no support, only through FFI. &gt; Of course opinions will differ on which languages are best, I'm just trying to be clear that I'm not wedded to Scala, I acknowledge its faults, and I'll switch languages if and when I see an alternative that I think is better. You probably won't switch because you ignore almost all of the important aspects of programming domains. So far, you only seem to care about HKT and typeclasses - which are useless, especially if you care about safety. Also, you hate most features of scala so, you don't really have any reason to switch or stick with it. &gt; If it was safe you wouldn't need macros for it. Macros are safe when they're hygienic and they'll fail if the thing you're creating with them is not compatible with the language. Welcome to the '70s. &gt; Macros allow arbitrary transforms of the AST, so a priori code that looks like anything could become code that does anything else. So, you think it's magic. You know, transforming the AST is what compiler do. &gt; You lose the ability to reason about your code the normal way, because you can't rely on code doing what it looks like. What kind of nonsense is this? &gt; If you're advocating tracked ownership being the default then you're still advocating tracking ownership for most values in your programs Yep because it makes sense if you prefer safety over a little discipline. So far, you advocated this too. &gt; (the overwhelming majority of which will not be associated with non-memory resources). Sure that's technically not quite "every single value" but my point stands. Stands where? &gt; My point is that if you're maintaining code that uses a macro, it's no different from if that macro were a language feature. Exactly. This is my point too - macros are just features imported from external sources. &gt; Typeclasses are just plain old values resolved via the same implicit mechanism that's also used for other things. Typeclasses(in scala) also involve method injection and the possibility to overwrite methods through the implicit TC evidence. &gt; Type lambdas are just a combination of structural types and type projections. And they're very ugly and barely useful. &gt; The cake pattern is just a combination of trait composition and type members. And is an anti-pattern. &gt; Shapeless' type-level lists just use the same "things ending in : bind to the right" rule as you do at value level. Extracting a type parameter in a pattern match works just like extracting a value parameter. You're aware that most of these are not scala's features and that scala has 3x as much features, right? &gt; Macros are too flexible/dangerous, They aren't. People say this about implicits too. &gt; and I have yet to see a case where they offered a substantial advantage over what I can do in Scala without macros. You can compare nim's marshal and json modules with scala's spray-json and see the advantages for yourself. Or look at scalaz and see what simulacrum could improve. Or compare slick with quill. Or think about the boilerplate it can remove. Or the advantages of compile-time computations/caching. &gt; It's not so rosy. I did a small amount of FFI into a couple of mainstream, well-regarded, widely-used libraries and I still hit more serious issues in those than in the whole rest of my codebase. Funny, we use such libraries all the time and we've poorly written C modules at work but I've yet to see those "huge" problems in practice. The language is dumb and unsafe but that's it. Macros(even C's dumb macros) and valgrind help a lot. &gt; My whole point is that on the JVM it's practical to minimize native FFI, to the point where you can reasonably e.g. isolate it into separate processes. Then you've a naive point because that's not a solution. &gt; Any kind of cross-cutting concern, it gives you a way to manage safely. Without HKT you have to use things like AOP which introduce subtle and difficult bugs like http://thecodelesscode.com/case/211 . (Or you could do it by hand, but the fact that people use AOP even given how error-prone it is shows how difficult managing cross-cutting concerns by hand is). HKT and TCs have diminishing returns. Compared to today's advanced features the typeclass pattern doesn't really offer much. You can't really show me anything useful what I can't do without it because at the end it's just a way to create limited ad-hoc abstractions.
I'm familiar with the Scala team but I don't know who the 'virtually present' people are. Are they all from companies using Scala? (I see one of them is representing Twitter)
&gt; Concepts can do more than typeclasses. They define "behaviour" better than any typeclass implementation in any language. What do you mean, concretely? &gt; So, is it flatMap or not? It is a `flatMap`. But it's not `Monad#flatMap`. &gt; if Validation#flatMap is not flatMap then 1. don't name it like that 2. define Monad properly. 3. define Validation#flatMap properly. Everything is defined properly. `Monad`, like most types, means something more than just its method signatures. (Can you even define `VerifiedMonad` in nim?) &gt; It should be. Why should it? It's just values implementing interfaces, you're just getting confused because your language needs this separate "concepts" feature. &gt; If your class doesn't have the method from its typeclass then you're doing it wrong for no sane reason. It's entirely normal to have a domain-specific name for an operation that can also be viewed as something more general. &gt; you can't do serialization without reflection or without allocating field info at compile time - just as it's done in nim. The difference is that typeclass derivation is restricted so that it will always be safe and understandable, whereas macros could do anything. &gt; If they're so better, then why people don't use it for security software? Because approximately no-one's putting serious development effort into security software, because there's no money in it. A sad state of affairs, but that's life. &gt; "modern C++" is not using it like C: use smart pointers, avoid manual allocation and use boost/std. Yeah, except then we see a major security vulnerability in a codebase that was doing all that (like that chrome sandbox issue) and it's suddenly "oh yeah and modern C++ also means don't do bitshifts" or whatever. &gt; Then you've this problem: on the jvm you need ffi. No you don't. &gt; LoL this bullshit. 99% of them can be catched by simply using valgrind. And yet somehow we keep seeing all these memory safety vulnerabilities even in big popular C/C++ libraries. &gt; Typeclasses(in scala) also involve method injection and the possibility to overwrite methods through the implicit TC evidence. Wtf are you talking about?
The notes should come up "soon" at https://docs.scala-lang.org/sips/minutes-list.html and from there you can check the attendees list for names/companies. For example from Decembers meeting &gt; Martin Odersky (@odersky), EPFL &gt; Jorge Vicente Cantero (@jvican), Scala Center &gt; Seth Tisue (@SethTisue), Lightbend &gt; Sébastien Doeraene (@sjrd), EPFL &gt; Eugene Burmako (@xeno-by), Twitter &gt; Iulian Dragos (@dragos), Triplequote &gt; Adriaan Moors (@adriaanm), Lightbend &gt; Miles Sabin (@milessabin], Independent &gt; Darja Jovanovic (@darjutak), Scala Center
&gt; why leave the familiar if you're just going to do the same thing? I come from an OOP background and still am very much an OOP programmer to be honest. One of the main reasons I prefer Scala over Java (which I used previously) is simply *less code*. But aside from that, I think you misunderstood the point what I was making: you said "Scala is a functional language". Which is true. But Scala is also Object Oriented, which makes it more appealing to us OOP programmers, because the switch is easier. I would never switch to a fully functional language such as Haskell unless I absolutely had to, but using Scala I can start with what I know (OOP) and slowly incorporate more FP elements in my coding, which is what I love about Scala. :)
&gt; It would be better if it could achieve the same things without needing multiple backends. Not goint to work. &gt; The tradeoff is probably worth it for the sake of being able to run in the browser, but multiple backends count as a negative other things being equal. First time I hear this sentence from any programmer ever. It's kind of bullshit but on a more obvious level. &gt; A little overhead (not that it's noticeable) is worth A lot of little overhead = a lot of overhead. That's why the JVM's GC is struggling - too much shit is managed. &gt; it for the safety of implementing something with normal language constructs rather than templates. Templates are safe. Radical typehacks are safe too but look terrible. &gt; There's no boilerplate with something like scala-arm, just a minimal marker so you can see when a call involves resource management (which is a distinction you want to be visible when reading code). scala-arm can't compete with templates - for example I can even manage memory manually and not expose it. Also, templates can abstract over boilerplate and introduce no runtime overhead. It's also a simple and stable feature which is good for many things. &gt; Better how? The only actual difference you've shown is that whether a type implements a concept is structural rather than nominal. As I've said a few times, concepts can describe behaviour and type relations more precisely. You can put additional constraints and I'm sure it's possible to define Monad so, that Validation won't be one. &gt; Languages need powerful but safe features - macros are too far in the "everything permitted" direction. Macros are powerful and safe and no, they don't allow everything. They only allow things which make sense. If you need a less powerful but simpler metaprogramming construct then use templates. &gt; From what you've shown so far concepts sound like a fiddly, restricted thing. It's quite the opposite: scala's typeclasses can only do a few tricks in an ugly way. &gt; And Scala doesn't need a dedicated language feature at all to achieve typeclasses, No, it needs multiple features to implement typeclasses the worst-looking way ever. It's very hard to like scala's TC boilerplate. &gt; So how come the game that became so popular, sold so much, and was enjoyed by so many players, wasn't a native implementation? Because Noch was a java programmer - he'd a good idea but no experience. That's why the first versions of minecraft were trash. Also, mc is still just a basic voxel game. It's not a "huge" creation at all. &gt; Minecraft prioritised other things over performance, and was successful because of that decision. No, it was because mc was a good idea. And you still can't deny that performance and memory-inefficiency were the worst issues. You're trying to get away from these facts but talking about "ideas" and "success" is not relevant here. We're talking about "fitness". And java isn't fit for gaming. Mc showed that. I'm on linux and cross-platform games matter to me. In my steam library most of my games(127 from 135) are linux-compatible. There's no java-based game in my library - most of them are native but there are 14 which are on mono(non of them are resource-intensive and all of them are indie games). &gt; Read the rest of the sentence. Have you read mine? &gt; Valgrind or not, these bugs keep happening in native libraries. You act like they're so common but IRL they're extremely rare because of valgrind. I'm working for this company for 2 years and yet I've seen no jira tickets where double-free was the problem. Also, there GCs for C/C++ too. &gt; So clearly you're wrong about what the "largest sections in the industry" are and/or about what kind of languages can be used in those sections. So clearly you just don't want to face the truths that most of our things are written in C/C++ and they still need to be maintained and that the whole web runs on them and on php and js. Plus our entire desktops, most of our tools etc etc. I can be java-free but can you be C/C++-free?
&gt; What do you mean, concretely? Visit the documentation[here](https://nim-lang.org/docs/manual.html#generics-concepts) and [here](https://nim-lang.org/docs/typetraits.html). &gt; It is a flatMap. But it's not Monad#flatMap. Then it's not flatMap. Or your Monad is wrong. &gt; Everything is defined properly. Monad, like most types, means something more than just its method signatures. (Can you even define VerifiedMonad in nim?) Just because you can't write down the monad laws in scala/haskell it doesn't mean your definition is right. It just means you're building on ad-hoc and poorly defined abstractions. &gt; Why should it? It's just values implementing interfaces, Because you need a language where you can tell it how to work. &gt; you're just getting confused because your language needs this separate "concepts" feature. you're just getting confused because your language needs this separate "typeclasses" feature. &gt; The difference is that typeclass derivation is restricted so that it will always be safe and understandable, whereas macros could do anything. This is an argument from someone who clearly doesn't understand what happens behind that library and doesn't understand macros at all. As I've said, start learning them and form an opinion later. &gt; Because approximately no-one's putting serious development effort into security software, because there's no money in it. A sad state of affairs, but that's life. What a load of bullshit! There are many people who develop security software in C/C++: firewalls, antivirus programs, crypto libraries, sandboxing software, file-encrypting sofware, encrypted cloud and messaging software etc etc - they *do* make a lot of money. &gt; Yeah, except then we see a major security vulnerability in a codebase that was doing all that (like that chrome sandbox issue) and it's suddenly "oh yeah and modern C++ also means don't do bitshifts" or whatever. You can do bitshifts in java too. But you'll surely never be able to compete with a JVM-based browser against something like chrome or firefox. Chrome defeated FF because it was faster(and had more agressive marketing due to google) despite FF having more features. &gt; No you don't. Then stay in the pink zone where everything is just unimportant string processing. &gt; And yet somehow we keep seeing all these memory safety vulnerabilities even in big popular C/C++ libraries. And yet somehow I don't see the statistics and usable comparisons, only whining about C/C++ libararies and not being aware of valgrind. &gt; Wtf are you talking about? Do you know how typeclasses work? Have you defined and implemented one yet? &gt; I'll take the pile of insults in the rest of your post as the gradual realisation that you're in the wrong here. You can take it as a sign that I'm aware of these: all you do here is try to bagatellize the importance of performance and FFI, do some bike-shedding with the typeclasses and try to magnify the benefits of typeclasses without evidence. From your part, you're just trying to get away with not acknowledging anything from outside. I thought you'll try to be open - as you said - but instead, you try to change reality because you're too opinionated. You talk about how you just want a small set of powerful features and yet you use scala(and ignore many of its benefits with poor reasoning). You think that the JVM has everything but I don't see that and I'm sure you can't prove it because it's obviously bullshit. You talk about safety and yet I don't see you running to *actually* safe features and languages which *actually* care about it. You want a general-purpose programming language and yet you use a high-level one which isn't good or usable for many cases like systems programming, real-time apps etc - and suck at many domains I've mentioned. At the end, you're lying only to *yourself*. You do no harm to me - you hide the truth from yourself because it seems like you don't care about change and you don't care about what's objectively better. In your heart, you're still a java programmer who has barely any reason to use scala. As I see it: you're a java programmer who really bought into category theory and don't spend much time looking outside. You say that the JVM is great for everything and that you can solve many issues with HKT and TCs - as a former java and scala programmer I don't buy it. My proof? Check out the industry.
&gt; Templates are safe. They checked once fully expanded, but developing the template itself is like working with untyped code. &gt; As I've said a few times, concepts can describe behaviour and type relations more precisely. What does that mean, concretely? How are they more precise, what can you actually do with them? I've pointed to a specific example of the kind of functionality I'm dubious about them being able to implement (`cataM`). &gt; I'm sure it's possible to define Monad so, that Validation won't be one. You'd have to implement something like Idris' `VerifiedMonad`, which not many languages can do in a usable way. But even in Idris, people find it useful to be able to make a nominal distinction rather than a proven one - lots of code is written with `Monad` rather than `VerifiedMonad`. &gt; It's quite the opposite: scala's typeclasses can only do a few tricks in an ugly way. Typeclasses can't do anything that ordinary code can't, but can do anything that ordinary code can, because they're just ordinary code - which is exactly what you want. Typeclass inference is a small language-level feature that indeed only allows "a few tricks", which again is as it should be - "magic" features should be as restricted as possible while meeting the needs of their use cases. &gt; No, it needs multiple features to implement typeclasses the worst-looking way ever. It doesn't have any dedicated typeclass features - all the features that are used to implement typeclasses existed in the language already before people figured out they could implement typeclasses with them, they weren't designed for it. &gt; Have you read mine? Yeah, you talked about how linear types (which Nim doesn't even have in the first place, but that aside) prevent double `free()` when I was talking about double `free()` from a native library called with FFI. Which in Nim is something you have to do even for something as basic as e.g. connecting to Postgresql. &gt; No, it was because mc was a good idea. And you still can't deny that performance and memory-inefficiency were the worst issues. That's exactly the point - performance and memory-inefficiency were its biggest problems, but they didn't stop the game being a huge hit, because they're ultimately not that important in the grand scheme of things. &gt; You're trying to get away from these facts but talking about "ideas" and "success" is not relevant here. We're talking about "fitness". And java isn't fit for gaming. Mc showed that. If someone can build a hugely successful game in it, it's fit for building games in, by definition. &gt; You act like they're so common but IRL they're extremely rare because of valgrind. I'm working for this company for 2 years and yet I've seen no jira tickets where double-free was the problem. I spent about 2 years using two native libraries over FFI and hit multiple memory safety vulnerabilities in both. One was ffmpeg which is supposed to be a poster child for using valgrind and fuzzers and all that stuff, but we still hit enough segfaults that it was worth separating out into a separate process. (Which meant we got to deal with the bugs in C++ libthrift as well, but that was better than having it bring down the main system). &gt; So clearly you just don't want to face the truths that most of our things are written in C/C++ and they still need to be maintained and that the whole web runs on them and on php and js. Plus our entire desktops, most of our tools etc etc. I can be java-free but can you be C/C++-free? I can be C/C++-free enough - the product I'm working on doesn't use any native libraries, so bugs in C/C++ code (other than the JVM itself) are not my problem. Which has made my programming life a lot easier than when I was working on the system I mentioned that was using FFI.
You must have missed the parts where we actually discuss viable ways to implement this. The most likely way was to generate one .class file per source file containing top-level declarations, and the very name of the generated class file would indicate that it contains such top-level declarations. For example, if there is a file `Foo.scala` declaring a top-level `def bar(x: Int): Int`, we would generate a file `toplevel$Foo.class`. We then only have to look for the .class files named `toplevel$XYZ.class` when populating the name table of a package. It is potentially a bit more work than reading *only* `package$.class`, but not fundamentally insane. Btw, the absence of implicits in Kotlin does not make their job any easier. Extension methods alone are enough to make this part of job as hard in Kotlin as in Scala. But again, none of that is fundamentally hard nor costly.
&gt; Visit the documentationhere and here. I did, I don't see anything that typeclasses can't do. The polymorphic notion of "statically evaluable" is superficially nicer than having distinct notation for compile- and runtime logic but terrifying in practice, because it sounds like concepts will just silently match if an expression happens to not be statically evaluable, even if that expression is false. &gt; Then it's not flatMap. Or your Monad is wrong. Nope. It is `flatMap`, it maps and it flattens. It's not monadic `flatMap` because it doesn't conform to the monad laws. &gt; you're just getting confused because your language needs this separate "typeclasses" feature. No it doesn't, that's my whole point! "Typeclass" is just a name for a pattern implemented in plain old ordinary code, and good practices for implementing typeclasses are just the usual good practices for writing ordinary code. &gt; firewalls, antivirus programs, crypto libraries, sandboxing software, file-encrypting sofware, encrypted cloud and messaging software etc etc But their ability to make money isn't connected to actual security. E.g. BlackICE famously introduced more security vulnerabilities (because it had memory safety vulnerabilities, because it was written in C/C++) than it was supposed to be protecting you from. &gt; You can do bitshifts in java too. The difference is in Java they will never be undefined behaviour. &gt; you'll surely never be able to compete with a JVM-based browser against something like chrome or firefox. Chrome defeated FF because it was faster(and had more agressive marketing due to google) despite FF having more features. If I thought there was money in making a better browser I'd give it a try, but I don't think there is. (Chrome makes money for Google by helping people use Google services, not directly) &gt; Do you know how typeclasses work? Have you defined and implemented one yet? I've implemented dozens or hundreds, thank you very much. Nothing is injected or overwritten. &gt; all you do here is try to bagatellize the importance of performance and FFI JVM code has worse performance than carefully-optimized native code, and in the rare cases where you need to use FFI you give up many of the advantages of the JVM. This I knew, your long list was just you claiming that a lot of domains need these things (and I know directly that many of the examples you claim are false, because I've used the JVM in those areas and not had those problems - it may or may not be easier to write embedded code in Nim or whatever, but it's certainly possible to use the JVM for it). I was hoping you'd tell me about some new disadvantages of the JVM and I'd learn something, instead you've just made a bunch of assertions that all come down to performance or FFI. And frankly since you say some outright false things very confidently ("we all know that java based webframeworks are not competitive when it comes to performance, scalability or safety") I can't trust the rest of your claims either. &gt; do some bike-shedding with the typeclasses and try to magnify the benefits of typeclasses without evidence. I've given you some specific examples of the kind of functionality they offer (`cataM`, serialization with compile-time type safety, safe AOP-like functionality for e.g. database-session-in-view) and how that's been useful in my experience. Far more specific and detailed than anything you've show about the advantages you're claiming. [skipping more personal attacks - they only show you know you're in the wrong] &gt; You say that the JVM is great for everything and that you can solve many issues with HKT and TCs - as a former java and scala programmer I don't buy it. My proof? Check out the industry. I'm working in the industry, using exactly those features to produce better programs that help my clients to be more successful and beat their competitors. You're right that the ultimate decider will be how things play out in the industry, but that takes time. I believe the industry will slowly, gradually come around to the benefits of higher-kinded types and typeclasses just as the industry has slowly, gradually come around to the benefits of memory safety, of safe ways of doing "railway-oriented programming" without giving up errors as values, of sum types, of explicit-but-lightweight async.... All of those were things that I found useful in ML-family languages before they became mainstream in the industry. I've been doing Scala for about 8 years now, and about half of the features I thought were important in it when I started are features that almost all new languages now have, and many of the things that I always thought were bad ideas (cake pattern, untyped akka actors, scalatest, SBT) are falling out of favour. So I'm going to put a certain amount of stock in my own experience.
It’s a big language with an intricate ecosystem. 
But Kotlin is a nice functional programming language, too. It is not as advanced as Scala in this regard, but way easier to master. Kotlin is good enough in most cases and has other strength like better tooling and better Java interop. And that could be the reason why Scala's growth is over.
I don’t find Sbt too much of a problem to deal with, though I’m excited for mill. My big problem is the ide support. Ensime is apparently not actively maintained, and IntelliJ never gets better so I’m not able to use cats well.
No, you misunderstood. This is not about what the compiler does, but the effects it has on users writing code. This hasn't been discussed at all. It makes users' lifes worse, because before they could inspect one source file to understand which implicts got added to scope With the proposed approach they need to look into all source files.
I am recent scala/spark developer and these are the reasons stopping me to go all in scala: 1. Tool support. scala ide/ eclipse/ intellij? sbt /maven??? 2. Less simple applications built around scala that people talk about here or on github and more about libraries. Its just overwhelming at once. 
Scala is a hybrid language in which you can do oo or fp style and imho this is it's strength and makes it kind of unique. But people that start with it think it's fp only and that scares them away. We should really start promoting it as a better Java, cause it can be no matter what others think and yes it excells in that area 100%. Just had a look at a very good Kotlin introduction video by Venkat S. and I thought - but this is Scala. And people move to Kotlin for it's simplicity and they really should do so to Scala which offers other benefits as well... Sorry if I didn't make myself clear, typing on my phone.. 
I think that's part of the reason. But also if we look at the universities and online courses that teach writing real world code do not encourage functional programming and Scala. The university classes that teach functional programming also do not use Scala for teaching. They almost always go ML/Haskell/Lisp way, and by the end of the term students have only written code that is harder to take to production. If we want Scala to be a mainstream language, we should encourage instructors in the industry (and to a certain extent also in academia) to focus on it.
You're right in that you can write OO code in Scala, but if you're not going to use any of the functional features of Scala then you might as well use Kotlin which has slightly better IDE support, a faster compiler, and both Jetbrains and Google throwing a lot of money at it.
I think that's the point here. Scala is the only language that is production ready and comes with some great features. I would love to write production code in Haskell, but the library ecosystem, documentation, and community support are just not there. Besides, there are few who are actively putting money on Haskell unlike Lightbend, Scala Centre, and EPFL.
I'm excited about Mill too. The thing is that you don't realise problems with SBT until you look at tooling that comes with other languages like Go and Rust. SBT works just fine, it's just that there is a lot of competition.
Also, looking at [a year ago](http://redmonk.com/sogrady/2017/03/17/language-rankings-1-17/), basically Scala was on par on an undefined position 11-13, so R, and Swift have visible moved up, and the relative distance between Scala and Shell still looks the same.
I think that's a very broken-window-theory point of view. Things being bad shouldn't be an excuse for making them worse. &gt; written in files whose name does not match the name of the class Enforce that package objects reside in files called "package.scala", done. &gt; we already need to go look everywhere, in theory That's a) not true in practice, and can be addressed (see above) and b) not the case with package objects. Even if the package objects is misnamed, it is still a _single_ source file. You don't need to inspect every source file within a package, like with the proposal. Are you going to implement that IDE support in IntelliJ, Eclipse, Atom, VSCode etc.? If you need IDE support to display the stuff added to scope coming from multiple files (maybe by showing them in a single editor pane!?) then maybe it's smarter to just stick with a single file in the first place. What's the point of a feature that requires IDE support (that will likely never be implemented) whose first step is to undo the main change this feature has, compared to package objects?
Compare it to maven and its a good one though ^^
this is promising https://github.com/julienrf/endpoints 
Have you tried [Atom IDE](https://ide.atom.io/) with [Scala](https://atom.io/packages/ide-scala) plugin? I'm new to Scala so just curious if its any better.
&gt; They checked once fully expanded, but developing the template itself is like working with untyped code. Templates are functions. You write a test for it before to see if it makes sense then it'll be smooth. &gt; What does that mean, concretely? What could it mean? Restrictions? &gt; How are they more precise, what can you actually do with them? I've pointed to a specific example of the kind of functionality I'm dubious about them being able to implement (cataM). I'd love to know cataM's purpose and laws first. &gt; You'd have to implement something like Idris' VerifiedMonad, which not many languages can do in a usable way. But even in Idris, people find it useful to be able to make a nominal distinction rather than a proven one - lots of code is written with Monad rather than VerifiedMonad. 1. `F is not Validation` - dumb way, but works. 2. define the actual difference. 3. describe monad better. &gt; Typeclasses can't do anything that ordinary code can't, Do you know what's a typeclass? &gt; but can do anything that ordinary code can, because they're just ordinary code - which is exactly what you want. Macros and templates are ordinary code too. &gt; Typeclass inference is a small language-level feature that indeed only allows "a few tricks", which again is as it should be - "magic" features should be as restricted as possible while meeting the needs of their use cases. Macros and templates are not magic at all. They're just AST modifiers and safe code-substitutions. &gt; It doesn't have any dedicated typeclass features Which is a reason for haskellers to hate scala. &gt; all the features that are used to implement typeclasses existed in the language already before people figured out they could implement typeclasses with them, they weren't designed for it. This is against your narrative - you said you need "a small subset of powerful features" but instead we've a bunch of half-assed features which implement another half-assed feature with a lot of boilerplate(what you dislike) and with ugly syntax(what you also dislike, like in SBT). &gt; Yeah, you talked about how linear types (which Nim doesn't even have in the first place, but that aside) prevent double free() when I was talking about double free() from a native library called with FFI. Linear types *and* valgrind. &gt; Which in Nim is something you have to do even for something as basic as e.g. connecting to Postgresql. Well, how else would you connect to it? Through slower, non-native drivers? Btw, I've used sqlite through the db_sqlite module in nim and even when the native driver failed I could catch exceptions in nim. I also haven't met with double-frees. &gt; That's exactly the point - performance and memory-inefficiency were its biggest problems, but they didn't stop the game being a huge hit, because they're ultimately not that important in the grand scheme of things. No, you just don't want to admit that the JVM was a bad choice because it doesn't fit your stupid narrative. Of course it didn't matter for a primitive game like mc. But it was a good case for the gaming industry: don't use the jvm for serious gaming projects. What do you think, why ms is rewriting it and why mc fans are hyping the new implementation? Hint: because it's better. &gt; If someone can build a hugely successful game in it, it's fit for building games in, by definition. Bullshit. Btw, show me the rest of the AAA games in java. I'm curious how big and studios write their engine in java. Here, a mirror: people wrote a lot more successful applications in C than in java, which means you should write everything in C because C is good at everything. Memory safety doesn't matter in the grand scheme because people use those softwares. &gt; I spent about 2 years using two native libraries over FFI and hit multiple memory safety vulnerabilities in both. One was ffmpeg which is supposed to be a poster child for using valgrind and fuzzers and all that stuff, but we still hit enough segfaults that it was worth separating out into a separate process. (Which meant we got to deal with the bugs in C++ libthrift as well, but that was better than having it bring down the main system). Segfaults? What is testing? What are static and dynamic code analyzers :S &gt; I can be C/C++-free enough OK, switch OS, browser, IM client, editor, password store, email client, use java-based games only - at least you'll have minecraft and some flappy-bird clone. Oh no, most of these apps don't have java ports... &gt; - the product I'm working on doesn't use any native libraries, One app? lol. &gt; so bugs in C/C++ code (other than the JVM itself) are not my problem. Instead, you'll have the bugs of the JVM's stdlib and the external libs. Remember when oracle didn't want to correct its default sorting algorithm for collections? &gt; Which has made my programming life a lot easier than when I was working on the system I mentioned that was using FFI. I'm pretty sure you've some kind of CRUD app. 
Genuinely curious, if it's deflating, where are people moving?
Kotlin is as much functional as JavaScript. Having functions does not make a language functional, nor is it even enough to confortably write functionally in it. What matters is immutability *by default*.
Better documentation? Better Java interop? Where does Scala fall short on those?
On your side-note, sometimes you have to due to cloud-provided databases like Redshift. To use that on your machine, you either have to mock it or use a similar technology. At my company, we use Postgres in our local dev and our integration test pipeline, while Redshift is used for dev/qa/production deployments. We use Flyway's placeholders to comment out Redshift-specific syntax (e.g. Compression types) for local builds. It leads to some subtle problems as you've mentioned. But its all we can do to stay agile.
I've had a pretty good experience w/ both [flyway-play!](https://github.com/flyway/flyway-play) and [flyway-sbt!](https://github.com/flyway/flyway-sbt) - though flyway-sbt can require a fair amount of boilerplate in some builds. The flyway migrations run at Play's application start-up and will fail my application if any migration fails. It also provides a pretty good UI integration w/ the Play Framework dev-mode execution. Sharing the config between the two has been satisfying as well; I've been able to share config between Slick and Flyway seamlessly. Curious to hear what your bad experience was here.
The ranking doesn't say, and probably can't say, that people are moving away from Scala. That's reading too much into it. For example, one reason for the loss of rank is that TypeScript is now ahead. Was that at the expense of Scala? My guess would be that it is mainly at the expense of JavaScript. Yet Scala's position would change based on that. The title of this post with "dropped" sends a message for which there doesn't seem to be solid evidence just based on the RedMonk ranking.
The documentation is not that bad, but it is a bit scattered. Scala is not a simple language and I it would be great something comprehensive and well structured like the [Rust documentation](https://doc.rust-lang.org/book/second-edition/) would exist. But I have to admit that the Rust documentation is outstanding and very few language have documentation of that quality. However, it would be really helpful in the case of Scala. The Java interop is not as seemless as with Kotlin, because Scala has many more advanced language constructs not present in Java. So you have to take extra care to write APIs that are usable from Java. And take a look at Akka how ugly it can be if the APIs are not carefully engineered for Java (like `$` in some references). Another point is the collection library. Scala collections are completely different than their Java counterparts, what is a good thing if you have a Scala-only System, but if you have a mixed system with Java collections implicit conversion is necessary what could hurt performance. And at least I felt the desire to rewrite everything in Scala (what is a good sign for Scala if considered in isolation). Kotlin in contrast feels much more natural with Java. That is not only because Kotlin uses Java's collections under the hood (but with nicer APIs) but also because of the null handling, which simply accepts the fact that null is part of Java. What do you do if you interface with a Java lib that could return null? Wrap everything in an `Option`? That is probably the best way, but it feels not as natural as Kotlins null checking.
There is not a canonical definition of what a functional programming is. However, I agree that immutability is an important part of functional language. And Kotlin is in this sense not a strict functional programming language, but that is true for Scala, too. Kotlin collections are immutable by default and you can create immutable objects as in Scala. The Kotlin developers plan to include a keyword for deep, compiler guaranteed immutability -- something I never heard of in Scala.
&gt; Are you going to implement that IDE support in IntelliJ, Eclipse, Atom, VSCode etc.? If not, who will be? It's already there, no? Type `scala.` in the editor, look at the autocomplete list, pick one value, ask for definition/implementation, no?
It could be that there are more ops roles being created. That would push up Python, Ruby, and "Shell" on this list. There are also more front-end / UI and mobile development jobs, which keep JavaScript and Swift high on the leaderboard. Scala is pretty common in my world (hard core backend, data engineering, big data) but there are a lot more developers in sheer numbers working on UI and keeping the infrastructure up and running.
&gt; If you look at it closer you can also express what types shouldn't be/do. I can do that with typeclasses too. It's usually a bad idea though. &gt; No runtime thing with concepts. Exactly, so they shouldn't look like runtime code. &gt; From where did you get that idea? From your link. &gt; Then express the monadic laws properly. How? How can you do that in concepts? [Here it is in Idris](https://github.com/idris-lang/Idris-dev/blob/master/libs/contrib/Interfaces/Verified.idr#L58). &gt; Techempower, where they compare raw frameworks without thinking about algorithmic backgrounds. TIOBE, which is a shitty popularity measuring site where they count SO questions and google searches. No problems with that, right? Who needs actual industrial data?! Not me! Well apparently you don't need any data at all since you've never shown any. &gt; Hmmm. [Here's the same code without any typeclasses](https://ideone.com/2IS6Sh), it does exactly the same thing (it's not injection or overwriting, but anyway it's nothing to do with typeclasses). &gt; No, I haven't seen a cataM sample. `cataM` is a standard well-known function, [here it is in Scala](https://github.com/slamdata/matryoshka/blob/master/core/shared/src/main/scala/matryoshka/Recursive.scala#L38), &gt; Your serialization totally depends on runtime mangling, not on typeclasses. No it doesn't, you said so yourself. &gt; ROFL you said almost nothing about those things. You must be trolling. No U. Just as with evidence, you can complain about how much justification I've given but it's more than you have. &gt; Memory safety came fast with storms. Typeclasses are there for decades and they're barely used. Memory safety had been around for decades (since the '60s at least) before the industry adopted it. &gt; Which ones? The things I listed - memory safety, sum types, errors-as-values-but-lightweight, explicit-but-lightweight async. &gt; It was 6 a not so long ago... 8 years doing Scala at all, 6 years doing it full-time. &gt; Previously, you've mentioned the cake pattern when I asked you about the set of "few but powerful features" - so what? So I think it's a bad idea, but the fact that it was implemented as a pattern in the language without needing any special language features still says something about the language design. &gt; I also don't see how SBT is failing. We've had a lot of threads here criticising it, people are looking for alternatives. &gt; Or how you plan to replace untyped akka actors - when being untyped is the point of the actor model. Lightbend are apparently releasing a typed version. In any case I've seen a lot less actor hype lately and a number of organisations moving away from them.
&gt; Templates are functions. You write a test for it before to see if it makes sense then it'll be smooth. That's the same argument dynamic language advocates use for not having types at all. &gt; I'd love to know cataM's purpose and laws first. I linked on the other branch. You pass it a (monadic) function for operating at one level of a tree-like datastructure, and it returns the function for operating on the whole thing. &gt; F is not Validation - dumb way, but works. Requires the two to know about each other though - if neither the author of `Validation` or `Monad` knows about the other then it will silently do the wrong thing. So the default is the wrong way around. &gt; Macros and templates are not magic at all. They're just AST modifiers and safe code-substitutions. They can transform code into arbitrarily different code. That's too magic at the point of use. &gt; This is against your narrative - you said you need "a small subset of powerful features" but instead we've a bunch of half-assed features which implement another half-assed feature Scala has a small set of features (e.g. no dedicated typeclass feature) and they're powerful (e.g. they're enough to implement typeclasses). That's exactly my point. &gt; Well, how else would you connect to it? Through slower, non-native drivers? Through drivers written in a memory-safe language, like one does on the JVM. &gt; I've used sqlite through the db_sqlite module in nim and even when the native driver failed I could catch exceptions in nim. Lucky you, but undefined behaviour is not guaranteed to only result in an exception. &gt; Here, a mirror: people wrote a lot more successful applications in C than in java, which means you should write everything in C because C is good at everything. Memory safety doesn't matter in the grand scheme because people use those softwares. Fewer and fewer new applications are using C - over the last 20 years C's marketshare has been in a steady decline as Java's has grown. C was a sensible choice in the distant past, when computer performance was much worse and security was much less important because most computers weren't networked. Today it's a bad choice, which is why we're seeing fewer and fewer new codebases started in C, and people gradually shifting away from C and onto safer languages. Of course some people are still using C but mostly because they're working on existing C codebases or using existing C libraries. &gt; Instead, you'll have the bugs of the JVM's stdlib and the external libs. Remember when oracle didn't want to correct its default sorting algorithm for collections? My memory is they corrected it as soon as the bug was reported. But in any case that's a much more local bug that's much easier to deal with than undefined behaviour or memory safety issues. &gt; Segfaults? What is testing? What are static and dynamic code analyzers :S Either they don't work, or C/C++ library maintainers (even big-name ones) don't use them. Bottom line is when I used FFI into C/C++ libraries I got segfaults, and when I didn't I didn't.
I'm really sick of it, and use anything mildly complicated and everything breaks till you're using type annotations to hold intellij's hand. even with ensime in the state it's in, i'm finding it a better option for me than intellij.
I view all language rankings as pure entertainment, yielding critical as well as entertaining discussions. And that's a good thing, because such discussions have the potential to foment change for the good. I've been doing Scala work for the past 5 years. I love working with Scala. And I've found no other. Yet, I'm also well aware of its profound shortcomings. I'm not blind. I feel choice of language is a passion of the heart, pure and simple. And without passion, who are we? Worry not about rankings. Obsess about all else. 
But Flink has a Scala lib (the documentation tries to point it as on par with the Java lib), and Kafka (specially in the KStreams and KTables) still have Scala bindings. Sure there are options, but as I guess Spark also had them, doesn't it?
&gt; I can do that with typeclasses too. It's usually a bad idea though. How? &gt; Exactly, so they shouldn't look like runtime code. And they don't. Not like your typeclass samples. They look worse than any macro I've seen so far. &gt; From your link. "The concept is a match if: 1.all of the expressions within the body can be compiled for the tested type 2. all statically evaluable boolean expressions in the body must be true" &gt; Show me where it's a language feature in Scala. It's a composite feature. Like OOP. &gt; How? How can you do that in concepts? By the right constraints. It's probably doable in scala too with a little boilerplate. &gt; Well apparently you don't need any data at all since you've never shown any. I've shown more than you for sure. And since you're just a zealot lier I'm pretty sure you can't show me anything. For the laugh, you started to attack the importance of performance and native code... Jeez, how low you can sink due to your insane stubbornness? &gt; Here's the same code without any typeclasses, it does exactly the same thing (for the record it's not injection or overwriting, but anyway it's nothing to do with typeclasses). Another weak argument: it's strongly related to typeclasses and the issue of nominal types and explicit implementations. You supposed to hate it because it can change the behaviour. But you don't because it doesn't fit your stupid narrative, right? &gt; cataM is a standard well-known function, here it is in Scala, "standard" and "well-known"?! Bullshit. Btw: ``` def cataM[M[_]: Monad, A](t: T)(f: AlgebraM[M, Base, A])(implicit BT: Traverse[Base]): M[A] = cata[M[A]](t)(_.sequence &gt;&gt;= f) ``` ^ if that cancer above is ok for you then you should really be silent about macros and how they look like. &gt; No it doesn't, you said so yourself. No, I didn't. It seems like you don't understand how shapeless work with fields. &gt; No U. Just as with evidence, you can complain about how much justification I've given but it's more than you have. Where is that "justification"? All I see is you doing bike-shedding, lying and being unprofessional af. The moment you started to bagatellize performance and other important things and started to put barely useful shit like typeclasses on a pedestal I knew that you're just another webshit who thinks he figured out life by writing a few abstractions. Good for you, that your applications don't need any quality and they only need to process basic text. You're talking about "safety" and you use a language with just basic memory safety. You're talking about a minimal set of powerful features - and here we're, at r/scala. You're such a fake. &gt; Memory safety had been around for decades (since the '60s at least) before the industry adopted it. You seem to be really new to the programming industry: at the '60 performance mattered even more than now and until the '90s we haven't had a memory-safe language with acceptable performance at any domain(except at editor scripting). &gt; 8 years doing Scala at all, 6 years doing it full-time. And 8 years ago when there were no monads in scala why did you use it? It seems like you got infected with acute category-theory-mania later. &gt; So I think it's a bad idea, but the fact that it was implemented as a pattern in the language without needing any special language features still says something about the language design. Dude, that's how patterns work. Don't act like you or the scala devs discovered a new road to paradise. &gt; We've had a lot of threads here criticising it, people are looking for alternatives. You mean you bring up your shitty maven in every thread, right? And that some people made "alternatives" which are barely used. &gt; Lightbend are apparently releasing a typed version. It's not a replacement, it's an alternative. &gt; In any case I've seen a lot less actor hype lately and a number of organisations moving away from them. I haven't seen any actor hype at all. Akka is just famous in the scala community.
openjdk version "1.8.0_151" OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-1~deb9u1-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)
I just started learning Scala and I wonder if it has some advantages over Kotlin?
&gt; That's the same argument dynamic language advocates use for not having types at all. Templates are typed. They can accept untyped arguments too. And if they don't work then they won't compile so, your argument is again, just air. &gt; I linked on the other branch. You pass it a (monadic) function for operating at one level of a tree-like datastructure, and it returns the function for operating on the whole thing. Doesn't sound like a special case. &gt; Requires the two to know about each other though - if neither the author of Validation or Monad knows about the other then it will silently do the wrong thing. So the default is the wrong way around. In scalaz they "know" about each other. &gt; They can transform code into arbitrarily different code. Just like typeclasses. &gt; That's too magic at the point of use. You mean like your cataM? &gt; Scala has a small set of features (e.g. no dedicated typeclass feature) and they're powerful (e.g. they're enough to implement typeclasses). Have you ever seen another language? Because scala's feature set is everything but small. With the way you need to implement typeclasses in it you need 4 features at least: traits, implicit classes, implicit values and typeclass constraints. &gt; That's exactly my point. Maybe that's your point but your sample is flawed. &gt; Through drivers written in a memory-safe language, like one does on the JVM. And what do you think how they'll connect? Through the magic portal? What do you think what functions will they invoke? Btw, if you're obsessed with safety then as I've said you're on the wrong platform. &gt; Lucky you, but undefined behaviour is not guaranteed to only result in an exception. It seems like you've never seen C/C++ code because undefined behaviour and segfaults are not usual in prod. I'm pretty sure you're using an OS and a browser written in them so, how much they crashed lately? &gt; Fewer and fewer new applications are using C Our application is new and have 7MLoC. That matters more than some toy crud app. &gt; over the last 20 years C's marketshare has been in a steady decline as Java's has grown. Both are declining. But C will stay a lot longer than java can hope to because it's not that replacable. Java is just legacy now, unlike C which can still serve well at certain domains. &gt; C was a sensible choice in the distant past, when computer performance was much worse and security was much less important because most computers weren't networked. Here are some facts: 1. Performance still matters and that's why people prefer C++, Rust etc. nowadays. C is primitive but it's irreplacable at some domains. 2. No one sane will write security software in purely GC'd languages. 3. Just because you don't like C/C++ we still need them. &gt; Today it's a bad choice, which is why we're seeing fewer and fewer new codebases started in C, and people gradually shifting away from C and onto safer languages. But not to the JVM. Instead, they choose C++, Rust, Nim, golang, dlang etc. &gt; Of course some people are still using C but mostly because they're working on existing C codebases or using existing C libraries. Or, they're working with 1. resource-constrained devices which probably need to be energy-efficient too and *professionals* don't want to waste CPU cycles and that tiny amount of RAM 2. crypto libraries, where *professionals* need full control 3. OSes, where the code and the memory management is unsafe anyway 4. drivers, because of the previous point 5. specialized machines, because C is still the go-to language of engineers And what you java devs do in java? Easy crud apps mostly. And the hubris coming after not doing anything complex... java devs are like javascript coders: they made some webapps and now they think they should do everything in it. That's why we've dog-slow IDEs and "modern" editors. &gt; My memory is they corrected it as soon as the bug was reported. Then your memory is wrong because another company made the correction and oracle didn't want to accept it for *years*. &gt; But in any case that's a much more local bug that's much easier to deal with than undefined behaviour or memory safety issues. Haha no, detecting issues in complex sorting algorithms is harder than writing a unit test - while ignoring the docs about undefined behaviour - or running valgrind or using RAII. See: the incident above. &gt; Either they don't work, or C/C++ library maintainers (even big-name ones) don't use them. Or the truth: you lied. Memory issues are nowhere near as usual as you try to make it look like. My OS is written in C but I haven't seen it crashing at all. My email client is C++ and it's crash-free too. Same is true about my C++ based IM, video player, document viewers, file managers etc etc. I've some games which crashed for me - and some of them were running on mono. I've seen FF crashing *after* the quantum releases - but how funny it is, because then it has received patches written in a memory safe language - rust. Btw, as I've said I'm working on a huge project. Segfaults and double-frees are almost non-existent for us. My code doesn't have them. Even the shitty modules are free of them what we've inherited from elsewhere. What we've are memory leaks - *like in java*. But you know what? We can fight against them and don't need to allocate 100GB heaps and introduce 10-15 minutes of garbage collection phases for our system to work. &gt; Bottom line is when I used FFI into C/C++ libraries I got segfaults, and when I didn't I didn't. Shouldn't have passed null everywhere, perhaps? Or at least, learn how to check for nulls. 
And then you jump to _one_ definition. While the rest is spread over all the other classes. So then you jump back, and jump to the next definition. And to the next. Scala has implicits. It's sometimes hard enough to figure out what happens, and that's with the context of having in them in one place. You absolutely don't want implicit definitions spread over multiple places. I'm not sure bringing up `scala.` supports your argument. The exact situation I'm warning against already exists with it. If you press auto-complete, you get items from multiple places already, many of them for no good reason. And if you have a look at the global namespace (which combines things from `java.lang`, `scala._`, `scala.package` and `scala.Predef`, you see what happens when people have completely lost track and control of what gets added to a scope: - #:: - +: - :+ - :: - &lt;:&lt; - =:= - ??? - AbstractMethodError - Any - AnyRef - AnyVal - AnyValCompanion - App - Appendable - ArithmeticException - Array - ArrayCharSequence - ArrayIndexOutOfBoundsException - ArrayStoreException - ArrowAssoc - AssertionError - AutoCloseable - BigDecimal - BigInt - Boolean - Boolean2boolean - BootstrapMethodError - BufferedIterator - Byte - Byte2byte - Char - CharSequence - Character - Character2char - Class - ClassCastException - ClassCircularityError - ClassFormatError - ClassLoader - ClassManifest - ClassNotFoundException - ClassValue - CloneNotSupportedException - Cloneable - Comparable - Compiler - Console - DelayedInit - Deprecated - DeprecatedConsole - DeprecatedPredef - Double - Double2double - DummyImplicit - Dynamic - Either - Ensuring - Enum - EnumConstantNotPresentException - Enumeration - Equals - Equiv - Error - Exception - ExceptionInInitializerError - FallbackArrayBuilding - Float - Float2float - Fractional - Function - Function0 - Function1 - Function10 - Function11 - Function12 - Function13 - Function14 - Function15 - Function16 - Function17 - Function18 - Function19 - Function2 - Function20 - Function21 - Function22 - Function3 - Function4 - Function5 - Function6 - Function7 - Function8 - Function9 - FunctionalInterface - IllegalAccessError - IllegalAccessException - IllegalArgumentException - IllegalMonitorStateException - IllegalStateException - IllegalThreadStateException - Immutable - IncompatibleClassChangeError - IndexOutOfBoundsException - IndexedSeq - InheritableThreadLocal - InstantiationError - InstantiationException - Int - Integer - Integer2int - Integral - InternalError - InterruptedException - Iterable - Iterator - Left - LinkageError - List - Long - Long2long - LowPriorityImplicits - Manifest - Map - MatchError - Math - Mutable - NegativeArraySizeException - Nil - NoClassDefFoundError - NoManifest - NoSuchElementException - NoSuchFieldError - NoSuchFieldException - NoSuchMethodError - NoSuchMethodException - None - NotImplementedError - NotNull - Nothing - Null - NullPointerException - Number - NumberFormatException - Numeric - Object - OptManifest - Option - Ordered - Ordering - OutOfMemoryError - Override - Package - Pair - PartialFunction - PartialOrdering - PartiallyOrdered - Predef - Process - ProcessBuilder - Product - Product1 - Product10 - Product11 - Product12 - Product13 - Product14 - Product15 - Product16 - Product17 - Product18 - Product19 - Product2 - Product20 - Product21 - Product22 - Product3 - Product4 - Product5 - Product6 - Product7 - Product8 - Product9 - Proxy - Range - Readable - ReflectiveOperationException - Responder - RichException - Right - Runnable - Runtime - RuntimeException - RuntimePermission - SafeVarargs - ScalaReflectionException - SecurityException - SecurityManager - Seq - SeqCharSequence - SerialVersionUID - Serializable - Set - Short - Short2short - Singleton - Some - Specializable - StackOverflowError - StackTraceElement - Stream - StrictMath - String - StringBuffer - StringBuilder - StringCanBuildFrom - StringContext - StringFormat - StringIndexOutOfBoundsException - SuppressWarnings - Symbol - System - Thread - ThreadDeath - ThreadGroup - ThreadLocal - Throwable - Traversable - TraversableOnce - Triple - Tuple1 - Tuple10 - Tuple11 - Tuple12 - Tuple13 - Tuple14 - Tuple15 - Tuple16 - Tuple17 - Tuple18 - Tuple19 - Tuple2 - Tuple20 - Tuple21 - Tuple22 - Tuple3 - Tuple4 - Tuple5 - Tuple6 - Tuple7 - Tuple8 - Tuple9 - TypeNotPresentException - UninitializedError - UninitializedFieldError - UniquenessCache - Unit - UnknownError - UnsatisfiedLinkError - UnsupportedClassVersionError - UnsupportedOperationException - Vector - VerifyError - VirtualMachineError - Void - _root_ - actors - akka - annotation - any2ArrowAssoc - any2Ensuring - any2stringadd - any2stringfmt - arrayToCharSequence - assert - assume - augmentString - beans - boolean2Boolean - booleanArrayOps - booleanWrapper - byte2Byte - byteArrayOps - byteWrapper - char2Character - charArrayOps - charWrapper - classManifest - classOf - collection - com - compat - concurrent - conforms - deprecated - deprecatedName - double2Double - doubleArrayOps - doubleWrapper - error - exceptionWrapper - fallbackStringCanBuildFrom - float2Float - floatArrayOps - floatWrapper - genericArrayOps - genericWrapArray - identity - implicitly - inline - instrument - int2Integer - intArrayOps - intWrapper - invoke - io - java - javax - jdk - jline - language - languageFeature - locally - long2Long - longArrayOps - longWrapper - management - manifest - math - native - noinline - optManifest - org - package - print - printf - println - readBoolean - readByte - readChar - readDouble - readFloat - readInt - readLine - readLong - readShort - readf - readf1 - readf2 - readf3 - ref - refArrayOps - reflect - remote - require - runtime - scala - seqToCharSequence - short2Short - shortArrayOps - shortWrapper - specialized - sun - swing - sys - text - throws - tools - transient - tuple2ToZippedOps - tuple3ToZippedOps - unaugmentString - unchecked - unitArrayOps - unwrapString - util - volatile - wrapBooleanArray - wrapByteArray - wrapCharArray - wrapDoubleArray - wrapFloatArray - wrapIntArray - wrapLongArray - wrapRefArray - wrapShortArray - wrapString - wrapUnitArray - xml
I can't say I've read most of the Rust documentation, but I've read a fair amount. I can understand you point of view. Scala has very good documentation explaining methods and classes, etc, or in other words, the scala docs are very good. But it does not have a very good documentation explaining concepts, or its philosophy, or maybe how to approach things. Something more like the [glossary](http://docs.scala-lang.org/glossary/index.html) or the [overviews](http://docs.scala-lang.org/overviews/index.html). And I think that is what you are complaining about. The best I can do is to point you to [Programming in Scala](http://shop.oreilly.com/product/0636920033073.do), which is the book from which I learned most of the advanced concepts of Scala (actually where I learned Scala from). The fact that Scala runs on the JVM, and has interoperability with Java does not mean it should follow the same philosophies as Java. Kotlin on the other hand aims to be 100% compatible with Java, its their goal. The only way to achieve that is by following the same philosophies as Java, using the same collections, using null, etc. You can improve on them like Kotlin does with nullable references, however you are still tied to null. The goal of Scala is to combine OOP and FP in a scalable way. And that philosophy implies having different collections (especially immutable collections), it implies not using null, etc. However because Scala runs on JVM and wants to take advantage of existing Java collections it must have interoperability with Java, such as null for example. Handling nulls in Kotlin feels more natural to you because you already have experience using nulls. But Options are not nulls, they are a new concept. They force you to think in a different way. In a way you do not have experience. Had you started programming only using Options, and then changed to a language using nulls you would feel the same way towards nulls as you feel towards Options. What I'm really trying to say is you shouldn't confuse the fact that something feels natural with the fact that its good or bad. Options may indeed be better even though they don't feel as natural. There is another factor which you must consider: how can you make technological progress, and introduce new concepts while at the same ensure you are 100% retro-compatible with the previous technology? Do you think its possible? Let me give you an example: Java does not have Higher-kinded types, which means it would probably be impossible to add them to a new language while at the same time ensuring the new language is 100% compatible with Java. But is that a good enough reason not to include them? Higher-kinded types simplify you code, you can share the same code amongst multiple classes which otherwise would be impossible. They give you extra type safety. These are very good reasons to include them in the new language even though they probably would make the new language not 100% compatible with Java. 
Which language has a global namespace that you like? I mean, would you remove primitive, function and tuple types from Predef? That said, I'd really love to see `any2stringadd` go.
it has a lot of features absent from kotlin. kotlin devs argue that these features were left out cause they make things too complicated. i don't agree tbh.
Hopefully the Mill team will alleviate the build tool issue. It's looking good so far, but so many frameworks are built on sbt it's hard to justify the switch right now.
&gt; Kotlin collections are immutable by default Kotlin's immutable collections are problematic from a performance perspective. This is because they are wrappers around mutable collections. So modifying the collection means that a complete copy needs to be made. Scala collections are [persistent data structures](https://en.wikipedia.org/wiki/Persistent_data_structure) which means that modified collection will share state with the old collection.
**Persistent data structure** In computing, a persistent data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. A data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
What is meant by FP can vary a lot depending on who you ask (it even varies over time). When many people say FP they mean something more akin to state management in [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language)). This means that most of your application should be a immutable and [side effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science)) should exist in tightly controlled areas of your application. It is possible to program this way in pretty much any language but there is a wide range in terms of how easy it to actually accomplish. There are a number of aspects of Scala that make it easier to program in this style of Kotlin (real immutable collections, for comprehensions, HKT, type classes, macros). These features make it easier to write code that uses common idioms for dealing with immutability and managing side effects.
hm.. my opinion is that the field name in Scala bind to database table, just as using member name of Scala class or schema mapper. That's one thing what schema mapper does to bind your variable to a virtual database table. Scala Quill[https://github.com/getquill/quill] support this. (and many dynamic language also use this convention (SQLalchemy))
hm.. my opinion is that the variable in Scala bind to field of database table, just as defining schema mapper we bind the variable name to database column name. In Doobie there's no schema mapper, so we have to pay attention to name mapping on every query. 
thanks for sharing, the Quill is looking awesome. I like it embraces the convension to bind class member name to database column name. (this is many lib of dynamic language does)
I think it is easy to overestimate the signal here. Scala has probably been inflated due to hype around Spark, Kafka and Flink. I think some of that hype is dying down as, for instance Tensorflow starts to eat into the hype space that Spark is also occupying. A second issue is, as others have noted, Kotlin is kind of a scala lite, so users who want a cleaner java may be pulled off a bit. After that, there is real, and I believe growing interest in FP, and scala is still the best FP option on the JVM.
&gt; That said, I'd really love to see any2stringadd go. https://github.com/scala/scala/pull/6315
I'm curious is any new feature in Kolin that not appear in Scala.
It's a somewhat often requested feature, and I'm not saying that you're wrong or anything. Just showing the other side, and that there is a good reason not to do that by default. In Doobie, you're just getting database results, and mapping those to data types. The data types and database fields are entirely independent and behave as they always do independently. I do suspect that at some point someone will contribute such a mapper, but for the reasons mentioned, it's probably never going to be the default.
It's possible to use Scala for Android, but there are many issues. One is the size of the bundle makes proguard mandatory. The other is that the Android API is very far from what you would want in Scala. So you either write pseudo-Java in Scala (which is painful) or you use one of the lib/frameworks written to make Android development more scalaish. Problem is they're often incomplete and poorly documented.
I don't work with Android, but you could try to source more answers at https://gitter.im/scala-android/sbt-android
i'm using Intellij professionally and personally for a year now to build a modest amount of projects using and to contribute to a bunch of opensource project based on Scala 2.11 and up. I think it is the way to go. It works flawlessly on my mac and linux machines, but i encountered some hiccups on my windows machine, but i'll probably attribute that to me being windows illiterate.
&gt; The template expansion process is untyped. And the end-result will be still typed. So, your argument again is bullshit. &gt; It isn't. It's a perfectly normal library function, one among dozens. It's just one example. \*It's not a special case between your ugly functions. &gt; Monad and Validation don't know about each other because Validation doesn't form a monad. They do know about each other if Monad can reach Validation. &gt; No, because typeclasses are ordinary code... Typeclasses also require metaprogramming: think about all the implicit hacks. They are not more ordinary than macros. &gt; There's nothing magic about cataM, it's a plain old function that behaves exactly like any other function. It's not a plain old function, it's a higher-order function operating with the monad spahetti. &gt; Ordinary compositional reasoning still works: if f(a) is b then cataM(g)(f(a)) is the same as cataM(g)(b), unlike with macros. "Unlike with macros"? Are you drunk or something? Macros just like functions can break that. &gt; They're implemented in Java, they'll *talk over the network* using the Java standard library methods that are specified for doing so. Aham. &gt; Everything is memory-safe except the JVM itself which I already talked about. So, not everything. Just the parts on the highest-level. Just like if you'd use RAII, a GC or custom memory pools with C++. &gt; Last browser crash was a few minutes ago, thank you very much. How convenient was that crash for your narrative! It's like you a miracle! It's like... you made that up. &gt; Sorting algorithms getting the wrong result are something you can detect with just a unit test. Is that why the bug was in the JVM's stdlib *for a decade*? &gt; Whereas undefined behaviour can manifest as memory corruption on another unrelated thread, or killing all your threads, or... You do realize that UBs are documented and you can avoid them, right? &gt; Nope. Maybe you're projecting. You're not capable of lying. Instead, you present an alternative truth. &gt; And which language was the part of the code where the crash happened? I'm sorry, I forgot to debug. Maybe it was C++. Maybe the rust code called some unsafe code. Maybe it was in javascript and one of the plugins went rogue - as they used to. It's funny because when FF crashes and you want to create a report you need to reproduce the bug *without plugins* because most of the crashes come from js, a *managed language* where the noobs throw nulls around like it's a ball.
&gt; And the end-result will be still typed. So, your argument again is bullshit. Only for trivial use cases. As soon as you're using templates that have any logic in, the lack of type safety in the template expansion process hurts you the same way it would in any other code. &gt; They do know about each other if Monad can reach Validation. What do you mean? `Monad` and `Validation` could be in completely separate libraries with different authors (they aren't, but there's no reason they couldn't be), they don't depend on each other at all. When a user uses both of them in the same project, that shouldn't mean that `Validation` suddenly magically becomes a `Monad` just because they imported both libraries. &gt; "Unlike with macros"? Are you drunk or something? Macros just like functions can break that. Functions (`cataM` included) operate on values, so if `f(a)` is `b`, then `someFunction(f(a))` is the same as `someFunction(b)`. This basic property is what makes compositional reasoning work. And macros violate it: `someMacro(f(a))` can do something very different from `someMacro(b)`. &gt; So, not everything. Just the parts on the highest-level. Just like if you'd use RAII, a GC or custom memory pools with C++. There is no memory-safe subset of C++, C++ fans like to pretend there is but they can never give you a concrete, enforceable specification. If there was, we'd see a linter people could use to enforce it, an equivalent of Rust's `unsafe`. Whereas Java code is memory safe, and the JVM implementation is clearly separated from safe code (Java's `native` is effectively the equivalent of Rust's `unsafe` in this context). &gt; Is that why the bug was in the JVM's stdlib for a decade? The bug was there for a decade because it was rarely triggered, not because it was hard to debug. &gt; You do realize that UBs are documented and you can avoid them, right? All kinds of things are undefined behaviour (e.g. arithmetic overflow, shifts that are too long, loops that don't exit) and one single instance of undefined behaviour invalidates your whole program - if you're lucky you get a segfault where the UB happened, but the compiler isn't required to do that, and it can also e.g. silently corrupt memory elsewhere in the program, call a function that you never wrote the call to, continue the program for several hundred lines and *then* segfault... &gt; It's funny because when FF crashes and you want to create a report you need to reproduce the bug without plugins because most of the crashes come from js, a managed language where the noobs throw nulls around like it's a ball. Think about how often you see null problems, then realise that's how often you're getting silent memory corruption when people write the same kind of code in C/C++.
&gt; You can do a negation by having two implicits at equal priority (so neither will resolve). But really a lot of the point of types is to be constructive, negation usually defeats the point. So, with metaprogramming: implicits. You like metaprogramming after all. &gt; Yeah, so what happens to non statically evaluable boolean expressions in the body? It *needs* everything at compile-time so, it'll try to compute it at compile-time if it can - otherwise it'll fail: "The concept is a match if: 1.all of the expressions within the body *can be compiled* for the tested type 2. all *statically* evaluable boolean expressions in the body must be true" - nim can perform compile-time computiations through the `static[T]` feature and that's how everything will match in the body. &gt; OOP languages tend to have dedicated OOP language features, e.g. class and extends. And what's your argument? Typeclass is a composite feature just like OOP. `class` is not used just for OOP but for class definitions. Classes != OOP. extends is for inheritance. extends is needed in scala for TC hierarchies too. &gt; Almost ~~everything's~~ nothing is explicit FTFY. Every typeclass is an ad-hoc abstraction which can only be used through the implicit system: the simulation of method injection and the evidence resolution. &gt; everything's properly structured, there are two different xy()s in two different namespaces with different behaviour which is exactly how things should be. No, I was able to override the type's behaviour in the implementation. Now, there are two different implementations with the same name. If someone were to remove the typeclass part because it's no longer requires then the program would work differently. TC is an ad-hoc abstraction with a bunch of implicit hacks. &gt; Implicit conversions I was unhappy with until IDEs started highlighting them. So, you need explicit IDE support for a dangerous niche feature just to not suffer with them... &gt; I've never said macros look bad. So, you feel you can back-pedal with a little word-play. &gt; The problem is they're too powerful, they allow code to look simple and pretty when it's doing something convoluted, Like when you define a category theory spaghetti with convoluted names, unenforced rules and constraints and bury them under multiple levels of implicit resolution? &gt; but the simplification is only superficial (since it's not compositional). It'll be as compositional as its author wants it to be. Macros are not meant to be used instead of functions. They're meant to do things which can't be done in the language and would require a bunch of small and almost useless features. They're meant to remove boilerplate and reduce programming errors by defining constraints which are impossible to define and implicitly generate safe-guards. They're meant to help generate *typesafe code at compile-time*. They're meant to perform compile-time computations and define compile-time requirements. Macros are about being ready for the coming of unforeseen issues. &gt; No U. Step through it in a debugger sometime, there is zero runtime reflection going on. I said this: you either use reflection or macros *or* mangle the static part of the program's memory with every metadata of every class. The latter increases the program's startup time and memory consumption which in turn makes it less desirable for memory-sensitive domains. &gt; and if there really wasn't one when I started there was certainly one soon after (ScalaZ 5.0 was released in November 2010). But even if it had just been "OCaml on the JVM" I'd've been interested. But scala is not ocaml on the jvm. It has *nothing* to do with ocaml. They're not related and monadic abstractions are rarely used with ocaml. &gt; The fact that it was possible to implement as a pattern is my point. Proves your point about what? That programming languages can implement patterns? Wow! &gt; The endless insults aren't helping your case at all, Dude, the level of your ignorance about other domains than webprogramming is an insult to every programmer. You think it's not an insult to lie and to be so stubborn that you can't acknowledge the reality? Because that's what you do: you've learnt a feature of a programming language, made some simple apps where it was ok not to care about anything besides avoiding basic issues and then here we're: you think you're better than every other programmer who can't fully use memory-safe languages at certain areas. Oh, performance and memory-efficiency don't matter but your implicit hacks do? Better give a presentation about it to those filthy stupid game devs and all the mechanic engineers! &gt; anyone reading will know it's because you're in the wrong. See my comment above. You're making yourself look like a clown for making up so much horseshit. I'd love to see how I'm "in the wrong", though. Is it because I won't acknowledge your shitty implicit hacks and try to focus on more important things? Is it because macros and macro-based features are more powerful and can increase safety more than TCs without bringing a lot of overhead? I'd love to see people agreeing with all the nonsense you've made up here: but people who don't suffer from the monad-delusion like you. So, you know: the majority of the industry. In 10 years, FP languages with purist communities will still be minorities because most of you rather do some bike-shedding about minor problems than to concentrate on the *grand scheme*. &gt; You need to work on your issues or get some help, you shouldn't be getting this angry about an internet discussion. Hahaha, said the guy who's so angry that he's ready to damn the importance of performance and other essential aspects of software development. You rather redefine sotware quality as "not crashing as much" than to admit that your platform and "techniques" can do more harm than good - they can make your language less fit for most domains. I'm *astonished* by how much you can lie and try to redefine reality just for an "internet discussion".
&gt; So, with metaprogramming: implicits. You like metaprogramming after all. No, I dislike it; it's occasionally necessary, but it should be restricted as much as possible without compromising important use cases, and it should be segregated from regular code so that the reader knows where to give special attention. &gt; It needs everything at compile-time so, it'll try to compute it at compile-time if it can - otherwise it'll fail: "The concept is a match if: 1.all of the expressions within the body can be compiled for the tested type 2. all statically evaluable boolean expressions in the body must be true" My point is: what about a boolean expression that can be compiled, but evaluates to false, but for some reason can't be evaluated statically? The part you quoted implies that the concept will still match. &gt; class is not used just for OOP but for class definitions. Classes != OOP. Classes are a dedicated OOP feature, OOP is the motivation for introducing them into the language. There's no language feature with that kind of relationship to typeclasses. &gt; Every typeclass is an ad-hoc abstraction which can only be used through the implicit system: the simulation of method injection and the evidence resolution. You don't have to use any kind of "method injection", you can use typeclass instances as normal objects and call methods on them directly. It's common to use extension methods with typeclasses but it's not necessary. Typeclasses rely on implicit resolution but that's standardised and restricted and works the same way for all typeclasses. It's a lot less ad-hoc than macros. &gt; No, I was able to override the type's behaviour in the implementation. No, there was no overriding. &gt; Now, there are two different implementations with the same name. There are two different methods with the same name, neither overrides the other. &gt; If someone were to remove the typeclass part because it's no longer requires then the program would work differently. No, if you removed the typeclass then the program wouldn't compile, because `AOps#xy()` won't be in scope. The unrelated method `B#xy()` wouldn't be magically picked up from a different scope just because it happens to have the same name.
&gt; Only for trivial use cases. For every use case. &gt; As soon as you're using templates that have any logic in, the lack of type safety in the template expansion process hurts you the same way it would in any other code. Templates are typesafe at the end. It doesn't matter how hard you try to invalidate templating but in the end it's no different from generics or typeclasses. &gt; What do you mean? Monad and Validation ~~could be~~ in completely separate libraries with different authors ^ &gt; Functions (cataM included) operate on values, so if f(a) is b, then someFunction(f(a)) is the same as someFunction(b). This basic property is what makes compositional reasoning work. And macros violate it: someMacro(f(a)) can do something very different from someMacro(b). You're comparing every macro to cataM. Are you drunk? &gt; There is no memory-safe subset of C++, C++ fans like to pretend there is but they can never give you a concrete, enforceable specification. I can: use stack and RAII. &gt; If there was, we'd see a linter people could use to enforce it, There are linters for that. But people rather use valgrind because it won't hit their productivity as much. &gt; an equivalent of Rust's unsafe. What?! Rust's unsafe is about calling unsafe code. &gt; Whereas Java code is memory safe, And that's it. It's memory safe by using a GC which is not good for plenty of domains. There are also cases when unsafe code is required. Or when your GC just can't guarantee things like deterministic deallocation and real-time processing. Or when your GC consumes so much memory that your app needs to "rest" for minutes. Or when your app's runtime has so much overhead due to all of that abstractions that you just can't guarantee that it'll work smoothly on an average computer. &gt; and the JVM implementation is clearly separated from safe code (Java's native is effectively the equivalent of Rust's unsafe in this context). You're mixing up FFI and unmanaged resources. &gt; The bug was there for a decade because it was rarely triggered, not because it was hard to debug. Define "rarely" triggered. There were multiple bug reports. &gt; All kinds of things are undefined behaviour LoL &gt; e.g. arithmetic overflow, shifts that are too long, loops that don't exit The 1. is documented and if you can't defend yourself it's your fault. And btw, it's documented *per platform*. Loops that don't exit? What's the problem here? &gt; and one single instance of undefined behaviour invalidates your whole program - if you're lucky you get a segfault where the UB happened, but the compiler isn't required to do that, What is debugger :S &gt; and it can also e.g. silently corrupt memory elsewhere in the program, Yeah, it'll call that magic spell. &gt; call a function that you never wrote the call to, continue the program for several hundred lines and then segfault... It also kills infants in their sleeps. These UBs, I tell you guyz... \s Btw, if you can't use function pointers then it only indicates that you don't know how to write tests and that you're a shitty programmer. Just because you bring up all the drawbacks you've found in google about unsafe memory management it doesn't mean they're that usual or that they're hard to catch. If you're stupid, you won't be safe with managed languages either. See: crypto hacks. Also, if you stick with the jvm you can be a little safe from basic errors but when your application will crash with out-of-memory-errors or stackoverflow errors what are you going to do? You think you can fine-tune your jvm or that you can protect yourself from endless recursion? 99% of the possible security vulnerabilities are in *ordinary code*. And if you channel your focus towards less important problems you won't be able to provide a smooth - or any good experience to your users. As I've said, just because all you do is simple text processing and your tricks can't do that much harm there it doesn't mean you'll able to use them at most domains *while creating high-quality applications*. &gt; Think about how often you see null problems, then realise that's how often you're getting silent memory corruption when people write the same kind of code in C/C++. Think about how often webshits can't read the documentation and then blame C/C++ for crashing their own things. Btw, write a browser in java and come back when you've conquered the industry. Show me how far your theoretical null-safety and your typeclasses can carry you.
&gt; No, I dislike it; it's occasionally necessary, but it should be restricted as much as possible, and it should be segregated from regular code so that the reader knows to give special attention when reading it. If it requires special attention and not reading the docs then it's a problematic feature - just like implicits. &gt; So what about a boolean expression that can be compiled, but evaluates to false, but for some reason can't be evaluated statically? If it can't be evaluated statically and it needs it statically then how do you plan to compile it or use it for the concept? The docs say that it needs to know everything at compile-time and that every expression needs to be true. For example, if you'd want to read the boolean from the command line then it'd either fail (or ignore if that's what you want) or try to ask for it at compile-time - depending on what's supported at compile-time computations. Suppose that there's an expression what it can compile but can't evaluate but it still compiles for some reason - then it still can't be a part of the matching function because the points say that it needs them statically. &gt; Classes are a dedicated OOP feature, False. It's a way to define classes. Classes != OOP. You can have classes and still not support OOP. &gt; OOP is the motivation for introducing them into the language. A little word-play again, huh? Your argument is still bullshit though. Class is just a keyword for types. We'd modify C that we'd use struct and have OOP with them. &gt; There's no language feature with that kind of relationship to typeclasses. The way traits work are that kind of "relationship". Type lambdas are even for an example. I doubt that it'd still be in scala or that it could get into the language without the monad fans. &gt; You don't have to use any kind of "method injection", you can use typeclass instances as normal objects and call methods on them directly. It's common to use extension methods with typeclasses but it's not necessary. And tell me, who does that? It's like saying inheritance is not really a part of OOP because you can copy the methods and tell the compiler how to treat the subtypes manually. &gt; Typeclasses rely on implicit resolution but that's standardised Is it standardised, though? Because every language do TCs differently and they usually have different limits. They way you can generate instances also varies depending on the instances' types. &gt; and restricted and works the same way for all typeclasses. Not restricted at all: see implicit conversions and type lambdas. Those two features introduce more ad-hoc-ness and uglyness than any other feature in scala. Also, type lambdas are *very* related to typeclasses - we could say that *they're designed for them*. &gt; It's a lot less ad-hoc than macros. Oh, you come with the "lesser evil" strawman now. Btw, TCs are indeed less ad-hoc but also far less powerful when it comes to introducing more safety and constraints. And less powerful when it comes to reducing boilerplate. Also, monadic abstractions introduce a lot more weird shit into programming languages than any macro that you or me will see in our lives. scalaz and cats? Most of their TCs and functions are way too complex, introduce a lot of overhead and barely have any benefits. What about the ad-hoc behaviour I've shown to you in my TC sample? People won't expect that and it's still doable. &gt; No, there was no overriding. Denial of Reality, again. I made an ad-hoc overriding. If someone knows the type's behaviour and then presented with that TC instance and a function expecting a matching abstraction then that person will expect the same behaviour. And it'll be surprise. &gt; There are two different methods with the same name, neither overrides the other. The 2nd one overrides the first one when used through abstractions. Ad-hoc overriding with ad-hoc abstractions and ad-hoc safety. &gt; No, if you removed the typeclass then the program wouldn't compile, You don't seem to understand: if I remove the abstraction requirement and just expect the raw type but leave the function's body as it was then it'll look like it does the same thing but not at the runtime. 
&gt; If it requires special attention and not reading the docs then it's a problematic feature Exactly, macro-style metaprogramming is a problematic feature. &gt; If it can't be evaluated statically and it needs it statically then how do you plan to compile it or use it for the concept? My point is what if you wrote a boolean expression (or refactored an existing one) that you thought could be evaluated statically but it actually couldn't (easy to do since it's the same syntax). &gt; The docs say that it needs to know everything at compile-time and that every expression needs to be true. They say "1.all of the expressions within the body can be compiled for the tested type 2. all statically evaluable boolean expressions in the body must be true". If you put a non-statically-evualuable boolean expression in there that passes 1. (it can be compiled) and passes 2. (it's not a statically evaluable boolean expression). &gt; Suppose that there's an expression what it can compile but can't evaluate but it still compiles for some reason - then it still can't be a part of the matching function because the points say that it needs them statically. No, the point only says that all the statically evaluable ones are used as part of the matching function. So it sounds like non-statically evaluable ones would just be silently ignored. &gt; We'd modify C that we'd use struct and have OOP with them. To the extent that OOP is practical in C, that's an argument for C being well designed. &gt; The way traits work are that kind of "relationship". Type lambdas are even for an example. I doubt that it'd still be in scala or that it could get into the language without the monad fans. You said that there were no monads in Scala 8 years ago - but those features were in the language already. &gt; And tell me, who does that? I do that, I've seen it done in other codebases. &gt; Is it standardised, though? Because every language do TCs differently and they usually have different limits. It's part of the language standard, so at least every typeclass will follow the same rules within the language (whereas if you do e.g. serialization with macros, two different serialization macros might follow different rules for how they work). &gt; TCs are indeed less ad-hoc but also far less powerful when it comes to introducing more safety and constraints. And less powerful when it comes to reducing boilerplate. A well-written macro can introduce safety constraints, but a badly written macro can destroy your ability to reason about code that uses it. Typeclasses are a better tradeoff - they cover many of the use cases for macros, but carry much less of the risks. &gt; What about the ad-hoc behaviour I've shown to you in my TC sample? It had nothing to do with typeclasses, as I showed you by showing you exactly the same thing in non-typeclass code. If you want to argue that implicit conversions are too ad-hoc and should be more restricted then I'd agree with that; unfortunately I've yet to see a language design that covers their important use cases (e.g. extension methods and the magnet pattern) without introducing something even more dangerous. &gt; The 2nd one overrides the first one when used through abstractions. No it doesn't, there's no override. &gt; if I remove the abstraction requirement and just expect the raw type but leave the function's body as it was then it'll look like it does the same thing If you call `.xy()` on an `A` then it will call `A#xy()` and if you call `.xy()` on a `B` then it will call `B#xy()`, and those might be different functions that do different things. That's true in almost every programming language.
Really interesting conversation. Love the way that discussion went and completely agree with its conclusion.
Looking at it. I find this "The database schema is represented by case classes. By default, quill uses the class and field names as the database identifiers" is interesting.
&gt; It doesn't matter how hard you try to invalidate templating but in the end it's no different from generics or typeclasses. It's different because generics and typeclasses get typechecked at every step of the way, following the normal typing rules of the language. So you can develop them the same way as any other code. &gt; I can: use stack and RAII. That's not memory safe, because there are many other things that are undefined behaviour in C/C++. &gt; is documented and if you can't defend yourself it's your fault. The only practical way to defend yourself is by not using C/C++ libraries. &gt; And btw, it's documented per platform. Nope. Undefined behaviour is undefined behaviour even if you're on a platform where it "should" work. Unaligned memory access on x86? Your program now has silent memory corruption even though the x86 ISA has no aligned load/store instructions. You know how your processor implements a 32-bit shift? Doesn't matter, it's still undefined behaviour, the compiler will still remove your security checks. Know your processor is twos-complement? Signed integer overflow is still undefined behaviour. &gt; Loops that don't exit? What's the problem here? A loop that doesn't exit is undefined behaviour in C++ (unless it performs particular side effects like writing to a `volatile` variable). &gt; Just because you bring up all the drawbacks you've found in google about unsafe memory management it doesn't mean they're that usual or that they're hard to catch. No googling needed, FTR. &gt; If you're stupid, you won't be safe with managed languages either. This stupid macho culture again. "Only stupid people need a guard on the saw," says the guy with seven fingers. Repetitive safety checks are the kind of thing computers are very good at, let the computer do it and free up the programmer's brain for other things. &gt; if you stick with the jvm you can be a little safe from basic errors but when your application will crash with out-of-memory-errors or stackoverflow errors what are you going to do? Perfect is the enemy of good. It's not like you don't get out-of-memory or stack overflow in C/C++, you just don't notice them so much because they're drowned out by all the memory safety errors. "Oh noes, the JVM only eliminates 95% of errors, not 100%". &gt; You think you can fine-tune your jvm or that you can protect yourself from endless recursion? When a language that can avoid endless recursion (like Idris) and has the other functionality I need reaches the point where I can get a job in it, I'll switch to it. In the meantime, endless recursion is a much less common problem than C/C++ memory safety vulnerabilities, and much easier to reliably diagnose and fix when it happens. &gt; 99% of the possible security vulnerabilities are in ordinary code. In C/C++ (or in languages where ordinary code relies on FFI) yes, because in those languages ordinary code is not memory safe and every bug is a security vulnerability. So use a memory-safe language and eliminate those vulnerabilities.
I have yet to see the front page for a programming language, library, framework, etc. that actually tells me anything meaningful about it. It seems to always be marketing-speak that conveys little useful information. Every time I have to go elsewhere...read answers on stackoverflow, an article on wikipedia, discussions on forums, etc. But the project's actual page is useless for decision making. Only good for downloads and documentation once I've already decided to use it.
[sbt-thanks](https://github.com/hywelandrews/sbt-thanks) Finds all of your maven dependencies, their github.com repository from metadata hosted on sonatype, and star's their GitHub repositories. This was inspired by a similar plugin for Cargo (https://github.com/softprops/cargo-thanks). 
It was an enormous strategic blunder for the Typesafe/Lightbend/Scala-Center folks to never officially support Scala on Android. 
I think you should try. I Recently started to build an android app myself using https://github.com/scala-android/sbt-android and https://github.com/47deg/macroid, and it looks promising so far.
That is incorrect. Haskell's `do` does much more. https://en.wikibooks.org/wiki/Haskell/do_notation
In my opinion, it's better to focus on one thing and do it well. Focus on making Scala the best language for server side. 
And coroutines. Scala has a few solutions that goes in this direction (http://storm-enroute.com/coroutines/, https://github.com/scala/scala-async, and recently https://github.com/ThoughtWorksInc/Dsl.scala) but it's not supported out of the box. I imagine a stable and simple enough macro system would allow Scala to have these constructs implemented in libraries, which I would prefer.
What information are you looking for on a front page for programming language? I can think of couple examples (choosing for mainstream languages mostly) I like, especially if they provide code examples. https://golang.org/ https://www.haskell.org/ https://kotlinlang.org/ (arguably the nicest sandbox envrionment, but is is one click aways) https://www.python.org/ https://www.ponylang.org/ https://www.dartlang.org/
I wrote a scala on android app not too long ago. It’s a pain to get set up, there’s some things that have changed that aren’t reflected in the docs. Once it’s up though, it’s pretty fantastic. I remember my incentive for trying it was that the example app that takes a video was 1kloc of Java, and I was able to do it with well under half that in scala. Language features like futures and promises saved me from callback hell, and made the overall experience of programming for Android much more pleasant 
I wasn't aware there was a plugin for play, I tried the sbt plugin some years ago and I didn't invested too much time trying to get it working, hard to remember what happened.
pretty sick tbh
Kotlin has no traits, no inner types, no path-dependent types, no anonymous classes, no intersection types (`with` keyword) no reified generics, no unions/coproducts and no structural types – it simply has far less tools for OO design than Scala. AFAIK Scala has the best contemporary OOP system in a statically typed language right now, even if you fully ignore its FP features.
&gt; Is normal programming languages you transform data by write groups of statements to execute in chains of functions You do the same in Scala. &gt; In Scala you put data in monads that describe transformations and use a functor (called map) to sequence those transformations. You can put data in monads and call `map`, but you are under no obligation to do so. You could always use normal function composition and call them one after the other without ever touching `map`. But monads are useful because they share a common "language" or "feature set"^(1) and therefore you can define patterns that work for any monad regardless of what it is. That is, you can describe sequential transformations regardless of context. &gt; Is that a loose but accurate description of how Scala works or their devs think? The relationship you describe between monads and functors is not quite accurate, but I'll touch on that later. Scala itself is not built around monads. It's built around being an mixed object-oriented and functional language and it provides facilities for working in both paradigms. Some of the functional facilities Scala provides are monadic types in the standard library (`Option`/`Try`/etc) and a language feature particularly useful when working with monads: for expressions. This is just one instance a pattern that works for any monad^(2). Because Scala is a mixed-paradigm language I'd hesitate to generalize "how Scala devs think" because it'll vary widely. That said, I feel there's a trend for Scala devs to embrace the FP side more than the OO side. &gt; Is there a relationship between map and a Functor or is a functor more general? There is a relationship and it's still a very general notion. I'll try an explanation, but my goal here is to provide a useful intuition, not be as precise as possible. The terminology "functor" and "monad" have origins in mathematics, especially category theory today, but for our purposes let's just say there are many "contexts". Some common contexts in Scala are the optional context (`Option`), the possibly-failed context (`Try`), and the async possibly-failed context (`Future`). The terms "functor" and "monad" describe the contexts themselves. There are many other terms which describe various properties of contexts but we'll stick with these two for now. A functor is "a context that can be mapped over". Put another way, a functor is something within which you can call `map` and get a new value in the same context. For instance, if you have an `Option[Int]`, then your context is `Option`, and the fact that you can call `map { _.toString }` to get an `Option[String]` or `map { _ * 2 }` to get a different `Option[Int]` makes `Option` itself a functor. You can do the same operation on `Try` and `Future` so those contexts are also functors. A monad is "a context that can be flat-mapped over". So, similarly to the argument for functor, if you have an `Option[Int]`, then the fact that you can call `flatMap` with a function `Int =&gt; Option[Int]` or `Int =&gt; Option[String]` and get the resulting `Option[Int]` or `Option[String]` is what makes `Option` a monad. You can do the same operation on `Try` and `Future` so those contexts are also monads. Given all that, each of the `Option`/`Try`/`Future` contexts is both a functor and a monad^(3). And this is what I meant when I said your description was not quite accurate. You don't always put data _in monads_ and then _use functors_ to sequence them. You can put data _into a context_, and if that context _is a functor_ or _is a monad_ then you know you can use functions (not functors) to sequence transformations in a way that's consistent with all other functor or monad contexts. In Scala the way you sequence them is by calling `map` or `flatMap`. Hope that helps! --- 1. The commonalities are: create a monad from a value (`apply`), sequence transformations (`flatMap`), and to a slightly lesser degree the monad laws (left/right identity and associativity) 1. More specifically for expressions are a pattern that works for anything, monad or otherwise, conforming to Scala's notion of sequencing: that it has the methods `map`, `flatMap`, and `withFilter`. While you can define monads with names other than these methods, sticking to them like the standard library does makes the for expression syntax work. 1. In the purest mathematical sense each context must also satisfy certain properties or "laws" to be considered a functor or a monad
At the same time kotlin devs add more than 50 keywords to the language and a fuckton of syntactic constructs (can you define a custom getter without googling for syntax? what about an intersection type using `where` keyword?). Kotlin is a mess, they try to extract specific concepts and syntax for a lot of things instead of unifying like Scala. Scala manages to unify a lot of concepts together - functions/objects/classes/traits/packages/primitives they're all treated the same – like objects. A scala programmer has to keep way less concepts in his head AND operate more power at the same time. That applies not just to scala's OO features, but to it's FP borrowings too – scala's implicits unify Haskell's MultiParamTypeClasses, FunctionalDependencies, IncoherentInstances, OverlappingInstances, UndecidableInstances, TypeFamilies (to some limited extent – with dependent function types), and perhaps other extensions, while being more modular, while not adding new syntax and burdening the programmer with more concepts. Scala's apply() and unapply() are natural first-order constructs, unlike Haskell's PatternSynonyms, Scala, really, is a tiny language, as it's DOT calculus shows. It's very far from a beastly languages like C++ (or Haskell for that matter) that are incredibly long journey to learn. If taught correctly, Scala can be learned in full quite fast.
I think you are seeing negativity that just isn't there. I found it interesting, that different people describe Scala so differently. That's why I said "interesting!". Nevertheless, thanks for your feedback, I have amended the comment to better reflect that.
Simon, just to understand from where you speak from, are you the author of Scala on Android (I'm guessing not)? Who was the snide remark addressed to?
They really made a mistake losing the OO meets FP line. *That* is the main selling point of Scala, IMO.
Sometimes the best way to do this is not the direct one. Consider which language Android developers who are happy with Kotlin on the smartphone will pick to develop their server-side part.
Which, to be fair, we're getting union type with dotty (or 2.13? I don't remember) and the Elvis operator is pointless in Scala where you're not dealing with nulls anyways. Or if you are, you could define it yourself, too.
I wonder if someone is tracing the list of all monad tutorials. How many of them are out there, how many reuse these pictures, how many use tacos, tortilla or shopping bags as metaphors? So many questions, so little answers...
Ok fair enough.
That’s it? Monads are nothing but things with map, flat map. I thought they were more complicated than that. Don’t we have Monads in Java as well then? 
Don't get me wrong, I like most part of Scala. I find `Option` to handle the absence of values very elegant. I like the Scala collections, too (although I don't like the implicit magic they do to satisfy the compiler). But the point is, that Scala feels like fighting the whole Java ecosystem! Kotlin in the opposite feels like a good neighbor in the Java neighborhood. You're right, that Kotlin is less progressive, but the language is only a part of the overall ecosystem and development experience. Kotlin just works better with all of the existing stuff and it doesn't feel wrong to use existing stuff with it. If I would want something completely new and better (TM), I would probably switch to Rust, since I find this language very elegant and powerful. It doesn't even have things like `null` or classes like Scala does for Java compatibility. But for now, the Java ecosystem feels (much) more productive for me and if I want to live in this ecosystem, I prefer a language that is a good citizen in it.
Scala has many more features, but they come with a price. Some features that Scala has over Kotlin: * (real) pattern machting * traits * higher kinded typed * partial functions * partially applied functions * functional collections * implicit parameters * implicit conversions * free method names like `#~*&gt;` possible But more features doesn't mean better language. 
Yes. `.stream()` There are some things that are monads, but the bag metaphor doesn't quite fit that nicely. Like Reader. But basically yeah, it's just math. Fancy words for simple things. 
Not quite, map and flatMap are in of themselves insufficient to describe monads. To be a monad, you must have a structure that fullfils the monad laws, no more, no less. http://eed3si9n.com/learning-scalaz/Monad+laws.html
Oh yeah. They don’t have them in List and other collections like in Scala. I wonder why they didn’t add map and flat map to List and other collections like in Scala? Why do they have to add monadic feature only to to Streams ? Backward compatibility? 
Yeah. They have satisfy the map and flat map criteria . Aren’t the my the laws? 
Nope, you could define map and flatmap on types in a way s.t. the types line up, but the laws are broken.
Hmmm.. okay. I will look into it. 
Can’t you just create a custom collector and do Stream.of(collector) to get those features? Won’t that be monadic? 
Interesting article I found about java monads. https://dzone.com/articles/whats-wrong-java-8-part-iv
That’s a lot of bikeshedding. A lot of effort for a very small changes which are ultimately based on assumptions I do not think are that obvious. Jargon is not good, but it’s a stronger part of Scala’s culture than many other languages’ and changing the copy here and there won’t change that. If you want to push that perception, you need a more integrated message which spans the official resources. If you want to get something like this right you’d need marketing—finding which groups to target and with what you can do it with. It’d be very exciting to see contributions from marketers actually, that sort of cross-functionality would be great to see. Maybe some Uni could take it up as a project? Branding &amp; marketing a language. That information could be used to steer Scala’s development as well. It’s an exciting prospect in my opinion. What I do love about this PR is how open and willing people were to look for a ew solution. Many seemed to have motivations which revolved around making the landing page more approachable which is compassionate and commendable. Those sort of things are sometimes hard to come by—great to se that Scala has them in such force. And I think the resulting change does do a better job at describing what good things come out of Scala’s features—so it’s not a bad change by no means in my opinion. 
Type classes
According to Kotlin it's less features yet more keywords == better language.
According to Kotlin it's less features yet more keywords == better language.
Same can be said about TypeScript.. the developers who are happy with it on the frontend will pick it to develop the server side of it (i.e. TypeScript can be used in conjuction with node.js). I don't know about you but I haven't had any good experience working in such environments where everyone is a "fullstack developer". I don't want to be a jack of all trades and master of none. I prefer to work in companies where I can focus solely on server side work, using the best tools available (Scala and Akka). Let the mobile / front-end teams do their own thing ;-) P.S. BTW it seems that React Native has the potential to make both Kotlin and Swift completely obsolete.
[My favourite example](https://developer.atlassian.com/blog/2015/08/optional-broken/)
&gt; It's different because generics and typeclasses get typechecked at every step of the way, You mean, like templates? Especially, when *you use them*? &gt; following the normal typing rules of the language. Follow the normal rules like java/scala's *unsound* typesystem? &gt; So you can develop them the same way as any other code. It's like bike-shedding on an entirely new level... &gt; That's not memory safe, because there are many other things that are undefined behaviour in C/C++. RAII is memory safe. Learn about it. &gt; The only practical way to defend yourself is by not using C/C++ libraries and turning off the computer. Because you **need** to use C/C++ and unsafe code. Scala is not a safe language. Memory-safe? Mostly. But most scala programmers use tools to ensure additional constraints with linters. We can defend ourselves with tools. Don't like to depend on external tools? Don't use unsafe languages which are on the jvm. &gt; Nope. Undefined behaviour is undefined behaviour even if you're on a platform where it "should" work. [Learn more](https://en.wikipedia.org/wiki/Undefined_behavior). &gt; A loop that doesn't exit is undefined behaviour in C++ (unless it performs particular side effects like writing to a volatile variable). \*Loops that doesn't exit *and* doesn't perform IO, which means they're useless loops. We've endless loops at work. They're serving us well. &gt; No googling needed, FTR. Well then you could use some googling to fact-check your delusions. &gt; This stupid macho culture again. "Only stupid people need a guard on the saw," says the guy with seven fingers. Repetitive safety checks are the kind of thing computers are very good at, let the computer do it and free up the programmer's brain for other things. If you like safety again then why don't you use Rust, Idris or Clean? Why do you use a platform with an unsound typesystem? Why do you use a platform where you can't guarantee thread-safety? Because you don't care about a fit solution you're just convinced by maniacs that the typeclass bike-shedding is the only thing you need. &gt; Perfect is the enemy of good. and the enemy of productivity and possibilities. That's why we need unsafe code and low-level access. That's why Rust still has ways to do unsafe code. &gt; It's not like you don't get out-of-memory or stack overflow in C/C++, you just don't notice them so much because they're drowned out by all the memory safety errors. Well, the JVM consumes 4-5x more memory than the same C/C++ program so, the out-of-memory error is definitely the JVM's biggest issue. Heard about those JVM apps with 100GB heaps? It's one of the reasons why java will never be used for nice AAA games. Look at minecraft, it's a primitive voxel game which consumes as much resources as a nice 3D game. &gt; "Oh noes, the JVM only eliminates 95% of errors, not 100%". When working with C/C++ apps the worst issues are memory leaks. double-free rarely comes up - even at the development phase. There's more "no-free". But at least, we can do something about it. We can fire up valgrind and done. On the JVM? Rewrite or tell the clients to buy 20-30GB+ RAM for things which could fit into 3-4GB and tell them to be patient when the GC kicks in. &gt; When a language that can avoid endless recursion (like Idris) and has the other functionality I need reaches the point where I can get a job in it, I'll switch to it. In the meantime, endless recursion is a much less common problem than C/C++ memory safety vulnerabilities, and much easier to reliably diagnose and fix when it happens. Endless recursion is more like a problem of FP users. For memory safety issues and leaks more than 99% can be detected with valgrind. Most of the time we don't even need valgrind because most of the allocations/deallocations will be generated automatically by the macros of the framework. &gt; In C/C++ (or in languages where ordinary code relies on FFI) yes, because in those languages ordinary code is not memory safe... RAII is ordinary code. Working with only the stack too(often used with crypto libs). We can argue that ordinary java code is not thread-safe - and thread-safety is much more problematic and harder to ensure than memory safety. Myth busted. &gt; ...and every bug is a security vulnerability. Every bug? Really? Never go full-retard. &gt; So use a memory-safe language and eliminate those vulnerabilities. What did you said previously? "Perfect is the enemy of good"? Btw, you're fucking funny. You fight against C/C++ and nim and rust. The latter two are memory safe and you've pretty much *nothing* against them besides some bike-shedding no one cares about. You live in a delusion where you can do everything with safe code and you can get away with awfully bad performance, long GC pauses, orbital memory leaks etc. You're not driven by logic, you're driven by mindless zealotry. All the way down in this thread you were trying to "solve" the biggest issues of PLT with duct tape. This is pure trolling.
Don't you mean Java? Because scalas list have map and flatmap methods.
&gt; Exactly, macro-style metaprogramming is a problematic feature. Not more than ad-hoc abstractions. See all the companies which left Scala due to all the typeclass spaghetti. &gt; My point is what if you wrote a boolean expression (or refactored an existing one) that you thought could be evaluated statically but it actually couldn't (easy to do since it's the same syntax). If it can't be evaluated statically then according to the docs: 1. it probably won't be used 2. or the compiler will try to ask for it at compile-time 3. will fail at compile-time/invalidate the instances 4. why did you write such a thing in an abstraction in the first place? Even if somehow you could manage to hack something it won't pass the review. &gt; They say "1.all of the expressions within the body can be compiled for the tested type 2. all statically evaluable boolean expressions in the body must be true". If you put a non-statically-evualuable boolean expression in there that passes 1. (it can be compiled) and passes 2. (it's not a statically evaluable boolean expression). It also says that it'll be probably ignored if it can't be evaluated. To test it I've created a test which was dependent on a constant, one where it read from stdin and another one where I declared an empty variable, used it in the concept and made it dependent on a file's content - and they failed. It seems like concepts only care about static stuff which are related *to type constraints*. &gt; To the extent that OOP is practical in C, that's an argument for C being well designed. Pulling a strawman, huh? &gt; You said that there were no monads in Scala 8 years ago - but those features were in the language already. Doesn't matter when it got in - but who asked for them and why and for what it's used. &gt; I do that, I've seen it done in other codebases. Where you do your own "injection" of course. But without the implicit classes, using typeclasses in scala is idiotic to the point where you're better off with using java. &gt; It's part of the language standard, Only the base features. &gt; so at least every typeclass will follow the same rules within the language Unless they don't. For example, when they use a different kind of instantiation. Or when they don't have injection. Or when they use macros. ad-hoc abstractions don't follow any "rules". &gt; whereas if you do e.g. serialization with macros, two different serialization macros might follow different rules for how they work What different rules? Are you making shit up again? As I've said a hundred times there are three ways to serialize *automatically*: 1. mangle the static memory space with the metadata of every class 2. reflection 3. compile-time boilerplate generation with macros - this can only be done one way just like the previous ones. I hope you understand that you *can't infer field-names normally*. &gt; A well-written macro can introduce safety constraints, but a badly written macro can destroy your ability to reason about code that uses it. Just like a badly written typeclass instantiation can destroy our ability to reason about a method's behaviour - as I've shown that to you. I'm kinda curious about badly written macros, though. I've used macro-based libraries in scala, I've written and used macro-based code in C and Nim and I've yet to see the problems. But let me quote you: "Perfect is the enemy of good" - macros at least can increase safety - *and they're extremely useful even at prod* - and it's better to just ignore the superficial fear of macros than to damn them. &gt; Typeclasses are a better tradeoff - they cover many of the use cases for macros, but carry much less of the risks. They're not a tradeoff, they're a whole different thing. I feel like I need to remind you what macros are capable of: 1. abstract over forgettable rituals(open/close, alloc/free) *without an overhead* 2. perform compile-time computations 3. provide deserialization without an overhead 4. extract information about the types and the algorithms to track behaviour and aid the programmer with the possibility to introduce safety constraints And what can typeclasses do? Create a way for ad-hoc abstractions and that's it. That's probably charming for some purists but for me it isn't. If I'd care about purism I'd use nim's [effect system](https://nim-lang.org/docs/manual.html#effect-system-tag-tracking) and the [nosideffect pragma](https://nim-lang.org/docs/manual.html#pragmas-nosideeffect-pragma) - they introduce less boilerplate and less confusion than any of that overcomplicated hacks in scalaz with dumb names which will just eventually kill your performance and scare away average programmers. Oh, you also care about enforced error handling? nim has an [efficient (exception) tracking system](https://nim-lang.org/docs/manual.html#effect-system-exception-tracking) which can reduce the boilerplate and the overhead in one shot. Rust's result types are also good because rust has *zero-cost abstractions* unlike the jvm. It's funny, because it seems like Rust does FP better: it can enforce thread-safety and monadic error-handling doesn't have a huge cost in it. With scalaz you can't even properly guarantee that something is pure and since you don't like macros and templates you'll be buried under the ugliness and boilerplate of its error-handling mechanics. &gt; It had nothing to do with typeclasses, Lies, the bug originates from the usage of typeclasses. &gt; as I showed you by showing you exactly the same thing in non-typeclass code. You didn't show me anything. &gt; If you want to argue that implicit conversions are too ad-hoc and should be more restricted then I'd agree with that; Implicit conversions are the only way to simulate typeclasses without making your code look like absolute shit. &gt; unfortunately I've yet to see a language design that covers their important use cases (e.g. extension methods Nim's Unified Call Syntax enables it to use a function like x(y, z) and x.(y, z). You can override the function(as in overloading) but not overwrite it which means that even if I'd have a way to override the function at a concept's imaginary instance it wouldn't work. &gt; and the magnet pattern) without introducing something even more dangerous. Nim has converters. Converters(implicit) and the magnet pattern are not that useful in practice. The magnet pattern is just another instance of bike-shedding. &gt; No it doesn't, there's no override. You've seen with your own eyes that I've passed the same class instance but with different expected types and it worked differently because it was **an ad-hoc overwriting**. Don't deny reality. &gt; If you call .xy() on an A then it will call A#xy() and if you call .xy() on a B then it will call B#xy(), and those might be different functions that do different things. That's true in almost every programming language. Get this into your head: when I pass a value to a function and it's supposed to call on e of its method then I'd expect it to work like it was defined for the type originally when I use it *through* a typeclass. This is an ad-hoc and dangerous behaviour. I get it, you're stubborn but please, leave it elsewhere. They way you try to push your little delusions and the puny way you try to deny reality is hilarious. If you don't want to be rational then leave but don't cry for me when I don't take you seriously. You make yourself look like a clown for trying to play minimax on feature importance due to your radically religious belief in typeclasses. Get real.
One thing that matters is sheer adoption. Think about it that way: If you have 100 users, you have a 1% chance to hit a bug first, and that's bad. As you have more and more users that chance becomes incredibly small. That's why adoption matters, even if they are using the language with completely different technology. They are using the same language, the same standard library, and probably a lot of the same third party libraries that you use. One thing that Kotlin did well is basic 101 of business development: - Compete on things your competitors are bad at, not the things they are good at. Kotlin is not trying to beat Scala on language power, but it has already beaten it in terms of IDE support, documentation and marketing. - Pick a small niche were you are focus on being really good and expand later, instead of trying to be everything for everyone at once. That's what they did with Kotlin on Android, that's what they did with Kotlin Native, and I expect they will do the same with Kotlin for JavaScript.
I've recently started working on a machine learning library called [doddle-model](https://github.com/picnicml/doddle-model). It can be summed up with three main characteristics: * it is built on top of [Breeze](https://github.com/scalanlp/breeze) * it provides immutable objects * it exposes its functionality through a scikit-learn-like API For a list of examples see [doddle-model-examples](https://github.com/picnicml/doddle-model-examples). If you have any questions/suggestions feel free to comment, I'd love to get feedback and contributors. Cheers.
Not really, if you want `Option` to be a value type, you'd have to think about deprecation _right now_.
Yeah, I meant Java. Java’s List and other other collections don’t have map, filter and flat map. We have to convert them to Streams first.
&gt; You mean, like templates? Especially, when you use them? Templates by definition are only typechecked after full expansion. So you have to debug the expansion process as though it were untyped code. &gt; Follow the normal rules like java/scala's unsound typesystem? The unsound cases are very rare in practice (unlike C/C++ where everyone who has done any serious work has encountered at least one segfault) and probably eliminated entirely by using wartremover (which, unlike "modern C++", is a single well-defined tool that you can run automatically to pass/fail your codebase) &gt; RAII is memory safe. C++ codebases that use RAII are still unlikely to be memory safe, because they generally still contain undefined behaviour. &gt; Learn more. Read your own link. &gt; If you like safety again then why don't you use Rust, Idris or Clean? I've told you repeatedly, Rust lacks HKT, Idris isn't quite mature/established enough (nor is Clean). Why do you keep asking these questions if you're not paying any attention to the answers? &gt; Why do you use a platform with an unsound typesystem? Why do you use a platform where you can't guarantee thread-safety? I'm always on the lookout for a better platform. But I've found it practical to avoid those issues in practice (by not using `null` and not using threading primitives directly). Whereas when I used systems without HKT or without typeclasses, that resulted in worse code and real bugs. &gt; and the enemy of productivity and possibilities. That's why we need unsafe code and low-level access. That's why Rust still has ways to do unsafe code. Maybe there are some use cases where those things are needed, but most of the time they're not. &gt; But at least, we can do something about it. We can fire up valgrind and done. On the JVM? Rewrite or tell the clients to buy 20-30GB+ RAM for things which could fit into 3-4GB and tell them to be patient when the GC kicks in. So on C/C++ running a memory profiler is so normal and expected that it goes without saying, but on the JVM you pretend they don't exist. Nice double standard there. &gt; We can argue that ordinary java code is not thread-safe - and thread-safety is much more problematic and harder to ensure than memory safety. Not my experience. Ordinary C/C++ code isn't thread-safe either, again you just don't notice because memory safety vulnerabilities are so much more common and problematic, even with all your valgrind and "modern" and what-have-you. &gt; "Perfect is the enemy of good"? What's your point? Memory-safe languages aren't perfect, but they're a lot better than non-memory-safe languages. &gt; The latter two are memory safe and you've pretty much nothing against them besides some bike-shedding no one cares about. Rust, as I've said repeatedly, I will happily adopt if and when it gets HKT and a good tool/library ecosystem (non-FFI). Nim I'd look at if it could do safe derivation and HKT or equivalent functionality, but it's even further away from having a good ecosystem (you can't even connect to a database without using FFI).
&gt; it probably won't be used Right, which is a problem if it was supposed to be checked. &gt; why did you write such a thing in an abstraction in the first place? Even if somehow you could manage to hack something it won't pass the review. It will pass review easily, because a non-statically-evaluable boolean expression looks exactly like a statically-evaluable boolean expression (something you're supposed to use in concepts), that's the problem I'm talking about. &gt; Pulling a strawman, huh? You said "We'd modify C that we'd use struct and have OOP with them", which I appreciate isn't actually a coherent sentence so maybe I misunderstood. &gt; without the implicit classes, using typeclasses in scala is idiotic to the point where you're better off with using java. Nonsense. &gt; Unless they don't. For example, when they use a different kind of instantiation. Or when they don't have injection. Or when they use macros. ad-hoc abstractions don't follow any "rules". What on earth are you talking about? Macros don't follow any rules, typeclass derivation does, that's my whole point. &gt; 3. compile-time boilerplate generation with macros - this can only be done one way just like the previous ones. I hope you understand that you can't infer field-names normally. If everyone writes their own macro for field names, they can end up behaving subtly differently. Whereas if you have typeclass derivation as a language-level feature (or even just a single shared macro that everyone reuses - as happens in languages where writing a custom macro is difficult and/or culturally discouraged) then it will follow the same rules every time. &gt; "Perfect is the enemy of good" - macros at least can increase safety - and they're extremely useful even at prod - and it's better to just ignore the superficial fear of macros than to damn them. I still haven't seen a convincing use case that couldn't be accomplished without them. &gt; abstract over forgettable rituals(open/close, alloc/free) without an overhead The difference between zero overhead and minimal overhead is, well, minimal, and in a language with HKT and typeclasses you can do resource management with minimal overhead without macros. &gt; perform compile-time computations Arbitrary compile-time computation is a bad idea; if you're going to have it at all it should at least look very different from run-time computation (as is the case when you implement it with typeclasses). &gt; provide deserialization without an overhead Can be done without custom macros in a language with HKT and typeclass derivation, as I showed you already. &gt; extract information about the types and the algorithms to track behaviour and aid the programmer with the possibility to introduce safety constraints Building a secondary type system in userspace code is not a sign of good language design. Type systems and safety constraints need to be used everywhere in the codebase to be effective (otherwise it's the same problem as FFI - you can have any number of constraints in your own code but that doesn't help if the libraries you call violate them) so they need to be part of the language design, or have a strong enough consensus in the library-writing community that they might as well be. &gt; You didn't show me anything. [Here's my post](https://www.reddit.com/r/scala/comments/82dulc/why_is_spark_built_with_maven/dvg72b2/). See the link "Here's the same code without any typeclasses"? &gt; Nim's Unified Call Syntax enables it to use a function like x(y, z) and x.(y, z). You can override the function(as in overloading) but not overwrite it Sounds like it works very similarly to using implicit conversions, and would have exactly the same behaviour that you've been complaining about all this time. &gt; The magnet pattern is just another instance of bike-shedding. What does that even mean? Saying "bike-shedding" won't make you magically right, just as your insults only show how wrong you know you are.
Agreed. I believe it could have made a big impact a few years ago.
&gt; Templates by definition are only typechecked after full expansion. Yep. &gt; So you have to debug the expansion process as though it were untyped code. Nope. When it gets expanded it'll either compile or not. &gt; The unsound cases are very rare in practice You just reminded me of that implicit cancer in the other thread. &gt; unlike C/C++ where everyone who has done any serious work has encountered at least one segfault And? Segfaults are visible and easily avoidable. &gt; and probably eliminated entirely by using wartremover Nope. Wartremover is not a magic tool. Do you have a rule for each unsound case? &gt; which, unlike "modern C++", is a single well-defined tool LoL "well-defined" - it's a basic and very limited linter - an external tool which is rarely used by scalaists. Compared to "modern C++" it has almost no users. I've used wartremover and alone it's barely an improvement. I've used it with 2 other linters with a bunch of scalac switches to get a basic feeling of safety. Of course I still couldn't enforce a bunch of useful constraints... &gt; that you can run automatically to pass/fail your codebase \*based on rules what you've defined - but since you don't like and know macros you probably won't learn how to use wartremover to enforce the things you want. And you'll need to depend on an external tool which is actually pure *metaprogramming*. &gt; C++ codebases that use RAII are still unlikely to be memory safe, because they generally still contain undefined behaviour. UB != memory safety. And don't say "generally" when you've never worked with C/C++. &gt; Read your own link. You need to learn about UB to suppress your misconceptions. &gt; I've told you repeatedly, Yeah, I've seen you like bike-shedding and superficial safety. &gt; Rust lacks HKT, But it has linear typing which can enforce far more safety constraints with less boilerplate, less spaghetti abstractions and no overhead than you can hope to do with typeclasses or HKT. &gt; Idris isn't quite mature/established enough (nor is Clean). It'll never be if people wait for the "magical event". In fact, it's not like scala is mature or established since it has almost no share in the industry. It's also not stable and you can't depend on scala2 code because of dotty. &gt; Why do you keep asking these questions if you're not paying any attention to the answers? I keep asking these questions because your answers are pretty much "I don't understand what's rust and I can't compare linear typing to my typeclass spaghetti; I also like to bike-shed about safety when I'm using a language where we can't guarantee anything; I don't understand what are macros, but I was told they're bad and I *believe* without a proof that I can do everything without them". Your answers reflect ignorance and I don't like that. &gt; I'm always on the lookout for a better platform. You most certainly not. Probably you meant "I'm impassionately searching for a platform where I can do the exact same thing in the exact same way because I don't want to learn". &gt; But I've found it practical to avoid those issues in practice (by not using null and not using threading primitives directly). C/C++ programmers also have such "practical" ways to avoid memory safety issues: don't make mistakes and use valgrind. Not using null you say? Do you enforce it everywhere? Probably not. Not using threading primitives? Haha, that's not the issue: it's passing mutable data without a safe strategy. &gt; Whereas when I used systems without HKT or without typeclasses, that resulted in worse code and real bugs. Worse code? It's hard to imagine worse code than what you do with typeclasses which can't enforce shit but looks like shit while introducing a huge amount of cognitive and runtime overhead. If an average programmer would need to maintain your "better code" he would need to study category theory for a few years first and develop some kind of fetish for ugly code. "real bugs" jeez, I think those imaginary times were the only times you've worked with serious industrial software. real bugs... &gt; Maybe there are some use cases where those things are needed, but most of the time they're not. This is not an argument. They're needed. Period. &gt; So on C/C++ running a memory profiler is so normal and expected that it goes without saying, but on the JVM you pretend they don't exist. Nice double standard there. And what do you plan to do with the memory profiler on the JVM? Try to find a little bit less-consuming algorithms? Because that's where you'll run out of your options. Imagine a game where you need to load areas - if you're loading the second area it's recommended to unload the first *completely* so, you don't need as much memory and people with average machines can use it - which is *very important*. On the JVM the GC won't help you. A bunch of garbage will probably stay in the memory and you'll need to tell the consumers that they need more than 8GB RAM and they'll rant that they computers can't run it and at the end they'll refund it. With optimized and unleaky C/C++ code where efficient area-loading is there, memory efficiency can still be a problem: see open-world games. C/C++ doesn't introduce unwanted overhead, however the JVM does due to its GC and JIT. Do you want to write your own GC for your own use case? Oh well, that's what people use to do with C/C++. Do you want to remove the JIT? You can't and if you could you'd kill your performance. Also keep in mind that GC pauses are there and the memory management is non-deterministic which is a huge problem at gaming and real-time applications(diagnostics etc.). In practice, you can forget your java memory profiles along with java at many domains. &gt; Not my experience. Ordinary C/C++ code isn't thread-safe either, again you just don't notice because memory safety vulnerabilities are so much more common and problematic, even with all your valgrind and "modern" and what-have-you. Speak, you pseudo-professional. Memory safety is so common and problematic that all we do is hunt for them and forget everything else. \s &gt; What's your point? Memory-safe languages aren't perfect, but they're a lot better than non-memory-safe languages. You're very naive. A lot better is totally dependent on the strategy of your memory-safety. With a normal GC it's definitely not better, just a basic alternative. A regular GC is just duct-tape. With nim and rust it's better - because they've better strategies than the JVM for memory safety. Rust does it with more boilerplate but it can yield better results and more safety at certain cases because it doesn't have any overhead. Nim does it with less boilerplate and more productivity - it may not be able to beat rust at every case when it comes to efficiency because it has deferrent reference counting instead of linear typing but when the compiler can optimize away the garbage it'll be very competitive. They're both more competitive as platforms than the JVM and they can be both used at more domains because neither of them are limited with their GCs. Nim's GC is optional and you can even do manual allocation and deallocation - it can also be abstracted away with templates. This means it can yield a solution for the allocation/deallocation problems I've mentioned. &gt; Rust, as I've said repeatedly, I will happily adopt if and when it gets HKT and a good tool/library ecosystem (non-FFI). I'm pretty sure at this point the library ecosystem is competitive. The tooling totally depends on what do you mean. I wonder what could you loose by giving up HKT if you've linear typing... &gt; Nim ... you can't even connect to a database without using FFI. I doubt anyone will care enough to change that because nim's FFI can be used pretty safely. Building the driver and the socket interaction is just unnecessary and unsafe work. 
Awesome name!
I think the answer lies in the direction of "less buggy code" but goes much deeper than that. To me FP and its close relation to CT is a good step in the direction of making software engineering more mature. One could perhaps say the same about OOP related concepts but the problem I have with those is that they in no way increase our ability to reason formally about the correctness of our software. FP on the other hand makes it a lot easier to formally prove that a program is correct. We're not there yet, I.e. we can't yet prove at scale that the software we produce is formally correct. As I see it this is related in part due to lack of tooling and in part due to the minimal place formal prove gets in education. In any case, as software is defining society and culture pretty heavily, I see a need for producing more correct software (and to bring morality into the equation but that is a whole differ argument).
&gt; if the answer is it gives you "equational reasoning and allow you to write less buggy code" then I feel that is not strong enough a case to justify the investment in learning these abstractions. I am a little curious about this one. What would in your mind be a strong enough case then? Let me preface this by saying that I think it is far more important that you write pure code that is referentially transparent than it is that you use fancy abstractions from category theory. The fancy abstractions are simple solutions to problems that tend to crop up once you do choose to walk the path of purity. As for the "killer app". I dont think you can point to any specific problem and say "that is where it is needed". It is much more the case that as your system becomes bigger you need to structure it to keep the system manageable. Category theory provides a set of tools for structuring large systems that are much more concise, clear than anything OOP has ever produced. The GOF design patterns are fine, but they are also incredibly vague. Category theory is not the only tool to provide structure to large code bases, but it is sane one that makes sense and has a solid logical foundation much like sql is a solid foundation for database queries.
CT and FP can help us construct programs that are correct (as is often suggested), but in my opinion the true killer app is that they help our programs *stay* correct as they evolve. CT gives us a very precise way to talk about *composition* and what it means, which is very important because this is how we manage complexity. FP gives us *equational reasoning* which lets us apply these compositional rules (and other lawful substitutions) freely, giving us the ability to refactor our programs confidently. Taken together, the practical super-power we gain is *maintainability*. 
Well you are entitled to your own opinion but in my opinion I don't see Kotlin going very far: * React Native from Facebook is killing it for cross platform mobile development and making Kotlin on Android completely irrelevant (or Swift). The biggest issue with mobile dev is that you have to develop twice (once for iPhone, once for Android) and Kotlin is not solving that problem and has heavy bias towards Android. * Kotlin for JavaScript ? Right now it seems that TypeScript has very wide adoption and it looks like it's going to be around for quite a while. But that market is so crowded, I don't want to compete in that market or even think about that market. * Kotlin for server side ? Yes, for a subset of developers who are moving away from Java: these developers consciously realize that they are not strong enough for Scala, they realize that Kotlin is a weaker language but they choose it as the the easier out. No big loss for Scala, in my opinion. 
&gt; if the answer is it gives you "equational reasoning and allow you to write less buggy code" then I feel that is not strong enough a case to justify the investment in learning these abstractions. I would recommend reading Paul Graham's [Beating the averages](http://www.paulgraham.com/avg.html), specifically about The Blub Paradox. 
&gt; Where did the doc state it's supposed to be checked if it isn't static? You can't tell whether it's static or not just by looking at it, since the syntax is the same either way. &gt; Unfortunately, Concepts only excepts boolean expressions related to types. That's the opposite of what you claimed earlier, and not what the documentation says. &gt; Typeclass "derivation" is not an explicit feature nor is it checked. You derive as you wish. You've different ways to derive because there are different cases - and you can do bad derivations easily. False. &gt; You only showed serialization - not deserialization I showed both, look again. &gt; you'll need dynamic instantiation and field discovery or something that can discover a class's structure at compile-time. i.e. typeclass derivation, as I've been saying the whole time. &gt; There's no consensus at all in the "library-writing community" that scalaz should be used everywhere. In fact, it's rather niche in libraries. Are the IO monad, effect tracking and exception tracking part of the language design or have a strong consensus in the community? Nope. There's a consensus around not having unmanaged side effects or exceptions, that's enough. &gt; Nope. The . is just syntax. So are implicit conversions.
I'm still not aware of any uses for partial functions that couldn't be solved just as easily as with a `A =&gt; Option[B]`. Does anyone have any applications for it besides for interacting with standard library methods?
I used partial functions in Future.recover() or recoverWith()
Could try ScalaFX http://www.scalafx.org/
&gt; React Native from Facebook is killing it for cross platform mobile development and making Kotlin on Android completely irrelevant (or Swift). The biggest issue with mobile dev is that you have to develop twice (once for iPhone, once for Android) and Kotlin is not solving that problem and has heavy bias towards Android. You don't have to conquer the world, just a part of it. We will never beat out javascript as typed languages, but that doesn't mean that typed languages can't take a good chunk of the market of people who don't like dynamic langs for applications. Kotlin has a real shot here. &gt; Kotlin for JavaScript ? Right now it seems that TypeScript has very wide adoption and it looks like it's going to be around for quite a while. But that market is so crowded, I don't want to compete in that market or even think about that market. This is mostly useful for projects whose backend is already Kotlin. Same with scalajs (sort of). &gt; Kotlin for server side ? Yes, for a subset of developers who are moving away from Java: these developers consciously realize that they are not strong enough for Scala, they realize that Kotlin is a weaker language but they choose it as the easier way out. No big loss for Scala, in my opinion. We _want_ sheer numbers though, because developers aren't only the ones in control, managers and whatnot also see these figures and make decisions. Also devs can be taught (at least the ones smart enough to see a benefit in a type system).
Flyway 5 has a paid edition that includes "undo" migrations
Flyway with flyway-play: the power of flyway with the same experience as play evolutions. By the way, postgres is one of the few databases that can run ddl in a transaction, which means if you have an error halfway through a migration you don't end up in an in-between state. If you're using it in production that's another reason it's worth using in development. I'd recommend learning how to run it in docker so you can easily have as many throwaway instances as you need. It's still slightly less convenient than H2 but not by all that much. 
yes. There was an experimental project by Doobie author, https://github.com/tpolecat/doobie-quill , where Quill provides name binding automatically.
As /u/naftoligug points out they are isomorphic, however syntactically the `orElse` method on partial functions allows them to be composed in a way that functions to options cannot (easily). Consider the following implementation of the Collatz function: val isOdd: PartialFunction[Int, Int] = { case d: Int if d % 2 == 1 =&gt; 3 * d + 1 } val isEven PartialFunction[Int, Int] = { case d: Int if d % 2 == 0 =&gt; d / 2 } val collatz = isEven orElse isOdd Doing the equivalent with two `Int =&gt; Option[Int]` functions would be quite cumbersome and would require evaluating the functions themselves rather than just calling the `isDefinedAt` method.
&gt; Kotlin is not solving that problem and has heavy bias towards Android That was literally the first niche they were targeting with Kotlin Native. Kotlin is not Scala, it's unlikely that they will drop the ball here. &gt; I don't want to compete in that market or even think about that market. If they only grab 2% of JavaScript developers, they are already bigger than Scala altogether. &gt; Yes, for a subset of developers who are moving away from Java: these developers consciously realize that they are not strong enough for Scala, they realize that Kotlin is a weaker language but they choose it as the easier way out. That's an incredibly arrogant thing to say. Assume some open-minded Java developer that does his research for better languages and finds Kotlin and Scala. A web search later, he has read that Kotlin solves Scala's problems, has IDE support and supports many platforms, while Scala's benefits over Kotlin are higher-kinded types (what's that? it's not documented on the official website?) context bounds (what's that? it's not documented on the official website?) and typeclasses (what's that? it's not documented on the official website?). That developer doesn't have to be intellectually disadvantaged (as you allege) to pick Kotlin.
You would add your logging context to the request using `WrappedRequest`, and pass that through in a custom action. This is essentially the same approach that `MessagesRequest` uses: https://github.com/playframework/playframework/blob/master/framework/src/play/src/main/scala/play/api/mvc/MessagesRequest.scala
This is *not* a good way to write modular code. It's a gateway to writing very lazy, poor code, and every single one of those examples could potentially throw exceptions at runtime because the author was too lazy to handle the domain in a total way. That's not "great" by any means.
Of course `isDefinedAt` still evaluates a large part of your functions. I would guess largely the same parts as when you would chain 2 `Option` returning functions.
Here's an simple example where I greatly benefited from these abstractions in a real world use case. My team had to set up a performance test program that runs daily in CI in a small timeframe. It required to implement a sequence of steps that, taken individually, are relatively easy to implement, but the sum of them was a pain, because each step had to deal with a different library / piece of infrastructure that none of us was too familiar with, and parallelising the work was gonna be hard if we worked using the classic OO approach. We adopted the "finally-tagless" approach to describe interfaces (or algebra if you prefer, though algebra usually come with invariants that are hard to come up with), and composed those interfaces together in a program. Each interface referenced NO CONCRETE-TYPE whatsoever. This let the implementor make the decision of what data is needed / what data is produced. And the return types of each method was wrapped into a higher-kinded type `F[_]`, representing the "effect" the computation was going to run in. This let you defer answering questions like "are we synchronous/asynchronous" . An algebra for us looks like : import cats.{ Functor, ~&gt; } abstract class ArtifactRepository[F[_], ArtifactInfo, JarFile] { self =&gt; def fetchArtifact(artifactReference: ArtifactInfo): F[JarFile] def mapK[G[_]](fk: F ~&gt; G): ArtifactRepository[G, ArtifactInfo, JarFile] = (artifactReference: ArtifactInfo) =&gt; fk(self.fetchArtifact(artifactReference)) def map[B]( f: JarFile =&gt; B )(implicit F: Functor[F]): ArtifactRepository[F, ArtifactInfo, B] = (artifactReference: ArtifactInfo) =&gt; F.map(self.fetchArtifact(artifactReference))(f) def contramap[B](f: B =&gt; ArtifactInfo): ArtifactRepository[F, B, JarFile] = (artifactReference: B) =&gt; self.fetchArtifact(f(artifactReference)) } This algebra for instance represent the ability to fetch a jar from a maven repository. Because none of the types are set, we can provide transformation methods that takes functions as parameters to help moving from one type to another. When you compose the algebras together, the types they operate on have to align. Then, when you have a bunch of those algebras, you can compose them in an abstract, sequential program by passing the algebras as parameters, aligning the types so that the result of one call can be passed to the next one, and stating that the abstract F[_] has an associated monad instance from scalaz or cats, which lets you describe the sequence in a for-comprehension. This makes the flow extremely readable and does not let the function cater to superfluous concerns. Then we divided the work between ourselves, each of us picked an algebra and provided an interpreter for it (read : choosing concrete types and implementing the interface), and since all the types were abstract we could work in isolation and not have to worry about the concrete types our coworkers needed. Then when we had all the implementation / unit-tests that go with it, we only had to bootstrap our application (write the main), leveraging the different transformation methods to align the types. Not only did it allow us to parallelise very efficiently, we also were able to switch from one interpreter to another one for one of the algebras, and no refactoring whatsoever was required. So what did this approach required from us : * knowing what a functor is (category theory) * knowing what a monad is (category theory) * knowing what a natural transformation is (category theory) * refrain ourselves from defining case classes as soon as we start solving the problem * a little knowledge of the ecosystem to decide what is the best library to implement an interpreter to an algebra What was the gain : * efficient parallelisation of tasks * ease of pivoting when you have to reverse a decision and switch a component/library for another * code that doesn't feel dirty because concerns aren't mixed * feeling great with ourselves for applying an approach that doesn't leave you feel like you've delivered a bowl of spaghetti 
&gt; Kotlin is a weaker language but they choose it as the easier way out Why Google discarded the strength of Scala for years? Is Kotlin developers a big loss for Android?
+1. Also, you should not discount the substantial productivity boost you get when maintaining a program. When an OOP moves to FP, they may have a hard time writing projects from scratch in initially, but the maintainability process becomes incredibly faster. In my experience, a developer tends to spend more time refactoring code than writing projects fresh projects.
Thx, everyone knows naming is hard, so I chose a generice one... :D
I don't think that's remotely similar. Kotlin still has if _and_ when.
If Scala has not any problems then there is no enhancement in Dotty.
Sorry, I didn't mean the merging, I meant that `when` can behave as your proposed `if` (though it's more limited).
thank you for all the time you put into this
 This seems to me to be pretty much like the R lang way-of-life (automaticly &amp; silently recover from deep errors), which will led to endless debug sessions. One of the mainly benefits of Option/Try vs null (or things like this) is that Option/Try enforces you to define a way of fix things in runtime.
&gt; As I see it this is related in part due to lack of tooling and in part due to the minimal place formal prove gets in education. Not to harp on your main point, but this isn't really true. There is a mathematical theory which basically says that proving a non trivial program to be entirely correct is practically impossible, this doesn't have anything to do with lack of education. Projects like SEL4 (formally verified kernel) attempt to circumvent this in different ways but again this isn't. A guy did a very good talk about this, I can find the video if you are interested.
A Java stream is effectively a builder for functional computation over a collection. They are not the streams people commonly think of like video/audio or other forms of asynchronous data passing.
Isn’t this intended behaviour? 
thank you for the article!
I would say it is perfectly safe to pattern match List in Scala. Things like: list match { case head :: tail =&gt; ... case Nil =&gt; ... } are quite normal. I see an issue with mixing interfaces and cherry-picking cases, and then being surprised that match is exhaustive only because quite a lot of things land into `case _ =&gt;`. With given `Seq` example I would probably do something like: seq.toList match { case a :: b :: Nil =&gt; a + b case a :: tail =&gt; 0 // 0 doesn't make much sense to me... case Nil =&gt; -1 } // ... but I handle all cases explicitly and let the compiler inform me if this is not exhaustive
I'd be definitely interested in the videos. Besides that I'm not under the illusion that any non-trivial program can be proven to be correct. Point still stands that it is easier to reason formally over FP programs than it is over the less formal approach taken with E.g. design patterns.
Letgo | Software Engineer | Barcelona (Spain) | Onsite | Full time Job description: https://gist.github.com/sergigp/e3da12968b6c28ece2aaa6438f49c3a3
http://jobs.letgo.com/ has links to http://careers.letgo.com/careers/?m=portal&amp;a=listings&amp;category=extraField192231&amp;option=Product and http://careers.letgo.com/careers/?m=portal&amp;a=listings&amp;category=extraField192231&amp;option=Technology - both 404 The link to the FAQ: https://help.letgo.nl/hc/nl/articles/214060363 also 404's The link to the terms and conditions, https://help.letgo.nl/hc/nl/categories/201676326 also 404s
It works fine on simple code but as soon as you start using fancy FP features (higher kinded types, implicits everywhere) IDEA often fails to parse the code properly and reports false errors.
scala.js with HTML GUI could be a reasonable option
I like Lagom for it's out of box Cassandra support 👌
That's really cool!
Wow, that's really cool! I found some [other link](https://cs.stackexchange.com/questions/37571/efficient-algorithm-to-compute-the-nth-fibonacci-number) that really helped out to understand the material. Using the expanded expression really simplifies it, rather than dealing with the floating points. But thank you for this it really opened up a whole 'nother approach to this problem. It might not be a good idea to derive it during an interview, but this is pretty cool! :) I've been wondering about the un-memoized approach for a while now
Yes. It generates it's debugging information on there (that being the compiled Java bytecode on it's .class files, which run on the JVM. Since the JVM installs are dependent on different computer architectures, the JVM would produce a different mapping, dependent on the computer architecture (such as x86ARM or MIPS assembly). The machine code contains byte mappings to run on the processor, which is a very large scale circuit device. An encoding, such as some arbitrary 32-bit or 64 bit binary number: 01010101010101010...010101 is some state in a processor that operates on the gate level to execution some instruction in a program register. The register can hold different address values or instructions (such as add, xor, multiply, load, store, etc). Depending on the processor it will have different instruction sets, which are all binary 0101010101 encodings that are mapped to some instructions, which use logic gates to execute that instruction. This is why an ARM architecture would be inheriently different from a MIPS architecture or some other computer architecture. Because those instructions are encoded differently for their circuit devices. A stack is just some reserved space in memory of those 010101 values, which can either have references or operations held in them. Then when the processor is running these operations, they "cycle" between instructions to execute (which is why you have different clock speeds on processors, say 3.4Ghz, 2.7Ghz, etc). These instructions use a program counter to then go onto the next instruction. If you think about a compiled language, such as C+, it will register into that assembly language. A nuance of this is that C++ can have those one off errors, where it's pointer to an array might go out of bounds to some arbitrary address in memory, but the program will still execute at runtime. Meanwhile, the JVM will catch those out of bounds exception, because the compiler has that built in to check these one-off errors. Think of that step as an optimization step for the compiler or interpreter. I think learning about computer architecture helps to understand what's going on in the internals, and how this debug information (variable bindings) could be referenced on a stack or heap.
Thank you for really deep and detailed explanation. It’s been really helpful.
Removing this because it was posted here a few days ago https://www.reddit.com/r/scala/comments/7x8q0y/literal_types_a_case_study/
This is exactly what I was thinking. Its so much clearer too what is going on. In any case that you would have used a partial function, and you would want an efficient `isDefinedAt`, you can just fail fast with a None return value before you do anything costly.
`for` can do much more too, but the main difference IMO is that the last step in `do` is still `&gt;&gt;=`, while in Scala it's `map`. What exactly is incorrect?
After a year of programming in Scala I decided to dig a bit deeper by reading books about concepts and stuff, and reading docs of every single interesting thing that I stumble upon. During that today I've been reading the style docs and I stumbled upon this ([link](https://docs.scala-lang.org/style/naming-conventions.html)): &gt; Note that the Java getter/setter paradigm was often used to work around a lack of first class support for Properties and bindings. In Scala, there are libraries that support properties and bindings. **The convention is to use an immutable reference to a property class that contains its own getter and setter.** I wonder how the correct use would exactly look here.. how should I define/name the getter and setter? Could someone please show me the simplest possible example of the idea mentioned here? Thank you
sbt -men 512M run
I think `when` is just a restricted `match`. The idea I had is to do away with the distinction between `if` and `when`/`match` completely, and only have one construct. If you have a look at the syntax above, one core idea of it is that it scales seamlessly from a simple if expression to a match expression. E. g. if foo == bar then 1 ... else 5 If the `...` is empty, it looks just like a standard if expression as it exists in every language. But you can add additional cases at `...` and have it act like a pattern match, without any changes to the syntactical structure.
Looks like they should hire a software engineer.
Spotted this on the Interwebs. Sorry, don’t remember where. using() becomes like Try with resources. object Control { def using[A &lt;: {def close() : Unit}, B](resource: A)(f: A =&gt; B): B = try { f(resource) } finally { resource.close() } } def readExampleJSON: String = { val resource = getClass.getResourceAsStream("/example.json") using(Source.fromInputStream(resource)) { source =&gt; source.mkString } } }
Fixed formatting: object Control { def using[A &lt;: {def close() : Unit}, B](resource: A)(f: A =&gt; B): B = try { f(resource) } finally { resource.close() } } def readExampleJSON: String = { val resource = getClass.getResourceAsStream("/example.json") using(Source.fromInputStream(resource)) { source =&gt; source.mkString } } 
I’m currently working on a Scala validator that is based on a value classes mechanism, https://github.com/ma-silva/dingo. I’m open to comments or suggestions to help improve.
I’m currently working on a Scala validator that is based on a value classes mechanism, https://github.com/ma-silva/dingo. I’m open to comments or suggestions to help improve.
I was working releasing [playsonify 1.1.0](https://github.com/AlexITC/playsonify), you can now play with it, [release notes](https://github.com/AlexITC/playsonify/releases/tag/1.1.0).
Autoscout24 | Senior Scala Engineer | Munich, Germany | ONSITE | Full Time It's a great place to work. Good engineering culture and engineers, agile and devops mindset all over the place. Job posting see: https://grnh.se/6tl5x3hj1 For any questions DM me or reply.
&gt;"equational reasoning and allow you to write less buggy code" I do not think you understand the impact pure FP has on your programming techniques. Most imperative programmers think that the strong types result in less bugs and that's why it does not look like I big deal to you. But the true reason FP results in less bugs is because it greatly simplifies your code. You have to deal with less things therefore you make less mistakes. Take for example map fusion in haskell. groupBy thisCriteria . sorBy thisFucntion . map doSomething . filter removeSomething Look at this chain of transforming input data into some result you need. The task is split into smaller simpler steps and the data flows from one step into another. An imperative programmer would never write it like this because he would be afraid that iterating the same list 4 times (!!!) would be too slow. Imperative programmer would try to rewrite it to loop once (for performance reasons), greatly complicating the code and in the process introducing a lot of subtle bugs. It will take him much more time to write it than for a haskell programmer to write that one line. And it will most likely contain bugs that are absent in the haskell version. But how can a haskell programmer ignore performance implications and loop 4 times? Well, because pure functions can be optimized by the compiler much more aggressively than any optimizations imperative compilers could possible utilize. In this case haskell has something called map fusion, where it would rewrite these 4 loops to be one loop, doing a lot for complicated work for you for free. Offloading a lot of mundane and complex work to compiler is the true power FP and category theory abstractions provide. 
Loan pattern. AFAIK originates from Lisps.
Hold it. No. **NO.** Do **not** use `NonFatal` here. Resources must *always* be closed, even on fatal exceptions. Use the [better-files](https://github.com/pathikrit/better-files/) library. It has a correct try-with-resources implementation.
Per https://github.com/akka/akka-http/issues/891 you can adapt one to the other (though you have to use a slight hack to treat a Java route as a Scala route).
You don't even need a custom `orElse`, this is just (a special case of) the kleisli `&gt;=&gt;` operator which some libraries already define.
What exactly to hold? In blog post there is no usage of NonFatal for closing resource. But in general I do agree, closing resource in recovery of Try isn't good whatsoever. Btw, probably it might be a good addition to the blog post.
I don't see that this is something that needs to be part of the core language, which benefits from being small and moving as much as possible out into libraries. What would be good is for a "platform" effort (like the Scala Platform) to recommend a "standard" way of doing resource management (e.g. scala-arm) and ensure that all libraries that were part of the "platform" did resource management the same way.
&gt; There is a proper way of working with resources/exceptions in jvm world. The JVM world is fundamentally flawed, and supports unsafe type coercion and checked exceptions. Are these *good* constructs to you? I imagine you don't use them, as no worthwhile programmer should because they aren't good constructs. We as programmers are forced to work around the flaws of the underlying language/machine and build our own DSL's to handle safe resource management. `try` and `Try` both have [inexcusably broken behavior](https://twitter.com/jdegoes/status/930430610937544704) &gt;This way is not supported by Scala Scala does not have good principles, and has such a rich 3rd party library ecosystem because the developers over at EPFL with Martin Odersky *are not good at what they do*. The collections library and `scalac` + `sbt` are great examples of this. &gt;Regarding functional frameworks, I'm not sure, either there is a way to return multiple exception, then it's fine, or there is a lack of using addSuppressed exception. this shows a misunderstanding of what I wrote. I can make `g` b any function `Throwable =&gt; A`. This means, my function can be something like (t: Throwable) =&gt; t match { case NonFatal(e) =&gt; ... case Runtime(e) =&gt; ... case _ =&gt; ... } and so on. This is not even a terribly 'nice' way of handling things, because it still defers to the broken `try` construct to handle things, which is not principled. Consider the IO monad instead.
I've never see "properties" or "bindings" used in what I'd think of as high-quality Scala. I suspect that documentation is out of date.
Having recently completed most of the Underscore book on Cats 1.0, I thought of asking a similar question - what are the killer features of Cats? I could only find minor conveniences over standard Scala. The answers you received to your question, is exactly what I expected. Because it stands to reason that the pursuit of mathematical purity in software engineering is difficult to explain to developers in concrete terms. And while developers are increasingly happy to use hybrid OO-FP languages, they are not rushing to use languages ( nor libraries ) rooted in the purity of category theory.
That wouldn't play well with the fact that `sealed` traits and classes need to be extended in the same file. Also, in Scala one often defines a lot of very small data classes, which I find are better defined in a unique file rather than spread of many.
This was [posted here 1 month ago](https://www.reddit.com/r/scala/comments/7ses42/top_15_scala_libraries_for_data_science_in_2018/), in the form of a medium article. Someone else also reposted [Literal Types: A Case Study](https://www.reddit.com/r/scala/comments/83veda/literal_types_a_case_study/dvl49h6/?context=0) which got re-hosted on functional.works-hub.com as well. I don't know what's going on, if some people are rearranging their blog posts on one site but let's just post new articles to this sub (at least, ones that haven't been posted here in the last few months).
How does this database compare to [TiKV](https://github.com/pingcap/tikv)? In particular what are the differences between the consensus algorithms (Paxos vs Raft)?
It's not clear from the position description if you provide/support relocation/VISA acquiring.
&gt; You can't tell whether it's static or not just by looking at it, since the syntax is the same either way. We can tell the same about types, constraints and simple code. At this case what we need is just type constraints. I guess you're afraid that after a refactoring you'll loose a sub-constraint. But the only way (I assume) for that if a compile-time expression can't be compile-time anymore. Type constraints will stay static but you can't use traditional bool expressions. &gt; That's the opposite of what you claimed earlier, and not what the documentation says. How so? It says what I've said: it needs static things. For example, you can't use `readLine(stdin) is "abc"` but you can use ` readLine(stdin) is string`. &gt; False. What's false about it? There's no explicit feature for derivation. &gt; I showed both, it's the same code in any case. Maybe the linked project has them both. &gt; i.e. typeclass derivation, as I've been saying the whole time. Show me a simple example where you can infer any class's structure(without modification) with typeclasses. Unless you've some kind of hidden feature in your mind I don't know about, it won't work. Because the ability to iterate over field names and types is metaprogramming. &gt; There's a consensus around not having unmanaged side effects or exceptions, that's enough. There isn't. You can't command every scala programmer to avoid side effects, mutability and exceptions at all cost. In fact, the scala programmers who came from java and use it as a better-java and those who don't use scalaz/cats won't have a problem with the mentioned features.
We have several people from Ukraine and Russia, so this is definitely possible.
&gt; It says what I've said: it needs static things. For example, you can't use readLine(stdin) is "abc" Are you sure? You said it they could "define behaviour", and the documentation says "all of the expressions within the body can be compiled for the tested type" which implies that you can include non-static expressions and it will check whether they compile. &gt; There's no explicit feature for derivation. It's an explicit language feature in e.g. Haskell or Rust. In Scala the current shapeless implementation uses a macro (though there is a compiler feature, "induction heuristics", to improve the performance). &gt; Show me a simple example where you can infer any class's structure(without modification) with typeclasses. Unless you've some kind of hidden feature in your mind I don't know about, it won't work. Because the ability to iterate over field names and types is metaprogramming. Not any class, only `case class`es, by design - you want [shapeless LabelledGeneric](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#generic-representation-of-sealed-families-of-case-classes), the documentation isn't great. It requires metaprogramming or an explicit language feature, agreed - obviously anything that can be implemented as a macro can also be implemented as a language feature - and typeclass derivation is in a sweet spot where you only need one language feature (or one shared macro) to enable a large set of use cases, rather than custom macros in every project - e.g. the library I linked to using this for JSON says "no macros were written during the creation of this library". &gt; There isn't. You can't command every scala programmer to avoid side effects, mutability and exceptions at all cost. In fact, the scala programmers who came from java and use it as a better-java and those who don't use scalaz/cats won't have a problem with the mentioned features. True, but those programmers are unlikely to be the ones writing libraries. Don't get me wrong, it's not ideal. But I've found it to be good enough in practice; even in Haskell or Idris or Rust you have backdoors that allow a library to "break its promises" (`unsafePerformIO` et al), so the real question is whether the language culture and published libraries do that in practice or not. I've found there's an established culture of Scala libraries that don't use those things that cover my needs.
&gt; If it's logically nonsense then it has the same problem what regular, buggy functions have. Exactly - but an important part of dealing with regular buggy functions is that you have typechecking even within the function body, not just on the return type, so if the function outputs "bullshit" you can use the typesystem to help you figure out what went wrong. &gt; It can't be compared to untyped code because buggy templates don't have effects on the runtime. I agree that a compile-time failure is less bad than a runtime failure. But it's comparable to untyped code in that the experience of debugging and fixing it is the same. &gt; Segfault != UB. Segfaults almost always result from UB. E.g. a null pointer deference is UB, as is accessing an array out of bounds (except for 1 past the end). &gt; And that case depends on variance - you can't fully check variance. Which case are you talking about? The one I'm thinking of can't happen if you ban `null` from your codebase (which wartremover has a rule for). &gt; What kind of argument is this? You think that every bug is related to memory safety or what? I think "modern C++" is a no true scotsman argument. &gt; It depends on the UBs of the language - your sentence is only true if the UB is about memory safety. The C/C++ standards impose no requirements on programs that invoke UB, therefore standards-compliant C/C++ compilers can and do compile UB to code that violates memory safety (e.g. by overwriting unrelated memory in your program). &gt; Also, you can write working and bug-free code while using UBs. You only need to know about the behaviour of the UB on your platform Only if you define "platform" to mean compiler, and popular compilers do not offer any guarantees in the presence of UB (certainly neither Clang nor GCC does). GCC compiles unaligned memory accesses on x86 to perform memory corruption in some cases. Both GCC and Clang compile signed integer overflow on x86 to perform memory corruption in some cases. GCC compiles 32-bit shifts of ints on x86 to perform memory corruption in some cases. Both GCC and Clang compile infinite loops to perform memory corruption in some cases. Etc. &gt; You only need to know about the behaviour of the UB on your platform or protect yourself against the edge-cases with regular code. True but much tricker than it looks, since there's no reliable way to tell whether your code could invoke UB in some cases. A standard that told you how to avoid UB entirely would have to be equivalent to a memory-safe language.
&gt; Scala does not have good principles, and has such a rich 3rd party library ecosystem because the developers over at EPFL with Martin Odersky are not good at what they do. The collections library and scalac + sbt are great examples of this. Was this *really* necessary...?
Calling people names like that is a bit on the nose, and your statement is a simplification. It implies that the Scala standard library is insufficient *only* due to the incompetence of its core developers. It's more nuanced than that. There is a conscious design element to it. Disagreeing with that design makes your ad hominem attacks look rather silly.
Okay
&gt; ad hominem There's no ad hominem, it's an assertion about their skill, no pejorative was used, and the assertion was justified with example, even if they the examples are loaded with preconceived opinions on how principled they are.
I'm not super familiar with the internals of TiKV, but I can speak about the differences between Beaker's consensus algorithm and RAFT. First, RAFT has a leader and Beaker does not. That means that multiple non-conflicting transactions can be simultaneously committed by different members of the cluster, and that expensive elections do not have to be held whenever the leader fails. Furthermore, the leader quickly becomes a performance bottleneck in RAFT. Second, Beaker has a more elegant recovery process. Recovery is directly integrated into its consensus protocol; no additional work is performed to keep replicas consistent. Third, RAFT is partition tolerant and Beaker (currently) is not. This is a theoretical limitation of Beaker, but practically speaking network partitions are rare events.
Given that you're posting this as an example, can you make it more idiomatic? At a minimum, attempt to remove the usage of null and var. Scala also provides a Try class that may be useful for what you're attempting.
Are you saying that adding the doProcessing stage is what causes your execution time to go up? Or the objectScanner? What does `doProcessing` actually do? You could try doing it in a future with .mapAsync
I included that additional stage just to show the intentions of the graph. Sorry for the confusion. I'm seeing the major increase in processing time by adding the `.via(JsonFraming.objectScanner(512))` stage. Which honestly doesn't make much sense to me based on what I understand of akka-streams. To my knowledge the JsonFraming should just be hammering through those ByteStrings turning them into Json records. Unless I'm missing something with the configuration which should all be standard akka stuff... Any idea what might be the reason?
Is 512 the buffer? Try a larger number maybe?
This is currently what is I'm using, but that still necessitates two interfaces, one for Java and Scala, whereas I'd love to just have one. Thanks for your response!
Yep, having a few people jump in and support changes seems to be almost mandatory to get anything done. It was the same story with the Jekyll upgrade a while ago.
Instrumental | Software Engineer | San Francisco Bay Area (Los Altos), CA, USA | ONSITE | Full Time Instrumental makes intelligent, data-driven tools to help companies identify issues on their assembly lines so they can ship higher quality products faster. We are working to improve the manufacturing of millions of things each day. We're growing our Software Engineering team to increase development of our core application so we can provide the most value possible for our users. While we do not ask for familiarity with the following, our stack includes a main Scala application, AWS hosting, a Python machine learning cluster, and TypeScript on our front-end. Instrumental Engineering values good tools, robust tests, and frequent deploys. We're a small but mighty team that consistently works collaboratively, are supportive of each other, and are all highly energized by the opportunity for such a large impact. We actively work to promote an inclusive environment. contact information jordan@instrumental.com
You can make the interface define a Java route (or a Scala route), and have the implementation use the adapter, no?
I've been using Gradle for my current project and I really like it, but I've been getting grumpy about the build times. Would switching to a build tool that supports keeping the compiler warm and Zinc 1.0's incremental compilation help much? Since I'm looking for something that's production-ready, I think the options for that would be SBT, Maven, and Pants. I'm a bit nervous about switching to SBT: I have bad memories of the last time I used it, about 5 years ago. And I don't really remember how it works. I've read that people just use templates for their builds. I'm not really comfortable with templates and not knowing how my build tool works. But I'm willing to put in some reading if this is really the best solution. I actually don't know Maven that well either. I think I last wrote a simple project even further back, but I remember it being somewhat tedious, but well-documented and straightforward. I probably won't go for Maven, but I'd like a second opinion. Pants seems promising, but I'm not sure the monorepo approach makes sense for my current project.
I would always see akka actors as a last resort, since they mean giving up type safety. If you can solve your problem without them - and it sounds like you can - then it's better to avoid them. Generally you'd only use actors if you needed to also have shared state that was entangled with the async operations - that's where they tend to shine (though personally even then I'd look to other alternatives).
&gt; I would always see akka actors as a last resort, since they mean giving up type safety. Agreed 100%. Make very, very sure you really need Akka before using it. In my experience, the vast majority of the time, you don't, and the tradeoffs aren't worth it.
&gt; though personally even then I'd look to other alternatives Anything in particular? Maybe https://nbronson.github.io/scala-stm/ ?
Hmm, I'm curious to know what alternative or alternatives would you suggest instead? Thanks for any info.
&gt; Anything in particular? Maybe https://nbronson.github.io/scala-stm/ ? I tend to use an actor-like implementation but done "by hand": a service class that holds the shared state, protected by a conventional mutex, and processes command objects (free monad). Possibly using fs2 to assemble pipelines.
Are you sure you can generalize over the whole of Akka? I am new to Scala as well and haven't used actors directly, but currently I am using Akka streams, and so far I like them very much. 
Interesting, I've always thought the opposite, we use play + akka and anything async we push to actors (that are usually composed of a bunch of other calls to actors). I haven't run into any major issues with it (fewer than in places where people were explicitly doing thread management themselves)
Fair enough. I was talking about actors. Streams have a different - and type safe! - API.
`IO`, and a well-defined algebra can subsume anything you'd like to do in Akka, without sacrificing monadicity or purity. Type actors have been experimental for close to a decade. It's very clear that they're not going to be a thing.
Thanks, that is a good read. 
'a well-defined algebra can subsume anything you'd like to do in Akka, without sacrificing monadicity or purity' sounds pretty pretentious ... 
I've just updated post with this: https://medium.com/@dkomanov/scala-try-with-resources-735baad0fd7d#0f9a. I found looks like idiomatic example which used NonFatal extractor incorrectly. Actually, I doubt that it's possible to get rid of var/null, because of addSuppressed exception... We need somehow to pass reference to finally block from catch block.
Don't get too caught up with actors. They are good for things like asynchronously managing state but can otherwise add a lot of overhead especially due to their lack of typesafety. It can also be easy to fall into the trap of using actors for everything which can lead to an overly complex system with very deep hierarchies. I would highly recommend you read the book "Functional and Reactive Domain Modeling" by Debashish Gosh. In it, he walks through real-world use cases for modeling your domains which I believe are similar to the sort of knowledge you're looking. For me, this book gave me the "aha!" moment that helped me understand how to take a functional and reactive approach to structuring applications.
With an http API and multiple instances of the app.
&gt; microservice-like backend with actors and akka-http but I kept stepping on my own toes with the asynchronous nature of it all. It seems like Lagom would be more your use-case for microservices. There's an interview that talks about synchronous vs asynchronous REST https://thenewstack.io/synchronous-rest-turns-microservices-back-monoliths/ and much of the theory behind Lagom is based on Pat Helland's [Data on the Inside vs Data on the Outside](https://blog.acolyer.org/2016/09/13/data-on-the-outside-versus-data-on-the-inside/), which is very relevant to microservices.
Thanks a lot! Ill give that book a look!
Ok interesting. Will read those tomorrow, thank you very much!
How is that at all pretentious?
What's your solution for distributed state? I'm still not sure how to handle it when sticking with pure FP style.
Check here https://github.com/ornicar/lila https://lichess.org . It uses akka, but I can't remember purpose
This is just a rehash from examples on Scala with Cats, and FP Simplified, which I personally find easier to understand.
Ah, but bidirectional communications is exactly what I was looking for a way to model. Is there a purely functional construct that can help there? Distributed consensus seems inherently very stateful as well. I suppose Mvar could work there though.
&gt; Exactly - but an important part of dealing with regular buggy functions is that you have typechecking even within the function body, not just on the return type, so if the function outputs "bullshit" you can use the typesystem to help you figure out what went wrong. Templates generate whole code which means you won't just get a result but proper code. The thing the template generated will be typechecked. &gt; I agree that a compile-time failure is less bad than a runtime failure. But it's comparable to untyped code in that the experience of debugging and fixing it is the same. I disagree. Fixing typing issues in templates is exactly like fixing regular type errors. &gt; Segfaults **almost** always result from UB. E.g. a null pointer deference is UB, as is accessing an array out of bounds (except for 1 past the end). ... &gt; Which case are you talking about? [This one](https://raw.githubusercontent.com/namin/unsound/master/doc/unsound-oopsla16.pdf) and [this one](https://issues.scala-lang.org/browse/SI-9912). &gt; The one I'm thinking of can't happen if you ban null from your codebase (which wartremover has a rule for). If you ban null then you won't be able to use any java API properly anymore. And it'll only solve the first case. Also, I feel like you can use similar values to null too. &gt; I think "modern C++" is a no true scotsman argument. No, it's not. We know *exactly* what's modern C++. You're trying to make it look like it's impossible to code in C++ safely which is not true. &gt; The C/C++ standards impose no requirements on programs that invoke UB, therefore standards-compliant C/C++ compilers can and do compile UB to code that violates memory safety (e.g. by overwriting unrelated memory in your program). And this is not a counter-argument. &gt; Only if you define "platform" to mean compiler, Why? An OS is a smaller target. &gt; and popular compilers do not offer any guarantees in the presence of UB I wasn't talking about guarantees. Stop redirecting the argument. &gt; True but much tricker than it looks, since there's no reliable way to tell whether your code could invoke UB in some cases. UBs are documented. &gt; A standard that told you how to avoid UB entirely would have to be equivalent to a memory-safe language. A guide is enough. I'm not sure what you're trying to express here. There's no need to invent a language just to be memory-safe.
You did math words!
[OH NO](https://www.youtube.com/watch?v=b2hTgSIuFZ4)
Great. So now a user uploaded a spreadsheet with 100k images to resize and upload to s3. You are destroying one webserver for 90 seconds to do process it?
You're on your way! 
I'm excited about Akka Typed, but I still think that actors are too general of a tool for most people's problems.
The company I'm currently working for is hiring Scala Engineers and Data Engineers. Particularly in my team, we are using `Http4s`, `Fs2` and `Cats Effect` among others, writing pure FP code. In other team guys are using `Eff`, `Cats Effect` and `Akka` as well. And in general we follow a `tagless final` design. Last but not least, the company is located in Tokyo, Japan, so you must be able to relocate. The company offers a relocation package. Take a look at the engineering site: https://engineering.paidy.com/ and please contact me if you think you could be a good fit :)
Synchronicity wastes a thread. Ideally, nothing should be synchronous. And also, ideally, all resource accesses should be submitted as early as possible, in a scatter-gather fashion. Synchronous code doesn’t scale and is a poor fit to the asynchronous reality. 
Hey, there is akka-typed which is awesome compared to regular akka. I don't even use untyped akka anymore :)
What kind of problems are you solving? Data pipelines? Web/mobile API? Starting with the technology and then looking for a problem makes simple things complex. 
I was just trying a simple web api. I realize that Im doing it backwards, but it was just an experiment on my end. Now my question was, whats a simple usecase that actually shows the strengths. Even books use things like webcrawlers that dont handle states and could just as easily work with futures.
No, no, I wont try to introduce akka to my company if the project isnt a perfect fit haha! I dont think I would recognize that right now though! Im just talking my own experimentation with the language and now different frameworks. So is there no everyday use for actors? That makes me sad, since I like the principles a lot (elasticity, resilience and so on).
 Yes.
A simple web API is a really good way to focus on the language. My path of coming to prefer Scala started with immutable state (Val's and collections), using the collection library instread of loops, avoiding type unsafe language features (http://www.lihaoyi.com/post/StrategicScalaStylePracticalTypeSafety.html), and using AnyVals in place of loosly typed strings and numbers. Each of those things eliminates entire categories of bugs: null pointers, loop counters, parameters out of order, etc. I find myself relating to the Type Tetris described in https://underscore.io/blog/posts/2017/04/11/type-tetris.html Even without being a expert, I trust the code I write in Scala more than other languages and it's easier to refactor later.
Source of inspiration Julien R. Foy : https://www.youtube.com/watch?v=6j5kZj17aUw I personally think this is a clearer style of FP for OOP guys like me than the original Type Class. F-Poop
You're not wrong, Walter, you're just an asshole.
Personally I'd focus on getting good incremental compilation in your IDE, because that's the part that affects you day-to-day, but I know some people have different workflows. Zinc might help with your command-line builds. My experience of SBT is the same as yours; I'd add that it's gone through multiple compatibility-breaking changes so many of the tutorials or templates you find no longer work. I'm a huge fan of Maven; "somewhat tedious, but well-documented and straightforward" sounds like a fair assessment (and it has better IDE support than anything else IME). Where I've seen people go wrong with Maven is in trying to fight its defaults or expecting to be able to "configure" it into doing something custom. If you want a custom build step, you should find or write a plugin for it, and when it comes to things like project layout or release tag format you have to be willing to live with the defaults that Maven gives you. &gt; Pants seems promising, but I'm not sure the monorepo approach makes sense for a small team with only a couple projects. To the extent that monorepos make sense at all, I'd think they'd make more sense for a small number of projects than a large one. My concerns with pants would be how many people are using it outside twitter, how much documentation is available, and side things like IDE integration, whether CI tools support it, whether there's a plugin available that implements some other step you need in your build, etc.
My example is to simplistic, I’ll summon Bill Venners and Odersky to explain https://www.artima.com/weblogs/viewpost.jsp?thread=270195 I think also video from Julien explains when you are passing more types params, you ended up having less familiar construct sprinkled with more type params. I need to digest better and find a better illustration about what makes me feel ATM is more familiar to me, sorry if I cant clarify myself this intuition
Thanks a lot for this sophisticated response! Ill look at using akka-http without actors for the moment! From the replies here I noticed, that I am still really confined in my way of thinking about apis and backend. **This community is awesome!** Already so many cool things suggested!
Why should the compiler give you a warning? The Seq trait is not sealed so who knows what you can stick there, List also extends Seq so the match makes sense from the point of view of the compiler.
&gt; People use it for parsers, No metaprogramming or typeclass is needed for that. &gt; database mappers, That's still serialization. &gt; comparators/differs, What and how? &gt; traversal-like functions... any kind of "walk the object graph" use case can use LabelledGeneric-based typeclass derivation rather than writing a custom macro. Do you think that macro users will write a custom macro for every specific case? Btw, if the traversal part is there for a macro than use can use them too. I'm not sure what shapeless brings here. &gt; I think the shapeless macros are the rare macro use case that should be promoted to a language feature To make the language more bloated... Don't. &gt; (typeclass derivation is already a language-level feature in e.g. Haskell or Rust). What kind of typeclass derivation are you talking about? &gt; Shapeless' author has already ended up making PRs to the compiler to improve support for the things that Shapeless uses, It's a bad sign in a language: a framework's authors modify and make the language bloated just to avoid simple macros. &gt; so I don't think there's much value in having them as macros rather than as language features. I don't think there's much value in typeclasses or in shapeless either. Macros are safe and their code are much easier to read and they don't make the language more bloated than it should be. &gt; I haven't seen much or any use of outright mutability. You're probably a webdev then. One of the weaknesses of the scala ecosystem is that there are a lot of libraries which pursue purism which makes their performance pretty bad. &gt; but I think there's a fairly established norm that library methods shouldn't mutate hidden state, shouldn't throw exceptions, and shouldn't return null, which is enough to make those libraries usable from purist code. Those rules will only be present if you work with very simple libraries which just contain simple functions. &gt; I'd very much like stronger guarantees, but the community consensus is if anything even more important (even Haskell's guarantees would be useless if the library community believed in using unsafePerformIO all the time). The "community consensus" won't save your from your coworkers and your mistakes.
The comment reads "multiple instances" not "destroy one webserver for 90 seconds". Down doot. 
I'm not a huge fan of actors. One of their big benefits is the supervisor, which restarts actors if they crash. Well, microservices today typically run in some sort of environment (Kubernetes, ECS) that monitors container health and restarts them for you. So just let the service crash and your deploy environment restart it. Another is the asynchronous calls. Again, not really important in microservices. Can it add performance? Certainly. Will you spend more time writing your code than just adding proper concurrency as needed? Almost certainly. Keep things simple. Just write your code, and if it would benefit significantly from a concurrency boost, do it. But chances are You Ain't Gonna Need It. As for Akka Http, write a service in Go using any of the many available muxxers, then tell me that Akka Http is even close to a good idea. 
&gt; Sure, the point is that when a template's output is wrong, it's harder to find where in the template code the error is Nope, the error will tell what it was expanded to. &gt; - it could be in an early stage or a late stage, because wrong results can propagate through the template logic, because the intermediate expansion isn't typechecked, only the end result. That's not a problem. Templates are not macros just typesafe and hygienic substitutions. &gt; I think I was being overly cautious: segfaults in C programs are always a result of UB. Certainly I can't think of any segfault I've seen that wasn't due to UB. Cast to an invalid type. &gt; First link is the one I was thinking of, it requires null. Yes, it requires null, just like non-trivial scala code. &gt; Second link is a compiler bug rather than an unsound type system design - per the comments the code shouldn't compile according to the language specification. The bug was reported almost 2 years ago and won't be fixed for 2.12. &gt; There are native-to-Scala libraries for most things one wants to do these days, Not an argument which supports you. &gt; and even in Java, most modern APIs shouldn't require null. Optional is not famous between java devs and null is still there. Btw, niche libraries are the reason of the jvm's victory over .net - if you throw away old code you'll just loose ecosystem. &gt; I've found it practical to ban it. Of course, you're a purist who don't care about performance and you don't need to use low-level and niche libraries. &gt; If there are a small number of cases where it's needed then one can suppress the warning in those cases and subject them to extra scrutiny in code review etc. We can subject our C/C++ code for extra code review too. &gt; , like an unsafe block or an unsafePerformIO, but I can't say I've found myself even needing that much tbh. Another anecdotal evidence. &gt; You can't - the trick only works with null because it's considered to be of any type, even types that should be impossible to have values of We can create such values. Plus we've implicit conversion too... &gt; Where? Where's the exact spec, where's a linter that checks it? What linter? There's cppcheck what you can use to lint. What spec do you want? We know about the modern and most important features of C++. Modern C++ is not a language - it's just about using the modern tools. If you want to check it then "subject them to extra scrutiny in code review". C++ isn't for novice webdevs who ask questions like "what is pointer :S". &gt; A language that permits undefined behaviour in general code cannot be considered memory-safe, because in such a language general code can perform invalid memory accesses (by performing undefined behaviour). Your argument wasn't about what we can "consider for every case". It's possible to be memory-safe in C/C++ and not all the bugs are coming from UB. If you don't agree then you're naive. &gt; You can't possibly know what UB will do just based on the OS. LoL you can just test it. &gt; You said "you can write working and bug-free code while using UBs". I don't think the code can be considered bug-free C code if compiling it with a standards-compliant C compiler will produce buggy behaviour. Now, that's a very naive thing to say. The C compiler won't "magically" insert bugs into your code. You write code and if you invoke an UB then it's your problem - UBs are documented. &gt; Sure, but it's very hard for a reviewer reading the code to tell whether there are cases where UB could happen. a + b could be UB. x &lt;&lt; 32 could be UB. *x could be UB. while(f()) { g(); } could be UB. Remember the code where the scala code worked differently with the same object and the same calls but with different parameter types in the two functions? &gt; There's no reliable checker, only difficult manual reasoning (since the type system doesn't distinguish between nullable and non-nullable pointers, or between integers bound below a certain size or not, or ...) and a bunch of tools that detect some UB but not all. \&gt;*Perfection is the enemy of good. &gt; But the guide doesn't exist either. There are many guides for safe C++. There are checks for reviewers too. &gt; People think a safe subset of C/C++ is easy until they try to actually formalise it, then it turns out to be impossible. You're not the one who'll tell what's possible and not. As I've mentioned, my company has this 7MLoC+ C/C++ codebase. We're not bothered by segfaults and core dumps. Memory leaks are the real problem. There's an interesting thing which is against your narrative: gamedev. Game developers have a complex job and tight deadlines but most of them can deliver crash-free experience. Most of my games never crashed for me. I've 130+ games on steam and I've experienced crashes with only 3: one which consumes a lot of VRAM by design and crashed due to a video driver regression - VRAM leaking, one which runs on the wine compatibility layer - and wine doesn't support every windows call, and one huge open-world game which leaks so much VRAM and RAM that it can crash pretty fast if you run other things in parallel.
&gt; What and how? Comparators for equality comparison and sorting, differs for showing differences (e.g. used in test assertions) - once you have the ability to view `case class`es as records the rest is just ordinary (recursive) implicit resolution. &gt; Do you think that macro users will write a custom macro for every specific case? If you're saying macros should be a language feature then you're presumably saying there are a lot of different use cases for macros. If we were only going to have two or three libraries that use macros in the ecosystem (and actually there's only one in the Scala ecosystem that I think is worthwhile, shapeless), it would be better to do something specific for those libraries rather than have to support custom macros in every codebase. &gt; Btw, if the traversal part is there for a macro than use can use them too. I don't know what this is supposed to mean. &gt; To make the language more bloated... Don't. I find they're widely used to be worth having as a language feature. Virtually every project I work on ends up depending on shapeless, so it's no more bloat than if that functionality was built into the language in the first place. &gt; What kind of typeclass derivation are you talking about? [`deriving` in Haskell](https://www.haskell.org/onlinereport/derived.html#derived-appendix), [`#[derive]` in rust](https://rustbyexample.com/trait/derive.html). &gt; It's a bad sign in a language: a framework's authors modify and make the language bloated just to avoid simple macros. It's a bad sign for macros: a library's author tries to implement the functionality they need with macros, but ultimately resorts to modifying the language. &gt; I don't think there's much value in typeclasses or in shapeless either. What's the value you think there is in macros? I've talked about my use cases, what are the use cases you have for macros that aren't covered by typeclasses and shapeless? &gt; Macros are safe and their code are much easier to read Good macros make code easier to read, bad macros make it impossible. The risk:reward ratio is too high. &gt; they don't make the language more bloated than it should be. Macros are worse for language maintainability. Individual macros impose the same maintenance costs on code as language features, because at the point of use there's no difference between the two. And having a macro API makes the language itself harder to evolve since it makes the AST part of the language's API. &gt; Those rules will only be present if you work with very simple libraries which just contain simple functions. Not my experience. At least, I've been able to find libraries that do the things I need without violating those rules. &gt; The "community consensus" won't save your from your coworkers and your mistakes. For that there's wartremover. Ban `null`, ban `throw`, ban `var` or at least flag it for extra code attention in code review, and then it's like working in a pure language.
Actually the JVM is more or well suited to handing actors in the same way the BEAM (the VM behind Erlang/Elixir) does. JVM allows hot reloading/patching (in a similar way that beam does) and from a performance perspective, akka actors are on par with what you get on Erlang. Interestingly there is an added bonus where the performance for JVM for local concurrent code (i.e. code that uses multiple cores) is **much** faster than BEAM. All of the juicy stuff that people use actors for on Erlang/Elixir is also available with akka actors on the JVM (i.e. recovery, hierarchical error handling, location transparency of actors). My final point is, that if you hate actors on JVM/Akka, you would also hate it on Erlang/Elixir. The style of programming is exactly the same, the abstraction model is exactly the same, and the way you treat errors is exactly the same (okay maybe not exactly, Scala/Java has some static typing where as Elixir/Erlang has none, so if you are coming from a statically typed environment its actually worse in this regard). The only reason Erlang/Elixir may be more approachable for this style of programming is because you don't really have a choice there.
So you think everyone uses for such common scenario actors? Oh wow. Ignorance is a bliss.
Compile time dependency injection via implicits is something I like very much.
I'm in a situation similar to yours, and I still struggle with this. The introduction of Scala was done about eight years ago at my company. The manager who introduced it moved up in the company and away from the Scala projects. I'm experienced with the language, but not on the level of the using fancy functional concepts like coyoneda or coproducts, and not so much on the type system programming either. But I am familiar with nearly all of the language features. There's still plenty of features that Java 8 doesn't have: * implicits -- allow for the pimp my library pattern, and conversions of similar types without a lot of boiler plate. I've used them to unify date APIs. * implicit parameters -- allow you to stop (explicitly) passing around those "context" objects to all your classes and methods * case classes -- an immutable data object with equals and hashCode and copy implemented for you, all in one line of code * pattern matching -- way more powerful than anything you ever see in a Java switch statement. Match, cast, extract and assign on the same line of code. * stronger types -- hard to argue for unless both parties have a good understanding of this stuff. But the type system in Scala is stronger than Java's will likely ever be. * better type inference -- hurry up, I hear this is coming in Java 10 with "var" * immutable by default -- Scala encourages immutability by default. Immutability will eliminate a bunch of bugs, and allows for easier concurrency * defaulted arguements -- stop overloading to provide default values, specify them in the function definition * named arguements -- if you wanna skip specifying a few parameters you can just call out the ones you're passing in. Eliminates most use cases of the builder pattern. * multiple inheritance -- trait composition essentially allows for multiple inheritance Even considering Java 8 streams, the syntax is still awful, and using them can be a bit clunky, especially when compared to Scala's. Lambda syntax is still also kinda ugly. It's not ever going to be as nice as Scala's. Scala is still hands down a better language, but convincing Java Programmers of that is going to be pretty hard. In my experience most of these programmers just want to stick with what they know, get their job done, then move on to the next task, get paid, repeat. No time for learning new things. You'll have to become a Scala evangelist, and even then there's a good chance you'll just turn into that "Scala guy". But also look at the companies point of view. When hiring, the pool for Java candidates is way bigger than Scala. So using something not many know will mean more on-the-job training (which seems like waste to those business people who don't know the difference between Java and Javascript). You've got a lot of barriers to overcome. I have been trying to overcome these for years, but it's hard as one man in a Java shop to push for change. Maybe you can find an ally in this fight. I've managed to show a few other engineers the light, and I hope one day there will be enough of us to make a better push for a change. 
For me the compelling case for Scala is that it offers safe replacements for reflection and AOP. In Scala you can do "walk the object graph" things like JSON serialisation with compile-time type safety (via shapeless-style typeclass derivation), whereas in Java you have to use unsafe reflection. And you can do things like `@Transactional` in a type-safe way (particularly helpful when it comes to unit testing - you can immediately see which methods need access to a database and which don't) by using HKT and custom monads; Treelog is another good, simpler example of the kind of thing that you'd have to use a "magic" interceptor to do in Java. Since reflection and AOP were two of the biggest sources of production bugs when I worked in Java, I find it's well worth being able to replace them with "plain old code", even when that code is a bit uglier than the "magic" solution. The other advantage I've found is that the language is flexible enough to replace config files with code - e.g. the rho or akka-http routing DSLs let you write Scala instead of a routing definition file, Slick database mappings are defined in code rather than an external file, Scala makes it practical to "wire up" your application by hand without needing a configured, reflection-based engine like Spring, there's a rules DSL somewhere that can replace using Drools with external rules config files... Misapplied changes to config files, or forgetting a config file when refactoring, are again big sources of production bugs in my experience.
In Scala functional features are embedded into language and it shows everywhere: in standard library, ecosystem and recommended practices. In Java AFAIR maps allow `Null`s, monads have kind of clumsy interface, that you can only access once you write `.stream()`, immutable data structures are second class citizens, because everybody and their mother uses mutable ArrayLists, Maps and Sets. (And immutable data in e.g. Guava had the same interface as mutable, but they were throwing exceptions on mutable ops...). That's why Java's 8 new features are not so convincing to me. Besides Scala still have much more to offer: * type classes and type class derivation (Java won't let you achieve sth like Circe in compile-time) * better type system (type bounds, variances, higher-kinded types) * ADT * macros If you want a good example of Java vs Scala I think implementing something with Akka HTTP and Akka Stream and comparing the implementations would show how much troublesome is Java when one needs to obtain the same power of expression.
Not to be a downer but for a senior in SF in the finance industry your starting salary is grossly lower than market rate. 
Just one example: print multiples of 3 from 0 to 30. Scala: (0 to 30 by 3).foreach(println) Java 8: IntStream.range(0,30).filter(n-&gt;n%3==0).forEach(System.out::println);
You should use technology that the team, on average, can be effective. That being said, imho, pattern matching by itself is worth using Scala over Java 8. 
Beyond safety and concision, I love Scala because it allows me to *model* problems in better ways than with most if not all mainstream languages. I can use a panoply of tools such as case classes, objects, pure functions, ADTs, smart constructors, traits, extension methods, typeclasses, and more, yet all of these features are overall quite neatly integrated together, some in fact building on others. Problem domains can be complex, and too often languages lack the tools to express them. You cannot reach perfection in modeling, but Scala gets you quite a few steps ahead. 
Scala is my preferred language (I use it with Spark), but it is hard to justify its use on a team. See: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.261.3185&amp;rep=rep1&amp;type=pdf Also (disclosure: I wrote this "Criticism" subsection of three sentences of the article): https://en.wikipedia.org/wiki/Scala_(programming_language)#Criticism
Anyone have heard of this book ? any thought on whether it's a "good" starting point to learn machine learning through scala ?
Where can I learn more about this? 
No idea about future plans but I have been using it for a year or more and did not notice any changes.
Might wanna read this article http://di-in-scala.github.io/
People often need to understand there is a problem before they can understand the motivation for the solution. One tactic you might try is to find pieces of your Java codebase that are rather verbose, or complicated because Java doesn't have a way to express it cleanly/simply (have any wildcard bounds, for instance?), or doesn't quite do what you want, or polluted with null checks, or caused an issue in production, or any other real issue you've had, and then show how Scala can help solve, prevent, or otherwise improve that situation.
Why do you feel that study indicates it is hard to justify Scala? The developers in that study averaged 4 years of Java experience vs 4 weeks of Scala experience. The outcomes don't seem very surprising based on that alone.
case classes + pattern matching + type inference is probably already a massive step up from java for people who don't care about functional programming.
lmao, TIL there's people being paid for dissing Scala
Yeah, that's honestly a pretty garbage comparison. I've been using scala for 2-3 years and even now, I feel like I keep learning more. I don't think I could code nearly as correctly or as fast in Java (though I've used Scala longer than I've used Java).
MEAP is dog shit.. books are stuck there for years ... like this one. 
* no null * safe * concise * productive * Doesn't need brittle, unmaintainable codegens for basic functionality (Lombok etc) * Doesn't need brittle. unmaintable runtime reflection and bytecode generation for basic functionality (AOP, Spring etc) * Never have to deal with runtime reflection in general * best OOP support in any language currently, traits are far, far more expressive than anything Java can offer * world-class FP support with implicits, for comprehensions &amp; macro materializers * higher-qualified and more passionate candidates 
This is borderline incoherent. You don't need a dozen paragraphs to explain "Parenthesis are optional parameter lists with a single parameter; this is often used with block expressions". I'm not sure if you understand still even after reading your conclusion. take this example: class Foo { def bar(f: Foo): Foo = f } val foo: Foo = new Foo val baz: Foo = new Foo val qux: Foo = foo bar baz How would you interpret the last line of this expression? In the scala language specification this form of method invocation is called Infix Notation.
Thanks to /u/alexelcu
&gt;Another is the asynchronous calls. Again, not really important in microservices Wouldn't this be especially important in mircroservices? If they are synchronous, wouldn't many of the microservices spend most of their time blocking threads and waiting for responses from other microservices? &gt; As for Akka Http, write a service in Go using any of the many available muxxers, then tell me that Akka Http is even close to a good idea. I don't understand this. Aren't Go muxxers (by which I assume you mean something [like this](https://github.com/inconshreveable/muxado)) something completely different than Akka Http? Wouldn't the Go equivalent roughly [be this](https://golang.org/pkg/net/http/#Server)? If an equivalent exists in Go then then what makes Akka Http such a bad idea?
&gt;Everyone I work with has either recently graduated or still studying and all they have ever known is Java. Our manager is conscious of this and is of the opinion that Java 8 introduces functional programming in Java so we should phase out our use of Scala. I get the feeling that the company just cannot afford to hire senior developers (or does not get the value..) and your manager wants to avoid Scala to keep labour costs down. I'm curious: where are you based ? 
Why are you fighting for the language? If there aren't features that you absolutely need in Scala that aren't present in Java 8, that should be obvious. Otherwise, it sounds like a better organizational decision to write new code in Java.
The java version will, I think, be faster, because of unboxing. A better example is anything involving grouping on a java 8 stream. Try it. You have to write your own spliterators to group.
Maybe I should provide a link that this was continued part of my attempt to learn typeclass : https://gist.github.com/wibisono/28d8808159e0c7103042d7c5c668edf7 Shared here just in case there are others who are learning about it also. Not for those who are expert.
&gt;It goes without saying though, that you can overdo this. And that there is a limit to how far this way of interpreting code in Scala can be stretched That is from the post. If you have a function `def foo(x:Int):String`, that is a function that can take an expression that evaluates to `Int`, takes it and does stuff to it and return a `String`. The effect of `foo` on an Int value is to transform it to a value of `String`. A more formal term would be to say that `foo` function was *applied* to the Int value. This is basically the same thing the post is saying. But instead of using `applying`, `annotate` was used. I personally have found the syntax for passing multiline expressions to a function confusing for a while now because of its use of {}. ``` foo() { val a = "Hello" val b = "world" s"$a $b" } ``` That is because I have long associated `{}` with defining things, like method, class etc. While `()` I associate with function invocation. So seeing `{}` at a place where I expected a `()` felt disorienting. I find that if I wrap `{}` with `()`, reading becomes easier. For example: ``` foo() ({ val a = "Hello" val b = "world" s"$a $b" }) ``` But this is generally not a style I see used in the wild. So instead of reading this as I would normally do, ie `foo` is a function being called with one empty parameter and a multiline expression... I now see it as having a multiline expression that evaluates to a value and `foo()` would be applied to that value, yielding the final result.
He's done amazing work on cats-effect, yes :-) The progression of features in the last 2 months or so has really been amazing to watch. And also huge thanks to https://github.com/gvolpe (not sure if that user is the same on reddit), who created the microsite.
Muxxers are completely different. Thus, I hate akka http. It is clunky, doesn't lent itself to abstraction or reuse, and is super nested. I despise it. As for async in microservices, concurrency is nice, but a tiny service shouldn't be doing a ton to get it's work done. Most microservices need to call database oray even two other services. If those calls must be parallelized, then sure, use akka actors. But you'll find that the bottlenecks in you architecture are not those calls. They are the bad algorithms, big database read/writes, data transforms, or any number of other things. There will be times to put in the effort to add concurrency to microservices, but the best time is *not yet*. So ignore actors until you are desperate for them. Then add them only where needed.
I am working on library for processing Time Series: https://github.com/carldata/timeseries
It sounds like you have a lot of turnover at your company, if you need to be able to replace people easily with "commodity developers" then you should just stick to Java. However you might also want to look at Kotlin, it gets you about 80% of the advantages of Scala while remaining much more approachable to the average Java developer. Plus Google has thrown some weight behind it by officially supporting it for Android development.
Worth pointing out that it doesn’t have to be an either/or. 
Appreciate the feedback! On our [AngelList](https://angel.co/leapfin/jobs/150204-senior-software-engineer) page we have listed $120k-160k. Does this seem more reasonable to you? We are flexible with salary and it totally depends on the candidate. Naturally, as a startup, equity is also a huge influencer and we are flexible in that area as well.
&gt; Muxxers are completely different. Thus, I hate akka http. If they don't do any the same thing then why would liking one make you hate the other? This feel like you are saying something like I like cars therefore I hate pineapples. &gt; It is clunky, doesn't lent itself to abstraction or reuse, and is super nested. I despise it. I haven't done much work with either. What about Akka Http makes it hard to reuse? What do you mean by 'super nested'? &gt; There will be times to put in the effort to add concurrency to microservices, but the best time is not yet. So ignore actors until you are desperate for them. Then add them only where needed. I feel like you equating async with actors. I agree that actors are not the place to start. However, async can be done much easier with futures. I think this is a good place to start since it isn't much more difficult than writing everything synchronously and it has the added benefit of separating side effecting code. 
But does the OP (or anyone really) really want to work for such a company where they constantly want "commodity developers" and don't hire senior people ? 
Sometimes, I briefly pause to puzzle over questions of this nature, and then I move on. 
Cqrs, distributed computing, things that people love/do with the otp Erlang framework. I just find that a lot of problems I have can be solved in other ways and then use horizontal scaling. So if you are excited about it, go for it. Another slight concern for how I understand your situation, is your team learns a new language and paradigm (which is vey deep) with Akka. Lagom as suggested by someone else seems good but don’t have any real experience with it.
Usually I do, too, but given the depth of Scala I'm really trying to understand it as clearly as possible from the get-go so that I have fewer problems down the line when I'm trying to understand advanced concepts.
You may want to watch Kevin Webber's talk about his transition from Java to Scala, and how to satisfy all stakeholders involved on the decision: https://vimeo.com/217843501 
It's cool, it was a really shitty/low effort comment. Your question is worthwhile. I was mostly poking fun at myself for being lazy about these kinds of questions.
Very cool! As a suggestion, it could use more of an explanation for what it does and how it's used.
This should be the top comment 
I don't _absolutely need_ to never see a NullPointerException again by using Scala but it sure feels nice and saves my company money
Not gonna lie, this sounds like a meme. Just needs `Kubernets Cloud BlockChain` added to the beginning of it for maximum 💯💯💯💯💯💯
Woah, this paper is crazy. Did anyone read it? I wonder if the author had ever even used Scala. Key takeaways. To train user how to do functional programming properly, teach them how to send messages with actors, or as Alan Kay would call it, the essence of OO To identify imperative programming look for these keywords: Abstract Object For Interestingly, to identify functional programming look for flatmap and map, so remember kids, to make your code more functional manually desugar your for comprehensions. But the most important finding I think is this: We took 13 developers with multiple years of java development. We taught them functional programming (actors and such) for 1 month. When we were done, we had 7 expert java programmers out of 13 and 7 expert Scala programmers out of 13! Drop everything, we need to figure out how they train and start a training company. 1 month to become an expert Scala programmer is truly amazing. I mean 1 month to make over half of your developers expert Scala programmers, we have already seen that it takes years for to make over half of your java programmers experts, this is clearly a huge win for Scala in learnability. 🤪 
Incredible amount of effort by Alex. Also, thanks for the mention :)
PR's are very welcome :D
I want to avoid IDE compilation, so that you can't have discrepancies between what the IDE does and the actual compile does, then accidentally break the build. On that advice, I tried out Maven with `scalor-maven-plugin` and I'm kind of iffy on the plugin. But Zinc 1 didn't seem to help with the compile times, and the startup time is not great compared to Gradle (probably because of Gradle's daemon). As a sidenote, while investigating, I learned about Gradle's continuous build execution (using the argument `-t`), which lets me keep the compiler JVM around to watch files and recompile my project. With that, even with old Zinc's less granular incremental building, the whole thing seems to work pretty nicely A small change takes less than a second, where before it took 10.
The racket goes like this: 8 year ago, through the then revolutionary art of data-mining (ahem online survey) , some blog published an article stating that proficient scala developers are hyper productive 10xers. So every upstart switched to scala - not for its technical merits - so that they would be staffed with 10xers. This was a move initiated by bean counters. Scala 10xers soon all flocked to Google and Amazon. Now these companies are back to hiring 1xer developers and college grads and realize that on average, well people are average! Now it dawns on bean counters that the scala candidate pool is much smaller than java. Can you read the bean counter's mind? Thou shalt know his next move!
An error: error: missing argument list for method /: in trait TraversableOnce Unapplied methods are only converted to functions when a function type is expected. You can make this conversion explicit by writing `/: _` or `/:(_)(_)` instead of `/:`.
I see... I get the same error now. I'm no expert, sorry I can't really help you here. What is interesting though is that this compiles fine in Dotty: val myList = List(1, 2, 3) val partiallyApplied = myList./:(0)_ val partiallyApplied2 = (0 /: myList)_ So apparently they fixed something in Dotty?
The recommendation is not use the infix notation with symbolic method names when has more than one parameter (https://docs.scala-lang.org/style/method-invocation.html). Use long names instead (eg: foldLeft). With this case, when (/:) is used, you must use an "inverse" chain of calls because the ':' at the last position. In conclusion, the call you are looking is: val partiallyApplied = (_)(0) /: myList But you can see now a annoying warning about avoid postfix operators :P
&gt; I want to avoid IDE compilation, so that you can't have discrepancies between what the IDE does and the actual compile does, then accidentally break the build. Fair enough. What I do is require a clean CI build before allowing a PR to be merged (this is really easy with travis on github), so that kind of problem would be caught before it went into master. It's a pretty rare case though IME. &gt; On that advice, I tried out Maven with scalor-maven-plugin. I'm kind of iffy on the quality of the plugin. Yeah the scalor one is pretty new and has some rough edges, I'm investigating it but my main project still uses the older scala-maven-plugin. That one I've found very reliable, but it might be still stuck on older zinc? &gt; As a sidenote, while investigating, I learned about Gradle's continuous build execution (using the argument -t), which lets me keep the compiler JVM around to watch files and recompile my project. With that, even with old Zinc's less granular incremental building, the whole thing seems to work pretty nicely: a small change takes less than a second, where before it took 10. Additionally, JIT compilation shaves 30% of the time off of a full rebuild after a few runs. Cool, glad you've found something that works. I would go back to your first point on this one though: I've seen more cases where zinc and continuous-rebuild processes allowed an accidental build breakage through than where the IDE did, so if I were you I'd want to have a process where you do a clean from-scratch build before anything goes into master.
Hi all — the website for Cats Effect is in progress, I'm writing some docs as we speak, because it is incomplete and the 0.10 release is ongoing. I did not plan for this to be announced yet, until the latest is in, but I guess people are happy to see it 😀 So keep in mind that the website you're seeing today is going to change until tomorrow 😜 The cancelable `IO` is coming! Cheers,
No problem. Well it could mean 2 things: it *should* work in Scala or it *shouldn't* work in Dotty. Maybe the Dotty team hasn't implemented this error yet and it could lead to some unexpected results? No idea. You'd have to ask someone who knows what he's talking about. :D I actually did try to make sure it also results in the same behavior, and it seems so: (this is in Dotty) val myList = List(1, 2, 3) val partiallyApplied = myList./:(0)((x, y) =&gt; {print(s"$x + $y | "); x + y}) println() val partiallyApplied2 = (0 /: myList)((x, y) =&gt; {print(s"$x + $y | "); x + y}) println() println(partiallyApplied) println(partiallyApplied2) Results in: 0 + 1 | 1 + 2 | 3 + 3 | 0 + 1 | 1 + 2 | 3 + 3 | 6 6
Surely guy didn't have much existence with newer commercial projects where tech leaders know what to do. Repeat after me: you don't have to use actors and that is okay. /joke
The recommendation is not use the infix notation with methods with symbolic name when has more than one parameter (https://docs.scala-lang.org/style/method-invocation.html). Use long names instead (eg: `foldLeft`). In this case, `/:` expecte two curried arguments. With the first argument you has a method `(0 /: myList)`. In order to convert it into a function you must use `(_)` with parenthesis: val partiallyApplied = (0 /: myList)(_) But the (*actual*) scala compiler needs the type of this second argument, then: val partiallyApplied = (0 /: myList)(_:(Int,Int)=&gt;Int) or, also: val partiallyApplied = (0 /: myList)(_:Function2[Int,Int,Int]) 
I can't believe I just read most of this pointless thread &gt;.&lt;
How does it compare to ScalaZ MonadIO?
Kinda noob here, how does this differ from Monix's Task? I mean from a semantics point of view, seems like it's nearly a drop in replacement? 
Hello, I'm the author of the Monix `Task` and the contributor of `cats-effect`. I answered this somewhere else, but copy/pasting it here as well — I'll include it in the docs somewhere ... `cats.effect.IO` and `monix.eval.Task` are very similar, plus I ended up working on both, so design decisions I made for `Task` ended up in `IO` as well. That said `IO` is designed to be a simple, reliable, pure reference implementation, whereas `Task` is more advanced if you care about certain things. For example `Task`'s run-loop is designed to provide certain fairness guarantees, depending on configuration. By default its run-loop does processing in batches, introducing thread forks once over a threshold. If you're using `Task` or `IO` as some sort of green threads, and you should because they are really good for that, then `Task` provides scheduling fairness out of the box, whereas your logic via `IO` will have to include manual `IO.shift` calls. Today I've released version `0.10` of cats-effect with the cancelable `IO`. Before this it was only Monix's `Task` that was cancelable and actually usable in concurrent scenarios (e.g. race conditions), so I imported the same underlying design that the other maintainers agreed to. That said the IO implementation is more conservative still — in both the cancelability of a task has to be opt-in, however with `Task` you can opt into auto-cancelable `flatMap` loops, whereas with `IO` you need manual calls to `IO.cancelBoundary`. So IO's design is to be very explicit about forking or cancelability, whereas `Task` affords some smartness in it. `Task` is also designed to interact better with the impure side and you'll have an easier time to get it adopted in hybrid projects. For example `Task#runAsync` calls return `CancelableFuture` results. With IO you have `unsafeToFuture`, but it's not the same thing, the same level of integration. `Task` also has a `memoize` that allows you cache executing tasks, but that in certain cases could break RT (since allocation of a mutable ref is a side effect), so you have to be a big boy when using it, but it's awesome if you've got interactions with the other side. See [issue #120](https://github.com/typelevel/cats-effect/issues/120) for why this won't happen for IO. Task also requires a `Scheduler` in its `runAsync`, which gets injected everywhere internally, so you don't need an `ExecutionContext` for operations like `sleep` or `shift` or whatnot. We fixed this `ExecutionContext` dependency for `IO` recently by creating an indirection in `Timer`, but it's still there. This matters in certain scenarios - for example with `Task` you get a `Task.deferFutureAction` which gives you an `ExecutionContext` for wrapping `Future` enabled APIs, so you no longer have any need to carry around that `ExecutionContext`, which isn't possible with `IO`. Also Monix has `TaskLocal`, which is like a `ThreadLocal`, but for `Task`. You can't implement such a thing for `IO` without modifying its run-loop, because you need to ensure that values get transported over async boundaries. So the TL;DR is that `IO` is simpler, more conservative, whereas `Task` affords some useful extra features and interacts better with the other side in projects that have impure modules, being less judgmental of how you use it. Both have virtues. But that is why the `cats-effect` project also provides type classes, such that projects that can provide polymorphic abstractions that can work with both. At this point projects like Doobie, FS2, Http4s or Monix's own `Iterant` allow you to use both `Task` and `IO`. Hope that answers your question.
It's been a pleasure working with Cats Effect, and I would encourage people to not be afraid of working with `cats.effect.IO` en lieu of Futures. The learning curve is slight, and the benefits are fantastic.
No reason to switch, yes. A big motivation for me to collaborate on cats-effect was to find ways to integrate Monix's `Task` in the community's projects and I succeeded, also with the help of Daniel Spiewak, who provided the initial design which was very good, I just improved it for concurrency. And `cats.effect.IO` doesn't provide anything that `Task` doesn't. It's more like a subset. And a Monix release is also coming btw. Thanks for using my stuff 🙂
We would not have a website without /u/volpegabriel indeed 👍❤️
&gt; Thanks for using my stuff 🙂 No, thank you for contributing with your very valuable personal time!
Tbh, it's object system is much better too. Things are just so much more elegant. Instance methods are under classes, Static methods under `object`, you can `new Trait`, `trait` is so much more powerful. I haven't used kotlin, so I can't compare to that, but Scala is definitely way better than Java even on an OO basis.
Emily, any feedback on Intellij support? I noticed recently Intellij is confused by Http4s HttpService[IO], as well as other advanced Cats components. Thanks in advance.
I only use Intellij intermittently (I'm an ENSIME fan), but from the times I have used it, yes, Intellij seems to be confused by Kind Projector in general, and frequently seemed confused by alot of the Kleisli-based underpinnings within Http4s. IO was not necessarily a huge problem - beyond the wiring components together at the endpoints and in my modules, the rest of the framework has worked seamlessly. 
What do you think about having some sort of [SafeApp](https://github.com/scalaz/scalaz/blob/series/8.0.x/effect/jvm/src/main/scala/scalaz/effect/SafeApp.scala)? It's a small thing, but it looks fun to have a pure entrypoint.
I'm curious how this will end comparing to `IO` in Scalaz 8, on which there was some discussion [a few months ago](https://www.reddit.com/r/scala/comments/7dg6fx/scalaz_8_io_pull_request_by_jdegoes_pull_request/).
Those `DEBUG` look like they are coming from the tests themselves and not SBT. So, if you want them to go away you will probably have to turn down logging in the application.
Just write some code in Scala and compare them to the equivalent Java 8 implementation, using the features people have mentioned. The biggest hurdle is getting people to buy into functional programming as a whole. Once that's done, it's easy enough to show java developers how annoying it is to write a bunch of sequential flatMaps vs. a nice for-comprehension, or just not needing to type out insanely long types every time.
Thanks for your timely feedback, Emily. I can assure you that HttpService[IO], even when declared on both sides of an assignment, emits a found: HttpService[IO], required: HttpService[IO] error. Is that not confusing? I've come across other errors. And while some category theory constructs are increasingly better interpreted by Intellij, others are clearly not. Let's hope Intellij support for Scala category theory libraries improves in the near term. Cheers! 
I'm thinking about ScalaZ 8 IO actually, some of my co-workers saw a talk about it at Scale by the Bay and they were pretty impressed.
&gt; I'm not a huge fan of actors. One of their big benefits is the supervisor, which restarts actors if they crash. Well, microservices today typically run in some sort of environment (Kubernetes, ECS) that monitors container health and restarts them for you. So just let the service crash and your deploy environment restart it. The primary point of actors is that they enforce the message passing architecture. The usefulness of it varies from place to place, but you seem to be missing what it's actually about. Indeed, much of the philosophy behind Actors is that your code should be managing the lifecycle of your services (as part of being able to handle failures gracefully), not an external thing that hides necessary complexity. But that aside, the supervisor system doesn't just restart actors, it provides lifecycle control that is far more granular (pause, resume, slow, etc), it allows soft restarts rather than full box restart, and it allows hierarchical lifecycle control that AFAIK Kubernetes doesn't support. You may not need this much control in your usecase but your alternative doesn't really mesh with what actors are about. &gt; Another is the asynchronous calls. Again, not really important in microservices. Can it add performance? Certainly. Will you spend more time writing your code than just adding proper concurrency as needed? Almost certainly. Keep things simple. Just write your code, and if it would benefit significantly from a concurrency boost, do it. But chances are You Ain't Gonna Need It. Async not important in microservices? What? If anything, Async is less important in monoliths give that all operations are local operations, but in a microservice architecture you're constantly communicating with other services and thus constantly waiting on network calls. Everytime you push the network calls to the background and do some other work, guess what you're doing? An async call. Not to mention your microservices probably communicate via some form of service discovery and message queues and logging, all of which likely involve asynchronous calls. I just find this statement utterly bizarre. &gt; As for Akka Http, write a service in Go using any of the many available muxxers, then tell me that Akka Http is even close to a good idea. I haven't touched the Source/Stream stuff on Akka Http for file uploads yet, but building simple REST servers on it seems trivially easy to me. I have no idea why you think it doesn't lend itself to abstraction or reuse, given I've easily written a dozen directives that I've reused throughout my routes, and all of it can be reused anywhere and be composed with each other trivially. 
Thank you very much!
Ah I see what you're saying. Yeah, I don't do that. Roughly, I have one entry point in my code for effect types, and the rest are in a tagless final style so that I don't end up polluting my codebase with specialization bias. Here's [a basic example](https://imgur.com/a/61JcY) I don't honestly know why you'd get that error. I'd raise a ticket with IDEA about it. 
Well, I find the responses here rather unhelpful, so I'll play devil's advocate: The main attributes of Actors are the following: 1. Message driven architecture (useful for a large system, in a small service it doesn't matter too much). 2. Built in supervision: ability to pause, resume, and control the life cycle of child actors. 3. Location transparency: actors can be easily distributed across nodes. So if you want to scale your service horizontally, it's very easy. You can't distribute futures like that. 4. Mutable state allowed inside each actor. May be useful for various optimizations. 5. Actors can stash away messages for future processing if it doesn't/can't handle it right now. So benefits for using actors may include: 1. If you want to distribute load across multiple nodes easily. 2. If you validate the data as you go, you can have one actor return the error as soon as it sees something and then you can tell the other actors to stop working. 3. If an actor goes down due to memory problems or whatever, you can easily restart it and not have it crap out. 4. If you want an actor to maintain local mutable state for something (time series data?) 5. If an actor detects a dependency is too overworked, it can stash the message and work on something else for a while. That being said, if you're actually doing data crunching, I recommend something like Spark which has built in support for distributed computation. I see actors as primarily useful for managing lots of services more easily.
Super helpful, thank you. We do have Cassandra/hdfs and spark underneath all of this, so will rely on that for some crunching
My company is actually discussing using actors to manage the Spark cluster. This is because Spark's error handling is quite awful so an actor can be a useful and smart lifecycle handler to restart/pause if needed, and it can store a local mutable cache. 
Yeah I was surprised to find that out about spark. Something goes a little wrong and it totally blows up and dumps a ridiculously long trace. There is drop malformed mode, though you have no idea what's being dropped.
1. Make a src/test/resources/logback-test.xml 2. set the root log level to error: &lt;root level="error"&gt; &lt;appender-ref ref="console"/&gt; &lt;/root&gt; 
Do you think Scala is likely to be dying? Things don’t seem to be looking bright according to that RedMonk index. I would hope that Kotlin didn’t end up taking a lot of potential users... mixing both OOP and functional paradigms sure does sound nice, and this seems to be the only language that does it so well. 
Perhaps I've just never seen Akka use correctly. I agree with many of your points about concurrency being important to service. I exaggerated when I said it wasn't important. My real concern, and one I have seen in production code, is that Akka streams can produce reliable concurrency, but it's a bit like swatting a fly with a howitzer. While it may do the job, you can get the same results with less. Akka also has a tendency to be self propagating: you start with a little, and suddenly the rest of your service needs to change to work around it. I have only recently begun working on a team that uses Scala (coming from Go), and I am startled by how much I hate Akka. Scala is fine, but Akka makes everything harder. Yet it seems that the moment you want to step outside the basics in Scala, all you have is Akka. I have not encountered a place where the minor performance gains from Akka are worth the drag they put on development time. Maybe if you have a mature product running at immensely high scale and every millisecond counts, then the overhead is worth it. But from where I stand, it's just a black hole where you throw in time. When I first started, I pointed out how we simply didn't need these tools in our services. After being poo-poo'd, my team members (after 4 months of head scratching) have decided to rip out all the Akka code and use Monix for reactive streams and nothing instead of actors. So yes, you might be able to tweak and balance every last detail with Akka, but when you need to deliver a product, you quickly figure out that the real bottlenecks like elsewhere.
Well, cats-effect 0.10 is available right now and the Monix Task since 2016 😉 Here's my presentation on it: https://www.youtube.com/watch?v=wi97X8_JQUk
&gt; My real concern, and one I have seen in production code, is that Akka streams can produce reliable concurrency, but it's a bit like swatting a fly with a howitzer. While it may do the job, you can get the same results with less. I don't think you're talking about Akka Streams, you're talking about Akka Actors. My coworker built a streaming csv verifier on Akka streams within about 40 lines, without ever touching an actor so I don't really see how it's a howitzer. Yeah the syntax is pretty arcane, but once you understand the patterns it's pretty easy and straightforward to build simple streams on top of it. Everything in Akka is built on top of the actors so you can see the power of it, but the docs specifically tell you that it's a tool for when you need strong control combined with easy scaling. In every use case they'd tell you that if you just want easy concurrency behavior, use Futures or something similar. Actors are for cases where you need much more than that. &gt; Akka also has a tendency to be self propagating: you start with a little, and suddenly the rest of your service needs to change to work around it. My company been using Akka http for years and we've never needed to build anything around it or touch actors besides instantiating the actor system on start. And since Akka http is built on top of actors, your statement rings false. &gt; I have only recently begun working on a team that uses Scala (coming from Go), and I am startled by how much I hate Akka. Scala is fine, but Akka makes everything harder. Again, I don't think you know what Akka is. Akka is a family of features built on top of Akka Actors, but in most cases you never need to touch actors because they already implemented a ton of stuff for you. I've used Akka-http for a year and it makes http routing trivially easy. &gt; Yet it seems that the moment you want to step outside the basics in Scala, all you have is Akka. That's not even remotely true. There is not a single thing, besides actors, that I can think of where Akka is your only option. &gt; I have not encountered a place where the minor performance gains from Akka are worth the drag they put on development time. Maybe if you have a mature product running at immensely high scale and every millisecond counts, then the overhead is worth it. But from where I stand, it's just a black hole where you throw in time. Performance isn't why you use Akka actors. I don't know what ever gave you that idea. Actors are all about satisfying the ideals of the Reactive Manifesto: https://www.reactivemanifesto.org/ Of which performance is only a very small portion. Actors are meant to be the building blocks of a system that satisfies the following characteristics: 1. Easily replicable, so there is no single point of failure. 2. Fault tolerant and easily recovers from failure; actors allow for easy hierarchical ownership of service lifecycles so that you can implement principled error handling without spaghetti error handlers everywhere. 3. Easily scalable. 4. Message driven architecture, so that you don't up with a Big Ball of Mud. Erlang's actor system is what enabled the building of the AXD 301 which achieved 99.9999999% (that's 9 9s) uptime. That's what actors are meant to help you to achieve. &gt; When I first started, I pointed out how we simply didn't need these tools in our services. After being poo-poo'd, my team members (after 4 months of head scratching) have decided to rip out all the Akka code and use Monix for reactive streams and nothing instead of actors. What if you wanted to distribute that Monix code across multiple nodes? It's fine if actors didn't fit your use case, but based on your comments it feels like your team didn't seem to understand what actors are even supposed to do. &gt; So yes, you might be able to tweak and balance every last detail with Akka, but when you need to deliver a product, you quickly figure out that the real bottlenecks like elsewhere. Akka isn't about performance bottle necks. I'm just bemused at how your team tried to use Akka Actors for 4 months but for some reason never bothered to read about what actors are supposed to accomplish?
Here you go: trait SafeApp { def main(args: List[String]): IO[Unit] protected implicit lazy val timer: Timer[IO] = IO.timer(scala.concurrent.ExecutionContext.global) final def main(args: Array[String]): Unit = main(args.toList).unsafeRunSync } I'm not convinced such a thing can make it into `cats-effect`, because it would have have to be parameterized (so not `IO`, but `F[_]`) and different data types require different things from the "environment". `IO` wants a `Timer` for doing time based operations, built on top of the JVM out of an available `ExecutionContext`. Monix's `Task` wants no such thing, its `Timer[Task]` being reusable and global with no dependency, but it wants a `Scheduler` in `runAsync`. I guess the restriction could be `F[_] : Effect`, so that solves it, as `Effect[Task]` has a `Scheduler` dependency, and `IO` does not, so what exactly are the values provided by environment in order for the thing to run, that depends on `F`. So it probably won't happen. There's virtue in staying small and not introducing stuff when not critical. Dump the above in your utils package, which you have anyway and you're done.
&gt; It will only tell you the full expansion, not whether intermediate partially expanded states were valid or not. Full expansion will be done at compilation. There's no "intermediate" state - if the template is fundamentally broken it'll fail even earlier. &gt; Nonsense, null is not required in Scala. Yes it's, if you want the good stuff from C/C++/java libs. &gt; But in C/C++ it's never "a small number of cases", as so much of the language is unsafe. Every + needs extra review. Every &lt;&lt; needs extra review. Every * or -&gt; needs extra review... And every scalaz operator and typelevel hack need extra review too then. &gt; Implicit conversions won't help you. That's my point: I can convert anything to anything. &gt; The spec for "modern C++". You said "We know exactly what's modern C++". If that was true, there would be a spec for it. Uhh, "modern" C++ is just using the new and safe features of C++. There's no need for a spec because it's in the C++ 14+ specs. &gt; It's theoretically possible, but only if all your code is perfect. It's possible practically - it depends on the scale. &gt; It's not practical, because very few people can write perfect code, and the tools aren't reliable enough to catch all your mistakes. Same true for scala and java. &gt; You can test what one compiler's build of your code does with a particular UB, that won't tell you anything about what another compiler's build might do with it. This is bullshit. How much compiler do you plan to use for one project?! Anyway, it's not like we're all buried under piles of compilers... &gt; You're the one being naive here. The C compiler absolutely will "magically" insert bugs into your code if your code invokes UB, Bullshit again: UBs are documented for the compiler per platform. &gt; even if you think you know what the UB "should" do. So you can't "write working and bug-free code while using UBs". I can. I can use indexing(with out-of-bounds) and my code can still work. Your arguments are all about perfectionism - the same thing you tried to fight against when I called out the JVM's weaknesses. &gt; It wasn't the same call, it was a different call; But it look like the same call and we can do that with the exact same instance. Don't even try to argue: that's a dirty and unsafe hack. &gt; Java isn't perfect but it doesn't have undefined behaviour in general code. and it can't be used for performance-sensitive, resource-constrained and real-time applications either. It also can't guarantee thread-safety. It's just a basic platform for the web - and for nothing else. &gt; There are many guides and checks because none of them is good enough. No, because some people like exceptions and some aren't. Some need to work with low-level and unsafe code and some need to develop high-level code. &gt; Google required two security-focused people to review all changes to their sandbox code but they still missed an UB. And by "UB" you meant a bug, right? &gt; Games tend to have extensive manual testing, Of course, because you can't automatize testing. &gt; and even then they crash pretty often. The shitty ports. &gt; Nier;Automata crashed for me on Tuesday. Which is known to be a shitty port. &gt; I've had three or four crashes in Overwatch. Have you reported it? What was the issue? &gt; Memory's Dogma crashed a couple of months back and I lost so much progress I've not gone back to it. lol no (auto)-saves? &gt; Basically every game I've played more than a couple of hours of in the last few months has crashed at least once except for Factorio. Wow, what's wrong with your system? Btw, do you play on windows? I play on linux and the last crash I've seen was with Skyrim(a rogue mod, I already deleted it) - which I played on wine because there's no native linux port. Non of the recent games I've played with crashed this year, except Skyrim and another one which ran out of memory with an error after some point.
&gt; Comparators for equality comparison and sorting, That one only needs a function. &gt; differs for showing differences (e.g. used in test assertions) - once you have the ability to view case classes as records the rest is just ordinary (recursive) implicit resolution. Test frameworks used to print the toString of the mismatched objects. &gt; If you're saying macros should be a language feature then you're presumably saying there are a lot of different use cases for macros. If we were only going to have two or three libraries that use macros in the ecosystem it would be better to do something specific for those libraries rather than have to support custom macros in every codebase. You can't know what you'll need before creating the language. And there's no way that macros will only be used by 2-3 libraries because macros can be used to remove boilerplate and to optimize code - and those can be used quite often. &gt; and actually there's only one in the Scala ecosystem that I think is worthwhile, shapeless) IMO, shapeless is rather ugly. I'd say quill is a better example. &gt; I don't know what this is supposed to mean. \&gt; Btw, if the traversal part is there for a macro than ~~use~~ you can use them too. FTFM &gt; I find they're widely used to be worth having as a language feature. Virtually every project I work on ends up depending on shapeless, so it's no more bloat than if that functionality was built into the language in the first place. You know that shapeless macros are not that safe "features" and not just a few features, right? To mimic the macros of shapeless you'd need to introduce different pseudo-macro features. &gt; It's a bad sign for macros: a library's author tries to implement the functionality they need with macros, but ultimately resorts to modifying the language. It's a bad mindset of certain developers: instead of using a simple and flexible feature they try to bloat the language with complex and specialized features. &gt; What's the value you think there is in macros? I've already enumerated its benefits. &gt; I've talked about my use cases, what are the use cases you have for macros that aren't covered by typeclasses and shapeless? Typeclasses can't cover shit. Shapeless uses macros. Shapeless still can't do many thing what we can do with macros. &gt; Good macros make code easier to read, bad macros make it impossible. The risk:reward ratio is too high. I can tell the exact same thing about your typelevel hacks - but this time it'd be true. Just look at the spray-json and nim-serialization samples. The latter looks like simple code while the other looks more like tasteless C-ish macro hacks. &gt; Macros are worse for language maintainability. Bullshit. Macros do two thing: generate and analyze *valid* code. If you want to argue that it's much easier to implement a new feature for every use-case of every macro then I'd say you've gone crazy. &gt; Individual macros impose the same maintenance costs on code as language features, Now, that's some seriously bullshit exaggeration! And you say that while you totally ignore your typelevel spaghetti. &gt; because at the point of use there's no difference between the two. Hmm, there are differences. I don't think you understand what are macros. &gt; And having a macro API makes the language itself harder to evolve since it makes the AST part of the language's API. Haha, because it's so much easier to evolve the language when it's full with different one-trick-ponies. &gt; Not my experience. At least, I've been able to find libraries that do the things I need without violating those rules. I assume you search for purists libraries. &gt; For that there's wartremover. Wartremover itself is pretty far from safety. &gt; Ban null, and forget all the useful C/C++/Java libraries... &gt; ban throw, and forget efficient exception tracking... &gt; ban var or at least flag it for extra code attention in code review, and then it's like working in a pure language. and forget efficiency for good. &gt; at least flag it for extra code attention in code review, and we'll need more and more "extra" attention. I wonder how you'll catch data-races with code reviews...
Thanks for the tip! I'll study your style. BTW, you need to trademark --- specialization bias.;)
get good.
Alex, this is amazing work. The guide is easy to read and flows naturally from topic to topic. Just for the sake of argument though, I’d ask: wouldn’t the `Future` example become pure and referentially transparent if we lifted it from a `val` to a `def`? It really seems to me that would have the delayed evaluation property that makes `IO` RT.
get out
&gt; Again, I don't think you know what Akka is This must be the case. I have yet to see any Akka library, let alone just actors, be used in a sensible manner. I'll reserve judgement until I work with Scala and Akka at a different company. 
Welcome to opensource, where documentation can be a mess and is often an afterthought (i'd say play is an exception to the rule). As a rule of thumb, for the most recent info, look as close to the source as possible. So for the mongo driver, go from the play website, to the reactivemongo website (you'll land on the 0.11 version docs, so go further) to main site, and find out that yes play 2.6 is supported in the most recent version. Play is pretty high level, lots of batteries included. When i started with it, i found the docs good enough, but yes, high level, not much low level docs. If you're looking for something more low-level (which might be a better match for your requirements, since you might just want a rest server if you plan a fullfledged single page app with scala js) just look at this reddits sidebar under "Web Development and Microservices". I like http4s, but there are other options. I'd avoid vert.x, it's stringy-typed and you might as well python then. (also look at https://index.scala-lang.org/ and https://github.com/lauris/awesome-scala to see what libraries are available). 
I must admit I like the play documentation so I don't share your pain point there. If you are looking for a lighter weight framework take a look at: http://http4s.org 
&gt; You certainly wont get better documentation than Play's in another framework We are talking about the [same thing](https://www.playframework.com/documentation/2.6.x/Home), right? I personally wouldn't say that those docs are better than [these](https://docs.djangoproject.com/en/2.0/) or (but last time I actually read them was years ago) [these](http://guides.rubyonrails.org/) or [these](http://vertx.io/docs/) or even [these](http://flask.pocoo.org/docs/0.12/). &gt; I'm not saying Play is better than what's available but I don't think your criticism of it is indicative of, well, problems with the framework. And I was not implying it. My concern is that it's difficult to get relevant information quickly, as compared with other mainstream frameworks, and I still think that is the case.
&gt; First of all, googling "play" is a recipe for disaster, Just a small observation: * It appears that many of the people here who like the documentation might be going directly to the documentation, and then looking for an answer. * Conversely you seem to be going to google and typing in a query. (I'm not an expert on Play, but I think this might be something other responses are almost saying, but not quite saying it directly)
No worries! Feel free to dm to discuss. &gt;BTW, you need to trademark --- specialization bias.;) I'm just following Knuth's principles; specialization is optimization, and optimization is the root of all evil :)
My first point was about general google-ability, whether you end up on the official documentation (but you often don't, also because it tends to be un-specific), stackoverflow, a blog, whatever. I noticed that I tend to end up on the Google Play store, instead, among many other unwanted targets. And yes, I know I can open the documentation.
Have you looked at Udash? It claims to be a good all in one frontend and backend scala solution.
&gt; You certainly wont get better documentation than Play's in another framework Thing is, I agree with this, but I at the same time, I also agree with the OP and that the documentation is not that great. Yes the framework has loads of documentation pages, but at the end of the day, they end up feeling unhelpful. I really can't put my finger to it, but It feels the documentation where written grudgingly with a wave of the hand, without the beginner in mind, thus leaving a lot of knowledge gap in its explanation. It serves as a good reference guide for someone who has slugged through working with Play and have figured out a lot of things. Not so good for someone not familiar with the framework. So yes, I will agree that compared to other documentation of mainstream frameworks out there (ror, vertx, django, springboot, etc), Plays documentation still feels lacking.
Why compare to a Python framework? My statement was in the context of Scala frameworks, not all frameworks for all languages. You're on r/scala not r/programming
&gt; My first point was about general google-ability, I agree; I'm not arguing with you. Projects named using a single common word is a giant pain. In Scala, it seems 80% of the libraries do that. The only thing I'm saying in my post is that you might have a much better experience if you change how you search for information. Practically everyone responding says their documentation is great (I wouldn't know as I have no experience with it).
I wasn't trying to proclaim that it's perfect, rather that its just the best documented Scala framework available. Certainly there are plenty of other frameworks in other languages that are better documented.
Does indeed look nice, thanks! Just a bit worried that it might not have much of a traction yet. 
Everyone has already commented on the docs, and I think play is pretty good. But if you plan on setting up more of a SPA than an MVC app, and you can deal with a library rather than a framework, then I'm a really big fan of Akka HTTP. It manages to be very minimal and just take care of the http part of your application while still being extremely comprehensive (the Scala Uri class in there is the best I've seen anywhere).
I faced a similar issue when working on a project at work. A previous programmer decided to use Play to serve a REST API. I took over after he left, and the project started getting more requirements, including adding authentication and authorization. I was surprised that there wasn't a straight forward way to do it in the latest version of Play. Googling for "play authentication" leads you to [this link](https://www.playframework.com/documentation/2.1.1/JavaGuide4), which is obviously out dated. I found a couple of other plugins which were not even supported in 2.6. One of them had a github issue open since august of 2017 and to upgrade compatibility to 2.6 and it was still open. I ended up switching to Spring Boot.
Maybe it needs artifact or manifest?
The props via new is the only way to force checking that you are only passing allowed props. Many of the libraries use a list and you can as well but a list can not be checked properly. I wrote a macros that turns a list into a trait that is type checked but I think its not the right way to go. No other scala js react libraries have this level of checking which I found crucial for react projects. Address demo should show that address on the right side as well. Perhaps I temporarily broke it when you accessed it. The selection is not preserved yet as I was working on the other demos such as the weather one. All the demoes are WIPs. Hopefully I’ll finish the movie demo soon, its base on 16.4 apis which are in flux. 
I have never dreamed of that and hope I never will!
&gt; That one only needs a function. A function will either be specific to a given type, or not typesafe if it uses something like reflection. With typeclass derivation you can derive comparisons for datatypes that are made up of sortable things, while not allowing sorting things that don't make sense to sort. &gt; Test frameworks used to print the toString of the mismatched objects. Which is not as helpful as it could be when you have large, complex objects with small differences. &gt; You can't know what you'll need before creating the language. I think we're seeing consensus gradually developing in language design, with a lot of convergence in recent languages. &gt; And there's no way that macros will only be used by 2-3 libraries because macros can be used to remove boilerplate and to optimize code - and those can be used quite often. The language should be good enough without them (and in my experience Scala is). &gt; You know that shapeless macros are not that safe "features" and not just a few features, right? To mimic the macros of shapeless you'd need to introduce different pseudo-macro features. They're safe, and it's only a few of them. `LabelledGeneric`/`Generic` could be a language feature (indeed I'd like to see case classes become more record-like). `Lazy` is already subsumed by a compiler patch. `Poly`-like functionality is already planned for Scala 3. I think most libraries that use Shapeless don't use anything beyond that? &gt; It's a bad mindset of certain developers: instead of using a simple and flexible feature they try to bloat the language with complex and specialized features. All language design is about choosing the right level of restriction. If we're using a type-safe language then we already believe in using systems to constrain what functions are allowed to do, and accept a certain amount of complexity as the cost of that, while trying to write our system in a way that's general enough to cover all the reasonable use cases. Typeclass derivation is the along the same lines - a restricted subset of what macros can do that's safer, but still covers the use cases. &gt; Shapeless still can't do many thing what we can do with macros. What things, specifically? &gt; I wonder how you'll catch data-races with code reviews... I'll avoid them by avoiding mutability at the language level.
&gt; And every scalaz operator and typelevel hack need extra review too then. No, because all scalaz operators are memory-safe just like any other scala code. &gt; That's my point: I can convert anything to anything. Implicit conversions are functions, they typecheck exactly as functions do. Nothing unsound about having a function that returns a different type from its input (indeed we'd struggle to do any programming without such functions). &gt; Same true for scala and java. No, because the effects of bugs in Scala and Java are local and limited, because they are memory-safe languages. &gt; But it look like the same call and we can do that with the exact same instance. Don't even try to argue: that's a dirty and unsafe hack. I showed you the exact same thing in Nim. It's normal programming language behaviour. &gt; Have you reported it? What was the issue? Just crashes every so often, as C/C++ programs generally do - it's not like I have a consistent reproduction to send. Why do you think these things come with crash reporters? Why do you think Chrome has separate processes for each tab and gives you a button to restart it when the tab crashes? Programs crashing is just so normal that we're inured to it, that's the reality of the tools people are still using. &gt; Wow, what's wrong with your system? Btw, do you play on windows? I play on linux and the last crash I've seen was with Skyrim(a rogue mod, I already deleted it) - which I played on wine because there's no native linux port. I've played various things on a number of systems, a mix of Windows, Linux and FreeBSD. Segfaults and similar crashes are just a part of life when you use current computer systems because so much of them is still implemented in memory-unsafe languages.
this, go to project structure and add it as a library (i think), a then assign that as a dependency to one of your modules
If you are using Google, then using a query and sticking "site:playframework.com 2.6.x" on the end will usually give you what you want. If you are looing for something general, then looking for "play framework" gives you more specific options in the way of blogs and articles.
You need to look for "play framework authentication 2.6.x" which will give you what you are looking for. https://www.playframework.com/documentation/2.6.x/ModuleDirectory#Authentication-(Login-&amp;-Registration)-and-Authorization-(Restricted-Access)
Or you can just read the Akka documentation? I don't understand the mentality of "I've only recently started using Scala/Akka, I'm not going to bother researching anything about it and I'm gonna proclaim my judgement of it immediately." Why? 
Ironically, I would need the very information I'm lacking :)
This will be highly useful in specialised cases, such as reloading an event-sourced JSON-based journal/log upon application start-up, as the typical bottleneck is the parsing. These cases however are rare.
I'm not sure I understand your full exposition, but you did nail it when you said that curly braces simply define multi-line expressions. Another way to think of it is curly braces give you a scope, and that scope can be used wherever an expression is expected. Sure, it's a little jarring that you don't have to wrap curly-brace expressions in parentheses to pass them as single parameters, but that's also what allows keywords and methods to exist on almost equal footing in the language syntax. I really miss all of these things when I work in other languages.
Guys, please post your comments with more context and details... or at least with a hint or a link It is hard to guess whether you are struggling due a limit in selection of JSON parsers, or maybe you have success in evolving of binary protocols for you open API? Maybe you eliminated the need for high-speed serialization by providing universal functions for your customers, which return right responses on all their requests in-process?
Thanks, Jsoniter looks great! Is it possible to have a scala.js version?
I think [this](https://github.com/jaliss/securesocial/pull/614) is the Github issue I was referring to in the previous comment: (which depends on [this](https://github.com/jaliss/securesocial/issues/611) and [this](https://github.com/jaliss/securesocial/issues/612) which was open since mid last year!). Scala 2.12 has been out for quite a while, as was Play 2.6. It was only merged a couple of weeks ago, 
Scala.js and Scala Native contributors are welcome! I'm glad to try to do it for you, but I have 0% experience in Scala.js, and even have no clue about which challenges should be resolved to have a success on this platform. 
&gt; I wasn't trying to proclaim that it's perfect, rather that its just the best documented Scala framework available. Certainly there are plenty of other frameworks in other languages that are better documented. I was not saying you are proclaiming this either. I was just responding to your statement: &gt;One of the benefits you get with Play! is the vast amount of documentation that comes with it, so I'm surprised to see that this is a pain point for you especially the part where you expressed surprised that the documentation is a pain point for the OP. My response was just trying to explain why the documentation could still be a pain point, even if as you claim, "its just the best documented Scala framework available" 
&gt; Let us know if you find good visualization packages. Been looking for one for a long time! Have you seen plotly-scala [0] or plotly-scalajs [1]? [0] https://github.com/alexarchambault/plotly-scala [1] https://github.com/DefinitelyScala/scala-js-plotlyjs 
Sure. From an OO perspective, the way I manage my database transactions is that I have a set of, essentially, command objects that represent database operations to carry out, and these can be combined into composite commands using standard `for`/`yield` sugar and standard library functions, and then I have a service that runs a (composite) command within a transaction. This has a bunch of nice properties: I can't ever forget to perform a transaction, because the type system won't let me *use* a command without running it (I can't pass a `CompositeCommand[UserId]` to a function that expects a `UserId`). But I can still reuse fragments of "has-to-happen-in-a-transaction" logic in bigger transactions, e.g. I can do something like: def managerOf(user: UserId): CompositeCommand[UserId] = ... def friendOf(user: UserId): UserId = ... //this one doesn't use the database for some reason def managersFriendsManager(user: UserId): CompositeCommand[UserId] = for { manager &lt;- managerOf(user) friend = friendOf(manager) mm &lt;- managerOf(friend) } yield mm The type system enforces that the actual execution of `managerOf` will only ever happen in a transaction (I realise that it's probably a read-only operation that doesn't need a transaction at all, it's just an example). When writing our test suite we can immediately see from the type signature that `managerOf` needs a database but `friendOf` doesn't (and if we don't notice, the type system will tell us when we come to actually write the test). But `managersManager` doesn't have to do two separate transactions. And unlike `@Transactional` we only need plain old Scala to do this, not any "magic" interceptor / reflective proxy / bytecode manipulation library / ... And since the command objects are just plain `case class`es, there's no risk of "leaking" a database connection handle out of the transactional method (indeed the above code can be in a module that doesn't even have access to the actual database-connection code at compile time). To a very limited extent you can do something like this in Java/Kotlin. E.g. Hibernate has a `DetachedCriteria` class, so you can form a command representing a single hibernate "select"-style query and pass it to a service that executes it in a transaction. But we'd like to extend that to different types of command for create/update/delete/.... Hibernate doesn't offer that, probably because in a language without pattern-matching you'd have to use a cumbersome "visitor pattern" to handle the different cases, whereas in Scala it's easy with a `sealed trait`/`case class` structure. Then supposing we have a type that represents a single command to execute, we'd have to write the "composite command" structure ourselves. The `CompositeCommand` I used above would actually just be a type alias for a standard Free Monad around our command type, but you can only implement it generically in a language that has higher-kinded types. In Java or Kotlin, the implementation would have to be specific to our specific commands, so we'd have to write it ourselves. Having a composite command type is no good if you can't actually compose commands to make it, and neither Java nor Kotlin has any equivalent to Scala's `for`/`yield`. (Kotlin has a handful of ad-hoc implementations like `?.` and `async`/`await` to replace the most common use cases for `for`/yield`, but it doesn't have anything that you can use with custom types or anything the Kotlin developers didn't think of). And even then, without HKT we can't reuse generic library methods for dealing with our `CompositeCommand` types. In Scala with HKT there are already a bunch of functions and datatypes that work for any `Applicative`/`Monad`/... that we can reuse. E.g.: // get a list of managers for a list of users, using traverse from cats def managersOf(users: Vector[UserId]): CompositeCommand[Vector[UserId]] = users traverse managerOf // matryoshka-style tree structure case class NodeF[A](user: UserId, children: Seq[A]) type OrgChart = Fix[NodeF] // monad transformer from cats type CmdOrError[A] = OptionT[CompositeCommand, A] // Verify the org chart is correct by doing a matryoshka-style traversal. // At every layer of the tree we check whether the manager of each child is their parent // in the tree, then return the manager of the current node for use in the check at the next level. // cataM takes care of both the tree-walking side of things and composing the monadic CmdOrErrors def verifyOrgChart(chart: OrgChart): CmdOrError[Unit] = chart.cataM[CmdOrError, UserId] { case NodeF(manager, expected) =&gt; if(expected.exists(manager.!=)) OptionT.none else OptionT.liftF(managerOf(manager)) }.map {_ =&gt;}
Thanks for the pointers, I'll have a look at those!
Here's one tutorial of how to use it in zeppelin https://www.zepl.com/viewer/notebooks/bm90ZTovL3pqZmZkdS9mOThiNjg5YTcxNmQ0Njk0OGE2YzE0OGZiMWU4NTgxMS9ub3RlLmpzb24
I manually created a project using SBT and then imported that into intellj and it works. Maybe some sort of bug with the IDE? 
reimporting it as a scala project seems to fix most issues yeah. i dont think it's a bug, it's just very confusing when you're using a few different tools
&gt; No, because all scalaz operators are memory-safe just like any other scala code. Memory-safety won't save you from hidden hacks. &gt; Implicit conversions are functions, they typecheck exactly as functions do. Nothing unsound about having a function that returns a different type from its input (indeed we'd struggle to do any programming without such functions). They're *implicit* functions and that's why they're dangerous. &gt; No, because the effects of bugs in Scala and Java are local and limited, because they are memory-safe languages. What a load of bullshit - how could they be "local" or "limited"? Memory-safety is not a magic solution which will make your language pure and bug-free. &gt; I showed you the exact same thing in Nim. It's normal programming language behaviour. It isn't desirable anyway. &gt; Just crashes every so often, as C/C++ programs generally do Sure. No lies. \s &gt; - it's not like I have a consistent reproduction to send. Which probably means your machine can't handle the load and crashes the game. &gt; Why do you think these things come with crash reporters? To make it easier to create quality bug-reports. &gt; Why do you think Chrome has separate processes for each tab and gives you a button to restart it when the tab crashes? Because plugins are written in js and they're unsafe. XFCE plugins are written in C but I've never seen them crashing anything. gnome-shell plugins are written in js and they DO cause a lot of issues. &gt; Programs crashing is just so normal that we're inured to it, that's the reality of the tools people are still using. It's so normal that I don't remember when was the last time I've seen an app crashing.
&gt; A function will either be specific to a given type, or not typesafe if it uses something like reflection. With typeclass derivation you can derive comparisons for datatypes that are made up of sortable things, while not allowing sorting things that don't make sense to sort. Or just use an implicit comparator function for the type. Or a comparator interface. Or a custom anonymous comparator at the call-site. &gt; Which is not as helpful as it could be when you have large, complex objects with small differences. A diff would be useless - give me the raw strings and I'll diff them in neovim and correct them in my editor. &gt; I think we're seeing consensus gradually developing in language design, with a lot of convergence in recent languages. I don't think so. Think about nim, golang, rust, kotlin, swift etc - they're more different than java and c++. &gt; The language should be good enough without them Yeah, maybe in dreamland. &gt; (and in my experience Scala is). Scala's performance is literally shit. And since it uses persistent data structures it's even worse and wastes more memory. &gt; They're safe, and it's only a few of them. Then go and count them all. All the reflections and macro calls. &gt; LabelledGeneric/Generic could be a language feature (indeed I'd like to see case classes become more record-like). I don't want to see that. It'd just increase the static space of the program with diminishing returns. &gt; Lazy is already subsumed by a compiler patch. Poly-like functionality is already planned for Scala 3. I think most libraries that use Shapeless don't use anything beyond that? Well, most of those "libraries" probably don't use macros and don't care about performance. &gt; All language design is about choosing the right level of restriction. There's no right-level of restrictions - the restrictions are up to the developers. &gt; If we're using a type-safe language then we already believe in using systems to constrain what functions are allowed to do, Macros are typesafe and without them you'd need to either bloat the language to hell or give up safety and the possibility to increase performance at certain cases. &gt; and accept a certain amount of complexity as the cost of that, Nope. Your anti-macro crusade is just an ideological masturbation at best. You'll be never able to create a simple, nice and effective language by filling it with one-trick ponies. Your superficial fears towards metaprogramming is unreasonable. &gt; while trying to write our system in a way that's general enough to cover all the reasonable use cases. Good luck with that. Btw, who decides what's "reasonable"? The purists who don't give a shit about efficiency and readability? Thanks, but no. &gt; Typeclass derivation is the along the same lines - a restricted subset of what macros can do that's safer, but still covers the use cases. Typeclass derivation barely does anything useful - I don't know where did you get the idea that they're. &gt; What things, specifically? Like literally most of the things I've enumerated as the benefits of macros? So far, you've only showed serialization - but that one uses macros too. You say you hate macros and yet you use a macro library - while trying to convince me that shapeless's features should be the language's features. Why don't you just go and create a language for shapeless then? Let's see how that'll work out. &gt; I'll avoid them by avoiding mutability at the language level. "My program doesn't have side-effects - because it doesn't exist and you can't run it so, you can't change anything". So, you avoid mutability at the language level? LoL, guess that kind of scala will be shitty at even more domains than java. So, the only things you'll be able to do in it are crud apps. Web developers never change. This pseudo-PLT bullshit is ridiculous "- how do you solve the problem of data-races? - no mutability, we don't care about efficiency or quality", " - and how do you solve the problem of memory-safety? - by an inefficient, non-deterministic algorithm which will make the app consume 3-4x as much resources + leaking and I'll just throw away the capabilities of real-time programming and the easy interaction with the OS and is low-level libraries.", "- and how do you plan to make metaprogramming easier? - either by creating a limited monster framework or by implementing every arbitrarily useful feature what we could do in a few lines of macros".
&gt; Or just use an implicit comparator function for the type. Or a comparator interface. Or a custom anonymous comparator at the call-site. And where does the implementation come from? Written by hand every time? &gt; A diff would be useless - give me the raw strings and I'll diff them in neovim and correct them in my editor. Using the strings you're already throwing away information. With a structured diff you can do more useful things like seeing if the differences are only in floating-point numbers and ignoring them below a certain threshold if so. &gt; Think about nim, golang, rust, kotlin, swift etc - they're more different than java and c++. They are, but all five are more similar to each other than Java is to Ruby. &gt; Then go and count them all. All the reflections and macro calls. I said what the relevant features were on the very next line. &gt; Like literally most of the things I've enumerated as the benefits of macros? You haven't given any specific examples.
&gt; And where does the implementation come from? Written by hand for each type? Wait, you want to generate comparators automatically?! That's not safe at all! &gt; Using the strings you're already throwing away information. Nonsense. &gt; With a structured diff you can do more useful things like seeing if the differences are only in floating-point numbers and ignoring them below a certain threshold if so. I can see that in the editor's diff too. But with the editor I can correct it easier. &gt; They are, but all five are more similar to each other than Java is to Ruby. Haha, no. The way they do or don't do generics, the memory management, the typesystems, the supported metaprogramming features, the way they do operator overloading, the default paradigms etc. - they're barely related. While the biggest difference between java and ruby is the static vs dynamic typesystem and the syntax which doesn't matter here. Programming languages are more diverse than ever. &gt; I said what the relevant features were on the very next line. Those are the framework's features, not the concrete macros. Btw, are you sure those are the only features which are available? &gt; You haven't given any specific examples. I did a few days ago, scroll up and see when I've enumerated them. Btw, you haven't shown me any example where typeclasses were useful or when they were the ones making something possible - your examples are dependent on macros.
Any convenient way to decouple deserializers for nested structures from the outer deserializer and dispatch them dynamically? ```scala abstract class Blackbox case class Message (foo: String, boxes: Seq[Blackbox]) implicit val codec1: JsonValueCodec[Blackbox] = &lt;custom implementation that fans out to JsonValueCodecs for concrete implementations&gt; implicit val codec2: JsonValueCodec[Message] = JsonCodecMaker.make[Message](... codec1 ...) ``` 
&gt; Wait, you want to generate comparators automatically?! That's not safe at all! It's safe if a thing is made up of comparable things (possibly recursively), which is how the derivation works (and that covers a lot of cases). In the cases where a comparator can't be safely derived it won't be derived at all. Derivation is safe. &gt; The way they do or don't do generics, the memory management, the typesystems, the supported metaprogramming features, the way they do operator overloading, the default paradigms etc. - they're barely related. While the biggest difference between java and ruby is the static vs dynamic typesystem and the syntax which doesn't matter here. The typesystem is a bigger difference than you make it sound, and the clearest example of convergence and consensus - new languages today have static typing with some form of type inference. Syntactic convergence is also gradually happening, error handling is more-or-less settled (we'll never see a major language with checked exceptions again). There are plenty of places where languages still differ, but the range is narrower than it used to be. &gt; I did a few days ago, scroll up and see when I've enumerated them. No, I've been asking you for specific cases the whole time and you haven't given them. &gt; Btw, you haven't shown me any example where typeclasses were useful or when they were the ones making something possible - your examples are dependent on macros. I've shown you cases where typeclasses with derivation are useful. I linked you to a library that implemented something using typeclass derivation without any macros in the library itself, which is what I'm advocating. The current implementation of typeclass derivation in scala happens to use a macro but that's neither here nor there.
1. If you can alter your abstract class to a sealed than you will get ADT which is already supported and doesn't require custom codec. 2. For custom codecs it is enough just add them to implicit scope, no need to pass them as parameters to the `make` call.
JSON can never be parsed as fast as any binary serialized data simply because the JSON format is longer and thus needs longer to load...
Ok, thanks. I wasn't sure if 2 was supported.
How do you find compile times and editor performance compare? I'm interested in something as minimal as possible for writing an API. All of Play's frontend-focused bloat has been off-putting, and the [slim-play](https://github.com/lloydmeta/slim-play) approach doesn't seem to actually remove any of the dependencies. akka-http's routing syntax and untyped-actors haven't made me too excited about trying it either. Anything utilising Cats, Scalaz or Shapeless makes me concerned about compilation speed and IntelliJ in a work environment though. 
Poor sarcasm from someone that probably has zero contributions, both to this channel and the Scala community ... and gets ~10 upvotes, to be the first comment I see. Oh Reddit, you crack me up. But yeah, I guess that when people make contributions to the ecosystem, they should get sarcasm. It's not like they are paid or anything.
Honestly, yeah shapeless can be slow, but anecdotally Akka-Http and Play slowed down my editor much more than http4s ever does (http4s doesn't use shapeless directly). 
Would you be averse to people writing microlibraries for those things? (e.g. a `cats-effect-io-jvm`). I appreciate that it's only one class but that can make a surprisingly big difference for someone getting started.
What exactly do you call a *hybrid* Scala/Scala.js project? That you say you need both `./sbt compile` and `./sbt fastOptJS` seems a bit weird to me, because `fastOptJS` *depends on* `compile` anyway, so doing `fastOptJS` is enough to get `compile` as well. You don't have a single sbt `project` that you use *both* as a JVM project and as a JS project, do you? Because that's not supported, and can have very weird consequences. If you need the same code compiling both to the JVM and to JS, you need a `crossProject`.
Thank you for the detailed response, that was very interesting!
There's https://functionalconf.com/, but yeah I see your point, the simplest explanation probably is supply/demand.
This was the first comment posted, and i can understand the sarcasm. This post wasn't clear at all, jsoniter was (and still is) nowhere mentioned, this post was just a link to seemingly random benchmarks. It wasn't even clear it was posted by a library author. Of course, contributions are welcome, esp when they improve things. Which after reading more posts in this topic i understand. But tip for the topic starter: please be clear when you post something. A title like "I created jsoniter, it's a fast json framework for scala. Here are the benchmarks, feedback please" would certainly have prevented a comment like this.
You just wrote 2 paragraphs of text to justify this behavior. No, it's not justifiable. This is nothing more than the _entitlement complex_ in action. &gt; It wasn't even clear it was posted by a library author A click on the link should have made it clear, as the author's [GitHub username](https://github.com/plokhotnyuk) is the same as his Reddit user. &gt; a reaction like "this is spam" If a link is spamy, then inform the moderators, it's the box on the right. Or at the very least inform other readers about the reasons you think it's spam. That reply, as it is, contains zero information and as far as jokes and sarcasm go, it's a pretty bad one — oh, he took the literal meaning, that's so funny, not. &gt; But tip for the topic starter: Be clear when you post something Sure, but tip for participants in these discussions: be courteous, because some of us have really long memories 😉
The transformer stack needs to die in a fire. It should be a transformer set.
&gt; First of all, googling "play" is a recipe for disaster, and it is very difficult for me to zero-in on something useful, and more often than not you are taken to the documentation of modules for 1.x release (where I would expect a big banner telling you it's old stuff, but no (for instance, google "play framework mongodb", get here, get excited, get disappointed) There actually is a warning: "You are viewing the documentation for Play 1. The documentation for Play 2 is here." However, it's not a big banner. Filed https://github.com/playframework/playframework.com/issues/200 to make it bigger. 
i just started Scala on Idea a few days ago and was having similar troubles. I'm a noob to both, so caveat emptor. my advice, once you are sure you have the JDK and Scala plugin installed is: 'exit project' so you see all your projects, and remove them all. just so you start with a clean slate. start a new scala/sbt project. make sure that src/main/scala is blue, i.e. 'sources root', and that src/test/scala is green, .i.e. 'test sources root'. you can change these via right clicking folder 'Mark directory as' in Project sidebar. note that the first couple of projects i created did not seem to be set up properly automatically, but once Idea 'stabilized' it seemed to work on subsequent projects. it should be set up for you, just something to look out for though. make sure you create your `object WTF extends App...` under src/main/scala. if right click still doesn't work, save and then under file -&gt; 'invalidate caches/restart'. the latter is a good thing to do (afaikt) whenever there is any kind of weirdness. it almost drove me crazy a couple of days ago, but it finally worked, although i have not gotten packages to work yet. hope that helps 
The proposal is based on the feedbacks from Rui Batista a blind person. At the Center, we fully support the initiative.
My bad. It was a while since I worked with Java's maps and I forgot what exactly bothered me. Now I think it was not the fact that you could put null there (though it is bad, and the fact that Scala still allows is is also bad), but that there were no difference between retrieving null value for a key and querying for non-existing key (unless I remembered this wrong again). So something like: val foo = Map("foo" -&gt; null) // if it was Java map foo.get("foo") == foo.get("bar") // == null
Honestly I find the original comment a humorous play of words and your self-righteous sarcasm very off-putting
&gt; It's safe if a thing is made up of comparable things (possibly recursively), which is how the derivation works (and that covers a lot of cases). In the cases where a comparator can't be safely derived it won't be derived at all and you'll get a compile time error. Derivation is safe. You're talking about typesafety. I'm talking about reasonability. &gt; The typesystem is a bigger difference than you make it sound, and the clearest example of convergence and consensus - new languages today have static typing with some form of type inference. Type inference is very old and the difference between the typesystems are sharp. Some have generics, the other doesn't. Some have linear typing. Some have pascalish typesystem. Some has OOP. Some hates OOP. Some has typeclasses. Some has higher-kinded types. Some comes from ML. Some has C++ish typesystem. &gt; Syntactic convergence is also gradually happening, Just look at the languages objectively I've mentioned: they're extremely different. C++ and java have more in common than the new languages. For example golang, rust and nim have very little syntax in common. &gt; error handling is more-or-less settled (we'll never see a major language with checked exceptions again). Checked exceptions were only implemented for java so, your argument is pointless. Rust uses result types and panics. Nim uses exceptions, effects and optionals. golang has if err != nil. The situation is less settled than 10-20 years ago when it was only exceptions and result types(mostly with MLs). &gt; There are plenty of places where languages still differ, but the range is narrower than it used to be. I don't know what bubble you're living in, but today's languages are extremely diverse. I've enumerated a bunch of examples for you. &gt; I'm advocating building those features into the language, so it doesn't matter how many macros there currently are. You won't be able to replace macros with language features efficiently without bloating the language, giving up important features and brainwashing the community with the purist dogma. &gt; They're the ones I think matter, and the ones I've seen used in the wider ecosystem. And by "the wider ecosystem" you probably meant the ecosystem where people like you are generally purists and don't care about performance, only how they'll try to force typelevel hacks into their code. &gt; No, I've been asking you for specific cases the whole time and you haven't given them. I'm not going to enumerate specific examples about how me or my companies uses/used macros to minimize boilerplate, safeguard resources, perform compile-time computations, analyze code for safety checks and generate specialized types. If you want to learn about metaprogramming then you can use the internet. I'm not your tutor but I showed you the way. &gt; I've shown you cases where typeclasses with derivation are useful. You haven't. &gt; I linked you to a library that implemented something using typeclass derivation without any macros in the library itself, The linked library used another library which uses macros and reflection. Your middleware is useless. &gt; which is what I'm advocating. And you're a hypocrite for that because the library uses macros behind the curtain. It's like saying you're against killing animals and then you go and buy the meat from the shop and say your hands are "clean" because you didn't kill those animals personally. &gt; The current implementation of typeclass derivation for scala happens to be a macro rather than a language feature but that's neither here nor there. Well, if it's a macro then it's a macro.
that is great and all, i really appreciate the attention you are giving to accessibility. i only wish you could listen equally well to the general feedback from the rest of us, non-blind developers. it would, to paraphrase you, make an "enormous difference to the experience of programming in Scala" to a *very large* number of users.
Awesome looks great! Thanks for the continued effort on this /u/alexeclu! When would you personally suggest to use the new cats effect over this? Do they fit different niches or is it eventually going to replace monix?
&gt; this does not mean we are not focused on improving Scala for non-visually impaired developers. mass-closing bug reports doesn't sound a lot like what you just described.
It's not a language change. It's a tool to convert scala code to text that can be consumed by a speech synthesizer. (ex: `cat scala | scala-espeak | espeak`). You can read the proposal here: https://github.com/scalacenter/advisoryboard/blob/master/proposals/016-verbal-descriptions.md
&gt; Type inference is very old Sure, but 10-20 years ago neither C++ nor Java had it. Today not only do new languages have it, but those languages have added it as well. &gt; Some have generics, the other doesn't. Some have linear typing. Some have pascalish typesystem. Some has OOP. Some hates OOP. Some has typeclasses. Some has higher-kinded types. Some comes from ML. Some has C++ish typesystem. There's still some variation, but it's much narrower than it was 10-20 years ago when we had languages with no types at all, languages with prototypal inheritance, purist OO-inheritance-for-everything languages... &gt; C++ and java have more in common than the new languages. Agreed, because Java was specifically designed that way. C++ and Java are not reflective of the variety of languages that were around 10-20 years ago. As I said before, all 5 languages you listed are more similar to each other than e.g. Java and Ruby. &gt; Rust uses result types and panics. Nim uses exceptions, effects and optionals. Which are different names for the same things. &gt; The situation is less settled than 10-20 years ago when it was only exceptions and result types(mostly with MLs). 10-20 years ago we had error codes, errno, result types, unchecked exceptions, and checked exceptions. &gt; I'm not going to enumerate specific examples Because you know typeclasses and HKT can do the same thing better. &gt; The linked library used another library which uses macros and reflection. Sure, it uses the one macro library that I've consistently said should be part of the language, and that in other languages is part of the language. &gt; Well, if it's a macro then it's a macro. If the macro was replaced with a language feature tomorrow, it would need zero changes to the library I linked, nor to other libraries that use the same feature. The way derivation is used in the community it's a de facto language feature.
ah, so a utility. i can see how that would be a good option, considering it wouldn't require current or past scala code to be made to be accessible, but there's of course the weakness that if the scala-espeak utility isn't maintained the language can become inaccessible again is there any talk of long-term support for this tool or proposal yet?
You guys are doing great! Will there be a proceedings?
Ye it was joke haha
Standard library containers are just containers. If your ruin your pointers and don't use RAII properly it's your own fault.
&gt; If your ruin your pointers and don't use RAII properly it's your own fault. Yeah, that's exactly the problem of an _unsafe_ language. You've just proved yourself wrong ;\^)
https://monix.io/blog/2018/03/20/monix-vs-cats-effect.html
It'd be nice if you finalized the post with a kind of "best practice" solution using `raiseError` and `attempt` so there's no confusing mismatch between `Try` and `IO`. `EitherT` is fundamentally useless here, and defeats the purpose.
I was attending. Just didn't check this channel. It was a great talk. The other his talk on Composing Programs was also pretty good.
I was wondering if there is any good reason to think that Scala will continue to stay strong, with those Redmonk Index reports showing that it's popularity has dropped three quarters in a row. 
Everyone read the survey last week. 
That's a great read, thanks ! Especially enjoying the honesty
Doesn't seem to be an isolated measurement: It is consistent with other metrics like [PYPL](http://pypl.github.io/PYPL.html), [Redmonk](http://redmonk.com/sogrady/2018/03/07/language-rankings-1-18/), [GitHub data](https://www.benfrederickson.com/ranking-programming-languages-by-github-users/), [Google trends](https://trends.google.com/trends/explore?q=%2Fm%2F0_lcrx4,%2Fm%2F091hdj), sentiment analysis on [Twitter](https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=scala%20kotlin&amp;src=typd).
You shouldn't use exception-based extractions. Put your regexes in a list and map the list with the according extraction function, something like `regex =&gt; Option[element]` and flatten the list after. 
There are many good reasons. Scala is a distinct language offering unique features and programming style and it is being actively developed. Of course it can fall and rise from time to time. Those who use the language just because they have to (existing project, no matter what language — just money) might switch because Scala is not in hype. Companies may decide to use java because it's easier to hire. But there will be always people who will prefer Scala because it's distinct and there will be companies who will not miss the opportunity to hire those enthusiasts.
It depends... mostly from used data structures and memory access patterns. Please see this great talk for details: https://www.infoq.com/presentations/functional-performance
How would you control the interaction of two effects that don't commute? Suppose you have a logging effect (`WriterT`) and a cancellation effect (`OptionT`). A computation logs something and then cancels. Do you get the log, or not? There's no one correct answer for the general case, and having a transformer stack allows users to choose which behaviour they want at the point of use (with something like final tagless style).
Scala didn't get a marketing push like many new languages in recent years - its popularity is purely organic and grounded in its technical merits. IMO Scala is the best language going on a technical level (in terms of languages with a similar level of mainstream popularity - I do think e.g. Idris offers improvements over Scala, but not enough to make up for the lack of a comparable library/tool ecosystem), and I expect that to show in the long term popularity. A drop in recent quarters is most likely due to Kotlin hype; Kotlin's design is superficially nicer than Scala's at the cost of underlying incoherence (e.g. I believe the language has all of the ad-hoc solutions listed on https://philipnilsson.github.io/Badness10k/escaping-hell-with-monads/ , each as a separate language feature, which means they're superficially better suited to those use cases but you don't get the generality of the Scala approach that lets you write reusable library code that can work with all these effects and more).
[Copy pasting from /r/rust] Great to see this here! I hacked out the little [proof of concept](https://github.com/scalacenter/advisoryboard/pull/37) in the original proposal. If anyone knows Scala, we need help with [input / expected output](https://github.com/MaxwellBo/scala-verbal-descriptions/issues/3) tests. I did a [similar thing for Python](https://github.com/MaxwellBo/neoreader) a while ago, that actually turned out pretty well!
If we're speaking anecdotally, the worst thing by far for the project at work is Slick's macros. One of the DAO files kills IntelliJ regardless of RAM settings, and an incremental compile on it can take a ridiculous amount of time. I don't even know where to begin refactoring it. Anyway, it's going to be a rainy public holiday tomorrow, so maybe I'll give http4s a try. Thanks for the response.
I love this change. Besides helping out blind users I think it will be useful for the community in a wider sense if it helps people standardize on the verbal representation of code as I have noticed many of us read the same code aloud in different ways which can hurt communication, for example when pair programming.
Monix is one of my favorite libraries in the Scala ecosystem. Thank you for all your work!
Sinisa Louc follows on from his hugely successful first article on 'Demystifying the Monad in Scala' with his Part 2, which tackles the subject from a more theoretical point of view. A seriously awesome read for anyone who's looking for a more in depth explanation into monads!
If you have a concrete idea, you can create a proposal for the center: https://github.com/scalacenter/advisoryboard#proposals-for-recommendation
Thank for the suggestion!
seems that main user in Asia is Japan. I'm also thinking explanations, 
Yes, I have seen Japan activity, but it still doesn't make sense. I would expect similar distribution across all countries... I have interviewed several people from India, so there are some developers. It's weird that there is no company to organize a conference... Sadly, I don't have any experience with developers from China 
&gt; sentiment analysis on Twitter Is that a real technique, or it just means "my anecdotal experience looking at a twitter feed"?
[Fix bugs before writing new code](http://wiki.c2.com/?FixBugsFirst) [The Joel Test: Step 5](https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/)
I know, a `SafeApp` would be useful. We just need to be wary of feature accretion, because once in, it will be really hard to remove it, this being a foundational library that other libraries depend on. &gt; Would you be averse to people writing microlibraries for those things? (e.g. a cats-effect-io-jvm) Of course not. We might end up with some sort of `cats-effect-contrib` too, where the same concerns for API stability don't apply.
Hi! Its great to hear you! Have a fun and post more jokes!!!
I am not the author of the article, but he is a colleague of my. I have passed it on to him and he agrees it would be a good idea to add a bit on using these constructs. Thanks for your feedback!
&gt; That consensus in language design does happen. Type inference is present in languages for decades. Even the implementations are different: just look at the mentioned languages. &gt; Unityping. We're not seeing it in new languages. Even e.g. Python is adding some level of type support. Python has unchecked optional types and they barely make any sense. &gt; And "no generics" is just Go but you keep using it as an example. We're talking about recent languages and at least I've mentioned a language which isn't 20+ years old. Golang totally ruins your narrative because it's extremely different. Just like Nim. And Rust. And Crystal. &gt; I'm comparing general-purpose programming languages; Ruby is not general purpose, it's a web programming language just like java. But the latter can be used at a larger scale at least. &gt; that we used to have a scripting language/systems language split and have now seen a convergence is part of my point. And no changes so far: desktops, browsers, editors, games - native for the engine, scripting for scripting. In fact, there's more and more scripting thanks to the nodejs plague. &gt; No they're not. Different mechanism, different behaviour. Hardly. errno is just a more standardized version of error codes. &gt; But we're not seeing new languages with that kind of design. And according to you it's because there's a "consensus" between new languages but in the reality, error codes are for low-level languages invented many decades ago. Leaving error codes behind is a 30 years old "trend". &gt; Exactly, it only does a few limited things, which makes it much safer to work with than general unconstrained metaprogramming Hygienic and typesafe macros are much easier to use and much more readable than your library sample. They can also do far more than a little serialization. &gt; but it covers all the important use cases. Absolutely not: this just further proves your laziness and ignorance about metaprogramming. As you said, shapeless can only do a few limited things but not more safely as you try to present it. &gt; Haskell already exists, so I don't have anything to prove. 1. Haskell doesn't prove shit, only that pure FP can do far less and much uglier than what macros can do - you can't even hope to achieve half of the benefits of macros with just purism. 2. haskell is a very bloated and poor-performant language: it just proves my point about purists only caring about line-count in *trivial* samples. 3. Haskell have tons of useless features, language extensions, typelevel hacks and it's still just an old and failed language. With your TCs and HKT you can't track effects effectively or get protection against data races or generate complex boilerplate without overhead or check the structure of your code against non-trivial constraints. If you remove macros the only thing you'll have is a kitchen-sink, like in haskell. So, if haskell works so great, why don't you go with the haskellers and "save" the industry from those evil non-purists? Why is haskell being a native and 30 years old language achieved so little in comparison? &gt; Sad to see you going back to the insults Saying out loudly how you act and think might sound like insults but they just express your ignorance about these topics. Your dogmatic purism and superficial fears towards metaprogramming are not logical. &gt; I guess you've realised you're losing the argument again. I you'd have proof to support any of your nonsensical and radical statements... but you just continue to repeat your misconceptions and ignore reality and hope that I'll believe you just like you believed the nonsense of those charlatans who filled your head with bullshit. "again" you say? First, you'd need to look into the industry and construct your arguments on a rational foundation. But your arguments are filled with radical hate against industry-proven techniques.
I don't think I've ever had an error upgrading Scala via Brew on the Mac. But, then, there's always a first: ==&gt; Upgrading 1 outdated package, with result: scala 2.12.5 ==&gt; Upgrading scala ==&gt; Downloading https://downloads.lightbend.com/scala/2.12.5/scala-2.12.5.tgz Already downloaded: /Users/objektwerks/Library/Caches/Homebrew/scala-2.12.5.tgz ==&gt; Downloading https://raw.githubusercontent.com/scala/scala-tool-support/0a217bc/bash-completion/s curl: (22) The requested URL returned error: 503 Loop detected Error: Failed to download resource "scala--completion" Download failed: https://raw.githubusercontent.com/scala/scala-tool-support/0a217bc/bash-completion/src/main/resources/completion.d/2.9.1/scala
I'm curious, what is the story for network IO when using monix and cats-effect? Does it have integration with NIO or it's using thread pool to read/write data?
Well, running Scala with Java 9 is a danger itself.
What do you mean? Any pointers what to look out for? 
&gt; Do you also have an opinion about kotlin? It seems like developers are most often looking towards kotlin mainly because it comes from the same people who have also developed their ide. I've been working on a Kotlin project. The language is awesome compared to Java, but I prefer Scala. My biggest issue with Kotlin is the current reliance (in industry at least) to use Java libraries. e.g. a Kotlin project is likely to use: Spring, Jackson, etc. Which kind of misses the point of the language and restricts your style. Kotlin will improve once it has it's own native libraries. I can only presume Scala was once like that.
Probably because it is already obsolete and unsupported, starting today.
Also, if you run the compiler with fatal warnings enabled, there might be new compilation failures.
I don't think so. Adoption is crucial for a language to survive. Due to the lack of interest over the years to tackle issues like documentation, marketing, tooling and compatibility, Kotlin had a huge opening and used it to its advantage. Kotlin devs didn't care about the 1% of Java developers who adopted Scala, but considered why the remaining 99% of Java developers didn't adopt Scala. They worked on addressing exactly these issues and their adoption rates speak for themselves. Their adoption comes from the same pool Scala attracted developers in the past, so it's only reasonable that less people will adopt Scala going forward. The numbers from RedMonk are in line with other metrics like [Stack Overflow survey](https://insights.stackoverflow.com/survey/2018/), [PYPL](http://pypl.github.io/PYPL.html), [GitHub data](https://www.benfrederickson.com/ranking-programming-languages-by-github-users/), [Google trends](https://trends.google.com/trends/explore?q=%2Fm%2F0_lcrx4,%2Fm%2F091hdj).
how is it related to scala. is your bot built using scala ?
Yup! I started building and training my NN with [neuroflow](https://github.com/zenecture/neuroflow). I then realise the bot won't have access to the neuroflow lib while compiling on riddles.io's servers, unless I upload the sourcecode for it and all of its transitive dependencies (hint: it's a lot). So I've written my own implementation of the evaluation phase of the net, for use at their servers. I still use neuroflow for the training phase, though, to avoid having to implement backprop and all of that.
Not that according to your Github data link, the following are also in steep decline: * JavaScript * Java * C * Haskell * Clojure * Elixir * Erlang * R And also I don't think the google trends is very negative. It looks like scala reached its highest interest ever, a couple weeks ago.
How do I specify this new `-Ybackend-parallelism` flag? I've tried w/ space, equals and `:`
You basically have three categories: - Languages like JavaScript, Java and C that have credible alternatives with a lot of steam which address the niche these languages have. Given the amount written in these languages, they will never go away though. That's why you see a _relative_ decline while they still have string growth in absolute numbers. - Functional languages have largely failed to keep up with the overall rise: Haskell, Elixir and Erlang show this very well. They are also well-entrenched in their niches with no large alternatives around that could threaten them. - Declining languages like Clojure, Ruby, PHP, Objective-C and CoffeeScript that have already reached their peak in the past and are now being rapidly replaced by better alternatives, with not much activity going toward new projects. If you look at the trajectory of Scala, it's most similar to Clojure in this regard. Unlike Haskell or Erlang it's not that unique to be immune to competition (or rather its flaws are too big to attract new developers with its unique abilities). Considering all pieces of evidence, I think it's reasonable to sort Scala into the last category. Google trends suggest that Kotlin will have surpassed Scala in interest by the end of the year.
&gt; But it's notable that all five languages you listed have some kind of type inference. "Some kind" - so much "consensus". All five languages have integers too. &gt; 10-20 years ago there were indeed some languages with type inference, but there were also languages without types at all and languages with full "manual" typing with no inference. That was more like 20-30 years ago. Btw, we still have new script languages. &gt; We're comparing the languages of today to the languages of 10-20 years ago, that's the whole point. Nope, we're comparing the new ones to each other because they're different. Just look at all the languages since 2000: they're very diverse. You just want to talk about some of the new statically typed ones to push your narrative. &gt; It became popular due to Rails, So, a webprogramming language. &gt; but the language was originally general-purpose and is used for non-web things (e.g. puppet, some FreeBSD package management tThey can do so much that you haven't been able to give a single example this whole thread.ools). Ruby wouldn't be a thing without rails. And btw, it's just a scripting language and by definition they're only good for small scripts and non-performance sensitive stuff. &gt; We can compare Java and Python if you prefer, the point goes through all the same. We can compare golang and rust too and your point will fail instantly. Or we can compare rust and nim. Or rust and crystal. Or swift and crystal. Of course, you don't know these languages that's why what you say is bullshit. &gt; Nodejs is a good example of the very change I'm talking about: people are increasingly writing the whole application in javascript rather than splitting between two languages. nodejs is just a minority and it only covers the minority of the webdevs. &gt; They can do so much that you haven't been able to give a single example this whole thread. You still think that denying reality and repeating the same nonsense will help you with your little crusade? Your only shitty example is totally based on macros. Look, if you're too dumb to understand what can you do with metaprogramming then don't even try to "argue" because it just makes you look like a tool. So far, you're just crying for one-trick pony features because your little typeclasses and HKT can't do shit without them - but if we'd have such features then there's no need for TCs. Of course, we don't need typeclasses because macros look nicer, they're easier to write and read and on top of that they can *actually* create great features like for example in nim they constructed/enhanced concepts, parallel disjoint checks, pragmas etc. And what can you do with typeclasses and purism? Nothing, just have half-assed "solutions" for effect tracking, concurrency and memory management. Then you can either give up most of the programming domains or apply your "extra" code review because that's all you have - the illusion of safety and productivity. And of course, as a zealot you won't admit it. Purists have more pride than wit.
I've been doing a crash course of scala recently. I don't come from a scala or java background, and I'm having some issues finding good information on how projects should be structured. I've done a lot of small exercises and I'm at the point where I'd like to build my first small app, but unsure of how everything I've learned so far translates into a full app. Any resources about this sort of thing or examples would be super helpful. Thanks!
Looks really nice! But it needs more documentation. 
Spark is using it or should have used it in any of the earlier versions... But for me nothing against the other two you pointed out but ik n my opinion .. it's way more better than Finagle 
Thank you for the feedback! I will try to update the readme with more details. Is there something specific that you were missing?
Have you looked at Scrooge? My instinct for things like this is to rely on existing standards as much as possible; if this is nicer than existing solutions then the README should explain those selling points.
I've had a hard time wrapping by head around typed actors, but this makes sense. I guess the "Is everything a state machine now?" feeling I had was actually the result of a very conscious design decision. 
What benefit do you see in RPC over HTTP communication? If RPC is not a must I would consider encoding protocol with [endpoints](https://github.com/julienrf/endpoints)
try/catch in Scala can use `PartialFunction[Throwable, T]` in it's handler import util.control.NonFatal val arithmetic: PartialFunction[Throwable, String] = { case e: java.lang.ArithmeticException =&gt; "division by zero" // more ... } val catchAll: PartialFunction[Throwable, String] = { case NonFatal(e) =&gt; "catch all: " + e.toString // more ... } try (1/0) catch(arithmetic /* more ... */ orElse catchAll)
Heroku is going to be the simplest to get everything up and running on, but their free tier will spin down apps when they're not used. Azure tends to have good pricing options and a generous free tier, but will probably take a little more work to integrate (e.g. I don't know how much support they have for deploy-on-push-to-github).
If you're talking about code structure I wouldn't worry - Scala's strong type system makes it very easy to refactor, so you don't need to build a "code architecture" that will scale to a large app; rather it's fine to start with everything in a single layer and factor out pieces as and when that becomes necessary. If you're talking about project structure in terms of filesystem and git repository, that's going to depend on what tools you're using. Personally I'd recommend using maven and sticking for its defaults in terms of how things are laid out (i.e. main source in `src/main/scala`, test source in `src/test/scala` etc.), using the release plugin to do releases and accepting how it formats VCS tags etc., stepping up to a multi-module project as and when needed. If you're using another build tool that will have its own way of doing things.
sbt doesn't work with it because it checks the jvm version and it's too new so it doesn't recognize it and asks you to download java newer than 6.
didn't 1.1.1 [fix that](https://github.com/sbt/sbt/blob/bde197f2fca768c29e02009915fe0c4ab5334c9d/notes/1.1.1.markdown)?
do I have to update the launcher for it? because otherwise, no.
Honestly I won't think about updating any project till 11. GA's out now so maybe a good time to do A-B breaking/performance testing and get a leg up on September.
Also based on your wording, you’re not _actually_ running separate sbt commands for each of the tasks, right? Sbt startup easily takes up most of the runtime of each run. Keep sbt open and run the tasks from an active sbt session, that will dramatically cut down on run time.
i never said anything not even remotely resembling anything like that
It doesn't work on Ubuntu 16.04: andriy@notebook:~$ sbt -java-home /usr/lib/jvm/java-9-oracle about [info] Updated file /home/andriy/project/build.properties: set sbt.version to 1.1.1 [info] Loading settings from idea.sbt,build.sbt ... [info] Loading global plugins from /home/andriy/.sbt/1.0/plugins [info] Loading project definition from /home/andriy/project [info] Updating ProjectRef(uri("file:/home/andriy/project/"), "andriy-build")... [info] Done updating. [info] Set current project to andriy (in build file:/home/andriy/) [info] This is sbt 1.1.1 [info] The current project is ProjectRef(uri("file:/home/andriy/"), "andriy") 0.1.0-SNAPSHOT [info] The current project is built against Scala 2.12.4 [info] Available Plugins: sbt.plugins.IvyPlugin, sbt.plugins.JvmPlugin, sbt.plugins.CorePlugin, sbt.plugins.JUnitXmlReportPlugin, sbt.plugins.Giter8TemplatePlugin, com.timushev.sbt.updates.UpdatesPlugin [info] sbt, sbt plugins, and build definitions are using Scala 2.12.4 andriy@notebook:~$ sbt -java-home /usr/lib/jvm/jdk-10 about The Java Development Kit (JDK) installation you have is not up to date. requires at least version 1.6+, you have version 10 Please go to http://www.oracle.com/technetwork/java/javase/downloads/ and download a valid JDK and install before running . 
I looked around and both Heroku as well as Azure but they are billed by deployed time, not actual execution time. If I have to pay for the app simply sitting idly it would be really cost ineffective. Otherwise I would need like a cron job to automatically put it up every once in a while. Thanks alot thou, I'll keep on searching
[Monix.io](https://monix.io/) looks quite neat, tho [older version](https://web.archive.org/web/20180117183356/https://monix.io/) was a bit more... highlighty. Also, I see a lot more [sbt-microsites](https://47deg.github.io/sbt-microsites/) recently, which I quite like. Make sure to put Quick Start section right under highlights :)
You need to download the latest sbt launcher
Yes, the bug is in the launcher.
&gt; And if 10-20 years ago there had been languages without integers, that would be progress. (Standardising on IEEE floating point was an important piece of consensus and progress in language design, FWIW) What a progress... &gt; Circular logic. Those languages do get used for large codebases, including a substantial subset of what we'd traditionally think of as performance-sensitive work. "Used" vs "fit". See how ruby worked out for twitter. Or php for facebook. The latter is more interesting because of the main components being written in C/C++/D/Java. Hack and js are used for the front-end only. &gt; Any of those pairs is more similar than Java and Ruby or Java and Python. Java and python are a little bit different but java and ruby are very similar because: they're both pure OO, both prefer single inheritance, both uses exceptions for error handling, have modern garbage collectors, run on VMs, have strong typing and minimalist module systems. The differences of ruby are: it's dynamically typed, doesn't generate bytecode for the user(unlike python) and the syntax is a bit different. We can safely say that the only meaningful difference is that one is designed to be interpreted and the other one is not. Python may not be pure OO but it shares a lot with ruby and java.
/u/joshlemer, can we ban this bot: https://www.reddit.com/user/TheDataIncubator
Yes
I'm using JDK 10 as a runtime. Looks like it's working fine. Compiled with JDK 8 though.
&gt; See how ruby worked out for twitter. Or php for facebook. Very well, given how successful they've been. Now that they're huge they're in the rare position where it's worth spending thousands of man-hours to get small performance improvements, so different languages make more sense - but they would never have got so big in the first place if they'd started in C/C++/D/Java. &gt; Java and python are a little bit different but java and ruby are very similar because: they're both pure OO, both prefer single inheritance, both uses exceptions for error handling, have modern garbage collectors, run on VMs, have strong typing and minimalist module systems. To the extent that Ruby is pure-OO, so are all the entries on your list; Ruby has traditional OO inheritance but also metaclasses and module mixins, so you don't model everything in the traditional-OO "dog is an animal" way the way you do in Java. Checked exceptions are very different from exceptions without that kind of mechanism, and the Ruby interpreter is not a VM in any of the usual senses. Saying they both "have strong typing" is pretty ambiguous but I can't see any level on which it's true: the difference between having a type system and not is much bigger than any "strong/weak" distinction, but even assuming you're ignoring that and talking about runtime types, Java's behaviour there is actually quite different (erasure). &gt; We can safely say that the only meaningful difference is that one is designed to be interpreted and the other one is not. Supposing for the moment this were true, even then we can see that all the languages on your list fall somewhere between the two: they're all more interpreter-friendly than Java but not so interpreter-reliant as Ruby.
I second ur thoughts with only one point to consider is , Scala creates so much of object s that gc has to be really efficient 
/u/kpws after having received a PM from the user, they explained they are not a bot, and so I have lifted the ban on the condition that they can only post at most once for any given event. We'll see how that goes for now.
they are still in violation of https://www.reddit.com/wiki/selfpromotion
Don't you mean JRE 10 as Runtime and JDK 8 as your Compiler?
It doesn't worked for me too, I tried latests from: https://dl.bintray.com/sbt/debian/
Yes! Whoops..
Using it from another language can still be possible if you define, e.g., an http API with it :) Here, I am call it from JavaScript: fetch(baseUrl + '/Api/subscribeWebPush', { method: 'POST', body: JSON.stringify({ subscription: subscription }), headers: { 'Authorization': currentAuth } })
Thank you - Monix.io looks very clean and simple, and I think you're right about having Quick Start on the front page. I'd also like the front page to be easily skimmable, so you can quickly determine if React4s is something you'd like to use or not.
It is working solution to overcome both a topic problem and Scala 2.12.5 compiler bug with macros on JDK 9: andriy@notebook:~/Projects/com/github/Sizmek/fast-string-interpolator$ sbt -java-home /usr/lib/jvm/java-8-oracle -no-colors clean ++2.12.5 'benchmark/jmh:run -jvm /usr/lib/jvm/jdk-10/bin/java -prof gc -rf json ff jdk-10_scala-2.12.5.json .*'
Hmm, it looks like the fix for Java 10 version parsing didn't actually make it into sbt 1.1.1. Until this is fixed (hopefully in 1.1.2), you should be able to run it by hand: java -jar /usr/share/sbt/bin/sbt-launch.jar -Xmx1536M
Would it interest more people if the title is changed to `Writing unit tests in your Scaladoc`?
something something anticommutators (/s)
No offense to all the work that'd been put into this, but if anyone in teams I ran started adding in a library that used the syntax you are enforcing in this it would not pass code reviews and would be removed. Its impossible to rationalise or infer the intent of the code from looking at it without existing intimate knowledge of the workings of the library which might be OK for bedroom haskell projects but nowhere else. 
I have added benchmark results with JDK 10: https://plokhotnyuk.github.io/jsoniter-scala/ In general results are the same as with JDK 9. No new big (&gt;50%) regressions or improvements was detected comparing with changes from JDK 8 to JDK 9: http://jmh.morethan.io/?sources=https://plokhotnyuk.github.io/jsoniter-scala/jdk8.json,https://plokhotnyuk.github.io/jsoniter-scala/jdk9.json No big changes was detected in benchmarks with samples of realistic messages, but some differences can be spotted for special cases. Most of benchmarks where throughput was decreased on 5-10% are with higher allocation rate, than others. It looks like changes in GC work or default options. Also in this results some improvements can be spotted in String to Int and back conversions, and in encoding/decoding of long (more than 100 chars) ASCII strings to/from bytes. 
I mean it's barely either of those things, it's just retrying io. What happens to your state when there's an infrastructure fault rather than something trivial? 
Hey! I think people would probably like to see a contrast with http://tpolecat.github.io/tut/ which is kind of the current king (pardon the pun) in the space. 
Also would be helpful to compare/contrast with [sbt-doctest](https://github.com/tkawachi/sbt-doctest) which is reasonably popular (used by cats for example).
Throughput of string interpolation with Scala 2.12.5 compiler improved in more than 2 times: https://sizmek.github.io/fast-string-interpolator/
What do you think of GraphQL?
I can accept that you don't like the style. Everybody has a different taste. But saying you cannot infer the intent of an API definition done with this lib is far-fetched. I mean it is just another way of doing standard HTTP URLs. In the end, if `:&gt;` is putting you (and maybe more people) off it should be a small change in the dsl to make it more HTTP like. Furthermore, I don't understand your comment about the need to know the inner workings do understand the dsl. What is it exactly you need to now to use `:&gt;`? To sum this up, I am happy to receive constructive criticism but your comment lacks the "constructive" part. You only express your personal distaste and do not suggest any improvements.
I like it for certain use cases. Why do you ask?
(not the person you're replying to) Symbolic names are inherently meaningless. That means that such an DSL has no semantic meaning. It's just a bunch of random characters as far as I can tell without knowing what meaning your library gives to those symbols. The requirement for such knowledge to even understand the _meaning_ of the code – as opposed to how it works in detail – is extremely expensive. I should not need so much outside context to read a simple line of code. To illustrate, I have no idea what this means: := :&gt; "fetch" :&gt; "user" :&gt; Query[String]('sortBy) :&gt; Get[List[User]] Whereas something like this would be much more straightforward, and will work with IDE autocompletion too: route(method = "fetch", path = "user", Query[String]('sortBy)), output = Get[List[User]]) (I can only guess how your DSL works, so some of those names are probably not right.) There are other ways to have meaningful names if a fixed list of method params does not work, e.g. route(method := "fetch", path := "user", query := Query[String]('sortBy)), output := Get[List[User]]) In this case `:=` would be creating an instance of e.g. `RouteParam`. Yes, it's a symbolic name, but its just one, and its semantic meaning is clear because it mirrors assignment. --- I also don't see much value in the brevity that symbolic DSLs provide. Scrolling is not the hard part of reading code, understanding the code is.
But to be honest you also had to learn how an HTTP route is defined before you knew what: Request(GET, "/fetch/user", ...) means. And the same applies here. You have to read the docs (&lt;5mins for the API part) to understand how it works. But this is something everybody has to do if he/she wants to use a new lib. But again, I understand that people may don't like the dsl. As stated it was inspired by Servant and a starting point. And I have to say I like your last suggestion and consider putting it into the lib as an alternative.
I disagree with above comments. I like the syntax, don't mind it, it takes me "an hour" (just like any other skill) to get used to but then it leads to very nice and terse way of reading, writing and reviewing stuff.
If I run into a problem with `(fetch _ :|: create _ :|: =:)` then how can I search google, stackoverflow, github issues, etc for it?
As this is an early stage project I don't think you will find something on SO yet. When it comes to issues you just open the issue page in Github. But as said this is very early stage so there are right now no open Bug issues. This is more like a Beta version. I hope people (besides me) try it and may find bugs and have suggestions on how to improve and open an issue/ask a question on the Github project. So if you run into a problem or have a question feel free to create an issue and I will try to respond in time.
A very interesting episode and while I usually agree with JDG on mostly everything, I think he's incorrect about the the reason for the backward-compat problems Haskell has regarding typeclasses. The fact that the fundamental ones were decided ages ago *is* a problem to be sure, but the problem is not that they're part of the standard library. (I think it's actually a huge problem that they're *not* part of the stdlib in Scala, but for different reasons.) I think the real reason is that very few Scala users are actually *using* and *relying* critically on these fundamental typeclasses (Monad, Functor, etc.) in Scala. At least in my experience, type classes in Scala tend to be much more ad-hoc and since you don't have to consider effects specially, you just end up with this mish-mash of imperative type-classes that just avoid Monad/Functor by accident of being imperative. In Haskell you can't really avoid M/F sensibly because you must use them to gain access to effects. That is, my hypothesis is that the #(developers using Haskell) is actually greater than #(developers using Scala using Monad, Functor, etc. abstractions). At least that's my experience. 
&gt; But this is something everybody has to do if he/she wants to use a new lib. I don’t think this has to be taken for granted. There are several good libraries where I haven’t had to read the docs to use some new functionality; I just typed whatever made sense in my head and it turns out that was the library API. Python’s Requests comes to mind. There is also a lower bar where you can fumble around with unknown APIs usinh autocomplete and get things working, also without reading the docs. This is easier, and totally achievable without any special talent.
Oh, sorry, a bit of a weird conflation there. I was thinking of a type class which doesn't actually express any of its effects in the types. It just "does things" when you call its method(s). The most innocuous example would be a type class method that throws exceptions without actually telling you that it may throw exceptions. (It could tell you with an Either[Throwable, X], for example.)
Code should, first and foremost, be easy to read. When reading routing code, all you should need to know for a high-level understanding is Http. Using special syntax makes it harder to read the code. What do you gain in return? 
I bet you work in the places `&gt;&gt;=` is frowned upon because of it's scary wizardry haskell syntax. Really, fear of symbols needs to go. Not everything should be a DSL, but something that attempts to be like servant for an http api certainly can be :|
The linked tweet was tweeted by [@http4s](https://twitter.com/http4s) on Mar 23, 2018 22:00:55 UTC ------------------------------------------------- http4s-0.18.4 released: - Timeout middleware cancels effect - Multipart parsing is streaming, faster - New expectOr method on client ------------------------------------------------- ^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
It solves a lot of problems that you might have for typed APIs, yet allows integration with other languages.
Sounds great. When can we see it? I mean don't get me wrong, it looks like you've got a fast json parser here, but if you're going to claim performance is what distinguishes you, show me a benchmark agains avro, colfer, acyclic kryo, or protobuf.
Checkout [endpoints](https://github.com/julienrf/endpoints), we re doing pretty much the same thing ;)
`tut` is different. sbt-example is used for tests on API reference, while tut is for tutorial. What comparable to tut is [Jupyter Scala](http://jupyter-scala.org/). For example we wrote some tutorials of machine learning in Jupyter Scala http://deeplearning.thoughtworks.school/demo/GettingStarted.html 
Our goal is making Scaladoc both a unit test and a readable documentation for API specification. The latter goal was done by the `scaladoc` tool. I believe a tool for the former goal should introduce minimized syntaxes for testing. Unlike sbt-doctest, sbt-example does not introduce any special syntax like `&gt;&gt;&gt;`, `scala&gt;`, because those syntax are not recognized by scaladoc's syntax highlighter. Instead, you just write normal ScalaTest DSL, and it runs as a normal ScalaTest test case, with the help of sbt-example's code generator. sbt-example is very concise since it did only one thing. The entire code base is only 306 lines of code including its tests and specification (written in Scaladoc of course). https://github.com/ThoughtWorksInc/sbt-example/blob/4.0.x/src/main/scala/com/thoughtworks/Example.scala On the contrary, `sbt-doctest` did not have Scaladoc: https://oss.sonatype.org/service/local/repositories/public/archive/com/github/tkawachi/sbt-doctest_2.12_1.0/0.7.2/sbt-doctest-0.7.2-javadoc.jar/!/com/github/tkawachi/doctest/index.html It's a little strange, consider `sbt-doctest` itself is a tool for Scaladoc。
I don't understand what you're talking about. &gt; I think he's incorrect about the the reason for the backward-compat problems Haskell has regarding typeclasses One problem is I don't know the argument you're referring to. The interview seems to be very interesting, but it's an hour long and has no transcript, so can't be skimmed. I don't like podcasts for this reason.
Is there something like python requests in Scala? My requirement is to send simple rest API calls to elastic from spark, would prefer it to be asynchronous. 
Library has 47 releases, all available on the Maven Central. Please, look how to use starting from README page: https://github.com/plokhotnyuk/jsoniter-scala Benchmarks already tests different Scala libraries that uses kryo-4 (KryoMacros, Chill) and protobuf-3 (ScalaPB, JavaPB) under hood. 
Start by structuring your code as library methods. Take dependencies as function parameters. Don't worry about implicits, dependency injection, etc. Just write the simplest code that could work, and use the types to your advantage. Consider using cats IO to isolate your side effects from pure code and you'll be on a really good path
WS is the most like requests, so for quick and dirty I'd consider that. For an application I cared about maintaining, I'd use http4s just because the server side is so strong, and frequently you'll want both server and client. Having the same library able to provide both is quite nice, and on top of that, it's very strong in terms of being safe a functional
As I already said, this dsl is the result of being inspired by Servant to build this lib. What I gained was to get a first idea I can follow during the implementation. But again, this is not something written in stone and I agree that it can put people of and you have to know the lib or Servant to understand it. Again, I will incorporate silentraqo proposal (or a variation of it) to improve UX and readability. I mean that was the whole point of this post, to get feedback to improve this project.
I am not that deep into the Scala implementation of GraphQL (sangria), but does it: - derives a client / server just from the definition - forbids on the type level that you send wrongly typed data, e.g. you defined to send an Int with your call but passed a String - can you share the API definition between server and client so that it is encoded on code and available at compile time Furthermore, Typedapi is thin layer over any HTTP client/server creating standard HTTP calls/endpoints where as GraphQL is an own protocol. I think it is way harder to introduce a new protocol compared to adding a layer over your server/client. And again GraphQL works for some use cases. Typedapi is just a wrapper around HTTP and therefore more generic I would say.
Hmm.. given the post's author is "Team Lead for Kotlin libraries @JetBrains", it's not very surprising that he favors Kotlin over Scala, is it? 
Great, another Kotlin thread.
Thanks a lot! It works greatly with JDK 10.
&gt; I’ve been a happy Java programmer for 15+ years, Java Champion, Certified Java Developer, Staunch Defender of Java, etc, etc, and just a few years ago I would have been as enthusiastic about it, too. If somebody had told me that Scala, for example, had had this particular type inference feature for more than 10 years, I would have just laughed and tried to convert this person back to the holiness of Java, since I have had first-hand experience of Scala’s limitations for real-life enterprise projects. Developer that unironically enjoys being stuck in "enterprise" design patterns gets ecstatic when he gets "java with goodies". Move along people. Only stupidity follows.
It's almost like he has some bias, working at the place where they literally develop kotlin. I like how in the article, aside from the tooling issues (which are fair), everything else is so vague about he develops code that he tries to pass it off as legitimate because "it happened to me!" not "I fucked up".
&gt; Very well, given how successful they've been. Now, you're trying to argue with "success" instead of looking up the technical issues. &gt; Now that they're huge they're in the rare position where it's worth spending thousands of man-hours to get small performance improvements, 1. Twitter has been rewritten a long time ago. 2. the performance increase was anything but small. &gt; so different languages make more sense - but they would never have got so big in the first place if they'd started in C/C++/D/Java. Speculation. &gt; To the extent that Ruby is pure-OO, so are all the entries on your list; I'm pretty sure you've no idea what you're talking about. I recommend you to learn about ruby and java to understand the concept of "pure-OO". &gt; Ruby has traditional OO inheritance but also metaclasses and module mixins, so you don't model everything in the traditional-OO "dog is an animal" way the way you do in Java. You've the wrong definition about OO: "dog is an animal" just represents a concrete type's relation to an abstraction, you don't need OO for that. &gt; and the Ruby interpreter is not a VM in any of the usual senses. Ruby doesn't really have an interpreter because it has a VM. Interpreters are very rare nowadays because languages like ruby compile the script to bytecode first(at least, partially) and then execute it. For example, python even provides a way to compile the source to bytecode. We still call them "interpreted" languages but what we mean is that we've a VM and we don't always need to work with bytecode. &gt; Saying they both "have strong typing" is pretty ambiguous, but I can't see any level on which it's true No, it's not ambiguous, you just want to attack the reality again. &gt; the difference between having a type system and not There's no such thing as "not having a typesystem". &gt; is much bigger than any "strong/weak" distinction, but even assuming you're ignoring that and talking about runtime types, Java's behaviour there is actually quite different (erasure). You're not going anywhere with this argument. You just act like strong and weak typing don't matter. It's quite obvious when you try to redefine everything, you know... &gt; Supposing for the moment this were true, even then we can see that all the languages on your list fall somewhere between the two: they're all more interpreter-friendly than Java but not so interpreter-reliant as Ruby. Java can be interpreted just as good as the rest - just look at the scala interpreter. Ruby is not "reliant" on an interpreter - it has its own VM and it can run on the JVM too.
Is it safe for a library (and its users) to be compiled with `-opt:l:inline` option if it depends only on `scala-library` and `scala-reflection` jars?
Found `-opt-inline-from` option, which can be provided with white list of allowed to be inlined packages to avoid binary incompatibilities of unwanted classes from rt.jar or some 3rd party dependency that can be introduced later.
Much better, thanks!
Link?
Originally saw that post in a Medium daily digest email. Was hoping for a good post about the issues of JDK 10...but instead was met with practically an ad for Kotlin.
Hoping to see an article where you use Tagless Final :-) I found what you had to say very valuable
.. because it is ha hotfix. All actual features are pushed to 2.12.7 - "2.12.7 is tentatively planned for August 2018."
You are right, a `Tuple` or a `Seq` of the serialized arguments would work, too. It even was that way before. But there is one major drawback: The names of the arguments get lost. This might even be fine for a lot of cases, but it is not so nice when using json serialization, because I like to have named arguments when using the API from another language (`POST Api/fun { "arg": "value" }` as opposed to `Post Api/fun [ "value" ]`). Another use-case would be to write a simple client facade for an existing API which would just work with json bodies. Autowire solves this by representing the arguments as a `String-Map` of serialized arguments which it again serializes. But for binary serialization this has the drawback of needing to encode all keys in the map (you cannot leave them out in the serialized payload like you can with a case class or schema). Another alternative would maybe be to use a `shapeless` record with named keys, but then this would become a dependency. Maybe the case-class generation treatment should be configurable/optional? Or do you have a better idea?
Is Kotlin tooling really that much better than Scala if you don't use JetBrains products?
https://twitter.github.io/scrooge/
Breaking: Scala doesn't make a great "better Java"
not really.. I've been doing Scala full time for almost 2 years and I've never had any issues with tooling 
People that complain for not liking the syntax might never heard or use Servant. Although there are things that could be improved (as with any software) I think what you have achieved is pretty awesome, keep up the good work! If I manage to find some time these days I will definitely look into https://github.com/pheymann/typedapi/issues/2, taking it as a personal challenge now XD
D: what are all those annotations?
I was looking at the jobs at zalando Berlin, plenty of scala there. Does anyone know what they use it for exactly? Same for Akka, it's huge so I guess they mostly use specific parts. Anyone interviewed there? What's the process like?
To start with, you don't need to have ScalaIDE, Ensime and IntelliJ installed to have "some" chance of being able to open a Scala project without everything being broken.
&gt; you quickly start discovering its limitations in tooling support, compile times, write-only code, backwards compatibility, interoperability Sounds like a reasonable criticism.
Looks decent. I'd throw in some example code to show how writing components with this library feels like, if it is feasible.
I think it's largely stuff around multiple versions/platforms. There is usually only one combination of the given build.sbt and IDE that works reliably, and that's the author's one. That's why I have always been thankful if some developers accidentally committed some IDE dotfiles to the repo. Then I at least knew which IDE to try first.
thanks and that is the spirit. I big thumbs up is waiting for your PR that fixes this issue :D
Looks interesting. What do you see the main difference/advantage over this lib: https://github.com/japgolly/scalajs-react
Thanks, yeah, that's a good idea.
Java annotations: For when _you_ are better than your compiler! On a serious note, they're a leftover from the java days of old where people actually believed this was a good thing to do. it kind of sucks to not have like 1/2 of that code not typecheck but tis is the lands of play framework.
I don't think op is claiming that this is a bad thing; they are just clarifying that this will not contain additional features.
The ye-olde commit your sbt to the repository strategy. I have seen that, and it does require a customization to the sbt command in emacs. But it mostly works with recent sbt versions. Intellij can also be set to use the local sbt. It only really works with fairly recent versions though. Grep, projectile, and global autocomplete generally never fails, and the sbt command line interface always works. I just generally fork and upgrade repos' sbt builds fail with my toolkit. But I could see the problem for 5 - 6 year - old projects. 
No, it's not the sbt binary that's incompatible, it's some parts of the build definitions that some IDEs just don't support.
Well, that's certainly true of all the IDEs, currently. I guess since I run sbt inside my editor and in Intellij while pairing, I don't really run much of the functionality beyond testOnly, update, debug, and compile through editor/ide interfaces. I assume you are referring to plugin info that's only available in sbt interactive mode. Things like lint errors from scalastyle/wartremover; formatting from scalafmt; errors from clippy, etc.? I guess I'm not fully following. Maybe some specific examples of features you'd expect you be supported would help me here. I can bind any sbt cli command to a keybinding in emacs - but getting extra feedback into the interfaces (outside of the *sbt* buffer or the sbt-shell in Intellij's case) would be a lot of work. It is technically within with my editor, but it's basically just a compile buffer running the cli. Not sure that really qualifies as integrated.
upvoted just for efforts to make world better. But there is too much json libs these days. 
If you like it, you should chip in a few $ into patreon to support it =) https://www.patreon.com/lihaoyi
&gt; It's almost like he has some bias, working at the place where they literally develop kotlin. So what if he does? That's an ad hominem attack. &gt; I like how in the article, aside from the tooling issues (which are fair), everything else is so vague about he develops code that he tries to pass it off as legitimate because "it happened to me!" not "I fucked up". He did say, "Scala was easy to dismiss for me"; it doesn't seem like he tried very hard to get into it. Sad. I expect many of us would feel similarly about Kotlin as he does about Java, and I don't think the author gave Scala enough of a chance to understand why.
&gt; So what if he does? That's an ad hominem attack. What do you mean "so what"? The reason it matters is obvious bias. When on earth will you hear someone go "Our competitor is objectively better than us!" in any way? Obviously, if you work for jetbrains, you're not going to shit talk their product. a review of something like this, to be in any way meaningful, should come from a place of neutrality: As a user, not as a developer. 
No, I mean opening a project in an IDE an having red squiggles everywhere, autocomplete not working, dependencies not being found.
does it still use defaults which are optimized for shared models? e.g. nulled properties are dropped or if value equals default value it is dropped or is it supposed to be used as a "standard" json library now?
Maybe he did not tried scala hard enough, but maybe he did. You can't tell from the article. My impression is he did tried scala, but as a better java. And I think he is right that kotlin is a better "improved java" than scala (I haven't tried kotlin though). Where scala shine is if you're looking for a functional programming language on the JVM. My point of view : - want an improved version of java -&gt; kotlin - want functional programming -&gt; scala The author was looking for a better java so he stick with kotlin and dismiss scala. Seems fair to me.
Can you give an example? To think of it, IDEA necer exhibited that problem for me (I'm mostly hacking Typelevel projects, i.e. cats, circe, algebra, spire, and my own stuff).
Basically anything where you have either different platforms or versions involved. Like targeting 2.12 and 2.11, with some subproject only supporting 2.11. Or having an SBT plugin (which requires a different version). Or wanting to support multiple platforms.
Yeah, I just don't allow those type of projects at work on code review. Cross-build all libs and sbt plugins, only aggregate projects with the same version. If I have a frankenbuild, I expect only the build tool to understand it. I trust sbt and the scala compiler errors more than red squigglies. You can fix some of that with ensime config changes, and I'm sure intellij changes. But, yeah, that's going to happen with weird builds. There aren't a lot of those, though, or shouldn't be. But it would be a major pain to do so. I'd ratherspend the time upgrading the project to a new scalaVersion, or cross-build the libs.
It would be very easy to just cross-build everything. Sadly this usually doesn't work and there is a reason why the version/platform matrix has to look this way.
Akka HTTP seems to check most of those boxes. The examples on their website encourages implicit conversions from HTTP routes to Akka Streams flows, but it looks like you can call `Route.handlerFlow(route)` manually. Separately it seems odd to spin up an actor system and materializer for a server API for request handlers that deal with futures instead of actors.
Finagle looks nice, but conversions between Twitter futures and Scala futures could get tiring.
Can't you just get the parameter name with reflection? Or do you have the annotation because you could, but you want it to look differently?
http4s is really "http 4 cats" -- the first line of the first example in their website wildcard-imports `cats.effect._`. You need to ramp up on and buy into cats if you want to commit to maintaining an http4s server in production. It also encourages wildcard importing the server API and defines a number of custom operators in https://http4s.org/v0.18/dsl
Haven't changed the way that default arguments are handled (e.g. elimination if default); if you want to make it configurable, send a PR to add a flag [here](https://github.com/lihaoyi/upickle/blob/master/upickle/src/upickle/internal/Macros.scala#L281) and [here](https://github.com/lihaoyi/upickle/blob/master/upickle/src/upickle/internal/Macros.scala#L361) to let people turn it off [globally](http://www.lihaoyi.com/upickle/#CustomConfiguration). If you just need to tweak it for one or two cases, you can use `readwriter[Js.Value].bimap[Bar]` to manually define those serializers. The [uJson](http://www.lihaoyi.com/upickle/#uJson) library is meant to be usable as a standalone JSON library, with it's fast parser, minimal Python-like AST and support to read/write the AST of any other JSON library with minimal configuration. If you just want to mangle some JSON and don't need to put it into Scala data structures, then this is for you.
[Play](https://github.com/playframework/play-scala-starter-example/) is actively maintained, has non-blocking IO, doesn't require you to declare implicits (I assume you mean implicit conversion or type enrichment) or import wildcards (again, I assume for implicits defined in the package object), and new symbolic operators / translation is not required.
IIRC, it's the design decision after *a lot* of attempts to figure out a sound way of applying types to actor systems: https://github.com/rkuhn/blog/blob/master/01_my_journey_towards_understanding_distribution.md
The problem is that you have to multiply that by the number of libraries you're using in your project. At my company, we have one major Scala piece of infrastructure. It serves a very important purpose and performs quite well. But because we're a polyglot team, people need to jump in and out of different language ecosystems, and this project has a reputation for not being approachable. And that's with a lot of effort to stick with tools and techniques that are fairly straightforward (no 3D chess on the type level). I can only imagine what people would think if part of the system looked like a soup of operators. I even made the mistake of introducing my own DSL that used clever applications of `-&gt;` and commas as implicit separators of concepts. I found every month, I'd forgotten what the heck I had done, and would have to look at the internals to reeducate myself on _my own DSL_. I've since thought hard about what the verbs and prepositions are that I was trying to cleverly represent, and the result is something far more intelligble to myself and others. To each their own, but I'm glad the ecosystem has migrated toward names over symbols for things that aren't natively symbolic.
This is a reduced example. The main point I was trying to make is that I'm not printing `x.content`, therefore there shouldn't be a reason to evaluate `Jsoup.connect(e.getLink).get.html` In the real code, sometimes fetching the document from the internet will be useful, most of the time it won't be though. That's why I'm trying to have a sort of `lazy` evaluation for `content`
So yeah, I already tried that `() =&gt; String` but thought afterwards that this wasn't great because this would be evaluated every time I access `x.content`, which is costly. I really want it evaluated (i.e : have it fetched from the internet) once. I don't know anything about cats-effect IO, so I'll have a look at that. Thanks.
&gt; - derives a client / server just from the definition Your server is effectively composed from all of the individual fields you want to provide. One major point of GraphQL is the ability to batch up a large number of operations to send from the client at once. There are tools, like Relay and Apollo Client, that compose queries and deliver data to individual points of consumption. &gt; - forbids on the type level that you send wrongly typed data, e.g. you defined to send an Int with your call but passed a String Yes. &gt; - can you share the API definition between server and client so that it is encoded on code and available at compile time There is a standard schema format that every server can present at run-time (or as a build step, if you like). How that is linked to the client is up to the developer
yea I get what you're saying. On the flip side, it's often same problem with named functions.
We use guice for dependency injection. It's the default way of doing DI with play.
You can get around that by: Case class foo(a: Int, private val getB: ()=&gt;String) { lazy val b = getB } Sorry about formatting, on phone I do recommend IO though because it's much more principled and predictable
The @Named is used by guice when you have several instances of the same type that you want to use in different places. For example, if you have two ways of sending a payment, via PayPal and a credit card processor, you might have a @Named("PayPal") paypalProcessor: PaymentProcessor and a @Named("CreditCard" creditCardProcessor : creditCardProcessor
Note that wildcard imports are never required. You can always figure out what's required and import that explicitly. This is what I do. The only time I use wildcard imports are in a very limited scope, never larger than a function, and only when it's a significant improvement. I do wish examples would show the actual imports in use rather than wildcards but I can see how they might be considered unimportant or distracting for the purposes of a quick demo.
Not the OP, but I'd like to know why implicits are so desired for scala developers. I work with a fairly large scala code base and implicits make the code difficult to read. I don't see how it is at all beneficial in the long run. I'm seriously asking what the benefit is so that I can maybe read the code with a more optimistic view.
The function `x.content` will get evaluated multiple times but the lazy val (and thereby the internet fetching) will only get evaluated once.
Ok, thanks for explaining!
This is reason I avoid all Twitter libraries.
Thank you for the pointer. I tried https://docs.gradle.org/current/userguide/play_plugin.html to build a Play project and it worked on the command line but IntelliJ isn't able to resolve any classes. Is the plugin not compatible with IntelliJ's auto-import support for Gradle?
&gt; I don't see how it is at all beneficial in the long run. It solves the issue of injecting a dependency of a particular class or typeclass, in which there should be one and only one of. What would you rather? Annotations? Also would you like having to pass around say, a monoid for some type `A` for every piece of code that you need to combine? How about propagating a particular piece of code polymorphically over a bunch of types? you would have to summon it explicitly every time. On top of this, implicits and typeclasses solve the issue of having to shove code into data classes and operate on them directly as is regular in OO, which ends up in a mix of mutable state and general awfulness. If you hate working with implicits, you can always pass them explicitly. I mean, the implicit parameter list can be treated as just another curried parameter list. Any "implicits that make it harder to read code" only really apply to newcomers to scala or people not used to working with something such as typeclasses (though I'm not trying to imply implicit == typeclasses, just rather the concept of one unique piece of code doing the work).
You'd need to make the parameter to `NewsItem` pass-by-name. (i.e. `content: =&gt; String` instead of `content: String`) and then store the by-name parameter in a `lazy val` field. However, unless something has changed recently, I believe by-name parameters can't be used in case classes (I haven't done much with Scala in several months). You could fix that by not using case classes at all, but maybe there's some reason that you want to use them. Other people are suggesting that you could make `content` into a lambda. That's true, but it will essentially mean that none of your instances of `NewsItem` will ever compare equal to each other, which is presumably why you made this a case class in the first place. 
I use https://github.com/adamw/macwire for compile-time dependency injection. I prefer explicit imports to avoid unexpected implicit conversions, and to help my colleagues understand where symbols come from when they're reviewing pull requests. I understand that calling other libraries might entail converting from Java futures. I was just hoping to not have to then also convert Scala futures to Twitter futures, Monix tasks, etc when working with the HTTP server API.
You don't need to convert implicitly. The `HttpEntity` constructor takes a byte array.
I used a very small amount of reflection macros in the past, and would definitely recommend Meta instead (specially if you are already using 2.12). Much easier to use and understand. But I'm no expert, and IIRC a couple of Meta maintainers/active developers are active here so I guess they'll be able to chime in.
The current state is "in limbo". Scala reflect macros are, just as they always were, experimental. That will not change for 2.13. They will eventually be retired. Their replacement is not here yet. That means it's also unclear what the transition path will be, and what features will and won't be present in a new system. White box macros in particular - where the macros return type is determined during expansion - may not have a future. For now, you can use scala refect macros. They won't stop working, but they may stop working post 2.14. Also, consider not using macros.
I think the current status could be described as: - scala.reflect: abandoned - scala macros: abandoned - scala marco paradise: abandoned - scala meta: pivoted to focus on tooling - scala meta paradise: abandoned Some people are going back to compiler plugins, which might be an option for you. E. g. you can emit/change trees with a compiler plugin, but like macros you can't use plugins to bug fix later stages of the compiler.
My post was never intended for Reddit, nor for /r/scala for that matter. It is not about Scala at all, nor it is even technical. I'm glad it is being downvoted out of here. 
cool!
Intellij is finicky overall with scala, however make sure you have the same version of scala the play project is on and open the sbt project and not gradle
Re: implicit conversions: That's a pretty bad argument against implicits in modern Scala, as there are basically no libraries left in the fp Scala ecosystem that rely on implicit conversions. Typeclasses sure. Conversions are frowned upon as an antipattern 
[previously on reddit](https://www.reddit.com/r/scala/comments/818qar/)
for the gradle plugin also use https://github.com/ia-toki/gradle-play-idea
`scala.reflect` macros will continue working through the 2.x series. They will no longer work in Scala 3. It has not yet been entirely decided how macros will work in Scala 3.
Most typelevel stuff is crosspublished for the JVM and JS, and it works seamlessly in IDEA. I just have to right click the .jvm directory of my subproject to run tests. I have no idea how and why it works.
Play will handle the HTTP side of things, but it does not handle business logic and domain functionality that you are talking about. I'd recommend keeping the service logic and database backend in different modules so that you can keep your database backend logic distinct from your service logic, along the lines of https://github.com/playframework/play-scala-isolated-slick-example Also, I think [Implementing Domain Driven Design](http://a.co/cGOoi0m) would be a really good fit for what you're talking about, as it directly talks about the design of business logic like this. I don't see the need for microservices in your use case -- as long as you keep everything well factored to modules and APIs, you should be able to evolve to microservices when you need them.
For clarification: I meant the pure FP scala ecosystem (scalaz, cats). Akka is not part of it. The lightbend ecosystem and pure FP ecosystem are rightfully divorced. On top of this, scala has at least a pretty cool mechanism when it comes to imports, which is import aliases as in `import cats.effect.{IO =&gt; CatsIO}` for the same of import conflicts, which make wildcards not nearly as bad.
Thank you for the information!
&gt; Separately it seems odd to spin up an actor system and materializer for a server API for request handlers that deal with futures instead of actors. All of the serving of requests is done via streams and actors. So _you_ might not be using actors, but akka certainly is
akka-http server dsl is built on implicits basically http://spray.io/blog/2012-12-13-the-magnet-pattern/
What do you mean by doesn't require users to declare implicits? What about execution contexts for future transformations?
I'm working on a web app with/for a colleague who has strong Java background and wanted to try out Scala showing no fear of hardcore things like shapeless as long as I explain them. So we're using it, and a lot of Typelevel libraries. While I'm at it, I'm putting things that alleviate some of our pains into open. One of those is macro for defining enums for enumeratum without too much boilerplate: https://github.com/oleg-py/enumeratum-macro We use a handful of enums, and for somebody with Java background syntax of defining one in idiomatic Scala looks like an ugly hack, and other macro-based solutions don't really have a good support for Intellij, which we both use. Any suggestions / issues are welcome. Another one is for easy integration of Quill and Monix: https://github.com/oleg-py/quill-monix This is still in the works (only JDBC/H2 that we use is implemented), and affected by an [issue with TaskLocal](https://github.com/monix/monix/issues/612), but it's quite promising - no longer do we have to wrap impure calls in `Task.eval` manually, I can tweak thread pool used for DB separately, and transactions with async code are as easy as they are for synchronous code with plain Quill.
Is that an implementation detail leaking in their api? Or maybe a dependency that could have a default?
I suspect you're way over-engineering. You should start with small modules that you grow organically. This doesn't sound like it requires the throughput/scalability that you think it will. Just write clean modular code, and break things out as required.