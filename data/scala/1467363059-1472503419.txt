Thank you Daxten, very much appreciated. In regards to (3) - could it not be a single DNS? record Like search.service.com, but when you hit the dns request would be routed to one of instances of service? Sorry if it sounds dumb this is not my area of expertise. In regards to (4) - it means you have not tried it or that Lagom is not about that...? Also there is question #0 :)
IME the big thing that people hire for is experience, unfortunately. See if you can find a way to use Scala in a professional setting (e.g. a standalone testing tool or similar at your current job). Failing that maybe try to make some PRs to big-name Scala projects (e.g. Spark itself) - in many open source projects there are plenty of low-hanging fruit, and being a contributor and familiar with the codebase's internals will be a big plus for some places.
FWIW I find microservices are rarely worth it in Scala, but I'll assume you have your reasons. (I also find Akka is a bad idea, but again, will assume you have your reasons) 0) Better to have the frontend talk directly to multiple services. What value would the gateway be adding? 1) Still valid. That said you probably can't implement anything useful without doing that up to a point. This is one of the problems of microservices. 2) Caches are fine - though see if you can manage without one first - since they get recreated if need be (they're not really state as such, their statefulness is an implementation detail). The point is that it should always be safe for your service to be restarted, or for successive requests to be handled by different instances. 3) If you have a single record then whatever keeps that record is a single point of failure. Now for many scenarios that's acceptable (e.g. maybe your infrastructure has a reliable load balancer and you're happy having one load balancer for each service, and that load balancer is a single point of failure for that service - this can scale up a very long way, even household-name sites), but for a "true" microservice scenario that's not good enough.
DNS is not magic, it's all just computers. You can have a DNS server that does round-robin resolution, sure, but you have to worry about how your service instances register themselves with it, how it detects when they've died and so on. And if you only have one DNS server doing those things, it would be a single point of failure, so you have to figure out how to have consensus among a cluster of them. Which is not to say that you wouldn't use the DNS protocol for part of your discovery mechanism - that's a perfectly reasonable strategy - but just saying "DNS" doesn't make the hard problems go away.
Stateless if possible, otherwise ephemeral. The operational key aspect is that it can vanish suddenly without causing long term data loss or corruption
Just pick one I was using, or that looked cool. It really doesn't matter which you pick. I mentioned spark because you said you wanted to work with it. 
If you're using JUnit 4, you might get warnings like the following: [warn] TraitToClassMixinTest.scala:24: JUnit tests in traits that are compiled as default methods are not executed by JUnit 4. JUnit 5 will fix this issue. [warn] @Test def test2(): Unit = { [warn] ^ You can use the following compiler plugin to fix those issues (and restore your beloved tests): https://github.com/scala-js/scala-2.12-junit-mixin-plugin
Thanks, but I prefer to separate the actual function implementation from the "call" (or the link to the call ;))
Why do you think microservices are rarely worth it in Scala? If not Scala, what would you use to write microservices? Or do you mean microservices are not worth it at all? 
Thank you Paul very much for taking time to write this response - it is very very helpful. 1) Only way I can write javascript is using ScalaJs. But I'm not really solving these problems as of _now_, as mentioned I'm just trying to get discussion going and to get better understanding of bigger picture. 2) Cool, I will check it out. 4) Yes, they are using SkyDNS. 5) Main benefit I see is that it gives us lots of for free, such as events when member goes down, distributed pub sub, discovery etc... And that in regards to (2), we can also use their binary format. So I think it is not unreasonable to consider it, but I'd be happy if someone challenged this thinking. slightly offtopic: This is why I love Scala community, that lots of experienced people come and help and pitch in. I just wished there were more responses, I mean there must be a lot more Scala devs talking about this and solving these issues, but knowledge is kind of hard to find (distilled between various google groups etc.) 
"Java" serialization meets this criteria, at least on desktop (Oracle JVM). Too bad Scala miss proper "Java" serialization - deserialization of List (with non-primitive inside) crashes on Android...
Don't hire Scalia developers. Hire great developers and use "get to learn scala" as a perk. 
&gt; 10+ years of Java enterprise experience, thinking they can just pick up the language Saw a job description recently that specified, they weren't hiring for a particular language, but were interested in developers who knew a minimum of three languages. That escapes the pitfall of hiring someone who knows Java and only knows Java. 
Does this mean i can negotiate lots of monies when a startup is trying to hire me as a scala dev?
TLDR : Hiring process in the industry is broken
Neither vallhalla or panama can replace Unsafe. Panama is a JNI replacement/supplement and Vallhalla adds some useful performance-related features, but doesn't let you do half of what Unsafe does. Even worse, Neither Panama Or valhalla will be out for JDK 9, and probably not even JDK 10.
Absolutely you can get hired at a Scala shop in a year, maybe less. If we can see by your github that you can use SBT, debug an issue, contribute a PR or two, understand the codebase enough to follow the style guidelines, you're pretty much there. If you can do the 'easy/medium' problems here http://aperiodic.net/phil/scala/s-99/ I'd say you're golden. 
What is your ability to work at high levels of abstraction? Did you study Scala a bit already (e.g. small personal project)? If not, your priority should not be money (yet), but to be surrounded by people who can teach you well. Without a supportive environment around you, learning Scala could be harsh.
I'm still a beginner at Scala, and whilst some libraries look incredibly overengineered and complicated, even if you don't end up using it professionally coding with it, and watching talks related to it will be eye opening compared to anything I've seen on Java alone. It would also show potential employers that you are interested in programming for programmings sake, and not just employment in my eyes.
It really depends where you leave. If you see loads of scala jobs, then go for it. It's a language that has a steep learning curve, so stick to it for the first few months. It's worth the effort. Scala is very interesting because it joins functional programming and object oriented programming, all with the possibility to use imperative style when needed. Once learned it allows you to write very expressive and readable code in just a few lines, by abstracting the repetitive stuff, or by making its syntax and style rather compact. It's not a silver bullet by any mean and the latest versions of Java bringing some of the selling points of scala to the masses. So in my opinion, it should heavily depends on where you leave or plan to leave.
&gt; what makes Scala special/important/worthwhile? The fact that it's an awesome language. And (in my opinion) the fact that most people use Scala because of its awesomeness, not career growth. &gt; Is it advised to learn this after Java I would say that basic Java is really simple, and at the same time it can give insights on some things in Scala that may seem not obvious otherwise.
This won't be a popular opinion. But you should pick a programming language that better suits your programming style/taste and not the programming language that offers better employment opportunities. You will be a much better programmer if you pick a language that you love. Why should you pick Scala? * Because it is one of the most consistent and coherent languages out there. * It has a very expressive type system. * You will be able to advance your programming experience along side with your knowledge of the language. * The language will slightly push you towards immutable and functional programming styles, thus making you think better about what you are coding. * There is a great community around it. Just look at this subreddit, the stackoverflow for scala, mailing lists, etc. * There are great tools to help you work with scala, IntelliJ, Eclipse, sbt, scoverage, etc.
Every time someone asks me why i changed to Scala this is the list of points that i come up with as well. I moved from Java to Python and from Python to Scala so i would say that the fact that Scala runs on top of the JVM and is statically-typed/ compiled while mantaining the modern style syntax (i.e. that python developers very much love) should also be considered a strengh! Much love from a fellow scala developer
As a Scala professional developer i can say that, yes, it does worth it. Thanks to a decent amount of awesome projects using Scala (Spark, Kafka, ...), the language is very hyped. Yet there are sill very few developers in Scala. There are not lots of offers but even less candidates. Furthermore, companies using Scala usually have a sexy modern stack, a challenging environment and usually are among the best in their field. So in terms of employment, that's pretty good. But the real benefit of learning Scala is not there. If you're ready to invest time and energy to learn and master the concepts Scala offers you, then you'll become a much better developer in any language. A language a just a way to express yourself. Java forces you to do things its way. It has a poor type system, little abstraction capabilities. On the contrary, Scala have a very rich type sytem. It is important because it means you can encode some of your invariants directly in the types to be checked at every compilation. Thus you gain a lot of verification while writing less tests. And it makes refactoring much easier! Scala is also a much more expressive language: you can mix higher-oder, classes, mixins, algebratic data types, pattern-matching, dependent types, ad-hoc polymorphism via ordinary classes or type-classes, etc Yes Java 8 have lambdas but Scala is so much more than that. It would be like saying that C (plain C, not C++) is an class-based object-oriented language like Java just because C has structs (yes structs are a very limited form class). Don't get me wrong, Java is actually a nice language. The truth is just that Scala is much more advanced that most of mainstream languages.
Learning Scala is worth it. It will challenge you sure, and once you know Scala, doing Java may be slightly inconvenient, but it will be walk in the park (now that you are used much more complex code, advanced types...). Scala has huge set of features - so once you know Scala, it is very easy to switch do different language, since it will have lots of features that you already learned in Scala. Not to mention that I believe Lightbend is pushing Scala towards mainstream (replacing old Java EE shops with Lagom), also rumours that Oracle is pulling the plug on Java EE is not helping it. It's not necesarily the kind of work I'd be happy to do (migrating old huge monolith systems and so.) but I think more Scala jobs, the better.
How's that relevant to OP's question?
I'm reposting since I posted this on last day last week and was probably missed by many: Are there any plans/updates about sbt running in server mode? In other words, can I run sbt on some high end machine to do incremental compilation / tests, and send results to my IDE such as IntelliJ (running on slow notebook)?
that's a very good point
There's this little guide I wrote a while ago: http://queirozf.com/entries/slick-3-reference-and-examples. There's an example of a transaction in Slick 3.
Other interesting company which use scala : Carbon IT, Zengularity, MFG lab, le Figaro, and many much more I am not aware of.
Depends what you mean by better. If you just want to get a job, then Java is better starting out, you will have an easier time finding jobs available and more importantly, you will probably have more people around that can help you as a junior developer. The number of available Scala jobs is small, and the number looking for juniors is even smaller. But, nothing precludes you from learning both. Where I work we "use Java" for everything in production but all my prototype stuff is done in Scala. It ends up making the whole development process faster, and the end code is better. 
OP thinks that Scala internal code is difficult to read. I assume he's referring to the standard library. I'm pointing out it's possible to exclude it and build your own. [psp-std](https://github.com/paulp/psp-std) would be one possibility.
The collections library does cause me a few headaches when trying to figure out whats going on under the hood. Things like calling a mapValues on a Map stops your map being serializable was a fun one that stands out in memory. I wouldn't ever think it slows/prevents adoption of scala as by then you're already somewhat invested and enjoying the other benefits too much. 
Option is clear to understand. Take something more complicated. https://github.com/scala/scala/blob/2.12.x/src/library/scala/collection/Map.scala To understand what map is doing I have to read about a dozen traits.
Bummer about the default trait / CHA performance regression.
The source and the data is available on [github](https://github.com/scalacenter/scaladex). Ideas and PR are more than welcome ;-). It's pretty helpful to find usage for new libraries. For example, a torrent stream library can be found in the [dependent of akka-stream](https://index.scala-lang.org/search?q=dependencies:akka\\/akka-http-experimental). I guess the [lib owner](https://github.com/Karasiq) does not have access to Netflix in Pyongyang.
It took me some time to get over the hump in learning how to use Protobuf and ScalaPB, but once I did, it's been smooth sailing. The builder pattern-style interface of the classes produced by ScalaPB can be a bit awkward (I still haven't found a great way to conditionally call or not call a builder method in a long chain), but once you get over the fact that your code from translating between your Protobuf classes and your domain classes is going to be a bit gnarly, it's gravy from there. I've been [looking for the way](https://github.com/trueaccord/ScalaPB/issues/104) to manually trigger ScalaPB class generation (rather than having it done upon every compile), so this article is very welcome!
Ha, don't be so self-deprecating. Why would you think this would be an unpopular opinion on the Scala subreddit?
&gt; I just want to know if other people think like me that Scala internal code is too difficult to read Yeah, Scala libraries use every trick in the language. &gt; and could be an impediment for new developers to choose scala. New developers can't read the library code in any language, much less Scala, lol. &gt; Just have a look to the code behind collections. You don't look at the code. You read chapter 25 of Programming in Scala and in "The Architecture of Scala Collections" it tells you how to add new collections so that map, flatmap, etc all work and the types line up. That being said, if you look in http://www.scala-lang.org/news/roadmap-next/ Scala “Aida” This release focuses on improving the standard library. &gt; Cleanups and simplification of the collections library: we plan to reduce the size of the collections library, providing some functionality as separate modules. Generally, we want to make them even easier to use and structure them so that they are more amenable to optimizations. Where needed, breaking changes will be announced using deprecation in Scala 2.12; regular use of the collections will likely be unaffected, but custom collections may need to be adapted to the simplified hierarchy. Reduce reliance on inheritance Make all default collections immutable (e.g. scala.Seq will be an alias of scala.immutable.Seq) Other small cleanups that are possible with a rewriting step (e.g. rename mapValues)
You could try using any other editor supporting Ensime. http://ensime.github.io/
What I thought was going to be an unpopular opinion was the idea that you should pick the language that you love and not the one with better employment opportunities.
&gt; Ideas and PR are more than welcome ;-) Here's an idea: how about listing a few examples queries on the website? As a new user, I had no idea whether I was supposed to search for a type, a method name, a topic like "graphics", or the name of the library. After experimenting a bit, it seems like it's the latter?
We [just merged](https://github.com/scalacenter/scaladex/pull/165) the default artifact selection, it will be in the next release.
I suggest trying another framework that is built in scala to see if the problem goes away. 
What IDE?
Can you provide more code? This works for me, for example. @ trait Writable defined trait Writable @ trait Reader { type Value &lt;: Writable val valueClassTag: ClassTag[Value] } defined trait Reader @ def read[V &lt;: Writable](files: String)(implicit vt: ClassTag[V]) = ??? defined function read @ class W extends Writable defined class W val r = new Reader{ type Value = W; val valueClassTag = classTag[W]; } r: AnyRef with Reader{type Value = ammonite.session.cmd8.W} = ammonite.session.cmd10$$anon$1@682ec8d @ read("nothing")(r.valueClassTag) scala.NotImplementedError: an implementation is missing @ implicit rct = r.valueClassTag rct: ClassTag[r.Value] = ammonite.session.cmd8$W @ read("nothing") scala.NotImplementedError: an implementation is missing
1. It will be available, but only if you start the JVM with a special flag. That's fairly unacceptable. 2. This is far from 90% of the use cases of Unsafe. See https://docs.google.com/document/d/1GDm_cAxYInmoHMor-AkStzWvwE9pw6tnz_CebJQxuUE/edit# 3. Varhandles are cool, but for a lot of use cases A*FU static final instances perform almost as well. http://shipilev.net/blog/2015/faster-atomic-fu/, especially with the recent fixes in JDK 8. 4. Have more details here? I don't see any mention of off-heap data on the project page: http://openjdk.java.net/projects/panama/. 5. Sweet. 6. Sounds good. Other use cases include: 1. Off-heap memory management. (Don't see anything in Panama about this. For instance, look at Netty's slab allocator.) 2. Serialization/de-serialization for libraries like Kryo. 3. Mocking and proxying. 4. More deterministic freeing of memory with DirectBuffer. 5. Volatile arrays. 
It feels strange to me having import org.scalaz._ import org.Scalaz._ all over my projects. Should I just accept it? Is this a good candidate for package objects? 
Bluntly, it's an absurd suggestion for this context.
Hey guys, I'm not exactly sure if anyone will see this any more but I have still been largely unsuccessful at finding another job. If anyone has any more suggestions or careers I can apply for I would love to hear from you, thank you!
I have ...
Does anyone know how to implement custom Wart rules (plug them to your project)? Or example project with that? I've found https://github.com/puffnfresh/wartremover-example-println but I'm having trouble getting it to work
To be fair, there are subtleties of the Java memory model, thread safety, the JVM, and tools (maven, gradle, etc) which senior devs are more likely know than juniors. Also, some companies use a mix of Java and Scala, and it helps to know both.
I have 10+ years of Java experience, and have no interest in either spring or hibernate. I love Scala and would never want to use Java instead. You are just talking out your ass. I also have 15+ years knowledge of C/C++ and no interest in going back there either.
not scala-related
Was not able to complete exercises due to [several errors](https://a.disquscdn.com/uploads/mediaembed/images/3906/3694/original.jpg). 
As someone who came from a Java (in college) and a professional PHP/Doctrine background who's interested in learning Scala for some personal projects, what's the deal with Scala and Hibernate? Searching around, there's a bunch of articles describing integration challenges between them, but they're all from years ago. I'm wondering if things have gotten better since then, or if it's the same and the old advice stands. For example, http://bhashitparikh.com/2014/10/28/hibernate-with-scala-options-with-less-pain.html and http://www.scala-lang.org/old/node/5699.html.
Same here.
&gt; Also, there's always the pitfall of not noticing failures with futures: One misplaced foreach can have you looking for the failure you observed, but can't pinpoint, forever. Does anyone have a way of handling uncaught exceptions in futures? Or at least getting them to a logger? Having to put .recover or .onFailure on everything is ridiculous.
The classic "We'll test it in production" design pattern, eh?
Improvement suggestions welcome.
All these flavours... and you choose to be salty.
Looks swish, but a bit disappointed by the response to a comment raised on: https://www.scala-exercises.org/shapeless/polymorphic_function_values The commenter points out that .headOption isn't predictable for a Set. I would hope the response would be more along the lines of "good point, we'll try and update that" than "not really our fault, maybe you can fix it".
There's nothing wrong with encouraging contributions. The problem with the old "feel free to send a patch" response is that it can come across as communicating a lack of interest in the issue raised. Personally, I'd hope your project would be more enthusiastic about honing the content. If you guys aren't that interested, why should we be?
Raul here, I added that comment. It did not imply anything you guys are speculating about. We are enthusiastic about the content and looking forward to improvements. We encourage people to contribute and we honor them showing the contributions right in the sections. Changing the collection from Set to say List is trivial and we tend to leave those for users that wish to contribute. We truly are an open project and have several contributions. That comments on the internet come across like that sometimes is natural and it was not our intention. Having said that if you want to improve that exercise we'd love that you'd participate. We have been busy creating a lot more content for many other libraries besides the 3 ones that it launched with. Check out all the new content modules getting added https://github.com/scala-exercises/ . Hope this helps.
I created a [github issue](https://github.com/scala-exercises/scala-exercises/issues/530) should be pretty quick to solve.
Some helpful links: * [Status of #](https://github.com/lampepfl/dotty/blob/master/docs/HigherKinded-v2.md#status-of-) * [The Essence of Scala](http://www.scala-lang.org/blog/2016/02/03/essence-of-scala.html) * [Scaling DOT to Scala](http://www.scala-lang.org/blog/2016/02/17/scaling-dot-soundness.html)
&gt; associativity law: m.map(f).map(g) == m.map(x → f(x).map(g)) This should actually be `m.map(f).map(g) == m.map(x → g(f(x))` Since the function applied to the functor isn't required to return a functor. You actually described the associative property of flatMap. 
You're right, my mistake. Thanks. Perhaps I would make it easier to read if I ditch the Scala syntax and write simply: F map f map g == F map (g ◦ f) where ◦ is function composition... What do you think? EDIT: added both forms, check out the edited article
You can also model that with scala: `m.map(f).map(g) == m.map(x → (f andThen g)(x))` or `m.map(f).map(g) == m.map(x → (g compose f)(x))` (untested)
I ended up naming point variables in all three examples "a", "b", "c" and I guess I forgot to change down there too... Thanks, fixed!
One more reason to make types non-nullable by default ;)
Is it not sufficient to write total functions in your Future code- to avoid exceptions and the need for .recover?
I agree that hiring will be your biggest obstacle. Sponsor and attend your local Scala meetup, buy them pizza or chicken wings, it's an insignificant cost compared to recruiting. You'll run into a mixture of experience Scala devs, and devs that are interested in learning Scala. The cost of training a dev interested in Scala (but weak in Scala) is much lower than hiring some random but strong Java dev, and asking them to learn Scala. Effective FP is a different way of thinking, and with the introduction of Java 8 - I think most people who "get it" will be interested in learning Scala. It also depends on what your product is. If it's web-based or data-science, then Scala is probably a good platform. If you're looking more at performance apps, like games, then you might wish to use C++ or similar.
Tell us more about this startup.
Make sure to hire at least one senior guy who will help new hires up to speed fast, and get your business to reap benefits of Scala fast.
Using chrome here. I'll make an issue later today. Seems like its the google analytics bit not working with the target always being undefined for some reason (https://github.com/scala/scala-lang/blob/master/resources/js/main.js#L504)
Scala is as good a choice as quality and correctness is a priority for your business. If you are far from product-market fit, and your primary activity is throwing sh*t at the wall to see what sticks, PHP, Ruby, Python etc are your friends. You will spend time debugging and you will have subtle bugs make it to production, but you will spin up new features really quickly and your hiring pool is bigger. If you are closer to understanding what your market needs and you are more focused on execution, using Scala will help you produce more correct software, spend less time debugging and more time on polish, and help you attract talent that understands the value of this (albeit from a smaller pool at a slower pace.) As an engineer I love the confidence that if it compiles it works, as a builder sometimes you just gotta ship something.
Or you can just use Play's routing DSL and set up an application in 38 lines including comments and imports: https://beachape.com/blog/2015/07/25/slim-play-app/
Yes because that looks extremely useful. Spray and AkkaHTTP can do the same thing. The point is you want a framework that works well for large and complex projects not hello worlds. Play is probably one of the better frameworks but some people prefer to use lower level libraries. If you read his post he was looking for alternatives to Play! not necessarily trying to replace it just considering other options.
So, you've been using Scala for 8 years(!) and you can't figure out Play? Color me skeptical ;-) Play's a very straightforward MVC framework with solid documentation. Having used it from the first milestone post-Play1 I'd say the biggest challenge is keeping up-to-date with releases, particularly the 2.4 to 2.5 migration. Lot of API churn, hoping that settles down moving forward. Saying that, if you really believe in static typing the dynamic language frameworks mentioned wouldn't even be on your radar; you'd be looking around in Scala, Haskell, OCaml, F#, etc. ecosystems for a good match.
Do you really need to use just one? I for one think that the speed with which you can do things in frameworks like Django or Rails is much higher and (since you're a startup!) you will want to be able to test out strategies to quickly see whether or not they work and pivot early (fail fast). Maybe try stuff out with Django, Rails et al and only develop in Scala stuff you really need performance or where you need the type safety?
You can build in Scala only the services you need / want to (using e.g. a microservices architecture) and develop "softer" stuff using python, ruby, etc, which will help you in hiring.
My 2 cents - you don't really need to use Play and stuff, unless you bound to some Spring-MVC-like boilerplate from the past. The modern things include building some RESTful services (you can use Play here, but also Spray.io/Akka HTTP is an option) and building the client-side using RacttJS/AngularJS/younameitJS.
Depends on the startup.
This might actually be a good solution for OP.
* Micro Services layer (mission critical) - Scala + Akka HTTP Micro services. (if you really don't think you'll be able to fill the Scala roles, then use Java 8 + JAXRS - Jersey) * UI Layer - Play (Java) / Express (Nodejs) / Rails (Ruby) / Django (Python) that make rest calls to the microsevices * Presentation layer - ReactJS + Bootstrap 
I agree with this sentiment. Most scala tools seem very easy to use. You can something complex in10 lines of code as a hello world. But once you have to do any useful these tools just break down. Either take long to dicipher the cool DSL or find a feature that has not been incorporated accurately. 
Play is a solid framework. But the documentation is amateurish compared to other non-scala frameworks. Look at the documents for Json in play. 3-4 pages of different styles. Then one goes on to read iteratees etc.. The documentation is written in a style to show all the cool DSL and type class magic that Play does. Not written for a programmer who wants to use it. The Activator packaging is also not the answer. They are mostly hello world integrations for different tools to illustrate the power of Scala. I agree with your skepticism. I wonder what I am missing. Hence last few months have tried using php, Python etc web frameworks and found it easier to work with to do basic tasks needed in a web app. 
Well in my opinion the real complexity is still there... Scala type system just "shows" it explicitly during compile time, which is imho benefit and makes code reviews easier etc. But I am biased - on the other spectrum there is golang and people succesfuly write services in it... success is still mostly defined by your engineering practices I would say.
Ah, the NYC are might be different from Austin TX (where I'm at). You should perhaps consider "why would someone want to work at my startup?" Personally, free pizza, trinkets, t-shirts, mugs, and other items are of little value or use to me compared to a paycheck. Another thing worth considering is presenting at the local Scala meetup. A CEO who can competently present/educate on a Scala topic might be worth far more than pizza or trinkets. I'm sure at some point in your career, you've been frustrated by CEO's or CTO's or manager who had limited technical talent, and yet made technical decisions they were not qualified to make. You should perhaps drop a very short, "We're hiring" at the beginning or end of the talk, but otherwise keep it mostly educational. 
As /u/teknocide mentions, you would probably use an implicit class: implicit class MyDoubleOps(x: Double) { def isReallyWhole: Boolean = x == math.floor(x) &amp;&amp; !x.isInfinite } Another important aspect with respect to for example DSLs is the ability to lift a value of a class `A` into a (possibly richer or more specific) class `B`. Using the standard search paths for implicits, one can enable this behavior by default, e.g. object Complex { implicit def real(x: Double): Complex = new Complex(x, 0) } case class Complex(re: Double, im: Double) { def + (that: Complex): Complex = new Complex(this.re + that.re, this.im + that.im) def * (that: Complex): Complex = new Complex(this.re * that.re - this.im * that.im, this.re * that.im + that.re * this.im) } Complex(1, 2) * 3 
[This](https://github.com/drone/drone) might be interesting as well even though it isn't scala specific. 
That's true. On the other hand, extension methods - I believe - are a lot less frequent in an average code base than you might think. When people come new to a language that allows these extension mechanisms, they are all thrilled to use them a lot (at least that's what I remember from my own encounter with Scala), only to realize after a short while that one does not need them _that_ much. So I'm a undecided whether one should add an explicit syntax for this.
You could ask what's the deal with JPA and hibernate, any why the standard hasn't evolved beyond how people interacted with persistent stores since 2004. Beyond that there is no deal with Scala and Hibernate, unless you're referring to the fact you can't use immutable data types and case classes with it. Other than that defining entity classes with vars and annotations works just fine.
Have you tried turning off type aware highlighting?
The way I see it, `X#A` is like a type-level equivalent of `Nothing`. `Int &lt;: X#A` because there never is a (concrete) `X#A`.
I have not... sorry, but where do I do that? I fail to find it in settings.
yet you did introduce a definition &gt; ...and ◦ is the standard mathematical notation for function composition (meaning “g after f”, or “apply f and then g”). and also &gt; I will not get technical, and by technical I mean mathematical.
Im fine with way it is. I dont think we need another construct in already rich language.
Your web page is hard to read. I suggest you add to the CSS: body { color: #000; font-weight: 400; }
OMG thanks! Don't know how that slipped through, I guess it's obvious to everyone what I meant :) fixed
I am looking to translate parts of my android app to scala (the goal being more concise-&gt;readable code). As I am pretty frightened by the prospect of proguard "possibly changing the behaviour" of the app - I'm undertesting already - I'm considering to translate just the android independent "model" to scala. Will that be considerably easier, i.e. can I just compile the scala model to a java-7-jar (how?) and android will swallow it without problems? Can I continue development of that model inside android studio? Has anyone tried (and documented) this?
I think it's far too easy to get hung up on this kind of stuff. Scala is an OO + FP language for a reason. I don't think that the features are mutually exclusive by any means. A lot of the Scala code that I've seen out in the wild has been a good mix of the two styles.
You should really read: http://www.lihaoyi.com/post/StrategicScalaStyleDesigningDatatypes.html The core issue is that Scala supports two principle ways to model your data: OO and ADT. Each has costs and benefits and you need to find the right trade-off for your particular domain, level of comfort, etc. For example one major benefit of OO is that extension in OO can be nearly cost free since new subtypes can share existing implementations of methods. However, the cost of this is the up-front design to ensure that your classes are actually extensible in EVERY way you might ever possibly need. The ADT approach to extension lives at a very different point in the cost/benefit space. With ADTs you get nothing for free, but you also don't have any up-front costs associated with designing your types "correctly". Instead all the effort is done after the fact via type classes. Keep in mind too that this is all a spectrum and you may have needs that live somewhere in the middle. 
I think this question, as often as it comes up is poorly framed. It's not so much functional programming and object oriented programming that are at odds as much as it is imperative programming and functional programming. Imperative programs trade the ability to reason about programs with the ability to use mutable state. The exact calculus this trade-off is the real subject of debate.
I consider inheritance the major failing of OOP. Its an extremely powerful feature that we've been trained to think isn't a big deal. From day one of our beginner's programming course, it's constantly thrust into our faces as "the way" to model "the world" (mutability has a similar "powerful feature we're desensitized to through ubiquity" story). Its very frequently unnecessary in Scala (with the exception of encoding ADTs and emulating modules). Look no further than the standards collection library as a prime example of the failings of trying to track functionality through deep inheritance hierarchies. Basing the type of functionality delivered on inheritance (mutable or immutable? lazy or strict?) ended up as a kind of rat's nest. And the failings of the cake pattern are well documented: http://igstan.ro/posts/2013-06-08-dependencies-and-modules-in-scala.html Also, I constantly try to remember the oft repeated saying "prefer composition over inheritance." It's the simpler approach and tracking encapsulation levels is much easier for me mentally: http://igstan.ro/posts/2011-09-09-how-inheritance-violates-encapsulation.html Finally, I think OOP frequently puts us on the wrong side of the expression problem. If we have to choose, we should want it to be easier to add functionality than to add new data types. Data types should remain relatively static. Meaning, I don't need to inherit new classes to instantiate new data types as much as I need to add new functionality to my existing data types. FP (functions applied to an ADT) makes it easier to do this than OOP (classes loaded with methods).
I recently started getting my feet wet in Scala, and I've been thinking about a data structure I have to use at work. It's basically a linked list, where the type of the first and last items must be of type A (or a subtype thereof), and everything in between must be of type B (or a subtype thereof). The hierarchies of A and B are disjoint. Is there any way to model this in such a way as to make it typesafe, without having to resort to moving the first and last elements out of the list, like so?: trait A trait B case class SpecialList(first: A, middle: Seq[B], last: A)
That was an informative read, thanks!
Basically what you have is a non-empty, arbitrary-length, heterogenous list/tuple. The types are so general that there is little you can safely do. The appropriate model will depend on what you *really* are modelling. Without knowing your use-case we can only speculate. &gt; without having to resort to moving the first and last elements out of the list You say it like it’s a bad thing? :-P It is pretty appropriate. What could you hope to achieve by having those elements as part of the inner seq? Let’s generalise it a bit: case class SpecialList[A,B](head: A, middle: Seq[B], last: A) { def size: Int = 2 + middle.size } Here you can see that the definition accommodates something like this: SpecialList(new RuntimeException(), Seq(null), Double.NaN) Those values basically have nothing in common and there is nothing ‘safe’ that can be done with all elements of the data structure. Even ‘counting’ the elements could be unsafe, if `middle` is a Stream with infinite elements. Likewise, why would you want head and last to be part of middle? What would this mean and what would you hope to achieve by doing it? If the head, middle and tail literally have nothing in common, it does not ‘make sense’ typewise. In the head, middle and tail *do* have something in common, then you can put them all in a homogenous Seq.
Ok, thank you for the answer :-)
Scala salary are usually higher than java ones : https://www.sitepoint.com/best-programming-language-learn-2015-job-demand-salaries/ That being said, I am a junior developer un Scala, started few weeks ago and really enjoying it. Learning basic scala is not a problem and there is awesome books to help you learn idiomatic complex scala (functionnal programming in scala). Definitely an experience worth it !
`host`, `user`, `password`, etc. but in essence, yes. I literally used `Task.async` to wrap [ftp4j](http://www.sauronsoftware.it/projects/ftp4j/). :-)
If you're getting started with Scala, I'd recommend gradle or SBT. They're much more modern builds tools, and you won't have to deal with XML. Gradle is becoming very popular in the JVM world and SBT was created specifically with Scala in mind
I gave Gradle a shot and honestly, I hated it in comparison. I haven't used SBT yet, but being able to continue to use Maven would be a huge + for me.
SBT is really powerful. You should really reconsider.
You may also want to look into [Polyglot Maven](https://github.com/takari/polyglot-maven), because it allows to write POMs in Scala syntax ([example](https://github.com/takari/polyglot-maven-examples/tree/master/scala)).
Valid points some of my arguments were bad. However, I don't think you interpreted the article correctly. He never chose sides. &gt;This was to be something the whole team could work on but half the team didn’t want anything to do with it. The developer who wrote it is a brilliant person but the fact it divided half the team was a problem. So yes in this case it is the non FP engineers who are causing issues. But it's a fact that if half of your engineers won't be productive its bad for business. There are two easy solutions have everyone write the code in a way that the entire team agrees upon which is what they chose. Or Hire all people who share a similar style (My comment about being disciplined). Yes you are correct this is a very general problem but it occurs very frequently with Scala I don't think that can be contested. For this reason alone I think it's more difficult to build a team around Scala than most other languages. It's nothing against Scala itself as a language. 
Run mvn archetype:generate and search for scala. There is a decent scala maven archetype already built that will scaffold your project 
Stick with Maven. We're in the process of moving projects to Maven because SBT doesn't provide good enough features to justify the DSL overdose. Here's a gist of a project pom.xml I'm converting to maven (scalatra, scalatest, uses npm to build frontend stuff). Let me know if it doesn't work. https://gist.github.com/mikko-apo/102bccf4bf367a63e8a68257a40aff66 edit. If you want to downvote, please leave message why you disagree. 
Just add the scala-maven-plugin to your maven pom and get going. It's as simple as: http://davidb.github.io/scala-maven-plugin/usage.html I can't help with the IntelliJ part but I'd recommend making sure you get the maven part working on the command line first. Once you can run `mvn install` and have it work (with a basic scala test file) then you can worry about the IntelliJ part (which may well Just Work).
In my opinion, it is not a very good way to learn scala. As you mentioned, you want to switch to scala and you want to learn in, what you should do is pretend for a second that you have newer done java or other jvm language. This will help you learn the whole scala ecosystem and will help you drop habits that would slow you down. For example, now you may start with maven, later when you will look for a web framework, you will choose spring and finally will add hibernate. Then when choosing coding style, you will use mutable objects everywhere and imperative programming, because that is what you are used to. Then finally you will say that it is no different than Java or probably that it is worse than Java, but that is because of how you use it. Take activator - https://www.lightbend.com/activator/download it will create a scala + sbt + play skeleton app for you and you will be able to start within minutes. Take a look at some functional programming articles and tutorials for scala. Take the whole scala ecosystem and then compare it to what you had in java, do not make hybrid monstrosities.
Can you give an example where SBT doesn't provide good enough features compared to Maven?
If you look at the source or operators like %, you can learn the plain-old-scala equivalent to the DSL.
Would folks be interested in having some "Scala celebrity" AMA's organized here? I was thinking of reaching out to people such people as Heather Miller, Bill Venners, Martin Odersky, Viktor Klang, Sébastien Doeraene, Konrad Malawski, Jonas Bonér, etc just to name a few, and seeing if they'd be interested in doing an AMA. Also, if you want to suggest somebody in particular, don't hesitate to let me know! 
Yes
Out of curiosity, can you point me to where that % is defined? I tried to look for it, but I gave up after 10 minutes.
inb4 neither me nor my masseuse fit in my mailbox 
Only way I know of is to use the monitoring from lightbend's subscription. I think you can use it for free in development but it costs money in production
The only way is to write your own mailbox implementation. 
They made a very big change regarding plugins in 0.13.5 (if I'm not mistaken) maybe you updated from lets say 0.11.X to 0.13.6 and that caused a lot of problems. Some plugins haven't updated to the new way of doing things which is very troublesome sometimes. The versioned dependencies, the build cycle, and the commands in the right context are very easy to achieve. Just as easily as you create complicated builds that are nasty to maintain, you can refactor them to make them much more maintainable. You can look at how Akka organizes their build files and you will see they have a very organized setup. My main problem with sbt is the lack of proper support for "parent poms", you almost need to create a project with your "base pom" then declare it as a dependency on your project project. In Maven its very simple you just declare that your pom as a parent pom.
Never mind. The constructors are private[sbt], so it's not possible to avoid using %. [http://www.scala-sbt.org/0.13/sxr/sbt/impl/DependencyBuilders.scala.html]
https://github.com/puffnfresh/wartremover-example-println looks a bit outdated, but anyway
Option has nothing to do with 64k memory, it's a part of JVM world where adding more memory is not an issue. Meanwhile, I don't see a reason why C couldn't have a safe compile-time way to handle undefined values without memory overhead.
 * Easily accessible/searchable documentation for *every* build setting * Build definition is self-documenting * Build definition can be communicated verbally (i.e. no symbols) * Build definitions less likely to read arbitrary files, use random numbers, open a GUI window etc. * Confidence in upgrade path to new releases. * Eclipse integration that: * doesn't require a manual step when you change the project definition * allows you to add a dependency using the GUI * autocompletes settings
Why would someone need Akka on the client side?
Getting "Source not found" on all the examples.
Can you please file an issue on https://github.com/andreaTP/akka.js-site that is where sources are taken from. Thanks!
&gt; Have you had to update a 3 year old SBT build with multiple inner-modules and lots of plugin dependencies (to outdated plugin versions)? The flexibility that makes SBT powerful is also its curse. There are a few reasons they aren't calling it Simple Build Tool anymore. SBT is bad because it has breakages between major revisions? Then don't update it, it's not like old versions of SBT stopped working. And it's not as if the 1 major maven update in almost 20 years didn't was all without breakage anyways. That why it's a major version bump.
We haven't got yet transparent remoting, we will work on that pretty soon. However you can roll your own communication using any available channel like websocket. Check out the chat demo for a POC on how that can be achieved.
I wouldn't bother with Ubuntu repositories. Just download the ZIP file, extract it anywhere and add `sbt/bin` to your `PATH`. For example, you can add in `.bashrc`: SBT_HOME=/home/user/DevTools/sbt PATH=$PATH:$SBT_HOME/bin This is arguably the most reliable way. Scala distribution is not needed, but you can download it just for easy REPL.
And how do i fix that?
That's strange, I don't remember this error. But here someone encountered something similar: https://github.com/yahoo/kafka-manager/issues/147 Apparently, they managed to fix it - probably you should do the same.
Take a look at the proxy settings on http://www.scala-sbt.org/1.0/docs/Setup-Notes.html . I ended up adding my proxy settings as java opts so they get picked up automatically.
It's important for Scala.js (and any other non-JVM targets for Scala code), so that you can use the Time API in code that works across multiple platforms.
I already have it installed.
+1 use sbt-extras. You can just commit the script to your repo and then you don't have to "install" anything at all other than a JDK.
This has never let me down (I've installed probably dozens of times...vms) https://gist.github.com/osipov/c2a34884a647c29765ed Just replace the versions with the newer up to date ones.
it is, in fact, possible. `val myDep = ModuleID("", "", "")` http://www.scala-sbt.org/0.13/sxr/sbt/ModuleID.scala.html#sbt;ModuleID
I have 40 mp4s from that class from 12/2015. But judging by the indexes in the file names its an incomplete set. I should check out the new extended course though.
At a quick glance, the new assignments look better. I've flunked the previous iteration because of the weird assignment that required using tons of HTTP-related Java stuff.
I thought Principles of Reactive Programming included teaching frameworks like Akka, no? I don't see it in the new course
can you please explain what exactly this file is, I want to add a repo for the sbt-launcher.jar to look in for the sbt eclipse plugin "https://dl.bintray.com/sbt/sbt-plugin- releases/com.typesafe.sbteclipse/sbteclipse-plugin/scala_2.10/sbt_0.13/4.0.0/" How do i tell it to look there because it keeps trying to access wrong repos and gives me Server access Error?
[**@flaviowbrasil**](https://twitter.com/flaviowbrasil): &gt;[2016-07-16 22:13:19 UTC](https://twitter.com/flaviowbrasil/status/754438793608859648) &gt;Anyone interested in writing composable sql with [#Scala](https://twitter.com/search?q=%23Scala).js? We need some help to integrate [#Quill](https://twitter.com/search?q=%23Quill) with [#Node](https://twitter.com/search?q=%23Node).js! [*github.com*](https://github.com/getquill/quill/pull/452) ---- [^[Mistake?]](/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=/4t6sew%0A%0APlease leave above link unaltered.) [^[Suggestion]](/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/joealcorn/TweetPoster) [^[Issues]](https://github.com/joealcorn/TweetPoster/issues) 
I think your effort is great man, truly, but I don't think this is the way for debugging. I've been using Ammonite and it is great, you also need to write code for debugging, but at least you are thrown into a repl, which is ideal imo. Ammonite is far from perfect, of course, actually it's very lacking regarding features, I'm sure the scala-debugger itself is lacking though. I recommend to watch this presentation http://undo.io/resources/presentations/cppcon-2015-greg-law-give-me-15-minutes-ill-change It'd be great if we ever get something like that on scala, and it seems I'm not alone on that: https://github.com/ensime/scala-debugger/issues/220
Are you talking about doing a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) on your nodes?
Have you tried the Internet Archive? They have archived around 40 gigabyte of Coursera videos. News about it from a few days back: https://news.ycombinator.com/item?id=12062116
Kind of new to scala. If I have an abstract base class and two case classes extending it as in the following snippet. I want the extending case classes to share some common method defined in the abstract class and since (I suppose) we want to avoid mutable state, the decrease method should return a copy of the object but I want these copies to be of the type that called the method. Can I do this without having to implement the decrease method in both Bar and Baz? abstract class Foo(x: Int) { def decrease(d: Int) = ??? // TODO: return copy of invoking class with x = x - d } case class Bar(x: Int) extends Foo(x) {} case class Baz(x: Int) extends Foo(x) {} object Test extends App { val bar = Bar(5) val newBar: Bar = bar.decrease(5) }
wit.ai Also, if you are open to working in a hedge fund, PM me.
To beat the dead horse, it's either a black mark against Scala or SBT that a there isn't a satisfactory way to define a build in real Scala code. Whether it's SBT, Twirl, or the Play router DSL, I think we have to ask ourselves what's missing from Scala as a language and why can't we fix it?
?
SBT is defined with Scala code. Twirl and Play routers are custom DSLs, but that just happens to be their choice. You can find other implementations that use Scala code. One Twirl alternative: http://www.lihaoyi.com/scalatags/ Systems that define routing using Scala include: http4s, spray, finatra, finch, akka-http, and others. It's almost harder to find a HTTP server that doesn't use Scala code (other than Play).
What places are there to work that take advantage of most of scala's best features/libraries? I'm thinking of Typeclasses, scalaz or cats, monocle, and shapeless. Most places I have looked seem to only be interested in using scala as a "better Java"
Verizon Labs and SlamData are two places I would definitely consider if I were looking for a job right now. You know people from both of those places via IRC (dm me if you need pointers).
Made a very simple PR.
He begins comparing to Scala around [20:33](https://youtu.be/EZD3Scuv02g?t=1233), if you're interested.
So running OpensSUSE Tumbleweed and Scala was running fine until I tried it the other day: ~&gt; scala Welcome to Scala 2.11.8 (OpenJDK 64-Bit Server VM, Java 9-internal). Type in expressions for evaluation. Or try :help. scala&gt; Failed to initialize compiler: object java.lang.Object in compiler mirror not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programmatically, settings.usejavacp.value = true. Failed to initialize compiler: object java.lang.Object in compiler mirror not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programmatically, settings.usejavacp.value = true. :q ~&gt; java -version openjdk version "9-internal" OpenJDK Runtime Environment (build 9-internal+0-2016-04-26-230040.abuild.jdk9-55b6d550828d) OpenJDK 64-Bit Server VM (build 9-internal+0-2016-04-26-230040.abuild.jdk9-55b6d550828d, mixed mode) ~&gt; echo $PATH /home/r/bin:/usr/local/bin:/usr/bin:/bin:/usr/bin/X11:/usr/games:/home/r/scala/bin ~&gt; echo $SCALA_HOME /home/r/scala Can anyone help with what could be causing this? Is it a Java environment issue? 
Java 9 is not supported yet: https://github.com/scala/scala-dev/issues/139 Java 9 has not been released yet anyway (and won't be until next year: http://mail.openjdk.java.net/pipermail/jdk9-dev/2015-December/003149.html), so you shouldn't be using it.
Sure, it's defined by Scala code, but you could make an interpreter for any language in Scala. My point is that the files themselves aren't Scala. I'm aware of (and do use) the alternatives. But the examples I quoted are probably the most popular tools in their classes. I think my question is a fair one: where do Scala's DSL capabilities come up short that one might reasonably choose to make a fully custom language in these cases?
I know, I would really love to finally use libraries like those in big projects. Funnily enough I didn't care much about the 2.12 release before this fix and the higher order unification one came into play!
IntelliJ has supported sbt files for a long time now. It is completely discoverable. Don't know about eclipse. Ensime definitely doesn't support it. 
I have never wanted to write a build definition as purely code in any language. It is not useful to me. Not at all a black mark. 
I believe there was talk of backporting the unification fix to 2.11.9. I don't know if that was ever actually decided. Maybe both could get backported.
Really? That sounds great. Does, for example, click-through-to-type work? i.e. click a type and IDE opens target source file at type definition location. In Eclipse I've only ever gotten reliable support for the build with a single project. For multi subproject builds, totally borked.
yes, everything works. IntelliJ treats it as just another scala file with some slightly special processing for the lack of top-level object structure. and has been the case since IntelliJ 13 or so (when they introduced sbt-structure and the SBT toolwindow in the scala plugin)
I encountered the same error some time ago. Just use Oracle Java (if you can of course). There are some issue with certificates in OpenJDK.
Sorry for the contrived example, but I'm trying to understand how these signatures with implicits work in Scala. class A(val n: Int, val s: String) // #1 class A(val n: Int, implicit val s: String) // #2 Do these effectively mean the same thing? I'm not seeing how the implicit in #2 can ever be taken into effect. Does it add any meaning? Is there any code that would compile with #2 but not #1 that I'm not seeing? I'm still new to Scala and implicits, so there are probably usages that I simply don't know about yet. I guess I'm confused because without the `val` I get this: class A(n: Int, s: String) // compiles class A(n: Int, implicit s: String) // ERROR class A(implicit n: Int, s: String) // compiles class A(implicit n: Int, implicit s: String) // ERROR Which makes sense to me. But with `val`, all four compile, so I'm wondering what meaning #2 and #4 have over #1 and #3: class A(val n: Int, val s: String) // #1 class A(val n: Int, implicit val s: String) // #2 class A(implicit val n: Int, val s: String) // #3 class A(implicit val n: Int, implicit val s: String) // #4
Which frameworks/libraries have been used? What's the version of Scala in the project? Is the build tool SBT? What does the app do?
I remember to deploy scala/Play stuff on heroku without much effort. check it out at https://devcenter.heroku.com/articles/deploying-scala-and-play-applications-with-the-heroku-sbt-plugin
download typesafe activator and put it into the root of the project. $ ./activator dist =&gt; package will be in target/universal/myapp.zip unzip on server and run $ ./bin/myappname -Dhttp.port=9000 if your friend did everything in a sane manner, this should give you some errors to work with (cant connect to db, missing migrations etc.) and you should be able to work from there (or ask here again) you only have to have java 8 installed on the server, there will be start scripts for both linux and windows in the bin folder if you want more help you can privately send me the following files: build.sbt or project/Build.scala, content of conf folder (if there is one, be sure to check the files for sensible data ;))
&gt; I'm not seeing how the implicit in #2 can ever be taken into effect. Does it add any meaning? It makes `s` available for implicit resolution within the body of `A`. Consider this REPL output: @ class A(n: Int, implicit private val s: String) { private def findsImplicitString(implicit str: String): String = { "called with " + str } def print(): Unit = { println(findsImplicitString) } } defined class A @ val a = new A(1, "a") a: A = ammonite.session.cmd2$A@1067bc4c @ a.print() called with a &gt; Is there any code that would compile with #2 but not #1 that I'm not seeing? Try removing the `implicit` from that example and see what you get :) &gt; I'm confused because without the val I get. . . &gt; class A(n: Int, s: String) // compiles That is a class with two constructor parameters which get bound to `private val` members. The parameters must be explicitly passed. &gt; class A(n: Int, implicit s: String) // ERROR Because you have `implicit` the default `private val` behavior is no longer in effect and you must state it yourself. Using `implicit private val` works, as in my example, and `implicit val` works, as you discovered, but makes the member public. You can also specify `var`. &gt; class A(implicit n: Int, s: String) // compiles That is a class with a constructor requiring no explicit arguments and an implicit parameter list. The implicit parameters are still bound to `private val` members and both are available for implicit resolution within the class. If you want to pass the implicit parameters explicitly you'll need this syntax: val a = new A()(1, "a") &gt; class A(implicit n: Int, implicit s: String) // ERROR Same problem as before: you need to specify visibility and `val`/`var`. &gt; class A(val n: Int, val s: String) // #1 That is a class with two constructor parameters which get bound to `public val` members. The parameters must be explicitly passed. &gt; class A(val n: Int, implicit val s: String) // #2 Same as \#1, except that `s` is also available for implicit resolution as described above. &gt; class A(implicit val n: Int, val s: String) // #3 Nearly the same as the third case above: a no-explicit-arg constructor with an implicit parameter list, except the parameters will be bound to public members. &gt; class A(implicit val n: Int, implicit val s: String) // #4 This is the same as \#3. The second `implicit` is redundant since it is part of an implicit parameter list already.
Hey guys thanks for the input so far. I am trying to get my developer counterpart to chime in with the answers to your questions. Sadly, he blew up his computer this afternoon. :|
&gt; ...In a perfect world we would like to spin up an Azure server... what kind of dystopian world are you imagining?!?!?
I mean error handling in for comprehension. fooos.latest.result.headOption is Slick's thing. You get Option[Foo]. By pattern I mean generally failing in for comprehension. I like it, I just haven't seen it before. What I've seen before was val xxx = for { foo &lt;- OptionT(fooos.latest.result.headOption) .... } yield ... xxx.run.map { case Some(x) =&gt; ... case None =&gt; throw LatestDoesNotExist } Which I don't like, because it makes other stuff in for comprehension being littered by optionTs and also some lines later you handle logic that is actually related to foo's headOption. That's just what raised my question. I don't see nothing wrong with the firstmentioned solution.
Are you sure that type is correct? `OptionT(headOption)` only makes sense if `headOption` is `F[Option[Foo]]` for some other monad `F` (perhaps this `DBIO` you're talking about?). In terms of the broader question of how to handle different effects, the question of what to do with an `Option` inside a `for`/`yield` block is largely the same question as it is in plain code. If you can `getOrElse` with a sensible default immediately, you almost always should. Carting around the `Option` to a later point is something you do if you want to handle a lot of different possible "failure" conditions in the same way at the same place, which is valuable, but it imposes a certain amount of code and debugging overhead (in terms of adding `for`/`yield`). You make a very similar tradeoff (in terms of adding `OptionT` etc.) when you make the same choice when working in your `DBIO`. If you can express all the effects you want to handle as just `DBIO` (e.g. because your `Option` being `None` is an effect that you want to handle in exactly the same way as the database connection being down) then sure, it's more efficient to do that than to stack two different effects. But you lose a bit of orthogonality/decoupling - it's nice to be able to handle each effect individually, and e.g. test some business logic that uses `Option`s without having to spin up a database test harness. There are a few approaches you can take to mitigating the overhead of the extra wrapper methods of having a monad transformer stack: * "Fix" the stack you want to use for most of your program, and write helper methods that do the specific versions of the generic things that you want * Write business-logic code generically in terms of just those monadic effects you need at that point, e.g. `F[_, _]: MonadListen[?, AuditLog]`. And then in real code you instantiate with `F = EitherT[WriterT[Future, ?, ?], ValidationFailure ?]` or whatever but you can instantiate for test or other use cases with just `F = Writer`. * Use a free coproduct style where you only have one monad but the list of possible effects is dynamic. (I'm trying to make an interoperable variant of this with my paperdoll project, with easy conversion back and forth to the traditional monad transformer style - very much a work in progress at the moment though) * Ongoing efforts towards a "proper" effect system in the language (scala 3) The above is a bit rushed but hopefully makes some kind of sense - will try and respond further to clarify.
Yes you are correct about type. Thanks you very much for explanotary response - I'll read it carefully few times now and learn from it :) Are there any good tutorials where I can learn about these best practices? Places to read watch where this kind of dicussions were already happened? 
Seriously consider giving up.
You seem to think I'm just being snarky. I'm not. I'm reflecting on the fact that you apparently got a project dump with literally _no idea_, not just how to build it, but how to _ascertain_ how to build it, and now you're reporting that your _developer's computer is DOA_. The combination of these puts you at least one order of magnitude out in probability of successfully taking on this project, especially since it points to other systemic issues, such as, apparently, lacking backups. In other words, I wish you no ill. The illness is already upon you.
What about separating the metadata from the case classes? Instead of having it as part of the entity object pass around something like `(Customer, Metadata)` when you need both pieces together or just `Customer` when that's all that's required/known. Then your controller only needs to create `Customer` with the data it knows and the metadata can be constructed later when all its necessary data is known. That also gives you some extra flexibility, e.g. to have separate db queries for when you need only select/update customer data or when you also need select/update the metadata.
I am starting to think that sbt's `libraryDependencies` setting is strictly more powerful than its `Project#dependsOn` dependency management method. Can someone justify using the latter (other than the added complexity of doing `publishLocal` for every change to a local project)? Can I bypass this issue by just using both `libraryDependencies` and `dependsOn`?
&gt;I've created a DAO from scratch I found your problem
Good question. I actually do not know what Conscript does. It's beautifully-typeset documentation at http://www.foundweekends.org/conscript/index.html is all but totally impenetrable to me, despite reading every page 3-4 times. At the very least, hopefully Ammonite's documentation is much clearer to read =P For example, I cannot find any information on how to use Conscript to run Scala scripts. That's the whole point of this feature. Obviously, on top of that I cannot see any information about using importing one conscript script from another conscript script. That's another key feature of this project, that makes scripts go from "one file toys" to actual re-usable software, importable from other scripts. Want to write a test suite? Have a `test.sc` script import your helper scripts, run their functions, and assert its output! Imagine how useful e.g. Python or Ruby would be if scripts were limited to one file and could not import others. Lastly, conscript seems to use SBT, which at least on my machine has a tremendous startup overhead. That alone makes it a no-go for any serious usage. Ammonite's 0.5s overhead isn't great, but it's an order of magnitude faster. Annoying, but not painful.
http://search.maven.org/#search%7Cga%7C1%7Cscalatra-sbt shows 0.4.0, 0.5.0, and 0.5.1 versions of scalatra-sbt. Try one of those. If that doesn't work (0.2.0 is pretty old), you can build and install it yourself by git clone https://github.com/scalatra/scalatra-sbt.git cd scalatra-sbt git checkout ed5d93b sbt publish-local 
or create something like case class Entity[T]( data: T, id: Long, dateCreated: Instant, dateUpdated: Instant, userAccess: Seq[Long] ) 
&gt; Okay, so you, and many people, don't like SBT. I think I've phrased myself poorly, but my beef here isn't with SBT as a whole. I totally agree with you on its strengths. Its biggest weakness, though, is that it's almost impossible for a beginner to actually understand how build.sbt maps to the underlying actual process. And I think that's starts with the attempt to wallpaper over challenging architectural decisions with a leaky quasi-Scala language that pretty quickly begins to exhibit its own set of quirks in any real project. To me, it seems like the core of the issue is that they want the build to be a data structure, so that it can be introspected and manipulated in all sorts of interesting ways, even interactively. In the words of a recent Underscore blog post, they chose to make a [transparent interpreter](http://underscore.io/blog/posts/2016/06/27/opaque-transparent-interpreters.html). I think this makes sense in many ways. My biggest complaint is that rather than simply dealing with that, we have this fiction of the .sbt file, in which you eventually *have* to understand that it's one big data structure anyway, just with a whole new semantics compared to vanilla Scala. But maybe this isn't an SBT problem. Maybe Scala should have a better way of building and working with interpreted data structures. It is, after all, a commonly used and very effective pattern. As much as I do hope to one day contribute to making the future I want a reality, in the meantime, what I can do is try to advocate to move things in the direction I want, as you put it. I've obviously done a poor job of it with my initial comment. Anyway, thanks for engaging my points, rather than just flaming me.
Here's a typeclass solution that allows "decrease" to be a member function of the parent class, but I don't see how you could avoid having to implement the a decrease function for each subtype. It also fails if the case class has different constructor arguments from the parent class, but it doesn't seem logically possible to infer that information from the parent anyway. abstract class Decreasor[T] { def decrease(i: Int): T } abstract class A[T &lt;: A[T]](i: Int) { def decrease(implicit decreasor: Decreasor[T]): T = decreasor.decrease(i) } case class B1(i: Int) extends A[B1](i) case class B2(i: Int) extends A[B2](i) implicit val decreaseB1 = new Decreasor[B1] { def decrease(i: Int): B1 = B1(i-1) } implicit val decreaseB2 = new Decreasor[B2] { def decrease(i: Int): B2 = B2(i-1) } B1(5).decrease // B1 = B1(4)
Huh, interesting, always assumed you had to `scalac file.scala` and then, once compiled, you could run it. Also, what's going on there with the top-level method? i.e. in normal scala `println(...)` at top-level would not compile. Seems like there's an implicit REPL-like wrapping of supplied code.
Yea but having Task with sematics of Future is what I'm arguing is wrong, not the different sematics of Future and Task. I'm just thinking if we have db.run :: Future if it's not worth having extension method db.runnableTask :: Task with correct Task sematics edit: clarification in other words - When I call a method which returns Task I don't expect that it does perform any side effect, and as an consumer of Task API I'd expect it to be re-runnable etc... am I wrong in this assumption? Sure, it may be easily breakable, but it is up the person implementing the method to ensure that it does not perform side effect, just returns the Task. Using Task only as Future in disguise doesn't feel "right" to me. As always I'm happy to be taught/learn otherwise.
You can even use this in the same way as shell scripts just add: #!/usr/bin/scala !# at the top of your file and chmod +x it
I think that the other emphasis of the language was to implement maximum reusability and in some ways its easier to do this very generic dispatch at runtime rather than at compile time (remember that this is what C# does, and it has a lot less problems there because of its simpler type system and the way its designed). Certain things are much easier with erasure, but other things are harder (like multi method dispatch), and it was parts of Fortress design to support all of the various mechanisms (function overloading, parametric polymorphism and multi method dispatch). As we now with Scala, supporting all of those things aint easy Also fortress has been designed for quite some time (2003 iirc) and I don't think people understood the merits of erasure as well back then versus now. 
Sure, absolutely. But in your original example, an implicit conversion from `() =&gt; Future[T]` to `Task[T]` is probably fiddlier to use than one from `Future[T]` to `Task[T]`.
You can find this kind of information with the scala index when organization publish open source libraries. [monocle](https://index.scala-lang.org/search?q=dependencies:julien-truffaut/monocle-core) * xdotai * slamdata * geotrellis * guardian We also want to allow endorsement for organization (if they dont have open source libraries).
Out of curiosity I looked up the specifications for the two languages that Fortress competed with- X10 and Chapel- and the former explicitly imitates Java *except* for type erasure, and Chapel also uses reified types and dynamic dispatch as far as I could tell. Seems to me you're exactly right.
Accept a `T` when inserting. Things that are determined by the database/controller should be in the `Entity[T]`. Things that are determined by your ordinary code should be directly in the `T`. Things that are determined by both... well, try not to have those. (More seriously, see if you can split any such field into a database-controlled component and an application-controlled component)
It might be worth updating sbt, and then updating the dependencies to their latest versions one by one. Hopefully this process wouldn't be too onerous, and it sounds like you may already effectively have done it, maybe minus updating sbt itself. But longer term, I think you'll want to get fully caught up, especially in any project that uses sbt plugins.
I suggest being careful about conflating a database id with any other kind of id in your domain. I wouldn't necessarily expect to receive a database id in any given query. If you really need a "customer id" that's always present, I think it makes sense to have that as part of the `Customer` class. But it also makes sense to have that id come from some kind of previously authenticated token you can verify and extract in your controller, not as part of the JSON data representing the rest of the customer. You also don't need to have the same database representation of your customer as you do in your controller. You can have separate types with their own fields that you map between as necessary. Something like `DbCustomer` when you need to go to/from the db, and `RawInputCustomer` for what you received in your controller, and `ValidatedCustomer` for after you have validated the raw data, and `RawOutputCustomer` defining how you serialize the data for sending back to the user. Then you have a methods like `def validate(ric: RawInputCustomer, t: AuthToken): ValidatedCustomer`, `def prepareForDb(c: ValidatedCustomer, md: Metadata): DbCustomer`, and `def insert(dbc: DbCustomer)`. The names might not be the best and I didn't include error handling, but it's pretty clear there's only one way to flow through such a system and each piece clearly states its requirements through types. You can probably abstract some generic code to handle similar flow for other types, but I also suggest being wary of abstracting too early before you know how the abstractions will actually be useful to you.
I recommend you separate your domain and persistence layer. I don't know about Scala but in Java EE it's bad practice to use the same entity for both form mapping and database access since the form mapping only needs to know about data fields and the persistence layer needs to know about data fields, Id's, time stamps etc.
&gt; I'm not using a relational database Ok that makes more sense then. what are you using?
Thanks, looks like the latter implicitly wraps expression in a generated Main class similar to `object Foo extends App {...}.
Yep you can! This is just better, with features like: - [Importing other scripts](http://www.lihaoyi.com/Ammonite/master/#ScriptImports) - [Loading Ivy dependencies](http://www.lihaoyi.com/Ammonite/master/#IvyDependencies) - [Taking strongly-typed script arguments](http://www.lihaoyi.com/Ammonite/master/#ScriptArguments) And a whole bunch of polish like: - More "obvious" mapping to Scala code (i.e. script wrapper-objects/packages match filesystem paths, no more `$iw.$iw.$iw` stuff) - Much better error messages (less useless-spam, accurate line-numbers) for runtime exceptions Overall, while the `scala` command does in theory support scripts, it's sufficiently unpolished to be unpleasant to use, and it's missing enough critical features it's hard to write scripts that are more than toy demos. 
Full disclosure: I'm the author (of the blog post and the libraries). Looking for any feedback. Thanks.
Double yes
Hi, How is MessageReceivedListener.handle run? Did you try with a simpler code having the same structure? 
The listener is handled by an event system that's part of the Discord API and it registered like this. val listeners = List(MessageReceivedListener, ReadyListener, TrackStartListener, TrackFinishListener) listeners.foreach(listener =&gt; { println("Registering: " + listener.getClass.getSimpleName) Bot.discordClient.getDispatcher.registerListener(listener) }) I know it's registered, and it does get messages (hence why what I send is printed when I type it) I'm fairly certain this is the only way for me to actually do this.
Yeah another thing to note is that if iirc, Guy Steele said that originally the language was designed to have its own VM, but then they ended up running it on Java later on. If they were going to run it on Java for scratch, it probably would have been easier to use erasure and have the JVM do the heavy lifting via class/type tags (in much the same way that Scala works)
&gt; Other than that though --unless I've missed some core teaching of Scala-- they are the leakiest of abstractions, forcing everyone up the chain to understand we're now in futures land, or use Await.result, which everyone drums into you that you should never do. You can write the higher-up things in terms of generic `F[_]: Monad` or similar. &gt; I keep having this feeling that either I'm missing something criticial or this is an emperor has no clothes situation. Unless the IO you're doing is non-blocking right down to the driver / socket you still need a thread open waiting for a response, correct?. Wrapping your DB calls in Futures doesn't magically give you more threads, it just increases the count of one pool while decreasing another. &gt; Even before that, you have to believe (as the article says) that threads are even a bottleneck for your situation, and the dealing with the overhead of moving a task from your current thread into another one is worth it. &gt; I'm reasonably sure this has never been the case for me, but maybe I'm just not working on cool enough problems. But I'm pretty sure if you're writing a bog standard crud app it's not the case for you either. Better to keep your code as simple as possible, deal with each web request as it comes in, in one thread, blocking all the way. Once you start having performance issues, that you can narrow down to be due to a lack of threads, then maybe converting your apis to futures is worth considering. Maybe. True, but these days a lot of systems will be making at least some HTTP calls, and there are plenty of nonblocking libraries for doing that. Back when I worked on a traditional one-thread-per-request Java system we'd almost never get starved of threads on our own database (which was network-local and fast), but if one customer's "callback" endpoint got slow then we could easily have ended up with every thread stuck making a call to them if we didn't have countermeasures. And honestly using `Future`s is not a lot of overhead in code. Sprinkle some `for`/`yield`s around and you're done. Indeed can save on having to write `val` everywhere.
&gt;Other than that though --unless I've missed some core teaching of Scala-- they are the leakiest of abstractions, forcing everyone up the chain to understand we're now in futures land, or use Await.result, which everyone drums into you that you should never do. Its not a leaky abstraction, the whole point of using `Future` is that you don't, and shouldn't need to care about *now*. `Future` is a value that may be computed in the future. You don't know when that is, because you can't predicted the future (no pun intended). Note that the idea of `Future` is that frameworks should interopt with the `Future` type. That is, as an example, your web frameworks should provide a way to return something has a type `Future` (i.e. `Await.async` in Play). And yes, your drivers and such do need to support the concept of async (it doesn't have to be `Future` per say, you can interopt), but this is nothing specific with `Future`. You have the exact same issue with other models for concurrency.
&gt; Its not a leaky abstraction, the whole point of using Future is that you don't, and shouldn't need to care about now. I'm not sure how that doesn't make it a leaky abstraction. What I mean is (psuedo code incoming): def getClient(id: Int): Client def doStuff(clientId: Int): Stuff = { val c = getClient(clientId) ... stuff happens ... return someStuff } If I change `getClient` from `: Client` to `: Future[Client]`, I **have** to change `doStuff` from `: Stuff` to `: Future[Stuff]`, right? And everyone who calls doStuff would have to deal with the fact that its a future too, and so on. The fact that getClient is using futures has _leaked_ up the stack.
&gt; You can write the higher-up things in terms of generic F[_]: Monad or similar. I'm not smart enough to have really dealt with this stuff unfortunately, so I'm not sure how this helps. Same example as the other person who commented: def getClient(id: Int): Client def doStuff(clientId: Int): Stuff = { val c = getClient(clientId) ... stuff happens ... return someStuff } How can I change this code so that `getClient` uses futures without the contract for `doStuff` changing?
 abstract class StuffService[F[_]: Monad] { def getClient(id: Int): F[Client] def doStuff(clientId: Int): F[Stuff] = for { c &lt;- getClient(clientId) ... stuff happens ... } yield someStuff } type SyncStuffService = StuffService[Id] type AsyncStuffService = StuffService[Future] The contract still has the `F[_]`-ness, and you have to ripple that all the way up, but again it's only generic - the middle layers don't have to know anything `Future`-specific (and you can add other effects e.g. audit logging, input validation, database transactions without having to change any layer that was written `F[_]`-generically)
My experience is that as soon as your app has any kind of cross-cutting concerns you need the `F[_]`-ness, and if you already have that then including `Future` in the monad stack doesn't add any more complexity.
not expert scala here but it seems **to me** that you are missing '=' in your defs. see http://stackoverflow.com/questions/944111/when-to-use-the-equals-sign-in-a-scala-method-declaration
&gt; There isn't a design for concurrent programming (that I am aware of) which isn't leaky then. 100% agree! &gt; Also if you are changing getClient to use a Future, you are doing it for a reason 100% disagree :P The main thrust of my point is that using async over sync adds complexity, which needs to be taken into account when using it, which often doesn't happen because a lot of people have a default rule that IO == async. Hope that's clear :-)
100% agree. The point I'm (clearly) struggling to get across is that if you already have a codebase with `: A` having to refactor everything to `: M[A]` for no genuine benefit (ie you're just using async because it's IO, not because you've proven that you'd get any performance benefit or that you need to) is not a good idea.
100% agree with this and the article. Clearly futures can give a HUGE perf gain. I mean netty to apache can be a (Edit small) perf difference but not that huge. It is also a huge disaster to code with. You have a future[List[T]], and a Option[Future[T]] and a Future[Option[T]] and you are doing all kinds of dances to get your ducks in a line. I think next large greenfield scala project I do I am going to ignore futures and code it straight up. If I hit scaling and I can't afford new hardware, I will step back and look at using futures. At least for me most projects never reach the point that "wow this project is making a lot of money, if only it had more perf so I could fit more on 1 server". The project tends to be a dud (in which case who cares), or is making tons of money (in which case, just buy more servers). This thought process is partially why I just did my latest project in Python. I figure if it blows up I can rewrite in Scala some day for more perf...
&gt; Clearly futures can give a HUGE perf gain. I mean netty to apache can be a 20:1 perf difference or more. Wow really! I'd love to read about that. Is there something you can link me to? One of my core "negative" future feelings is that we've (at my prior work where I used Scala) often used them "just because", and not because that code was slow and we needed to make it fast. It would be cool to see an example of how helpful they can be in that respect.
Well and in many cases Futures WILL make code slower. If your code did a bunch of math and returned a result, there is no IO - so futures would just add crap and make your total capacity lower. Most code has some kind of IO, so I suspect most code has some kind of benefit from futures... Edit - So the more I google up benchmarks, the less and less difference I see. Perhaps modern blocking code is much better at longer running type stuff, in which case I have even MORE of a strong feeling to avoid NIO in most cases.
The guy is being a bit snarky. Would need to hear more, but it does appear you could be in over your head. Which is fine if you have patience to learn how everything works. Just don't expect to be the next uber next week based on current tech + knowledge. 
In truth, I didn't know about http4s (thanks for mentioning it). It looks pretty cool. Finagle is also very cool, but strikes me as kind of large. I do like their use of Futures. Mostly I just wanted the experience of writing this. So it was mostly for fun. 
Or maybe you weren't using that functionality at all. (One of the things I really dislike about `Future` is how it conflates several unrelated concerns).
Unless you're wanting to also process, say, Either and Option too, and then you're looking at something like Monad Transformers. That's certainly a big leap up in conceptual overhead.
&gt; you'll have to just put try catch blocks around everything, or what? Or try/catch blocks at a small number of places, e.g. your REST entry point. Unchecked exceptions don't disturb your existing code at the places where you don't care to check -- that's one of their advantages (and there are disadvantages).
Shrug. It's just one more piece of indirection. Write the `Either`ey parts in terms of `F[_] : MonadError[?, MyErrorType]` and then you only need the transformer stack in the one place where you concretely instantiate everything.
So you talk to your database and that is IO and returns a future. Now you need to deal with futures all over, it spreads.. even if the rest of what you do isn't IO.
What repos are you trying to understand that are that hard to read that you have to add string debug output?
These are really neat. Is there anything that you found harder to express in Scala, compared to Clojure? What about the other way around?
&gt; Limitations See the neg test cases for for constructs that are not allowed in a async block See the issue list for which of these restrictions are planned to be dropped in the future. See #13 for why await is not possible in closures, and for suggestions on ways to structure the code to work around this limitation. If there are language constructs not allowed in the block section of the macro, it just isn't as useable as a future. It's my block, and I should be allowed to do what I wish within it. It adds statements to a mostly expression language, overloads if and match, doesn't do anything syntactically that a for comprehension with an Await.result cannot already do, and requires adding yet another dependency to our projects. The Readme is unclear how deep those block restrictions go - if I call a function with a restricted statement in it, is that allowed? It loses the property of closure so I have to add args to my async functions. That's why I don't use it, plus Future is a pretty easy thing to learn and use. I guess I don't care about the type noise anymore, and don't mind using traverse and sequence occasionally. The type of monad often carries semantic meaning -- additional combinators. So I don't mind them spreading to other interfaces.
On a previous project, we were using lots of Akka actors (this was a few years ago, before "reactive" became the hot buzzword). An actor can only be active on one thread at a time. What often happened was that I needed to query the DB, do something with the result, then send a response back to the caller. In these situations (as long as there are no issues of mutable state), I would do the DB request in a Future, and have the post-processing stuff happen in a callback (using onComplete, for example). This way, the actor is not blocked while waiting on the DB. There were some gotchas you had to worry about, but it seemed to work well.
woo
Some really great talks! Thanks for posting!
Any random GitHub repo where I don't know how it works? I mean it's one thing to skim over the words in the code and understand what the person writing the code is trying to say. It's another to actually understand how it works. What repos don't require any string debug output? I mean other than basic simple things that you already know.
If you're like me and like to have sample code you can startup and debug take a look at this simple project I put together to demonstrate the idea last week. You can swap out Option, Future, Xor, etc with no change to business logic. I wish the syntax was a little cleaner, but it works well. https://github.com/ShaneDelmore/InsulatingBusinessLogic/blob/master/src/main/scala/domain/UserActions.scala
What I wrote should just topo-sort an ActorRef DAG and call it in order. Nothing complex, and not something I intend to open source. I'm not sure if I'll ever use it for anything. Yours seems more robust and flexible.
What do you recommend for people who use an asynchronous container that isn't `scala.concurrent.Future`? async seems to be hard-coded to `Future`, pity there isn't some way of abstracting it to work with alternative implementations :-)
Yes, that could happen. But you dont have to wait until the end to log everything at once. Now you have the choice to log blocks of the size of your choice at any moment in time. Having control of the situation is definitely better and more powerful. If you think about you could compose your log blocks anyway you want. You do 3 transformations that are related, and then you log in everything down. The you do another series of transformations and log again. Now you have the power to do more, with more control and simplicity
Great question (and I also apologize for the downvotes people are giving you) I'm a fan of futures but also despise when my co-workers unnecessarily write signatures like you've pointed out: def getClient(id: Int): Future[Client] It makes the function hard to test (e.g. I have to Await.result in tests and in the REPL), relatively harder to reason about, and it reeks of a premature conflation of what should be two separate concerns: business logic and concurrency i.e. what you want to do and how you want to do it. So why am I still a fan of futures (and tasks and actors)? Because it's usually trivial to separate your concerns and, rather than declare a function's async nature in the signature, simply lift your function into an async environment at the call site: //Declaration/Implementation site def getClient(id: Int): Client = ??? def doStuff(clientId: Int): Stuff = ??? //Call site. Note that "stuff" actually depends on "client" so it's fine to wrap the method calls in futures directly in the for comprehension as opposed to outside of the comprehension where they could both run concurrently val stuffFuture = for { client &lt;- Future(getClient(id = 1234)) stuff &lt;- Future(doStuff(client.id)) } yield stuff Does this always work? Of course not. There are times when our asynchrony is at a very low level or a major, heavily intertwined aspect of a 3rd party library. But, for the most part, business logic should strive to look like this. Which is why I got really excited at some of the design aspects of Akka Typed in which the behavior of the actor was completely decoupled from the async nature of the actor runtime. The behavior was just a plain old method.
If you want TCP level connection, I think you can use Akka HTTP that offer connection-level client-side API[1]. [1] http://doc.akka.io/docs/akka/2.4.8/scala/http/client-side/connection-level.html
Fair enough that stream lib is limited to 32 bits. But with 32 bits, we can easily store 100 million objects with 0.1% accuracy in membership requests. That should work for most use cases. But if it's not enough, we can chain multiple BitSets (and ofcourse use a 64 bit hashing algorithm) to get past the 32-bit array limitation. Therefore, I am still not sure if Unsafe is really the best choice in the implementation.
Sink.head has a materialised value of a Future of the first thing to go through the stream, which you are mapping, so you already have a Future[(Try[HttpResponse], DownloadContext)] which you are mapping. Just make your .map map from ((Try[HttpResponse], DownloadContext)) =&gt; Download instead of doing the callback, and you will have a Future[Download] at the end of it. You might want to make it a flatMap instead of a map so you can map the Failure to Future.failed and the success case to another Future (especially since you need to do more IO to extract the response). def run(uri: Uri, context: DownloadContext): Future[Download] = Source.single(HttpRequest(uri = uri) -&gt; DownloadContext(uri)) .log("DL")(log) .via(requestResponseFlow) .runWith(Sink.head) .flatMap { case (Failure(ex), _) =&gt; Future.failure(ex) case (Success(response), ctx) =&gt; extractDownloadFromResponse(response, ctx) } Obviously, you would then need to implement extractDownloadFromResponse in a similar way to return a Future[Download] and not actually process it. Note: The above is the easiest way to modify what you already have to give you a Future[Dowload], but it would be simpler and more efficient to do the mapping with .flatMapConcat before you run it: def run(uri: Uri, context: DownloadContext): Future[Download] = Source.single(HttpRequest(uri = uri) -&gt; DownloadContext(uri)) .log("DL")(log) .via(requestResponseFlow) .flatMapConcat { case (Failure(ex), _) =&gt; Source.single(Failure(ex)) case (Success(response), ctx) =&gt; // You would also handle redirects by returning a Source for the redirected stream here. if (checkResponseIsConsideredSuccess(response)) response.entity.dataBytes.map(Success(_)) else Source.single(Failure(someException)) } } .runWith(Sink.reduce((left, right) =&gt; left.flatMap { leftVal =&gt; right.map { rightVal =&gt; leftVal + rightVal }})) .map(Download(uri, _)) 
Hi. First of all Scala compiles to Java Byte Code. For this reason, the application runs like Java application. you probably know how a Java application runs. Scala applications can be packaged by SBT, Maven or Ant but SBT is the simplest way even SBT Assembly plugin makes it easier. Install SBT Assembly plugin and run "sbt assembly" command. After this process you can use "javar -jar x.jar" command with created JAR file. Best.
This is great! Thanks. I'll give it a whirl tomorrow
I am writing the server. What I am trying to do is when a client connects to the server I want the server to save a reference to that client so that the server can send out private message to that client over tcp connection. I tried to use the Akka serializing approach but I could not get it to work.
I find the [doc](http://doc.akka.io/docs/akka/current/scala/io-tcp.html#Accepting_connections) quite helpful. class SimplisticHandler extends Actor { import Tcp._ def receive = { case Received(data) =&gt; sender() ! Write(data) case PeerClosed =&gt; context stop self } } sender() returns the ActorRef where you can write to. Or try the new fancy [streaming](http://doc.akka.io/docs/akka/2.4.8/scala/stream/stream-io.html) API. 
Or try to take a step back and just try it with basic Scala or Java since Akka brings a lot to complexity to the table.
Yeah I already wrote the java and scala basic versions. Wanted to try out Akka since I see a lot of people using it. Ill just do some more reading. Thanks for the help though.
+ Most of Kotlin's features looks like they've been nerfed because of beginner-friendliness and java compatibility - wouldn't it be easier to just write|use a Scala style guide, like [the one used by Databricks](https://github.com/databricks/scala-style-guide)? Because as far I can think it's just [NIH](https://en.wikipedia.org/wiki/Not_invented_here).
Avro Schema. "Our plugin takes as input a datatype schema in the form of a JSON object, whose format is based on the format defined by Apache Avro." There is a mapping in between Avro Schema and JSON schema: https://github.com/fge/json-schema-avro
Wow. This made everything so much simpler. Thanks.
To be able to have a continuously running graph that allows feedback, you have to basically re-implement how akka http itself is implemented. The source code is here https://github.com/akka/akka/tree/master/akka-http-core/src/main/scala/akka/http/impl/engine/client Long story short, you need an akka Actor which is a ActorPublisher and ActorSubscriber to act as the core, and run your feedback back into it. Using singleRequest is much better in this situation. 
A limited "blessed" set of overloads is how you end up using the same operator to mean left-bitshift or write data. Or Python programmers getting excited because a new release lets them use `@` as an operator. Part of me would like to see a Scala-like language with symbolic method names completely banned. Certainly there has been a lot of tastelessness in popular Scala libraries. But allowing some specific operator overloads and not others seems like the worst of both worlds.
If Scalaz `Task[A]` can be though of as `Future[Throwable \ / A]` (in one aspect), is there some nice way that I could have something like `Future[T \ / A]`... so I could model `T` as some sealed hierarchy of potentials errors (or coproduct ...), and then I could pattern match on all potential errors instead of, as in `Task`, generic `handle` which is partial function from Throwable. so then in for comprehension, I could fail it with: `catchable.fail(someError) where someError &lt;: T` instead of `catchable.fail(someError) where someError &lt;: Throwable`. Benefit would be that I'd like my handle not to be partial function, so that I be sure that I handled all potential "failures"... Or does it not make sense? p.s: I know I can implement this stuff on my own. But as usual, I'm looking for some insights.
This discussion went too far. 1. No one stops anybody from using Java EE with Scala so using this as a indication that somebody will deny any progress in is just poor. 2. I earn for my life doing Java EE. I don't feel that this pushed me into Kotlin camp. I'm happy that many people like Kotlin. I'm not. That's why I started learning Scala despite some objections ans so far am very happy with my choice. And I'm still JEE "professional". Do you have any problem with this? 3. Like it or not but Scala wouldn't lift off for years without Java EE which popularity secures Java SE which usage secures JVM existence which is so far default option for Scala. 4. … and that's also a big win because it makes much easier for Scala to interoperate with legacy Java code bases, lowers learning curve for Java devs (especially for those familiar with Java 8) and grants higher availability of potential Scala devs.
And even it they would be, why not? For me shortcuts in http://www.scala-graph.org/guides/core-initializing.html are example of how one shouldn't create operators in Scala. But on the other hand I'm neither user nor creator of this library and who knows, maybe thats the best they could do it and it works? Then if one has library where emoticon-like operators are in use, how the author with such low knowledge of Scala eco knows that they are bad? In the end it's always teams decision to choose their conventions and stick with them. If one abuses it then blame the whole team, as it's code reviewer responsibility to check if code fits those conventions and the whole team responsibility to treat code-review seriously.
&gt; I earn for my life doing Java EE. I don't feel that this pushed me into Kotlin camp. I'm happy that many people like Kotlin. I'm not. That's why I started learning Scala despite some objections ans so far am very happy with my choice. And I'm still JEE "professional". Do you have any problem with this? Do you believe JEE is valuable? In my experience JEE is a negative-productivity technology, bad enough that I don't want to admit to working with it - I wouldn't want to call myself a JEE professional any more than I'd want to call myself a Drools professional or an ESB professional or an Oracle professional - and so anyone who would willingly call themselves a "JEE professional" has very different taste from me to say the least, and frankly I would weigh such a person's opinion less when seeking technology advice. &gt; Like it or not but Scala wouldn't lift off for years without Java EE which popularity secures Java SE which usage secures JVM existence which is so far default option for Scala. I don't believe Java needed any such "securing". Java SE would have succeeded with or without Java EE, IMO.
thanks for the input! I'm going to make github issues for both of those points. 
Thanks for reply. It is another missing piece of puzzle for me :) I have a followup question - Suppose I use EitherT (although it's not really relevat...) So lets say `FooService.barMethod` can fail on some domain specific stuff like `X_Error` and `Y_Error`. Also `BarService.barMethod` can fail on stuff `Y_Error` and `Z_Error`. I can represent it via sealed traits, but then Y error is duplicated... sealed trait foobarMethodErrors case object X_Error extends foobarMethodErrors case object Y_Error extends foobarMethodErrors sealed trait barbarMethodErrors case object Y_Error extends barbarMethodErrors case object Z_Error extends barbarMethodErrors note: i found this very painful to write and to read as well The way I see it I should just model those errors as coproduct, would something like this work or is it overkill? How would you handle it? FooService.barMethod: EitherT[Task, X_Error :+: Y_Error :+: HNil, A] ... BarService.barMethod: EitherT[Task, Y_Error :+: Z_Error :+: HNil, A] I'm afraid some people will murder me if they see this in codebase. tldr.: how to model failure as subset of set of all potential known domain failures without repeating existing stuff in sealed hierarchies -per-method-. p.s.: I am not experienced enough with FP, but I have feeling that I have to fight Scala language to get things done the way I feel are right sematically. I attribute it to my incompetence &amp; inexperience, which results in these questions... but surely I'm not the only beginner who encounters them... I will probably get downvoted for this, but I have feeling scala does not wish to make FP programming very pleasurable. I'm starting to understand why people use it as better java or go with Kotlin I guess. I don't have experience with other as advanced languages as scala, so I can't really compare.
"per se" GUI on the JVM in general is awkward. In the long term it's good that people can't/don't just slap some bindings on a C library, but in the short term yeah it is frustrating. I don't have any easy answers.
I ... really empathize with you here. Feel like I'm going through some similar things. This particular question, even, is something for which I haven't been able to find a solution I like.
Jupyter notebooks supposedly support scala (although I haven't actually set up an environment). It really would be nice if there were a plotting library that could work inline. Maybe a google chart wrapper?
https://github.com/andypetrella/spark-notebook#spark-notebook
Is it still fine to use the first edition of programming in scala or has enough changed from 2008?
It might be better to think about this in terms of who spawns the Server actor. I recently went through trying to understand this and I found that what I was looking for in most cases was `context.parent` which is the actor that spawned my server actor. Have your client actor spawn the `server` actor then reply to the client by calling `context.parent ! message`
Hi All, I'm working in MNC(2+ Years) but want to make future in Data Scientist field can anyone suggest me complete ROADMAP how to start scala,spark,hadoop,python to achieve my goal in 3 months any free great online courses available to achieve my target (self learning)
There are a whole bunch of such libraries, none of which seems to have taken hold yet. See https://github.com/pelotom/effectful/ for one example.
If I was just doing low-volume CRUD, I probably wouldn't bother with Scala in the first place. But I would say it's usually pretty hopeless to try to convert a deeply synchronous codebase into something that is usefully asynchronous. That said, in my experience, it's not all that bad to just deal with the asynchrony up-front. You can do it for roughly the same price as working with `Option`. But asynchrony has been particularly useful in the web space due to the characteristics of web programming: it's bursty and I/O heavy. If I've got a Rails API serving a single-page front-end, that application probably generates a bunch of database requests at the same time. It's quite nice IMO to have a back-end that is capable of juggling all that wait time efficiently. I think you are on to *something*, although you don't quite get there. If you haven't seen http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/, you might want to check that out for something that strikes at the heart of the problem you bring up. In general, we don't have a wholly satisfactory solution for writing code that should be able to suspend itself, yet have that capability be more or less transparent to its caller. One proposal I've read: http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/.
One interesting aspect of the article is that if you program in this way, you can no longer effectively use failed futures. (Or at least you wouldn't be able to access those APIs in the middle layers of your app.) So then I suppose you'd want to create a type that represents failure cases and abstract over that, and then you start heading down a rabbit hole.
Kotlin also suffers from ambiguity by overloading common operators for appending single items and multiple items. I ran into this bug really early: https://youtrack.jetbrains.com/issue/KT-9992
*Argumentum ad hominem*
Maybe try something with Future and Promise? I solved a similar problem recently myself (except with jquery/scalajs instead of apache), here's what the end result looks like http://pastebin.com/BWe9jQqK edit, now without a silly mistake: http://pastebin.com/NqGLDmHN 
If postString never returns null, you can write the same thing as: def sendResponse(json: String) = http.client.Util.releaseConnection(http.client.Util.postString(url, json, "application/json")) If postString does return null, you can write your own releaseConnection wrapper: def releaseConnection(conn: Connection) = if (conn != null) http.client.Util.releaseConnection(conn) 
Yep backticks generally turn invalid function names into valid ones... Well, that is, except for the family of def backtick themselves. They appear doomed to a fate of inescapable invalidity, and go down in the ship of syntax that our threads may `yield`, our `type`s be `type`d not `tpe`d, and that our css `class`es be not `className`d.
I know not everyone will agree with me, but any of those are fine. I have coworkers who will write code in each of those respective styles. All of them are fine for me. Maybe one of them takes a bit longer for me to syntactically parse, but that is so irrelevant in the grand scheme of understanding an application or what the code intends to do that it doesn't bother me to have different styles. 
There's very little public information about it that I've seen, and I've been looking. So I think the answer is no. (I suspect that may be why you didn't get a response)
I find the implied functionality behind postString a bit weird. When would it ever return null, and why does it create a connection that remains open? It seems to always return Unit so there is no way to react on potential errors. If it is important to leave the possibility to keep the connection open I would pass it as an argument to postString, alternatively create and initialize a StringPoster class that takes the connection as a constructor argument.
Looks like you're wrapping the jQuery-call in an unnecessary Future.
It's not strictly necessary, but I like the way Future behaves more than I like the way JQuery deferred behaves. It lets me just flatten the thing to get my result and not have to deal with the null nastiness seen in the parent comment. If you've got a cleaner way of representing the result of getJSON here, please show me edit: Oh duh, not the Future monad but the Future block. Mea culpa!
Just remove the `Future { … }` block: you're already returning the future managed by the promise and using jquery's getJSON is asynchronous as evident by the done/fail callback hooks. edit: to clarify, I would remove line 4 and 8.
I'll have to agree with other people, shorter doesn't mean readable. My question, to rephrase, was "What is the most readable code?" Although I was asking, what is idiomatic scala, the reason I'm asking that is I want to know what is the most readable code for experienced functional programmers?
There is a video from ScalaDays that explains the reasons behind this programming model. https://www.youtube.com/watch?v=7lulYWWD4Qo&amp;index=11&amp;list=PLLMLOC3WM2r7kLKJPHKnyJgdiBGWaKlJf
If I might interrupt the circlejerk: it's incredibly arrogant to suggest that the only reason the Kotlin language designers have made their choices is out of ignorance of "The Better, Scala Way". There are obviously trade-offs that have been made; to present the other side, here are some Kotlin advantages: * Kotlin operator precedence rules are vastly simpler. e.g. in Scala, `42 =/= 43 &amp;&amp; 42 =/= 43` will compile, `42 !== 43 &amp;&amp; 42 !== 43` will not * Kotlin operators can be called naturally from Java, as they are desugared to standard Java-friendly method calls * Kotlin does not have the complexity of associativity depending on the last character of the method name * Symbol soup is prevented as arbitrary symbolic operators are disallowed
Linking here to see if anyone might have a solution
Does a sealed trait and a bunch of case objects not work for your use case?
In Scala enums were never meant to be a native language feature. These days I use https://github.com/lloydmeta/enumeratum
So does that mean probability and statistics? Calculus? Linear Algebra? Diff Eq? 
enum sucks. Long live sealed algebras
They don't enumerate.
But sealed algebras unlike enums don't have implicit ordering based on declaration! enum Day {Mon, Tue, Thu, Fri} assert(Day.Mon &lt; Day.Tue) 
You can [create a SIP (Scala Improvement Process)](http://docs.scala-lang.org/sips/sip-submission.html).
Play is a pretty heavy framework, in my opinion, and not really suited for anything with "micro" in the name. You should probably take a look into Finagle/Finch. I am working on a microservices generator based on Finch and Quill that allows you to have endpoints that look like: private[this] def getUser = get(prefix :: long("id").should(positive[Long]) :: authorize) { (id: Long, u: MaidenAuthUser) =&gt; render { User.get(id) } } Which handles all model lookups, authentication, param validation, etc... Finch is *definitely* worth checking out.
They do wih macros. See Enumeratum
I think the most important consideration is how "big' is actually "micro". The whole notion of MicroService development is not about spawning hundreds of services, but about extracting parts of the application as self-contained services, where it makes sense. Size of MicroService in Domain Driven Design is usually somewhere between Bounded Context and Aggregate, meaning they can be quite big. Some even argue it is best to start with a monolith and extract away what can be standalone during development. Having too small microservices is even named anti-pattern, called "nanoservices", according to [Wikipedia]( https://en.wikipedia.org/wiki/Microservices#Nanoservices) [Martin Fowler's guide](http://martinfowler.com/microservices/) might help, question of size is mentioned there as well.
So we already have a monolithic web service already which is being broken up into micro. So thankfully we don't have the problem which you are describing. but we do have a problem where 1. the caller needs to know about N number of individual endpoints. 2. debugging is now very hard because each thing like logging is happening individually. 3. Once endpoints have been exposed to the client app, we must run them forever otherwise the client apps break. these are pretty serious problems and play/spray do not help us in these....
I've heard that one does not simply write micro services. You write the monolithic app first but keeping an eye on keeping things separate (see separation of concerns). Then this application gets broken down into smaller concerned micro services. This I think came from the author who first suggested Microservices. I'll look up the sources later. As for the code, write your own small library. It's Good practice!
For me that's a dirty trick from the seventies, when enums where just ints with an alternate name... Most of the time there are no natural ordering, and when there is, you could create an Ordering type class or extend the Ordered trait.
Id say this is common sense but sadly uou get enforced into microservices first, because yout coworkers used to design their monolyth as big ball of cyclical dependecies/mud which they could not separate.
People have been talking about it forever. I believe I've seen proposals for Scala 3. I'm not hopeful about any of them ever achieving consensus though - it's too bikesheddy an issue to ever solve. In the meantime I just use Java enums in my projects.
This is a totally reasonable question. I've usually seen microservices become the solution to a monolithic web app which is on the verge of collapsing under its own weight to become a black hole that nobody's time can escape from. So you split that app into two services. Your product grows. Then you have four srevices. Another team joins. Then you get 7 services. And so on, and so on. Each service should have a well defined function, scaling properties, and type of state that it manages (owns a given database or other resource). These services might do really simple things, like encapsulate a few tables, and serve only one or two endpoints. Now you can deploy a new set of functionality completely independently from what some other team is doing. Freedom! Individual services might be only sort of useful since they only have a fraction of the whole picture. But your system is all about how well your orchestrate services together to get the job done. That's the goal, at least. The how of it is: Take pieces of functionality that are somewhat unrelated, split them into separate services. Find some agreed protocol for them to communicate with each other, possibly via REST. Add a service discovery layer, and a clean way to deploy new versions of these services frequently. Spray or Akka-HTTP work well for micro-services, since they act as libraries, can be globbed onto existing codebases to provide HTTP support, and it's very low effort to work with them to provide just one or two HTTP routes which serve and accept JSON. We use Akka-HTTP to build our service oriented architecture at Twilio, and our role of thumb is to take each service as small and focused in scope as possible. As you brought up, one of the key parts of a service oriented infrastructure is service discovery. How a given service can find other services to do work with. There's many solutions to this, including CoreOS's etcd. Another popular approach is to install Haproxy on every node for every service, assign ports for different service types (account service is on 9993, inventory service is on 8873), and push updates to haproxy config when nodes go up and down. Do you need microservices? There's non-trivial overhead to building a system like this. Inter process protocol versioning and compatibility. Service discovery. Maintenance of many nodes. But the motivation becomes more compelling when you have many teams building things at once, especially when some of those things are communal and some are specifically built to serve exactly one product. Twilio uses microservices because it lets us iterate quickly and tune scaling profiles without disrupting other functionality, and throwing up a couple of Akka-HTTP routes is super easy. Our goal is to let many nimble engineering teams make many deploys to production a day. That's really tough to do with a single monolithic app.
Also use enumeratum, can vouch for its usefulness
Find another job. I'm serious, I made the mistake of staying at a company like that. They are going to go bankrupt, so you can either find another job now on your terms or later in a rush.
I hope not. Enums are an anti-pattern. Use a ADT with an Order typeclass instead.
One example is a list of error types for collecting metrics, I want to add them to a collection with an initial value.
&gt; Play is a pretty heavy framework, in my opinion, and not really suited for anything with "micro" in the name. What do you mean by "heavy"? What are the costs of using Play for microservices compared to other frameworks?
This is one of the seemingly mundane features I get lots of questions about from colleagues such as: 1. Why is it so much more complicated than Java enums? 2. What is the "= Value" for and what does it mean? 3. My code won't compile def foo(barEnum: BarEnum) but the type exists.... what do you mean I need to use BarEnum.Value or add an import? Why's that?
This is a bit disengenuous -- you get more separation with a network boundary than with a compile time boundary. For example, you can pass a closure containing code that runs module B into module A (which doesn't know about module B). You can't really do that over TCP.
Scala can be just as good with recursion as Haskell, as it performs [tail call optimization](http://stackoverflow.com/questions/310974/what-is-tail-call-optimization). This example, however, is much better suited for the iterative approach, and in non tail-call optimized languages the recursive approach here would quickly blow the stack frame. I prefer the iterative approach here because it's a one-to-one correlation with the semantics of your program: repeatedly read messages until you get a logout message. The recursive approach doesn't buy you anything in terms of abstraction, and it requires a second or two of making sure all the recursive calls are there to convince oneself that it is indeed just repeating itself until it gets a logout message. In general, if you have two solutions that are semantically the same, pick the one that is more readable. Problems where recursion becomes more readable include ones that model mathematics, break up a bigger problem into multiple smaller ones, and that process certain data structures (cons lists come to mind). Note that your while loop doesn't have an explicit exit condition. I would recommend either replacing the `while true` with something along the lines of `clientHasMessage()`, or adding a `break` statement in your `Msg.LOGOUT` branch.
True up to a point (there are ways to serialise things with a certain amount of behaviour), though I don't really see the concern? The closure might "run in" module A but it doesn't have any access to module A's internals.
&gt; The default build is now using Scala 2.11 rather than Scala 2.10. I can't wait for Scala 2.12 to be released so we can live in version conflict hell all over again.
But, next time you have to insert Wed between Tue and Thu, you have to touch the other case objects too. Also, the Java enums automatically give you Reader capability e.g. Day.valueOf("Thu"). I think the point is not that "enums are not possible in Scala" but more like "enums are way too verbose to emulate in Scala". Even a simple sealed algebra does not easily give you ordering based on declaration order or reader capapbilities etc.
I was really surprised to see that they list it as both 'web' and 'mobile', but not 'enterprise'. 
Sure, but I've heard people say SQL is low level. That is to say, it's a rather ambiguous term Now, the biggest disadvantage of Akka in my experience is the fact that you are restricted by the communication protocol (Akka actors can only communicate with other Akka actors) whereas a thrift or HTTP protocol layer on top of them allows for multiple languages to work together Aaaaaaand if you want to go that way, Play is great because it has Akka baked right in. Sure it's "heavy" (again, a loose term) but it's totally up to the developer which parts of the framework to use. Or use finagle to define a thrift interface. Another thing to point out, micro services are not defined by the binary size of your dependencies but by the structure of your code. Of course you can use play to build micro services (in fact, I would start there)
If you want to make it as functional as possible you should look to pass the received message explicitly (rather than calling the non-transparent `clientIn()`), avoid the cast, and use a proper ADT where the different types of messages are represented by different types rather than just having a `getType` field. In terms of the loop you might want to look at something like fs2 (previously scalaz-stream), which can express pipelines that possibly run forever without losing the ability to manage effects/resources as you do with an imperative `while` loop. (That's quite possibly overkill - maybe the `while` expresses your intent clearly enough - but if you want "as functional as possible" then that's the direction to go)
Yes, but how often do you add new error types ? How do you compare the hassle of having to maintain an ordered list of instance in contrast to having to increase the language specs ?
&gt; I don't think the current-SBT approach of special-casing scala-the-language for cross-building is really viable. We need build tooling that knows how to cross-build against any number of dependencies, and conventions for how to name the resulting artifacts. It's not special cased. You can cross build against a product of any number of dependencies. 90% of people don't know how to do that however. Take for example, libraries cross built against scala and scala.js at the same time, or sbt plugins which are cross build aginst of the version of SBT they're compiled with.
Awesome. Are there some samples or blogs about usig ETCD or HAPROXY with Scala apps?
Does Lagom have a scala API? I thought those guys were only looking at java. Also does it have any mechanism of service discovery? that seems to be my biggest pain point. I don't want people to know about individual micro-services. I want them to have an illusion that there is one place which is offering the all the end points, but secretly behind-the-scene there are 100s of individual web-services.
So any guidance on how can i use either Zookeepr / Consul with Scala? In the end, what I want to achieve is that people are transparently routed to the individual web-service without really knowing about individual end-points.
I would suggest Consul since it's a lot easier. It has an easy http api to use if I remember correctly, since it's basically a key-value store. So on startup, you send a request to Consul that says your service is there basically. I personally really like this series of articles for learning about anything that enables microservices: [Here](https://www.nginx.com/blog/introduction-to-microservices/)
Look at [Consul](http://consul.io)?
wow!!! thank you so much. 
I'm not sure that's all there is to it. The separate channels effectively establish multiple typed (!) mailboxes per actor, and then the combinators on event streams allow some interesting effects that they show to be awkward when working with a callback from a single mailbox.
I'm not aware of an alternative. If you could explain further what your problem is someone may be able to suggest an alternative way of solving it; performing side-effects inside a constructor is quite unusual.
See if you can have a strongly typed interface to wherever you're getting the messages from.
Sure. In the code base, whenever one encounters a situation where one would throw an exception (for instance, when there are network connectivity issues) we wrap the exception up in an Issue before starting recovery procedures. An `Issue` is a trait that adds information to the exception : when it was thrown, in what part of the code base it occurred in, human explanations, etc etc. We have an `AbstractIssue` that extends our trait, which merely grabs the time it was created on construction. New `Issue`s are new classes that extend the abstract class, that add case problem dependent information to it. Any `Issue` can be turned into a simple case class that maps to a Slick database. The goal then is that anytime an `Issue` is created, it gets logged into a database we have setup. What I'm trying to avoid is having to add code to every existing Issue in our code base. Ideally I could only add the code to the trait/abstract class. Currently I can achieve this using `DelayedInit`, but I'm hesitant about it due to its various bugs and deprecated status.
What are best practices for storing older versions of Scaladocs in gh-pages? Context: We are building a framework in Scala - as part of our CI pipeline, every build generates (and overwrites) previous versions of our ScalaDocs which are hosted on gh-pages. We'd like to maintain older versions so users can reference the documentation that corresponds to the version of our framework they're using. Really appreciate any insight or best practices you can share. Thank you!
"Don't use JSON use XML" is a position I don't hear often. Could you elaborate?
&gt; Scala can be just as good with recursion as Haskell, as it performs tail call optimization It performs *self* tail call optimization, but not the more general TCO. However, using `TailRec[A]` and the `TailCalls` utility object, you can use trampolining to work around the problem.
It's really just about API enforcement and version drift. If you take microservices seriously and get to the point where you have a dozen or more of them that are supposed to evolve independently, you now have a pretty nasty exponential consistency problem among their APIs. JSON doesn't have a mechanism for addressing this. For all its flaws, XML, with its XML Schema Document (XSD), does. You can even make the XSD for a service's API available at a fixed, known URL so that your API is discoverable.
How will they interop with Java code? Will they be source compatible with Java enums?
You are confusing the helpfully named 'Typed Actors' and 'Akka Typed'. The one linked has pretty much been declared a failed experiment - The Akka documentation itself (the one you linked to) recommends not to use it except for interfacing to the actor system from 'plain old code' - (http://doc.akka.io/docs/akka/snapshot/scala/typed-actors.html#When_to_use_Typed_Actors) and a blog post here (http://letitcrash.com/post/19074284309/when-to-use-typedactors). This is due to the 'leaky' RPC abstraction - I personally like this post (https://news.ycombinator.com/item?id=4174899) on the subject. I tried typed actors manny moons ago and ran into exactly these kinds of problems. 'Akka Typed' is new and experimental (I had missed its release) - [documented here] (http://doc.akka.io/docs/akka/snapshot/scala/typed.html) and they state 'We expect that this module will stay experimental for multiple major releases of Akka and the plain akka.actor.Actor will not be deprecated or go away anytime soon.' knutwalkers implementation seems more complete/stronger to me. Akka Typed appears to only type the top level entry actor only (I may well have not fully grokked it yet). I hope the library author will share his thoughts on the strengths/weaknesses. 
If that's all it is, then, indeed, that's a total non-issue.
I'd go iterative personally, Stack Overflows worry me, and I don't personally think trampolines are worth the extra complexity.
WTF? That's basically the 101 of "things not to do when designing collection apis". Didn't they have Java's `remove` to learn from?
&gt; it's incredibly arrogant to suggest that the only reason the Kotlin language designers have made their choices is out of ignorance of "The Better, Scala Way". How is it arrogant of it's true? &gt; Kotlin operators can be called naturally from Java, as they are desugared to standard Java-friendly method calls But you lost the ability to tell what method is called when you use operators in Kotlin without having the translation table in your head. Additionally, if Java code uses one of the "special" names, you lose the ability to use operator syntax in Kotlin for that method, because you can't retroactively put `@operator` on Java class files. &gt; Symbol soup is prevented as arbitrary symbolic operators are disallowed You can use any operator you want in Kotlin as long as you put them in backticks.
An increase in the size of the language spec has no impact on me. The array of enum values should be immutable, as it is in Java.
They did. Three times. And then deprecated them in turn.
Add sbt-doge and then `so clean; much package; very publish`?
This list is weird. HTML is not even a programming language and some are missing. Where is Elixir for instance?
By doing it increased the complexity of the language without any benefit: If you want to have two different names, why not define two methods with exactly those names? There is no special hard-coded language rule necessary. &gt; I am saying that the language has different priorities and has made its trade-offs accordingly. ... and that's what people have said all along the way: The decisions taken by "different priorities and trade-offs" have turned out to be worse in general. In some cases they are just repeating the mistakes made in earlier versions of Scala.
Hey why is this in /r/scala?
I thought this would be relevant given the onging discussions about Scala core, and the Scala Center project by EPFL.
Perhaps he meant Scala Native?
I agree with that as far as it goes, but even for something like streaming I don't see why you'd need untyped actors to implement it - I mean doesn't fs2 offer the same kind of functionality all built on typed futures? I can see that for very high performance you need to avoid having a stack, but surely we can offer something safer (tail calls and CPS?) than just offering an API for arbitrary unidirectional messages and relying on users to get it right?
I'm not sure they are ready for this. They don't even support multiple Rust versions or even any kind of versioned artifacts. Changed Rust version? Enjoy waiting while everything recompiles, even if it was already compiled for this version before. Their uncritical fixation on Haskell will be their undoing.
The thing about Rust is that Mozilla has such a terrible track record. I mean, Klabnik literally talks shit about anybody that mentions capitalism, preaches about Communism and hates white people. It's not really a group I want to be associated with. On top of that, why the fuck anyone would download a systems language off of Github in the first place is insane to me. Rust isn't even close to being a standardized language, whereas C has 50 years of somewhat consistent semantics. I'm not saying that C is a 'more advanced' language than Rust, but I don't see the hype behind Rust. Why not just write a C compiler that implements memory borrowing and ownership? Like, I can't imagine going to my boss and trying to pitch Rust over C/C++ that everybody already knows, is standardized and well-understood, and already installed on almost everything, AND doesn't rely on Github to not get MITM'd. I'm constantly amazed at how quickly people throw their own security out of the window. I'd think that a group so concerned with memory-safety would also be concerned about downloading binaries over HTTPS connections that we already KNOW are compromised. Mind-boggling.
&gt; Actors are perceived to be difficult to use because there are often misused. Unfortunately the community has been very poor at presenting valid use cases, so end users incorrectly reach for Akka as "the way to do concurrency in Scala". Completely agree. And it has ramifications in other areas as well. For example, a lot of problems get scapegoated onto `Future` which don't have anything specifically to do `Future`, but has more to do with peoples pains of combining `Future`'s and Actors (since Actors are asynchronous you need to use asynchronous methods so you don't block the actor and `Future` is the ideal mechanism for that)
If you use semantic versioning you can store the api like this: `https://example.org/api/1.2/...`. Since any change to the patch should not change the public api. If you publish to maven central you can use https://javadoc.io.
&gt; How do actors help with that? I mean you can give an actor a dedicated dispatcher but you can equally well give a future its own execution context, no? Actors help because they give you fine grained control (with their mailbox which is essentially a queue) at every process in the business logic and then you could individually handle how load was handled. It was a pretty niche problem in our usecase, but its also a problem that isn't that rare &gt; I agree that this piece is useful. I just don't see why we need all the rest of actors. What do you mean by the rest of actors? &gt; Most of all I don't see why solving any of these problems requires giving up proper types. The main selling points (or more accurately advantages) vs other methods are location transparency which then leads to a host of other problems (like unbounded state machines). Its very hard to do this (well) in a typed environment, which is pretty evident in the issues that people have when trying to make typed actors. Iterating again, that in good design, most of these things are hidden away (at least if you are dealing with high level stuff). Actors may be doing crazy stuff under the hood, but end users usually only have to deal with a `Future`, but using `Future` or `Task` to do these things is pretty painful
I assume LLVM doesn't Count?
I mean...Java seems to do fine between versions.
I think LLVM is orthogonal to the issue being discussed (which is removing binary incompatibilities). Then again, its hard to understand what /u/huhlig means
Yeah. Scala upgrades are a walk in the park. These problems just don't exist in Scala. There are people who publish libraries against 3 different major versions of Scala. Imagine the ecosystem would discard any compiled artifacts after a version change and require to recompile everything. That's Rust. Rust's build system has zero support for multi-version builds. This will be a huge issue if Rust wants to increase their adoption outside the circle of early adopters.
&gt; Huh Scala was (and still is) appealing to me because of how much it improved over preexisting languages at enabling non-trivial manipulations in a type-safe way. But it feels like it has been a long time since the language has broken new ground. Well, besides the already mentioned stall on settling on a better enum and type system enhancements due in Dotty, what I mean is that it's been a while since there's been anything done to aid in basic programming techniques. Take destructuring -- it can do a whole lot, but if you want a to pull out fields by keyword inline with the existing syntax, you're out of luck. Or `async` -- there's the scala-async library, but it's still quite limited. Or better support for abstracting over arity (this may be coming with Dotty, I hear, though). Much recent progress has been accomplished through macros and compiler plugins. It's great that the language is extensible in these ways, but there's also something to be said for enhancing the core, because otherwise, you can end up with a wide array of incompatible implementations of the same concept.
&gt; But that doesn't mean it should be beyond criticism. I think nobody is arguing that. But adding things to a language has real costs and with so many things added and removed over time I think it's getting increasingly harder to find a new "configuration" of features that covers as much ground as it does currently. &gt; One has to recognize that the same argument of "you can't please everyone" could have been applied to every useful feature in Scala. Sure. But assume we are currently pleasing 90% of the people. 10% are displeased due to various reasons. Now we fix one of these reasons, and turn 1 of those 10% into pleased users. But now we have to take into account that _all_ of the 90% of the happy people as well as the other 9% who didn't care about this particular feature have now to learn another new feature they didn't care about. If even 5% of those are unhappy about the additional busywork, we have _lost_ 4% of happy people, not gained 1%. You can adjust the percentages as much as you like: The threshold for introducing new things will still be much higher than you probably expect. &gt; ES2015 ... I think ES2015 is a great example of what Scala has done so successfully in the past. What you propose is more like ES2016. Its new features? - Special syntax for exponentiation - `contains` on arrays, named `include` (thanks to the guy who already fucked up the promises spec earlier) Both things are so pointless that the mere existence of ES2016 is wasting everybody's time. Let's not even talk about the sense (or lack thereof) of adding special syntax for such a "popular" operation to a language famous for not having a usable set of number types.
I'm very curious about more trip reports with Free along with using Coproduct/Inject. We're starting to build some services with it, but we don't really know how it will endure in terms of adoption by the team, code maintenance, growing complexity (I know the point is to partially to manage complexity), and performance. Everything looks incredibly promising, but I would just love more data. Kelley Robinson's post and talk are excellent. And I know that the Verizon Oncue folks (hi paultypes) are big advocates. I also remember seeing a great talk from a gentleman at an Australian company who started designing programs with it, to good results. Does anyone else have any advice for building production (enterprisey) services with Free? Positive or negative experiences? Weird edge cases or things to look out for? One weird tip Runar doesn't want you to know?
Agreed. I, too, will be interested in hearing thoughts from the experts. It's intriguing that this idea hasn't taken off.
&gt; The author gives two examples of Monads, This is actually a free monoid, not a free monad. &gt; Because it seems to me that both examples are losing information (and they're losing the same information). You're partially right: they both lose information, but they are not losing the same information. The free monoid keeps just enough information so that it can be replaced with any other monoid. Since every law-abiding monoid is associative, it is fine to lose the information about where the parentheses were in `a append (b append c)`.
I'm in the process of introducing this at a second company, with a team of engineers many of who are learning scala on the job. In this situation I'm gradually moving wholly towards the "reasonably priced" architecture Runar describes. What I've found through wrapping my own head around this pattern, as well as repeatedly introducing it to others is that the organization of concerns is really important (examples with multiple dsls in one file add enough noise to hinder understanding), as is "hiding the wires". The approach I've seen in what examples I've found (including the one outlined in Runars slides) involves smart constructors per dsl, which introduces type machinery in your domain code. As I've iterated on this, I've arrived at a pattern that keeps domain code down to ADT and natural transformation, and every iteration has gotten a better reception from coworkers. The most recent when presented to some newer scala devs was seen as an obvious pattern. Timely enough, I just wrapped this into a lib and published. Example use is here - https://github.com/mblink/composefree/blob/master/example/src/main/scala/Example.scala With this approach all of the type mechanics live in the lib, and the most plumbing you have to do in your code is to define your coproduct types and OR some interpreters together. Adding new commands and using dsls has 0 reference to Free or it's supporting mechanics, and adding new dsls really only requires you adjust your coproduct def and interpreter def (as seen here - https://github.com/mblink/composefree/blob/master/example/src/main/scala/Compose.scala) As for experience using it, nothing but positive so far. I love the clarity I get approaching new functionality by starting out defining an api as an ADT. Once implemented, I love the flexibility I have to unify my DI for a disparate array of concerns all on creation of an interpreter, and the ability to narrow the code that actually "does" things. On the negative side, it was a long road, and I got plenty of pushback along the way. The light at the end is realizing there's a way to make this accessible and maintainable.
Yeah, so as another commenter pointed out, it's a pretty significant red flag.
I would actually recommend the Programming in Scala 3rd edition that was recently released
You need to watch Martin's keynote for ScalaDays Berlin. He outlines the reasons why Scala stagnated for 2.11/2.12. Basically it comes down to moving to Java8, since they have to maintain Java6/7(2.11) and Java8(2.12) in parallel they didn’t want to extend the language too much in 2.12. So 2.13/2.14 are going to be completely feature driven (From what I can tell anyway). All of the "planned" 2.13 features are already implemented in Dotty, they are just working on stabilization and performance now and the benefits are huge.
Thanks for the advice. I went for the iterative approach.
(You are actually talking about monoids, not monads.) The term "lose information" has a very concrete meaning in category theory. For any *other* monoid M, and any function `f: T =&gt; M`, there is a function `nat(f): List[T] =&gt; M.` This function `nat(f)` also preserves monoid operations - i.e., `nat(f)(append(x, y)) == append(nat(f)(x), nat(f)(y))`. Now suppose T itself were a monoid. Then you would discover that `nat(identity)(point(t)) == t`- you can recover T itself no matter what T is. In contrast, if T were something more restrictive - say integers - then you can't always get T back out for every possible T. I.e., suppose `T = List[Int]` and `point(t) = t.length`.
Thanks, I missed that in my quick initial scan.
By "benchmarking with an Android app" you mean compilation to JVM classes, or the additional step of compilation to Dalvik, too? Also, I guess an android app might need proguard, which may slow things down on its own. So, essentially, I mean there should be more careful benchmarks, if any. I agree that my own statement above becomes unproved, since I didn't do proper benchs. I thought it's just a general wisdom known to all, following all articles I've read about the 2 languages. But that may be outdated/wrong.
Just don't use the rendering engine. Running a JSON API? It's pretty simple. val json:JsValue = ??? // some domain payload return Ok(json)
I probably spoke too soon. I have only used Play in very large, megalithic applications and was not impressed at all. Maybe it's time for another spin. 
Upvote cuz title
https://github.com/SidneyXu/AndroidDemoIn4Languages: Language Spend (secs) Java ≈ 9 Scala ≈ 21 Kotlin ≈ 16 Note that in Scala's case this _includes_ ProGuarding. It also includes typed resource generation which has an impact on method count and compile time. Scala's sbt-android plugin is also the only one that supports cached and incremental ProGuarding, which isn't reflected in the benchmark (but pushes time from "making a change" to "new version of the app, deployed on your smartphone" to a second or two). I also had a look at a pure JVM codebase (no Android stuff): What matters most is how much work you let the compiler handle for you. If you write Scala as if it were Java, compile-times are fast, not Java-fast, but fast. But devs obviously see value in the features Scala provides over inferior languages like Java or Kotlin, and use them. This of course increases compile-times, but also makes it hard to compare 1000 lines of Scala with 1000 lines of e. g. Kotlin. Are 1000 lines of Scala compiling slowly, if the concepts expressed in those 1000 lines can't even be expressed in Java/Kotlin?
&gt; https://github.com/SidneyXu/AndroidDemoIn4Languages Thanks, that's a interesting stuff! &gt; if the concepts expressed in those 1000 lines can't even be expressed in Java/Kotlin? This is obviously off-topic, the only thing I mentioned is compile times. (After all, I love most aspects of Scala and I use it for a long time already.)
This is terrific, thank you lihaoyi!
&gt; (hi paultypes) Hi! (Said in my best [Joker-to-Harvey-Dent](https://www.youtube.com/watch?v=O4Ytmpb-ReQ) tone...) &gt; One weird tip Runar doesn't want you to know? That's between me, Rúnar, and several cocktails at [The Fountainhead](http://www.yelp.com/biz/the-fountainhead-bar-san-jose). 
I had the same thought about the implicit def, but buried in the README is this little warning: &gt; Some people will think about a implicit conversion to avoid having to write freek[PRG] but believe my own experience, inference in for-comprehension isn't so logical in Scala and as soon as you manipulate more complex programs, implicit conversion makes inference break with hardly understandable errors. 
My experience doing this for the past year with monad transformers is not so negative. Yes there are some times where the compiler can't track severe type gymnastics, but on average I find I have to help things along maybe one in every 50 or so loc. Well worth the implicit conversion IMO. As it relates to this exact use case, notice the import here https://github.com/mblink/composefree/blob/master/example/src/main/scala/Example.scala#L11 What that means is that for an unmodified dsl operation, you are pretty reliably going to get the correct type through implicit conversion. Where you are doing some transformations, say something like Some(freeop).sequenceU, you have the "op" syntax to fall back on, which I still find visually improved over the freek approach since the syntax enhancement here already imposes a target type, and your lifting call does not require a type param. In the end these are largely aesthetic motivations, but I've personally found those details make a world of difference introducing this kind of architecture to folks who don't already have a strong background or interest in FP.
No. They're the wrong kind. It is true that any *generic* free monoid would necessarily be isomorphic to `List`, which happens to form a monad. But beware of assuming that other monads behave like `List` - IMO this is the biggest source of confusion for monad beginners. Look at `Writer` or perhaps `Either` for a more typical example of monad behaviour.
For what it's worth if you compile with `-Ywarn-value-discard` you will get a warning in cases like this. In combination with `-Xfatal-warnings` this becomes a compile error. It would be really nice to be able to turn off value discarding altogether. It's an awful "feature".
Hi! This is very interesting, I was actually discussing about this lately and could not recall if someone did it... thanks for mentioning it :) Honestly I would be surprised it perform better than other approachs (my gut feeling is that it would be due to the different evaluation strategy in scala vs haskell), but I'll definitely have to work that out and include this one in the benchmarks in order to find the best trade-off for `Free`/`FreeT`. Thanks!
Whatever Haskell does should be emulate-able. (Whether I've noticed to add laziness/caching in all the places that need it is a separate question). My impression from casual conversations is that the constant factors for the reflection without remorse approach are huge, so you have to be dealing with truly enormous chains for the asymptotic advantages to be worthwhile. Best of luck with it anyway.
&gt; but instead is an "Engineering discipline", as if engineering is not also a form of applied mathematics. And this is the critical point which you missed when people say that Computer Science is closer to engineering versus mathematics, its precisely the difference of "applied mathematics" versus just "pure mathematics". Engineering (as well as other disciplines) obviously rely on mathematics, but a lot of concessions are made that come a result of having to deal with reality (either directly or indirectly). This obviously comes up in programming as well. Its nothing wrong with the education system, and blaming the "education system" isn't getting anywhere
Yeah. I do think this is a real downside to the scalaz approach - representing actions as values is great, but `&lt;function0&gt;` rather than a stack trace makes understanding running code a lot harder.
&gt; Are there any actual differences or it's the same idea expressed differently? Or is it just some common ground between them? No, they're very different ideas. Free with coproducts takes a list of effects and expresses computations with them all in the same monad. Monad Transformers takes two different monads and stacks their effects. &gt; Why do we need `FreeT? To stack the Free with other monads. &gt; Is Free's avoiding stack overflow the key difference? One key difference, if evaluating whether not to describe effects with free, or stack effects with monads is that the ordering of effects with monad transformers cannot be changed. Another key difference is that Free allows you to delay effects, no effects happen until you transform your Free into an effectful monad.
This isn't a difference between Scala and Haskell. The monad transformer approach exists in Scala (e.g. ScalaZ contains `MonadReader` and `MonadState`) and the free coproduct approach exists in Haskell. With the transformer approach you tend to apply your interpreters in order, whereas with the Free approach effects can be interleaved and will happen in the order they were expressed. E.g. if you have a `Writer` and an `Either` effect and you write a program like: for { _ &lt;- log(Vector(1)) _ &lt;- fail("oh noes") _ &lt;- log(Vector(2)) } yield {} then arguably the "right" result is for the log to contain `1` but not `2`, and this will (usually) happen with the Free approach, whereas with a monad transformer approach you'll either end up with both log entries if you make the log effect outermost, or no log at all if you make the failure effect outermost. I would expect the main reason to have `FreeT` is interoperability with the transformer approach. Avoiding stack overflow is an implementation detail, not important.
If you're seeing `&lt;function0&gt;` everywhere, you should see if you could use https://github.com/lihaoyi/sourcecode and give some of those functions automatic macro-generated names. That was the original purpose of the library (to give names to my own `&lt;function1&gt;`s...) and it makes things far easier to debug when your lambdas are named by their full path of enclosing names, or by the file/line-number at which they're defined.
Describes some design problems that I found in quite a few different Scala implementations of priority queues on GitHub. The major symptom of this design problem is that it is possible to somehow mix two different incompatible orderings within the same priority queue, leading to nonsense results. I show one way to avoid these errors by using a somewhat fancier API.
What benefit does the new HTTP client [Gigahorse](http://eed3si9n.com/gigahorse/) provide over an Akka HTTP client? I see the blog post [here](http://eed3si9n.com/gigahorse-010) but the only thing I glean from it is that it only wants AHC as a dependency or something. In general, is that kind of dependency prevention worth an entirely new library? Is there another reason I'm missing (e.g. use of AHC/Netty as a more stable counterpart)?
If you need java interop, use a java enum. They're really rare to use in Scala. 
uh, you don't need enums in scala.js
I have a few question for people using scala.js for production apps: * what libraries do you use? What libraries from the JS ecosystem do you make use of? * what do you use for css? What do you use to build it? * does error reporting with sourcemaps work properly? (E.g raygun) * any particular pain points that might be deal breakers? Thanks!
I had expected there to be a version using path-dependent types for heaps, but after pretty extensive looking, I couldn't find one. That was the impetus for writing this in the first place. (I did find ~8 implementations that either didn't support merge or did suffer from problems I described or both.) And you're right; applicative functors would be perfect here, if Scala supported them.
But according to its signature it returns a new heap. Why does it matter that it's different from the others?
I don't really understand how build files work/are organized/what they need/where they belong. So far I haven't needed to make one from scratch, but if I did, do you have any recommendations on how to make one? 
Not right now, it seems. Ideally I'd want @main def main(args: String*) = println(args) to work, but it seems to fail at the moment Main.sc:8: could not find implicit value for parameter e: scopt.Read[String*] def apply() = ammonite.main.Router.generateRoutes[$routesOuter.type]($routesOuter) ^ Could you open an issue? Then it'll be tracked and someone can submit a fix to make it work :)
I remember that prototype pretty well, as it's how I got to finally understand `Free` :-) Something similar could definitely be implemented in the upcoming Scalaz RTS, probably as an opt-in to not impact performance though.
The answer is simple, don't use a java enum 
 ├── build.sbt │ scalaVersion := "2.11.8" ├── project │ └── build.properties │ sbt.version=0.13.12 └── src └── main └── scala └── org.example └── Main.scala package org.example object Main { def main(args: Array[String]): Unit = { println("Hello, World!") } }
Nice work!
Yes. Note that `List` is not itself a monoid (at least in the category of Scala types) because it is not a type; rather for any type `A`, `List[A]` forms a (free) monoid. 
Just because it's built into the runtime in Haskell shouldn't necessarily make it any more efficient than what we can do by hand, surely?
Hi ! Thanks for the feedback. 1 - Since I didn't enjoy using Argonaut at all to parse JSON, I'll totally look into spray-json. 2 - From my understanding, L12 unwrap the Option or returns null only if the request to the API returned nothing (which is what I intended) The idea behind that was to provide a set of functions that can be used on the JVM without any knowledge of Scala-specific idioms (Options, scala.collection, etc...).
Sometimes you don't have a choice. If all the existing code expects that it can put your "enum" into an EnumMap or an EnumSet you better supply a real enum.
Let me check again ... it _has_ to be out there somewhere.
Ok, I found it: https://www.youtube.com/watch?v=dEC2LHSA5LQ It's called EquaSets, from the Scalatic library. Usage: https://gist.github.com/bvenners/d07ca7bf2cf8021aab63 Did you find this one and it suffered the issues you mentioned, or is this an implementation you didn't find? While looking for it, I just saw that you commented in scalaz: https://github.com/scalaz/scalaz/issues/1236 As a warning, I think it's a waste of time arguing with them. For these people everything that Haskell does is perfect, because that's how Haskell does it. Here is the old discussion from two years ago: https://github.com/scalaz/scalaz/issues/671 It's one of the reasons people are sick and tired of the scalaz folks. Cats is probably more about the scalaz people that are _not_ involved than about the code of scalaz. 
Lots of extraneous braces - in Scala if you have a block containing a single statement it should probably just be that statement. In a "real" project you would probably want to use a library to parse command line arguments rather than doing it by hand. What you have is fine though. You might like to consider string interpolation for the messages you print, though IMO what you have with `+` is fine. `Source.fromFile` leaks file handles, though that's not an issue in an app like this that runs once and then exits. It's unfortunate that we have issues like this in the standard library. Don't pass `Iterator`s around - they're stateful which makes them error-prone, and they don't show properly when debugging. Better to pass the `Iterable` and then do the iteration inside the method. If you really need a stream then use something like fs2, but that's overkill for a project like this. `createWordMap` can be written very elegantly (as a one-liner) using Cats or ScalaZ 's `foldMap`, without any mutability. If you don't want to use them I would still prefer to use an immutable map built up via `foldLeft` rather than a mutable map and a `foreach`. (Theoretically the mutable map is maybe more efficient, but it's not worth worrying about unless it becomes an actual performance bottleneck - clarity and correctness first, performance second). It's probably overkill on a project this small, but if you want to get in the habit of good idiomatic scala practice, it might be worth creating a type to distinguish sorted words from unsorted words (either a `case class`, optionally using `AnyVal`, or a tagged type). Ideally make it so that the only way to create a sorted word is by sorting (e.g. make the constructor `private`, and have a factory method in the companion object). That way it would be impossible to ever accidentally use an unsorted `String` as a key in the word map. `possible =&gt; {toWords.get(possible)}` is just `toWords.get`. This is one of those things you learn to spot once you've been doing functional style for a bit. Don't use `l.::` - `::` is designed to be used as an infix operator associating to the right, so calling `.::` is very confusing. I think your `sortWith` call could just be `sortBy(_.length)`? The chain of `map`/`flatMap` calls at the end can probably be written more clearly as a `for`/`yield`. You don't necessarily need to pull out all these `val`s when you're just going to use them immediately (unless you find them helpful). E.g. rather than `val results = ...; results foreach println` I would probably just do `... foreach println`. That's probably more subjective though. Welcome to Scala and all that - hope you like the language, feel free to ask if anything was unclear.
Excellent, I'll take a look. I had not seen EquaSets, probably because I was specifically looking for priority queues/heaps.
&gt; I don't get this the-sky-is-falling meme. There has never been more activity in the Scala ecosystem. Really? It feels like everything has slowed down to me. Certainly the language itself feels frustratingly *almost* right - a couple more releases comparable to 2.11 would have done it - and then suddenly the core language has frozen for who knows how many years to work on features I don't care about.
&gt; The major symptom of this design problem is that it is possible to somehow mix two different incompatible orderings within the same priority queue, leading to nonsense results. In your fixed `Heap`, is it immune to E. Kmett's [comment](https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu)? &gt; Since you can pass any dictionary anywhere to any implicit you can't rely on the canonicity of anything. If you make a Map or Set using an ordering, you can't be sure you'll get the same ordering back when you come to do a lookup later. This means you can't safely do hedge unions/merges in their containers. It also means that much of scalaz is lying to itself and hoping you'll pass back the same dictionary every time. Note - I'm not sure if Kmett's comment is applicable to your `Heap` code, but I'm asking to learn.
Good work! I do something like this for Apache Kafka using [scalatest-embedded-kafka](https://github.com/manub/scalatest-embedded-kafka) without containers. Integration testing has saved my bacon!
I don't see how this is a problem. Especially in a code base that is used across jvm and js, these sort of hacky design choices you describe don't apply. It sounds like you are contriving an issue. 
&gt;But with 32 bits, we can easily store 100 million objects with 0.1% accuracy in membership requests. That should work for most use cases. True, 32bits covers most cases but not in big data where 1 billion of elements is pretty common. Actually "100 million objects with 0.1% accuracy" is near its limits, you will get a lot of collisions for numbers greater than that. &gt; But if it's not enough, we can chain multiple BitSets (and ofcourse use a 64 bit hashing algorithm) to get past the 32-bit array limitation. Yes, it's an option. But a slower one. The "chaining" of BitSets (or arrays) is slower and degree of it depends on implementation. In case of array of BitSets it's additional `mod` (which is slow) , branching, more reads to get the pointer to the BitSet. It's slow enough. The fastest option is to have several fields inside the "CombineBitSet" class and a field with count of bits inside each of them, then branch based on that knowledge across the BitSet fields. This option is just ~10% slower. I don't have benchmarks unfortunately.
Indeed. This library could be useful if I need to test using external software not for the JVM. Apache Spark does something similar using Spotify's Docker client. They have lots of boilerplate code per container.
Thanks for all your comments. I have been trying to use Scala more over past few months (we have a few projects at my work that I think would benefit from its use). &gt; In a "real" project you would probably want to use a library to parse command line arguments rather than doing it by hand. What you have is fine though. Coming from a Java background I am familiar with [JCommander](http://jcommander.org/#Scala) for command line parsing, would you recommend anything else? &gt; `Source.fromFile` leaks file handles, though that's not an issue in an app like this that runs once and then exits. It's unfortunate that we have issues like this in the standard library. I recall worrying about this. Is there not a standard way of handling this similar to Java's try-with-resources? &gt; Don't pass `Iterator`s around - they're stateful which makes them error-prone, and they don't show properly when debugging. Better to pass the `Iterable` and then do the iteration inside the method. If you really need a stream then use something like fs2, but that's overkill for a project like this. By this I assume you mean collecting the results into some kind intermediate iterable like a `Seq` and passing that around? &gt; `createWordMap` can be written very elegantly (as a one-liner) using Cats or ScalaZ 's `foldMap`, without any mutability. If you don't want to use them I would still prefer to use an immutable map built up via `foldLeft` rather than a mutable map and a `foreach`. (Theoretically the mutable map is maybe more efficient, but it's not worth worrying about unless it becomes an actual performance bottleneck - clarity and correctness first, performance second). I am not familiar with Cats, ScalaZ or even the built in `foldLeft` so it looks like I have some reading to do. I would have much preferred it to be built up using Immutable maps so thanks for that. &gt; The chain of `map`/`flatMap` calls at the end can probably be written more clearly as a `for`/`yield`. I am completely unfamiliar with `for`/`yield` so will give that a look into. Thanks again.
Fixed, thank you.
I don't think your disparaging of scalaz is necessary. Check out the reply to the new issue: &gt; That's true, but we deliberately make the simplifying assumption that you will define typeclass instances in a globally coherent way, and use Tag or other newtype mechanisms if you wish to introduce incompatible instances, not the introduction of different implicits into scope. That's perfectly reasonable and fair. The old discussion was a bit abrasive though (largely due to Tony Morris...)
Totally agree with your point. Comparing testcontainers and docker-it-scala, testcontainers provides you with some preconfigured scenarios out of the box e.g. integration with selenium remote drivers connected to selenium server inside container. But anyway, docker-it-scala is a great lib and can be used in the same way as I described in my post.
I think you hugely overestimate the importance of those things for the rest of the community (I agree that they might feel very important to you) and underestimate the effort required. It doesn't follow from "other people are not working on the things I like" that progress and improvements in Scala have slowed down. I can't talk about Lightbend, but I think the Scala compiler team is not driven by those goals you seem to ascribe to Lightbend's business. Even if no Scala dev cared about using Java 8, the simple fact is that without Java 8 support we would not be able to compile Scala anymore! Do you want to deploy some outdated and unsupported JVMs in your dev team, because scalac crashes on Java 8?
Use docker and sbt-native-packager
Whats a good way to model this without typechecking errors? I may be modelling it wrong, but hopefully my intent is clear from the code snippet - i want to model a set of commands that my system can respond to and then generate random sequences of commands to test it using the types. sealed trait Cmd[+K,+V] case class Get[K](k:K) extends Cmd[K,_] case class Set[K,V](k:K,v:V) extends Cmd[K,V] case class Delete[K](k: K) extends Cmd[K,_] val cmds: List[Cmd[String,ByteBuffer]] = List[Cmd[String,ByteBuffer]](Get("a"),Set("a",ByteBuffer.allocate(10)),Delete("a")) ^ Last line errors, it expects a `Cmd[String,ByteBuffer]` type for `Get("a")` but gets a `Cmd[String]` type. case Get(a:A) =&gt; val setKeys = cmds.filter(x =&gt; x.isInstanceOf[Set[A,B]]).map(_.asInstanceOf[Set[A,B]].k) val getKey = setKeys(Random.nextInt(setKeys.length)) cmds.take(cmds.length-1) :+ Get(getKey) ^ Last line errors I can make it work by including `K` and `V` in Get and Delete but that seems kind of wrong..
Anything typeclass-based requires an extra layer of ceremony in Scala, but it's necessary for writing libraries that will work with types from other libraries that the user can't customize. I don't know about Slick but I've seen (and written) shapeless-style code that would be an order of magnitude shorter in Idris or similar, sure. Though that kind of code tends to be doing things that would be impossible to do with type safety in any (non-Scala) mainstream language. Compare [what hibernate does](https://docs.jboss.org/hibernate/orm/4.2/manual/en-US/html/ch06.html#types-registry): a (sessionfactory-)global mutable registry of string -&gt; handler mappings that are invoked via reflection, with all the performance and safety implications that has. This isn't a dig at hibernate - rather that's the only way to offer this kind of important functionality in Java (Jackson does something very similar). In a language like Python you end up using a similar approach, but everything is already reflection already.
exitOuputStream would be ambigous with [OutputStream](https://docs.oracle.com/javase/7/docs/api/java/io/OutputStream.html)
it sounds to me like "typed actors are so good and easy to use, so people forget there are actors underneath, and then they get problems". i used typed actors, didn't abstract over their nature, and that was the best akka experience i had.
Is the roadmap previously published on the Scala website wrong? I mean obviously it is in that Scala 2.12 is well behind schedule, but is the part about no incompatible changes (which effectively means no language changes) in 2.12 or the version after that no longer true? This was the trigger for the typelevel fork, so it's not like I'm making some crazy misreading of the document. No remotely serious language should expect its users to trawl through commit history, conference videos or twitter to find out what's going on. I notice the roadmap is no longer there (at least at its original URL), and the SI-2712 fix would appear to go against that roadmap (and is exactly the kind of thing I was worried would not happen), but there's not been any new roadmap or announced change in policy. &gt;Yes, because it was back-ported from the work happening on providing full Java 8 support for 2.12. Without this efforts, you wouldn't be able to compile Scala today on Java 8. Well, whatever branch it started in, it was released over 2 years ago. There haven't been any user-facing improvements to Java 8 support since then (except perhaps to the using-Scala-from-Java case). SAM support was there in 2.11. Performance is good, a cleanly architected compiler is good, no-one can object to those things. But taking a step back, a project that has to stop feature development for several years to allow for refactoring has serious issues. Scala-the-language is nice, but not yet nice enough to withstand waiting 4+ years (2.11 to whenever the next release in which incompatible changes are permitted comes out) with 100% frozen compatibility. 
It doesn't fail to interop. You can use jvm-only features as much as you like as long as you're on the jvm. This is a failure of your design
Well, I disagree, and I think you haven't understood the issue yet.
There isn't an issue. Create an enum in Java for jvm. Create an adt with the same constant values for Scala.js only. There is no problem. There is a problem in your design. 
Good idea, I will name it `exitOutputToStream` to be consistent with `toList`, `toSet`, etc
&gt; Like, I remember the fork, and I can still find e.g. http://typelevel.org/blog/2014/09/02/typelevel-scala.html. Yes, they announced it. Just like Typesafe made plans about 2.12. Or Rod Johnson about Scala in 2020. Sometimes things happen differently from the way stuff was predicted. No Orwell going on. &gt; Typelevel really did state an intention to keep the language 100% compatible until 2017. No? &gt;&gt; it will be 2017 at the earliest before any of this sees the light of day This just refers to the fact that by Typesafe's original plans improvements desired by Typelevel would ship by or after 2017 (and their unhappiness about it), not that Typelevel wanted to keep it that way. Many improvements have been merged, and it's obviously not 2017. &gt; they should announce it as such, not try to pretend it's always been this way I'm not sure anyone is pretending anything. I believe people are just busy with working on code, not communication.
&gt; They didn't just announce it. It happened. I remember using it. I remember using features that still aren't present in official scalac. I also remember an early 2.12 branch that had no new features! The world has moved on since then. I don't get what's your point in playing these semantic games. &gt; Typelevel had patches ready to merge that Typesafe was refusing to merge until the version after the version after 2.12 for compatibility reasons. No, they had pointless patches mainline didn't want in general, like identifiers ending with `'` or Short literals, or things that were implemented incorrectly like the changes to irrefutable patterns. &gt; To then have a change of position and not follow up on it is amateurish at best. You must be the last person in the community to realize that Typesafe completely sucks at communicating things. :-)
&gt; Nobody has explained yet why a concept – that has been shown to not work for almost a decade in Haskell – is suddenly the best thing ever when copied verbatim into Scala. Because for a lot of Scala code it's a fairly reasonable technique. Actually, it's very reasonable even in Haskell. It's only been shown 'not to work' in the sense that it's annoying and infeasible in some corner cases (well, and monoids).
Thanks. And your `merge` examples demonstrate this same issue when trying to merge two `Heap`'s w/ different orderings. Not until reading this post did I understand Kmett's comment.
Ditto, and then people (big team) started making assumptions because 'its just an interface' and it all went downhill from there....
Also checkout https://github.com/lihaoyi/sourcecode
Is it worth doing something like this? trait InsertUserDb extends ((String, String, String) =&gt; ConnectionIO[UserUUID]) { def apply(email: String, firstName: String ... ) = {...} } so then I can have dependency only on that one single method instead of whole DAO where most methods won't be used by caller...? i.e. def insertUser(insert: InsertUserDb) = ... insert(a, b, c) instead of def insertUser(da: UserDAO) = ... da.insert(a,b,c) then in tests people have to "mock" all other methods of UserDAO where as it is not even relevant (or look into implementation to see it's not relevant)... I would like everything to take functions, and then store implementation in just objects, link all together in main.scala... Is it weird? How do you solve interface segregation? (Meaning also, if UserDAO has 10 consumers, and one of them needs another User-dao related method that is not there, we add it, but then all other 9 consumers get it while they don't need it! - it seems incredibly broken to me! ) It looks and feels weird though. I feel like what I would want is only functions passed around... but passing named one feels cleaner than having methods with arguments such as `(String, String, String) =&gt; ConnectionIO`
You shouldn't need to mock anything if you're using `ConnectionIO` - the whole point of that style is that a `ConnectionIO[A]` is just an ordinary pure value that doesn't do anything. (You might need a test interpreter I guess, but that would necessarily have to interpret the whole of `ConnectionIO`). You can split out an interface into multiple traits sure (though in that case I wouldn't bother extending `=&gt;` - rather I'd make something like `trait InsertUserDb {def insertUser(...) = ...}` so that you can have a real `UserDao` that implements all of the traits) . To my mind it's a question of which distinctions are important enough to express. I might split my dao interface into read-only and read-write components (although realistically I'd do that by having two variants of `ConnectionIO`), because it's valuable to be able to see that a given service only accesses the database in a read-only way. But I probably wouldn't bother distinguishing between insert/delete/update, because does it matter that this service does deletes but not updates? In terms of going the whole way to statelessness... eh. If you go full functional then you say that all classes are either pure (stateless) behavior or data. Provided the implementation doesn't contain any state, a trait is just syntax sugar for a bundle of functions (you could use a case class full of functions if you really wanted to enforce this). I think it's valid to group a bunch of functions together for convenience. OTOH if everything really is pure then there's no reason to ever mock it out, at which point it's valid to just put the functions in `object`s and call them directly, and the pure part of your project doesn't need any wiring up. (The actual interpreter that runs `ConnectionIO`s will have state - socket handlers and the like - as will the part that runs web routes or whatever your program exposes - so those parts are probably best handled with traditional classes. But you could have all the business logic for a webapp ending up as one big `Request =&gt; ConnectionIO[Response]` (defined in an object that delegates to smaller objects) and then your main only contains code that runs an interpreter for that)
Thank you. My biggest problems are, how to design those "bundles of functions", that they fit together for everything, including cross cutting concerns. I always hit wall at this stage. First try: Bundle them, related to data they work with, i.e. trait JobDao { def insert: Task[UUID] def find: Task[Option[Job]] def delete: Task[Unit] def update(t: Job): Task[Unit] } All is nice and feels right, lets say on my "job" rest endpoint. But now suddenly (please excuse my artifical example), we have a Report endpoint, that also needs to use `find` method. Now - I don't want to pass whole `JobDao` there because that's like getting whole jungle just when you need banana. Another try: Bundle them, related to use cases? trait ReportEndpoint { def findJob: Task[Job] def generateReport(job: Job): Task[Report] def sendEmailWithReport(r: Report): Task[Email] } But now, when some other "use case" may need to find job, it will need to again implement same thing as `findJob` does. I am completely lost as to how am I supposed to know how to bundle them together so that it doesn't become huge pain at one point or another. 
I do feel your pain, although I'm not sure Id formulate it "hard to reason about". I'd probably say it's easy to use, but it's not simple library ( check out this talk: https://www.youtube.com/watch?v=rI8tNMsozo0), and does not help comprehension of codebase. But typed checked queries are nice. For those reason I now prefer https://github.com/tpolecat/doobie.
When using `for yield` with Scala Futures we can break the chain of execution by adding `if` in between. I know that `if` is translated into `withFilter`. Often I need to handle the error and it can be done by using `recover` or `recoverWith`. However, if there were several `if` statements inside `for yield`, I do not know which of them triggered the error. What is the best way to differentiate different `if` when handling them?
Cross-cutting concerns are what monads are for. The reason we use `ConnectionIO` is because access to the database is a cross-cutting concern. If you want to be fully decoupled then you write your business logic in `F[_]`-generic classes that just compose monadic functions without knowing the specific monad and then these classes can be truly ignorant of those cross-cutting concerns. Personally I don't usually go that far, but I'll often define a type (something like `Operation[A]`) that's my "standard command stack" for this application (and actually it's something like `EitherT[({type L[B] = WriterT[ConnectionIO, AuditLog, B]})#L, ClientError, A]`), and then my business-logic classes are just written in terms of `Operation`. It may help to think of objects (or rather traits) as capabilities. You didn't get the jungle, you got a capability to harvest bananas, nuts and berries. Maybe it's important to distinguish between functions that have the capability to harvest berries and functions that have the capability to harvest bananas, or maybe it isn't. I actually think this goes to the heart of what programming is - deciding which things are similar enough to handle uniformly (or where there's enough similarity to factor out), and which things are different enough that they need to be handled distinctly. A well structured program should reflect the structure of the business problem you're solving, because that structure has to be expressed somehow - the ideal program is one that exactly expresses the business problem and no more. How that translates into actual practice I'm less sure. Again though, if everything is stateless pure functions then it really doesn't matter what has access to what - a pure function never gives you any capability you didn't already have (and should never need to be mocked), because you could always have just done whatever it is the function does, and calling the function doesn't affect anything else. Everyone has the capability to create database access commands - it's only the capability to execute them that you need to be worried about. The command creation might as well be a global-static function on an `object` somewhere.
I'd record more information in a full-sized `Future`: for { ... _ &lt;- if(...) Future.successful({}) else Future.failed(new MyException(myFailureInformation)) ... } yield ... (In practice I'd probably put the success-or-failure inside the future (using `Either` or similar) rather than just using failure of the future, because that way you can have a lot more type safety around your possible failures, but that means using a monad transformer and I can understand not wanting the overhead of that)
I suppose that's equivalent to `Future.failed`. In general I try to avoid having `throw` except for unexpected system-ey failures where the only thing you can do is retry at high level (in which case I tend to `throw new RuntimeException()`) - any specific failure that I can handle locally or specially is better expressed as a value, via `Either` or similar - but yeah, if you don't want to go down this route I guess that approach is as good as anything.
What's the reason you are trying to do this? You are signing up for a world of pain.
Oh, I see. I did not think about using `if` as a regular if, and not the one converted to `withFilter`. I like this approach. Thanks!
I am on linux but if you use the very latest unstable build (which I am) as of a few days ago you're able to switch out docker-java for the Spotify docker client which supports mac (plus the new native docker). As to how well that works I'm not sure as I don't have a mac.
Yeah don't bother with maven, dead tool, just use SBT.
I wouldn't use activator - probably better to start from nothing, and write some Scala that you understand. (IMO a framework that requires you to generate a bunch of files is a bad framework - I prefer Wicket for web pages and Spray for REST APIs). If you really want to go this route then post the full code that activator has generated. Where's that "Get /count controllers.CountController.count" coming from?
"Get /count controllers.CountController.count" comes from the routes file
Right, so if you want us to be able to help solve your problem, you need to *post the routes file*. And the `.scala` file where `CountController` comes from. And any other files that affect the actual error. (Probably easiest to put the whole project on github and link to that)
Ok...it's a simple project generated by Activator, I don't understand why I have to modify things in order to run the app. I will try to do it with Spray. Does it work with Maven easily?
Not sure if you are aware but Spray is basically being deprecated and being absorbed into the Akka project as the Akka-HTTP module. Most of the api is very similar but not identical, so if I were you I would look into that. 
If you really need Maven, the simplest thing to do is to rely on sbt under the hood, and to bind each maven step to a sbt step. So if you run `mvn clean compile` it will run `sbt clean compile`behind. To make this work you will still need to use build.sbt to define your build options (for example dependencies) but you will be able to plug some maven plugins (for example custom packaging, maven assemblies etc.). Example of binding : http://stackoverflow.com/questions/3491937/i-want-to-execute-shell-commands-from-mavens-pom-xml
He should use SBT. Maven is not dead, though.
https://www.playframework.com/documentation/2.5.x/PlayConsole directly run *compile* or *run* from the activator console started from your project. then you can invoke *dist* to create a production build. also you better off using sbt for dependencies but in any case activator will do the typical build tasks.
And this is also very awesome. https://www.gitbook.com/book/jaceklaskowski/spray-the-getting-started/details
Thanks seems like this is the perfect case for a free monad - identical to the sample in the cats tutorial.
What is for (concept?) / how would one use: cats.Capture Catchable vs MonadError fs2.Effect? 
What is `cats.Capture`? I can't see it in their scaladocs. `Catchable` is basically `MonadError` specialized to `Throwable`. `fail` is `raiseError`, and `attempt` is `map {\/-(_)} andThen handleError(-\/(_))` (pure syntax sugar, and makes just as much sense to have in `MonadError` as it does in `Catchable`, but it's easy enough to implement it yourself). `fs2.Effect` is for capturing side-effecting functions. Ideally you wouldn't ever need it, but in practice you often have a small piece that's not worth modeling explicitly where you just want to mutate some global state or some such, so you can use `delay` or `suspend` to capture that. (fs2 in general is about having reusable lazy streams). E.g. if you had a function that makes a web request to an external service you might define that using `suspend`, and then the request will only happen when the stream is run and will happen each time the stream is run ( which is necessary for referential transparency - see https://www.reddit.com/r/scala/comments/3zofjl/why_is_future_totally_unusable/ for the reasoning of why it's better to do it that way even though it can end up being less efficient than making the web request only once ). In a way it's a statement that `F` can act as a sin-bin type for effects that you can't or won't model explicitly (see the way Haskellers talk about `IO`). I'm not sure what `unsafeRunAsync` is doing in that typeclass though.
Hm, your example is good for `Free` and bad for transformers. Could you provide a contrary example where transformers fit better?
Not really, since I think `Free` is better than transformers. One advantage of transformers is that provided you're not doing anything too generic you can stack the same transformer twice (e.g. you can have `OptionT[OptionT[Future, ?], ?]` if you want to have two distinct ways a given computation can be `None`, whereas in `Free` whichever interpreter you ran first would eat all the `None`s), but that's easy enough to work around in the `Free` case with some variation on tagged types. There might be a performance argument to be had (I don't really know enough to comment on that side of things) but it's never been an issue in the systems I've worked on.
Well yes, it's the same as `Future.failed`, but is a bit more concise when you only need to break the flow in a `for{...}` (no need for `Future.unit` on alternative)
I vaguely remember Martin describing in his JVMLS talk on dotty, that parallelism though not yet implemented was an orthogonal concern to dotty's design, which seems like exactly how a design should work to me.
There's a couple more examples: https://beachape.com/blog/2015/07/25/slim-play-app/ and I'm putting together a REST API example: https://github.com/playframework/play-rest-api
It's simpler and probably about as fast to just compile entire files in parallel, one per CPU.
Do the scaladocs suffice? http://www.scala-lang.org/api/2.11.5/#scala.collection.immutable.Traversable http://www.scala-lang.org/api/2.11.5/#scala.collection.immutable.Iterable etc
Conceptually I see `Catchable` as just a convenience variant of `MonadError`. If you're using your `UUID` in an fs2 stream then yeah that's the "right" way to do it. If you create a `Task[UUID]` and then immediately call `.run` on it then obviously there's no point. It's only if you want to use `gen` in a lazy context that it's worthwhile, IMO, though I guess you could do it anyway and avoid the risk of e.g. accidentally filling a list with the same UUID. If you're using doobie (given your references to `ConnectionIO` elsewhere) then IIRC it has its own equivalent to `Effect`.
Unwatchable with that blinking line, but maybe I can just listen to the audio.
Thanks!
I am not sure how much simpler it would be: it seems you have to do pretty much the same job to find dependencies, except for the cases where the dependency is clearly to a member of the same type (or companion object), or by extending a class/trait in the same file (but this isn't too common). 
A few comments: `exitValue` is usually called `exitCode`. Some of those consequent commands could be called `andThen`, just like functions in this code: `val t = {x: Int ⇒ 2*x} andThen {y: Int ⇒ 3*y}` `lineStream` should probably NOT be renamed to `exitOutputToStream`. This is because the process does not have to exit in order to have an output Stream. Better call somehow like `outputStream`. Overall, using symbolic method names would be good, but the change need to be a very high quality since it would be hard to deprecate/delete stuff. _Each_ of those methods should be carefully thought about.
As far as collections go this might help: http://www.scala-lang.org/docu/files/collections-api/collections_2.html
Hey, yeah I use the same style, it's similar to the way you'd organize code in pure FP languages. I also use 'Internal' modules for functions that are used but not exposed by modules. This has advantage over 'private' functions as they are still accessible if you really need to, e.g. for testing.
Yes, I use tons of modules. It works well for me.
Wonder how C#/.net Core compares in speed to this? Be great if the author would add. Thanks.
 Scala's compiler is much more advanced - it needs to do a lot more stuff than java's compiler - it will save a lot of LoC at the long run. So, at the end of the day, your java project will be 5-10x as much code and the compilation speed of java will look worse.
 SBT compiles the changed files only. Same true for maven, except that you'll need mvn clean install most of the time for things to work.
it really depends on what you are doing and what you are using. macro based libraries will bring your compile time down really fast. 
It'd be interesting too see when the Scala code doesn't use "for" and there would be an implementation with [scala native](https://github.com/scala-native/scala-native)(this would be fair when comparing to C++).
&gt; Using vectors... Oh, undoubtedly :-) My [renderer](https://github.com/ttsiodras/renderer) is full of them, and I used them as stepping stones in past efforts to optimize code ([35x speedup](https://www.thanassis.space/straylight.html)) ... If speed matters, it feels really good to know you can jump into low-level constructs (and even lower than vectors - e.g. in the speedup above I ended up using SSE and CUDA as well). But there is a price to pay for using mutable constructs - usually the code is harder to maintain when mutation of state enters the picture. Unless speed is of the essence, I nowadays strive for simpler, stateless code - and only do "internal", function-scoped mutation in places that matters. This project was for fun, though - so you're absolutely right :-) *UPDATE*: Added mian2zi3 solution as a branch. 
Yes, sorry I can't do nothing about it. I've just tried myself in Android Firefox. In Android Chrome is much better though.
it is unreadable everywhere
Anecdotally, as of two years ago, yes, significantly. Worked on a few codebases simultaneously, some Java, some Scala, and for our setup, it was a significant pause waiting for tests to run. They might have done some good optimisation work in the last couple of years, though.
You mean they'll bring the compile time _up_.
~ compile is the answer
Yes. It's worth it.
If you download IntelliJ IDEA, you can literally write your code in Java and paste it as Scala. There's no difference there if you want to write it like Java.
Read the Scala books first.
I really didnt like the coursera course on scala. I found the book Scala for the Impatient to be a good starting point. Once you feel like you have enough of a handle on the language syntax and collections libraries, I really recommend Functional Programming in Scala (the red book). That book was the most helpful for me in terms of understanding how to use scala as a functional language and not just a less ceremonious java.
You shouldn't get too upset about the fact that switching to a functional approach is hard, the more if you have always and only been working with oo/Java experience. You only need time and a less demanding introduction to the new way of doing things. As other have said, choose a good introductory book and give yourself some time to let it sink in. I suggest Neal Ford's Functional Thinking as a nice, soft read to get into the basics of functional approach for newcomers. Then a solid book on scala. The red book, Functional programming in scala, will take a good time to sink in and it's not a good starter in my opinion, but in the long run it's one of the best material when you're ready for advanced concepts on writing professionally with fp
Scala has a pretty steep learning curve in general imo. Took me a long time to understand some of the concepts. I would stick with it. I found this to be very useful: http://shop.oreilly.com/product/0636920033073.do
Yes its atleast double the compile time of java in my experience for most mid sized projects. However this is really not that long and after that first compile you will only be compiling the changes you are making to the code and will hardly notice it.
Or better yet, the third edition which came out recently
While purists may not agree, I think it's OK to start out with "Java-ish" code and progressively move to more idiomatic Scala as you learn new concepts.
eh yeah. that's what i meant ;)
ScalaForFunAndProfit is a parody site. Probably the "hardest" part of writing Scala as an OO style is using traits effectively, considering they can also have implementation. Probably the best thing you can gain from using Scala (while still programming as if it's Java) is not needing to write any more getters and setters, and using Scala property methods. http://dustinmartin.net/getters-and-setters-in-scala/ as well as slight usage of operator overloading where it makes sense. E.G. you might have a class that represents formatted texts with styles, you could use the + operator to join them like strings. I'd still use immutability where possible / needed as well as using functions and lambdas . Just don't go as crazy with implicits and abstract syntax tree's or some of the more functional things. You can always use them where they make sense, after all, Scala is a hybrid / fusion language between functional and object.
Some of your statements are inaccurate. &gt; Because Unit is isomorphic with all other Scala AnyVal types Unit can't be isomorphic to any type which offers more than one value. &gt; any of them can be converted to Unit as required for return signatures to match. Further, any value type, e.g. type with concrete values, can be converted. Any type at all can be coerced to Unit. &gt; Because Unit is converted from any other value type, Unit can be converted to (). That is, Unit can be converted from a type to an instance by the complier, sort of. This can lead to confusion in code. I'm not sure what this part means, but you can't convert a type to an instance. That's a category error. It's like trying to build a house out of architecture books. It's true that if you know a value is of type Unit then you can definitely produce that value, because there is only one value of type Unit. &gt; def myProcedure(n:Int):Unit = {n * n; Unit} // pre-compiled &gt; def myProcedure(n:Int):Unit = {n * n; Unit; ()} // post-compiled `Unit` is a companion object. It has no relationship to `()`, the value of type Unit. Similarly, the object `scala.Option` isn't of type `scala.Option[_]`. You could have put a ham sandwich where `Unit` is in the "pre-compiled" version and the post-compiled version would look the same. Except with a ham sandwich.
Yes, but if you use IntelliJ with incremental compilation etc. the difference is tiny for most builds.
Can be more succinct without all the extra braces :P.
For the records, I am not practice Scala at work but I happen to follow the Scala courses in Coursera and mainly working in Java for the past 5, 6 years now. I would recommend to post your question on the discussion forum. People tend to be very helpful. I found that "Structure and Interpretation of Computer Programs" book do help a lot, a bit dry to read and I have not finish yet. A lot of the homework based on the book as well. So, it is kind of good to study the first course and reading the book at the same time. 
Oh, I didn't notice the bottom of of the Scala for fun and profit page
Hmm, but I don't want to make it verbose like Java
I was thinking of using Scala IDE for Eclipse
actually even: https://www.amazon.de/Programming-Scala-Martin-Odersky/dp/0981531687/ref=sr_1_1?ie=UTF8&amp;qid=1470657985&amp;sr=8-1&amp;keywords=programming+in+scala is more straightforward then coursera course. I actually never finished the coursera course,but I work on other scala projects in my free time (https://github.com/playframework/playframework). Actually I have all thee versions and the version three is really targeted as a step-by-step guide for beginners till intermediates (at the end). (I have all versions, the second version as ebook and the real one). Some coursera stuff is really academical, but some problems are really problems you need to understand at some point.
Speaking as a professional Scala Dev myself I want to reassure you that you absolutely are not inadequate or dumb, far from it! Firstly, it IS a hard course if you don't have any FP background despite the claims made on the website. Secondly, don't be dazzled by the apparent cleverness of Scala or FP. Its ultimately simple BUT it takes time to understand that simplicity. Simple != easy. It also a new paradigm for you and that means a whole new way of thinking. Whilst I like the Coursera courses I think they really underestimate this point. My advice would be to take your time. There's no shame at all in really trying to understand things properly. Take that thing that you don't understand, simplify it if you can, draw things out on paper and sooner or later it will click. I do the same, there's no point in rushing just to gain a shallow understanding. I'd rather grok something and master it even if it takes me a while. It will make learning more advanced things easier in the long run. There's no secret or mystery to this stuff. It just takes time but I promise you that its a million percent worth it. You can do it :)
I glossed this over in the initial post because I didn't want to write a novel, but I would say that I'm equally adept with c# as I am with java. I also have a little node code in production as well! Lambdas, I totally get that and use them heavily. One thing I've always struggled with is that I have no "formal" training in data structures or algorithms. I was an information systems major in college, which meant that I took a ton of business classes and just enough programming to be dangerous. I have regretted not going CS since graduating. Having said that, I've come up with designs to solutions using standard design patterns before I ever knew what they were! I recall early in my career implementing the factory pattern before I even knew what it was. I've since dug into the rest of the "gang of 4" and leverage those where appropriate. Any suggestions for a good functional-only language? I figured scala would be a good place to start due to my familiarity with the jvm.
Thanks much, I've added some of these resources to my queue. Going to take it slooow and not give up.
Yes, it is incredibly slow. One fond memory I have is my old https://github.com/lihaoyi/Metascala project. At just about 3000 lines of code, it took about **60 seconds** to compile on my old machine. **60 seconds** For 3000 lines of code! This wasn't a particularly advanced projects. Notably, it had no macros, barely used implicits at all, and no fancy type-system or monadic or reactive stuff. Just bog standard classes and case classes, with code that's mostly function/method calls and some pattern matching. Java++ really. And It was compiling at one line per 20ms on my laptop. For reference, 17ms is how long a typical video game these day takes to simulate *every blade of grass in a field blowing in the wind*. 65ms is how long it takes a brute-force depth-first Java sudoku solver to solve a 9x9 puzzle. I don't really think compiling a single line of Scala "Java++" code should really be more expensive than animating every blade of grass in a field blowing in the wind, nor do I think compiling 4 lines of Scala "Java++" code should cost more than solving a 9x9 sudoku puzzle. I mean, clearly it *does*; but I don't think it *should* Yes, if you write your code in Java instead of Scala, it probably will be 10x as many lines of code. No, it probably won't be as slow: *that 10x as much Java code will still probably compile faster than the original 1x of Scala code*. I mean, I'm still using Scala, so obviously I think it's worth it. Incremental compiling helps somewhat if you avoid circular dependencies (https://github.com/lihaoyi/acyclic helps a lot with that). Apparently the new experimental Dotty/Scala.NEXT compiler that is in the works is already 2x faster and expected to speed-up even further. But there's no hiding the fact that, as a base line, the current compiler is really, truly slow. 
Hey, just a word of encouragement: Leaving the comfort zone of Java is never easy. But you are far ahead of those who came from more functional languages in terms of effort and determination. Consider it like this: The inadequacy you feel is a way in which your brain tells you that it discovered new ground you didn't even know it was there before. Don't worry about when it makes "click". I think cheating is OK if you do it to get an understanding what the authors of the task actually intended you to do, especially when you have to unlearn so much Java stuff at the same time. There are many assignments where I failed miserably. Don't worry. You'll be fine. PS: If you like to read a book, there are various Scala books targeted at experienced devs like "Scala for the Impatient".
Yes, it absolutely is. I've worked with Java projects at least 2x-3x the size of the current Scala one, and they took less than half the time to compile. Note that the Java projects used gradle, and our Scala project does not make heavy use of implicits or macros. (There is one isolated library that does do both and it takes *two to three minutes* to compile a few *100* lines of code!)
That book keeps coming up. I think i'm going to start there. Related to cheating; As soon as I saw the answer, 8 times out of 10, it made sense immediately. I could have stared at it for hours and never came up with the solution on my own though. THAT was the most frustrating part. Thanks for the words of encouragement. 
Looking good, though you've still got the outer set in `solve_recursive_count`
What advantages does this have compared to using an interface/trait?
&gt; I glossed this over in the initial post because I didn't want to write a novel, but I would say that I'm equally adept with c# as I am with java. I also have a little node code in production as well! Lambdas, I totally get that and use them heavily. Do you use `map` and `reduce`? (they go by various names). I think the biggest help to me when learning Scala was my Python experience, but a very functional style of Python that I'd adopted partly based on my ML experience. &gt; Any suggestions for a good functional-only language? Scala is very ML influenced, and Standard ML is what worked for me. (Standard ML is the purist/academic-oriented variant, which might make it a better learning language than Scala/F#/OCaml). The lecturer who taught me wrote *ML for the Working Programmer*. For me it worked well to have a language where you simply don't have the option of doing it the wrong way, though there were definitely times where I hated the typechecker and just wanted to be able to do one little cast... definitely a "bash my head against it until I see" experience. So might have the same problems as you had with Scala, I don't know. It's hard to know which of the things I did were good ways to learn and which I was just lucky with. I would definitely recommend against SICP (which I see someone else recommending you), but again that's just my own experience. &gt; I figured scala would be a good place to start due to my familiarity with the jvm. Maybe. Certainly it's a good way to be productive quickly - use all the existing libraries you know, just write Java without semicolons (and maybe with fewer type declarations) and you already have a marginally more productive Java. If you're familiar with the runtime semantics of the JVM then hopefully a lot of the edge cases won't bother you. I'm thinking of things like `ClassTag`, which is a horrible thing to have to explain to a beginner who doesn't understand why we need it on the JVM.
Scala is a pretty compromised functional language. I guess that's not really fair. Let me rephrase: when the inevitable tensions between OO and FP have arisen in Scala's design, Scala has traded off in favor of OO vs. FP. Partly as a consequence, there aren't really good _introductory_ resources for learning FP in Scala. Don't get me wrong; [the Red Book](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653/ref=sr_1_1?ie=UTF8&amp;qid=1470667394&amp;sr=8-1&amp;keywords=functional+programming+in+scala) is a stunning accomplishment, and I'm proud that my review is the top-rated customer review. But what it is not is _introductory_. Personally, I suggest starting with something like [Real World OCaml](https://realworldocaml.org/). OCaml is a reasonably popular OO/FP hybrid that, thanks in part to not having Java compatibility as a requirement, I think achieves a better integration than Scala does, and certainly supports a more direct, consistent use of FP principles than Scala does. At the same time, it is not dogmatically religiously opposed to OO, so it won't make you jump through flaming hoops the way Haskell would. Having said that, maybe after working through Real World OCaml, I'd consider [Haskell Programming From First Principles](http://haskellbook.com/), which, although introductory, _will_ have you jumping through flaming hoops. But by then, you'll be prepared for them. Finally, the above-mentioned Red Book will take everything you've learned from Real World OCaml and the Haskell Book and situate them in the context of Scala's syntax and semantics, which are different from both OCaml and Haskell. Finally, as others have said: do _not_ believe you are stupid or incompetent just because learning an actual _different paradigm_ takes serious time and effort. It took serious time and effort for everyone else, too.
I'm in almost exactly the same boat as you. I've been reading 'Learning Scala' and I've had to read a couple chapters twice. It's pretty complicated for someone new to FP.
Hi! why can't I call the map function as follows: val it = Iterable(1, 2, 3); it.map[Int, Seq[Int]](_ + 1) ?
I think it's important to be driven by the concrete use cases - start with the problem that you want to solve, the commonality you want to factor out, and see how that naturally gives rise to these abstractions. I also think it's important to start with monads other than collections, because while most collections do form monads, most monads are not collections and it's very misleading to assume they will all behave like collections. http://m50d.github.io/2013/01/16/generic-contexts.html was my effort in this direction.
By default the `CanBuildFrom` machinery will only resolve the "right" target collection. You can either accept that the result will be `Iterable`: scala&gt; it.map(_ + 1) res3: Iterable[Int] = List(2, 3, 4) Or explicitly pass `breakOut`: scala&gt; import scala.collection.breakOut import scala.collection.breakOut scala&gt; it.map[Int, Seq[Int]](_ + 1)(breakOut) res2: Seq[Int] = Vector(2, 3, 4)
Hey folks! I'm reading in a text file of key-value pairs, and would like them to be loaded into the application as a Map. So far I've got that much working, and it works great, except that the values are intended to be a mix of types (String, Int and Bool to be exact). When reading in the file with the `io.Source.fromFile(filename).readLines` approach, it seems everything is read in as a String. The destination Map is already cast as Map[String,Any], so it should be able to accept the values as non-String types. 1) How can I convert the read-in values to the necessary type? 2) How can I test each value to determine which type it needs when read in from the file?
Thanks. Makes sense
I'd like to start learning macros, but I have a dilemma: The future of existing reflection-based macros is a dead end for future compiler versions, but that is where all of the documentation and example code is. Is it still helpful to learn from these examples and docs? Or should I try to get by with the sparse documentation of scala.meta? In particular, I have an interest in the following areas: Type Providers (I *love* F# Type Providers), Compile Time SQL Validation (Slick has started down this path), automatic serialization/deserialization, and SQL data marshalling functions (marshalling from a denormalized table/query structure into tree-ish or other structures represented by core and user defined data types). 
That is fantastic! I'll give this a try and report back if all goes well. Thank you!
Rather than attempt to learn a "pure functional" language why not find a good resource on formal logic? This will help you more than anything. Especially considering your information systems major probably barely touched it at all, whereas it is the direct foundation upon which comp sci is founded and the reason why there is a close relationship between mathematics, comp sci, and philosophy. They are all in the formal sciences.
&gt; Hmm, this is a bit of exaggeration. Not even close - just look at the architecture of a java ee/spring app and you'll be terrified. &gt; My experience is that, while Scala codebases are less verbose, you still end up with a noticeably slower compiler. It depends - if you don't use implicits everywhere, reject postfix ops, separate top level objects into different files(1 class or trait/file) and modularize well then you won't get bad compiler performance.
Interesting thought. You're right, i describe my degree as "remedial programming" and mostly a waste of money considering how useless I was as a developer after graduation. It took a long time to become useful. Was a dev for an embarrassingly long time before i even knew what "Big O" notation was. Nested iterations have a smell, but not because I was taught to see a bad pattern. I guess it's just instinct at this point. Shame! Got any good resources you care to share?
&gt; Not even close - just look at the architecture of a java ee/spring app and you'll be terrified. I've worked on plenty of Java EE apps, Spring apps and Scala apps. A Java app won't be 5-10 times larger unless you've been doing something spectacularly wrong. The only type of code that might approach that ratio is DTO-type POJOs, which are massively overblown compared to case classes, but that's only going to constitute a fraction of your codebase. &gt; you won't get bad compiler performance. You don't get javac compiler performance. I find it very noticeable. &gt; if you don't use... Scala is the only language I've used where you have to worry about the compile-time cost of using a library or language construct. I really would rather not have to care.
That also has incremental compilation.
Thanks for the criticism: Doesn't value discarding (converting to Unit) ostensibly creation the isomorphism between Unit and all value types where they can all be treated as the same thing with respect to return types? Per the quoted Scala spec, any `value type` can be converted to Unit. You can't return `scala.Nothing`, so I didn't say "anything" could be converted. The point about `Unit` type being converted to `()` (value), is that it can easily create a misunderstanding, which is the point of the article. A junior dev returns `Future[Unit]` but it's really converted to `()` and no Future is ever executed on the threadpool. So yes, `Unit` isn't `()`, but the conversion syntax can make it seem so. &gt; You could have put a ham sandwich where Unit is in the "pre-compiled" version and the post-compiled version would look the same. Except with a ham sandwich. Exactly. That was my point. I'll try to make it clearer in the article. 
Seconded on the recommendation regarding Functional Programming in Scala. Easy to read and great for an introduction into how to think Functionally. The book also comes along with great exercises to help you learn the concepts. 
Honestly, what you are lacking is probably more algorithms skills than FP knowledge. It's hard doing all the recursive stuff without having done them before. You should pickup scheme and go through SICP. You're probably a bit behind on the algorithmic knowledge. Try to avoid thinking in design patterns in Scala for the meanwhile, they're really a lot less useful once you have all the power afforded by FP, there are still common patterns used, but you'll notice that just because the language is structured with all these powerful abstractions you won't need nearly as many 'design' patterns anymore.
Nah, it should be fine. You can write performant Scala that's more-or-less indistinguishable from Java for the vast majority of use-cases. (In extremis, if you're looking at some pushing-the-envelope application, say HFT, arguably Java would be a better choice, simply because there's a more obvious mapping between code and its runtime cost -- and you won't benefit from using nice functional abstractions anyway, because you won't be able to afford them.)
I've only touched on the use of SBT in tutorials and classwork insomuch as being told "paste in these lines then open IntelliJ". So, I agree - this may be a great opportunity to redo my approach. Thanks again!
I'm a learn-by-doing type, so I had a good experience completing the Scala Katas: https://technologyconversations.com/2014/03/10/scala-tutorial-through-katas/
By fire, do something
I used Odersky's coursera on Scala as an intro to FP. If you want to learn Scala as a first programming language, Atomic Scala can be a good reference. I bought the second edition and it's pretty easy to get into. http://www.atomicscala.com/ Full disclosure: I use Scala at work, and it's my fourth programming lang.
IntelliJ has various inspections and refactorings that will tell you what you can shave off once it's done the Java conversion.
The big ones are the Lightbend stack and Twitter's Finagle stack.
1. Immutability = easy button for concurrency &amp; scalability. Functional programming has tools for dealing with the difficulties of working with immutable data. 2. Composition and referential transparency! Finally, a true abstraction that doesn't leak. 3. Thinking in terms of higher order operations such as map, reduce, flatMap, fold, etc. This makes code *much* more readable as other languages tend to let one simply munge up all of those operations into one or two loops. Much more difficult to reason about what's going on then. 4. Type-classes - adhoc-polymorphism (add methods to a type *after* its declaration). Only import methods/type-classes you need, hide the rest. Compile-time method call dispatch (you probably didn't need dynamic binding anyway). Describe behavior you need in your method by requiring implicit type-class without worrying about details. 5. Monads! What a massive game changer!! They can express repeating code patterns that are interleaved between lines of code (plus a lot of other things!). You simply "lift" your code into the Monad and the repeated pattern is run "behind the scenes" between your lines of code. 6. Formal type systems and category theory. Let your code lean on things constructed from a very solid well-researched foundation when you need it.
Actors are more like a toolbox. How you use them is completely dependent on the application. For example a web server might have an actor that receives all requests and then dispatches them to a pool of worker actors. Mailbox size is generally not an issue, but it can show up when scaling. Depends on what you by crash, actors don't recover from system crashes any better than any other code. What they can do is "supervise" other actors and restart them when appropriate.
Second that. SICP is going to be a great help --- in you case, I would recommend against Scala, Haskell or ML as a first language to try FP concepts in, the syntax would be another burden. Scheme syntax, on the other hand, is a breeze. 
Don't dive in monads first. The Functional programming in Scala book has a clear path to monads, and they appear only in the second half of the book.
"Scala for the Impatient" will give you an instant confidence shot, and you can start to write non-idiomatic Scala code after skimming through the first chapters. Gives a great motivation boost. It will not however help you with the FP parts, that require a paradigm shift and a few headaches. There are lots of assignments in the Functional programming in Scala book, which helps when you are stuck and have a look at the solution. If you continue (or restart) the Coursera material, have a look at the forums if you are stuck. The solutions are never given, but you can assemble piece by piece a few hints.
The reason why I asked about crashing is if a mailbox gets large you will loose messages. Maybe if there's a persistent mailbox it helps I'm just not really sure how would you know what he did with the last message. 
And to keep my word and to deliver my report - this worked wonderfully. I thank you for your help!
Clojure is not statically typed. Most scala folks like the typesafety. Fantom seems to be a dezent language but I'm not familiar with it. Scalas eco system is a huge one in comparison though, so that might be a valid reason.
I don't think insulting people does anyone any good. Note that these traits: - type safety - refactoring - modularity - richer toolbox / less time bikeshedding ... are not really related to FP ;-)
Macros are here to stay, but are for people knowing what they are doing. And you don't really have any documentation. What you do is to ask around for how to solve some problem and then somebody will give you a pointer to some project that already did it and then you end up copying and cargo-culting, praying that in the end it will work and that you won't crash the compiler. Macros are terrible, but considered a necessary evil by many library authors. But don't expect to have a good time while dealing with macros. The only redeeming quality is that at the very least, with enough effort, users can have a good experience. On SQL stuff, I don't like Slick. Quill is better: http://getquill.io/
No. Is such a thing even possible? I don't believe in such a thing even if it existed. I don't see anything wrong in doing your own thing, until you discover something better. That's how learning works best. Many people when they encounter a solution, like a new design pattern, or a new type, a new type-class, are only attracted to learn more if they can relate to certain problems they've had in the past. It takes time to understand Applicative or Monad or Traverse or Free, not because they are complex concepts, but because they are so generic that you need experience to understand their usefulness. And when it comes to FP, well, in the end what matters is referential transparency. Everything else flows from this basic trait. And to achieve referential transparency in your code, you'll end up with questions - like how can I compose these 2 things, how can I move data from here to here, how can I react to updates from the UI, etc, etc. And some of these problems lead us to search or to ask others. To learn more about FP stuff, just jump on the typelevel/cats Gitter channel or on Scalaz IRC channel on Freenode, notice the discussions, ask questions, read the source code on GitHub, etc. You'll quickly get the hang of it.
I tried hard not to insult, I'm still not sure where I did. That depends what your definition of FP is. But some do more, some do less, but since we are im Scala subreddit, in relation to Scala they all matter very much (lets say in comarision to Java). Especially if you read the text, not just labels. I.E word modularity is thrown around in OOP as well, yet it's full of leaky abstractions
It actually says in the title that this course is not about Scala, but about FP principles and it was modeled after SICP, which is an introductory programming course. Why everyone on virtually every forum claims that this is the best way to learn Scala is beyond me. The fastest way that I found (and I even wrote [a blog post about it](http://appliedscala.com/blog/2016/learning-scala/)) is to quickly learn the basics and then write a web app using Play. For this, even reading Horstmann is an overkill. If you have 10 years of experience, you don't need 300+ pages on syntax, 50 should be more than enough.
Was referring to the sentence on "extreme OOP fanatics". Some people claim there are multiple definitions to FP. There aren't. Functional programming is *programming with (mathematical) functions*, as in functions that always return / map the same output for a given input. You can also say FP is programming with (immutable) values, but that follows simply from programming with functions. Now of course, there are multiple consequences to this. You achieve "referential transparency", being the goal we want, as that's important for "equational reasoning". You also achieve better *composability*, because things tend to compose better when you don't have mutability to worry about, because mutability is not composable. But for example *modularity*, well, that only requires *sane APIs*. As I like saying, the automotive industry doesn't have any problems with mutability when producing car parts, because the interfaces that bind those components are very much a standard. And you know what else is really modular? The whole Internet. Of course, FP does help expose saner APIs, because you end up having extra restrictions leading to fewer surprises. But the word isn't just thrown around, OOP is simply good at modularity, leaky abstractions notwithstanding. And in general the problem of modularity is a though one, no silver bullet and all that. Also, FP doesn't mean static typing. LISP is the oldest FP language (family), it has withstood the test of time and it isn't static. Scala is great at type safety, refactoring and does have a richer toolbox, but those aren't properties of functional programming, but of the language and its type system. FP is simply about referential transparency and Clojure for example can also be great at it.
actually that's really what happens. we have scala projects without too much macro's (akka only stuff) which compiles as fast as java. but than you want to have cool SQL libraries/Json libraries and you start to include Macro's (on json you could avoid it but sql stuff without macro is just akward). 
"Functional programming" means many things to many people. * map, flatMap, reduce/fold/foldLeft, lambda, filter: I think pretty much everyone agrees these are useful things to have. For me part of the value is the Smalltalk thing of having everything be implemented with ordinary methods rather than language keywords: you don't have to worry about what the syntax is or which parts can or can't be factored out, because all your looping constructs and so on are just Plain Old Methods. And the other part is that each of these things is a lot more specific than a `for` loop - at the start it's intimidating to have to learn five or six different ways to do things that you used to use `for` for, but you reap the rewards when you come back to read or refactor the code. * similarly, I think pretty much everyone now accepts that pattern matching and case classes are a good idea * immutability just makes everything so much easier to debug. You don't have to keep the state in your head because there is no state: once you look at a value it will always be that value. If you want "the list after this transformation", you make that a new value, and you can easily see what the transformation did because you have both the old and the new version. I think everyone agrees that code like that is easy to work on - just a lot of people don't believe they could possibly express the behaviour they need in that kind of style without it becoming horribly complicated. Again it's often a case of learning five or six different patterns that all replace parts of a thing that you did with "just a variable". * monad/applicative/etc. are honestly just ridiculously-named examples of the standard good practice of factoring out common code. In any sufficiently large system you'll find two or more things that behave the same and when you pull out the common interface you need it turns out to be that of a monad. And the libraries contain a bunch of standard functions that work with that interface already. I think it's just badly named and badly explained, which leads to people saying things like "we won't allow monads in our codebase" which is like saying you're going to do accounting without allowing maths - I mean it doesn't make any sense, but at the same time if the guy whose job is to balance the ledgers started going on about how the ledgers form a semi-abelian category then you can understand why his manager might say something like that. * representing everything as values you can inspect compare for equality is a great idea that I think most OO people appreciate. At the same time functional people completely undermine this by saying that functions should be values, and so you end up with the same guy who was talking about equational reasoning also advocating a style that turns `User("bob", 2)` into `Suspend(Pure({}), &lt;function1&gt;)`. I'm not sure how to square this circle. I think part of it is that functional programming eliminates a large enough proportion of errors that functional experts stop needing to know how to debug, but if you tell people that they won't believe you and I don't blame them. Relatedly, honestly the concrete advantage of doing functional programming is that I can write far fewer unit tests and still have the same bug rate, but if I talk about that then I sound like a cowboy coder who's just too lazy to write tests, so instead I talk about achieving lower defect rates with the same number of tests which is the kind of thing everyone pays lip service to but no-one actually wants. The dark secret of (a lot of) programming is that if your program crashes 5% of the time that's probably fine, but it's not socially acceptable to acknowledge how little we actually care about correctness, so whenever we talk about programming we have these weird distorted conversations where we pretend our priorities are very different from what they actually are * typing is super useful once it's lightweight enough to use for everything. You have to design with the grain of it. When I was a Python programmer I said things like "you still need test coverage, so what are the types buying you?" Honestly the answer is: if your types are good enough you don't need test coverage. But like in my previous point, you can't *say* that. * higher-kinded types basically let you replace all the stuff you used annotations for in Java (or monkey-patching in dynamic languages) with ordinary code that you can refactor in the ordinary way. This is huge, but it only shows up on large codebases, because small codebases don't need annotations or monkey-patching and no-one puts these things in code examples because everyone knows they're ugly. I mean I can sit here and tell you that Scala lets you write beautiful codebases without these hacks, but again I'm not sure why you'd believe me, because every language's advocates will claim that. Bit of a rant, but hope that ended up making some sense.
So I'm sorry, I'd like to apologize to that person feeling insulted. That is correct and I agree with that, nonetheless my comment was reaction to op's "I'd rather hear testimonies from people who actually use scala." so it is relevant to him. I had erlang in mind when I was writing about type system (since I did erlang before scala), but I still considered it very much relevant to OP's question. Sure, we can be much stricter about it and definitions, but that would not spark much of a conversation. We'd also have to agree that your-mentioned definition of FP is the one under which we are discussing further topics. Exactly because you said that some people claim there are multiple definitions :) I can't relate to your examples - i.e. I don't understand what you mean when you say "internet is modular" , precisely how it relates to OOP being modular. Same goes for car parts.. My dad worked in a factory at somewhat higher position and he'd argue. They had huge problems with creating lots of waste, missing deadlines, insufficient quality, measurements-off. Lots of that is due to that contrants - "api" just not cutting it enough (lots of global mutable state not mentioned in contract, inlcuding machine settings, know-how of each person etc). Same way leaky abstraction/interfaces just dont cut it in OOP. I don't know if that was the kind of example you were thinking of (I guess not what I thought), but the world where I live in I identify lots of problems due to mutable state. And then there are bugs in boening you fly with, car you ride, and even rovers on mars. From countless of discussions with my dad I would say FP-like-concepts would benefit them HUGELY. (tghink of conceptual way, i.e. we took SCRUM from factories) I dont know what you mean by internet is modular / how it relates to OOP not being modular. We're you thinking mutable? I don't consider world "mutable". The world you see now is a different/new world than the one you saw 1 minute ago. Mutable "world" would mean that what happened 1minute ago is invalidated by present. But I know we were talking about modularity and I drifted away, sorry. P.S: I'm trying not to sound like ass, and probably failing, but it's probably my English failing me (especially) at argumentation. Please understand that everything I wrote is in good manners : ) I respect your opinion and it made me think, so thank you for that
Some of people in my close range want to make everything into free monads, and I'm not completely convinced it's neccesary. The example that is given is from talk about Free Monads, where there is something like: def fetchUser(id: UserId): Future[User] def deleteUser(id: UserId): Future[Unit] claim (from the talk is), that this is bad because you have to hit network in your tests, and free monads are way to go. So those friends prefer (same as is suggested in talk), to just make it: def fetchUser(id: UserId): FetchUserAction[User] def deleteUser(id: UserId): DeleteUserAction[Unit] And now they can decide which interpreter to use, for tests one not using Future, and in production Future based. This is where I am completely lost. Where is free monad better than doing this? def fetchUser[Eff[_]: Monad](id: UserId): Eff[User] def deleteUser[Eff[_]: MonadError](id: UserId): Eff[Unit] // or something weaker than monad, whatever we require In tests Eff would be, lets say writer monad that I inspect, while in production it could be Future monad... why is free monad much better than this? At least here I can tell from type signature required capabilities by each method... I'd use free monad to wrap some external effectful api, like jdbc... or prehaps to compose different algebras instead of having one fixed large monadic stack, but in this example, it seems like it's none of that. What am I missing? What piece of puzzle am I missing again :( P.S: I'm not sure I'd go even that far with `Eff`... only if it's truly that important for you to test this... but lets say fetchUser connects to db and well, fetches the user, there's not much to test and I'd with skipping it, and in places where it is used (which need `UserId =&gt; Future[User]` I'd pass `_ =&gt; Future.now { fixedUser}` or something among these lines 
We might be understanding different things when referring to "modularity". I understand the concept of the black box that can be integrated into a system just by knowing its input and its expected output. And if that input and that output are properly understood, maybe documented, it all works out fine. And such a black box could then be swapped with another black box that might have a different implementation. But we don't care about implementation, we care about its behavior. And the system would keep on working. And this isn't necessarily about *composition* btw. On the automotive industry, I'm probably out of my league :-) but my car has a good API. Give me something with a stick, 3 pedals and a steering wheel and I can drive it. I also have a repair shop I go to and replaced some parts, like the clutch, with stuff not original and worked out fine. Similarly for the Internet. HTTP is ugly, has badly documented parts, but my browsers can connect to any HTTP servers, without carrying whether it's Nginx, Apache, or IIS. My email client connects through SMTP/IMAP to both Fastmail and Gmail and it works. Now I know these are old protocols with cruft underneath and implementing them isn't for mere mortals, but it works. And I mean, OOP is basically about subtyping, about the Liskov substitution principle. It was meant for building modules, that's what it does. 
&gt; In tests Eff would be, lets say writer monad that I inspect, while in production it could be Future monad... why is free monad much better than this? At least here I can tell from type signature required capabilities by each method... You'd have to create a new typeclass to represent "can represent a HTTP call in this monad", no? I can see an equivalence between: sealed trait MyEffectsF[A] case class Effect1F(id: UserId) extends MyEffectsF[String] case class Effect2F(group: GroupId) extends MyEffectsF[Seq[Int]] def something: Free[MyEffectsF, User] = ... and: trait MyEffectsTC[F[_]] { def injectEffect1(id: UserId): F[String] def injectEffect2(group: GroupId): F[Seq[Int]] } def something[F: MyEffectsTC]: F[User] = ... but then you have to implement the typeclass instances for `MyEffectsTC` for both your real and test representations, so it just ends up being an extra layer of indirection and more code, and I'm not sure what your test implementation would even be other than `MyEffectsF` so don't you end up writing that anyway? I mean if you try to use something like `Task` then you call your service under test and get a `Task[User]` and you haven't made any HTTP calls yet and that's great... but you can't really do anything with that `Task[User]` except, well, run it, at which point you're back at the problem of being unable to test without making HTTP calls. &gt; I'd use free monad to wrap some external effectful api, like jdbc... What's the distinction you're drawing between something like JDBC and something like a HTTP call? To my mind they're very similar.
FWIW I find SBT unnecessarily (unless you're a library author who needs to cross-build) confusing. I would recommend sticking with maven wherever possible. I am very much in the minority in this regard though.
Linking to the Hacker News item directly will probably kill it ;-)
The search functionality seems good not great. Definitely a site worth a visit, but it'll need work to become the definitive resource.
Hey, thanks for the feedback. Which query did you make where you didn't get an expected result ?
and also http://pedrorijo.com/blog/scala-compiler-review-code-warnings/
This is amazing! Thank you for the great work!
&gt; It has a module system, including first-class modules, which are better than Haskell's typeclasses. Those modules are sweet, but do not provide means for ad-hoc polymorphism and are not a replacement for type-classes. Because of this there is a proposal to introduce a mechanism similar to Scala's implicit parameters, see: http://www.lpw25.net/ml2014.pdf &gt;&gt; it's effectively single-threaded with no in-process concurrency support... &gt; &gt; Even assuming this is true, what does that have to do with it being a good FP language or not? It's relevant to it being a good language in general. You mentioned "Java compatibility as a requirement", I was simply mentioning what that means, as many people learn things to apply them to their problems. And even though Ocaml developers do love Ocaml, for people like me expecting what the JVM provides, it can lead to disappointment as a first impression in their journey to learn FP. &gt; Besides, it isn't true: feel free to pick among Async, Lwt, or Netmulticore. Ocaml does not do preemptive multi-threading, period. The available libraries, including the ones you mentioned, are either about cooperative event loops (e.g. like in Javascript) or about inter-process message passing. &gt; Structural typing and row polymorphism mean you have straightforward object literals and type compatibility the way you would intuitively expect. Polymorphic variants are great glue between the OO world and the FP world. Type inference is complete. Honestly, this complaint makes me wonder whether you've actually used the language, or just read things about it and concluded you didn't like it. Oh, I'm not saying that it doesn't have cool features. But the glue you're mentioning is not between OO and FP, but between OO and Hindley-Milner. And they do mix like oil and water. &gt; Look at the content of Oleg Kiselyov's site. Almost everything he writes is offered in both Haskell and OCaml. Here are several of his articles on ML, all but one (IIRC) for OCaml, one for Standard ML. Thanks for that link, didn't know about that site. Clearly I have much to learn. &gt;&gt; Anyway, I think it's bad advice to learn a whole new programming language, by reading a book starting from "Hello World", just to achieve some supposed enlightenment and then get back to your old one. It never happens, ESR was wrong. &gt; &gt; Once again, I'll await citations and links to sources, this is so utterly ridiculous. In the English language the phrasing "I think" clearly denotes personal opinion, for what is clearly a subjective measurement, so the ridiculous part is asking for citations and sources. &gt; A "meager' book you admit to not having read, that's packed to the gills with helpful exercises. Having admitted to not having read the book already, I think it should be pretty clear that my critique was not for this particular book. I object in general to the idea of learning a whole new programming language in order to achieve knowledge about advanced concepts that could be instead practiced using the language you already know. Because in addition to juggling with new strange concepts, you now have to also learn new syntax and be frustrated with new tooling - like how are the build tools of Ocaml and Haskell? Beginner friendly? Yeah, thought so. &gt; In the future, please consider dumping your uninformed opinions on your priest, shrink, or bartender rather than here, wasting all our time. Ooooo, I feel like I stepped over some kind of territory here.
&gt; Is Free really that new? The Haskell implementation seems to date to 2008 New to Scala. &gt; Huh? fs2 et al are all based on iteratees. Iteratees very much did work out. Play framework dropped them in 2.5 &gt; Sure, but I think the "generic monads" approach ends up being much more complex (and frankly is far more of a "shiny new toy") than Free. Hence, "exploration". 
Scalaz Free dates to 2012. Play has its own priorities. I really don't see iteratees going away, they're the right way to implement streams, and it sounds like Play intends to remain compatible with that at least. 
Don't be afraid to fail the assignments. Submit wrong or missing answers and fail! Then move on. It's not the end of the world. It's very rare that the later lessons in the course will require a direct understanding of the earlier assignments. In fact I very much doubt it.
searching for play-json shoud probably list playframework at the first page. but with having relevance it's only at the 5th page. Searching for http should list akka(-http) somewhere I visited 10 pages and after the second page there are mostly only libraries that adds something to existing http-servers.
How does that work?
Yes, this is what I had in mind, thanks for full demonstration.
Wow that's some seriously subtle trolling -- m50d -- You and are just going to have to disagree here. (A pattern that seems to be emerging with my interactions with you) Maybe hard to accept but my opinion and experiences are just as valid as your own. If you disagree based on your own experiences, I'm cool with that! Simply say that and be done with it. No need to pick at the details of what I'm saying and avoid my main point.
But you are -- because even that isn't my point. You are arguing against my perception which benefits neither of us. In my experience, this year, I have seen a big uptick in the number of blog articles, talks and posts about the free monad in Scala. You have a different experience? Awesome thanks, I accept that. But it doesn't change my view. My point is: Don't base technology choices for a business on what seems new and interesting, but instead pick things that balance the burden of forcing others to learn new technologies against the utility those technologies bring to the organization.
&gt; My point is: Don't base technology choices for a business on what seems new and interesting, but instead pick things that balance the burden of forcing others to learn new technologies against the utility those technologies bring to the organization. All true. I just strongly disagree with the implication that free monads bring less utility (and are more hyped) than generic monad style, which was what I took from your first response. Certainly I don't see how generic monad style offers any help with /u/fromscalatohaskell 's original motivating example of testing without performing actual network requests.
You might have to settle for learning scala's existing json libraries. They have almost universally chosen to represent arbitrary json as an AST because it preserves type safety. That said, sometimes frustration with a thing is a signal that you do not understand it and would benefit from taking the time to do so. I don't know anything about your experience level on the topic, and I'm certainly no expert, but that advice has been almost universally applicable to the other devs I've worked with.
We used Drop Wizard metrics in production at my last job. From what I can tell it's the *de facto* standard for Java metrics. There's the obvious overhead from measuring anything at all, but the metrics library is quite well done and its impact is minimal, IMO.
I used drop wizard extensively in high scale production. It's the BEST. My new job uses finagle with thrift and I hate it, trying to convince them to go to dropwizard
It doesn't look like get/post/delete could be passed as functions, since they're calls to the ws object. You could call a def for your recoverWith code that took a function, since you're basically recursing on that. I don't see anything else straight away that you could easily pull out that would give you much of a gain.
Thanks for the reply. The methods just looked so similar that I felt there might be a way to abstract it. A more senior coworker of mine said it might be possible to refactor everything out to then call makeRequest("GET") and the other params passed implicitly but I couldn't figure out how to do it (implicit looks for type rather than name and where are too many Strings / basic types)
You can also ask your question here: http://codereview.stackexchange.com
They have some sort of algorithm to prevent vote rings and abuse. Not sure how it works, but they take into account that you just came to that item from an external referrer and I was right - this announcement never made it to the front page, even though it had enough votes. It was buried. And isn't the first time I've seen this.
It's a big deal, because if the match doesn't succeed, then a [MatchError](http://www.scala-lang.org/api/current/#scala.MatchError) gets triggered at runtime. That warning has saved me so many times, that nowadays I opt-in to turning all warnings in errors by the compiler (with the -Xfatal-warnings option). Just add the wildcard brach.
Actually the problem is actually not that there aren't good parsers / ast's. The problem is more, that there are too many with their own AST. That make's it really really hard to interop between some tools.
You can see some compile/execution comparison at https://www.youtube.com/watch?v=Q_Z_8HXt68k
Don't want to add fuel onto the fire, but I agree that learning OCaml isn't the best step for being good with Scala. It misses a lot of things which are expected (even with the more purely functional programming, i.e. HKT), and the OO system is more disparate when compared to Scala. I don't exactly buy into the "if you want to learn how to use this language, then learn the other language X first". The statement is already tautological, if your recommendation for learning a language is to learn another language then simply put you are not going to be learning how to program in that language idiomatically. Learning another language may make sense if you are pushing the boundaries and trying to learn new techniques, but this should come after you are comfortable with the original language you are trying to learn, not before.
/u/kilotaras has the right idea. These are all methods on `WSRequest` that return `Future[WSResponse]`. So you can do something like: def wrappedCall(path: String, perform: WSRequest =&gt; Future[WSResponse]) = perform(ws(s"$base$path").queryString(...).withHeaders(...)) .recoverWith { ...} def get(path: String) = wrappedCall(path, {_.get()}) def post[T: Writeable: ContentTypeOf](path: String, body: T) = wrappedCall(path, {_.post(body)}) ...
&gt; As soon as I saw the answer, 8 times out of 10, it made sense immediately. I could have stared at it for hours and never came up with the solution on my own though. Yeah, that's normal! :-)
Great point. In their code I do see them use DropWizard. https://github.com/erikvanoosten/metrics-scala/blob/master/src/main/scala/nl/grons/metrics/scala/Meter.scala So I wonder if codehale is a wrapper on top of dropwizard? 
&gt; I can get rid of the error if I add case (_, _) =&gt; () but that does not help because I have to call Future.successful(()) at the end anyway and I only want bazz() or foobar() to be called when bar.food is Food.bread. It seems to me you want a final `case (_, _) =&gt; Future.successful(())`, then, and to drop the final `Future.successful(())`. That is, you want the `Future.successful(())` to be a catch-all case, which is essentially what you want the code you posted to mean, but doesn't quite. &gt; Is there a use case where I should exhaustively pattern match or is it fine to ignore it and use this as an alternative to nested if/else's You should always exhaustively pattern-match (when you pattern-match) for the reason others have explained: the compiler is letting you know that you haven't told it how to handle everything that could happen, and if you don't address that, the result could be a crash at runtime. In general, keep in mind that "throwing an exception" means, by definition, the code doesn't know what to do with some value it got. So don't throw exceptions. :-) As others have noted, this seems like a good opportunity to create a `sealed trait` hierarchy, and just write a `case` for each extension. Problem solved. So this is one approach to not throwing exceptions. Unfortunately, sometimes you can't avoid exceptions. A good example is in talking to the outside world, because that can always fail. That's when I suggest using `Option`, `Either`, `Try`, `\/`, `Task`... to model the result. The idea is to represent "success _or_ failure" in the return type, and use the facilities these "failure monads" offer to deal with them later. By the way, a note on theory: we call a function that returns a value for any possible input it gets a "total function," and one that doesn't necessarily a "partial function." A `case` expression is a partial function (literally, a [`PartialFunction`](http://blog.bruchez.name/2011/10/scala-partial-functions-without-phd.html) in Scala), but you can (sometimes) make it a total function by handling all possible cases. So generally, prefer to write total functions, and when you have a partial function, first try to make it total by handling all values it can get. When that isn't possible, try to make it total by shrinking its domain (e.g. making an unsealed `trait`, which can be extended at any time, `sealed`, and then handling all the extensions of it). When that isn't possible, enlarge the codomain (or "return type"), again typically by using a sum type of some kind (success _or_ failure), and ideally by using some sort of failure monad, so you can compose these functions in meaningful ways.
It's not a wrapper, they just changed the name in the Meter.scala file: ``` com.codahale.metrics.{Meter =&gt; DropwizardMeter} ``` Metrics is different from the HTTP framework. 
&gt; Do you consider Akka Http to be production ready? What does it matter what we think, the developers have flagged as no longer experimental.
The developers have one perspective. Users might have a different one.
&gt; "Outmoded" is not a real argument I'm not sure why you put outmoded in quotes, but yes it's not an argument, it's a description. &gt; Maven is mature As in it's never going to change, despite how hopelessly broken it is. &gt; well documented Doesn't make it less awful. &gt; widely supported, has excellent IDE integration, As in, everyone just completely reimplements maven in their IDE, sure that's "integrated". &gt; importantly it enforces consistent project structure and doesn't allow arbitrary code in build definitions - all areas where SBT and gradle fall down. Patently, untrue. SBT is no less declarative then maven. This assumption is based off a tired and incorrect meme that "XML is declarative" when they actually mean structural. They just keep using the word declarative because that's the word using on Maven's home page to describe itself.
I use akka-http in production. There are occasional issues, but it's working good.
Not really true, there is "akka-http-core" and "akka-http-experimental". The truth is that most of the useful stuff is in "akka-http-experimental" :)
underscore.io has some good examples for using macros https://github.com/underscoreio/essential-macros
that's basically untrue. if you create your own abstraction you barley need anything from akka-http-experimental. if you directly want to use akka you probably will.
What type of issues?
You are a small political party (the metaphor almost works). It's going to take time, but if you show people that your option is better (reddit, blog posts, help on Gitter scala), progress will be slow but sure :-) (Unfortunately, I tend to support small political parties, and the growth is rarely as good as one might wish, even if they seem to offer a better solution than the mainstream. If you can crack this, they might also want to hear from you).
Argonaut is fantastic. Why don't you like it?
Have you worked on similar-vintage builds in SBT or Gradle? I've used all three, and Maven is by far the best of the bunch.
[removed]
Makes sense, I've heard this a lot. Thanks. I have still have not gotten my head around haskell's lazyness and it's implications.
Not a shame at all. Scalaz is awesome. Can be a bit of a challenge to learn but the `#scalaz` IRC channel is very helpful. Cats is a similar library that's smaller and younger, but is committed to providing consistent and approachable documentation. You might give that a look, depending on what you're doing. (Full disclosure, I am ostensibly a maintainer of both projects, although I don't have time to contribute much to either).
If you **aren't** using scalaz or cats, you're either writing them yourself piecemeal, rewriting boilerplate over and over again, and/or making unchecked assumptions that can blow up at runtime. There isn't much crazy about scalaz. The complaints about symbols are valid from a certain perspective, but you don't really **have** to use symbols. The core abstractions in the library are powerful and useful. Functor/Applicative/Monad, Foldable/Traversable (and their \*1 variations), Semigroup/Monoid. There's nothing crazy about them...they're all exceedingly simple all things considered. I highly recommend learning scalaz or cats as it'll make you a better programmer 100%.
When I first started using Scala, my strategy was very much "brute-force search various combinations `+`s and `-`s until the compiler stopped complaining." If my search wasn't fruitful, I would ask for help on the mailing list...
It's not one or the other - you can do the latter if you like. Sometimes people use the first style because you can do: object MyObject { ...} import MyObject._ class MyObject { /* use functions from the companion object */ } which obviously you can't do in the same way with the other approach; that's the only advantage I'm aware of.
Like how in mathematics you shouldn't use a theorem you can't prove, you probably shouldn't use a ScalaZ construct you couldn't write for yourself. ScalaZ isn't one monolithic thing - something like `Validation` or `\/` is just an ordinary datatype that you could write in 5 minutes. Whereas even after ~6 years of Scala I've never used a `Profunctor` and have only a vague idea of what I'd do with it if I did. There's no shame in writing something out the long way first and then seeing how ScalaZ can simplify it, and that's the approach I'd recommend rather than cargo-culting ScalaZ tools. But in a production system of course you use the widely standardized library (well, if you have the choice I'd favour Cats rather than ScalaZ for political reasons) rather than reimplementing it yourself. There's no point being incompatible with everyone else.
IIRC the compiler will error on that because then everything in the companion object is in scope for its own definition. I can't check at the moment though, so by all means try it.
I've did the check and everything seems to be working fine, maybe that was the case for older Scala compilers?
For political reasons? Why's that so?
ScalaZ is actively hostile to documentation though. E.g. I believe they have an explicit policy of never having comments. Cats is not yet well documented but its policies give it a chance of getting there eventually.
I think that's unfair. Quasar is impressive engineering (though I disagree with the design) and he's done some serious study in formal methods (though again I disagree with his approach).
especially it's even hard to find out what they *really* do
I'm a .NET developer (5yrs) with a degree in Math. Lately I've been getting bored with the ecosystem etc... and have been looking for a project or something to keep me coding. I started the "Functional Programming Principles in Scala" mooc to learn Scala but I just don't have the time to keep up with the course and work on enough code to keep up with actually building the skill of coding functionally. What sources have you found that give a good mathematical background in functional programming that can be broken down into smaller more practicable bites? 
True, but few would reject contributions that added or included them (which AIUI ScalaZ does)
As fast as Java doesn't sound like a fantastic selling point. I guess then why not nearly any other language?
It seems that the problem is not Play but rather how you're querying the database. I think you should consider using ScalikeJDBC or Slick or Doobie all of which are perfectly capable of returning lists.
Because many other languages are not as fast as Java; Node.js is easily 5x slower, and Ruby/Python easily 40-50x slower. Java has terrible startup times, annoying warmup times, huge memory footprint and long GC pauses, but it's steady-state throughput is actually really good
This looks like a typical use case for actors. Though you could totally write a state machine based on futures.
When Scala's type system isn't mentioned, it usually means they picked Scala for the wrong reasons.
&gt; You normally can do anything without it in a much more clear way. FWIW, I have never found this to be the case _once you know what you're doing with scalaz_. If you don't, then chances are you're trying to use a different paradigm (FP) than the one most people are familiar with, and then it will be "unclear" by definition.
Some comments provide negative value. That doesn't justify not having any, of course. It just means not all comment PRs should be accepted.
Ob. political note: some of us have a _very_ different experience with Tony Morris than [/u/m50d](https://www.reddit.com/user/m50d) describes. In particular, I will go ahead and say: &gt; He seems to be in the habit of upsetting people for fun, including Scala newcomers who go there looking for help. is straight-up slander. He is insistent to the point of dogmatism on principles, yes, and doesn't have any patience for equivocation. But if you genuinely want to know why he says what he says and are open to being informed, he'll explain, helpfully, without rancor. Offer even one whiff of "gotcha" or "well, it's all just a matter of opinion" and yes, he'll detonate like a hand-grenade. I find that among his more favorable qualities. **Update:** He is, for example, one of the coauthors of the [NICTA Functional Programming Course](https://github.com/NICTA/course). He _likes_ genuinely helping people who genuinely want to learn. He doesn't like having his time wasted by people who want to argue with him about whether it's worth it or not, or about programming paradigm metaphysics generally.
Unfortunately play isolated slick doesnt work with my version of play but slick seems to be the consistent choice here so I'll give it a try!
How are we supposed to extrapolate what's wrong with existing json parsers?
Ended up choosing ScalikeJDBC as I was most productive with it the fastest! 
DI in Play can happen either at runtime with `@Inject()(db: DBApi)`: https://www.playframework.com/documentation/2.5.x/ScalaDependencyInjection Or compile-time, using `DBComponents with HikariCPComponents`: https://www.playframework.com/documentation/2.5.x/ScalaCompileTimeDependencyInjection 
The first half of the site is just a few buzzwords thrown at the page. And that example makes it even harder to know exactly for what their product could be used and how to use it. They don't even provide a small demo / Screenshots / whatever. They just throw out buzzwords.
After reading the reviews on that all I have to say is "Would that I had two upvotes to give". I should have the book tomorrow.
Didn't know if most people here knew that the SIP committee posts their monthly meetings on YouTube, so here it is.
How do I fail computation in free monad style? I.e.: for { x &lt;- foo(1) y &lt;- foo(2) if (x &lt; y) ???stop??? _ &lt;- sendEmail() } yield ()
No need to answer ... I figured it out with Freek / Xor...
Also re dtos, Lombok makes them nearly as succinct as scala
&gt; I find that among his more favorable qualities. Being unable to cope with someone who disagrees with you is not a favourable quality, unless you're an aspiring cult leader.
I knew this was a tpolecat message without looking at the username just from the "ostensibly"
Are meeting notes up anywhere? I'd much rather read for 5min than listen for 50min :)
Glyph wrote a post called [Unyielding](https://glyph.twistedmatrix.com/2014/02/unyielding.html) with this sort of example, probably demoing exactly what you have in mind. And if you read [Mike Bayer's thoughts on asyncio and databases](http://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/), he points Glyph still gets it wrong (he's not calling Glyph out, and Glyph probably intentionally glossed over it because he was talking about concurrency in Python not databases). Edit: to be clear, I'm talking about Mike points out you need a database transaction for the transfer to ensure correctness. 
Thanks, that's exactly the post I was thinking of. Your second link is talking a lot of sense, but I don't see it as contradicting the first? I think the second is talking about threads that run independently (or at least where the only point of communication is the database) for which threads are effective (though arguably at that point why bother with threads when you could just run multiple processes - but that's a separate discussion) whereas the first is talking in the context of a single task that requires coordination (which is very hard to do correctly with threads).
This isn't entirely correct though, there is a new python library for postgres async (see https://github.com/magicstack/asyncpg and its blog post at https://magic.io/blog/asyncpg-1m-rows-from-postgres-to-python/) and it does blow all other postgres python libraries out of water. Even for database connections which typically return requets really fast, unless that response is comparable to CPU cycles (it almost always isn't), than async is going to provide faster results. That of course means your entire stack needs to be async, but if that is the case then it will almost always be faster
This is blatantly untrue, and trying to exclude someone from something is way worse than saying something mean. You are everything wrong with open-source. Open source doesn't mean "you can only say nice things" it means that it should be an open discussion, where people aren't excluded. Weeaboos these days. 
Note that there is an advantage to this method of doing async programming, in that it will **always maintain the stack**. By always, I do really mean always. In the purely functional approach, you either * Blow the stack by using full TCO * Dont have full TCO, usually maintaining the stack in most circumstances, however you then have to deal with creating trampolines or stack rewinders The stack is obviously very important when it comes to debugging applications, particularly in production in usage scenarios which cant always be replicated when testing. Also this approach does have better performance since it maps more closely to how CPU's work. You don't have to worry about boxing (any functional approach is going to be boxing your async computation in some monadic type) and it does give more control over performance. Like most things, its a compromise
The guy that wrote this probably knows more academically than the majority of Scala programmers, so thats a very incorrect accusation Also note that this is based on a mathematical model, in the same way that lambda calculus also has a mathematical model. Its just used very differently
It actually wouldn't be too bad, its somewhat similar to how https://github.com/scala/async works. You do have weird cases, like how you are going to treat closures in context of yield and stuff like that
Asyncpg looks promising, I'm still wrapping my brain around asyncio and structuring projects with it (my first attempt was basically `async def` all the way down because I was porting a sloppily designed weekend project). But I was talking about how the transfer needs to be wrapped in a database transaction to ensure correctness while yielding/awaiting. I'm going to edit that in for clarity. 
The key point is this: "He is insistent to the point of dogmatism on principles, yes, and doesn't have any patience for equivocation. But if you genuinely want to know why he says what he says and are open to being informed, he'll explain, helpfully, without rancor." &gt; Being unable to cope with someone who disagrees with you... He's not unable to cope with someone who disagrees with him. He's unwilling to deal with someone who insists on the truth of their opinion about matters that are not matters of opinion.
seconding /u/TheOsuConspiracy Look into akka messaging specifically akka camel 
If that's really what you want to pursue, then I strongly recommend [The Haskell Book](http://haskellbook.com/). There are both other languages and other books, but Haskell will enforce the FP discipline, and this book really does take you from 0. So it won't be _easy_ by any means (at least, I wouldn't expect it to be), but it will teach you what you want to learn.
Thanks! Makes sense. 
Trying not to come across as facetious here, but sometimes what is and what isn't a matter of opinion...is itself a matter of opinion. In matters of programming practice, which is by no means a science, there are very few things that are known with any certainty. 
"Shame", if that has to be a thing in software development, should be attached to making poor engineering decisions. If you believe Scalaz is overall a poor engineering choice, and yet still use it because you think it's cool -- that's shameful. If you believe Scalaz is a great engineering choice, but choose something worse because you're worried about how you will be perceived by others, that's also shameful. Look at the merits of the library, discuss its merits with others, and decide on that basis, rather than peer pressure.
I think the point of the article is that it's not so much comparing delimited continuations to monads, but scoped continuations to monad transformers. But even when it comes to monads, I still think most programmers have trouble understanding the abstraction. I personally find it irritating to have to remap my logic into a monadic flow when some chain of operations has to be adapted to live within a monad.
He can say what he likes on his own blog or whatever. But when we're talking about an IRC channel where newcomers come to ask for help, it should be a place for that. Before I encountered Morris I used to think that words could never be harmful, that all you had to do was ignore them. But if you think about it from the other side: we're only human, we all have our flaws. If someone very intelligent, devoted, and practiced at upsetting people spends several hours a day working at it, is it any wonder that they eventually figure out an approach that works?
There you go again: It SHOULD be this, it SHOULD be that. You have no right to restrain a person's speech. I completely disavow the idea that words are anything more than words. I've been told all sorts of things, I've been called controversial, been threatened, been lashed out at, called every name under the sun and a few from the other side, you name it. I have never once changed my tune when it comes to this issue of censorship. You don't realize what a slippery slope it is to start putting chains on what people can say, and then what people can do, and then what people can think, and then what people can be, and then there's no person left. You must despise people like Morris, people like me, for some reason that I don't think I could ever fathom. You must think of me and see some sort of vile, terrible monster, but I think of people like you and I see a person who was just taught the wrong thing. Maybe you don't/didn't know any better, but I'm here to tell you that you're on the wrong side. On top of that, it's a one-way street when it comes to making people upset. I have a very easy time making people upset. I know all the things they want me to react to, I give them just enough room to think they're gonna win for a second, and then I'll pull the rug out. They'll realize that I really, truly just don't care what they think about me, and something about that realization that the only way they could ever influence me is to physically force me to do something terrifies them. I guess they realize how powerless they are, and they think that they're fighting some pseudo-noble cause or sensibilities or political correctness, or whatever it is. If I tell you to go fuck yourself, it's your own fault for getting upset about it. You can't possibly expect me to take any accountability for your actions, just as you wouldn't ask a stranger to clean up after some other stranger's dog, or something like that. I realize this comment is too long to seem like it's worth reading, but I wrote it anyways because I was highly caffeinated and even though I realized what a waste of time this is halfway through, I figured I'd just write the rest for the hell of it. So, there it is.
Looks like: * sip-12 (Uncluttering Scala's syntax for control structures) was rejected * sip-16 (Self cleaning macros) was rejected in favor of a new proposal slated for the end of the month * sip-27 (trailing commas) postponed for more details on ambiguities and wanting more detail on interaction with an upcoming HList design. * sip-23 (Literal-based singleton types) had positive interest, but was postponed for document improvements.
Just looked at the Play API docs, and saw `DB` is deprecated, so normally I wouldn't be writing anything with it, but ... this design has been in my head all day and I need to get it out, so :-) import java.sql.ResultSet /** Contains utilities for working with databases. */ package object db { /** Wraps a ResultSet and adds a method to iterate over the results in a type-safe way. */ implicit class ResultSetIterator(rs: ResultSet) extends AnyVal { /** Returns an iterator over the query results using the given function to parse a row from the results into a typed value. */ def iterator[A](parse: ResultSet =&gt; A): Iterator[A] = new Iterator[A] { private var _hasNext: Boolean = true override def hasNext: Boolean = _hasNext override def next: A = { _hasNext = rs.next; parse(rs) } } } } private def getLoads( startDateValidity: ValidDate, endDateValidity: ValidDate): Traversable[Load] = { val queryString = determineLatencyQueryString( startDateValidity, endDateValidity, "DATA__LOAD", "load", "load.MSGSEC, load.DATETIME, lt.CONNECTION") DB.withConnection("historical") { conn =&gt; conn.createStatement executeQuery queryString iterator { rs =&gt; Load( formatDateStamp(rs getString "DATETIME"), rs getInt "MSGSEC", rs getString "CONNECTION") } } }
Thanks! That only made me more interested so now I'm watching it at 2x speed :D
Disappointingly no discussion of session initiation protocol 
Ron's blog posts are so dense with incoherent ideas I have trouble understanding his thesis. Someone please help, the best I can understand it is that you can represent all monads as continuations, and monads are hard to understand, so even though they're the same, monads are bad? Is that what he's attempting to say? 
I don't think it's fair to bring up my issues with some of the Cats committers everytime someone mentions the library, and I think you should extend the Scalaz the same courtesy. I've personally had very unpleasant interactions with some of the Cats committers (to the point that I won't contribute to the project), and only had pleasant experiences with the Scalaz folk (which I know is not everyone's experience). There is a contingent of users in the Cats community that actively go around and publicly wish ill on the Scalaz project (and it's contributors?) and overall exhibit a very mean spirited attitude that is pretty much the antithesis of the so called 'welcoming community'. I know that some of Tony's behavior has been unacceptable, and almost all the other committers in Scalaz have let him know when that was the case. He has had temporary bans in the chat. Tony has publicly (in emails) and privately apologized to individuals for his behavior. I know of no such apologies from the offending Cats folks. Some of the Cats committers are friends, coworkers and overall fantastic people, which is why despite my own personal issues with a couple maintainers, I am happy to recommend either library and I hope they both succeed. I find it personally sad and frustrating to not see that attitude reciprocated, especially from very well respected members of /r/scala and the overall community such as yourself. EDIT: If you've *recently* had an unpleasant interaction with a member of Scalaz, then yes, absolutely bring it up to me or other maintainers so we can address it. 
Well said!
I'll certainly agree that there are lines that can be crossed, and that Tony's crossed them sometimes. I'm only disagreeing with the characterization of him randomly insulting people just because they're new to a methodology or technology, and in particular, just for the fun of it. _That_ is slanderous. There are plenty of other criticisms of Tony that fall short of that.
Forgive me, English isn't my first language, so I may at times be incoherent (although I guess I'm sometimes incoherent in any language :)). Although, as this particular post says, it won't make much sense unless you watch the talk first. I guess my thesis in the talk (as far as I recall; it's been a year) can be split into two interconnected parts, a theoretical one and a pragmatic one. 1) The theoretical part: The primitive mathematical (semantic) building block of imperative languages is the continuation. Unfortunately, the word is overloaded, but I mean it in the most basic way, the one [used by John Reynolds](http://www.cs.cmu.edu/afs/cs/user/jcr/ftp/histcont.pdf) to simply mean "a program point" (in imperative code). Non-imperative functional languages (i.e., pure functional languages) don't have the continuation as the primitive building block, but the notion of a mathematical functions, as the basic denotation. Functions are a good approximation for many computations, but the concept of a (single) mathematical function isn't rich enough to describe all programs (interaction, concurrency), and so the function (unfortunate pun) of the continuation must be regained. This is done in the form of the monad, which functionally simulates the state transitions embodied by continuations (in the basic sense). 2) The pragmatic part: as a consequence of 1, there is no need to use monads in languages that already have continuations -- i.e. all imperative languages. Doing so is basically treating imperative subroutines as functions (hopefully using a pure subset of the language), and then re-creating the primitive continuation in the form of a monad. This is not only redundant but results in an embedded language that does not integrate well with the rest of the code. There are rare cases where monadic code is convenient (collection manipulation), but when it comes to IO/concurrency, the *only* reason why monads have been used in imperative languages is due to a poor implementation of threads. You just need a good one. 3) Bonus: Even though I believe that a good implementation of threads is all that's necessary, if you want to see how flexible continuations can be you may want to reify them (delimited continuations). Doing so gives you the same power monads do in languages like Haskell, but in a more idiomatic way, and in a way that happens to compose nicer than monad transformers (I know Oleg Kiselyov mimics "scoped" continuations in monads and uses handlers, just like in the scoped continuations code in my post). I believe this is unrequired for 99% of programs and programmers, but it does demonstrate the nice "algebra" reified continuations give you, and I know this kind of thing is important to people who like the pure-functional style; I just wanted to demonstrate that the same abstractions exist in the imperative world in a different form (or, rather, a similar form but with different foundations). Finally, when I gave the talk (a year ago) I believed that monads are wrong for *imperative* languages, including imperative functional ones (except in pure data transformations on collections) but are perfectly fine in a language like Haskell. My position has changed since then, and I now believe monads are the wrong way to represent effects and interaction even in PFP languages, mostly because they are hard to grok. I think that new ideas are necessary (linear types can replace monads for those tasks in some cases, I believe, but I'm not knowledgable enough about them to comment further on that point).
&gt; I think the second is talking about threads that run independently... whereas the first is talking in the context of a single task that requires coordination (which is very hard to do correctly with threads). I agree. This is precisely concurrency (a feature of the problem domain; subtasks *compete* for resources) vs. parallelism (a feature of the *algorithm*; subtasks *cooperate*). As I say in the talk, monadic composition is great for parallelism.
&gt;Forgive me, English isn't my first language, so I may at times be incoherent Your english is fine. &gt;Non-imperative functional languages (i.e., pure functional languages) don't have the continuation as the primitive building block, but the notion of a mathematical functions (sometimes "partial"), as the basic denotation. Functions are a decent approximation for many computations, but the concept of a mathematical function isn't rich enough to describe all programs (interaction, concurrency), and so the continuation must be regained. This isn't true. A pure function can describe concurrency just fine. You are just saying that because there's an ISO between a monad and a continuation, that it's a continuation. That's just one perspective. It's valid, but not *more* valid than the other. &gt;This is done in the form of the monad, which functionally simulates the state transitions embodied by continuations This is utterly false. You seem to continually misunderstand what monads are actually doing and what a state transition means. &gt;This is not only redundant but results in an embedded language that does not integrate well with the rest of the code. This is also just utterly false. &gt; The pragmatic part: as a consequence of 1, there is no need to use monads in languages that already have continuations -- i.e. all imperative languages. So what? Another true but vacuous statement. This is like saying there's no need to use while loops in a language that has a go-to. You can implement one with the other. If you like continuations, fine. But they're still monads if they follow the monad laws, you understand that right? If you want to prefer your own continuation based syntactic sugar be my guest. But you are continually trying to hide your very subjective opinion in language that makes it sound factual. I don't see how you can abstract over delimited continuations as well as you can monads. This is useful. I'm guessing you'll argue that this isn't important cause it's a "brain-teasing chain of lambdish indirection" which is more opinion disguised as fact. Honestly, I think you should spent a lot more time trying to understand functional languages and abstraction before continuing on your crusade. 
I can accept that viewpoint.
I would recommend slick for database access layer. May be useful: http://pedrorijo.com/blog/play-slick/
Doh! Too late to suggest another option? https://github.com/rocketfuel/sdbc
I don't know how your method learning goes, but what's been working for me is coding a program that solves a specific problem of mine at the time, and figuring out what I don't know as it comes up. (I.e. how to take various key-value pair inputs, save them to a file and read them back at a later time.) This has been accompanied by the Scala for the Impatient book, which is great because it introduces the data types and shows how they're used, then moves on.
Alright, I might consider it. Maybe I'm just used to the typical OneToX JPA stuff but right now it feels like a lot of work with Slick to just access the database.
I learned scala before Java. I just picked a non critical project and started building, used Google or IRC when I got stuck. I believe my first project was a stock price web scraper that copied all the prices of a few tickers. After my first project I went and read the full Martin Odersky book on scala, it made a lot more sense after my first project.
How about fix, or else isolate the bug. Most of the time, tasks are slow in a reproducible way. Rescheduling only makes them run just as slow on a different worker, but now there is more work overall in the system, so it's likely to perform worse. Leave speculation alone, it's a solution in search of a problem. As a side note, most of the time tasks are slow due to imperfect partitioning, so spend your first efforts ensuring the partitioning is fair. Google's Dataflow does LiquidSharding, which is a strategy to dynamically repartition, which is a whole different beast than the naive speculation Spark implements.
Refer to some good books.. like programming in scala second or third edition and scala for impatient. And practice. Try to solve small set of problems at first just to get the hang of it.
how is deadbolt for authentication? Never used it i am a beginner tho.
We use anorm because, after surveying the various ORM-style solutions, we agreed with the arguments layed out in the docs (https://www.playframework.com/documentation/2.5.x/ScalaAnorm). So far so good, putting all the queries in a singleton that handles deserialization has worked out. I encourage you to frame your use cases in the frameworks you are considering and weigh the pros and cons for yourself to see what you prefer. There is no right or wrong answer.
You need to read Odersky's Programming in Scala. You can do your own small projects without investing huge effort, but if you try looking at other people's code without reading that whole thing (preferably twice), you will get stumped immediately. It is the hardest language to learn I have ever encountered, and I know C, C++, Prolog, Java, Php, javascript, python, coffee script and others. I picked up Python in two weeks, but it was a year before I felt proficient in Scala. Even now there are areas I stay away from. It's worth it if you are going to be working on a large ongoing project. If you're doing a little 4 week project that will be done and dusted in that time, it doesn't really matter what you use, Python or whatever is fine. On a large project that will grow and need maintenance and upgrades for years Scala shines as the strong typing and good tooling allows you to refactor with confidence. Especially if you need a web front end where you can use Scala.js and Akka/Spray on back end, no other language comes close.
I don't think there's one 'best' way, but the way I learned was by writing a lot of scala code, and then learning ways to improve it or refactor it to be more idiomatic. It's more important to continue writing scala, than to figure out how to get it 'right' the first time. Your toy application is not going to come crumbling down if you use an array when a list would better. You will get lost in the typing though (frequently when you are just starting out!). I still sometimes get lost and I am one of the maintainers of one of the most advanced scala libraries. Fighting with the types is a natural process of using a good type system, because it will save you time later by preventing what would be run time bugs in other languages. Basically I'm saying its normal to feel frustrated or even stupid when learning the language, but like the rest of us, eventually things start clicking. Just make sure you're having fun in the process . :-)
Unrelated, but thank you for scalafmt. My team uses it now and its amazing, its even part of our build status now
Downvoting doesn't make it less true, unfortunately.
So what would your view be from behind the veil of ignorance, if you didn't know whether you were going to be one of the people who finds it easy to upset people and is hard to upset, or the other way around? Making upsetting people a free-for-all means allowing smart people to hurt stupid people with no recourse, which is as morally dubious as making violence a free-for-all and allowing strong people to hurt weak people with no recourse. To the extent that all laws about words are a slippery slope, all laws are a slippery slope - after all any law puts chains on what people do. There are varieties of speech that need to be protected. What Morris does is not remotely close to that.
I'd like to publish it on Sonatype so one won't have to go through the `publishLocal` phase, if that's what you mean ;-)
This looks pretty good, but one thing I've wondered about Scala.JS is what about the barrier between the client and server? How much further can this be taken so that buttons can be mapped back to server side calls, and via what mechanisms.
&gt; For me ScalaZ will always represent Morris (and I think his is the first name most people would associate with it), and the library is nowhere near useful enough to be worth giving him credibility. Me too.
&gt; The complaints about symbols are valid from a certain perspective, but you don't really have to use symbols. You do have to know the symbols though, because aliases are not quite first-class. E.g. you can use `Disjunction` in your code as long as it's correct, but as soon as you pass the wrong type to a method you have to know what `\/` is or you'll get very confused by the compiler error.
We started off with SORM (do not recommend; please avoid) and are in the process of moving to Slick. Slick is somewhat frustrating to get into (and we had to wrap some pieces to get it to have the structure we preferred) but once all is said and done it generates the SQL we want - which is all we care about.
For the server you could certainly use futures - the list of connected clients is shared state, so you'll still have to use a thread-safe datastructure for that. You will need to use an API that allows you to do a "wait on all these sockets and return the first one that receives data" - either native NIO, or something like netty or akka-io (which may lack type safety). Honestly this case is simple enough that I'm not sure you'd even need futures - forwarding the messages to the clients is fire-and-forget and something like `Executor` is perfectly adequate for that (though your async I/O API will probably offer a "send asynchronously" method so you don't need to do it explicitly yourself) - the big advantage comes from switching from an API where you need one thread for each socket to an API where a single thread can handle all the sockets. I wouldn't bother with actors - they're overkill for when you only have one piece of state, and you have to sacrifice type safety for it. If you really want to eliminate the concurrently-accessed shared state, use fs2 (where you thread state explicitly through function calls instead), though be prepared to go a long way down the rabbit hole. For the client, the way many Java GUI libraries are designed you need to have one GUI thread, so you'll need at least two separate "things" (the GUI thread and the thread that's accessing the socket), so you might as well just use the two threads. (Unless there's some way to do Java GUI in fs2 or similar style? I'm not aware of such a thing) Futures are very useful when you need to compose a sequence of async calls (i.e. call a, then call b with the result you got from a, and so on) without having to expend a whole thread for each "chain". But I don't think this task needs them.
I liked the look of Atomic Scala, though it came out after I'd learned the language, so I can't speak to it for actually learning. There are some parts of Scala (e.g. `ClassTag`) that you just have to accept are JVM implementation details, and either put up with them or learn the details of why it's that way. Regarding collections: there are a bunch of specialized data structures mainly for people who particularly care about performance (a demographic that doesn't use Python, or drops down to C or Fortran when they do). Assuming you don't, I'd recommend just using `Seq(...)`, `Map(...)`, and `Set(...)` (Scala doesn't regard tuples as sequences, and in idiomatic scala (almost) all things should be immutable so there's no need to choose between mutable and immutable collections), and writing functions that take `Seq`, `Map` and/or `Set`. For unfortunate implementation-detail reasons ScalaZ requires you to use an explicit subtype of `Seq` (but apparently not for `Map` or `Set`, which seems very inconsistent of them), so if you want to use ScalaZ functionality the best thing I can suggest is to use `Vector(...)` instead of `Seq`, write functions that accept and return `Vector`s, and deal with occasionally writing `.toVector` when you have a `List` or `Array`. Sorry. (Similarly, if you ever get back a mutable `Seq`/`Map`/`Set` from somewhere, just call `.toSeq`/`.toMap`/`.toSet`) Honestly if you don't need to be on the JVM you may be better off with OCaml or Haskell or F#. I love Scala but it's very much a language of awkward compromises, some of which are forced on it by the JVM. If you want to learn functional programming for the sake of becoming a better programmer I'd recommend a more purist language (Standard ML is how I learned) where you don't have the option of just reaching for the OO style solutions. 
So they mean reactive queries? Why not say that?
&gt; horrid hacks like this Thanks for the kind words :\^) 
Yes! Looking forward to it.
I personally think Javascript is much more useful base knowledge in programming Scala than Java is. Scala gets a lot of credit for being "the" prototypical object-functional language, but when you think about it, Javascript had that feature much earlier. Scala's real innovation is in _typed_ object-functional programming. And it sounds like the types are where you're getting hung up, not necessarily the Java-ness. I learned mixins via JS and a lot of functional technique from [Javascript Allongé](https://leanpub.com/javascriptallongesix/read). That made the classic Coursera course and ["red book"](https://www.manning.com/books/functional-programming-in-scala)'s early chapters feel reasonably accessible. Those are both really good ways to start understanding the type system. What I feel is missing a bit in the Scala ecosystem is a good on-ramp into building real software. [Activator](http://www.lightbend.com/community/core-tools/activator-and-sbt) templates are a good way to start with base projects, but the beginner is largely on their own on figuring out how to build out from there.
In a basic free monad you can just have a value in your ADT that represents failure (and have it implement `MyType[Nothing]`). But yeah if you're using a coproduct style then it's another effect in the coproduct.
Another example is routing. With the help of Binding.scala, you can bind URL or hash to a bindable expression, then you can switch your web page via some simple `match`/`case` expressions. On the other hand, ReactJS does not provide routing functionality, you have to use a third party library react-router. Unfortunately, even react-router has less features than Binding.scala. For example, you cannot pass a custom parameter to a react-router managed component. We can see the source code of Binding.scala's TodoMVC to learn how easy routing in Binding.scala could be: https://github.com/ThoughtWorksInc/todo/blob/8fb2839/js/src/main/scala/com/thoughtworks/todo/Main.scala#L41
&gt; He deliberately upsets people (don't even try to tell me that it's not deliberate, no-one could consistently optimise for being taken personally by accident)... The fact that _you_ consistently take it personally does not make it deliberate. Have you considered that the two of you may simply have a personality conflict? &gt; ...when doing so is not necessary and serves no visible purpose. An ethos that Tony and I definitely share is that the facts are the facts and we are not responsible for anyone's emotional reaction to them. The alternative is an _incredibly_ perverse prioritization of people's feelings over the facts, which has no place in _programming computers_. Now, sure, sometimes these reactions can be mitigated by some investment of time and energy in helping people overcome them, but as I've written before, that can easily become a sunk cost, and in my experience—and I suspect Tony's, too—it does too often to be willing to make the investment often. &gt; I think it's fair to characterise that as (seeming to be) "for fun". Well, "seeming to be" is an important qualifier, especially if you consider other interpretations of events. &gt; That he only does it to people he disagrees with does not make it better. You keep saying "disagree" as if the only issues at stake were matters of opinion. 99% of the time when I've seen Tony go off, it's been precisely because people are attempting to debate with him _as if_ the issue at hand were a matter of opinion when it isn't.
Is there a simple way of handling snapshots? That was a pain point I hit in my first foray into Bintray.
In the past, you have a reason to use ReactJS -- server-side rendering for SEO. However, server-side rendering is not necessary nowaday because Google indexes contents created from JavaScript. For example, In this search page https://www.google.com/#q=inurl:todomvc.com%2Fexamples%2Fbinding-scala , the indexed contents in the page summary section like `Written by Yang Bo. Part of TodoMVC.` are dynamically generated by Binding.scala.
I don't know; I only used it for releases. 
React is MV* and removing JSX from the equation gives you a fully programmatic frontend framework with all the advantages of the shadow dom diff-ing. Especially leveraging [ScalaJS React](https://github.com/japgolly/scalajs-react), you have a full Scala webstack. Also, I'm for explicitly separating concerns. So, just like how I dislike angular or JSX for that matter, having logic in your view is tightly coupling your view and controllers, which is imo backwards. Edit: "However, Binding.scala, as an add-on, has more features than the framework, ReactJS". Your add-on has more features then a web framework developed by Facebook which is powering some of the largest web properties currently on the web? Objectively wrong. You have a nice project but it isn't a web framework.
ScalaJS-React has a [full router with a very simple DSL](https://github.com/japgolly/scalajs-react/blob/master/doc/ROUTER.md). It also can be frontend or backend rendered (imo you should be doing frontside rendering).
You literally misunderstand how the State monad works, you've demonstrated this in numerous comments you've made previously. I'm sure I have an imperfect understanding of a lot of things, but I'm not spending all my time talking about things I don't quite grok. 
&gt;The fact that you consistently take it personally does not make it deliberate. He uses emotionally-loaded terms where neutral ones would be easier. He uses more neutral terms when talking to established people. I know what I saw. &gt; Have you considered that the two of you may simply have a personality conflict? Of course. If I hadn't seen a number of other people driven off the channel by the same statements then that would be a plausible explanation. Nor am I the only one saying this kind of thing about Morris. &gt;Now, sure, sometimes these reactions can be mitigated by some investment of time and energy in helping people overcome them, but as I've written before, that can easily become a sunk cost, and in my experience—and I suspect Tony's, too—it does too often to be willing to make the investment often. I'm not claiming he merely states facts without regard to people's feelings. I'm claiming he goes out of his way to be upsetting. &gt;You keep saying "disagree" as if the only issues at stake were matters of opinion. 99% of the time when I've seen Tony go off, it's been precisely because people are attempting to debate with him as if the issue at hand were a matter of opinion when it isn't. Even if he were only upsetting people who got something objectively wrong (which I don't believe - Morris makes mistakes too), it would not be ok.
&gt;What I am claiming is that I have exactly zero examples of him simply attacking a newcomer for being a newcomer. If the claim is "every newcomer he attacked made at least one factual mistake before he attacked them" then... well, I doubt it, but I can't immediately remember counterexamples. But that would not be enough to make it ok. If the claim is that everyone he attacked "took it upon themselves to thought-police a perfectly good answer because they didn't understand that answer, and had a knee-jerk reaction against scalaz, sgainst category theory, and ultimately against Tony." then I have direct experience to the contrary on multiple occasions. I have certainly seen him be the first to "thought-police" a perfectly good answer, the first to give a knee-jerk response, and the first to make things personal. 
I would argue the opposite but ok.
Binding.scala does not force you to tightly couple your views and controllers, nor force you not to. Binding.scala just provides atomic binding expressions. You decide how to layout your code. I think everyone talking about software design should read this Martin Odersky's blog [Simple or Complicated ?](http://lampwww.epfl.ch/~odersky/blogs/isscalacomplex.html). Binding.scala is like Lego Technik Martin mentioned, while ReactJS is like Playmobil. Martin also said: &gt; in the end, compact code can be assimilated more quickly than verbose code. Binding.scala has [a very tiny API](https://oss.sonatype.org/service/local/repositories/releases/archive/com/thoughtworks/binding/unidoc_2.11/8.0.0/unidoc_2.11-8.0.0-javadoc.jar/!/com/thoughtworks/binding/package.html), whereas it is expressive for users. For example, ReactJS's TodoMVC DEMO has 488 lines of code, comparing to Binding.scala's 154 lines of code. 
It's done: https://github.com/LPTK/Boilerless#using-boilerless
While its not ready yet, I assumed akka-http was the spiritual successor to Spray as the highlight or go-to framework. Kinda like how express.js is for Node.js. You're right that there are substantial alternatives from established cos. like Twitter out there. I'm wondering if the Scala community has concerns similar to what the node.js community had with Joyent when they (supposedly) started focusing more on their business than OSS projects and contributions. 
&gt; I don't what from akka-http you're waiting for, but some other people last week seemed to think akka-http was still experimental. So perhaps there's a messaging problem from Lightbend. Last I tried playing with it, the library still had major performance problems, the api for the web service client was a complete mess and not documented at all, and the websocket test client had so many bugs that it was unusable. That was only a couple months ago. I haven't been able to find any evidence of fixes, so perhaps there really is a messaging problem. 
&gt; Is there any hint of maturity around akka-typed? The short answer is No. The long answer is No.
I'd wager that this overhead would only really matter some pretty niche cases. But certainly Scala could improve its dedication to zero-cost abstractions, as is deeply ingrained in the Rust culture.
You're right. Play vs. node.js and Scala vs. ES would be a better comparison. I think I stand corrected on my thoughts on Lightbend's influence. Thanks! :)
It says that variables and not just function parameters can be marked ```@local```. I could see this providing some value in helping to guarantee that local mutability used in an otherwise pure function remains local. Mark the ```ListBuffer``` as ```@local``` for instance. Although that's easy to manually confirm...
Thanks. Would that stop the computation? ( interpretation of following ADTs?). If yes, I dont see it. You mention basic free monad. Is there something more? I wondered last night if it could be possible to implement FreeMonadError.
Every command has whatever semantics your interpreter gives it - you can have an interpreter that stops the computation whenever it hits `SendEmail()` if you like (provided the monad you're interpreting *into* has a way to represent stopping the computation). But by making the failure command return `Nothing` you force any interpreter to stop computing, because there's no way it could provide an instance of `Nothing`. I mean the basic free monad as distinct from the free-monad-over-coproduct style. I think it's worth drawing that distinction. What do you mean by FreeMonadError?
Every command has whatever semantics your interpreter gives it - you can have an interpreter that stops the computation whenever it hits `SendEmail()` if you like (provided the monad you're interpreting *into* has a way to represent stopping the computation). But by making the failure command return `Nothing` you force any interpreter to stop computing, because there's no way it could provide an instance of `Nothing`. I mean the basic free monad as distinct from the free-monad-over-coproduct style. I think it's worth drawing that distinction. What do you mean by FreeMonadError?
I understand the state monad just fine. You and I just mean different things when we say "state" because we think of programs using two different theoretical traditions. It is a well known problem (Leslie Lamport [wrote about it at some length](http://research.microsoft.com/en-us/um/people/lamport/pubs/state-machine.pdf)[1]), that people who are into programming languages, especially those that emphasize denotational semantics or sort of do (not just in real programming languages, but in any of the programming "calculi" of the more linguistic side of theory), have a very hard time discussing state, because they just think about it in a completely different way, one dictated by the particular language's own notion of state. [1]: E.g., he writes "When one thinks only in terms of language, linguistic differences obscure fundamental similarities... Mentioning certain parts of the state has been considered politically incorrect, and methods have been proposed that avoid mentioning them, or even mentioning the state at all."
What do you mean?
But how would you stop the interpreter in that case? Only way i see is throwing exception (since cant provide instance Nothing)... so by impure interpreters (which is fine since its where effects happen anyway i guess) ? FreeMonadError meaning that I could "fail" monad comp with MonadError something along lines for { Xopt &lt;- foo X &lt;- if (xopt isdefined) x.get else monaderror.fail(...) Sorry im on phone commuting right now
&gt; But how would you stop the interpreter in that case? Only way i see is throwing exception (since cant provide instance Nothing)... so by impure interpreters? If you're interpreting into a monad with an error effect then you use that error effect (via `MonadError` or directly). If you're interpreting all the way into `Id` then yes, with an impure interpreter, just as the interpreter has to e.g. execute network requests. The interpreter is supposed to be the point where the impurity goes (or at least the less-purity - often you interpret into `Task` or equivalent, so you still have purity until you actually come to `.run`) You can and should define `MonadError` instances for particular cases of `Free` where your ADT has a way to represent errors. (Same with `MonadTell` etc. - you define those instances if you have a corresponding representation in your ADT). In free coproduct style you can probably do something slightly cleverer where you define an error effect type and then a `MonadError` instance that's available if and only if that error effect is part of the coproduct. I haven't actually done that myself yet though.
In the future, the whole-program linker/optimizer could encode sealed hierarchies in a more efficient format in most cases, I would think.
Akka is the part I would avoid, honestly, at least the actors part. I think it's overhyped, overcomplicated and very rarely necessary for the things it's used for.
I mean I never once heard anyone agree with him about enumerations.
&gt; But also I think the Java 8 support that's the focus of 2.12 is a much higher priority for Lightbend than it is for the wider community (who mostly don't call Scala libraries from Java). So you're not interested in indy for structural calls? Reduced binary and class loading times? A faster compiler from outputting less bytecode? Reduced stack sizes? Symbol Literals? compile time linking? These are all benefits from taking advantage of Java 8 support that doesn't have anything to do with Java interop. To say that 2.12's features are just for Lightbend and Java with no community interest demonstrates you have no idea what you're talking about. 
If it's a the top level you don't need it otherwise see http://stackoverflow.com/a/26081039/449071
holy moly :D thanks edit: I've seen it in Circe on top objects... so I was hoping for some very sophisticated reason :-) edit2: Does anyone think Scala is pretty complicated language? I always find out something that amazes me.
It pretty clearly says in the projects title [FS2: Functional Streams for Scala \(previously 'Scalaz-Stream'\)](https://github.com/functional-streams-for-scala/fs2#fs2-functional-streams-for-scala-previously-scalaz-stream). 
Hi, I'm the author. The article itself is not so important: it just explores in more detail an idea I vaguely outlined in the talk. Scoped continuations -- which you describe well -- is just a name I gave to a form of reified continuations that some (like Oleg Kiselyov) call "delimited continuations with multiple named prompts". I think "scoped continuations" is more catchy. Personally, I think reified continuations of any kind are unnecessary (for 99.9% of programs written by 99.9% of developers); non-reified continuations or, simply, threads/processes -- provided there's a good implementation -- are all that's necessary (when I say threads I don't imply any specific synchronization/sharing mechanism like shared memory or message-passing, mind you, only the notion of having one or more sequential processes). The whole reason why I described scoped continuations at all is just as a tangible demonstration of why continuations (even non-reified) are as mathematically/abstractionally rich as monads, and, in fact, carry some advantages, if you think you need that sort of thing (I don't).
scalaz-stream was a streaming/process library written on top of scalaz. For 0.9 it was ported off scalaz (becoming not tied to any particular effect type) and renamed to fs2. This has the practical advantage that it's not tied to specific releases of scalaz (previously they had to cross-build), but I would guess there might be political reasons too. OTOH it probably involves some duplication of functionality that exists in scalaz. That scalaz-stream looks to be a fork of the 0.8 branch, intending to retain the hard dependency on scalaz. (I would speculate Morris is arguing that the maintainers who renamed the project weren't real maintainers or something. I have heard him make sweeping statements about project history before that turn out to, shall we say, rely on a peculiar definition of the words involved)
&gt; Compared to Python, I'd say that JS is a great deal more functional from the perspective of having first-class functions. How so? Functions are absolutely first-class in Python, passing them as arguments or returning them is common. They're easier to use because the language has straightforward lexical scoping, and a bit more concise (`def f(x):` or `lambda x:` rather than `function(x){}`). No multiple-statement lambdas is occasionally irritating, but functional style never uses statements in the first place. &gt; Python goes through lot of trouble to keep people thinking mostly procedurally. By comparison, function expressions and higher-order functions are extremely common in real world JS programming. I don't think that's at all true. Python has proper `map` by default, list comprehensions, and a whole load of functional utilities in the standard library. Python is a broad church and there are some styles that don't make much use of the functional aspects, but that's if anything even more true for JS. &gt; JS only recently got classes, but it's had prototypal OOP facilities forever, as an underlying mechanism for implementing class-based OOP and also more trait-like patterns. Prototypal inheritance was always weird though; most of the people I knew either avoided OOP style entirely, or used a library to emulate "proper" OO. Either way traditional OO wasn't really first-class. &gt; But I still think modern idiomatic JS is typically programmed in a much more functional manner than the other popular dynamic or OO languages. Not my experience. There's a subculture in which functional style is idiomatic, sure, but I think comparable subcultures exist for Python/Ruby/etc., and those languages are if anything more suited to it.
I think akka has very decent documentation, explaining how things work under the hood etc. Play not so much. Anything particular you dislike?
I too dont think the difference between 2.11 and 2.12 is good enough for me that would warrant me looking forward to it. Don't get me wrong, I appreciate it, but look for example at C# releases. The new ones always bring so much nice stuff that it made me geniunely look forward to it and use pre-releases to play with.
Interesting. What would be then reason _not_ to use fs2? If it can be used with both scalaz and cats and seens to be actively maintaned (scalajs coming Ive heard). I guess only if you're bothered by "shims" (fs2-scalaz) and you're 100% scalaz shop...?
I can imagine some of the APIs can be nicer by being specialised to the ScalaZ types. Maybe less indirection in some cases. But I suspect it's mostly political.
It's not, I'm not actually sure where the code for it is but I'll try to find it. It's no longer a great project for a beginner, it changed quite a bit from a simple main function to where it now uses Akka to multi thread.
Not neccesarily limited to scala, but how would you use the async mongodb driver for scala? currently in my toy project I'm just doing Await.result(observable.toFuture(), Duration(10, TimeUnit.SECONDS)) but I suppose this remove all benefits of doing async work. What would be better? * returning the observable * returning Futures * returning a scalaz Task * something completely different edit: As in, how would you create something like a "repository"
Functor and Semigroup come with laws that those operations must satisfy. So, for example, subtraction does not form a semigroup, because it isn't associative. I agree the names aren't that helpful, but they do have a particular semantics that isn't easy to capture in a catchy name. So "Functor" and "Semigroup", which already have known meanings in mathematics, aren't terrible choices.
Offtopic: you may be interested in https://github.com/spray/sbt-revolver It forks the JVM so killing an instance is not far from `kill -9`. Or maybe the simpler sbt-s `fork := true` will do for your use case?
You're completely right. In a perfect world that would be the best thing to do. But ScalaZ and all advanced functional programming techniques have the reputation to be obscure. Why do you think the [play framework prefered reimplementing type-classes](https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.libs.functional.package) instead of directly using ScalaZ? Yes functional programming scares people. That's unfortunately a fact.
My point is not to detract away from Python's functional capabilities, but to point out that modern Javascript isn't so different in technique from Scala programming, setting aside the major difference in type system. I'd argue it's just as easy, to learn to work with the types coming from that background than it is to learn functional technique coming from Java, with all its boilerplate. One of Scala's principle achievements is letting people program in the flexible style and syntax of dynamic languages, but with rigorous typing.
I think the point that /u/acjohnson55 was making is that modern JS development does use techniques which are more typical in functional languages compared to Python (for example) As an example, Promises/Future's are currently considered the idiomatic way to handle concurrent code and it happens that most of these implementations follow a monadic pattern (however not always correctly). Same deal with rX (reactive extensions) which also follow a functional pattern. In python you typically see a more OO/imperative design (probably for similar reasons as Java, it put an emphasis on "classes"). Its not really a comparison of features (i.e. what the languages are capable of) but more of how the code is idiomatically written. Most python code that I read seems to resemble a less verbose version of Java (with some extra sprinkles,i.e. tuples), where as most modern Javascript code is actually a bit closer to clojure with algol syntax/dynamically typed scala (mainly due to the lack of "classes" that javascript had). Obviously its not exactly the same, but I don't think the comparison is completely inapt
Anybody care to compare this to enumeratum? I just barely started using it and I don't think I can really get the feel for the comparative benefits of the two. 
We might be in danger of veering off into semantics, but I view "perception" and "culture" as a part of software engineering, simply because programming is ultimately as much about communication between people as it is between people and a machine.
&gt; Making it personal is for people who know they're wrong. Says the person claiming to speak for the community over "subjective feelings" while simultaneously spreading misinformed half truths about the current roadmap. There's nothing personal about claiming you don't know what you're talking about, because you've demonstrated pretty clearly that you don't.
https://github.com/tpolecat/tut
You certainly can write Scala pretty Pythonically! I think our difference in opinion may boil down to a wide variation in what constitutes idiomatic JS (perhaps less so Python, given its stylistic monoculturalism) and an even wider variation in what constitutes idiomatic Scala. I haven't written Python professionally in about 3 years. But I very frequently use both JS and Scala, and my JS style is very similar -- even more so now with ES6. While Scala usage has certainly had a big influence here, I also attribute part of my migration to very functional JS usage to modern JS writings, like [Javascript Allongé](https://leanpub.com/javascriptallongesix/read), and working with libraries like [Redux](http://redux.js.org/) and [Ramda](http://ramdajs.com/0.22.1/index.html). I read Javascript Allongé before learning Scala, and it was the first book to convince me that JS actually does have a soul, and that soul is FP. I began adopting that style and learning Scala helped hammer it in. And while Javascript didn't _innovate_ anything in its early days, I think it's fair to say that it gave (and gives!) a _huge_ swath of the dev world their first exposure to first-class functions, HOFs and closures.
&gt; And while Javascript didn't innovate anything in its early days, I think it's fair to say that it gave (and gives!) a huge swath of the dev world their first exposure to first-class functions, HOFs and closures. I mean that's not false, but phrasing it like that suggests this is something distinctive about Javascript. Basically all languages have those things now, no? (I guess functions are not *quite* first class in Java or C).
/r/tpolecat
Major enhancement include: * Scala.js support * Async style tests * users who compile with -Ywarn-value-discard will not get warnings for tests that end in an assertion or matcher expression * Enhanced Matchers for collections * (and [more](http://www.scalatest.org/release_notes/3.0.0#highlights))
That's not a subreddit (yet?) I believe you want /u/tpolecat
How does this compare to mobx? The concept feels very similar. Does it support transactions?
MobX's concept looks similar to Binding.scala . But I found that almost every MobX's feature can be done with one same API in Binding.scala. I did not understand why MobX has so large amount of API.
Does anyone have a good link or tutorial about how to write Modules in Play Scala? Everything i've found so far is about Plugins but they've been deprecated.
Anorm. It seems to be the best of the JDBC wrappers from my limited testing. ORMs are stupid. Source: I don't like ORMs
How can ScalaFX achieve an API like this object HelloStageDemo extends JFXApp { stage = new JFXApp.PrimaryStage { title.value = "Hello Stage" width = 600 height = 450 scene = new Scene { fill = LightGreen content = new Rectangle { x = 25 y = 40 width = 100 height = 100 fill &lt;== when(hover) choose Green otherwise Red } } } } I can see in their code in the `Stage` class they have def scene_=(s: Scene) { delegate.setScene(s.delegate) } but when I try and make a class like this class Foo { var myString: Option[String] = None def string_=(string: String) = myString = Some(string) } val foo = new Foo { string = "hey" } I get an error Error:(7, 4) not found: value string string = "hey" ^
Assignment methods are only possible when there's an accompanying `def` with the same name. That `def` must have either no parameter lists, or exactly one implicit parameter list. @ class Foo { private var myString: Option[String] = None def string_=(string: String) = myString = Some(string) def string: Option[String] = myString } defined class Foo @ val f = new Foo() f: Foo = $sess.cmd2$Foo@9504a58 @ f.string = "test" @ f.string res5: Option[String] = Some("test")
is it fine to use TASK as IO monad? Half of the functions then end up being `-&gt; Task`... Feels weird.
Thats what I tried with Freek library but its quite a lot of boilerplate and compile time is growing quite a lot when mixing algebras and interpreters and programs :/
Freek etc. are new and fairly experimental, and IMO you only need that flexibility if you're writing some kind of middleware, certainly not for a single application. I'd try using a conventional free monad with a fixed ADT first.
Then my question, how does play.api.libs.functional.Applicative[F[\_]] manage to scare the Java community less than scalaz.Applicative[F[\_]] still stands.
Only what you're aware of can scare you. Functional programming is reputed to be difficult. But, in order to be popular, Play needs to be seen as a simple and accessible framework for everyone with an easy-as-possible API. So Play have a very simple Java and Scala DSLs which requires no background from you. Internally they can use what they want. User generally do not look the code anyway. But they do look at dependencies. Having the jar file *scalaz-core-xxx.jar* in the classpath is much more visible than the well hidden package [play.api.libs.functional.package](https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.libs.functional.package). Perceptions are perceptions ... Nowadays, the HotSpot and OpenJDK JVMs are very fast but i'm sure you can still find people that will tell you it was a bad choice of yours to develop on the JVM because of its supposed slowness.
Offtopic here: &gt; no idea what you're talking about. &gt; Making it personal is for people who know they're wrong. That's just an unfortunate cultural difference. I observed that English-natives find this especially offensive, while for me it's just sub-equal form of "You are poorly familiar with the topic". :) Out of curiosity, would that offend you?
Yep that's what I'm [talking](https://www.reddit.com/r/scala/comments/4xt8km/weekly_scala_ask_anything_and_discussion_thread/d6mj2o2) about. 
Does anyone know a good lib for DB testing but integrated with ScalaTest? something like DBUnit (Spring DBUnit integration) etc?
&gt; putting all the queries in a singleton that handles deserialization has worked out I'm interested to learn more about this approach. What is your typical return type? Perhaps `Future[Either[Throwable, Person]]` or without the `Future` wrapper? Why did you decide to put all queries in a singleton? 
This is great, thank you. 
Interesting! Quite rare nowadays to see the sources only available as an archive and no documentation/tutorial accessible from the main site.
That is one of the many examples I tried to emulate and didn't work. I'm running the app through the play built in engine, nothing fancy. Just activator run. As for enabling the ScalaPlugin, I did do that at one point ------------------------- // inside sbt for index lazy val index = project.in(file(".")).enablePlugins(PlayScala) ---------------------------- but for some reason each submodule became a sort of carbon copy of the main project. That is each module (index or register) received automatically their *own* module folder with another index/register. 
I don't understand, could you explain how this applies to Monix?
Scala.js is definitely one of the best options for browser apps. The only other nice options I've found is Purescript (very nice functional language) or Elm (pretty nice functional language too). However they both lack in terms of libraries, tool support, server side support etc. compared to Scala.
So in https://www.playframework.com/documentation/2.5.x/SBTSubProjects#splitting-the-route-file the example is: name := "myproject" lazy val admin = (project in file("modules/admin")).enablePlugins(PlayScala) lazy val main = (project in file(".")) .enablePlugins(PlayScala).dependsOn(admin).aggregate(admin) Notice how the sub project "admin" has `enablePlugins(PlayScala)`. For every Play project, whether its a subproject or not, you need to enable PlayScala. So you need to change: lazy val index = project.in(file("modules/index")) lazy val register = project.in(file("modules/register")) to lazy val index = project.in(file("modules/index")).enablePlugins(PlayScala) lazy val register = project.in(file("modules/register")).enablePlugins(PlayScala) 
Yeah, there are some blog posts and talks here and there, but it's not very systematic. This is probably the reason why people don't dive into this stuff, which is a shame because it's not too difficult. A book that can be read from cover to cover may change things :)
I like the idea of creating something dedicated to do streams. Not a streaming library based on some other library. Does anyone know something about performance of Monix? Or is ability to cooperate on multiple machines?
What a coincidence! _Documentation_ section on the right side of the main page must have been added _really recently_. It contains tutorial and SBT usage. Just below documentation is also a link to GitHub repo. :)
I found this guy has the pdf to the instructions on github: https://github.com/vasnake/Principles-of-Reactive-Programming
tried both versions, did not work, same error message :(
Yeah I did. Same error message.
This is like a simpler graphql, interesting for sure.
I've been following Monix for some time now, and it's looking great. The "Scheduler" class makes way more sense than the previous ExecutionContext/ExecutorService approach, and the referentially transparent API but mutable internals give a nice balance between performance and reasonability.
Yeah that's exactly what I was thinking. Much easier to integrate but still giving some of the benefits
Is there another way to configure classes but annotations?
Have you tried by chance to contact the Coursera help center? I will try 
Not right now, but I'm open to suggestions! What would you want instead?
I just don't like the fact I have to couple my domain model case classes with annotations.
Would a `@ExposeAll` annotation on the class work well for your use case? The other option would be something like `implicit val myLoadable = Loadable.exposeAll[MyUnannotatedClass]`
Fwiw, the ["our customers are hiring"](https://www.lightbend.com/customers/our-customers-are-hiring) page on lightbend.com currently lists four companies hiring for NYC: Zocdoc, HBC Digital, Bloomberg, Two Sigma. 
Shoutout to my team at bloomberg: https://careers.bloomberg.com/job/detail/49254
Meanwhile, some popular statically-typed languages don't even have type parameters at all. And people *like* them, inferior type system and all. SMH.
This is a very confused article. I have seen those claims about Scala's infix notation.
Excellent article! I regret that there so few languages supporting type-constructor polymorphism. By the way, does OCaml have these days a nice syntax for Higher-kinded types (something simpler than the encoding in [Lightweight Higher-kinded Polymorphism](https://www.cl.cam.ac.uk/~jdy22/papers/lightweight-higher-kinded-polymorphism.pdf)) ?
But still I'd have to mix algebras, right? Like UserRepo and TodoRepo or something... but I don't need Freek for that. Another pain point with Freek is that I don't understand it and I find it difficult to use something that is totally beyond my abilities...
If you want to have two separate sets of commands, and express that separateness in the type system, and also need to interleave those commands to run some construct that uses both kinds in in ways that depend on each other, and want to preserve full independence between the command-set types, then you need something like Freek. In practice you can often do something like allow one type of command to embed the other type (there are times when this is the principled thing to do e.g. "an async command is either a database command or ...", but there are also times when you use it as an escape hatch). Or you can often carry the separation up to the top level - e.g. if you're just combining results from both in a web response, at the web-service level it's fine to just call both interpreters - as long as the business logic for the user components and the business logic for the todo component are separate, you're going to test them separately, so that's where you get the advantage from using the free monad. As always, remember why you're doing this. I'd absolutely agree with not using things you don't understand. In the case of Freek I wouldn't even try to use it until you're familiar with the traditional fixed-ADT free monad. (I'm actually in the process of writing my own freek-like library, paperdoll - I can't say I understand it all myself yet, but it's been a helpful exercise. But one that only made sense to do once I had real programs that used multiple different free monads and was looking for a way to make it easier to combine them)
Or you could join us: https://cakesolutions.bamboohr.co.uk/jobs/view.php?id=6
ClojureScript is quite mature and has a ton of libraries, as well as excellent Js interop.
This makes a lot of sense and helps me to solidify my understanding of hows' and whens' . Thank you.
&gt; Does Scala have the Principal typing theorm? If by that you mean "is there a supertype of all types", then yes, and it is `Any`. Read [Unified Types](http://docs.scala-lang.org/tutorials/tour/unified-types.html) for an overview. &gt; What about traits - objects to achieve the concept of "type classes" as in Haskell? Scala has a notion of [traits](http://docs.scala-lang.org/tutorials/tour/traits), yes, but a trait is not by itself like a Haskell typeclass. For that, Scala has the [typeclass pattern](http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html), which relies on Scala's implicit resolution mechanism.
I've been thinking about something like this via shapeless records for a while. Arguably they're the "right" way to express something like JSON (and something like spray-json-shapeless can generate formatters for them completely automatically but safely and without reflection), and there are simple/natural ways to lift a `case class` to a record for the simple case and express "manual" record types (and then align a case class instance to that) for the cases where you want a subset. It's very much compile-time-only though - I can't see any way to do a truly dynamic subset with that approach.
I need as much static type checking as possible to find my mistakes :)
Unfortunately in Scala itself you hit the same barrier as soon as you want to go up another level. There's no way to write a "kind-polymorphic" type or function - more than once I've found myself literally copy/pasting code just to add `[_]` everywhere. You can't, AIUI, expose the concept of a category directly to write code that is "category-polymorphic". If we believe that the ability to abstract to higher levels is important (as this article seems to), then Scala doesn't give a great account of itself.
&gt; Given that both hygiene and joint compilation still require &gt; significant research to materialize, we decided not to include them &gt; in this proposal. We think that even without these features I've just read the whole proposel to know if joint compilation would've been possible... But it's good to know that the idea is still there.
We explored two potential ways to achieve joint compilation: A) "JIT compilation" of macro impls, where the macro engine would typecheck the macro impl and all its dependencies, then extract all these trees into a separate compilation run and finally compile that run. B) Interpretation of macro impls, where we'd go through the tree nodes constituting a macro impl and interpret them on the fly, typechecking the dependencies if necessary. Here are the problems that we encountered: 1) Both approaches are not at all trivial to implement (i.e. we don't have stable versions of either JIT compilation or interpretation of ASTs), but that's not the only issue. 2) The main challenge here is ensuring that the trees that we're compiling/intepreting are typechecked (because without typechecking we won't be able to execute them). Due to the way how the Scala compiler works internally, at the moment when a macro expands, other parts of the program may still be untyped. Therefore, going through the call graph of a macro impl is not that simple. 3) We could postulate that macro expansion happens after typer. By that time, all trees in the program are already typechecked, so the problem #2 gets taken care of. However, that makes it impossible to do whitebox expansion, and that is quite problematic (as explained in the "Losing whiteboxity" part of the SIP). To sum it up, we're actively exploring the design space, but so far we have more questions than answers. Of course, we also welcome contributions, so if you'd be interested in helping us out, please drop me an email!
My solution has been to break things down into small independent components that I can reason about individually. The app will combine these pieces in a way that it needs to solve high level problems. I find that the functional style and immutability make it quite natural to write code that's not coupled. When I use a library, I don't need to know its internal implementation details. I just need to know the API that it exposes. Whether you use static or dynamic typing, it will be difficult to reason about a large monolith app. So, the idea is to break it up into pieces, that each deal with a particular problem. Ideally, these will also become reusable libraries that you can use for other projects. I strongly believe that code should be understandable without static typing. There are other benefits to static typing, such as refactoring, but it shouldn't be used a crutch to write code you couldn't maintain otherwise. 
I really like [Polymer](https://www.polymer-project.org/1.0/docs/devguide/feature-overview) But I don't really understand enough about sbt, ScalaJS, and server side frameworks to work out the best way to integrate it into a new project. It would be great to be able to create reusable web components, that use ScalaJS, and were able to https://github.com/lihaoyi/autowire between server and client. From the outside it seems like it would make it almost as nice as developing mobile apps / JavaFX applications. Any help or pointers on the best way to set this up would be appreciated. 
I don't know a lot about Polymer. You should try [scalajs-react](https://github.com/japgolly/scalajs-react). You can get started with this [template](https://github.com/MasseGuillaume/ScalaJsReactTemplate).
It is indeed a type constructor of kind `* -&gt; *`. But you can't use that type constructor (higher-kinded type) in Java; you can't alias it, you can't parameterize a generic method with it, you can't even write it down. (You can write `List&lt;String&gt;` or `List&lt;?&gt;`, and for backward-compatibility reasons you can write `List` to refer to a type very much like `List&lt;?&gt;`, but you can't write List in a way that lets you later apply it to `String` to get the type `List&lt;String&gt;`).
I haven't read your Pre-SIP yet, I will in a bit, (but I have read your earlier meta musings). But would it be possible to maybe split the space up a bit into expanding the cases that don't care about the types of their arguments and don't use code from the current compiler run expand? A lot of macro use boils down to emitting a tree of code that needs (typically for performance reasons) to be inlined at the callsite. I am thinking things like the spire [cfor macro](https://github.com/non/spire/blob/master/macros/src/main/scala/spire/macros/Syntax.scala#L137). But the basic rule is that macros that do not inspect their arguments in any way, or use local code, those macros don't require a separate compile. I understand that even the spire macro i listed sort of breaks this because it calls `util.{clean,name}` methods. But if that method was defined in a separate compilation unit (e.g. CommonMacroUtils) perhaps it would all work out to be ok.`[1]` Then as you iterate and improve the macro engine this restriction can be relaxed to include more and more cases, maybe even someday the whitebox ones. And just to provide a counter-example this excludes stuff like macros used for serialization libraries, which inspect a case class to get it's arguments, or macros that generate a list of enum members from an enclosing object those would still require the separate compilation. `1`: Because all macros currently require a separate compilation unit anyhow, I can't say how unfeasible that separate compilation constraint makes writing macros in the wild, and how useless it may make this suggestion.
Yes exactly! I have about `inline` in earlier scala.meta posts you have written. It really caused me to see macros as being able to be categorized in 2 ways do they use reflection or not? (N.B. there are additional ways to do the categorizations as well e.g. whitebox/blackbox) Is there any chance that can be walled off to emitting within the same compilation run? Typically performance stuff is really just braindead simple copy pastes of simple patterns. For example you can't use .foreach() ... well you could if it was an inline macro ... but I digress, so you have to write a while loop instead. Now the code is immensly less clear, but the performance is up to snuff for your business requirements. But you've dinged the maintence complexity counter. 1. The code is harder to read at the callsite, and 2. It's harder to fixup en-masse and requires, literally, text searches. A lot of these uses aren't something global like pow, but rather some business domain object has a couple of clients and you don't want to write the same logic 2 or 3 times in each of the clients if you can avoid it. 2 or 3 isn't a lot but it adds up rapidly over the course of a large project. Granted macros have a ton of other necessary uses, for things that boil down to emitting code based on compile time reflection, e.g. autowire, serialization, database stuff, enums etc. However there is a ton of low hanging very valuable fruit in inline, and having that work in the same compilation pass is needed when dealing with domain objects because you can't just keep spitting off more and more projects for each level.
Why drop `warning` and `error`? Is it not useful for a metaprogram to be able to issue these? For instance, a macro that takes non-Scala code as a string literal, compiles it, and emits corresponding Scala would need to be able to issue errors and warnings.
When can we expect a stable macro implementation and proper documentation for it?
If you're fairly comfortable writing scala as a 'Java++', I'd give [The Red Book](https://www.manning.com/books/functional-programming-in-scala) a try. It's a challenging book coming from Java or other imperative/OO languages, but very rewarding. As you make your way through the book you'll end up deriving a lot of 'functional' programming tools/utilities. Optionally you could dive in to using them. Check out https://github.com/scalaz/scalaz/
Seems like exactly what I need. Thank you!
I've defined an implicit monoid for a custom type in a trait. The companion object makes use of this implicit. How can I make use of this implicit in a completely different class? I've tried importing the trait, I've tried putting the implicit on the companion object. I currently have this implicit defined twice, once in the trait, and once in the class that needs it.
You'll have to be more specific. `_.toString` ? `{d: Double =&gt; f"$d%.3f" }` ?
Well, my use case is mostly hypothetical, so far. I've been idly pondering what it would take to reimplement XML literals, using a macro that parses the XML at compile time and emits Scala code that, at run time, generates a corresponding sequence of StAX events. Such a parser may find errors in the input (unbound namespace prefixes, attribute values that don't conform to schema, and so on) in several places. If it has to abort immediately, then it can only report one of those errors at a time, not all at once. Speaking of XML literals, another thought I had was that it should scan the enclosing scopes (method, class, package object, etc) for namespace bindings declared by Scala annotations (e.g. `@xmlns("example", "http://example.com/example-namespace")`). This would be very useful, as it would save the programmer the trouble of writing `xmlns:example="http://example.com/example-namespace"` in every single XML literal. With `scala.reflect` macros, this can apparently be done using `Context#internal.enclosingOwner`. Will `scala.meta` offer some way of examining annotations on the surrounding scopes like that?
For the record, there is a brand new proposal in the works to introduce a new proper `inline` keyword in Scala as part of a new macro system. See [SIP-NN: Inline/meta](https://gist.github.com/xeno-by/9d7a709b1ba7c2ee64cfedcc5d264bd5).
This is nothing special about lift-json or AnyVal. This is the simple consequence of the fact that `Option[A]` and `Option[B]` are the exact same type at runtime due to the type erasure and you can cast between them without any runtime issues until you try to pull the content out of them: scala&gt; Some(1).asInstanceOf[Option[String]] res0: Option[String] = Some(1) scala&gt; res0.get java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String ... 33 elided What happens above is that the `read` method is generic, so all casts to `Bar` inside of it do not happen at runtime, and the compiler finally casts the `Option` returned by it to `Option[Bar]` to match the context it was called in. Now, the only problem with lift-json there might be is that it ignores the provided return type (it gets passed to the `read` method in a manifest) and tries to guess what type of object you want instead of using the provided type hint.
I don't think anybody can tell you *when* exactly, but it might be useful to know that there is a brand new proposal for macros in Scala: [SIP-NN: Inline/meta](https://gist.github.com/xeno-by/9d7a709b1ba7c2ee64cfedcc5d264bd5). This is just an external point of view, but I would expect it to take quite a bit of time to become fully stable.
I know about the deal with `Option[A].asInstanceOf[Option[B]]`, that's just a consequence of erasure. The problem that worries me a lot with `read` is that it creates naked `A`s references pointing to `B` values. In your example that would mean that `res0.get` actually gives you a reference of type `String` to a value of type `Int` and fails only when you try to assign it to an `Int` or call `toLowerCase` on it. That can't just be explained with erasure as far as I can understand; I'd like to see with a minimal example that re-creates the behaviour with vanilla Scala code on a REPL but I can't re-create it no matter how I write the code.
I would find the source code for the stream in current Slick, then see if it exists in 2.0 github land. Perhaps it moved and you can't find the same code, but should get you 90% there...
Well, if `A` is a type variable, there's nothing the runtime can do to prevent storing any kind of reference there, it will only be actually cast when getting assigned to a monomorphic variable of an incompatible type. The following works fine: def f[A](z: Any):A = z.asInstanceOf[Option[A]].get val o: Any = f[String](Some(1)) but this will fail: val s = f[String](Some(1)) // s is inferred to be a String Compiler emits a `checkcast` instruction whenever it is necessary, so that the JVM validates the class file, but it doesn't add it everywhere. In the above example, assigning to `o` would always work, so `checkcast` wasn't emitted. There is nothing in your example that works differently. It's a problem with lift-json that it ignores type hints, you won't get a `ClassCastException` inside `read`, because everything is erased in there and any crash will happen in your code as soon as it will attempt to cast to `Foo`. The code you posted did not: the first `println` call casts `Foo` to `Any` (despite at compile time looking like a cast from `Bar` to `Any`), the `Option.apply` call casts `Foo` to `Any` (again, despite looking like a cast from `Bar` to `Any`), assignment casts `Option` to `Option`, the second `println` call casts `Option` to `Any`. Scala's type system does not give any hard guarantees when `asInstanceOf` over generic types is involved. Neither does Java's, this is an inherent problem with JVM generics.
Because, except for Spark, Scala really isn't mainstream in most shops and would just flat out not be allowed.
Open a ticket on their github asking how to do it?
There are people who pick up Scala to create property based tests for Java code, basically introducing Scala to their shop through the testing backdoor
I think due to Odersky and his Coursera course, there's plenty of folks who are or have used Scala to learn FP, specifically so they can do that and stay within the Java ecosystem.
final https://en.wikipedia.org/wiki/Final_(Java)#Final_variables
PM me. I work in a small hedge fund in NYC. We use Scala and do bunch of ML work.
Immutability! Before Java, I did a lot in C++ and I'm still not sure, if immutability everywhere is a better solution than const correctness, since copying object graphs isn't exactly cheap. But I learned from Scala, that it's not so expensive as I thought, at least in most cases.
I had no idea you could import implicits from an object like that. Very cool, thank you!
It's great to see who's invested in Scala now, and that this group is steadily growing. But also: &gt; we should balance OSS &amp; commercial Yes, please!
This is very true. Companies only want one language to deal with if they can help it.
You can import any member, be it method or value, from any object.
Can't quite catch your meaning; are you saying that you've implemented a custom type as a trait (`trait CustomType`), or that you have a trait that contains a custom type (`trait A { class CustomType { ... } }`)? If the former, you can put the implicit monoid instance in the trait's companion object and get it anywhere by importing `CustomType`; if the latter, I'd check if the custom type really needs to be dependent on instance objects of the trait, or if it just needs to be _associated_ with the trait somehow, in which case I'd put it in the trait's companion object, and then put the implicit monoid instance in the custom type's companion object: trait A { ... } object A { class CustomType { ... } object CustomType { implicit monoid: Monoid[CustomType] = ... } } Then you can get it anywhere by importing `A.CustomType`. The trick in all this, as /u/m50d [pointed out](https://www.reddit.com/r/scala/comments/4z07r0/weekly_scala_ask_anything_and_discussion_thread/d6t30pi), is that Scala automatically brings all the type's implicits into scope if you have them in the type's companion object and you import the name of the type.
I learned a number of useful things from Scala: - Immutable objects are easier to reason about than mutable and have good performance in many cases. - Testing is actually fun in the absence of side effects. - Nulls suck and Option is a good alternative that you cannot forget to check, thanks compiler. - Generics are very useful. Stuff like F-bounded polymorphism helps to share code among classes yet making sure they stay separated from each other, which is not quite possible without it. - Lazy evaluation for function parameters can come in handy. Speaking of lazy evaluation: was developing a browser-based document editor at my former workplace in Typescript / Java, and at one point we had to implement a feature that didn't fit into design of our rendering/layouting at all, which was also quite messy by that time. We took a month to do a complete rewrite. We separated rendering and layouting, making rendering really trivial and layouting was done mostly using small and pure functions with immutable objects in large quantities and a quick port of Scala Stream (lazy cons list). Resulting version was much more testable and very flexible, and had surprisingly good performance even before any optimizations. It took a day to implement required feature to its full extent and cut down implementation time for later ones, much easier to debug (you can drop stack frames as much as you want if your functions don't have side effects). Also, before Scala I switched from Java to Python and was astonished by how little code was required to actually get stuff done. Tried Scala later and was surprised by the fact that it's about the same amount of code, it doesn't suck despite being strongly typed, which was an important shift for me.
One trivial example from lihaoyi's recent post about Scala.js was that the Java behaviour of `String#split` is crazy (dropping 0-length results), but Scala.js implements the same behaviour for compatibility. Obv. one wouldn't just change `String#split` - to justify breaking compatibility one would need to write a full replacement for `java.lang` and probably other things, which would be an enormous undertaking.
&gt; Scala is a language that is mostly learned by devs who are using it in the industry Why would that be? &gt; people who want to learn pure functional programming... What does that have to do with anything? &gt; The people who tend to pick up Scala do so for the purpose of using it. What does that have to do with anything? You mean people learn Haskell without the intention of using it?
I learned that I no longer liked Java.
Immutability is interesting. I definitely use it for small "value types" (like points, vectors etc.) in all languages, but to use it pervasively for all application data is very cumbersome if you don't get help from the language. In Haskell (and Scala to some extent) you can do it, but I just wouldn't bother in Java, C++ or Rust. Performance is another issue. If performance matters (and it usually does) I tend to use immutability where I know I won't take a significant performance hit (again mostly value types but not collections for example).
Any kind of high volume systems: data processing, app backends and webservices, real-time decision systems and so on.
Indeed generic classes are type-level functions but are not expressible as such in Java. Generic type parameters must in Java must be fully applied types.
Can you elaborate? Thinking of it, `copy` methods, `case classes` and maybe lenses (which are made possible by the typesystem) help us with immutability and big datastructers. Are there more points that I'm missing?
I would prefer that aswell. scala-native and scala.js do the same thing which android did and why they now be a target against copyright. even if it's fair use, oracle has still wrapped his hands against us.
especially a senior engineer? that makes no sense. A senior engineer in XYZ is a junior in scala?!
&gt; Why would that be? JVM interop, seen as an "easier" and more approachable language than Haskell. For Java devs, you could ease your way into Scala and start using more and more functional idioms. &gt;What does that have to do with anything? Just saying Scala is not the usual choice for people that purely want to learn pure functional programming, and as such, is mostly picked up for usage in industry. &gt; What does that have to do with anything? You mean people learn Haskell without the intention of using it? I'm not saying people don't pick up Haskell to not use it, but there's a large amount of people that learn Haskell purely to open their minds. There's also a whole lot less shops out there that use Haskell compared to Scala.
None of what you say explains why Scala wouldn't be learned by people who are using it outside of industry. &gt; Just saying Scala is not the usual choice for people that purely want to learn pure functional programming, and as such, is mostly picked up for usage in industry. That's a blatant non sequitur. There are plenty of people who learn functional programming with other languages and plenty of people who learn Scala for other reasons... &gt; I'm not saying people don't pick up Haskell to not use it, but there's a large amount of people that learn Haskell purely to open their minds. There's plenty of people who learn other languages purely to open their minds, and what people do with Haskell has nothing to do with whether or not people learn Scala just for the job.
&gt; None of what you say explains why Scala wouldn't be learned by people who are using it outside of industry. I never said that, I said it's mostly learned by people who are using it in industry. The ratio of people learning it for their job instead of for purely academic reasons is much higher than most other functional languages. &gt; There are plenty of people who learn functional programming with other languages and plenty of people who learn Scala for other reasons... Sure, but compared to other languages, Scala is learned more for industrial usage. Almost everyone who wants to learn FP goes with an ML family language or a Lisp-like language. The vast majority don't start with Scala. 
I had not done any Scala previously, and I managed to complete the first two courses (Functional Programming Principles and Functional Program Design) in less than a week. I liked the way the material was presented and I think the assignments were great. What are your expectations? The second course is relatively short, but it _is_ pretty thorough.
How can I collapse Seq (Seq (1,2,3), Seq (4,5), Seq (6,7,8)) into Seq ( Seq (1,4,6), Seq (1,4,7), Seq (1,4,8), Seq (2,4,6), Seq (2,4,7)... Seq (3,5 8))? I'be tried a few solutions, but nothing works. I'm convinced that the answer is recursion, but I can't figure it out.
I submitted the paper this is based on a couple weeks ago. Not intending to spam the subreddit with Reactors stuff. I haven't used it at all yet. I'm just really interested in seeing more discussion around the merits of the project, especially given the momentum behind Akka. Really curious whether this represents something that will eventually eclipse or be subsumed into Akka, or what. At the very least, it seems like an interesting fusion of message-passing and stream concurrency models, which seem to be duals in some sense.
I got about halfway through the FPP course a couple years ago before I dropped it because of similar reasons (and time). I felt like I spent hours coming up with a solution that was terrible (compared to his) because I didn't have the insider knowledge about the problems that were being solved. My solutions would regularly be 15 or 20 lines long and his would be about 1 to 3. I never felt like I was actually learning why his solutions could be so short because I didn't have enough familiarity with the problems I was solving. It ended up being discouraging more than anything. It was as though being out of the academic programming world for so long was a hindrance. Business software is defined so much differently (if at all) that I found myself programming for edge cases that didn't exist if I had understood the problem and the language better.
The intro course is easily one of, if not, the best on coursera. It was the first online course I took and I loved it so much I took a few more coursera courses and was completely disappointed. The rest just felt like garbage made by people who had no idea to adjust to the online course media. The reactive one is still one of the better courses but its not as good. I heard they have a second one that might be a replacement to the reactive course. Need to look into it.
Working as a backend dev involves more work than just writing Scala code. There are a ton of auxiliary skills, both hard and soft that take years of experience to develop. 
Did your eat n sleep that week?
This is a very cool project, not spam at all. It looks incredibly useful. I've been using a very watered down version of something similar for some of my work but I'm going to have to give this a go.
Here's what I came up with for non-empty lists. It's ugly and probably performs terribly but I think it does the job. def orangeconeconspiracy(ss: Seq[Seq[Int]]): Seq[Seq[Int]] = { ss.drop(1).foldLeft(ss.head.map(Seq.apply(_)))( (acc, l) =&gt; for { a &lt;- acc i &lt;- l } yield { a :+ i } ) }
I am taking this course right now (3 weeks behind) and here is my opinion: As first course, that give you almost no knowledge about Scala itself as a language, instead this short course dive a little deeply to some FP concepts. To be honest, i like this course much less than previous: if first course give you basic understanding of FP from zero, that course do not dive into given ideas very deep. Assignments pretty the same, where you supposed to write classes full of scala-hacking-one-liners. One week (there 4 weeks in course) was very strange: almost all lectures was about implementing some electronic simulation system and then assignment was about very different thing. This is strange also because of deep FP is not fitting in 4 weeks in my opinion, so we spend 1/4 of course to chewing some practical project with no theory. So, if you want to learn Scala, i think this course is not best investment of time, but if you spend some time to first course, looking for mastering FP, you can give a chance (actually course is very short and can be completed in one week with enough spare time) to this course, maybe you can get insight, that i do not got from it. And you can always drop this course, if you start to get wrong feelings :)
I had tried something similar to avoid the drop call but it was only giving me empty results. I wonder what I did differently, especially since the rest is the same. Edit: ah, I see; I had used `Seq.empty[Seq[Int]]` instead of `Seq(Seq.empty[Int])`
Do you have a good resource for the how that you mention?
Nope, not really, unfortunately. Culturally, the Scala community is pretty averse to global prescriptions. It can be useful to explore open source projects, tutorials for frameworks like Play and Akka, and Activator templates. The three biggest projects I've worked on grew out of their POCs rather organically, and were refactored as better patterns emerged. The end result is eventually satisfying in each case, but not without churn and debate. The biggest lesson I can impart is to not use singletons for the big application building blocks. It always seems to end up regrettable. Instead, model services as classes (or traits) -- the basic multi-layer architecture. Use some dependency methodology to wire everything up. There are many options for this: - Dependency injection framework - Trait stacking, with dependencies as abstract members - Cake pattern (trait stacking to an extreme) - Constructor parameters of an abstract type - Implicit arguments to individual methods - Function arguments to constructor parameters (my personal favorite) - Reader monad The difference between each is a matter of taste and appetite for boilerplate.
&gt; Reactive Platform. There's definitely some hype-train here when it comes to reactive platform, but what Chobeat is basically saying is "use Akka with Scala". I mean, that's not the only way to get a reactive platform, because many of the ideas they've implemented originated in the Erlang platform. Couple that platform with the Actor model, and you've basically got equivalence with Erlang without having to leave the JVM behind and move to BEAM. While Akka and Erlang clearly work, and I guess Node does too to a lesser extent, I'm not really convinced they're heads and shoulders above a standard multi-tier REST architecture. And now, just to confuse things a bit more, there's "reactive REST". It's clear that there are benefits to moving concurrency back into the server from the client without worrying so much about concurrency and synchronization and, while that may be a huge enough benefit to be worth it, one can't really know for sure without a rather large leap of faith and bending your brain to reactive patterns and possibly functional programming as well. That's not time wasted IMO, but it can be a difficult foundation on which to build a project and a team when you've got to convince them of such a big change.
Yes, the second course looks like it's kind of like the later topics on the old Progfun combined with the earlier topics of the old Reactive course. Then it looks like the third course takes a different tack on concurrency, rather than introducing Akka. I think this makes sense, as Akka is a big topic to cover in a way that's as cursory as the reactive class. Hopefully they bring it back in a dedicated class, if only to cover some basic usages. But in my experience so far, it seems like most usages of Akka probably either are more natural with Akka Streams or working with the library's read-built abstractions than writing actors on your own. 
Yeah, I figured it was just some barely-known manifesto that only a few people were wanking off to.
I see the point of the original post. I disagree with almost everything you said. Why bother with Scala at all if you aren't going to use any of the more powerful parts of the language -- this makes zero sense. Just use Java ffs. http://doc.akka.io/docs/akka/current/scala/typed.html or Scalaz can be used to have type-safe actors. For web UIs you may want to look at scala.js -- you can use react or angular via that path. And dont both with sbt or scalatest? Use Maven and JUnit? Really? Holy shit.
&gt; Why bother with Scala at all if you aren't going to use any of the more powerful parts of the language The point is to use the parts that provide the most value, and avoid changing everything at once. I think the parts I recommended *are* the more powerful parts of the language (or rather, the parts with the best power-to-weight ratio). &gt; http://doc.akka.io/docs/akka/current/scala/typed.html Experimental. Such projects from akka do not have a good track record. I've tried using "akka's current experimental typed actors" before; fool me once shame on you, etc. &gt; or Scalaz can be used to have type-safe actors ScalaZ doesn't offer actors at all AFAIK? fs2/scalaz-streams is a very good way to achieve what it gives you (*if* you need it) but it's not actors. &gt; For web UIs you may want to look at scala.js -- you can use react or angular via that path. Point - I don't see that as quite the same kind of thing (I suppose I should have explicitly said server-side web UI). I don't know that Scala.js is mature enough to recommend widely, but if it works for you then great. &gt; And dont both with sbt or scalatest? Use Maven and JUnit? Really? Yes, really. I don't think the complexity:power ratio of either of those is good enough to be worth using.
I did end up buying a copy of SICP and you're right. Some of the things in the course were pretty much a straight copy from that book including the algorithm you were supposed to be using. It felt like cheating somehow though since I was just translating lisp into scala and not really learning why or how scala actually worked. And even after doing that, my algorithms sucked compared to his and were much longer. He could boil a lot of it down to one line when I just couldn't. After seeing his solutions I would frequently feel like I never would have come up with it without a deeper understanding of the language.
There's an ever-increasing trend, as far as I can see, to start to pull in more functional paradigms into other languages. But since those languages have foundations which don't account for those paradigms it can be am uphill battle. On the other end of the spectrum, going fully functional throws out a lot of useful constructs and sometimes provides too much of an abstraction away from the underlying machine. I see Scala, then, as a forerunner of the place that languages are going in any case. But as a natural consequence that makes it the stage upon which the drama of reconciling those schools of thought are going to be resolved. The style guides I've seen reflect the backgrounds of the teams who created them, as one would expect. The hope is that over time we have less of "I have a Java background coming to Scala" or "I have a Haskell background coming to Scala" and more "I have a Scala background", and a recognition that the same kinds of problems are being faced and solved by each paradigm in similar ways, perhaps differing mostly by terminoloy. Either Scala will be the proving ground that the ideas can be synthesized or show that the camps are too different after all and other languages should not try to be as all encompassing. My bet is on the former so I see the multiple paradigms as a boon, but if you're less optimistic then it'll be a bane. I'll also throw in that Scala.js and scala-native are also going to cause a lot of churn as wrll, not only because the language will have to decide what level of abstraction it's going to support or require but also figure out what guarantees it will make that all platforms must adhere to but because different platforms might happen to favor one approach to a problem over another for reasons that only makes sense on that platform.
And pattern matching doesn't have a high power:weight ratio? I'd be hard pressed to find Scala features that reduce boilerplate more than pattern matching w/ destructuring. &gt; Experimental. Such projects from akka do not have a good track record. I've tried using "akka's current experimental typed actors" before; fool me once shame on you, etc. It will be stable soon, and akka-typed is not the old "typed actors". I personally love the new stuff they've been adding -- the streams library, which you suggest to me here, was one of those experimental APIs that you're bashing here. Akka-HTTP is still somewhat experimental, and is excellent. &gt; ScalaZ doesn't offer actors at all AFAIK? https://github.com/scalaz/scalaz/blob/series/7.3.x/concurrent/src/main/scala/scalaz/concurrent/Actor.scala Scalaz has has actors for years now, they are in the concurrent submodule and are mentioned on the front page of the github site. They are type safe. &gt; I don't know that Scala.js is mature enough to recommend widely, but if it works for you then great. Not mature compared to what? They have an excellent website and tutorials, a fairly stable API, full IDE support, SBT integration. Projects been around for years. Not sure what else you're looking for here. http://www.scala-lang.org/news/2015/02/05/scala-js-no-longer-experimental.html &gt; Yes, really. I don't think the complexity:power ratio of either of those is good enough to be worth using. Beyond the fact that most popular libraries actually use SBT, including the Scala compiler and Scalaz -- SBT provides quite a bit of functionality, like a REPL with your classpath correctly configured, and integrates with intellij natively. Again, I'm not sure what it is you're waiting for here. ScalaTest (or specs) allows you access to tools like ScalaCheck. Given that you claim that you would prefer Haskell it seems strange to me that you wouldn't want to employ a test framework that supports property based testing. You can also still keep tight integration with JUnit while using it, if you have legacy tests. I wonder if you've really looked at these things you're putting down. You seem to have missed an awful lot of detail. 
I also personally disagree with many of your suggestions, but I don't think that makes your suggestions invalid, by any means. I think this is part of what makes adopting Scala so intimidating. The design space for architecture and language tactics is _huge_. To provide some counterpoints: - IntelliJ doesn't take much time at all to ramp up on. Although, it can still be arcane to tweak, if you ever need to dive deep into project structure. Eclipse was much less useful for me last I tried it, but perhaps it has greatly improved. - I'd definitely suggest biting the bullet and learning SBT. I've got my issues with its overly complex design, but so much of the build-time ecosystem is built on it. - I find Scalatest (and specs2) quite easy to use. I'd say it's a matter of taste. If you want dynamic language-style test suite sugar and other helpers, they're fine options. In any case, also consider generative testing with Scalacheck, as a way to add value over the traditional approach. - Agreed on cake pattern and reader monad -- just pass in your dependencies as parameters. You can get a lot of mileage from that, until your project gets huge. - Play is a good option if you need web UI stuff like form processing and asset bundling. For the latter, it may make sense just to use a JS tool chain, though. - Agree that for the most part, actors belong at the library level. Akka Streams can be a pretty useful tool though for concurrency and long-running processes, beyond what a plain `Future` gets you. I've gotten _a ton_ of mileage out of Akka's higher-level constructs, and only use actors directly in my libraries. - Big difference from you on pattern matching. Pattern matching is a huge win, IMO, combined with sealed type trees for algebraic data types, and `PartialFunction` APIs like `collect`. Data-based branching and destructuring in a compact, consistent syntax makes so much of my domain logic much more expressive than many other languages. It can certainly be abused, so it takes some care. - Agreed 100% that Scala needs language-level support for most of Shapeless's useful features. The inability to operate on `case class`s in abstract ways is really frustrating. I'm hopeful that Dotty + Scala Meta + ??? will largely obviate its basic uses. I think where there's maybe common ground between our points of view is to default to YAGNI. Start really simple, and add new language constructs and libraries when you think they'll pull their weight. In my experience, the way to do it is to prototype a refactor of something with a new tool or construct, and then decide as a team whether to pursue it. The Scala ecosystem is filled with incredibly powerful tools, but you'll be in a world of pain if you reach for a brand new abstraction for every little problem. These tools tend to be very thorough, but also have steep learning curves. It can be quite rewarding to apply a new fancy tool to a problem, but that must be balanced against the ramp-up time, but many many days can be lost this way. A lot of Scala shops learn YAGNI the hard way.
&gt; And pattern matching doesn't have a high power:weight ratio? I'd be hard pressed to find Scala features that reduce boilerplate more than pattern matching w/ destructuring. Destructuring assignments (i.e. *irrefutable* patterns) may be worthwhile. Pattern matching as in `match`/`case` usually either should be a method call ("flatMap that shit" etc.) or indicates bad design (e.g. something that should be a typeclass). Look at the example at the end of http://typelevel.org/blog/2014/11/10/why_is_adt_pattern_matching_allowed.html - the exact same code can be dangerous or not dangerous depending on the context. IMO it ends up being safer to never `match` - certainly if you're not building with `-Xfatal-warnings`. &gt; the streams library, which you suggest to me here, was one of those experimental APIs that you're bashing here. Different organizations mean different things by "experimental". &gt; Scalaz has has actors for years now, they are in the concurrent submodule and are mentioned on the front page of the github site. They are type safe. Fair enough - wasn't aware of them. They may be typesafe in a narrow sense (though if they don't support `become` then they're not actors in the conventional meaning of the term), but anything built around `A =&gt; Unit` is inherently unprincipled and loses you most of the safety advantages of Scala. &gt; Beyond the fact that most popular libraries actually use SBT, including the Scala compiler and Scalaz There's no real alternative to SBT if you're a library that needs to cross-build, as I mentioned originally. That doesn't mean it's worthwhile for projects that don't need to. &gt; SBT provides quite a bit of functionality, like a REPL with your classpath correctly configured So does the maven scala plugin &gt; and integrates with intellij natively. So does maven &gt; Again, I'm not sure what it is you're waiting for here. Good reference documentation, mainly, less symbol-soup in build definitions, ideally not allowing build behaviour to depend on arbitrary code (though I suspect that's a non-goal for SBT). But even if SBT offered all the good things maven does, the only reason to switch to it immediately would be if it offered compelling functionality that maven doesn't have. And it doesn't (unless and until you need to cross-build). &gt; Given that you claim that you would prefer Haskell it seems strange to me that you wouldn't want to employ a test framework that supports property based testing. I've never been a fan of property-based testing - IMO general properties should be encoded into the type system, and unit tests should be either simple "smoke tests" (in which case you only really need one example), or tests of logic that you weren't able to reduce to general properties. In general the better the code gets (supported by language features) the lest tests I find myself needing. If you find property-based testing valuable then sure, use ScalaCheck, and that's a good reason to use ScalaTest. What I've experienced as an all too common antipattern is using ScalaTest to write exactly the same tests you would in JUnit, just with different keywords, in which case it's just change for the sake of change (or the sake of fitting in with the Scala cool crowd). &gt; I wonder if you've really looked at these things you're putting down. You seem to have missed an awful lot of detail. How am I supposed to respond to this? I mean for what it's worth I've been doing Scala professionally for 6+ years, I've contributed to shapeless and scalaz, I've ported systems onto and off sbt, scalatest, cake, and play. But this shouldn't be a conversation about our credentials.
&gt; A typical sbt file is much smaller and simpler to read than a typical maven file. Smaller in terms of characters perhaps, but I'd very much disagree with simpler. &gt; Also in maven for most things you will need a plugin, which does not happen for most things in sbt. Plugins enforce that things are properly structured. I think they end up working a lot better than having each build reproduce the same thing in its own way, or building a lot of the functionality into the "core" tool. &gt; What is the problem of "tests in plain english"? I don't think they actually add any value - you never end up with non-programmers actually reading the tests, much less writing them, and they end up making the tests harder for programmers to understand than plain code would be. &gt; This directly contradicts this: I wouldn't recommend porting *onto* Spring, I'm just saying it's not worth porting away from it immediately. Similarly if you're already using Play or typesafe-config in your codebase I'd stick with it for the time being. I just wouldn't adopt them in newly-Scala projects. &gt; Also Wicket has no Scala API I said "probably". Wicket is good enough to be an exception. (Also there is a wicket-scala library that provides some adaptation). &gt; and also Junit JUnit is barely a library, more an interface specification. I don't mean you should use a bunch of library code from JUnit, I mean you should write your tests as plain old classes with methods that are annotated `@Test` so that IDE integration will work properly but other than that it's just plain old code as far as possible. &gt; Some UI libraries for scala: Scalatags Twirl Beard I'm aware. I think something more component-based would be a lot better. &gt; So you don't like to use play because akka actors do not have a good complexity:power ratio and because it uses the typesafe-config which is not typesafe. I also think Play directly encourages a type-unsafe style through its APIs (e.g. there's a method called `.as` or similar that just does a hidden `.asInstanceOf`). &gt; Also can you read a configuration file, which is just text, in a type safe way? Can you do it better then typesafe config? The problem is the other side - most of the time I don't want to use a configuration file, I want to configure my library with a strongly typed object (e.g. if I want to set a timeout in Akka, I should be able to pass that as a strongly typed `Duration`, not write some text in a `.conf` file and rely on Akka parsing it correctly). Most other config systems allow this (even some in Java), but libraries that use typesafe-config generally don't. The text parsing side of it I can take or leave - it has some good features and it's easy enough to layer something better on top (where I'm forced to use it I've written spray-json-shapeless -like automatic typeclass derivation to parse directly to case classes). &gt; This is not true, actors and futures have different purposes, and you CAN'T use futures instead of actors in most cases. Sure. There are genuine use cases for actors. But they're very overhyped, and using actors for something that could be done with just futures is a mistake I've seen made more than once, particularly by organizations that are new to Scala.
If a "match" expression is bad design, then so are all of the other flow control primitives. I mean you can implement for, while, etc with recursion if you like. At that point I would ask why we aren't writing clojure. And in obvious cases Scala provides syntactic sugar, i.e. for+yield turning into map and flatMap. As I see it, map and flatMap are tools that engender code reuse. If the branches of your matches are all functions that you reuse throughout your code, then, certainly, use map and flatMap. But there are tons of use cases in which the 'match' branches will contain code that is not suitable for reuse anywhere else in your program, let alone anywhere else in the object. In these cases it's almost certainly a mistake to use 'map', as it adds little-to-no safety, engenders no code reuse, and is likely less efficient (i.e. @switch). Paint brush application, while preserving some kind of security-blanket consistency, is simply not a rational way to select your flow control. Rules like "never use X", in most cases, give people an excuse to write code naively. Instead, let's have an effing code review and learn from each other and grow as engineers. I think our disconnect on sbt, in part, is that I reject maven explicitly, where you seem to think it's a nice application. Id use gradle if sbt wasn't an option. Maven has turned into a wasteland of dead plugins and aged howtos -- not to mention the verbose nightmare that any significant POM entails. And it's really a pain to extend, whereas sbt and gradle can both easily be scripted since they're basically just DSLs. They are just more advanced tools in general. -- and im not doubting your credentials, im just debating your stance..
There's a lot of value to be gained from using them, I'd agree. But a scenario that can easily happen is: developer A introduces clever code using one of those libraries, developer B complains that they can't understand it, tech lead bans that library. Rightly or wrongly, people are suspicious of these things and very eager to cry YAGNI, so I find it's a lot better to introduce them in a context of code reviews etc. where you very clearly demonstrate the advantage they're bringing. (And frankly some newcomers to scalaz/cats *do* end up overengineering things that could be expressed more directly).
&gt; At Verizon Labs, we use purely-functional Scala on a team with about 250 developers. It can be done. It just can't be done by putting all the developers in their cubicles and wishing them godspeed. You guys enforce purity in all your functions and encapsulate all state in monads? Why are you guys using Scala instead of Haskell then? JVM interop?
&gt; If a "match" expression is bad design, then so are all of the other flow control primitives. `match` is bad design because it is *unsafe* (some of the time, with no way to tell the safe cases apart locally). It allows writing functions that violate parametricity. &gt; At that point I would ask why we aren't writing clojure. Because a good type system is very valuable (and optional type systems don't work). &gt; As I see it, map and flatMap are tools that engender code reuse. If the branches of your matches are all functions that you reuse throughout your code, then, certainly, use map and flatMap. But there are tons of use cases in which the 'match' branches will contain code that is not suitable for reuse anywhere else in your program, let alone anywhere else in the object. In these cases it's almost certainly a mistake to use 'map', as it adds little-to-no safety, engenders no code reuse, and is likely less efficient (i.e. @switch). I don't understand? My position is that for an `Option` pattern-match, `map` or `flatMap` where appropriate are always superior to pattern-matching in the cases where they provide the functionality that you want - if it's code you're not going to reuse, just pass it inline, it's still more concise than pattern matching. Being able to adopt a global "no `match` anywhere in our codebase" rule does increase safety, and performance is very likely to be as close as makes no difference (except when you need a `match` to make a function `@tailrec` - even then I'd only optimize if profiling shows that you actually need to). &gt; Paint brush application, while preserving some kind of security-blanket consistency, is simply not a rational way to select your flow control. Rules like "never use X", in most cases, give people an excuse to write code naively. Instead, let's have an effing code review and learn from each other and grow as engineers. Some constructs really are just bad. Scala evolved organically over 10+ years and there are parts that have been superseded and parts that were bad ideas in the first place. There are lots of cases of "it depends", but there are also cases where absolute rules are appropriate. &gt; Maven has turned into a wasteland of dead plugins and aged howtos Not my experience. It doesn't change very often because it doesn't need to; it's mature and working, and the job of performing builds hasn't changed a lot. &gt; not to mention the verbose nightmare that any significant POM entails. For a typical module it's just a list of dependencies and maybe enabling a few plugins. Plugin configuration is usually common, so you write it once in your parent (and in general, being encouraged to stick to the defaults as closely as possible is a good thing). &gt; And it's really a pain to extend, whereas sbt and gradle can both easily be scripted since they're basically just DSLs. If you're going to have actual logic, that logic belongs in first-class code, subject to code reviews, testing, a proper release cycle and so on; anything less is a recipe for disaster. The maven plugin model is great for forcing this, because it means any custom build step is encapsulated as a plugin which is a first-class module. Being able to write ad-hoc `if(tuesday)` in your build definition (where many devs wouldn't even think to look for dynamic behaviour) is a nightmare for maintainability. Maven is much better at ensuring that all your projcets have consistent build processes, and anyone who's familiar with one project can easily work on all of them.
&gt; I just don't find it gives a compelling advantage. Its refactoring works a bit more often than Eclipse's, but not often enough to be a step change. Its presentation compiler is significantly less reliable than Eclipse's (worst of all, it will frequently show broken code as ok). Sounds like Eclipse is worth another look. I'm encouraged to hear it works with https://github.com/LPTK/Boilerless, which IntelliJ does not. &gt; I just found it much worse than Wicket for that side of things, in terms of both type safety and general model (component APIs are so much nicer than template-based ones). Agreed. I experimented with using Scalatags instead of Play (Twirl) templates, and was really happy with that. ScalaCSS worked nicely for me, too. There are some interesting opportunities for Universal Scala, although I haven't actually tried it. Not really familiar with Wicket, but worth a look! &gt; In the case of sealed type trees I'd rather have an explicit fold method - it's just safer. Interesting, I can see that point. I guess you'd have to write `fold`s, or use some kind of boilerplate scrapping to really take advantage here, you'd lose some of the additional branching options patterns provide, and you'd lose the ability to use other APIs, like `collect`. I rarely actually use `match`. Oh yeah, from your original comment, I don't have the same problem with Typesafe Config. I think it's quite nice. Sure, the config files themselves aren't type-safe, but I think it's a reasonable compromise to exercise the discipline of reading config at boot-time, failing fast if something is invalid. &gt; The trouble is that PartialFunction subtypes Function, which is utterly backwards and probably unfixable, making it very unsafe. Yeaaaah. With you there. I suppose it's to allow people to reuse the handy partial function sugar where total functions are expected, which is quite useful, but it seems like it would have been better to try to accomplish this some other way.
How far down the FP rabbit hole do you guys go? Are you guys sticking to the core features in Scala or are you heavily utilizing the categorical theory constructs?
&gt; One of my first jobs was a trying to fix a mail system written in C. There was a feature for mail-handling rules, of which there were only 5, easily represented as several if-statements or a switch-statement. &gt; A bored junior developer decided to implement an extensible interpreter-based approach, in which a small program was encoded in an ASCII string attached to the message. It's not _obviously_ nuts, but I would immediately ask two questions: 1. What are the existing, or at least next-quarter predictable, use-cases for this extensibility? Who's writing these rules? Who's the customer? 2. Such an approach seems likely to have a visible effect on mail-handling scalability. Have you measured this? With what methodology? How do the results compare to the old system? Are there SLAs that may be affected by this? &gt; I've also encountered a depressing number of programmers who do not even know what a graph is. These people will never understand free monads or co-yonedas. You have two choices: education or replacement. I don't mean to sound harsh, but actually not everyone is cut out to write software for any given organization.
I've been looking through the codebase for the last few weeks, and sending some patches. The organization of the framework looks really good, and development is heavily test-driven and benchmark-driven. The core design is amazingly simple, yet allows building powerful abstractions on top of it. On the other hand, I don't think it is a huge learning step for anybody with existing knowledge of actor frameworks - although they have typed channels and first-class event sources, one can always create a reactor in the "untyped" mode, and use event sources as if using an ordinary "receive" statement.
We're a scalaz and scalaz-stream shop, and employ roughly half of the committers to scalaz, scalaz-stream/fs2, Cats, and http4s. My colleagues are people like Rúnar Bjarnason, Daniel Spiewak, Tim Perrett, Stew O'Connor, Cody Allen, Joe Barnes, Ryan Delucchi, Vincent Marquez, Ross Baker... Examples of our work can be found on our [GitHub](https://github.com/oncue) pages. To the extent "monads," "applicatives," "functors," etc. are "category theoretic," yes, we make extensive use of them. (You can just as well—in fact, I think more easily—look at them through a much less scary abstract algebra lens).
No, that compiles to regular functions. It's because in terms of oo inheritance, pf includes apply like function and adds a method that it doesn't have, isDefinedAt
BTW, I'm sure you have seen the Scala Days video "The Free Monad is Not Free". This pretty much confirmed my suspicions about the free monad, and Odersky pointed out that such solutions typically are an order of magnitude slower than conventional code. I'm sure there are classes of problems where the free monad is essential; most code is not like that.
&gt; Well, the interpreter solution was obviously nuts; it was buggy, and was very hard to debug. Yeah, but you already said it was in C. :-) &gt; And there were no known future requirements for more rules (classic YAGNI). This is the criticism that's actually damning, of course. &gt; Graphs are pretty easy however, compared to the advanced monadic and higher-kinded type stuff. It probably won't surprise you to hear that I find this to be _categorically false_, if you'll pardon the pun I didn't see until I'd already typed it. :-) &gt; I would say I've only worked with one person who knew that stuff; he had a PhD in math, and wrote code that most developers had difficulty understanding, including my boss who was an amazing Java engineer, a true "rock star". This is the type of feedback I see/hear a lot, and honestly, it just baffles me. Let me break it down just a bit: &gt; one person... knew that stuff &gt; &gt; most developers had difficulty understanding &gt; &gt; my boss who was an amazing Java engineer, a true "rock star". What inferences can we draw from this? Serious question.
That's really cool, thanks for the info.
&gt; if it's code you're not going to reuse, just pass it inline, it's still more concise than pattern matching sealed trait Useless case object A extends Useless case object B extends Useless case class C (v: Int) extends Useless val x: Useless = // some value from the ADT val y: Int = x match { case A =&gt; 10 case B =&gt; 20 case C(v) =&gt; 10 * v } Could you show me how that's more concise without refactoring the data? Assume A, B, and C have other meanings than just what you see in the immediate context. Or what about match guards? Do you think those are less concise too? case class XYZ(a: Int, b: Int, c: Int) val m = 10 def (v: XYZ) = v match { case XYZ(a, b, c) if c == 0 =&gt; a + b / m case v: XYZ =&gt; v.a + v.b / v.c } As for the maven stuff, I completely disagree. We've had nothing but success with gradle. It significantly improved our build process, and reduced our build-centric LOC by half or more. Each line of code you have to maintain has a cost, and less boilerplate and less code is always better. The idea that maintaining a whole other project to do a simple ad-hoc task is somehow less costly -- that's completely backwards. And if you want to write a plugin and keep it organized, you can do that in gradle too.
Good idea, that at least cleans up the source a bit. Fewer method and class definition lines.
It very much depends upon what you mean by "essential." For us, correctness and horizontal scalability are essential. Maximizing throughput is not. So even if naked CPU performance is out by an order of magnitude, it's likely not to affect us significantly adversely. That said, sure, we like it when `Free` gets faster, for example when its performance [doubled in scalaz 7.2.0](https://groups.google.com/forum/#!topic/scalaz/BXW6BVxYEPE), or when someone comes up with a better representation that makes performance [linear instead of quadratic](http://mandubian.com/2015/04/09/freer/). The situation is the same as it ever was: premature optimization is the root of all evil. Referential transparency isn't some nice-to-have; it's the bit that's essential to us. And we get performance improvements for, er... free... every time someone improves `Free`. It's a very nice win-win.
&gt; At Verizon Labs, we use purely-functional Scala [...] Why not use Clojure?
Because we rely on types to help us a lot.
&gt; source: http://jimplush.com/talk/2015/12/19/moving-a-team-from-scala-to-golang/ They even admit they should have had code reviews in place. That's not a failing of Scala, that's people failing Development 101.
I can tell you my Java rock star boss was one of the most impressive people I've worked with in my 30 year career. Perhaps with a couple years of Scala experience, he could learn to puzzle out the PhD written code. I could understand it, but it wasn't exactly easy. The previous company will remain unnamed, but it had an amazing amount of talented people, although many of them might be considered young "brogrammers". The level of Scala use there was mostly in the "better Java" category.
I'll admit to only making it part way through Functional Programming In Scala. I'm an old school imperative guy, slowly getting the FP religion. I just don't know how far down the rabbit-hole is worthwhile. At Verizon, you apparently have a large number of Scala FP experts such as Runar and Daniel Spiewak. You seem to have an elite team, not at all typical of average developers. This also loops back to previous points. Scala is not just a language for advanced FP programmers, it can be used as a superior alternative to Java. If one guy on the team brings in scalaz, the result can be code that only that person understands.
&gt; Could you show me how that's more concise I thought we were still talking about cases where `map`/`flatMap` were appropriate. &gt; without refactoring the data? No - my point is that you probably *should* refactor the data. &gt; Or what about match guards? Do you think those are less concise too? Actually yes, in that example: def f(v: XYZ) = { val XYZ(a, b, c) = v a + b / (if(c == 0) m else c) } &gt; Each line of code you have to maintain has a cost, and less boilerplate and less code is always better. The idea that maintaining a whole other project to do a simple ad-hoc task is somehow less costly -- that's completely backwards. I think it's occasionally worth something being harder when that something is something you shouldn't be doing. (Compare e.g. mutable variables in ML, which are deliberately clumsy). http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html &gt; And if you want to write a plugin and keep it organized, you can do that in gradle too. The trouble is that you can't enforce it. There's no way require all builds to follow the same pattern, with any deviations encapsulated as plugins. And for things like code review, style enforcement and so on, you've now got an odd-one-out file format (for either gradle or SBT) with its own rules (or not even any real rules, in the case of `.gradle` - the implementation is the spec). I should say here that one of the reasons I'm a big fan of Scala is that you can use it for everything - core systems and throwaway scripts alike - so I really like to have the only files that have logic in be `.scala` files that follow the normal rules for such.
Ouch. I think mocking (as distinct from stubbing) is a sign of poor design at the best of times, acceptable only when dealing with third-party libraries, but for value types there's absolutely no excuse. (I also think default mocks cause far more trouble than they solve). You might be able to stop that happening by declaring your case classes `final` (good practice anyway), though there are mocking frameworks that work around that too.
If you want to feel like some kind of superior individual because you enjoy using something "complicated" with "steep learning curve", then go a ahead. The article posted is about moving from Scala to Go. You don't hear Rob Pike, Ken Thompson or Robert Griesemer talking about "sifting wheat from chaff", they just wanted to have a stupid simple, efficient, crisp language. You don't hear them debating style, spaces, tabs, curly braces or other way to make code readable. Every line of Go source is expected to go through gofmt / goimports which decides this for you. There are no pointless discussions about language, style or paradigm superiority, just people who want to get some work done. Again, I enjoy Scala. I wrote my thesis on FP and Scala, but there is something refreshing about not worrying about stuff you're trying to portray with your comment.
Yeah, I get that, but it flies in the face of total function being substitutable where a partial function is expected, not vice versa. It only really makes sense if partial functions are just total functions that may happen to throw exceptions. Of course this is exactly how it is today. I'm with m50d that this seems like a bad move.
Not sure I get that last point -- do you mean you want your code to be able to dynamically configure libraries, rather than statically setting your configuration at the outset? I feel like most libraries I'm used to allow for this, falling back to HOCON by default.
&gt; do you mean you want your code to be able to dynamically configure libraries, rather than statically setting your configuration at the outset? It's actually that I want the config to be *more* static - I want to be able to just say `ActorSystem(config=ActorSystemConfig(routing=RoutingConfig(timeout=4 seconds)))` or some such in my code, and have that interface be strongly typed. &gt; I feel like most libraries I'm used to allow for this Some do, but too many don't, IME.
&gt; I guess I hit a nerve and triggered you. Back atcha. I have no specific love for reactive patterns as of yet, but your comments indicated zero critical thought around the subject, so a little criticism shouldn't be a surprise.
&gt; I'm not sure what you provided is any shorter or more concise than what i had, It is - just look at it on the page. And it cleanly separates the destructuring from the logic, pushing the `if` down to the part where there's an actual difference in behaviour. &gt; In short -- matching is useful, and to go further, I believe there are situations in which there aren't a lot of better solutions. There are cases where it's more concise than the alternative - that's probably true of all possible language features. But not a large number of cases, and not enough to justify the safety loss, IMO. A guard can be transformed into an ordinary `if` that will only ever be slightly longer; a match on an unsealed type is a bad idea often enough that it shouldn't be concise syntax; matching on sealed types (ADTs) would be ok if you could tell it apart (locally, textually) from unsafe matching, and does save having to implement a `fold` method, but doing without it isn't so hard: all the standard library types offer `fold` already, and between them they cover most of the cases where you would ever want a "generic" ADT. When you have an ADT that's specific to your project, 95% of the time what you do with a pattern match is better off implemented as an ordinary abstract method. &gt; It's used liberally in Scalaz, Shapeless Shapeless mainly uses it for `Inl`/`Inr` - I think those cases would be better using `.eliminate`. The article I linked shows the typelevel folks have at least some concern about pattern matching, with complex rules about when it's ok and when it isn't. I go further than most in opposing it completely, but I don't think you'll find a lot of full-blooded support. &gt; That's what team leadership and code reviews are for. Every tool or language has anti-patterns and code smells. Leadership and code reviews are a fallback. Eliminating classes of errors by using tools that make them impossible is what Scala is all about. &gt; And again, Scalaz, Shapeless, the Scala compiler itself, .. almost any project of repute is using SBT. If I picked my tools based on popularity then I'd never be using Scala in the first place. I think SBT is the least bad option for projects that need to cross-build. I think some people use SBT because it's fashionable. And I think some people have made bad choices. &gt; I think you have to ask yourself why those projects are doing fine with tons of contributors if your position holds. I think they succeed despite SBT rather than because of it. One certainly sees a fair few complaints about SBT, and indeed a fair few people giving up on Scala entirely and blaming SBT.
I don't watch videos; I've seen the slides. They seemed to conflate a *Scala* problem that `Free` *works around* with a problem with `Free`. Free monad Scala is often an order of magnitude slower than conventional Scala; note that an order of magnitude slower than conventional Scala is still substantially faster than Ruby/Python/etc., i.e. still more than fast enough for a large class of problems.
There are actually two ideas here that you may end up needing to distinguish. Things you can combine (with an associative operator) form a `Semigroup` and this abstraction is provided by both cats and scalaz. Combining *parameterized structures* like `List` or `Option` without regard to element type is a distinct notion called `SemigroupK` in cats and `Plus` in scalaz. The distinction is evident with `Option`. scala&gt; Option(1) |+| Option(2) // semigroup (combine elements when present) res1: Option[Int] = Some(3) scala&gt; Option(1) &lt;+&gt; Option(2) // plus (leftmost non-empty, if any) res2: Option[Int] = Some(1) In any case you might look at one or both of these libraries to see how they do it. And I have a worked example [here](http://tpolecat.github.io/2013/10/12/typeclass.html) that might be helpful.
&gt; You don't hear Rob Pike, Ken Thompson or Robert Griesemer talking about "sifting wheat from chaff" You hear broadly similar things when they talk about e.g. assertions. &gt; There are no pointless discussions about language, style or paradigm superiority It's a young language. Give it time.
I replied in the gist. 
Just to be clear: `Set(x)` is syntax for a standard Scala set, but we're trying to implement our own here. One could define an `object Set` and implement an `apply` method on it - but implementing that is the same problem we're solving by defining `singletonSet`.
How are you running it? You might be conflicting with the standard Scala `Set` type which does have a suitable `apply` method. For example, if I name it `MySet` I get type MySet = Int =&gt; Boolean defined type MySet MySet(1) cmd4.sc:1: not found: value MySet &gt; what data is there to loop through? The range of possible values: `[-1000, 1000]` &gt; The description of the problem does not say we should use the "bound" to solve the problem. Given that everything is functions, the only way to find all the elements you need to transform is to iterate the possible values and check them.
&gt; How are you running it? You might be conflicting with the standard Scala Set type. For example, if I name it MySet I get https://codepad.remoteinterview.io/AuthoritativeLuxuriousBambooMiguel &gt; The range of possible values: [-1000, 1000] No, I do not believe that is the assignment. &gt; Given that everything is functions, the only way to find all the elements you need to transform is to iterate the possible values and check them. Again, I do not believe you are correct.
&gt; codepad Yep - you're referencing a built in Scala `Set` then. Add `println(Set)` and you'll see that you get a `scala.collection.immutable.Set` instead of an error. &gt; I do not believe you are correct That's fine, but I've done this assignment before, I have an idea of what I'm talking about :)
I'm not sure I've ever seen something overengineered at work with scalaz. There really isn't much in that library, and there's even less in cats. I've seen people create new case classes and then write Semigroups/Monoids for them in order to solve problems instead of using foldLeft or other Scala collections API methods (groupBy, mapValues, stuff like that) We use a lot of ```*&gt;``` along with values of ```Validation[E, Unit]``` or ```E \/ Unit``` as a pure FP equivalent of fast return guards from imperative languages. I've used Tagged types along with Scala's singleton types in order to disambiguate arguments to the callback to an ```Apply[F].applyN```. I was parsing a CSV, so I tagged each parsed column's Validation's Success type with the singleton type of the Enum value of the column it came from. I've used Coproducts of Free Monads in order to unit test code using SQS, S3, and a SQL database by proxying each of those side effects with mutable collections.
aslkdsfakljasdfkjlasdfjkldasfjkl;sdafjkl In the assignment every other time we were supposed to use a previously written function, it was called out explicitly. For map, none were, so I assumed nah. How was I supposed to know to implement exists? I'm upset with that assignment question. I even left a note in the Coursera discussion area about whether map was supposed to implement anything and nobody responded. Anyway, sorry for being indignant. I've just been wracking my brain about this problem for two days and assumed using exists would be cheating.
&gt; I'm not sure I've ever seen something overengineered at work with scalaz. There really isn't much in that library, and there's even less in cats. I've seen people write code generically in terms of typeclasses where it's only ever called with one concrete type, and also your example. &gt; I've seen people create new case classes and then write Semigroups/Monoids for them in order to solve problems instead of using foldLeft or other Scala collections API methods (groupBy, mapValues, stuff like that) Yeah. The thing is there's a lot of overhead to the typeclass encoding in Scala, so I think the bar ought to be pretty high (or Scala ought to support a more efficient way to express these things). &gt; I've used Coproducts of Free Monads in order to unit test code using SQS, S3, and a SQL database by proxying each of those side effects with mutable collections. Why coproducts? Do you have logic that needs to operate on all of them at the same time, and also logic that operates on only one of them where this distinction is important? I've used most of what you list (and plenty more besides) but I've not yet found a use case where I felt coproducts would have really given me much advantage over the traditional free monad.
&gt; The problem is the other side - most of the time I don't want to use a configuration file, I want to configure my library with a strongly typed object (e.g. if I want to set a timeout in Akka, I should be able to pass that as a strongly typed Duration, not write some text in a .conf file and rely on Akka parsing it correctly). Most other config systems allow this (even some in Java), but libraries that use typesafe-config generally don't. The text parsing side of it I can take or leave - it has some good features and it's easy enough to layer something better on top (where I'm forced to use it I've written spray-json-shapeless -like automatic typeclass derivation to parse directly to case classes). Typesafe config recommends that in your libraries you use a `Settings` class that receives a `Config` object in its constructor (something like [this](https://github.com/ist-dsi/kadmin/blob/master/src/main/scala/pt/tecnico/dsi/kadmin/Settings.scala)) with a default value delegating to `ConfigFactory.load()`. Then in your "main class" constructor you receive a Settings with a default value of `new Settings()`. There are multiple advantages to this design: * If the end user just invokes `new MainClass()`, then your default settings will be used (which in most cases will just use the settings defined in the `reference.conf` or a deployed application.conf). * If the user wants to override settings we can do so in two ways, either change the `application.conf`. Or create a new instance of `Settings` overriding the needed fields/methods. * We can even use a mixed approach and read the settings from the deployed application.conf be also create a instance of Settings that overrides certain configurations. The idea of using a strongly type object directly and not needing a configuration text file is really appealing, but to do so you would need the scala compiler (or some other interpreter) in your deploy machine that would parse the configuration. 
&gt; Typesafe config recommends that in your libraries you use a Settings class that receives a Config object in its constructor (something like this) with a default value delegating to ConfigFactory.load(). Then in your "main class" constructor you receive a Settings with a default value of new Settings(). That's better (and would be a substantial improvement over what e.g. `ActorSystem` currently does), but it would be better to decouple the `Settings` type from the logic for loading it from config. i.e. have `Settings` be a simple `case class`, with an alternate constructor or factory method that builds one from a `Config`. &gt; The idea of using a strongly type object directly and not needing a configuration text file is really appealing, but to do so you would need the scala compiler (or some other interpreter) in your deploy machine that would parse the configuration. The deployment machine needs some way to get its config, sure. That might be hardcoded in my code (maybe with a basic `if(environment == "dev") Dev else ...`, where you just have `object`s in the codebase representing the dev/stag/prod configurations). It might be loaded from a database. It might be serialized to disk, it might be stored in Zookeeper, etc. And sure for most approaches it'll be stored as a String and then parsed into a strongly typed object. But it doesn't have to be. The library ought to accept a strongly typed object, and the default way to get that object is by parsing a weakly typed config, but there should be an interface that lets me pass in a strongly typed config if I've managed to get one somehow.
&gt; I've seen people write code generically in terms of typeclasses where it's only ever called with one concrete type, and also your example. I don't see a problem with that necessarily. I prefer to write small helper functions in terms of ```F[_]: Foldable/Traverse/Functor/Monad``` etc than use Seq or some other collections hierarchy trait. I will often only call the method with List/Vector/mutable.Buffer (.asScala) but usually when I'm writing the helper, I haven't settled on/thought about which collection I'll be using. &gt; The thing is there's a lot of overhead to the typeclass encoding in Scala, so I think the bar ought to be pretty high (or Scala ought to support a more efficient way to express these things). Not sure I follow. Defining a Semigroup for a user-defined type is as simple as doing ```implicit val mytypesSemigroup: Semigroup[MyType] = Semigroup.instance(_ whateverMethod _)``` and Monoid is just ```implicit val mytypesMonoid: Monoid[MyType] = Monoid.instance(_ whateverMethod _, someZeroValue)```. Unless you meant performance overhead? &gt; Why coproducts? I think I worded things wrong there. By "Coproducts of Free Monads" I mean the approach suggested by the "Reasonably Priced Monads" talk and others like it. I think I should have not used that of and said "Free Monad Coproducts" to be correct.
&gt; If you want to feel like some kind of superior individual because you enjoy using something "complicated" with "steep learning curve", then go a ahead. I realize I left no context to my remarks, but I assure you, you've got me all wrong. I've looked at Scala, and I'll probably end up having another look at it. But, to me, it's some kind of math geek thing—an A.V. Club. I find the syntax ugly, and while I completely understand that functional programming has its advantages when it comes to multiprocessing, I'm not keen on designing entire programs that way, which is what all the "cool kids" programming in Scala seem to be bent on doing. Now, I actually have nothing against people wanting to "do their own thing." If Scala is some kind of secret handshake among some programmers, that's fine. But, like I said, I get the feeling that that's at least a part of it. If that's the case, let's not pretend that it isn't. I guess what I'm saying is that language isn't just a syntax; it's also culture. It seems to me like Scala programmers are working very hard to create a culture. The language is ostensibly "hybrid," but no one seems to be interested in the OOP parts of it all that much. If I've got it all wrong, I genuinely would appreciate being enlightened. Maybe I've developed a prejudice?
&gt; ... their purely-functional nature makes them very safe to use even naïvely (they won't have side-effects you weren't expecting, by definition).... I wouldn't be so sure. Side effects are tricky things, for example: https://github.com/typelevel/cats/issues/1296
Is this the github page? https://github.com/oncue or is there somewhere somthing more?
While you can use "as" in Play JSON, the primary API is based on mapping through implicit Reads and Writes, which are typed.
?
&gt; I don't see a problem with that necessarily. I prefer to write small helper functions in terms of F[_]: Foldable/Traverse/Functor/Monad etc than use Seq or some other collections hierarchy trait. I will often only call the method with List/Vector/mutable.Buffer (.asScala) but usually when I'm writing the helper, I haven't settled on/thought about which collection I'll be using. That can feel like overengineering, at least in the early stages. I'm never quite sure where the right point to go from generic to specific is. It doesn't help that cats/scalaz don't offer instances for `Seq` (IMO wrongly - and it's not like it's a general policy, they happily offer instances for `Map` rather than specific subtypes). &gt; Not sure I follow. Defining a Semigroup for a user-defined type is as simple as doing implicit val mytypesSemigroup: Semigroup[MyType] = Semigroup.instance(_ whateverMethod _) and Monoid is just implicit val mytypesMonoid: Monoid[MyType] = Monoid.instance(_ whateverMethod _, someZeroValue). Unless you meant performance overhead? I hadn't seen that sugar, but even then you've often got to create the companion object and it's always going to be an extra line compared to having your type implement a trait.
This probably isn't what you want to do though. You don't want a separate `Monoid[MyType]` for every instance of `MyType` - you want a single, static, global `Monoid[MyType]`. (You also want to be able to get the `Monoid[MyType]` without having an instance of `MyType`, because you want to be able to use it to get a "zero" instance).
Consider this scenario. Let's say there's a new developer who is new to Scala and we tell them that it runs on multiple platforms. Great! So then they run some code on each of the platforms and see different behavior, let's say in one case an exception is thrown, another returns null, and a third returns NaN. I think it's reasonable to imagine that that devolper's first reaction is that the Scala ecosystem must be kinda shitty if things like this could happen, and then a second thought might be to figure out which result is actually supposed to happen. There should be some specification as to what is the correct answer, or a note that can explain the answer is expected to differ by platform. And that specification shouldn't just talk about expected values but also nonfunctional requirements like performance guarantees because that really really matters for some applications. Or think of it like this: everyone rightly thinks that writing code targeting IE6 was a pain in the ass because it had small but important inconsistencies with what was expected by the standard. Now imagine if there wasn't the standard to fall back to, but every browser just did their own thing. It would essentially be impossible to code anything because each implementation could claim that theirs was the correct version, or change the behavior between versions without notice and claim that latest is the correct implementation. I'm bringing up JVM features not because I think all platforms should implement all of those features but because it highlights the need to seriously consider when the line in the sand is drawn where on one side is standard Scala and on the other side is platform implementation details. There is the option to dump almost all of that and just keep essentially the syntax, but that's an extremely low bar and would honestly gut most of the utility of the language. I'm also not saying that we need to figure everything out before we work on any of the platforms, but each of the platforms should expect to make changes as more things are standardized. Standardization takes a long time, especially when there are constraints in tension with each other like the separate platforms almost certainly will have. I'm very optimistic that we'll get there and do great stuff along the way but it definitely feels way too premature to say we're already past the finish line. Edit to add: My intent here is not to troll. I'm just a huge fan of the language that wants to ensure we're paying due diligence to ensure that the language stays healthy and easy to work with as possible. Edit to add respond to question of technical fact. Maybe I should just let this be but I don't want misinformation to go unchallenged. &gt; That accessing a long is a single operation is not even true on the JVM to start with. Please see the `lload` [bytecode instruction](https://en.wikipedia.org/wiki/Java_bytecode_instruction_listings). It's harder to grab a reasonable sample of the Scala.js definition of Long since it's a bit spread out, but this sample defining come constants helps illustrate that the representation of Long is composed of multiple Numbers similar to the way BigInt would be, though granted it's constrained to just the two numbers. $c_sjsr_RuntimeLong$.prototype.init___ = (function() { $n_sjsr_RuntimeLong$ = this; this.Zero$1 = new $c_sjsr_RuntimeLong().init___I__I(0, 0); this.One$1 = new $c_sjsr_RuntimeLong().init___I__I(1, 0); this.MinusOne$1 = new $c_sjsr_RuntimeLong().init___I__I((-1), (-1)); this.MinValue$1 = new $c_sjsr_RuntimeLong().init___I__I(0, (-2147483648)); this.MaxValue$1 = new $c_sjsr_RuntimeLong().init___I__I((-1), 2147483647); return this }); Maybe it's one of those things that will never matter, but sometimes timing does matter. Could there be a case of a cryptographic algorithm that assumes all references to Int and Long take the same time would be open to a timing attack due to that difference?
That's it, as far as our open source goes.
 // http://alvinalexander.com/scala/how-to-open-read-text-files-in-scala-cookbook-examples object Control { def using[A &lt;: { def close(): Unit }, B](resource: A)(f: A =&gt; B): B = try { f(resource) } finally { resource.close() } } To me, that `def` line—it's like a stick in the eye.
I'm concerned about the future though. Folks are saying the right thing *now* but forks have a way of, well, forking. I don't want to live in a future world where I can't take advantage of a typelevel library because I don't want to use the typelevel scala, or vice versa
I made a mistake in my original post I don't have a companion object. Just a trait and a class. The project uses di get instances of the class. Where should I put the implicit in this case? I've just been using implicit M: Monoid[my type] to get zero and append
What can we conclude from this article? Software development is hard! Not because of languages but because real life problems are hard. What can they really achieve with such a mentality? Imagine they want to build a scalable system. Will they give up because distributed computing is not as easy as they thought? Even with wonderful tools such as Spark/Cassandra/etc, you **need** to understand what you're doing. If the only thing they want is pressing magic buttons and assembling already-made solution, they won't go very far. Their complain that there are too many ways to solve a problem in Scala is just plain ridiculous. Scala offers a box with many tools. That's great! We can pick the right tool for the problem we're facing. It may sound simpler using only a hammer, but then I wish you good luck for screwing or cutting trees. &gt; I know scala was meant to be developer friendly (and it is!) , but is it project friendly from the point of CTO? What is your opinion on this? Depends on the CTO. I think we all have more or less the same goal: delivering on time, good-enough products to please our clients. Immutability, typing, monads, higher-oder, etc are not just toys, they are tools to improve both development time and software reliability. Like any new paradigm or technology, it may be outside of your comfort zone. But when your API is down because a piece of code mutated a data structure used somewhere else, or if you were serious about mutual exclusion but you got a deadlock or just hight latency, then you start counting how much money you're loosing per hour. And even if your API is still up, CTOs tends to get nervous when machines keeps crashing. Software engineering is not only about writing code, it is about delivering software that works. Because at the end, what the client wants is just that it works like expected. So don't fear learning new techniques, don't fear the theory if it helps your making better software (and faster too!). What would you think of a space engineer refusing to learn and use relativity because "it's just theoretical stuff"? Scala is a wonderful language because it opens many new ways to solve problems more efficiently with more maintainable code. Just look at how dependency injection is done in Java? Look at JSON (de)serializing? In Scala, thanks to implicits you can customize the serialization with no efforts and because macros works at compile time you can be sure that the compiler checked lots of things for you. Indeed someone refusing to learn how implicits work **will** run into problems in Scala. But how is it different from someone refusing trying to understand OO in Java? I'm sorry to say that, but using a language while refusing to learn it is completely absurd and will inevitably end in tears.
&gt; I wouldn't use play, akka actors (except as an implementation detail that you never directly touch), or anything that uses typesafe-config if you can help it. These libraries have very poor typesafety which destroys the main advantage of using Scala. More generally any library designed to be used from Java is probably not worth bothering with. This is really incorrect. There is nothing that makes these libraries less typesafe, i.e. * The only thing not typesafe about Play would be its support for non Scala based languages on client side Javascript. One could also make an argument about Guice (I am also personally not a favour of Guice and have heavily argued against it), but apart from that Play is as typesafe in general as any other language. Note that it is possible to avoid using Guice in order to prevent the runtime issues (which is what I do) * Typesafe config is about runtime configuration, it is going to fail at runtime if you provide invalid config at runtime (obviously). Unless you want to hardcode your environment variables/system properties, there isn't real way around this. That being said there is ficus (https://github.com/iheartradio/ficus) an excellent libraries that lets you do code like this As an example with ficus (which is ontop of typesafe config) config.as[Option[Int]]("authorisation.timeout") That is as typesafe as you are going to get without being overkill, i.e. you can do scala.util.Try{ config.as[Option[Int]]("authorisation.timeout") } If you want and handle the error, but you are pretty much going to do something similar as the default behaviour anyways (i.e. the default fail at runtime if you provide a `String` instead of an `Int` but there isn't any other sane way to handle that problem apart from crashing the app and providing an error message saying you have the wrong type) tl;dr Runtime config libraries are going to fail at runtime Also the stance on pattern matching is highly surprising, its on of the most powerful and loved features of Scala. Akka is obviously untyped (not going to disagree with that), that more comes with actor model that it inherits though. Its not really untyped because they were being lazy or explicitly chose it to be untyped because they thought it was better
I have seen it plenty of times, in fact I personally looking at code that is scalaz/cats over-engineered right now (programmer admitted it, but he also said he was trying to learn the pure FP idiom). Also the amount of type boilerplate machinery to get things like monad transformers to work (particularly when you have a monad transformer stack with more than 3 types) kind of makes me question the merit of this style of programming. In more detail, having to define manual `lift`/`liftM`/`identity` helper type functions all over the place (this is even with the kind projector plugin enabled to help with cases of HKT inference) as well as having to define custom `object syntax` objects to make the code easier to work with at least in my opinion has probably killed any of the advantages of that style of programming. I kind of agree with Odersky on this one, that things like Monad transformers (which cats, and even more heavily scalaz emphasis) are a pretty terrible abstraction to reason with, and I have seen some really crazily overengineed "FP" style code as a result (I think one of our Scala projects where I work had an extreme example of this and is now considered to be recoded because few people can understand/work with it)
&gt; Syntax should look like syntax, IMO; I think that's a clearer way to say that than Java's `extends` This is where I think it's fair to agree to disagree. `&lt;:` strikes me as being more cryptic than `extends` does. But to some people it's a better choice. You say Scala can pack a lot into a single line. I'm guessing that's a plus for you. That's not necessarily a negative for me, but I don't find it to be an unqualified plus. To me, too much gets packed into a single line. Some people love that; some people don't. What I'm going to say I've said before, when this subject has come up. I think the clearest divide between programmers is how they react to the word *terse.*
&gt; There should be some specification as to what is the correct answer, or a note that can explain the answer is expected to differ by platform. This kind of upfront over-specification is the reason why Oracle never got anything done in terms of Java-to-JS or Java-to-native. In Scala.js deviations from the JVM behaviour are thoroughly documented with options to opt into more precise, but slower emulation of what the JVM does. These are often obscure, subtle differences which are subject to change. It's pointless trying to start specifying what different platforms have to do if we don't even know where the issues are. Scala.js'/Scala-Native's approach is exactly the right one. Focus on problems people actually have and make real existing code work, not indulge in producing paper with text on it. &gt; Please see the lload bytecode instruction. Didn't know that works for Longs without incurring any dereferencing and unboxing costs. &gt; Could there be a case of a cryptographic algorithm that assumes all references to Int and Long take the same time would be open to a timing attack due to that difference? Literally all code is open to timing attacks. You will need to use platform-specific utilities like manually writing assembly code or calling specific JS APIs to avoid that. Anything else is just playing defense against ongoing optimizer improvements in language runtimes.
True, there's TCO, but it's not just that. FP algorithms are mostly recursive algorithms, recursivity being used to express all sorts of loops and data-structures. And with a strict language it is very easy to end up with a recursive loop that doesn't finish, possibly building an infinite list or something, that will eventually crash with an OOM in the best case scenario. When I happen to develop such code in Scala, I always end up thinking about stuff like "will this suspend the evaluation or will it continue looping?". Applying FP concepts can also lead to very inefficient algorithms (e.g. because of applying map, filter, etc. ops on strict data-structures). My favorite example is that for selecting the first 10 elements of a list based on their ordering, the first instinct you have is to first sort it and then `take(10)`. Which is normally inefficient, but with a truly lazy sorting operation, this would have the complexity of Quickselect. Of course, space leaks do happen in Haskell and when they do, from what I understand they are hard to investigate. But memory leaks are also hard to investigate on the JVM as well, what helps us is the available tooling (e.g. YourKit Profiler) and I'm not convinced that the tooling for Haskell couldn't be better.
&gt; It doesn't help that cats/scalaz don't offer instances for Seq (IMO wrongly - and it's not like it's a general policy, they happily offer instances for Map rather than specific subtypes). scalaz technically does have a Foldable instance for Seq, but it's not imported by default. Or rather, there is one for scala.collection.Iterable in [scalaz.std.iterable.iterableSubtypeFoldable](https://oss.sonatype.org/service/local/repositories/releases/archive/org/scalaz/scalaz_2.11/7.1.3/scalaz_2.11-7.1.3-javadoc.jar/!/index.html#scalaz.std.iterable$). It's not imported by default because it would cause implicit ambiguity with things like List. But I have an implicit sitting around that is ```implicit val bufferFoldable: Foldable[mutable.Buffer] = iterableSubtypeFoldable[mutable.Buffer]``` so I don't see why the same couldn't be done with Seq. You'd still have to be careful about importing it though due to implicit ambiguity I think. 
Check out [FreeK](http://perevillega.com/freek-and-free-monads) maybe?
&gt; Significant amount of boilerplate(much more than with haskell) There are a few projects that attempt to reduce boilerplate involved, FreeK is one thats mentioned here often, but there's at least one more who's name escapes me. Perhaps a bit of shapeless can also help with that. I don't think there's any consensus yet on what less boilerplate should look like, or how to get there yet. &gt; Heavy compilation I have no idea what this means. &gt; Poor IDE support: Idea's editor constantly indicated errors here and there, yet everything was compiling. Idea's type aware highlighting is fundamentally broken and should be turned off, which why they have that icon in the status bar for doing just that. &gt; Although their signatures are essentially the same, they cannot be composed right away. You can use the inject typeclass and define both programs in term of a effect F[_] that contain HttpOp and LogOp, the order in which you define them won't matter. ie, def program[F[_]](ticketId: String)(implicit I: LogMoves[F], J: HttpMoves[F]): F[Unit] = ... 
I'd guess Twitter is probably bigger, I believe Morgan Stanley had a bunch, but yeah we're pretty high up there when it comes to number of scala programmers. 
So, if I understand this correctly, this is for getting bugfixes ahead of them being merged in the Lightspeed version?
I am not advocating for an upfront spec. I *am* advocating for some spec to be developed alongside the development of the other platforms. I *am* advocating that the platforms be ready to adapt to that standardization as it happens, even if it goes against the defaults of the underlying platform. Alternatively, they can be ready with an explicit and extensive list of differences. I *am* advocating that we draw clear lines between what is part of the language and what is part of the platform, balancing making the language more powerful by having more features standard vs the burden on the platforms to implement those features. I *am* advocating we avoid phrases like "Perfect Scala Compatibility" like /u/lihaoyi used in his recent First Principles blog post when there are known incompatibilities with the base JVM-entangled language spec and no platform independent language spec exists. I *am* advocating that if we're saying a platform is compatible with what developers use that were clear on what segment or developers we're talking about; the incompatibilities that are inconsequential to one could be of great importance to another.
Do you have some presentation or blog or something on how you work? How big are teams, what are challenges with FP, what would you recommend what not or so :) im sure lots of smaller scala shops would be interested. 
https://github.com/monixio/monix/blob/master/monix-eval/shared/src/main/scala/monix/eval/Task.scala#L113 If you squint a bit, this is a free monad, no? It's just you combined the interpreter with the algebra in the way you run Task. Cool stuff btw, I hope when we pull in the new design of Task/Free for Scalaz 8 (Alois Cochard is working on it), we can collaborate with you. 
&gt; Having each config hardcoded in your code base is also a bad idea, since in other to change some setting you would need to publish a new version of your artifact. That's fine - you'd have to do that for any code change. I believe in treating config like code as much as possible - i.e. make it part of the same release/test cycle as code is.
&gt; Yes you don't need to use sbt. You can even build your apps manually if you want to. But sbt is by far the most reliable build tool i have ever seen! I've not found a compelling advantage for SBT - maven is if anything more reliable. &gt; And in addition, It is extremely flexible. Indeed it is, but I would consider that a liability in a build system. &gt; Start with Futures. When you got the point, learn the actor model and them understand streaming libs and why back-pressure is so important. I'd actually recommend using streaming libs that will take care of backpressure for you second, and actors last - they're the lowest-level and hardest to use correctly.
Man... thanks... that is gold and butter :-)
nice!
Notes: https://gist.github.com/SethTisue/867380f2ad0ba3037e8fbe06fc6b62d1
The two most common and gentle ways to learn Scala are: * The Coursera Course https://www.coursera.org/learn/progfun1 * Martin Odersky's book https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687
:-) I love the new type-class encoding in Scato / Scalaz 8 btw. Alois Cochard and JB have been doing cool work there.
If you are volunteering go right ahead. Otherwise I'd rather have the Scala.js and Scala-Native devs work on things they deem important, because they have demonstrated that they have the right ideas.
&gt; The type itself lives in a different library which does not have scalaz, so the monoid cannot be defined there. Where do you define implicits for types which you did not write. Oh, in that case just on an `object` somewhere (or even a `package object`). But it should definitely be a singleton - it doesn't make sense for the monoid instance to have behaviour of its own. &gt; Like a Java date to a SQL implicit used throughout the codebase? FWIW I'd generally prefer to pimp a `.asSql`/`.asJava` method on rather than do a completely implicit conversion. Compare how the community has moved from using `JavaConversions` to `JavaConverters` for collections. &gt; The M gets taken as a type parameter on the methods that need to act on a monoid. Then i add an implicit conversion on that M making it a monoid. This sounds a little confused. In your other examples it looked like `M` was an ordinary (value) parameter, not a type parameter. You shouldn't ever need to implicitly convert something to a monoid - rather you define an implicit instance of the monoid typeclass, and then the implicit conversion to `MonoidOps` (or similar) will allow you to use monoid-related methods on your values.
God, that kind of stuff still makes me queasy. I think I would prefer there were just things that you could not express rather than having to read a line like that.
use the main method object HelloWorld { def main(args: Array[String]) { println("Hello, world!") } } `App` uses `DelayedInit` and can cause initialization issues. With the `main` method you know what's going on. object HelloWorld extends App { var a = 1 a + 1 override def main(args: Array[String]) { println(a) // guess what's the value of a ? } } [spoiler](/s"a = 0") 
What's the advantage of the 4 map specializations? Is there research that most maps are 4 or less and so they get some massive improvement with this change? Seems weird to have this framework level inconsistency 
I have started to always place typeclass instances to corresponding class's companion object, so there are no orphan instances... is it generally valid approach? Are there any drawbacks? I like it because it makes it simple where to look for implicits first and no import is required, with class you get all capabilities etc... It is quite different from kind-of-oop-ish layered architecture, where formatting would be probably last(presentation) layer with dependency to model kind-of-thing. Any thoughts?
The book "Programming in Scala" says: *"Similarly, the `scala.collection.immutable.Map()` factory method will return a different class depending on how many key-value pairs you pass to it, as shown in Table 17.4. As with sets, for immutable maps with fewer than five elements, a special class devoted exclusively to maps of each particular size is used, to maximize performance. Once a map has five or more key-value pairs in it, however, an immutable HashMap is used."* I'm sure there was thought behind these custom implementation, but I definitely don't have the complete story.
I went through the Norris slides and what he's saying makes sense. He finally reduces this: case class Person(name: String, age: Int) def getPerson(rs: ResultSet): Person = { val name = rs.getString(1) val age = rs.getInt(2) Person(name, age) } to this: case class Person(name: String, age: Int) get[Person] Fair enough. Again though, he started the discussion with "let's define an algebra", then moved on to "let's make it interpretable", and then voila! Out popped doobie, a brand new library which makes dealing with JDBC like was done above much cleaner. This is great stuff, if you want to write a library. Like, if you're going to write the next Hibernate level library, like Norris did. But at that point, comparing his two code samples is a strawman, because raw JDBC access without a library is going to suck in any language. So... now deal with this from the point of view of an application developer under a deadline. My next system is going to be a .NET web application that ties together multiple services provided by very large existing Java modules that do ungodly things in a stateful way to large free standing documents that get passed between the modules in a processing pipeline that has evolved over the last 12 or so years. In the process, I hope I get to use libraries as elegant as doobie, but really, ensuring the client is happy will have NOTHING to do with defining an algrebra and interpreting it. It will have everything to do with ensuring all the messy side effects that the FP community shun actually work correctly. TDD is our only hope is this environment. That's the reality of application developers everywhere. Maybe FP can help sort that mess out someday, but I suspect not because one can't write an algebra around something so... organic, to put it kindly.
5 upvotes and 121 comments? Wat? 
This reduces allocation pressure. A `scala.collection.immutable.HashMap` is constructed using an hash trie data structure (allocating many small objects), while maps with 1-4 values store their keys and values as fields. From a quick look at the source code, an immutable HashMap will allocate at least n + 1 objects to store n elements.
Actually I think this specializations may benefit a little. Guava (A famous Java libraries) did this and then reverted it because it offers awful performance in some cases due to megamorphic calls. (You can see details https://github.com/google/guava/issues/1268 and https://www.reddit.com/r/java/comments/4iznsp/openjdk_9_changeset_add_initial_compact_immutable/) When I read "Programming in Scala" I soon realized this problem, but when I deep dived and found `collection.immutable.Map` has 17 subclasses (Include `Map1`, `Map2`,`Map3`,`Map4`). So I guess removing these four classes may benefit nothing and remaining these classes could reduce memory sometimes though I haven't benchmarked it or have any strong evidence.
(A) represents a Type https://twitter.github.io/scala_school/type-basics.html , encasing it in parentheses has no significance to the best of my knowledge
I wouldn't use the word "can't". Certainly it takes time to create some sort of clean wrapper, it's difficult to magic that away. And if you're dealing with clients/management who believe in deadlines above all else, and never spare time for refactoring, code debt, etc., the code is probably going to end up looking horrible, no matter what. But if you're looking for actual case studies showing that free monads can be used in a pragmatic way to handle business logic, see https://www.youtube.com/watch?v=rK53C-xyPWw for one example (and they seem fairly happy with the results). 
They want three methods on three lines with try/catch, with a mutable state machine resource handler that they implicitly understand, not a one-liner.
`A =&gt; B` is a function from type `A` to type `B`. As i_darlington said, the parentheses around the input type are optional, so you could write `(A) =&gt; B`. If you have a function with more than one input argument ("of an arity greater than one"), you have to use parentheses, like `(A, B) =&gt; C`, a function from two arguments of type `A` and `B` respectively to `C`.
&gt; Is there an "easy" way to write effectful programs in a functional way? ...Significant amount of boilerplate(much more than with haskell) Isn't the obvious answer to your question "yes" if you're already wanking off to Haskell here? &gt; Is this pain essential to FP, to the way scala does FP Apparently not for most other people.
Yes, like that but without the Future wrapper. The reasoning for the singleton is to provide a fixed entry point for all DB access, and it there wasn't any good reason to make the caller instantiate an object to do so. If we start to have too many queries that this class becomes gross, we may have to refactor.
I think what you're seeing is the pattern found in all continuation monads, not just Free.
The approach works fine when writing applications if you've already paid the one-time cost to fully understand how to use it. Then you don't fall behind in your sprint. You can "know" a List is empty too but people still use NonEmptyList all the time because then they don't have to rely on callers to "know" and hope for the best.
why can't work together with lightbend to improve this instead of forking. Sorry I don't quite understand. Thanks.
&gt; Graphs are pretty easy however, compared to the advanced monadic and higher-kinded type stuff Just wanted to say that this is definitely not true. Graph problems are definitely not easy, being among the hardest problems in computer science. Quick, can you describe the algorithm for finding the shortest path between two nodes without looking it up? And no backtracking allowed, or otherwise it isn't practical. Dijkstra's algorithm is basic graph stuff. I know I couldn't do it without a refresher, even though I learned it since high-school when I was going to programming contests. And at least this problem has a known polynomial solution, but graphs are an endless source of NP-complete problems which still need optimal solutions and that's when the shit really hits the fan. But I can recite the Monad type and its laws by heart. The "advanced monadic and higher-kinded type stuff" is no more difficult than OOP along with the patterns in gang of four. You're basically talking about a way of achieving polymorphism as an alternative to subtyping (type-classes), HKT are of course a requirement for that, whereas the Monad is nothing more than a design pattern expressible as a type, much like Iterable.
&gt; Often I do want to. Or I might have some other way to store the config in a typed way (e.g. protobuf). I agree with offering the ability to parse an untyped source at runtime, but I'd like an interface to pass in a strongly typed config if I have one. Sure thing, but this is pretty orthogonal with the library itself. You can't blame typesafe config for being untyped when the whole goal of the library is to deal with system properties/environment variables (which are fed just as strings). Like I said before, there is https://github.com/iheartradio/ficus which gives you as much type guarantees as you can in that scenario &gt; True enough, but I've yet to see a compelling use case for the untypedness (particularly given that you usually have to go to the trouble of expressing your commands as values). You usually don't have control over this, and if you are deploying stuff in docker containers (and whatnot) then to configure things such as where your database points to, you do need environment variables. Even if you do hardcode or type these things, you are either shifting the problem somewhere else so you don't have to deal with it or if you truly hardcode it you have the scenario where you literally have to make separate builds for any environment or slight changes you want 
&gt; You can't blame typesafe config for being untyped when the whole goal of the library is to deal with system properties/environment variables (which are fed just as strings). Right, I blame libraries that use typesafe-config. Using typesafe-config in your application can make sense - parse the config into case classes (easy enough) and then use them for whatever, including configuring libraries (which don't need to have any dependency on typesafe-config themselves - the library config should just be a case class, and the application can generate that case class however it sees fit). But libraries that only offer an untyped interface to configure them are probably libraries that don't take type safety very seriously. &gt;&gt; True enough, but I've yet to see a compelling use case for the untypedness (particularly given that you usually have to go to the trouble of expressing your commands as values). &gt; You usually don't have control over this, and if you are deploying stuff in docker containers (and whatnot) then to configure things such as where your database points to, you do need environment variables. I actually meant actors rather than config in that part. &gt; if you truly hardcode it you have the scenario where you literally have to make separate builds for any environment or slight changes you want Yeah, that's my preferred approach. Far too many production issues I've seen have been caused by something that was "just a config change". Make all changes in code and then everything will go through the normal release process.
&gt; Personally I use monads (basically everything I work with is a Monad, i.e. Future,DBIO etc etc), but I don't use transformers, I just explicitly do the transformations myself (although sometimes I do make some helper functions) What do you mean by "the transformations"? As far as I know there aren't any transformations (the name makes no sense, but it wouldn't be the first FP construct for which that's the case). Use case example: say we want to do some user input validation (represented by `Invalid \/ ?`) that needs to access the database (`DBIO`) to check that things exist. To my mind it's worth using `EitherT[DBIO, Invalid, ?]` so that we can still use `for`/`yield` to compose the validation steps - it's clunky but less clunky than explicit nested maps at every step.
Thank you for being a polite user on reddit! --- *This bot was created by [kooldawgstar](http://reddit.com/u/kooldawgstar), if this bot is an annoyance to your subreddit feel free to ban it. [Fork me on Github](http://www.github.com/kooldawgstar/PoliteUsersBot) For more information check out /r/Polite_Users_Bot!*
Thank you for fully automatically and mindlessly assuming every post with a certain keyword is meant politely! But hey, it's the sentiment that counts. *This bot was created by [Spritetm](http://reddit.com/u/spritetm) For more information check out /r/Polite_Users_Bot_Bot!*
Nothing particularly bad, imho - `experimental` == `snapshot` so you never know :)
Scalatest3 still acts real weird in IntelliJ, I'm hoping IntelliJ works that out because it doesn't seem like Scalatest is doing anything super different.
In the end it comes down to language philosophy. Do you want a language that only kept adding things since Java 1.0 and deal with all the terrible things that have accumulated over time (but also enjoy the benefits of being able to use said code), or do you want a language that tries – in 2016 – to be the best language it can be in 2016, and – in 2017 – the best language it can be in 2017. I'd say roughly half of the complaints in the original blog have been addressed, and half of them are part of the nature of the language. If for instance being able to do things in multiple ways is an issue, then all multi-paradigm languages need to be avoided. That might get harder over time as most languages, including Java, adopt functional elements. While Java still has some advantages in IDE support in Eclipse/IntelliJ, Scala is closing that gap quickly, while also having much better support than Java in editors like Atom, Emacs, Sublime, Vim, ... Looking at other tooling, Scala is way ahead already with things like SBT, MiMa, SBT-Android, etc. In the end, use the right tool for the job, and if the job is being Java, Java will always be the best option.
I'm really looking forward to Scala 2.12, but the roadmap on the [website](http://www.scala-lang.org/news/2.12-roadmap) it pretty outdated. When is the planned release of 2.12 (final)? 
If you think a language is a "poor choice for mainstream development", then introducing it in your project, even for throwaway tests is a mistake, as usage of programming languages has a tendency to leak outside the intended original scope, plus it introduces a tooling and maintenance burden - now all of a sudden you need to worry about building and executing that code and the software developers need another IDE plugin and yes, to learn a new language, etc. Also, the phrase "*using the right tool for the job*", when it comes to programming languages, is in general bullshit. Yes you can have separate teams working with different programming languages, on different components; not a great idea, because it creates silos, as in people so specialized that they won't move around between components, but a good choice if you have those people already trained. Yes, you can have polyglots jumping around between components every once in a while. No, you cannot have the same people, working in multiple programming languages, at the same time. The context switching is too great and it's hard enough to assimilate the best practices and tools of one ecosystem, let alone many. And you really need to make up your mind. If Java is "good for mainstream development", then why isn't it for tests as well? Aren't tests mainstream?
I can't say anying specifically about the roadmap, but I wouldn't hold my breath, they seem to be facing some unexpected performance problems: http://scala-lang.org/blog/2016/07/08/trait-method-performance.html
I'm going to go further and claim that without some experience in implementing at least basic graph algorithms, then you don't know conceptually what a graph is. I also know how a house looks like, but don't know anything about building one. Furthermore you won't find many libraries for solving graph problems, for obvious reasons. On red-black trees, that's a very specific and non-default choice, and if you have to use one, you have to at least know the advantages versus a HashSet, or a splay tree, or a Patricia trie. Seriously, algorithms and data-structures are hard, the rabbit hole is deep and monads are way easier.
Genuine question, since scala is base in jvm doesn't it kind of share same tech burden as java for compatibility and is probably not the best language it can be in 2016?